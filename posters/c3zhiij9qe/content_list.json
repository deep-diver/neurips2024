[{"type": "text", "text": "VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guangyan Chen1 Meiling Wang1 Te Cui1 Yao Mu2 Haoyang Lu1 ", "page_idx": 0}, {"type": "text", "text": "Tianxing Zhou1 Zicai Peng1 Mengxiao Hu1 Haizhou Li1 Li Yuan3 Yi Yang1 ", "page_idx": 0}, {"type": "text", "text": "Yufeng Yue1 \u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Beijing Institute of Technology 2 The University of Hong Kong 3 Peking University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual imitation learning (VIL) provides an efficient and intuitive strategy for robotic systems to acquire novel skills. Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable performance in vision and language reasoning capabilities for VIL tasks. Despite the progress, current VIL methods naively employ VLMs to learn high-level plans from human videos, relying on pre-defined motion primitives for executing physical interactions, which remains a major bottleneck. In this work, we present VLMimic, a novel paradigm that harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos. Specifically, VLMimic first grounds object-centric movements from human videos, and learns skills using hierarchical constraint representations, facilitating the derivation of skills with fine-grained action levels from limited human videos. These skills are refined and updated through an iterative comparison strategy, enabling efficient adaptation to unseen environments. Our extensive experiments exhibit that our VLMimic, using only 5 human videos, yields significant improvements of over $27\\%$ and $21\\%$ in RLBench and real-world manipulation tasks, and surpasses baselines by over $37\\%$ in long-horizon tasks. Code and videos are available at our home page. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual Imitation Learning (VIL) has demonstrated remarkable efficacy in addressing various visual control tasks within intricate environments [1; 2; 3; 4; 5; 6; 7; 8; 9; 10]. Diverging from conventional approaches reliant on precise robot action labels, which often necessitates substantial human effort for data collection. Researchers increasingly turn to learning from human-object interaction videos that are easily accessible to reduce high data requirements. ", "page_idx": 0}, {"type": "text", "text": "Existing methods for skill acquisition leveraging video data can be broadly categorized into two classes. One typical approach learns efficient visual representations for robotic manipulation through self-supervised learning from large volumes of videos[11; 12; 13; 14; 15; 16; 17; 18; 19; 20? ]. Another approach focuses on learning task-relevant priors to guide robot behaviors or derive a heuristic reward function for reinforcement learning [21; 14; 21; 22; 23; 24; 25; 26; 27; 28; 29]. ", "page_idx": 0}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/24cfe468f13e6e67e3588b1af74dee5bbe3cf518055337824409b0fa6be38e47.jpg", "img_caption": ["(a) Typical VIL (b)VLM as Planner  (c) VLM as Visual Imitation Leaner for Fine-grained Actions (d) Success Rates Figure 1: Illustration of our VLMimic. (a) Typical VIL methods struggle to generalize to unseen environments, and (b) current methods naively utilize VLMs as planners, encounter difficulties in generating low-level actions. (c) VLMimic grounds human videos to obtain action movements, and learns skills with fine-grained actions, while the skill adapter updates skills for generalization. (d) Our method achieves superior performance given a limited collection of human videos. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, these approaches often encounter challenges when generalizing to unseen environments.   \nTherefore, efficiently acquiring generalizable skills from limited videos remains highly challenging. ", "page_idx": 1}, {"type": "text", "text": "An appealing prospect for handling this challenge is to employ large pretrained models by encapsulating extensive prior knowledge from broad data. Recent advances in vision-language models (VLMs) provide particularly promising tools in this regard, with their emergent and fast-growing conceptual understanding, commonsense knowledge, and reasoning abilities. However, current VIL methods [30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40] naively employ VLMs to learn high-level plans, and typically rely on a repertoire of pre-defined motion primitives. This reliance on individual skill acquisition is often considered a major bottleneck of the system due to the lack of large-scale robotic data. The question then arises: how can we leverage VLMs to learn even fine-grained action levels directly from human videos, eliminating the reliance on predefined primitives? ", "page_idx": 1}, {"type": "text", "text": "However, adapting VLMs to achieve visual imitation learning for fine-grained actions is non-trivial due to the following critical reasons: (I) Lack of fine-grained action recognition ability. Despite existing advancements in VLMs, they still struggle to recognize low-level actions in videos. To overcome this obstacle, a human-object interaction grounding module is proposed, which parses videos into multiple segments, and estimates object-centric actions for subsequent analysis. Such that the intricate low-level action recognition task is converted into the pattern reasoning task, which is more tractable for existing VLMs. (II) Difficulty for VLMs in understanding motion signals. Motion signals are characterized by inherent redundancy, hindering models from extracting valuable information. To overcome this challenge, we propose hierarchical constraint representations for VLM reasoning, which exhibit semantic constraints through visualized actions and illustrate geometric constraints using keypoint values. This representation effectively reduces redundancy and facilitates a comprehensive understanding, enabling our method to learn skills from a limited set of human videos. (III) Disparities in demonstration and target scenes. Demonstration and execution scenes may involve different objects and tasks, impeding direct skill transfer. To this end, we propose a skill adapter with an iterative comparison strategy, which updates skills by iteratively contrasting with the demonstrated knowledge, facilitating the adaptation of learned skills to unseen scenes. ", "page_idx": 1}, {"type": "text", "text": "Based on the above analysis, we present VLMimic, an approach that employs VLMs to directly learn even fine-grained action levels from a limited number of human videos, and generalize to novel scenes. As shown in Fig. 1, our method parses videos into multiple segments and captures object-centric movements using the human-object interaction grounding module. Then, a skill learner employing hierarchical constraint representations extracts knowledge from estimated motions, deriving skills with fine-grained actions. In unseen environments, a skill adapter with an iterative comparison strategy revises and updates the learned skills based on observations and task instructions. Extensive experiments demonstrate that VLMimic achieves strong performance across various scenes, utilizing only 5 human videos without requiring additional training. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions can be summarized as follows: (I) We propose VLMimic, a novel visual imitation learning framework empowered by VLMs, to learn generalizable robotic skills from human demonstration videos. VLMimic features a skill learner for knowledge extraction and a skill adapter for iterative skill refinement, enabling efficient skill acquisition and adaptation. (II) We build an effective human-object interaction grounding algorithm to enhance fine-grained action recognition capabilities, and propose hierarchical constraint representations for VLM reasoning to reduce information redundancy and facilitate comprehensive action comprehension. (III) Our method outperforms other methods by over $27\\%$ on the RLBench. In real-world manipulation tasks, VLMimic achieves an improvement exceeding $21\\%$ in seen environments and $34\\%$ in unseen environments. Moreover, VLMimic exhibits an improvement of over $37\\%$ in long-horizon tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Learning from Human videos ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Conventional learning approaches necessitate access to expert demonstrations, which include observations and precise actions for each timestep. Drawing on human capabilities, learning from observation offers efficient and intuitive methods for robots to develop new skills. A plethora of recent researches explore leveraging large-scale human video data to improve robot policy learning [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 41]. Representative methods, R3M [13] and MVP [12], which employ the internet-scale Ego4D dataset [11] to pretrain visual representations for subsequent imitation learning tasks. Another thread of work [21; 22; 23; 24; 25; 26; 27; 28; 29] focuses on learning task-relevant priors from videos to guide robot behaviors or derive a heuristic reward function for reinforcement learning. Learning by watching [27] learns human-to-robot translation, the resulting representations are used to guide robots to learn robotic skills. WHIRL [21] infers trajectories and interaction details to establish a prior, but it learns policy through real-world exploration and requires a large number of rollouts to converge. GraphIRL [24] performs graph abstraction on the videos followed by temporal matching to measure the task progress, and a dense reward function is employed to train reinforcement learning algorithms. Despite these advancements, acquiring generalizable skills efficiently from limited demonstration videos remains highly challenging. ", "page_idx": 2}, {"type": "text", "text": "2.2 Visual Imitation Learning with VLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Motivated by the notable success of VLMs across various domains, recent research [32; 33; 34; 35; 36; 37] investigate their potential in VIL. GPT-4V for Robotics [33] analyzes videos of humans performing tasks and outputs robot programs that incorporate insights into affordances. Digknow [32] distills generalizable knowledge with a hierarchical structure, enabling the effective generalization to novel scenes. Demo2code [37] generates robot task code from demonstrations via an extended chainof-thought and defines a common latent specification to connect the two. VLaMP [34] predicts visual planning from videos through video action segmentation and forecasting, handling long video history and complex action dependencies. However, these approaches often rely on predefined movement primitives or pre-trained skills to execute lower-level actions, thereby only partially solving the control stack. In contrast, our investigation aims to push these boundaries and learn all lower-level actions for the robot, eliminating the reliance on predefined primitives. ", "page_idx": 2}, {"type": "text", "text": "3 VLMimic ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Considering video demonstrations $\\boldsymbol{\\nu}$ of a human performing manipulation tasks, recorded using an RGB-D camera. The overall pipeline of VLMimic is illustrated in Fig. 2. Our method first grounds human videos, segmenting them into subtask intervals $\\{\\tau_{i}\\}_{i=1}^{V}$ and capturing object-centric interactions $\\boldsymbol{\\mathit{I}}$ . A skill learner with hierarchical representations then extracts knowledge from the obtained interactions, deriving skills with fine-grained actions. In unseen environments, a skill adapter employs an iterative comparison strategy to revise and update the learned skills based on observations and task instructions. ", "page_idx": 2}, {"type": "text", "text": "3.1 Human-object Interaction Grounding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Despite VLMs demonstrating proficiency in various vision tasks, they still struggle with fine-grained action recognition within videos. To mitigate this limitation, a four-stage process, illustrated in Fig. 3, ", "page_idx": 2}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/3a19a8df32bb7a6130cf37d0d09d7b511c8ffdf4449297f84a32952a81e3e0ba.jpg", "img_caption": ["", "(c) Skill Adapter with Iterative Comparison "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of our VLMimic. (a) The human-object interaction grounding module parses videos into multiple segments and captures object-centric movements. Then, (b) a skill learner extracts knowledge from action motions and derives skills. In novel scenes, (c) a skill adapter updates the learned skills to facilitate adaptation. ", "page_idx": 3}, {"type": "text", "text": "is utilized to extract object-centric interactions for skill learning, transforming this intricate problem into pattern reasoning problems, typically more tractable for existing VLMs. ", "page_idx": 3}, {"type": "text", "text": "Task recognition. Keyframes $\\kappa$ are intermittently extracted from videos $\\boldsymbol{\\nu}$ , vision foundation models VFM [42; 43; 44] are utilized to detect objects within these frames. Utilizing keyframes $\\kappa$ and textual detection results $\\mathbf{}T_{d}$ , VLMs are instructed to transcribe videos into task instructions $\\mathbf{\\boldsymbol{T}}_{t}$ , and compile the task-related objects $\\scriptstyle{T_{o}}$ into textual information. The object information is predicated on the initial frame of the video data, comprising a list of object names and their spatial relationships. The task recognition procedure is formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb T}_{d}=\\mathrm{VFM}({\\pmb{\\kappa}}),\\quad{\\pmb T}_{t},{\\pmb T}_{o}=\\mathrm{VLM}({\\pmb T}_{d},{\\pmb K}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Video parsing. Videos are parsed into segments $\\{\\tau_{i}\\}_{i=1}^{V}$ , using interaction markers that identify interaction periods. SAM-Track [45; 46; 47; 48; 49] predicts hand and task-related object masks for each frame, and corresponding point clouds $\\mathcal{P}$ are generated through back-projection. Markers are then identified by determining the interaction start time $\\pmb{t}_{i}$ and end time $\\scriptstyle t_{e}$ , partitioning videos $\\boldsymbol{\\nu}$ into multiple segments. Segments with hand motion trajectory lengths below than $\\gamma$ are filtered out, yielding final set of segments $\\{\\tau_{i}\\}_{i=1}^{V}$ . Concretely, the interaction markers are obtained as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd=\\operatorname{dist}(\\mathcal{P}),\\;\\;\\;t_{i}=\\{t|d^{t-1}\\!>\\!\\epsilon\\wedge d^{t}<\\epsilon\\},\\;\\;\\;\\;t_{e}=\\{t|d^{t-1}\\!<\\!\\epsilon\\wedge d^{t}>\\epsilon\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where function dist calculates the distance between any two point clouds. ", "page_idx": 3}, {"type": "text", "text": "Subtask recognition. Each segment $\\tau_{i}$ is analyzed by VLMs, which generate a subtask textual description ${\\mathbfcal{T}}_{\\tau_{i}}$ , and categorize the segment into grasping or manipulation phases based on the interacting entities and ${\\mathbf{}}T_{\\tau_{i}}$ . VLMs also identify master objects $O_{m}$ and slave objects $O_{s}$ . In the grasping phase, the agent performs a reach-and-grasp action targeting $O_{m}$ , designating the hand as $O_{s}$ . In the manipulation phase, the agent employs $O_{s}$ to interact with $O_{m}$ . ", "page_idx": 3}, {"type": "text", "text": "Object-centric interaction extraction. FrankMocap [50] and the Iterative Closest Point (ICP) algorithm [51; 52] are employed to derive precise hand pose trajectories, which are subsequently converted into robot gripper pose trajectories. Furthermore, BundleSDF [53] is employed for object reconstruction, and FoundationPose [54] is leveraged for object pose estimation based on reconstructed objects $^o$ . In grasping phases, interactions $\\boldsymbol{\\mathit{I}}$ are represented as grasp poses at handobject contacts. For manipulation phases, $\\boldsymbol{\\mathit{I}}$ are defined as trajectories of slave objects $O_{s}$ relative to master objects $O_{m}$ . This object-centric paradigm facilitates efficient skill acquisition and enables VLMimic to accommodate demonstrations across diverse viewpoints. ", "page_idx": 3}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/9f7558cbee9ffa6385f4896365e8bb91c74a8bac9f5541650247e3ab69eed558.jpg", "img_caption": [], "img_footnote": ["Figure 3: Illustration of Human-object interaction grounding module. (a) It recognizes tasks and related objects from human videos, (b) parses videos into multiple segments based on this information, and subsequently (c) identifies object-centric interactions within each segment. "], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Skill Learner with Hierarchical Representations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A straightforward approach for learning skills involves directly discerning the numerical trajectory patterns [55; 56]. However, VLMs face challenges in reasoning about inherently redundant motion signals, limiting their ability to extract valuable information. To reduce redundancy and foster comprehensive comprehension, hierarchical constraint representations are proposed for skill learning, as illustrated in Fig. 2. These representations exhibit semantic constraints via visualized interaction $\\textstyle I_{v}$ and further detail the fine-grained geometric constraints by integrating keypoint values $V_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "Learning with hierarchical constraint representations. Rendering interaction $\\boldsymbol{\\mathit{I}}$ , and textual notations $\\textstyle\\mathbf{T}_{n}$ on objects $o$ to derive visualized interaction $I_{v}$ , VLMimic facilitates reasoning capabilities to analyze semantic constraints $\\Phi_{s}$ by encouraging VLMs to attend to objects and their related actions, and integrating keypoint values $V_{k}$ and object properties $P_{o}$ (e.g., 3-D bounding boxes) to derive geometric constraints $\\Phi_{g}$ . Formally, constraints are learned as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I_{v}=\\operatorname{Render}([I,T_{n}],O),\\quad\\Phi_{s}=\\operatorname{S}_{1}(I_{v})\\quad\\Phi_{g}=\\operatorname{G}_{1}(\\Phi_{s},V_{k},P_{o}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{S}_{\\mathrm{l}}$ , and $\\mathrm{G}_{\\mathrm{l}}$ are functions to learn semantic, and geometric constraints, respectively. ", "page_idx": 4}, {"type": "text", "text": "(I) Grasping constraints. Inspired by task space regions (TSRs) [57], the grasping constraint $\\Phi_{g}$ can be approximated as a series of bounded regions $\\{\\pmb{\\mathcal{R}}_{i}\\}_{i=1}^{N_{C}}$ . Interactive grasp poses $\\boldsymbol{\\mathit{I}}$ are exhibited on objects, each associated with an index notation $\\textstyle{T_{n}}$ . These visualized interactions $\\scriptstyle{I_{v}}$ are presented to VLMs, leveraging their inherent knowledge and visual understanding ability to summarize semantic constraints $\\Phi_{s}$ and group these poses. Geometric constraints $\\Phi_{g}$ , represented as bounded regions, are derived by calculating ranges of grasp pose values $V_{k}$ within the same group, and associating them with object properties $P_{o}$ . This approach simplifies the complex task of constraint region generation into a series of visual understanding based multiple-choice question answering. Moreover, representing constraints through object properties enhances generalization across objects. ", "page_idx": 4}, {"type": "text", "text": "(II) Manipulation constraints. Interaction trajectory $\\boldsymbol{\\mathit{I}}$ is delineated on the master object $O_{m}$ , incorporating keypoints $V_{k}$ in the textual prompt. Semantic constraints $\\Phi_{s}$ are identified by VLMs based on the visualized interaction $\\textstyle I_{v}$ and subtask description ${\\mathbfcal{T}}_{\\tau_{i}}$ . Geometric constraints $\\Phi_{g}$ are then formulated based on semantic constraints $\\Phi_{s}$ , keypoint values $V_{k}$ , and object properties $P_{o}$ , expressing $\\Phi_{g}$ via the trajectory code. The code comprises two components: parameter estimation functions $f_{p}$ , which derives trajectory parameters from object properties, and trajectory generation functions $\\pmb{f}_{s}$ , employing estimated parameters to generate a sequence of slave object poses relative to the master object, promoting effective generalization across various objects and spatial configurations. ", "page_idx": 4}, {"type": "text", "text": "During execution, grasp candidates are uniformly sampled within the learned grasping constraints, and object-centric trajectories predicted from manipulation constraints are converted to end-effector trajectories in the world frame using each grasp candidate of the slave object and object poses of master and slave objects. The resulting end-effector trajectory candidates are evaluated using motion planner, such as OMPL [58], the trajectory with the highest fraction is selected. ", "page_idx": 4}, {"type": "text", "text": "Knowledge bank construction. A knowledge bank $_B$ is established to archive both high-level planning and low-level skill insights, storing knowledge with key-value pairs $(\\boldsymbol{k}_{i},\\boldsymbol{v}_{i})$ . High-level planning knowledge is indexed using task description $\\mathbf{\\boldsymbol{T}}_{t}$ as keys, paired with the consequent action sequence ${\\mathbf{}}T_{\\tau}$ as values. For low-level skill knowledge, keys are constituted by the object images and subtask description ${\\mathbfcal{T}}_{\\tau_{i}}$ , and values comprise reconstructed objects, as well as semantic constraints $\\Phi_{s}$ and geometric constraints $\\Phi_{g}$ representing learned skills. ", "page_idx": 5}, {"type": "text", "text": "3.3 Skill Adapter with Iterative Comparison ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Even though the skill learner exhibits efficient skill acquisition, the demonstration and execution scenes may differ in objects and tasks, impeding direct skill transfer to unseen environments. To mitigate these challenges, VLMs are instructed to adapt skills via an iterative comparison strategy, as depicted in Fig. 2. This approach updates learned skills by iteratively contrasting with the demonstrated knowledge, thereby enabling effective adaptation of retrieved skills to novel scenes. ", "page_idx": 5}, {"type": "text", "text": "High level planning. High-level planning knowledge ${\\mathbf{}}T_{\\tau}$ is retrieved from knowledge bank $_B$ based on the task instruction, which acts as the in-context example for VLMs, along with the scene observation. VLMs serve as a physically-grounded task planner [59; 60], generating a sequence of actionable steps and descriptions of task-related objects ${\\mathbf{}}T_{o}$ . ", "page_idx": 5}, {"type": "text", "text": "Iterative comparison. In each iteration, VLMs perform a comparative analysis between the adapted interaction $\\boldsymbol{\\mathit{I}}$ and retrieved interaction $\\hat{I}$ , subsequently updating the skill constraints $\\Phi_{s}$ and $\\Phi_{g}$ . This iterative process persists until either convergence is achieved or the maximum number of iterations $N_{I}$ is reached. This approach facilitates reasoning in VLMs by directing their attention to discrepancies, and enables VLMs to pinpoint the best available solution through an iterative process. The adapting procedure at the $i$ -th iteration can be formally represented as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{I}_{\\boldsymbol{v}}^{i}=\\mathrm{Render}(\\Phi_{g}^{i},\\mathcal{O}),\\quad\\Phi_{s}^{i+1}=\\mathrm{S}_{\\mathrm{a}}(\\hat{\\boldsymbol{I}}_{\\boldsymbol{v}},\\boldsymbol{I}_{\\boldsymbol{v}}^{i},\\hat{\\mathbf{g}}_{s},\\Phi_{s}^{i}),\\quad\\Phi_{g}^{i+1}=\\mathrm{G}_{\\mathrm{a}}(\\hat{\\Phi}_{g},\\Phi_{g}^{i},\\Phi_{s}^{i+1},V_{k},P_{o}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\Phi}_{g}$ and $\\hat{\\Phi}_{s}$ denote referential constraints, extracted from the knowledge base. The functions $\\mathrm{S_{a}}$ and $\\bar{\\mathrm{G_{a}}}$ adapt semantic and geometric constraints, respectively. ", "page_idx": 5}, {"type": "text", "text": "(I) Grasping constraint adaptation. As the grasping orientation is typically derivable from position constraints using grasping models [61; 62; 63], our work focuses on transferring position constraints. The visualized grasping position space is discretized into an $m\\times n$ grid $(m,n\\in Z)$ and annotated with textual notations $\\textstyle\\mathbf{T}_{n}$ , obtaining $\\boldsymbol{I}_{v}^{0}$ . VLMs are instructed to update semantic constraints $\\Phi_{s}$ , by contrasting with the referential interaction $\\hat{I}_{v}$ and semantic constraints $\\hat{\\Phi}_{s}$ , and to adapt geometric constraints $\\Phi_{g}$ by sampling $K$ outputs of grasping region selection. The updated $\\Phi_{g}$ are then visualized for the next iteration. The 3-D positional region is represented using two perspectives, and the consistency of the selected regions for the overlapping area validates the VLM outputs. The obtained constraints are expressed via object properties to enhance generalization. ", "page_idx": 5}, {"type": "text", "text": "(II) Manipulation constraint adaptation. VLMs are instructed to iteratively self-summarise and update manipulation constraints based on the task instruction and scene differences. VLMimic generates trajectories adhering to geometric constraints $\\Phi_{g}$ , which are exhibited on master objects. VLMs are instructed to analyze the deviation of the adapted interaction $\\textstyle I_{v}$ from the referential interaction $\\hat{I}_{v}$ to revise semantic constraints $\\Phi_{s}$ , and geometric constraints $\\Phi_{g}$ undergo refinement predicated on the updated $\\Phi_{s}$ , along with trajectory keypoint values $\\mathbf{}V_{k}$ and object properties $P_{o}$ . ", "page_idx": 5}, {"type": "text", "text": "Failure reasoning. Despite the ability of VLMs to generate effective constraints, environmental noise, such as trajectory estimation errors, impedes successful task execution. Thus, we leverage VLMs to detect and address failures during execution by providing them with perceptual results, such as object pose and robot end-effector trajectories, enabling autonomous failure identification and reasoning for rectification. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Baselines. VLMimic is compared with five representative methods: (1) R3M-DP that utilizes the pre-trained R3M visual representation [13] with the state-of-the-art (SOTA) diffusion policy [7]; (2) Diffusion Policy (DP) [7], a SOTA end-to-end policy method; (3) GraphIRL [24], a method that employs graph abstraction and learns reward functions for reinforcement learning (RL); (4) Code as Policy (CaP) [64], an LLM-driven method that re-composes API calls to generate new policy code; and (5) Demo2code [37], an LLM-driven planner method that translates demonstrations into task code. We modify it to integrate the analysis results from GPT-4V for Robotics [33], enabling it to transcribe videos into code. R3M-DP and DP are trained using the robot demonstrations with paired observation and action sequences. GraphIRL is trained in simulators with paired robot videos, Demo2code and our method learns skills with human videos in real-world experiments and robot videos in simulation experiments. ", "page_idx": 5}, {"type": "table", "img_path": "C3ZHiij9QE/tmp/53e2eeb8009eb270db33a29c3d012b2709c65d5108aa70d233116e4f1df7a265.jpg", "table_caption": ["Table 1: Success rates on RLbench. \"Obs-act\", \"Template\", and \"Video\" indicate paired observationaction sequences, code templates, and videos performing subtasks. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 Simulation Manipulation Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup. To assess our approach on challenging robotic manipulation tasks, the RLBench [65] benchmark is utilized for simulation tasks. Due to the unavailability of human videos in simulations, demo2code and our method utilize robot videos captured from a single-camera perspective during demonstrations, incorporating robot gripper trajectories. ", "page_idx": 6}, {"type": "text", "text": "Results. We investigate the capacity of VLMimic to acquire skills from a limited collection of video demonstrations, without requiring additional training. Our evaluation encompasses 12 manipulation tasks, as detailed in Table 1, demonstrating that our method surpasses all other methods in 11 out of these tasks. Our method, learned with only 5 human videos, obviously outperforms R3M-DP and DP by over $61\\%$ in overall performance, despite both being trained on 100 robot demonstrations. Compared to $\\mathrm{CaP}$ and demo2code, our method demonstrates an improvement exceeding $27\\%$ , highlighting the significant performance enhancements facilitated by the VLMimic framework. ", "page_idx": 6}, {"type": "text", "text": "4.2 Real-world Manipulation Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup. The real-world testing environment (E) is divided into \"seen\" (SE) and \"unseen\" (UE) categories. The \"seen\" category allows for testing in the environment where demonstrations were collected, whereas the \"unseen\" category involves testing in a distinct environment characterized by different objects and layouts. Success criteria are human-evaluated and the success rate is calculated from 10 randomized object positions and orientations. ", "page_idx": 6}, {"type": "text", "text": "Results: To validate the effectiveness of VLMimic in real-world settings, we conduct experiments involving 14 challenging real-world manipulation tasks selected from recent robotics research [66; 67; 4; 68]. Quantitative results, presented in Table 2, demonstrate that VLMimic clearly outperforms other methods across all tasks, particularly in the \"unseen\" environment (UE). VLMimic achieves an ", "page_idx": 6}, {"type": "table", "img_path": "C3ZHiij9QE/tmp/c063fef21bd7766fc417894e87a109958c0b4d1b6d22c4c7a84e0e6cc3ea900f.jpg", "table_caption": ["Table 2: Success rates on real-world manipulation experiments. \"Obs-act\", \"Template\", and \"Video\" indicate paired observation-action sequences, code templates, and videos performing subtasks. \"SE\" and \"UE\" are seen and unseen environments. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "C3ZHiij9QE/tmp/d5107de9c53bc795c85f5fecef8b6ca19c94f8b8f8895eae805b1366eed4525f.jpg", "table_caption": ["Table 3: Success rates on long-horizon tasks. \"Obs-act\", \"Template\", and \"Video\" indicate observationaction sequences, code templates, and videos performing tasks. "], "table_footnote": ["improvement exceeding $21\\%$ in SE and more than $34\\%$ in UE. Results reveal the outstanding ability of VLMimic to acquire skills from human videos and adapt them to unseen environments. "], "page_idx": 7}, {"type": "text", "text": "4.3 Real-world Long-Horizon Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental setup. Since baseline methods struggle to complete long-horizon tasks in the UE setting, experiments are conducted in the SE setting. All other experimental settings are consistent with those in the real-world manipulation task. ", "page_idx": 7}, {"type": "text", "text": "Results. The performance of VLMimic on long-horizon tasks is quantitatively evaluated by its successful completion of six distinct tasks, each comprising at least five subtasks. Experimental results, as depicted in Table 3, obviously exhibit a substantial enhancement achieved by our method over baseline methods. These outcomes suggest that the proposed method is capable of developing robust skills, thereby achieving promising performance in even long-horizon tasks. ", "page_idx": 7}, {"type": "text", "text": "4.4 Robustness against viewpoint variance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The keypoint-centric representation approach enables our method to tolerate different observational perspectives. To demonstrate the robustness of our method to varying viewpoints. Experiments are conducted in real-world unseen environments, utilizing distinct viewpoints, as shown in Figure 4, where the first angle serves as the default perspective used in our experiments. Experimental results shown in Table 4 prove that our method exhibits only a $7\\%$ fluctuation in performance under varying viewpoints, demonstrating the resilience of VLMimic to viewpoint changes. ", "page_idx": 7}, {"type": "table", "img_path": "C3ZHiij9QE/tmp/8a43c745a92b6d70e83caabfbd95ea11dce6700d3d59d6c1097a179f4a36aee8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/5f17c18ec17a5c439aa29e00745b3049a32c338e17e3d182e4e4da352e4ba4e4.jpg", "img_caption": ["Figure 5: Examples of failure cases. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Real-world failure cases ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 5 elucidates scenarios that present significant challenges for resolution through VLM reasoning. These scenarios encompass: (I) The task execution may exceed the hardware limitations of the physical robot, inducing inverse kinematics (IK) errors. (II) Incomplete environmental perception increases the risk of obstacle collisions, leading to task failure. Since the training datasets for VLMs exhibit a significant lack of data related to robot dynamics, these models lack associated knowledge, exhibiting a limited capacity for error analysis and struggling to infer correction strategies when confronted with these failures. ", "page_idx": 8}, {"type": "text", "text": "4.6 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comprehensive ablation studies are conducted to investigate the fundamental designs of our VLMimic approach. The effects of these design decisions are assessed by measuring the success rate on realworld manipulation tasks, which is computed across 10 randomized object positions and orientations. ", "page_idx": 8}, {"type": "text", "text": "Hierarchical constraint representations. Table 5 (a) analyzes three distinct constraint representations. Variants that exclusively reason semantic constraints or directly obtain geometric constraints without semantic analysis, lead to diminished performance. The results exhibit that hierarchical constraint representations enhance skill acquisition capabilities, demonstrating the pivotal role in facilitating the understanding and reasoning capabilities of VLMs. ", "page_idx": 8}, {"type": "text", "text": "Grasping learning. Table 5 (b) presents variants and their respective performance. The first variant utilizes VLMs for direct prediction of constraint region values, resulting in a significant performance decline. The second variant employs the DBScan clustering algorithm to group grasp poses and derive constraints as bounded regions. However, this method only considers numerical distributions without incorporating grasping common sense, leading to performance degradation. ", "page_idx": 8}, {"type": "text", "text": "Number of human videos. Table 5(c) presents an analysis of the impact of human video quantity on performance. Results indicate that our method attains high success rates on complex tasks with a single human video demonstration, and increasing the number of videos yields performance gains. The results show that our approach can efficiently learn generalizable skills from a limited number of human videos. We choose to use 5 demonstration videos to balance data availability and performance. ", "page_idx": 8}, {"type": "text", "text": "Comparison strategy. Table 5 (d) analyzes the impact of the comparison strategy in skill adapters. Variants compare constraints exclusively utilizing either visualized interactions or keypoints exhibit decreased success rates. Visual comparison facilitates semantic contrast in VLM, while keypoint values provide fine-grained geometric information. Experimental results illustrate that our strategy facilitates reasoning for both semantic and geometric constraint adaptation. ", "page_idx": 8}, {"type": "text", "text": "Number of iterations. We conduct an analysis on the impact of iteration count in skill adapter and search for the optimal choice, as shown in Table 5(e). Reducing the number of iterations to 0 results in a noticeable decrease in performance. Strong results are observed in the initial iteration, with modest improvements in subsequent iterations. The findings indicate that this iterative approach enhances the effectiveness of skill adaptation by enabling VLMs to identify the best available solution. For enhanced performance, 4 iterations are selected. ", "page_idx": 8}, {"type": "table", "img_path": "C3ZHiij9QE/tmp/3c6e1fcf0b0e83e9066d9a71abebde6b05005538913ceba012d5cd09244c0699.jpg", "table_caption": ["Table 5: Ablation experiments with VLMimic on real-world manipulation experiments. \"SE\" and \"UE\" are seen and unseen environments. Default settings are marked in gray . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Failure reasoning. The impact of failure reasoning is investigated in Table 5(f). The success rate exhibits an upward trend with increasing iterations, reaching an elbow point at 2 iterations, providing an optimal trade-off between real-time performance and success rate. Failure reasoning proves crucial for tasks demanding high-precision manipulation, which are susceptible to environmental noise. It enhances both the success rate and the robot\u2019s ability to operate in intricate environments. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present VLMimic, a novel approach that harnesses VLMs to learn skills with even fine-grained action levels from a limited number of human videos, and effectively generalize them to unseen environments. VLMimic first extracts object-centric interactions from human videos, and learns skills based on these interactions, using hierarchical constraint representations. In unseen environments, these skills are updated through an iterative comparison strategy. Extensive experiments conducted on various manipulation and challenging long-horizon tasks demonstrate the superior performance achieved by our VLMimic, utilizing only 5 human videos without requiring additional training, exhibiting strong skill acquisition and adaptation capabilities. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Despite the promising performance exhibited by VLMimic, current VLMs are still limited by inference latency and computational resource requirements. We believe that the progression of lightweight VLMs will mitigate these limitations. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ajay Mandlekar, Danfei Xu, Roberto Mart\u00edn-Mart\u00edn, Silvio Savarese, and Li Fei-Fei. Learning to generalize across long-horizon tasks from human demonstrations. arXiv preprint arXiv:2003.06085, 2020.   \n[2] Albert Tung, Josiah Wong, Ajay Mandlekar, Roberto Mart\u00edn-Mart\u00edn, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Learning multi-arm manipulation through collaborative teleoperation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 9212\u20139219. IEEE, 2021.   \n[3] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726\u2013747. PMLR, 2021.   \n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.   \n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.   \n[6] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.   \n[7] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.   \n[8] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models as adaptive self-evolving planners. arXiv preprint arXiv:2302.01877, 2023.   \n[9] Mingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, and Wei Zhan. Human-oriented representation learning for robotic manipulation. arXiv preprint arXiv:2310.03023, 2023.   \n[10] Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, and Ping Luo. Skilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16467\u201316476, 2024.   \n[11] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.   \n[12] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.   \n[13] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.   \n[14] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In Conference on Robot Learning, pages 654\u2013665. PMLR, 2023.   \n[15] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement learning with videos: Combining offline observations with interaction. arXiv preprint arXiv:2011.06507, 2020.   \n[16] Ashley D Edwards and Charles L Isbell. Perceptual values from observation. arXiv preprint arXiv:1905.07861, 2019.   \n[17] Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen Tian, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Learning predictive models from observation and interaction. In European Conference on Computer Vision, pages 708\u2013725. Springer, 2020.   \n[18] Annie S Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions from\" in-the-wild\" human videos. arXiv preprint arXiv:2103.16817, 2021.   \n[19] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. The International Journal of Robotics Research, 40(12-14):1419\u20131434, 2021.   \n[20] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In Conference on Robot Learning, pages 537\u2013546. PMLR, 2022.   \n[21] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. arXiv preprint arXiv:2207.09450, 2022.   \n[22] Maximilian Sieb, Zhou Xian, Audrey Huang, Oliver Kroemer, and Katerina Fragkiadaki. Graph-structured visual imitation. In Conference on Robot Learning, pages 979\u2013989. PMLR, 2020.   \n[23] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta. Third-person visual imitation learning via decoupled hierarchical controller. Advances in Neural Information Processing Systems, 32, 2019.   \n[24] Sateesh Kumar, Jonathan Zamora, Nicklas Hansen, Rishabh Jangir, and Xiaolong Wang. Graph inverse reinforcement learning from diverse videos. In Conference on Robot Learning, pages 55\u201366. PMLR, 2023.   \n[25] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning multi-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv:1912.04443, 2019.   \n[26] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1118\u20131125. IEEE, 2018.   \n[27] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samarth Sinha, and Animesh Garg. Learning by watching: Physical imitation of manipulation skills from human videos. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7827\u20137834. IEEE, 2021.   \n[28] Oier Mees, Markus Merklinger, Gabriel Kalweit, and Wolfram Burgard. Adversarial skill networks: Unsupervised robot skill learning from video. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 4188\u20134194. IEEE, 2020.   \n[29] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991\u20131002. PMLR, 2022.   \n[30] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et al. Robocodex: Multimodal code generation for robotic behavior synthesis. arXiv preprint arXiv:2402.16117, 2024.   \n[32] Guangyan Chen, Te Cui, Tianxing Zhou, Zicai Peng, Mengxiao Hu, Meiling Wang, Yi Yang, and Yufeng Yue. Human demonstrations are generalizable knowledge for robots. arXiv preprint arXiv:2312.02419, 2023.   \n[33] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. arXiv preprint arXiv:2311.12015, 2023.   \n[34] Dhruvesh Patel, Hamid Eghbalzadeh, Nitin Kamra, Michael Louis Iuzzolino, Unnat Jain, and Ruta Desai. Pretrained language models as visual planners for human assistance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15302\u201315314, 2023.   \n[35] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. arXiv preprint arXiv:2404.03384, 2024.   \n[36] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   \n[37] Yuki Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and Sanjiban Choudhury. Demo2code: From summarizing demonstrations to synthesizing code via extended chain-of-thought. Advances in Neural Information Processing Systems, 36, 2024.   \n[38] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding. Languagempc: Large language models as decision makers for autonomous driving. arXiv preprint arXiv:2310.03026, 2023.   \n[39] Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. arXiv preprint arXiv:2310.08582, 2023.   \n[40] Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, and Yanfeng Lu. Dag-plan: Generating directed acyclic dependency graphs for dual-arm cooperative planning. arXiv preprint arXiv:2406.09953, 2024.   \n[41] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, and Franziska Meier. Modelbased inverse reinforcement learning from visual demonstrations. In Conference on Robot Learning, pages 1930\u20131942. PMLR, 2021.   \n[42] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model. arXiv preprint arXiv:2306.03514, 2023.   \n[43] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao, Ying Shan, et al. Caption anything: Interactive image description with diverse multimodal controls. arXiv preprint arXiv:2305.02677, 2023.   \n[44] Ting Pan, Lulu Tang, Xinlong Wang, and Shiguang Shan. Tokenize anything via prompting. arXiv preprint arXiv:2312.09128, 2023.   \n[45] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[46] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[47] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[48] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[49] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023.   \n[50] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: Fast monocular 3d hand and body motion capture by regression and integration. arXiv preprint arXiv:2008.08324, 2020.   \n[51] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, volume 1611, pages 586\u2013606. Spie, 1992.   \n[52] Szymon Rusinkiewicz and Marc Levoy. Efficient variants of the icp algorithm. In Proceedings third international conference on 3-D digital imaging and modeling, pages 145\u2013152. IEEE, 2001.   \n[53] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas M\u00fcller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 606\u2013617, 2023.   \n[54] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. arXiv preprint arXiv:2312.08344, 2023.   \n[55] Yen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a robot to walk with large language models. arXiv preprint arXiv:2309.09969, 2023.   \n[56] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.   \n[57] Dmitry Berenson, Siddhartha Srinivasa, and James Kuffner. Task space regions: A framework for poseconstrained manipulation planning. The International Journal of Robotics Research, 30(12):1435\u20131460, 2011.   \n[58] Ioan A Sucan, Mark Moll, and Lydia E Kavraki. The open motion planning library. IEEE Robotics & Automation Magazine, 19(4):72\u201382, 2012.   \n[59] Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Al\u00e1n Aspuru-Guzik, and Animesh Garg. Replan: Robotic replanning with perception and language models. arXiv preprint arXiv:2401.04157, 2024.   \n[60] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023.   \n[61] Hao-Shu Fang, Chenxi Wang, Minghao Gou, and Cewu Lu. Graspnet-1billion: A large-scale benchmark for general object grasping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11444\u201311453, 2020.   \n[62] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 2023.   \n[63] Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, and Huazhe Xu. Robo-abc: Affordance generalization beyond categories via semantic correspondence for robot manipulation. arXiv preprint arXiv:2401.07487, 2024.   \n[64] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500. IEEE, 2023.   \n[65] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.   \n[66] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.   \n[67] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint arXiv:2211.11736, 2022.   \n[68] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023.   \n[69] Peize Sun, Shoufa Chen, Chenchen Zhu, Fanyi Xiao, Ping Luo, Saining Xie, and Zhicheng Yan. Going denser with open-vocabulary part segmentation. arXiv preprint arXiv:2305.11173, 2023.   \n[70] Peize Sun, Shoufa Chen, and Ping Luo. Grounded segment anything: From objects to parts. https: //github.com/Cheems-Seminar/grounded-segment-any-parts, 2023.   \n[71] Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph J\u00e4hne, Lukas Hausperger, and Simon Haddadin. The franka emika robot: A reference platform for robotics research and education. IEEE Robotics & Automation Magazine, 29(2):46\u201364, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In human-object interaction grounding module, the Tokenize Anything [44] model is employed during task recognition to improve fine-grained scene understanding ability. The textual detection results are integrated using VLMs to generate concise task descriptions and detailed object information. The videos are segmented using a threshold $\\epsilon$ of $2c m$ . Segments with hand motion trajectory lengths below $\\gamma=10c m$ are discarded. During the grasping constraint learning phase, the number of regions $N_{c}$ is automatically determined by the VLMs. In manipulation constraint learning, keypoints are obtained by uniformly sampling 10 points. For the skill adapter, the maximum number of iterations is set to $N_{I}=4$ . During grasping constraint adaptation, visualized grasping position space is discretized into a $5\\times5$ grid, with $K=4$ outputs sampled per iteration. ", "page_idx": 14}, {"type": "text", "text": "During skill execution, the pretrained Grounded-segment-any-parts model [69; 70] is used to generate segmentation maps of queried objects or parts. These segmentation maps are then utilized to predict object-centric pose sequences using codes generated by VLMs. FoundationPose [54] is employed to track object poses, transforming the object-centric poses into the world frame. The robotic arm\u2019s motion planning is facilitated by the integration of the MoveIt module, renowned for its comprehensive motion planning capabilities, and the OMPL [58] (Open Motion Planning Library), which offers a suite of advanced algorithms for efficient path planning and obstacle avoidance. Upon action completion, the real-time object positions are used to assess task success until manual confirmation or a preset time is reached. In case of failure detection, object and gripper poses are employed for failure reasoning, where the gripper poses are estimated using the attatched QR scan. ", "page_idx": 14}, {"type": "text", "text": "B Experimental Setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Baseline setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "R3M-DP [13] and DP [7] employ a CNN-based network architecture for its robustness across diverse tasks. These methods are trained on robot demonstrations using default hyper-parameters, robot demonstrations consist of paired observation and action sequences. ", "page_idx": 14}, {"type": "text", "text": "To ensure that GraphIRL [24] is trained and tested under the same scenario in the SE setting. GraphIRL is trained in the simulator with corresponding paired robot videos, and the SE results are obtained from the same simulated environments, while UE results are acquired from real-world scenarios under the UE setting. Since the original GraphIRL method struggle to learn the gripper switch information, we additionally provide this information to GraphIRL. ", "page_idx": 14}, {"type": "text", "text": "Following Cap [64], the primitives for Cap [64] and demo2code [37] include: move to pos, rotate by quat, set vel, open gripper, close gripper, pick obj, and place at pos. Cap employs natural language instruction directly for reasoning, Demo2code generates code from textual video analysis results provided by GPT-4V for Robotics [33], a video analysis approach for robotics, enabling demo2code to learn from human videos. Specifically, the detailed task analysis results and affordance analysis outcomes from GPT-4V for Robotics are incorporated as contextual information within the textual prompt for demo2code. ", "page_idx": 14}, {"type": "text", "text": "B.2 Real-world experimental setup ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experiments are conducted on Franka Emika [71], employing three RGB-D cameras (ORBBEC Femto Bolt) for environment observation: one at the top right of the table, one at the top left, and one mounted on the robot\u2019s wrist. All cameras start recording and return real-time RGB-D observations at a frequency of $30\\,\\mathrm{Hz}$ . All experiments are evaluated on an Intel i7-10700 CPU with an RTX 3090 graphics card. ", "page_idx": 14}, {"type": "text", "text": "C Details of the long-horizon task designs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The definition of our long-horizon tasks is listed below. For each task, the initial state and subgoals are pre-defined. The whole task is completed if and only if all subgoals are completed in the correct order. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Make a pie ", "page_idx": 15}, {"type": "text", "text": "\u2013 Initial state: On the table, there is a bowl filled with sauce, a pan containing pie, a brush placed on the shelf, and a microwave.   \n\u2013 Criteria: Brush the sauce on the pie and put the pie in the microwave to heat up.   \n\u2013 Subgoals: (1) Grasp the bowl; (2) Pour sauce from the bowl to the pie; (3) Grasp the brush; (4) Spread sauce; (5) Open the microwave; (6) Place the pan in the microwave; (7) Close the microwave; (8) Turn on the microwave. ", "page_idx": 15}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/d129500b4c0f67cfacb91738be5896825083740ff2e32e871099de14bf3f5609.jpg", "img_caption": ["Figure 6: Visualization of the make-a-pie task. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "\u2022 Wash pan ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2013 Initial state: The pan that needs to be washed is located on the left side of the table, the pan rack and brush are on the right side of the table, and the sink and faucet are in the middle of the table.   \n\u2013 Criteria: Rinse the pan with water, scrub it with a brush, and place the pan on the rack.   \n\u2013 Subgoals: (1) Place the pan in the sink; (2)Align the faucet with the pan; (3) Turn on the faucet; (4) Turn off the faucet; (5) Place the pan on the table; (6) Wipe the pan with a brush; (7) Place the pan on the pan rack. ", "page_idx": 15}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/0b83d7aa8a11f8a328e3a80e310fde66d14cf12d245eae1393308e791bee621c.jpg", "img_caption": ["Figure 7: Visualization of the wash-pan task. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "\u2022 Make cucumber slices (Make slices) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2013 Initial state: The refrigerator is to the left of the table, the cutting board is on the shelf to the right of the table, next to which is a knife inserted into the knife rack. \u2013 Criteria: Take the cucumber out of the refrigerator and cut it with a knife. \u2013 Subgoals: (1) Place the cutting board on the table; (2) Open the refrigerator; (3) Place the cucumber on the cutting board; (4) Close the refrigerator; (5) Remove the knife from the knife rack; (6) Cut the cucumber; (7) Place the knife back on the knife rack. ", "page_idx": 16}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/152443de582549f102a3481cef8a27d620c4b1056b21eb8675412532a5e4fca3.jpg", "img_caption": ["Figure 8: Visualization of the make-cucumber-slices task. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "\u2022 Make coffee ", "page_idx": 17}, {"type": "text", "text": "\u2013 Initial state: The coffee machine and capsules are placed on the tabletop, with the capsule chamber placed on the cup.   \n\u2013 Criteria: place the coffee capsule into the capsule chamber, insert it into the coffee machine, place a cup under the coffee machine\u2019s dispenser, and finally turn on the coffee machine.   \n\u2013 Subgoals: (1) Grasp the coffee capsule; (2) Place the coffee capsule in the capsule chamber; (3) Grasp the capsule chamber; (4) Insert the capsule chamber into the coffee machine; (5) Place the cup under the coffee machine\u2019s water outlet; (6) Grasp the lever; (7) Turn on the coffee machine. ", "page_idx": 17}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/945b19328fd7c36c90c9435a755bcef83777eaff3565b2342da8b946a3008343.jpg", "img_caption": ["Figure 9: Visualization of the make-coffee task. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "\u2022 Clean table ", "page_idx": 17}, {"type": "text", "text": "\u2013 Initial state: Bananas, mangoes, cups, and paint brushes are scattered on the table. In addition, there is a drawer, a plate, and a dust brush on the table.   \n\u2013 Criteria: Put the fruits (banana and mango) back into the plate, and put the tools (cup and paint brush) back into the drawer, and sweep the tabletop with the dust brush.   \n\u2013 Subgoals: (1) Place the banana on the plate; (2) Place the mango on the plate; (3) Open the drawer; (4) Place the brush in the drawer; (5) Place the cup in the drawer; (6) Close the drawer; (7) Grasp the brush; (8) Sweep the tabletop. ", "page_idx": 17}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/ecb0beee84ce690c649f0bca04205a760a058099188dffe2e24742b8c5312e6b.jpg", "img_caption": ["Figure 10: Visualization of the clean-table task. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.3 Chemistry Lab ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 Chemistry experiments (Chem. exp.) ", "page_idx": 18}, {"type": "text", "text": "\u2013 Initial state: On the desktop, there are two beakers, two conical flasks, a test tube rack equipped with a test tube, along with a retort stand fitted with a funnel. \u2013 Subgoals: (1) Place conical flask A under the funnel. (2) Pour the contents of the test tube into beaker A. (3) Pour the contents of beaker A into conical flask B. (4) Pour the contents of beaker B into conical flask B. (5) Shake the mixed solution in conical flask B. (6) Pour the contents of conical flask B into the funnel. ", "page_idx": 18}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/e9e029b9a69ebf5ce2fd96fa22ae626bd597c950552b7f7e7994c8ee091ec323.jpg", "img_caption": ["Figure 11: Visualization of the Chemistry Lab task. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Visualization of experimental results ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/837aac686d326af2902e21c277a37850acfecda37a18dfa208d866b71e946707.jpg", "img_caption": ["Figure 12: Manipulated objects in SE setting. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/184fcbbf883bb46447f197836e0fe55e109936e19b0b90d89351b31e31b50eed.jpg", "img_caption": ["Figure 13: Manipulated objects in US setting. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/10603d2cdd235d71b0e072774fcbdf9a867df51b77437cb6bdea573405cfcc6e.jpg", "img_caption": ["Figure 14: Visualization of manipulation task results in seen environments. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "C3ZHiij9QE/tmp/e45e55798eeb39a8be710650f7948034d36cc88851ceb99c8f67ef3ad944050d.jpg", "img_caption": ["Figure 15: Visualization of manipulation task results in unseen environments. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: please refer to Sec. 5 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve theoretical derivations. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Experimental setup please refer to Sec. 4 and Appendix B, and implementation details please refer to Appendix A ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code and data will be made pubilcly accessible. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Experimental setup please refer to Sec. 4 and Appendix B, and implementation details please refer to Appendix A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: please refer to Sec. 4 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: please refer to Appendix B.2 ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research presented in this paper adheres to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not foresee obvious undesirable ethical or social impacts at this moment. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve the release of data or models that have a high risk for misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The creators and original owners of assets used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not introduce any new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experiments and research in this paper do not involve human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experiments and research in this paper do not involve human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]