[{"figure_path": "Wq6aY6fC2H/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of the neural regression collapse. The red dots represent the sample features, the blue arrows represent the row vectors of the last layer weight matrix, and the yellow plane represents the plane spanned by the principal components of the sample features. Here the target dimension is n = 2. The feature vectors and weight vectors collapse to the same subspace. The angle between the weight vectors takes specific forms governed by the covariance matrix of the targets.", "description": "This figure visualizes the neural regression collapse phenomenon.  Before the collapse (left panel), the sample features (red dots) are scattered and not aligned with the principal components (yellow plane) or the weight vectors (blue arrows). After training and the collapse (right panel), the feature vectors and the weight vectors have collapsed into the same subspace spanned by the principal components of the feature vectors.  This subspace is two-dimensional (n=2) in this example. The angles between the weight vectors are now determined by the covariance matrix of the target variables.", "section": "1 Introduction"}, {"figure_path": "Wq6aY6fC2H/figures/figures_5_1.jpg", "caption": "Figure 2: Prevalence of NRC1-NRC3 in the six datasets. Train/Test MSE and the coefficient of determination (R2) are also shown.", "description": "This figure shows the training and testing performance across six different datasets for a neural multivariate regression task.  The metrics displayed include training and testing mean squared error (MSE), R-squared (R2), and the three components of Neural Regression Collapse (NRC1, NRC2, NRC3).  NRC1-3 values close to zero indicate the presence of neural collapse. The plot shows that as the training progresses, both training and testing errors decrease, while the R-squared increases and approaches 1, suggesting improved model performance.  The near-zero NRC1-3 values across all datasets demonstrate a high prevalence of neural regression collapse in the examined scenarios.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_5_2.jpg", "caption": "Figure 3: Explained Variance Ratio (EVR) for the first 5 principal components (PC).", "description": "This figure displays the Explained Variance Ratio (EVR) for the top 5 principal components (PCs) of the feature matrix H during training, across six different datasets.  The EVR represents the proportion of variance in the data explained by each PC. The figure demonstrates that for all datasets, after a short training period, a significant amount of variance is captured by the first *n* principal components, where *n* is the target dimension. This observation strongly supports the claim of feature vector collapse to an *n*-dimensional subspace. In contrast, the variance for other principal components remains very low or zero. This visualization adds strong empirical support to the claim of neural regression collapse (NRC).", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_6_1.jpg", "caption": "Figure 4: The optimal value of \u03b3 for NRC3.", "description": "This figure shows the optimal value of \u03b3 for NRC3 (Neural Regression Collapse 3) across four different datasets: Reacher, Swimmer, Hopper, and Carla 2D.  NRC3 is a measure of the alignment between the last-layer weight vectors and the target covariance matrix, and its optimal value indicates the degree of collapse observed. The x-axis represents \u03b3/\u03bbmin, where \u03b3 is a constant and \u03bbmin is the minimum eigenvalue of the target covariance matrix. The y-axis shows the value of NRC3.  The plots reveal dataset-specific optimal values for \u03b3, demonstrating the diverse nature of neural collapse in regression tasks.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_6_2.jpg", "caption": "Figure 7: Train/Test MSE, R2, and NRC1-3 under different weight decays for MuJoCo datasets.", "description": "This figure presents the training and testing Mean Squared Errors (MSE), the coefficient of determination (R2), and the three metrics for Neural Regression Collapse (NRC1-3) across three MuJoCo datasets (Reacher, Swimmer, Hopper) under varying weight decay values (AWD).  Each dataset is displayed as a column of subplots, showing how the training and testing performance, as well as the extent of neural collapse (NRC1-3), changes with different levels of weight decay. The results demonstrate that the extent of neural collapse is influenced by the weight decay parameter. With larger weight decay values, the collapse (NRC1-3) is generally more pronounced, while smaller values or no weight decay lead to less pronounced collapse.", "section": "3.2 Experimental validation of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_9_1.jpg", "caption": "Figure 2: Prevalence of NRC1-NRC3 in the six datasets. Train/Test MSE and the coefficient of determination (R2) are also shown.", "description": "This figure shows the results of the experiments on six different datasets (Swimmer, Reacher, Hopper, Carla 1D, Carla 2D, UTKFace). For each dataset, it displays the training and testing mean squared error (MSE), the coefficient of determination (R2), and the three metrics of Neural Regression Collapse (NRC1, NRC2, NRC3) over the training epochs. The figure demonstrates that as training progresses, the training and testing errors decrease, and the R2 values increase, indicating that model performance stabilizes. More importantly, this figure shows the convergence of NRC1, NRC2, and NRC3 towards zero for all six datasets, confirming the prevalence of neural regression collapse across various datasets and tasks.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_16_1.jpg", "caption": "Figure 2: Prevalence of NRC1-NRC3 in the six datasets. Train/Test MSE and the coefficient of determination (R2) are also shown.", "description": "This figure shows the results of six different experiments on six different datasets.  Each experiment trained a deep neural network model for a multivariate regression task. The plots show how training and testing mean squared error (MSE) and the R-squared (R2) value change as the number of training epochs increases.  It also shows how three metrics related to neural regression collapse (NRC1, NRC2, and NRC3) change with increasing epochs. The three NRC metrics measure aspects of the collapse of the last layer feature vectors and weight vectors.  The results are consistent across all six datasets.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_17_1.jpg", "caption": "Figure 8: Train/Test MSE, R<sup>2</sup>, and NRC1-3 under different weight decays for Carla and UTKFace datasets.", "description": "This figure presents the training and testing mean squared errors (MSE), the coefficient of determination (R<sup>2</sup>), and the three Neural Regression Collapse (NRC) metrics (NRC1, NRC2, NRC3) for the CARLA (2D and 1D versions) and UTKFace datasets.  Different lines represent different values for the weight decay parameter (AWD), ranging from 0 to 1e-1.  The results show the effect of varying the weight decay hyperparameter on the convergence speed and the extent of neural collapse observed during training.  The goal is to demonstrate that neural regression collapse is still observed across various datasets even when the weight decay is small.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_18_1.jpg", "caption": "Figure 2: Prevalence of NRC1-NRC3 in the six datasets. Train/Test MSE and the coefficient of determination (R2) are also shown.", "description": "This figure displays the results of the neural regression collapse experiment conducted on six different datasets.  For each dataset, it shows the training and testing mean squared error (MSE), the R-squared (R2) value which represents the goodness of fit, and three metrics (NRC1, NRC2, NRC3) that quantify the degree of neural regression collapse.  The x-axis represents the number of epochs (training iterations), while the y-axis shows the values of the metrics. The figure demonstrates that across all six datasets, neural regression collapse (NRC) occurs, as evidenced by the convergence of NRC1, NRC2, and NRC3 to zero during training.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/figures/figures_19_1.jpg", "caption": "Figure 10: Comparison of the norms of W and H with fixed \u03bbw and varying \u03bbH. The columns from left to right represent the model\u2019s average feature norm and the norms for w\u2081 and w\u2082, respectively.", "description": "This figure shows the impact of varying the regularization parameter \u03bbH on the norms of the weight matrix W and the feature matrix H, while keeping \u03bbw constant. The norms are plotted against the number of training epochs for the Reacher and Swimmer datasets.  Each subfigure shows the norm of the feature vectors (H) and the norms of the weight vectors (w1 and w2).  The plots demonstrate how changing \u03bbH affects the final norms of the features and weights after training.", "section": "Connection to whitening"}, {"figure_path": "Wq6aY6fC2H/figures/figures_19_2.jpg", "caption": "Figure 11: Residual errors \u025b(2) versus \u025b(1) for both the randomly initialized model and the trained model after convergence on the Reacher dataset. The color of the points indicates the ratio z(2)/z(1).", "description": "This figure visualizes the residual errors of a 2-dimensional regression task on the Reacher dataset.  The left panel shows the residual errors for a randomly initialized model, while the right panel shows the errors after the model has converged through training.  Each point represents a data sample, with its color indicating the ratio of the second and first standardized target components (z(2)/z(1)).  The plot demonstrates that after training, the residual errors become uncorrelated and resemble white noise, aligning with the theoretical predictions made earlier in the paper.", "section": "4.2 One-dimensional univariate case"}]