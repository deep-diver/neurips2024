[{"figure_path": "Wq6aY6fC2H/tables/tables_4_1.jpg", "caption": "Table 1: Overview of datasets employed in our neural regression collapse analysis.", "description": "This table summarizes the six datasets used in the neural regression collapse analysis.  It shows the dataset name, the size of the dataset (number of data points), the input type (raw state or RGB image), the target dimension (n, which indicates the number of continuous values being predicted), the target correlation (correlation coefficients between the target components for datasets with multiple target dimensions), and the minimum eigenvalue (\u03bbmin) of the covariance matrix of the target vectors.  The target dimension and target correlations are key indicators for understanding the nature of the multivariate regression problems,  and \u03bbmin gives insight into the overall distribution of target data and will play a role in the mathematical explanation of neural collapse.", "section": "3 Prevalence of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/tables/tables_13_1.jpg", "caption": "Table 2: Hyperparameter settings for experiments with weight decay on MuJoCo datasets.", "description": "This table details the hyperparameters used in the experiments conducted on the MuJoCo datasets (Swimmer, Reacher, and Hopper).  It specifies settings for the model architecture (number of hidden layers, hidden layer dimension, activation function, number of linear projection layers), training process (epochs, batch size, optimizer, learning rate, weight decay), experimental setup (seeds), and compute resources (CPU, number of workers, memory, approximate execution time).  The values given are specific to each dataset for optimal performance.", "section": "3.2 Experimental validation of neural regression collapse"}, {"figure_path": "Wq6aY6fC2H/tables/tables_14_1.jpg", "caption": "Table 3: Hyperparameters of ResNet for Carla and UTKface datasets.", "description": "This table lists the hyperparameters used for training the ResNet models on the CARLA and UTKFace datasets.  It shows the architecture details, training settings, computational resources used, and the approximate execution time. This information is essential for understanding the experimental setup and reproducing the results. The architecture section includes the backbone of hidden layers, last layer dimension, and final layer activation function.  The training section includes epochs, batch size, optimizer, momentum, learning rate, multistep gamma, seeds, compute resources, number of compute workers, requested memory and approximate average execution time.", "section": "A.2 CARLA and UTKface"}]