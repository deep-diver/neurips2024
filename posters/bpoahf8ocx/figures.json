[{"figure_path": "bPOaHf8OcX/figures/figures_1_1.jpg", "caption": "Figure 1: The proposed Vivid-ZOO generates high-quality multi-view videos of a dynamic 3D object from text. Each row illustrates six frames drawn from a generated video for one viewpoint.", "description": "This figure shows example results of multi-view video generation using the proposed Vivid-ZOO model.  Each row represents a different viewpoint, showing six frames from a generated video. The text prompt used is displayed below the image.  This illustrates the model's ability to generate temporally coherent and spatially consistent videos from a textual description.", "section": "1 Introduction"}, {"figure_path": "bPOaHf8OcX/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of the proposed Vivid-ZOO. Left: Given a text prompt, our diffusion model generates multi-view videos. Instead of training from scratch, the multi-view spatial module reuses the pre-trained multi-view image diffusion model, and the multi-view temporal module leverages the 2D temporal layers of the pre-trained 2D video diffusion model to enforce temporal coherence. Right: Jointly reusing the pre-trained multi-view image diffusion model and temporal 2D layers poses new challenges due to the large gap between their training data (multi-view images of synthetic 3D objects versus real-world 2D videos). We introduce 3D-2D alignment and 2D-3D alignment to address the domain gap issue.", "description": "This figure illustrates the architecture of the Vivid-ZOO model, a diffusion-based pipeline for generating multi-view videos from text.  The left side shows the workflow: a text prompt is fed into a CLIP model, the output is combined with camera pose information and passed through the multi-view spatial and temporal modules. These modules reuse pre-trained models (a multi-view image diffusion model and a 2D video diffusion model), connected via alignment layers to bridge the domain gap between their training data. The right side highlights the domain gap problem (synthetic 3D object data vs. real-world 2D video data) and how the alignment layers help solve it.", "section": "3 Multi-view video diffusion model"}, {"figure_path": "bPOaHf8OcX/figures/figures_5_1.jpg", "caption": "Figure 3: Our multi-view temporal module, where 3D-2D alignment layers are trained to align features to the latent space of the 2D temporal attention layers, and the 2D-3D alignment layers project them back.", "description": "This figure shows the architecture of the multi-view temporal module used in the Vivid-ZOO model.  It leverages pre-trained 2D temporal layers from a video diffusion model to ensure temporal coherence in the generated multi-view videos.  However, to bridge the domain gap between the pre-trained multi-view image diffusion model and the 2D video model, two alignment layers (3D-2D and 2D-3D) are introduced. The 3D-2D alignment layer maps features to the latent space of the 2D temporal layers, and the 2D-3D alignment layer projects the features back to the feature space of the multi-view model, effectively combining the two models while preserving their learned features.", "section": "3.2 Multi-view temporal module"}, {"figure_path": "bPOaHf8OcX/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison on T2MVid generation. Although MVDream generates spatially 3D consistent images among views (the 1st column), MVDream + IP-AnimateDiff breaks the spatial 3D consistency among its generated videos. Instead, our method generates high-quality multi-view videos with large motions while maintaining temporal coherence and spatial 3D consistency.", "description": "This figure compares the multi-view video generation results of three different methods: MVDream, MVDream + IP-AnimateDiff, and the proposed method. MVDream generates spatially consistent images but lacks temporal coherence. MVDream + IP-AnimateDiff fails to maintain spatial consistency when generating videos. The proposed method generates high-quality videos with vivid motions and maintains both temporal coherence and spatial consistency.", "section": "4.1 Qualitative and quantitative results"}, {"figure_path": "bPOaHf8OcX/figures/figures_7_1.jpg", "caption": "Figure 5: Visual comparison of the contributions of our multi-view spatial module", "description": "This figure compares the results of multi-view video generation using three different methods: (1) w/o MS w SD, which uses Stable Diffusion without a multi-view spatial module; (2) MVdream + IP-AnimateDiff, which combines a multi-view image diffusion model and a 2D video diffusion model; and (3) Ours, which is the proposed Vivid-ZOO model. The figure shows that the proposed model generates videos with higher quality and better multi-view consistency than the other two methods, demonstrating the effectiveness of the multi-view spatial module in maintaining geometric consistency across views.", "section": "4.1 Qualitative and quantitative results"}, {"figure_path": "bPOaHf8OcX/figures/figures_8_1.jpg", "caption": "Figure 6: Visual comparison of the contributions of our multi-view temporal module", "description": "This figure compares the results of the proposed method with and without the multi-view temporal module.  The left column shows results from a model that uses only the spatial module and low-rank adaptation of the temporal module (TM LoRA). This model is referred to as \"w/o MT w TM LORA\". The right column shows results from the full model, demonstrating the effect of including the multi-view temporal module on the generated multi-view videos.  The improvement in temporal coherence and overall video quality is evident.", "section": "4.2 Ablation study and discussions"}, {"figure_path": "bPOaHf8OcX/figures/figures_22_1.jpg", "caption": "Figure 1: The proposed Vivid-ZOO generates high-quality multi-view videos of a dynamic 3D object from text. Each row illustrates six frames drawn from a generated video for one viewpoint.", "description": "This figure shows several example outputs from the Vivid-ZOO model.  Each row presents a sequence of six frames from a single viewpoint of a generated multi-view video.  The videos depict a dynamic 3D object (in this case, a wasp) as it moves, showing how the model generates both realistic motion and consistent views from multiple perspectives. The text prompt used to generate these videos is: \"a yellow and black striped wasp bee, 3d asset\".", "section": "1 Introduction"}, {"figure_path": "bPOaHf8OcX/figures/figures_23_1.jpg", "caption": "Figure II: Text prompt: an astronaut riding a horse, 3d asset", "description": "The figure shows six frames from a generated multi-view video sequence for a dynamic 3D object, an astronaut riding a horse. Each row represents a different viewpoint (View 1 to View 4), showcasing the multi-view consistency and temporal coherence achieved by the proposed method. The generated videos exhibit vivid motions and maintain 3D consistency across different views.", "section": "F Additional results"}, {"figure_path": "bPOaHf8OcX/figures/figures_23_2.jpg", "caption": "Figure III: Text prompt: A full-bodied tiger walking, 3d asset", "description": "This figure shows six frames from four different viewpoints of a generated video of a walking tiger.  The video was generated using the Vivid-ZOO model, which is the subject of the paper. Each row represents a different camera angle, showcasing the multi-view aspect of the generation. The tiger's motion is fluid and realistic across all viewpoints, demonstrating the model's ability to maintain both temporal and spatial consistency.", "section": "F Additional qualitative visualized results"}, {"figure_path": "bPOaHf8OcX/figures/figures_24_1.jpg", "caption": "Figure IV: Text prompt: a blue flag attached to a flagpole, with a smooth curve, 3d asset", "description": "This figure shows six frames from a generated multi-view video sequence of a blue flag attached to a flagpole.  The video is generated from a text prompt describing the scene. Each row represents a different viewpoint, showing the flag from multiple angles. The smooth curve of the flagpole is clearly visible in each frame.", "section": "F Additional results"}, {"figure_path": "bPOaHf8OcX/figures/figures_24_2.jpg", "caption": "Figure 1: The proposed Vivid-ZOO generates high-quality multi-view videos of a dynamic 3D object from text. Each row illustrates six frames drawn from a generated video for one viewpoint.", "description": "This figure shows sample results from the Vivid-ZOO model.  Each row represents a single viewpoint, showing six frames of a generated video of a dynamic 3D object (a wasp bee in this case). The model successfully generates videos that are both temporally coherent and spatially consistent across multiple viewpoints.", "section": "1 Introduction"}, {"figure_path": "bPOaHf8OcX/figures/figures_25_1.jpg", "caption": "Figure 5: Visual comparison of the contributions of our multi-view spatial module", "description": "This figure compares the results of multi-view video generation using different models.  The \"w/o MS w/ SD\" model uses only Stable Diffusion without the multi-view spatial module. The \"Ours\" model incorporates the multi-view spatial module, which improves the geometric consistency and alignment of the generated multi-view videos. The results demonstrate the effectiveness of the multi-view spatial module in generating high-quality multi-view videos that maintain spatial 3D consistency.", "section": "4.2 Ablation study and discussions"}, {"figure_path": "bPOaHf8OcX/figures/figures_25_2.jpg", "caption": "Figure VII: Text prompt: a panda is dancing", "description": "This figure shows six frames from four viewpoints of a generated video of a panda dancing. Each row represents a different viewpoint, showing the panda's movement from different angles. The figure demonstrates the model's ability to generate high-quality, temporally consistent multi-view videos from a text prompt.", "section": "4.1 Qualitative and quantitative results"}, {"figure_path": "bPOaHf8OcX/figures/figures_26_1.jpg", "caption": "Figure VIII: Text prompt: a pixelated Minecraft character walking, 3d asset", "description": "This figure shows six frames from a generated multi-view video sequence. Each row represents a different viewpoint (View 1 to View 4), and each column shows a different frame within a single viewpoint. The generated video depicts a pixelated Minecraft character walking. The figure illustrates the ability of the model to generate high-quality, temporally consistent multi-view videos from a text prompt.", "section": "F Additional results"}, {"figure_path": "bPOaHf8OcX/figures/figures_26_2.jpg", "caption": "Figure IX: Visual comparison of the contributions of our 3D-2D alignment layers", "description": "The figure shows a visual comparison of multi-view videos generated with and without the 3D-2D alignment layer. The left column shows videos generated without the 3D-2D alignment layer, which results in inconsistent appearance and pose across the views. The right column shows videos generated with the 3D-2D alignment layer, demonstrating more consistent and coherent results across multiple views.", "section": "4.2 Ablation study and discussions"}, {"figure_path": "bPOaHf8OcX/figures/figures_27_1.jpg", "caption": "Figure X: Visual comparison of the contributions of our 2D-3D alignment layers", "description": "This figure shows a visual comparison of the results obtained with and without the 2D-3D alignment layers. The left side shows the results without the 2D-3D alignment, while the right side shows the results with the 2D-3D alignment. Each row represents a different view of the same scene. The figure demonstrates the importance of the 2D-3D alignment layers for improving the quality and consistency of the generated multi-view videos.", "section": "4.2 Ablation study and discussions"}]