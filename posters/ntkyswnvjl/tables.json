[{"figure_path": "NTkYSWnVjl/tables/tables_5_1.jpg", "caption": "Table 1: The results of adversarial attacks on the ImageNet dataset. Each score represents the mean success rate of the attack, mean L0 norm and mean the number of queries. In terms of the success rate, a higher value signifies better performance, whereas for the L0 norm and the number of queries, lower values are indicative of superior performance. The best method is highlighted in bold.", "description": "This table presents a comparison of different adversarial attack methods (OnePixel, ScratchThat, Pixle, and RFPAR) on various ImageNet classification models.  For each model and attack method, it shows the mean success rate (higher is better), the mean L0 norm (lower is better, indicating fewer pixel modifications), and the mean number of queries (lower is better, indicating higher efficiency).  The best-performing method for each model is highlighted in bold.", "section": "3.2 Evaluation of Classification Attacks"}, {"figure_path": "NTkYSWnVjl/tables/tables_7_1.jpg", "caption": "Table 2: Attack Results on Object Detection Models. The subscripts after RFPAR denote a pixel attack rate, a. RM indicates the average percentage of objects removed from the clean image. Lo represents the average ||\u03b4||0. Query denotes the average number of queries made to the victim model. Higher RM, lower mAP, lower Lo, and lower Query values indicate better performance.", "description": "This table presents the results of the RFPAR attack on two object detection models, YOLOv8 and DDQ.  It shows the average percentage of removed objects (RM), the mean average precision drop (mAP), the average L0 norm (Lo - number of pixels changed), and the average number of queries needed for the attack at different pixel attack rates (\u03b1). Lower mAP and Lo values, as well as higher RM values, represent better attack performance.", "section": "3.3 Evaluation of Object Detection Attacks"}, {"figure_path": "NTkYSWnVjl/tables/tables_7_2.jpg", "caption": "Table 3: Comparison to other methods. RD means reduction in mAP.", "description": "This table compares the performance of three query-based black-box attacks (PRFA, GARSDC, and RFPAR) on the YOLO object detection model.  The results show the reduction in mean Average Precision (mAP) and the number of queries required for each attack.  RFPAR demonstrates comparable performance to GARSDC in terms of mAP reduction while significantly reducing the number of queries needed.", "section": "3.3 Evaluation of Object Detection Attacks"}, {"figure_path": "NTkYSWnVjl/tables/tables_7_3.jpg", "caption": "Table 4: Comparison on dataset. ATA means the ratio of altered pixels to the image size.", "description": "This table compares the performance of the RFPAR attack on the MS-COCO and Argoverse datasets. It shows the average percentage of objects removed (RM), the reduction in mean Average Precision (mAP), the percentage of the image area attacked (ATA), and the average number of queries made to the victim model.  The results demonstrate that RFPAR is effective in removing objects across different datasets, although its effectiveness in reducing mAP is limited in datasets with a high density of objects.  Specifically, on Argoverse with higher image resolution,  RFPAR achieved a high object removal rate while attacking a small portion of the image.", "section": "3.4 Experiments on a Larger Scale Data"}, {"figure_path": "NTkYSWnVjl/tables/tables_16_1.jpg", "caption": "Table 5: The results of transformer-based classifiers.", "description": "This table presents the results of adversarial attacks on different transformer-based image classification models: ViT-L, Swin-V2, and Deit.  For each model, it shows the success rate (SR), the average L0 norm (representing the number of pixels modified), and the average number of queries needed to generate an adversarial example.  Higher success rates and lower L0 norms and query counts indicate better attack performance.  The table compares the performance of three different attack methods: OnePixel, Pixle, and RFPAR (the proposed method).", "section": "3.3 Evaluation of Classification Attacks"}, {"figure_path": "NTkYSWnVjl/tables/tables_16_2.jpg", "caption": "Table 6: The results of object detection models.", "description": "This table compares the performance of the RFPAR attack with different pixel attack rates (from 0.01 to 0.05) on two object detection models: YOLOv8 and DDQ.  The metrics used are RM (the average percentage of objects removed), mAP (mean Average Precision), Lo (the average number of perturbed pixels), and Query (the average number of queries made to the model). Higher RM and lower mAP values indicate better attack performance, while lower Lo and Query values indicate greater efficiency.", "section": "3.3 Evaluation of Object Detection Attacks"}, {"figure_path": "NTkYSWnVjl/tables/tables_16_3.jpg", "caption": "Table 7: The performance of RFPAR on transformer-based models with different iteration limits", "description": "This table presents the results of applying the RFPAR attack to various transformer-based models (ViT-B, ViT-L, ViT-H, Swin-V2, and DeiT-B) with two different maximum iteration limits: 100 and 200.  It shows the success rate of the attacks (higher is better), the L0 norm (lower is better, representing the sparsity of the attack), and the number of queries required (lower is better). Comparing the results across different models and iteration limits helps to understand the impact of these factors on RFPAR's effectiveness.", "section": "Additional Experiments"}, {"figure_path": "NTkYSWnVjl/tables/tables_17_1.jpg", "caption": "Table 8: The results of adversarial trained models.", "description": "This table presents the results of adversarial attacks against adversarially trained models (Adv. ViT and Adv. ResNeXt101).  It compares the performance of OnePixel, Pixle, and RFPAR in terms of success rate (higher is better), L0 norm (lower is better, representing the number of pixels changed), and the number of queries (lower is better). The results show that RFPAR outperforms the other methods on both models, demonstrating its effectiveness even against adversarially trained models.", "section": "F Experiments on Adversarially Trained Models"}, {"figure_path": "NTkYSWnVjl/tables/tables_17_2.jpg", "caption": "Table 9: Query for ablation study.", "description": "This table presents the number of queries required for different attack methods in an ablation study. The ablation study focuses on evaluating the impact of Initialization (I) and Memory (M) on the model's performance. RFPAR represents the baseline attack method without I and M, while RFPARI, RFPARM, and RFPARM+I represent variations of the method with different combinations of I and M. The table shows that the inclusion of memory significantly increases the number of queries required, while the addition of initialization does not significantly affect the query count.", "section": "G Query in Ablation study"}, {"figure_path": "NTkYSWnVjl/tables/tables_17_3.jpg", "caption": "Table 1: The results of adversarial attacks on the ImageNet dataset. Each score represents the mean success rate of the attack, mean L\u221e norm and mean the number of queries. In terms of the success rate, a higher value signifies better performance, whereas for the L\u221e norm and the number of queries, lower values are indicative of superior performance. The best method is highlighted in bold.", "description": "This table presents a comparison of different adversarial attack methods (OnePixel, ScratchThat, Pixle, and RFPAR) on various image classification models (VIT-B, ResNeXt50, RegNetX-32GF, DenseNet161, MNASNet, and MobileNet-V3) using the ImageNet dataset.  The metrics used for comparison are success rate (higher is better), L\u221e norm (lower is better, indicating fewer pixels modified), and the number of queries (lower is better, indicating higher efficiency). RFPAR's performance is highlighted in bold, showing its superior performance compared to other methods.", "section": "3.2 Evaluation of Classification Attacks"}]