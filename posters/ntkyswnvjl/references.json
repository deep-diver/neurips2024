{"references": [{"fullname_first_author": "Szegedy, C.", "paper_title": "Intriguing properties of neural networks", "publication_date": "2014-00-00", "reason": "This paper is foundational for the field of adversarial attacks, introducing the concept and highlighting the vulnerability of deep learning models to such attacks."}, {"fullname_first_author": "Goodfellow, I. J.", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2015-00-00", "reason": "This work significantly advanced the understanding of adversarial examples by providing explanations and techniques for generating and defending against them."}, {"fullname_first_author": "Vargas, D. V.", "paper_title": "One pixel attack for fooling deep neural networks", "publication_date": "2019-00-00", "reason": "This paper introduced a novel and highly effective pixel-based black-box attack, which directly inspired the current research by demonstrating the potential of minimal perturbations to fool models."}, {"fullname_first_author": "Hitaj, B.", "paper_title": "Scratch that! An evolution-based adversarial attack against neural networks", "publication_date": "2019-00-00", "reason": "This paper introduced a sophisticated evolutionary approach to generating adversarial examples, showing significant improvements over previous methods and thus informing the techniques used in this work."}, {"fullname_first_author": "D\u00e1ntoni, D.", "paper_title": "Rearranging pixels is a powerful black-box attack for RGB and infrared deep learning models", "publication_date": "2023-00-00", "reason": "This recent paper further advanced pixel-based attacks, providing a benchmark for comparison and demonstrating the continuing relevance and importance of this minimal perturbation approach."}]}