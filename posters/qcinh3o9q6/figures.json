[{"figure_path": "QCINh3O9q6/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons between our proposed CION with other pre-training methods. In (a), the instance-level method mines instance-invariance by contrastive learning on augmented views of each image, completely ignoring the invariance within different images of the same person. In (b), the single-video tracklet-level method mines tracklet-invariance by contrastive learning on images of each tracklet in single video, significantly ignoring the invariance in images of the same person across different videos. In (c), our CION learns identity-invariance by correlating the images of the same person across different videos, thus leading to better representation learning.", "description": "This figure compares three different pre-training methods for person re-identification: instance-level, single-video tracklet-level, and the proposed cross-video identity-correlating pre-training (CION).  The instance-level method only considers augmentations of a single image. The single-video tracklet-level method considers temporal information within a single video but ignores cross-video identity consistency. In contrast, CION explicitly correlates images of the same person across different videos to learn identity-invariance, resulting in better representation learning.", "section": "1 Introduction"}, {"figure_path": "QCINh3O9q6/figures/figures_4_1.jpg", "caption": "Figure 2: A toy example for Sliding Range and Linking Relation.", "description": "This figure illustrates the Sliding Range and Linking Relation concepts used in the CION framework for cross-video identity correlation.  A long video sequence is conceptually divided into smaller segments (tracklets) represented by the boxes. The Sliding Range (blue dashed lines) moves across the entire video, considering a window of tracklets at each position. Inside the Sliding Range, the algorithm identifies linking relations (green dashed lines) between tracklets that likely correspond to the same person across different segments, thus correlating images of the same person across different videos.", "section": "3.2 Progressive multi-level denoising"}, {"figure_path": "QCINh3O9q6/figures/figures_5_1.jpg", "caption": "Figure 3: Self-distillation with identity guidance. The overall structure shares a similarity with [1], while the concept of identity is introduced. We illustrate it in the case of Nid = 2 and pairs of views (xt, xs) for simplicity. Tt and Ts represent different random transformations. All transformed views from images of the same person will engage in contrastive learning.", "description": "This figure illustrates the identity-guided self-distillation process used in the CION framework. It shows how a student network learns to match the output probability distribution of a teacher network by minimizing the cross-entropy between them.  The key difference from existing self-distillation methods is the introduction of identity information; contrastive learning is performed on augmented views from images of the same person.  The figure highlights the use of a teacher network (f\u03b8t), a student network (f\u03b8s), and different random transformations (Tt and Ts) applied to the input images.", "section": "3.3 Identity-guided self-distillation"}, {"figure_path": "QCINh3O9q6/figures/figures_8_1.jpg", "caption": "Figure 1: Comparisons between our proposed CION with other pre-training methods. In (a), the instance-level method mines instance-invariance by contrastive learning on augmented views of each image, completely ignoring the invariance within different images of the same person. In (b), the single-video tracklet-level method mines tracklet-invariance by contrastive learning on images of each tracklet in single video, significantly ignoring the invariance in images of the same person across different videos. In (c), our CION learns identity-invariance by correlating the images of the same person across different videos, thus leading to better representation learning.", "description": "This figure compares three different person re-identification pre-training methods: instance-level, single-video tracklet-level, and the proposed cross-video identity-correlating pre-training (CION).  It highlights the limitations of the previous methods, which fail to fully capture the identity invariance across different videos. CION is shown to address this issue by correlating images of the same person from multiple videos, enabling better representation learning. ", "section": "1 Introduction"}, {"figure_path": "QCINh3O9q6/figures/figures_8_2.jpg", "caption": "Figure 1: Comparisons between our proposed CION with other pre-training methods. In (a), the instance-level method mines instance-invariance by contrastive learning on augmented views of each image, completely ignoring the invariance within different images of the same person. In (b), the single-video tracklet-level method mines tracklet-invariance by contrastive learning on images of each tracklet in single video, significantly ignoring the invariance in images of the same person across different videos. In (c), our CION learns identity-invariance by correlating the images of the same person across different videos, thus leading to better representation learning.", "description": "This figure compares three different person re-identification pre-training methods: instance-level, single-video tracklet-level, and the proposed cross-video identity-correlating pre-training (CION).  The instance-level method only considers variations within a single image, neglecting cross-image consistency for the same person.  The single-video tracklet-level method considers consistency within a single video but ignores cross-video consistency.  CION, in contrast, explicitly focuses on learning identity-invariance by correlating images of the same person across different videos.", "section": "1 Introduction"}, {"figure_path": "QCINh3O9q6/figures/figures_8_3.jpg", "caption": "Figure 6: The fine-tuning performance consistently keeps improving as the training data increases. This indicates our method\u2019s potential for further improved performance with increasing training data.", "description": "This figure shows the scalability of the CION model to large-scale training data.  The x-axis represents the percentage of the training data used (from 10% to 100%), while the y-axis shows the mAP (mean Average Precision) achieved on the Market1501 and MSMT17 datasets.  Both datasets show a consistent increase in mAP as the training data size increases, indicating that the model benefits from more data and that there is potential for even better results with even more training data.  This demonstrates the scalability and robustness of the CION pre-training method.", "section": "4.4 Ablation studies and analyses"}, {"figure_path": "QCINh3O9q6/figures/figures_15_1.jpg", "caption": "Figure 1: Comparisons between our proposed CION with other pre-training methods. In (a), the instance-level method mines instance-invariance by contrastive learning on augmented views of each image, completely ignoring the invariance within different images of the same person. In (b), the single-video tracklet-level method mines tracklet-invariance by contrastive learning on images of each tracklet in single video, significantly ignoring the invariance in images of the same person across different videos. In (c), our CION learns identity-invariance by correlating the images of the same person across different videos, thus leading to better representation learning.", "description": "This figure compares three different pre-training methods for person re-identification: instance-level, single-video tracklet-level, and the proposed cross-video identity-correlating pre-training (CION).  The instance-level method only considers augmentations of single images, ignoring cross-image consistency for the same person. The single-video tracklet method compares images within a single video tracklet but ignores the identity invariance across different videos.  In contrast, CION explicitly models the identity correlation across different videos to learn better identity-invariant representations.", "section": "1 Introduction"}, {"figure_path": "QCINh3O9q6/figures/figures_15_2.jpg", "caption": "Figure 1: Comparisons between our proposed CION with other pre-training methods. In (a), the instance-level method mines instance-invariance by contrastive learning on augmented views of each image, completely ignoring the invariance within different images of the same person. In (b), the single-video tracklet-level method mines tracklet-invariance by contrastive learning on images of each tracklet in single video, significantly ignoring the invariance in images of the same person across different videos. In (c), our CION learns identity-invariance by correlating the images of the same person across different videos, thus leading to better representation learning.", "description": "This figure compares three different pre-training methods for person re-identification: instance-level, single-video tracklet-level, and the proposed cross-video identity-correlating (CION) method.  The instance-level method only considers variations within a single image, neglecting cross-image consistency for the same person. The single-video tracklet method considers consistency within a video but ignores cross-video identity invariance. The CION method, however, explicitly correlates images of the same person across different videos to learn identity-invariance, leading to improved representation learning.", "section": "1 Introduction"}]