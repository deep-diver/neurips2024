[{"figure_path": "Dzk2cRUFMt/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy evaluation on node and graph classification. All tabular results (%) are in mean\u00b1 standard deviation across five seeds run, with best bolded and runner-up underlined.", "description": "This table presents the accuracy results of node and graph classification tasks using different methods.  The results are presented as mean \u00b1 standard deviation across five independent runs, with the best-performing method in each case highlighted in bold and the second-best underlined.  The table compares several methods, including Graph Convolutional Networks (GCNs) and variations of the proposed RAGRAPH framework with different training and fine-tuning strategies.", "section": "5.2 Retrieval-Augmented Graph Results"}, {"figure_path": "Dzk2cRUFMt/tables/tables_9_1.jpg", "caption": "Table 2: Performance evaluation (%) on link prediction.", "description": "This table presents the performance of different methods on three link prediction datasets (TAOBAO, KOUBEI, and AMAZON). The performance is evaluated using two metrics: Recall and nDCG (Normalized Discounted Cumulative Gain).  For each dataset, the table shows the average Recall and nDCG scores for each method, along with the standard deviation across multiple runs.  The methods compared include baselines such as LightGCN, SGL, MixGCF, SimGCL, GraphPro+, and variations of the proposed RAGRAPH model (Vanilla/NF, Vanilla/FT, PRODIGY/NF, PRODIGY/FT, RAGRAPH/NF, RAGRAPH/FT, and RAGRAPH/NFT). The results provide a comparison of the proposed model's performance against existing state-of-the-art methods on the link prediction task.", "section": "5.2 Retrieval-Augmented Graph Results"}, {"figure_path": "Dzk2cRUFMt/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy evaluation on node and graph classification. All tabular results (%) are in mean\u00b1standard deviation across five seeds run, with best bolded and runner-up underlined.", "description": "This table presents the accuracy results for node and graph classification tasks across several datasets.  Multiple methods are compared, including various versions of the proposed RAGRAPH framework (with and without fine-tuning, and with noise prompt tuning).  The results are shown as mean \u00b1 standard deviation across five independent runs, highlighting the best performing method in bold and the second-best in underlined.", "section": "5.2 Retrieval-Augmented Graph Results"}, {"figure_path": "Dzk2cRUFMt/tables/tables_25_1.jpg", "caption": "Table 4: Statistics of the experimental datasets and summary of datasets.", "description": "This table presents a comprehensive overview of the eight datasets used in the experiments, encompassing both static and dynamic graph datasets. For each dataset, it provides key statistics such as the number of nodes and edges per graph, graph density, the number of graphs, the number of classes for graph-level and node-level classification tasks, the number of node features, the number of node classes (if applicable), the snapshot granularity (for dynamic datasets), the type of task (node-level, edge-level, graph-level), the type of dataset (static or dynamic), and how the dataset is partitioned for training and testing.", "section": "5.1 Experimental Setup"}, {"figure_path": "Dzk2cRUFMt/tables/tables_26_1.jpg", "caption": "Table 1: Accuracy evaluation on node and graph classification. All tabular results (%) are in mean\u00b1 standard deviation across five seeds run, with best bolded and runner-up underlined.", "description": "This table presents the accuracy results for node and graph classification tasks using various methods.  The results are presented as the mean \u00b1 standard deviation across five separate runs with different random seeds.  The best performing method for each task is highlighted in bold, while the second best is underlined.  This allows for a comparison of the performance of different models across different datasets and tasks.", "section": "5.2 Retrieval-Augmented Graph Results"}]