[{"type": "text", "text": "Xinke Jiang\u2660\u2217, Rihong $\\mathbf{Q}\\mathbf{i}\\mathbf{u}^{\\bullet*}$ , Yongxin $\\mathbf{X}\\mathbf{u}^{\\pmb{\\adesuit}\\ast}$ , Wentao Zhang\u2662, Yichen $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{\\diamond}$ , Ruizhe Zhang\u2660 Yuchen Fang\u2661, $\\mathbf{X}\\mathbf{u}\\mathbf{C}\\mathbf{h}\\mathbf{u}^{\\star}$ , Junfeng $\\mathbf{Zhao^{\\bullet\\bullet}}$ , Yasha Wang\u2660\u22c6\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "{xinkejiang, rihongqiu, xuyx, ruizhezhang}@stu.pku.edu.cn {wentaozh2001, yichenzhu2014, fyclmiss}@gmail.com {chu_xu, zhaojf, wangyasha}@pku.edu.cn   \n\u2660Key Laboratory of High Confidence Software Technologies (Peking University), School of Computer Science, Peking University, China \u2662No Affiliation, \u2661University of Electronic Science and Technology of China \u2663Big Data Technology Research Center, Nanhu Laboratory, Jiaxing, China \u22c6Peking University Information Technology Institute, Tianjin Binhai, China ", "page_idx": 0}, {"type": "text", "text": "https://github.com/Artessay/RAGraph/ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGRAPH), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the RAGRAPH adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that RAGRAPH significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that RAGRAPH consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) [5, 48, 97, 63, 126] have recently burgeoned a surge of interest in both academic and industry communities due to their robust capability to model complex, real-world data in diverse domains, including societal [72, 55, 80], biochemical [17, 111, 107], and traffic-related [54, 23, 44, 21] fields and etc [53, 37, 68, 15, 25, 24]. Utilizing a message-passing mechanism [48, 29], GNNs have transcended traditional node embedding approaches [28, 79, 95], enabling the capture of intricate relationships within data through sophisticated architectures and advanced graph representation learning techniques [48, 50, 54, 18, 97]. However, the challenge of generalizing GNNs across different modalities, domains [62, 61], and tasks remains largely unexplored [56, 113]. This is in stark contrast to the significant successes of large models such as GPTs [74, 75] in NLP and Sora [64] in CV, presenting a crucial frontier for further research and realms for graph data generalizing. ", "page_idx": 0}, {"type": "text", "text": "In graph learning tasks, providing the necessary context is crucial for graph generalization [129, 51, 73, 134], i.e., retrieve similar shopping context as illustrated in Figure 1 (c). Therefore, our insight is to enhance the model\u2019s generalization ability and prediction accuracy by retrieving necessary contexts during graph learning through retrieval. Retrieval-Augmented Generation (RAG) represents a prominent methodology, significantly augmenting language model functionalities through the integration of a dynamic retrieval mechanism during the generation process [135, 77] (e.g., a person asks what animal it is, and we use some visual [138] or text retrieval [2] methods to retrieve more descriptive features or even the wanted category). RAG enriches not only accurate and reliable content but also reduces factual errors, addressing challenges such as incorrect answers, hallucinations, and limited interpretability in knowledge-intensive tasks [40, 2, 1], obviating the need for updating model parameters and could be generalized even in unseen scenarios. ", "page_idx": 1}, {"type": "text", "text": "However, how to enable retrieval-augmented generation for graph learning, i.e., retrieving the user\u2019s historical purchasing behavior to enhance recommendation ability [30, 113, 35] and identifying fraud crimes by searching for similar fraudulent relationship behaviors [85, 63], still remains unexplored and faces the following challenges C1& C2. ", "page_idx": 1}, {"type": "text", "text": "C1. The first challenge is how to leverage the retrieved context i.e., features $(X)$ and labels $(Y)$ into the GNNs model under dynamic changing scenarios. Previous studies, such as PRODIGY [73], have adopted the concept of in-context learning (ICL) by constructing consistent and static task graphs for each specific task or dataset. These task graphs determine labels through the calculation of similarities using hidden vectors, employing a few-shot learning approach. However, PRODIGY\u2019s reliance on a fixed set of examples as rules may not sufficiently address and generalize the variety of scenarios encountered in real-world settings, which is particularly problematic in dynamically changing environments, as the system focuses primarily on teaching the direct mapping paradigm from inputs to outputs $(X\\to Y)$ ), rather than truly integrate the input $(X)$ and output $(Y)$ data into the analysis. In contrast to RAG, PRODIGY struggles to incorporate external information ( $X$ and $Y$ ) related to data nodes, which is crucial for enriching the learning process in graph-based systems. ", "page_idx": 1}, {"type": "text", "text": "$^{C2}$ . Moreover, it is challenging to develop a tune-free prompt mechanism to support retrieved knowledge and be applicable to seamlessly switch unseen scenarios and multi-tasks. Numerous initiatives have been undertaken in the realm of graph pre-training [33, 116, 34, 81, 98, 36, 7, 88, 125], however, the challenge persists in designing a plug-and-play RAG module that can seamlessly interface with already pre-trained models. Insights derived from prior investigations into the graph prompt [9, 26, 90, 65, 113, 20, 94], the knowledge obtained by RAG can be facilitated and injected into prompt via a plug-and-play manner. ", "page_idx": 1}, {"type": "text", "text": "For endeavoring to address these two challenges previously mentioned, we put forward the General Retrieval- Augmented Graph Learning Framework (RAGRAPH). Drawing inspiration from the success of RAG on LLMs [135] and the ICL on GNNs [73] (we detail the difference between RAG and ICL in Appendix E), we constructed a toy graphs vector library by chunking from resource graphs, where the library key stores key information, including environmental, historical, structural, and semantic details, while ", "page_idx": 1}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/23f9caa4763d85c18f6977e9e576efad39de8db8c1b4c9c72052dbff62f05958.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a) RAG in NLP utilizes retrieval to enhance model responses, based on a query to retrieve related features (e.g., a tail, primarily feeds on mice) and answers (e.g., Cat). (b) In CV, RAG employs similar photo retrieval to enhance model comprehension, assisting in downstream tasks such as inpainting or image question answering. (c) For GNNs, RAG could leverage retrieval of similar historical subgraphs or scenarios to aid in graph-based tasks (e.g., recommendations or fraud detection). ", "page_idx": 1}, {"type": "text", "text": "node features and label information (task-specific output vector) are stored as values. For downstream tasks, the key value of the query node would be leveraged to retrieve toy graphs by the key similarities, and the stored features (X) and labels (Y) would be aggregated structurally to provide essential knowledge to the query node, instead of the mapping paradigm, to address challenge $c$ . In prompt mechanism design, we start by transferring features and task-specific output from the toy graphs to their master nodes (the central node of the toy graph) via message-passing. Subsequently, features from the master nodes and the query node\u2019s neighbors are aggregated to the query node, along with the task-specific output from master nodes. This process could be parameter-free, indicating that our model can be applied across different tasks and datasets without the need to fine-tune for downstream tasks, effectively addressing challenge C2. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In summary, our contributions are listed as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 To the best of our knowledge, our proposed framework, RAGRAPH, is the first to integrate RAG with pre-trained GNNs. By constructing a key-value vector library for toy graphs, RAGRAPH facilitates explicit plug-and-play access to pre-trained GNNs, achieving commendable performance even without fine-tuning, demonstrating its superiority on cross-task and cross-dataset capabilities.   \n\u2022 Our RAGRAPH employs a classic message-passing mechanism and introduces a well-designed prompt mechanism to integrate knowledge. This approach effectively incorporates the retrieved knowledge $X$ and $Y$ from toy graphs, into the pre-trained GNNs model, enhancing the accuracy and relevance of the model\u2019s outputs.   \n\u2022 We have extensively tested RAGRAPH on both static and dynamic graphs across multiple levels of graph tasks (node, edge, and graph). The results validate the effectiveness of our model, showing significant improvements over state-of-the-art baselines in both fine-tuned and tuning-free scenarios, particularly in cross-dataset validations. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Retrieval-Augmented Generation on Large Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "RAG integrates an external knowledge retrieval component and through prompt engineering into pretrained language models to enhance factual consistency, thus improving the reliability and interpretability of LLM responses [131, 135, 49, 22, 43, 118, 57, 110, 127]. Traditional RAG approaches utilize retriever models to source relevant documents from extensive knowledge corpora [106, 82, 69, 47], which are then processed further by reader models\u2014primarily LLMs [76, 84]. Furthermore, several studies focus on fine-tuning reader LLMs by applying prompt-tuning with retrieved knowledge or using RAG API calls [67, 40, 2, 115, 101, 128, 60]. While RAG has seen considerable success in the NLP field, it has also been applied to tasks involving joint visual and text retrieval [138, 59, 58, 8, 124, 10], code retrieval [66, 133], audio retrieval [6, 31] and video retrieval [3, 100]. Although there have been applications of RAG on structured data such as KG-RAG for knowledge graphs [46, 43, 86, 92, 93, 38], these primarily leverage the text information of knowledge graph nodes to enhance language or graph models. In contrast, there are no significant studies utilizing RAG on structured graphs without text information to enhance pre-trained GNNs. Our work aims to extend this successful approach similarly to graph data, to enhance the capabilities of pre-trained GNNs, and can be adapted to various tasks and across different graphs without additional fine-tuning by integrating a plug-and-play RAG module. ", "page_idx": 2}, {"type": "text", "text": "2.2 Graph Prompt Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by the application of pre-training models [74, 75] and prompt learning [102, 133, 41] in NLP, recently, learning on the graph has been divided into pre-training models on large-scale graph data [33, 116, 34, 81, 73, 130, 104, 89, 119, 121, 122, 120, 123], with or without labels, followed by fine-tuning model parameters via prompts for diverse downstream tasks [65, 113, 73, 137, 89, 94]. The adoption of prompting mechanisms in graph learning represents a promising avenue to overcome the constraints of traditional graph representation methods, striking a balance between flexibility and expressiveness [91]. For instance, VNT [94] utilizes virtual nodes as prompts to refine the application of pre-trained graph models. GraphPrompt [65] introduces a task-specific readout mechanism to tailor models for various tasks, while GraphPro [113] implements spatial- and temporal-based gating mechanisms suited for dynamic recommendation systems. Furthermore, PRODIGY [73] constructs task graphs (prompts) and data graphs to enhance the model\u2019s ICL capabilities. Leveraging the successes in graph prompt learning, we aim to inject retrieved knowledge via prompt into pre-trained GNNs to support downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In RAGRAPH, we focus on RAG on multi-level graph tasks. For consistency, we define the graphs as dynamic graphs, considering static graphs as the special cases within this framework. The subsequent definition provides a detailed description of toy graphs, including the definitions of keys and values utilized in RAGRAPH. Additionally, inspired by GraphPrompt [65], we have unified node-level, edgelevel, and graph-level tasks into a cohesive framework, and employ query graphs to tackle downstream tasks with precision. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. (Dynamic Graph) Let $\\mathcal{G}\\!=\\!\\{G_{t}\\}_{t=1}^{T}$ denote a dynamic graph comprising a sequence of graph snapshots, each represented as a static graph $G_{t}\\!=\\!(V_{t},E_{t},X_{t},A_{t},Y_{t})$ . $\\begin{array}{r}{\\mathcal{V}\\!=\\!\\bigcup_{t=1}^{T}\\!V_{t}\\!=\\!\\{v_{1},...,v_{n}\\}}\\end{array}$ defines the combined set of nodes across all snapshots and $\\begin{array}{r}{\\mathcal{E}=\\bigcup_{t=1}^{T}E_{t}\\subseteq\\mathcal{V}\\times\\mathcal{V}}\\end{array}$ is the edge set, where $V_{t}$ and $E_{t}$ represent the nodes and edges of the $t$ -th snapshot, respectively. Feature matrix $X_{t}\\!=\\!\\{x_{v}\\,|\\,v\\in\\mathcal{V}\\}\\in\\mathbb{R}^{n\\times d}$ contains the feature vectors for the nodes in the $t$ -th snapshot, where $d$ is the feature dimension. $A_{t}$ denotes the edge weight matrix at time $t$ , where edge weight $A_{t}[i,j]\\in(0,1]$ if $v_{i},v_{j}\\in V_{t}$ and $(v_{i},v_{j})\\in E_{t}$ , and 0 otherwise. Furthermore, $Y_{t}$ represents the task-related labels associated with nodes, edges, or the graph at time $t$ . Note that a graph is static if $T=1$ and for consistency in terminology, we unify static graphs as a particular instance of dynamic graphs. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. (Toy Graph Vector Base) Let $\\mathcal{G}^{\\mathcal{R}}=\\{G_{t}^{\\mathcal{R}}\\}_{t=1}^{T}$ denote a dynamic resource graph. We chunk $\\mathcal{G}^{\\mathcal{R}}$ into snapshots and take each node in $\\mathcal{G}^{\\mathcal{R}}$ as the master node $v_{m}$ of the corresponding toy graph, and then store $v_{m}$ with its neighbors within $k$ hops as subgraphs. Data augmentation techniques [54, 132] such as node dropout, edge dropout, and random noise addition are employed on subgraphs to enhance the robustness and variability when generating each toy graph $G^{\\mathcal{T}}$ (c.f. Section 4.1 for details). Each toy graph $G^{\\mathcal{T}}\\subseteq\\mathcal{G}^{\\mathcal{R}}$ is associated with a specific timestamp $\\tau$ and master node $v_{m}\\in\\mathcal{V}$ , with each toy graph\u2019s scale being considerably smaller in scale compared to their corresponding $\\mathcal{G}^{\\mathcal{R}}$ . ", "page_idx": 3}, {"type": "text", "text": "$\\pmb{\\mathrm{\\Sigma}}$ Toy graphs can be retrieved using keys that include the timestamp $\\tau$ , the hidden embedding of the master node $h_{m}^{\\tau}\\!\\in\\!\\mathbb{R}^{f_{1}}$ (e.g., embedded by pre-trained GNNs in RAGRAPH), the environmental key (e.g., the neighbors set $\\scriptstyle{\\mathcal{N}}(v_{m}^{\\tau})\\,=\\,\\{v_{i}^{\\tau}\\,|\\,A_{\\tau}[{\\dot{m}},i]\\,>\\,0,v_{i}^{\\tau}\\,\\in\\,G^{T}\\}\\}$ ) and the structure-based position-aware code $s_{m}^{\\tau}$ (cf. Appendix C.2 for details). $\\pmb{\\varphi}$ By retrieving based on key similarity (c.f. Section 4.2 for details), we can obtain the required values of $G^{\\mathcal{T}}$ , i.e. task-specific output vector $\\{o_{i}^{\\tau}\\!\\in\\!\\mathbb{R}^{f_{2}}|v_{i}\\!\\in\\!G^{T}\\}$ and hidden embeddings $\\{h_{i}^{\\tau}\\in\\mathbb{R}^{f_{1}}|v_{i}\\in G^{T}\\}$ of the master node and its neighbors, where $f_{1}$ and $f_{2}$ represent the dimensions. Finally, we denote the key-value vector base for the toy graph as $\\mathcal{G}^{\\mathcal{T}}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3. (A Unified Graph Task Definition) Given a dynamic graph $\\mathcal{G}$ , it can be divided into training and testing subsets, i.e. $\\mathcal{G}\\!=\\!\\mathcal{G}_{\\mathrm{train}}\\!\\cup\\!\\mathcal{G}_{\\mathrm{test}}$ based on either snapshot or node set partitioning. The label $y_{i}$ of a node $v_{i}$ , edge $(v_{i},v_{j})$ or subgraph $G_{i}$ can be observed only if they belong to $\\mathcal{G}_{\\mathrm{train}}$ . The objective of label prediction is to predict test labels $Y_{\\mathrm{test}}\\!\\in\\!\\mathcal G_{\\mathrm{test}}$ . Following GraphPrompt [65], we unify the three types of graph learning tasks (node-level, edge-level, and graph-level) into a single framework via similarity comparison $\\mathbf{sim}(\\cdot,\\cdot)$ of the task-specific output vector (abbreviated as $O$ , where each entry is $o$ ) with the ground-truth (i.e., the one-hot vector or the prototype embedding under few-shot setting). It\u2019s noted that $o$ can be either low-dimensional (with the dimension equal to the number of predicted classes) under normal settings [48, 126], or high-dimensional under few-shot settings [65] or in link prediction tasks [113, 30]. In our experiment, $\\pmb{\\mathrm{\\Sigma}}$ for node-level and graph-level tasks, the downstream tasks are given in few-shot settings following [65]: For node / graph classification on a node / graph set, let $\\mathcal{C}$ be the set of classes with $y_{i}\\in\\mathcal{C}$ denoting the class label of node / graph. For each node / graph class, the class prototypical output vector is calculated by the mean value of the $\\kappa$ -shot set $\\mathcal{D}$ : $\\tilde{o_{c}}\\!=\\!\\frac{1}{\\kappa}\\!\\sum_{(i,y_{i})\\in\\mathcal{D},y_{i}=c}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ . The class $y_{i}$ of the node or graph is determined by calculating similarity with the class prototype as: $y_{i}\\!=\\!\\mathrm{argmax}_{c\\in\\mathcal{C}}\\\\mathtt{s i m}\\!\\left(o_{i},\\!\\tilde{o}_{c}\\right)$ . $\\pmb{\\varphi}$ For edge-level tasks, to predict a link between nodes $v_{i}$ and $v_{q}$ , if $\\exists v_{j},(v_{i},v_{j})\\in\\mathcal{E}_{\\mathrm{train}}\\in\\mathcal{G}_{\\mathrm{train}}$ and $\\mathbf{sim}(o_{i},o_{q})\\geq\\mathbf{sim}(o_{i},o_{j})+\\epsilon$ , we regard $(v_{i},v_{q})$ as linked. Following PRODIGY [73] and GraphPrompt [65], we also apply a query graph $G^{\\mathcal{Q}}$ that includes the center node and its neighbors within $k$ hops. Specifically, for graph-level task, we apply a full-link virtual node as the center node inside the query graph $G^{\\mathcal{Q}}$ . ", "page_idx": 3}, {"type": "text", "text": "4 RAGRAPH Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce RAGRAPH, a general and novel retrieval-augmented graph learning framework that can operate on arbitrary graphs with or without additional fine-tuning, as illustrated in Figure 2. Initially, in Section 4.1, we elucidate the methodology for constructing the Resource Toy Graphs. Subsequently, in Section 4.2 we detail the Toy Graphs Retrieval Process. Finally, the Training and Inference processes are elaborated in Section 4.3, which utilize retrieved toy graphs from two propagation views\u2014intra and inter-propagation\u2014and handle two types of information: hidden embeddings and task-specific output vectors in two techniques (noisy trainable approach or parameter-free approach). The main notations of RAGRAPH are summarized in Table 3, Appendix A. For enhanced clarity, the Toy Graph Construction is outlined in Algorithm 1 (cf. Appendix C.5) and the Training and Inference with Toy Graphs Retrieval are detailed in Algorithm 2 (cf. Appendix C.5). Moreover, in Appendix C.4, we theoretically prove the effectiveness of applying RAG on GNNs from the perspective of mutual information gain. ", "page_idx": 4}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/6ba754d13a720aa2ba1df528f21b900f2acbd38588462f5fa263d5aff9500162.jpg", "img_caption": ["Figure 2: The overall framework of RAGRAPH. $\\pmb{\\mathrm{\\Sigma}}$ Given resource graph $\\mathcal{G}^{\\mathcal{R}}$ , we chunk it and augment toy graphs $\\{G^{\\mathcal{T}}\\}$ , and feed them into pre-trained GNNs to generate hidden embeddings via the encoder and task-specific output vectors via decoder, which are stored as values. Keys such as environment, history, position-aware, and hidden embeddings are stored to form the key-value database of toy graphs $\\mathcal{G}^{\\mathcal{T}}$ . $\\pmb{\\varphi}$ For a given query graph $G^{\\mathcal{Q}}$ , the keys are fetched to retrieve the topK toy graphs $G_{\\mathrm{topK}}^{\\check{T}}$ from the database. $\\pmb{\\otimes}$ Leveraging $G_{\\mathrm{topK}}^{\\mathcal{T}}$ , intra- and inter-propagation are performed to propagate hidden embeddings and task-specific output vectors to pass retrieved knowledge to center node $v_{c}$ . Through a weighted fusion, the aggregated output is used to perform graph-, node- and edge-level tasks. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 Toy Graphs Embedding Pipeline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In graph-based learning, nodes with higher connectivity\u2014typically with higher degrees\u2014often hold more significance, meaning their information is more extensively learned during graph-pre-training processes. Conversely, less important nodes\u2014those in the long tail\u2014often have their features overlooked. This issue is particularly pronounced in LLMs performing RAG, where the predominance of common knowledge overshadows the long-tail knowledge that RAG is meant to leverage. To tackle this, we construct toy graphs using an inverse importance sampling strategy, thereby countering this bias by preferentially sampling and augmenting toy graphs that accentuate the long-tail knowledge. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Inverse Importance Sampling Strategy. To achieve this, we calculate each node\u2019s importance $I(v)$ for node $\\bar{v}\\in G_{\\tau}^{\\mathcal{R}}$ by combining PageRank ${\\mathrm{PR}}(v)$ and Degree Centrality $\\mathrm{DC}(v)$ using the formula $I(v)\\!=\\!\\alpha\\mathbf{P}\\mathbf{R}(v)\\!+\\!(1\\!-\\!\\alpha)\\mathbf{D}\\mathbf{C}(v)$ , where $\\alpha\\!\\in\\!(0,1)$ is the balance weight. We reverse the node importance with $\\begin{array}{r}{I^{\\prime}(v)\\!=\\!\\frac{1}{I(v)+\\epsilon},\\!\\epsilon\\!\\to\\!0}\\end{array}$ , normalize it to obtain node $v_{i}$ \u2019s sampling probabilities $\\begin{array}{r}{p_{i}\\!=\\!\\frac{I^{\\prime}(v_{i})}{\\sum_{j=1}^{n}\\!I^{\\prime}(v_{j})}}\\end{array}$ , and perform weighted sampling function WEIGHTEDSAMPLING $(G_{\\tau}^{\\mathcal{R}},p_{i})$ to prioritize nodes with higher sampling probability (lower importance) according to $p_{i}$ . When sampling, for each master node $v_{m}$ , we generate its $k$ -hop neighbors, termed an ego net $G_{\\tau}^{e}(v_{m})$ . Given the constrained size of the resource graph, we adopt data augmentation techniques commonly used in contrastive learning [54, 117, 116] to enhance the representativeness and diversity of the resultant toy graphs. ", "page_idx": 5}, {"type": "text", "text": "Toy Graphs Augmentation Strategy. For augmentation, we first calculate the average reversed importance $\\Bar{I}^{\\prime}(G_{\\tau}^{\\breve{e}}(v_{m}))$ of the nodes within an ego graph as $\\begin{array}{r}{\\bar{I}^{\\prime}(G_{\\tau}^{e}(v_{m}))\\!=\\!\\frac{1}{|G_{\\tau}^{e}(v_{m})|}\\!\\sum_{v\\in G_{\\tau}^{e}(v_{m})}\\!I^{\\prime}(v)}\\end{array}$ , which then determines the number of augmentations $n_{\\mathrm{aug}}(G_{\\tau}^{e}(v_{m}))=\\lfloor K\\cdot\\bar{I}^{\\prime}(G_{\\tau}^{e}(v_{m}))\\rfloor$ , where $K$ is a scaling constant that adjusts the intensity of the augmentation. For node $v_{i},v_{j}\\in\\bar{G}_{\\tau}^{e}(v_{m})$ , the augmentation techniques DATAAUGMENTATION $(G_{\\tau}^{e}(v_{m}),n_{\\mathrm{aug}})$ employed include: ", "page_idx": 5}, {"type": "text", "text": "\u2022 \u2776Node Dropout: $v_{i}\\!\\in\\!G_{\\tau}^{e}(v_{m})$ has a probability of being dropped: $p(v_{i}\\;\\mathbf{being\\;dropped})\\!=\\!1\\!-\\!p_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\pmb{\\varphi}$ Addition of Gaussian Noise: we add gaussian noise to node features as augmentation $X^{\\prime}(v_{i})\\,{=}$ $X(v_{i})\\!+\\!\\mathcal{N}(0,\\!\\sigma^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\pmb{\\otimes}$ Node Interpolation: a new node feature $X^{\\prime}(v_{n e w})$ is created by linearly combining the features of two existing nodes $v_{i}$ and $v_{j}$ , calculated as $X^{\\prime}(v_{n e w})=\\lambda X(v_{i})+(\\`-\\lambda)X(v_{j}),v_{i},v_{j}\\in G^{T}$ . And the edge weight between the new node $v_{n e w}$ and node $v_{i}$ is updated to $\\lambda A[i,j]$ and node $v_{j}$ is $(1\\!-\\!\\lambda)A[i,\\!j]$ accordingly [108]. ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\pmb{\\mathbb{\\otimes}}$ Edge Rewriting: we alter connections based on the average of the involved nodes\u2019 sampling probabilities, expressed as $p((v_{i},v_{j})$ being rewired) $\\begin{array}{r}{=\\frac{p_{i}+p_{j}}{2}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Key-Value Pairs Construction. After completing the sampling and augmentation procedures, the generated toy graphs are transformed into key-value pairs for storage [109]. Specifically, we collect each master node\u2019s $v_{m}$ historical information (timestamps $\\tau$ ), environmental information (neighbors $\\mathcal{N}(v_{m}^{\\tau}))$ , structural encodings $s_{m}^{\\tau}$ (as described in the Appendix C.2), and the hidden embeddings $h_{m}^{\\tau}$ (obtained by processing the toy graph through the frozen pre-trained GNNs) and store them as keys at the master node $v_{m}$ of the toy graph. Additionally, we store task-specific output vectors $\\{o_{i}^{\\tau}|v_{i}\\!\\in\\!\\dot{G}^{T}\\}$ and hidden embeddings $\\{h_{i}^{\\tau}|v_{i}\\in G^{T}\\}$ as values at each node of the toy graph. For storage of these key-value pairs, we utilize the FAISS vector library [14] to facilitate accelerated retrieval and storage. ", "page_idx": 5}, {"type": "text", "text": "4.2 Toy Graphs Retrieval Process ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After constructing the key-value toy graphs vector database, we proceed with the retrieval process for sub-tasks according to the four sub-similarities between the key values of the master node $v_{m}$ in the toy graph and the center node $v_{c}$ in the query graph, as detailed in Appendix C.3. The final similarity score is a weighted combination of these factors, and the topK toy graphs are selected as the retrieval results: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S(v_{c},v_{m})\\!=\\!\\mathbf{w}\\times[S_{\\mathrm{time}}(v_{c},v_{m}),S_{\\mathrm{structure}}(v_{c},v_{m}),S_{\\mathrm{enviononent}}(v_{c},v_{m}),S_{\\mathrm{semantic}}(v_{c},v_{m})]^{\\mathrm{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{w}=[w_{1},w_{2},w_{3},w_{4}]$ are the hyper-parameterized weights attributed to the time, structure, environment, and semantic similarities, respectively. Using this composite similarity, we rank and retrieve the top $K$ toy graphs: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G_{\\mathrm{TopK}}^{\\mathcal{T}}\\!=\\!\\mathrm{TopK}_{G^{\\mathcal{T}}\\in\\mathcal{G T}}S(v_{c},v_{m}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "cwrihteerrei $G_{\\mathrm{TopK}}^{\\mathcal{T}}$ rperoprceesses netns stuhree ss tuhbaste tw oe fr tetoryi egvrea tphhes  mthoastt  rbeelset vamnatt tcohy t ghrea qpuhse rbya sbeads eodn  oa nc tohme pcroehmebnisnievde similarity measure, incorporating historical, structural, and environmental information. ", "page_idx": 5}, {"type": "text", "text": "4.3 Training and Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 4.3.1, we detail the Knowledge Injection Propagation process, which includes two distinct propagation manners. Next, in Section 4.3.2, we present our approach for combining the retrieved hidden embeddings with the task-specific output vectors. Additionally, to enhance the robustness of RAGRAPH, a noise-based prompt tuning strategy is introduced in Section 4.3.3. ", "page_idx": 6}, {"type": "text", "text": "4.3.1 Knowledge Injection Propagation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After retrieving the top $\\mathcal{K}$ toy graphs $G_{\\mathrm{TopK}}^{\\mathcal{T}}$ , knowledge, specifically the task-specific output vectors $O$ and hidden embeddings $H$ , is propagated from these toy graphs to the master nodes (Toy Graph Intra Propagation) and then to the center node $v_{c}$ (Query-Toy Graph Inter Propagation). This propagation utilizes message-passing mechanisms via GNNs (cf. Appendix C.1). Each master node $v_{m}$ in the toy graphs is connected to the center node $v_{c}$ of the query graph based on the similarity scores $S(v_{c},v_{m})$ computed in Eq.(1) and the connection weights dictate the influence of each toy graph, ensuring that graphs with higher similarity have a more substantial impact. This process can be implemented using either a parameter-free or a learnable approach. Moreover, it is worth noting that for learnable methods, the parameters of GNN are different. ", "page_idx": 6}, {"type": "text", "text": "$\\pmb{\\mathrm{\\Sigma}}$ Toy Graph Intra Propagation Within each toy graph, information ${\\bf z}$ is propagated from neighbors to the master node using pre-trained GNNs. The task-specific output vectors $o$ and hidden embeddings $h$ from the neighbors are aggregated and transmitted to the master node. For each node $v_{i}$ in a toy graph $G^{\\mathcal{T}}$ , the GNN aggregates information from its neighbors ${\\mathcal{N}}(v_{i})$ to update the master node $v_{m}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{m}\\!=\\!\\mathbf{G}\\mathrm{NN}\\big(\\!\\left\\{\\mathbf{z}_{i}\\:\\middle|\\:v_{i}\\!\\in\\!\\mathcal{N}(v_{m})\\right\\}\\!\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{z}_{i}$ and ${\\bf z}_{m}$ represent the hidden embeddings $h_{i},h_{m}$ or task-specific output vectors $o_{i},o_{m}$ of the neighbor nodes and master node, respectively. For parameter-free situations, we can prepare ${\\bf z}_{m}$ in advance when constructing the toy graph to improve inference efficiency. ", "page_idx": 6}, {"type": "text", "text": "$\\pmb{\\varphi}$ Query-Toy Graph Inter Propagation Next, information from the toy graphs is aggregated to the query graph. Specifically, during propagation, information ${\\bf z}$ from the neighbors and master node of the toy graph is propagated to the center node using the same pre-trained GNNs. For a center node $v_{c}$ in the query graph $G^{\\gtrless}$ , the GNN aggregates hidden embeddings $H$ from its neighbors $\\mathcal{N}(v_{c})$ and the master node $v_{m}$ from the toy graph: ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{c}\\!=\\!\\mathrm{GNN}\\big(\\{h_{i}\\,|\\,v_{i}\\!\\in\\!\\mathcal{N}(v_{c})\\cup\\{v_{m}\\}\\big\\}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When propagating the task-specific output vector $O$ , only the master node\u2019s information is passed to the center node: ", "page_idx": 6}, {"type": "equation", "text": "$$\no_{c}\\!=\\!\\mathbf{G}\\mathbf{NN}\\big(\\{o_{i}\\,|\\,v_{i}\\!\\in\\!\\{v_{m}\\}\\big\\}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For scenarios where the propagation mechanism is learnable, attention mechanisms can be adapted on the edges. In parameter-free scenarios\u2014where there are no learnable weights\u2014the attention on the edges is determined based on the edge weights from the previous resource graph. ", "page_idx": 6}, {"type": "text", "text": "4.3.2 Knowledge Fusion Layer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, at the data fusion layer, the aggregated hidden embeddings $H$ of the center node $v_{c}$ are processed through the pre-trained GNN\u2019s decoder DECODER $(\\cdot)$ to obtain an output vector $O$ . This output vector is then combined with the aggregated task-specific output vector in a weighted manner to produce the final output for downstream tasks as illustrated in Definition 3. The combined output is formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{o}_{c}\\!=\\!\\gamma o_{c}\\!+\\!(1\\!-\\!\\gamma)\\mathrm{DECODER}(h_{c}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma$ is a reweighting hyper-parameter. The resulting vector $\\hat{O}_{c}$ is then utilized to perform node-, graph-, or edge-level tasks via a similarity function. ", "page_idx": 6}, {"type": "text", "text": "For the same task, the decoder can be directly used to generate outputs. For different tasks, the decoder can be masked, allowing the model to utilize pre-computed embeddings without additional training. Furthermore, the decoder can be fine-tuned to better meet the specific requirements of each task, providing both flexibility and optimized performance. This approach ensures that the model effectively integrates and leverages information from both the toy graphs and the query graph, enhancing its effectiveness in various downstream tasks through the use of the aggregated task-specific output vector. ", "page_idx": 6}, {"type": "text", "text": "4.3.3 Noise-based Graph Prompting Tuning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When prompt tuning, RAGRAPH employs the same prompt loss function $\\mathcal{L}_{\\mathrm{prompt}}$ as the backbone model (e.g., GraphPro, GraphPrompt). However, to mitigate the challenge of noise retrieval\u2014a common issue in traditional RAG where highly related but irrelevant data is often retrieved\u2014we enhance the training process by incorporating noise data to bolster model robustness, motivated by [53]. Specifically, we implement two types of noise integration strategies: ", "page_idx": 7}, {"type": "text", "text": "\u2022 \u2776Inner-Toy-Graph Noise: This strategy involves artificially introducing irrelevant nodes $(v_{j}\\notin$ $G_{\\tau}^{e}(v_{m}))$ ) into the toy graph during its construction, complementing other augmentation techniques. \u2022 $\\pmb{\\varphi}$ Toy-Graph Noise: Throughout the training phase, we not only retrieve the topK toy graphs that are most relevant but also deliberately include the bottomK toy graphs to incorporate noise knowledge. ", "page_idx": 7}, {"type": "text", "text": "The integration of these noise elements is intended to enhance the model\u2019s ability to distinguish relevant information from irrelevant information, significantly improving its robustness and overall performance in downstream tasks by noise training. However, during the inference stage, we do not incorporate the noise. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct a series of experiments to evaluate the performance of RAGRAPH against state-of-the-art baselines on three dynamic and five static datasets on three-level graph tasks. Further details and experiment results are provided in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. We use four static datasets PROTEINS, COX2, ENZYMES and BZR for graph classification and node classification, as well as three dynamic datasets TAOBAO, KOUBEI and AMAZON for link prediction. More details about these datasets can be found in Table 4 in Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Methods and Baselines. We consider three versions of our proposed framework RAGRAPH: 1) RAGRAPH/NF, which indicates we utilize the plug-and-plag RAGRAPH without fine-tuning on the train set; 2) RAGRAPH/FT, which employs prompt tuning on the train set with RAG; and 3) RAGRAPH/NFT, which applies noise prompt tuning on the train set with RAG. For the baseline of the dynamic graph, we choose LightGCN [30], SGL [103], MixGCF [39], SimGCL [117], GraphPro [113] and GraphPro+PRODIGY [73]. For the static graph, we choose GCN [48], GraphSAGE [29],GAT [97], GIN [105], GraphPrompt [65], GraphPrompt+PRODIGY [73] as baselines. In addition, we denote \u2019/NF\u2019 and \u2019/FT\u2019 respectively to represent without fine-tuning and fine-tuning. A detailed description of baselines can be referred to in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "Settings and Evaluation. We establish a training-resource split with the remainder of the data reserved as unseen during fine-tuning. For static graphs, the split is based on node partitioning with the ratio of $50\\%{:}30\\%$ [65], while for dynamic graphs, it is based on partitioning snapshots with the history snapshots as resource graph [73]. For fair comparisons, for methods employing PRODIGY and RAGRAPH, $\\pmb{\\mathrm{\\Sigma}}$ we fine-tune models using the training set while retrieving the resource graph to prevent information leakage and over-fitting; $\\pmb{\\varphi}$ when testing, we retrieve the combined training and resource graphs. For other methods, fine-tuning was directly performed on the combined train and resource set for fairness. For the evaluation of static graphs, we refer GraphPrompt, utilizing pre-trained GNNs for both node- and graph-level tasks within a $k$ -shot classification framework. For dynamic graphs, we follow GraphPro to employ pre-trained GNNs on a substantial dataset fraction, with fine-tuning and testing conducted on later snapshots. Moreover, we pre-train GraphPro and GraphPrompt unsupervised on other datasets within the similar domain following [65, 73] to avoid information leakage. For classification tasks, we utilize the accuracy as evaluation matric; For link prediction tasks, we use standard metrics Recall $@_{\\mathrm{k}}$ and nDCG $@_{\\mathrm{k}}$ at $k\\!=\\!20$ , in line with existing methodologies [30, 103, 117]. The metrics used in the experiment are detailed in Appendix D.2 and the implementation details of RAGRAPH and baselines are in Appendix D.4. ", "page_idx": 7}, {"type": "table", "img_path": "Dzk2cRUFMt/tmp/79f51b46c8bb87b514d12ae8bb0158ee46e6c68ae15dc6ed900c5368e41e1440.jpg", "table_caption": ["Table 1: Accuracy evaluation on node and graph classification. All tabular results $(\\%)$ are in mean\u00b1 standard deviation across five seeds run, with best bolded and runner-up underlined. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Retrieval-Augmented Graph Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed, we conduct experiments and report the results of the three graph tasks for static graph and dynamic graph, as illustrated in Table 1 and Table 2. From the reported accuracy, we can find the following observations: ", "page_idx": 8}, {"type": "text", "text": "Outperforming SOTA Methods. First, our proposed RAGRAPH outperforms almost all the baselines across the three graph tasks, demonstrating the effectiveness of RAGRAPH in transferring knowledge from the pre-training to downstream tasks compared to traditional GNNs i.e., GCN and GraphSAGE. It achieves the highest average accuracy across almost all tasks on ENZYMES, with an improvement of at least $5.19\\%$ in the static graph, and up to $1.81\\%$ on the dynamic graph over the best baseline PRODIGY/FT. We argue that by virtue of the integration of hidden embedding and task-specific output vector, RAGRAPH is able to comprehend more knowledge than simply learns the paradigm from $X\\rightarrow Y$ . Second, compared with the models of PRODIGY/NF and RAGRAPH/NF, the introduction of noise training in noise prompt tuning also improves the robustness of the model, avoiding the influence of a large amount of noise on the information aggregation inside the query graph. ", "page_idx": 8}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/c9ea8cc05e9de1883174028b5ab8e9f5ab87f93f5906f359b5b531438dfd9ba4.jpg", "img_caption": ["Strong Retrieval-Augmented Performance on Unseen Datasets. We observe that PRODIGY/NF and RAGRAPH/NF are better to Vanilla/NF, indicating that the retrieval knowledge truly works when testing on unseen datasets. Moreover, the difference between PRODIGY/NF and PRODIGY/FT is much greater than that of RAGRAPH, which also indicates that a simple learning paradigm for ICL is not enough and that RAGRAPH can achieve acceptable results even on unseen downstream datasets without the need for sophisticated fine-tuning. ", "Figure 3: Hyper-parameter study with hops ${.k}$ (Left) from 1 to 5 andtopk from 1 to 20 (Right) on node classification with PROTEINS, and ENZYMES datasets with the setting in Table 1. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Dzk2cRUFMt/tmp/83f2b00f53eacda8baa9dc493cfadd530263fc14f77e2a0656def922578e05cb.jpg", "table_caption": ["Table 2: Performance evaluation $(\\%_{o})$ on link prediction. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Hyper-parameter Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we examine the impact of various hyper-parameters on RAGRAPH. We specifically analyze the effects of varying the number of hops $k$ in toy graphs from the list [1,2,3,4,5] and the number of linked toy graphs topK from the list [1,5,10,15,30,50] to verify the sensitive: ", "page_idx": 9}, {"type": "text", "text": "Figure 3 (Left) illustrates relationships between accuracy and the toy graph hop $k$ . We observe that as $k$ increases, the volume of retrieved knowledge grows exponentially. However, an excessive accumulation of knowledge not only fails to enhance accuracy but also introduces increased irrelevant noise that burdens the GNNs. Notably, accuracy shows a trend of initial improvement followed by a decline as $k$ is increased. This pattern suggests that at lower $k$ values, the retrieved information tends to consist of isolated, less useful knowledge. In contrast, at higher $k$ values, the GNNs struggle to process extensive reasoning chains, leading to the utilization of complex and abundant information that is less effective than even the baseline model\u2019s performance. Figure 3 (Right) shows effects on accuracy with different numbers of toy graphs topK. As with the previous figure, increasing topK demonstrates that an excessive amount of knowledge can hinder the GNNs\u2019 comprehension capabilities. Conversely, smaller topK results in insufficient knowledge to enhance performance on downstream tasks. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced RAGRAPH, a novel and general framework that enhances Graph Neural Networks (GNNs) by integrating Retrieval-Augmented Generation (RAG) techniques. This plug-and-play approach improves GNNs\u2019 ability to generalize to unseen data by retrieving relevant information. Experimental results show that RAGRAPH outperforms state-of-the-art methods in various graph learning tasks, demonstrating its adaptability and robustness. While RAGRAPH is currently limited to retrieving subgraphs, future research could explore using more graph-structured data such as nodes, edges, and trees to further enhance its capabilities. In general, our work provides valuable insights and serves as a reference for future Large Graph Models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (No.U23A20468). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Asai, S. Min, Z. Zhong, and D. Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41\u201346, 2023.   \n[2] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In ICLR, 2024.   \n[3] S.-V. Bogolin, I. Croitoru, H. Jin, Y. Liu, and S. Albanie. Cross modal retrieval with querybank normalisation. In CVPR, 2022. [4] K. M. Borgwardt, C. S. Ong, S. Sch\u00f6nauer, S. Vishwanathan, A. J. Smola, and H.-P. Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005. [5] H. Cai, V. W. Zheng, and K. C.-C. Chang. A comprehensive survey of graph embedding: Problems, techniques, and applications. IEEE TKDE, 2018.   \n[6] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister. Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition, 2023. [7] M. Chen, W. Zhang, W. Zhang, Q. Chen, and H. Chen. Meta relational learning for few-shot link prediction in knowledge graphs. arXiv preprint arXiv:1909.01515, 2019. [8] W. Chen, H. Hu, X. Chen, P. Verga, and W. W. Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. In EMNLP, 2022. [9] X. Chen, S. Zhang, Y. Xiong, X. Wu, J. Zhang, X. Sun, Y. Zhang, Y. Zhao, and Y. Kang. Prompt learning on temporal interaction graphs. arXiv:2402.06326, 2024.   \n[10] Z. Cheng, J. Zhang, X. Xu, G. Trajcevski, T. Zhong, and F. Zhou. Retrieval-augmented hypergraph for multimodal social media popularity prediction. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201924, page 445\u2013455, New York, NY, USA, 2024. Association for Computing Machinery.   \n[11] X. Chu, Y. Jin, X. Wang, S. Zhang, Y. Wang, W. Zhu, and H. Mei. Wasserstein barycenter matching for graph size generalization of message passing neural networks. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[12] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri. The power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, volume 17 of SIGIR 2024, page 719\u2013729. ACM, July 2024.   \n[13] H. Cui, Z. Lu, P. Li, and C. Yang. On positional and structural node features for graph neural networks on non-attributed graphs, 2021.   \n[14] M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazar\u00e9, M. Lomeli, L. Hosseini, and H. J\u00e9gou. The faiss library, 2024.   \n[15] Y. Duan, G. Zhang, S. Wang, X. Peng, Z. Wang, J. Mao, H. Wu, X. Jiang, and K. Wang. Catgnn: Enhancing credit card fraud detection via causal temporal graph neural networks. ArXiv, abs/2402.14708, 2024.   \n[16] Y. Duan, J. Zhao, pengcheng, J. Mao, H. Wu, J. Xu, S. Wang, C. Ma, K. Wang, K. Wang, and X. Li. Causal deciphering and inpainting in spatio-temporal dynamics via diffusion model. 2024.   \n[17] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NeurIPS, 2015.   \n[18] P. L. Eli Chien, Jianhao Peng and O. Milenkovic. Adaptive universal generalized pagerank graph neural network. In ICLR, 2021.   \n[19] F. Fang, Y. Bai, S. Ni, M. Yang, X. Chen, and R. Xu. Enhancing noise robustness of retrievalaugmented language models with adaptive adversarial training, 2024.   \n[20] T. Fang, Y. Zhang, Y. Yang, C. Wang, and L. Chen. Universal prompt tuning for graph neural networks. In NeurIPS, 2024.   \n[21] Y. Fang, Y. Qin, H. Luo, F. Zhao, B. Xu, L. Zeng, and C. Wang. When spatio-temporal meet wavelets: Disentangled traffic forecasting via efficient spectral graph attention networks. In ICDE, 2023.   \n[22] L. Gao, X. Ma, J. Lin, and J. Callan. Precise zero-shot dense retrieval without relevance labels, 2022.   \n[23] X. Gao, H. Chen, and J. Haworth. A spatiotemporal analysis of the impact of lockdown and coronavirus on london\u2019s bicycle hire scheme: from response to recovery to a new normal. GIS, 2023.   \n[24] X. Gao, J. Haworth, D. Zhuang, H. Chen, and X. Jiang. Uncertainty quantification in the roadlevel traffic risk prediction by spatial-temporal zero-inflated negative binomial graph neural network (stzinb-gnn). GIScience 2023, 2023.   \n[25] X. Gao, X. Jiang, J. Haworth, D. Zhuang, S. Wang, H. Chen, and S. Law. Uncertainty-aware probabilistic graph neural networks for road-level traffic crash prediction. Accident Analysis & Prevention, 208:107801, 2024.   \n[26] Z. Gao, X. Sun, Z. Liu, Y. Li, H. Cheng, and J. Li. Protein multimer structure prediction via PPIguided prompt learning. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[27] M. Gjoka, M. Kurant, C. T. Butts, and A. Markopoulou. Walking in facebook: A case study of unbiased sampling of osns. In 2010 Proceedings IEEE INFOCOM, pages 1\u20139, 2010.   \n[28] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In SIGKDD, 2016.   \n[29] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017.   \n[30] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR, 2020.   \n[31] Z. He, W. Hao, W.-T. Lu, C. Chen, K. Lerman, and X. Song. Alcap: Alignment-augmented music captioner. In ICASSP, 2023.   \n[32] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021.   \n[33] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n[34] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. Gpt-gnn: Generative pre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1857\u20131867, 2020.   \n[35] J. Huang, G. Cai, J. Zhu, Z. Dong, R. Tang, W. Zhang, and Y. Yu. Recall-augmented ranking: Enhancing click-through rate prediction accuracy with cross-stage data. In WWW, 2024.   \n[36] K. Huang and M. Zitnik. Graph meta learning via local subgraphs, 2020.   \n[37] M. Huang, Y. Liu, X. Ao, K. Li, J. Chi, J. Feng, H. Yang, and Q. He. Auc-oriented graph neural network for fraud detection. In WWW, 2022.   \n[38] Q. Huang, H. Ren, and J. Leskovec. Few-shot relational reasoning via connection subgraph pretraining. In NeurIPS, 2022.   \n[39] T. Huang, Y. Dong, M. Ding, Z. Yang, W. Feng, X. Wang, and J. Tang. Mixgcf: An improved training method for graph neural network-based recommender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 665\u2013674, 2021.   \n[40] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave. Atlas: Few-shot learning with retrieval augmented language models, 2022.   \n[41] X. Jiang, Y. Fang, R. Qiu, H. Zhang, Y. Xu, H. Chen, W. Zhang, R. Zhang, Y. Fang, X. Chu, J. Zhao, and Y. Wang. Tc-rag:turing-complete rag\u2019s case study on medical llm systems. ArXiv, abs/2408.09199, 2024.   \n[42] X. Jiang, Z. Qin, J. Xu, and X. Ao. Incomplete graph learning via attribute-structure decoupled variational auto-encoder. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM \u201924, page 304\u2013312, New York, NY, USA, 2024. Association for Computing Machinery.   \n[43] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang, H. Ding, X. Chu, J. Zhao, and Y. Wang. Hykge: A hypothesis knowledge graph enhanced framework for accurate and reliable medical llms responses, 2024.   \n[44] X. Jiang, D. Zhuang, X. Zhang, H. Chen, J. Luo, and X. Gao. Uncertainty quantification via spatial-temporal tweedie model for zero-inflated and long-tail travel demand prediction. In CIKM, 2023.   \n[45] R. Y. Jiaxuan You and J. Leskovec. Position-aware graph neural networks. In ICML, 2019.   \n[46] B. Jin, Y. Zhang, Q. Zhu, and J. Han. Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. In SIGKDD, pages 1020\u20131031, 2023.   \n[47] J. Kim, S. M. Jaehyun Nam, J. Park, S.-W. Lee, M. Seo, J.-W. Ha, and J. Shin. Sure: Summarizing retrievals using answer candidates for open-domain qa of llms. In ICLR, 2024.   \n[48] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016.   \n[49] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W. tau Yih, T. Rockt\u00e4schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledgeintensive nlp tasks, 2021.   \n[50] G. Li, M. M\u00fcller, A. Thabet, and B. Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, 2019.   \n[51] H. Li, X. Wang, Z. Zhang, and W. Zhu. Out-of-distribution generalization on graphs: A survey, 2022.   \n[52] K. Li, Y. Chen, Y. Liu, J. Wang, Q. He, M. Cheng, and X. Ao. Boosting the adversarial robustness of graph neural networks: An ood perspective. In International Conference on Learning Representations, 2024.   \n[53] K. Li, Y. Liu, X. Ao, J. Chi, J. Feng, H. Yang, and Q. He. Reliable representations make a stronger defender: Unsupervised structure refinement for robust gnn. In SIGKDD, 2022.   \n[54] R. Li, T. Zhong, X. Jiang, G. Trajcevski, J. Wu, and F. Zhou. Mining spatio-temporal relations via self-paced graph contrastive learning. In SIGKDD, 2022.   \n[55] S. Li, R. Xie, Y. Zhu, X. Ao, F. Zhuang, and Q. He. User-centric conversational recommendation with multi-aspect user modeling. In SIGIR, 2022.   \n[56] X. Li, D. Lian, Z. Lu, J. Bai, Z. Chen, and X. Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. In NeurIPS, volume 36, 2024.   \n[57] X. Li, R. Zhao, Y. K. Chia, B. Ding, S. Joty, S. Poria, and L. Bing. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources, 2023.   \n[58] W. Lin and B. Byrne. Retrieval augmented visual question answering with outside knowledge. In EMNLP, 2022.   \n[59] W. Lin, J. Mei, J. Chen, and B. Byrne. Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers, 2024.   \n[60] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis, L. Zettlemoyer, and S. Yih. Ra-dit: Retrieval-augmented dual instruction tuning. In ICLR, 2024.   \n[61] Y. Liu, X. Ao, F. Feng, and Q. He. Ud-gnn: Uncertainty-aware debiased training on semihomophilous graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1131\u20131140, 2022.   \n[62] Y. Liu, X. Ao, F. Feng, Y. Ma, K. Li, T.-S. Chua, and Q. He. Flood: A flexible invariant learning framework for out-of-distribution generalization on graphs. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1548\u20131558, 2023.   \n[63] Y. Liu, X. Ao, Z. Qin, J. Chi, J. Feng, H. Yang, and Q. He. Pick and choose: a gnn-based imbalanced learning approach for fraud detection. In Proceedings of the Web Conference 2021, pages 3168\u20133177, 2021.   \n[64] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, and L. Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models, 2024.   \n[65] Z. Liu, X. Yu, Y. Fang, and X. Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In WWW, 2023.   \n[66] S. Lu, N. Duan, H. Han, D. Guo, S. won Hwang, and A. Svyatkovskiy. Reacc: A retrievalaugmented code completion framework. In ACL, 2022.   \n[67] H. Luo, Y.-S. Chuang, Y. Gong, T. Zhang, Y. Kim, X. Wu, D. Fox, H. Meng, and J. Glass. Sail: Search-augmented instruction learning, 2023.   \n[68] J. Luo, W. Zhang, Y. Fang, X. Gao, D. Zhuang, H. Chen, and X. Jiang. Timeseries suppliers allocation risk optimization via deep black litterman model. ArXiv, abs/2401.17350, 2024.   \n[69] K. Ma, H. Cheng, Y. Zhang, X. Liu, E. Nyberg, and J. Gao. Chain-of-skills: A configurable model for open-domain question answering, 2023.   \n[70] H. Mao, X. Chen, Q. Fu, L. Du, S. Han, and D. Zhang. Neuron campaign for initialization guided by information bottleneck theory. In CIKM, 2021.   \n[71] S. Maskey, R. Levie, Y. Lee, and G. Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4805\u20134817. Curran Associates, Inc., 2022.   \n[72] S. Matsugu, Y. Fujiwara, and H. Shiokawa. Uncovering the largest community in social networks at scale. In IJCAI, 2023.   \n[73] K. Mishchenko and A. Defazio. Prodigy: An expeditiously adaptive parameter-free learner. In NeurIPS, 2023.   \n[74] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.   \n[75] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[76] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[77] S. Parashar, Z. Lin, T. Liu, X. Dong, Y. Li, D. Ramanan, J. Caverlee, and S. Kong. The neglected tails of vision-language models. In CVPR, 2024.   \n[78] A.Paszke, S.Gross, F.Massa, A.Lerer, J.Bradbury, G.Chanan, T.Killeen, Z.Lin, N.Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.   \n[79] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, 2014.   \n[80] Y. Qin, Y. Fang, H. Luo, F. Zhao, and C. Wang. Next point-of-interest recommendation with auto-correlation enhanced multi-modal transformer network. In SIGIR, 2022.   \n[81] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1150\u20131160, 2020.   \n[82] Y. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering, 2021.   \n[83] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI Conference on Artificial Intelligence, pages 4292\u20134293, 2015.   \n[84] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D. Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In ICLR, 2024.   \n[85] S. Sharma and R. Sharma. Identifying possible rumor spreaders on twitter: A weak supervised learning approach. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2021.   \n[86] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, C. Villouta-Reyes, G. Cerono, Y. Shi, A. Rizk-Jackson, S. Israni, C. A. Nelson, S. Huang, and S. E. Baranzini. Biomedical knowledge graph-enhanced prompt generation for large language models, 2023.   \n[87] X. Su and T. M. Khoshgoftaar. A survey of collaborative filtering techniques. Adv. in Artif. Intell., 2009, Jan. 2009.   \n[88] J. Sun, Y. Zhou, and C. Zong. One-shot relation learning for knowledge graphs via neighborhood aggregation and paths encoding. Transactions on Asian and Low-Resource Language Information Processing, 21(3):1\u201319, 2021.   \n[89] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. In SIGKDD, 2022.   \n[90] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan. All in one: Multi-task prompting for graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining (KDD\u201923), page 2120\u20132131, 2023.   \n[91] X.Sun, J.Zhang, X.Wu, H.Cheng, Y.Xiong, andJ.Li. Graphpromptlearning: Acomprehensive survey and beyond. arXiv:2311.16534, 2023.   \n[92] Y. Tan, H. Lv, X. Huang, J. Zhang, S. Wang, and C. Yang. Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining, 2024.   \n[93] Y. Tan, Z. Zhou, H. Lv, W. Liu, and C. Yang. Walklm: A uniform language model fine-tuning framework for attributed graph embedding. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, NeurIPS, volume 36, pages 13308\u201313325. Curran Associates, Inc., 2023.   \n[94] Z. Tan, R. Guo, K. Ding, and H. Liu. Virtual node tuning for few-shot node classification. In SIGKDD, 2023.   \n[95] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015.   \n[96] B. Teji, S. Roy, D. S. Dhami, D. Bhandari, and P. H. Guzzi. Graph embedding techniques for predicting missing links in biological networks: An empirical evaluation. IEEE Transactions on Emerging Topics in Computing, 12(1):190\u2013201, 2024.   \n[97] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks. In ICLR, 2018.   \n[98] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li. Task-adaptive few-shot node classification. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022.   \n[99] S. Wang, Y. Dong, X. Huang, C. Chen, and J. Li. FAITH: Few-shot graph classification with hierarchical task graphs. In International Joint Conference on Artificial Intelligence, 2022.   \n[100] X. Wang, L. Zhu, and Y. Yang. T2vlad: Global-local sequence alignment for text-video retrieval. In CVPR, 2021.   \n[101] Y. Wang, R. Ren, J. Li, W. X. Zhao, J. Liu, and J. Wen. Rear: A relevance aware retrieval augmented framework for open-domain question answering, 2024.   \n[102] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.   \n[103] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie. Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 726\u2013735, 2021.   \n[104] L. Xia, B. Kao, and C. Huang. Opengraph: Towards open graph foundation models, 2024.   \n[105] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In ICLR, 2019.   \n[106] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro. Retrieval meets long context large language models. In ICLR, 2024.   \n[107] Y. Xu, X. Chu, K. Yang, Z. Wang, P. Zou, H. Ding, J. Zhao, Y. Wang, and B. Xie. Seqcare: Sequential training with external medical knowledge graph for diagnosis prediction in healthcare data. In Proceedings of the ACM Web Conference 2023, pages 2819\u20132830, 2023.   \n[108] Y. Xu, X. Jiang, X. Chu, Y. Xiao, C. Zhang, H. Ding, J. Zhao, Y. Wang, and B. Xie. Protomix: Augmenting health status representation learning via prototype-based mixup. In Knowledge Discovery and Data Mining, 2024.   \n[109] Y. Xu, K. Yang, C. Zhang, P. Zou, Z. Wang, H. Ding, J. Zhao, Y. Wang, and B. Xie. Vecocare: Visit sequences-clinical notes joint learning for diagnosis prediction in healthcare data. In IJCAI, volume 23, pages 4921\u20134929, 2023.   \n[110] Y. Xu\\*, R. Zhang\\*, X. Jiang\\*, Y. Feng, Y. Xiao, X. Ma, R. Zhu, X. Chu, J. Zhao, and Y. Wang. Parenting: Optimizing knowledge selection of retrieval-augmented language models with parameter decoupling and tailored tuning. arXiv preprint arXiv:2410.10360, 2024.   \n[111] K. Yang, Y. Xu, P. Zou, H. Ding, J. Zhao, Y. Wang, and B. Xie. Kerprint: local-global knowledge graph enhanced diagnosis prediction for retrospective and prospective interpretations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5357\u20135365, 2023.   \n[112] S. Yang, X. Jiang, H. Zhao, W. Zeng, H. Liu, and Y. Jia. Faima: Feature-aware in-context learning for multi-domain aspect-based sentiment analysis. In COLING, 2024.   \n[113] Y. Yang, L. Xia, D. Luo, K. Lin, and C. Huang. Graphpro: Graph pre-training and prompt learning for recommendation. In WWW, 2024.   \n[114] J. Yoo, N. Ahn, and K.-A. Sohn. Rethinking data augmentation for image super-resolution: A comprehensive analysis and a new strategy. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8372\u20138381, 2020.   \n[115] O. Yoran, T. Wolfson, O. Ram, and J. Berant. Making retrieval-augmented language models robust to irrelevant context. In ICLR, 2024.   \n[116] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812\u20135823, 2020.   \n[117] J. Yu, H. Yin, X. Xia, T. Chen, L. Cui, and Q. V. H. Nguyen. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, pages 1294\u20131303, 2022.   \n[118] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu. Chain-of-note: Enhancing robustness in retrieval-augmented language models, 2023.   \n[119] X. Yu, Y. Fang, Z. Liu, Y. Wu, Z. Wen, J. Bo, X. Zhang, and S. C. Hoi. Few-shot learning on graphs: from meta-learning to pre-training and prompting. arXiv preprint arXiv:2402.01440, 2024.   \n[120] X. Yu, Z. Liu, Y. Fang, Z. Liu, S. Chen, and X. Zhang. Generalized graph prompt: Toward a unification of pre-training and downstream tasks on graphs. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[121] X. Yu, Z. Liu, Y. Fang, and X. Zhang. Dygprompt: Learning feature and time prompts on dynamic graphs. arXiv preprint arXiv:2405.13937, 2024.   \n[122] X. Yu, J. Zhang, Y. Fang, and R. Jiang. Non-homophilic graph pre-training and prompt learning. arXiv preprint arXiv:2408.12594, 2024.   \n[123] X. Yu, C. Zhou, Y. Fang, and X. Zhang. Multigprompt for multi-task pre-training and prompting on graphs. In WWW, 2024.   \n[124] Z. Yuan, Q. Jin, C. Tan, Z. Zhao, H. Yuan, F. Huang, and S. Huang. Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pre-training. In CVPR, 2023.   \n[125] C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V. Chawla. Few-shot knowledge graph completion. In AAAI, 2020.   \n[126] R. Zhang, X. Jiang, Y. Fang, J. Luo, Y. Xu, Y. Zhu, X. Chu, J. Zhao, and Y. Wang. Infinitehorizon graph fliters: Leveraging power series to enhance sparse information aggregation. ArXiv, abs/2401.09943, 2024.   \n[127] R. Zhang, Y. Xu, Y. Xiao, R. Zhu, X. Jiang, X. Chu, J. Zhao, and Y. Wang. Kapo: Knowledgeaware preference optimization for controllable knowledge selection in retrieval-augmented language models. arXiv preprint arXiv:2408.03297, 2024.   \n[128] Y. Zhang, Z. Chen, Y. Fang, L. Cheng, Y. Lu, F. Li, W. Zhang, and H. Chen. Knowledgeable preference alignment for llms in domain-specific question answering, 2023.   \n[129] Z. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu. Graph meets llms: Towards large graph models, 2023.   \n[130] H. Zhao, A. Chen, X. Sun, H. Cheng, and J. Li. All in one and one for all: A simple yet effective method towards cross-domain graph pretraining, 2024.   \n[131] R. Zhao, H. Chen, W. Wang, F. Jiao, X. L. Do, C. Qin, B. Ding, X. Guo, M. Li, X. Li, and S. Joty. Retrieving multimodal information for augmented generation: A survey, 2023.   \n[132] J. Zhou, C. Xie, Z. Wen, X. Zhao, and Q. Xuan. Data augmentation on graphs: A technical survey, 2023.   \n[133] S. Zhou, U. Alon, F. F. Xu, Z. Wang, Z. Jiang, and G. Neubig. Docprompting: Generating code by retrieving the docs. In ICLR, 2023.   \n[134] X. Zhou, R. Lumbantoruan, Y. Ren, L. Chen, X. Yang, and J. Shao. Dynamic bi-layer graph learning for context-aware sequential recommendation. ACM Trans. Recomm. Syst., 2(2), apr 2024.   \n[135] F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. Retrieving and reading: A comprehensive survey on open-domain question answering, 2021.   \n[136] K. Zhu, X. Feng, X. Du, Y. Gu, W. Yu, H. Wang, Q. Chen, Z. Chu, J. Chen, and B. Qin. An information bottleneck perspective for effective noise filtering on retrieval-augmented generation. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044\u20131069, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics.   \n[137] Y. Zhu, J. Guo, and S. Tang. Sgl-pt: A strong graph learner with graph prompt tuning, 2023.   \n[138] Y. Zhu, Z. Ou, X. Mou, and J. Tang. Retrieval-augmented embodied agents. In CVPR, 2024. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The notations in this paper are summarized in Table 3. ", "page_idx": 18}, {"type": "table", "img_path": "Dzk2cRUFMt/tmp/4a08df10f514fc2abe9944bb385403b3085cd5e43d3ad4c6747251532f8ee5bf.jpg", "table_caption": ["Table 3: Notations Tables in RAGRAPH "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B More Motivation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Why Toy Graph Augmentation is needed ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The reasons for toy graph augmentation: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Expanding toy graph base, enriching the scale of the knowledge repository [114]. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Simulating Real-World Scenarios: Real-world graphs often encounter challenges such as missing nodes [42], noisy attributes [52], and unexplored connections [96]. We introduce node dropout, noise injection, and edge removal to simulate these scenarios accurately. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Addressing Graph Domain Shift: To mitigate domain shift between the graph knowledge base and testing graphs, our augmentations employ Mixup techniques such as Node Interpolation and Edge Rewiring. These techniques interpolate between training samples to generate synthetic samples, effectively smoothing decision boundaries in embedding and reducing the model\u2019s sensitivity to minor variations in input data, thereby stabilizing predictions on domain shift testing samples [108]. ", "page_idx": 18}, {"type": "text", "text": "B.2 Why Noise-based Graph Prompt Tuning is needed ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To address inherent challenges in toy graph quality, we introduce Noise-based Graph Prompting Tuning (c.f. Section 4.3.3). This method involves fine-tuning the model with artificially introduced noisy toy graphs (Inner-Toy-Graph Noise & Toy-Graph Noise), inspired by noise-tuning techniques in NLP [19, 12, 115]. Our approach enhances the model\u2019s robustness against real-world retrieval noise, as evidenced by superior performance compared to traditional tuning methods (in Main Text Tables 1 and 2). This approach reduces the stringent requirement for an exceptionally high-quality graph vector base, thereby ensuring robust performance across various tasks within our RAGRAPH, and significantly mitigating data quality impacts. ", "page_idx": 19}, {"type": "text", "text": "B.3 Difficulty to construct and maintain high-quality and diverse graph vector base ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In RAGRAPH, the toy graph base largely leverages significant prior research datasets in pre-trained GNNs [65, 104, 73, 123], which are trained on meticulously curated graph datasets and cover diverse domains, suchasbiology, chemistry, medicinerecommendationtasks, etc. Forexample, thePROTEINS dataset [4], derived from cryo-electron microscopy and X-ray crystallography, and the ENZYMES dataset [99], based on EC enzyme classification, are meticulously annotated by medical experts. ", "page_idx": 19}, {"type": "text", "text": "B.4 Why Inverse Importance Sampling Strategy is needed ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The adoption of the Inverse Importance Sampling strategy is crucial. In RAGRAPH, subgraphs are sampled as toy graphs, where nodes with higher degrees (non-long-tail knowledge, extensively learned and embedded into GNN parameters) are more frequently included in subgraphs due to their extensive connections with neighbors, resulting in higher frequency in toy graph base [27]. Conversely, nodes with low degrees (long-tail knowledge), are more important but ignored. To mitigate this issue, we propose this by prioritizing nodes with lower degrees to capture long-tail knowledge when sampling. ", "page_idx": 19}, {"type": "text", "text": "B.5 Why Four Similarities are needed ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In practical applications, the four similarities all contribute to performance improvement and we state the significance as follows: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Time information is crucial to predict future states or trends [113] via node history, i.e. in social networks, analyzing historical user interaction aids in predicting future behaviors.   \n\u2022 Structure pertains to how nodes are interconnected and overall graph topology, vital for capturing similar graph structure patterns [13, 42, 112]. In transportation networks, factories are always located on the outer ring of the city, sharing similar structural connectivity, aiding in the discovery of spatiotemporal patterns [54, 16].   \n\u2022 Sharing similar neighborhoods is essential for evaluating node similarity and correlation. In recommendations, shared purchase histories between users and products indicate potential interests, akin to collaborative filtering [87].   \n\u2022 Semantic information measures similarity based on features [73]. In knowledge graphs, identifying relevant subgraphs to query nodes enhances retrieval accuracy based on semantic similarity. ", "page_idx": 19}, {"type": "text", "text": "B.6 Why Knowledge Fusion is needed ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Fusion and decoder here represent one of the core contributions of RAGRAPH: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Overall Task Perspective: For the same tasks, the decoder can be directly employed to obtain outputs. For different tasks, the decoder can be masked and utilize pre-computed embeddings without training or be tuned to better adapt. This underscores our primary contribution, where the decoder functions as a versatile \"plug-and-play\" and \"tune-free\" component.   \n\u2022 Integral Fusion Strategy: Fusion Strategy facilitates concurrent information propagation from toy graphs X (hidden embeddings) and Y (task-specific output vector) to query graph, aligning with our secondary contribution. ", "page_idx": 19}, {"type": "text", "text": "B.7 How RAGRAPH works on par with RAG in NLP and CV ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In NLP, RAG enhances the generation of LLM by retrieving relevant information via prompts. Similarly, in RAGRAPH, we enhance downstream graph learning by integrating information from retrieved toy graphs. Using these toy graphs with shared patterns assists the model inference. In our framework, the \"generation\" involves the retrieval-enhanced Graph Prompt: Toy Graph Intra Propagate & Query-ToyGraph Inter Propagate to propagate retrieved knowledge (X and Y) into the query graph. To illustrate, we analyze this from both experiment and theory. ", "page_idx": 20}, {"type": "text", "text": "1. Experiment 1: We perform a case study to illustrate how \"generation\" works by displaying specific instances of node vectors in Appendix D.6. ", "page_idx": 20}, {"type": "text", "text": "2. Experiment 2: In traditional GNN tasks, GCN, GAT, and GIN typically expand their receptive fields through stacked message-passing layers or neighborhood subgraph sampling for inference. Patterns learned in these contexts are often localized within the constrained receptive field. In contrast, in RAGRAPH, we observe that subgraphs sharing similar patterns often exhibit properties more aligned with downstream tasks. These subgraphs provide richer information for inference compared to simply enlarging receptive fields. As shown in Main Text Tables 1 and 2, Figure 3, RAGRAPH\u2019s strategy of incorporating toy graphs significantly outperforms baselines. ", "page_idx": 20}, {"type": "text", "text": "3. Theory 1: Furthermore, we provide a theoretical justification of retrieval augmentation in GNNs (see Appendix B.4). From an information-theoretic perspective, introducing RAG knowledge into GNNs enhances the mutual information between input features $X$ and output labels $Y$ , such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\nI(X,R A G;Y)\\!\\geq\\!I(X;Y),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "thereby improving the performance of downstream tasks. This is aligned with the information theory of RAG in NLP [71]. ", "page_idx": 20}, {"type": "text", "text": "4. Theory 2: Recent studies [11, 136] also suggest the generalization error diminishes with an increase in the node number of the graph in Theorem 1.1 [136]: the generalization error between the expected loss $R_{\\mathrm{exp}}(\\Theta)=\\mathbb{E}_{(x,y)\\sim\\mu_{G}}[\\bar{\\mathcal{L}}(\\Theta(x),y)]$ and empirical loss $\\begin{array}{r}{R_{\\mathrm{emp}}(\\Theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\Theta(x^{i}),\\dot{y}^{i})}\\end{array}$ are super bounded: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|R_{\\mathrm{exp}}(\\Theta)\\!-\\!R_{\\mathrm{emp}}(\\Theta)|\\!\\leq\\!\\sqrt{\\frac{C}{m}q(n)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C$ represents the model complexity (e.g., parameters), $m$ denotes the training set size, and $q(n)=\\mathbb{E}_{n\\sim\\nu_{G}}[n^{-\\frac{1}{D+1}}]$ depends on the average graph size (node number) with $\\nu$ as the graph size distribution and $D$ is the metric-measure space dimension. In RAGRAPH, retrieving similar toy graphs significantly increases the number of graph nodes (via Query-Toy-Graph Inter Propagate, linking toy graph nodes to query graph), significantly augmenting $n$ while reducing $q(n)$ . Consequently, the upper bound of generalization error decreases, promoting smoother graph learning convergence and enhancing pattern learning. ", "page_idx": 20}, {"type": "text", "text": "C Further Methods Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Revisiting Graph Neural Networks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The goal of a GNN is to learn node embeddings based on an iterative aggregation of messages from the local network neighborhood. We use embedding matrix $\\{\\mathbf{z}_{v}^{(L)}\\}_{v\\in\\mathcal{V}}$ to denote the embedding for all the nodes after applying an $L$ -layer GNN. The $l$ -th layer of a GNN, $\\{\\mathbf{z}_{v}^{(L)}\\}\\!=\\!\\mathbf{GNN}^{(l)}\\big(\\{\\mathbf{z}_{v}^{(l-1)}\\}\\big)$ , can be written as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{u\\rightarrow v}^{(l)}\\!=\\!\\mathbf{M}\\mathrm{SG}^{(l)}(\\mathbf{z}_{u}^{(l-1)},\\!\\mathbf{z}_{v}^{(l-1)}),}\\\\ &{\\mathbf{z}_{v}^{(l)}\\!=\\!\\mathbf{A}\\mathrm{GG}^{(l)}\\big(\\!\\{\\mathbf{m}_{u\\rightarrow v}^{(l)}\\,\\vert\\,u\\!\\in\\!\\mathcal{N}(v)\\},\\!\\mathbf{z}_{v}^{(l-1)}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{z}_{v}^{(l)}$ is the embedding for $v\\in V$ after passing through $l$ layers, $\\mathbf{z}_{v}^{(0)}\\mathop{=}x_{v}$ or $h_{v}$ or $o_{v}$ , $\\mathbf{m}_{u\\rightarrow v}^{(l)}$ is the message embedding, and $\\mathcal{N}(v)$ is the set of direct neighbors of $v$ . Different GNNs can have various definitions of message-passing functions $\\mathbf{MSG}^{(l)}(\\cdot)$ and aggregation functions $\\operatorname{AGG}^{(l)}(\\cdot)$ and these two functions could be parameter-free. ", "page_idx": 20}, {"type": "text", "text": "C.2 Key Construction of Position-aware Code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Given a randomly selected node anchor set $\\nu_{S}\\subset\\nu$ , we calculate the minimal distances, a.k.a. hops between the two node sets. Suppose $v_{u}\\in\\mathcal{V},v_{w}\\in\\mathcal{V}_{S}$ , the distance similarity between node $v_{u}$ and $v_{w}$ can be depicted as $d i s(v_{u},v_{w})$ . By normalizing the similarity to [0, 1], distance-to-centroid $d2c(v_{u},v_{w})$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nd2c(v_{u},v_{w})\\!=\\!\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle{\\frac{1}{d i s(v_{u},v_{w})\\!+\\!1}},}&{\\mathrm{if}\\;d i s(v_{u},v_{w})\\!<\\!d i s_{q}}\\\\ {\\displaystyle{0,}}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "here hyperparameter $d i s_{q}$ is the maximum hops, the distance beyond this boundary is considered invalid. The structure feature of node $v_{u}$ is $d2c(v_{u},\\mathcal{V}_{S})$ . By collecting all distances with anchor-set $\\nu_{S}$ , the structure $S\\!\\in\\!R^{n\\times|\\mathcal{V}_{S}|}$ is written as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d2c(v_{u},\\mathcal{V}_{S})\\!=\\![d2c(v_{u},\\!v_{w})|v_{w}\\in\\mathcal{V}_{S}\\subset\\mathcal{V}],}\\\\ &{S\\!=\\![d2c(v_{u},\\mathcal{V}_{S})|v_{u}\\in\\mathcal{V}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $[\\cdot]$ means the concatenation operation. The distance-to-centroid feature converts the nonEuclidean structure to the Euclidean structure. $d2c$ dramatically reduces the size of the matrix and meanwhile contains more structure information instead of identifier information, the size of the anchor set is $\\log_{2}n$ follows P-GNNs[45, 42]. ", "page_idx": 21}, {"type": "text", "text": "C.3 Similarity Functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the history key, we adopt an exponential decay function to measure the time similarity values. We smooth the impact of time differences and provide a controlled decay coefficient $\\eta\\!>\\!0$ . The time similarity, $S_{\\mathrm{time}}$ , between the same node $v_{c}$ and $v_{m}$ with different timestamp $t(v_{m}),t(v_{c})$ , is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{\\mathrm{time}}(v_{c},\\!v_{m})\\!=\\!e^{-\\eta|t(v_{c})-t(v_{m})|},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\eta$ is a positive parameter that controls the rate of exponential decay. ", "page_idx": 21}, {"type": "text", "text": "For the environment key, we match the environment of node $v$ using Jaccard similarity to compare the sets of neighbors $\\mathcal{N}(v_{c})$ in the query graph and $\\mathcal{N}(v_{m})$ in the toy graph: ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{\\mathrm{environment}}(v_{c},v_{m})\\!=\\!\\frac{|\\mathcal{N}(v_{c})\\cap\\mathcal{N}(v_{m})|}{|\\mathcal{N}(v_{c})\\cup\\mathcal{N}(v_{m})|}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the hidden embedding key, we input the query graph into pre-trained GNNs to obtain the hidden embedding for the query node, with the similarity defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{c}\\!=\\!\\mathbf{G}\\mathbf{NN}\\!\\left(X_{G^{Q}}\\right)\\!,\\quad S_{\\mathrm{semantic}}\\!\\left(v_{c},v_{m}\\right)\\!=\\!\\mathtt{c o s i n e}\\!\\left(h_{c},h_{m}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the position-aware code, we denote $s_{c},s_{m}$ as the position-aware code of node $v_{c},v_{m}$ , and utilize cosine similarity as before, defined as $\\scriptstyle S_{\\mathrm{structure}}(v_{c},v_{m})\\,=\\,\\mathtt{c o s i n e}(s_{c},s_{m})$ . ", "page_idx": 21}, {"type": "text", "text": "C.4 Proof of the Effectiveness of RAG ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we will theoretically prove that introducing RAG knowledge can significantly improve the predictive performance of the model. ", "page_idx": 21}, {"type": "text", "text": "Assume $X$ represents the input features, $Y$ represents the target output labels, and $R A G$ represents external knowledge related to the input features (or even the output labels). We analyze from the mutual information view, where $I(X;Y)$ quantifies the dependency between $X$ and $Y$ , which reflects the performance of the model, the larger the value, the better the performance of the model [54, 70]. By introducing RAG knowledge $R A G$ into GNNs, we can effectively increase the mutual information between the input features $X$ and the output labels $Y$ as $I(X,R A\\dot{G};Y)\\!\\geq\\!I(X;Y)$ , thereby improve the model\u2019s downstream task performance. The derivation is as follows: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{X,R\\ A G,Y\\rightleftharpoons\\atop\\gamma,R\\ A G,Y}p(X,R A G,Y)\\log\\frac{p(X,R A G,Y)}{p(X,R A G)p(Y)}-\\sum_{X,Y}p(X,Y)\\log\\frac{p(X,Y)}{p(X)p(Y)}}\\\\ &{=\\displaystyle\\sum_{X,R A G,Y\\rightleftharpoons\\atop\\gamma,R\\ A G,Y}p(X,R A G,Y)\\log\\frac{p(X,R A G,Y)}{p(X,R A G)p(Y)}-\\sum_{X,R A G,Y\\in\\mathcal{X}}p(X,R A G,Y)\\log\\frac{p(X,Y)}{p(X)p(Y)}}\\\\ &{=\\displaystyle\\sum_{X,R A G,Y\\rightmoon}p(X,R A G,Y)\\log\\Bigg(\\frac{p(X,R A G,Y)}{p(X,R A G)}\\cdot\\frac{p(X)}{p(X,Y)}\\Bigg)}\\\\ &{=\\displaystyle\\sum_{X,R A G,Y\\rightmoon}p(X,R A G,Y)\\log\\frac{p(X,R A G,Y)}{p(R A G,Y)p(X,Y)}}\\\\ &{=\\displaystyle\\sum_{X,R A G,Y\\rightmoon}p(R A G,Y)\\log\\frac{p(R A G,Y)}{p(R A G|X)p(Y)}}\\\\ &{=\\displaystyle\\sum_{X,R A G,Y\\rightmoon}p(R A G,Y|X)p(X)\\log\\frac{p(R A G,Y|X)}{p(R A G|X)p(Y|X)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$\\begin{array}{r}{=I(R A G;\\mathbf{Y}\\,|\\,X)\\ge0,}\\end{array}$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{\\sum_{X,R A G,\\mathbf{Y}}\\!=\\!\\sum_{X}\\!\\sum_{R A G}\\!\\sum_{\\mathbf{Y}}}\\end{array}$ . Moreover, $I(R A G;\\mathbf{Y}\\mid X)$ measures that how much additional knowledge $R A G$ provides to assist in predicting $Y$ based on $X$ , this term will approach zero when the $R A G$ is noise to the prediction task. In summary, the integration of RAG knowledge can enhance the mutual information between $X$ and $Y$ , thereby improving the performance and accuracy of downstream tasks. ", "page_idx": 22}, {"type": "text", "text": "C.5 Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we will provide a detailed description of the algorithms of Toy Graph Construction (Algorithm 1) and Training and Inference with Toy Graphs Retrieval (Algorithm 2). ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1. We outline the process for constructing toy graphs in Algorithm. 1. In line 1, the toy graph database $\\mathcal{G}^{\\mathcal{T}}$ is initialized. ", "page_idx": 22}, {"type": "text", "text": "Lines 2-15 describe the steps to construct toy graphs by iterating through each snapshot of the dynamic resource graph $\\mathcal{G}^{\\mathcal{R}}$ . ", "page_idx": 22}, {"type": "text", "text": "In more detail, lines 3-5 details calculate the importance and reverse importance of each node within the snapshot. Following this, lines 6-7 involve normalizing the sampling probabilities according to the reverse importance values. The selection of a master node and the generation of its $k$ -hop ego network are carried out in lines 8-9. Subsequently, line 10 involves augmenting the toy graph through specific data augmentation techniques. ", "page_idx": 22}, {"type": "text", "text": "Lines 11-16 detail the generation of key-value pairs for each toy graph. This includes saving the timestamp as the history key, the neighbors of the master node as the environment key, the structural encoding as the structure key, the hidden embedding as the semantic key, and the task-specific output vectors as the value. Each toy graph is then stored in the database $\\mathcal{G}^{\\mathcal{T}}$ . ", "page_idx": 22}, {"type": "text", "text": "Ultimately, in line 18, the algorithm returns the toy graph database $\\mathcal{G}^{\\mathcal{T}}$ . ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2. We introduce the algorithm for training and inference with toy graph retrieval in Algorithm. 2. Initially, in line 1, we define the required inputs, including the testing graph $\\mathcal{G}_{\\mathrm{test}}$ , the toy graph database $\\mathcal{G}^{\\mathcal{T}}$ , and other relevant parameters. The final output is the aggregated result $\\tilde{o}_{c}$ . ", "page_idx": 22}, {"type": "text", "text": "The RETRIEVETOYGRAPHS function, detailed in lines 3-11, initializes an empty similarity list and iterates over each toy graph in the database. Lines 5-6 compute various similarity metrics, and the overall similarity is determined in line 7. This similarity score is then added to the list. After sorting by similarity, the topK toy graphs are retrieved and returned in line 11. ", "page_idx": 22}, {"type": "text", "text": "Within the PROPAGATION function (lines 12-17), each retrieved toy graph undergoes intra-propagation in line 14. The intra-propagation step follows in line 16, ultimately returning the propagated results $\\mathbf{z}_{c}$ . ", "page_idx": 22}, {"type": "text", "text": "The KNOWLEDGEFUSION function, found in lines 18-21, combines the outputs from previous steps.   \nThe final combined outputs $\\tilde{o}_{c}$ are generated by the decoder and returned in line 21. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Toy Graph Construction ", "page_idx": 23}, {"type": "text", "text": "Require: DynamicResourceGraph $\\mathcal{G}^{\\mathcal{R}}$ , nodeimportancebalanceweight $\\alpha$ , toygraphscalingconstant $K$ , maximum hop $k$ ", "page_idx": 23}, {"type": "text", "text": "Ensure: Toy graph embedding key-value database $\\mathcal{G}^{\\mathcal{T}}$   \n1: Initialize toy graph database $\\dot{\\mathcal{G}}^{\\mathcal{T}}\\gets\\emptyset$   \n2: for each snapshot $G_{\\tau}^{\\mathcal{R}}\\!\\in\\!\\mathcal{G}^{\\mathcal{R}}$ do \u25b7Construct Toy Graphs   \n3: for each node $v\\in G_{\\tau}^{\\mathcal{R}}$ do   \n4: Calculate importance $I(v)\\!\\gets\\!\\alpha\\mathbf{P}\\mathbf{R}(v)\\!+\\!(1\\!-\\!\\alpha)\\mathbf{D}\\mathbf{C}(v)$   \n5: Reverse node importance I\u2032(v)\u2190I(v1)+\u03f5   \n6: end for   \n7: for each node $v\\!\\in\\!G_{\\tau}^{\\mathcal{R}}$ do   \n8: Normalize sampling probabilities $p_{i}\\leftarrow\\frac{I^{\\prime}(v_{i})}{\\sum_{j=1}^{n}I^{\\prime}(v_{j})}$   \n9: end for   \n10: Sample master node $v_{m}\\gets\\mathbb{W}$ EIGHTEDSAMPLING $(G_{\\tau}^{\\mathcal{R}},p_{i})$ based on probability $p_{i}$   \n11: Generate $k$ -hop ego net $G_{\\tau}^{e}(v_{m})$ for node $v_{m}$   \n12: Augment toy graph $\\{G^{\\mathcal{T}}\\}\\leftarrow$ DATAAUGMENTATION $(G_{\\tau}^{e}(v_{m}),n_{\\mathrm{aug}})$ with $n_{\\mathrm{aug}}\\big(G_{\\tau}^{e}(v_{m})\\big)=$   \n$\\lfloor K\\cdot\\bar{I}^{\\prime}(G_{\\tau}^{e}(v_{m}))\\rfloor$   \n13: for each $G^{\\mathcal{T}}\\bar{\\in}\\{G^{\\mathcal{T}}\\}\\,.$ do \u25b7Generate keys-values pair   \n14: Save timestamp $\\tau$ as history key   \n15: Save neighbors $\\mathcal{N}(v_{m}^{\\tau})$ of master node $v_{m}$ as environment key   \n16: Save structural encoding $s_{m}^{\\tau}$ of node $v_{m}$ via Eq. (9) as structure key   \n17: Save hidden embedding $h_{m}^{\\tau}$ by feeding $G^{\\mathcal{T}}$ into pre-trained GNNs as semantic key   \n18: Save the hidden embedding $\\{\\dot{h}_{i}^{\\tau}|v_{i}\\!\\in\\!\\check{G}^{T}\\}$ as value   \n19: Save task-specific output vectors $\\{o_{i}^{\\tau}|v_{i}\\in G^{T}\\}$ by feeding $\\{h_{i}^{\\tau}|v_{i}\\!\\in\\!G^{T}\\}$ into decoder as   \nvalue   \n20: Store toy graph $G^{\\mathcal{T}}$ into database $\\mathcal{G}^{\\mathcal{T}}$   \n21: end for   \n22: end for   \n23: return Toy graph database $\\mathcal{G}^{\\mathcal{T}}$ ", "page_idx": 23}, {"type": "text", "text": "The main algorithm begins in line 22. If fine-tuning is enabled and the prompt loss has not converged, lines 23-34 detail the process of toy graph retrieval and propagation for each query graph. This includes the optional addition of noise in lines 26-29. The hidden embedding and task-specific output vectors are propagated in lines 30-31, and the aggregated outputs are fused in line 32. The prompt loss is computed, and fine-tuned parameters are updated in lines 33-34. ", "page_idx": 23}, {"type": "text", "text": "If fine-tuning is not required, lines 35-39 describe a similar process of toy graph retrieval and propagation, without the fine-tuning steps. The aggregated outputs are computed directly. ", "page_idx": 23}, {"type": "text", "text": "D Further Experiment Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Datasets Statics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We employ eight benchmark datasets for evaluation including four public static classification datasets for node- and graph-level tasks. ", "page_idx": 23}, {"type": "text", "text": "(1) PROTEINS [4] is a collection of protein graphs, including the amino acid sequence, conformation, structure, and features such as active sites of the proteins. Each node represents a secondary structure, while each edge illustrates the neighboring relationship either within the amino acid sequence or in 3D space. The nodes are divided into three categories, and the graphs are classified into two distinct classes. This dataset is used for node and graph classification tasks, containing 1,113 graphs with an average of 39.06 nodes and 72.82 edges per graph, with a density of 4.8e-2. ", "page_idx": 23}, {"type": "text", "text": "(2) COX2 [83] is a dataset of molecular structures, including 467 cyclooxygenase-2 inhibitors. Each node represents an atom, and each edge signifies a chemical bond between atoms, such as single, double, triple, or aromatic bonds. All the molecules belong to two categories. This dataset is used for ", "page_idx": 23}, {"type": "text", "text": "Require: Testing graph $\\mathcal{G}_{\\mathrm{test}}$ , toy graph database $\\mathcal{G}^{\\mathcal{T}}$ , pre-trained GNN model $\\mathrm{{GNN}_{\\Theta_{0}}(\\cdot)}$ , number of   \nTopK toy graphs to retrieve, similarity weights $w_{1},w_{2},w_{3},w_{4}$ , fine-tuning flag $f$ ine_tune, noise   \nprompt-tuning flag add_noise   \nEnsure: Aggregated output $\\tilde{o}_{c}$   \n1: function RETRIEVETOYGRAPH $;(G^{\\mathcal{Q}},{\\mathcal{G}}^{\\mathcal{T}},T o p K)$   \n2: Initialize similarity list $\\{S\\}\\!\\leftarrow\\!\\emptyset$   \n3: for each toy graph $G^{\\mathcal{T}}\\bar{\\in}\\mathcal{G}^{\\mathcal{T}}$ do   \n4: Calculate time similarity $S_{\\mathrm{time}}$ , environment similarity $S_{\\mathrm{environment}}$ , structure similarity   \n$S_{\\mathrm{structure}}$ and semantic similarity $S_{\\mathrm{s}\\mathrm{t}}$ emantic   \n5: Compute similarity $S\\!\\gets\\!w_{1}\\!\\cdot\\!S_{\\mathrm{time}}\\!+\\!w_{2}\\!\\cdot\\!S_{\\mathrm{structure}}\\!+\\!w_{3}\\!\\cdot\\!S_{\\mathrm{environment}}\\!+\\!w_{4}\\!\\cdot\\!S_{\\mathrm{semantic}}$   \n6: Add $\\bar{(G^{\\mathcal{T}},S)}$ to $\\{S\\}$   \n7: end for   \n8: Sort $\\{S\\}$ by similarity in descending order   \n190:: rReetturirenv eR teotrpieKv etdo yt ogyr agprhasp $G_{\\mathrm{TopK}}^{\\mathcal{T}}\\!\\leftarrow\\!\\overline{{\\{G^{\\mathcal{T}}\\!\\in\\!\\{S\\}}}}$ $G_{\\mathrm{TopK}}^{\\mathcal{T}}$ with topK similarities}   \n11: end function   \n12: function PROPAGATION $\\mathrm{I}(G^{\\mathcal{Q}},G_{\\mathrm{TopK}}^{\\mathcal{T}})$   \n13: for each toy graph $G^{T}\\!\\in\\!G_{\\mathrm{TopK}}^{T}\\,\\mathbf{do}$   \n14: Perform Intra Propagation ${\\bf z}_{m}\\leftarrow$ INTRAPROPAGATION $(G^{\\mathcal{T}})$   \n15: end for   \n16: $\\mathbf{z}_{c}\\gets\\mathrm{INTERPROPAGATION}(G^{\\mathcal{Q}},G_{\\mathrm{TopK}}^{\\mathcal{T}})$   \n17: return $\\mathbf{z}_{c}$   \n18: end function   \n19: function KNOWLEDGEFUSION $(h_{c},\\ o_{c})$   \n20: Combined output $\\hat{o}_{c}\\!\\gets\\!\\gamma o_{c}\\!+\\!(1\\!-\\!\\gamma)\\mathrm{DECODER}(h_{c})$   \n21: return Combined outputs $\\tilde{o}_{c}$   \n22: end function   \n23: if fine_tune & $\\mathcal{L}_{\\mathrm{prompt}}$ not converged then   \n24: for each query graph $G^{\\mathcal{Q}}\\in\\mathcal{G}_{\\mathrm{test}}$ do \u25b7Toy Graph Retrieval and Propagation   \n25: $G_{\\mathrm{TopK}}^{\\mathcal{T}}\\leftarrow$ RETRIEVETOYGRAPHS $(G^{\\mathcal{Q}},{\\mathcal{G}}^{T},T o p K)$   \n26: if add_noise then   \n27: for each toy graph $G^{\\mathcal{T}}\\!\\in\\!G_{\\mathrm{TopK}}^{\\mathcal{T}}$ do   \n28: Introduce noise $G^{T}\\gets\\mathrm{ADDNOISE}(G^{T})$ \u25b7Inner-Toy-Graph Noise   \n2390:: eAnddd  fboorttomK toy graphs to $G_{\\mathrm{TopK}}^{\\mathcal{T}}$ \u25b7Toy-Graph Noise   \n31: end if   \n32: $\\begin{array}{r l}&{h_{c}\\gets\\mathrm{PROPAGATION}(G^{\\mathcal{Q}},G_{\\mathrm{TopK}}^{\\mathcal{T}})}\\\\ &{o_{c}\\gets\\mathrm{PROPAGATION}(G^{\\mathcal{Q}},G_{\\mathrm{TopK}}^{\\mathcal{T}})}\\end{array}$ $\\triangleright$ Propogate hidden embedding   \n33: $\\triangleright$ Propogate task-specific output vector   \n34: Aggregated outputs $\\hat{o}_{c}\\gets\\mathrm{KNOWLEDGEFUSION}(h_{c},o_{c})$   \n35: Compute loss $\\mathcal{L}_{\\mathrm{prompt}}$ via $\\tilde{o}_{c}$ and $\\hat{O}_{c}$ $\\triangleright$ Based on task-specific loss function   \n36: Update fine-tuned parameters $\\Theta$ by minimizing $\\mathcal{L}_{\\mathrm{prompt}}$   \n37: end for   \n38: else   \n39: for each query graph $G^{\\mathcal{Q}}\\in\\mathcal{G}_{\\mathrm{test}}$ do $\\triangleright$ Toy Graph Retrieval and Propagation   \n40: $G_{\\mathrm{TopK}}^{\\mathcal{T}}\\!\\gets\\!\\bar{\\mathrm{RETRIEVEToYGRAPHS}}(G^{\\mathcal{Q}},\\mathcal{G}^{\\mathcal{T}},T o p K)$   \n41: $h_{c}\\gets\\mathsf{P R O P A G A T I O N}(G^{\\mathcal{Q}},G_{\\mathrm{ToDK}}^{\\mathcal{T}})$ \u25b7Propogate hidden embedding   \n42: $o_{c}\\gets\\mathrm{PROPAGATION}(G^{\\mathcal{Q}},G_{\\mathrm{TopK}}^{\\mathcal{T}})$ $\\triangleright$ Propogate task-specific output vector   \n43: Aggregated outputs $\\hat{O}_{c}\\gets$ KNOWLEDGEFUSION $(h_{c},\\!o_{c})$   \n44: end for   \n45: end if   \n46: return Aggregated outputs $\\tilde{o}_{c}$ ", "page_idx": 24}, {"type": "table", "img_path": "Dzk2cRUFMt/tmp/67754c39a05c09764c4abb7eaf0f0adfa8586ed5305fb0331b7c82074362a136.jpg", "table_caption": ["Table 4: Statistics of the experimental datasets and summary of datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "graph classification tasks, with each graph having an average of 41.22 nodes and 43.45 edges and a density of $2.6\\mathrm{e}{-2}$ . ", "page_idx": 25}, {"type": "text", "text": "(3) ENZYMES [99] is a dataset of 600 enzymes collected from the BRENDA enzyme database. These enzymes are labeled into 6 categories according to their top-level EC enzyme classification. This dataset is used for node and graph classification tasks, with each graph having an average of 32.63 nodes and 62.14 edges and a density of $5.9\\mathrm{e}{-2}$ . ", "page_idx": 25}, {"type": "text", "text": "(4) BZR [83] is a collection of 405 ligands for the benzodiazepine receptor. Each ligand is represented by a graph, and all ligands are categorized into two groups. This dataset is used for graph classification tasks, with each graph having an average of 35.75 nodes and 38.36 edges and a density of 3.0e-2. ", "page_idx": 25}, {"type": "text", "text": "Additionally, we leverage three publicly available datasets encompassing a wide array of real-world scenarios in dynamic recommendation (link prediction): ", "page_idx": 25}, {"type": "text", "text": "(5) The TAOBAO dataset captures implicit feedback data from Taobao.com, a prominent Chinese e-commerce platform, collected over a span of 10 days. This dataset is used for edge classification tasks, containing 204,168 nodes and 8,795,404 edges, with a density of 8.6e-4. ", "page_idx": 25}, {"type": "text", "text": "(6) The KOUBEI dataset records 9 weeks of user interactions with nearby stores on Koubei, a platform integrated within Alipay. This dataset is used for edge classification tasks, containing 221,366 nodes and 3,986,609 edges, with a density of 3.3e-4. ", "page_idx": 25}, {"type": "text", "text": "(7) The AMAZON dataset comprises a collection of product reviews sourced from Amazon, spanning a duration of 13 weeks. This dataset is used for edge classification tasks, containing 238,735 nodes and 876,237 edges, with a density of 6.2e-5. ", "page_idx": 25}, {"type": "text", "text": "These datasets\u2019 detailed statistics are summarized in Table 4. The \"Task\" column provides information about the type of downstream task conducted on each dataset: \"Node\" denotes node classification tasks, \"Graph\" signifies graph classification tasks, and \"Edge\" indicates tasks related to link prediction. The \"Type\" column indicates the type of graph dataset: \"Dynamic\" for dynamic dataset $t\\!\\geq\\!1$ , and \"Static\" for static dataset $t\\!=\\!1$ . For dynamic datasets, the \"Snapshot Granularity\" denotes the time granularity for each dataset. In our experimental setup, for dataset partition, dynamic graphs are partitioned according to snapshots, while static graphs are partitioned either by node or by the entire graph. ", "page_idx": 25}, {"type": "text", "text": "D.2 Evaluation Matrices ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Node and Graph classification evaluation. For the node classification, we use the prediction accuracy to measure the model. ", "page_idx": 25}, {"type": "text", "text": "Link prediction evaluation. For the link prediction, we evaluate the recall and ranking quality of the effects of recommendation following previous studies [117, 30]. We use Recall $@_{\\mathrm{k}}$ and $\\mathrm{NDCG}@\\mathbf{k}$ as metrics. Note that this task should be a binary task. We denote the topk largest value as $r e l_{i j},j\\in[1,k]$ for node $v_{i}$ . ", "page_idx": 25}, {"type": "text", "text": "Recall $@_{\\mathrm{k}}$ measures the ratio of true positive links contained in the top $k$ predicted links for each node: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{Recall@{k=\\frac{1}{n}\\sum_{\\substack{i=1}\\,j=1}^{n}\\sum_{\\substack{j=1}}^{k}}}\\frac{r e l_{i j}}{\\sum\\mathbb{I}(A[i\\colon]\\setminus0)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $r e l_{i j}\\!=\\!1$ if the $j$ -th predicted link for node $v_{i}$ exists, otherwise $0.\\;\\mathbb{I}(\\cdot)$ is the indicator function, and if $A[i{\\cdot}]>0$ then $\\bar{\\mathbb{I}(A[i:]\\!>\\!0)}\\!=\\!1$ . ", "page_idx": 26}, {"type": "text", "text": "$\\mathrm{NDCG}@\\mathbf{k}$ (Normalized Discounted Cumulative Gain) is computed by normalizing $\\mathrm{DCG}@\\mathbf{k}$ (Discounted Cumulative Gain) which accounts for the position of correctly predicted links. DCG $@_{\\mathrm{k}}$ is defined as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{{DCG@}}k\\!=\\!\\frac{1}{n}\\!\\sum_{i=1}^{n}\\!\\!\\sum_{j=1}^{k}\\!\\!\\frac{r e l_{i j}}{\\log_{2}(j\\!+\\!1)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.3 Baseline Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we present the details of baselines. ", "page_idx": 26}, {"type": "table", "img_path": "Dzk2cRUFMt/tmp/4e8d6179ac42b714b199b138c84fa1215e721887fb3d0c599d42d1eb028a55f5.jpg", "table_caption": ["Table 5: Baseline Code URLs of Github Repository "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "\u2022 GCN [48]: GCN is an end-to-end learning framework for graph-structured data. It utilizes neighborhood aggregation to integrate structural information, which is particularly effective in node classification and graph classification tasks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 GraphSAGE [29]: GraphSAGE, is a general and inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. ", "page_idx": 26}, {"type": "text", "text": "\u2022 GAT [97]: GAT is a spatial domain method, which aggregates information through the attentionlearned edge weights. ", "page_idx": 26}, {"type": "text", "text": "\u2022 GIN [105]: GIN utilizes a multi-layer perceptron to sum the results of GNN and learns a parameter to control residual connection. ", "page_idx": 26}, {"type": "text", "text": "\u2022 LightGCN [30]: LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. ", "page_idx": 26}, {"type": "text", "text": "\u2022 SGL [103]: SGL is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. ", "page_idx": 26}, {"type": "text", "text": "\u2022 MixGCF [39]: MixGCF generates the synthetic negative by aggregating embeddings from different layers of raw negatives\u2019 neighborhoods to perform collaborative filtering. ", "page_idx": 26}, {"type": "text", "text": "\u2022 SimGCL [117]: SimGCL applies unsupervised contrastive learning to enhance representation learning, making it suitable for link prediction tasks. It is applied to dynamic graphs to test its adaptability and performance.   \n\u2022 GraphPro [113]: GraphPro extends GraphPrompt by introducing spatial and temporal prompts tailored for dynamic graph learning, enhancing the ability to capture both structural and temporal relationships within graph data.   \n\u2022 GraphPrompt [65]: GraphPrompt integrates pre-training and downstream tasks using a unified template approach and employs task-specific prompts to enhance sub-task learning, applicable to both dynamic and static graph contexts.   \n\u2022 PRODIGY [73]: PRODIGY focuses on facilitating downstream tasks through in-context examples and learning from the $X\\!\\to\\!Y$ paradigm. It is implemented to enhance learning in both dynamic and static graphs by leveraging contextual learning strategies. ", "page_idx": 27}, {"type": "text", "text": "D.4 Implementation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Implementations are done using the PyTorch 2.3.0 framework [78] in Python 3.11, on an Ubuntu server equipped with 1 V100 GPU and an Intel(R) Xeon(R) CPU. ", "page_idx": 27}, {"type": "text", "text": "In node and graph classification tasks: For baseline GCN [48], we employ a 2-layer architecture and set the hidden dimension as 256. For GraphSAGE [29], we utilize the mean aggregator and employ a 2-layer architecture. The hidden dimension is also set to 256. For GAT [97], we employ a 2-layer architecture and set the hidden dimension as 256. Besides, we apply 8 attention heads in the first GAT layer. Similarly, for GIN [105], we also employ a 2-layer architecture and set the hidden dimension as 256. For GraphPrompt, we follow [65] to employ a 2-layer GCN as the backbone and set the hidden dimensions as 256. ", "page_idx": 27}, {"type": "text", "text": "In the link prediction task: For LightGCN, SGL, MixGCF, SimGCL and GraphPro, we employ a 3-layer GNN architecture and set the hidden dimension as 64 with Low-Rank Adaptation (LoRA) [32] rank equals to 16. For GraphPro, the backbone graph encoder is SimGCL. ", "page_idx": 27}, {"type": "text", "text": "Moreover, for all three tasks, the hyper-parameters of baselines are based on the recommended values provided in the paper. In PRODIGY and RAGRAPH, $k$ is set to 2, topK is set to 5, $\\gamma$ is set to 0.8 for PROTEINS and 0.5 for ENZYMES in node level, $\\gamma$ is set to 0.5 for PROTEINS, 0.6 for COX2, 0.8 for ENZYMES and 0.5 for BZR in graph level, $\\alpha\\!=\\!\\lambda\\!=\\!0.5,K\\!=\\!3,w_{1}\\!=\\!w_{2}\\!=\\!w_{3}\\!=\\!0.05,w_{4}\\!=\\!0.85$ . ", "page_idx": 27}, {"type": "text", "text": "D.5 Resource Graph Scalability Study ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/5d2cdae654687ef32c214432ede59af55bbbaf349f399bfc34e5fcfb290e5e72.jpg", "img_caption": ["(a) Node Classification on ENZYMES dataset (b) Node Classification on PROTEINS dataset "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: Performance comparisons of RAGRAPH and several baselines with different proportions of training and resource data. ", "page_idx": 27}, {"type": "text", "text": "We assess the impact of varying amounts of training and resource data on model performance. As illustrated in Figure 4, we vary the proportion of train and resource graph size from $10\\%$ to $80\\%$ , with increments of $10\\%$ , and conduct experiments on node classification tasks using the ENZYMES and PROTEINS datasets, respectively. For comparative analysis, we select GIN, GraphPrompt, and PRODIGY as baseline models. To ensure fairness in our experiments, we maintain a consistent ratio of train to resource data at 3:5 during fine-tuning, utilizing the sum of these as a retrieval database. ", "page_idx": 27}, {"type": "text", "text": "As shown in Figure 4, there is a clear trend where the accuracy of the model improves as the proportion of the dataset increases. However, the rate of accuracy improvement starts to plateau once a certain dataset proportion is reached (i.e., $30\\%$ in PROTEINS for PRODIGY and RAGRAPH, $40\\%$ in PROTEINS for GraphPrompt). Among the evaluated models, GIN and GraphPrompt show the slowest convergence rates, whereas PRODIGY converges at a moderate pace, and RAGRAPH converges the fastest. This rapid convergence in PRODIGY and RAGRAPH is attributed to its ability to engage in effective knowledge retrieval, significantly enhancing the model\u2019s comprehension abilities. Remarkably, both PRODIGY and RAGRAPH can achieve commendable results in downstream tasks even with a small proportion of the dataset. Compared to PRODIGY, RAGRAPH exhibits superior performance because while PRODIGY primarily learns a mapping from $X$ to $Y$ , RAGRAPH not only learns this mapping but also integrates additional knowledge into GNNs more effectively. This integration becomes increasingly beneficial as the dataset proportion grows, allowing RAGRAPH to outperform other models, particularly at higher data volumes where it can better leverage its knowledge integration capabilities. ", "page_idx": 28}, {"type": "text", "text": "D.6 Qualitative Analyses of Toy Graphs Retrieving ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we conduct qualitative analyses of the toy graphs retrieving experiment. For the sake of understanding, we conduct experiments under normal settings where the dimensionality of the task-specific output vector is equal to the number of classes. ", "page_idx": 28}, {"type": "text", "text": "On the ENZYMES dataset, for a 3-class node classification task, regarding node \"13984\", which belongs to class 3, if we only use the GraphPrompt Backbone, the resulting one-hot encoding is [0.28,0.34,0.38]. ", "page_idx": 28}, {"type": "text", "text": "However, since the node is of class 3, we expect the one-hot encoding to be as close as possible to [0,0,1]. In RAGRAPH retrieval, taking the top 3 retrieved graphs as examples, the connection weights for these 3 toy graphs to query graphs are 0.5, 0.7, and 0.1, respectively, and their corresponding label one-hot encodings are [0,0,1], [0,0,1], and [0,1,0]. Therefore, the result obtained by propagating the task-specific output vector through toy graphs is: [0,0.1,1.2], and after normalization, the result is [0,0.08,0.92]. ", "page_idx": 28}, {"type": "text", "text": "Meanwhile, the vector obtained by propagating toy graphs hidden embedding and via decoder is [0.37,0.32,0.66]. The retrieval of toy graphs notably enhances performance at both the task-specific output vector and hidden embedding levels. The final vector is obtained through a weighted sum with $\\gamma=0.5$ in Eq(6) is [0.185,0.20,0.79], after normalization the result is [0.157,0.170,0.673], which greatly enhances the model\u2019s discriminative ability compared to GraphPrompt [0.28,0.34,0.38]. ", "page_idx": 28}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/001bb5c66ab05b2ffe28ce699144ee84aeee76f8e7493a56eaa80dc79aa54c48.jpg", "img_caption": ["Figure 5: Qualitative analyses of toy graphs retrieving \u2013 how \u201cgeneration\u201d works. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Dzk2cRUFMt/tmp/2182997831131d00129f5978465236e396d5ca15c4356be094bb817c8ab54cd2.jpg", "img_caption": ["Figure 6: Difference Illustration between PRODIGY and RAGRAPH. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E Difference between ICL (PRODIGY) and RAG (RAGRAPH) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we explore the distinctions between PRODIGY and RAGRAPH from several critical perspectives, as illustrated in Figure 6: ", "page_idx": 29}, {"type": "text", "text": "\u2022 PRODIGY: This approach utilizes fixed examples as rules, which may not be optimal for dynamic and evolving scenarios. PRODIGY primarily focuses on learning direct mappings from $X$ to $Y$ through in-context learning. However, it encounters challenges in integrating external information that is more pertinent to the query node. This is particularly problematic when the distribution of each node belonging to the same label class varies, and simply learning the mapping based on the prototype node will somehow be misleading. \u2022 RAGRAPH: In contrast, RAGRAPH is designed to handle non-static, streaming knowledge, making it well-suited to dynamic graph structures and evolving tasks. It actively retrieves relevant knowledge on-demand, effectively incorporating information about both $X$ and $Y$ from external sources into GNNs. Moreover, RAGRAPH can operate without the need for model fine-tuning, providing substantial flexibility. This adaptability enables RAGRAPH to excel in tasks that require continuous adaptation to changing conditions and the integration of external, relevant information. ", "page_idx": 29}, {"type": "text", "text": "In summary, we argue that a qualified Retrieval-Augmented Generation (RAG) system for Graph Learning should fulfill several essential criteria to effectively support complex reasoning tasks: 1) It should retrieve ample feature and task-related label information, analogous to how attributes are gathered in the NLP domain to stimulate the reasoning capabilities of LLMs; 2) The system should adapt to new tasks or unseen datasets without requiring fine-tuning of model parameters; 3) Knowledge within the system must be dynamically updated and stored, ensuring current and relevant data utilization. ", "page_idx": 29}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our work builds on the widespread application of Retrieval-Augmented Generation (RAG) in large language models (LLMs) and aims to extend its success to graph data, thereby constructing graph foundation models. This approach allows models to transfer rapidly without requiring learnable parameters, avoiding potential performance degradation from fine-tuning pre-trained models. As a result, RAG is particularly effective in domains with scarce and long-tail data, such as network anomaly detection, rare disease diagnosis/treatment, supply chain disruption, and new user recommendations. ", "page_idx": 29}, {"type": "text", "text": "Additionally, our model establishes an excellent paradigm by incorporating retrieved features and label information into the learning process, significantly enhancing the model\u2019s understanding capabilities. Our work provides valuable insights and serves as a reference for future Large Graph Models. ", "page_idx": 30}, {"type": "text", "text": "G Data Ethics Statement ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To evaluate the efficacy of this work, we conducted experiments that only use publicly available datasets, namely, PROTEINS, COX2, ENZYMES, BZR3, TAOBAO, KOUBEI and AMAZON in accordance to their usage terms and conditions if any. We further declare that no personally identifiable information was used, and no human or animal subject was involved in this research. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the introduction section, we delineate the problems addressed by this work and outline our contributions. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: In the conclusion section, we highlight the limitations of the current work and suggest directions for future research. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the complete theoretical proofs in Appendix C.4. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide detailed experiment settings in Appendix D.4. Besides, code is anonymously available at https://anonymous.4open.science/r/GLM-RAG-049D/. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Code is anonymously available at https://anonymous.4open.science/r/ GLM-RAG-049D/. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide experiment settings in Section 5.1 and Appendix D. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: For each experiment, we conducted 5 repeated experiments and reported the standard deviation in Section 5. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in Appendix D.4. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: I have read the NeurIPS Code of Ethics and I confirm our research in the paper conforms with Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have discussed the potential impacts in Appendix F. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The graph neural network framework proposed in our paper does not extend to application domains requiring safeguards. Additionally, the datasets used are widely-used node classification datasets, thus eliminating the need for specific safeguards. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide source links for all datasets and baselines in Appendix D, and we have cited all referenced works. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We release our code anonymously at https://anonymous.4open.science/ r/GLM-RAG-049D/. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]