[{"type": "text", "text": "Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yifei Xia Fangcheng Fu Wentao Zhang Peking University Peking University Peking University yifeixia@stu.pku.edu.cn ccchengff@pku.edu.cn wentao.zhang@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jiawei Jiang Wuhan University jiawei.jiang@whu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bin Cui Peking University bin.cui@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the remarkable achievements of large language models (LLMs), the demand for fine-tuning and deploying LLMs in various downstream tasks has garnered widespread interest. Parameter-efficient fine-tuning techniques represented by LoRA and model quantization techniques represented by GPTQ and AWQ are of paramount significance. However, although these techniques have been widely adopted in single-task scenarios, research is scarce in multi-task scenarios. To be specific, we find that mainstream quantization methods would prevent the base LLM from being shared among tasks, so current LLM serving systems are infeasible to integrate LLM quantization with multiple LoRA adapters to achieve memory-efficient multi-task serving. Moreover, existing LLM serving systems lack support for dynamic task addition and overlook the workload differences among tasks, leading to inefficiencies in multi-task scenarios. ", "page_idx": 0}, {"type": "text", "text": "This work proposes LoRA-Inlaid, an efficient multi-task LLM serving system. On the one hand, LoRA-Inlaid designs a flexible and efficient multi-task quantization algorithm (MLGPTQ) that facilitates the sharing of a single quantized model for multiple LoRA adapters, which significantly reduces the memory consumption for model deployment. Meanwhile, it supports adding LoRA adapters for new tasks on the fly, without sacrificing the stability of online services. On the other hand, LoRA-Inlaid develops a novel multi-task scheduling algorithm guided by output length prediction and grouping among different tasks, which effectively shrinks the memory consumption and avoids frequent switching of LoRA adapters. Empirical results verify that LoRA-Inlaid outperforms existing state-of-the-art LLM serving systems by up to $1.58\\times$ in terms of throughput, $1.76\\times$ in terms of average latency, $2\\times$ in terms of job completion time, and $10\\times$ in terms of SLO Attainment, while maintaining the same level of model quality. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated impressive effectiveness in various domains [26, 30, 31, 39], and the demand of deploying LLMs in downstream tasks continues to grow [4, 10, ", "page_idx": 0}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/83283b5f2deb9490184433230fdde38147b9655a721f98653b15346071c6e6c2.jpg", "table_caption": ["Table 1: Comparison of supported features of different LLM serving systems "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "20, 24, 34, 43, 44, 46, 47]. Given the explosive increase in model size and the limitations of hardware resources, \u201cparameter-efficient fine-tuning\u201d (PEFT) and \u201cquantization-then-deployment\u201d have become the most common pathways for deploying LLMs in downstream tasks [50]. On the one hand, PEFT techniques, represented by LoRA (Low-Rank Adaptation) [16], only train small-scale adapters to adapt the base model to a specific task, significantly reducing the cost of model fine-tuning. On the other hand, low-bit quantization techniques like GPTQ and AWQ [12, 22] can substantially reduce the memory requirements of model deployment and alleviate memory access overhead during inference, while maintaining model quality. ", "page_idx": 1}, {"type": "text", "text": "Although mainstream LLM serving systems like vLLM and TensorRT-LLM [19, 29] have integrated support for the quantized deployment of fine-tuned models, these systems focus on single-task serving scenarios. With the rising demand for various downstream tasks, efficiently supporting multi-task servicing scenarios has become increasingly crucial. This has led to the emergence of multi-task serving systems supporting multiple LoRA adapters concurrently, such as S-LoRA and Punica [5, 35]. These systems share a unified base model across different tasks and activate different LoRA adapters based on the incoming requests, enabling the simultaneous processing of multiple tasks in a single batch. However, in multi-task scenarios, existing systems still face three major challenges. ", "page_idx": 1}, {"type": "text", "text": "First, existing multi-task serving systems cannot effectively incorporate mainstream model quantization methods such as GPTQ and AWQ. Specifically, these quantization methods require calibration of numerical distributions using task-specific datasets, and the quantization process for each task necessitates activating the corresponding LoRA adapter. Consequently, the base models after quantization are divergent across different tasks, and thus it is infeasible to share a unified quantized model. This limitation leads to performance deficiencies or even unavailability in resource-constrained scenarios. ", "page_idx": 1}, {"type": "text", "text": "Second, in practical multi-task serving scenarios, it would be necessary to add new tasks in real time. However, existing systems only support a static number of tasks and are incapable of dynamically adding LoRA adapters. More importantly, after a quantized model is deployed, current solutions do not support any subsequent quantization and deployment for new tasks without affecting the existing tasks. In contrast, adding new tasks typically requires suspending and restarting the serving process, which severely harms the stability and robustness of online services. ", "page_idx": 1}, {"type": "text", "text": "Third, incoming requests for different tasks inevitably have workload variations (such as request length, processing time, etc.) and require loading different LoRA adapters for processing. Existing systems overlook these issues during the scheduling for multi-task requests, and thus necessitate loading a large number of adapters in a single scheduling step as well as frequently switching adapters between adjacent scheduling steps, leading to significant efficiency degradation. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we develop LoRA-Inlaid, a resource-efficient and high-performance system for multi-task LLM serving. The main contributions of this paper are as follows. ", "page_idx": 1}, {"type": "text", "text": "To begin with, we propose an innovative multi-task quantization algorithm termed MLGPTQ (MultiLoRA GPTQ), which utilizes multi-task data to perform joint quantization on the base model. This allows the quantized base model to be shared across multiple tasks. In addition, it supports incremental quantization for newly added tasks without impacting the performance of online services. ", "page_idx": 1}, {"type": "text", "text": "Subsequently, we introduce a novel multi-task scheduling strategy based on output length prediction and grouping. This effectively reduces memory consumption and memory swapping overhead in multi-task scenarios, significantly enhancing overall system performance. ", "page_idx": 1}, {"type": "text", "text": "Based on these two techniques, we develop a brand new multi-task LLM serving system, namely LoRA-Inlaid. As shown in Table 1, LoRA-Inlaid integrates multi-task quantization, enables dynamic task addition, and employs the multi-task scheduling strategy, achieving high-performance and flexible multi-task LLM serving in resource-constrained environments. ", "page_idx": 1}, {"type": "text", "text": "Finally, experimental results demonstrate that, compared to existing systems, LoRA-Inlaid can increase throughput by up to $1.58\\times$ , reduce average latency and job completion time by up to $1.76\\times$ and $2\\times$ , improve SLO Attainment by up to $10\\times$ , and support larger-scale language models under the same resource constraints, all while maintaining nearly the same level of model quality. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Low-Rank Adaptation. LoRA [16], short for Low-Rank Adaptation, is one of the most widely used parameter-efficient fine-tuning (PEFT) techniques. Unlike full-parameter fine-tuning, LoRA fine-tunes only a small adapter, which consists of much fewer parameters than the base model, significantly reducing the training cost. The key idea behind LoRA is that the fine-tuning process should only introduce small changes to the weight matrix of the base model (denoted by $\\mathbf{W}\\in\\mathbf{\\bar{\\mathbb{R}}}^{m\\times n},$ ), so we can learn two small, low-rank matrices (denoted by $\\mathbf{A}\\in\\mathbb{R}^{r\\times n}$ , $\\mathbf{B}\\in\\mathbb{R}^{m\\times r}$ where $r\\ll m,n)$ , and approximate such changes with the product of two matrices (i.e., $\\Delta\\mathbf{W}\\approx\\mathbf{B}\\mathbf{A}$ ). ", "page_idx": 2}, {"type": "text", "text": "Low-bit Quantization. Low-bit quantization [7, 8, 12, 22, 23, 42] shrinks the model size effectively and thus reduces the memory requirement when deploying the model. In addition, it usually helps to improve efficiency by decreasing the memory access overhead of the model weights. Consequently, it has been widely adopted in LLM serving. There are various quantization paradigms, with post-training quantization (PTQ) being among the most popular ones. Typically, PTQ computes $X_{\\mathrm{INT}}=\\mathtt{R o u n d}(\\alpha\\subset\\mathtt{L i p}(X_{R}/\\alpha,Q_{\\mathrm{min}},Q_{\\mathrm{max}}))$ , where $X_{R}$ represents the real-valued parameters before quantization, $X_{\\mathrm{INT}}$ represents the parameters after quantization to integers, $Q_{m i n}$ and $Q_{m a x}$ denote the minimum and maximum values of the quantization range, and $\\alpha$ represents the scaling factor. Various PTQ methods calculate the quantization knobs like $\\alpha$ with diverse approaches or implement different approximation methods. While mainstream PTQ methods (e.g., GPTQ [12], AWQ [22]) have a common ground that they need to calibrate the numerical distribution based on a small task-specific dataset (a.k.a. the calibration set), since numerous studies have revealed the accuracy after quantization with dataset calibration is usually significantly higher than that without dataset calibration [17]. Therefore, this paper focuses on quantization with dataset calibration. ", "page_idx": 2}, {"type": "text", "text": "Scheduling in LLM Serving. With the explosive applications of LLMs, more and more studies try to evolve the scheduling strategies in LLM serving for better performance. Early systems like FasterTransformer [28] rely on request-level scheduling. Notably, Yu et al. [45] introduced Orca, the first iteration-level scheduling with first-come-first-serve (FCFS) order for better batching. Building on this, mainstream LLM serving systems leverage various batching approaches, such as continuous batching in vLLM [19] and in-flight batching in TensorRT-LLM [29]. FastServe [40] takes the semi-information of requests (e.g., input length, processed time, etc.) into account and tries to minimize average job completion time. However, none of these scheduling strategies consider the characteristics of multi-task scenarios, as we will discuss in $\\S3.3$ . ", "page_idx": 2}, {"type": "text", "text": "Multi-task Serving Systems. Since the LoRA fine-tuning technique keeps the base model unaltered, it is feasible to share the same base model across multiple LoRA adapters, so that we can serve requests from multiple tasks within a single batch. Punica [5] and S-LoRA [35] are two notable multi-task serving systems, putting forward the initial efforts to support multi-task LLM serving with multiple LoRA adapters. Specific optimization techniques are proposed. For instance, the Segmented Gather Matrix-Vector (SGMV) kernel is developed to enhance memory and computation efficiency when processing requests from different tasks together. In addition, to allocate more GPU memory to intermediate results (typically, KV cache), existing systems maintain the LoRA adapters in CPU memory and only preserve a relatively small number of LoRA adapters in GPU memory. When a LoRA adapter outside GPU memory is needed, it is necessary to perform memory swapping between the CPU and GPU memory. ", "page_idx": 2}, {"type": "text", "text": "3 LoRA-Inlaid ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overview of LoRA-Inlaid is depicted in Figure 1. Given an LLM with multiple LoRA adapters for various downstream tasks, LoRA-Inlaid initiates a joint quantization process (\u00a73.1), which produces a unified quantized base model that can be shared across the adapters. During the online serving, if new tasks are to be included on the fly, LoRA-Inlaid facilitates a dynamic task addition process (\u00a73.2) that efficiently conducts incremental re-quantization and seamlessly deploys the added tasks. ", "page_idx": 2}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/77b18ac76fddc5c1b0427b8fdce84ccbe6efd96641bba4b84485e7b4ff48d1ad.jpg", "img_caption": ["Figure 1: Design overview of LoRA-Inlaid. The workflow is labeled with numbers in the diagram. $\\textcircled{\\scriptsize{1}}$ Quantize and Deploy indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. $\\circled{2}$ Schedule involves utilizing a multi-task scheduling strategy for $\\circled{3}$ Inference. If a new task is detected, it invokes $\\textcircled{4}\\textcircled{A}d d\\it{\\,T a s k}$ to dynamically add the new task without interrupting the ongoing services. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Furthermore, LoRA-Inlaid employs a multi-task scheduling strategy (\u00a73.3) that takes the workload differences into account for better efficiency. ", "page_idx": 3}, {"type": "text", "text": "3.1 Multi-task Joint Quantization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As introduced in $\\S2$ , mainstream quantization methods require task-specific datasets for calibration. In addition, they mostly follow the Forward-Aggregate Info-Modify Weight-Quant paradigm in Figure 2. This paradigm first simulates the activation distribution for a given task through Forward propagation and Aggregates Information of this specific task. Subsequently, it uses the aggregated information to Modify model Weights to adapt to the task. Finally, the quantization knobs like scales $\\alpha$ are calculated based on the modified weights to Quantize the base model. ", "page_idx": 3}, {"type": "text", "text": "However, in multi-task scenarios, since different tasks should provide diverse calibration sets and necessitate unique LoRA adapters for computation, the quantized models of different tasks are inevitably divergent. Intuitively, if we wish to tweak existing quantization methods to make the quantized model shareable across tasks, we should quantize the model without any LoRA adapters. In addition, we should either (i) quantize the model without calibration or (ii) quantize the model with a mixed calibration set consisting of the datasets from all tasks. ", "page_idx": 3}, {"type": "text", "text": "However, these approaches fail to accurately capture the unique numerical distribution of each task, and suffer from severe accuracy loss (as evaluated in $\\S4.2\\rangle$ ). Below we first elaborate on the reason why these approaches fail with the widely used GPTQ [12] and then propose our solution1. ", "page_idx": 3}, {"type": "text", "text": "Drawbacks of GPTQ in multi-task scenarios. Directly applying GPTQ in multi-task scenarios has the following drawbacks. First, as aforementioned, GPTQ can only quantize the model without any LoRA adapters, which is infeasible to accurately capture the correct activation information for multiple tasks during Forward. Second, in Aggregate $I n f\\pmb{o}$ , since the calibration sets from all tasks are mixed, GPTQ simply accumulates the information from different tasks into one Hessian matrix, making each task\u2019s specific information diluted and losing the emphasis on critical information from different tasks. Third, in Modify Weight, GPTQ relies on the na\u00efve, mix-aggregated Hessian matrix, overlooking the varying importance across tasks, which results in suboptimal outcomes. These drawbacks make the direct application of GPTQ in multi-task scenarios ineffective. ", "page_idx": 3}, {"type": "text", "text": "Our MLGPTQ (Multi-LoRA GPTQ) Algorithm To address these drawbacks, we propose a multitask quantization algorithm termed MLGPTQ. Our algorithm enables joint quantization of multiple tasks to retain only one quantized base model, while effectively maintaining the model accuracy by capturing the numerical distributions of all tasks. The goal of MLGPTQ is to minimize the errors of activations before and after quantization, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{Q(\\mathbf{W})}{\\arg\\operatorname*{min}}\\,||\\sum_{t=1}^{T}((\\mathbf{W}+\\mathbf{B}_{t}\\mathbf{A}_{t})\\mathbf{X}_{t}-(Q(\\mathbf{W})+\\mathbf{B}_{t}\\mathbf{A}_{t})\\mathbf{X}_{t})||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/23ab19ebc16dfa2d27df91d03e3b7ef3c8fe0b267b9022405f0acc3baf9aad39.jpg", "img_caption": ["Figure 2: Process of MLGPTQ vs GPTQ. Both MLGPTQ and GPTQ follow the Forward-Aggregate InfoModify Weight-Quant paradigm. MLGPTQ primarily improves the first three steps, aiming to better gather and highlight critical information for all tasks. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $T$ denotes the number of tasks, ${\\bf A}_{t}$ and $\\mathbf{B}_{t}$ are the low-rank adapter matrices of the $t$ -th task, $\\mathbf{X}_{t}$ is the input of $t$ -th task, and W and $Q(\\mathbf{W})$ denote the original and quantized weights of a layer. As shown in Figure 2, During Forward, MLGPTQ loads the corresponding LoRA adapters based on each task, accurately computing the activations. In Aggregate Info, unlike GPTQ\u2019s na\u00efve mixaggregation that disrupts task-specific information, MLGPTQ derives the max-aggregation to solve the objective in Eq. 1 (the derivation can be found in the Appendix A), which has the following form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla{\\bf W}=-\\frac{w_{q}-Q(w_{q})}{(\\mathbf{H}_{t^{*}}^{-1})_{q q}}\\mathbf{H}_{t^{*}}^{-1}e_{q},\\mathrm{~where~}t^{*}=\\underset{t\\in\\{1,2,\\cdots,T\\}}{\\arg\\operatorname*{max}}\\big(\\mathbf{H}_{t}^{-1}\\big)_{q q},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{H}_{t}$ denotes the Hessian matrix of the $t$ -th $\\mathrm{task},w_{q}$ is the $q$ -th parameter in W. To be formal, there are primarily two steps in Aggregate Info. First, it calculates the Hessian matrix information for each task individually (i.e., compute $\\bar{\\{\\mathbf{H}_{t}^{-1}\\}}_{t=1}^{T})$ Second, it aggregates the most important information from each one into a max-aggregated Hessian matrix (i.e., $\\mathbf{\\tilde{H}}_{t m p}=\\tt M a x A g g(\\{H_{\\it t}^{-1}\\}_{\\it t=1}^{T}))$ . In Modify Weight, MLGPTQ utilizes the max-aggregated Hessian matrix to adjust the weights according to Eq. 2. Finally in Quant, we utilize the modified weights for quantization. Due to space constraints, we only present the core concept of MLGPTQ here. Interested readers are referred to Appendix A for a complete derivation as well as the detailed algorithm. ", "page_idx": 4}, {"type": "text", "text": "3.2 Dynamic Task Addition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In real-world online services, there is a need for dynamic task addition (i.e., adding new LoRA adapters). In single-task scenarios, adding new tasks typically requires launching more services with extra hardware resources, which does not affect the services for existing tasks. In multi-task scenarios, there would be interference since all tasks share the same base model. However, we find that none of the existing multi-task serving systems address this problem, lacking a proper solution. ", "page_idx": 4}, {"type": "text", "text": "Nevertheless, adding new LoRA adapters on the fly in LoRA-Inlaid is inherently far from trivial since the multi-task quantization poses two challenges: (1. Unseen Distributions) Since the MLGPTQ algorithm is invoked before the new tasks are involved, the quantized model has not captured the distribution information about the new tasks, making it infeasible to work with the new LoRA adapters directly. (2. Serving Interruption) Directly re-quantizing the model requires a substantial amount of memory, so it necessitates pausing the ongoing serving for a while to reserve available space for re-quantization, harming the stability of online services. To support dynamic task addition in multi-task scenarios, LoRA-Inlaid tackles these two obstacles, respectively. ", "page_idx": 4}, {"type": "text", "text": "To capture the information of new tasks, a na\u00efve solution is to perform full quantization once there are new tasks. Denote $T_{1},T_{2}$ as numbers of existing and new tasks, respectively. The na\u00efve solution runs the two steps of Aggregate Info above with $T=T_{1}+T_{2}$ . However, this leads to redundant computation of $\\!\\!\\!\\left\\{\\mathbf{H}_{t}^{-1}\\right\\}\\!\\!_{t=1}^{\\widetilde{T_{1}}\\!}$ . In addition, given the commutative property of the max-aggregation operation, we have $\\begin{array}{r}{\\texttt{M a x A g g}(\\{\\mathbf{H}_{t}^{-1}\\}_{t=1}^{T})=\\texttt{M a x A g g}(\\texttt{M a x A g g}(\\{\\mathbf{H}_{t}^{-1}\\}_{t=1}^{T_{1}^{\\star}}),\\texttt{M a x A g g}(\\{\\mathbf{H}_{t}^{-1}\\}_{t=T_{1}+1}^{T_{2}^{\\star}})}\\end{array}$ , where the first term $\\mathtt{M a x A g g}(\\{\\mathbf{H}_{t}^{-1}\\}_{t=1}^{T_{1}})$ has already been computed as $\\mathbf{H}_{t m p}$ in the previous quantization. Inspired by this, LoRA-Inlaid caches $\\mathbf{H}_{t m p}$ so that the incremental quantization can be done as follows. In Forward, it capture the activation information of new task $T_{1}+1,\\cdot\\cdot\\cdot,T_{2}$ . In $\\boldsymbol{A}$ ggregate ", "page_idx": 4}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/0249ef4e276cbd5c564cde5272f4d924d899c03e34000e9239732bfd7fb1312b.jpg", "img_caption": ["Figure 3: Length distributions of different tasks. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/84fd1db2702f164fb31096610156a283486573be33763ef52af6439682417233.jpg", "img_caption": ["Figure 4: Number of tasks in each scheduling step of different scheduling strategies. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "$I n f\\pmb{o}$ , it computes the Hessian matrices for new tasks $\\{\\mathbf{H}_{t}^{-1}\\}_{t=T_{1}+1}^{T_{2}}$ , and then max-aggregates the $T_{2}+1$ matrices (i.e., $\\{\\mathbf{H}_{t}^{-1}\\}_{t=T_{1}+1}^{T_{2}}$ and the cached $\\mathbf{H}_{t m p}^{(c a c h e d)})$ . At last, it performs Modify Weight and Quant, as introduced in $\\S3.1$ . By doing so, incremental quantization with $T_{2}$ tasks is identical to full quantization with $T_{1}+T_{2}$ tasks, while avoiding redundant computation. ", "page_idx": 5}, {"type": "text", "text": "To avoid halting the ongoing services, LoRA-Inlaid spawns a background thread for incremental quantization. Moreover, it is done in a layer-by-layer manner to reduce the memory consumption \u2014 for each (unquantized) model weight, we load it from CPU memory to GPU memory, perform incremental quantization, remove it from GPU memory, and proceed to the next model weight. The IO between CPU-GPU is overlapped with computation. Thus, LoRA-Inlaid supports seamless task addition on the fly and has very little influence on the ongoing services, as evaluated in $\\S4.4$ . ", "page_idx": 5}, {"type": "text", "text": "Putting them together, LoRA-Inlaid develops an asynchronous, layer-wise re-quantization mechanism, which accomplishes incremental quantization with the new tasks and cached Hessian matrices asynchronously, without interrupting the serving. ", "page_idx": 5}, {"type": "text", "text": "3.3 Multi-task Scheduling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Despite extensive research on scheduling strategies for LLM serving, these approaches primarily focus on single-task serving, leaving the unique characteristics in the multi-task scenarios neglected. Below we analyze two limitations of existing scheduling strategies in multi-task serving. Besides, due to the space constraint, we briefly introduce the corresponding solutions in LoRA-Inlaid, while leaving the details of our multi-task scheduling algorithm in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Limitation 1: Divergent Output Length Distributions Leading to High Average Completion Time. As shown in Figure 3, the distributions of input and output lengths vary significantly across different tasks, while requests of the same task exhibit clustering effects. Current strategies mainly rely on semi-information (e.g., input length, processed time, etc.) to make the scheduling decisions, but do not consider the information of output length since it is not the prior knowledge. Intuitively, this may work fine for single-task scenarios where the vast majority of requests fall within the same workload and thus the clustering effect exists. However, it is unsuitable for multi-task scenarios due to the divergent output length distributions across different tasks. Eventually, we find that existing scheduling strategies suffer from heavy performance degradation when applied to multi-task serving. ", "page_idx": 5}, {"type": "text", "text": "Solution 1: Scheduling Guided by Output Length Prediction. Existing research has shown that the output lengths can be accurately predicted by a small, distilled model given the requests [49]. Inspired by this, we leverage a number of small models, to predict the output lengths of incoming requests. Particularly, upon receiving a new request, we predict its output length on CPU using a small model (255MB). Note that the output length prediction takes about 16 milliseconds for one request on CPU, while it takes about 200 milliseconds or more to finish the inference of one request on GPU. Hence, we can completely overlap the prediction, without occupying any GPU computing resources. Based on the predictions, we employ a Shortest Remaining Time First (SRTF) scheduling, which prioritizes requests with the shortest remaining processing time and has been proven to minimize the average completion time in the field of job scheduling [37]. ", "page_idx": 5}, {"type": "text", "text": "Limitation 2: Excessive Tasks Involved in each Step Leading to Expensive Memory Access Overhead. Due to the randomness and dynamicity of request arrivals, multiple tasks are to be scheduled in each step. However, owing to the lack of consideration upon the task for each request, existing scheduling ", "page_idx": 5}, {"type": "text", "text": "Table 2: Model quality of different approaches under different tasks (std-dev given in parentheses). GPTQ and AWQ are in gray background color since the quantized models produced by them cannot be shared across different tasks. We mark the best multi-task quantization approaches (i.e., the best among MLGPTQ, GPTQtweaked, AWQtweaked, and RTN) in bold. ", "page_idx": 6}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/2889f5ac9b3c6655093088c8fd98cf5d71bee6f4fec07cfe54d34aab7d9713a9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "strategies typically involve a great number of tasks, and the system has to load lots of LoRA adapters in each step, as shown in Figure 4. As it is well known that the decoding phase of LLM inference is usually bounded by the memory bandwidth, the need for loading more LoRA adapters further exacerbates this issue. Worst still, we observe that the sets of involved tasks vary significantly between consecutive steps. As introduced in $\\S\\,2$ , multi-task serving systems only preserve limited GPU memory space for LoRA adapters, and swap them between CPU and GPU memory when necessary. Consequently, existing scheduling strategies force the system to frequently swap LoRA adapters, rendering performance degradation due to the expensive memory swapping overhead. ", "page_idx": 6}, {"type": "text", "text": "Solution 2: Reducing Tasks Involved via Grouping. To address the limitation, we adopt a simple yet effective grouping-based approach, which partitions requests into groups according to their tasks to guide scheduling. On one hand, to avoid involving excessive tasks in each step, we set a grouping coefficient $\\beta$ (10 by default) and keep the number of involved tasks below $\\beta$ in each step. On the other hand, to alleviate the memory swapping overhead, we prioritize tasks involved in the previous step, aiming to use the LoRA adapters for more consecutive steps. In addition, we maintain a starvation queue based on the waiting time to get rid of starvation, striking a good balance among the tasks. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Hardware Environment. All experiments are conducted on one RTX 4090 GPU or one RTX 3090 GPU, with GPU memory capacity of 24GB. Detailed specifications can be found in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "Datasets and Workloads. For accuracy tests, we consider 12 tasks in total, including six translation tasks [38] (trans-fr, trans-cs, trans-id, trans-nl, trans-da, trans-sw), one text summarization task [14] (xlsum), one table summarization task [48] (QTsum), one code generation task [13] (tiny-codes), one math QA task [6] (GSM8k), one medical QA task [2] (med-qa), and one malicious detection task [1] (malicious). Detailed descriptions are provided in Appendix C.2. For efficiency tests, we follow prior works [19, 35] to generate different levels of request rates using the Gamma process. ", "page_idx": 6}, {"type": "text", "text": "Models. We conduct experiments with LLaMA2-7B and LLaMA2-13B [25]. For accuracy tests, open-source fine-tuned models are used, as detailed in Appendix C.2. For efficiency tests, following S-LoRA [35], we consider LoRA adapters with different ranks (8, 16, 32, 64) across the served tasks to simulate diverse serving scenarios. ", "page_idx": 6}, {"type": "text", "text": "Metrics. For the accuracy test, we focus on SacreBLEU (S_BLEU) [18], ROUGE [21] and Accuracy (Acc). Details of these metrics are provided in Appendix C.3. For efficiency tests, the considered metrics include throughput, average request latency, job completion time (JCT), and SLO (service level objective) Attainment [33] (the percentage of requests completed within the expected latency). By default, we test the serving for 1 minute, with an expected latency of 6 seconds. ", "page_idx": 6}, {"type": "text", "text": "Baselines. For accuracy tests, we focus on two kinds of baselines introduced in $\\S3.1$ , i.e., (i) the floating-point round-to-nearest quantization without calibration (denoted as RTN), and (ii) GPTQ [12] and AWQ [22] with a mixed calibration set from all tasks (denoted as $\\mathrm{GPTQ}_{t w e a k e d}$ and $\\mathrm{AWQ}_{t w e a k e d})$ . ", "page_idx": 6}, {"type": "text", "text": "To be fair, these baselines quantize the base model without any LoRA adapters to ensure the quantized model is shareable. Besides, to facilitate the comparison to single-task quantization, we further consider GPTQ and AWQ for each task individually with the corresponding calibration set and LoRA adapter (denoted as GPTQ and AWQ), which cannot generate a shareable quantized model though. ", "page_idx": 7}, {"type": "text", "text": "For efficiency tests, we compare LoRA-Inlaid with vLLM [19] and S-LoRA [35]. For vLLM, we quantize each model into 4-bit and launch multiple processes (each with one quantized model) on the same GPU to achieve multi-task serving. For S-LoRA, which does not support deploying quantized models, we deploy one half-precision (16-bit) model and let multiple LoRA adapters share it. For LoRA-Inlaid, we use 4-bit quantization in all efficiency tests. Note that although the model is quantized, the computation during inference is still executed in half-precision (i.e., the model weights are dequantized before computation). ", "page_idx": 7}, {"type": "text", "text": "4.2 Model Quality after Quantization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first assess the model quality after quantization of different approaches. Table 2 presents the model quality of different tasks under different metrics. It can be seen that all quantization methods incur accuracy drops compared to no quantization. For 4-bit quantization, the average accuracy drops for MLGPTQ, $\\mathrm{GPTQ}_{t w e a k e d}$ , $\\mathrm{AWQ}_{t w e a k e d}$ , and RTN are $1.70\\%$ , $4.72\\%$ , $4.50\\%$ , $4.02\\%$ , respectively. For 3-bit quantization, MLGPTQ consistently achieves the best results, outperforming GPTQtweaked and $\\mathrm{AWQ}_{t w e a k e d}$ by $59.30\\%$ , $69.98\\%$ in average. While RTN suffers from up to $74.41\\%$ accuracy drops. In addition, for GPTQ and AWQ, which could not produce a shareable model, the average accuracy drops are very close to MLGPTQ (e.g., $1.00\\%$ , $1.89\\%$ respectively for 4-bit quantization). This proves that our work achieves comparable model quality against single-task quantization while enabling multi-task sharing after quantization. ", "page_idx": 7}, {"type": "text", "text": "We also conduct experiments to anatomize the effectiveness of MLGPTQ. To be more comprehensive, five additional metrics (ROUGE1, ROUGE2 [21], NIST_MT [9], METEOR [9], and G_BLEU [41], detailed in Appendix C.3) for evaluating machine translation quality are included. In addition, we further consider a variant of MLGPTQ termed MLGPTQno_target, which intentionally excludes the calibration set and LoRA adapter of a target task during quantization (the other tasks are not affected). ", "page_idx": 7}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/234cc7a3548335825057884dad39ed20eec2e01a0a94bc0ba216312fe8944c85.jpg", "img_caption": ["Figure 5: Effectiveness anatomy. The radar charts show the relative accuracy drops compared to no quantization (outer is better). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "According to the results in Figure 5, we point out that the effectiveness of MLGPTQ stems from two factors: $\\textcircled{\\scriptsize{1}}$ whether the information (e.g., activation and Hessian matrix) of each task is correctly captured, and $\\circled{2}$ whether the data distribution of each task is involved during the forward pass of quantization. In particular, $\\mathrm{GPTQ}_{t w e a k e d}$ (missing $\\textcircled{1}.$ ) and $\\mathsf{M L G P T Q}_{n o\\_t a r g e}$ t (missing $\\textcircled{2}]$ ) exhibit higher accuracy drops compared to MLGPTQ and GPTQ (both fulfilling $\\textcircled{1}$ and $\\textcircled{2}$ ) in almost all metrics. These results verify that the design of MLGPTQ fits multi-task quantization well. ", "page_idx": 7}, {"type": "text", "text": "4.3 End-to-end System Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Throughput, Latency, and JCT. We evaluate the system performance with various numbers of tasks and request arrival rates. The throughput, latency, and JCT are shown in Figure 6. Overall, LoRA-Inlaid consistently outperforms S-LoRA and vLLM. S-LoRA fails to serve LLaMA2-13B due to the lack of support for deploying quantized models. Since vLLM necessitates launching multiple processes to achieve multi-task serving, the memory consumption grows linearly w.r.t. the number of tasks, and it encounters out-of-memory (OOM) errors in several cases. In contrast, LoRA-Inlaid supports all cases well. More importantly, since LoRA-Inlaid is able to reserve more memory for intermediate results (e.g., KV cache) in serving, it achieves higher performance than the baselines. For instance, LoRA-Inlaid surpasses S-LoRA by $26.5\\%$ , $31.3\\%$ , $24.1\\%$ on average, and up to $58.1\\%$ , $76.3\\%$ , and $99.9\\%$ , in terms of throughput, latency, and JCT, respectively. ", "page_idx": 7}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/c5161db98bc1f4b0b7dd2ab0307823a0bfa16c64bb0cf00663b30908e531b781.jpg", "img_caption": ["Figure 6: System performance in terms of throughput (higher is better), latency (lower is better), and JCT (lower is better) under various request rates ( $\\chi$ -axis) and numbers of tasks $(T)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/3f10c9e95b57d3b3f6224b1f4210844c4069fd6b90ba1682134c7829a2b4219e.jpg", "img_caption": ["Figure 7: SLO Attainment (higher is better) under various serving loads (RTX 4090). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/48134f428767c3ecc3df7243d9dbe09de86db6f3b671fb7d6e1de274b73f1a22.jpg", "table_caption": ["Table 3: Scalability comparison in terms of throughput (reqs/s, higher is better) under different request rates and number of LoRA adapters (LLaMA2-7B@RTX 4090). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "SLO Attainment. We also assess the SLO Attainment under different serving loads by varying the request rates and maximum request lengths. The results are shown in Figure 7. In short, compared to S-LoRA and vLLM, LoRA-Inlaid improves the SLO Attainment by $3.9{\\times},8.5{\\times}$ on average, and up to $10\\times$ , $38\\times$ , respectively. Furthermore, we observe that as the request rate or maximum sequence length increases, S-LoRA and vLLM experience a steep decline in SLO Attainment while LoRA-Inlaid does not. This demonstrates the excellent adaptability of LoRA-Inlaid to various serving loads. ", "page_idx": 8}, {"type": "text", "text": "4.4 More Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Scalability. We investigate the scalability w.r.t. number of tasks. As shown in Table 3, vLLM suffers from significant performance decline, dropping by $56\\%-73\\%$ when the number of tasks increases from 2 to 4, and eventually encountering out-of-memory (OOM) errors when the number of tasks reaches 5. In contrast, under all experimented request rates, the throughput of LoRA-Inlaid hardly declines, even with 1000 tasks served simultaneously. S-LoRA also supports a large number of tasks, while LoRA-Inlaid consistently achieves better performance across all kinds of workloads. ", "page_idx": 8}, {"type": "text", "text": "Ablation Studies of Multi-task Scheduling and Multi-task Quantization. We compare different scheduling strategies on LoRA-Inlaid. The results are shown in the left of Figure 8. \u201cOurs (w/o group)\u201d, \u201cOurs (w/o prediction)\u201d and \u201cOurs (w/o SRTF)\u201d represent three variants of our multi-task scheduling strategy without task grouping, without output length prediction and without the predictionbased SRTF, respectively. \u201cFIFO\u201d is the strategy adopted in S-LoRA and vLLM, and \u201cSkip-join ", "page_idx": 8}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/9d5424e39bac3be5c356f030e97783824f3acc7d40451c5e985bef5396d90d08.jpg", "img_caption": ["Figure 8: Left: Ablation studies of scheduling strategies on LoRA-Inlaid (100 tasks). Right: Effectiveness of quantiza-Figure 9: Impact of dynamic task addition on tion to SLO Attainment (Llama-2-13B is not shown due toonline throughput (LLaMA2-7B $@$ RTX 4090 with OOM of the other two methods). 100 tasks initially, and the request rate is 30 reqs/s). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "MLFQ\u201d represents the strategy in FastServe [40]. It is evident that our multi-task scheduling strategy achieves the best performance in terms of SLO Attainment. The designs of task grouping, output length prediction, and SRTF increase the SLO Attainment by $1.16\\times$ , $1.23\\times$ and $2.27\\times$ on average, respectively. We also explore the individual impact of multi-task quantization as shown in the right of Figure 8. Specificically, we consider a variant of LoRA-Inlaid, which disables quantization (i.e., the served model is not quantized), denoted as \u201cOurs (w/o quant)\u201d. The results show that multi-task quantization brings $39\\%$ improvement (\u201cOurs\u201d vs. \u201cOurs (w/o quant)\u201d) when serving the 7B model. Additionally, without quantization, it will lead to OOM when serving the 13B model. ", "page_idx": 9}, {"type": "text", "text": "Dynamic Task Addition. We evaluate the ability of dynamic task addition in LoRAInlaid by adding 1, 5, and 10 tasks to a heavily loaded service on the fly. The results in Figure 9 show that the throughput undergoes $10\\%{-13\\%}$ of degradation during the task addition, regardless of the number of tasks added. This is worthwhile given ", "page_idx": 9}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/8efd3e4b2c9d1ad31833e9739cabb437072cb13c50208bebc893ae0a0c956567.jpg", "table_caption": ["Table 4: Time cost of quantization. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "that the online service need not be interrupted. Meanwhile, to evaluate the time consumption of dynamic task addition, we conducted an experiment where there are 5 tasks in the ongoing service and another 5 tasks need to be added. We measured the time cost of three approaches: \u201cFull Quant\u201d, which halts the serving and performs full quantization with 10 tasks, \u201cIncr Quant offilne\u201d, an offilne variant (which halts the serving) of our incremental quantization on the 5 new tasks without layer-by-layer quantization, and \u201cIncr Quant\u201d, our incremental quantization with the 5 new tasks, which works concurrently with the ongoing service. As shown in Table 4, by avoiding the redundant computation, the time cost of forward process and calculation of Hess matrix can be reduced greatly, accelerating quantization. Moreover, although the layer-by-layer mechanism slows down the quantization by 1.26 $\\times$ due to the extra IO, it reduces the memory greatly and does not halt the serving. These empirical results validate the flexibility and robustness of LoRA-Inlaid for multi-task serving. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focused on LLM serving in multi-task scenarios and developed a multi-LoRA task serving system, namely LoRA-Inlaid. On one hand, we designed a flexible and efficient dynamic multi-task quantization algorithm that supports the joint quantization of models for multiple tasks, significantly reducing the memory requirements for model deployment. We also facilitated realtime dynamic task addition, enhancing the stability and flexibility of online services. On the other hand, we introduced a novel multi-task scheduling strategy based on output length prediction and grouping, effectively resolving the issues of high memory overhead and frequent memory swapping when applying existing strategies in multi-task scenarios. Extensive experiments demonstrated that LoRA-Inlaid significantly outperforms existing LLM serving systems. ", "page_idx": 9}, {"type": "text", "text": "Despite the effectiveness of LoRA-Inlaid, it still has several limitations. First, our quantization does not detect the existence of malicious or poisoning tasks, which might be intentionally crafted to harm the other tasks. Second, our scheduling does not consider the fairness among tasks (e.g., balancing the total numbers of output tokens for all tasks), which may be essential for shared service platforms. Third, it only supports language tasks while requiring some system re-designs for multi-modal tasks. We wish to leave the exploration of these issues as future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by National Natural Science Foundation of China (U22B2037, U23B2048), China National Postdoctoral Program for Innovative Talents (BX20230012), China Postdoctoral Science Foundation (2024M750103), Beijing Natural Science Foundation (4244080), research grant No. SH-2024JK29, the Fund of Kunpeng and Ascend Center of Excellence (Peking University), and High-performance Computing Platform of Peking University. Fangcheng Fu and Bin Cui are the corresponding authors. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] malicious-600k. https://huggingface.co/datasets/bgspaditya/malicious-600k, 2023.   \n[2] Medical_mmlu. https://huggingface.co/datasets/medalpaca/medical_meadow_ mmmlu, 2023. [3] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, 2005.   \n[4] Hong Chen, Kang Yuan, Yanjun Huang, Lulu Guo, Yulei Wang, and Jie Chen. Feedback is all you need: from chatgpt to autonomous driving. Sci. China Inf. Sci., 66(6), 2023.   \n[5] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. CoRR, abs/2310.18547, 2023.   \n[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.   \n[7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems (NeurIPS 2022), 2022.   \n[8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems (NeurIPS 2023), 2023.   \n[9] George Doddington. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research (HLT 2002), 2002.   \n[10] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018.   \n[11] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate posttraining quantization and pruning. Advances in Neural Information Processing Systems (NeurIPS 2022), 2022.   \n[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-training quantization for generative pre-trained transformers. CoRR, abs/2210.17323, 2022.   \n[13] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.   \n[14] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Rahman, Shafiq Joty Zadeh, Eduard Hovy, Naeemul Hoque, and Hamdy Mubarak. Xlsum: A large-scale multilingual abstractive summarization dataset. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), 2021.   \n[15] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. Inference without interference: Disaggregate LLM inference for mixed downstream workloads. CoRR, abs/2401.11181, 2024.   \n[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations 2022 (ICLR 2022), 2022.   \n[17] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning (ICML 2021), 2021.   \n[18] William JF Keenan. Sacre bleu: faith, fashion and freedom: Marist foundation garments 1817\u20131862. In Materializing Religion, pages 132\u2013153. Routledge, 2017.   \n[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP 2023), pages 611\u2013626, 2023.   \n[20] Zhi Lei, Guixian Zhang, Lijuan Wu, Kui Zhang, and Rongjiao Liang. A multi-level mesh mutual attention model for visual question answering. Data Sci. Eng., 7(4):339\u2013353, 2022.   \n[21] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, pages 74\u201381, 2004.   \n[22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ: activationaware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023.   \n[23] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. arXiv preprint arXiv:2310.16836, 2023.   \n[24] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345, 2019.   \n[25] Meta. Llama 2: Advancing ai through collaboration. https://about.fb.com/news/2023/ 07/llama-2/, July 2023. Accessed: 2024-04-21.   \n[26] Meta. Introducing meta llama 3: The most capable openly available llm to date. https: //ai.meta.com/blog/meta-llama-3/, 2024.   \n[27] Xupeng Miao, Xiaonan Nie, Hailin Zhang, Tong Zhao, and Bin Cui. Hetu: a highly efficient automatic parallel distributed deep learning system. Sci. China Inf. Sci., 66(1), 2023.   \n[28] NVIDIA. Fastertransformer: A fast and memory-efficient library for transformer models. https://github.com/NVIDIA/FasterTransformer, 2023. Accessed: 2024-05-17.   \n[29] NVIDIA. Tensorrt-llm. https://github.com/NVIDIA/TensorRT-LLM, 2023.   \n[30] OpenAI. ChatGPT: Optimizing Language Models for Dialogue, 2022. https://openai.com/ blog/chatgpt.   \n[31] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.   \n[32] Abhay K. Parekh and Robert G. Gallager. A generalized processor sharing approach to flow control in integrated services networks: the single-node case. IEEE/ACM Trans. Netw., 1(3):344\u2013 357, 1993.   \n[33] Muhammad Raza. Service level objectives (slos) explained, 2018. Accessed: 2024-05-17.   \n[34] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020.   \n[35] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion Stoica. S-lora: Serving thousands of concurrent lora adapters. CoRR, abs/2311.03285, 2023.   \n[36] Xian-He Sun and Xiaoyang Lu. The memory-bounded speedup model and its impacts in computing. J. Comput. Sci. Technol., 38(1):64\u201379, 2023.   \n[37] Andrew S. Tanenbaum and Herbert Bos. Modern Operating Systems. Pearson Education, 4 edition, 2014. Includes comprehensive coverage of various scheduling algorithms, including Shortest Remaining Time First (SRTF).   \n[38] J\u00f6rg Tiedemann. Parallel data, tools and interfaces in opus. In Lrec, volume 2012, pages 2214\u20132218. Citeseer, 2012.   \n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Annual Conference on Neural Information Processing Systems 2017 (NeurIPS 2017), 2017.   \n[40] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920, 2023.   \n[41] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation, 2016.   \n[42] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning (ICML 2023), 2023.   \n[43] Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. Improving BERT fine-tuning via self-ensemble and self-distillation. J. Comput. Sci. Technol., 38(4):853\u2013866, 2023.   \n[44] Yuemei Xu, Han Cao, Wanze Du, and Wenqing Wang. A survey of cross-lingual sentiment analysis: Methodologies, models and evaluations. Data Sci. Eng., 7(3):279\u2013299, 2022.   \n[45] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for transformer-based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521\u2013538, 2022.   \n[46] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International conference on machine learning (ICML 2020), 2020.   \n[47] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536, 2019.   \n[48] Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, et al. Qtsumm: A new benchmark for query-focused table summarization. arXiv preprint arXiv:2305.14303, 2023.   \n[49] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline. Advances in Neural Information Processing Systems (NeurIPS 2023), 2023.   \n[50] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. Pets: A unified framework for parameter-efficient transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages 489\u2013504, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Details of MLGPTQ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Derivation of MLGPTQ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "MLGPTQ is based on GPTQ, adapting the corresponding LoRA adapter according to the activation values of the input data to minimize the error in activation values before and after quantization. For only one task, GPTQ aims to solve the following problem: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{Q(\\mathbf{W}_{l})}||\\mathbf{W}_{l}\\mathbf{X}_{l}-Q(\\mathbf{W}_{l})\\mathbf{X}_{l}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{W}_{l}$ represents the weight of $l$ -th layer in the base model, $Q(\\mathbf{W}_{l})$ represents the quantized version of $\\mathbf{W}_{l}$ , $\\mathbf{X}_{l}$ is the input of $l$ -th layer. In other words, the objective of GPTQ is to find a $Q(\\mathbf{W}_{l})$ for each layer\u2019s weight $\\mathbf{W}_{l}$ through layer-wise quantization, in order to minimize the changes in activation values. Since we are discussing quantization within a single layer, we will omit the subscript $l$ for simplicity in the rest of this section. ", "page_idx": 13}, {"type": "text", "text": "As proved by [11], solving Eq. 3 can be transformed into solving Eq. 4 as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{Q(\\mathbf{W})}E:=\\sum_{i=1}^{d_{\\mathrm{row}}}\\left\\|\\mathbf{W}_{i,:}\\mathbf{X}-Q(\\mathbf{W})_{i,:}\\mathbf{X}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since the model has converged through training before quantization, existing works generally assume the model has reached a local minimum. Thus, when we add a small adjustment $\\nabla\\mathbf{W}$ to the parameter $\\mathbf{W}$ , according to Taylor expansion, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla E=\\left(\\frac{\\partial E}{\\partial\\mathbf{W}}\\right)^{T}\\nabla\\mathbf{W}+\\frac{1}{2}\\nabla\\mathbf{W}^{T}\\cdot\\mathbf{H}\\cdot\\nabla\\mathbf{W}+O(\\Vert\\nabla\\mathbf{W}\\Vert^{3}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{H}\\,=\\,\\partial^{2}E/\\partial\\mathbf{W}^{2}$ represents the Hessian matrix. Again, since the model has converged, existing works generally assume its first-order partial derivative is close to zero and thus negligible. By neglecting the first-order partial derivative and higher-order terms, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla E=\\frac{1}{2}\\nabla\\mathbf{W}^{T}\\cdot\\mathbf{H}\\cdot\\nabla\\mathbf{W}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that our goal is to quantize $\\mathbf{W}$ to $Q(\\mathbf{W})$ . Denote $\\nabla w_{q}=Q(w_{q})-w_{q}$ , where $w_{q}$ represents the $q$ -th element of $\\mathbf{W}$ . Then, the problem to solve can be re-written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\boldsymbol{q}}\\left\\{\\underset{\\boldsymbol{\\nabla}\\mathbf{W}}{\\arg\\operatorname*{min}}\\left(\\frac{1}{2}\\boldsymbol{\\nabla}\\mathbf{W}^{T}\\cdot\\mathbf{H}\\cdot\\boldsymbol{\\nabla}\\mathbf{W}\\right)\\left[e_{\\boldsymbol{q}}^{T}\\boldsymbol{\\nabla}\\mathbf{W}+w_{\\boldsymbol{q}}=\\boldsymbol{Q}(w_{\\boldsymbol{q}})\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $e_{q}$ represents a unit vector with a value of 1 at position $q$ and 0 elsewhere. Since this is a constrained convex optimization problem, based on the method of Lagrange multipliers, it is necessary to solve the following equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{L}=\\frac{1}{2}\\boldsymbol{\\nabla}\\mathbf{W}^{T}\\cdot\\mathbf{H}\\cdot\\boldsymbol{\\nabla}\\mathbf{W}+\\lambda(e_{q}^{T}\\boldsymbol{\\nabla}\\mathbf{W}+w_{q}-Q(w_{q})).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By taking the partial derivatives of $\\nabla W$ and $\\lambda$ , and setting them to zero to find the steady-state solution, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1}{2}(\\mathbf{H}+\\mathbf{H}^{T})\\nabla\\mathbf{W}+\\lambda e_{q}=0}\\\\ {e_{q}^{T}\\nabla\\mathbf{W}+w_{q}-Q(w_{q})=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Solving this, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda=\\frac{w_{q}-Q(w_{q})}{(\\mathbf{H}^{-1})_{q q}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla{\\bf W}=-\\frac{w_{q}-Q(w_{q})}{({\\bf H}^{-1})_{q q}}{\\bf H}^{-1}e_{q},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla E=\\frac{(w_{q}-Q(w_{q}))^{2}}{2(\\mathbf{H}^{-1})_{q q}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(\\mathbf{H}^{-1})_{q q}$ represents the value at the diagonal position $(q,q)$ of ${{\\bf{H}}^{-1}}$ , which is the inverse of the Hessian matrix. ", "page_idx": 13}, {"type": "text", "text": "For the Hessian matrix, we say that $\\mathbf{H}=2\\mathbf{X}\\mathbf{X}^{T}$ by proving the following Lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Let a be a $1\\times n$ row vector and $X$ be an $n\\times m$ matrix. The Hessian matrix of the quadratic form $\\lVert a X\\rVert_{2}^{2}$ is $2X X^{T}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $a=[a_{1},a_{2},\\ldots,a_{n}]$ be a $1\\times n$ row vector, and let $X=[x_{i j}]$ be an $n\\times m$ matrix. Let\u2019s denote $y=a X$ . Then, $y$ is a $1\\times m$ row vector with elements $y_{j}$ defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\ny_{j}=\\sum_{i=1}^{n}a_{i}x_{i j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|a X\\|_{2}^{2}=(a X)\\cdot(a X)^{T}=\\sum_{j=1}^{m}y_{j}^{2}=\\sum_{j=1}^{m}\\left(\\sum_{i=1}^{n}a_{i}x_{i j}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can expand this expression and re-write it as a quadratic form, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|a X\\|_{2}^{2}=\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\sum_{k=1}^{n}a_{i}a_{k}x_{i j}x_{k j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To find the Hessian matrix of this quadratic form, we treat it as a quadratic form in $a$ . Let $Q$ be the coefficient matrix of this quadratic form. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|a X\\|_{2}^{2}=a Q a^{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The $(i,k)$ element of $Q$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{i k}=\\sum_{j=1}^{m}x_{i j}x_{k j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the matrix $Q$ can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ=X X^{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "And the Hessian matrix is twice $Q$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nH=2Q=2X X^{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the Hessian matrix of $\\|a X\\|_{2}^{2}$ is $2X X^{T}$ , which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "GPTQ quantizes the weight W row by row. For each row, according to Eq. 12 mentioned above, it finds the minimum $w_{q}$ that leads to an increase in the loss function due to quantization, then calculates scales via \u03b1 = max(Wi,:Q)\u2212max(Wi,:), performs quantization using $\\alpha$ , and finally update the remaining values using Eq. 11. This process repeats until all parameters have been updated. ", "page_idx": 14}, {"type": "text", "text": "MLGPTQ considers the scenario of quantization for multiple tasks. During the forward propagation process, it dynamically loads the corresponding LoRA adapter for each task to simulate the correct activation values for the respective tasks. Consequently, the problem we need to solve is as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underset{Q(\\mathbf{W})}{\\arg\\operatorname*{min}}\\,||\\sum_{t=1}^{T}((\\mathbf{W}+\\mathbf{B}_{t}\\mathbf{A}_{t})\\mathbf{X}_{t}-(Q(\\mathbf{W})+\\mathbf{B}_{t}\\mathbf{A}_{t})\\mathbf{X}_{t})||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $T$ denotes the number of tasks, ${\\bf A}_{t}$ and $\\mathbf{B}_{t}$ are the low-rank adapter matrices of the $t$ -th task, $\\mathbf{X}_{t}$ is the input of $t$ -th task, and W and $Q(\\mathbf{W})$ denote the original and quantized weights of a layer. ", "page_idx": 14}, {"type": "text", "text": "Denote $\\widetilde{\\mathbf{W}_{t}}:=\\mathbf{W}+\\mathbf{B}_{t}\\mathbf{A}_{t}$ , then the problem is re-written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underset{Q(\\mathbf{W})}{\\arg\\operatorname*{min}}\\sum_{i=1}^{d_{\\mathrm{row}}}\\left\\|\\sum_{t=1}^{T}(\\widetilde{\\mathbf{W}}_{\\mathbf{t}i,:}\\mathbf{X}_{t}-Q(\\widetilde{\\mathbf{W}}_{\\mathbf{t}})_{i,:}\\mathbf{X}_{t})\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Same as GPTQ, we could get $T$ Hessian matrix $\\mathbf{H}_{t}$ , where $t\\in[1,T]$ . To minimize the objective function, we can obtain the updating formulas for $\\mathbf{W}$ and $E$ in MLGPTQ: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla{\\mathbf{W}}=-\\frac{w_{q}-Q(w_{q})}{(\\mathbf{H}_{t^{*}}^{-1})_{q q}}\\mathbf{H}_{t^{*}}^{-1}e_{q},\\nabla E=\\frac{(w_{q}-Q(w_{q}))^{2}}{2\\left((\\mathbf{H}_{t^{*}}^{-1})_{q q}\\right)},\\mathrm{~where~}t^{*}=\\underset{t\\in[1,T]}{\\arg\\operatorname*{max}}\\left((\\mathbf{H}_{t}^{-1})_{q q}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The updating method described here leads to the max-aggregation method proposed in $\\S\\,3.1$ , which always selects the Hessian matrix of the task that minimizes $\\nabla E$ for updating, ultimately reducing the overall error and thus better-guiding parameter updates. ", "page_idx": 15}, {"type": "text", "text": "A.2 Pseudocode of MLGPTQ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide the pseudocode of MLGPTQ in Algorithm 1, and we also highlight the differences compared to directly applying GPTQ to multi-task scenarios, which is termed $\\mathrm{GPTQ}_{t w e a k e d}$ in our work (i.e., with a mixed calibration set and without any LoRA adapters). ", "page_idx": 15}, {"type": "text", "text": "Due to the high complexity and numerical instability of the process described in Appendix A.1, we leverage the following optimizations to accelerate the quantization, partly inspired by the practical implementation of GPTQ [12]. ", "page_idx": 15}, {"type": "text", "text": "Random Order Optimization. GPTQ requires updating weights in the order that produces the smallest quantization error $\\nabla E$ . For $\\mathbf{W}\\in\\mathbb{R}^{m\\times n}$ , the complexity of GPTQ is $O(m n^{\\bar{3}})$ . However, using a random order achieves similar results and facilitates GPU parallel optimization [12]. ", "page_idx": 15}, {"type": "text", "text": "Batch Processing. Since weight updates between different columns of the same matrix W are non-redundant, we use batch processing and delayed updates, with 128 columns processed at a time, to enhance computation speed. ", "page_idx": 15}, {"type": "text", "text": "Cholesky Decomposition. Using numerically stable Cholesky decomposition to pre-compute the necessary information increases computational stability. ", "page_idx": 15}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/28cf71a457458a0073f51eb7b542825db18a868c238232d6eec859eb8efa8a9c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Details of our Multi-task Scheduling Strategy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our multi-task scheduling strategy is based on task grouping and prediction-based SRTF. Compared to other scheduling strategies, it is well-suited for multi-task scenarios, achieving excellent results. The pseudocode is shown in Algorithm 2, with the helper functions generate_new_batch and schedule_new_batch in Algorithm 3 and 4. There are four queues maintaining different requests in our system: $\\textcircled{1}$ prefill_reqs: requests that have not yet entered the prefill stage (i.e., new requests that have not yet been served). $\\circledcirc$ decoding_reqs: requests in the decoding stage. $\\circled{3}$ hungry_prefill_reqs: requests in a starving state that have not yet been served. $\\textcircled{4}$ hungry_decoding_reqs: requests in a starving state in the decoding stage. ", "page_idx": 16}, {"type": "text", "text": "First, we check if running_batch is empty (line 7) to determine whether the system is in a cold start phase (i.e., the previous scheduling step did not have any requests to serve). If yes, we perform a cold start by calling generate_new_batch to get a new batch to schedule from prefill_reqs (line 8). If new_batch is not empty, we proceed to prefill with new_batch (line 10-11). Otherwise, the system remains idle since there are no requests (line 13). If running_batch is not empty, we check if we have consecutively executed max_cont_decode decoding steps (line 16). If yes, we call generate_new_batch to schedule new requests from prefill_reqs and hungry_prefill_reqs (line 17-20). Otherwise, we check if the continuous scheduling for a batch has reached a pre-defined threshold max_cont_decode_one_batch (line 22). If yes, we call schedule_new_batch to schedule new requests in the decoding stage from decoding_reqs and hungry_decoding_reqs according to the SRTF and grouping strategy (line 23-25). Otherwise, we continue processing the current running_batch (line 27-28). This two-level threshold strategy effectively reduces the frequent swapping of LoRA adapters and KV caches due to frequent batch switching. Additionally, by adjusting the threshold size, we can ensure the immediacy and flexibility of scheduling. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Our multi-task scheduling strategy based on grouping and SRTF. ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/18b07cb8d22190f70f666da3fc908118c6f64108d6452c73e8d0b092d036e82a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "1: function GENERATE_NEW_BATCH(prefill_reqs, hungry_prefill_reqs)   \n2: Sort prefill_reqs by len(prompt) $^+$ predict_output_len ascending   \n3: Sort hungry_prefill_reqs by waiting_time descending, then len(prompt) $^+$ predict_output_len   \nascending   \n4: $n e w\\bar{\\_}b a t c h\\gets\\emptyset$   \n5: for each req in hungry_prefill_reqs do   \n6: if can_add_req(req, new_batch) and meet_max_lora(req, new_batch) then   \n7: new_batch.append(req); hungry_prefill_reqs.remove(req)   \n8: end if   \n9: end for   \n10: for each req in prefill_reqs do   \n11: if can_add_req(req, new_batch) and meet_max_lora(req, new_batch) then   \n12: new_batch.append(req); prefill_reqs.remove(req)   \n13: end if   \n14: end for   \n15: if new_batch not full then   \n16: for each req in hungry_prefill_reqs + prefill_reqs do   \n17: if can_add_req(req, new_batch) then   \n18: new_batch.append(req); prefill_reqs.remove(req)   \n19: end if   \n20: end for   \n21: end if   \n22: for each req in prefill_reqs and hungry_prefill_reqs do   \n23: req.waiting_tim $?\\leftarrow$ req.waiting_time $+\\,1$   \n24: end for   \n25: for each req in prefill_reqs do   \n26: if req.waiting_time $\\geq$ Threshold then   \n27: prefill_reqs.remove(req); hungry_prefill_reqs.append(req)   \n28: end if   \n29: end for   \n30: return new_batch   \n31: end function ", "page_idx": 17}, {"type": "text", "text": "Algorithm 4 The utility function schedule_new_batch for Algorithm 2. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: function SCHEDULE_NEW_BATCH(running_batch,decoding_reqs,hungry_decoding_reqs)   \n2: Sort decoding_reqs by predict_output_len \u2212len(output) ascending   \n3: Sort hungry_decoding_reqs by waiting_time descending, then predict_output_len\u2212len(output)   \nascending   \n4: new_batch \u2190\u2205   \n5: for each req in hungry_decoding_reqs do   \n6: if can_add_req(req, new_batch) and req.lora $\\in$ running_batch then   \n7: new_batch.append(req); hungry_decoding_reqs.remove(req)   \n8: end if   \n9: end for   \n10: for each req in decoding_reqs do   \n11: if can_add_req(req, new_batch) and req.lora $\\in$ running_batch then   \n12: new_batch.append(req); decoding_reqs.remove(req)   \n13: end if   \n14: end for   \n15: if new_batch not full then   \n16: for each req in hungry_decoding_reqs $^+$ decoding_reqs do   \n17: if can_add_req(req, new_batch) then   \n18: new_batch.append(req); decoding_reqs.remove(req)   \n19: end if   \n20: end for   \n21: end if   \n22: for each req in decoding_reqs do   \n23: req.waiting_time $\\leftarrow$ req.waiting_time + 1   \n24: end for   \n25: return new_batch   \n26: end function ", "page_idx": 17}, {"type": "text", "text": "C More Experimental Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Experimental Environment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The GPU platforms for evaluation are shown in Table 5. The RTX 3090 platform is equipped with Intel(R) Core(TM) i9-10900X CPU $@$ $\\textsuperscript{\\textregistered3.70}\\mathrm{GHz}$ and 256GB host memory, while the RTX 4090 platform is equipped with Intel(R) Xeon(R) Gold 6330 CPU $@$ 2.00GH and 512GB host memory. ", "page_idx": 18}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/58cc997c80d8b8d587c346eb9448208a501a27b346e12297ab7e8f72f1db83d0.jpg", "table_caption": ["Table 5: Experimental GPU platforms in detail. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Summary of Evaluated Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We primarily select 12 major datasets for testing, covering tasks such as machine translation, text summarization, table summarization, code generation, math QA, medical QA and malicious detection. For each task, we use open-source models from Hugging Face2that have been fine-tuned using the training set of the corresponding dataset and evaluated on the test set. The summary is presented in Table 6. ", "page_idx": 18}, {"type": "table", "img_path": "HfpV6u0kbX/tmp/d21c0b87eceec6b15680f6396e44c2c38470d7f162d5b9b6a9eff98f9c252bf5.jpg", "table_caption": ["Table 6: Dataset Summary "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "For the machine translation tasks of different languages, we consider the classic bilingual translation dataset OPUS [38], specifically choosing six translation tasks: French-to-English, Czech-to-English, Indonesian-to-English, Vietnamese-to-English, Danish-to-English, and Swedish-to-English, considering the diversity of languages. ", "page_idx": 18}, {"type": "text", "text": "For the text translation task, we consider the XLSum dataset [14], a comprehensive and diverse dataset comprising 1.35 million professionally annotated text-summary pairs extracted from the BBC. ", "page_idx": 18}, {"type": "text", "text": "For the table summarization task, we consider the QTSumm dataset [48], which is a large-scale dataset for query-centric summarization tasks on tabular data. It contains 7,111 human-annotated query-summary pairs and 2,934 tables covering various topics. ", "page_idx": 18}, {"type": "text", "text": "For the code generation task, we consider the tiny-codes dataset [13]. This dataset consists of 16 million short and clear code snippets, aiding LLM models in learning how to reason using both natural and programming languages. The dataset includes a variety of programming languages such as Python, TypeScript, JavaScript, Ruby, Julia, Rust, $C++$ , Bash, Java, C#, and Go. ", "page_idx": 18}, {"type": "text", "text": "For the math QA task, we choose the GSM8k (Grade School Math 8K) dataset [6], which consists of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning. ", "page_idx": 19}, {"type": "text", "text": "For the medical QA task, we choose medical_meadow_mmmlu [2], which contains $3.79\\mathbf{k}$ medical multiple choice question. ", "page_idx": 19}, {"type": "text", "text": "For malicious detection task, we choose malicious-600k [1], which contains 641k URLs to determine whether they are malicious. ", "page_idx": 19}, {"type": "text", "text": "C.3 Summary of Metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "(1) SacreBLEU [18], represented as S_BLEU, is a classic machine translation evaluation standard. It scores by comparing the n-gram overlap between the machine translation output and one or more reference translations, while also considering a brevity penalty to prevent favoring overly short translation outputs. ", "page_idx": 19}, {"type": "text", "text": "(2) rouge1 [21], represented as ROUGE1, calculates the overlap ratio of words between the machinegenerated text and the reference text, i.e., the unigram (single word) match rate. ", "page_idx": 19}, {"type": "text", "text": "(3) rouge2 [21], represented as ROUGE2, similar to rouge1, indicates the bigram (two-word sequence) match rate. ", "page_idx": 19}, {"type": "text", "text": "(4) nist_mt [9], represented as NIST_MT, is an improvement on BLEU that gives higher weight to less common words, encouraging diversity and accuracy in translation. ", "page_idx": 19}, {"type": "text", "text": "(5) meteor [3], abbreviated as METEOR, calculates the score based on the harmonic mean of precision and recall and introduces a penalty factor to penalize excessive mismatches. ", "page_idx": 19}, {"type": "text", "text": "(6) google_bleu [41], abbreviated as G_BLEU, is an improvement on BLEU that adjusts for smoothing methods, n-gram weights, and brevity penalties to optimize its performance. ", "page_idx": 19}, {"type": "image", "img_path": "HfpV6u0kbX/tmp/b60e48de7fff4feb18c47d785b2b0d8c212a4ec000ae5af151c3b7411679b2f9.jpg", "img_caption": ["D Multi-LoRA AWQ Migration "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Our work centers on GPTQ due to its widespread use, but our solution can also adapt to AWQ. We presented the differences between MLAWQ and AWQtweaked in Alg 5. As introduced in $\\S3.1$ , most quantization methods follow the Forward-Aggregate Info-Modify Weight-Quant paradigm. In essence, AWQtweaked smooths outliers by multiplying weights with a smoothing factor, best_s, to minimize per-channel quant error: ", "page_idx": 20}, {"type": "text", "text": "\u2022 In Forward, the input is multiplied by the weights to create an unquantized monitor, guiding min error quantization (line 1).   \n\u2022 In Aggregate Info, the average of all samples and weights is calculated for each channel (lines 2&3) to determine smoothing factor $s$ (line 7). W and X are smoothed to remove outliers (lines 8&9). Then, smoothed W is pseudo-quantized (quantize-then-dequantize to simulate round loss) and compared to the unquantized monitor for quantization error (line 10). This process iterates over various ratios (line 6), selecting the factor with the smallest error as best_s (line 11). Then, this best_s is used to Modify the weight (line 12), followed by the Quant (line 13) process using the modified weight. ", "page_idx": 20}, {"type": "text", "text": "The drawbacks discussed in $\\S3.1$ also exist for AWQtweaked in multi-task quantization: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Forward (line 1): It can\u2019t pass LoRA adapters during activation distribution simulation, causing quantization bias during inference.   \n\u2022 Aggregate Info: It uses $X_{\\mathrm{mean}}=X$ .mean(0), a naive mixed average of multi-tasks\u2019 info. Since each task affects each channel differently, simply averaging blurs distributions, ignoring individual effects. ", "page_idx": 20}, {"type": "text", "text": "As explained in $\\S3.1$ and shown in Fig 2, MLGPTQ mainly improves the first three steps to tackle GPTQ\u2019s issues in multi-task scenarios. Similarly, we can fix AWQ\u2019s issues to create a better multi-task quantization algorithm MLAWQ: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Forward: MLAWQ loads corresponding LoRA adapter for each task to participate in forward propagation, accurately simulating real activation distribution (line 1). \u2022 Aggregate Info: Instead of mixing and averaging features of each column across all tasks to compute $s$ , MLAWQ computes the average for each task separately to get $s_{i}$ (line 3&7). Then it calculates quantization error for each column rather than the entire matrix (line 10). If the $i$ -th task results in the smallest quantization error for the $j$ -th column, it sets best_s $[j]=s_{i}[j]$ (line 11). This approach allows optimal error minimization, showing each task\u2019s individual effect on different channels, enhancing Aggregate Info (lines 3&6-11), and improving the Modify Weight (line 12) and Quant (line 13) processes. ", "page_idx": 21}, {"type": "text", "text": "In summary, our work identifies common drawbacks of current single-task quantization methods in multi-task scenarios. By addressing these issues, we can develop more precise multi-task quantization algorithms. ", "page_idx": 21}, {"type": "text", "text": "E Broader Impact and Future Work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This work proposes LoRA-Inlaid, a brand new LLM serving system for the multi-task scenario. LoRA-Inlaid is featured with a series of innovations, specifically the multi-task joint quantization algorithm, the dynamic task addition mechanism, and the multi-task scheduling strategy. Considering the booming research and applications of LLMs in various downstream tasks, we believe LoRA-Inlaid has the potential to gain widespread adoption and shed light on the high-performance and resourceefficient system designs for follow-up works in the field of LLM serving. However, there are several issues that LoRA-Inlaid does not consider currently. ", "page_idx": 22}, {"type": "text", "text": "On the one hand, the multi-task quantization algorithm in LoRA-Inlaid does not involve the detection of malicious or poisoning tasks that may bring negative impacts on the other tasks. One of the most typical use cases of LoRA-Inlaid is the personalization of LLMs, where clients can upload their data to create personalized LoRA adapters using the same base model (or directly upload their self-tuned LoRA adapters). The server is responsible for serving requests from all these clients using the proposed LoRA-Inlaid system. Fortunately, these LoRA adapters are independently manufactured, so we can apply malicious detection to them individually. For instance, the server can prepare a rich set of evaluations to assess the security risks of each LoRA adapter, including violence, discrimination, unlawful responses, etc. If any LoRA adapters fail to pass the evaluation, the server can reject serving them. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, the multi-task scheduling strategy in LoRA-Inlaid ignores the fairness among different tasks (e.g., controlling the number of output tokens to be close), which may make our work ineffective under some settings. To measure fairness among tasks, we can compute a weighted combination of numbers of input and output tokens for each task. This is because the prefilling and decoding phases in LLM inference have different workload characteristics [15](also why these tokens differ in online service pricing). Then, we can borrow the idea of Weighted Fair Queueing (WFQ) [32] for scheduling different tasks. We wish to address these issues in the future. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 3 and Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Section 5 and Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Appendix A.1, ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Section 4.1, Appendix C.1, Appendix C.2, and we have provided the link to source code. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 4.1, Appendix C.2, and we have provided the link to source code. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 4.1, Appendix C.1, Appendix C.2, and Appendix C.3. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 4 and Appendix C.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Appendix E. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 26}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Not applicable. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]