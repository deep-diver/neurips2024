[{"figure_path": "Xp8qhdmeb4/figures/figures_1_1.jpg", "caption": "Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSFF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone.", "description": "This figure compares different feature fusion schemes used in various architectures for medical image segmentation.  It illustrates how different methods (UNet, UNet++, CoTr, DiNTS) combine multi-scale features extracted from a backbone network.  The main focus is on E2ENet (e), which utilizes a Dynamic Sparse Feature Fusion (DSFF) mechanism, dynamically learning the most informative features to integrate and reducing redundancy. The thickness of red lines in E2ENet visually represents the sparsity of the feature fusion, with thicker lines indicating stronger connections and thinner lines indicating weaker ones.", "section": "1 Introduction"}, {"figure_path": "Xp8qhdmeb4/figures/figures_2_1.jpg", "caption": "Figure 2: The overall architecture of the proposed E2ENet consists of a CNN backbone that extracts multiple levels of features. These features are then gradually aggregated through several stages, during which the multi-scale features are fused using a fusion operation.", "description": "This figure illustrates the architecture of the E2ENet model.  It shows the Efficient backbone extracting features at multiple levels, which are then fed into a Feature Pyramid. These features are gradually aggregated across multiple stages using a Dynamic Sparse Feature Fusion (DSFF) mechanism.  In each stage, the features are fused using a fusion operation that includes LeakyReLU and InstanceNorm, then passed through a Shift CNN.  Finally, the fused features are processed by an Output Module to generate the final output feature map. The figure visually depicts how bottom-up, top-down, and forward connections are integrated for multi-scale feature fusion in the E2ENet network.", "section": "2.1 The Architecture"}, {"figure_path": "Xp8qhdmeb4/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of our Dynamic Sparse Feature Fusion (DSFF) mechanism. The fusion operation starts from sparse feature connections and allows the connectivity to be evolved after training for \u0394T epochs. During each evolution stage, a fraction of kernels with smaller L\u2081 norms will be zeroed out (red dotted line), while the same fraction of other inactivated connections will be reactivated randomly, keeping the feature sparsity S constant during training (blue solid line).", "description": "This figure illustrates the Dynamic Sparse Feature Fusion (DSFF) mechanism.  The DSFF mechanism starts with sparse connections between feature maps.  Over a period of \u0394T training epochs, the network dynamically adjusts these connections. Connections with low importance (measured by the L1 norm of their kernels) are removed (zeroed out, shown in red), while the same number of inactive connections are randomly reactivated (shown in blue). This process repeats every \u0394T epochs, maintaining a constant sparsity level (S) throughout training.  The figure visually represents this iterative process of sparsity adjustment and connection evolution.", "section": "2.2 Dynamic Sparse Feature Fusion"}, {"figure_path": "Xp8qhdmeb4/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration of restricted depth-shift in 3D Convolution of our E2ENet. The input features (left) are firstly split into 3 parts along the channel dimension, and then shifted by {-1,0,1} units along the depth dimension respectively (middle). After that, 3D CNNs with kernel size 1\u00d73\u00d73 are performed on the feature maps (middle) to generate the output features (right).", "description": "This figure illustrates the restricted depth-shift operation used in E2ENet's 3D convolutions.  The input feature maps are divided into three parts along the channel dimension. Each part is then shifted by -1, 0, or 1 unit along the depth dimension. Finally, 3D convolutions with a 1x3x3 kernel are applied to these shifted feature maps to capture 3D spatial relationships while maintaining the efficiency of 2D convolutions.", "section": "2.3 Restricted Depth-Shift in 3D Convolution"}, {"figure_path": "Xp8qhdmeb4/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison of E2ENet and nnUNet on the AMOS-CT challenges.", "description": "This figure shows a qualitative comparison of the segmentation results obtained using E2ENet and nnUNet on the AMOS-CT dataset.  The comparison highlights the differences in the segmentation of various organs, such as the stomach, esophagus, and duodenum, showcasing E2ENet's improved ability to distinguish between these closely located organs. The image includes visual representations of the ground truth, nnUNet segmentations, and E2ENet segmentations for better comparison.", "section": "3.5 Qualitative Results"}, {"figure_path": "Xp8qhdmeb4/figures/figures_9_1.jpg", "caption": "Figure 6: The proportions of feature connections during training with the DSFF mechanism at a feature sparsity level of 0.8 on the AMOS-CT challenge.", "description": "This figure visualizes how the Dynamic Sparse Feature Fusion (DSFF) mechanism learns to connect features from different levels during training.  It shows the proportions of feature connections from \"upward\", \"downward\", and \"forward\" directions at a feature sparsity level of 0.8. The proportions are shown at three different training epochs (100th, 400th, and 1000th). The visualization helps to understand how the DSFF mechanism adapts and learns to filter out less important connections, making the feature fusion more efficient during the training process.", "section": "3.6 Feature Fusion Visualization"}, {"figure_path": "Xp8qhdmeb4/figures/figures_19_1.jpg", "caption": "Figure 3: Illustration of our Dynamic Sparse Feature Fusion (DSFF) mechanism. The fusion operation starts from sparse feature connections and allows the connectivity to be evolved after training for \u0394T epochs. During each evolution stage, a fraction of kernels with smaller L\u2081 norms will be zeroed out (red dotted line), while the same fraction of other inactivated connections will be reactivated randomly, keeping the feature sparsity S constant during training (blue solid line).", "description": "This figure illustrates the Dynamic Sparse Feature Fusion (DSFF) mechanism.  It shows how the fusion operation starts with sparse connections and then adapts during training.  Over time, less important connections are removed, while a similar number of previously inactive connections are re-activated at random. This process maintains a constant level of sparsity (S) throughout training, improving efficiency by focusing on the most informative feature connections.", "section": "2.2 Dynamic Sparse Feature Fusion"}, {"figure_path": "Xp8qhdmeb4/figures/figures_20_1.jpg", "caption": "Figure 2: The overall architecture of the proposed E2ENet consists of a CNN backbone that extracts multiple levels of features. These features are then gradually aggregated through several stages, during which the multi-scale features are fused using a fusion operation.", "description": "This figure illustrates the architecture of the Efficient to Efficient Network (E2ENet), a novel model designed for efficient 3D medical image segmentation. The model consists of a convolutional neural network (CNN) backbone responsible for extracting multiple levels of features from the input 3D image. These features are then hierarchically fused through several stages, involving a fusion operation that combines adjacent features to fully leverage the information across different scales. The fusion process incorporates multi-scale features, progressively aggregating information through the model's stages, resulting in the final output.", "section": "2.1 The Architecture"}, {"figure_path": "Xp8qhdmeb4/figures/figures_20_2.jpg", "caption": "Figure 5: Qualitative comparison of E2ENet and nnUNet on the AMOS-CT challenges.", "description": "This figure shows a qualitative comparison of the segmentation results obtained using E2ENet and nnUNet on the AMOS-CT dataset.  The image shows example slices from the dataset and highlights the differences in segmentation quality between the two models. Red boxes are used to highlight areas of particular interest where the differences in performance are more pronounced. The goal is to visually demonstrate the improved accuracy of E2ENet in segmenting challenging structures.", "section": "3.5 Qualitative Results"}, {"figure_path": "Xp8qhdmeb4/figures/figures_21_1.jpg", "caption": "Figure 10: Qualitative comparison of the proposed E2ENet and nnUNet on BraTS Challenge in MSD.", "description": "This figure shows a qualitative comparison of the segmentation results of the proposed E2ENet and the nnUNet baseline method on the BraTS challenge dataset. The figure displays four sample images from the BraTS dataset, along with their corresponding segmentation masks generated by both methods.  The segmentation masks are color-coded to represent the different tumor regions of interest: edema (ED), enhancing tumor (ET), and non-enhancing tumor (NET).  The red boxes highlight specific areas where the difference between the methods' segmentations is most apparent.  The comparison aims to demonstrate E2ENet's ability to improve segmentation accuracy, particularly in challenging areas.", "section": "3.5 Qualitative Results"}, {"figure_path": "Xp8qhdmeb4/figures/figures_21_2.jpg", "caption": "Figure 11: The learning curve of E2ENet on AMOS-CT, with green dotted vertical lines indicating the epochs of weight activation and deactivation. The blue line represents the ratio of weight deactivation/reactivation throughout the training process.", "description": "This figure shows the training and validation loss curves for the E2ENet model trained on the AMOS-CT dataset.  The green dotted lines mark the epochs where the dynamic sparse feature fusion (DSFF) mechanism updates the network's topology by activating or deactivating connections (weights). The blue line shows the ratio of deactivation to activation at each step. The plot illustrates how the training loss initially increases after topology updates, but the overall loss decreases over time, demonstrating the effectiveness of the DSFF mechanism for improving model efficiency.", "section": "A.10 Convergence Analysis"}, {"figure_path": "Xp8qhdmeb4/figures/figures_22_1.jpg", "caption": "Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSFF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone.", "description": "This figure compares different feature fusion schemes in various architectures, including UNet, UNet++, CoTr, and DiNTS.  It highlights the unique dynamic sparse feature fusion (DSFF) mechanism used in E2ENet, which allows for efficient and adaptive fusion of multi-scale features by dynamically learning the information flow paths.  The thickness of the red lines in the E2ENet diagram represents the sparsity of the connections, demonstrating the model's efficiency.", "section": "1 Introduction"}, {"figure_path": "Xp8qhdmeb4/figures/figures_22_2.jpg", "caption": "Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSFF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone.", "description": "This figure compares different feature fusion schemes used in various neural network architectures for image segmentation.  It highlights the differences between UNet, UNet++, CoTr, DiNTS, and the proposed E2ENet.  The key difference is how E2ENet dynamically learns sparse connections between features, leading to efficient feature fusion, unlike the other methods that use dense connections or complex search algorithms. The purple nodes represent features from the backbone network, green represents the fused features, and the red lines represent the information flow, with line thickness in E2ENet indicating the strength of the learned connection.", "section": "1 Introduction"}, {"figure_path": "Xp8qhdmeb4/figures/figures_22_3.jpg", "caption": "Figure 6: The proportions of feature connections during training with the DSFF mechanism at a feature sparsity level of 0.8 on the AMOS-CT challenge.", "description": "This figure visualizes how the Dynamic Sparse Feature Fusion (DSFF) mechanism dynamically learns to fuse multi-scale features from three different directions (upward, forward, and downward) during training. The figure illustrates the proportions of feature map connections from these directions to a specific fused feature node at different training epochs (initial, 100th, 400th, and 1000th epoch) with a feature sparsity level of 0.8 on the AMOS-CT challenge. The proportions are presented visually, showing the learned importance of features from different directions for optimal fusion at each stage of the training process.", "section": "3.6 Feature Fusion Visualization"}, {"figure_path": "Xp8qhdmeb4/figures/figures_23_1.jpg", "caption": "Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSFF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone.", "description": "This figure compares different feature fusion schemes used in various architectures for image segmentation.  It shows how different methods, including UNet, UNet++, CoTr, and DiNTS, handle the integration of multi-scale features extracted from the backbone network.  The main focus is on E2ENet, highlighting its dynamic sparse feature fusion (DSFF) mechanism, which allows for adaptive learning of the feature fusion process, resulting in efficient feature integration.", "section": "1 Introduction"}]