[{"type": "text", "text": "E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Boqian $\\mathbf{W}\\mathbf{u}^{1,2,*}$ , Qiao Xiao3,\u2217, Shiwei ${\\bf L i u^{4}}$ , $\\mathbf{Lu}\\ \\mathbf{Y}\\mathbf{in}^{5}$ , Mykola Pechenizkiy3, Decebal Constantin Mocanu2,3, Maurice van Keulen1, Elena Mocanu1 ", "page_idx": 0}, {"type": "text", "text": "1 University of Twente, 2 University of Luxembourg, 3 Eindhoven University of Technology, 4 University of Oxford, 5 University of Surrey {b.wu, m.vankeulen,e.mocanu}@utwente.nl {q.xiao,m.pechenizkiy}@tue.nl, shiwei.liu@maths.ox.ac.uk, l.yin@surrey.ac.uk, decebal.mocanu@uni.lu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have evolved as the leading approach in 3D medical image segmentation due to their outstanding performance. However, the ever-increasing model size and computational cost of deep neural networks have become the primary barriers to deploying them on real-world, resource-limited hardware. To achieve both segmentation accuracy and efficiency, we propose a 3D medical image segmentation model called Efficient to Efficient Network (E2ENet), which incorporates two parametrically and computationally efficient designs. i. Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy. ii. Restricted depth-shift in 3D convolution: it leverages the 3D spatial information while keeping the model and computational complexity as 2D-based methods. We conduct extensive experiments on AMOS, Brain Tumor Segmentation and BTCV Challenge, demonstrating that E2ENet consistently achieves a superior trade-off between accuracy and efficiency than prior arts across various resource constraints. E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while saving over $69\\%$ parameter count and $27\\%$ FLOPs in the inference phase, compared with the previous best-performing method. Our code has been made available at: https://github.com/boqian333/E2ENet-Medical. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D medical image segmentation plays an essential role in numerous clinical applications, including computer-aided diagnosis [Yu et al., 2020] and image-guided surgery systems [Ronneberger et al., 2015]. Over the past decade, the rapid development of deep neural networks has achieved tremendous breakthroughs and significantly boosted the progress in this area [Zhou et al., 2018, Huang et al., 2020, Isensee et al., 2021]. However, the model sizes and computational costs are also exploding, which deter their deployment in many real-world applications, especially for 3D models where resource consumption scales cubically [Hu et al., 2021, Valanarasu and Patel, 2022]. This naturally raises a research question: Can we design a 3D medical image segmentation method that trades off accuracy and efficiency better, subjected to different resource availability? ", "page_idx": 0}, {"type": "text", "text": "Accurately segmenting organs is a challenging task in 3D medical image segmentation due to the variability in size and shape even among the same type of organ, caused by factors such as patient anatomy and disease stage. This diversity amplifies the difficulty to accurately identify and distinguish the boundaries of different organs, leading to potential errors in segmentation. One of the main solutions for accurate medical image segmentation is to favorably leverage the multi-scale features extracted by the backbone network, but this remains a long-standing problem. Pioneering work UNet [Ronneberger et al., 2015] utilizes skip connection to propagate detailed information to high-level features. More recently, $\\mathrm{UNet++}$ [Zhou et al., 2018], CoTr [Xie et al., 2021], and DiNTS [He et al., 2021] have introduced more complex neural network architectures (e.g., dense skip connection and attention mechanism) and optimization techniques (e.g., neural architecture search (NAS) [Elsken et al., 2019]) for cross-scale feature fusion. NAS-based methods search for network topology and operators (e.g., 3x3 Convolution and Maxpool) and subsequently optimize model weights. However, these methods typically require substantial computational resources to explore network topologies and evaluate numerous candidate architectures, making them time-consuming. For instance, C2FNAS [Yu et al., 2020] requires nearly one GPU-year to discover a single 3D segmentation architecture, while DiNTS [He et al., 2021] improves search efficiency but still needs 5.8 GPU days to find a single architecture. In contrast to NAS approaches, our method does not require costly architecture search time and instead directly optimizes and searches for sparse topologies within the predefined architecture. ", "page_idx": 0}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/6f9a076cac021d81dac502d83b916e4d96c3a0ecc6f38cc298d89c35adadbcb8.jpg", "img_caption": ["Figure 1: A comparison of feature fusion schemes. The purple nodes depict features extracted from the backbone, while the green nodes depict the fused features. In particular, in DiNTS (d), red lines indicate information flow paths determined through neural architecture search techniques. In E2ENet (e), the red lines with different widths represent sparse information flows determined by the DSFF mechanism, allowing for efficient feature fusion. E2ENet is capable of dynamically learning how many of the features to fuse are derived from the backbone. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose the Efficient to Efficient Network (E2ENet), a model that efficiently incorporates both bottom-up and top-down features from the backbone network in a dynamically sparse pattern, achieving an improved accuracy-efficiency trade-off. As shown in Figure 1 (e), E2ENet incorporates multi-scale features from the backbone into the final output by gradually fusing adjacent features, allowing the network to fully utilize information across various scales. To prevent unnecessary information aggregation, we propose a dynamic sparse feature fusion (DSFF) mechanism and embed it in each fusion node. The DSFF mechanism adaptively integrates informative multi-scale features and filters out unnecessary ones during the course of the training process, significantly reducing the computational overhead without sacrificing performance. Additionally, to further improve efficiency, our E2ENet employs a restricted depth-shift strategy within 3D convolutions, derived from the temporal shift [Lin et al., 2019] and 3D-shift [Fan et al., 2020] strategies used in efficient video action recognition. This allows the 3D convolution operation with a kernel size of $(1,3,3)$ to capture 3D spatial relationships while maintaining the parameter complexity of 2D convolutions and reducing computational cost. To evaluate the performance of our proposed E2ENet, we conducted extensive experiments on the AMOS-CT [Ji et al., 2022], Brain Tumor Segmentation in the Medical Segmentation Decathlon (MSD) [Antonelli et al., 2022], and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) [Landman et al., 2015] challenges. We found that E2ENet can effectively trade-off between segmentation accuracy and efficiency compared to both convolution-based and transformer-based architectures. In particular, on the AMOS-CT challenge, E2ENet achieves competitive accuracy with a mDice of $90.0\\%$ , while being $69\\%$ smaller and using $27\\%$ fewer FLOPs during inference. With the DSFF mechanism filtering out $90\\%$ of feature connections, E2ENet further reduces resource costs without significantly sacrificing accuracy. ", "page_idx": 1}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we first present the overall architecture of E2ENet, which allows for the fusion of multi-scale features from three directions. Next, we describe the proposed DSFF mechanism, which adaptively selects informative features and filters out unnecessary ones during training. Lastly, to further increase efficiency, we introduce the use of restricted depth-shift in 3D convolution. ", "page_idx": 1}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/8b20b97a5e8714f59731f86e10e5825951cb63df402fe2dbdb45b06cfdcc6760.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The overall architecture of the proposed E2ENet consists of a CNN backbone that extracts multiple levels of features. These features are then gradually aggregated through several stages, during which the multi-scale features are fused using a fusion operation. ", "page_idx": 2}, {"type": "text", "text": "2.1 The Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Figure 2 provides an overview of the proposed architecture. Given the input 3D image $I_{i n}\\ \\in$ $\\mathbb{R}^{\\widecheck{D}\\times H\\times W^{\\sharp}}$ , the CNN backbone extracts feature maps at multiple scales, represented by $\\mathbf{x}^{0}\\,=$ $\\left(\\mathbf{x}^{0,1},\\mathbf{x}^{0,2},\\ldots,\\mathbf{x}^{0,L}\\right)$ , where $L$ is the total number of feature scales. The feature at level $i$ , $\\mathbf{\\dot{x}}^{0,i}$ is a tensor with dimensions $d_{i}\\times h_{i}\\times w_{i}\\times c_{i}$ , where $d_{i},h_{i},w_{i}$ and $c_{i}$ represent the depth, height, width, and number of channels of the feature maps at level $i$ , respectively. It is worth mentioning that the spatial resolution of the feature maps decreases as the level increases, while the number of channels increases up to a maximum of 320. To fully exploit the hierarchical features extracted by the CNN backbone, these features are aggregated across multiple stages, as depicted in Figure 2 (Dynamic Sparse Feature Fusion section). At stage $j$ ( $0<j\\leq L-1)$ ), the features at level $i$ are obtained by fusing the adjacent features from the previous stage along three directions: ", "page_idx": 2}, {"type": "text", "text": "1. \u201cDownward flow\u201d (in yellow): The high-resolution feature $\\mathbf{x}^{j-1,i-1}$ , which provides richer visual details, is passed downward; 2. \u201cUpward flow\u201d (in blue): The low-resolution feature $\\mathbf{x}^{j-1,i+1}$ , which captures more global context, is passed upward; 3. \u201cForward flow\u201d (in red): The features $x^{j-1,i}$ , which maintain their spatial resolution, are passed forward for further information integration. The fused cross-scale feature maps at the $i$ -th level of the $j$ -th stage can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}^{j,i}=\\left\\{\\begin{array}{l l}{\\mathcal{F}^{j,i}([\\mathbf{x}^{j-1,1},\\mathcal{U}(\\mathbf{x}^{j-1,2})]),}&{i=1;}\\\\ {\\mathcal{F}^{j,i}([\\mathcal{D}(\\mathbf{x}^{j-1,i-1}),\\mathbf{x}^{j-1,i},\\mathcal{U}(\\mathbf{x}^{j-1,i+1})]),}&{o t h e r s,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{F}^{j,i}(.)$ is a fusion operation, consisting of a convolution operation followed by Instance Normalization (IN) [Ulyanov et al., 2016] and a Leaky Rectified Linear Unit (LeakyReLU) [Maas et al., 2013]. $\\mathcal{U}(.)$ and $\\dot{\\mathcal{D}}(.)$ denote the up-sampling and down-sampling, respectively. [.] denotes the concatenation operation. In the convolution operation, the input feature maps, which have a channel feature ma number of Cij,ni (Cij,ni i $C_{i n}^{j,i}=c_{i-1}+c_{i}+c_{i+1}$ $C_{o u t}^{j,i}$ $c_{i}+c_{i+1}\\,$ ${\\bf\\nabla}C_{o u t}^{j,i}=c_{i})$ )r.e  fUunsleikd ea tnhde $\\mathrm{UNet++}$ d mtoo dperlo, dwuhciec oh uotpnluyt considers bottom-up information flows for image segmentation, our proposed E2ENet architecture incorporates both bottom-up and top-down information flows. This allows E2ENet to make use of both high-level contextual information and fine-grained details in order to produce more accurate segmentation maps (experimental results can be found in Table 2). ", "page_idx": 2}, {"type": "text", "text": "2.2 Dynamic Sparse Feature Fusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Such multi-stage cross-level feature propagation provides a more comprehensive understanding of the images but can also introduce redundant information, necessitating careful handling in feature fusion to ensure efficient interaction between scales or stages. Our proposed Dynamic Sparse Feature Fusion (DSFF) mechanism addresses the issue of multi-scale feature fusion in an intuitive and effective way. It optimizes the process by enabling selective and adaptive use of features from different levels during training. This results in a more efficient feature fusion process with lower computational and memory overhead. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The DSFF mechanism is applied in each fusion operation, allowing the fusion operation $\\mathcal{F}^{j,i}(\\cdot)$ to select the informative feature maps from the input fused features. The feature map selection process is controlled by a binary mask Mj,i(\u00b7) \u2208{0, 1}Cij,ni \u00d7Coj,uit, which are trained to filter out $S$ unnecessary feature map connections by zeroing out the corresponding kernels in $\\mathcal{F}^{j,i}(\\cdot)$ . With the DSFF mechanism, the output of the fusion operation is then computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{c_{o u t},:,:,:}^{j,i}=\\sigma\\big(\\mathcal{Z}\\big(\\sum_{c=0}^{C_{i n}^{j,i}}(\\tilde{\\mathbf{x}}_{c,:,:,:}^{j,i}*(\\mathbf{M}_{c,c_{o u t}}^{j,i}.\\theta_{c,c_{o u t},:,:,:}^{j,i})\\big)\\big)\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\mathbf{x}}_{c,:,:}^{j,i}$ is the input fused feature map at the $c$ -th channel. \u03b8cj,,icout,:,:,: is the kernel (feature map ca ocnonnevcotiloutni,o ans  oinp eFriagtiuorne , )  itsh taht ec pornondeuctcst  tohfe $c$ s-tcha lianrp (uit. ef.e, aton $c_{o u t}$ m-taht roixu t(pi.uet. ,f $^*$ is $\\cdot$ $\\mathbf{M}_{c,c_{o u t}}^{j,i}\\bar{)}$ $\\hat{\\theta}_{c,c_{o u t},:,:,:}^{j,i})^{2}$ ", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{M}_{c,c_{o u t}}^{j,i}$ denotes the existence or absence of a connection between the $c$ -th input feature map and the $c_{o u t}$ -th output feature map. $\\bar{\\mathcal{Z}}(\\bar{.})$ and $\\sigma(.)$ denote the Instance Normalization and LeakyReLU, respectively. Thus, the core of DSFF mechanism is the learning of binary masks during the training. At initialization, each binary mask is initialized randomly, with the number of non-zero entries $\\lVert\\mathbf{M}^{j,i}\\rVert_{0}$ equals $(1\\,-\\,S)\\,\\times\\,C_{i n}^{j,i}\\,\\times\\,C_{o u t}^{j,i}$ . Here, $S$ $(0<S<1)$ ) is a hyperparameter called the feature sparsity level, which determines the percentage of feature map connections that are inactivated. The activated connections can be updated throughout the course of training, while the feature sparsity level $S$ remains constant. This enables efficiency in both testing and training, as the exploitation of connections remains sparse throughout the training process. ", "page_idx": 3}, {"type": "text", "text": "Every $\\Delta T$ training epoch, the activated connections with lower importance are removed, while the same number of deactivated connections are randomly reactivated, as shown in Figure 3. The importance of activated connection is determined by the $L_{1}$ norm of the corresponding kernel. That is, for $\\mathbf{M}^{j,i}$ , the importance score of connection between $c_{i n}$ -th input feature and $c_{o u t}$ -th output is \u03b8cj,iin,cout,:,:,:\u22251. Our intuition is simple: if a feature map connection is more important (i.e., has a larger effect on output accuracy) than others, the $L_{1}$ norm of the corresponding kernel should be larger. However, the importance score is suboptimal during training, as the kernels are gradually optimized. Randomly reactivating previous \u201cobsolete\u201d connections with re-initialization can avoid unrecoverable feature map abandonment and thoroughly explore the representative feature maps that contribute to the final performance. ", "page_idx": 3}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/221f6ec479b62f5236d1c2554ecae09933dd5cdf84319173e76eee1ab277f44c.jpg", "img_caption": ["Figure 3: Illustration of our Dynamic Sparse Feature Fusion (DSFF) mechanism. The fusion operation starts from sparse feature connections and allows the connectivity to be evolved after training for $\\Delta T$ epochs. During each evolution stage, a fraction of kernels with smaller $L_{1}$ norms will be zeroed out (red dotted line), while the same fraction of other inactivated connections will be reactivated randomly, keeping the feature sparsity $S$ constant during training (blue solid line). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The DSFF mechanism allows for the exploration and exploitation of multi-scaled sparse features through a sparse-to-sparse training method. Our method selects features at multiple levels, as opposed to relying solely on feature selection at the input layer [Sokar et al., 2022, Atashgahi et al., 2022]. Additionally, it is a plastic approach to fuse features that can adapt to changing conditions during training, as opposed to static methods that rely on one-shot feature selection, as shown in the comparison of dynamic and static sparsity in Table 2 of [Mostafa and Wang, 2019]. More details of the training process are elaborated in the Algorithm 1 in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2.3 Restricted Depth-Shift in 3D Convolution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In 3D medical image segmentation, the 2D-based methods (such as the 2D nnUNet [Isensee et al., 2021]), which apply the 2D convolution to each slice of the 3D image, are computationally efficient but can not fully capture the relationships between slices. To overcome this limitation, we take inspiration from the temporal-shift [Lin et al., 2019] and 3D-shift [Fan et al., 2020] in efficient video action recognition, and the axial-shift [Lian et al., 2021] in efficient MLP architecture for vision. Our proposed E2ENet incorporates a depth-shift strategy in 3D convolution operations, which facilitates inter-slice information exchange and captures 3D spatial relationships while retaining the simplicity and computational efficiency of 2D-based method. Temporal-shift [Lin et al., 2019] requires selecting a Shift Proportion (the proportion of channels to conduct temporal shift). Axial-shift [Lian et al., 2021] and 3D-Shift optimize the learnable 3D spatiotemporal shift. We have made refinements to the channel shifting technique by shifting along the depth dimension and incorporating constraints on the shift size. This adaptation is thoughtfully designed to align with the distinctive needs of sparse models employed in medical image segmentation. ", "page_idx": 4}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/3ec1d423a8d0872fb8c4f921de3730515bb59e750eaaaf9b0ead1065335151dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: Illustration of restricted depth-shift in 3D Convolution of our E2ENet. The input features (left) are firstly split into 3 parts along the channel dimension, and then shifted by $\\{-1,0,1\\}$ units along the depth dimension respectively (middle). After that, 3D CNNs with kernel size $1\\!\\times\\!3\\!\\times\\!3$ are performed on the feature maps (middle) to generate the output features (right). ", "page_idx": 4}, {"type": "text", "text": "In our method, we employ a simple depth shift technique by shifting all channels while constraining the shift size to be either $+1$ , 0 or $^{-1}$ , as shown in Figure 4. This choice is motivated using dynamic sparse feature fusion, where the feature maps contain sparse information. If the shift magnitude is too large, it can result in an insufficient representation of channels or an excessive representation of depth information, which can have a negative impact on the effectiveness of the shift operation (experimental results can be found in Table 3). ", "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we compare the performance of our E2ENet model to baseline methods and report results in terms of both segmentation quality and efficiency. In addition, we will perform ablation studies to investigate the behavior of each component in the E2ENet model. To further analyze the performance of our model, we will examine its generalizability and the effect of model capacity, and present qualitative results by visualizing the predicted segmentations on sample images. We will also visualize the feature fusion ratios to gain insights into which features play an important role in the segmentation process. The description of the dataset and experimental setup can be found in Appendix A.4 and A.5. ", "page_idx": 4}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/40fd0926e8d0ebe87da66442c394716eeeebad6eb12c629d9e82e6b0efe2f8d8.jpg", "table_caption": ["Table 1: Quantitative comparisons of segmentation performance on the validation set of AMOS-CT dataset. \u2217denotes the results with postprocessing. "], "table_footnote": ["The inference FLOPs are calculated based on the patch sizes of $1\\times$ $128\\times128\\times128$ without considering postprocessing cost. "], "page_idx": 4}, {"type": "text", "text": "3.1 Comparison with SOTA methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "AMOS-CT Challenge. To comprehensively validate our method, we compare it to several stateof-the-art CNN-based models (e.g. nnUNet [Isensee et al., 2021], and Vnet [Milletari et al., 2016]) and transformer-based models (e.g. CoTr [Xie et al., 2021], nnFormer [Zhou et al., 2021], UNETR [Hatamizadeh et al., 2022], and Swin UNETR [Tang et al., 2022]). We record the mDice (class-wise Dice can be found in Table 10 of Appendix), Params, inference FLOPs, and PT score 3 on the validation set 4 of the AMOS-CT challenge in Table 1. E2ENet with a feature sparsity (s) of 0.8 achieves comparable performance, with a mDice of $90.3\\%$ , while being $3\\times$ smaller in model size and requiring less computational cost during the inference phase compared to the top-performing lightweight model, nnUNet. As the feature sparsity of E2ENet increases, the number of model parameters and inference FLOPs can be further reduced without significantly compromising segmentation performance. This indicates that there is potential to trade-off performance and efficiency by adjusting the feature sparsity of E2ENet. We also report the results of mean normalized surface dice (mNSD), the official segmentation metric for the AMOS challenge, to provide supplementary information on boundary segmentation quality. ", "page_idx": 5}, {"type": "text", "text": "BraTS Challenge in MSD. E2ENet demonstrates superior performance in terms of mDice compared to other state-of-the-art methods (DiNTS [He et al., 2021], $\\mathrm{UNet++}$ [Zhou et al., 2018] and nnUNet [Isensee et al., 2021]). Additionally, it is a resourceefficient network that is competitive with the baselines, as evidenced by its small model size and low inference FLOPs. Specifically, E2ENet model a feature sparsity level of $90\\%$ has only $7.63\\,\\mathrm{M}$ parameters, which is significantly smaller than other models yet still outperforms them in terms of ", "page_idx": 5}, {"type": "text", "text": "Table 2: 5-fold cross-validation of segmentation performance on the BraTS Challenge training set in the MSD. Note: ED, ET, and NET denote edema, enhancing tumor, and nonenhancing tumor, respectively. ", "page_idx": 5}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/0a29718fbbf1bb0109085494e204762404d503749f945c0f5fc19bee2237c5dd.jpg", "table_caption": [], "table_footnote": ["1 The inference FLOPs are calculated based on patch sizes of $4\\times128\\times128\\times128$ . The number of parameters and inference FLOPs for DiNTS are not reported because calculating them is time-consuming. This is due to the fact that the architecture for DiNTS is not readily available for the dataset and must be found using Neural Architecture Search (NAS). "], "page_idx": 5}, {"type": "text", "text": "mDice. The results on the BraTS dataset, which are MRI images, further validate the effectiveness and efficiency of E2ENet across both CT and MRI medical image analysis. ", "page_idx": 5}, {"type": "text", "text": "Due to the limited space and the similarity between the BTCV and AMOS-CT challenges, both of which involve multi-organ segmentation in CT images, the results of the BTCV challenge are provided in Appendix A.8.2. ", "page_idx": 5}, {"type": "text", "text": "3.2 Ablation Studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we investigate the impact of two factors on the performance of E2ENet: (i) the DSFF mechanism and (ii) restricted depth-shift in 3D convolution. Specifically, we consider the following scenarios: (#1) w/ DSFF: DSFF mechanism is used to dynamically activate feature map, otherwise, all feature maps are activated during training; (#2) w/ shift: restricted depth/height/width shifting is applied on feature maps before the convolution operation, otherwise, the convolution is performed directly as a standard 3D convolution without shifting operation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Table 3: Ablation study on the effects of the DSFF mechanism and restricted depth-shift in 3D convolution on the validation set of the AMOS-CT Challenge. ", "page_idx": 5}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/7ccc17e4b8c53716ea9673b7205f2dbd41fc61d2f3c58059dc61269e3d4501ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Effect of DSFF Mechanism. Table 3 (rows 1 and 3) demonstrates that the E2ENet with the DSFF mechanism, achieved comparable performance while using three times fewer parameters and inference FLOPs. Table 4 shows that removing the DSFF mechanism caused the mDice score to drop from $74.5\\;\\%$ (row 4) to $74.1\\;\\%$ (row 2) on the BraTS challenge, while also increasing the number of parameters by more than $2\\times$ and the inference FLOPs by nearly $3\\times$ . These ablation study results highlight that dynamic sparse feature fusion (DSFF) helps reduce resource costs while maintaining segmentation performance. ", "page_idx": 6}, {"type": "text", "text": "To further investigate the impact of the DSFF mechanism, we compared E2ENet with other multi-scale medical image segmentation methods on the AMOS-CT validation dataset, as shown in Table 5. These methods include DeepLabv3 [Chen et al., 2017], CoTr [Xie et al., 2021], and MedFormer [Gao et al., 2022]. DeepLabv3 uses atrous convolution to capture multi-scale context, CoTr integrates multi-scale features using attention, and MedFormer employs all-to-all attention for comprehensive multi-scale fusion, addressing both semantic and spatial aspects. ", "page_idx": 6}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/1cddbaf70bb24855fe7af1d5d65e707df91ad5763abee0ba6cca2ff54239394c.jpg", "table_caption": ["Table 4: Ablation study on the effects of the DSFF mechanism and restricted depth-shift in 3D convolution, evaluated through 5-fold cross-validation of segmentation performance on the BraTS Challenge training set in the MSD. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We observed that with a sparsity ratio of 0.9, E2ENet outperforms DeepLabv3 in terms of mDice while significantly reducing computational and memory costs by more than $3\\times$ and nearly $10\\times$ , respectively. Additionally, E2ENet with a sparsity ratio of 0.8 matches MedFormer\u2019s performance while significantly reducing computational and memory costs by $3\\times$ and $4\\times$ , respectively. This demonstrates the benefits of multi-scale feature aggregation for medical image segmentation, with E2ENet\u2019s DSFF mechanism being much more efficient than other multi-scale methods. ", "page_idx": 6}, {"type": "text", "text": "Moreover, as shown in Table 2, E2ENet with the DSFF mechanism outperforms other feature fusion architectures, such as $\\mathrm{UNet++}$ and DiNTS, on the BraTS challenge. Overall, the results from the BraTS and AMOS-CT challenges demonstrate that the DSFF mechanism provides an effective and efficient approach for feature fusion. ", "page_idx": 6}, {"type": "text", "text": "Finally, we also studied the impact of topology update frequency $(\\Delta T)$ in the DSFF mechanism and observed that our algorithm is not sensitive to the hyperparameter $\\Delta T$ . ", "page_idx": 6}, {"type": "text", "text": "Effect of Restricted Depth-Shift in 3D Convolution. We evaluated the effectiveness of our restricted shift strategy by comparing the performance of E2ENet and its variants. Without restricted depth-shift, the mDice decreased from $74.5\\%$ (row 4) to $73.9\\%$ (row 3) for the BraTS challenge as shown in Table 4, and decreased from $90.1\\%$ (row 4) to $88.2\\%$ (row 3) for AMOSCT (as shown in Table 3). In Table 3, when comparing E2ENet with kernel sizes of $3\\mathrm{x}3\\mathrm{x}3$ ", "page_idx": 6}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/5526de461d5679882f5626e307130a02cff434793f0b04faee8e9f5cb204417c.jpg", "table_caption": ["Table 5: Comparison with other multi-scale medical image segmentation methods on the AMOSCT validation dataset. The mDice score for MedFormer is sourced from [Gao et al., 2022]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "(rows 8 and 9) to E2ENet with kernel sizes of $1\\mathrm{x}3\\mathrm{x}3$ combined with a depth shift (rows 1 and 3), their segmentation accuracy remains the same, whether DSFF is used or not. In Table 4, we find that E2ENet with kernel sizes of 1x3x3 combined with a depth shift (rows 2 and 4) even improves segmentation accuracy, whether without DSFF or with DSFF, when compared to kernel sizes of 3x3x3 (rows 5 and 6). This further demonstrates that our proposed efficient Restricted Depth-Shift 3D Convolutional layer, which utilizes a $1\\mathrm{x}3\\mathrm{x}3$ kernel with restricted depth shift, is equivalent to a $3\\mathrm{x}3\\mathrm{x}3$ kernel in terms of segmentation accuracy. Moreover, it offers significant savings in computational and memory resources. This demonstrates that the use of a 1x3x3 kernel with restricted depth shift is functionally equivalent to a 3x3x3 kernel in terms of segmentation accuracy, all the while offering savings in computational and memory resources. ", "page_idx": 6}, {"type": "text", "text": "Rows 9 and 10 of Table 4 demonstrate that when restricted shift is applied to the height or width dimensions, the model\u2019s performance decreases compared to E2ENet (4th row) with restricted shift on the depth dimension, called as restricted depth-shift. We also observed that the performance of 3D convolution with kernel sizes of $1\\times3\\times3$ (row 3), $3\\times1\\times3$ (row 7), and $3\\times3\\times1$ (row 8) decreased significantly without restricted shift compared to their counterparts with restricted shift on the corresponding dimensions (rows 4, 9, and 10, respectively). These phenomena demonstrate the effectiveness of restricted shift, especially restricted depth-shift, for medical image segmentation. ", "page_idx": 7}, {"type": "text", "text": "Furthermore, we evaluated the impact of different shift sizes on the model\u2019s performance. As shown in Table 3, we compared the performance of E2ENet with shift sizes of $(-1,0,1)$ to $(-2,0,2)$ , $(-3,0,3)$ and $(-7,0,7)$ , and observed a decrease in performance from $90.1\\%$ to $89.8\\%$ , $8\\bar{9}.7\\%$ and $87.6\\%$ , respectively, as the shift size increased. Increasing the shift size means considering more depth-wise information at the expense of channel-wise information, leading to an insufficient repre", "page_idx": 7}, {"type": "text", "text": "Table 6: Evaluating the generalizability of E2ENet on the AMOS-MRI dataset. ${\\mathrm{CT}}\\to{\\mathrm{MRI}}$ : pretrained on the CT dataset and fine-tuned on the MRI dataset; MRI: trained solely on the MRI dataset; $\\scriptstyle\\mathrm{CT}+\\mathrm{MRI}$ : trained on both the CT and MRI datasets. Results for all models, except E2ENet and nnUNet $\\left(\\mathbf{CT}\\right)\\mathbf{MRI}$ , are sourced from the AMOS website. ", "page_idx": 7}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/c362515601ea6fc72a59e5614a4cb7638590d0ac38af20429daf65f06ddd7575.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "sentation of channels, as discussed in Section 2.3. Additionally, a large shift size (equivalent to a large kernel size) leads to a loss of local spatial relationships, which are crucial for segmentation. This results in a blurring effect that reduces the precision of boundary alignment, particularly affecting metrics like mNSD, which rely heavily on accurate boundary information. Therefore, we use a shift size of $(-1,0,1)$ in our restricted depth-shift strategy as the default setting in our experiments. Finally, we plot a critical distance diagram to show the statistical significance of our modules in Appendix A.8.3. ", "page_idx": 7}, {"type": "text", "text": "3.3 Generalizability Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the generalizability of the resulting architectures, we compared our model, pre-trained on AMOS-CT and fine-tuned on AMOS-MRI, with nnUNet, which was pre-trained on AMOS-CT and fine-tuned on AMOSMRI, and both models are fine-tuned for 250 epochs. It is worth noting that the topology (weight connections) is determined through AMOS-CT pretraining and remains fixed during the fine-tuning phase. From Table 6, we discovered that the E2ENet architecture, initially designed for AMOS-CT, can be effectively applied to AMOSMRI as well. This adaptation resulted in improved performance compared to nnUNet and other baselines. nnUNet ", "page_idx": 7}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/c24bf351a6a5da4442ddff7e7e8ff4ddc46183da7484c041f594e4df6b75c875.jpg", "table_caption": ["Table 7: Quantitative comparisons of segmentation performance on AMOS-CT test dataset. \u2021and \u2020denote the results with and without postprocessing that are reproduced by us. \u2217 indicates the results with postprocessing. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and these baselines were either transferred from CT, trained solely on MRI data, or jointly on MRI and CT datasets. ", "page_idx": 7}, {"type": "text", "text": "In the AMOS-CT dataset, there is a domain shift between the training and test datasets due to variations in the image acquisition scanners [Ji et al., 2022]. Thus, we also assess the generalizability of our approach by evaluating its performance on the AMOS-CT test dataset. As demonstrated in Table 7, E2ENet exhibits comparable performance even in the presence of domain shift when compared to nnUNet. This is achieved while maintaining lower computational and memory costs. ", "page_idx": 7}, {"type": "text", "text": "3.4 Model Capacity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To ensure a fair comparison, we scale down nnUNet by reducing the number of channels at level 1 from 32 to 27 and decreasing the total number of feature scales from 5 to 4, resulting in a modified version referred to as nnUNet (-). Additionally, we scale up E2ENet by increasing the number of channels at level 1 from 48 to 58, creating a modified version referred to as E2ENet $(+)$ . We evaluated the performance of these models, and the results can be found in Table 8. It is worth noting that scaling down nnUNet resulted in decreased performance in terms of mDice and mNSD, while scaling up E2ENet led to an increase in mDice and comparable performance in terms of mNSD. This indicates that the comparable performance of the memory and computation-efficient E2ENet is not attributed to the dataset\u2019s requirement for a small number of parameters and computations. ", "page_idx": 8}, {"type": "text", "text": "3.5 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we compare the proposed E2ENet and nnUNet qualitatively on three challenges. To make the comparison easier, we highlight detailed segmentation results with red dashed boxes. ", "page_idx": 8}, {"type": "text", "text": "Figure 5 presents a qualitative comparison of our proposed E2ENet with nnUNet on the AMOSCT challenge. As a widely used baseline, nnUNet has been evaluated on multiple medical image segmentation challenges. Our results ", "page_idx": 8}, {"type": "text", "text": "Table 8: A comparison under a similar FLOPs budget on the AMOS-CT challenge in two cases: when nnUNet is scaled down and when E2ENet is scaled up. ", "page_idx": 8}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/5a326490cf8b2c1274d976cb9ccf10935718dd817fe7edcd8d9c890d66b76489.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "demonstrate that, on certain samples, E2ENet can improve segmentation quality and overcome some of the challenges faced on the AMOS-CT challenge. For example, as shown in the first column, E2ENet more accurately distinguishes between the stomach and the esophagus, which is challenging due to their close proximity. In the second column, E2ENet better differentiates the duodenum from the background, while in the third column, E2ENet accurately identifies the precise boundaries of the liver, a structure that is prone to over-segmentation. These examples demonstrate the potential of our proposed method to improve the accuracy of medical image segmentation to some extent, especially in challenging cases such as distinguishing between closely located organs or accurately segmenting complex shapes. The Qualitative Results of BTCV and BraTS Challenge can be found in Appendix A.9.1. ", "page_idx": 8}, {"type": "text", "text": "3.6 Feature Fusion Visualization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore how the DSFF mechanism fuses features from three directions (upward, forward, and downward) during training. To shed light on this mechanism, we specifically analyze the proportions 5 of feature map connections from different directions to a specific fused feature node, considering a feature sparsity level of 0.8. After training, we observed in Figure 6 (d) that the DSFF module learned to assign greater importance to features from the \"forward\" directions for most fused feature nodes. For example, in the first feature level, the flow and processing of features can be observed as if they were passing through a fully convolutional neural network (FCN), which preserves the spatial dimensions of the input image. At the second feature level, the original image is downsampled by a factor of $1/2$ and flows through another FCN. Therefore, from this perspective, E2ENet can take advantage of FCN to effectively incorporate and preserve multi-scale information. ", "page_idx": 8}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/44ff24ba6ae2943246c27089bbdfd519d4aeed1f1fab7956d1e8c4235570a5e5.jpg", "img_caption": ["Figure 5: Qualitative comparison of E2ENet and nnUNet on the AMOS-CT challenges. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Simultaneously, the complementary feature flows from the \u2019upward\u2019 and \u2019downward\u2019 directions provide richer information. At the later fusion step of the lower feature levels, the \u2019upward\u2019 information becomes more dominant than the \u2019downward\u2019 information. This prioritization of upward feature flow is similar to the design of the decoders in UNet and $\\mathrm{UNet++}$ . While E2ENet alleviates the semantic gap more effectively, the proportion of feature map connections in the \u2019downward\u2019 direction always has a certain ratio. This also allows the network to better preserve low-level information and integrate it with high-level information, leading to improved performance in capturing fine details. Interestingly, this trend becomes increasingly apparent during training, as illustrated in Figure 6 (a), (c) and (d). ", "page_idx": 9}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/ab7c241cc37355d1419c29dbd9b4c03ea5514f7220a62d6cd8c6fe19f35e42ec.jpg", "img_caption": ["Figure 6: The proportions of feature connections during training with the DSFF mechanism at a feature sparsity level of 0.8 on the AMOS-CT challenge. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, our aim is to address the challenge of designing a 3D medical image segmentation method that is both accurate and efficient. By proposing a dynamic sparse feature fusion mechanism and incorporating restricted depth-shift in 3D convolution, our E2ENet improves performance on 3D medical image segmentation tasks while significantly reducing computational and memory overhead. The dynamic sparse feature fusion mechanism demonstrates its ability to adaptively learn the importance of each feature map, zeroing out the less important ones. This results in a more efficient feature representation without compromising performance. Additionally, the experiments demonstrate that restricted depth-shift in 3D convolution enables the model to capture spatial information more effectively and efficiently. Extensive experiments on three benchmarks demonstrate that E2ENet consistently achieves a superior trade-off between accuracy and efficiency compared to previous state-of-the-art baselines. While E2ENet provides a promising solution for balancing accuracy and computational cost, future work could explore the potential of a learnable shift offset, which may further improve performance. Additionally, as plug-and-play components, the DSFF mechanism and restricted depth-shift could be interesting to apply to other 3D segmentation models to explore their potential in the future. Furthermore, future advancements in hardware support for sparse neural networks could fully unlock the potential of sparse training methods. ", "page_idx": 9}, {"type": "text", "text": "5 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Qiao Xiao is supported by the research program \u2018MegaMind - Measuring, Gathering, Mining and Integrating Data for Self-management in the Edge of the Electricity System\u2019, (partly) financed by the Dutch Research Council (NWO) through the Perspectief program under number P19-25. Elena Mocanu is partly supported by the Modular Integrated Sustainable Datacenter (MISD) project funded by the Dutch Ministry of Economic Affairs and Climate under the European Important Projects of Common European Interest - Cloud Infrastructure and Services (IPCEI-CIS) program for 2024-2029. This research used the Dutch national e-infrastructure with the support of the SURF Cooperative, using grant no. EINF-7499. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Michela Antonelli, Annika Reinke, and Annette Kopp-Schneider Bennett A. Landman Geert Litjens Bjoern Menze Olaf Ronneberger Ronald M. Summers Bram van Ginneken Michel Bilello Patrick Bilic Patrick F. Christ Richard K. G. Do Marc J. Gollub Stephan H. Heckers Henkjan Huisman William R. Jarnagin Maureen K. McHugo Sandy Napel Jennifer S. Golia Pernicka Kawal Rhode Catalina Tobon-Gomez Eugene Vorontsov James A. Meakin Sebastien Ourselin Manuel Wiesenfarth Pablo Arbel\u00e1ez Byeonguk Bae Sihong Chen Laura Daza Jianjiang Feng Baochun He Fabian Isensee Yuanfeng Ji Fucang Jia Ildoo Kim Klaus Maier-Hein Dorit Merhof Akshay Pai Beomhee Park Mathias Perslev Ramin Rezaiifar Oliver Rippel Ignacio Sarasua Wei Shen Jaemin Son Christian Wachinger Liansheng Wang Yan Wang Yingda Xia Daguang Xu Zhanwei Xu Yefeng Zheng Amber L. Simpson Lena Maier-Hein M. Jorge Cardoso Spyridon Bakas, Keyvan Farahani. The medical segmentation decathlon. Nature Communications, 2022. ", "page_idx": 10}, {"type": "text", "text": "Zahra Atashgahi, Ghada Sokar, Tim van der Lee, Elena Mocanu, Decebal Constantin Mocanu, Raymond Veldhuis, and Mykola Pechenizkiy. Quick and robust feature selection: the strength of energy-efficient sparse training for autoencoders. Machine Learning, 111(1):377\u2013414, 2022.   \nHu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-Unet: Unet-like pure transformer for medical image segmentation. arXiv:2105.05537, 2021.   \nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017.   \nYinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: Attention over convolution kernels. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 11027\u201311036. Computer Vision Foundation / IEEE, 2020.   \n\u00d6zg\u00fcn \u00c7i\u00e7ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-Net: learning dense volumetric segmentation from sparse annotation. In International conference on medical image computing and computer-assisted intervention, pages 424\u2013432. Springer, 2016.   \nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. CoAtNet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021.   \nSt\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. ConViT: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, pages 2286\u20132296. PMLR, 2021.   \nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997\u20132017, 2019.   \nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943\u20132952. PMLR, 2020.   \nLinxi Fan, Shyamal Buch, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and Li Fei-Fei. RubiksNet: Learnable 3D-shift for efficient video action recognition. In European Conference on Computer Vision, pages 505\u2013521. Springer, 2020.   \nY. Gao, M. Zhou, D. Liu, and D. Metaxas. A data-scalable transformer for medical image segmentation: Architecture, model efficiency, and benchmark. arXiv preprint arXiv:2203.00131, 2022.   \nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \nAli Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. UNETR: Transformers for 3D medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 574\u2013584, 2022.   \nYufan He, Dong Yang, Holger Roth, Can Zhao, and Daguang Xu. DiNTS: Differentiable neural network topology search for 3D medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5841\u20135850, 2021.   \nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(241): 1\u2013124, 2021.   \nXiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, Yulun Zhang, and Haoqian Wang. Pseudo 3D auto-correlation network for real image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16175\u201316184, 2021.   \nHuimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, and Jian Wu. UNet $3+$ : A full-scale connected UNet for medical image segmentation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1055\u20131059. IEEE, 2020.   \nNabil Ibtehaz and M Sohel Rahman. MultiResUNet: Rethinking the U-Net architecture for multimodal biomedical image segmentation. Neural Networks, 121:74\u201387, 2020.   \nFabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnU-Net: a selfconfiguring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203\u2013211, 2021.   \nFabian Isensee, Constantin Ulrich, Tassilo Wald, and Klaus H Maier-Hein. Extending nnU-Net is all you need. arXiv:2208.10791, 2022.   \nSiddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-KAST: Top-K always sparse training. Advances in Neural Information Processing Systems, 33:20744\u201320754, 2020.   \nDebesh Jha, Pia H Smedsrud, Michael A Riegler, Dag Johansen, Thomas De Lange, P\u00e5l Halvorsen, and H\u00e5vard D Johansen. ResUNet++: An advanced architecture for medical image segmentation. In 2019 IEEE International Symposium on Multimedia (ISM), pages 225\u20132255. IEEE, 2019.   \nYuanfeng Ji, Haotian Bai, Chongjian GE, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhanng, Wanling Ma, Xiang Wan, and Ping Luo. AMOS: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \nDavood Karimi, Serge Didenko Vasylechko, and Ali Gholipour. Convolution-free medical image segmentation using transformers. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 78\u201388. Springer, 2021.   \nB Landman, Z Xu, J Igelsias, M Styner, T Langerak, and A Klein. Miccai multi-atlas labeling beyond the cranial vault\u2013workshop and challenge. In Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault\u2014Workshop Challenge, 2015.   \nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2018.   \nChen Li, Yusong Tan, Wei Chen, Xin Luo, Yuanming Gao, Xiaogang Jia, and Zhiying Wang. Attention Unet++: A nested attention-aware U-Net for liver ct image segmentation. In 2020 IEEE International Conference on Image Processing (ICIP), pages 345\u2013349. IEEE, 2020.   \nDongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. AS-MLP: An axial shifted mlp architecture for vision. In International Conference on Learning Representations, 2021.   \nJi Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.   \nShiwei Liu and Zhangyang Wang. Ten lessons we have learned in the new\" sparseland\": A short handbook for sparse neural network researchers. arXiv preprint arXiv:2302.02596, 2023.   \nShiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems, 34:9908\u20139922, 2021a.   \nShiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei, and Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware. Neural Computing and Applications, 33(7):2589\u20132604, 2021b.   \nShiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need dense overparameterization? in-time over-parameterization in sparse training. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 6989\u20137000. PMLR, 2021c.   \nShiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More ConvNets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv:2207.03620, 2022.   \nAndrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In Proceedings of the 30th International Conference on Machine Learning, number 1, page 3. Atlanta, Georgia, USA, 2013.   \nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 Fourth International Conference on 3D vision (3DV), pages 565\u2013571. IEEE, 2016.   \nDecebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann machines. Machine Learning, 104(2):243\u2013270, 2016.   \nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9(1):1\u201312, 2018.   \nHesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Machine Learning, pages 4646\u20134655. PMLR, 2019.   \nOzan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, Ben Glocker, and Daniel Rueckert. Attention U-Net: Learning where to look for the pancreas. In Medical Imaging with Deep Learning, 2018.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-assisted Intervention, pages 234\u2013241. Springer, 2015.   \nJiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491, 2024.   \nAmber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv:1902.09063, 2019.   \nGhada Sokar, Zahra Atashgahi, Mykola Pechenizkiy, and Decebal Constantin Mocanu. Where to pay attention in sparse training for feature selection? In Advances in Neural Information Processing Systems, 2022.   \nZhuo Su, Linpu Fang, Wenxiong Kang, Dewen Hu, Matti Pietik\u00e4inen, and Li Liu. Dynamic group convolution for accelerating convolutional neural networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and JanMichael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI, 2020.   \nMingxing Tan, Ruoming Pang, and Quoc V Le. EfficientDet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10781\u201310790, 2020.   \nHidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems, 33:6377\u20136389, 2020.   \nYucheng Tang, Riqiang Gao, Ho Hin Lee, Shizhong Han, Yunqiang Chen, Dashan Gao, Vishwesh Nath, Camilo Bermudez, Michael R Savona, Richard G Abramson, et al. High-resolution 3D abdominal segmentation with random patch network fusion. Medical Image Analysis, 69:101894, 2021.   \nYucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. Self-supervised pre-training of swin transformers for 3D medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20730\u201320740, 2022.   \nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.   \nJeya Maria Jose Valanarasu and Vishal M Patel. UNeXt: MLP-based rapid medical image segmentation network. arXiv:2203.04967, 2022.   \nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2019.   \nHaonan Wang, Peng Cao, Jiaqi Wang, and Osmar R Zaiane. UCTransNet: rethinking the skip connections in U-Net from a channel-wise perspective with transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 2441\u20132449, 2022.   \nWenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, and Jiangyun Li. TransBTS: Multimodal brain tumor segmentation using transformer. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 109\u2013119. Springer, 2021.   \nZiyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, and Lei Li. Mamba-unet: Unet-like pure visual mamba for medical image segmentation. arXiv preprint arXiv:2402.05079, 2024.   \nYu Weng, Tianbao Zhou, Yujie Li, and Xiaoyu Qiu. NAS-Unet: Neural architecture search for medical image segmentation. IEEE Access, 7:44247\u201344257, 2019.   \nQiao Xiao, Boqian Wu, Yu Zhang, Shiwei Liu, Mykola Pechenizkiy, Elena Mocanu, and Decebal Constantin Mocanu. Dynamic sparse network for time series classification: Learning what to \u201csee\u201d. In Advances in Neural Information Processing Systems, 2022.   \nYutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. CoTr: Efficiently bridging CNN and transformer for 3D medical image segmentation. In International Conference on Medical Image Computing and Computerassisted Intervention, pages 171\u2013180. Springer, 2021.   \nZhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560, 2024.   \nQihang Yu, Dong Yang, Holger Roth, Yutong Bai, Yixiao Zhang, Alan L. Yuille, and Daguang Xu. C2FNAS: coarse-to-fine neural architecture search for 3d medical image segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 4125\u20134134. Computer Vision Foundation / IEEE, 2020.   \nGeng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. MEST: Accurate and fast memory-economic sparse training framework on the edge. Advances in Neural Information Processing Systems, 34:20838\u201320850, 2021.   \nHong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnFormer: Interleaved transformer for volumetric segmentation. arXiv:2109.03201, 2021.   \nYuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei Han, Elliot Fishman, and Alan L Yuille. Prioraware neural network for partially-supervised multi-organ segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10672\u201310681, 2019.   \nZongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. UNet+ $^+$ : A nested U-Net architecture for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pages 3\u201311. Springer, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Impact Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In an era dominated by over-parameterized models, designing resource-aware AI models is becoming increasingly important, especially for time-consuming tasks like medical segmentation. Our insights into model efficiency training have the potential to broaden the application of deep neural networks in this area. Overall, this work advances our fundamental understanding of dynamic sparse training and offers future perspectives for scalable and efficient AI models. We do not anticipate any negative societal impacts resulting from this research. ", "page_idx": 14}, {"type": "text", "text": "A.2 Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 3D Medical Image Segmentation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Convolutional neural networks (CNNs) have become the dominant architecture for 3D medical image segmentation in recent years (e.g. 3D UNet [\u00c7i\u00e7ek et al., 2016], UNet $^{++}$ [Zhou et al., 2018], $\\mathrm{UNet}3+$ [Huang et al., 2020], PaNN [Zhou et al., 2019] and nnUNet [Isensee et al., 2021]), due to their ability to capture local and weight-sharing dependencies [d\u2019Ascoli et al., 2021, Dai et al., 2021]. However, some recent methods have attempted to incorporate transformer modules into CNNs (e.g. CoTr [Xie et al., 2021], TransBTS [Wang et al., 2021]), or use pure transformer architectures (e.g. ConvIt [Karimi et al., 2021], nnFormer [Zhou et al., 2021], Swin UNet [Cao et al., 2021]), in order to capture long-range dependencies. These transformer-based approaches often require large amounts of training data, longer training times, or specialized training techniques, and can also be computationally expensive. Most recently, a novel architecture called Mamba [Gu and Dao, 2023] has shown potential for computational efficiency as a State Space model in handling long sequences and has been applied to medical image segmentation tasks [Ruan and Xiang, 2024, Xing et al., 2024, Wang et al., 2024]. However, it has led to underwhelming performance compared to state-of-the-art convolutional models. In this paper, we propose an alternative method for efficiently incorporating 3D contextual information using a restricted depth-shift strategy in 3D convolutions, and further improving performance through adaptive multi-scale feature fusion. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Feature Fusion in Medical Image Segmentation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Multi-scale feature fusion is a crucial technique in medical image segmentation that allows a model to detect objects across a range of scales, while also recovering spatial information that is lost during pooling [Wang et al., 2022, Xie et al., 2021]. However, effectively representing and processing multi-scale hierarchy features can be challenging, and simply summing them up without distinction can lead to semantic gaps and degraded performance [Wang et al., 2022, Tan et al., 2020]. To address this issue, various approaches have been proposed, including adding learnable operations to reduce the gap with residuals [Ibtehaz and Rahman, 2020], attention blocks [Oktay et al., 2018]. More recently, $\\mathrm{UNet++}$ [Zhou et al., 2018] and its variants [Li et al., 2020, Huang et al., 2020, Jha et al., 2019] have adapted the gating signal to dense nesting levels, taking into account as many feature levels as possible. NAS-UNet [Weng et al., 2019] tries to automatically search for better feature fusion topology. While these methods have achieved better performance, they can also incur significant computational and information redundancy. Dynamic convolution [Su et al., 2020, Chen et al., 2020] utilizes coefficient prediction or attention modules to dynamically aggregate convolution kernels, thereby reducing computation costs. In our paper, we propose an intuitive approach to optimizing multi-scale feature fusion, which enables selective leveraging of sparse feature representations from fine-grained to semantic levels through the proposed dynamic sparse feature fusion mechanism. ", "page_idx": 14}, {"type": "text", "text": "A.2.3 Sparse Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recently, sparse training techniques have shown the possibility of training an efficient network with sparse connections that match (or even outperform) the performance of dense counterparts with lower computational cost [Mocanu et al., 2018, Liu et al., 2021b]. Beginning with [Mocanu et al., 2016], it has been demonstrated that initializing a static sparse network without optimizing its topology during training can also yield comparable performance in certain situations [Lee et al., 2018, Tanaka et al., 2020, Wang et al., 2019]. However, Dynamic Sparse Training (DST), also known as sparse training with dynamic sparsity [Mocanu et al., 2018], offers a different approach by jointly optimizing the ", "page_idx": 14}, {"type": "text", "text": "Require: Dataset $\\mathcal{X}$ with label $\\boldsymbol{\\wp}$ ; feature sparsity $S$ ; backbone $f_{\\Theta}(.)$ ; Output Module: $f_{o u t}$ ; Total training epochs: $T$ ; evolution period: $\\Delta T$ ; connection updating number: $\\begin{array}{r}{f_{d e c a y}\\left(\\Delta T;\\alpha,T\\right)=\\frac{\\alpha}{2}\\left(1+\\cos\\left(\\frac{\\Delta T\\pi}{T}\\right)\\right)}\\end{array}$ , represents the number of updated connections during the initial topology update, which is set to $1/2$ ; Loss function: ${\\mathcal{L}}(.)$ ; fusion operation: $\\mathcal{F}^{j,i}(\\cdot)$ with convolution kernels $\\theta^{j,i}$ , where the numbers of input and output channel are $C_{i n}^{j,i},C_{o u t}^{j,i}$ .   \n1: $\\mathbf{M}^{j,i}\\gets$ random initialize masks for all levels and stages, satisfying that $\\lVert\\mathbf{M}^{j,i}\\rVert_{0}$ equals $\\left(1-\\right.$ $S)\\times C_{i n}^{j,i}\\times C_{o u t}^{j,i}$   \n2: for $t=1$ to $T$ do   \n43:: SGaemneprlaet ea  bmautlctih- $I_{t},Y_{t}\\sim\\mathcal{X},\\mathcal{Y}$ s: $\\left(\\mathbf{x}^{0,1},\\mathbf{x}^{0,2},\\ldots,\\mathbf{x}^{0,L}\\right)=f_{\\Theta}(I_{t})$   \n5: for each stage $j=1$ to $L-1$ do   \n6: for each level $i=1$ to $L-j$ do   \n7: if $i=1$ then   \n8: $\\begin{array}{r l}&{\\mathbf{x}^{j,i}=\\mathcal{F}^{j,i}([\\mathbf{x}^{j-1,1},\\mathcal{U}(\\mathbf{x}^{j-1,2})])}\\\\ &{\\mathbf{se}}\\\\ &{\\mathbf{x}^{j,i}=\\mathcal{F}^{j,i}([\\mathcal{D}(\\mathbf{x}^{j-1,i-1}),\\mathbf{x}^{j-1,i},\\mathcal{U}(\\mathbf{x}^{j-1,i+1})])}\\end{array}$   \n9: e   \n10:   \n11: end if   \n12: end for   \n13: end for   \n14: $\\begin{array}{r l r l r l r}{\\lefteqn{l_{t}^{-\\operatorname{sur}\\theta}}}&{{}=}&{4/7\\mathcal{L}(f_{o u t}\\left(\\mathbf{x}^{L-1,1}\\right),Y_{i})}&{{}+}&{{}}&{2/7\\mathcal{L}(f_{o u t}\\left(\\mathbf{x}^{L-2,2}\\right),\\mathcal{D}(Y_{i}))}&{{}+}&{{}}&{\\end{array}$   \n15: if $(t\\bmod{\\Delta T})===0$ then   \n16: for each stage $j=1$ to $L-1$ do   \n17: for each level $i=1$ to $L-j$ do   \n18: $u=(C_{i n}^{j,i}\\times C_{o u t}^{j,i})f_{d e c a y}\\left(t;\\alpha,T\\right)(1-S)$   \n19: $I S\\gets$ importance score ( $\\mathit{L}_{1}$ Norm of corresponding kernel) for activated each feature connection   \n20: $\\mathbb{I}_{a c t i v a t e}=R a n d o m K(\\mathbb{I}_{i n a c t i v a t e},u)$   \n21: $\\mathbb{I}_{i n a c t i v a t e}=A r g T o p K\\left(-I S,u\\right)$   \n22: $\\mathbf{M}^{j,i}\\gets$ Update $\\bar{\\mathbf{M}}^{j,i}$ using $\\mathbb{I}_{i n a c t i v a t e}$ and Iactivate   \n23: end for   \n24: end for   \n25: else   \n26: Training the E2ENet using SGD optimizer   \n27: end if   \n28: end for ", "page_idx": 15}, {"type": "text", "text": "sparse topology and weights during the training process starting from a sparse network [Liu et al., 2021a, 2022, Evci et al., 2020, Jayakumar et al., 2020, Mostafa and Wang, 2019, Yuan et al., 2021]. This allows the model\u2019s sparse connections to gradually evolve in a prune-and-grow scheme, leading to improved performance compared to naively training a static sparse network [Liu et al., 2021c, Xiao et al., 2022]. In contrast to prior methods that aim to find sparse networks that can match the performance of corresponding dense networks, we aim to leverage DST to adaptively fuse multi-scale features in a computationally efficient manner for 3D medical image segmentation. ", "page_idx": 15}, {"type": "text", "text": "A.3 Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.4 Datasets and Experiment Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "AMOS-CT: The Abdominal Multi-Organ Segmentation Challenge (AMOS) [Ji et al., 2022] task 1 consists of 500 computerized tomography (CT) cases, including 200 scans for training, 100 for validation, and 200 for testing. These cases have been collected from a diverse patient population and include annotations of 15 organs. The scans are from multiple centers, vendors, modalities, phases, and diseases. ", "page_idx": 15}, {"type": "text", "text": "BTCV: The Beyond the Cranial Vault (BTCV) abdomen challenge dataset 6 consists of $30\\,\\mathrm{CT}$ scan images for training and 20 for testing. These images have been annotated by interpreters under the supervision of radiologists, and include labels for 13 organs. ", "page_idx": 16}, {"type": "text", "text": "BraTS: The Brain Tumor Segmentation Challenge in the Medical Segmentation Decathlon (MSD) [Antonelli et al., 2022, Simpson et al., 2019] consists of 484 MRI images from 19 different institutions. These images contain three different tumor regions of interest (ROIs): edema (ED), non-enhancing tumor (NET) and enhancing tumor (ET). The goal of the challenge is to segment these ROIs in the images accurately. ", "page_idx": 16}, {"type": "text", "text": "A.5 Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our work, we utilized the PyTorch toolkit [Paszke et al., 2019] on an NVIDIA A100 GPU for all our experimental evaluations. We also used the nnUNet codebase [Isensee et al., 2021] to pre-process data before training our proposed E2ENet model. For the AMOS dataset, we used the nnUNet codebase as the benchmark implementation. ", "page_idx": 16}, {"type": "text", "text": "For training, we use the stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.01, which is gradually decreased through a \u201cpoly\u201d decay schedule. The optimizer is configured with a momentum of 0.99 and a weight decay of $\\mathrm{\\bar{3}\\times10^{-5}}$ . The maximum number of training epochs is 1000, with 250 iterations per epoch. For the loss function, we combine both cross-entropy loss and Dice loss as in [Isensee et al., 2021]. To improve performance, various data augmentation techniques such as random rotation, scaling, filpping, adding Gaussian noise, blurring, adjusting brightness and contrast, simulating low resolution, and Gamma transformation are used before training. ", "page_idx": 16}, {"type": "text", "text": "We employ a 5-fold cross-validation strategy on the training set for all experiments, selecting the final model from each fold and simply averaging their outputs for the final segmentation predictions. In the testing stage, we employ the sliding window strategy, where the window sizes are equal to the size of the training patches. Additionally, post-processing methods outlined in [Isensee et al., 2022] are applied for the AMOS-CT dataset during the testing phase. ", "page_idx": 16}, {"type": "text", "text": "A.6 The Architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The backbone generates a total of $L=6$ multi-scale feature levels, each with a specified number of channels: [c1, c2, c3, c4, c5, c6] $=$ [48, 96, 192, 320, 320, 320]. At each level of feature generation, there are two convolution layers with a kernel size of (1, 3, 3), followed by instance normalization and the application of leaky ReLU activation. The down-sampling ratios for each level are as follows: ((1, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)). ", "page_idx": 16}, {"type": "text", "text": "A.7 Evaluation Metrics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.7.1 Mean Dice Similarity Coefficient ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To assess the quality of the segmentation results, we use the mean Dice similarity coefficient (mDice), which is a widely used metric in medical image segmentation. The mDice is calculated as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nm D i c e=\\frac{1}{N}\\sum_{j=1}^{N}\\frac{2|\\mathbf{y}_{j}\\cdot\\hat{\\mathbf{y}}_{j}|}{(|\\mathbf{y}_{j}|+|\\hat{\\mathbf{y}}_{j}|)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $N$ is the number of classes, $\\cdot$ is the pointwise multiplication, $\\mathbf{y}_{j}$ and $\\hat{\\mathbf{y}}_{j}$ represent the ground truth and predicted masks of the $j$ -th class, respectively, which are encoded in one-hot format. $\\frac{2|\\mathbf{y}_{j}\\cdot\\hat{\\mathbf{y}}_{j}|}{(|\\mathbf{y}_{j}|+|\\hat{\\mathbf{y}}_{j}|)}$ is the Dice of $j$ -th class, which measures the overlap between the predicted and ground truth segmentation masks for that class. ", "page_idx": 16}, {"type": "text", "text": "A.7.2 Number of Parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The size of the network can be estimated by summing the number of non-zero parameters (Params), which includes the parameters of activated sparse feature connections (kernels) and parameters of the ", "page_idx": 16}, {"type": "text", "text": "backbone. The calculation is given by the following equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP a r a m s=\\|\\Theta\\|_{0}+\\sum_{j=1}^{L-1}\\sum_{i=1}^{L-j}\\sum_{c_{i n}=1}^{C_{i n}^{j,i}}\\sum_{c_{o u t}=1}^{C_{o u t}^{j,i}}\\mathbf{M}_{c_{i n},c_{o u t}}^{j,i}\\|\\theta_{c_{i n},c_{o u t}}^{j,i}\\|_{0}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\Theta$ is the parameter from backbone, $L$ is the total number of feature levels, $\\mathbf{M}^{j,i}$ is a matrix of size $C_{i n}^{j,i}\\times C_{o u t}^{j,i}$ , and $\\mathbf{M}_{c_{i n},c_{o u t}}^{j,i}$ indicates whether the kernel $\\theta_{c_{i n},c_{o u t}}^{j,i}$ connecting the $c_{i n}$ -th input and $c_{o u t}$ -th output feature map exist or not. The $L_{0}$ norm \u03b8cj,iin,cout\u22250 provides the number of non-zero entries of \u03b8cj,i $\\theta_{c_{i n},c_{o u t}}^{j,i}$ ", "page_idx": 17}, {"type": "text", "text": "A.7.3 Float Point Operations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Floating point operations (FLOPs) is a commonly used metric to compare the computational cost of a sparse model to that of a dense counterpart [Hoefler et al., 2021] 7. In our comparison, it is calculated by counting the number of multiplications and additions performed in only one forward pass of the inference process without considering postprocessing. The inference FLOPs are estimated layer by layer and depend on the sparsity level of the network. For each convolution or transposed convolution layer, the inference FLOPs is calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF L O P s_{c o n v}=(2K_{d}K_{h}K_{w}C_{i n}(1-S)+1)\\times C_{o u t}H W D,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $K_{d},K_{h}$ and $K_{w}$ are the kernel sizes in depth, height and width; $S$ is the feature sparsity level, for layers that are not part of the DSFF mechanism, $S=0$ is used; $C_{i n}$ and $C_{o u t}$ are the numbers of input feature and output feature; $H,W$ and $D$ are the height, width and depth of output features. For each fully connected layer, the inference FLOPs is calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF L O P s_{f c}=(2C_{i n}(1-S)+1)\\times C_{o u t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.7.4 Performance Trade-Off Score ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The accuracy-efficiency trade-offs could be further analyzed, from comparing resource requirements to describing holistic behaviours (including mDice, Params and inference FLOPs) for the 3D image segmentation methods. To quantify these trade-offs, we introduce the Performance Trade-Off (PT) score, which is defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP T=\\alpha_{1}\\frac{m D i c e}{m D i c e_{m a x}}+\\alpha_{2}(\\frac{P a r a m s_{m i n}}{P a r a m s}+\\frac{F L O P s_{m i n}}{F L O P s}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\alpha_{1}$ and $\\alpha_{2}$ are weighting factors, which control the trade-off between accuracy performance and resource requirements, and $m D i c e_{m a x}$ , $P a r a m s_{m i n}$ , and $F L O P s_{m i n}$ denote the highest mDice score, the smallest number of parameters, and the lowest inference FLOPs among the compared methods for a specific dataset, respectively. The term $\\frac{m D i c e}{m D i c e_{m a x}}$ measures the segmentation accuracy, while $\\begin{array}{r}{\\frac{P a r a m s_{m i n}}{P a r a m s}+\\frac{F L O P s_{m i n}}{F L O P s}}\\end{array}$ F LF OLPO sP msin measures the resource cost. ", "page_idx": 17}, {"type": "text", "text": "In most cases, we consider both segmentation accuracy and resource cost to be equally important, thus we set $\\alpha_{1}=1$ and $\\alpha_{2}=1/2$ in the following experiments. However, we also explore the impact of different choices of $\\alpha_{1}$ and $\\alpha_{2}$ , as detailed in Section A.9. The PT score serves as a valuable metric for evaluating the trade-offs between segmentation accuracy and efficiency. ", "page_idx": 17}, {"type": "text", "text": "A.7.5 Discussion on Running Time ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Given the relatively restricted support for sparse operations in current off-the-shelf commodity GPUs and TPUs without sparsity-aware accelerators, we did not attempt to achieve practical speedup during training. Instead, we chose to implement our models with binary masks in our work. The promising benefits of dynamic sparsity presented in this study have not yet translated into actual speedup. Accelerating training time will be a focus for our next work. ", "page_idx": 17}, {"type": "text", "text": "Although not the focus of our current work, it would be interesting for future work to examine the speedup results of sparse operation during training, using such specialized hardware accelerators. For example, at high unstructured sparsity levels, XNNPACK 8has already shown significant speedups over dense baselines on smartphone processors. ", "page_idx": 18}, {"type": "text", "text": "Although the support for unstructured sparsity on GPUs remains relatively limited, its practical relevance has been widely demonstrated on nonGPU hardware, such as CPUs and customized accelerators. For example, FPGA accelerators have demonstrated significant acceleration and ", "page_idx": 18}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/b016be4e48bda6ffd187cced644019d05248156fc196e6ba6ca71ce41b5a9235.jpg", "table_caption": ["Table 9: Comparison of inference speeds between E2ENet and nnUNet. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "energy efficiency for unstructured sparse RNNs, outperforming commercial CPUs and GPUs by effectively leveraging the FPGA\u2019s embedded resources. Additionally, DeepSparse 9 has shown impressive results in deploying large-scale, BERT-level sparse models on modern Intel CPUs. This approach achieved a $10\\times$ compression in model size with under a $1\\%$ accuracy loss, a $10\\times$ increase in CPU inference speed with less than a $2\\%$ accuracy reduction, and an impressive $29\\times$ CPU inference speedup with a maximum accuracy drop of $7.5\\%$ [Liu and Wang, 2023]. ", "page_idx": 18}, {"type": "text", "text": "Inspired by these advancements, we adopted an approach based on DeepSparse. As shown in Table 9, we conducted experiments with patches of images sized $32{\\times}32{\\times}32$ as input, comparing the CPU wall-clock timings for online inference between our proposed E2ENet and nnUNet on an Intel Xeon Platinum 8360Y CPU with 18 cores. We acknowledge that, while our proposed models with sparsity do achieve speedups in practical inference, they are not as pronounced as those observed with BERT-level sparse models. This is primarily due to the nature of segmentation and 3D convolution operations. However, this presents a promising avenue for our future work. ", "page_idx": 18}, {"type": "text", "text": "A.8 More Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.8.1 Class-wise Dice of AMOS-CT ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 10: Quantitative comparisons (class-wise Dice $(\\%)\\uparrow$ , mDice $(\\%)\\uparrow$ , Params $(\\mathbf{M})\\downarrow$ , inference $\\mathrm{FLOPs}(\\mathrm{G})\\downarrow$ , PT score\u2191and $\\mathrm{mNSD}(\\%)\\uparrow$ ) of segmentation performance on the validation set of AMOSCT dataset. Bold indicates the best and underline indicates the second best. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gall: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor: aorta IVC: inferior vena cava, Pan: pancreas, RAG: right adrenal gland, LAG: left adrenal gland, Duo: duodenum, Bla: bladder, Pro/Uth: prostate/uterus. The class-wise Dice, mDice and mNSD results of baselines, except for nnUNet, are collected from the [Ji et al., 2022]. \u2020 indicates the results without postprocessing that are collected from the AMOS website. \u2021 denotes the results with postprocessing that are reproduced by us. \u2217indicates the results with postprocessing. ", "page_idx": 18}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/b583a2636a37deb408724ae82842d60ed660bbbbbb9885316d39b16d67270140.jpg", "table_caption": [], "table_footnote": ["3 The inference FLOPs are calculated based on the patch sizes of $1\\times128\\times128\\times128$ without considering postprocessing cost. "], "page_idx": 18}, {"type": "text", "text": "A.8.2 BTCV Challenge ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compare the performance of our E2ENet model to several baselines (CoTr [Xie et al., 2021], RandomPatch [Tang et al., 2021], PaNN [Zhou et al., 2019], UNETR [Hatamizadeh et al., 2022], and nnUNet [Isensee et al., 2021]) on the test set of BTCV challenge, and report class-wise Dice, mDice, Params and inference FLOPs on the test set in Table 11. It is worth noting that nnUNet is a strong performer that uses an automatic model configuration strategy to select and ensemble two best of multiple U-Net models (2D, 3D and 3D cascade) based on cross-validation results. In contrast, E2ENet is designed to be computationally and memory efficient, using a consistent 3D network configuration. Swin UNETR [Tang et al., 2022] is among the best on the leaderboard for this challenge. However, we do not include it in our comparison because it employs self-supervised learning with extra data. This falls outside of our goal of trading off training efficiency and accuracy without using extra data. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Our proposed E2ENet, a single 3D architecture without cascade, has achieved comparable performance to nnUNet, with mDice of $88.3\\%$ . Additionally, it has a significantly smaller number of parameters, $11.25\\;\\mathrm{M}$ , compared to other methods such as nnUNet $(30.76\\,\\mathrm{M})$ , CoTr (41.87 M), and UNETR (92.78 M). ", "page_idx": 19}, {"type": "text", "text": "Table 11: Quantitative comparisons of segmentation performance on BTCV test set. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gall: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor: aorta IVC: inferior vena cava, Veins: portal and splenic veins, Pan: pancreas, AG: adrenal gland. The results (class-wise Dice and mDice) for these baselines are from [Hatamizadeh et al., 2022]. + denotes that the training of $\\mathrm{UNETR^{+}}$ is without using any extra data outside the challenge. The results of $\\mathrm{\\DeltannUNet^{\\ddag}}$ , E2ENet and Hausdorff Distance $\\mathrm{(HD)}\\downarrow$ of UNETR are from the standard leaderboard of BTCV challenge, while the results of nnUNet are from the free leaderboard. ", "page_idx": 19}, {"type": "table", "img_path": "Xp8qhdmeb4/tmp/53161ac8564793086f6107431e3599c2299f55b617261e5a55997caa1b730e32.jpg", "table_caption": [], "table_footnote": ["The inference FLOPs are calculated based on the patch sizes of $1\\times96\\times96\\times96$ . The codes for RandomPatch and PaNN are not publicly available, so it is not possible for us to determine their model size and inference FLOPs. "], "page_idx": 19}, {"type": "text", "text": "A.8.3 Statistical Significance of Designed Modules ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/d0297290713dd90b36685b42aa9831c3373e9eeb50535257532fa9116d5f6c71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "To demonstrate the advantages of individual modules, we plot a critical distance diagram using the Nemenyi post-hoc test with a p-value of 0.05 to establish the statistical significance of our modules. In Figure 7, the top line represents the axis along which the methods\u2019 average ranks, and a lower value indicates better performance. Methods joined by thick horizontal ", "page_idx": 19}, {"type": "text", "text": "Figure 7: The critical distance diagram on the AMOS-CT validation dataset, with the evaluation metric being mDice. ", "page_idx": 19}, {"type": "text", "text": "black lines are considered not statistically different. From the diagram, we can clearly observe that E2ENet with depth shift significantly outperforms E2ENet without depth shift. Additionally, the incorporation of dynamic sparse feature fusion into E2ENet results in a substantial reduction in both the number of FLOPs (from 23.90M to 11.23M) and parameters (from 3069.55G to 969.32G) while maintaining comparable performance, without any significant performance degradation. ", "page_idx": 19}, {"type": "text", "text": "A.9 The Impact of Weighting Factors $\\alpha$ on PT Score ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we investigate the impact of the weighting factors $\\alpha_{1}$ and $\\alpha_{2}$ on the Performance Trade-off Score for the AMOS-CT challenge, as defined in Equation 7. These factors are used to balance the trade-off between accuracy and resource cost. A higher value of $\\alpha_{1}$ prioritizes accuracy, while a lower value emphasizes resource cost. Figure 8 shows that as $\\alpha_{1}$ decreases, the gaps in the Performance Trade-off Score between E2ENet and other methods become larger, indicating that our method is more advantageous when prioritizing resource cost. However, even when $\\alpha_{1}$ is set to be 20 times greater than $\\alpha_{2}$ , which prioritizes accuracy over resource cost, the Performance Trade-off Score of E2ENet remains superior to other baselines. This result indicates that our proposed E2ENet architecture is highly efficient in terms of computational cost and memory usage while achieving excellent segmentation performance on the AMOS-CT challenge. ", "page_idx": 19}, {"type": "text", "text": "A.9.1 Qualitative Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "BTCV Challenge In Figure 9 (b), we present a qualitative comparison of our proposed E2ENet method with nnUNet as a baseline model on the BTCV challenge. Our results demonstrate the effectiveness of our proposed method in addressing some of the challenges of medical image segmentation. For example, as shown in the first and third columns, our E2ENet method accurately distinguishes the stomach from the background without over- or under-segmentation, which can be difficult due to the low contrast in the image. In the second column, E2ENet performs well in differentiating the stomach from the spleen. These examples suggest that our DSFF module can effectively encode feature information for improved performance in medical image segmentation. ", "page_idx": 19}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/170d5659c9e334de53d0332905eac11be3ec22f688b9932ecee122d6b7077d83.jpg", "img_caption": ["Figure 8: Comparison of Performance Trade-Off score between E2ENet and other models on AMOSCT challenge with varying $\\alpha_{1}$ and $\\alpha_{2}$ values. E2ENet outperforms other baselines in achieving a better trade-off between accuracy and efficiency across different preferences. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/bfa145ea93598eb6fbcdb05e77803d7b4489fac1dd1885308283ba2bd4959eb6.jpg", "img_caption": ["Figure 9: Qualitative comparison of the proposed E2ENet and nnUNet on AMOS-CT and BTCV challenges. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "BraTS Challenge in MSD Figure 10 presents a qualitative comparison of our proposed E2ENet method with the nnUNet on the BraTS challenge with highly variable shapes of the segmentation targets. Based on the results of the baseline model, nnUNet, we observed that accurately distinguishing the edema (ED) from the background is difficult, as the edema tends to have less smooth boundaries. Our results suggest that E2ENet may have some potential to improve the distinguishability of the edema boundaries, as evidenced by the relatively better segmentation results in the first, second, and fourth columns. Moreover, E2ENet accurately differentiates the enhanced tumor (ET) from the edema, as shown in the third column, which is a challenging task due to the similarity in appearance between these two regions, and the dispersive distribution of ET. These findings suggest that E2ENet is a promising method for accurately segmenting brain tumors in challenging scenarios. ", "page_idx": 20}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/95fbc88cefc257a93a3573be7ba678ad7fa628dd2a6abe665b74ab61a9d27fb0.jpg", "img_caption": ["Figure 10: Qualitative comparison of the proposed E2ENet and nnUNet on BraTS Challenge in MSD. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "A.10 Convergence Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we analyze the convergence behavior of E2ENet by examining the loss changes during topology updating (kernel activation/deactivation epochs), comparing it with the best-performing baseline nnUNet, and studying the impact of topology update frequency. From Figure 11, we observed that the activation/deactivation of weights initially led to an increase in training loss. However, over the long term, the training converged. Additionally, we compared the learning curve of E2ENet with that of nnUNet and found that E2ENet converged even faster than nnUNet, as shown in the subplot in Figure 12 (a). To account for the effect of the number of parameters, we scaled down nnUNet to have a similar number of parameters as E2ENet and observed that it converged even more slowly than the original nnUNet. We also studied the impact of topology update frequency. As shown in Figure 12 (b), when the topology updating frequency is increased, the convergence speed may decrease slightly, but the impact is not significant. ", "page_idx": 21}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/4a7a822d52528803f444b7986cc7a10d35678d7a9355f9eacaeb719620239dea.jpg", "img_caption": ["Figure 11: The learning curve of E2ENet on AMOS-CT, with green dotted vertical lines indicating the epochs of weight activation and deactivation. The blue line represents the ratio of weight deactivation/reactivation throughout the training process. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/0e2ab62d825bc01f0492550f09d85eb3591c415cf3dbb6473a1d896012d06fd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: (a) Comparing the learning curve of E2ENet with that of nnUNet and scaled-down nnUNet (referred to as nnUNet (-)); (b) Comparing the learning curve of E2ENet with different topology update frequencies. ", "page_idx": 22}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/c2e5bfbbee1f301be1a73b236282be1f23a0655b002be3838fb4525127774fe6.jpg", "img_caption": ["A.11 Organ Volume Statistics and Class-wise Results Visualization "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: (a) The organ volume statistics of AMOS-CT training dataset. (b) Class-wise Dice of nnUNet without postprocessing (visualization of Table 1). (c) Class-wise Dice differences between E2ENet with feature sparsity 0.7 without postprocessing and nnUNet without postprocessing on AMOS-CT validation dataset. The positive value means that E2ENet outperforms nnUNet, and vice versa.) ", "page_idx": 22}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/2491c5b8aa53556bb226b44627bbec50360da066970ff4f417e5d84a4e04351d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 14: (a) The organ volume statistics of BTCV training dataset. (b) Class-wise Dice of nnUNet (visualization of Table 11). Note that AG denotes the average of the right and left adrenal glands (RAG and LAG). (c) Class-wise Dice differences between E2ENet with feature sparsity 0.7 and nnUNet on BTCV test dataset. The positive value means that E2ENet outperforms nnUNet, and vice versa. ", "page_idx": 22}, {"type": "text", "text": "In this section, we analyzed the relationship between organ volume and segmentation accuracy on the AMOS-CT, BTCV, and BraTS challenges. The results, depicted in Figures 13, 14 and 15, showed that small organs with relatively low segmentation accuracy. For the AMOS-CT challenge, RAG (right adrenal gland), LAG (left adrenal gland), Gall (gallbladder), and Eso (esophagus) are more challenging to accurately segment. This may be due to the fact that smaller organ volumes provide less visual information for the segmentation algorithm to work with. However, our proposed method, E2ENet, also demonstrated comparable (or better) performance on these small organs, particularly for the organ \u201cLAG\u201d, in which the Dice improved from $81.7\\%$ to $82.4\\%$ . On the BTCV challenge, the Dice of \u201cGall\u201d, which is considered to be the most challenging organ, improves from $75.3\\%$ to $78.1\\%$ when using E2ENet compared to nnUNet. For the BraTs challenge, E2ENet demonstrates the most significant improvement in the Dice score of the \"ET\" region, which is considered the most challenging class, with an increase of $0.7\\%$ . ", "page_idx": 22}, {"type": "image", "img_path": "Xp8qhdmeb4/tmp/799a1759c473d616c8e60fdd1d0940d35d75267244da82ef1af4b0000d0a50a3.jpg", "img_caption": ["Figure 15: (a) The organ volume statistics of BraTS training dataset. (b) Class-wise Dice of nnUNet (visualization of Table 2) (c) Class-wise Dice differences between E2ENet with feature sparsity 0.7 and nnUNet on 5-fold cross-validation of the training dataset. The positive value means that E2ENet outperforms nnUNet, and vice versa. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "These results indicate that by applying the DSFF mechanism, E2ENet is able to effectively utilize multi-scale information, potentially leading to improved performance in segmenting small organs. It is important to note that other factors, such as the quality and resolution of the medical images, as well as the complexity of the anatomy being imaged, may also impact the performance of the segmentation algorithms. Future work could focus on further exploring the potential impact of these factors on segmentation accuracy. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have claimed that our proposed Efficient to Efficient Network (E2ENet) in the abstract and introduction. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The efficiency in this paper is theoretical, we still need future work for hard work support, which we claim in conclusion. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We don\u2019t include theoretical results for proof ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide implementation details in A.5, and the code and dataset links in the supplementary materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The dataset we use is accessible online, and the code is provided in the supplementary materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 25}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide implementation details in A.5. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In Section A.8.3, we plot a critical distance diagram using the Nemeny post-hoc test with a p-value of 0.05 to establish the statistical significance of our modules. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the type of hardware used for the implementation in Appendix A.5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We include border impact in Section A.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The Licenses names for existing assets are CC-BY 4.0. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]