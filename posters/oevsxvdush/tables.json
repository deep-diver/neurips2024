[{"figure_path": "oEVsxVdush/tables/tables_8_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on three datasets: Cars3D, Shapes3D, and MPI3D.  The models are categorized into three groups: symbolic scalar-tokened compositional representations, symbolic vector-tokened compositional representations, and fully continuous compositional representations.  The scores show the level of disentanglement achieved by each model, with higher scores indicating better disentanglement. The table highlights the superior performance of the Soft TPR Autoencoder in terms of disentanglement compared to the baseline models.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_8_2.jpg", "caption": "Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  It compares the performance of various models, including those using symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The results highlight the superior disentanglement performance achieved by the Soft TPR model proposed in the paper.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_8_3.jpg", "caption": "Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  It compares the performance of various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models.  The results show the Soft TPR model achieves state-of-the-art disentanglement across all three datasets.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_9_1.jpg", "caption": "Table 4: Downstream FoV R\u00b2 scores (odd columns) and sample efficiencies (even columns) on the MPI3D dataset.", "description": "This table shows the R-squared scores and sample efficiencies of downstream regression models trained with different numbers of samples (100, 250, 500, 1000, 10000) from the MPI3D dataset.  The odd columns represent the R-squared score, a measure of how well the model fits the data. The even columns show sample efficiency, which is the ratio of the model's performance with a limited number of samples to its performance with all available samples.  It compares different representation learning models, highlighting the sample efficiency of Soft TPR in the low sample regime.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_9_2.jpg", "caption": "Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  It compares the performance of symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The scores provide a quantitative measure of how well each model disentangles the underlying factors of variation in the data. Higher scores indicate better disentanglement.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_9_3.jpg", "caption": "Table 6: Sample efficiencies for FoV Regression.", "description": "This table presents the sample efficiency results for the FoV regression task across three different datasets (Cars3D, Shapes3D, MPI3D).  Sample efficiency is calculated as the ratio of the downstream model's R-squared score when trained with a limited number of samples (100, 250, etc.) to its R-squared score when trained with all available samples. The table compares the sample efficiency of the Soft TPR model against the baseline models using the explicit TPR and the original variants of other baseline models.  Higher values indicate better sample efficiency (i.e., the model performs well even with fewer samples).", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_9_4.jpg", "caption": "Table 7: Effect of model properties on disentanglement performance (MPI3D dataset)", "description": "This table shows the results of an ablation study on the MPI3D dataset.  The goal was to determine the contribution of different components of the Soft TPR Autoencoder model to disentanglement performance. The table compares DCI scores achieved under various conditions: with only weak supervision, with explicit filler dependency, with semi-orthogonality, and with all these properties included.  The \"Full\" row represents the complete model, demonstrating that all aspects contribute to superior disentanglement performance.", "section": "More Ablation Experiments"}, {"figure_path": "oEVsxVdush/tables/tables_20_1.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table presents the FactorVAE scores achieved by different representation learning models at various stages of training (100, 1000, 10000, 100000, and 200000 iterations). It compares the performance of symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The table helps to evaluate how quickly different models learn the inherent compositional structure during the training process.", "section": "5.2 Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_20_2.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table shows the FactorVAE scores achieved by different models at various training iterations (100, 1k, 10k, 100k, and 200k).  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  It demonstrates the convergence rate of different representation learning approaches on the Cars3D dataset, specifically focusing on the FactorVAE score as a measure of disentanglement.", "section": "C.4 Representation Learning Convergence"}, {"figure_path": "oEVsxVdush/tables/tables_21_1.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table shows the FactorVAE scores achieved by different models at various training iterations (10^2, 10^3, 10^4, 10^5, and 2*10^5).  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The table helps to compare the convergence speed and the final disentanglement level of different models.  The \"+\" symbol in the model names indicates a dimensionality-matched control group.", "section": "C.4 Representation Learning Convergence"}, {"figure_path": "oEVsxVdush/tables/tables_21_2.jpg", "caption": "Table 11: Hyperparameter values.", "description": "This table shows the hyperparameter values used for the Soft TPR Autoencoder model.  It lists architectural hyperparameters (DR, NR, DF, NF) and loss function hyperparameters (\u03bb1, \u03bb2, \u03b2).  The values are specified for three different datasets: Cars3D, Shapes3D, and MPI3D, indicating that the hyperparameters were tuned separately for each dataset.  The \"fixed\" notation for \u03b2 indicates that this parameter was held constant across all datasets during the hyperparameter optimization process. ", "section": "B.4 Model Hyperparameters and Hyperparameter Tuning"}, {"figure_path": "oEVsxVdush/tables/tables_22_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  It compares the performance of symbolic scalar-tokened models (SlowVAE, Ada-GVAE-k, GVAE, ML-VAE, Shu), symbolic vector-tokened models (VCT, COMET), and the proposed fully continuous compositional representation (Ours).  The results show that the Soft TPR significantly outperforms other methods in terms of disentanglement.", "section": "5.1 Compositional Structure / Disentanglement"}, {"figure_path": "oEVsxVdush/tables/tables_23_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D dataset.  It compares the performance of various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models.  The results are presented as means \u00b1 standard deviations, calculated over five random runs.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_26_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  It compares the performance of various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models.  The scores indicate the degree of disentanglement achieved by each model, with higher scores generally suggesting better disentanglement. The table also includes results for parameter-controlled models (denoted by *) which have an equivalent number of parameters to the Soft TPR model to ensure fair comparison. These models were created to rule out the impact of additional parameters on the disentanglement results.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_27_1.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG scores for different models on three datasets (Cars3D, Shapes3D, MPI3D).  The BetaVAE score and MIG score are two metrics used to evaluate the disentanglement of a representation learning model.  Higher scores generally indicate better disentanglement. The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The table shows the performance of different models in achieving disentanglement. The \"=\" symbol indicates that the score is 1.000, which is the maximum possible score for BetaVAE.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_34_1.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table shows the FactorVAE scores achieved by different models at various training iterations on the Cars3D dataset.  It compares the performance of symbolic scalar-tokened models (SlowVAE, Ada-GVAE-k, GVAE, MLVAE, Shu), symbolic vector-tokened models (VCT, COMET), and the proposed fully continuous compositional representation (Ours). The scores indicate the degree of disentanglement achieved by each model at different stages of training.", "section": "5.2 Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_34_2.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D dataset.  It compares the performance of various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The results show that the Soft TPR model achieves superior disentanglement compared to other models, particularly on the more challenging datasets.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_34_3.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG scores for different models across three datasets: Cars3D, Shapes3D, and MPI3D.  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  Each model's performance is evaluated at various stages of training, indicated by the number of iterations. The BetaVAE and MIG scores are metrics for evaluating the disentanglement of the learned representations.", "section": "5 Results"}, {"figure_path": "oEVsxVdush/tables/tables_35_1.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG disentanglement scores for different models across three datasets (Cars3D, Shapes3D, MPI3D).  The models are categorized into symbolic scalar-tokened compositional representations, symbolic vector-tokened compositional representations, and fully continuous compositional representations.  The scores represent the average performance over five random runs, providing a measure of how well each model separates the factors of variation in the data.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_35_2.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table presents the FactorVAE scores for different models at various iterations during training.  It compares the performance of symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The results show how the FactorVAE score changes over time for each model, indicating the models' convergence rate and the quality of the disentangled representations learned.", "section": "5.2 Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_35_3.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D dataset.  It compares the performance of various models, including those using symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations. The table shows the scores obtained at different stages of training (100, 1000, 10000, 100000, and 200000 iterations).  It highlights the superior disentanglement performance achieved by the Soft TPR model compared to traditional approaches.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_36_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The table allows for a comparison of the disentanglement performance of different representation learning approaches, highlighting the superior performance of the proposed Soft TPR model.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_36_2.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG disentanglement scores for different models on three datasets (Cars3D, Shapes3D, MPI3D).  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The scores are presented for various stages of training (iterations 100, 1000, 10000, 100000, 200000), allowing for an analysis of convergence speed and the final disentanglement performance of each model.", "section": "5.1 Compositional Structure / Disentanglement"}, {"figure_path": "oEVsxVdush/tables/tables_36_3.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D dataset.  It compares various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models. The scores indicate the level of disentanglement achieved by each model, where higher scores represent better disentanglement.  The table highlights the superior performance of the proposed Soft TPR model compared to existing approaches. ", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_37_1.jpg", "caption": "Table 16: FactorVAE and DCI scores", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D dataset.  It compares the performance of various models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The scores indicate the degree of disentanglement achieved by each model, with higher scores representing better disentanglement.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_37_2.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG disentanglement scores for different models across three datasets (Cars3D, Shapes3D, MPI3D).  It shows the scores at various stages of training (iterations). The models are categorized into symbolic scalar-tokened compositional representations, symbolic vector-tokened compositional representations, and fully continuous compositional representations.  The table allows for comparison of disentanglement performance between different model types and the impact of training duration on disentanglement.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_37_3.jpg", "caption": "Table 17: BetaVAE and MIG scores", "description": "This table presents the BetaVAE and MIG disentanglement scores for different models, categorized by their representational type (symbolic scalar-tokened, symbolic vector-tokened, and fully continuous).  The scores are shown for different stages of training (100, 1,000, 10,000, 100,000, and 200,000 iterations) across three datasets (Cars3D, Shapes3D, MPI3D). The table allows for the comparison of disentanglement levels achieved by different models and across different training stages, highlighting the performance of the Soft TPR Autoencoder.", "section": "5.1 Compositional Structure / Disentanglement"}, {"figure_path": "oEVsxVdush/tables/tables_42_1.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table presents the FactorVAE scores achieved by different representation learning models at various training iterations (100, 1k, 10k, 100k, and 200k iterations).  It compares the performance of models employing symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations, highlighting the convergence speed and the final performance of each approach on the Cars3D dataset for disentanglement.", "section": "Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_42_2.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table presents the FactorVAE scores achieved by different representation learning models at various stages of training (10\n2, 10\n3, 10\n4, 10\n5, and 2 \u00d7 10\n5 iterations). The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models.  The table showcases how the FactorVAE score, a metric for measuring disentanglement, changes over time for each model, offering insights into their convergence rates. The results show the performance of each model on the Cars3D dataset in terms of achieving disentangled representations.", "section": "5.2 Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_43_1.jpg", "caption": "Table 18: Representation learner convergence on the Cars3D dataset (Factor score)", "description": "This table presents the FactorVAE scores achieved by different representation learning models at various stages of training (10\n<sup>2</sup>, 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>, and 2 \u00d7 10<sup>5</sup> iterations).  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations. The table shows how the FactorVAE score, a metric for evaluating the degree of disentanglement, changes as the models train.  This allows for a comparison of how quickly different representation learning methods converge toward a disentangled representation.", "section": "5.2 Representation Learner Convergence Rate"}, {"figure_path": "oEVsxVdush/tables/tables_45_1.jpg", "caption": "Table 33: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset", "description": "This table shows the classification accuracy of the downstream WReN model on the abstract visual reasoning dataset.  The accuracy is evaluated at different stages of representation learning (100, 1000, 10000, 100000 and 200000 iterations).  The table compares the performance of several models: Slow-VAE, Ada-GVAE-k, GVAE, MLVAE, Shu, VCT, COMET, and the authors' Soft TPR model.  The results are presented with standard deviations to illustrate the uncertainty of the results.", "section": "C.4 Downstream Performance"}, {"figure_path": "oEVsxVdush/tables/tables_52_1.jpg", "caption": "Table 34: Downstream regression model sample efficiency on the Cars3D dataset", "description": "This table presents the sample efficiency results for downstream regression models trained on Cars3D dataset using different representation learning models. Sample efficiency is calculated by dividing the R-squared score of the model trained with a limited number of samples (100, 250, 500, 1000, 10000) by the R-squared score of the model trained with all available samples. The results are categorized by representation learning model type: symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The table shows that the Soft TPR model generally achieves higher sample efficiency than other models, especially in low sample regimes.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_52_2.jpg", "caption": "Table 34: Downstream regression model sample efficiency on the Cars3D dataset", "description": "This table presents the sample efficiency of downstream regression models on the Cars3D dataset. Sample efficiency is calculated by dividing the R-squared score of the model trained on a limited number of samples (100, 250, 500, 1,000, and 10,000) by the R-squared score of the model trained on all available samples. The results are shown for different representation learning methods, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The table helps to assess how well each method performs with limited data, indicating its sample efficiency.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_53_1.jpg", "caption": "Table 36: Downstream regression model sample efficiency on the MPI3D dataset", "description": "This table presents the sample efficiency results for downstream regression models on the MPI3D dataset. Sample efficiency is calculated as the ratio of the model's performance with a limited number of samples (100, 250, 500, 1000, 10000) to its performance when trained with all samples. The results are broken down by model type (symbolic scalar-tokened, symbolic vector-tokened, fully continuous) and show the mean and standard deviation of the R2 ratio for each model and sample size.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_60_1.jpg", "caption": "Table 34: Downstream regression model sample efficiency on the Cars3D dataset", "description": "This table presents the sample efficiency results for downstream regression models trained on Cars3D data.  Sample efficiency is calculated by dividing the R-squared score of a model trained on a smaller dataset (100, 250, 500, 1000, 10000 samples) by the R-squared score of the same model trained on the full dataset.  Results are presented for several models, categorized as symbolic scalar-tokened compositional representations, symbolic vector-tokened compositional representations, and fully continuous compositional representations. The table shows the mean and standard deviation for each model and sample size.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_60_2.jpg", "caption": "Table 34: Downstream regression model sample efficiency on the Cars3D dataset", "description": "This table presents the results of downstream regression model sample efficiency on the Cars3D dataset. Sample efficiency is calculated by dividing the model's performance when trained with a limited number of samples (100, 250, 500, 1,000, 10,000) by its performance when trained with all samples. The table shows that the Soft TPR model outperforms other models, particularly when trained on smaller datasets.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_61_1.jpg", "caption": "Table 34: Downstream regression model sample efficiency on the Cars3D dataset", "description": "This table presents the sample efficiency results for downstream regression models trained on Cars3D dataset. Sample efficiency is calculated by dividing the R-squared score of a model trained on a limited number of samples (100, 250, 500, 1000, 10000) by the R-squared score of the same model trained on the full dataset.  The table compares the sample efficiency of different models, including symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The results show that Soft TPR model demonstrate superior sample efficiency compared to the other models, especially when only a small number of samples are available.", "section": "5.3 Downstream Models"}, {"figure_path": "oEVsxVdush/tables/tables_64_1.jpg", "caption": "Table 33: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset", "description": "This table presents the classification accuracy results of the downstream WReN model on the abstract visual reasoning dataset for different numbers of samples used to train the representation learners. It shows how the performance of the downstream model changes as the representation learner receives more training data. The table compares various representation learning models, including the proposed Soft TPR Autoencoder, symbolic alternatives (Ada-GVAE, GVAE, MLVAE, Shu), and vector-tokened methods (VCT, COMET), highlighting the effect of the representation learning methodology on downstream model performance.", "section": "C.4.2 Downstream Performance"}, {"figure_path": "oEVsxVdush/tables/tables_75_1.jpg", "caption": "Table 41: Hyperparameter values of ablation setting", "description": "This table shows the hyperparameter values used in an ablation study on the MPI3D dataset.  It includes architectural hyperparameters such as the dimensionality of the role and filler embedding spaces (DR, DF), the number of role and filler embedding vectors (NR, NF), and loss function hyperparameters such as lambda1 (\u03bb1), lambda2 (\u03bb2), and beta (\u03b2). The values are presented for both the original experiment and the ablation experiment.", "section": "C.6 More Ablation Experiments"}, {"figure_path": "oEVsxVdush/tables/tables_75_2.jpg", "caption": "Table 42: Disentanglement metric scores on the MPI3D dataset", "description": "This table presents the disentanglement metric scores obtained on the MPI3D dataset for two hyperparameter configurations: the original configuration and an ablation configuration.  The metrics used are FactorVAE score, DCI score, BetaVAE score, and MIG score. The table allows for comparison of model performance across these metrics under different hyperparameter settings, providing insight into model robustness and sensitivity to hyperparameter choices.", "section": "C.3 Disentanglement"}, {"figure_path": "oEVsxVdush/tables/tables_75_3.jpg", "caption": "Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3", "description": "This table presents the FactorVAE and DCI disentanglement scores for different models on the Cars3D, Shapes3D, and MPI3D datasets.  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representation models.  The results show the performance of each model in terms of disentanglement, where higher scores indicate better disentanglement. The Soft TPR model significantly outperforms all other baselines.", "section": "Results"}, {"figure_path": "oEVsxVdush/tables/tables_77_1.jpg", "caption": "Table 44: Comparison of multiplicative dimensionality", "description": "This table compares the dimensionality of the Soft TPR model with other baselines for three different datasets: Cars3D, Shapes3D, and MPI3D.  It shows that while the Soft TPR model has a higher dimensionality compared to the scalar-tokened models, its dimensionality is considerably lower than the vector-tokened models (VCT and COMET), demonstrating improved scalability in relation to the number of roles (FoV types) and fillers (FoV tokens). The table highlights the trade-off between representational expressivity and computational efficiency.", "section": "D.4 Dimensionality"}, {"figure_path": "oEVsxVdush/tables/tables_77_2.jpg", "caption": "Table 44: Comparison of multiplicative dimensionality", "description": "This table compares the dimensionality of the representations produced by different models for the three disentanglement datasets (Cars3D, Shapes3D, MPI3D).  It highlights the multiplicative growth of dimensionality in the Soft TPR approach resulting from the tensor product of role and filler embedding spaces (DR and DF). However, it also shows that the Soft TPR's dimensionality (DF * DR) can be smaller than the total number of role-filler bindings (n) in the datasets.  The table contrasts the Soft TPR's dimensionality with those of baseline models which use either symbolic scalar-tokened or symbolic vector-tokened compositional representations.", "section": "D.4 Dimensionality"}, {"figure_path": "oEVsxVdush/tables/tables_78_1.jpg", "caption": "Table 46: Comparison of FLOPs required for a forward pass of batch size 16", "description": "This table compares the number of floating point operations (FLOPs) required for a single forward pass of a batch size of 16 for different models.  The models are categorized into symbolic scalar-tokened, symbolic vector-tokened, and fully continuous compositional representations.  The FLOPs are reported for each of the three datasets used in the experiments: Cars3D, Shapes3D, and MPI3D.  The table highlights the computational efficiency of the Soft TPR approach compared to the baselines.", "section": "D Limitations and Future Work"}]