[{"figure_path": "L4uaAR4ArM/figures/figures_0_1.jpg", "caption": "Figure 1: (Left) Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (Bottom Right) Perplexity (PPL) on One Billion Words (LM1B) benchmark.", "description": "This figure shows the architecture of the proposed Masked Diffusion Language Model (MDLM) and compares its performance to other masked language models.  The left panel illustrates the MDLM training process which uses a weighted average of masked cross-entropy losses. The top right panel highlights that, unlike standard MLM, MDLM's objective is a variational lower bound and allows for efficient ancestral sampling.  The bottom right panel presents a comparison of perplexity scores on the One Billion Words benchmark, demonstrating the superior performance of MDLM.", "section": "3 Simple Masked Diffusion Models"}, {"figure_path": "L4uaAR4ArM/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (Bottom Right) Perplexity (PPL) on One Billion Words (LM1B) benchmark.", "description": "This figure illustrates the MDLM architecture and its training process. The left panel shows the model architecture, where the input is masked and processed through multiple masked diffusion layers before reaching the final output. The top right panel visually represents how MDLM's objective is derived using a weighted average of masked cross-entropy losses, and how the weighted average relates to a variational lower bound, a more principled method in this context. The bottom right panel compares the perplexity of MDLM with other state-of-the-art models on the One Billion Words benchmark, showing that MDLM achieves the lowest perplexity.", "section": "3 Simple Masked Diffusion Models"}, {"figure_path": "L4uaAR4ArM/figures/figures_34_1.jpg", "caption": "Figure 1: (Left) Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (Bottom Right) Perplexity (PPL) on One Billion Words (LM1B) benchmark.", "description": "This figure demonstrates the architecture of the proposed Masked Diffusion Language Model (MDLM) and compares its performance to other masked language models. The left panel shows the MDLM architecture, highlighting its use of a weighted average of masked cross-entropy losses during training. The top right panel contrasts MDLM's objective function with that of traditional masked language models (MLMs), emphasizing MDLM's derivation from a principled variational lower bound and its support for efficient ancestral sampling. The bottom right panel presents a bar chart comparing the perplexity scores achieved by MDLM and various other models on the One Billion Words (LM1B) benchmark, showcasing MDLM's superior performance.", "section": "1 Introduction"}, {"figure_path": "L4uaAR4ArM/figures/figures_35_1.jpg", "caption": "Figure 1: (Left) Our proposed masked diffusion language model (MDLM) is trained using a weighted average of masked cross entropy losses. (Top Right) In comparison to masked language models (MLM), MDLM's objective correspond to a principled variational lower bound, and supports generation via ancestral sampling. (Bottom Right) Perplexity (PPL) on One Billion Words (LM1B) benchmark.", "description": "This figure presents a comparison between different language modeling approaches using the One Billion Words benchmark. The left panel shows the MDLM architecture, which uses a weighted average of masked cross-entropy losses for training.  The top-right panel highlights the advantages of MDLM over traditional masked language models (MLM):  MDLM's objective function is a principled variational lower bound, enabling efficient ancestral sampling for text generation.  The bottom-right panel displays the perplexity results of several models, including MDLM, which shows improved performance compared to other methods.", "section": "3 Simple Masked Diffusion Models"}]