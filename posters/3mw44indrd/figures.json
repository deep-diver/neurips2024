[{"figure_path": "3MW44iNdrD/figures/figures_1_1.jpg", "caption": "Figure 1: Our work re-visits SOTA fair T2I generation, ITI-GEN. We question ITI-GEN's central idea of prompt learning via alignment between the directions of prompt embeddings and reference image embeddings. (a) We observe degradation in images generated through ITI-GEN's learned prompts. We note that the direction of reference image embeddings could include unrelated concepts beyond tSA differences (e.g., variations in accessories) resulting in learning of distorted prompts using ITI-GEN. Furthermore, we observe misalignment between the direction of credible hard prompts and that of reference images/learned prompts. (b) As our main contribution and to further understand how these distorted prompts affect the image generation process, we deep dive into the denoising network and analyze the cross-attention maps, revealing their abnormalities e.g., higher activity for maps associated with non-tSA tokens (\u201cof\u201d, \u201ca\u201d). We examine the degraded global structures resulting from these distorted prompts in the early denoising steps. (c) Building on insights from our analysis, we propose a solution to address distorted prompts while maintaining competitive fairness. Our solution FairQueue includes two ideas: prompt queuing and attention amplification. Er and E\u2081 are CLIP text and image encoder resp. [20]. T, F, P are the base prompt, hard prompt with minimal linguistic ambiguity, and ITI-GEN prompt, resp.", "description": "This figure shows the main idea of the paper. It starts with the observation that the state-of-the-art (SOTA) fair text-to-image (T2I) generation method, ITI-GEN, suffers from degraded image quality.  The authors analyze the cross-attention maps of the ITI-GEN model to identify the root cause, which is the sub-optimal learning objective leading to distorted prompts. Based on this analysis, the authors propose a novel solution, called FairQueue, which addresses the quality issue while achieving competitive fairness.", "section": "A Closer Look at Prompt Learning for Fair Text-to-Image Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_4_1.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods (HP, ITI-GEN, and FairQueue) for generating images based on text prompts with minimal linguistic ambiguity. HP shows high quality and fairness, while ITI-GEN shows degraded quality and slightly lower fairness. FairQueue achieves comparable performance to HP, indicating that it successfully addresses the quality degradation issues of ITI-GEN while maintaining competitive fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps of the denoising process between hard prompts and ITI-GEN prompts for the tSA=Smiling. It highlights that ITI-GEN tokens show abnormal attention patterns compared to the hard prompts, leading to degraded global structure in the early denoising steps.", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_6_1.jpg", "caption": "Figure 4: Analyzing the accumulated cross-attention maps for the denoising process in our proposed prompt switching analysis I2H and H2I. Here, we use two tSAs: Smiling, High Cheekbones. For each tSA, we show the accumulated cross-attention maps for H2I and I2H, with some quantitative results. In the H2I (I2H) experiment, the first row shows the accumulated cross-attention maps during the early denoising steps with HP (ITI-GEN) as the input prompt, and the second row shows the maps during later steps, after switching to ITI-GEN (HP). Observation 1: Learned tokens in ITI-GEN affect early denoising steps, degrading global structure synthesis; such degraded global structure disrupts the final output. This is observed in I2H. Observation 2: Learned tokens in ITI-GEN works decently in the later stage of the denoising process if the global structure is synthesized properly. This is observed in H2I. As we show in Supp, similar observations can be made for other samples and other tSAs. Bottom: Histograms of our proposed metrics on cross-attention maps demonstrate the abnormalities in many samples.", "description": "This figure visualizes the accumulated cross-attention maps during the denoising process for two tSAs (Smiling and High Cheekbones) using two proposed prompt switching analysis methods (I2H and H2I). The results show that ITI-GEN's learned tokens negatively affect the early denoising steps, leading to degraded global structure. However, if the global structure is properly synthesized in the early steps, the ITI-GEN tokens perform adequately in later steps. The bottom part shows histograms of proposed metrics on cross-attention maps, demonstrating abnormalities in many samples.", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_9_1.jpg", "caption": "Figure 5: Ablation Study: Comparing FairQueue performance when varying i) attention amplification factor, c or ii) Prompt Queuing transition point from T \u2192 P for tSA Smiling.", "description": "This ablation study analyzes the effect of varying attention amplification scaling factors (c) and prompt queuing transition points on the performance of the FairQueue model for the tSA (target sensitive attribute) Smiling. The results show that increasing the attention amplification factor generally improves fairness until a saturation point is reached (c=10), beyond which quality and semantic preservation degrade.  The optimal prompt queuing transition point is found to be at 0.2l (where l is the total number of denoising steps), balancing quality, fairness, and semantic preservation.", "section": "5 Experiments"}, {"figure_path": "3MW44iNdrD/figures/figures_9_2.jpg", "caption": "Figure 6: Illustration of samples generated by ITI-GEN and FairQueue with a new Base Prompt T' = Et{\"A headshot of a doctor\"} via pre-pending. FairQueue improves sample quality and ability to preserve the original sample's semantics while mainly adapting only the tSA.", "description": "This figure compares image generation results using ITI-GEN and FairQueue when using a new base prompt. The samples show that FairQueue better maintains the semantics of the original prompt while accurately generating the target sensitive attribute.  ITI-GEN struggles to maintain the original prompt's semantics and produces some lower quality results.", "section": "A.3.2 More Illustrations for Training Once-for-All Tokens"}, {"figure_path": "3MW44iNdrD/figures/figures_17_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "The figure compares the cross-attention maps during the denoising process between hard prompts (HP) and ITI-GEN prompts. It highlights three key observations: ITI-GEN tokens have abnormal activities, non-tSA tokens are abnormally more active, and issues created by ITI-GEN tokens degrade the global structure in the early denoising steps. ", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_18_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts. The comparison highlights three key observations: ITI-GEN tokens show abnormal activity, non-tSA tokens are abnormally more active in the presence of ITI-GEN tokens, and ITI-GEN tokens degrade the global structure in early denoising steps. ", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_19_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S1 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "The figure compares cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts. It highlights the abnormalities in ITI-GEN prompts, showing how they lead to degraded global structures in the generated images.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_20_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l=50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S1 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts.  Three key observations highlight the abnormalities in ITI-GEN's attention maps: unrelated regions are attended to, non-tSA tokens show abnormally high activity, and global structure degrades in the early stages of denoising.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_21_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "The figure compares cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts.  It highlights three key observations showing that ITI-GEN prompts have abnormal activities, non-tSA tokens are abnormally more active, and that ITI-GEN tokens degrade global structure in the early denoising steps, compared to HP.", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_22_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts.  It highlights abnormalities in ITI-GEN's cross-attention maps, particularly in the early steps, indicating that ITI-GEN tokens attend to unrelated regions and non-tSA tokens are abnormally active. This leads to degraded global structure in the generated images.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_23_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts.  Three key observations highlight abnormalities in ITI-GEN's attention maps compared to HP: abnormal activity of ITI-GEN tokens, higher activity of non-tSA tokens, and degradation of global image structure in early stages.", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_24_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "The figure compares cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts. It highlights three key observations: 1) ITI-GEN tokens show abnormal activity; 2) non-tSA tokens are abnormally active with ITI-GEN tokens; 3) ITI-GEN tokens degrade global image structure in early steps.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_25_1.jpg", "caption": "Figure 15: Cross-attention maps during the denoising process with HP (left) and ITI-GEN (right) prompts. tSA=Gray Hair.", "description": "This figure visualizes the cross-attention maps during the denoising process for a single sample generation, comparing the hard prompt (HP) approach with the Inclusive T2I Generation (ITI-GEN) method.  The cross-attention maps show how the different tokens in the prompts interact with different parts of the image at each denoising step.  The generated images at each step are also shown below the maps. This visualization helps to understand how the ITI-GEN prompts lead to abnormalities in cross-attention, particularly in the early stages of denoising, contributing to the degraded image quality observed in the ITI-GEN approach. The target sensitive attribute (tSA) for this example is \"Gray Hair\".", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_26_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for both hard prompt (HP) and ITI-GEN prompt for the tSA Smiling.  It highlights three key observations that demonstrate abnormalities in the ITI-GEN prompt's cross-attention maps, particularly in the early stages of the denoising process, leading to degraded global structures in the generated image.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_27_1.jpg", "caption": "Figure 15: Cross-attention maps during the denoising process with HP (left) and ITI-GEN (right) prompts. tSA=Gray Hair.", "description": "This figure visualizes cross-attention maps during the denoising process for a single sample generation, comparing the behavior of Hard Prompts (HP) and ITI-GEN prompts. Each row represents a denoising step, showing the attention maps for each token in the prompt. The images at the bottom show the generated images at each step.  The figure aims to illustrate how ITI-GEN prompts differ from HPs in their attention patterns, specifically highlighting potential abnormalities in ITI-GEN's attention mechanisms, especially in the early stages of the generation process, which could lead to degraded sample quality.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_28_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for both HP and ITI-GEN prompts. Three key observations are highlighted: ITI-GEN tokens show abnormal activities; non-tSA tokens are abnormally active with ITI-GEN tokens; ITI-GEN tokens degrade the global structure in the early denoising steps. ", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_29_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares cross-attention maps during the denoising process for both hard prompts (HP) and ITI-GEN prompts. The three key observations highlighted are that ITI-GEN tokens show abnormal activities; non-tSA tokens are abnormally more active in the presence of ITI-GEN tokens; and the global image structure is degraded in the early denoising steps due to ITI-GEN tokens.  The analysis reveals abnormalities in cross-attention maps of the learned prompts, especially in the early denoising steps, that result in synthesizing improper global structures.", "section": "Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_30_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares cross-attention maps during the denoising process for a sample image generated using Hard Prompts (HP) and Inclusive T2I Generation (ITI-GEN) methods.  It highlights abnormalities in ITI-GEN's cross-attention maps, particularly in the early stages, leading to degraded global image structure and lower quality samples. The observations are supported by highlighting three key issues in ITI-GEN\u2019s attention maps.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_31_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for both HP and ITI-GEN prompts, highlighting the abnormalities in ITI-GEN's attention patterns.  Key observations include abnormal activity in ITI-GEN tokens, higher activity of non-tSA tokens, and degradation of global structure in early steps.  The comparison helps to explain how ITI-GEN's distorted prompts negatively impact the image generation process.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_32_1.jpg", "caption": "Figure 22: Histograms for cross-attention analysis in prompt switching experiments I2H and H2I: tSA = Chubby.", "description": "The figure presents histograms visualizing the results of a cross-attention analysis comparing hard prompts (HP) and ITI-GEN prompts for the tSA (Target Sensitive Attribute) \"Chubby\". The analysis is performed within the framework of prompt switching experiments, specifically I2H and H2I, which involve switching prompts during the denoising process. The histograms illustrate the distribution of the amplitude and central moment of cross-attention maps for non-tSA tokens (\"of\", \"a\") and tSA tokens. The purpose is to quantitatively demonstrate abnormalities observed in the cross-attention maps, especially in the early steps of the denoising process when using ITI-GEN prompts, which contribute to the degraded quality of generated images.  The comparison highlights differences between the HP and ITI-GEN approaches, showcasing the impact of distorted prompts learned by ITI-GEN on the image generation process.", "section": "A.2 Cross-attention analysis"}, {"figure_path": "3MW44iNdrD/figures/figures_32_2.jpg", "caption": "Figure 22: Histograms for cross-attention analysis in prompt switching experiments I2H and H2I: tSA = Chubby.", "description": "This figure presents histograms that visualize the results of a quantitative analysis performed on cross-attention maps.  The analysis is part of a prompt switching experiment (I2H and H2I) designed to investigate the effects of distorted tokens generated by the ITI-GEN method on the image generation process of a text-to-image model. The histograms show the distribution of two metrics:  amplitude and central moment. These metrics quantify the abnormality of attention maps, specifically focusing on the activities of non-tSA tokens (like \u201cof\u201d and \u201ca\u201d) and tSA tokens, respectively, during the denoising process. This analysis aims to understand how ITI-GEN's learning objective may lead to distortion of learned tokens and ultimately impact image generation quality.", "section": "A.2 Cross-attention analysis"}, {"figure_path": "3MW44iNdrD/figures/figures_32_3.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating text-to-image samples: hard prompts (HP), Inclusive T2I Generation (ITI-GEN), and the proposed FairQueue.  It shows that HP performs best for unambiguous attributes, but ITI-GEN suffers from quality degradation, impacting fairness.  FairQueue achieves competitive performance to HP in both quality and fairness, especially for those attributes that have less linguistic ambiguity.", "section": "3 A Closer Look at Prompt Learning for Fair Text-to-Image Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_33_1.jpg", "caption": "Figure 3: Comparison of cross-attention maps during the denoising process with HP (left) and ITI-GEN (right). Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of l = 50 steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: \u2460 ITI-GEN tokens S\u2081 have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. \u2461 non-tSA tokens like \u201cof\u201d and \u201ca\u201d are abnormally more active in the presence of ITI-GEN tokens. \u2462 As compared to the HP counterpart, issues created by ITI-GEN tokens (\u2460 & 2) degrade the global structure in the early denoising steps (e.g., Step 15), for example, human face in HP vs some unrelated structure in ITI-GEN. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).", "description": "This figure compares the cross-attention maps during the denoising process for hard prompts (HP) and ITI-GEN prompts.  The comparison highlights abnormalities in ITI-GEN's attention maps, showing that ITI-GEN tokens have abnormal activities compared to HP tokens and degrade the global structure, especially in the early denoising steps. Non-tSA tokens like \"of\" and \"a\" are also abnormally more active with ITI-GEN prompts.", "section": "3.2 Analyzing the Effect of ITI-GEN Prompts in T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_34_1.jpg", "caption": "Figure 26: Illustration of FairQueue samples utilizing different attention amplification scaling factors (c), tSA=Smiling. In each row, we utilize the same seed and prompt queuing transition point.", "description": "This figure shows the effect of changing the attention amplification scaling factor (c) in the FairQueue model.  Multiple rows of images are shown, each row representing a different scaling factor, and each column using a different seed/transition point for the same prompt. The changes to c showcase the impact of this hyperparameter on the degree to which the subject of the prompt is expressed in the resulting image.", "section": "A.3 More on Ablation Studies"}, {"figure_path": "3MW44iNdrD/figures/figures_34_2.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating images based on text prompts: Hard Prompts (HP), Inclusive T2I Generation (ITI-GEN), and the proposed FairQueue.  It shows sample images and quantitative metrics (Fairness Discrepancy (FD), Text-Alignment (TA), Fr\u00e9chet Inception Distance (FID), and DreamSim (DS)) for two target sensitive attributes (tSAs) with minimal linguistic ambiguity. The results demonstrate that FairQueue achieves competitive fairness and high image quality, outperforming ITI-GEN and matching the performance of HP (for unambiguous tSAs).", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_1.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating images based on text prompts: hard prompts (HP), Inclusive T2I Generation (ITI-GEN), and the proposed FairQueue.  The comparison is done using metrics such as fairness discrepancy (FD), text alignment (TA), Fr\u00e9chet Inception Distance (FID), and DreamSim (DS).  The results show that HP achieves the best performance for unambiguous sensitive attributes but is limited in scope. ITI-GEN improves fairness but degrades image quality. The proposed FairQueue provides a good balance between quality and fairness, often outperforming the other methods.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_2.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and cannot be used for general fair T2I generation purposes, as it cannot be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods (Hard Prompts, ITI-GEN, and FairQueue) for generating images based on different sensitive attributes (Smiling, High Cheekbones). Hard Prompts achieve high quality and fairness but are limited to unambiguous attributes. ITI-GEN improves fairness but degrades image quality. FairQueue achieves comparable performance to Hard Prompts, enhancing both quality and fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_3.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "The figure compares the performance of three methods (Hard Prompt, ITI-GEN, and FairQueue) for generating images based on text prompts that include sensitive attributes.  It shows that Hard Prompt performs well on unambiguous attributes but poorly on others, ITI-GEN has quality issues, while FairQueue provides both better quality and better fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_4.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating text-to-image (T2I) samples: Hard Prompts (HP), Inclusive T2I Generation (ITI-GEN), and the proposed FairQueue.  The comparison focuses on target sensitive attributes (TSAs) with minimal linguistic ambiguity.  The results show that HP produces high-quality, fair images, but only for unambiguous TSAs; ITI-GEN shows moderate sample quality degradation affecting fairness; and FairQueue achieves comparable or even better performance than HP.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_5.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and cannot be used for general fair T2I generation purposes, as it cannot be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the image generation results of three methods: HP, ITI-GEN, and FairQueue.  It uses two sensitive attributes (smiling and high cheekbones) that have minimal linguistic ambiguity. HP shows high quality and fairness but only works for unambiguous attributes. ITI-GEN shows degraded quality compared to HP, while FairQueue achieves competitive fairness and even surpasses HP's quality in many cases.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_6.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating images from text prompts: Hard Prompts (HP), Inclusive T2I Generation (ITI-GEN), and the proposed FairQueue.  It shows that HP produces the highest-quality images with good fairness, but only works for unambiguous attributes. ITI-GEN improves fairness but reduces image quality. FairQueue achieves comparable quality to HP while maintaining competitive fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_36_7.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods for generating images: HP (Hard Prompts), ITI-GEN (Inclusive T2I Generation), and FairQueue (the proposed method).  It shows sample images generated by each method for two different sensitive attributes (Smiling and High Cheekbones) and includes quantitative metrics like Fairness Discrepancy (FD), Text Alignment (TA), Fr\u00e9chet Inception Distance (FID), and DreamSim (DS) to evaluate fairness, quality, and semantic preservation. The results demonstrate that ITI-GEN degrades image quality, while FairQueue achieves comparable or superior performance to HP, especially for attributes with linguistic ambiguity.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_1.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three different methods for generating images from text prompts: Hard Prompts (HP), Inclusive Text-to-Image Generation (ITI-GEN), and the proposed FairQueue method.  The comparison is based on fairness, quality (FID and TA), and semantic preservation (DS) metrics for two target sensitive attributes (smiling and high cheekbones).  The results show that HP has the best performance, but it is limited to unambiguous attributes; ITI-GEN suffers from reduced quality, which also impacts fairness; and FairQueue achieves a balance between the two, outperforming ITI-GEN in many cases.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_2.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "The figure compares the performance of three methods (HP, ITI-GEN, and FairQueue) for generating images based on text prompts, focusing on fairness and quality. HP shows the best performance for unambiguous attributes, while ITI-GEN suffers from quality degradation, and FairQueue demonstrates comparable performance to HP while maintaining fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_3.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods (HP, ITI-GEN, and FairQueue) for generating images of people with different attributes.  The results are shown using quantitative metrics such as Fairness Discrepancy (FD), Text-Alignment (TA), Fr\u00e9chet Inception Distance (FID), and DreamSim (DS). The figure shows that FairQueue achieves the best balance between fairness and image quality. It also demonstrates the limitation of using Hard Prompts (HP) which works only for attributes with minimal linguistic ambiguity.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_4.jpg", "caption": "Figure 1: Our work re-visits SOTA fair T2I generation, ITI-GEN. We question ITI-GEN's central idea of prompt learning via alignment between the directions of prompt embeddings and reference image embeddings. (a) We observe degradation in images generated through ITI-GEN's learned prompts. We note that the direction of reference image embeddings could include unrelated concepts beyond tSA differences (e.g., variations in accessories) resulting in learning of distorted prompts using ITI-GEN. Furthermore, we observe misalignment between the direction of credible hard prompts and that of reference images/learned prompts. (b) As our main contribution and to further understand how these distorted prompts affect the image generation process, we deep dive into the denoising network and analyze the cross-attention maps, revealing their abnormalities e.g., higher activity for maps associated with non-tSA tokens (\u201cof\u201d, \u201ca\u201d). We examine the degraded global structures resulting from these distorted prompts in the early denoising steps. (c) Building on insights from our analysis, we propose a solution to address distorted prompts while maintaining competitive fairness. Our solution FairQueue includes two ideas: prompt queuing and attention amplification. Er and E\u2081 are CLIP text and image encoder resp. [20]. T, F, P are the base prompt, hard prompt with minimal linguistic ambiguity, and ITI-GEN prompt, resp.", "description": "The figure shows the process of re-visiting SOTA fair text-to-image generation (ITI-GEN). It shows the observation of degraded images generated by ITI-GEN's learned prompts, analyzes the abnormalities in cross-attention maps and proposes a solution (FairQueue) to address the quality issue. ", "section": "1 Introduction"}, {"figure_path": "3MW44iNdrD/figures/figures_37_5.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "The figure compares the performance of three different methods (HP, ITI-GEN, and FairQueue) for generating images based on text prompts related to sensitive attributes (SAs).  HP performs well in terms of fairness and quality but only for unambiguous SAs.  ITI-GEN shows degraded image quality, while FairQueue achieves comparable or even better performance than HP for various SAs, successfully balancing quality and fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_6.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods (Hard Prompts, ITI-GEN, and FairQueue) for generating images based on different sensitive attributes (tSAs).  It shows that Hard Prompts perform best for unambiguous tSAs, while ITI-GEN suffers from degraded quality and FairQueue offers a good compromise between quality and fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_7.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the image generation quality and fairness of three different methods: Hard Prompts (HP), ITI-GEN, and the proposed FairQueue.  The results show that HP performs best overall for unambiguous tSAs but is limited in its applicability; ITI-GEN suffers from decreased quality; and FairQueue achieves comparable or superior performance to HP while maintaining fairness.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_37_8.jpg", "caption": "Figure 2: T2I generation performance of HP, ITI-GEN [16], and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities. Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-GEN moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).", "description": "This figure compares the performance of three methods (HP, ITI-GEN, and FairQueue) for generating images based on text prompts that include sensitive attributes.  It shows that HP generates high-quality images but has limited applicability.  ITI-GEN, while improving fairness, sacrifices image quality.  FairQueue achieves comparable or better image quality than HP while maintaining high fairness, suggesting it's the superior approach.", "section": "3.1 Limitations of Prompt Learning for Fair T2I Generation"}, {"figure_path": "3MW44iNdrD/figures/figures_41_1.jpg", "caption": "Figure 43: PCA Analysis on CLIP text embedding for ITI-GEN, well-defined HP and Base Prompt. Utilizing a pre-trained CLIP text encoder we attain the text-embedding for the i) Base Prompt T=\"A headshot of a person\", ii) ITI-GEN tokens, and iii) selected well-defined Hard Prompts for SA\u2208 {Gender (Male), Young}. Then we apply Principle Component Analysis (PCA) for dimensional reduction. We remark that these same text embeddings are later used in the SDM for sample generation.", "description": "This figure shows the PCA analysis of CLIP text embeddings for ITI-GEN, well-defined hard prompts (HP), and base prompts (T).  The analysis visualizes the embeddings in a 2D space, revealing the relationships between different prompt types for the sensitive attributes (SAs) Gender (Male), Young, and Smiling.  It highlights how ITI-GEN embeddings differ from the base and HP embeddings, suggesting the inclusion of unrelated concepts and potential causes of quality degradation in ITI-GEN image generation.", "section": "C Limitations and Broader Impact"}]