[{"figure_path": "FbUSCraXEB/figures/figures_0_1.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "This figure shows the performance of various attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  The x-axis represents different types of attacks (None, DC, UE, AR, NTGA, SN, OPS), while the y-axis shows the accuracy. The bars are grouped by learning method (SL and SimCLR, a contrastive learning method). It illustrates that most attacks are effective against SL but not against CL, highlighting a potential weakness of using only SL-based availability attacks for data protection.", "section": "Introduction"}, {"figure_path": "FbUSCraXEB/figures/figures_1_1.jpg", "caption": "Figure 2: Illustration of our proposed method. Separated by a vertical dashed line, the left side shows the process of generating the poisoning attack, while the right side depicts the training process on the poisoned dataset. On the generation side, above the horizontal dashed line are the existing methods based on contrastive error minimization, while below the dashed line are our proposed methods based on supervised error minimization/maximization (the blue flow). Our attack leverages the stronger contrastive augmentations to obtain effectiveness against both supervised learning and contrastive learning algorithms. Label information is involved in both our method and CL-based methods.", "description": "This figure illustrates the proposed method of generating poisoning attacks against both supervised and contrastive learning.  It shows a comparison between existing CL-based poisoning methods (top) and the proposed SL-based methods (bottom). The key difference is the use of stronger contrastive augmentations in the SL-based approach. The left side illustrates the attack generation process, while the right side demonstrates the training process on the poisoned data, showing how the attacks render models unusable.", "section": "4 Method"}, {"figure_path": "FbUSCraXEB/figures/figures_1_2.jpg", "caption": "Figure 3: Attack performance of our methods on ImageNet-100.", "description": "This figure shows the performance of the proposed AUE and AAP attacks against various supervised and contrastive learning algorithms on the ImageNet-100 dataset.  It compares the accuracy achieved by clean data against the accuracy after applying the attacks, demonstrating the effectiveness of the proposed methods in reducing the accuracy of both supervised and contrastive learning models.", "section": "5 Experiments"}, {"figure_path": "FbUSCraXEB/figures/figures_4_1.jpg", "caption": "Figure 4: InfoNCE loss decreases with CE loss.", "description": "This figure shows the relationship between cross-entropy loss (CE loss) and InfoNCE loss during the training of a supervised ResNet-18 classifier on CIFAR-10 using contrastive augmentations.  The plot demonstrates that as the cross-entropy loss decreases (indicating improved model performance), the InfoNCE loss also decreases. This observation suggests that using strong contrastive augmentations in a supervised learning framework can implicitly optimize the contrastive loss, mimicking the behavior of contrastive learning.", "section": "4.1 Mimic Contrastive Learning with Supervised Models"}, {"figure_path": "FbUSCraXEB/figures/figures_5_1.jpg", "caption": "Figure 5: (a) Contrastive losses during SimCLR training under UE and AUE attacks. (b) Alignment and uniformity gaps during the SimCLR training on CIFAR-10 poisoned by our AUE attack.", "description": "This figure shows two subfigures. Subfigure (a) presents the contrastive losses during SimCLR training when using the UE and AUE attacks on CIFAR-10. Subfigure (b) illustrates the alignment and uniformity gaps observed during the SimCLR training on a CIFAR-10 dataset that was poisoned using the AUE attack.  The comparison of contrastive losses and the alignment/uniformity gap between UE and AUE attacks highlights the effectiveness of AUE in deceiving contrastive learning algorithms by significantly reducing the contrastive loss and increasing the gap between the poisoned and clean data.", "section": "4.2 Augmented Unlearnable Examples (AUE)"}, {"figure_path": "FbUSCraXEB/figures/figures_7_1.jpg", "caption": "Figure 6: Time consumption of poisoning generation.", "description": "This bar chart displays the time required for generating poisoning attacks using different methods. The methods are CP, TP, TUE, AUE, and AAP.  The chart shows that AUE and AAP are significantly faster than CP, TP, and TUE.", "section": "5.3 Efficiency of Poisoning Generation"}, {"figure_path": "FbUSCraXEB/figures/figures_14_1.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "The figure shows the attack performance of different availability attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  It demonstrates that most existing attacks are ineffective against both SL and CL simultaneously, highlighting a potential vulnerability in data protection strategies.", "section": "1 Introduction"}, {"figure_path": "FbUSCraXEB/figures/figures_15_1.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "This figure shows the performance of various attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  It illustrates that many existing attacks are effective against SL but fail against CL, highlighting a vulnerability in the use of availability attacks for data protection.  The graph shows the accuracy remaining after different attacks are applied, comparing the results for SL and CL models.", "section": "1 Introduction"}, {"figure_path": "FbUSCraXEB/figures/figures_16_1.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "The figure shows the performance comparison of various attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  It illustrates that most existing attacks are ineffective against both SL and CL simultaneously, highlighting a security risk in data protection using availability attacks. The x-axis represents different types of attacks, and the y-axis represents the accuracy. We can see that the clean data achieves a high accuracy in both SL and CL, while most attacks only slightly decrease the accuracy of SL.  Only a few attacks, such as AP and SEP, show a substantial reduction in the accuracy of both SL and CL.", "section": "1 Introduction"}, {"figure_path": "FbUSCraXEB/figures/figures_16_2.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "This figure shows the performance of various availability attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  The x-axis represents different attacks, while the y-axis shows the accuracy.  It highlights that most existing attacks are ineffective against CL, even when SL algorithms have failed, indicating a vulnerability in data protection using only SL-based availability attacks.", "section": "1 Introduction"}, {"figure_path": "FbUSCraXEB/figures/figures_18_1.jpg", "caption": "Figure 1: Attacks against SL and CL on CIFAR-10.", "description": "This figure shows the performance of various availability attacks against supervised learning (SL) and contrastive learning (CL) algorithms on the CIFAR-10 dataset.  It highlights that most existing attacks are ineffective against both SL and CL simultaneously, demonstrating the need for new approaches that consider both learning paradigms. The attacks are compared against a baseline of no attack, showing a significant reduction in accuracy for several attacks.", "section": "1 Introduction"}]