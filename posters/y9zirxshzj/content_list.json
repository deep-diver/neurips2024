[{"type": "text", "text": "Causal Discovery from Event Sequences by Local Cause-Effect Attribution ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Joscha C\u00fcppers\u25e6 CISPA Helmholtz Center for Information Security joscha.cueppers@cispa.de ", "page_idx": 0}, {"type": "text", "text": "Sascha Xu\u25e6 CISPA Helmholtz Center for Information Security sascha.xu@cispa.de ", "page_idx": 0}, {"type": "text", "text": "Ahmed Musa\u2022   \nInstitute for Structural Analysis TU Dresden   \nahmed.musa@tu-dresden.de   \nJilles Vreeken   \nCISPA Helmholtz Center   \nfor Information Security   \njv@cispa.de ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects. ", "page_idx": 0}, {"type": "text", "text": "We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the CASCADE algorithm, which adds edges in topological order. Extensive evaluation shows that CASCADE outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Suppose we are considering a multivariate event sequence. What caused a specific event to happen? Which variables are causes of each other? Data-driven methods can infer causal relationships from observed data. Existing methods for discovering causal networks from event sequence data [1\u20133] are based on Granger causality [4]. This purely predictive notion defines a variable $X$ to be a cause of another variable $Y$ if the past of $X$ helps to predict the future $Y$ . It is a relatively weak notion of causality that excludes instantaneous effects and is often unable to discover true causal dependencies; in Granger causality, baking a cake is causal to a birthday. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we instead build upon Pearl\u2019s model of causality, which assumes the existence of an an underlying causal structure in the form of a directed acyclic graph (DAG) [5]. In our context, such a graph describes the causal relationships between types of events, such as alarms in a network. We propose a new causal model for event sequences based on a one-to-one matching of individual events, where we model the process of one individual event of a certain type possibly causing an individual event of another type. In our model, we take into account the uncertainty of whether an event is actually caused or independently generated, the uncertainty of an event actually causing an effect or failing to do so, and the uncertainty of the delay between cause and effect. As we will show, our model has several advantages, such as a clear notion of what event caused another and the identifiability for both instant and non-instant effects. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We base our theory on the Algorithmic Markov Condition (AMC) [6], which postulates that the true causal model achieves the lowest Kolmogorov complexity. As Kolmogorov complexity is not computable, we instantiate it via the Minimum Description Length (MDL) principle [7]. We show that our score is consistent, identifies the true causal direction for both instantaneous and delayed effects, and formally connect it to Hawkes processes. To discover causal networks in practice, we introduce the CASCADE algorithm, which adds edges in topological order. Through extensive empirical evaluation, we show that CASCADE performs well in practice and outperforms the state of the art by a wide margin. On synthetic data, CASCADE recovers the ground truth without reporting spurious edges, and on real-world data, it returns graphs that correspond to existing knowledge. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We write $i\\rightarrow j$ when $S_{i}$ is a cause of $S_{j}$ and $p a(j)$ for the set of parents of node $j$ . We assume faithfulness, sufficiency, and the causal Markov condition [8]. ", "page_idx": 1}, {"type": "text", "text": "Information-Theoretic Causal Discovery The Algorithmic Markov Condition (AMC) postulates that the factorization of the joint distribution according to the true causal network achieves the lowest Kolmogorov complexity [6]. The Kolmogorov complexity $K(x)$ of a binary string $x$ is the length of the shortest program $p$ for a universal Turing machine $\\boldsymbol{\\mathcal{U}}$ that computes $x$ and halts [9]. For a distribution $P$ , it is the length of the shortest program that uniformly approximates $P$ arbitrarily well, ", "page_idx": 1}, {"type": "equation", "text": "$$\nK(P)=\\operatorname*{min}_{p\\in0,1^{*}}\\{|p|:\\forall_{y}|\\mathcal{U}(p,y,q)-P(y)|\\leq\\frac{1}{q}\\}\\;.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The AMC states that the Kolmogorov complexity of the joint distribution $P(X)$ is the sum of the complexities of the conditional distributions $P(X_{i}|p a(i))$ of the true DAG $G^{*}$ , i.e. ", "page_idx": 1}, {"type": "equation", "text": "$$\nK(P(X))=\\sum_{i=1}^{p}K\\left(P(X_{i}|p a(i))\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "up to a constant independent of the input. Due to, among others, the halting problem, Kolmogorov complexity is not computable, but we can approximate it from above. A statistically well-founded way to do so is by Minimum Description Length (MDL) [7, 10]. For a fixed class of models $\\mathcal{H}$ , MDL identifies a description length $L$ of encoding data $X$ together with its optimal model, ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(X\\mid\\mathcal{H})=\\operatorname*{min}_{h\\in\\mathcal{H}}\\left(L(X\\mid h)+L(h)\\right)~.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Next, we introduce the assumed data generating process, its corresponding model class $\\mathcal{H}$ and encoding length function $L$ , and show under which conditions it can be identified. ", "page_idx": 1}, {"type": "text", "text": "3 Theory ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To be able to infer causal relationships from observational data, we need to make assumptions about the underlying data-generating process [5]. The key assumption we make here is that an individual event of type $i$ at time $t$ with probability $\\alpha_{i,j}$ causes an individual event of type $j$ at time $t^{\\prime}\\geq t$ . To illustrate, we give a toy example in Fig. 1 in which event sequence $S_{i}$ causes event sequence $S_{j}$ . The individual events in $S_{i}$ occur uniformly at random. The first and third events in $S_{i}$ cause events in $S_{j}$ , resp. with a delay of 0.2 and 0.3. The other two events in $S_{i}$ do not cause events in $S_{j}$ , denoted by a delay of $\\infty$ . The final event in $S_{j}$ is due to noise, marked by $N_{j}$ . ", "page_idx": 1}, {"type": "image", "img_path": "y9zIRxshzj/tmp/7cb0255d1f2909c180e5f837b0e2e47e59c557ad955e8dcdadca406d6824effa.jpg", "img_caption": ["Figure 1: Cause-effect matching, where $S_{i}$ causes $S_{j}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Next, we formally describe the causal mechanism. We differentiate between source and effect nodes. ", "page_idx": 2}, {"type": "text", "text": "Source nodes are nodes $i$ in $G^{*}$ with an empty parent set $p a(i)$ . For source nodes $i$ we assume that the events in $S_{i}$ occur uniformly at random with a rate of $\\lambda_{i}$ events per time unit. This mechanism, commonly known as a homogeneous Poisson process, is used, for example, as a model for accident rates requiring hospital admission [11]. In this work, we focus on the delay times between individual events, denoted as $d_{k}$ for the delay $t_{k}-t_{k-1}$ and as $\\Delta_{i\\rightarrow i}\\,=\\,\\{d_{k}\\}_{k=1}^{n_{i}}$ for the sequence. For a Poisson process, the delay times are independently and exponentially distributed. Thus, we model a source event sequence $S_{i}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{i}=\\{t_{k}\\}_{k=1}^{n_{i}}\\,,\\,\\mathrm{where}\\,t_{k}=\\sum_{l=1}^{k}d_{l},\\quad\\Delta_{i\\to i}=\\{d_{k}\\sim\\mathrm{Exp}(\\lambda_{i})\\,i i d\\}_{k=1}^{n_{i}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Effect nodes are nodes $j$ in $G^{*}$ with at least one parent $p a(j)$ . For each effect node $j$ , the individual events in $S_{j}$ are either caused by an individual event in an $S_{i}$ with $i\\in p a(j)$ or due to noise. That is, reasoning from the causing node $i$ , every event $t_{k}\\,\\in\\,S_{i}$ may trigger an event of the effect $S_{j}$ with a probability of $\\alpha_{i,j}$ . If triggered, an individual event in $S_{j}$ will occur after a random delay $d_{k}$ , drawn from a cause-effect specific delay distribution $\\Phi_{i,j}$ parameterized by $\\theta_{i,j}$ , e.g. the rate $\\lambda$ of an exponential distribution, and $\\alpha_{i,j}$ . If no event is triggered, then we model the delay as infinite, i.e. $d_{k}=\\infty$ . The sequence of delays $\\Delta_{i\\to j}$ from $S_{i}\\to S_{j}$ is modeled as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta_{i\\rightarrow j}=\\{d_{k}\\}_{k=1}^{n_{i}}\\,,\\qquad d_{k}\\sim\\Phi_{i,j}(\\alpha_{i,j},\\theta_{i,j})\\,i i d\\,,\\qquad\\phi_{i,j}(d)=\\left\\{\\begin{array}{l l}{1-\\alpha_{i,j}}&{\\mathrm{if~}d=\\infty}\\\\ {p(d;\\theta_{i,j})\\cdot\\alpha_{i,j}}&{\\mathrm{else~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi_{i,j}(d)$ denotes the density of the delay distribution. Thus, given the event sequence $S_{i}$ of the cause and delays $\\Delta_{i\\to j}$ , the individual events in $S_{j}$ caused by $S_{i}$ are obtained by adding the delays $d_{k}$ to the time stamps $t_{k}$ of the individual cause events, with the reconstruction function $f$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{i,j}(S_{i},\\Delta_{i\\rightarrow j})=\\{t_{k}+d_{k}\\ |\\ d_{k}\\neq\\infty\\},\\ \\mathrm{for}\\ k=1,\\dots,n_{i}\\ .\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In addition, individual events in $S_{j}$ can also be due to noise. Like for source nodes, we assume these a Poisson process as per Eq. (1) with rate $\\lambda_{j}$ , i.e. $N_{j}\\sim\\mathsf{P o i s s o n}(\\lambda_{j})$ . Putting this together, given causal structure $G^{*}$ , an effect event sequence $S_{j}$ is generated by taking the union of the individual delays from the causal parents $p a(j)$ and the time stamps due to noise $N_{j}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{j}=\\left(\\bigcup_{i\\in p a(j)}f_{i,j}\\left(S_{i},\\Delta_{i\\rightarrow j}\\right)\\right)\\cup N_{j}\\ .\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Next, we instantiate an MDL score for this causal model and consider its identifiability. ", "page_idx": 2}, {"type": "text", "text": "3.1 Minimum Description Length Instantiation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now develop a score for our causal model using MDL [7]. It consists of the cost of the data given the model, $L(S\\mid\\Theta)$ , i.e. the negative log-likelihood of the data, and the cost of the model, i.e. that of the parameters, $L(\\Theta)$ , and that of the graph, $L(G)$ , all measured in bits. ", "page_idx": 2}, {"type": "text", "text": "Data Cost The cost of data in bits directly corresponds to its negative log-likelihood, i.e. the likelihood of each delay as per Eq. (2) over all the event sequences corresponding to the parents of node $j$ and that of the noise events. Formally, we have ", "page_idx": 2}, {"type": "equation", "text": "$$\nL\\left(S_{j}\\mid S_{p a(j)},\\,\\Theta\\right)=\\sum_{i\\in p a(j)}\\sum_{d_{k}\\in\\Delta_{i\\to j}}-\\log(\\phi_{i,j}(d_{k}))+\\sum_{d_{l}\\in\\Delta_{j\\to j}}-\\log\\left(\\phi_{j,j}(d_{l})\\right)\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The first term encodes those events that were caused by the parent $S_{i}$ through the delays $\\Delta_{i\\to j}$ . Here, we use a Shannon-optimal coding that requires $-\\log(\\phi_{i,j}(d_{k}))$ bits per sample [7]. In the second term, we encode all remaining events as noise using the delay distribution of a Poisson process. For source events, i.e. variables without any parents, only the noise term is present. ", "page_idx": 2}, {"type": "text", "text": "The cost of all sequences is then simply $\\begin{array}{r}{L(S\\mid G,\\Theta)=\\sum_{j\\in[p]}L(S_{j}\\mid S_{p a(j)},\\Theta).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Parameter Cost Next, we define the costs of the DAG, $L(G)$ , and that of the parameters, $L(\\Theta)$ . We encode the DAG in topological order. Per node we encode its number of parents $|p a(i)|$ and identify which those are, i.e. $\\begin{array}{r}{L(G)=\\sum_{k=0}^{d-1}\\left(\\log(k)+\\log\\binom{k}{|p a(i)|}\\right)}\\end{array}$ . Depending on their type, we encode the parameters . For parameters ${\\dot{\\boldsymbol{\\theta}}}\\in\\mathbb{N}$ we use $L_{\\mathbb{N}}$ , the MDL-optimal encoding for integers [12]. It is defined as $L_{\\mathbb{N}}(z)=\\log^{*}z+\\log c_{0}$ where $\\log^{*}z$ is the expansion, $\\log z+\\log\\log z+\\cdot\\cdot\\cdot$ in which we only include positive terms. To ensure this is a valid encoding, i.e. one that satisfies the Kraft inequality, we set $c_{0}=2.865064$ [12]. For parameters $\\theta\\in\\mathbb{R}$ we use $L_{\\mathbb{R}}(\\theta)=L_{\\mathbb{N}}(d)+L_{\\mathbb{N}}(\\lceil\\theta\\,\\cdot\\,$ $10^{d}7\\,\\mathrm{)}+1$ as the number of bits needed to encode a real number up to a user-specified precision [7]. For an edge $i\\rightarrow j$ , the parameters are the trigger probability $\\alpha_{i,j}$ and those of the delay distribution $\\phi$ . For the cost of an edge we hence have $\\begin{array}{r}{L(\\bar{i}\\stackrel{\\cdot}{\\rightarrow}\\bar{j})=L_{\\mathbb{R}}(\\bar{\\alpha}_{i,j})^{\\circ}\\!\\!+\\sum_{\\theta\\in\\phi_{i,j}}L(\\theta)}\\end{array}$ . For $\\Theta$ as a whole, we have $\\begin{array}{r}{L(\\Theta)=\\sum_{i\\to j\\in G}L(i\\to j)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "The overall MDL score is then ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(S\\mid G,\\Theta)+L(G)+L(\\Theta)\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Identifiability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now study the identifiability guarantees of our model and score, i.e. under what conditions we can identify from a given pair which is the cause and which the effect. Consider a pair of event sequences $S_{i}$ and $S_{j}$ , where $S_{i}\\to S_{j}$ and the cause $S_{i}$ is a source event while $S_{j}$ is an effect event. ", "page_idx": 3}, {"type": "text", "text": "Instant Effects We begin with the case of instant effects only. Instant effects are observed when the sampling frequency of the data, e.g. a daily time scale, is insufficient to pick up a difference in time, such as a financial crash that can spread across the globe within hours. It is well-known that Granger causality cannot identify the causal direction for instant effects [13]. In Pearl\u2019s causal framework, on the other hand, the causal direction between two binary variables is identifiable [14\u201316]. We can build upon these results and show that our causal model and MDL-based score can identify the causal direction for non-deterministic instant effects. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $S_{i}$ be an event sequence generated by a Poisson process as per Eq. (1) and $S_{j}$ be an effect of $S_{i}$ as per Eq. (3), with, low noise $\\lambda_{j}<(1-\\alpha_{i,j})\\lambda_{i},$ , and a trigger probability $\\alpha_{i,j}<1$ . In the case of exclusively instant effects, i.e. $\\phi_{i,j}(d)=\\delta(d)$ , where $\\delta(d)$ is the Dirac delta function, the MDL score in the true causal direction is lower than in the anti-causal direction, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n_{i}\\to\\infty}L(S_{j}\\mid S_{i},\\Theta_{1})+L(S_{i}\\mid\\Theta_{1})<L(S_{i}\\mid S_{j},\\Theta_{2})+L(S_{j}\\mid\\Theta_{2})\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We provide the full proof in the Appx. A.1, the general idea is under a non-deterministic trigger mechanism, i.e. $\\alpha_{i,j}<1$ . Then, in the causal direction, we can fully explain $S_{j}$ with $S_{i}$ , but not vice-versa, as the cause is generated by a Poisson process. If $\\alpha_{i,j}=1$ , i.e. the process is deterministic, we always observe cause and effect together, making them indistinguishable. ", "page_idx": 3}, {"type": "text", "text": "Delayed Effects Next, we consider the case of exclusively delayed effects. Here, there is an inherent asymmetry in the benefit of knowing the cause versus the effect. As shown by Didelez [17] for marked point processes, and later used by $\\mathrm{Xu}$ et al. [1], Eichler et al. [18] for Granger causality in Hawkes processes, the intensity of observing the cause after an event of the effect is unchanged. That is, the future of the cause is independent of the past of the effect, while if a cause triggers an effect, the intensity of the effect is increased by the cause. We have the following identifiability guarantee. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Let $S_{i}$ be an event sequence generated by a Poisson process as per Eq. (1) and $S_{j}$ be an effect of $S_{i}$ as per Eq. (3), such that $H\\left(\\phi_{j,j}\\right)>H(p(;\\theta_{i,j}))+\\alpha_{i,j}^{-1}H\\left(\\mathcal{B}(\\alpha_{i,j})\\right)+\\alpha_{j,j}^{-1}H\\left(\\mathcal{B}(\\alpha_{j,j})\\right),$ , where $H$ denotes the entropy and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ the Bernoulli distribution. ", "page_idx": 3}, {"type": "text", "text": "Then the matching in the anti-causal direction $\\Delta_{j\\to i}$ of the effect $S_{j}$ to the cause $S_{i}$ has a worse MDL score than the true matching $\\Delta_{i\\to j}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(S_{j}\\mid S_{i},\\Theta_{i\\to j})+L(S_{i}\\mid\\Theta_{i})<L(S_{i}\\mid S_{j},\\Theta_{j\\to i})+L(S_{j}\\mid\\Theta_{j})\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We provide the full proof in the Appx. A.2. In the anti-causal direction $S_{j}\\rightarrow S_{i}$ , the delay times follow the same exponential distribution of $\\mathrm{Exp}(\\lambda_{i})$ , leading to no gain in score compared to the self-delay encoding. On the other hand, in the true causal direction, knowing the times of the cause leads to a better knowledge of the delay and hence a lower cost, so long as the delay distribution $\\phi_{i,j}$ provides a better description than treating it as noise. This requirement is closely related to the algorithmic Markov condition, which postulates that the shortest description of a variable is given through its parents. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Connection to Hawkes Processes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Hawkes processes [19] are analytically convenient and well-suited for modeling real-world processes where events trigger further events, e.g. earthquakes triggering aftershocks. Consequently, the majority of methods focusing on Granger causality are based on Hawkes processes [1\u20133]. The Hawkes process extends the Poisson process by incorporating the influence of past events on the intensity, i.e. the rate of occurrence of future events. This is done by means of excitation functions $v_{i,j}(t-t_{k})$ , which increase/inhibit the intensity of future events based on past events. The intensity function of a Hawkes process under a DAG structure is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{j}(t)=u_{j}+\\sum_{i\\in p a(j)}\\sum_{t_{k}<t,t_{k}\\in S_{i}}v_{i,j}\\left(t-t_{k}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Each event $t_{k}\\in S_{i}$ increases the intensity of seeing an effect by $v_{i,j}(t-t_{k})$ . The main difference between our model and a Hawkes process is our direct trigger model from cause to effect. In a Hawkes process, an event of type $i$ increases the intensity and, therewith, the probability of effect events occurring. That is, contrary to our framework, in a Hawkes process there is no explicit one-to-one relationship between causing and effect events, i.e. no one event can be attributed solely to causing another. Nonetheless, in Appendix A.5 we show how to identify $S_{i}$ as a parent of $S_{j}$ by constructing a sequence of delays $\\Delta_{i\\to j}$ with the most-influential past event and therewith $\\phi_{i,j}$ . If $\\phi_{i,j}$ fulfills Theorem 2, we can identify $S_{i}$ as a parent of $S_{j}$ . Hence, should the data be generated by a Hawkes process, our method can still pick up the causal relationship between the two event classes, so long as there are sufficiently many events where $S_{i}$ is the primary cause. ", "page_idx": 4}, {"type": "text", "text": "4 Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With our model in place, we now turn to the problem of discovering the underlying causal structure from an observed sequence of events. In recent years, several methods that find and proceed on a topological ordering of the true graph have been introduced [20\u201322], which outperform other score-based frameworks such as GES [23] in terms of accuracy. We here propose the CASCADE algorithm that instantiates this idea for information-theoretic scores. We prove that in the limit, it recovers not only the correct topological ordering but also the correct parent set of each node. ", "page_idx": 4}, {"type": "text", "text": "CASCADE derives its guarantees from the gain in bits of adding an edge $i\\rightarrow j$ to the model, i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(i\\rightarrow j\\mid\\Theta)=L(S_{j}\\mid S_{p a(j)},\\Theta)-L(S_{j}\\mid S_{p a(j)\\cup i},\\Theta\\cup\\theta_{i,j})+L(i\\rightarrow j)\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The edge cost $L(i\\rightarrow j)$ is constant and independent of the number of samples $n_{i}$ . In the limit $n_{i}\\to\\infty$ , the gain inherits the identifiability guarantees from Sec. 3.2, such that $g(i\\rightarrow j\\mid\\Theta)>$ $g(j\\rightarrow i\\mid\\Theta)$ if $S_{i}$ is a true ancestor of $S_{j}$ . In other words, the gain of an edge is greater in the causal than in the anti-causal direction. ", "page_idx": 4}, {"type": "text", "text": "4.1 High Level Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "CASCADE initializes the model $\\Theta$ with an empty graph $G$ and without any causal edges. During the search, we maintain a set of nodes $C=[p]$ , from which we remove nodes in a topological order of $G^{*}$ . We iterate over the following four steps until $C$ is empty. ", "page_idx": 4}, {"type": "text", "text": "1. Source Node Selection: Select that node $i\\in C$ with minimal gain for any edge $j\\rightarrow i$ , $j\\in C$ , i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{i\\in C}\\ \\operatorname*{max}_{j\\in C}g(j\\to i\\mid\\Theta)-g(i\\to j\\mid\\Theta)\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2. Edge Adding: Add all outgoing edges from $i\\rightarrow j$ , $j\\in C$ , to $G$ that improve our score.   \n3. Edge Pruning: Remove all incoming edges $j\\rightarrow i$ from $G$ that harm our score. ", "page_idx": 4}, {"type": "text", "text": "Each iteration, CASCADE selects that node $i$ , which has the minimal achievable gain when adding any edge $j\\rightarrow i$ to the current graph $G$ , expressed in Eq. (4); below, we will show that under our causal model this node is guaranteed to be a true source of the graph $G^{*}$ . We then add all edges from $i$ to nodes $j\\in C$ that improve our score; provided that all true causal edges $i\\rightarrow j$ were added, there is now at least one node $j\\in C$ whose parents are all accounted for, that in the next iteration can be identified as a source. We remove edges $j\\rightarrow i$ from $G$ to remove shortcuts. By repeating this process, CASCADE proceeds in a topological order of the true graph $G^{*}$ . In total, CASCADE requires $p$ iterations, leading to an overall cubic complexity $O(p^{3})$ . ", "page_idx": 5}, {"type": "text", "text": "Source Node Selection. To identify a source node in the graph, we can use the identifiability guarantees from Sec. 3.2. They show that the gain $g(i\\rightarrow j\\mid\\bar{\\Theta})$ correctly orients the edge $i\\rightarrow j$ in the unconfounded bi-variate case. We additionally require that the edge gain is pathwise oracle, i.e. it can identify the direction of the path from $i$ to $k$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Given an event sequence $S$ generated by a causal structure $G^{*}$ , let $S_{i}$ be a source node of $G^{*}$ and $S_{v}$ be a descendant of $S_{i}$ , where there exists a path $i\\rightarrow j\\rightarrow\\cdot\\cdot\\cdot\\rightarrow v$ in $G^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Then, the gain in the causal direction of the path $g(i\\rightarrow v\\mid\\Theta)-g(v\\rightarrow i\\mid\\Theta)$ is greater. ", "page_idx": 5}, {"type": "text", "text": "We provide the proof in Appx. A.3. We can now show that the criterion in Eq. (4) selects nodes in a topological ordering of $G^{*}$ . Initially, CASCADE has to identify a true source of $G^{*}$ , i.e. a node $i$ without parents. For that node $i$ , all other nodes $j$ are either ancestors or independent of $i$ . If $i$ is an ancestor of $j$ , then $g(j\\to i\\mid\\Theta)-g(i\\to j\\mid\\Theta)<0.$ , i.e. the gain in the anti-causal direction is lower. If $i$ is independent of $j$ , then $g(j\\to i\\mid\\Theta)=0$ and $g(i\\xrightarrow{}j\\mid\\Theta)=0$ . Hence, the maximum achievable gain for a node without parents is zero. ", "page_idx": 5}, {"type": "text", "text": "Now consider a node $v$ which does have a parent. For this node, there exists an ancestor $u$ which is a true source. Hence, for that pair $g(u\\to v\\mid\\Theta)-g(v\\to u\\mid\\Theta)>0$ . Consequently, the maximum achievable gain is positive, whilst for a source node, we can maximally achieve zero, allowing us to identify true sources with Eq. (4). ", "page_idx": 5}, {"type": "text", "text": "In the next step, we add all outgoing edges from the source $i$ to $G$ that improve the score. As $G^{*}$ is a DAG, we are now guaranteed to have another node $j$ , whose incoming edges are all accounted for in $G$ . Then, as per the causal model from Eq. (3), the only events that remain are those of the noise $N_{j}$ . Hence, $j$ is now a source node for which the guarantees from above apply. By repeating this process, CASCADE thus follows a topological order of $G^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Edge Addition Given a source $i$ , CASCADE adds all outgoing edges $i\\;\\rightarrow\\;j$ that improve the score. We restrict the set to nodes $j\\in C$ from the candidate set only, i.e. to nodes further down the topological order. By the Algorithm Markov Condition, the description length of the true set of parents of a node $j$ is smaller than the description length of any other set of parents, and hence the gain of the true edge is positive in the limit of $n_{i}\\to\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "When adding an edge $i\\;\\rightarrow\\;j$ , where there is already an edge $v~\\rightarrow~j$ , we use an Expectation Maximization approach to attribute all events to their respective cause. That is, we first find the bi-variate alignment $\\Delta_{i\\to j}$ using all events in $S_{j}$ . Now, it is very likely that there are conflicts between $\\Delta_{i\\to j}$ and $\\Delta_{v\\to j}$ , as the same event can be attributed to both $i$ and $v$ . In those cases, we choose that event where the density $\\phi_{i,j}/\\phi_{v,j}$ is higher and set the delay to infinity in the other matching. After re-assigning all events, we refit the delay distribution function $\\phi_{i,j}$ using the new matching. ", "page_idx": 5}, {"type": "text", "text": "Edge Pruning Lastly, we deal with removing any shortcuts that have been added in the previous iteration. With the previous two steps, we are guaranteed to have a superset of all true causal edges incoming to $i$ . Fortunately, we can prune such edges directly with MDL by removing any incoming edge $i\\rightarrow j$ that does not improve the MDL score. In the chain graph $i\\rightarrow j\\rightarrow v$ , we would remove $i\\rightarrow v$ as the edge $j\\rightarrow v$ is sufficient to explain the data. In practice, given the current set of parents of $i$ in $G$ , we search for the true set of parents by starting with the empty set and greedily adding only those edges that improve the score. As we show in Appx. A.3, a shortcut always has a lower gain than the true edge and hence will not be re-added. In this manner, we are asymptotically left with only the true causal parents. We can now finally show the consistency of CASCADE. ", "page_idx": 5}, {"type": "image", "img_path": "y9zIRxshzj/tmp/d5810b7df0a33f914fa567b7894b89dc2a8a9d328615f60aa4d78a261f8dc997.jpg", "img_caption": ["Figure 2: Causal chain "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Given an event sequence $S$ , where each individual subsequence $S_{i}$ was generated as per Eq. (3) by an underlying causal graph $G^{*}$ . Assuming all $\\Delta_{i\\to j}$ are the true causal matchings. Under the Algorithmic Markov Condition, CASCADE recovers the true graph $G^{*}$ for $n\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "We postpone the proof to Appx. A.7. In the experiment section, we show that CASCADE recovers the true DAG even in challenging settings and works well on real-world data. ", "page_idx": 6}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Causal discovery on observational data is an active research topic. Two main research directions exist: constraint-based [5] and score-based [23, 24] methods. Our approach belongs to the latter and is based on the Algorithmic Markov Condition [6]. While Kolmogorov complexity is uncomputable, Marx and Vreeken [10] formally showed that if we instantiate the AMC with two-part MDL [7], we, on expectation, achieve the same results. MDL has been successfully used for bivariate causal inference [25, 15, 26], causal discovery [27], identifying hidden confounding [28], identifying mechanisms shifts [29], and identifying selection bias [30]. ", "page_idx": 6}, {"type": "text", "text": "In this paper, we consider point processes. Particularly close to our method are Hawkes processes [31] as a way to model the influence of past events onto future events. As such, our work is also related to the concept of transfer entropy [32], which measures the influence in terms of Shannon entropy. Budhathoki and Vreeken [33] proposed an MDL-based method for bivariate causal inference on event sequences, which is unsuitable for learning a global causal structure. ", "page_idx": 6}, {"type": "text", "text": "Existing methods for discovering causal graphs from event sequence data focus on different instantiations of Granger causality and can mostly be categorized by different intensity functions. Most common are parametric approaches with different regularizing [34, 2, 1]. ADM4 [34] uses the nuclear matrix norm in combination with lasso, THP [2] uses BIC for regularization. The method MDLH by Jalaldoust et al. [3] is most closely related, as they also use MDL for regularization. NPHC [35] takes a non-parametric approach by using a moment matching method to fit second and third-order integrated cumulants. A recent development is neural point processes. Mei and Eisner [36] propose a deep neural network that learns the dependencies [36], which Xiao et al. [37] extended to include attention mechanisms. Zhang et al. [38] first learn a neural point process and then use a feature importance attribution method to obtain a weight matrix of pairwise variable influence. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate CASCADE on both synthetic and real-world data. CASCADE is implemented in Python. We provide the source code, along with the synthetic data generator and the used real-world datasets online.3 We compare our method to four of state of the art methods: THP [2] as representative for the regularized parametric approaches, CAUSE [38] as representative for the neural point processes and NPHC [35] as a representative non-parametric approach, and MDLH [39] who also rely on MDL, as our most closely related competitor. CAUSE and NPHC do not return a graph but rather a weight matrix where the weight indicates the strength of the causal relation. On synthetic data, we can obtain a graph by thresholding such that we optimize the $F1$ score. ", "page_idx": 6}, {"type": "text", "text": "6.1 Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the estimated graphs in terms of structural similarity by the Structural Hamming Distance (SHD) [40], in terms of causal similarity by the Structural Intervention Distance (SID) [41], and predictive performance by F1 score. To compare graphs of different sizes, we report the scores normalized by the maximally achievable SHD/SID and show the unnormalized scores in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "NPHC, CAUSE, and MDLH can and often do return cyclic graphs. As SID is strictly only defined for acyclic graphs, we omit these methods from the SID evaluation. ", "page_idx": 6}, {"type": "image", "img_path": "y9zIRxshzj/tmp/bf9397748cdd43895373a85ec94befe8c51e5675efb1af32e3f72355fd11784d.jpg", "img_caption": ["Figure 3: DAG recovery in different settings. We show normalized SHD, normalized SID, and $F1$ score, the $Y$ -axis are truncated for better visualization. In (a) we vary the number of event types, on the SID score we observe that the graph reported by CASCADE is casually, the most similar to the true DAG. In (b) we decrease the noise, CASCADE does recover a close causal graph, even under high noise. Finally, in (c) we increase the number of parents of a collider, we observe that a high number of parents does not pose a problem for CASCADE. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Synthetic Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin by comparing all methods on data with known ground truth. To this end, we generate synthetic data. We generate both data within and outside our causal model and vary aspects such as noise intensity, number of event types and the number of parents of a variable. We describe the full data-generating process in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Sanity Check We start with a sanity check on data without any structure over 20 variables, CASCADE correctly does not report any causal edge. THP reports in $45\\%$ of the cases at least one spurious edge. We omit the results of CAUSE and NPHC as it is unclear how to choose a meaningful, non-trivial threshold, in this setting. MDLH did not terminate within 96 hours. ", "page_idx": 7}, {"type": "text", "text": "Scalability We evaluate how well each method scales under an increasing number of variables. We vary the number of nodes, which correspond to the number of unique event types, from 5 to 50 and report the results in Fig. 3a. As MDLH did not terminate within 96 hours for 15 variables, we omit it from here on out. For a lower number of nodes, both CASCADE and THP obtain far better results than NPHC and CAUSE. With increasing event types, all methods SID and F1 scores decrease. Amongst all methods, CASCADE scales best with an increasing number of nodes, whereas Granger causality based methods such as THP and NPHC find many spurious edges of connected but not causal variables. On the other hand, CASCADE is the most accurate method for a higher number of nodes, showing the efficacy of its causal model and MDL-based approach. ", "page_idx": 7}, {"type": "text", "text": "Noise Next, we assess the impact of noise, which are events that is not caused by any parent. To this end, we vary the \u2018cause\u2019 probability and the fraction of events due to additive noise. We do so by varying a noise parameter $a$ , adding an additional $n_{i}\\cdot a$ events to $S_{i}$ (additive noise), and by setting the \u2018cause\u2019 probability $\\alpha=1-a$ , i.e. we decrease additive noise and increase trigger probability. We show the results in Fig. 3b. We observe that CASCADE does quite well for high noise and that for noise levels of $a=0.7$ and lower, it (mostly) recovers the true DAG. All other methods perform considerably worse. ", "page_idx": 7}, {"type": "text", "text": "Colliders Matching an effect event to the correct parent, resp. modeling the correct excitation, becomes increasingly challenging for a larger number of parents. We test this through a setting where half $\\big(\\big lceil\\frac{p-1}{2}\\big\\rceil\\big)$ the variables converge into a collider, and the other half $\\big(\\big\\lfloor\\frac{p-1}{2}\\big\\rfloor\\big)$ are independent. We vary the total number of variables, $p$ and we show the results in Fig. 3c. We observe that CASCADE achieves almost perfect results. THP is robust, but with an increasing number of nodes, it starts to miss edges. Beyond 100 variables, it does not terminate within 24 hours. To validate that our method can recover structures with multiple colliders, we repeat the same experiment where $10\\%$ of nodes are colliders. That is, for 50 event types, 5 are colliders and 23 direct causes of all 5 colliders. The remaining 22 are independent. Resulting in an $F1$ score of 0.97 for 50 event types, slightly decreasing to 0.82 for 200 event types; as such CASCADE can deal well with multiple colliders. ", "page_idx": 8}, {"type": "text", "text": "Instantaneous Effects Next, we evaluate performance under instantaneous effects. First, we consider data with exclusively instant effects. CASCADE achieves an average unnormalized SHD of 32.8. The second best-performing method, NPHC, achieves 46.85. Next, we generate a setting where $90\\%$ of the effects are instantaneous and the others occur with a small delay. CASCADE improves to an SHD of 19.45, while NPHC achieves the second lowest average with 47.5. We provide all results in the Appendix B. ", "page_idx": 8}, {"type": "text", "text": "Hawkes Processes Finally, we evaluate how effectively CASCADE recovers the true DAG on data generated by a Hawkes process. We vary the intensity of the excitation function, i.e., the expected number of events generated per cause. We show the results in Figure 4. We observe that CASCADE performs best when our assumptions hold, when there is one effect per cause or fewer, but still demonstrates strong performance across all settings. ", "page_idx": 8}, {"type": "text", "text": "6.3 Real-World Data ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "y9zIRxshzj/tmp/ea4b2f0f14d0289e25c47f24eb1b19e724fecf3c3fd543528284034888f65a96.jpg", "img_caption": ["Figure 4: DAG recovery on data generated by a Hawkes process. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We evaluate CASCADE on three distinct datasets of real-world event sequences. We begin by evaluating CASCADE on a dataset of network alarms, where the causal structure is known. ", "page_idx": 8}, {"type": "text", "text": "Network Alarms This data was provided by Huawei for the NeurIPS 2023 CSL-competition4 and consists of data from a simulated network of devices in which alarms can cause other alarms. We run all methods and get an (unnormalized) SHD score of 42 for CASCADE, 127 for THP, 214 for NPHC, and 1564 for CAUSE. As the network connectivity structure is known, we can take it into account during the search. THP supports this natively, CASCADE can be trivially constrained to only consider the given edges. CASCADE correctly identifies 142 out of 147 causal edges, THP 20. Neither method reports spurious edges. We show the full recovered graph in Appendix B.5. ", "page_idx": 8}, {"type": "text", "text": "Global Banks Second, we run CASCADE on a daily return volatility dataset [42], we follow the preprocessing of Jalaldoust et al. [39], specifically we turn the time series into an event sequence by rolling a one year window over the data and register an event if the last value is among the top $10\\%$ . The dataset includes the 96 world\u2019s largest publicly traded banks. We show the largest discovered subgraph in Fig. 5. In addition, three unconnected sub-graphs are discovered, one covering two banks in Australia and two others connecting banks in Japan, which we provide in Appendix B.5. ", "page_idx": 8}, {"type": "text", "text": "Daily Activities We run CASCADE on a dataset of recorded daily activities [43]. Our method reports plausible causal connections such as Sleeping $E n d\\rightarrow$ Showering Start $\\rightarrow$ Showering $E n d\\rightarrow$ Breakfast Start $\\rightarrow$ Breakfast End, etc. We show the complete graph in the Appendix B.5. This result reinforces the suitability of our causal model and CASCADE for real-world data, and illustrates the potential of our method to discover causal structures in a wide range of applications. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We studied the problem of causal discovery from event sequences, we propose a cause-effect matching approach to learn a fully directed acyclic graph (DAG). To this end, we introduced a new causal model and an MDL based score. We proposed the CASCADE algorithm to discover causal graphs through a topological search from observational data. Finally, we evaluated CASCADE on synthetic and realistic data. On synthetic data, we find that CASCADE is either the best or close to the best-performing method across all settings, both within and outside our causal model. In particular, whenever conditions get challenging, e.g. due to noise or with multiple colliders, CASCADE outperforms all other methods by a significant margin. We examined how CASCADE performs on real world event sequences, where the true data-generating process may lie outside our causal model. We found that CASCADE recovers meaningful graphs that match with a common understanding of the world. ", "page_idx": 8}, {"type": "image", "img_path": "y9zIRxshzj/tmp/c01ae6def572841482f9cb9c1c496aa257dea78167a69b21e389b3d4a781ca16.jpg", "img_caption": ["Figure 5: Result of CASCADE on the Global Banks dataset, we show the largest subgraph, we highlight the 10 largest, by assets, banks. We clearly see CASCADE recovers locality and that larger banks have a strong influence on the market, both information not provided in the input. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations As is necessary, we have to make causal assumptions. The most prominent in our work is the direct matching between a cause event and an effect event \u2013 which precludes modeling of a single event causing multiple other events, as well as multiple events jointly causing a single effect event \u2013 and that we only consider excitatory effects \u2013 which precludes modeling the absence of events due to a cause. Our proof of identifiability for instantaneous effects depends on the strengths of the trigger resp. noise probabilities. The identifiability of the model seems provable via the independence of these, but how to operationalize this into an effective score and search algorithm are open questions. ", "page_idx": 9}, {"type": "text", "text": "Future Work Currently, our structural equations are \u2018or\u2018 relations over the parent\u2018s variables. An interesting future direction would be to explore \u2018and\u2018 relations, e.g., A and B together cause C. This raises several questions, like how close to each other A and B have to occur or if the order matters. Another interesting future direction is to allow matching of multiple causing events to one event, where each parent could have caused the event. This would allow us to answer counterfactual questions, such as if a causing event had not occurred, would we nevertheless observe its effect? This strongly relates to the firing squad example by Pearl [5], where multiple guards shoot a prisoner at the same time; if one guard did not shoot, the prisoner would still have died. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hongteng Xu, Mehrdad Farajtabar, and Hongyuan Zha. Learning granger causality for hawkes processes. In International conference on machine learning, pages 1717\u20131726. PMLR, 2016.   \n[2] Ruichu Cai, Siyu Wu, Jie Qiao, Zhifeng Hao, Keli Zhang, and Xi Zhang. Thps: Topological hawkes processes for learning causal structure on event sequences. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[3] Amirkasra Jalaldoust, Kate\u02c7rina Hlav\u00e1c\u02c7kov\u00e1-Schindler, and Claudia Plant. Causal discovery in hawkes processes by minimum description length. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6978\u20136987, 2022.   \n[4] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society, pages 424\u2013438, 1969.   \n[5] Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2nd edition, 2009.   \n[6] Dominik Janzing and Bernhard Sch\u00f6lkopf. Causal inference using the algorithmic markov condition. IEEE Transactions on Information Theory, 56(10):5168\u20135194, 2010.   \n[7] Peter Gr\u00fcnwald. The Minimum Description Length Principle. MIT Press, 2007.   \n[8] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.   \n[9] Ming Li, Paul Vit\u00e1nyi, et al. An introduction to Kolmogorov complexity and its applications, volume 3. Springer, 2008.   \n[10] Alexander Marx and Jilles Vreeken. Formally justifying mdl-based inference of cause and effect. In AAAI Workshop on Information-Theoretic Causal Inference and Discovery (ITCI\u201922), 2022.   \n[11] Donald C Weber. Accident rate potential: An application of multiple regression analysis of a poisson process. Journal of the American Statistical Association, 66(334):285\u2013288, 1971.   \n[12] Jorma Rissanen. A universal prior for integers and estimation by minimum description length. 11(2):416\u2013431, 1983.   \n[13] Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press, 2017. ISBN 0262037319.   \n[14] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Causal inference on discrete data using additive noise models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(12):2436\u20132450, 2011.   \n[15] Kailash Budhathoki and Jilles Vreeken. Accurate causal inference on discrete data. In Proceedings of the IEEE International Conference on Data Mining (ICDM\u201918). IEEE, 2018.   \n[16] Murat Kocaoglu, Alexandros Dimakis, Sriram Vishwanath, and Babak Hassibi. Entropic causal inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[17] Vanessa Didelez. Graphical models for marked point processes based on local independence. Journal of the Royal Statistical Society Series B: Statistical Methodology, 70(1):245\u2013264, 2008.   \n[18] Michael Eichler, Rainer Dahlhaus, and Johannes Dueck. Graphical modeling for multivariate hawkes processes with nonparametric link functions. Journal of Time Series Analysis, 38(2): 225\u2013242, 2017.   \n[19] Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83\u201390, 1971.   \n[20] Peter B\u00fchlmann, Jonas Peters, and Jan Ernest. Cam: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 42(6):2526\u20132556, 2014.   \n[21] Paul Rolland, Volkan Cevher, Matth\u00e4us Kleindessner, Chris Russell, Dominik Janzing, Bernhard Sch\u00f6lkopf, and Francesco Locatello. Score matching enables causal discovery of nonlinear additive noise models. In International Conference on Machine Learning, pages 18741\u201318753. PMLR, 2022.   \n[22] Spencer Compton, Kristjan Greenewald, Dmitriy A Katz, and Murat Kocaoglu. Entropic causal inference: Graph identifiability. In International Conference on Machine Learning, pages 4311\u20134343. PMLR, 2022.   \n[23] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507\u2013554, 2002.   \n[24] Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour. A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images. International journal of data science and analytics, 3:121\u2013129, 2017.   \n[25] Alexander Marx and Jilles Vreeken. Telling cause from effect using mdl-based local and global regression. In 2017 IEEE international conference on data mining (ICDM), pages 307\u2013316. IEEE, 2017.   \n[26] Sascha Xu, Osman Mian, Alexander Marx, and Jilles Vreeken. Inferring cause and effect in the presence of heteroscedastic noise. In Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2022.   \n[27] Osman A Mian, Alexander Marx, and Jilles Vreeken. Discovering fully oriented causal networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8975\u20138982, 2021.   \n[28] David Kaltenpoth and Jilles Vreeken. Nonlinear causal discovery with latent confounders. In Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2023.   \n[29] Sarah Mameche, David Kaltenpoth, and Jilles Vreeken. Learning causal models under independent changes. In Proceedings of Neural Information Processing Systems (NeurIPS). PMLR, 2023.   \n[30] David Kaltenpoth and Jilles Vreeken. Identifying selection bias from observational data. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). AAAI, 2023.   \n[31] Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83\u201390, 1971.   \n[32] Thomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461, 2000.   \n[33] Kailash Budhathoki and Jilles Vreeken. Causal inference on event sequences. In Proceedings of the 2018 SIAM International Conference on Data Mining, pages 55\u201363. SIAM, 2018.   \n[34] Ke Zhou, Hongyuan Zha, and Le Song. Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes. In Artificial Intelligence and Statistics, pages 641\u2013649. PMLR, 2013.   \n[35] Massil Achab, Emmanuel Bacry, St\u00e9phane Ga\u00efffas, Iacopo Mastromatteo, and Jean-Fran\u00e7ois Muzy. Uncovering causality from multivariate hawkes integrated cumulants. Journal of Machine Learning Research, 18(192):1\u201328, 2018.   \n[36] Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating multivariate point process. Advances in neural information processing systems, 30, 2017.   \n[37] Shuai Xiao, Junchi Yan, Mehrdad Farajtabar, Le Song, Xiaokang Yang, and Hongyuan Zha. Learning time series associated event sequences with recurrent point process networks. IEEE transactions on neural networks and learning systems, 30(10):3124\u20133136, 2019.   \n[38] Wei Zhang, Thomas Panum, Somesh Jha, Prasad Chalasani, and David Page. Cause: Learning granger causality from event sequences using attribution methods. In International Conference on Machine Learning, pages 11235\u201311245. PMLR, 2020.   \n[39] Amirkasra Jalaldoust, Kate\u02c7rina Hlav\u00e1c\u02c7kov\u00e1-Schindler, and Claudia Plant. Causal discovery in hawkes processes by minimum description length. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6978\u20136987, 2022.   \n[40] Markus Kalisch and Peter B\u00fchlman. Estimating high-dimensional directed acyclic graphs with the pc-algorithm. Journal of Machine Learning Research, 8(3), 2007.   \n[41] Jonas Peters and Peter B\u00fchlmann. Structural intervention distance for evaluating causal graphs. Neural computation, 27(3):771\u2013799, 2015.   \n[42] Mert Demirer, Francis X Diebold, Laura Liu, and Kamil Yilmaz. Estimating global bank network connectedness. Journal of Applied Econometrics, 33(1):1\u201315, 2018.   \n[43] Fco Javier Ord\u00f3nez, Paula De Toledo, and Araceli Sanchis. Activity recognition using hybrid generative/discriminative models on home environments using binary sensors. Sensors, 13(5): 5460\u20135477, 2013.   \n[44] Dominique MA Haughton. On the choice of a model to fit data from an exponential family. The annals of statistics, pages 342\u2013355, 1988. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Theory ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof - Identifiability on Instant Effects ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 1. Let $S_{i}$ be an event sequence generated by a Poisson process as per Eq. (1) and $S_{j}$ be an effect of $S_{i}$ as per Eq. (3), with, low noise $\\lambda_{j}<(1-\\alpha_{i,j})\\lambda_{i},$ , and a trigger probability $\\alpha_{i,j}<1$ . ", "page_idx": 13}, {"type": "text", "text": "In the case of exclusively instant effects, i.e. $\\phi_{i,j}(d)=\\delta(d).$ , where $\\delta(d)$ is the Dirac delta function, the MDL score in the true causal direction is lower than in the anti-causal direction, i.e. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n_{i}\\to\\infty}L(S_{j}\\mid S_{i},\\Theta_{1})+L(S_{i}\\mid\\Theta_{1})<L(S_{i}\\mid S_{j},\\Theta_{2})+L(S_{j}\\mid\\Theta_{2})\\ .\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let $n_{i}$ be the number of events in $S_{i}$ and $n_{j}$ the number of events in $S_{j}$ . As $n_{i}\\to\\infty$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(S_{i})=n_{i}H(\\phi_{i,i}),\\qquad L(S_{j}|S_{i})=n_{i}H(\\mathcal{B}(\\alpha_{i,j}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the Bernoulli distribution. In the reverse direction, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(S_{j})=n_{j}H(\\phi_{j,j}),\\qquad L(S_{i}|S_{j})=n_{i}H(B(\\alpha_{i,i}))+(n_{i}-n_{j})H(\\phi_{i,i})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To show ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\;L(S_{i})+L(S_{j}|S_{i})<L(S_{j})+L(S_{i}|S_{j})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad n_{i}H(\\phi_{i,i})+n_{i}H(B(\\alpha_{i,j}))<n_{j}H(\\phi_{j,j})+n_{i}H(B(\\alpha_{i,i}))+(n_{i}-n_{j})H(\\phi_{i,i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "$\\alpha_{i,j}=\\alpha_{i,i}$ since every event that does not cause an i, can not be explain by $\\mathrm{j}$ in the reverse direction, hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n_{i}H(\\phi_{i,i})+\\displaystyle\\frac{n_{i}H(\\mathcal{B}(\\alpha_{i,j}))}{n_{i}H(\\phi_{i,i})}<n_{j}H(\\phi_{j,j})+\\displaystyle\\frac{n_{i}H(\\mathcal{B}(\\alpha_{i,i}))}{n_{i}H(\\phi_{i,i})}+(n_{i}-n_{j})H(\\phi_{i,i})}}\\\\ {{n_{i}H(\\phi_{i,i})<n_{j}H(\\phi_{j,j})+n_{i}H(\\phi_{i,i})-n_{j}H(\\phi_{i,i})}}\\\\ {{\\displaystyle\\frac{n_{i}H(\\phi_{i,i})}{n_{j}H(\\phi_{i,i})<n_{j}H(\\phi_{j,j})+\\displaystyle\\frac{n_{i}H(\\phi_{i,i})}{n_{i}H(\\phi_{i,i})}-n_{j}H(\\phi_{i,i})}}}\\\\ {{\\displaystyle H(\\phi_{i,i})<n_{j}H(\\phi_{j,j})}}\\\\ {{H(\\phi_{i,i})<H(\\phi_{j,j})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For $H(\\phi_{i,i})<H(\\phi_{j,j})$ to hold $n_{i}>n_{j}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nn_{i}\\propto\\lambda_{i}\\;,\\qquad n_{j}\\propto\\alpha_{i,j}\\lambda_{i}+\\lambda_{j}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{i}>\\alpha_{i,j}\\lambda_{i}+\\lambda_{j}}\\\\ {\\lambda_{i}-\\alpha_{i,j}\\lambda_{i}>\\lambda_{j}}\\\\ {(1-\\alpha_{i,j})\\lambda_{i}>\\lambda_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It directly follows that $H(\\phi_{i,i})<H(\\phi_{j,j})$ , and hence for $n_{i}\\to\\infty$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(S_{j}|S_{i},\\Theta_{1})+L(S_{i}|\\Theta_{1})<L(S_{i}|S_{j},\\Theta_{2})+L(S_{j}|\\Theta_{2})\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Proof - Identifiability on Delayed Effects ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 2. Let $S_{i}$ be an event sequence generated by a Poisson process as per Eq. (1) and $S_{j}$ be an effect of $S_{i}$ as per Eq. (3), such that $H\\left(\\phi_{j,j}\\right)>H(p(;\\theta_{i,j}))+\\alpha_{i,j}^{-1}H\\left(\\mathcal{B}(\\alpha_{i,j})\\right)+\\alpha_{j,j}^{-1}H\\left(\\mathcal{B}(\\alpha_{j,j})\\right)$ , where $H$ denotes the entropy and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ the Bernoulli distribution. ", "page_idx": 13}, {"type": "text", "text": "Then the matching in the anti-causal direction $\\Delta_{j\\to i}$ of the effect $S_{j}$ to the cause $S_{i}$ has a worse MDL score than the true matching $\\Delta_{i\\to j}$ , i.e. ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(S_{j}\\mid S_{i},\\Theta_{i\\to j})+L(S_{i}\\mid\\Theta_{i})<L(S_{i}\\mid S_{j},\\Theta_{j\\to i})+L(S_{j}\\mid\\Theta_{j})\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We will show that the delays between $S_{i}$ itself, i.e. $\\Delta_{i\\to i}$ , and the delays between $S_{j}$ to $S_{i}$ , i.e. $\\Delta_{i\\to j}$ , are equivalent. ", "page_idx": 14}, {"type": "text", "text": "Proof. We consider a source event $S_{i}$ with exponentially distributed delays, i.e. $\\Delta_{i\\to i}\\sim\\exp(\\lambda_{i})$ . Consider any event $t_{k}\\in S_{j}$ , then the intensity of observing an event in $S_{i}$ at time $t>t_{k}$ is given by $\\lambda_{i}$ . The distribution of the delay to the next event in $S_{i}$ is exponential with $\\lambda=\\lambda_{i}$ . Thus, the difference between ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(S_{i}|S_{j})-L(S_{i})=\\sum_{d_{k}\\in\\Delta_{i\\to j}}\\log(\\lambda_{i})-d_{k}/\\lambda_{i}-\\sum_{d_{l}\\in\\Delta_{i\\to i}}\\log(\\lambda_{i})+d_{l}/\\lambda_{i}=0\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It remains to show that the likelihood in the causal direction is better when conditioning effect on the cause, i.e. $L(S_{j}|S_{i})<L(S_{j})$ . ", "page_idx": 14}, {"type": "text", "text": "Gain by $i\\rightarrow j$ For $i$ to cause $j$ it has to provide information about $j$ , that is the cost of selecting which $i$ events cause $j$ , and with what dealys. Additional it has to ofset the cost which $j$ events do not have to be encoded as a self delay. Formally this is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nn_{i}H({B(\\alpha_{i,j})})+n_{i,j}H(p(;\\theta_{i,j}))+n_{j}H({B(\\frac{n_{i,j}}{n_{j}})})<n_{i,j}H(\\phi_{j,j})\\quad,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $n_{i,j}$ are the number of events in $j$ caused by $i$ , and $H(p(;\\theta_{i,j}))$ is the entropy of distribution described by the pdf $p$ . ", "page_idx": 14}, {"type": "text", "text": "As $n\\to\\infty$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(S_{j})=n_{j}H(\\phi_{j,j})\\qquad L(S_{j}|S_{i})=n_{i}H(\\phi_{i,j})+(n_{j}-n_{i,j})H(\\phi_{j,j})+n_{j}H(\\mathcal{B}(\\alpha_{j,j}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To show, ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(S_{j})>L(S_{j}|S_{i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n_{j}H(\\phi_{j,j})>n_{i}H(\\phi_{i,j})+(n_{j}-n_{i,j})H(\\phi_{j,j})+n_{j}H(B(\\alpha_{j,j}))}}\\\\ {{n_{i,j}H(\\phi_{j,j})+\\displaystyle\\frac{(n_{j}-n_{i\\cdot j})H(\\phi_{j,j})}{n_{i,j}H(\\phi_{j,j})>n_{i}H(\\phi_{i,j})+\\displaystyle\\frac{(n_{j}-n_{i\\cdot j})H(\\phi_{j,j})}{n_{j}H(B(\\alpha_{j,j}))}}+n_{j}H(B(\\alpha_{j,j}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we can substitute $n_{i}H(\\phi_{i,j})=n_{i,j}H(p(;\\theta_{i,j}))+n_{i}H(\\mathcal{B}(\\alpha_{i,j}))$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nn_{i,j}H(\\phi_{j,j})>n_{i,j}H(p(;\\theta_{i,j}))+n_{i}H(\\mathcal{B}(\\alpha_{i,j}))+n_{j}H(\\mathcal{B}(\\alpha_{j,j}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, note that the number of caused items $\\alpha_{i,j}n_{i}=n_{i,j}$ and $\\alpha_{j,j}n_{j}=n_{i,j}$ , then it follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{i,j}H(\\phi_{j,j})>n_{i,j}H(p(;\\theta_{i,j}))+\\displaystyle\\frac{n_{i,j}}{\\alpha_{i,j}}H(\\mathcal{B}(\\alpha_{i,j}))+\\displaystyle\\frac{n_{i,j}}{\\alpha_{j,j}}H(\\mathcal{B}(\\alpha_{j,j}))}\\\\ &{\\quad H(\\phi_{j,j})>H(p(;\\theta_{i,j}))+\\displaystyle\\frac{1}{\\alpha_{i,j}}H(\\mathcal{B}(\\alpha_{i,j}))+\\displaystyle\\frac{1}{\\alpha_{j,j}}H(\\mathcal{B}(\\alpha_{j,j}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, we show that $i\\rightarrow j$ is identifiable. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof - Path Identifiability ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 3. Given an event sequence $S$ generated by a causal structure $G^{*}$ , let $S_{i}$ be a source node of $G^{*}$ and $S_{v}$ be a descendant of $S_{i}$ , where there exists a path $i\\rightarrow j\\rightarrow\\cdot\\cdot\\cdot\\rightarrow v$ in $G^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "Then, the gain in the causal direction of the path $g(i\\rightarrow v\\mid\\Theta)-g(v\\rightarrow i\\mid\\Theta)$ is greater. ", "page_idx": 14}, {"type": "text", "text": "Proof. We begin by proving that the path identifiability holds for a triplet of nodes $i\\rightarrow j\\rightarrow v$ , by constructing a new alignment from $i\\rightarrow v$ . ", "page_idx": 14}, {"type": "text", "text": "From $\\Delta_{i\\to j}$ and $\\Delta_{j\\rightarrow v}$ we can construct $\\Delta_{i\\to v}$ . For each $d_{k}\\,\\in\\,\\Delta_{i\\to j}$ , there is a corresponding $d_{l}\\in\\Delta_{j\\rightarrow v}$ , i.e. the trigger time of the triggered event. To construct $\\Delta_{i\\to v}$ , we consider the following cases: ", "page_idx": 14}, {"type": "text", "text": "2. Let $d_{l}\\in\\Delta_{j\\rightarrow v}$ be the delay of event $a$ of type $j$ where $a$ has been caused by delay $d_{k}$ . If $d_{k}\\neq\\infty$ for $d_{k}\\in\\Delta_{i\\to j}$ and $d_{l}=\\infty$ then then $d_{k}\\in\\Delta_{i\\to v}$ , is set to $d_{k}=\\infty$ . 3. Let $d_{l}\\in\\Delta_{j\\rightarrow v}$ be the delay of event $a$ of type $j$ where $a$ has been caused by delay $d_{k}$ . If $d_{k}\\neq\\infty$ for $d_{k}\\in\\Delta_{i\\to j}$ and $d_{l}\\neq\\infty$ then then $d_{k}\\in\\Delta_{i\\to v}$ , is set to $d_{k}=d_{k}+d_{l}$ . ", "page_idx": 15}, {"type": "text", "text": "As $\\Delta_{i\\to v}$ is another valid alignment, and $i$ remains a source node, the guarantees of Theorem 1 and 2 hold, i.e. the path is identifiable. This extends to a path of arbitrary length. Consider an additional edge $v\\rightarrow w$ , then we construct the alignment $\\Delta_{i\\to w}$ by considering the delays of $\\Delta_{i\\to v}$ and $\\Delta_{v\\to w}$ . Furthermore, we can show that the true path $j\\rightarrow v$ has a better gain than the shortcut $i\\rightarrow v$ , so that we can remove the shortcut in the pruning stage. (1) Since $i\\rightarrow j$ and $j\\rightarrow v$ are independent processes, it follows either $\\mathrm{Var}(\\phi_{i,v})>\\bar{\\mathrm{Var}}(\\phi_{j,v}\\bar{\\mathrm{\\Gamma}}).$ , or $\\alpha_{i,v}>\\alpha_{j,v}$ and by that a more costly description of $v$ . (2) If $j\\to v\\not\\in G$ , we can construct a new function $f_{i,v}\\,=\\,f_{j,v}(f_{i,j}(S_{i},\\Delta_{i\\rightarrow j})\\cup N_{j},\\Delta_{j\\rightarrow v})$ by Theorem 2 it follows that edge $i\\rightarrow v$ improves our score. ", "page_idx": 15}, {"type": "text", "text": "(3) Assume $j\\to v\\in G$ then each individual $v$ event that is matched to by $i\\rightarrow v$ is already matched to by $j\\rightarrow v$ , and from (1) we know it does so cheaper, hence we get no gain by adding the shortcut $i\\rightarrow v$ . ", "page_idx": 15}, {"type": "text", "text": "A.4 Consistency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Here we will show that $L(S^{n},\\Theta)$ asymptotically behaves like $B I C.L(S,\\Theta)$ directly corresponds to the log likelihood, which we rewrite as $\\log p(S^{n}|\\Theta,G)$ Our approach can be instantiated with arbitrary delay distribution, to show consistency we have to upper bound the number of parameters by ${\\mathcal{O}}(\\log n)$ , this trivially holds for the parametric setting we focus on in this paper, because $|p a(i)|\\,\\in\\mathcal{O}(\\log n)$ [27]. The encoding of the graph $G$ is independent of $n$ , i.e. fixed for a given network, hence in $\\mathcal{O}(1)$ . Finally this results at, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log p(S^{n}|\\Theta,G)+c\\log n+\\mathcal{O}(1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we set $\\textstyle c={\\frac{d}{2}}$ where $d$ is the number of free parameters, arriving at the $B I C$ score. ", "page_idx": 15}, {"type": "text", "text": "From Haughton [44] and Chickering [23] we know that $B I C$ identifies a Markov equivalence class of the true DAG. For the identifiability of undirected edges we refer to Theorem 1 and 2. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.5 Connection to Hawkes Processes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The key difference between a linear Hawkes process and our model is the assumption of direct triggers, that is the mechanism of one event and one event only causing another. In a linear Hawkes process \u2018cause events\u2019 increase the intensity and therewith the probability of events occurring. However, one can generally not label for a specific event another as the \u2018cause\u2019, as each event is the result of a multitude of causes. ", "page_idx": 15}, {"type": "text", "text": "In this section, we are going to explore under which conditions we can identify a Hawkes process under our causal model. ", "page_idx": 15}, {"type": "text", "text": "Given an event sequence $S_{j}=\\{t_{k}\\}_{t_{k}=0}^{n_{j}}$ generated by a linear Hawkes process, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda_{j}(t)=u_{j}+\\sum_{i\\in p a(j)}\\sum_{t_{k}<t,t_{k}\\in S_{i}}v_{i,j}\\left(t-t_{k}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We construct a set of primary causes for each event $t_{k}\\in S_{j}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{j}=\\left\\{(t,i,t_{k})\\mid t\\in S_{j},\\;(i,t_{k})=\\underset{(i,t_{k}),\\;i\\in p a(j),t_{k}<t,t_{k}\\in S_{j}}{\\arg\\operatorname*{max}}v_{i,j}(t-t_{k})\\right\\}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we consider as primary cause of an event that past event with the highest influence at time point $t$ from a causal parent $i\\in p a(j)$ . Using these delays, we construct an alignment (mapping of delays) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{i\\rightarrow j}=\\{\\beta(t_{k})\\mid t_{k}\\in S_{i}\\}\\qquad\\beta(t_{k})=\\left\\{{t-t_{k}\\atop\\infty}\\quad\\mathrm{if}\\ v_{i,j}(t-t_{k})>u_{j}\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t=\\arg\\operatorname*{max}_{t\\in S_{j}\\wedge\\exists(t,i,t_{k})\\in C_{j}}v_{i,j}(t-t_{k})$ if $\\nexists(t,i,t_{k})\\in C_{j}$ then $\\beta(t_{k})=\\infty$ . We consider $t_{k}\\in S_{i}$ a cause of an event $t\\in S_{j}$ if it is the primary cause of $t$ and $t_{k}$ has no stronger influence on any other event of $S_{j}$ . Finally, the influence has to be stronger than that of the base intensity of $S_{j}$ . ", "page_idx": 16}, {"type": "text", "text": "To be able to identify a causal edge between $S_{i}$ and $S_{j}$ the improvement gained by this alignment must outweigh the edge cost $L(i\\rightarrow j)$ . For this, there are two conditions: firstly, the number of primary cause events from $i$ to $j$ , i.e. those instances where an event from $S_{i}$ has the maximum influence on an event from $S_{j}$ , must be large enough. This is the case as long as $|\\Delta_{i\\to j}|$ increases with $n_{j}$ , i.e. the total number of events of $S_{j}$ . Then, in the limit $n_{j}\\to\\infty$ the number of primary cause events is large enough to offset the constant edge cost. ", "page_idx": 16}, {"type": "text", "text": "The achievable score gain is obtained by constructing a delay distribution from $|\\Delta_{i\\to j}|$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{i,j}(d)=\\binom{1-\\alpha_{i,j}}{p_{\\Delta_{i\\to j}}(d)\\cdot\\alpha_{i,j}}\\quad\\mathrm{else}^{\\mathrm{~}}\\quad\\mathrm{where}\\;\\alpha_{i,j}=1-P(\\Delta_{i\\to j}=\\infty)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If this density $\\phi_{i,j}(d)$ fulfills the conditions of Theorem 2, we can identify $S_{i}$ as a parent of $S_{j}$ in the limit of $n_{j}\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "In conclusion, CASCADE can identify a causal pair generated under a Hawkes process, if there exist sufficiently many events from $S_{i}\\to S_{j}$ , where $v_{i,j}(t)$ has the strongest influence on $\\lambda_{j}(t)$ for some of the $t$ . By aligning the delays of these events, we can identify the causal edge and recover the underlying causal structure. ", "page_idx": 16}, {"type": "text", "text": "A.6 Empirical Evaluation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "[From main paper] To empirically evaluate how effectively CASCADE recovers the true DAG on data generated by a Hawkes process, we generate synthetic data using the tick library5. We vary the intensity of the excitation function, i.e., the expected number of events generated per cause. We show the results in Figure 6. We observe that CASCADE performs best when our assumptions hold, when there is one effect per cause or fewer, but still demonstrates strong performance across all settings. ", "page_idx": 16}, {"type": "image", "img_path": "y9zIRxshzj/tmp/9850d56c6376b53f33770e5ce0e920370d093ca308558baadaf76c672711188e.jpg", "img_caption": ["Figure 6: DAG recovery on data generated by a Hawkes process. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.7 Consistency of Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 4. Given an event sequence $S$ , where each individual subsequence $S_{i}$ was generated as per Eq. (3) by an underlying causal graph $G^{*}$ . Assuming all $\\Delta_{i\\to j}$ are the true causal matchings. Under the Algorithmic Markov Condition, CASCADE recovers the true graph $G^{*}$ for $n\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We begin by proving that in the first step, CASCADE identifies a true source node of $G^{*}$ . We denote the parents of $i$ in the true causal graph $G^{*}$ as $p a(i)$ , whilst we write for the parents in the graph maintained by CASCADE as $p a^{\\prime}(i,G)$ . ", "page_idx": 16}, {"type": "text", "text": "Let $i\\in[p]$ be a node of $G^{*}$ without parents, i.e. a source. By Theorem 3, for a path $i\\rightarrow\\cdot\\cdot\\cdot\\rightarrow v\\in G^{*}$ the gain in the causal direction $\\bar{g(i\\,\\,\\to\\,v\\,\\,\\mid\\,\\Theta)}$ is greater than the gain in the reverse direction $g(v\\rightarrow i\\mid\\Theta)$ . ", "page_idx": 16}, {"type": "text", "text": "For all nodes $v\\in[p],v\\neq i,$ , $v$ is either an descendant of $i$ or unrelated. ", "page_idx": 16}, {"type": "text", "text": "1. If $v$ is a descendant of $i$ , then the gain of $g(i\\;\\rightarrow\\;v\\;\\mid\\;\\Theta)$ is greater than the gain of $g(v\\rightarrow i\\mid\\Theta)$ .   \n2. If $v$ is unrelated to $i$ , then the gain in both sides is $0$ . ", "page_idx": 16}, {"type": "text", "text": "Hence it follows, that for a node $i$ where $p a(i)=\\emptyset$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in C}g(j\\to i\\mid\\Theta)-g(i\\to j\\mid\\Theta)=0\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, consider a node $i$ with parents $p a(i)\\neq\\emptyset$ . Then, as $G^{*}$ is a DAG, there exists an ancestor $v$ of $i$ , where $v$ is a source node, i.e. $p a(v)=\\ddot{\\varnothing}$ . For that $v$ , it holds that the gain from $v$ to $i$ is greater than the gain from $i$ to $v$ . Hence, for a node $i$ with parents, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in C}g(j\\to i\\mid\\Theta)-g(i\\to j\\mid\\Theta)>0\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by taking the argmin over all nodes, CASCADE identifies the true source node of $G^{*}$ , i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{i\\in C}\\operatorname*{max}_{j\\in C}g(j\\to i\\mid\\Theta)-g(i\\to j\\mid\\Theta)\\implies p a(i)\\cap C=\\emptyset\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Edge Addition Now, we show that CASCADE always identifies a true causal edge for $n_{i}\\to\\infty$ . First, note that given a source node $i$ , there do not exist any incoming causal edges in the graph $G^{*}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\np a(i)\\cap C=\\emptyset\\implies\\nexists j\\in C:j\\to i\\in G^{*}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, by fitting outgoing edges only, we test all possible edges for $i$ and never add a false oriented edge, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall j\\in C:i\\to j\\in G^{*}\\implies j\\to i\\notin G\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we recall that the true causal graph $G^{*}$ is the graph that minimizes the description length of the data as per the Algorithm Markov Condition. Hence, adding a true causal edge to the graph will result in a lower description length, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall j\\in C,i\\to j\\in G^{*}:L(S_{j}|S_{p a^{\\prime}(j,G)},\\Theta^{\\prime})>L(S_{j}|S_{p a^{\\prime}(j,G)\\cup i},\\Theta^{\\prime}\\cup\\theta_{i,j})\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Edge Removal Consider the node $i$ , where $p a(i)\\cap C=\\emptyset$ , and given a graph $G$ where all true causal edges have been added, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall j\\in\\bar{C}:\\forall j\\to v\\in G^{*}:j\\to v\\in G\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for $i$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall j\\in p a(i):i\\to j\\in G\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows, that $p a^{\\prime}(i,G)\\supseteq p a(i)$ . By the Algorithm Markov Condition, the shortest description length of the data is achieved by the true causal graph $G^{*}$ . Hence, a superset of the true parents of $i$ will result in a higher description length, and it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\nL(S_{i}|S_{p a^{\\prime}(i)},\\Theta^{\\prime})>L(S_{i}|S_{p a(i)},\\Theta)\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by testing that subset of the parents of $i$ results in a lower description length, CASCADE identifies the true parents of $i$ . ", "page_idx": 17}, {"type": "text", "text": "Overall Consistency For $n_{i}\\to\\infty$ , we note that in each step for the node $i$ it holds that ", "page_idx": 17}, {"type": "text", "text": "1. $i$ has no parents in the candidate set $p a(i)\\cap C=\\emptyset$ .   \n2. We add no false oriented edges to the graph, as $p a(i)\\cap C=\\emptyset\\implies\\nexists j\\in C:j\\to i\\in G^{*}$ .   \n3. We add all true edges $\\textit{i}\\to\\textit{j}$ to the graph $G$ , i.e. $\\forall j\\;\\;\\in\\;\\;C,i\\;\\;\\to\\;\\;j\\;\\;\\in\\;\\;G^{*}$ : $L(S_{j}|S_{p a^{\\prime}(j,G)},\\Theta^{\\prime})\\stackrel{\\_}{>}L(S_{j}|S_{p a^{\\prime}(j,G)\\cup i},\\Theta^{\\prime}\\stackrel{\\_}{\\cup}\\bar{\\theta}_{i,j})$ .   \n4. For $i$ , the current graph $G$ contains a superset of all true parents, i.e. $p a^{\\prime}(i,G)\\,\\supseteq\\,p a(i),$ while the description length of the data is minimized by the true graph $L(S_{i}|S_{p a^{\\prime}(i)},\\Theta^{\\prime})>$ $L(S_{i}|S_{p a(i)},\\Theta)$ . ", "page_idx": 17}, {"type": "text", "text": "Hence, by repeating the edge addition and pruning in a topological order, in the limit of $n_{i}\\to\\infty$ under our causal model and by the Algorithm Markov Condition, CASCADE identifies the true causal graph $G^{*}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we provide additional detail on the synthetic data generation and the experiment setup. Additional we provide further metrics on the synthetic experiments. For the real-world data we provide additional results. ", "page_idx": 18}, {"type": "text", "text": "B.1 Synthetic Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We generate synthetic data according to our causal model. We discretize the timestamps to 1 million unique timestamps. Throughout the experiments we vary the following parameters: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Variables: Number of unique events types $p$ .   \n\u2022 Edges: The total number edges in the generating causal graph $G*$ .   \n\u2022 Delay Distribution: For all synthetic experiments we generate delays according to geometric distribution (as a discretize instantiation of the exponential).   \n\u2022 Delay Distribution Parameter: For each causal edge we sample the rate $\\lambda$ uniformly from a specified range.   \n\u2022 Cause probability: For each causal edge we sample $\\alpha$ uniformly from a specified range.   \n\u2022 # Source Events: Number of events sampled for source nodes (variables without any parents in the DAG).   \n\u2022 Additive noise parameter: percentage of additional added events to the caused events, also applies to source nodes, where # Source Events are considered as \u2018caused\u2019.   \n\u2022 Instant effect: Except for the \u2018Instant Effect\u2019 experiments no instant effects are created. ", "page_idx": 18}, {"type": "text", "text": "For all experiments, unless otherwise stated a random DAG is generated. And for each parameterization 20 independent samples are generated. ", "page_idx": 18}, {"type": "text", "text": "Sanity Check We set the number of types to 20 and generate 100 root events per source node (in this every node is a source node). ", "page_idx": 18}, {"type": "text", "text": "Increase of Event Types In this experiment we increase the number of event types $p$ from 5 to 40, We set the number of edges to $(d^{2}-d)/(2*5)$ , that is $20\\%$ of all possible edges. To avoid overly many events in the colliders we set the number of root events to 20, for 40 variables this results in up to $\\approx30.000$ events. We do not include any additive noise and set $\\alpha=1$ . For the delay distribution, we sample $\\lambda$ from a range between of [0.3, 1]. ", "page_idx": 18}, {"type": "text", "text": "Decrease of Noise In this experiment we increase the probability of $\\alpha$ , and decrease the fraction of additive noise. We set the number of variables to 20 and set the number of root events to 100. We sample $\\lambda$ from a range of [0.1, 0.4]. ", "page_idx": 18}, {"type": "text", "text": "Distribution Misspecification To further evaluate robustness of CASCADE we test recovery on generated data where the actual distribution does not match the assumed distribution. To this end, we change the assumed distribution of CASCADE and use the same setup as the previous experiment (Increase of Event Types) with 20 unique events. For the, true, exponential we observe an average F1 score of 0.82, for the Poisson 0.81, with a Normal distribution 0.76, and uniform 0.75. While recovery is best when assumed and generating distribution match CASCADE still performs well under misspecification. ", "page_idx": 18}, {"type": "text", "text": "Multiple Parents For this experiment we specify a DAG, where $\\textstyle\\lceil{\\frac{n-1}{2}}\\rceil$ are direct parents and $\\textstyle{\\lfloor{\\frac{n-1}{2}}\\rfloor}$ are independent, the $n^{t h}$ node is the collider. We plant 30 events per root cause and increase the number of variables from 50 to 200. We add $30\\;\\%$ of additive noise and set the cause probability randomly between 0.9 and 0.6. We repeat the same experiment where $10\\%$ of nodes are colliders. That is, for 50 event types, 5 are colliders and $\\textstyle\\lceil{\\frac{p-5}{2}}\\rceil$ direct causes of all 5 colliders. The remaining $\\textstyle{\\left\\lfloor{\\frac{p-5}{2}}\\right\\rfloor}$ are independent. We show the results in Table 1. ", "page_idx": 18}, {"type": "table", "img_path": "y9zIRxshzj/tmp/191ff7ed0d55737db8cd3468716f2bc1709f6e775362ff4c43c4b104bef0d577.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 1: Average $F1$ score on Multiple Parents experiment with multiple colliders. ", "page_idx": 19}, {"type": "table", "img_path": "y9zIRxshzj/tmp/6c33747a8529f083f192c7f7306ba58ceedec2293377aa8a1594e34c8af3cff5.jpg", "table_caption": [], "table_footnote": ["Table 2: Average results on $\\overline{{90\\%}}$ instant data "], "page_idx": 19}, {"type": "text", "text": "Instant Effects For the instant effects experiments we again use 20 variables with 100 root events, we shift the geometric delay distribution and set $\\lambda=0.9$ , such that $90\\%$ of the events are generated at the same timestamp. We randomly sample the trigger probability between 0.7 and 0.5. For the exclusively instant effects we set $\\lambda=1$ . We show the full results in Table 2 and ", "page_idx": 19}, {"type": "text", "text": "B.2 Method Parameterization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "CASCADE We set the precision parameter for all experiments to 2. For all synthetic experiments we consider events as potential causes of at most 100 timestamps. For all experiments we consider a geometric distribution, which we shift back to cover instant effects. ", "page_idx": 19}, {"type": "text", "text": "MDLH For the results of the Increase event types experiment we use the sparse version, where we set the maximum degree to the true maximal degree and set $\\mathrm{T=}1000$ . In an effort to reduce runtime with higher number of types (i.e. nodes), we tested it with ${\\mathrm{T}}{=}100$ , where it also did not terminate within 96 hours. ", "page_idx": 19}, {"type": "text", "text": "Other For all other competing methods we used the default parameters. ", "page_idx": 19}, {"type": "text", "text": "B.3 Compute Recourses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All experiments where executed on a internal cluster on compute nodes equipped with a AMD EPYC 7773X 64-Core Processor (2.2 GHz; Turboboost: $3.5\\:\\mathrm{GHz})$ ), with 2 TB of RAM, while in practice a fraction of that was necessary. We provide the average runtimes below. ", "page_idx": 19}, {"type": "table", "img_path": "y9zIRxshzj/tmp/27d1356c907c700fd4c4432b03952557bb5987c853afa5f1ee3a3c2179478a76.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 3: Results of instant effects, we omit the results of THP as it only reports empty DAGs ", "page_idx": 19}, {"type": "table", "img_path": "y9zIRxshzj/tmp/34f206761830c36079de2a2785c076154911a514dd3b25c337e7b7e8f7a8144b.jpg", "table_caption": [], "table_footnote": ["Table 4: Mean runtime, in seconds, of Increase Event Types Experiment "], "page_idx": 20}, {"type": "table", "img_path": "y9zIRxshzj/tmp/5448cfe1b72dec5cbd2ebba1b501d44c338c0ec010332c32c80a80a14e7747c2.jpg", "table_caption": [], "table_footnote": ["Table 5: Mean runtime, in seconds, under increasing Noise. "], "page_idx": 20}, {"type": "text", "text": "B.4 Network Alarms ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the provided dataset each event happens on a specific device. In addition to the event sequences a topology $\\tau$ over the devices is provided. An event can cause an event on each neighboring device, in addition to the device where the event occurred. To support this we can include a matching for connected devices. That is if $\\{a,b\\}\\in{\\mathcal{T}}$ we include $\\Delta_{i\\rightarrow j}^{(a,\\bar{b})}$ \u2206i j and \u2206i(b,aj) , we include both directions since events on $a$ can cause events on $b$ and events on $b$ can cause events on $a$ . For all devices we include the self loop \u2206i(a,aj) . ", "page_idx": 20}, {"type": "text", "text": "B.5 Real World Experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we provide Causal Graphs reported by CASCADE and for the Global Banks dataset additionally the result of THP. ", "page_idx": 20}, {"type": "table", "img_path": "y9zIRxshzj/tmp/ef33139c16a8cc47fff8bcf15fddbee1935ed7a0c5d3a8542419e3bd2d7a9728.jpg", "table_caption": [], "table_footnote": ["Table 6: Mean runtime, in seconds, of Increase Event Types (Collider Experiment) "], "page_idx": 20}, {"type": "image", "img_path": "y9zIRxshzj/tmp/8b2c750c9eb53b40a93edd56fa5e61404686601ddad8e1eca104cb508684b396.jpg", "img_caption": ["Figure 7: SHD, SID, and F1 score for the synthetic experiments "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "y9zIRxshzj/tmp/37477534476fa66b49a0d4a08cac2d2d3ae2820805e44094558dc1340336a327.jpg", "img_caption": ["Figure 8: Recovered Causal Graphs on the two Daily Activities datasets. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "y9zIRxshzj/tmp/205b43ff949c097c0812ecafd840c049b111df57ecf0de8b5e28d01788aaee0d.jpg", "img_caption": ["Figure 9: Recovered Causal Graph on Network Alarms dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "y9zIRxshzj/tmp/c1567e1e4416412382c510f9680e0dfad73401007b38ebfa3b96545e8d1afc25.jpg", "img_caption": ["Figure 10: DAG reported by CASCADE on the Global Banks dataset [42]. We omit unconnected nodes for clarity. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "y9zIRxshzj/tmp/2b1d698dbed84dd6934c49161416e2bbce1622d812545e17e29aea116afd26b5.jpg", "img_caption": ["Figure 11: DAG reported by THP on the Global Banks dataset [42]. We omit unconnected nodes for clarity. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are supported by the findings and the main body of the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss limitations and assumptions in the conclusion. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We clearly state all theorems and proof them in the appendix. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our results can be fully reproduced, we provide all the data and code online. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the full code along with the synthetic data generator. All used dataset are publicly available. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide an experiment setup description in the appendix. We did not do any hyperparameter tuning. Our method does not require data splitting. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We show Box-Plots for all (synthetic) experiments. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we provide a detailed description of the compute recourse in the appendix. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research complies with NeurIPS Code of Ethics, we do not expect any harmful consequences resulting from our research. ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provided citations sources for all datasets. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: We use only already publicly available datasets, and do not introduce any new. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No Human Subjects where involved. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 25}]