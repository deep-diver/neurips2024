[{"figure_path": "r7mj17BKzw/tables/tables_3_1.jpg", "caption": "Table 1: Performance overhead of AAE. We break down the averaged inference runtime per sample from the MNIST dataset. TAAE denotes time spent on loading classical data into quantum state using AAE, and Ttotal refers to total runtime.", "description": "The table shows the runtime breakdown for quantum neural network (QNN) inference using approximate amplitude encoding (AAE).  It compares the time taken for loading data into quantum states using AAE (TAAE) against the total inference time (Ttotal). The difference (Ttotal - TAAE) represents the time spent on actual QNN computations, excluding data loading.  The results highlight that AAE's data loading time dominates the overall inference time, especially as the number of qubits increases.", "section": "3.1 Motivation"}, {"figure_path": "r7mj17BKzw/tables/tables_4_1.jpg", "caption": "Table 2: Fidelity comparison between SuperEncoders trained with different loss functions.", "description": "This table presents the fidelity achieved by three different SuperEncoder models trained using distinct loss functions (L1, L2, and L3).  The fidelity metric reflects how closely the quantum state generated by the SuperEncoder matches the target quantum state.  Higher fidelity indicates better performance. The results highlight the significant improvement in fidelity when using state-oriented loss functions (L2 and L3) compared to the parameter-oriented loss function (L1).", "section": "3.2 Design Methodology"}, {"figure_path": "r7mj17BKzw/tables/tables_6_1.jpg", "caption": "Table 3: Comparison between AE, AAE and SuperEncoder in terms of runtime and fidelity.", "description": "This table compares the performance of three quantum state preparation methods: Amplitude Encoding (AE), Approximate Amplitude Encoding (AAE), and SuperEncoder.  For each method, it shows the average fidelity achieved (how accurately the target quantum state was prepared) and the average runtime (how long it took to prepare the quantum state) across five different data distributions (Uniform, Normal, Log-normal, Exponential, and Dirichlet).  The results demonstrate SuperEncoder's significantly faster runtime compared to AAE, while maintaining comparable fidelity.", "section": "4.2 Evaluation on Synthetic Dataset"}, {"figure_path": "r7mj17BKzw/tables/tables_8_1.jpg", "caption": "Table 4: Performance of different QSP methods in HHL algorithm. \u2018Avg err\u2019 denotes the average relative errors between AAE/SuperEncoder and AE.", "description": "This table presents the performance comparison of three quantum state preparation (QSP) methods (AE, AAE, and SuperEncoder) within the context of Harrow-Hassidim-Lloyd (HHL) algorithm.  The results show the output values for five different linear equations (b0-b4), where AE is considered the ground truth. The \u2018Avg err\u2019 row displays the average relative error between AAE and SuperEncoder, compared to AE, indicating the accuracy of the approximate methods.  Note that a lower error percentage signifies better approximation of the AE.", "section": "4.3 Application to Downstream Tasks"}, {"figure_path": "r7mj17BKzw/tables/tables_13_1.jpg", "caption": "Table 1: Performance overhead of AAE. We break down the averaged inference runtime per sample from the MNIST dataset. TAAE denotes time spent on loading classical data into quantum state using AAE, and Ttotal refers to total runtime.", "description": "This table shows the breakdown of the runtime for Quantum Neural Network (QNN) inference.  It highlights that the time spent on Approximate Amplitude Encoding (AAE) for data loading (TAAE) constitutes the vast majority of the total inference time (Ttotal), which increases dramatically with the number of qubits.  This demonstrates AAE's runtime inefficiency as a significant bottleneck in QNN applications.", "section": "3.1 Motivation"}, {"figure_path": "r7mj17BKzw/tables/tables_14_1.jpg", "caption": "Table 6: Number of blocks and corresponding depth of AAE/SuperEncoder.", "description": "This table shows the number of blocks and the resulting circuit depth for the parameterized quantum circuit (PQC) used in approximate amplitude encoding (AAE) and SuperEncoder for different numbers of qubits.  The depth is calculated as 3 times the number of blocks because each block consists of a rotation layer and two CNOT layers, each adding a depth of 1.  The table shows that SuperEncoder's circuit depth is linearly related to the number of qubits, enabling scalability.", "section": "3.2 Design Methodology"}, {"figure_path": "r7mj17BKzw/tables/tables_14_2.jpg", "caption": "Table 7: Performance evaluation on larger quantum states (6-qubit and 8-qubit). The last separate row shows the results of AAE for comparison.", "description": "This table presents the fidelity results achieved by SuperEncoder, AAE, and AE when performing quantum state preparation (QSP) for quantum states with 4, 6, and 8 qubits.  Each row represents a different probability distribution used to generate the quantum states. The 'Avg' row provides the average fidelity across all distributions for each qubit number. The 'Avg-AAE' row shows the average fidelity obtained by the Approximate Amplitude Encoding (AAE) method for comparison. The table highlights the tradeoff between SuperEncoder's speed and the fidelity of the prepared states compared to the more accurate but slower AAE method.", "section": "4.2 Evaluation on Synthetic Dataset"}, {"figure_path": "r7mj17BKzw/tables/tables_15_1.jpg", "caption": "Table 8: Impact of increasing network width. Here h refers to the size of hidden units.", "description": "This table presents the results of an experiment to evaluate the effect of increasing the width of the hidden layers in the SuperEncoder model.  The experiment compared the performance of the model with different numbers of hidden units (h), specifically 512 and 4*2<sup>6</sup> for 6-qubit quantum states, and 512 and 16*2<sup>8</sup> for 8-qubit states.  The results are shown for various datasets (Uniform, Normal, Log-normal, Exponential, and Dirichlet), and the average fidelity is calculated for each configuration.  The goal was to determine if increasing the width improves the model's fidelity in preparing quantum states.", "section": "4.2 Evaluation on Synthetic Dataset"}, {"figure_path": "r7mj17BKzw/tables/tables_15_2.jpg", "caption": "Table 9: Fidelity improvements after fine-tuning SuperEncoder using a dataset consisting of different distributions.", "description": "This table shows the average fidelity achieved by SuperEncoder on a test dataset of various distributions before and after fine-tuning the model on a dataset with additional distributions.  The results demonstrate that fine-tuning SuperEncoder improves its performance across all distributions, showcasing the benefit of incorporating a wider range of data during training.", "section": "B.3 Impact of Training Datasets"}]