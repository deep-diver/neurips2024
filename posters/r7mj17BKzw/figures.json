[{"figure_path": "r7mj17BKzw/figures/figures_1_1.jpg", "caption": "Figure 1: Breakdown of normalized runtime for QNN inference. Original data are listed in Table 1.", "description": "This figure shows a bar chart comparing the normalized runtime of Quantum Neural Network (QNN) inference.  It breaks down the total runtime into two components: the time spent on Approximate Amplitude Encoding (AAE), and the remaining time.  The chart clearly shows that the time spent on AAE dominates the total runtime, especially as the number of qubits increases. This highlights the major bottleneck in using AAE for QNN inference: the loading of classical data into quantum states.", "section": "3 SuperEncoder"}, {"figure_path": "r7mj17BKzw/figures/figures_2_1.jpg", "caption": "Figure 2: An example PQC with two blocks, with each block consisting of a rotation layer (filled blue) plus an entangler layer (filled red).", "description": "This figure shows an example of a Parameterized Quantum Circuit (PQC) used in quantum state preparation.  It consists of two blocks. Each block has a rotation layer (shown in blue) and an entangler layer (shown in red). The rotation layer applies single-qubit rotation gates (Ry gates in this example) to each qubit, and the entangler layer applies multi-qubit gates (CNOT gates in this example) to entangle the qubits. The parameters (\u03b8i) of the Ry gates are adjustable and are optimized during the quantum state preparation process to approximate the target quantum state.", "section": "2.2 Quantum State Preparation"}, {"figure_path": "r7mj17BKzw/figures/figures_3_1.jpg", "caption": "Figure 3: Comparison between AAE and SuperEncoder.", "description": "The figure compares the inference processes of Approximate Amplitude Encoding (AAE) and SuperEncoder.  In (a), AAE's iterative parameter updates for approximating the target quantum state are shown, highlighting its lengthy runtime.  In contrast, (b) illustrates SuperEncoder\u2019s direct parameter estimation from a classical neural network, showcasing its speed advantage. Both methods utilize a parameterized quantum circuit (PQC) to prepare the quantum state for a quantum neural network.", "section": "3 SuperEncoder"}, {"figure_path": "r7mj17BKzw/figures/figures_4_1.jpg", "caption": "Figure 4: Virtualization of states generated by SuperEncoder trained with different loss functions. L2 is omitted as it produces very similar results to L3.", "description": "This figure visualizes the quantum states generated by the SuperEncoder model trained using three different loss functions: L1, L2, and L3. Each subfigure represents a different loss function used for training, demonstrating how the choice of loss function affects the model's ability to accurately reconstruct the target quantum state. The visual representation of the quantum state helps in understanding the model's performance and identifying any potential issues or limitations.", "section": "3.2 Design Methodology"}, {"figure_path": "r7mj17BKzw/figures/figures_4_2.jpg", "caption": "Figure 5: Convergence of different loss functions.", "description": "This figure shows the convergence curves of three different loss functions (L1, L2, and L3) during the training of the SuperEncoder model.  The x-axis represents the training step, and the y-axis represents the loss value.  The plot visually demonstrates the different convergence behaviors of the three loss functions.  L2 and L3 show significantly faster convergence and lower final loss values compared to L1, suggesting their superiority for training the SuperEncoder model.", "section": "3.2 Design Methodology"}, {"figure_path": "r7mj17BKzw/figures/figures_5_1.jpg", "caption": "Figure 6: Landscape virtualization of different loss functions.", "description": "This figure visualizes the loss landscape for three different loss functions used in training the SuperEncoder model: L1 (parameter-oriented training), L2 (state-oriented training using MSE), and L3 (state-oriented training using fidelity).  The plots show the loss value as a function of two parameters.  The landscapes reveal crucial information about the difficulty of optimizing each loss function.  L1 exhibits a relatively flat landscape with fewer distinct minima, indicating difficulty in converging to a good solution. In contrast, L2 and L3 possess more defined minima, suggesting a smoother and easier optimization process. This difference in landscape characteristics explains why L2 and L3 significantly outperform L1 in terms of the fidelity of the generated quantum states.", "section": "3.2 Design Methodology"}, {"figure_path": "r7mj17BKzw/figures/figures_7_1.jpg", "caption": "Figure 7: Comparison between AE, AAE, and SuperEncoder in terms of circuit depth and fidelity on real devices.", "description": "This figure compares the performance of three quantum state preparation methods (AE, AAE, and SuperEncoder) in terms of circuit depth and fidelity on real quantum devices.  The left panel shows that AE's circuit depth scales exponentially with the number of qubits, while AAE and SuperEncoder maintain shallower depths. The right panel shows the fidelity achieved by each method. While AE shows high fidelity for smaller numbers of qubits, its fidelity drops dramatically as the number of qubits increases.  AAE and SuperEncoder maintain relatively high fidelity, even for a higher number of qubits, although AAE generally outperforms SuperEncoder in terms of fidelity.", "section": "4.2 Evaluation on Synthetic Dataset"}, {"figure_path": "r7mj17BKzw/figures/figures_7_2.jpg", "caption": "Figure 8: Schematic of a QNN (above) and test accuracies of QSP methods on the QML task (below).", "description": "This figure shows the architecture of a Quantum Neural Network (QNN) used in a quantum machine learning task.  The top part illustrates the QNN structure, which includes an encoder block (where classical data is encoded into a quantum state using different QSP methods) followed by multiple entangler layers. The bottom part of the figure presents the test accuracies achieved using each of the three QSP methods: AE (Amplitude Encoding), AAE (Approximate Amplitude Encoding), and SuperEncoder.  The results show that SuperEncoder achieves similar performance to AAE and AE in this particular QML task.", "section": "4.3 Application to Downstream Tasks"}, {"figure_path": "r7mj17BKzw/figures/figures_7_3.jpg", "caption": "Figure 2: An example PQC with two blocks, with each block consisting of a rotation layer (filled blue) plus an entangler layer (filled red).", "description": "This figure shows an example of a Parameterized Quantum Circuit (PQC) used in the paper.  It has two blocks, each comprised of a rotation layer and an entangler layer. The rotation layer consists of single-qubit Ry gates, while the entangler layer is made up of CNOT gates. The color coding in the figure highlights these layers. This is a type of hardware-efficient ansatz, meaning its structure is compatible with the physical constraints of current quantum hardware and avoids the need for additional SWAP gates.  The PQC is used as part of the Approximate Amplitude Encoding (AAE) method and aims to approximate the target quantum state by iteratively adjusting the rotation angles (parameters).", "section": "2.2 Quantum State Preparation"}, {"figure_path": "r7mj17BKzw/figures/figures_13_1.jpg", "caption": "Figure 10: Detailed workflow of SuperEncoder.", "description": "The figure illustrates the workflow of the SuperEncoder model. The target quantum state is fed into a multi-layer perceptron (MLP) which generates the circuit parameters. These parameters are then used by the parameterized quantum circuit (PQC) to prepare a quantum state. Finally, the prepared state is compared to the target state to calculate the loss, which is used for training the MLP. This process eliminates the need for iterative parameter tuning in approximating quantum states.", "section": "A Implementation Details"}, {"figure_path": "r7mj17BKzw/figures/figures_14_1.jpg", "caption": "Figure 11: Fidelity vs. # blocks for 4-qubit states using AAE.", "description": "This figure shows the relationship between the fidelity of quantum state preparation and the number of blocks in an ansatz (a parameterized quantum circuit) using Approximate Amplitude Encoding (AAE). The x-axis represents the number of blocks in the AAE ansatz, and the y-axis represents the fidelity achieved in preparing a 4-qubit quantum state.  The graph shows that the fidelity increases rapidly at first as the number of blocks increases, eventually reaching a plateau near 1.0 (perfect fidelity) after approximately 8 blocks. This indicates that increasing the number of blocks beyond a certain point does not significantly improve the fidelity of the quantum state preparation.", "section": "3.2 Design Methodology"}]