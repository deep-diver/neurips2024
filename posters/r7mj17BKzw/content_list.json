[{"type": "text", "text": "SuperEncoder: Towards Iteration-Free Approximate Quantum State Preparation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Numerous quantum algorithms operate under the assumption that classical data   \n2 has already been converted into quantum states, a process termed Quantum State   \n3 Preparation (QSP). However, achieving precise QSP requires a circuit depth that   \n4 scales exponentially with the number of qubits, making it a substantial obstacle in   \n5 harnessing quantum advantage. Recent research suggests using a Parameterized   \n6 Quantum Circuit (PQC) to approximate a target state, offering a more scalable   \n7 solution with reduced circuit depth compared to precise QSP. Despite this, the need   \n8 for iterative updates of circuit parameters results in a lengthy runtime, limiting its   \n9 practical application. To overcome this challenge, we introduce SuperEncoder,   \n0 a pre-trained classical neural network model designed to directly estimate the   \n11 parameters of a PQC for any given quantum state. By eliminating the need for   \n2 iterative parameter tuning, SuperEncoder represents a pioneering step towards   \n13 iteration-free approximate QSP. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Quantum Computing (QC) leverages quantum mechanics principles to address classically intractable   \n16 problems [47, 36]. Various quantum algorithms have been developed, encompassing quantum  \n17 enhanced linear algebra [15, 48, 45], Quantum Machine Learning (QML) [26, 19, 1, 33, 50, 3],   \n18 quantum-enhanced partial differential equation solvers [31, 13], etc. A notable caveat is that those   \n19 algorithms assume that classical data has been efficiently loaded into a specific quantum state, a   \n20 process known as Quantum State Preparation (QSP).   \n21 However, the realization of QSP presents significant challenges. Ideally, we expect each element of   \n22 the classical data to be precisely transformed into an amplitude of the corresponding quantum state.   \n23 This precise QSP is also known as Amplitude Encoding (AE). However, a critical yet unresolved   \n24 problem of AE is that the required circuit depth grows exponentially with respect to the number of   \n25 qubits [34, 41, 29, 46, 49]. Extensive efforts have been made to alleviate this issue, but they fail to   \n26 address it fundamentally. For example, while some methods introduce ancillary qubits for shallower   \n27 circuit [57, 56, 2], they may encounter an exponential number of ancillary qubits. Other methods aim   \n28 at preparing special quantum states with lower circuit depth, being only effective for either sparse   \n29 states [12, 32] or states with some special distributions [14, 17]. To summarize, realizing AE for   \n30 arbitrary quantum states still remains non-scalable due to its exponential resource requirement with   \n31 respect to the number of qubits. Moreover, in the Noisy Intermediate-Scale Quantum (NISQ) era [42],   \n32 hardware has limited qubit lifetimes and confronts a high risk of decoherence errors when executing   \n33 deep circuits, further exacerbating the problem of AE.   \n34 In fact, precise QSP is unrealistic in the present NISQ era due to the inherent errors of quantum   \n35 devices. Hence, iteration-based Approximate Amplitude Encoding (AAE) emerges as a promising   \n36 technique [59, 35, 52]. Specifically, AAE constructs a quantum circuit with tunable parameters, then   \n37 it iteratively updates the parameters to approximate a target quantum state. Since the updating of   \n38 parameters can be guided by states obtained from noisy devices, AAE is robust to noises, becoming   \n39 especially suitable for NISQ applications. More importantly, AAE has been shown to have shallow   \n40 circuit depth [35, 52], making it more scalable than AE.   \n41 Unfortunately, AAE possesses a drawback that signifi  \n42 cantly undermines its potential advantages \u2014 the lengthy   \n43 runtime stemming from iterative optimizations of param  \n44 eters. For example, when a Quantum Neural Network   \n45 (QNN) [3] is trained and deployed, the runtime of AAE   \n46 dominates the inference time as we demonstrated in Fig. 1.   \n47 Since loading classical data into quantum states becomes   \n48 the bottleneck, the potential advantage of QNN dimin  \n49 ishes no matter how efficient the computations are done   \n50 on quantum devices.   \n51 Compared to AAE, AE employs a pre-defined arithmetic   \n52 decomposition procedure to construct a circuit, thereby   \n53 becoming much faster than AAE at runtime. Therefore,   \n54 it is natural to ask: can we realize both fast and scalable   \n55 methods for arbitrary QSP? This is precisely the question   \n56 we tackle in this paper. Overall, we present three major   \n57 contributions.   \n58 \u2022 Given a Parameterized Quantum Circuit (PQC) $U(\\pmb\\theta)$ that approximates a target quantum state,   \n59 with $\\pmb{\\theta}$ the parameter vector. We show that there exists a deterministic transformation $f$ that could   \n60 map an arbitrary state $|d\\rangle$ to its corresponding parameters $\\pmb{\\theta}$ . Consequently, the parameters can be   \n61 designated by $f$ without time-intensive iterations.   \n62 \u2022 We show that the mapping $f$ is learnable by utilizing a classical neural network model, which   \n63 we term as SuperEncoder. With SuperEncoder, you can have your cake and eat it too, i.e.,   \n64 simultaneously realizing fast and scalable QSP. We develop a prototype model and shed light on   \n65 insights into its training methodology.   \n66 \u2022 We verify the effectiveness of SuperEncoder on both synthetic dataset and representative down  \n67 stream tasks, paving the way toward iteration-free approximate quantum state preparation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "r7mj17BKzw/tmp/677e0910762137253ca813e284c2702e6843e3cd98a41b554b4d6c714bb42884.jpg", "img_caption": ["Figure 1: Breakdown of normalized runtime for QNN inference. Original data are listed in Table 1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 In this section, we commence with some basic concepts about quantum computing [36], and then   \n70 proceed to a brief retrospect of existing QSP methods. ", "page_idx": 1}, {"type": "text", "text": "71 2.1 Quantum Computation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 We use Dirac notation throughout this paper. A pure quantum state is defined by a vector $\\left|\\cdot\\right\\rangle$ named   \n73 \u2018ket\u2019, with the unit length. A state can be written as $\\begin{array}{r}{|\\psi\\rangle=\\sum_{j=1}^{N}\\alpha_{j}|j\\rangle}\\end{array}$ with $\\textstyle\\sum_{j}|\\alpha_{j}|^{2}=1$ , where   \n74 $|j\\rangle$ denotes a computational basis state and $N$ represents the dimension of the complex vector   \n75 space. Density operators describe more general quantum states. Given a mixture of $m$ pure states   \n76 $\\{\\bar{|}\\psi_{i}\\rangle\\}_{i=1}^{m}$ with probabilities $p_{i}$ and $\\begin{array}{r}{\\sum_{i}^{m}\\bar{p}_{i}=1}\\end{array}$ , the density operator $\\rho$ denotes the mixed state as   \n77 $\\textstyle\\rho=\\sum_{i=1}^{m}p_{i}|\\psi_{i}\\rangle\\langle\\psi_{i}|$ with $\\operatorname{Tr}(\\rho)=1$ , where $\\langle\\cdot|$ refers to the conjugate transpose of $\\left|\\cdot\\right\\rangle$ . Generally,   \n78 we use the term fidelity to describe the similarity between an erroneous quantum state and its   \n79 corresponding correct state.   \n80 The fundamental unit of quantum computation is the quantum bit, or qubit. A qubit\u2019s state can be   \n81 expressed as $\\psi=\\alpha|0\\rangle+\\beta|1\\rangle$ . Given $n$ qubits, the state is generalized to $\\begin{array}{r}{\\left|\\psi\\right\\rangle\\stackrel{}{=}\\sum_{j}^{2^{n}}\\left|j\\right\\rangle}\\end{array}$ , where   \n82 $|j\\rangle\\,=\\,|j_{1}j_{2}\\cdot\\cdot\\cdot\\,j_{n}\\rangle$ with $j_{k}$ the state of $k\\mathrm{th}$ qubit in computational basis, and $\\begin{array}{r}{j\\,=\\,\\dot{\\sum_{k=1}^{n}}\\,2^{n-k}j_{k}}\\end{array}$ .   \n83 Applying quantum operations evolves a system from one state to another. Generally, t hese operations   \n84 can be categorized into quantum gates and measurements. Typical single-qubit gates include the   \n85 Pauli gates $\\mathbf{\\bar{\\boldsymbol{X}}}\\equiv\\left[\\mathbf{\\Sigma}_{1}^{0}\\mathbf{\\Sigma}_{0}^{1}\\right]$ , $\\dot{Y}\\equiv\\left[\\begin{array}{l l}{0}&{-\\dot{\\iota}}\\\\ {\\dot{\\iota}}&{0}\\end{array}\\right]$ , $Z\\equiv\\left[1^{\\cal\\Delta},\\stackrel{\\cal0}{\\cal-}1\\right]$ . These gates have associated rotation operations   \n86 $R_{P}(\\theta)\\equiv\\mathrm{e}^{-i\\theta P/2}$ , where $\\theta$ is the rotation angle and $P\\in\\{X,Y,Z\\}^{1}$ . Muti-qubit operations create   \n87 entanglement between qubits, allowing one qubit to interfere with others. In this work, we focus on   \n88 the controlled-NOT (CNOT) gate, with the mathematical form of $\\mathrm{CNOT}\\equiv|0\\rangle\\langle0|\\otimes{\\bf I}_{2}+|1\\rangle\\langle1|\\otimes X$ .   \n89 Quantum measurements extract classical information from quantum states, which is described by   \n90 a collection $\\{M_{m}\\}$ with $\\textstyle\\sum_{m}M_{m}^{\\dagger}M_{m}=\\mathbf{I}$ . Here, $m$ refers to the measurement outcomes that may   \n91 occur in the experiment, with a probability of $p(m)=\\langle\\psi|M_{m}^{\\dagger}M_{m}|\\psi\\rangle$ . The post-measurement state   \n92 of the system becomes $M_{m}|\\psi\\rangle\\bar{/}p(m)$ .   \n93 A quantum circuit is the graphical representation of a series of quantum operations, which can be   \n94 mathematically represented by a unitary matrix $U$ . In the NISQ era, PQC plays an important role   \n95 as it underpins variational quantum algorithms [11, 39]. Typical PQC has the form of $U(\\pmb\\theta)=$   \n96 $\\prod_{i}U_{i}(\\theta_{i})V_{i}$ , where $\\pmb{\\theta}$ is its parameter vector, $U_{i}(\\theta_{i})=\\mathrm{e}^{-i\\theta_{i}P_{i}/2}$ with $P_{i}$ denoting a Pauli gate, and   \n97 $V_{i}$ denotes a fixed gate such as CNOT. For example, a PQC composed of $R_{y}$ gates and CNOT gates   \n98 is depicted in Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "r7mj17BKzw/tmp/7503a051a73468b98ae3710c9fc402bb2b19a348dec67f7b5a4f3ae0642029c7.jpg", "img_caption": ["Figure 2: An example PQC with two blocks, with each block consisting of a rotation layer (filled blue) plus an entangler layer (filled red). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "99 2.2 Quantum State Preparation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 Successful execution of many quantum algorithms requires an initial step of loading classical data   \n101 into a quantum state [5, 15], a process known as quantum state preparation. This procedure involves   \n102 implementing a quantum circuit to evolve a system to a designated state. Here, we focus on amplitude   \n103 encoding and formalize its procedure as follows. Let $^d$ be a real-valued $N$ -dimensional classical   \n104 vector, AE encodes $^d$ into the amplitudes of an $n$ -qubit quantum state $|d\\rangle$ , where $N=2^{n}$ . More   \n105 specifically, the data quantum state is represented by $\\begin{array}{r}{|d\\rangle=\\sum_{j=0}^{N-1}d_{j}|j\\rangle}\\end{array}$ , where $d_{j}$ denotes the $j$ th   \n106 element of the vector $^d$ , and $|j\\rangle$ refers to a computational basis state. The main objective is to generate   \n107 a quantum circuit $U$ that initializes an $n$ -qubit system by $\\begin{array}{r}{U|0\\rangle^{\\otimes n}=\\sum_{j=0}^{N-1}\\alpha_{j}|j\\rangle}\\end{array}$ , whose amplitudes   \n108 $\\{\\alpha_{j}\\}$ are equal to $\\{d_{j}\\}$ . It is widely recognized that constructing such a circuit generally necessitates   \n109 a circuit depth that scales exponentially with $n$ [34, 41]. This property makes AE impractical in   \n110 current NISQ era, as decoherence errors [23] can severely dampen the effectiveness of AE as the   \n111 number of qubits increases [52].   \n112 In response to the inherent noisy nature of current devices, approximate amplitude encoding has   \n113 emerged as a promising technique [59, 35, 52]. Specifically, AAE utilizes a PQC (a.k.a. ansatz) to   \n114 approximate the target quantum state by iteratively updating the parameters of circuit, following   \n115 a similar procedure of other variational quantum algorithms [39, 11]. AAE has been shown to be   \n116 more advantageous for NISQ devices due to its ability to mitigate coherent errors through flexible   \n117 adjustment of circuit parameters, coupled with its lower circuit depth [52]. We denote an ansatz as   \n118 $U(\\pmb\\theta)$ , where $\\pmb{\\theta}$ refers to a vector of tunable parameters for optimizations. A typical ansatz consists   \n119 of several blocks of operations with the same structure. For example, a two-block ansatz with 4   \n120 qubits is shown in Fig. 2, where the rotation layer is composed of single-qubit rotational gates   \n121 $R_{y}(\\theta_{r})=\\mathrm{e}^{-i\\theta_{r}Y/2}$ , and the entangler layer comprises CNOT gates. Note that the entangler layer is   \n122 configurable and hardware-native, which means that we can apply CNOT gates to physically adjacent   \n123 qubits, thereby eliminating the necessity of additional SWAP gates to overcome the topological   \n124 constraints [27]. This type of PQC is also known as hardware-efficient ansatz [20], being widely   \n125 adopted in previous studies of AAE [59, 35, 52]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "126 3 SuperEncoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 3.1 Motivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "128 Although AAE can potentially realize high fidelity QSP with $O(\\mathrm{poly}(n))$ circuit depth [35] with $n$   \n129 the number of qubits, it requires repetitive online tuning of parameters to approximate the target   \n130 state, which may result in an excessively long runtime that undermines its feasibility. Specifically, we   \n131 could consider a simple application scenario in QML. The workflow with AAE is depicted in Fig. 3a.   \n132 During the inference stage, we must iteratively update the parameters of the AAE ansatz for each   \n133 input classical data vector, which may greatly dampen the performance. To quantify this impact, we   \n134 measure the runtime of AAE-based data loading and the total runtime of model inference. As one can   \n135 observe from Table 1, AAE dominates the runtime, thereby becoming the performance bottleneck.   \n136 The necessity of time-intensive iterations is grounded in the following assumption \u2014 Given an   \n137 arbitrary quantum state $|\\psi\\rangle$ , there does not exist a deterministic transformation $f:\\left|\\psi\\right\\rangle\\rightarrow\\pmb{\\theta}$ , where   \n138 $\\pmb{\\theta}$ refers to the vector of parameters enabling a PQC to prepare an approximated state of $|\\psi\\rangle$ . This   \n139 assumption seems intuitively correct given the randomness of target states. However, we argue that a   \n140 universal mapping $f$ exists for any arbitrary data state $|\\psi\\rangle$ . Taking a little thought of AE, we see that   \n141 it implies the following conclusion: given an arbitrary state $|\\psi\\rangle$ , there exists an universal arithmetic   \n142 decomposition procedure $g\\;:\\;|\\psi\\rangle\\;\\rightarrow\\;U$ satisfying $U|0\\rangle\\,=\\,|\\psi\\rangle$ . Inspired by this deterministic   \n143 transformation, it is natural to ask: is there an universal transformation $g^{\\prime}:|\\dot{\\psi}\\rangle\\rightarrow U^{\\prime}$ satisfying   \n144 $E(U^{\\prime}|0\\rangle,|\\psi\\rangle)\\leq\\epsilon?$ Here $E$ denotes the deviation between the prepared state by a circuit $U^{\\prime}$ and the   \n145 target state, and $\\epsilon$ refers to certain acceptable error threshold. Since the structure of PQC in AAE   \n146 is the same for any target state, $U^{\\prime}$ is determined by $\\pmb{\\theta}$ . Then, the problem is reduced to exploring   \n147 the existence of $f:\\left|\\psi\\right\\rangle\\rightarrow\\pmb{\\theta}$ . Should $f$ exist, the overhead of online iterations could be eliminated,   \n148 resulting in a novel QSP method being both fast and scalable. ", "page_idx": 3}, {"type": "table", "img_path": "r7mj17BKzw/tmp/582b264b553990a8c7d6a314cfc9600f88f3567167c7f7c5d586729a752e937a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "r7mj17BKzw/tmp/25b1bfb358ae3929cba76902ad9175f347ba6c93360e6117155eb88b66b08ad2.jpg", "img_caption": ["Table 1: Performance overhead of AAE. We break down the averaged inference runtime per sample from the MNIST dataset. $T_{\\mathrm{AAE}}$ denotes time spent on loading classical data into quantum state using AAE, and $T_{\\mathrm{total}}$ refers to total runtime. ", "Figure 3: Comparison between AAE and SuperEncoder. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "149 3.2 Design Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "150 Let $|\\psi\\rangle$ be the target state, and $U(\\pmb\\theta)$ be the PQC used in AAE with $\\pmb{\\theta}$ the optimized parameters.   \n151 Our goal is to develop a model, termed SuperEncoder, to approximate the mapping $f^{^{\\breve{\\prime}}\\cdot\\mid\\psi\\rangle}\\to\\pmb\\theta$ .   \n152 Referring back to the scenario in QML, the workflow with SuperEncoder becomes iteration-free, as   \n153 depicted in Fig. 3b.   \n154 Since neural networks could be used to approximate any continuous function [6], a natural solution is   \n155 to use a neural network to approximate $f$ . Specifically, we adopt a Multi-Layer Perceptron (MLP) as   \n156 the backbone model for approximating $f$ . However, training this model is nontrivial. Particularly, we   \n157 find it challenging to design a proper loss function. In the remainder of this section, we explore three   \n158 different designs and analyze their performance. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "r7mj17BKzw/tmp/9bbcdaf01cd048b17d3250c5a314214106b6e3244fed079c37e369ad1ec87156.jpg", "img_caption": ["(a) Target state. (b) SuperEncoder- ${\\mathcal{L}}_{1}$ (c) SuperEncoder- $\\mathcal{L}_{3}$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: Virtualization of states generated by SuperEncoder trained with different loss functions. $\\mathcal{L}_{2}$   \nis omitted as it produces very similar results to $\\mathcal{L}_{3}$ . ", "page_idx": 4}, {"type": "text", "text": "159 The first and most straightforward method is parameter-oriented training \u2014 setting the loss function   \n160 ${\\mathcal{L}}_{1}$ as the MSE between the target parameters $\\pmb{\\theta}$ from AAE and the output parameters $\\hat{\\pmb\\theta}$ from   \n161 SuperEncoder. To evaluate the performance of ${\\mathcal{L}}_{1}$ , we train a SuperEncoder using MNIST dataset,   \n162 and test if it could load a test digit image into a quantum state with high fidelity. All images are   \n163 downsampled and normalized into 4-qubit states for quick evaluation.   \n164 Unfortunately, results in Table 2 show that ${\\mathcal{L}}_{1}$ achieves poor   \n165 performance. The average fidelity of prepared quantum states   \n166 is only 0.6208. As demonstrated in Fig. 4, $\\mathcal{L}_{1}$ generates a state   \n167 that losses the patterns of the original state. Additionally, utiliz  \n168 ing $\\mathcal{L}_{1}$ implies that we need to first generate target parameters   \n169 using AAE, of which the long runtime hinders pre-training on   \n170 larger datasets. Consequently, required is a more effective loss   \n171 function design without involving AAE. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "r7mj17BKzw/tmp/ed79843a50158d2499042fbf983972fed533a7c3dfe25873116ed4bc500f1116.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 2: Fidelity comparison between SuperEncoders trained with different loss functions. ", "page_idx": 4}, {"type": "text", "text": "172 To address this challenge, we propose a state-oriented training   \n173 methodology, which employs quantum states as targets to guide   \n174 optimizations. Specifically, we may apply $\\hat{\\pmb\\theta}$ to the circuit and exe  \n175 cute it to obtain the prepared state $\\hat{\\psi}$ . Then it is possible to calculate   \n176 the difference between $\\hat{\\psi}$ and $\\psi$ as the loss to optimize SuperEncoder.   \n177 In contrast to parameter-oriented training, this approach applies to   \n178 larger datasets as it decouples the training procedure from AAE. We   \n179 utilize two different state-oriented metrics, the first being the MSE   \n180 between $\\hat{\\psi}$ and $\\psi$ , denoted as $\\mathcal{L}_{2}$ , and the second is the fidelity of   \n181 $\\hat{\\psi}$ relative to $\\psi$ , expressed as $\\mathcal{L}_{3}=1-|\\langle\\hat{\\psi}|\\psi\\rangle|^{2}$ [25]. Results in   \n182 Table 2 show that ${\\mathcal{L}}_{2}$ and $\\mathcal{L}_{3}$ achieve remarkably higher fidelity than   \n183 ${\\mathcal{L}}_{1}$ . Besides, we observe that $\\mathcal{L}_{3}$ prepares a state very similar to the   \n184 target one (Fig. 4), verifying that state-oriented training is more effective than parameter-oriented   \n185 training.   \n186 Landscape Analysis. To understand the efficacy of these loss functions, we further analyze their   \n187 landscapes following previous studies [28, 40, 18]. To gain insight from the landscape, we plot Fig. 6   \n188 using the same scale and color gradients [18]. Compared to state-oriented losses $\\mathcal{L}_{2}$ and $\\mathcal{L}_{3}$ ), ${\\mathcal{L}}_{1}$ has   \n189 a largely flat landscape with non-decreasing minima, thus the model struggles to explore a viable   \n190 path towards a lower loss value, a similar pattern can also be observed in Fig. 5. In contrast, $\\mathcal{L}_{2}$   \n191 and $\\mathcal{L}_{3}$ have much lower minima and successfully converge to smaller loss values. Furthermore, we   \n192 observe from Fig. 6 that $\\mathcal{L}_{3}$ has a wider minima than $\\mathcal{L}_{2}$ , which may indicate a better generalization   \n193 capability [40].   \n194 Gradient Analysis. Based on the landscape analysis, we adopt $\\mathcal{L}_{3}$ as the loss function to train   \n195 SuperEncoder. We note that $\\mathcal{L}_{3}$ can be written as $1-\\langle\\psi|\\hat{\\psi}\\rangle\\langle\\hat{\\psi}|\\psi\\rangle$ . If $\\hat{\\rho}$ is a pure state, it is equivalent   \n196 to $|\\hat{\\psi}\\rangle\\langle\\hat{\\psi}|$ . Then $\\mathcal{L}_{3}$ is given by $\\mathcal{L}_{3}=1-\\langle\\psi|\\hat{\\rho}|\\psi\\rangle$ .   \n197 This re-formalization is important as only the mixed state $\\hat{\\rho}$ could be obtained in noisy environments.   \n198 Suppose an $n$ -qubit circuit is parameterized by $m$ parameters $\\hat{\\pmb{\\theta}}=[\\hat{\\theta}_{1},\\dots,\\hat{\\theta}_{k},\\dots,\\hat{\\theta}_{m}]$ . Let W be   \n199 the weight matrix of MLP, with $k,l$ the element indices. We analyze the gradient of $\\mathcal{L}_{3}$ w.r.t. $W_{k,l}$ to   \n200 showcase its feasibility in different quantum computing environments. ", "page_idx": 4}, {"type": "image", "img_path": "r7mj17BKzw/tmp/e1ded6f3b1cfd612488f77cd8b98a9050d450cfcc0f9e0df9377e87f3a33b77e.jpg", "img_caption": ["Figure 5: Convergence of different loss functions. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "r7mj17BKzw/tmp/b70b3f8a1be2cf130f5c7fed3749acd27e190ea2be1f65a159ba60ec687a46c7.jpg", "img_caption": ["Figure 6: Landscape virtualization of different loss functions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{W_{k,l}}\\mathcal{L}_{3}=\\frac{\\partial\\mathcal{L}_{3}}{\\partial W_{k,l}}=-\\langle\\psi|\\frac{\\partial\\hat{\\rho}}{\\partial W_{k,l}}|\\psi\\rangle}}\\\\ &{=-\\langle\\psi|\\left[\\frac{\\sum_{j=1}^{m}\\frac{\\partial\\hat{\\rho}_{1,1}}{\\partial\\theta_{j}}\\frac{\\partial\\theta_{j}}{\\partial W_{k,l}}}{\\vdots}&{\\ddots}&{\\sum_{j=1}^{m}\\frac{\\partial\\hat{\\rho}_{1,N}}{\\partial\\theta_{j}}\\frac{\\partial\\theta_{j}}{\\partial W_{k,l}}}\\right]|\\psi\\rangle,}\\\\ &{}&{\\ddots}&{\\vdots}\\\\ {\\sum_{j=1}^{m}\\frac{\\partial\\hat{\\rho}_{N,1}}{\\partial\\theta_{j}}\\frac{\\partial\\theta_{j}}{\\partial W_{k,l}}}&{\\ddots}&{\\sum_{j=1}^{m}\\frac{\\partial\\hat{\\rho}_{N,N}}{\\partial\\theta_{j}}\\frac{\\partial\\theta_{j}}{\\partial W_{k,l}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "201 The calculation of $\\frac{\\partial\\theta_{j}}{\\partial W_{k,l}}$ can be easily done on classical devices using backpropagation supported by   \n202 automatic differentiation frameworks. Therefore, we only focus on \u2202\u2202\u03c1\u02c6\u03b8i,j . In a simulation environ  \n203 ment, the calculation of $\\hat{\\rho}$ is conducted via noisy quantum circuit simulation, which is essentially a   \n204 series of tensor operations on state vectors. Therefore, the calculation o f \u2202\u2202\u03c1\u02c6\u03b8i,j is compatible with   \n205 backpropagation. The situation on real devices becomes more complicated. On real devices, the   \n206 mixed state $\\hat{\\rho}$ is reconstructed through quantum tomography [7] based on classical shadow [55, 16].   \n207 Here, for notion simplicity, we denote the process of classical shadow as a transformation $\\boldsymbol{S}$ , and   \n208 denote the measurement expectations of the ansatz as $U({\\hat{\\theta}})$ . Thus the reconstructed density ma  \n209 ${\\hat{\\rho}}\\,=\\,S(U({\\hat{\\pmb\\theta}}))$ $\\hat{\\rho}_{i,j}$ $\\hat{\\theta}_{k}$ $\\sum{\\boldsymbol{u}}\\;{\\frac{\\partial{\\hat{\\rho}}_{i,j}}{\\partial U({\\hat{\\pmb{\\theta}}})}}\\,{\\frac{\\partial U({\\hat{\\pmb{\\theta}}})}{\\partial{\\hat{\\boldsymbol{\\theta}}}_{k}}}$   \n210 Here $\\frac{\\partial\\hat{\\rho}_{i,j}}{\\partial U(\\hat{\\pmb\\theta})}$ can be efficiently calculated on classical devices using backpropagation, as $\\boldsymbol{S}$ operates   \n211 on expectation values on classical devices. However, $U({\\hat{\\theta}})$ involves state evolution on quantum   \n212 devices, where back-propagation is impossible due to the No-Cloning theorem [36]. Fortunately,   \n213 it is possible to utilize the parameter shift rule [8, 4, 53] to calculate $\\frac{\\partial U(\\hat{\\pmb\\theta})}{\\partial{\\theta_{k}}}$ In this way, the   \n214 gradients of the circuit function $U$ with respect to $\\theta_{j}$ are \u2202\u2202U\u03b8(k\u03b8\u02c6) = 12 (U(\u03b8+) \u2212U(\u03b8\u2212)), where   \n215 $\\theta_{+}=[\\theta_{1},\\dots,\\theta_{k}+\\textstyle\\frac{\\pi}{2},\\dots,\\theta_{m}],\\theta_{-}=[\\theta_{1},\\dots,\\theta_{k}-\\textstyle\\frac{\\pi}{2},\\dots,\\theta_{m}]$ . To summarize, training SuperEn  \n216 coder with $\\mathcal{L}_{3}$ is theoretically feasible on both simulators and real devices. ", "page_idx": 5}, {"type": "text", "text": "217 4 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "218 4.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "219 Datasets. To train a SuperEncoder for arbitrary quantum states, we need a dataset comprising a wide   \n220 range of quantum states with different distributions. To our knowledge, there is no dataset dedicated   \n221 for this special purpose. A natural solution is to use readily available datasets from classical machine   \n222 learning domains (e.g., ImageNet [9], Places [58], SQuAD [44]) by normalizing them to quantum   \n223 states. However, QSP is essential in various application scenarios besides QML. The classical data to   \n224 be loaded may not only contain natural images or languages but also contain arbitrary data (e.g., in   \n225 HHL algorithm [15]). Therefore, we construct a training dataset adapted from FractalDB-60 [21] with   \n226 60k samples, a formula-driven dataset originally designed for computer vision without any natural   \n227 images. We also construct a separate dataset to test the performance of QSP, which consists of data   \n228 sampled from different statistical distributions, including uniform, normal, log-normal, exponential,   \n229 and Dirichlet distributions, with 3000 samples per distribution. Hereafter we refer this dataset as the   \n230 synthetic dataset.   \n231 Platforms. We implement SuperEncoder using PennyLane [34], PyTorch [37] and Qiskit [43].   \n232 Simulations are done on a Ubuntu server with 768 GB memory, two 32-core Intel(R) Xeon(R) Silver   \n233 4216 CPU with $2.10\\:\\mathrm{GHz}$ , and 2 NVIDIA A-100 GPUs. IBM quantum cloud platform2 is adopted to   \n234 evaluate the performance on real quantum devices.   \n235 Metrics. We evaluate SuperEncoder and compare it to AE and AAE in terms of runtime, scalability,   \n236 and fidelity. Runtime refers to how long it takes to prepare a quantum state. Scalability refers to how   \n237 the circuit depth grows with the number of qubits. Fidelity evaluates the similarity between prepared   \n238 quantum states and target quantum states. Specifically, the fidelity for two mixed states given by   \n239 density matrices $\\rho$ and $\\hat{\\rho}$ is defined as $F(\\rho,\\bar{\\rho})=\\operatorname{Tr}\\big(\\sqrt{\\sqrt{\\rho}\\hat{\\rho}\\sqrt{\\rho}}\\big)^{2}\\in[0,1]$ . A larger $F$ indicates a   \n240 better fidelity.   \n241 Implementation. We implement SuperEncoder using an MLP consisting of two hidden layers.   \n242 The dimensions of input and output layers are respectively set to $2^{n}$ and $m$ , where $n$ refers to the   \n243 number of qubits and $m$ refers to the number of parameters. We adopt $\\mathcal{L}_{3}$ as the loss function.   \n244 Training data are down-sampled, flattened, and normalized to $2^{n}$ -dimensional state vectors. We   \n245 adopt the hardware efficient ansatz [20] (Fig. 2) as the backbone of quantum circuits and use the   \n246 same structure for AAE. Given a target state, a pre-trained SuperEncoder model is invoked to   \n247 generate parameters and thus the circuit for QSP. While for AAE, we employ online iterations for   \n248 each state. For AE, the arithmetic decomposition method in PennyLane [34, 4] is adopted. We   \n249 defer more details about implementation to Appendix A. Our framework is open-source at https:   \n250 //anonymous.4open.science/r/SuperEncoder-A733 with detailed instructions to reproduce   \n251 our results. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "252 4.2 Evaluation on Synthetic Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "253 For simplicity and without loss of generality, we focus our discussion on the results of 4-qubit QSP   \n254 tasks. The outcomes for larger quantum states are detailed in Appendix B.1. The parameters of both   \n255 AAE and SuperEncoder are optimized based on ideal quantum circuit simulation.   \n256 Runtime. The runtime and fidelity results, evaluated on the synthetic dataset, are presented in Table 3.   \n257 We observe that SuperEncoder runs faster than AAE by orders of magnitudes and has a similar   \n258 runtime to AE, affirming that SuperEncoder effectively overcomes the main drawback of AAE. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "r7mj17BKzw/tmp/d0949bb68797f9b6cb6ffbab6e5fb8b989e1b8a8c4bbd7803f11cdc82f3265bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 3: Comparison between AE, AAE and SuperEncoder in terms of runtime and fidelity. ", "page_idx": 6}, {"type": "image", "img_path": "r7mj17BKzw/tmp/61bd9ba3de4bd992a6429a8c9bb84f53aadff7310e9cc57cf25fef08ddc8881e.jpg", "img_caption": ["Figure 7: Comparison between AE, AAE, and SuperEncoder in terms of circuit depth and fidelity on real devices. ", "(a) Scaling of circuit depth w.r.t. # qubits. (b) Fidelity of different QSP methods on ibm_osaka. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "259 Scalability. Although AE runs fast, it exhibits poor scalability since the circuit depth grows exponen  \n260 tially with the number of qubits. The depth of AAE is empirically determined by increasing depth   \n261 until the final fidelity does not increase, same depth is adopted for SuperEncoder. We deter the details   \n262 of determining the depth of AAE/SuperEncoder to Appendix A. As shown in Fig. 7a, the depth of   \n263 AE grows fast and becomes much larger than AAE/SuperEncoder, e.g., the depth of AE for a 8-qubit   \n264 state is 984, whereas the depth of AAE/SuperEncoder is only 120.   \n265 Fidelity. From Table 3, it is evident that SuperEncoder ex  \n266 periences notable fidelity degradation when compared with   \n267 AAE and AE. Specifically, the average fidelity of SuperEn  \n268 coder is 0.9307, whereas AAE and AE achieve higher av  \n269 erage fidelities of 0.9994 and 1.0, respectively. Note that,   \n270 although AE demonstrates the highest fidelity under ideal   \n271 simulation, its performance deteriorates significantly in   \n272 noisy environments. Fig. 7b presents the performance of   \n273 these three QSP methods on quantum states with 4, 6, and   \n274 8 qubits on the ibm_osaka machine. While the fidelity   \n275 of AE is higher than AAE/SuperEncoder on the 4-qubit   \n276 and 6-qubit states, its fidelity on the 8-qubit state is only   \n277 0.0049, becoming much lower than AAE/SuperEncoder. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "r7mj17BKzw/tmp/1d043cc4276ab05da4ca0bc6d57983741768029f9f229070c5f44c54149e144b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 8: Schematic of a QNN (above) and test accuracies of QSP methods on the QML task (below). ", "page_idx": 7}, {"type": "text", "text": "278 This decline is primarily attributed to its large circuit depth as shown in Fig. 7a. ", "page_idx": 7}, {"type": "text", "text": "279 4.3 Application to Downstream Tasks ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "r7mj17BKzw/tmp/6db185cbef79ae4dd5e568eb9c197a0bb95f7dcbd40273fb2a735c7444f00971.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Quantum Machine Learning. We first apply SuperEncoder to a QML task. MNIST dataset is adopted for demonstration, we extract a sub-dataset composed on digits 3 and 6 for evaluation. The quantum circuit that implements a QNN is depicted in Fig. 8, which consists of an encoder block and $m$ entangler layers. Here the encoder block is implemented via QSP circuits, either AE, AAE, or SuperEncoder, of which the parameters are frozen during the training of QNN. The test results are shown in Fig. 8, we observe that SuperEncoder achieves similar performance with AAE and AE. The reason lies in the fact that classification tasks can be robust to noises. Consequently, approximate QSP (AAE and SuperEncoder) with a certain degree of fidelity loss is tolerable. ", "page_idx": 7}, {"type": "text", "text": "292 Figure 9: Schematic of $\\mathrm{HHL}$ .   \n293   \n294 ", "page_idx": 7}, {"type": "text", "text": "HHL Algorithm. Besides QML, quantum-enhanced linear algebra algorithms are another important set of applications that heavily rely on QSP. The most famous algorithm is the HHL algorithm [15]. The ", "page_idx": 7}, {"type": "text", "text": "295 problem can be defined as, given a matrix $\\mathbf{A}\\in\\mathbb{C}^{N\\times N}$ , and a vector $\\mathbf{b}\\in\\mathbb{C}^{N}$ , find $\\bar{\\mathbf{x}}\\in\\mathbb{C}^{N}$ satisfying   \n296 $\\mathbf{Ax}=\\mathbf{b}$ . A typical implementation of HHL utilizes the circuit depicted in Fig. 9. The outline of   \n297 $\\mathrm{HHL}$ is as follows. (i) Apply a QSP circuit to prepare the quantum state $|\\mathbf{b}\\rangle$ . (ii) Apply Quantum   \n298 Phase Estimation [10] (QPE) to estimate the eigenvalue of $\\mathbf{A}$ (iii) Apply conditioned rotation gates   \n299 on ancillary qubits based on the eigenvalues (R). (iv) Apply an inverse QPE (QPE_inv) and measure   \n300 the ancillary qubits to reconstruct the solution vector x. Note that, HHL does not return the solution $\\mathbf{x}$   \n301 itself, but rather an approximation of the expectation value of some operator $\\mathbf{M}$ associated with $\\mathbf{x}$ , e.g.,   \n302 $\\mathbf{x}^{\\dagger}\\mathbf{M}\\mathbf{x}$ . Here, we adopt an optimized version of HHL proposed by Vazquez et al. [51] for evaluation.   \n303 To compare the performance between different QSP methods, we construct linear equations with   \n304 fixed matrix A and operator M, while we sample different vectors from our synthetic dataset as b.   \n305 Results are concluded in Table 4. Unlike QML, HHL expects precise QSP, thus we take the results   \n306 from AE as the ground truth values and compare the relative error between AAE/SuperEncoder and   \n307 AE. The relative error of SuperEncoder is $2.4094\\%$ , while the error of AAE is only $0.3326\\%$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "308 4.4 Discussion and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "309 The results of our evaluation can be concluded in two folds.   \n310 (i) SuperEncoder effectively eliminates the iteration over  \n311 head of AAE, thereby becoming both fast and scalable.   \n312 However, it has a notable degradation in fidelity. (ii) The   \n313 impact of fidelity degradation varies across different down  \n314 stream applications. For QML, the fidelity degradation is   \n315 affordable as long as the prepared states are distinguish  \n316 able across different classes. However, algorithms like   \n317 HHL rely on precise QSP to produce the best result. In   \n318 these algorithms, SuperEncoder suffers from higher error   \n319 ratio than AAE. ", "page_idx": 8}, {"type": "table", "img_path": "r7mj17BKzw/tmp/e10cbf0fc90e55a0204b9961c0ffd0a65d496450cc51cfb4468bcaca27d993a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Performance of different QSP methods in HHL algorithm. \u2018Avg err\u2019 denotes the average relative errors between AAE/SuperEncoder and AE. ", "page_idx": 8}, {"type": "text", "text": "320 Note that, the current evaluation results may not reflect the   \n321 actual performance of SuperEncoder on real NISQ devices.   \n322 Recent work has shown that AAE achieves significantly better fidelity than AE does [52]. This is due   \n323 to the intrinsic noise awareness of AAE, as it could obtain states from noisy devices to guide updating   \n324 parameters with better robustness. In essence, the proposed SuperEncoder possesses the same nature   \n325 as AAE. Unfortunately, although the noise-robustness of AAE can be evaluated on a small set of test   \n326 samples, it is difficult to perform noise-aware training for SuperEncoder as it requires a large dataset   \n327 for pre-training. Consequently, SuperEncoder relies on huge amounts of interactions with noisy   \n328 devices, thereby becoming extremely time-consuming. As a result, the effectiveness of SuperEncoder   \n329 in noisy environments remains largely unexplored, which we leave for future exploration. More   \n330 discussion about this perspective is in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "331 5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "332 Besides QSP, there are other methods for loading classical data into quantum states. These methods   \n333 can be roughly regarded as quantum feature embedding primarily used in QML, which maps classical   \n334 data to a completely different distribution encoded in quantum states. A widely used embedding   \n335 method is known as angle embedding. Li et al. have proven that this method has a concentration issue,   \n336 which means that the encoded states may become indistinguishable as the circuit depth increases [26].   \n337 Lei et al. proposed an automatic design framework for efficient quantum feature embedding, resolving   \n338 the issue of concentration [24]. The central idea of this framework is to search for the most efficient   \n339 circuit architecture for a given classical input, which is also known as Quantum Architecture Search   \n340 (QAS) [38, 30, 54]. While the application scenario of quantum feature embedding is largely limited   \n341 to QML, QSP has broader usage in general quantum applications, distinguishing SuperEncoder from   \n342 all aforementioned work. ", "page_idx": 8}, {"type": "text", "text": "343 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "344 In this work, we propose SuperEncoder, a neural network-based QSP framework. Instead of iteratively   \n345 tuning the circuit parameters to approximate each quantum state, as is done in AAE, we adopt a   \n346 different approach by directly learning the relationship between target quantum states and the required   \n347 circuit parameters. SuperEncoder combines the scalable circuit architecture of AAE with the fast   \n348 runtime of AE, as verified by a comprehensive evaluation on both synthetic dataset and downstream   \n349 applications. ", "page_idx": 8}, {"type": "text", "text": "350 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "351 [1] Amira Abbas, David Sutter, Christa Zoufal, Aur\u00e9lien Lucchi, Alessio Figalli, and Stefan   \n352 Woerner. The power of quantum neural networks. Nature Computational Science, 1(6):403\u2013409,   \n353 2021.   \n354 [2] Israel F Araujo, Daniel K Park, Teresa B Ludermir, Wilson R Oliveira, Francesco Petruccione,   \n355 and Adenilton J Da Silva. Configurable sublinear circuits for quantum state preparation.   \n356 Quantum Information Processing, 22(2):123, 2023.   \n357 [3] Johannes Bausch. Recurrent quantum neural networks. Advances in neural information   \n358 processing systems, 33:1368\u20131379, 2020.   \n359 [4] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Shahnawaz Ahmed, Vishnu   \n360 Ajith, M Sohaib Alam, Guillermo Alonso-Linaje, B AkashNarayanan, Ali Asadi, et al. Pen  \n361 nylane: Automatic differentiation of hybrid quantum-classical computations. arXiv preprint   \n362 arXiv:1811.04968, 2018.   \n363 [5] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth   \n364 Lloyd. Quantum machine learning. Nature, 549(7671):195\u2013202, 2017.   \n365 [6] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural   \n366 networks with arbitrary activation functions and its application to dynamical systems. IEEE   \n367 Transactions on Neural Networks, 6(4):911\u2013917, 1995.   \n368 [7] Marcus Cramer, Martin B Plenio, Steven T Flammia, Rolando Somma, David Gross, Stephen D   \n369 Bartlett, Olivier Landon-Cardinal, David Poulin, and Yi-Kai Liu. Efficient quantum state   \n370 tomography. Nature communications, 1(1):149, 2010.   \n371 [8] Gavin E Crooks. Gradients of parameterized quantum gates using the parameter-shift rule and   \n372 gate decomposition. arXiv preprint arXiv:1905.13311, 2019.   \n373 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n374 scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern   \n375 recognition, pages 248\u2013255. Ieee, 2009.   \n376 [10] Uwe Dorner, Rafal Demkowicz-Dobrzanski, Brian J Smith, Jeff S Lundeen, Wojciech   \n377 Wasilewski, Konrad Banaszek, and Ian A Walmsley. Optimal quantum phase estimation.   \n378 Physical review letters, 102(4):040403, 2009.   \n379 [11] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization   \n380 algorithm. arXiv preprint arXiv:1411.4028, 2014. https://doi.org/10.48550/arXiv.   \n381 1411.4028.   \n382 [12] Niels Gleinig and Torsten Hoefler. An efficient algorithm for sparse quantum state preparation.   \n383 In 2021 58th ACM/IEEE Design Automation Conference (DAC), pages 433\u2013438. IEEE, 2021.   \n384 [13] Javier Gonzalez-Conde, \u00c1ngel Rodr\u00edguez-Rozas, Enrique Solano, and Mikel Sanz. Simulating   \n385 option price dynamics with exponential quantum speedup. arXiv preprint arXiv:2101.04023,   \n386 2021.   \n387 [14] Javier Gonzalez-Conde, Thomas W Watts, Pablo Rodriguez-Grasa, and Mikel Sanz. Efficient   \n388 quantum amplitude encoding of polynomial functions. Quantum, 8:1297, 2024.   \n389 [15] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems   \n390 of equations. Physical Review Letters, 103(15):150502, 2009. https://doi.org/10.1103/   \n391 PhysRevLett.103.150502.   \n392 [16] Hsin-Yuan Huang. Learning quantum states from their classical shadows. Nature Reviews   \n393 Physics, 4(2):81\u201381, 2022.   \n394 [17] Jason Iaconis, Sonika Johri, and Elton Yechao Zhu. Quantum state preparation of normal   \n395 distributions using matrix product states. npj Quantum Information, 10(1):15, 2024.   \n396 [18] Christian Cmehil-Warn Jacob Hansen. Loss landscapes. In ICLR Blog Track, 2022. https://loss  \n397 landscapes.github.io/Loss-Landscapes-Blog/2022/12/01/loss-landscapes/.   \n398 [19] Weiwen Jiang, Jinjun Xiong, and Yiyu Shi. A co-design framework of neural networks and   \n399 quantum circuits towards quantum advantage. Nature Communications, 12(1):579, 2021.   \n400 https://doi.org/10.1038/s41467-020-20729-5.   \n401 [20] Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M.   \n402 Chow, and Jay M. Gambetta. Hardware-efficient variational quantum eigensolver for small   \n403 molecules and quantum magnets. Nature, 549(7671):242\u2013246, September 2017.   \n404 [21] Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada,   \n405 Nakamasa Inoue, Akio Nakamura, and Yutaka Satoh. Pre-training without natural images. In   \n406 Proceedings of the Asian Conference on Computer Vision, 2020.   \n407 [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint   \n408 arXiv:1412.6980, 2014.   \n409 [23] Philip Krantz, Morten Kjaergaard, Fei Yan, Terry P Orlando, Simon Gustavsson, and William D   \n410 Oliver. A quantum engineer\u2019s guide to superconducting qubits. Applied physics reviews, 6(2),   \n411 2019.   \n412 [24] Cong Lei, Yuxuan Du, Peng Mi, Jun Yu, and Tongliang Liu. Neural auto-designer for enhanced   \n413 quantum kernels. In The Twelfth International Conference on Learning Representations, 2023.   \n414 [25] Nelson Leung, Mohamed Abdelhafez, Jens Koch, and David Schuster. Speedup for quantum   \n415 optimal control from automatic differentiation based on graphics processing units. Physical   \n416 Review A, 95(4):042318, 2017. https://doi.org/10.1103/PhysRevA.95.042318.   \n417 [26] Guangxi Li, Ruilin Ye, Xuanqiang Zhao, and Xin Wang. Concentration of data encoding in   \n418 parameterized quantum circuits. Advances in Neural Information Processing Systems, 35:19456\u2013   \n419 19469, 2022.   \n420 [27] Gushu Li, Yufei Ding, and Yuan Xie. Tackling the qubit mapping problem for nisq-era quantum   \n421 devices. In Proceedings of the Twenty-Fourth International Conference on Architectural   \n422 Support for Programming Languages and Operating Systems, pages 1001\u20131014, 2019. https:   \n423 //doi.org/10.1145/3297858.3304023.   \n424 [28] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss   \n425 landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n426 [29] Gui-Lu Long and Yang Sun. Efficient scheme for initializing a quantum register with an   \n427 arbitrary superposed state. Physical Review A, 64(1):014303, 2001.   \n428 [30] Xudong Lu, Kaisen Pan, Ge Yan, Jiaming Shan, Wenjie Wu, and Junchi Yan. Qas-bench:   \n429 rethinking quantum architecture search and a benchmark. In International Conference on   \n430 Machine Learning, pages 22880\u201322898. PMLR, 2023.   \n431 [31] Michael Lubasch, Jaewoo Joo, Pierre Moinier, Martin Kiffner, and Dieter Jaksch. Variational   \n432 quantum algorithms for nonlinear problems. Physical Review A, 101(1):010301, 2020.   \n433 [32] Rui Mao, Guojing Tian, and Xiaoming Sun. Towards optimal circuit size for sparse quantum   \n434 state preparation. arXiv e-prints, pages arXiv\u20132404, 2024.   \n435 [33] Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum circuit   \n436 learning. Physical Review A, 98(3):032309, 2018.   \n437 [34] Mikko M\u00f6tt\u00f6nen, JJ Vartiainen, Ville Bergholm, and Martti M Salomaa. Transformation of   \n438 quantum states using uniformly controlled rotations. Quantum Information and Computation, 5,   \n439 2005.   \n440 [35] Kouhei Nakaji, Shumpei Uno, Yohichi Suzuki, Rudy Raymond, Tamiya Onodera, Tomoki   \n441 Tanaka, Hiroyuki Tezuka, Naoki Mitsuda, and Naoki Yamamoto. Approximate amplitude   \n442 encoding in shallow parameterized quantum circuits and its application to financial market   \n443 indicators. Physical Review Research, 4(2):023136, 2022.   \n444 [36] Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information. 2010.   \n445 [37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n446 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative   \n447 style, high-performance deep learning library. Advances in neural information processing   \n448 systems, 32, 2019.   \n449 [38] Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig, Vedran Dunjko,   \n450 and Onur Danaci. Curriculum reinforcement learning for quantum architecture search under   \n451 hardware errors. In The Twelfth International Conference on Learning Representations, 2024.   \n452 [39] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J   \n453 Love, Al\u00e1n Aspuru-Guzik, and Jeremy L O\u2019brien. A variational eigenvalue solver on a photonic   \n454 quantum processor. Nature communications, 5(1):4213, 2014. https://doi.org/10.1038/   \n455 ncomms5213.   \n456 [40] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley.   \n457 Relative flatness and generalization. Advances in neural information processing systems,   \n458 34:18420\u201318432, 2021.   \n459 [41] Martin Plesch and C\u02c7aslav Brukner. Quantum-state preparation with universal gate decomposi  \n460 tions. Physical Review A, 83(3):032302, 2011.   \n461 [42] John Preskill. Quantum computing in the NISQ era and beyond. Quantum, 2:79, 2018.   \n462 [43] Qiskit contributors. Qiskit: An open-source framework for quantum computing, 2023.   \n463 [44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: $100{,}000{+}$ questions   \n464 for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.   \n465 [45] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. Prediction by linear regression on a   \n466 quantum computer. Physical Review A, 94(2):022342, 2016.   \n467 [46] Vivek V Shende, Stephen S Bullock, and Igor L Markov. Synthesis of quantum logic circuits. In   \n468 Proceedings of the 2005 Asia and South Pacific Design Automation Conference, pages 272\u2013275,   \n469 2005.   \n470 [47] Peter W Shor. Polynomial-time algorithms for prime factorization and discrete logarithms   \n471 on a quantum computer. SIAM review, 41(2):303\u2013332, 1999. https://doi.org/10.1137/   \n472 S0036144598347011.   \n473 [48] Siddarth Srinivasan, Carlton Downey, and Byron Boots. Learning and inference in hilbert space   \n474 with quantum graphical models. Advances in Neural Information Processing Systems, 31, 2018.   \n475 [49] Xiaoming Sun, Guojing Tian, Shuai Yang, Pei Yuan, and Shengyu Zhang. Asymptotically   \n476 optimal circuit depth for quantum state preparation and general unitary synthesis. IEEE   \n477 Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023.   \n478 [50] Jinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing Liu, Kaining Zhang, Wei Yi, Wan  \n479 rong Huang, Chaoyue Wang, Xingyao Wu, et al. Recent advances for quantum neural networks   \n480 in generative learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n481 [51] Almudena Carrera Vazquez, Ralf Hiptmair, and Stefan Woerner. Enhancing the quantum linear   \n482 systems algorithm using richardson extrapolation. ACM Transactions on Quantum Computing,   \n483 3(1):1\u201337, 2022.   \n484 [52] Hanrui Wang, Yilian Liu, Pengyu Liu, Jiaqi Gu, Zirui Li, Zhiding Liang, Jinglei Cheng,   \n485 Yongshan Ding, Xuehai Qian, Yiyu Shi, et al. Robuststate: Boosting fidelity of quantum state   \n486 preparation via noise-aware variational training. arXiv preprint arXiv:2311.16035, 2023.   \n487 [53] David Wierichs, Josh Izaac, Cody Wang, and Cedric Yen-Yu Lin. General parameter-shift rules   \n488 for quantum gradients. Quantum, 6:677, 2022.   \n489 [54] Wenjie Wu, Ge Yan, Xudong Lu, Kaisen Pan, and Junchi Yan. Quantumdarts: differentiable   \n490 quantum architecture search for variational quantum algorithms. In International Conference   \n491 on Machine Learning, pages 37745\u201337764. PMLR, 2023.   \n492 [55] Ting Zhang, Jinzhao Sun, Xiao-Xu Fang, Xiao-Ming Zhang, Xiao Yuan, and He Lu. Ex  \n493 perimental quantum state measurement with classical shadows. Physical Review Letters,   \n494 127(20):200501, 2021.   \n495 [56] Xiao-Ming Zhang, Man-Hong Yung, and Xiao Yuan. Low-depth quantum state preparation.   \n496 Physical Review Research, 3(4):043200, 2021.   \n497 [57] Jian Zhao, Yu-Chun Wu, Guang-Can Guo, and Guo-Ping Guo. State preparation based on   \n498 quantum phase estimation. arXiv preprint arXiv:1912.05335, 2019.   \n499 [58] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A   \n500 10 million image database for scene recognition. IEEE transactions on pattern analysis and   \n501 machine intelligence, 40(6):1452\u20131464, 2017.   \n502 [59] Christa Zoufal, Aur\u00e9lien Lucchi, and Stefan Woerner. Quantum generative adversarial networks   \n503 for learning and loading random distributions. npj Quantum Information, 5(1):103, 2019.   \n504 The structure of our Appendix is as follows. Appendix A provides more details of implementing   \n505 SuperEncoder. Appendix B provides additional numerical results to illustrate the impact of state   \n506 sizes, model architectures, and training datasets. Appendix $\\mathbf{C}$ analyzes the estimated runtime of   \n507 training SuperEncoder on real devices. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "508 A Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "509 In this section, we elaborate the missing details of SuperEncoder in the main text. ", "page_idx": 13}, {"type": "text", "text": "510 The overarching workflow of SuperEncoder is illustrated in Fig. 10. The target quantum states are   \n511 input to the MLP model. Then, the MLP model generates predicted parameters based on the target   \n512 states. Afterwards, the parameters are applied to the PQC to obtain the prepared quantum states.   \n513 Finally, we calculate the loss based on the prepared states and target states and optimize the weights   \n514 of MLP through backpropagation. ", "page_idx": 13}, {"type": "image", "img_path": "r7mj17BKzw/tmp/04cddd72c00216684de3559e20f6bf407df0efc0f3b0b32808c6c5b0ed234472.jpg", "img_caption": ["Figure 10: Detailed workflow of SuperEncoder. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "515 The settings of MLP and PQC are as follows. ", "page_idx": 13}, {"type": "text", "text": "516 MLP. As listed in Table 5, we implement a two-layer MLP. Each layer consists of 512 neurons. We   \n517 employ Tanh as the activation functions since $\\pmb{\\theta}$ represents the angles of rotation gates, ranging from   \n518 $-\\pi$ to $\\pi$ .   \n519 PQC. The circuit structure is the same with the one depicted in Fig. 2, except that the number of   \n520 blocks is determined dynamically through empirical examinations. Specifically, we utilize AAE to   \n521 approximate a target state while increasing the number of blocks. The number of blocks is designated   \n522 when the resulting state fidelity no longer increases. For example, Fig. 11 demonstrates how fidelity   \n523 changes while increasing the number of blocks. As one can observe, the fidelity converges when the   \n524 number of layers is larger than 8. Hence, the number of layers is set to be 8 for 4-qubit quantum   \n525 states. We follow the same procedure to set the number of blocks for other state sizes. Each block   \n526 has the same structure, consisting of a rotation layer and an entangler layer. Given an $n$ -qubit system,   \n527 a rotation layer comprises n $R_{y}$ gates, each operating on a distinct qubit. The entangler layer is   \n528 composed of two CNOT layers. The first CNOT layer applies CNOT gates to $\\{(q_{0},q_{1}),({\\bar{q}}_{2},q_{3}),\\dots\\}$ ,   \n529 and the second CNOT layer applies CNOT gates to $\\bar{\\{(q_{1},q_{2}),(q_{3},\\bar{q}_{4}),\\dots\\}}$ . Hence, the depth of   \n530 a block is 3. Let $l$ be the number of blocks; then the dimension of the parameter vector is given   \n531 by $\\dim(\\pmb\\theta)\\,=\\,n\\times l,$ , and the depth of AAE/SuperEncoder is $3\\times l$ . We conclude the settings of   \n532 AAE/SuperEncoder used throughout this study in Table 6. ", "page_idx": 13}, {"type": "table", "img_path": "r7mj17BKzw/tmp/6e8d739ee664ef2507e8c356209ab5e7fe7b5ddf093b63e4812723557bce01d1.jpg", "table_caption": ["Table 5: MLP based SuperEncoder. $n$ refers to the number of qubits. $\\overline{{\\pmb\\theta}}$ denotes the parameter vector. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "r7mj17BKzw/tmp/72810f43cee71a49cdd4d359c9ee2201125fde1c1593835d52231800eb4bf2ba.jpg", "img_caption": ["Figure 11: Fidelity vs. # blocks for 4-qubit states using AAE. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "r7mj17BKzw/tmp/a16123a467d6a415239e84df40d7b87ffee77489a138ad7588c5285435517913.jpg", "table_caption": ["Table 6: Number of blocks and corresponding depth of AAE/SuperEncoder. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "533 The hyperparameters for training SuperEncoder and optimizing AAE are as follows. ", "page_idx": 14}, {"type": "text", "text": "534 Training Hyperparameters for SuperEncoder. Throughout our experiments, the number of epochs   \n535 are consistently set to be 10. For 4-qubit states, we set bath_size to 32, while we set it 64 for   \n536 6-qubit and 8-qubit states. We adopt Adam optimizer [22] with a learning rate of 3e-3 and a weight   \n537 decay of 1e-5.   \n538 Hyperparameters for AAE. To optimize the parameters of AAE, we also use the Adam optimizer,   \n539 with a learning rate of 1e-2 and zero weight decay. For all quantum states, we train the AAE for 100   \n540 steps. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "541 B More Numerical Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "542 B.1 Results on Larger Quantum States ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "543 In line with the main text, we train the SuperEncoder for 6-qubit and 8-qubit quantum states using   \n544 FractalDB-60 as the training dataset. Then we evaluate the performance of SuperEncoder on the   \n545 synthetic test datasets. As shown in Table 7, the average fidelity on 6-qubit and 8-qubit states are   \n546 0.8655 and 0.7624 respectively. In Appendix B.2, B.3, we discuss potential optimizations to alleviate   \n547 this performance degradation. ", "page_idx": 14}, {"type": "table", "img_path": "r7mj17BKzw/tmp/dec61165833e235562118b48b7049aeb674f9aca778312fd2c74d34f0b1a1f21.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 7: Performance evaluation on larger quantum states (6-qubit and 8-qubit). The last separate row shows the results of AAE for comparison. ", "page_idx": 14}, {"type": "text", "text": "549 As a preliminary investigation, the optimal model architecture for SuperEncoder still requires further   \n550 exploration. Currently, we have set the size of the hidden units at a constant 512 (Table 5). However,   \n551 as the number of qubits, $n$ , increases, a wider network architecture may become necessary. To   \n552 showcase the impact of model width, we adjust the size to $4\\times2^{n}$ for 6-qubit states and $16\\times2^{n}$ for   \n553 8-qubit states, and compare their performance with the original settings, as shown in Table 8. As   \n554 evident from the results, this simple adjustment significantly enhances the fidelity of SuperEncoder,   \n555 suggesting that there is substantial potential to boost SuperEncoder\u2019s performance by developing a   \n556 more tailored network architecture. ", "page_idx": 15}, {"type": "table", "img_path": "r7mj17BKzw/tmp/4a781b673d2d3d543b8d39a7806ab9ecb6fa97fdb380d7497ac0ed404510d38b.jpg", "table_caption": [], "table_footnote": ["Table 8: Impact of increasing network width. Here $\\overline{h}$ refers to the size of hidden units. "], "page_idx": 15}, {"type": "text", "text": "557 B.3 Impact of Training Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "558 In addition to refining the model architecture, the development of a specially designed dataset for   \n559 pre-training SuperEncoder is essential. Currently, the dataset utilized is FractalDB [21], which is   \n560 originally designed for computer vision tasks. However, given the wide range of applications of QSP,   \n561 there is a need to accommodate diverse types of classical data from various domains. Therefore, how   \n562 to create a comprehensive dataset that could fully unleash the potential of SuperEncoder remains an   \n563 open question. While developing a pre-trained model that performs well in all kinds of applications   \n564 may be challenging, we advocate for a strategy that combines pre-training with fine-tuning for the   \n565 practical deployment of SuperEncoder, similar to the approach used with foundation models in   \n566 classical machine learning. To substantiate this approach, we have compiled a separate dataset that   \n567 encompasses a variety of statistical distributions not limited to those utilized for evaluation (but with   \n568 different settings). As demonstrated in Table 9, after fine-tuning, the performance of SuperEncoder   \n569 improves by approximately 0.03. ", "page_idx": 15}, {"type": "table", "img_path": "r7mj17BKzw/tmp/f11ef89fe84eb13aa500e41d55175aaea4ce34170fc9b539b9604bfffdd8d589.jpg", "table_caption": ["Table 9: Fidelity improvements after fine-tuning SuperEncoder using a dataset consisting of different distributions. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "570 C Runtime Estimation for Training on Real Devices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "571 Although we have theoretically analyzed the feasibility of training SuperEncoder using states from   \n572 real devices (Section 3.2), its practical implementation poses significant challenges. Specifically,   \n573 state-of-the-art quantum tomography techniques, such as classical shadow [55, 16], require numerous   \n574 snapshots, each measuring a distinct observable.   \n575 To train SuperEncoder, each sample in the training dataset necessitates one classical shadow to obtain   \n576 the prepared state. For instance, with the FractalDB-60 dataset, one training epoch requires 60,000   \n577 classical shadows. Our experiments on the IBM cloud platform reveal an average runtime of 3.02   \n578 seconds per circuit job excluding queuing time. Suppose the number of snapshots is 1000, then the   \n579 total runtime to train SuperEncoder for 10 epochs is about 1,812,000,000 seconds3, roughly 57 years,   \n580 making the process prohibitively expensive and time-consuming.   \n581 However, quantum tomography is under active investigation, and we expect more efficient techniques   \n582 to emerge for acquiring noisy quantum states from real devices. Additionally, with the advancement   \n583 of quantum computing system, future systems may have tightly integrated quantum-classical hetero  \n584 geneous architectures (shorter runtime per job) while being capable of executing numerous quantum   \n585 circuits in parallel (jobs within a classical shadow can execute in parallel). Hence, we anticipate the   \n586 training of SuperEncoder to be feasible in the future. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "587 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "94 Guidelines:   \n95 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n96 made in the paper.   \n97 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n98 contributions made in the paper and important assumptions and limitations. A No or   \n99 NA answer to this question will not be perceived well by the reviewers.   \n00 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n01 much the results can be expected to generalize to other settings.   \n02 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n03 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "04 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: SuperEncoder sacrifices fidelity, as discussed in Section 4.4. ", "page_idx": 17}, {"type": "text", "text": "8 Guidelines:   \n9 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n0 the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n2 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n3 violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors   \n5 should reflect on how these assumptions might be violated in practice and what the   \n6 implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n8 only tested on a few datasets or with a few runs. In general, empirical results often   \n9 depend on implicit assumptions, which should be articulated.   \n0 \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution   \n2 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n3 used reliably to provide closed captions for online lectures because it fails to handle   \n4 technical jargon.   \n5 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n6 and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n8 address problems of privacy and fairness.   \n9 \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n2 judgment and recognize that individual actions in favor of transparency play an impor  \n3 tant role in developing norms that preserve the integrity of the community. Reviewers   \n4 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "635 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "636 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n637 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All these necessary contents for theoretical results are included in Section 3.2. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our code is open-source with instructions to reproduce our results, as described in Section 4.1. We also describe the details of experiment settings in Appendix A. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "697 \u2022 The answer NA means that paper does not include experiments requiring code.   \n698 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n699 public/guides/CodeSubmissionPolicy) for more details.   \n700 \u2022 While we encourage the release of code and data, we understand that this might not be   \n701 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n702 including code, unless this is central to the contribution (e.g., for a new open-source   \n703 benchmark).   \n704 \u2022 The instructions should contain the exact command and environment needed to run to   \n705 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n706 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n707 \u2022 The authors should provide instructions on data access and preparation, including how   \n708 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n709 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n710 proposed method and baselines. If only a subset of experiments are reproducible, they   \n711 should state which ones are omitted from the script and why.   \n712 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n713 versions (if applicable).   \n714 \u2022 Providing as much information as possible in supplemental material (appended to the   \n715 paper) is recommended, but including URLs to data and code is permitted.   \n716 6. Experimental Setting/Details   \n717 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n718 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n719 results?   \n720 Answer: [Yes]   \n721 Justification: We illustrate the experimental settings in Section 4.1, and provides additional   \n722 details in Appendix A.   \n723 Guidelines:   \n724 \u2022 The answer NA means that the paper does not include experiments.   \n725 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n726 that is necessary to appreciate the results and make sense of them.   \n727 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n728 material.   \n729 7. Experiment Statistical Significance   \n730 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n731 information about the statistical significance of the experiments?   \n732 Answer: [No]   \n733 Justification: Throughout our experiments, we set the random seed to be fixed for all libraries   \n734 we used.   \n735 Guidelines:   \n736 \u2022 The answer NA means that the paper does not include experiments.   \n737 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n738 dence intervals, or statistical significance tests, at least for the experiments that support   \n739 the main claims of the paper.   \n740 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n741 example, train/test split, initialization, random drawing of some parameter, or overall   \n742 run with given experimental conditions).   \n743 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n744 call to a library function, bootstrap, etc.)   \n745 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n746 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n747 of the mean.   \n748 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n749 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n750 of Normality of errors is not verified.   \n751 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n752 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n753 error rates).   \n754 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n755 they were calculated and reference the corresponding figures or tables in the text.   \n756 8. Experiments Compute Resources   \n757 Question: For each experiment, does the paper provide sufficient information on the com  \n758 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n759 the experiments?   \n760 Answer: [Yes]   \n761 Justification: We describe the computer resources used in this paper in Section 4.1.   \n762 Guidelines:   \n763 \u2022 The answer NA means that the paper does not include experiments.   \n764 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n765 or cloud provider, including relevant memory and storage.   \n766 \u2022 The paper should provide the amount of compute required for each of the individual   \n767 experimental runs as well as estimate the total compute.   \n768 \u2022 The paper should disclose whether the full research project required more compute   \n769 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n770 didn\u2019t make it into the paper).   \n771 9. Code Of Ethics   \n772 Question: Does the research conducted in the paper conform, in every respect, with the   \n773 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n774 Answer: [Yes]   \n775 Justification: We have read the code of ethics and followed its requirements.   \n776 Guidelines:   \n777 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n778 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n779 deviation from the Code of Ethics.   \n780 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n781 eration due to laws or regulations in their jurisdiction).   \n782 10. Broader Impacts   \n783 Question: Does the paper discuss both potential positive societal impacts and negative   \n784 societal impacts of the work performed?   \n785 Answer: [NA]   \n786 Justification: This work has no societal impact.   \n787 Guidelines:   \n788 \u2022 The answer NA means that there is no societal impact of the work performed.   \n789 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n790 impact or why the paper does not address societal impact.   \n791 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n792 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n793 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n794 groups), privacy considerations, and security considerations.   \n795 \u2022 The conference expects that many papers will be foundational research and not tied   \n796 to particular applications, let alone deployments. However, if there is a direct path to   \n797 any negative applications, the authors should point it out. For example, it is legitimate   \n798 to point out that an improvement in the quality of generative models could be used to   \n799 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n800 that a generic algorithm for optimizing neural networks could enable people to train   \n801 models that generate Deepfakes faster.   \n802 \u2022 The authors should consider possible harms that could arise when the technology is   \n803 being used as intended and functioning correctly, harms that could arise when the   \n804 technology is being used as intended but gives incorrect results, and harms following   \n805 from (intentional or unintentional) misuse of the technology.   \n806 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n807 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n808 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n809 feedback over time, improving the efficiency and accessibility of ML).   \n810 11. Safeguards   \n811 Question: Does the paper describe safeguards that have been put in place for responsible   \n812 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n813 image generators, or scraped datasets)?   \n814 Answer: [NA]   \n815 Justification: This paper poses no such risks as our released model and datasets are only   \n816 able to be used for quantum state preparation.   \n817 Guidelines:   \n818 \u2022 The answer NA means that the paper poses no such risks.   \n819 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n820 necessary safeguards to allow for controlled use of the model, for example by requiring   \n821 that users adhere to usage guidelines or restrictions to access the model or implementing   \n822 safety filters.   \n823 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n824 should describe how they avoided releasing unsafe images.   \n825 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n826 not require this, but we encourage authors to take this into account and make a best   \n827 faith effort.   \n828 12. Licenses for existing assets   \n829 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n830 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n831 properly respected?   \n832 Answer: [Yes]   \n833 Justification: We use an open-source dataset FractalDB, we cite the original paper and   \n834 indicates the version we use in Section 4.1.   \n835 Guidelines:   \n836 \u2022 The answer NA means that the paper does not use existing assets.   \n837 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n838 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n839 URL.   \n840 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n841 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n842 service of that source should be provided.   \n843 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n844 package should be provided. For popular datasets, paperswithcode.com/datasets   \n845 has curated licenses for some datasets. Their licensing guide can help determine the   \n846 license of a dataset. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We submit our assets in zip file and also put them on the anonymous github repository, we have included a README file with detailed descriptions.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]