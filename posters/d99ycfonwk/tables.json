[{"figure_path": "d99yCfOnwK/tables/tables_4_1.jpg", "caption": "Table 1: The importance of using both losses in CDM. We demonstrate the importance of using both the CE and MSE losses at training. We report the results for CIFAR-10 test-set. FID is reported on 50k samples which were generated using DDIM scheduler with 50 steps. As shown by Rhodes et al. [35], to avoid the density-chasm problem, the classification problem should be sufficiently hard to avoid trivial classifier solutions. This leads to low classification accuracy results.", "description": "This table compares the performance of a classification diffusion model (CDM) trained using different loss functions: cross-entropy (CE) only, mean squared error (MSE) only, and both CE and MSE.  It demonstrates that using both CE and MSE is crucial for achieving optimal classifier performance, as measured by classification accuracy, cross-entropy loss, MSE loss, Fr\u00e9chet Inception Distance (FID), and negative log-likelihood (NLL). The results highlight the importance of balancing the classification task with the denoising objective to avoid the density-chasm problem, which is a known issue in density ratio estimation methods.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}, {"figure_path": "d99yCfOnwK/tables/tables_6_1.jpg", "caption": "Table 1: The importance of using both losses in CDM. We demonstrate the importance of using both the CE and MSE losses at training. We report the results for CIFAR-10 test-set. FID is reported on 50k samples which were generated using DDIM scheduler with 50 steps. As shown by Rhodes et al. [35], to avoid the density-chasm problem, the classification problem should be sufficiently hard to avoid trivial classifier solutions. This leads to low classification accuracy results.", "description": "This table compares the performance of a classification diffusion model (CDM) trained with different loss functions: cross-entropy (CE) only, mean squared error (MSE) only, and both CE and MSE.  The results show that using both CE and MSE leads to significantly better classification accuracy, lower MSE, lower FID, and most importantly, much lower NLL (negative log-likelihood). The table highlights the importance of including the MSE loss for achieving an optimal classifier, which is crucial for accurate likelihood estimation, a key advantage of the proposed CDM approach.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}, {"figure_path": "d99yCfOnwK/tables/tables_8_1.jpg", "caption": "Table 2: Image generation quality. We compare the FID (lower is better) achieved by a DDM and a CDM using three sampling schemes for CelebA and CIFAR-10. For conditional CIFAR-10 we train a DDM ourselves, as no model in the original implementation [19] supports CFG.", "description": "This table compares the FID scores of CDMs and DDM models on CelebA and CIFAR-10 datasets using three different sampling methods: DDIM sampler (50 steps), DDPM sampler (1000 steps), and 2nd order DPMS (25 steps).  Lower FID scores indicate better image generation quality.  A separate comparison is shown for unconditional and conditional CIFAR-10 generation, highlighting the performance in both scenarios.", "section": "4.2 Denoising Results, Image Quality and Negative Log Likelihood"}, {"figure_path": "d99yCfOnwK/tables/tables_8_2.jpg", "caption": "Table 3: NLL (bits/dim) calculated on the CIFAR-10 test-set. For each model we specify the number of NFEs required for calculating the NLL. CDM achieves state-of-the-art NLL among methods that use a single NFE.", "description": "This table presents a comparison of the negative log-likelihood (NLL) achieved by various generative models on the CIFAR-10 dataset.  The NLL measures how well a model estimates the probability density of the data.  Lower NLL indicates better performance. The table also specifies the number of neural function evaluations (NFEs) needed to compute the NLL for each method.  A key finding is that the proposed Classification Diffusion Model (CDM), along with its variants CDM(UNIF.) and CDM(OT), achieves state-of-the-art NLL among methods requiring only a single NFE. This demonstrates the efficiency and effectiveness of the CDM in likelihood estimation.", "section": "4.3 Different Noise Scheduling for Better Likelihood Estimation"}, {"figure_path": "d99yCfOnwK/tables/tables_13_1.jpg", "caption": "Table 1: The importance of using both losses in CDM. We demonstrate the importance of using both the CE and MSE losses at training. We report the results for CIFAR-10 test-set. FID is reported on 50k samples which were generated using DDIM scheduler with 50 steps. As shown by Rhodes et al. [35], to avoid the density-chasm problem, the classification problem should be sufficiently hard to avoid trivial classifier solutions. This leads to low classification accuracy results.", "description": "This table compares the performance of a classifier trained using only cross-entropy loss (CE), only mean squared error loss (MSE), and both CE and MSE.  It highlights that using both losses is crucial for achieving high classification accuracy, low CE and MSE, and good FID (Fr\u00e9chet Inception Distance) and NLL (negative log-likelihood) scores. The results demonstrate that only using CE or MSE leads to suboptimal performance.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}]