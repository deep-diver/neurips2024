[{"figure_path": "d99yCfOnwK/figures/figures_1_1.jpg", "caption": "Figure 1: Samples from CDMs (left) trained on CelebA 64 \u00d7 64 and on CIFAR-10, compared to samples from TRE models [35] (right) trained on MNIST and CIFAR-10. To date, DRE methods have failed to capture the distributions of complex, high-dimensional data, and have been demonstrated only on toy examples or on the simple MNIST dataset. The right pane shows results from TRE, the state-of-the-art DRE method, which fails to capture the distribution of CIFAR-10. CDM is the first DRE-based method that can successfully learn the distribution of images.", "description": "This figure compares the image generation results of Classification Diffusion Models (CDMs) and Telescoping Density Ratio Estimation (TRE) models.  The left side shows samples generated by CDMs trained on CelebA (faces) and CIFAR-10 (objects). The right side shows samples from TRE, a state-of-the-art density ratio estimation method, trained on MNIST and CIFAR-10.  The figure highlights the superior performance of CDMs in generating high-quality images, especially for complex datasets like CIFAR-10, where TRE fails.  This demonstrates the effectiveness of the proposed CDM method for learning high-dimensional image distributions.", "section": "1 Introduction"}, {"figure_path": "d99yCfOnwK/figures/figures_3_1.jpg", "caption": "Figure 2: A diagram of CDM (right) compared with DDM (left). A DDM functions as an MMSE denoiser conditioned on the noise level, whereas a CDM operates as a classifier. Given a noisy image, a CDM outputs a probability vector predicting the noise level, such that the t-th element in this vector is the probability that the noise level of the input image corresponds to timestep t in the diffusion process. A CDM can be used to output the MMSE denoised image by computing the gradient of its output probability vector w.r.t the input image, as we show in Theorem 3.1.", "description": "This figure illustrates the core difference between a Classification Diffusion Model (CDM) and a Denoising Diffusion Model (DDM).  In a DDM, a denoiser network takes a noisy image and a timestep as input, producing a denoised image as output. In a CDM, a classifier network receives only the noisy image and outputs a probability distribution over possible timesteps indicating the amount of noise.  Crucially, the CDM leverages the gradient of this probability distribution to effectively act as a denoiser, enabling it to generate images and calculate likelihoods with improved efficiency and accuracy.", "section": "Method"}, {"figure_path": "d99yCfOnwK/figures/figures_4_1.jpg", "caption": "Figure 1: Samples from CDMs (left) trained on CelebA 64 \u00d7 64 and on CIFAR-10, compared to samples from TRE models [35] (right) trained on MNIST and CIFAR-10. To date, DRE methods have failed to capture the distributions of complex, high-dimensional data, and have been demonstrated only on toy examples or on the simple MNIST dataset. The right pane shows results from TRE, the state-of-the-art DRE method, which fails to capture the distribution of CIFAR-10. CDM is the first DRE-based method that can successfully learn the distribution of images.", "description": "This figure compares image samples generated by Classification Diffusion Models (CDMs) and the state-of-the-art density ratio estimation (DRE) method, TRE.  It demonstrates that unlike previous DRE methods, CDMs are capable of generating high-quality images from complex datasets such as CelebA and CIFAR-10, showcasing their success in learning the distribution of high-dimensional data.", "section": "1 Introduction"}, {"figure_path": "d99yCfOnwK/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison between the log probability of a noise-level classifier trained using the CE loss alone and a model trained using CE and MSE. Since the SoftMax operator is invariant to an additive factor, we subtract the maximal value from the vector (i.e., f\u03b8(xt) \u2190 f\u03b8(xt) \u2212 max(f\u03b8(xt))) for visualization. We utilize the connection we developed between the optimal classifier and the MMSE denoiser to incorporate the MSE loss in DRE training, as depicted in Algorithm 1. As evident, without considering MSE, the prediction accuracy of the classifier is limited to the vicinity of the correct label, unlike the model trained using both CE and MSE, which yields accurate predictions globally. The essence of an optimal classifier lies in its capability to predict the correct probability vector for all entries, rather than solely for the correct label. As can be seen, this necessitates the incorporation of the MSE loss.", "description": "This figure compares the log probability of a noise-level classifier trained using only cross-entropy (CE) loss against one trained with both CE and mean squared error (MSE) loss.  It demonstrates that using only CE loss limits the classifier's accuracy to the correct label's vicinity. However, incorporating MSE loss enables accurate predictions across all entries, highlighting the importance of MSE for optimal classifier performance.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}, {"figure_path": "d99yCfOnwK/figures/figures_7_1.jpg", "caption": "Figure 4: Denoising performance. The plots show the MSEs (top) and the ratio between the MSEs (bottom) achieved by a pre-trained DDM and by a CDM with the same architecture, as a function of the noise level (timestep t). The CDM significantly outperforms the pre-trained DDM at high noise levels, while demonstrating comparable performance at lower noise levels.", "description": "This figure compares the Mean Squared Error (MSE) of denoising between a pre-trained Denoising Diffusion Model (DDM) and a Classification Diffusion Model (CDM) across different noise levels represented by the timestep t.  The top plots show the MSE for both models. The bottom plots display the ratio of the MSEs of the DDM to the CDM.  The results demonstrate that the CDM achieves significantly lower MSE than the DDM at high noise levels, while showing comparable performance at lower noise levels. This indicates that the CDM is a more effective denoiser, particularly in scenarios with significant noise.", "section": "4.2 Denoising Results, Image Quality and Negative Log Likelihood"}, {"figure_path": "d99yCfOnwK/figures/figures_7_2.jpg", "caption": "Figure 5: Denoising results. The figure depicts a comparison between denoising results on the CelebA dataset for several different noise levels, obtained with a CDM and with a pre-trained DDM with the same architecture. The right column shows the models\u2019 predictions for pure Gaussian noise, which should theoretically be the expectation of the prior distribution. As observed, DDM outputs a highly noisy image, whereas CDM generates an image much closer to the mean of the dataset.", "description": "This figure compares the denoising performance of a Classification Diffusion Model (CDM) and a Denoising Diffusion Model (DDM) on the CelebA dataset.  For several noise levels (represented by timesteps t=600, 700, 800, 900, 1000), the figure shows the original image, the noisy image, the DDM's denoised image, and the CDM's denoised image. The rightmost column shows the models' attempts to denoise pure Gaussian noise; ideally, they should produce an image resembling the average of the dataset. The CDM outperforms the DDM, producing denoised images that are significantly closer to the original images, especially at higher noise levels, and its reconstruction from pure noise is also much closer to the mean image.", "section": "4.2 Denoising Results, Image Quality and Negative Log Likelihood"}, {"figure_path": "d99yCfOnwK/figures/figures_16_1.jpg", "caption": "Figure 1: Samples from CDMs (left) trained on CelebA 64 \u00d7 64 and on CIFAR-10, compared to samples from TRE models [35] (right) trained on MNIST and CIFAR-10. To date, DRE methods have failed to capture the distributions of complex, high-dimensional data, and have been demonstrated only on toy examples or on the simple MNIST dataset. The right pane shows results from TRE, the state-of-the-art DRE method, which fails to capture the distribution of CIFAR-10. CDM is the first DRE-based method that can successfully learn the distribution of images.", "description": "This figure compares image samples generated by Classification Diffusion Models (CDMs) and the state-of-the-art method for density ratio estimation (TRE).  The left side shows high-quality image samples from CDMs trained on CelebA and CIFAR-10 datasets, demonstrating the ability of the new model to generate realistic images of complex objects. In contrast, the right side displays samples from TRE, showing its failure in capturing complex distributions, which highlights the significance of the CDM approach.", "section": "1 Introduction"}, {"figure_path": "d99yCfOnwK/figures/figures_17_1.jpg", "caption": "Figure 1: Samples from CDMs (left) trained on CelebA 64 \u00d7 64 and on CIFAR-10, compared to samples from TRE models [35] (right) trained on MNIST and CIFAR-10. To date, DRE methods have failed to capture the distributions of complex, high-dimensional data, and have been demonstrated only on toy examples or on the simple MNIST dataset. The right pane shows results from TRE, the state-of-the-art DRE method, which fails to capture the distribution of CIFAR-10. CDM is the first DRE-based method that can successfully learn the distribution of images.", "description": "This figure compares image samples generated by Classification Diffusion Models (CDMs) and Telescoping Density Ratio Estimation (TRE).  The left side shows high-quality images generated by CDMs trained on CelebA and CIFAR-10 datasets.  The right side shows samples from TRE, a state-of-the-art DRE method, which struggles to generate comparable quality images, particularly on the more complex CIFAR-10 dataset.  The figure highlights the success of CDMs in learning high-dimensional image distributions, a challenge that previous DRE methods have failed to overcome.", "section": "1 Introduction"}, {"figure_path": "d99yCfOnwK/figures/figures_18_1.jpg", "caption": "Figure 8: Comparison of denoising between a model trained using only CE and one trained using both CE and MSE. We note that the denoising results for the model trained using CE alone are poor, but are better for high noise levels than for lower ones. This resonates with our conclusion that models trained only with CE are only accurate near the real noise level: As denoising with CDM relies on Theorem 3.1, we would expect deteriorating denoising quality the further the real noise level is from T + 1, as fo(x)[T + 1] is always used for denoising.", "description": "This figure compares the denoising capabilities of two models: one trained with cross-entropy (CE) loss only, and another trained with both CE and mean squared error (MSE) loss.  The results show significantly better denoising performance for the model trained with both losses, especially at lower noise levels. This highlights the importance of incorporating MSE loss in the model training for accurate denoising.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}, {"figure_path": "d99yCfOnwK/figures/figures_19_1.jpg", "caption": "Figure 3: Comparison between the log probability of a noise-level classifier trained using the CE loss alone and a model trained using CE and MSE. Since the SoftMax operator is invariant to an additive factor, we subtract the maximal value from the vector (i.e., f\u03b8(xt) \u2190 f\u03b8(xt) \u2212 max(f\u03b8(xt))) for visualization. We utilize the connection we developed between the optimal classifier and the MMSE denoiser to incorporate the MSE loss in DRE training, as depicted in Algorithm 1. As evident, without considering MSE, the prediction accuracy of the classifier is limited to the vicinity of the correct label, unlike the model trained using both CE and MSE, which yields accurate predictions globally. The essence of an optimal classifier lies in its capability to predict the correct probability vector for all entries, rather than solely for the correct label. As can be seen, this necessitates the incorporation of the MSE loss.", "description": "This figure compares the log probability of noise level predicted by a classifier trained only with cross-entropy loss and a classifier trained with both cross-entropy and mean squared error loss. It shows that using only cross-entropy loss limits the accuracy of the classifier to the vicinity of the true label, while adding MSE loss allows the classifier to achieve accurate predictions globally, which is essential for obtaining accurate log likelihood estimation.  The SoftMax function is used for probability normalization in the figure.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}, {"figure_path": "d99yCfOnwK/figures/figures_19_2.jpg", "caption": "Figure 3: Comparison between the log probability of a noise-level classifier trained using the CE loss alone and a model trained using CE and MSE. Since the SoftMax operator is invariant to an additive factor, we subtract the maximal value from the vector (i.e., f\u03b8(xt) \u2190 f\u03b8(xt) \u2212 max(f\u03b8(xt))) for visualization. We utilize the connection we developed between the optimal classifier and the MMSE denoiser to incorporate the MSE loss in DRE training, as depicted in Algorithm 1. As evident, without considering MSE, the prediction accuracy of the classifier is limited to the vicinity of the correct label, unlike the model trained using both CE and MSE, which yields accurate predictions globally. The essence of an optimal classifier lies in its capability to predict the correct probability vector for all entries, rather than solely for the correct label. As can be seen, this necessitates the incorporation of the MSE loss.", "description": "The figure compares the log probability of a noise-level classifier trained using only cross-entropy (CE) loss against one trained with both CE and mean squared error (MSE) loss.  It shows that using only CE loss restricts accurate predictions to the vicinity of the correct label.  Incorporating MSE loss allows for globally accurate predictions, demonstrating the importance of MSE for optimal classifier training in the context of density ratio estimation.", "section": "4.1 The Importance of Using Both Losses for Achieving an Optimal Classifier"}]