[{"type": "text", "text": "Differentially Private Graph Diffusion with Applications in Personalized PageRanks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rongzhe Wei Georgia Institute of Technology rongzhe.wei@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Eli Chien Georgia Institute of Technology ichien6@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Pan Li Georgia Institute of Technology panli@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. Protecting the privacy of graph data is challenging due to its interconnected nature. This work proposes a novel graph diffusion framework with edge-level differential privacy guarantees by using noisy diffusion iterates. The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications. We also introduce a novel $\\infty$ -Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI practically applicable. We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph diffusion, characterized by propagating signals across networks, is used in a variety of realworld applications. Variants of graph diffusion such as PageRank [1] and heat kernel diffusion [2] has revolutionized the domains such as web searching [3], community detection [4\u20137], network analysis [8,9] and advancements in graph neural networks [10\u201313]. Despite their widespread applications, directly releasing diffusion vectors can inadvertently leak sensitive graph information and raise privacy concerns. Hoskins et al. [14] demonstrate that the access to a small subset of random walk-based similarities (e.g., commute times, personalized PageRank scores) could disclose significant portions of network\u2019s edges, a phenomenon known as link disclosure [15]. Such attacks, for instance, may enable advertisers to deploy invasive advertising tactics [16] or reveal sensitive transaction information within financial networks [17]. Consequently, it becomes critically important to design graph diffusion algorithms with privacy safeguards. ", "page_idx": 0}, {"type": "text", "text": "Differential privacy (DP) is recognized as a gold standard used for characterizing the privacy risk of data processing algorithms [18]. However, the inherently interconnected nature of graph-structured data renders the adaptation of DP to graphs non-trivial [19]. Previous studies often conduct the analysis of output sensitivity and adopt output perturbation to keep graph data private, which include the study on differentially private personalized PageRanks (PPRs) [20], and other relevant graph algorithms such as max flow-min cut [21], graph sparsification [22], spectral analysis [23,24]. However, output perturbation-based approaches often provide a less-than-ideal utility-privacy tradeoff. Numerous studies suggest that incorporating noise during the process, rather than at the output, can potentially enhance utility-privacy tradeoffs [25]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a graph diffusion framework that ensures edge-level DP guarantees based on noisy diffusion iterates. Our framework is the first to incorporate privacy amplification by iterations (PABI) technique [26] into graph diffusions. As graph diffusion can be viewed as iterating contraction maps in $\\ell_{1}$ space, we adopt per-iterate Laplace noise due to its better performance than the Gaussian mechanism commonly adopted in previous PABI frameworks and provide new analysis dedicated to Laplace noise. We also propose a novel $\\infty$ -Wasserstein distance tracking analysis that can tighten the state-of-the-art PABI bound [27] that relies on the space diameter, which makes the bound valid for practical usage. Noticing diffusion from low-degree nodes may introduce high sensitivity, our framework also also proposes a theory-informed degree-based thresholding function at each step diffusion to improve the utility-privacy tradeoff. Lastly, we specialize our framework in the computation of PPR for node ranking tasks. Extensive experiments reveal the advantages of our framework over baselines, especially under stringent privacy requirements. ", "page_idx": 1}, {"type": "text", "text": "1.1 More Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Extensive research has been dedicated to privacy protection within graphs, specifically in the release of graph statistics and structures under DP guarantees [28, 29]. The primary techniques to safeguard graph structures involve the Laplace and exponential mechanisms [30]. Early contributions by Nissim et al. [31] calibrated noise based on the smooth sensitivity of graph queries, expanding beyond output perturbation. Karwa et al. [32] improved the efficiency of privacy-preserving subgraph-counting queries by calibrating Laplace noise according to smooth sensitivity. Different from these methods, Zhang et al. [33] employed the exponential mechanism [34] to enhance privacy protections. Concurrently, Hay et al. [35] developed a constraint-based inference algorithm as a post-processing step to improve the quality of degree sequences derived from output perturbation mechanisms. Further, Kasiviswanathan et al. [36] used a top-down degree projection technique to limit the maximum degree in graphs, thus controlling the sensitivity of degree sequence queries. Additional efforts in privately releasing graph statistics include outlinks [37], cluster coefficients [38], graph eigenvectors [39], and edge weights [40]. ", "page_idx": 1}, {"type": "text", "text": "In the realm of graph diffusion, the most related work to ours is by Epasto et al. [20], which focuses on releasing the PPR vector using forward push [41] and Laplace output perturbation. Some studies have shown that injecting noise during the process may offer better privacy-utility trade-offs compared to output perturbation methods [25] and our study compared with [20] provides another use case. Traditionally, the composition theorem [42] is used to track privacy guarantees for iterative algorithms, but it results in a bound that may diverge with the number of iterations. Recently, the technique of PABI [26,27] was introduced to strengthen the privacy analysis of adding noise during the process, which demonstrates a non-divergent privacy bound for releasing final results if the iterations adopt contraction maps [27]. This substantially tightens the divergent bound given by the naive application of the DP composition theorem [43,44]. Our framework also benefits from this advantage and we further adapt Altschuler et al.\u2019s analysis [27] to incorporate the Laplace mechanism and provide a tightened space diameter tracking, which makes the bound practically applicable in the graph diffusion applications. Besides, works that share a similar spirit in leveraging PABI have been conducted for other scenarios including machine unlearning [45] and improving hidden state DP [46]. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ represent an undirected graph, where $\\mathcal{V}$ is the set of nodes and $\\mathcal{E}$ is the set of edges, equipped with an adjacency matrix $\\mathbf{A}\\in\\overline{{\\{0,1\\}}}^{n\\times n}$ , where $n$ denotes the total number of nodes, i.e., $n=|\\nu|$ . By establishing an order for the nodes within the graph, we denote $\\mathbf{d}=[d_{1},d_{2},...,d_{n}]^{T}$ as the degree vector. Additionally, let $\\mathbf{D}=\\mathrm{diag}(\\mathbf{d})$ and $\\mathbf{e}_{i}$ signifies the $i$ -th standard basis. We denote $\\mathcal{L}(\\mathbf{0},\\bar{\\sigma})$ and $\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ as the zero mean Laplace and Gaussian distributions, respectively. We define the set $[n]=\\{1,2,\\dots,n\\}$ , and $X_{i:j},i,j\\in\\mathbb{Z}_{+},i\\leq j$ as joint couple of $(X_{i},\\overbar{X_{i+1}},\\ldots;X_{j})$ . ", "page_idx": 1}, {"type": "text", "text": "Graph Diffusion. First, we introduce the concept of Graph Diffusion $\\mathcal{D}$ , which is commonly characterized by a series of diffusion map $\\phi_{k}$ defined by the random walk matrix ${\\bf P}={\\bf A}{\\bf D}^{-1}$ [6,12, ", "page_idx": 1}, {"type": "text", "text": "47]. Formally, We define the graph diffusion $\\mathcal{D}(\\mathbf{s})$ with the initial seed s as ", "page_idx": 2}, {"type": "text", "text": "$\\mathcal{D}(\\mathbf{s})=\\operatorname*{lim}_{K\\rightarrow\\infty}\\mathbf{s}_{K}=\\operatorname*{lim}_{K\\rightarrow\\infty}\\phi_{K}\\circ\\dots\\circ\\phi_{1}(\\mathbf{s})$ where $\\phi_{k}(\\mathbf{x})=\\left(\\gamma_{1,k}\\mathbf{P}+\\gamma_{2,k}\\right)\\mathbf{x}+\\gamma_{3,k}\\mathbf{s}.$ (1) where $\\textbf{s}\\in\\mathbb{R}^{|\\nu|}$ is a stochastic vector on the graph, and $\\gamma_{1,k}\\,+\\,\\gamma_{2,k}\\,+\\,\\gamma_{3,k}\\,=\\,1$ . Let $\\gamma_{\\mathrm{max}}~=$ $\\operatorname*{max}_{k}|\\gamma_{1,k}|+|\\gamma_{2,k}|$ denote the Lipschitz constant of the graph diffusion mapping, and $\\gamma_{\\mathrm{max}}^{(1)}\\;=\\;$ $\\operatorname*{max}_{k}|\\gamma_{1,k}|$ denote the maximum diffusion coefficient. $\\mathbf{s}_{K}$ is the diffusion vector at time $K$ . The essence of a diffusion process is to model how an initial vector s propagates through the graph over time. Coefficient $\\gamma_{i,k}$ \u2019s control how different resources contribute to the $k$ th step diffusion. When taking $\\gamma_{1,k}=1-\\gamma_{3,k}=\\beta$ and $\\gamma_{2,k}=0$ , Eq. (1) is recognized as the PageRank Diffusion [48] with teleport probability $1-\\beta$ . The Exponential kernel diffusion, which includes the specific case of the Heat Kernel diffusion [2], can also be characterized with the composition of diffusion mappings via the infinitely divisible property [49]. ", "page_idx": 2}, {"type": "text", "text": "Personalization. Personalized graph diffusions, tailored to individual nodes or localized neighborhoods, play a crucial role in many real-world applications. These include recommendation systems [50], where personalized diffusions improve suggestion relevance, community detection for identifying subgroups within larger networks [7], targeted marketing strategies for enhancing campaign effectiveness [51]. These diffusions are defined by setting the graph diffusion vector s as ${\\bf{e}}_{i}$ for an individual node or $\\begin{array}{r}{\\mathbf{s}=\\sum_{i\\in S}\\mathbf{e}_{i}/|S|}\\end{array}$ for a neighborhood set $S$ . In this paper, we primarily discuss the single-node case w hile our analysis can be generalized to a set of seed nodes. ", "page_idx": 2}, {"type": "text", "text": "Privacy Definition. Differential Privacy (DP) [18, 42] is widely recognized as the standard framework for providing formal privacy guarantees for algorithms that process sensitive data. This framework has further been extended under R\u00e9nyi divergence [44]. Its principles have been applied to safeguard sensitive structures within graph algorithms, an metric noted as Edge-level R\u00e9nyi Differential Privacy (RDP). Details on the conversion from RDP to DP are elaborated in App. E. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Edge-level RDP [52,53]). A randomized graph algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\alpha,\\epsilon)$ -edge-level RDP if for any adjacent graphs $\\mathcal{G},\\mathcal{G}^{\\prime}$ that differs in a single edge, we have $\\begin{array}{r}{\\mathcal{D}_{\\alpha}(\\mathcal{A}(\\mathcal{G})\\|\\mathcal{A}(\\mathcal{G}^{\\prime}))\\leq\\epsilon}\\end{array}$ , where the R\u00e9nyi Divergence $\\begin{array}{r}{\\mathcal{D}_{\\alpha}(X\\|Y)=\\frac{1}{\\alpha-1}\\log\\mathbb{E}_{x\\sim\\nu}\\left(\\frac{\\mu(x)}{\\nu(x)}\\right)^{c}}\\end{array}$ with $X\\sim\\mu,Y\\sim\\nu$ . ", "page_idx": 2}, {"type": "text", "text": "More practical cases find that the seed node (user) of personalized graph diffusion algorithms is already aware of their direct connections within the network, such as one\u2019s friend list in social networks, and one\u2019s transaction record in financial networks, and therefore protecting the edges directly attached to the seed node becomes unnecessary. Instead, the focus of privacy protection shifts towards obscuring the connections between the remaining nodes. To address this specific need, we follow the previous study [20] and introduce Personalized Edge-level RDP. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Personalized Edge-level RDP [20,54]). Consider a graph $\\mathcal{G}$ and a personalized graph algorithm $\\boldsymbol{\\mathcal{A}}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies personalized $(\\alpha,\\epsilon)$ -edge-level RDP if for any node v as the seed node in $\\mathcal{G}$ , and for any graph $\\mathcal{G}^{\\prime}$ adjacent to $\\mathcal{G}$ differing by one edge not incident to v, we have $\\begin{array}{r}{\\mathcal{D}_{\\alpha}(\\mathcal{A}(\\mathcal{G},v)\\|\\mathcal{A}(\\mathcal{G}^{\\prime},\\overset{\\cdot}{v}))\\leq\\epsilon}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This study centers on a category of $\\gamma_{\\mathrm{max}}<1$ Lipschitz continuous graph diffusions, encompassing prevalent techniques such as PageRank [1] and PPR [55]. It is noted that each diffusion map $\\phi_{k}$ within graph diffusion maintains the total mass of the diffusion vector $\\mathbf{s}_{k}$ , owing to the condition $\\begin{array}{r}{\\sum_{i\\in[3]}\\Bar{\\gamma}_{i,k}=1}\\end{array}$ and the property of the random walk matrix $\\mathbf{P}$ being a left stochastic matrix. This observation entails that the diffusion map $\\phi_{k}$ in Eq. (1) constitutes a strictly contraction map in the metric space $(\\mathbb{R}^{|\\mathcal{V}|},\\|\\cdot\\|_{1})$ . Consequently, graph diffusion can be construed as a composite of contraction maps. The PABI technique has been devised to privatize contractive iterations by injecting random noise per iteration. Empirical studies suggest that distributing noise throughout the diffusion steps can provide improved utility-privacy trade-offs compared to output perturbation alone [25]. This insight serves as a key motivation for employing PABI to establish privacy-preserving graph diffusion. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries: Privacy Amplification by Iteration ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The technique of Privacy Amplification by Iteration (PABI), originally introduced by Feldman et al. [26] for convex risk minimization problems via noisy gradient descent, bounds the privacy loss of an iterative algorithm without releasing the full sequence of iterates. This approach applies to processes generated by Contractive Noisy Iteration (CNI) defined as follows. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Contractive Noisy Iteration (CNI) [26]). Consider a Banach space $(\\mathcal{X},\\|\\cdot\\|)$ with an initial random state $X_{0}\\,\\in\\,{\\mathcal{X}}$ , a series of contractions (i.e., $c$ -Lipschitz functions, $c\\leq1\\,$ ) $\\psi_{k}:$ $\\mathcal X\\rightarrow\\mathcal X$ , and a sequence of noise random variables $\\{\\xi_{k}\\}$ . Defining $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ as a convex bounded set, the Contractive Noisy Iteration $C N I(X_{0},\\{\\psi_{k}\\},\\{\\xi_{k}\\},B)$ is governed by the update rule: ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{k+1}=\\mathcal{P}_{B}[\\psi_{t}(X_{k})+\\xi_{k+1}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{P}_{B}$ is the projection operator onto $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , respecting the norm $\\Vert\\cdot\\Vert$ . ", "page_idx": 3}, {"type": "text", "text": "In the PABI analysis by Feldman et al. [26], gradient descent is conceptualized as a contractive mapping $\\psi_{t}$ in the $\\ell_{2}$ space. Leveraging an additive Gaussian noise mechanism after each iteration, i.e., $\\xi_{k}\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ , leads to the observation that the R\u00e9nyi divergence of identical CNIs with differing initial conditions $X_{0}$ and $X_{0}^{\\prime}$ decays inversely with respect to the total number of iterations $K$ . Specifically, it is observed that $\\begin{array}{r}{\\mathcal{D}_{\\alpha}(X_{K}\\|X_{K}^{\\prime})\\;\\leq\\;\\frac{\\alpha\\|\\mathbf{X}_{0}-\\mathbf{X}_{0}^{\\prime}\\|_{2}}{2K\\sigma^{2}}}\\end{array}$ . Altschuler et al. [27] further extended this framework with improved bound as follows: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $X_{K}$ and $X_{K}^{\\prime}$ denote the outputs from $C N I(X_{0},\\{\\psi_{k}\\},\\{\\xi_{k}\\},B)$ and $C N\\bar{I}(X_{0},\\{\\psi_{k}^{\\prime}\\},\\{\\xi_{k}^{\\prime}\\},B),$ , respectively, where $\\xi_{k},\\xi_{k}^{\\prime}~\\sim~\\mathcal{N}(0,\\sigma^{2}I_{d})$ . Define distortion $\\rho_{\\mathrm{~\\tiny~:=~}}$ $\\operatorname*{sup}_{k,x}\\|\\psi_{k}(x)\\,-\\,\\psi_{k}^{\\prime}(x)\\|$ and let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ have diameter $D$ . If $\\{\\psi_{k}\\}$ and $\\{\\psi_{k}^{\\prime}\\}$ are contractions with coefficient $c<1$ , then for any $\\tau\\in\\{0,\\ldots,K-1\\}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}(X_{K}\\|X_{K}^{\\prime})\\leq\\frac{\\alpha}{\\sigma^{2}}\\left[\\underbrace{(K-\\tau)\\rho^{2}}_{D i s t o r t i o n\\:A b s o r p t i o n}+\\underbrace{c^{2(K-\\tau)}D^{2}}_{P A B I}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The bound in Eq. (3) demonstrates that the R\u00e9nyi divergence between two CNIs can be quantified by the cumulative R\u00e9nyi divergence over Gaussian noise with a distortion factor $\\rho$ (the Distortion Absorption term), complemented by a PABI term. The latter indicates that identical contractive transformations applied to bounded processes reduce privacy leakage in an exponential manner. Note that the bound in Eq. (3) is convex with respect to $\\tau$ , optimized selection of $\\tau$ leads to non-divergent upper bound $\\tilde{\\rho}(\\ln(\\bar{D}^{2}/\\tilde{\\rho})+1)$ where $\\tilde{\\rho}=\\dot{\\rho}^{2}/2\\ln(1/\\dot{c})$ . ", "page_idx": 3}, {"type": "text", "text": "Note on Parameter Set Diameter $_{D}$ . The privacy bound in Eq. (3) relies on the assumption of bounded diameter $D$ of the parameter set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ to upper bound $\\infty$ -Wasserstein distance (definition in App. A) between the coupled CNI processes $X_{\\tau}$ and $X_{\\tau}^{\\prime}$ . Although in theory, the upper bound of Eq. (3) only depends on $\\log D$ by optimizing $\\tau$ , we notice that the value $D$ is important to get a practically meaningful privacy bound. To tighten this term, we will introduce a novel $\\infty$ -Wasserstein distance tracking method that circumvents the need for the diameter parameter in Lemma 3 (detailed later): A high-level idea is to track the $\\infty$ -Wasserstein distance between noisy iterates via constructed couplings instead of using the default set diameter as an upper bound. ", "page_idx": 3}, {"type": "text", "text": "Note on Noise Random Variables $\\xi_{k}$ . The traditional PABI analysis primarily examines gradient descent within the $\\ell_{2}$ space employing the Gaussian mechanism. In contrast, in the context of graph diffusions, modifications to Proposition 1 are necessary to accommodate the $\\ell_{1}$ norm and the application of Laplace noise. This adaptation to the Laplace mechanism, crucial for the graph diffusion applications, has not been previously addressed in the literature to our knowledge. ", "page_idx": 3}, {"type": "text", "text": "3.2 Private Graph Diffusions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now introduce our noisy graph diffusion framework, designed to ensure edge-level RDP and its personalized variant. Our approach consists of injecting Laplace noise into the contractive diffusion process and integrating a graph-dependent thresholding function to mitigate the high sensitivity associated with perturbations of low-degree nodes. ", "page_idx": 3}, {"type": "text", "text": "Given a graph diffusion process $\\mathcal{D}=\\{\\phi_{k}\\}_{k=1}^{\\infty}$ , we introduce a noisy graph diffusion $\\mathcal{D}_{K,\\sigma}$ where $K$ denotes the diffusion steps and $\\sigma$ is the standard deviation of the added noise, constructed by a series of composing noisy graph diffusion mappings $\\phi_{k,\\sigma}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{K,\\sigma}=\\phi_{K,\\sigma}\\circ\\phi_{K-1,\\sigma}\\circ\\cdots\\circ\\phi_{1,\\sigma},\\mathrm{~where~}\\phi_{k,\\sigma}(\\mathbf{s}_{k-1})=\\phi_{k}(f(\\mathbf{s}_{k-1}))+\\xi_{k}^{(1)}+\\xi_{k}^{(2)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f$ is a graph-dependent degree-based function set as $f(\\mathbf{x})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},-\\eta\\cdot\\mathbf{d}),\\eta\\cdot\\mathbf{d})$ with a threshold parameter $\\eta$ to balance privacy-utility trade-off. Specifically, $f$ clips the values of the diffusion vector according to node degrees. Notably, the thresholding function $f$ allows for negative signals, capturing scenarios where the diffusion coefficient $\\gamma_{1,k}$ can be negative. Noise variables $\\xi_{k}^{(1)}$ and $\\xi_{k}^{(2)}$ are independently sampled from a Laplace distribution ${\\mathcal{L}}(\\mathbf{0},\\sigma)$ . It is noteworthy that our framework can also be extended to accommodate Gaussian distributions. However, Gaussian noise has been shown to be suboptimal for graph diffusion in $\\ell_{1}$ space, with empirical evidence provided in App. D.4. ", "page_idx": 4}, {"type": "text", "text": "Design of Thresholding Function $\\pmb{f}$ . In the noisy graph diffusion process, the role of the graph-dependent thresholding function $f$ is twofold. Firstly, $f$ ensures a bounded distance between the coupled diffusions over two adjacent graphs, analogous to the role of the general projection operator PB in the standard CNI as defined in Eq. (2). ", "page_idx": 4}, {"type": "text", "text": "Such a bounding effect is also crucial for the later analysis of $\\infty$ -Wasserstein distance tracking in Lemma 3. Secondly, and more critically, our theoretical analysis reveals that edge perturbation affecting low-degree nodes results in increased distortion at each diffusion step (illustrated in Fig. 1). Uniform thresholding coupled with randomness injection for all nodes typically yields suboptimal performance in such cases. Our degree-dependent design naturally controls the distortion per iteration caused by low-degree nodes which helps with reducing the added noise. More detailed distortion analysis on $f$ is shown later in Lemma 2. The threshold parameter $\\eta$ is commonly employed to optimize the privacy-utility trade-off in practical applications [20]. The empirical benefits of $f$ are explored in experiments detailed in Sec. 4.2. ", "page_idx": 4}, {"type": "image", "img_path": "aon7bwYBiq/tmp/a0abaa88a1bb4006799708cbf9ed0c32f3d54a00f9f25ffef2b3889a8d98c2be.jpg", "img_caption": ["Figure 1: Illustration of Distortion from Edge Perturbations over Adjacent Graphs for Nodes with Low and High Degrees. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Discussion on Dual Noise Injection. Our framework   \nemploys a noise-splitting technique, injecting dual Laplace noise at each diffusion step to construct non-divergent privacy bounds, as outlined in Eq. (3). Theoretical justifications for this design is provided in the proof sketch. ", "page_idx": 4}, {"type": "text", "text": "Following this, we present our main result on the privacy guarantee of noisy graph diffusion: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Privacy Guarantees of Noisy Graph Diffusions). Given a graph $\\mathcal{G}$ , an associate graph diffusion $\\mathcal{D}=\\{\\phi_{k}\\}_{k=1}^{\\infty}$ , then noisy graph diffusion mechanism $\\mathcal{D}_{K,\\sigma}$ ensures edge-level $(\\alpha,\\epsilon)$ -RDP with $\\epsilon$ satisfies: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\operatorname*{min}_{\\tau\\in\\{0,1,\\ldots,K-1\\}}\\left[\\left(K-\\tau\\right)\\cdot g_{\\alpha}\\left(\\sigma,\\rho_{d i f f}\\right)+g_{\\alpha}\\left(\\sigma,\\frac{\\rho_{d i f f}\\cdot\\left(1-\\gamma_{m a x}^{\\tau}\\right)}{1-\\gamma_{m a x}}\\cdot\\gamma_{m a x}^{K-\\tau}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{g_{\\alpha}(\\sigma,\\rho)\\,=\\,\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}\\rho)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}\\rho))}\\end{array}$ denotes the R\u00e9nyi divergence induced by the Laplace mechanism $[44]_{;}$ , and $\\rho_{d i f f}=\\operatorname*{max}(4\\gamma_{m a x}^{(1)},2\\gamma_{m a x})\\cdot\\eta$ represents the maximum single-step distortion incurred by diffusion on adjacent graphs that involves Lipschitz continuity coefficient $\\gamma_{m a x}$ , and maximum diffusion coefficient $\\gamma_{m a x}^{(1)}$ . ", "page_idx": 4}, {"type": "text", "text": "By selecting $\\begin{array}{r}{\\tau=\\lceil K-\\ln((\\frac{1}{\\rho_{d i f f}}+\\frac{1}{1-\\gamma_{m a x}})\\ln\\frac{1}{\\gamma_{m a x}})/\\ln(\\frac{1}{\\gamma_{m a x}})\\rceil,}\\end{array}$ , privacy budget $\\epsilon$ remains bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon\\lesssim\\frac{\\rho_{d i f f}}{\\sigma\\cdot\\ln\\left(\\frac{1}{\\gamma_{m a x}}\\right)}\\left[\\ln\\left(\\left(\\frac{1}{\\rho_{d i f f}}+\\frac{1}{1-\\gamma_{m a x}}\\right)\\ln\\frac{1}{\\gamma_{m a x}}\\right)+1\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The privacy bound in Eq. (5) consists of two components: the distortion absorption term (the first term on the RHS) and the PABI term (the second term in RHS). Distortion absorption quantifies the cumulative R\u00e9nyi divergence over Laplace noise with single-step distortion $\\rho_{\\mathrm{diff}}$ , while the PABI term quantifies the exponential decay rate, echoing the result in Eq. (3). However, a key difference lies in our approach; instead of leveraging the projected set diameter $D$ to control the distance between coupled CNIs, our proposed $\\infty$ -Wasserstein tracking method yields a more practical term, \u03c1diff1\u00b7(\u22121\u03b3\u2212m\u03b3axm\u03c4ax). Further details and utility evaluations of this tool are presented in the proof sketch and Sec. 4.2, respectively. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The function $g_{\\alpha}(\\sigma,\\rho)$ , which measures R\u00e9nyi divergence for the Laplace mechanism, increases with distortion $\\rho$ and decreases with noise scale $\\sigma$ . This behavior implies that reducing distortion and increasing the noise scale enhances privacy. To achieve better calibrated noise within a given privacy budget $\\epsilon$ , we calculate the two terms in Eq. (5) for each $\\tau$ . Leveraging the monotonicity of $g_{\\alpha}(\\sigma,\\rho)$ , we employ a binary search to identify the appropriate noise scale $\\sigma$ . The optimal noise scale is then determined by selecting the minimum value across various $\\tau$ values, achieving this efficiently with linear complexity relative to $\\tau$ . ", "page_idx": 5}, {"type": "text", "text": "It is important to note that the maximum single-step distortion $\\rho_{\\mathrm{diff}}$ in Eq. (5) is tight and conveys several messages. First, as defined in Eq. (1), when the diffusion process is relatively slow (i.e., $\\gamma_{1,k}<\\gamma_{2,k})$ , the distortion remains tight, governed by the Lipschitz constant $\\gamma_{\\mathrm{max}}$ of the diffusion mapping. In contrast, when the diffusion is relatively fast (i.e., $\\gamma_{1,k}\\,\\geq\\,\\gamma_{2,k}\\mathrm{)}$ , the distortion bound becomes asymptotically tight, depending on graph structures, with worst-case scenarios detailed in App. B.1. ", "page_idx": 5}, {"type": "text", "text": "In Eq. (6), we demonstrate the convergence of the privacy budget with respect to diffusion steps $K$ . This approach differs from the adaptive composition theorem [42], which analyzes how privacy guarantees degrade when composed mechanisms are applied. Although this method has commonly been employed to protect privacy in graph learning models [52,53,56], it leads to a linear increase in the privacy budget with the number of iterations $K$ under R\u00e9nyi divergence [44], potentially resulting in unbounded losses as $K$ grows to infinity. More importantly, even for a small number of diffusion steps, our framework achieves a significantly better privacy budget under practical PPR diffusion settings, as illustrated in Fig. 2. Further empirical evaluations are detailed in App. D.4. ", "page_idx": 5}, {"type": "image", "img_path": "aon7bwYBiq/tmp/1e7c1d7712b74fe00893131d9ec9f39ca0fc6c55b407690e6004db65bf1eb539.jpg", "img_caption": ["Figure 2: RDP vs. Total Diffusion Step $K$ with $\\gamma_{1,k}~=~0.8,\\gamma_{2,k}~=~$ $0,\\gamma_{3,k}=0.2,\\alpha=2,\\sigma=0.01$ , and \u03b7 = 10\u22125. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Proof Sketch of Theorem 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Proof Idea. Similar to Eq. (3), the privacy loss of adjacent graph diffusion processes can be bounded as the sum of distortion absorption term incurred by Laplace noise and a PABI term at intermediate step $\\tau$ (Step 1 & 2). Subsequently, we explore degree-based thresholding to manage distortion, achieving a superior utility-privacy tradeoff (Step 3), and introduce $\\infty$ -Wasserstein distance tracking to further tighten the divergence at $\\tau$ (Step 4). ", "page_idx": 5}, {"type": "text", "text": "Step 1: Interpretation of Iterates as Conditional CNI Sequences. Consider the coupled graph diffusions $\\mathcal{D}=\\bar{\\{\\phi_{k}\\}}_{k=1}^{\\infty}$ and $\\mathcal{D}^{\\prime}=\\{\\phi_{k}^{\\prime}\\}_{k=1}^{\\infty}$ , and the thresholding functions $f$ and $f^{\\prime}$ , operating over adjacent graphs $\\mathcal{G}$ and $\\mathcal{G}^{\\prime}$ , respectively. In each diffusion step, the first noise component constructs noisy iterates, while the second noise component is used to absorb distortion incurred between the adjacent graphs. We encapsulate the discussion as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{s}_{k}=\\underbrace{\\phi_{k}(f(\\mathbf{s}_{k-1}))+\\xi_{k}^{(1)}}_{=}+\\xi_{k}^{(2)},\\ \\mathbf{s}_{k}^{\\prime}=\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{s}_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}+\\xi_{k}^{\\prime(2)}\\overset{d}{=}\\underbrace{\\phi_{k}(f(\\mathbf{s}_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}}_{=}+\\tilde{\\xi}_{k}^{\\prime(2)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where\u03bek , \u03bek $\\xi_{k}^{(1)},\\xi_{k}^{(2)},\\xi_{k}^{\\prime(1)},\\xi_{k}^{\\prime(2)}\\;\\sim\\;\\mathcal{L}(\\mathbf{0},\\sigma)$ , and $\\tilde{\\xi}_{k}^{\\prime(2)}\\,\\sim\\,\\mathcal{L}\\left(\\phi_{k}^{\\prime}(f^{\\prime}({\\bf s}_{k-1}^{\\prime}))-\\phi_{k}(f({\\bf s}_{k-1}^{\\prime})),\\sigma\\right)$ , and $\\underline{{\\underline{{d}}}}$ denotes equality in distribution. ", "page_idx": 5}, {"type": "text", "text": "When the distortion $\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{s}_{k-1}^{\\prime}))\\,-\\,\\phi_{k}(f(\\mathbf{s}_{k-1}^{\\prime}))$ is absorbed by the conditional event of noise variables, i.e., \u03be(k2) $\\xi_{k}^{(2)}\\:=\\:\\tilde{\\xi}_{k}^{(2)}$ , the coupled diffusion vectors evolve with identical CNIs through the contractive mapping $\\phi_{k}\\circ f$ . Note that the Laplace distribution is essential for fully exploiting $\\ell_{1}$ distortion in our analysis. ", "page_idx": 5}, {"type": "text", "text": "Step 2: Bounding Privacy Loss through Distortion Absorption and PABI. The privacy loss of coupled iterates $\\bar{D_{\\alpha}}(\\mathbf{s}_{K}\\|\\mathbf{s}_{K}^{\\prime})$ can be bounded by the distortion from graph diffusion, and PABI: ", "text_level": 1, "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\boldsymbol{\\alpha}}(\\mathbf{s}_{K}\\|\\mathbf{s}_{K}^{\\prime})\\leq\\mathcal{D}_{\\boldsymbol{\\alpha}}(\\xi_{\\tau+1:K}^{(2)}\\|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)})+\\operatorname*{sup}_{\\boldsymbol{\\zeta}}\\mathcal{D}_{\\boldsymbol{\\alpha}}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\boldsymbol{\\zeta}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}=\\boldsymbol{\\zeta})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This inequality arises from leveraging the post-processing and strong composition rules of R\u00e9nyi divergence. Here, $\\zeta$ represents a joint noise realization, and the parameter $\\tau$ is introduced to balance the privacy leakage from the two terms \u2014 the divergence between the shifted noise variables accumulated from step $\\tau+1$ to step $K$ (Distortion Absorption), and the divergence across conditional CNIs employing identical transformations $\\phi_{k}\\circ f$ (PABI). ", "page_idx": 6}, {"type": "text", "text": "Step 3: Bounding Distortion Absorption ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Absorption of Distortion in Laplace Distribution). For any $\\tau\\,\\in\\,\\{0,1,...,K-1\\}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\alpha}(\\xi_{\\tau+1:K}^{(2)}||\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)})\\leq(K-\\tau)g_{\\alpha}(\\sigma,\\tilde{\\rho})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\tilde{\\rho}$ quantifies the maximum distortion introduced by a single-step diffusion and is determined by the thresholding function $f$ normalized by node degrees, i.e., $\\frac{[f(\\mathbf{s}_{k})]_{i}}{d_{i}}$ ", "page_idx": 6}, {"type": "text", "text": "The observation on $\\tilde{\\rho}$ highlights the importance of a degree-based design for the thresholding function $f$ . Uniform thresholding across all nodes results in distortion proportional to d1 , introducing unnecessarily large noise induced by low-degree nodes and degrading overall performance. This in principle inspires the choice of $f$ relying on node degrees. Consequently, $\\tilde{\\rho}$ is tightly bounded by $\\bar{\\rho_{\\mathrm{diff}}}=\\operatorname*{max}\\bar{(4\\gamma_{\\mathrm{max}}^{(1)},2\\gamma_{\\mathrm{max}})}\\cdot\\eta.$ ", "page_idx": 6}, {"type": "text", "text": "Step 4: Upper Bounding PABI with $\\infty$ -Wasserstein distance tracking. To perform tight privacy analysis for the second term in Eq. (7), we develop a novel $\\infty$ -Wasserstein distance tracking method for coupled CNIs, where we denote the $\\infty$ -Wasserstein distance at step $\\tau$ by $w_{\\tau}$ . This method discards the original boundedness condition in PABI (Eq. (3) Second Term), which relies on the diameter $D$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (PABI with $\\infty$ -Wasserstein Distance Tracking). Given two coupled graph diffusions mentioned above, for any $\\tau\\in\\{0,1,...,\\dot{K}-\\check{1}\\}$ , any noise realization $\\zeta$ , we have ", "page_idx": 6}, {"type": "image", "img_path": "aon7bwYBiq/tmp/c4fa4a06944a69706a92169635aa6c73820f9f5bde8a3a40a7a30268792457ad.jpg", "img_caption": ["Figure 3: Setting: Graph Diffusion with $\\gamma_{1,k}\\,=\\,0.8,\\gamma_{2,k}\\,=\\,0,\\gamma_{3,k}\\,=$ 0.2. "], "img_footnote": [], "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta||\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}=\\zeta)\\leq g_{\\alpha}(\\sigma,\\gamma_{m a x}^{K-\\tau}w_{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the tracked $\\infty$ -Wasserstein distance over coupled CNIs is given by $\\begin{array}{r}{w_{\\tau}=\\frac{\\rho_{d i f f}\\cdot(1-\\gamma_{m a x}^{\\tau})}{1-\\gamma_{m a x}}}\\end{array}$ and is naturally upper bounded by1\u03c1d\u03b3iffmax $\\frac{\\rho_{d i\\!\\!/\\!\\!f}}{1\\!-\\!\\gamma_{m a x}}:=w$ . ", "page_idx": 6}, {"type": "text", "text": "We argue that using $w_{\\tau}$ (or the upper bound $w$ ) instead of the default diameter $D$ is crucial to make the algorithm practically useful. There is no numerical evaluation in the previous study [27]. tNhue mreearli-cwalo crlod mBplaorgisCoant ableotgw edeatna $w$ ta (ndde tthaiel eddi aimn eSteerc . o4f )t,h irse islhluolstdriantge df uinn ctFiiogn. $\\begin{array}{r}{D=\\eta\\!\\cdot\\!\\sum_{i=1}^{|\\mathcal{V}|}d_{i}}\\end{array}$ , eurss-inofg$w$ magnitude improvement, which is still significant even if $D$ impacts privacy loss via a logarithmic term. Further empirical validations demonstrating significant utility improvements are detailed in Sec. 4.2. ", "page_idx": 6}, {"type": "text", "text": "By substituting the bounds from Eq. (8) and Eq. (9) into Eq. (7), we establish Theorem 1. ", "page_idx": 6}, {"type": "text", "text": "3.4 Personalized Graph Diffusion Algorithms with Application in PPR Diffusion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In practice, graph diffusions often originate from a single node $\\mathbf{e}_{i}$ , personalizing the algorithm to this seed node (user). Since the output is provided only to the seed node, protecting its edge connections (one-hop neighbors) becomes unnecessary, ensuring no privacy leakage in the first diffusion step under personalized privacy guarantees. Consequently, the thresholding function is tailored as follows: $f(\\mathbf{\\bar{x}})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},-\\bar{\\eta}\\cdot\\tilde{\\mathbf{d}}),\\eta\\cdot\\tilde{\\mathbf{d}})$ , where $\\left[\\tilde{\\mathbf{d}}\\right]_{j}=[\\mathbf{d}]_{j}$ for $j\\neq i$ and $[\\tilde{\\mathbf{d}}]_{i}$ can be set to any positive threshold, i.e., no control is needed for the diffusion over seed node. We employ personalized edge-level RDP (Definition 2), caring two adjacent graphs with a difference only in a single edge not linked directly to the seed node. This approach is encapsulated in the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Privacy Guarantees for Personalized Noisy Graph Diffusions). Given a graph $\\mathcal{G}$ , an associate graph diffusion $\\mathcal{D}=\\{\\phi_{k}\\}_{k=1}^{\\infty},$ , then personalized noisy graph diffusion mechanism $\\mathcal{D}_{K,\\sigma}$ with corresponding $f(\\mathbf{x})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},-\\eta\\cdot\\tilde{\\mathbf{d}}),\\eta\\cdot\\tilde{\\mathbf{d}})$ ensures personalized edge-level $(\\alpha,\\epsilon)$ -RDP ", "page_idx": 6}, {"type": "text", "text": "with \u03f5 satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\operatorname*{min}_{\\tau\\in\\{0,1,\\ldots,K-1\\}}\\left[(K-\\tau)\\cdot g_{\\alpha}\\left(\\sigma,\\rho_{d i f f}\\cdot\\mathbb{1}_{\\tau\\neq0}\\right)+g_{\\alpha}\\left(\\sigma,\\frac{\\rho_{d i f f}\\cdot\\left(1-\\gamma_{m a x}^{\\tau}\\right)}{1-\\gamma_{m a x}}\\cdot\\gamma_{m a x}^{K-\\tau}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where 1 denote indicator function. ", "page_idx": 7}, {"type": "text", "text": "Note that a key difference from Theorem 1 is that in personalized privacy settings, there is no privacy leakage in the first diffusion step $(K=1)$ ). ", "page_idx": 7}, {"type": "text", "text": "PPR Application. Among various graph diffusions, PPR stands out as a prevalent node proximity metric extensively used in graph mining and network analysis. We may apply our noisy diffusion framework to PPR diffusion. We consider PPR with lazy random walk as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathcal{D}}(\\mathbf{s})=(1-\\beta)\\sum_{k=0}^{\\infty}\\beta^{k}\\mathbf{W}\\mathbf{s}=\\operatorname*{lim}_{K\\rightarrow\\infty}\\phi_{K}\\circ\\cdot\\cdot\\circ\\phi_{1}(\\mathbf{s}),{\\mathrm{~where~}}\\phi_{k}(\\mathbf{x})=\\beta\\mathbf{W}\\mathbf{x}+(1-\\beta)\\mathbf{s}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where lazy random walk matrix $\\mathbf{W}={\\textstyle\\frac{1}{2}}(\\mathbf{P}+\\mathbf{I})$ and $(1-\\beta)$ represents teleport probability with $\\beta\\in(0,1]$ . ", "page_idx": 7}, {"type": "text", "text": "Our framework incorporates noise into the diffusion process of each step of PPR. The privacy guarantees for this noisy PPR are derived from Theorem 4 with $\\rho_{\\mathrm{diff}}\\,=\\,2\\beta\\eta$ and $\\gamma_{\\mathrm{max}}\\,=\\,\\beta$ . Note that, since $\\gamma_{j,k}>0$ for all $j\\in\\{1,2,3\\}$ in PPR scenarios, all signals propagating among nodes should be non-negative. Consequently, the degree-based thresholding function $f$ can be further modified as $f(\\mathbf{x})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},\\mathbf{0}),\\eta\\cdot\\tilde{\\mathbf{d}})$ . ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present empirical evaluations to support our theoretical findings. Specifically, we apply the widely-used PPR algorithm (Sec. 3.4) to real-world graphs. We focus on the accuracy of noisy PPR in ranking tasks under personalized edge-level DP due to its practicality as noted in [20]. ", "page_idx": 7}, {"type": "text", "text": "Benchmark Datasets. We conduct experiments on the following datasets: BlogCatalog [57], a social network of bloggers with 10,312 nodes and 333,983 edges; Flickr [57], a photo-sharing social network with 80,513 nodes and 5,899,882 edges; and TheMarker [58], an online social network with 69,400 nodes and 1,600,000 edges. ", "page_idx": 7}, {"type": "text", "text": "Baselines. Our experimental study includes two baselines. DP-PUSHFLOWCAP [20] is the only private PPR method using Laplace output perturbation, adapting the approximate PPR algorithm with push operations [41]. Edge-Flipping is the other baseline, which uses a randomized response mechanism [42] on the adjacency matrix, excluding seed node-connected edges in personalized scenarios. Entries are replaced with values in $\\{0,1\\}$ uniformly at random with probability $p$ (detailed in App. E), or retained otherwise. This method requires $\\mathcal{O}(|\\mathcal{V}|^{2})$ time to generate a private adjacency matrix and increases its edge density, which limits its practicality. Both our approach and DP-PUSHFLOWCAP offer better scalability. A comparison of running times between different approaches is provided in App. D.2. In all experiments, we only report results if a single trial can be completed within 12 hours on an AMD EPYC 7763 64-Core Processor, and thus Edge-Flipping cannot be run on Flickr. ", "page_idx": 7}, {"type": "text", "text": "Metrics. For utility, we employ two ranking-based metrics: normalized discounted cumulative gain at $\\textsf{R}(\\mathrm{NDCG@R})$ and Recall $\\mathbb{\\mathbf{\\oplus}R}$ [59], where $\\mathbf{R}$ denotes the cutoff point for the top-ranked items in the list. In our experiments, $\\mathbf{R}$ is set to 100. For privacy assessments, we utilize the personalized edge-level $(\\epsilon,\\delta)$ -DP, with $\\delta$ set to #e1dges following [53]. To ensure a fair comparison, both DP-PUSHFLOWCAP and Edge-Flipping are analyzed using RDP. The privacy budgets for all methods are subsequently converted to DP from RDP results, as elaborated in App. E. All results are reported as averages over 100 independent trials, with $95\\%$ confidence intervals. ", "page_idx": 7}, {"type": "text", "text": "4.1 Evaluating Privacy-Utility Tradeoffs on Ranking Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this series of experiments, we aim to assess the ranking performance of our noisy graph diffusion (as delineated in Sec. 3.4) compared with baselines on real-world graphs. We specifically examine privacy budget $\\epsilon$ ranging from $\\dot{1}0^{-2}$ to 1, PPR with parameter $\\beta=0.8$ . Considering that both our approach and DP-PUSHFLOWCAP employ a thresholding parameter $\\eta$ to balance the privacy-utility trade-off, we select $\\eta$ from a set of seven values spanning orders of magnitude from $\\mathrm{i0^{-10}}$ to $10^{-\\check{4}}$ , a range empirically determined to be optimal across various datasets for both methods $(\\eta=10^{-6}$ is chosen for DP-PUSHFLOWCAP in [20]). For each experiment, we randomly choose an initial seed for diffusion and execute PPR for 100 iterations following [20]. We report the average NDCG $@100$ score and Recall $@100$ compared to the standard noise-free PPR diffusion (Eq. (11)) over 100 independent trials in Fig. 4 and Fig. 5 respectively. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "aon7bwYBiq/tmp/0d984b84bb9694f66663e587fb7a062e742aa45bd0c064f172e91ad2efe30cf8.jpg", "img_caption": ["Figure 4: Trade-off between NDCG and Personalized Edge-level Privacy. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results for NDCG $@100$ . As illustrated in Fig. 4, our noisy graph diffusion surpasses both baselines across all three datasets, where values below 0.7 are ignored. In a strong privacy regime $(\\epsilon\\leq0.5)$ , our approach demonstrates significant improvement over DP-PUSHFLOWCAP, which relies on output perturbation. This validates our claim that a noisy process achieves a superior privacy-utility tradeoff in stringent privacy settings. ", "page_idx": 8}, {"type": "text", "text": "Results for Recall $@100$ . Fig. 5 illustrates the overlap of the top-100 predictions of privacypreserving PPR variants with standard PPR. Across all datasets, our method outperforms two baselines for $\\epsilon$ values ranging from $10^{-2}$ to 1, further substantiating the advantages of our framework. ", "page_idx": 8}, {"type": "image", "img_path": "aon7bwYBiq/tmp/34ce9f777b32db11bc0ccff3418ee61d7a5450dbf259d385783597d3b7364ebc.jpg", "img_caption": ["Figure 5: Trade-off between Recall and Personalized Edge-level Privacy. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Additional experiments on the sensitivity of ranking performance over variations in $\\eta$ are deferred to App. D.3, which demonstrate that our approach is significantly more robust on the choice of the hyperparameter $\\eta$ . ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct an ablation study to verify the effectiveness of our theory-guided designs, including the degree-based thresholding function $f$ , the $\\infty$ -Wasserstein distance tracking tool. Experiments were conducted on BlogCatalog, utilizing PPR diffusion with parameter $\\beta=0.8$ , and a total of $K=100$ diffusion steps. Additional ablation studies focusing on variations in noise type and comparative analyses of noise scales across different methods are detailed in App. D.4. ", "page_idx": 8}, {"type": "text", "text": "Degree-based Thresholding Function & $\\infty$ -Wasserstein Tracking. We evaluate the effectiveness of our graph-dependent thresholding function $f$ and $\\infty$ -Wasserstein distance tracking. ", "page_idx": 8}, {"type": "text", "text": "We vary $\\eta$ uniformly across seven discrete values in from $10^{-4}$ to $\\mathrm{i0^{-10}}$ and report the optimal performance for all methods in Fig. 6. Firstly, we compare the degreebased thresholding function $f(\\mathbf{s})\\;\\;=\\;\\;\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},0),\\eta$ \u00b7 d) (Red ) against its graph-independent variant $\\begin{array}{r l}{\\tilde{f}(\\mathbf{s})}&{{}=}\\end{array}$ $\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},0),\\eta\\cdot\\mathbf{1})\\ (\\mathrm{Red\\\\times})$ , which uniformly clips over nodes regardless of their degree. As $\\epsilon$ varies from 0.1 to 3, the degree-based thresholding function consistently outperforms its graph-independent counterpart, with a margin of 0.15 to 0.2 in the NDCG score. This verifies the effectiveness of graph-dependent thresholding function $f$ , which can balance the privacy-utility trade-off more effectively by focusing less on sensitive low-degree nodes. Note that the graph-dependent (in)dependent thresholding function $f\\left(\\tilde{f}\\right)$ naturally induces a ", "page_idx": 8}, {"type": "text", "text": "Thresholding & $W_{\\infty}$ Tracking ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "aon7bwYBiq/tmp/975f3225aa6b300073d53f82d0c966bc03f3833256109e00de2978251fa2086b.jpg", "img_caption": ["Figure 6: Effectiveness of degreebased thresholding function $f$ and $\\infty$ -Wasserstein distance tracking. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "diameter $\\eta|\\mathcal{V}|\\left(2\\eta|\\mathcal{E}|\\right)$ , respectively, which can be utilized in the modification of Theorem 1 for privacy accounting. However, the graph dependent thresholding function $f$ results in a significantly larger diameter, which substantially degrades utility. This underscores the importance of leveraging $\\infty$ -Wasserstein tracking. ", "page_idx": 9}, {"type": "text", "text": "We showcase the benefits of our $\\infty$ -Wasserstein distance tracking analysis (Red Lines) on the privacy-utility tradeoff against diameter induced bounds derived from $\\ell_{1}$ projection (Green Lines) or thresholding function-induced diameter (Blue Lines). The privacy bound of the $\\ell_{1}$ projection is established based on a Laplace modification of Eq. (3) with a diameter of 1. As shown in Fig. 6, methods employing $\\infty$ -Wasserstein distance tracking analysis consistently outperform those based on $\\ell_{1}$ projection and thresholding-induced diameter. We conclude that our $\\infty$ -Wasserstein tracking analysis and the design of graph-dependent thresholding function $f$ markedly enhance the privacyutility tradeoff. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we introduce a noisy graph diffusion framework for edge-level privacy protection, utilizing a novel application of PABI in $\\ell_{1}$ space. By incorporating a theory-guided design for a graph-dependent thresholding function and employing a new $\\infty$ -Wasserstein distance tracking tool, we outperform SOTA methods in ranking performance on benchmark graph datasets. ", "page_idx": 9}, {"type": "text", "text": "Societal Impact and Limitations. Our work advances the development of DP graph algorithms, offering strong privacy protection when properly implemented. For its limitations, we refer readers to standard textbooks on the subject [42]. The effectiveness of our noisy graph diffusion framework has been demonstrated primarily in the context of PPR diffusion. Extending these results to other types of graph diffusions, such as heat kernel diffusion, is a promising direction for future research. This paper focuses on edge-level privacy protections; exploring extensions to node-level DP protections could further broaden the scope of our research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledge ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by NSF awards CCF-2402816, IIS-2239565, and JPMC faculty award 2023. The authors would like to thank Haoyu Wang, Haoteng Yin for the valuable discussion. The authors also would like to thank Alessandro Epasto for the discussion about the implementation of the outputperturbation-based differentially private Personalized Pagerank. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] L. Page, S. Brin, R. Motwani, T. Winograd, et al., \u201cThe pagerank citation ranking: Bringing order to the web,\u201d 1999.   \n[2] F. Chung, \u201cThe heat kernel as the pagerank of a graph,\u201d Proceedings of the National Academy of Sciences, vol. 104, no. 50, pp. 19735\u201319740, 2007.   \n[3] J. Cho and U. Schonfeld, \u201cRankmass crawler: A crawler with high pagerank coverage guarantee.,\u201d in VLDB, vol. 7, pp. 23\u201328, Citeseer, 2007.   \n[4] R. Andersen, F. Chung, and K. Lang, \u201cUsing pagerank to locally partition a graph,\u201d Internet Mathematics, vol. 4, no. 1, pp. 35\u201364, 2007.   \n[5] S. Fortunato, \u201cCommunity detection in graphs,\u201d Physics reports, vol. 486, no. 3-5, pp. 75\u2013174, 2010.   \n[6] K. Kloster and D. F. Gleich, \u201cHeat kernel based community detection,\u201d in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1386\u20131395, 2014.   \n[7] P. Li, I. Chien, and O. Milenkovic, \u201cOptimizing generalized pagerank methods for seedexpansion community detection,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[8] G. Iv\u00e1n and V. Grolmusz, \u201cWhen the web meets the cell: using personalized pagerank for analyzing protein interaction networks,\u201d Bioinformatics, vol. 27, no. 3, pp. 405\u2013407, 2011. [9] D. F. Gleich, \u201cPagerank beyond the web,\u201d siam REVIEW, vol. 57, no. 3, pp. 321\u2013363, 2015.   \n[10] J. Gasteiger, A. Bojchevski, and S. G\u00fcnnemann, \u201cPredict then propagate: Graph neural networks meet personalized pagerank,\u201d arXiv preprint arXiv:1810.05997, 2018.   \n[11] J. Gasteiger, S. Wei\u00dfenberger, and S. G\u00fcnnemann, \u201cDiffusion improves graph learning,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[12] E. Chien, J. Peng, P. Li, and O. Milenkovic, \u201cAdaptive universal generalized pagerank graph neural network,\u201d arXiv preprint arXiv:2006.07988, 2020.   \n[13] B. Chamberlain, J. Rowbottom, M. I. Gorinova, M. Bronstein, S. Webb, and E. Rossi, \u201cGrand: Graph neural diffusion,\u201d in International Conference on Machine Learning, pp. 1407\u20131418, PMLR, 2021.   \n[14] J. Hoskins, C. Musco, C. Musco, and B. Tsourakakis, \u201cInferring networks from random walkbased node similarities,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.   \n[15] L. Backstrom, C. Dwork, and J. Kleinberg, \u201cWherefore art thou $\\mathrm{r3579x?}$ anonymized social networks, hidden patterns, and structural steganography,\u201d in Proceedings of the 16th international conference on World Wide Web, pp. 181\u2013190, 2007.   \n[16] I. Ullah, R. Boreli, and S. S. Kanhere, \u201cPrivacy in targeted advertising: A survey,\u201d arXiv preprint arXiv:2009.06861, 2020.   \n[17] Q. Zhong, Y. Liu, X. Ao, B. Hu, J. Feng, J. Tang, and Q. He, \u201cFinancial defaulter detection on online credit payment via multi-view attributed heterogeneous information network,\u201d in Proceedings of the web conference 2020, pp. 785\u2013795, 2020.   \n[18] C. Dwork, F. McSherry, K. Nissim, and A. Smith, \u201cCalibrating noise to sensitivity in private data analysis,\u201d in Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pp. 265\u2013284, Springer, 2006.   \n[19] C. Liu, S. Chakraborty, and P. Mittal, \u201cDependence makes you vulnberable: Differential privacy under dependent tuples.,\u201d in NDSS, vol. 16, pp. 21\u201324, 2016.   \n[20] A. Epasto, V. Mirrokni, B. Perozzi, A. Tsitsulin, and P. Zhong, \u201cDifferentially private graph learning via sensitivity-bounded personalized pagerank,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 22617\u201322627, 2022.   \n[21] A. Gupta, K. Ligett, F. McSherry, A. Roth, and K. Talwar, \u201cDifferentially private combinatorial optimization,\u201d in Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 1106\u20131125, SIAM, 2010.   \n[22] R. Arora and J. Upadhyay, \u201cOn differentially private graph sparsification and applications,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[23] S. Wang, Y. Zheng, X. Jia, and X. Yi, \u201cPrivacy-preserving analytics on decentralized social graphs: The case of eigendecomposition,\u201d IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[24] Y. Wang, X. Wu, and L. Wu, \u201cDifferential privacy preserving spectral graph analysis,\u201d in Advances in Knowledge Discovery and Data Mining: 17th Pacific-Asia Conference, PAKDD 2013, Gold Coast, Australia, April 14-17, 2013, Proceedings, Part II 17, pp. 329\u2013340, Springer, 2013.   \n[25] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308\u2013318, 2016.   \n[26] V. Feldman, I. Mironov, K. Talwar, and A. Thakurta, \u201cPrivacy amplification by iteration,\u201d in 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp. 521\u2013 532, IEEE, 2018.   \n[27] J. Altschuler and K. Talwar, \u201cPrivacy of noisy stochastic gradient descent: More iterations without more privacy loss,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 3788\u2013 3800, 2022.   \n[28] S. Ji, P. Mittal, and R. Beyah, \u201cGraph data anonymization, de-anonymization attacks, and deanonymizability quantification: A survey,\u201d IEEE Communications Surveys & Tutorials, vol. 19, no. 2, pp. 1305\u20131326, 2016.   \n[29] J. H. Abawajy, M. I. H. Ninggal, and T. Herawan, \u201cPrivacy preserving social network data publication,\u201d IEEE communications surveys & tutorials, vol. 18, no. 3, pp. 1974\u20131997, 2016.   \n[30] Y. Li, M. Purcell, T. Rakotoarivelo, D. Smith, T. Ranbaduge, and K. S. Ng, \u201cPrivate graph data release: A survey,\u201d ACM Computing Surveys, vol. 55, no. 11, pp. 1\u201339, 2023.   \n[31] K. Nissim, S. Raskhodnikova, and A. Smith, \u201cSmooth sensitivity and sampling in private data analysis,\u201d in Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pp. 75\u201384, 2007.   \n[32] V. Karwa, S. Raskhodnikova, A. Smith, and G. Yaroslavtsev, \u201cPrivate analysis of graph structure,\u201d Proceedings of the VLDB Endowment, vol. 4, no. 11, pp. 1146\u20131157, 2011.   \n[33] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava, and X. Xiao, \u201cPrivate release of graph statistics using ladder functions,\u201d in Proceedings of the 2015 ACM SIGMOD international conference on management of data, pp. 731\u2013745, 2015.   \n[34] F. McSherry and K. Talwar, \u201cMechanism design via differential privacy,\u201d in 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pp. 94\u2013103, IEEE, 2007.   \n[35] M. Hay, C. Li, G. Miklau, and D. Jensen, \u201cAccurate estimation of the degree distribution of private networks,\u201d in 2009 Ninth IEEE International Conference on Data Mining, pp. 169\u2013178, IEEE, 2009.   \n[36] S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova, and A. Smith, \u201cAnalyzing graphs with node differential privacy,\u201d in Theory of Cryptography: 10th Theory of Cryptography Conference, TCC 2013, Tokyo, Japan, March 3-6, 2013. Proceedings, pp. 457\u2013476, Springer, 2013.   \n[37] C. Task and C. Clifton, \u201cA guide to differential privacy theory in social network analysis,\u201d in 2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pp. 411\u2013417, IEEE, 2012.   \n[38] Y. Wang, X. Wu, J. Zhu, and Y. Xiang, \u201cOn learning cluster coefficient of private networks,\u201d Social network analysis and mining, vol. 3, pp. 925\u2013938, 2013.   \n[39] F. Ahmed, A. X. Liu, and R. Jin, \u201cPublishing social network graph eigenspectrum with privacy guarantees,\u201d IEEE Transactions on Network Science and Engineering, vol. 7, no. 2, pp. 892\u2013 906, 2019.   \n[40] X. Li, J. Yang, Z. Sun, J. Zhang, et al., \u201cDifferential privacy for edge weights in social networks,\u201d Security and Communication Networks, vol. 2017, 2017.   \n[41] R. Andersen, F. Chung, and K. Lang, \u201cLocal graph partitioning using pagerank vectors,\u201d in 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pp. 475\u2013 486, IEEE, 2006.   \n[42] C. Dwork, A. Roth, et al., \u201cThe algorithmic foundations of differential privacy,\u201d Foundations and Trends\u00ae in Theoretical Computer Science, vol. 9, no. 3\u20134, pp. 211\u2013407, 2014.   \n[43] P. Kairouz, S. Oh, and P. Viswanath, \u201cThe composition theorem for differential privacy,\u201d in International conference on machine learning, pp. 1376\u20131385, PMLR, 2015.   \n[44] I. Mironov, \u201cR\u00e9nyi differential privacy,\u201d in 2017 IEEE 30th computer security foundations symposium (CSF), pp. 263\u2013275, IEEE, 2017.   \n[45] E. Chien, H. Wang, Z. Chen, and P. Li, \u201cStochastic gradient langevin unlearning,\u201d arXiv preprint arXiv:2403.17105, 2024.   \n[46] E. Chien and P. Li, \u201cConvergent privacy loss of noisy-sgd without convexity and smoothness,\u201d arXiv preprint arXiv:2410.01068, 2024.   \n[47] W. Ye, Z. Huang, Y. Hong, and A. Singh, \u201cGraph neural diffusion networks for semi-supervised learning,\u201d arXiv preprint arXiv:2201.09698, 2022.   \n[48] L. Page, S. Brin, R. Motwani, and T. Winograd, \u201cThe pagerank citation ranking: Bring order to the web,\u201d in Proc. of the 7th International World Wide Web Conf, 1998.   \n[49] R. Kondor and J.-P. Vert, \u201cDiffusion kernels,\u201d kernel methods in computational biology, pp. 171\u2013192, 2004.   \n[50] M. G\u00f6ksedef and \u00b8S. G\u00fcnd\u00fcz-\u00d6g\u02d8\u00fcd\u00fcc\u00fc, \u201cCombination of web page recommender systems,\u201d Expert Systems with Applications, vol. 37, no. 4, pp. 2911\u20132922, 2010.   \n[51] S. Hwang and Y. Lee, \u201cIdentifying customer priority for new products in target marketing: Using rfm model and textrank,\u201d Innovative Marketing, vol. 17, no. 2, p. 125, 2021.   \n[52] S. Sajadmanesh, A. S. Shamsabadi, A. Bellet, and D. Gatica-Perez, \u201c $\\{{\\mathrm{GAP}}\\}$ : Differentially private graph neural networks with aggregation perturbation,\u201d in 32nd USENIX Security Symposium (USENIX Security 23), pp. 3223\u20133240, 2023.   \n[53] E. Chien, W.-N. Chen, C. Pan, P. Li, A. Ozgur, and O. Milenkovic, \u201cDifferentially private decoupled graph convolutions for multigranular topology protection,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[54] M. Kearns, M. Pai, A. Roth, and J. Ullman, \u201cMechanism design in large games: Incentives and privacy,\u201d in Proceedings of the 5th conference on Innovations in theoretical computer science, pp. 403\u2013410, 2014.   \n[55] G. Jeh and J. Widom, \u201cScaling personalized web search,\u201d in Proceedings of the 12th international conference on World Wide Web, pp. 271\u2013279, 2003.   \n[56] S. Sajadmanesh and D. Gatica-Perez, \u201cProgap: Progressive graph neural networks with differential privacy guarantees,\u201d in Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pp. 596\u2013605, 2024.   \n[57] R. Zafarani and H. Liu, \u201cSocial computing data repository at ASU,\u201d 2009.   \n[58] R. A. Rossi and N. K. Ahmed, \u201cThe network data repository with interactive graph analytics and visualization,\u201d in AAAI, 2015.   \n[59] K. J\u00e4rvelin and J. Kek\u00e4l\u00e4inen, \u201cCumulated gain-based evaluation of ir techniques,\u201d ACM Transactions on Information Systems (TOIS), vol. 20, no. 4, pp. 422\u2013446, 2002.   \n[60] T. Van Erven and P. Harremos, \u201cR\u00e9nyi divergence and kullback-leibler divergence,\u201d IEEE Transactions on Information Theory, vol. 60, no. 7, pp. 3797\u20133820, 2014.   \n[61] M. Gil, F. Alajaji, and T. Linder, \u201cR\u00e9nyi divergence measures for commonly used univariate continuous distributions,\u201d Information Sciences, vol. 249, pp. 124\u2013131, 2013. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Preliminaries 15 ", "page_idx": 13}, {"type": "text", "text": "B Privacy Guarantees for Noisy Graph Diffusion: Analytical Proofs 15 ", "page_idx": 13}, {"type": "text", "text": "B.1 Main Proof 15 ", "page_idx": 13}, {"type": "text", "text": "C Discussion on Personalized Graph Diffusion 17 ", "page_idx": 13}, {"type": "text", "text": "D Additional Experiments 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Datasets and Experimental Environment 18   \nD.2 Running Time . . 18   \nD.3 Sensitivity of Ranking Performance to Variations in Threshold Parameter \u03b7. 18   \nD.4 Ablation Study . . 19 ", "page_idx": 13}, {"type": "text", "text": "E RDP to DP Conversion 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F Proof of Lemmas 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 Proof of Lemma B.1 20   \nF.2 Proof of Lemma B.2 22   \nF.3 Proof of Lemma B.3. 24   \nF.4 Proof of Lemma B.4. 24 ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, definitions and lemmas are introduced to facilitate the presentation of proofs. Recall that the contraction mapping denote a self-mapping $l$ -Lipschitz continuous function with $l\\in[0,1]$ , and the $\\infty$ -Wasserstein distance is defined as $\\begin{array}{r}{W_{\\infty}(\\mu,\\nu)=\\operatorname*{inf}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\cos\\operatorname*{sup}_{(x,y)\\sim\\gamma}\\|x\\!-\\!y\\|}\\end{array}$ , where $\\Gamma(\\mu,\\nu)$ represents the collection of all couplings between $\\mu$ and $\\nu$ . Further, define $\\mathcal{R}_{\\alpha}(\\sigma,\\rho)\\,:=$ $\\mathrm{sup}_{\\mathbf{r}:\\|\\mathbf{r}\\|\\leq\\rho}\\,\\mathcal{D}_{\\alpha}(\\xi\\!+\\!\\mathbf{r}\\|\\xi)$ , where $\\underline{{\\xi}}\\sim\\mathcal{L}(\\mathbf{0},\\sigma)$ , measures the extent to which the Laplacian distribution can absorb shifts of magnitude $\\rho$ . For clarity, $\\mathcal \u1e0a D \u1e0c _{\\alpha}(\\mu_{\\parallel\\nu})$ and $\\mathcal{D}_{\\alpha}(X\\|Y)$ , with $X\\sim\\mu$ and $Y\\sim\\nu$ , are used interchangeably when the context is clear. Besides, we denote $X_{i:j},i,j\\in\\mathbb{Z}_{+},i\\leq j$ as joint couple of $(X_{i},X_{i+1},\\ldots,X_{j})$ . We introduce Shifted R\u00e9nyi Divergence as follows: ", "page_idx": 14}, {"type": "text", "text": "Definition A.1 (Shifted R\u00e9nyi Divergence). Let $\\mu$ and $\\nu$ be distributions defined over a Banach space $(\\mathcal{X},\\|\\cdot\\|)$ . For parameter $z\\geq0$ and $\\alpha\\geq1$ , the $z$ -shifted R\u00e9nyi divergence between $\\mu$ and $\\nu$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal D_{\\alpha}^{(z)}(\\mu\\|\\nu)=\\operatorname*{inf}_{\\mu^{\\prime}:W_{\\infty}(\\mu,\\mu^{\\prime})\\leq z}\\mathcal D_{\\alpha}(\\mu^{\\prime}\\|\\nu)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following two lemmas illustrate how shifted R\u00e9nyi divergence behaves under noise convolution and contraction mapping. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1 (Shift-reduction lemma [26]). Let $\\mu,\\nu,\\varrho$ be probability distributions on $\\mathbb{R}^{n}$ . For any $\\rho\\ge0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}^{(z)}(\\mu*\\varrho\\|\\nu*\\varrho)\\leq\\mathcal{D}_{\\alpha}^{(z+\\rho)}(\\mu\\|\\nu)+\\operatorname*{sup}_{\\mathbf r:\\|\\mathbf r\\|\\leq\\rho}\\mathcal{D}_{\\alpha}(\\varrho+\\mathbf r\\|\\varrho).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.2 (Contraction-reduction lemma [27]). Suppose $\\psi,\\psi^{\\prime}$ are random functions from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{n}$ such that (i) each is a strict $c$ -contraction almost surely, and (ii) there exists a coupling of $(\\psi,\\psi^{\\prime})$ under which $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x}}\\|\\psi(\\mathbf{x})-\\psi^{\\prime}(\\mathbf{x})\\|\\le\\rho}\\end{array}$ almost surely. Then for any probability distributions $\\mu$ and $\\nu$ on $\\mathbb{R}^{n}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\alpha}^{(c z+\\rho)}(\\psi_{\\#}\\mu\\|\\psi_{\\#}^{\\prime}\\nu)\\leq\\mathcal{D}_{\\alpha}^{(z)}(\\mu\\|\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Further, we recall two properties of RDP in our analysis. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3 (Post-processing Property of R\u00e9nyi Divergence). For any R\u00e9nyi parameter $\\alpha\\geq1$ , any random function $f$ , and any probability measures $\\mu,\\nu$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}(f_{\\#}\\mu\\Vert f_{\\#}\\nu)\\leq\\mathcal{D}_{\\alpha}(\\mu\\Vert\\nu)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.4 (Strong composition for RDP). For any R\u00e9nyi parameter $\\alpha\\geq1$ , and any two sequences of random variables $X_{1},\\ldots,X_{k}$ and $Y_{1},\\ldots,Y_{k}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal D}_{\\alpha}\\big(\\mathbb{P}_{X_{1:k}}\\|\\mathbb{P}_{Y_{1:k}}\\big)\\leq\\sum_{i=1}^{k}\\operatorname*{sup}{\\mathcal D}_{\\alpha}\\big(\\mathbb{P}_{X_{i}|X_{i-1}=x_{i-1}}\\|\\mathbb{P}_{Y_{i}|Y_{i-1}=x_{i-1}}\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Privacy Guarantees for Noisy Graph Diffusion: Analytical Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the main proof for the results in Theorem 1 as detailed in Sec. 3. ", "page_idx": 14}, {"type": "text", "text": "B.1 Main Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem $^{\\,I}$ . Given a seed s, a total diffusion step $K$ , a noise scale $\\sigma$ , and degree-based thresholding functions $f,f^{\\prime}$ , we consider two coupled graph diffusions $\\mathcal{D}_{K,\\sigma}(\\mathbf{s}),\\mathcal{D}_{K,\\sigma}^{\\prime}(\\mathbf{s})$ which propogate on two joint edge-level adjacent graphs $\\mathcal{G},\\mathcal{G}^{\\prime}$ respectively with diffusion mapping $\\phi_{k},\\phi_{k}^{\\prime},k\\in$ $[K]$ . Specifically, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{D}_{K,\\sigma}(\\mathbf{s}):}&{\\mathbf{s}_{k}=\\phi_{k}(f(\\mathbf{s}_{k-1}))+\\xi_{k}^{(1)}+\\xi_{k}^{(2)},1\\le k\\le K,}\\\\ {\\mathcal{D}_{K,\\sigma}^{\\prime}(\\mathbf{s}):}&{\\mathbf{s}_{k}^{\\prime}=\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{s}_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}+\\xi_{k}^{\\prime(2)},1\\le k\\le K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\xi_{k}^{(1)},\\xi_{k}^{\\prime(1)},\\xi_{k}^{(2)},\\xi_{k}^{\\prime(2)}$ are distributed according to the law ${\\mathcal{L}}(\\mathbf{0},\\sigma)$ . ", "page_idx": 15}, {"type": "text", "text": "Step 1: Distortion of Single-Step Graph Diffusion In each diffusion step, a distortion over the diffusion mappings $\\phi_{k}\\circ f$ is introduced due to the graph adjacency between the graph diffusions $\\mathcal{D}_{K,\\sigma}(\\mathbf{s})$ and $\\mathcal{D}_{K,\\sigma}^{\\prime}(\\mathbf{s})$ . This distortion, which is crucial for the subsequent analysis, is characterized by the following lemma: ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1 (Distortion of Graph Diffusion). Given two graph diffusions $\\mathcal{D}_{K,\\sigma}(\\mathbf{s}),\\mathcal{D}_{K,\\sigma}^{\\prime}(\\mathbf{s})$ mentioned above. Let $f,f^{\\prime}$ denote corresponding graph-dependent degree-based thresholding operators, i.e., $f(\\mathbf{x})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},-\\eta\\cdot\\mathbf{d}),\\eta\\cdot\\mathbf{d})$ . The diffusion distortion satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\|\\phi_{k}(f(\\mathbf{x}))-\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{x}))\\|_{1}\\leq\\operatorname*{max}(4\\gamma_{\\operatorname*{max}}^{(1)},2\\gamma_{\\operatorname*{max}})\\cdot\\eta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where Lipschitz constant $\\gamma_{\\mathrm{max}}\\mathrm{~=~}\\mathrm{max}_{k}\\left|\\gamma_{1,k}\\right|+\\left|\\gamma_{2,k}\\right|$ , and maximum diffusion coefficient $\\gamma_{\\mathrm{max}}^{(1)}=$ $\\operatorname*{max}_{k}|\\gamma_{1,k}|$ . ", "page_idx": 15}, {"type": "text", "text": "We denote this single-step graph diffusion distortion bound as $\\rho_{\\mathrm{diff}}$ , i.e., $\\rho_{\\mathrm{diff}}=\\operatorname*{max}(4\\gamma_{\\mathrm{max}}^{(1)},2\\gamma_{\\mathrm{max}})\\cdot\\eta$ ", "page_idx": 15}, {"type": "text", "text": "According to the proof, this distortion analysis is tight and conveys several key insights. First, when the graph diffusion process diffuses relatively slow, i.e., $\\gamma_{1,k}<\\gamma_{2,k}$ , the distortion is tight and primarily governed by the Lipschitz constant of the diffusion mapping. In contrast, when the diffusion is relatively fast, the distortion is controlled by the maximum diffusion coefficient $\\gamma_{1,k}$ . In this latter scenario, the bound becomes asymptotically tight when the graph structure satisfies the condition that the nodes connected by the perturbed edge have no common neighbors, and their degrees tend to infinity. We raise a double star graph as an toy example that satisfies the above no common neighbor condition (see Fig. 7). ", "page_idx": 15}, {"type": "image", "img_path": "aon7bwYBiq/tmp/bd6ea2d601eaa3dc45ddb65ee736b8de4b56f86d908e12d15f549acf784e43be.jpg", "img_caption": ["Figure 7: Double Star Graph. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Step 3: Bounding the Privacy Loss via Shift Absorption and PABI. Next, we consider the noisy diffusion process, where Laplacian noise is introduced during graph propagation. Note that, we focus on injecting noise at the initial step of the diffusion process, although the analysis can be directly extended to include noise injection at intermediate steps. For $k\\,\\geq\\,1$ , drawing upon the noise-splitting approach outlined in [27], we mitigate the shifts caused by diffusion and thresholding by integrating these factors into the noise distribution. Consequently, we construct conditional CNI sequences with identical diffusion mappings for both processes. Specifically, we reformulate the diffusion process $\\mathcal{D}_{K,\\Pi}^{\\prime}(\\mathbf{s})$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{s}_{k}^{\\prime}=\\phi_{k}(f(\\mathbf{s}_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}+\\tilde{\\xi}_{k}^{\\prime(2)},\\mathrm{~where~}\\tilde{\\xi}_{k}^{\\prime}\\sim\\mathcal{L}(\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{s}_{k-1}^{\\prime}))-\\phi_{k}(f(\\mathbf{s}_{k-1}^{\\prime})),\\sigma_{k})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we introduce \u03be\u02dc\u2032k(2 as shifted Laplacian noise and noise scale $\\sigma_{k}=\\sigma$ . Therefore, the coupled graph diffusion processes can be summarized as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf s_{k}=\\underbrace{\\phi_{k}(f(\\mathbf s_{k-1}))+\\xi_{k}^{(1)}}_{\\mathrm{Identical~CM}}+\\xi_{k}^{(2)},\\ \\mathbf s_{k}^{\\prime}=\\underbrace{\\phi_{k}(f(\\mathbf s_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}}_{\\mathrm{Identical~CNI}}+\\tilde{\\xi}_{k}^{\\prime(2)},\\ \\forall k\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that our objective is to establish an upper bound for $\\mathcal{D}_{\\alpha}\\big(\\mathbf{s}_{K}\\big|\\big|\\mathbf{s}_{K}^{\\prime}\\big)$ , we claim that for any parameter $\\tau\\in\\{0,1,...,K-1\\}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}\\|\\mathbf{s}_{K}^{\\prime})}&{\\overset{(a)}{\\leq}\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K},\\xi_{\\tau+1:K}^{(2)}\\|\\mathbf{s}_{K}^{\\prime},\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)})}&{\\quad{\\scriptstyle(22)}}\\\\ &{\\overset{(b)}{\\leq}\\mathcal{D}_{\\alpha}(\\xi_{\\tau+1:K}^{(2)}\\|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)})+\\displaystyle\\operatorname*{sup}_{\\zeta_{\\tau+1:K}}\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}=\\zeta_{\\tau+1:K})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\zeta_{\\tau+1:K}$ is a noise realization, $(a)$ is by the post-processing inequality of R\u00e9nyi Divergence (Lemma A.3), and $(b)$ is from the strong composition rule for R\u00e9nyi divergence (Lemma A.4). ", "page_idx": 15}, {"type": "text", "text": "As demonstrated in Eq. 23, the privacy leakage can be upper bounded by the R\u00e9nyi divergence across joint Laplacian distributions with a shift (shift absorption term) and the divergence across conditional CNIs employing identical transformations $\\phi_{k}\\circ f$ (PABI term). The parameter $\\tau$ is introduced to balance the privacy leakage from shifts between noise distributions against the leakage from CNIs starting from different initial conditions. Both terms can be further bounded as detailed in the following lemmas. It is important to emphasize that, for the latter, we leverage $\\infty$ -Wasserstein distance tracking method that get rid of the diameter requirement in original PABI analysis. ", "page_idx": 16}, {"type": "text", "text": "(1) Upper bounding shift absorption term: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.2 (Absorption of Shift in Laplacian Distribution). Given two coupled graph diffusions mentioned above, for any $\\tau\\geq0$ , the shift absorption term can be upper bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}(\\xi_{\\tau+1:K}^{(2)}||\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)})\\leq\\sum_{k=\\tau+1}^{K}g_{\\alpha}(\\sigma_{k},\\rho_{\\mathrm{diff}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where distortion $\\rho_{\\mathrm{diff}}\\;\\;=\\;\\;\\operatorname*{max}(4\\gamma_{\\mathrm{max}}^{(1)},2\\gamma_{\\mathrm{max}})\\;\\cdot\\;\\eta$ , and shifted Laplace function $\\begin{array}{r l}{g_{\\alpha}(\\sigma,\\rho)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}\\rho)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}\\rho))}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "(2) Upper bounding PABI term: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.3 (PABI with Laplacian Distribution). Given two coupled graph diffusions mentioned above, for any $\\tau\\geq0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\zeta_{\\tau+1:K}}\\mathcal{D}_{\\alpha}\\big(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\big)\\leq\\mathcal{D}_{\\alpha}^{(w_{\\tau})}\\big(\\mathbf{s}_{\\tau}\\big)\\big(\\mathbf{s}_{\\tau}^{\\prime}\\big)+g_{\\alpha}\\big(\\sigma_{K},\\gamma_{\\operatorname*{max}}^{K-\\tau}w_{\\tau}\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The lemma demonstrates that the PABI term is controlled by the shifted R\u00e9nyi divergence at step $\\tau$ and a privacy amplification term that decays exponentially at a rate of $\\gamma_{\\mathrm{max}}^{K-\\tau}w_{\\tau}$ . In previous analyses of the PABI for the Gaussian mechanism, [27] utilized the diameter of the bounded parameter set to further constrain the shift $w_{\\tau}$ . However, we have found that this bound does not satisfactorily achieve a favorable privacy-utility trade-off in practice. Subsequently, we develop a $\\infty$ -Wasserstein distance tracking method to more effectively bound this PABI term, offering a more practical approach. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4 (PABI with $\\infty$ -Wasserstein Distance Tracking). Given two coupled graph diffusions mentioned above, for any $\\tau\\geq0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{\\infty}(\\mathbf{s}_{\\tau}\\|\\mathbf{s}_{\\tau}^{\\prime})\\leq\\frac{\\rho_{\\mathrm{diff}}\\cdot\\left(1-\\gamma_{\\operatorname*{max}}^{\\tau}\\right)}{1-\\gamma_{\\operatorname*{max}}}=w_{\\tau},\\ \\mathcal{D}_{\\alpha}^{(w_{\\tau})}(\\mathbf{s}_{\\tau}\\|\\mathbf{s}_{\\tau}^{\\prime})=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where distortion $\\rho_{\\mathrm{diff}}=\\operatorname*{max}(4\\gamma_{\\mathrm{max}}^{(1)},2\\gamma_{\\mathrm{max}})\\cdot\\eta.$ ", "page_idx": 16}, {"type": "text", "text": "By summarizing the above results (Lemma B.2, B.3, and B.4), we conclude the final results: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{D}_{\\boldsymbol{\\alpha}}(\\mathbf{s}_{K}\\|\\mathbf{s}_{K}^{\\prime})\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathcal{D}_{\\boldsymbol{\\alpha}}(\\mathbf{s}_{K}|\\mathbf\\mathbf{\\tilde{s}}_{K}^{\\prime})\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C Discussion on Personalized Graph Diffusion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed in Sec. 3.4, in many personalized scenarios, the definition of personalized edge-level privacy shifts to protect edges that are not incident to the source node. This adjustment offers additional benefits in distortion analysis (Lemma B.1): no distortion occurs in the first step. According ", "page_idx": 16}, {"type": "text", "text": "to the proof of Lemma B.1, given a seed node $s$ with the corresponding initial vector s, and letting $(u,v)$ denote the edge differing on adjacent graphs $\\mathcal{G}$ and $\\mathcal{G}^{\\prime}$ where $s\\notin\\{u,v\\}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|(\\mathbf{A}\\mathbf{D}^{-1}-\\mathbf{A}^{\\prime}\\mathbf{D}^{\\prime-1})\\mathbf{s}\\|_{1}\\leq2\\left({\\frac{[\\mathbf{s}]_{u}}{d_{u}}}+{\\frac{[\\mathbf{s}]_{v}}{d_{v}}}\\right)=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $[\\mathbf{s}]_{u}=[\\mathbf{s}]_{v}=0$ . Thus, when analyzing noisy graph diffusion in a personalized scenario, this distinction results in a better bound for Lemma B.2. Further, Eq. (29) demonstrate that we can relax the thresholding over seed node to seek for better privacy-utility trade-offs. Therefore, for uniform noise scheduling, we obtain the corresponding results in Theorem 4: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon=\\operatorname*{min}_{\\tau\\in\\{0,\\ldots,K-1\\}}\\left[g_{\\alpha}(\\sigma,\\frac{\\rho_{\\mathrm{diff}}\\cdot(1-\\gamma_{\\mathrm{max}}^{\\tau})\\cdot\\gamma_{\\mathrm{max}}^{K-\\tau}}{1-\\gamma_{\\mathrm{max}}})+(K-\\tau)\\cdot g_{\\alpha}(\\sigma,\\rho_{\\mathrm{diff}}\\cdot\\mathbb{1}_{\\tau\\neq0})\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Datasets and Experimental Environment ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As mentioned in Section 4, this paper includes three benchmark datasets: BlogCatalog, TheMarker, and Flickr. Their details are included in Table 1. ", "page_idx": 17}, {"type": "table", "img_path": "aon7bwYBiq/tmp/f5b4c5135fb400e5a3de3843fa3ccaba54137d6c2998320b8d10705a15164c52.jpg", "table_caption": ["Table 1: Benchmark datasets and their statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Experiments were performed on a server with two AMD EPYC 7763 64-Core Processors, 2TB DRAM, six NVIDIA RTX A6000 GPUs (each with 48GB of memory). ", "page_idx": 17}, {"type": "text", "text": "D.2 Running Time ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We report the running time for a single trial of each method across all datasets in Fig. 8. As illustrated in the figure, all single trial experiments of our method and DP-PUSHFLOWCAP are completed within 1 minute. In contrast, the edgeflipping mechanism exhibits significantly longer running times, ranging from 16 minutes to over 12 hours as the size of the graph increases. ", "page_idx": 17}, {"type": "image", "img_path": "aon7bwYBiq/tmp/cc7f15663cc2feeca27dc48dd06e160b7b88b6568425fdd0fa8e7e79bcb2acfe.jpg", "img_caption": ["Figure 8: Running Time for a Single Trial of Experiments with a Privacy Budget of $\\epsilon=0.1$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.3 Sensitivity of Ranking Performance to Variations in Threshold Parameter $\\eta$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this series of experiments, we further investigate the sensitivity of the NDCG ranking performance with respect to the selection of $\\eta$ . Specifically, we enhance the granularity of the $\\eta$ values within the existing range from $1\\times10^{-10}$ to $1\\,\\bar{\\times}\\,10^{-4}$ by selecting 20 equidistant points for each dataset within this interval. The NDCG ranking performance is depicted in Fig. 9. The left column represents the transition curve of our method, the middle column denotes the performance of DP-PUSHFLOWCAP, and the right column illustrates the performance gap between the two methods. For each privacy budget $\\epsilon$ , we highlight the optimal $\\eta$ corresponding to the best performance (yellow for our method, cyan for DP-PUSHFLOWCAP). As demonstrated in the right column, when $\\eta$ varies from $1\\times10^{-10}$ to $\\bar{1}\\times10^{-4}$ , our method consistently outperforms DP-PUSHFLOWCAP up to $\\mathrm{1\\times10^{-6}}$ . This indicates the superior stability of hyperparameter selection for our method in terms of ranking performance. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "aon7bwYBiq/tmp/f433c6b7f9f3155ab77a3dc7cb6913d14dee8668c1fbdbdb52bb1e41c1be061b.jpg", "img_caption": ["Figure 9: Transition Curves of Thresholding Parameter $\\eta$ Relative to Privacy Budget $\\epsilon$ for Three Benchmark Datasets. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.4 Ablation Study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we conduct additional ablation studies to compare the noise distributions, and noise schedules between our Theorem 1 and the Composition Theorem. Following the experimental settings outlined in Sec. 4.2, we perform experiments on the BlogCatalog dataset, utilizing PPR diffusion with a parameter $\\beta=0.8$ and a total of $K=100$ diffusion steps. ", "page_idx": 18}, {"type": "text", "text": "Noise Distributions: Laplace versus Gaussian. We investigate the effectiveness of injecting Laplace noise as compared to Gaussian noise within our analysis framework. Recall that standard PABI analysis has primarily focused on the case of Gaussian noise. We report the ranking performance in Fig. 10 (Left). Within a strong privacy regime where $\\epsilon$ ranges from 0.1 to 1, the performance gap between the two noise types is approximately 0.05. This finding supports the superiority of Laplace noise injection for graph diffusion processes in $\\ell_{1}$ space and is of practical importance. ", "page_idx": 18}, {"type": "text", "text": "Theorem 1 versus Composition Theorem. As we discussed in Section 3.2, one can naively adopt the DP composition theorem [43, 44] to establish the privacy guarantee for the same noisy graph diffusion. However, it is a general approach and does not take the contraction properties of graph diffusion into account, which leads to a worse privacy bound in our case. We examine the calibrated noise scales $\\sigma$ under edge-level DP, converted from RDP. Figure 10 (Right) compares noise scales derived from our Theorem 1 (Red Line) against those from the standard RDP composition theorem (Blue Line). Our method achieves a noise scale that is 10 times smaller than that required by the composition theorem. ", "page_idx": 18}, {"type": "image", "img_path": "aon7bwYBiq/tmp/0e8a9a042df2dd295235adac71a53ae3efc406afe30ae19e1e1f79ff3dbb8e39.jpg", "img_caption": ["Figure 10: Left: Ranking performance with Laplace and Gaussian noise injection. Right: Comparison of noise scales between our method (Theorem 1) and the composition theorem under the DP metric. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "E RDP to DP Conversion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "First, we present the standard results on converting RDP to DP: ", "page_idx": 19}, {"type": "text", "text": "Proposition E.1 (Conversion from RDP to DP [44]). If $\\mathcal{M}$ is an $(\\alpha,\\epsilon_{R D P})$ -RDP mechanism, then it also satisfies $(\\epsilon_{D P},\\delta)$ -differential privacy, where $\\begin{array}{r}{\\epsilon_{D P}=\\epsilon_{R D P}+\\frac{\\log\\frac{1}{\\delta}}{\\alpha-1}}\\end{array}$ for any $\\delta\\in(0,1)$ . ", "page_idx": 19}, {"type": "text", "text": "To determine the DP guarantees of the three mechanisms, we first calculate the RDP for each mechanism. Specifically, we use Theorem 4 for our method, sensitivity analysis in Theorem 4.3 from [20] with the dimensional Laplace mechanism under R\u00e9nyi divergence (Eq. (46)) for DP-PUSHFLOWCAP, and Proposition 5 from [44] for EdgeFlipping. For all three mechanisms, the expression \u03f5RDP+ l\u03b1og 1\u03b4 is convex in $\\alpha$ . By setting $\\begin{array}{r}{\\delta=\\frac{1}{\\#\\mathrm{edges}}}\\end{array}$ #e1dges, we then search for the optimal \u03b1 to minimize this expression and obtain $\\epsilon_{\\mathrm{DP}}$ . ", "page_idx": 19}, {"type": "text", "text": "F Proof of Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Given any $\\mathbf{x}\\,\\in\\,\\mathbb{R}^{n}$ , without loss of generality, we assume that the edge sets of two graphs satisfy $\\mathcal{E}\\,=\\,\\mathcal{E}^{\\prime}\\stackrel{}{\\cup}(1,2)$ , i.e., we remove edge $(1,2)$ from graph $\\mathcal{G}$ , resulting in graph $\\mathcal{G}^{\\prime}$ . Furthermore, let $\\mathcal{N}(i)$ denote the neighbors of node $i$ , and define the following index sets: $A:=\\mathcal{N}(1)\\backslash\\{2\\}$ , $B:=\\mathring{\\mathcal{N}}(2)\\backslash\\{1\\},C:=\\overline{{A}}\\cap B,A^{\\prime}:=A\\backslash C,$ and $B^{\\prime}:=B\\backslash C$ . Additionally, we can rearrange the node indices in $\\mathcal{G}$ such that $A^{\\prime}\\,=\\,\\{3,\\dots,|A^{\\prime}|+2\\}$ , $B^{\\prime}=\\left\\{|A^{\\prime}|+3,\\ldots,|A^{\\prime}|+|B^{\\prime}|+2\\right\\}$ , and $C=\\{|A^{\\prime}|+|B^{\\prime}|+3,\\ldots,|A^{\\prime}|+|B^{\\prime}|+|C|+2\\}$ . We adopt the shorthand notation $\\dot{f_{i}}=[\\dot{f}(\\mathbf{x})]_{i}$ $\\begin{array}{r l}{\\centering}&{\\mathrm{~\\textit~{~\\textcent~}~}[\\boldsymbol{A}\\mathrm{~}|\\mathrm{~\\scriptsize~+~}|\\boldsymbol{D}\\mathrm{~}|\\mathrm{~\\scriptsize~+~}\\boldsymbol{\\mathfrak{o}},\\boldsymbol{\\cdot~}\\boldsymbol{\\cdot},|\\boldsymbol{A}\\mathrm{~}|\\mathrm{~\\scriptsize~+~}|\\boldsymbol{D}\\mathrm{~}|\\mathrm{~\\scriptsize~+~}|\\mathrm{~\\scriptsize~\\textcent~}|\\mathrm{~\\scriptsize~+~}}\\\\ {\\mathrm{~\\small~\\textmd~{~\\textmd~{~and~}~}~}\\Delta f_{i}\\ensuremath{\\mathrm{~\\scriptsize~=~}}[f(\\mathbf{x})]_{i}-[f^{\\prime}(\\mathbf{x})]_{i}.\\mathrm{~\\scriptsize~We~first~consid}}\\end{array}$ e.r eS $d_{1},d_{2}\\;\\geq\\;2$ ,- baansde dd ethfrinees $m_{1}:=$ $\\begin{array}{r}{-\\frac{f_{1}}{d_{1}(d_{1}-1)},m_{2}\\;:=\\;-\\frac{f_{2}}{d_{2}(d_{2}-1)},\\Delta m_{1}\\;:=\\;\\frac{\\Delta f_{1}}{d_{1}-1},\\Delta m_{2}\\;:=\\;\\frac{\\Delta f_{2}}{d_{2}-1}}\\end{array}$   \nfunction $f$ is symmetric, WLOG, we can assume $f$ is non-negative, i.e., $f(\\mathbf{x})=\\operatorname*{min}(\\operatorname*{max}(\\mathbf{x},\\mathbf{0}),\\eta\\,.$ $\\mathbf{d}$ ). We have the following ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\|\\phi_{k}(f(\\mathbf{x}))-\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{x}))\\|_{1}=\\|\\phi_{k}(f(\\mathbf{x}))-\\phi_{k}^{\\prime}(f(\\mathbf{x}))+\\phi_{k}^{\\prime}(f(\\mathbf{x}))-\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{x}))\\|_{1}}\\\\ &{{=}\\|\\gamma_{1,k}(\\mathbf{P}-\\mathbf{P}^{\\prime})f(\\mathbf{x})+(\\gamma_{1,k}\\mathbf{P}^{\\prime}+\\gamma_{2,k}\\mathbf{I})(f(\\mathbf{x})-f^{\\prime}(\\mathbf{x}))\\|_{1}}\\\\ &{{=}\\|\\gamma_{1,k}(\\mathbf{A}\\mathbf{D}^{-1}-\\mathbf{A}^{\\prime}\\mathbf{D}^{\\prime-1})f(\\mathbf{x})+(\\gamma_{1,k}\\mathbf{A}^{\\prime}\\mathbf{D}^{\\prime-1}+\\gamma_{2,k}\\mathbf{I})(f(\\mathbf{x})-f^{\\prime}(\\mathbf{x}))\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r l}&{\\begin{array}{c c c c c}{\\equiv\\left\\|\\gamma_{1,k}\\left[\\begin{array}{c c c c c}{0}&{\\frac{1}{\\omega}}&{0}&{\\cdots}&{0}\\\\ {\\frac{1}{\\omega_{1}}}&{0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\frac{\\cdots}{\\omega_{1}}}&{\\cdots}&{\\cdots}&{\\cdots}\\\\ {0_{n}(\\frac{1}{d_{1}}-\\frac{\\cdots}{d_{1}-1})}&{a_{n}(2\\frac{\\cdots}{d_{2}}-\\frac{1}{d_{2}-1})}&{0}&{\\cdots}&{0}\\end{array}\\right]\\left[\\begin{array}{c}{f_{1}}\\\\ {f_{2}}\\\\ {f_{n}}\\end{array}\\right]}\\\\ &{+(\\gamma_{1,k}\\mathbf{A}^{\\mathrm{T}}\\mathbf{D}^{-1}+\\gamma_{2,k})\\left[\\begin{array}{c}{\\Delta f_{1}}\\\\ {\\vdots}\\\\ {\\Delta f_{n}}\\end{array}\\right]\\right\\|_{1}}&&{(34)}\\\\ &{=\\left\\|\\gamma_{1,k}\\cdot\\left[\\frac{f_{2}}{d_{2}},\\frac{f_{1}}{d_{1}},\\underbrace{\\operatorname*{m}_{1,\\cdots}}_{|\\Delta|\\cdot|\\cdot\\nabla_{n n}|,\\cdots},\\underbrace{m_{1}}_{||\\cdot\\nabla_{|\\mathbb{T}|}\\operatorname*{max}},\\underbrace{m_{2}\\cdots\\cdot m_{2}}_{||\\cdot\\nabla_{|\\mathbb{T}|}\\operatorname*{mex}},\\underbrace{m_{1}+m_{2},0,\\cdots,0}_{||\\cdot\\nabla_{|\\mathbb{T}|}}\\right]}\\\\ &{\\quad+\\,\\gamma_{1,k}\\cdot\\left[0,0,\\underbrace{\\Delta m_{1}\\dots\\sum_{i=1}^{N}m_{1}}_{|\\cdot\\nabla_{|\\mathbb{T}|}\\operatorname*{mex}},\\underbrace{\\Delta m_{2}\\dots\\sum_{i=1}^{N}2m_{2}\\cdot\\Delta m_{1}+\\Delta m_{2},\\dots,\\Delta m_{1}+\\Delta m_{2}}_{|\\cdot\\nabla_{|\\mathbb{T}|}\\operatorname*{mex}}\\right]+\\gamma_{2,k}\\cdot\\left[\\Delta f_{1},\\Delta f_{1},0,0\\right]}\\\\ &{\\mapsto\\gamma_{1,k}\\cdot\\left[\\begin{array}{c}{0}\\\\ {\\sum_{i^{\\prime}}\\Delta f_{1}}\\\\ {\\vdots}\\\\ {\\Delta$ ..., 0]T \u22251 (35)   \n$\\begin{array}{r l}&{\\le|\\gamma_{1,k}|\\cdot(|\\displaystyle\\frac{f_{1}}{d_{1}}|+|\\displaystyle\\frac{f_{2}}{d_{2}}|)+|\\gamma_{1,k}|\\cdot|A^{\\prime}|\\cdot|(\\displaystyle\\frac{1}{d_{1}}-\\displaystyle\\frac{1}{d_{1}-1})f_{1}+\\displaystyle\\frac{\\Delta f_{1}}{d_{1}-1}|+|\\gamma_{1,k}|\\cdot|B^{\\prime}|\\cdot|(\\displaystyle\\frac{1}{d_{2}}-\\displaystyle\\frac{1}{d_{2}-1})f_{2}+}\\\\ &{\\quad+\\,|\\gamma_{1,k}|\\cdot|C|\\cdot|(\\displaystyle\\frac{1}{d_{1}}-\\displaystyle\\frac{1}{d_{1}-1})f_{1}+\\displaystyle\\frac{\\Delta f_{1}}{d_{1}-1}+(\\displaystyle\\frac{1}{d_{2}}-\\displaystyle\\frac{1}{d_{2}-1})f_{2}+\\displaystyle\\frac{\\Delta f_{2}}{d_{2}-1}|+|\\gamma_{2,k}|(|\\Delta f_{1}|+|\\Delta f_{2}|)}\\end{array}$ d 1| \u2206f2 (36)   \n$\\begin{array}{r l}&{\\overset{(a)}{=}\\lvert\\gamma_{1,k}\\rvert\\cdot(\\frac{f_{1}}{d_{1}}+\\frac{f_{2}}{d_{2}})+\\lvert\\gamma_{1,k}\\rvert\\cdot\\lvert A^{\\prime}\\rvert\\cdot\\frac{\\lvert d_{1}\\Delta f_{1}-f_{1}\\rvert}{d_{1}(d_{1}-1)}+\\lvert\\gamma_{1,k}\\rvert\\cdot\\lvert B^{\\prime}\\rvert\\cdot\\frac{\\lvert d_{2}\\Delta f_{2}-f_{2}\\rvert}{d_{2}(d_{2}-1)}}\\\\ &{\\quad+\\lvert\\gamma_{1,k}\\rvert\\cdot\\lvert C\\rvert\\cdot\\lvert\\frac{d_{1}\\Delta f_{1}-f_{1}}{d_{1}(d_{1}-1)}+\\frac{d_{2}\\Delta f_{2}-f_{2}}{d_{2}(d_{2}-1)}\\rvert+\\lvert\\gamma_{2,k}\\rvert\\cdot(\\Delta f_{1}+\\Delta f_{2})}\\\\ &{\\overset{(b)}{\\leq}\\lvert\\gamma_{1,k}\\rvert\\cdot(\\frac{f_{1}}{d_{1}}+\\frac{f_{2}}{d_{2}})+\\lvert\\gamma_{1,k}\\rvert\\cdot\\frac{\\lvert d_{1}\\Delta f_{1}-f_{1}\\rvert}{d_{1}}+\\lvert\\gamma_{1,k}\\rvert\\cdot\\frac{\\lvert d_{2}\\Delta f_{2}-f_{2}\\rvert}{d_{2}}+\\lvert\\gamma_{2,k}\\rvert\\cdot(\\Delta f_{1}+\\Delta f_{2})}\\end{array}$ 7) (38)   \n$\\begin{array}{r l}&{\\overset{(c)}{=}\\lvert\\gamma_{1,k}\\rvert\\cdot(\\frac{f_{1}}{d_{1}}+\\frac{f_{2}}{d_{2}})+\\lvert\\gamma_{1,k}\\rvert\\cdot\\frac{f_{1}-d_{1}\\Delta f_{1}}{d_{1}}+\\lvert\\gamma_{1,k}\\rvert\\cdot\\frac{f_{2}-d_{2}\\Delta f_{2}}{d_{2}}+\\lvert\\gamma_{2,k}\\rvert\\cdot(\\Delta f_{1}+\\Delta f_{2})}\\\\ &{=\\displaystyle\\sum_{j=1}^{2}\\lvert\\gamma_{1,k}\\rvert(f_{j}^{\\prime}-\\frac{(d_{j}-2)f_{j}}{d_{j}})+\\lvert\\gamma_{2,k}\\rvert\\cdot(f_{j}-f_{j}^{\\prime})}\\\\ &{\\overset{(d)}{\\leq}\\operatorname*{max}(4\\lvert\\gamma_{1,k}\\rvert,2(\\lvert\\gamma_{1,k}\\rvert+\\lvert\\gamma_{2,k}\\rvert))\\cdot\\eta\\leq\\operatorname*{max}(4\\gamma_{\\operatorname*{max}}^{(1)},2\\gamma_{\\operatorname*{max}})\\cdot\\eta}\\end{array}$ (39) (40) (41) ", "page_idx": 20}, {"type": "text", "text": "where (a) follows from the non-negativity of the function $f$ , and (b) is derived from $|A^{\\prime}\\cap C|=d_{1}\\!-\\!1$ and $|B^{\\prime}\\cap C|=d_{2}-1$ . Equality is obtained when $C=\\varnothing$ , i.e., when there are no common neighbors between nodes 1 and 2. $(c)$ is obtained from Result 1 and $(d)$ originates from Result 2. ", "page_idx": 20}, {"type": "text", "text": "Result 1. Consistent with the above notations, we have $f_{j}-d_{j}\\Delta f_{j}\\geq0$ for $j\\in\\{1,2\\}$ . ", "page_idx": 20}, {"type": "text", "text": "Results 2. Consistent with the above notations, we have $\\begin{array}{r}{|\\gamma_{1,k}|(f_{j}^{\\prime}-\\frac{(d_{j}-2)f_{j}}{d_{j}})+|\\gamma_{2,k}|(f_{j}-f_{j}^{\\prime})\\leq}\\end{array}$ $\\operatorname*{max}(2|\\gamma_{1,k}|,|\\gamma_{1,k}|+|\\gamma_{2,k}|)\\eta$ for $j\\in\\{1,2\\}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Result 1. From the above definitions, we have $f_{j}\\,=\\,[f(\\mathbf{x})]_{j}\\,=\\,\\mathrm{min}(\\mathrm{max}(x_{j},0),\\eta\\cdot d_{j})$ and $f_{j}-d_{j}\\Delta f_{j}=d_{j}f_{j}^{\\prime}-(d_{j}-1)f_{j}$ where $x_{j}$ is the $j$ -th entry of $\\mathbf{x}$ . Now, consider the following cases: ", "page_idx": 20}, {"type": "text", "text": "\u2022 When $x_{j}\\leq0$ , we have $f_{j}-d_{j}\\Delta f_{j}=0$ .   \n\u2022 When $x_{j}\\in(0,(d_{j}-1)\\eta]$ , it follows that $f_{j}-d_{j}\\Delta f_{j}=d_{j}\\eta x_{j}-(d_{j}-1)\\eta x_{j}=\\eta x_{j}\\geq0.$ \u2022 When $x_{j}\\in((d_{j}-1)\\eta,d_{j}\\eta]$ , we obtain $f_{j}-d_{j}\\Delta f_{j}=(d_{j}-1)(d_{j}-x_{j})\\eta^{2}\\geq0.$ . \u2022 When $x_{j}>d_{j}$ , $f_{j}-d_{j}\\Delta f_{j}=0$ .   \nIn conclusion, based on the above cases, the result is proven.   \nProof of Result 2. We consider different cases: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$\\begin{array}{r}{x_{j}\\le0,|\\gamma_{1,k}|(f_{j}^{\\prime}-\\frac{(d_{j}-2)f_{j}}{d_{j}})+|\\gamma_{2,k}|(f_{j}-f_{j}^{\\prime})=0.}\\end{array}$ ", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 When $x_{j}\\in(0,(d_{j}-1)\\eta]$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\gamma_{1,k}|(f_{j}^{\\prime}-\\frac{(d_{j}-2)f_{j}}{d_{j}})+|\\gamma_{2,k}|(f_{j}-f_{j}^{\\prime})=\\frac{2|\\gamma_{1,k}|\\eta[\\mathbf{x}]_{j}}{d_{j}}\\leq\\frac{2|\\gamma_{1,k}|(d_{j}-1)\\eta}{d_{j}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 When $x_{j}\\in((d_{j}-1)\\eta,d_{j}\\eta]$ , we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~|\\gamma_{1,k}|(f_{j}^{\\prime}-\\frac{(d_{j}-2)f_{j}}{d_{j}})+|\\gamma_{2,k}|(f_{j}-f_{j}^{\\prime})}\\\\ &{=\\rvert\\gamma_{1,k}\\lvert\\eta\\left(\\frac{(d_{j}-1)d_{j}-(d_{j}-2)x_{j}}{d_{j}}\\right)+\\lvert\\gamma_{2,k}\\rvert\\eta(x_{j}-(d_{j}-1))}\\\\ &{\\leq\\operatorname*{max}(\\frac{2\\lvert\\gamma_{1,k}\\rvert(d_{j}-1)\\eta}{d_{j}},(\\lvert\\gamma_{1,k}\\rvert+\\lvert\\gamma_{2,k}\\rvert)\\eta)\\leq\\operatorname*{max}(2\\lvert\\gamma_{1,k}\\rvert,\\lvert\\gamma_{1,k}\\rvert+\\lvert\\gamma_{2,k}\\rvert)\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that, based on the derivations in $(b)$ and $(d)$ , we conclude that the bound is asymptotically tight, with the worst-case scenario occurring when the nodes connected by the perturbed edge have no common neighbors, and their degrees increase to infinity. ", "page_idx": 21}, {"type": "text", "text": "Additionally, when $d_{1}=1$ or $d_{2}=1$ , it can be directly shown that the distortion is upper bounded by $\\operatorname*{max}(\\gamma_{\\operatorname*{max}}^{(1)},2\\gamma_{\\operatorname*{max}})\\cdot\\eta$ . ", "page_idx": 21}, {"type": "text", "text": "F.2 Proof of Lemma B.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To prove this lemma, we introduce result on the R\u00e9nyi divergence for high-dimensional Laplacian distributions with shift. The details of this result and its proof are presented following the lemma. ", "page_idx": 21}, {"type": "text", "text": "Result: Given a shift $\\mathbf{h}\\in\\mathbb{R}^{|\\mathcal{V}|}$ , for two Laplacian distributions ${\\mathcal{L}}(\\mathbf{0},\\sigma)$ and ${\\mathcal{L}}(\\mathbf{h},\\sigma)$ , if $\\|\\mathbf{h}\\|_{1}\\leq\\rho$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma)\\|\\mathcal{L}(\\mathbf{h},\\sigma))\\leq\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}\\rho)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}\\rho))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma Proof. With the above result, we can upper bound the R\u00e9nyi divergence over joint noise distributions. For $\\tau\\geq0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}(\\xi_{\\tau+1:K}^{(2)}\\|\\tilde{\\xi}_{\\tau+1:K}^{(2)})\\overset{(a)}{\\leq}\\sum_{k=\\tau+1}^{K}\\operatorname*{sup}_{\\zeta_{\\tau+1:k-1}}\\mathcal{D}_{\\alpha}(\\xi_{k}^{(2)}|\\xi_{\\tau+1:k-1}^{(2)}=\\zeta_{\\tau+1:k-1}\\|\\tilde{\\xi}_{k}^{\\prime(2)}|\\tilde{\\xi}_{\\tau+1:k-1}^{\\prime(2)}=\\zeta_{\\tau+1:k-1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathop{\\le}^{(b)}\\sum_{k=\\tau+1}^{K}\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma_{k})\\|\\mathcal{L}({\\mathbf h}_{k},\\sigma_{k}))\\overset{(c)}{\\le}\\sum_{k=\\tau+1}^{K}g_{\\alpha}(\\sigma_{k},\\rho_{\\mathrm{diff}})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(a)$ arises from strong composition rule for R\u00e9nyi divergence (Lemma A.4), $(b)$ is from the definition of $\\xi_{k}^{(2)},\\tilde{\\xi}_{k}^{(2)}$ with shift $\\mathbf{h}_{k}=(\\phi_{k}\\circ f)(\\mathbf{x})-(\\phi_{k}^{\\prime}\\circ f^{\\prime})(\\mathbf{x})$ , and $(c)$ is derived by combining the results that the $\\ell_{1}$ norm of the shift is upper bounded by $\\|\\mathbf{h}_{k}\\|_{1}\\,\\leq\\,\\rho_{\\mathrm{diff}}\\,=\\,\\operatorname*{max}(4\\gamma_{\\mathrm{max}}^{(1)},2\\gamma_{\\mathrm{max}})$ (Lemma B.1) and the above Laplace bound under R\u00e9nyi divergence (Eq. (46)) with $g_{\\alpha}(\\sigma,\\rho)\\;=\\;$ $\\begin{array}{r}{\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}-\\exp(\\frac{\\alpha-1}{\\sigma}\\rho)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}\\rho_{\\mathrm{diff}}))}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Summarizing the above, we prove the lemma. ", "page_idx": 21}, {"type": "text", "text": "Proof of Result. Now consider Laplacian distribution, for $\\mathbf{h}\\in\\mathbb{R}^{|\\mathcal{V}|}$ , let $h_{i}$ denote the $i$ -th entry of the vector, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal D_{\\alpha}(\\mathcal L(\\mathbf0,\\sigma)\\|\\mathcal L(\\mathbf h,\\sigma))\\overset{(a)}{=}\\sum_{i=1}^{|\\mathcal V|}\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}|h_{i}|)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}|h_{i}|))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(a)$ is from the additivity of R\u00e9nyi divergence [60] and the R\u00e9nyi divergence over onedimensional Laplacian noise [61]. By considering the worst case $\\mathbf{h}$ , we can reformulate the problem as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{h}}{\\mathrm{maximize}}\\sum_{i=1}^{|\\mathcal{V}|}\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}|h_{i}|)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}|h_{i}|))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the above optimization problem, let $\\begin{array}{r c l c r c l}{{\\lambda_{1}}}&{{=}}&{{\\frac{\\alpha}{2\\alpha-1},\\lambda_{2}}}&{{=}}&{{\\frac{\\alpha-1}{\\sigma},\\lambda_{3}}}&{{=}}&{{\\frac{\\alpha-1}{2\\alpha-1}}}\\end{array}$ and $\\lambda_{4}~=$ $\\frac{\\alpha}{\\sigma}$ . Define $\\begin{array}{r}{f(h_{i})\\ =\\ \\frac{1}{\\alpha-1}\\ln(\\lambda_{1}\\exp(\\lambda_{2}\\cdot h_{i})\\,+\\,\\lambda_{3}\\exp(-\\lambda_{4}\\,\\cdot\\,h_{i}).}\\end{array}$ Consider the gradient term $\\nabla\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma)\\vert\\vert\\mathcal{L}(\\mathbf{h},\\sigma))$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma)\\|\\mathcal{L}(\\mathbf{h},\\sigma))}{\\partial h_{i}}=\\frac{\\partial f(h_{i})}{\\partial h_{i}}=\\frac{\\lambda_{1}\\lambda_{2}\\exp(\\lambda_{2}h_{i})-\\lambda_{3}\\lambda_{4}\\exp(-\\lambda_{4}h_{i})}{\\lambda_{1}\\exp(\\lambda_{2}h_{i})+\\lambda_{3}\\exp(-\\lambda_{4}h_{i})}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Further for Hessian matrix $\\nabla^{2}\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma)\\|\\mathcal{L}(\\mathbf{h},\\sigma))$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial^{2}\\mathcal{D}_{\\alpha}\\left(\\mathcal{L}(\\mathbf{0},\\sigma)\\right\\Vert\\mathcal{L}\\left(\\mathbf{h},\\sigma\\right)\\right)}{\\partial h_{i}^{2}}=\\frac{\\lambda_{1}\\lambda_{3}\\left(\\lambda_{2}+\\lambda_{4}\\right)^{2}\\exp\\left(\\lambda_{2}h_{i}\\right)\\exp\\left(-\\lambda_{4}h_{i}\\right)}{\\lambda_{1}\\exp\\left(\\lambda_{2}h_{i}\\right)+\\lambda_{3}\\exp\\left(-\\lambda_{4}h_{i}\\right)}>0,}\\\\ &{\\frac{\\partial^{2}\\mathcal{D}_{\\alpha}\\left(\\mathcal{L}(\\mathbf{0},\\sigma)\\right\\Vert\\mathcal{L}\\left(\\mathbf{h},\\sigma\\right)\\right)}{\\partial h_{i}h_{j}}=0,i\\neq j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the eigenvalue criterion for positive definiteness, $\\nabla^{2}D_{\\alpha}({\\mathcal L}(\\mathbf{0},\\sigma)\\|{\\mathcal L}(\\mathbf{h},\\sigma))~~\\succ~~0.$ , i.e. $\\mathcal{D}_{\\alpha}(\\mathcal{L}(\\mathbf{0},\\sigma)\\vert\\vert\\mathcal{L}(\\mathbf{h},\\sigma))$ is a convex function. As the feasible set is convex, maximum is obtained on the boundary of feasible set. Thus, the problem can be further formulated as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\mathbf{h}}\\sum_{i=1}^{|V|}\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}|h_{i}|)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma}|h_{i}|))\\mathrm{~s.t.~}\\|\\mathbf{h}\\|_{1}=\\rho_{\\mathrm{~o~p~}}(\\mathbf{h},\\mathbf{\\tilde{h}})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we use the adjustment method to solve the above objective methods. First, we define $\\begin{array}{r}{L(h_{1},h_{2},\\ldots,h_{|\\mathcal{V}|})=\\sum_{i=1}^{|\\mathcal{V}|}f(h_{i})}\\end{array}$ and we fix $h_{3},\\ldots,h_{|\\gamma|}$ . We aim to optimize: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{h_{1},h_{2}}L(h_{1},h_{2},h_{3},...,h_{|\\mathcal{V}|})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with respect to $h_{1}$ and $h_{2}$ , considering the constraints $\\|\\mathbf{h}\\|_{1}=\\rho$ . This is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathop{\\operatorname{maximize}}_{h_{1},h_{2}}\\sum_{i=1}^{2}L(h_{1},h_{2},\\ldots,h_{|\\mathcal{V}|}),{\\mathrm{~s.t.~}}h_{1}+h_{2}=\\rho-\\sum_{i=3}^{|\\mathcal{V}|}h_{i}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "uDneifvianrei $\\begin{array}{r}{c_{t}=\\rho-\\sum_{i=t}^{\\nu}h_{i}}\\end{array}$ c. ulSaitnicneg $c_{3}$ ei sd efriixveadt,i vaend $h_{2}\\,=\\,c_{3}\\,-\\,h_{1}$ , the objective function become a ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial h_{1}}=\\frac{(\\lambda_{1}\\lambda_{3}\\lambda_{4}+\\lambda_{1}\\lambda_{2}\\lambda_{3})(\\exp(\\lambda_{2}h_{1}+\\lambda_{4}(h_{1}-c_{3}))-\\exp(\\lambda_{2}(c_{3}-h_{1})-\\lambda_{4}h_{1}))}{(\\lambda_{1}\\exp(\\lambda_{2}h_{1}+\\lambda_{3}\\exp(-\\lambda_{4}h_{1})))(\\lambda_{1}\\exp(\\lambda_{2}(c_{3}-h_{1}))+\\lambda_{3}\\exp(-\\lambda_{4}(c_{3}-h_{1})))}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The derivative $\\frac{\\partial L}{\\partial h_{1}}$ 1 reaches 0 when h1 = c23 . Furthermore, \u2202\u2202hL1 $\\begin{array}{r}{\\frac{\\partial L}{\\partial h_{1}}<0}\\end{array}$ for $\\begin{array}{r}{h_{1}<\\frac{c_{3}}{2}}\\end{array}$ , and $\\begin{array}{r}{\\frac{\\partial L}{\\partial h_{1}}>0}\\end{array}$ for $\\begin{array}{r}{h_{1}>\\frac{c_{3}}{2}}\\end{array}$ . Thus, the maximum value of the function is attained at the endpoints. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{h_{1},h_{2}}L(h_{1},h_{2},h_{3},\\ldots,h_{|\\mathcal{V}|})\\leq\\operatorname*{max}\\left[L(c_{3},0,h_{3},\\ldots,h_{|\\mathcal{V}|}),L(0,c_{3},h_{3},\\ldots,h_{|\\mathcal{V}|})\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As the function is symmetric, two endpoints attain the same value, i.e. $L(c_{3},0,h_{3},...\\,,h_{|\\mathcal{V}|})\\,=$ $L(0,c_{3},h_{3},\\ldots,h_{|\\mathcal{V}|})$ . Now, we aim to maximize the objective function by each time adjustment two variables and fixed the rest variables unchanged: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{h_{1},h_{2},\\ldots,h_{|\\mathcal{V}|}}{\\mathrm{maximize~}}L(h_{1},h_{2},h_{3},\\ldots,h_{|\\mathcal{V}|})=\\underset{h_{1},h_{2},h_{3},\\ldots,h_{|\\mathcal{V}|}}{\\mathrm{maximize~}}L(h_{1},h_{2},h_{3},\\ldots,h_{|\\mathcal{V}|})}\\\\ &{\\le\\underset{h_{3},\\ldots,h_{|\\mathcal{V}|}}{\\mathrm{maximize~}}\\ \\underset{h_{1}+h_{2}=\\rho-c_{3}}{\\mathrm{maximize~}}L(h_{1},h_{2},h_{3},\\ldots,h_{|\\mathcal{V}|})}\\\\ &{\\overset{(a)}{\\le}\\underset{h_{3},h_{4},\\ldots,h_{|\\mathcal{V}|}}{\\mathrm{maximize~}}L(\\rho-c_{3},0,h_{3},\\ldots,h_{|\\mathcal{V}|})\\le\\underset{h_{4},\\ldots,h_{|\\mathcal{V}|}}{\\mathrm{maximize~}}L(\\rho-c_{4},0,0,\\ldots,h_{|\\mathcal{V}|})}\\\\ &{\\le\\cdots\\le L(\\rho,0,0,\\ldots,0)=\\frac{1}{\\alpha-1}\\ln(\\frac{\\alpha}{2\\alpha-1}\\exp(\\frac{\\alpha-1}{\\sigma}\\rho)+\\frac{\\alpha-1}{2\\alpha-1}\\exp(-\\frac{\\alpha}{\\sigma})\\rho;}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(a)$ is from Eq. (59), and for each inequality, we adjust the values of $h_{1}$ and $h_{i}$ to maximized the objective function while keep the rest variables fixed. From the above reasoning, we maximized the R\u00e9nyi divergence over two Laplacians with shift. ", "page_idx": 22}, {"type": "text", "text": "F.3 Proof of Lemma B.3. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Tmoa ipnrloy vlee vtehrea guep pthere  bdoeufinnidt ioofn $\\begin{array}{r}{\\operatorname*{sup}_{\\zeta_{\\tau+1:K}}\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}\\ =\\ \\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}\\ =\\ \\zeta_{\\tau+1:K})\\,}\\end{array}$ ,n dweer noise convolution and contraction mapping (Lemma A.1 and Lemma A.2). Specifically, recall that $\\begin{array}{r}{\\mathcal{R}_{\\alpha}(\\boldsymbol{\\sigma},\\boldsymbol{\\rho})=\\operatorname*{sup}_{\\mathbf{r}:\\|\\mathbf{r}\\|\\leq\\rho}\\mathcal{D}_{\\alpha}(\\xi\\!+\\!\\mathbf{r}\\|\\xi)}\\end{array}$ where $\\underline{{\\xi}}\\sim\\mathcal{L}(\\mathbf{0},\\sigma)$ , define $\\varphi_{k}=\\phi_{k}\\circ f$ , for any $\\zeta_{\\tau+1:K}$ , define $w_{K}=0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K})}\\\\ &{}&{=\\mathcal{D}_{\\alpha}^{(w_{K})}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K})}\\\\ &{}&{\\overset{(a)}{\\leq}\\mathcal{D}_{\\alpha}^{(w_{K}+\\mathbf{a}_{K})}(\\varphi_{K}(\\mathbf{s}_{K-1})|\\xi_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1}\\|\\varphi_{K}(\\mathbf{s}_{K}^{\\prime})|\\tilde{\\xi}_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1})+\\mathcal{R}_{\\alpha}(\\sigma_{K},\\mathbf{a}_{K})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\mathcal{V}_{\\alpha^{-1}}^{\\alpha^{-1}\\times\\operatorname{sup}_{\\tau}}\\langle\\varphi_{K}(\\mathbf{s}_{K-1})|\\xi_{\\tau+1:K-1}^{\\tau}=\\zeta_{\\tau+1:K-1}\\rVert\\varphi_{K}(\\mathbf{s}_{K})|\\xi_{\\tau+1:K-1}=\\zeta_{\\tau+1:K-1}+\\mathcal{N}_{\\alpha}(\\sigma_{K},\\mathbf{a}_{K})}\\\\ &{\\overset{(b)}{\\le}\\mathcal{D}_{\\alpha}^{(\\frac{K+\\alpha+K}{5\\operatorname*{sup}_{\\tau}})}\\big(\\mathcal{S}_{K-1}\\big|\\xi_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1}\\big\\rVert\\mathfrak{s}_{K}^{\\prime}\\big|\\tilde{\\xi}_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1}\\big)+\\mathcal{R}_{\\alpha}(\\sigma_{K},\\mathbf{a}_{K})\\quad\\quad\\mathrm{(67)}}\\\\ &{\\overset{\\mathrm{we.~=~\\sigma_{k}^{K-1}}\\times\\frac{\\mu_{K+\\alpha,K}^{(1)}}{\\mu_{\\mathrm{gap}}}}{=}\\mathcal{D}_{\\alpha}^{(\\mathrm{w}_{K-1})}\\big(\\mathtt{s}_{K-1}\\big|\\xi_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1}\\big\\rVert\\mathfrak{s}_{K}^{\\prime}\\big|\\tilde{\\xi}_{\\tau+1:K-1}^{(2)}=\\zeta_{\\tau+1:K-1}\\big)+\\mathcal{R}_{\\alpha}(\\sigma_{K},\\mathbf{a}_{K})}\\\\ &{\\le\\cdots\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\Theta)}\\\\ &{\\le\\mathcal{D}_{\\alpha}^{(\\nu_{+1})}\\big(\\mathtt{s}_{\\tau+1}\\big|\\xi_{\\tau+1}^{(2)}=\\zeta_{\\tau+1}\\big\\|\\mathfrak{s}_{\\tau+1}^{\\prime}\\big\\|\\tilde{\\xi}_{\\tau+1}^{(2)}=\\zeta_{\\tau+1}\\big)+\\displaystyle\\sum_{k=\\tau+2}^{K}\\mathcal{R}_{\\alpha}(\\sigma_{k},\\mathbf{a}_{k})}\\\\ &{\\le\\mathcal{D}_{\\alpha} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ is from Lemma A.1, and $(b)$ is derived from Lemma A.2 as $\\varphi_{k}$ is $\\gamma_{\\mathrm{max}}$ -contractive (composition of two contractions). Further, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{w_{\\tau+1}=\\gamma_{\\operatorname*{max}}w_{\\tau}-\\mathbf{a}_{\\tau+1}}\\\\ {{w_{\\tau+2}}=\\gamma_{\\operatorname*{max}}w_{\\tau+1}-\\mathbf{a}_{\\tau+2}=\\gamma_{\\operatorname*{max}}^{2}w_{\\tau}-\\gamma_{\\operatorname*{max}}\\mathbf{a}_{\\tau+1}-\\mathbf{a}_{\\tau+2}}\\\\ {\\ldots=\\cdot\\cdot\\cdot}\\\\ {{w_{K}}=\\gamma_{\\operatorname*{max}}^{K-\\tau}w_{\\tau}-\\displaystyle\\sum_{k=0}^{K-\\tau-1}\\gamma_{\\operatorname*{max}}^{k}\\mathbf{a}_{K-k}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By setting $\\mathbf{a}_{\\tau+1}=0,...,\\mathbf{a}_{K-1}=0$ , we have $\\mathbf{a}_{K}=\\gamma_{\\operatorname*{max}}^{K-\\tau}w_{\\tau}$ . Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\alpha}(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}=\\zeta_{\\tau+1:K})\\leq\\mathcal{D}_{\\alpha}^{(w_{\\tau})}(\\mathbf{s}_{\\tau}\\|\\mathbf{s}_{\\tau}^{\\prime})+\\mathcal{R}_{\\alpha}(\\sigma_{K},\\gamma_{\\operatorname*{max}}^{K-\\tau}w_{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From the result in the proof of Lemma B.2, $\\mathcal{R}_{\\alpha}(\\sigma_{K},\\gamma_{\\mathrm{max}}^{K-\\tau}w_{\\tau})=g_{\\alpha}(\\sigma_{K},\\gamma_{\\mathrm{max}}^{K-\\tau}w_{\\tau})$ . Summarizing the above, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathscr{D}_{\\alpha}\\big(\\mathbf{s}_{K}|\\xi_{\\tau+1:K}^{(2)}=\\zeta_{\\tau+1:K}\\|\\mathbf{s}_{K}^{\\prime}|\\tilde{\\xi}_{\\tau+1:K}^{\\prime(2)}=\\zeta_{\\tau+1:K}\\big)\\leq\\mathscr{D}_{\\alpha}^{(w_{\\tau})}(\\mathbf{s}_{\\tau}\\|\\mathbf{s}_{\\tau}^{\\prime})+g_{\\alpha}(\\sigma_{K},\\gamma_{\\operatorname*{max}}^{K-\\tau}w_{\\tau})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "F.4 Proof of Lemma B.4. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To prove this lemma, we consider tracking the $\\infty$ -Wasserstein distance of the coupled iterates. Given $\\mathcal{D}_{K,\\Pi}$ and $\\mathcal{D}_{K,\\Pi}^{\\prime}$ , for $\\tau\\geq1$ , recall that for any $k(k\\geq1)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{K,\\Pi}(\\mathbf{s}):\\mathbf{s}_{k}=\\phi_{k}(f(\\mathbf{s}_{k-1}))+\\xi_{k}^{(1)}+\\xi_{k}^{(2)},\\ \\mathcal{D}_{K,\\Pi}^{\\prime}(\\mathbf{s}):\\mathbf{s}_{k}^{\\prime}=\\phi_{k}^{\\prime}(f^{\\prime}(\\mathbf{s}_{k-1}^{\\prime}))+\\xi_{k}^{\\prime(1)}+\\tilde{\\xi}_{k}^{\\prime(2)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any step $k(k\\,\\geq\\,1)$ , let $\\mu_{k}$ and $\\nu_{k}$ denote the distribution of $\\mathbf{s}_{k}$ and $\\mathbf{s}_{k}^{\\prime}$ respectively. Further, define ${\\tilde{\\mu}}_{k}$ and $\\tilde{\\nu}_{k}$ denote the distribution of $\\mathbf{S}_{k}=(\\xi_{1:k},\\tilde{\\xi}_{1:k})$ and $\\mathbf{S}_{k}^{\\prime}=(\\xi_{1:k}^{\\prime},\\tilde{\\xi}_{1:k}^{\\prime})$ , respectively. For step 1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{\\infty}(\\mu_{1},\\nu_{1})=\\operatorname*{inf}_{\\pi_{1}\\in\\Gamma(\\mu_{1},\\nu_{1})}\\displaystyle\\operatorname*{ess}_{(\\mathbf{s}_{1},\\mathbf{s}_{1}^{\\prime})\\sim\\pi_{1}}\\|\\mathbf{s}_{1}-\\mathbf{s}_{1}^{\\prime}\\|_{1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\underset{\\Tilde{\\pi}_{1}\\in\\Gamma(\\Tilde{\\mu}_{1},\\Tilde{\\nu}_{1})}{\\operatorname*{inf}}\\,\\underset{(\\mathbf{S}_{1},\\mathbf{S}_{1}^{\\prime})\\sim\\Tilde{\\pi}_{1}}{\\mathrm{ess\\,sup}}\\,\\Vert\\phi_{1}(f(\\mathbf{s}_{0}))+\\xi_{1}^{(1)}+\\xi_{1}^{(2)}-\\phi_{1}^{\\prime}(f^{\\prime}(\\mathbf{s}_{0}))-\\xi_{1}^{\\prime(1)}-\\xi_{1}^{\\prime(2)}\\Vert_{1}}\\\\ &{\\overset{(a)}{\\leq}\\underset{(\\mathbf{S}_{1},\\mathbf{S}_{1}^{\\prime})\\sim\\Tilde{\\pi}_{1}^{*}}{\\mathrm{ess\\,sup}}\\,\\Vert\\phi_{1}(f(\\mathbf{s}_{0}))-\\phi_{1}^{\\prime}(f^{\\prime}(\\mathbf{s}_{0}))\\Vert_{1}\\leq\\rho_{\\mathrm{diff}}}\\\\ &{=(\\mathbf{S}_{1},\\mathbf{S}_{1}^{\\prime})\\sim\\Tilde{\\pi}_{1}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ is from selecting a coupling $\\tilde{\\pi}^{*}$ such that r.v.s $\\mathbf{S}_{1}$ and ${\\bf S}_{1}^{\\prime}$ are identical. ", "page_idx": 24}, {"type": "text", "text": "Next, for any $\\tau$ , following the above procedure, define $\\tilde{\\pi}_{k}^{*}\\in\\Gamma(\\mu_{k},\\nu_{k})$ such that $\\mathbf{S}_{k},\\mathbf{S}_{k}^{\\prime}$ are identical, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad W_{\\infty}(\\mu_{\\tau},\\nu_{\\tau})=\\underset{\\pi_{\\tau}\\in\\Gamma(\\mu_{\\tau},\\nu_{\\tau})}{\\operatorname*{inf}}\\underset{(\\mathbf{s}_{\\tau},\\mathbf{s}_{\\tau}^{\\prime})\\sim\\pi_{\\tau}}{\\mathrm{sss\\up}}\\,\\Vert\\mathbf{s}_{\\tau}-\\mathbf{s}_{\\tau}^{\\prime}\\Vert_{1}}\\\\ &{\\leq\\underset{\\Tilde{\\pi}_{\\tau}\\in\\Gamma(\\Tilde{\\mu}_{\\tau},\\Tilde{\\nu}_{\\tau})}{\\operatorname*{inf}}\\underset{(\\mathbf{s}_{\\tau},\\mathbf{s}_{\\tau}^{\\prime})\\sim\\Tilde{\\pi}_{\\tau}}{\\mathrm{ess\\up}}\\,\\Vert\\phi_{\\tau}(f_{\\tau}(\\mathbf{s}_{\\tau-1}))+\\xi_{\\tau}^{(1)}+\\xi_{\\tau}^{(2)}-\\phi_{\\tau}^{\\prime}(f_{\\tau}^{\\prime}(\\mathbf{s}_{\\tau-1}^{\\prime}))-\\xi_{\\tau}^{\\prime(1)}-\\xi_{\\tau}^{\\prime(2)}\\Vert\\mathbf{1}}\\\\ &{\\leq\\underset{(\\mathbf{s}_{\\tau},\\mathbf{s}_{\\tau}^{\\prime})\\sim\\Tilde{\\pi}_{\\tau}}{\\mathrm{ess\\up}}\\Vert\\phi_{\\tau}(f_{\\tau}(\\mathbf{s}_{\\tau-1}))+\\xi_{\\tau}^{(1)}+\\xi_{\\tau}^{(2)}-\\phi_{\\tau}^{\\prime}(f_{\\tau}^{\\prime}(\\mathbf{s}_{\\tau-1}^{\\prime}))-\\xi_{\\tau}^{\\prime(1)}-\\xi_{\\tau}^{\\prime(2)}\\Vert\\mathbf{1}}\\\\ &{=\\underset{(\\mathbf{s}_{\\tau}-1,\\mathbf{s}_{\\tau-1}^{\\prime})\\sim\\Tilde{\\pi}_{\\tau}^{*}}{\\mathrm{ess\\up}}\\Vert\\phi_{\\tau}(f_{\\tau}(\\mathbf{s}_{\\tau-1}))-\\phi_{\\tau}^{\\prime}(f_{\\tau}^{\\prime}(\\mathbf{s}_{\\tau-1}^{\\prime}))\\Vert\\Vert}\\\\ &{=\\underset{(\\mathbf{s}_{\\tau}-1,\\mathbf{s}_{\\tau-1}^{\\prime})\\sim\\Tilde{\\pi}_{\\tau-1}^{*}}{\\mathrm{ess\\up}}\\Vert\\phi_{\\tau}(f_{\\tau}(\\\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Following the analysis in Lemma B.1, by induction, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad_{\\mathrm{exp-tory}}}&{\\quad_{\\mathrm{(S)}}=}&{0}\\\\ &{\\le}&{\\exp\\operatorname*{sup}_{r\\rightarrow1}\\quad_{\\forall r=1}^{r}\\quad_{\\forall r=1}\\quad_{-\\infty}\\quad_{r}(f_{r}(\\mathbf{s}_{r-1}^{\\prime}))\\rVert_{1}}\\\\ &{\\le}&{\\exp\\operatorname*{sup}_{r\\rightarrow1}\\quad_{\\forall r=1}^{r}\\quad_{\\forall r=1}\\quad_{-\\infty}\\quad_{r}(f_{r}(\\mathbf{s}_{r-1}^{\\prime}))\\rVert_{1}+\\left\\lVert\\phi_{r}(f_{r}(\\mathbf{s}_{r-1}^{\\prime}))-\\phi_{r}^{\\prime}(f_{r}^{\\prime}(\\mathbf{s}_{r-1}^{\\prime}))\\right\\rVert_{1}}\\\\ &{\\le}&{0}\\\\ &{\\stackrel{(a)}{\\le}}&{\\exp\\exp\\operatorname*{sup}_{r\\rightarrow1}\\quad_{\\forall r=1}\\cdot\\left\\lVert\\mathbf{s}_{r-1}-\\mathbf{s}_{r-1}^{\\prime}\\right\\rVert_{1}+\\rho_{\\mathrm{arf}}}&{\\quad_{\\mathrm{()}}=}&{0}\\\\ &{\\le}&{0}\\\\ &{\\le}&{\\exp\\operatorname*{sup}_{r\\rightarrow1}\\cdot\\left\\lVert\\mathbf{s}_{r-1}\\right\\rVert_{1}+\\rho_{\\mathrm{arf}}\\left(1+\\gamma_{\\operatorname*{max}}+\\cdots+\\gamma_{\\operatorname*{max}}^{r-2}\\right)}\\\\ &{\\stackrel{(b)}{\\le}}&{\\exp\\exp\\left(\\frac{r^{2}-1}{r_{0}\\mathbf{s}_{r}}\\cdot\\left\\lVert\\mathbf{s}_{1}-\\mathbf{s}_{1}^{\\prime}\\right\\rVert_{1}+\\rho_{\\mathrm{arf}}\\left(1+\\gamma_{\\operatorname*{max}}+\\cdots+\\gamma_{\\operatorname*{max}}^{r-2}\\right)\\right.}\\\\ &{\\stackrel{(c)}{\\le}}&{\\exp\\left(\\phi_{1}s_{r-1}^{\\prime}\\right)\\cdot\\left\\lVert\\mathbf{s}_{1}-\\gamma_{\\operatorname*{max}}\\right\\rVert_{1}+\\rho_{\\mathrm{arf}}\\cdot\\left(1-\\gamma_{\\operatorname*{max}}^{r-1}\\right)}\\\\ &{\\stackrel{(c)}{\\le}}&{0}\\\\ &{\\quad\\left.\\rho_{\\mathrm{art}}\\cdot\\left(1-\\gamma_{r}\\right)\\right\\lVert\\mathbf{s}_{ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ is from Lemma B.1, $(b)$ is from induction, and $(c)$ arises from Eq. (78). ", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{\\infty}(\\mu_{\\tau},\\nu_{\\tau})\\leq\\frac{\\rho_{\\mathrm{diff}}\\cdot(1-\\gamma_{\\operatorname*{max}}^{\\tau})}{1-\\gamma_{\\operatorname*{max}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, when we set $\\begin{array}{r}{w_{\\tau}=\\frac{\\rho_{\\mathrm{diff}}\\cdot(1-\\gamma_{\\mathrm{max}}^{\\tau})}{1-\\gamma_{\\mathrm{max}}}}\\end{array}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\alpha}^{(w_{\\tau})}(\\mathbf{s}_{\\tau}\\|\\mathbf{s}_{\\tau}^{\\prime})=\\operatorname*{inf}_{\\mu_{\\tau}^{\\prime}:W_{\\infty}(\\mu_{\\tau},\\mu_{\\tau}^{\\prime})\\leq w_{\\tau}}\\mathcal{D}_{\\alpha}(\\mu_{\\tau}^{\\prime}\\|\\nu_{\\tau})=0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the equality is achieved by selecting $\\mu_{\\tau}^{\\prime}=\\nu_{\\tau}$ ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All main claims are substantiated both theoretically (Sections 3, Appendices A, and B) and empirically (Section 4, Appendix D). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have discussed the limitations in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The complete proofs are presented in Appendix B with proof sketch outlined in Section 3. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have uploaded our code in supplementary file. All information to reproduce the main experimental results are discussed in Section 4 and Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have uploaded our code in the supplementary file with instructions (README.md) to reproduce the results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The experimental details are all included in Section 4 and Appendix D. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have included $95\\%$ confidence interval for our main experiments in Section 4 (Fig. 4 and Fig. 5). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have detailed the compute workers and reported the running time of each algorithm in Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The research presented in this paper adheres to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have discussed societal impacts in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not involve the development or release of models, data, or technologies that carry a high risk of misuse, such as pretrained language models, image generators, or scraped datasets. Consequently, discussions on specific safeguards for such contexts are not applicable. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: As detailed in Section 4 and Appendix D of our manuscript, we have meticulously cited all open-source benchmark datasets and baseline methods utilized in our research. Each asset is attributed to its original source. Please note that the benchmark datasets in this paper do not include licenses. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have included well-documented new assets in our submission, which are provided in an anonymized zip file. These assets are accompanied by comprehensive documentation covering all relevant details such as the training procedures, licensing information, and any limitations of the assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]