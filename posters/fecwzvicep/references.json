{"references": [{"fullname_first_author": "Pierre-Luc Bacon", "paper_title": "The option-critic architecture", "publication_date": "2017-00-00", "reason": "This paper introduces the Option-Critic architecture, a key concept in hierarchical reinforcement learning that is directly relevant to the proposed HIPO framework."}, {"fullname_first_author": "David Silver", "paper_title": "Mastering the game of Go without human knowledge", "publication_date": "2017-00-00", "reason": "This seminal paper demonstrates the power of deep reinforcement learning in achieving superhuman performance on complex tasks, providing a strong foundation for the field and context for the present work."}, {"fullname_first_author": "Richard S Sutton", "paper_title": "Between MDPS and semi-MDPS: A framework for temporal abstraction in reinforcement learning", "publication_date": "1999-00-00", "reason": "This foundational paper provides a theoretical framework for temporal abstraction in reinforcement learning, which underpins the hierarchical approach adopted in HIPO."}, {"fullname_first_author": "Dweep Trivedi", "paper_title": "Learning to synthesize programs as interpretable and generalizable policies", "publication_date": "2021-00-00", "reason": "This paper is highly relevant as it introduces the method of program synthesis and embedding space used in HIPO for creating the programmatic options."}, {"fullname_first_author": "Guan-Ting Liu", "paper_title": "Hierarchical programmatic reinforcement learning via learning to compose programs", "publication_date": "2023-00-00", "reason": "This paper is directly related to HIPO, proposing a hierarchical approach using programmatic policies, and providing a baseline method for comparison."}]}