[{"figure_path": "yVzWlFhpRW/tables/tables_8_1.jpg", "caption": "Table 1: Mean and standard deviation of episode return for ten runs per trained model.", "description": "This table presents the mean and standard deviation of episode returns obtained from ten independent training runs for each of the five models (Baseline, Replacement, Ray Mask, Generator Mask, and Distributional Mask) across four different environments (Seeker, 2D Quadrotor, 3D Quadrotor, and Walker2D).  The values provide a quantitative comparison of the performance of each model in each environment, showing the average reward achieved and the variability in performance across multiple training runs. The data reflects the final model performance after training completion.", "section": "5.2 Results"}, {"figure_path": "yVzWlFhpRW/tables/tables_17_1.jpg", "caption": "Table 1: Mean and standard deviation of episode return for ten runs per trained model.", "description": "This table presents the mean and standard deviation of the episode return achieved by different reinforcement learning methods across ten independent runs.  The methods compared include a baseline (PPO without action masking), a replacement method (replacing actions outside the relevant set with uniformly sampled actions), and the three proposed continuous action masking methods (Ray Mask, Generator Mask, and Distributional Mask). The results are shown for four different environments: Seeker, 2D Quadrotor, 3D Quadrotor, and Mujoco Walker2D.", "section": "5.2 Results"}, {"figure_path": "yVzWlFhpRW/tables/tables_17_2.jpg", "caption": "Table 1: Mean and standard deviation of episode return for ten runs per trained model.", "description": "This table presents the mean and standard deviation of the episode return for each of the trained models across ten runs.  The models include the baseline (no action masking), a replacement method, and the three proposed continuous action masking methods (Ray, Generator, and Distributional masks).  The results are shown for four different environments: Seeker, 2D Quadrotor, 3D Quadrotor, and Walker2D.  The table quantifies the performance of each method in each environment by providing a summary statistic across multiple trials.", "section": "5 Results"}, {"figure_path": "yVzWlFhpRW/tables/tables_19_1.jpg", "caption": "Table 4: PPO hyperparameters for the Seeker environment.", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the Seeker Reach-Avoid environment.  It shows the settings for five different training configurations: the baseline PPO, Ray Mask, Generator Mask, Distributional Mask, and Replacement methods.  Each row specifies a different hyperparameter, such as learning rate, discount factor, steps per update, and others.  The values show how these parameters were tuned for each approach to optimize performance in this specific environment.", "section": "5.1 Seeker Reach-Avoid"}, {"figure_path": "yVzWlFhpRW/tables/tables_19_2.jpg", "caption": "Table 5: PPO hyperparameters for the 2D Quadrotor environment.", "description": "This table presents the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the 2D Quadrotor experiment.  It shows the settings used for the baseline (standard PPO) and the three action masking methods (Ray Mask, Generator Mask, and Distributional Mask), as well as the replacement agent. The hyperparameters include learning rate, discount factor, steps per update, optimization epochs, minibatch size, max gradient norm, entropy coefficient, initial log standard deviation, value function coefficient, clipping range, GAE lambda, activation function, hidden layers, and neurons per layer.", "section": "5 Numerical experiments"}, {"figure_path": "yVzWlFhpRW/tables/tables_20_1.jpg", "caption": "Table 6: PPO hyperparameters for the 3D Quadrotor environment.", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm when training on the 3D Quadrotor environment.  Different hyperparameter configurations are shown for each of five approaches: Baseline, Ray Mask, Generator Mask, Distributional Mask, and Replacement. Each row represents a specific hyperparameter (e.g., learning rate, discount factor, etc.), and each column displays the value used for that hyperparameter in each of the five training approaches. The hyperparameter settings were obtained through hyperparameter optimization with 50 trials for each method.", "section": "A.9 Hyperparameters for learning algorithms"}, {"figure_path": "yVzWlFhpRW/tables/tables_20_2.jpg", "caption": "Table 7: PPO hyperparameters for the Walker2D environment.", "description": "This table lists the hyperparameters used for training the Proximal Policy Optimization (PPO) algorithm in the MuJoCo Walker2D environment.  It compares hyperparameter settings across five different training runs: the baseline PPO, a replacement method, and three continuous action masking methods (ray mask, generator mask, and distributional mask).  Each row represents a specific hyperparameter, and each column shows the value used for each of the five training runs.", "section": "5 Numerical experiments"}]