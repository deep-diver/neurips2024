[{"figure_path": "yVzWlFhpRW/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of masking methods in action space A with a hexagon-shaped relevant action set Ar. The ray mask radially maps the actions towards the center of the relevant action set. The generator mask employs the latent action space A\u00b9, which is the generator space of the zonotope modeling the relevant action set. The distributional mask augments the policy probability density function so that it is zero outside the relevant action set.", "description": "This figure illustrates three different continuous action masking methods.  The hexagon represents the relevant action set (Ar) within a larger action space (A). (a) Ray Mask: Actions are scaled along rays originating from the center of Ar to its boundary. (b) Generator Mask: Uses a latent action space (A\u00b9) that maps to Ar via a zonotope's generator matrix (G). (c) Distributional Mask: Modifies the policy's probability distribution to be zero outside Ar.", "section": "4 Continuous action masking"}, {"figure_path": "yVzWlFhpRW/figures/figures_6_1.jpg", "caption": "Figure 2: The Seeker Reach-Avoid environment with state and action space. The agent (black) has to reach the goal (gray) while avoiding the obstacle (red). The center of the action space is illustrated by a cross and the relevant action set A r for the current state is shown in green. The state set reachable at the next time step, by the relevant action set, is S\u0394t.", "description": "This figure illustrates the Seeker Reach-Avoid environment used in the paper's experiments. It shows the state space (S), which is a 2D area where the agent (black dot) needs to reach the goal (gray square) while avoiding collision with a circular obstacle (red circle). The relevant action set (Ar) for the current state is highlighted in green within the overall action space (A). The reachable set (S\u0394t) at the next time step, given the relevant actions, is also shown.", "section": "5.1 Environments"}, {"figure_path": "yVzWlFhpRW/figures/figures_7_1.jpg", "caption": "Figure 3: Average reward curves for benchmarks with transparent bootstrapped 95% confidence interval.", "description": "This figure presents the average reward curves for four different reinforcement learning benchmark environments (2D Quadrotor, 3D Quadrotor, Seeker Reach-Avoid, and Mujoco Walker2D).  Each curve represents the performance of a different approach: Baseline (no action masking), Replacement (actions outside the relevant set are replaced by a sample from the relevant set), Ray Mask, Generator Mask, and Distributional Mask. The shaded regions around each line represent the 95% bootstrapped confidence intervals, indicating the variability in performance across multiple training runs.  The figure shows the average reward accumulated over a certain number of training steps (time). This visualization allows for a comparison of the training efficiency and final performance of each method.", "section": "5 Numerical experiments"}, {"figure_path": "yVzWlFhpRW/figures/figures_18_1.jpg", "caption": "Figure 3: Average reward curves for benchmarks with transparent bootstrapped 95% confidence interval.", "description": "This figure presents the average reward curves obtained during training for four different reinforcement learning environments: Seeker Reach-Avoid, 2D Quadrotor, 3D Quadrotor, and Mujoco Walker2D.  Each curve represents a different method for action masking: Baseline (no masking), Replacement (replacing invalid actions with random valid ones), Ray Mask, Generator Mask, and Distributional Mask.  The shaded areas around each curve depict 95% bootstrapped confidence intervals, illustrating the variability in performance across multiple training runs.  The graph shows the cumulative reward achieved over the course of training, indicating which methods lead to better performance and faster convergence.", "section": "5 Numerical experiments"}, {"figure_path": "yVzWlFhpRW/figures/figures_19_1.jpg", "caption": "Figure 5: Qualitative deployment results for ten initial states and one goal-obstacle configuration for the Seeker environment. The top half shows ten trajectories with randomly sampled starting states. The lower half depicts the relevant action set (green polygon) for each time step along one trajectory.", "description": "This figure shows ten example trajectories for each of the five reinforcement learning approaches used in the Seeker Reach-Avoid environment.  The top panel displays the trajectories themselves. The bottom panel highlights the relevant action sets (green polygons) at each timestep for a single trajectory to illustrate how the action masking methods restrict the agent's possible actions to a smaller, state-dependent set.  The figure demonstrates that the action masking methods effectively guide the agent to reach the goal while avoiding collisions, which is not achieved by the baseline or replacement method.", "section": "5.1.1 Seeker Reach-Avoid"}]