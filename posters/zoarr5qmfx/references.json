{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of prompt engineering, demonstrating the effectiveness of large language models as few-shot learners, which directly influences the current work."}, {"fullname_first_author": "B. Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-04-01", "reason": "This paper introduces a significant soft prompt tuning method, providing a key baseline for comparison and contextualizing the current research on parameter-efficient tuning methods."}, {"fullname_first_author": "X. Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "publication_date": "2021-01-01", "reason": "This paper presents another important soft prompt optimization method, offering a valuable alternative approach to the technique explored in this paper."}, {"fullname_first_author": "X. Liu", "paper_title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks", "publication_date": "2021-10-01", "reason": "This work provides a strong soft prompt tuning approach that enhances the versatility of prompts, offering a relevant comparison method and providing insights into the efficiency of prompt optimization."}, {"fullname_first_author": "M. Deng", "paper_title": "RLPrompt: Optimizing discrete text prompts with reinforcement learning", "publication_date": "2022-05-01", "reason": "This work focuses on hard prompt optimization with reinforcement learning, offering a strong baseline and providing valuable insights into the application of RL to this area, thus complementing the current study."}]}