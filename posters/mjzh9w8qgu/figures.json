[{"figure_path": "mJZH9w8qgu/figures/figures_4_1.jpg", "caption": "Figure 1: Standard IRL (left), the naive method for in-trajectory learning (middle), and our method (right).", "description": "This figure visualizes three different approaches to reward updates in inverse reinforcement learning (IRL). The left panel shows the standard IRL method, which compares complete expert and learner trajectories. The middle panel illustrates a naive in-trajectory method, comparing only the trajectory prefixes up to the current time step. The right panel presents the proposed MERIT-IRL method, which creates a complete trajectory by combining the expert's trajectory prefix with a learner-generated suffix, allowing for a comparison of complete trajectories even with incomplete data.", "section": "4.1 The proposed algorithm"}, {"figure_path": "mJZH9w8qgu/figures/figures_8_1.jpg", "caption": "Figure 2: In-trajectory learning performance.", "description": "This figure shows the in-trajectory learning performance of MERIT-IRL and other baselines on four different tasks: HalfCheetah, Walker, Hopper, and Stock Market.  The x-axis represents the time step of the expert trajectory, and the y-axis represents the cumulative reward of the learned policy at each time step. The shaded areas indicate the standard deviation across 10 different runs of each algorithm. The figure demonstrates that MERIT-IRL achieves comparable performance to the expert policy using only partial trajectories, highlighting its ability to learn incrementally from ongoing trajectories.  In contrast, the other baselines (IT-IRL, Naive MERIT-IRL, and Naive IT-IRL) struggle to match the expert's performance without using the full trajectory.  The Hindsight baseline, which uses the complete trajectory, serves as an upper bound for comparison.", "section": "5 Experiments"}, {"figure_path": "mJZH9w8qgu/figures/figures_9_1.jpg", "caption": "Figure 3: Learning performance on the active shooting scenario.", "description": "This figure visualizes the in-trajectory learning performance in an active shooting scenario.  Subfigures (b) through (g) show heatmaps of the learned reward function at different time steps (t) during the ongoing trajectory.  The heatmaps show how the learned reward function becomes more precise at locating the goal area over time. The trajectory of the shooter starts at the bottom left and ends at the top right. Subfigure (h) shows the success rate (the rate that the learned policy successfully reaches the goal) for different algorithms.", "section": "5 Experiments"}]