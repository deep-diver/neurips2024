<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO &#183; NeurIPS 2024</title>
<meta name=title content="No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO &#183; NeurIPS 2024"><meta name=description content="Deep RL agents trained under non-stationarity suffer performance collapse due to representation degradation; this work reveals this in PPO and introduces Proximal Feature Optimization (PFO) to mitigat..."><meta name=keywords content="Machine Learning,Reinforcement Learning,üè¢ CLAIRE,EPFL,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO"><meta property="og:description" content="Deep RL agents trained under non-stationarity suffer performance collapse due to representation degradation; this work reveals this in PPO and introduces Proximal Feature Optimization (PFO) to mitigat‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="üè¢ CLAIRE, EPFL"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/cover.png"><meta name=twitter:title content="No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO"><meta name=twitter:description content="Deep RL agents trained under non-stationarity suffer performance collapse due to representation degradation; this work reveals this in PPO and introduces Proximal Feature Optimization (PFO) to mitigat‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO","headline":"No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO","abstract":"Deep RL agents trained under non-stationarity suffer performance collapse due to representation degradation; this work reveals this in PPO and introduces Proximal Feature Optimization (PFO) to mitigat\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/wy9ugrmwd0\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Machine Learning","Reinforcement Learning","üè¢ CLAIRE, EPFL"],"mainEntityOfPage":"true","wordCount":"5380"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/wy9ugrmwd0/cover_hu18260461489255969514.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/wy9ugrmwd0/>No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5380 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/Wy9UgrMwD0/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/Wy9UgrMwD0/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/machine-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/reinforcement-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Reinforcement Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-claire-epfl/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ CLAIRE, EPFL</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#ppos-rep-collapse>PPO&rsquo;s Rep Collapse</a></li><li><a href=#trust-region-issues>Trust Region Issues</a></li><li><a href=#representation-loss>Representation Loss</a></li><li><a href=#proximal-feature-opt>Proximal Feature Opt</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#ppos-rep-collapse>PPO&rsquo;s Rep Collapse</a></li><li><a href=#trust-region-issues>Trust Region Issues</a></li><li><a href=#representation-loss>Representation Loss</a></li><li><a href=#proximal-feature-opt>Proximal Feature Opt</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>Wy9UgrMwD0</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Skander Moalla et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=Wy9UgrMwD0" target=_blank role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/94803 target=_blank role=button>‚Üó NeurIPS Homepage
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://huggingface.co/spaces/huggingface/paper-central?tab=tab-chat-with-paper&amp;paper_id=Wy9UgrMwD0&amp;paper_from=neurips" target=_blank role=button>‚Üó Chat</a></p><audio controls><source src=https://ai-paper-reviewer.com/Wy9UgrMwD0/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Deep reinforcement learning (RL) agents often struggle with non-stationarity during training, leading to performance issues. This paper focuses on Proximal Policy Optimization (PPO), a widely used RL algorithm. Previous research highlighted this issue in off-policy algorithms, but this work extends the analysis to on-policy methods like PPO.</p><p>The study reveals that PPO agents are susceptible to representation collapse, characterized by a decrease in the representation rank and capacity loss‚Äîsimilar to off-policy algorithms. This collapse is worsened by higher non-stationarity and is linked to the trust region mechanism in PPO becoming ineffective. Importantly, the paper introduces Proximal Feature Optimization (PFO), a novel auxiliary loss function that regularizes representation dynamics, mitigating the performance issues observed.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-56d17efdb46e19a9442c4e95eabf7376></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-56d17efdb46e19a9442c4e95eabf7376",{strings:[" Reinforcement learning (RL) agents trained with Proximal Policy Optimization (PPO) suffer from representation collapse, leading to performance degradation. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e2d29eedced5296e46479b709161c225></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e2d29eedced5296e46479b709161c225",{strings:[" This collapse is linked to a deterioration in representation quality that weakens PPO's trust region mechanism. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-f7c5efa93e2b04bd6b73fae2e1fb9300></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-f7c5efa93e2b04bd6b73fae2e1fb9300",{strings:[" Proximal Feature Optimization (PFO), a novel regularization technique, effectively mitigates representation collapse and improves PPO agent performance. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in reinforcement learning as it <strong>uncovers a critical link between representation collapse, trust region issues, and performance degradation in Proximal Policy Optimization (PPO)</strong>. It <strong>introduces a novel regularization technique</strong> to mitigate these problems and provides valuable insights into improving the robustness and stability of deep reinforcement learning algorithms.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_4_1.jpg alt></figure></p><blockquote><p>This figure illustrates the deterioration of the policy network representation in PPO, specifically on the ALE/Phoenix-v5 game. As training progresses, the norm of the pre-activations in the penultimate layer steadily increases, while the feature rank (using PCA) decreases. Increasing the number of optimization epochs per rollout exacerbates this trend, leading to a performance collapse. Importantly, this collapse is not due to the value network, whose rank and capacity remain relatively stable. The figure shows how capacity loss (the ability to fit random targets) worsens for both the actor and the critic networks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/tables_15_1.jpg alt></figure></p><blockquote><p>This table lists all the hyperparameters used for the experiments conducted on the Arcade Learning Environment (ALE). It details settings for the environment (e.g., sticky actions, frame skip, observation transformations), the data collection process (e.g., total steps, parallel environments), model architecture (e.g., convolutional and linear layers, activation function), the optimization process (e.g., optimizer, learning rate, loss functions), and logging frequency.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">PPO&rsquo;s Rep Collapse<div id=ppos-rep-collapse class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ppos-rep-collapse aria-label=Anchor>#</a></span></h4><p>Proximal Policy Optimization (PPO) is a popular reinforcement learning algorithm known for its stability. However, this paper reveals a critical vulnerability: <strong>representation collapse</strong>. Under conditions of strong non-stationarity (where the data distribution shifts dramatically during training), PPO&rsquo;s representational capacity degrades. This isn&rsquo;t a simple performance drop; it&rsquo;s a fundamental failure to learn meaningful features, which the authors term a &ldquo;loss of plasticity.&rdquo; The trust region mechanism, usually a strength of PPO, becomes ineffective when representations collapse, leading to a catastrophic performance failure. Critically, this collapse impacts the actor (the policy network) even if the critic (the value network) maintains reasonable performance. This suggests that <strong>representation quality is crucial for the effective function of PPO&rsquo;s trust region</strong>, and that simply constraining policy updates isn&rsquo;t enough to prevent fundamental representational failures. The research emphasizes that <strong>early detection of representation degradation</strong> is essential, suggesting the implementation of auxiliary loss functions to regularize representation dynamics.</p><h4 class="relative group">Trust Region Issues<div id=trust-region-issues class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#trust-region-issues aria-label=Anchor>#</a></span></h4><p>The concept of a trust region in reinforcement learning, particularly within algorithms like Proximal Policy Optimization (PPO), aims to constrain policy updates, preventing drastic changes that might destabilize training. However, this paper reveals that <strong>trust regions are not immune to the detrimental effects of representation collapse</strong>. When a model&rsquo;s representation of the environment deteriorates‚Äîindicated by reduced feature rank and capacity loss‚Äîthe trust region mechanism becomes ineffective. This is because the assumptions underlying the trust region, such as gradient orthogonality across states, no longer hold. <strong>Collapsing representations lead to highly correlated gradients</strong>, making the clipping mechanism designed to confine policy changes within the trust region ineffective. Consequently, the policy can still drastically shift, even with the trust region in place, ultimately resulting in performance collapse. <strong>The study highlights the intricate link between representation quality and the effectiveness of the trust region</strong>, demonstrating that representation collapse exacerbates trust region failure, creating a vicious cycle that undermines the algorithm&rsquo;s stability and prevents recovery.</p><h4 class="relative group">Representation Loss<div id=representation-loss class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#representation-loss aria-label=Anchor>#</a></span></h4><p>Representation loss, a critical issue in reinforcement learning (RL), signifies the degradation of an agent&rsquo;s ability to effectively represent the environment&rsquo;s state space. <strong>Non-stationarity</strong>, inherent in RL due to the ever-changing policy, exacerbates this problem. As the agent&rsquo;s policy improves, the distribution of states and rewards it observes shifts, making it difficult for the network to maintain a consistent and informative representation. This manifests as a <strong>decrease in the rank of the learned representation</strong>, meaning the network relies on fewer features to represent a wider range of states, limiting its ability to discriminate between nuanced situations. Consequently, performance suffers, and the agent&rsquo;s ability to learn new tasks (plasticity) diminishes. <strong>Capacity loss</strong>, another aspect of representation loss, reflects the reduced ability of the network to fit arbitrary target functions. These issues are often observed in conjunction with the failure of mechanisms such as trust regions, designed to stabilize training, highlighting a deep connection between representation quality and learning stability in RL. Addressing representation loss requires developing effective methods to regularize representation dynamics and enhance the network&rsquo;s ability to adapt to non-stationary data, thus maintaining plasticity and maximizing performance.</p><h4 class="relative group">Proximal Feature Opt<div id=proximal-feature-opt class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#proximal-feature-opt aria-label=Anchor>#</a></span></h4><p>Proximal Feature Optimization (PFO) is presented as a novel technique to address the issue of representation collapse in Proximal Policy Optimization (PPO). <strong>PFO regularizes the change in the network&rsquo;s pre-activations</strong>, aiming to keep them within a controlled range during policy updates. This is in contrast to PPO&rsquo;s trust region mechanism, which focuses on the policy outputs. The rationale is that by controlling the representation dynamics, the method seeks to improve the reliability of the trust region in PPO, preventing its failure under conditions of representation degradation. The paper presents empirical evidence suggesting that PFO effectively mitigates the performance collapse associated with representation collapse in PPO, showing improvements in representation metrics (feature rank and capacity loss) and performance. <strong>The intervention is simple to implement</strong>, adding an auxiliary loss term to the existing PPO objective, and doesn&rsquo;t require extensive model changes. The results, however, are environment-specific, suggesting that the effectiveness of PFO might depend on the characteristics of the environment. <strong>Further research is suggested to investigate the generalizability of PFO</strong> and to explore how it interacts with other factors contributing to representation issues in reinforcement learning.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this paper could explore several promising avenues. <strong>Extending the Proximal Feature Optimization (PFO) loss</strong> to other RL algorithms beyond Proximal Policy Optimization (PPO) is crucial to assess its generalizability and impact on representation collapse. Investigating the interaction between PFO and other interventions (e.g., Adam modifications) might reveal synergistic effects or unexpected trade-offs. A deeper theoretical analysis could formalize the connection between representation collapse, trust region issues and the observed performance degradation. This involves understanding why trust regions are ineffective under poor representations. <strong>Furthermore, a more comprehensive exploration of reward sparsity&rsquo;s role</strong> in exacerbating representation collapse is needed, potentially leading to tailored solutions for sparse reward environments. Finally, applying these findings to continual learning settings and analyzing the long-term effects of representation degradation in those scenarios would be beneficial. These future directions collectively aim at creating robust and adaptable RL agents capable of handling the complexities of non-stationarity.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_4_2.jpg alt></figure></p><blockquote><p>This figure shows the representation dynamics of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game. It demonstrates how increasing the number of optimization epochs (which increases non-stationarity) leads to a deterioration in the agent&rsquo;s representation. Specifically, the norm of pre-activations in the policy network increases, and the feature rank decreases. The capacity loss (both actor and critic) increases as the rank deteriorates. Notably, this collapse is primarily in the actor, not the critic. The figure highlights the connection between representation collapse and performance collapse in PPO.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_5_1.jpg alt></figure></p><blockquote><p>This figure shows that the policy network in a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits deteriorating representation metrics as training progresses. Specifically, the norm of the pre-activations in the penultimate layer consistently increases, while the feature rank (principal component analysis) decreases. Increasing the number of optimization epochs per rollout exacerbates these trends, leading to a collapse in the policy&rsquo;s performance. Importantly, this collapse is not caused by the value network, which remains largely unaffected. The capacity loss (inability to fit arbitrary targets) also worsens for both the policy and value networks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_5_2.jpg alt></figure></p><blockquote><p>The figure shows that the policy network of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits a decrease in representation quality over time, as measured by the norm of pre-activations and feature rank. Increasing the number of optimization epochs per rollout exacerbates this deterioration, ultimately leading to a collapse in the policy&rsquo;s performance. Interestingly, the value network&rsquo;s representation remains relatively stable, suggesting that the performance collapse is primarily driven by the actor network&rsquo;s inability to maintain a good representation. The capacity loss for both the actor and critic networks also increases significantly as the representation quality declines.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_6_1.jpg alt></figure></p><blockquote><p>This figure shows a simulation of a toy setting to illustrate the effect of rank collapse on the trust region in PPO. Two state-action pairs (x, a1) and (y, a1) are considered, with Œ± representing the linear relationship between their feature representations (œÜ(y) = Œ±œÜ(x)). The left panel (Œ± > 0) demonstrates that when the gradients of the unclipped samples align with those of the clipped samples, the probability ratios will continue to go beyond the clipping limits. The right panel (Œ± &lt; 0) shows that when the gradients counteract, the ratios can be lower than the clipping limits. This illustrates how collapsed representations, where features are highly correlated, can bypass the clipping mechanisms of PPO and lead to trust-region violations.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_8_1.jpg alt></figure></p><blockquote><p>This figure shows the effects of different interventions to mitigate the performance collapse of PPO agents. It compares the results of no intervention, regularizing features (PFO), resetting Adam&rsquo;s optimizer, sharing the actor-critic trunk, and using Adam with equal betas. The top and middle rows show results for ALE games Phoenix and NameThisGame, respectively, while the bottom row presents results for Gravitar. The results indicate that regularizing features and sharing the actor-critic trunk improve representations and mitigate performance collapse in Phoenix and NameThisGame. However, sharing the trunk negatively impacts performance in Gravitar, possibly due to reward sparsity causing value network collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_23_1.jpg alt></figure></p><blockquote><p>The figure shows how the performance and representation metrics of a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix-v5 game deteriorate over time. Specifically, it illustrates how increasing the number of optimization epochs (a way to increase non-stationarity) worsens the problem. The key metrics tracked are: the norm of pre-activations in the policy network, the feature rank (approximated rank via PCA) of the policy and critic networks, and the capacity loss (ability to fit random targets) of both networks. The results show a clear pattern where increased non-stationarity leads to higher pre-activation norms, lower feature rank, and higher capacity loss, ultimately resulting in a collapse in the agent&rsquo;s performance. Notably, this collapse is not caused by the critic network (value network).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_24_1.jpg alt></figure></p><blockquote><p>This figure shows the change in performance and representation metrics for a PPO-Clip agent on ALE/Phoenix-v5. It demonstrates that as training progresses, the norm of the pre-activations of the policy network&rsquo;s penultimate layer increases while its rank decreases. This deterioration is accelerated when increasing the number of optimization epochs per rollout. The collapse in the policy&rsquo;s rank coincides with a collapse in performance. Notably, the critic network does not exhibit this same deterioration.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_24_2.jpg alt></figure></p><blockquote><p>This figure illustrates how the policy network of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits deteriorating performance and representation quality as training progresses. Specifically, it shows that the norm of the pre-activations in the penultimate layer of the policy network steadily increases while the rank (a measure of representational capacity) decreases. Increasing the number of optimization epochs per rollout exacerbates this trend, leading to a performance collapse, while the critic&rsquo;s representation remains largely unaffected. This highlights the connection between representation collapse and performance collapse, indicating that the trust region mechanism in PPO is ineffective in preventing the representation quality deterioration.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_25_1.jpg alt></figure></p><blockquote><p>This figure displays how the performance and representation of the policy network of a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix game changes over time. As the agent trains (environment steps increase), the norm of the pre-activations increases, and the rank of the representation decreases. This happens more quickly when more optimization epochs are used per rollout, indicating that non-stationarity exacerbates the issue. The collapse in representation rank corresponds to a collapse in the policy&rsquo;s performance. This is contrasted with the critic network which remains stable throughout. Additionally, the capacity loss (ability to fit random targets) of both networks shows a clear correlation with the representation collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_26_1.jpg alt></figure></p><blockquote><p>This figure shows how the performance and representation metrics of a Proximal Policy Optimization (PPO) agent evolve during training on the ALE/Phoenix-v5 game. Specifically, it demonstrates the deterioration of the representation (increasing pre-activation norm and decreasing feature rank) and the eventual collapse in policy performance as the number of training epochs increases. It also highlights that the value network&rsquo;s representation remains relatively stable, indicating that the performance collapse is primarily driven by the actor network and its capacity loss.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_26_2.jpg alt></figure></p><blockquote><p>The figure displays the results of training a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix-v5 game. It demonstrates how increasing the number of epochs per rollout (a way to control non-stationarity) leads to a deterioration in the policy network&rsquo;s representation. Key metrics tracked include the norm of pre-activations (increasing), feature rank (decreasing), and capacity loss (increasing). The deterioration in the policy&rsquo;s representation directly impacts its performance, leading to a performance collapse. Notably, this collapse is not influenced by the value network, which maintains a high rank and doesn&rsquo;t experience similar deterioration. The figure visually shows that increased non-stationarity, brought about by more epochs per rollout, negatively impacts representation quality, which in turn leads to performance collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_26_3.jpg alt></figure></p><blockquote><p>This figure shows that in the ALE/Phoenix-v5 environment, the policy network of a Proximal Policy Optimization (PPO) agent experiences a decline in representation quality over time. As training progresses, the norm of the pre-activations (a measure of the representation&rsquo;s complexity) increases consistently, while its rank (a measure of the representation&rsquo;s dimensionality) decreases. This deterioration is accelerated by increasing the number of optimization epochs per rollout, which amplifies the effects of non-stationarity. The consequence is a collapse in the policy&rsquo;s performance, while the value network remains unaffected. The figure also highlights the worsening capacity loss (the agent&rsquo;s ability to fit new targets) for both the policy and value networks.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_26_4.jpg alt></figure></p><blockquote><p>The figure shows the changes in several metrics during the training of a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix-v5 game. It illustrates how the performance of the agent&rsquo;s policy network degrades over time. This degradation is linked to changes in the representation learned by the network. Specifically, the norm of the pre-activations increases while the rank decreases. Increasing the number of optimization epochs per rollout worsens this effect, leading to performance collapse. Interestingly, the value network&rsquo;s performance and representation remain relatively stable.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_27_1.jpg alt></figure></p><blockquote><p>The figure shows the representation and performance metrics of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game. It demonstrates that as the number of optimization epochs per rollout increases (increasing non-stationarity), the norm of the policy network&rsquo;s pre-activations grows, while the feature rank decreases. This deterioration in representation coincides with a collapse in policy performance, but not in the value network&rsquo;s performance, nor its representation. The capacity loss for both networks also worsens.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_28_1.jpg alt></figure></p><blockquote><p>The figure displays how the representation metrics (feature rank and capacity loss) and performance of a PPO agent on the ALE/Phoenix-v5 game deteriorate over training time. The deterioration is accelerated by increasing the number of optimization epochs per rollout, which simulates stronger non-stationarity. It shows a consistent increase in the norm of pre-activations, a decrease in feature rank, and a capacity loss that worsens in both the actor (policy network) and the critic (value network). This deterioration is linked to a performance collapse of the agent. Importantly, the critic network maintains a high rank and capacity, suggesting that the performance collapse is primarily driven by the deterioration of the actor&rsquo;s representation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_28_2.jpg alt></figure></p><blockquote><p>This figure displays the results of training a Proximal Policy Optimization (PPO) agent on the Atari game Phoenix. It shows how the representation quality, measured by the norm of pre-activations, rank of the feature matrix, and capacity loss, deteriorates over time, leading to a collapse in performance. Increasing the number of optimization epochs per rollout worsens this degradation, showing the impact of non-stationarity. Interestingly, the value network&rsquo;s representation remains relatively stable, indicating that the problem is specific to the policy network.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_29_1.jpg alt></figure></p><blockquote><p>This figure shows the results of several interventions to mitigate the negative effects of non-stationarity on the performance of PPO agents. The interventions include regularizing the difference between features of consecutive policies (PFO), sharing actor-critic features, resetting Adam optimizer, and using equal betas for Adam. The results are presented as boxplots showing the distributions of various metrics (episode return, excess ratio, number of dead neurons, norm of pre-activations, feature rank, capacity loss) across different interventions. The findings show that PFO and sharing the actor-critic trunk are beneficial for most of the games/tasks, while using equal betas for Adam demonstrates improvement on some games but doesn&rsquo;t prevent collapse. In particular, sharing the actor-critic network in the game with sparse reward (Gravitar) produces a worse policy representation due to rank collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_29_2.jpg alt></figure></p><blockquote><p>The figure shows the training curves for a Proximal Policy Optimization (PPO) agent trained on the Atari game Phoenix. The curves illustrate how the performance of the PPO agent deteriorates along with the degradation of its representation. As the number of epochs increases, the representation collapse and performance collapse happen earlier during training, while the value network&rsquo;s performance remains stable. The results highlight a relationship between non-stationarity, representation degradation, and performance collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_30_1.jpg alt></figure></p><blockquote><p>This figure shows the results of several interventions to mitigate the performance collapse of PPO agents. Boxplots compare the performance (episode return), representation quality (feature rank, pre-activation norm, dead neurons), and trust region behavior (excess ratio) across different interventions and environments (ALE and MuJoCo). It demonstrates that regularizing the feature representations and reducing non-stationarity (with PFO) improves representation metrics and performance. However, sharing the actor-critic trunk negatively impacts representation and performance in sparse reward environments.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_30_2.jpg alt></figure></p><blockquote><p>This figure shows the results of an experiment where a Proximal Policy Optimization (PPO) agent was trained on the Atari game Phoenix. The top two plots show the norm of the pre-activations and the feature rank of the policy network over the course of training. As the number of optimization epochs per rollout increases, which simulates stronger non-stationarity, the pre-activation norm increases while the feature rank decreases. This leads to a collapse in the policy&rsquo;s performance. The bottom two plots show the feature rank and capacity loss of the critic network. In contrast to the actor network, the critic network&rsquo;s representation remains relatively stable even as the policy network collapses, suggesting that the collapse is not primarily driven by the critic&rsquo;s performance. The capacity loss (ability to fit random targets) metric illustrates worsening model plasticity for both actor and critic networks as training progresses.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_31_1.jpg alt></figure></p><blockquote><p>This figure shows how the representation quality of the policy network in a Proximal Policy Optimization (PPO) agent deteriorates over time during training. It demonstrates that increasing the number of optimization epochs (which increases non-stationarity) accelerates the decline in representation rank and increases the norm of pre-activations. This deterioration is accompanied by a performance collapse, while the value network remains relatively unaffected. The figure also shows a decrease in the ability of both the policy and value networks to fit arbitrary targets (capacity loss).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_31_2.jpg alt></figure></p><blockquote><p>This figure illustrates the deterioration of policy representation in a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game. It shows that as the number of epochs per rollout increases (increasing non-stationarity), the norm of pre-activations in the policy network increases, while its rank decreases. This representation degradation leads to a collapse in policy performance. Interestingly, the critic network&rsquo;s representation remains relatively stable, highlighting that the performance collapse is primarily driven by the actor&rsquo;s deteriorating representation and capacity to fit arbitrary targets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_32_1.jpg alt></figure></p><blockquote><p>This figure displays the results of training a Proximal Policy Optimization (PPO) agent on the Atari game Phoenix. It shows how several key metrics change over the course of training. The main observation is that as the agent&rsquo;s performance deteriorates, the representation learned by the policy network degrades. This degradation is characterized by an increase in the norm of pre-activations (a measure of the representation&rsquo;s complexity), and a decrease in the feature rank (a measure of the representation&rsquo;s dimensionality). The figure also illustrates that increasing the number of training epochs (which increases the non-stationarity of the training data) exacerbates this phenomenon, leading to a more rapid collapse in performance. Interestingly, the performance of the critic network (which estimates the value function) remains stable, suggesting that the problem is specific to the policy network&rsquo;s representation learning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_32_2.jpg alt></figure></p><blockquote><p>The figure shows how the representation metrics and performance of a Proximal Policy Optimization (PPO) agent change over time during training. Specifically, it illustrates how increasing the number of epochs (iterations) per rollout, which increases non-stationarity, leads to a worsening of the representation (increasing pre-activation norm, decreasing feature rank) and ultimately a collapse in policy performance. Importantly, it shows that this collapse is not due to problems with the value network, but rather problems with the policy network.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_33_1.jpg alt></figure></p><blockquote><p>This figure displays the performance and representation metrics of a PPO-Clip agent trained on the ALE/Phoenix-v5 game. It shows that as the number of optimization epochs per rollout increases, the norm of the pre-activations in the policy network increases, while its rank decreases, eventually collapsing. This collapse coincides with a drop in performance. Interestingly, the critic network&rsquo;s representation remains relatively stable, suggesting that the collapse is primarily driven by the policy network&rsquo;s deteriorating representation. Capacity loss (the network&rsquo;s ability to fit arbitrary targets) also worsens for both policy and critic networks. The figure highlights the interconnectedness of representation quality, trust region effectiveness, and overall performance in PPO.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_33_2.jpg alt></figure></p><blockquote><p>The figure shows the impact of increasing non-stationarity on the representation and performance of a PPO agent. As the number of epochs increases, the norm of pre-activations in the policy network increases while its rank decreases. This representation collapse is linked to a performance collapse, unlike the critic, whose performance and representation do not collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_34_1.jpg alt></figure></p><blockquote><p>This figure shows the deterioration of representation metrics in PPO agents over time. It tracks several metrics for both actor (policy) and critic networks during training on the ALE/Phoenix-v5 game, and plots these against the number of environment steps. It shows that increased non-stationarity (achieved by increasing the number of epochs) exacerbates the degradation of the policy representation and leads to a collapse in performance. This is reflected in a sharp increase in the norm of pre-activations, a decrease in feature rank and a significant increase in capacity loss.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_34_2.jpg alt></figure></p><blockquote><p>The figure shows the results of training a Proximal Policy Optimization (PPO) agent on the Atari game Phoenix. It demonstrates that as training progresses and the agent improves, the network&rsquo;s representation starts to degrade. This degradation is characterized by a consistent increase in the norm of pre-activations, while the feature rank (a measure of the representation&rsquo;s dimensionality) steadily decreases. Increasing the number of training epochs per rollout, which increases non-stationarity, exacerbates this degradation, ultimately leading to a collapse in the agent&rsquo;s performance. The figure also contrasts the policy network&rsquo;s degradation with the value network, illustrating that the value network&rsquo;s representation remains relatively stable. Finally, it shows that the capacity loss for both networks (their ability to regress to random targets) worsens.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_35_1.jpg alt></figure></p><blockquote><p>This figure shows how the representation and performance of a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix-v5 game deteriorate during training. It tracks several metrics: the norm of pre-activations in the policy network, the feature rank of the policy (using Principal Component Analysis to approximate rank), the capacity loss of the policy network, and the same metrics for the critic network. Increasing the number of optimization epochs per rollout (a way to increase the non-stationarity of the training data) exacerbates the problem, leading to a decrease in the representation rank and an increase in the norm of the pre-activations. This ultimately results in a collapse in performance of the agent, which is not attributable to the critic network&rsquo;s performance. The capacity loss (inability to fit arbitrary targets) increases for both the actor and critic networks as the representation deteriorates.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_35_2.jpg alt></figure></p><blockquote><p>The figure displays how representation metrics and performance of a Proximal Policy Optimization (PPO) agent change over training time. The plot shows that the norm of the policy network pre-activations consistently increases, whereas the feature rank decreases, leading to a performance collapse. Increasing the number of epochs per rollout (increasing non-stationarity) aggravates this effect. Interestingly, the value network&rsquo;s metrics remain stable, indicating the problem is specific to the policy.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_36_1.jpg alt></figure></p><blockquote><p>This figure displays the results of training a Proximal Policy Optimization (PPO) agent on the ALE/Phoenix-v5 game. It shows that as the number of epochs per rollout increases (increasing non-stationarity), the performance of the agent deteriorates. This is linked to a decline in the quality of the representation learned by the policy network, evidenced by an increase in the norm of pre-activations and a decrease in feature rank. The figure also highlights that the capacity loss (inability to fit arbitrary targets) increases for both the policy and the value network, exacerbating the performance collapse. Interestingly, the collapse primarily affects the policy network, whereas the value network remains relatively unaffected.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_36_2.jpg alt></figure></p><blockquote><p>The figure shows how the performance and representation metrics of a Proximal Policy Optimization (PPO) agent change during training on the ALE/Phoenix-v5 game. It demonstrates that as the number of epochs (optimization steps) per rollout increases, the norm of pre-activations in the policy network increases while its rank (a measure of representation quality) decreases. This deterioration is linked to a capacity loss (the agent‚Äôs inability to fit arbitrary targets) and ultimately leads to a performance collapse. Importantly, the critic network‚Äôs representation remains stable, indicating that the problem is specific to the policy network. The figure visually presents the changes in these metrics across different numbers of epochs.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_36_3.jpg alt></figure></p><blockquote><p>This figure shows how the policy network of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits a deterioration in its representation over time. The norm of the pre-activations in the penultimate layer steadily increases, while the rank of the representation decreases. Increasing the number of optimization epochs per rollout exacerbates this issue, leading to a collapse in the policy&rsquo;s performance. Notably, the value network remains unaffected by this representation collapse. The figure also demonstrates a worsening of the capacity loss, indicating a decreasing ability of both the actor and the critic to fit arbitrary targets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_36_4.jpg alt></figure></p><blockquote><p>This figure shows how the representation quality of the policy network in a Proximal Policy Optimization (PPO) agent deteriorates over time during training. It uses ALE/Phoenix-v5 as an example, demonstrating that as the number of training epochs increases (representing higher non-stationarity), the norm of pre-activations in the penultimate layer increases, and the feature rank (PCA) decreases. This deterioration is linked to a performance collapse in the PPO agent, while the critic network remains relatively unaffected. Notably, both the policy and critic exhibit capacity loss (worsening ability to fit random targets). The figure visually depicts the correlation between representation collapse and performance collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_38_1.jpg alt></figure></p><blockquote><p>This figure shows that the policy network of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits deteriorating representation metrics over time. Specifically, the norm of pre-activations in the penultimate layer increases while the feature rank decreases. Increasing the number of optimization epochs (which increases non-stationarity) accelerates this trend, ultimately resulting in performance collapse of the agent&rsquo;s policy. Importantly, the critic network&rsquo;s performance and representation remain stable throughout, highlighting that the problem is specific to the actor. The capacity loss, which indicates the network&rsquo;s ability to fit arbitrary targets, increases for both actor and critic, further illustrating the representational decline.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_39_1.jpg alt></figure></p><blockquote><p>This figure shows that the policy network of a Proximal Policy Optimization (PPO) agent trained on the ALE/Phoenix-v5 game exhibits deteriorating representation metrics over time. Specifically, the norm of pre-activations in the penultimate layer increases steadily, while the feature rank (as determined by Principal Component Analysis) decreases. Increasing the number of optimization epochs per rollout (to amplify the effects of non-stationarity) worsens these trends, ultimately causing a performance collapse of the policy network. Importantly, this collapse is not caused by the value network, whose rank remains high. The capacity loss of both networks, indicating their decreasing ability to regress to arbitrary targets, also increases as the representation deteriorates.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_39_2.jpg alt></figure></p><blockquote><p>This figure shows the evolution of various metrics during the training of a Proximal Policy Optimization (PPO) agent on the Atari game Phoenix. It demonstrates that as the training progresses, the norm of the policy network&rsquo;s pre-activations increases while its rank decreases. Increased training epochs (more non-stationarity) exacerbates this effect, ultimately leading to a performance collapse. Interestingly, the critic network&rsquo;s performance and representation remain relatively stable, highlighting that the collapse is specific to the actor. The capacity loss of both actor and critic also increases, indicating reduced learning ability.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_39_3.jpg alt></figure></p><blockquote><p>The figure shows how the representation metrics (norm of pre-activations, feature rank, and capacity loss) of the policy and value networks in a Proximal Policy Optimization (PPO) agent evolve during training, particularly as the number of optimization epochs per rollout increases. The results show a deterioration of the representation and ultimately lead to performance collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_40_1.jpg alt></figure></p><blockquote><p>The figure displays the dynamics of representation metrics and performance of a Proximal Policy Optimization (PPO) agent trained on the Atari game Phoenix. It shows that as the agent&rsquo;s performance deteriorates, the norm of its policy network&rsquo;s pre-activations increases while its rank decreases, indicating a deterioration of the representation. Increasing the number of optimization epochs per rollout exacerbates these effects, leading to a performance collapse. Importantly, this collapse is not caused by a failure of the value network, highlighting that the problem is specific to the policy network&rsquo;s representation learning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/figures_40_2.jpg alt></figure></p><blockquote><p>This figure shows the relationship between the representation quality of a PPO agent and the effectiveness of its trust region mechanism. Each point represents the average probability ratios that fall outside the clipping limits of the trust region and an average representation metric (dead neurons, feature rank, and feature norm). The results demonstrate that when the representation quality deteriorates (characterized by a high number of dead neurons, a low feature rank, and a high feature norm), the probability ratios significantly drop below the clipping limit, suggesting a failure of the trust region to prevent catastrophic changes to the policy in this regime.</p></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/tables_16_1.jpg alt></figure></p><blockquote><p>This table lists the hyperparameters used in the toy setting to simulate the effects of rank collapse on the trust region, as shown in Figure 5 of the paper. It includes details about the environment (sampling of states and actions), the policy network architecture, and the optimization process (clipping epsilon, optimizer, learning rate, minibatch size, number of epochs, and number of steps). This setup is designed to create a simplified scenario to demonstrate how the PPO trust region constraint can be bypassed when representations collapse.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/tables_18_1.jpg alt></figure></p><blockquote><p>This table lists the hyperparameters used for the Arcade Learning Environment (ALE) experiments in the paper. It includes details about the environment setup (sticky actions, frameskip, image resizing), the data collection process (number of environments, total steps), model architecture (convolutional and linear layers), the optimization process (optimizer, learning rate, clipping epsilon, entropy bonus), and logging frequency.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/Wy9UgrMwD0/tables_19_1.jpg alt></figure></p><blockquote><p>This table lists the hyperparameters used in the MuJoCo experiments. It covers environment settings (frameskip, maximum steps per episode, etc.), observation transformations (normalization), data collection details (total steps, number of parallel environments), model architecture (activation functions, layer sizes), optimization parameters (advantage estimator, value loss, policy loss, learning rate, betas, etc.), and logging frequency for various metrics (training, capacity). The table also indicates whether or not minibatch normalization and linearly annealing were applied. In short, it provides a complete specification of the experimental setup for the MuJoCo reinforcement learning tasks.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-838e05b2919803e9ae35707cfa131505 class=gallery><img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/Wy9UgrMwD0/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/&amp;title=No%20Representation,%20No%20Trust:%20Connecting%20Representation,%20Collapse,%20and%20Trust%20Issues%20in%20PPO" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/&amp;text=No%20Representation,%20No%20Trust:%20Connecting%20Representation,%20Collapse,%20and%20Trust%20Issues%20in%20PPO" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/&amp;subject=No%20Representation,%20No%20Trust:%20Connecting%20Representation,%20Collapse,%20and%20Trust%20Issues%20in%20PPO" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/Wy9UgrMwD0/index.md",oid_likes="likes_posters/Wy9UgrMwD0/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/prbsez8rnv/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/riol7kbskv/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>