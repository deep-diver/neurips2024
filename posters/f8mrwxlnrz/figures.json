[{"figure_path": "f8MrWxlnRz/figures/figures_1_1.jpg", "caption": "Figure 1: Bounding boxes produced by GFocal [23], GFocal-V2 [22], and AIRS, where GFocal, GFocal-V2 still tend to generate unnecessary bounding boxes resulting from false positive anchors, comparing to the proposed AIRS model.", "description": "The figure compares the bounding boxes generated by GFocal, GFocal-V2, and AIRS.  GFocal and GFocal-V2 produce many false positive bounding boxes (unnecessary boxes), whereas AIRS more accurately identifies and outlines the objects of interest.", "section": "1 Introduction"}, {"figure_path": "f8MrWxlnRz/figures/figures_3_1.jpg", "caption": "Figure 2: AIRS training and test pipelines", "description": "This figure illustrates the training and testing pipelines of the Adaptive Important Region Selection (AIRS) framework. The training pipeline (a) shows how the RL agent interacts with the FPN (Feature Pyramid Network) to select patches and generate training tuples.  The testing pipeline (b) shows how the trained RL agent generates a binary mask that filters out unnecessary bounding boxes. The figure also visually explains the hierarchical search process used by AIRS.", "section": "3 Methodology"}, {"figure_path": "f8MrWxlnRz/figures/figures_8_1.jpg", "caption": "Figure 3: (a)-(b) Average number of detections per test image based on the bounding box area on MS COCO and OpenImages V4. (c) Ablative study on epistemic uncertainty to deep Q-evaluation.", "description": "This figure presents a comparison of the average number of detections per image based on bounding box area size for the MS COCO and OpenImages V4 datasets, comparing the proposed AIRS model with GFocal.  It also includes an ablative study showing the impact of epistemic uncertainty on deep Q-evaluation, demonstrating the effectiveness of incorporating uncertainty to guide exploration during the learning process. The graphs illustrate how AIRS improves detection performance for small and medium-sized objects, particularly in the OpenImages V4 dataset, by dynamically balancing exploration and exploitation and reducing false positives.", "section": "4 Experiments"}, {"figure_path": "f8MrWxlnRz/figures/figures_19_1.jpg", "caption": "Figure 4: RL-augmented detection process", "description": "This figure illustrates the process of RL-augmented object detection.  The input image is passed through a Feature Pyramid Network (FPN), generating multi-scale feature maps.  An RL agent interacts with the FPN, selecting patches (regions) to analyze hierarchically, starting from coarser levels.  The agent's decisions are guided by an evidential Q-learning process, which incorporates uncertainty to balance exploration and exploitation.  The result is a binary RL mask that filters out less informative patches, improving the accuracy of object detection in the subsequent 'class+box' subnets.", "section": "3.2 Description of Key Components"}, {"figure_path": "f8MrWxlnRz/figures/figures_20_1.jpg", "caption": "Figure 5: Detailed Workflow of AIRS", "description": "This figure illustrates the detailed workflow of the Adaptive Important Region Selection (AIRS) framework. It starts with a selected patch from the Feature Pyramid Network (FPN) which passes through a feature extractor and RNN to generate a state representation.  The state is input to an evidential Q-network, which outputs evidential Q-values and uncertainty estimates for each action. The action interaction and reward calculation modules translate the action into the location of the next patch, balancing exploration and exploitation. The resulting training tuple is added to a replay buffer for Q-learning updates.", "section": "3 Methodology"}, {"figure_path": "f8MrWxlnRz/figures/figures_21_1.jpg", "caption": "Figure 5: Detailed Workflow of AIRS", "description": "This figure shows a detailed workflow of the Adaptive Important Region Selection (AIRS) framework.  It illustrates how the RL agent interacts with the feature pyramid network (FPN), generating state representations, obtaining evidential Q-values, selecting actions (downward or upward movements), and receiving rewards. The process involves generating state representations using an RNN, calculating evidential Q-values with epistemic uncertainty, selecting actions based on masked evidential Q-values, and updating the network parameters via Q-learning. The figure also shows the interaction protocol with the FPN and the calculation of reward values.", "section": "3 Methodology"}, {"figure_path": "f8MrWxlnRz/figures/figures_21_2.jpg", "caption": "Figure 5: Detailed Workflow of AIRS", "description": "This figure illustrates the detailed workflow of the Adaptive Important Region Selection (AIRS) framework. It shows how the RL agent interacts with the environment (FPN), generates states, selects actions, receives rewards, and updates its policy through evidential Q-learning. The process involves generating feature embeddings, calculating evidential Q-values, applying masks to filter out invalid actions, and updating the agent's policy based on rewards. The process continues until termination or reaching a maximum time step.", "section": "3 Methodology"}, {"figure_path": "f8MrWxlnRz/figures/figures_24_1.jpg", "caption": "Figure 8: Results w/ and w/o AIRS generated masks on an aerial dataset [16] and a new challenging MSCOCO subset, both of which contain many small objects: (a) and (c) are the GFocal detection results while (b) and (d) are the results from AIRS after applying the RL masks. As can be seen, those grey boxes are the false positive bounding boxes, most of which possess irrelevant backgrounds or partial objects, masked out by the RL agent (i.e., 0 masks), while the red boxes are the remaining true positive boxes, which are kept the by the RL agent (i.e., 1 masks).", "description": "This figure compares the object detection results of GFocal and AIRS on an aerial dataset and a challenging subset of MS COCO.  The images show that AIRS, guided by its reinforcement learning (RL) agent, effectively suppresses false positive bounding boxes (grey) that often result from detecting irrelevant background or partially covered objects.  AIRS accurately identifies and keeps the true positive bounding boxes (red).", "section": "D Additional Experiments"}, {"figure_path": "f8MrWxlnRz/figures/figures_25_1.jpg", "caption": "Figure 3: (a)-(b) Average number of detections per test image based on the bounding box area on MS COCO and OpenImages V4. (c) Ablative study on epistemic uncertainty to deep Q-evaluation.", "description": "This figure presents the results of experiments comparing the average number of detections per image from the proposed AIRS model and a baseline model (GFocal) across three datasets (MS COCO, OpenImages V4, and Pascal VOC 2012).  The graphs in (a) and (b) show a breakdown of the number of detections by bounding box size (small, medium, large), revealing how AIRS effectively reduces false positives, especially for smaller objects.  Graph (c) depicts the Q-learning curves, showcasing the effect of incorporating epistemic uncertainty (EU) into the AIRS model. The comparison demonstrates that the inclusion of EU significantly improves the model's ability to explore under-represented regions of the feature space, leading to better overall performance.", "section": "4 Experiments"}, {"figure_path": "f8MrWxlnRz/figures/figures_26_1.jpg", "caption": "Figure 10: Qualitative comparisons between GFocal (Left) and AIRS (Right) detection results on three datasets: COCO 10a-10d, PASCAL VOC 2012 10e-10h and OpenImages V4 10i-10l", "description": "This figure shows a qualitative comparison of object detection results between GFocal and AIRS on three different datasets (COCO, PASCAL VOC 2012, and OpenImages V4).  For each dataset, two images are displayed, one showing the GFocal results and the other showing the AIRS results. Red boxes are used to represent the bounding boxes predicted by the models. The figure aims to illustrate AIRS' ability to reduce false positive detections while maintaining high recall compared to GFocal, especially in complex scenarios with many small or partially occluded objects.", "section": "4.4 Qualitative Analysis"}]