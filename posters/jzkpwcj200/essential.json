{"importance": "This paper is crucial because **robust LLM evaluation is critical for progress in the field.**  Current methods often rely on limited prompt sets, leading to unreliable results. This work's efficient multi-prompt evaluation technique is important for building better leaderboards and for applications such as LLM-as-a-judge and prompt optimization. Its theoretical guarantees and empirical validations on multiple benchmarks enhance the reliability and applicability of the proposed method.", "summary": "PromptEval efficiently estimates LLM performance across many prompts, providing robust performance metrics and enabling reliable LLM comparisons.", "takeaways": ["PromptEval provides accurate performance quantile estimates with significantly reduced evaluation cost compared to single-prompt methods.", "The method is theoretically sound, demonstrating consistent performance distribution estimation.", "PromptEval is applicable to various LLM evaluation contexts, including LLM-as-a-judge and best prompt identification."], "tldr": "Large language model (LLM) evaluation is currently limited by using only a few prompts, potentially impacting the reproducibility of results.  This approach is problematic as it doesn't represent the real-world diversity of prompts LLMs will encounter.  This leads to inconsistent LLM rankings and inaccurate performance assessments, hindering the development of robust and reliable benchmarks. \nPromptEval addresses these issues by efficiently estimating performance distributions across a large number of prompts. The proposed method uses Item Response Theory and advanced statistical techniques to borrow strength across prompts and examples, producing accurate performance quantiles, even with limited evaluations.  This allows for more robust performance comparisons and identification of the most effective prompts.  Experiments demonstrated its effectiveness on various benchmarks, showing that PromptEval consistently estimates performance distributions and enables accurate quantile estimations with a small evaluation budget.", "affiliation": "University of Michigan", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "jzkpwcj200/podcast.wav"}