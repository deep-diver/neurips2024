{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-03", "reason": "This paper introduces the MMLU benchmark, a crucial dataset used for evaluating large language models' performance across multiple tasks, forming the foundation of many experiments in the current paper."}, {"fullname_first_author": "Aarohi Srivastava", "paper_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models", "publication_date": "2022-06-04", "reason": "This paper introduces the BIG-bench benchmark, another key dataset used for evaluating large language models, which is used in the current paper's experiments to compare its methodology's efficacy."}, {"fullname_first_author": "Avia Efrat", "paper_title": "Lmentry: A language model benchmark of elementary language tasks", "publication_date": "2022-11-02", "reason": "This paper introduces the LMentry benchmark, yet another dataset used for evaluating large language models in the current study, which is used to demonstrate and validate the methodology of the paper."}, {"fullname_first_author": "Li Cai", "paper_title": "Item response theory", "publication_date": "2016-00-00", "reason": "This paper introduces Item Response Theory (IRT), a statistical framework which underpins the methodology presented in this paper to improve efficiency and accuracy of large language models' evaluation."}, {"fullname_first_author": "Moran Mizrahi", "paper_title": "State of what art? a call for multi-prompt llm evaluation", "publication_date": "2024-01-00", "reason": "This paper highlights the limitations of single-prompt LLM evaluation and advocates for multi-prompt evaluation methods, which directly motivates and inspires the methodology presented in the current paper."}]}