[{"figure_path": "jzkpwcj200/figures/figures_0_1.jpg", "caption": "Figure 1: Average estimation error for performance quantiles across 100 templates given a limited budget (in multiples of one-template MMLU evaluations).", "description": "This figure displays the average estimation error for various performance quantiles (5th, 25th, 50th, 75th, and 95th) when evaluating a large language model (LLM) using different prompt templates.  The x-axis represents the performance quantile being estimated, while the y-axis shows the average estimation error. Different colored lines represent different evaluation budgets, expressed as multiples of the computational cost required to evaluate one template on the MMLU benchmark. The shaded areas around the lines represent the confidence intervals for the estimation errors. The figure demonstrates the effectiveness of PromptEval, as the estimation error generally decreases as the budget increases, indicating its ability to provide accurate performance estimates across a large number of prompt templates with a limited evaluation budget.", "section": "Abstract"}, {"figure_path": "jzkpwcj200/figures/figures_7_1.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "The figure displays the Wasserstein-1 distance, a measure of the difference between the true performance distribution and the estimated distribution using different methods (avg, PE-Rasch, PE-discrete, PE-EmbPT, PE-EmbFT) across three benchmarks (MMLU, LMentry, BBH) at different evaluation budget sizes.  Lower values indicate better estimation accuracy. The shaded areas represent the standard error across multiple random evaluations.  The figure shows that PromptEval methods (PE variants) significantly outperform the baseline average method (avg) across all benchmarks and budget sizes.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_7_2.jpg", "caption": "Figure 3: Performance quantile estimation errors for varying quantiles (columns) and benchmarks (rows).", "description": "This figure shows the performance of different variations of PromptEval in estimating various performance quantiles (5th, 25th, 50th, 75th, and 95th) across three different benchmarks (MMLU, LMentry, and BBH).  The x-axis represents the evaluation budget, and the y-axis represents the estimation error.  The different colored lines represent different variations of the PromptEval method, allowing for a comparison of their performance under varying conditions. The shaded area around each line indicates the uncertainty in the estimation.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_8_1.jpg", "caption": "Figure 4: Estimating LLM-as-a-judge distribution of scores for 100 prompt variations given to the judge.", "description": "This figure shows the performance distribution estimations using different methods (ground truth, average, PromptEval using Rasch model, and PromptEval using pre-trained embeddings) for four different LLMs (Cohere Command, Qwen1.5-7B-Chat, Mistral-7B-Instruct-v0.2, LLaMa-2-70B-Chat) on the AlpacaEval 2.0 benchmark with 100 prompt variations. The Wasserstein-1 distance (W1) is used to measure the estimation error for each method.", "section": "6.1 Estimating the distribution of scores for the LLM-as-a-judge framework"}, {"figure_path": "jzkpwcj200/figures/figures_8_2.jpg", "caption": "Figure 5: Best-prompt identification.", "description": "This figure compares the performance of PromptEval against the TRIPLE-GSE baseline in a best-prompt identification task.  It shows the regret (difference between the best prompt's performance and the chosen prompt's performance) across different evaluation budgets.  Different versions of PromptEval (using different types of prompt covariates) are compared, demonstrating its superior performance across various settings.", "section": "6.2 Best-prompt identification"}, {"figure_path": "jzkpwcj200/figures/figures_9_1.jpg", "caption": "Figure 6: Accuracy spread across 57 subjects.", "description": "The figure shows the distribution of accuracy spreads (the difference between the maximum and minimum accuracy across different prompt templates) for various LLMs across the 57 subjects in the MMLU benchmark.  The x-axis represents the spread in accuracy, and the y-axis represents the density. Each curve represents a different LLM.  The figure illustrates the variability in LLM performance depending on the prompt used, even within a single subject.  This variability highlights the importance of methods like PromptEval, which aim to estimate LLM performance across a range of prompts.", "section": "7 Analysis of prompt sensitivity on MMLU"}, {"figure_path": "jzkpwcj200/figures/figures_15_1.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "The figure shows the Wasserstein-1 distance between the estimated and true performance distributions for three different benchmarks (MMLU, BIG-bench Hard, and LMentry).  It compares the performance of PromptEval against a baseline method ('avg').  Different variations of PromptEval are shown, each using a different method to estimate performance.  The x-axis shows the evaluation budget and the y-axis represents the Wasserstein-1 distance which measures the difference between distributions. Lower values indicate better estimation accuracy. The results show PromptEval outperforms the baseline across all benchmarks and budget levels.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_15_2.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "This figure compares the performance of PromptEval against a baseline method in estimating the performance distribution of LLMs across different prompt templates.  It shows the Wasserstein-1 distance, a metric quantifying the difference between the true and estimated performance distributions for three benchmarks (MMLU, BBH, and LMentry) across various evaluation budgets. The different colored lines represent different variations of the PromptEval method, highlighting their effectiveness in accurately estimating the LLM performance distribution, even with limited evaluations.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_16_1.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "The figure displays the effectiveness of different variations of PromptEval (PE) against the \"avg\" baseline strategy in estimating the performance distribution across prompt templates.  It shows Wasserstein-1 distance errors for performance distribution estimation on three benchmarks (MMLU, LMentry, and BBH) across different evaluation budgets. Five variations of PromptEval are compared: PE-Rasch, PE-discrete, PE-EmbPT, PE-EmbFT, and the average baseline.  The results demonstrate that PromptEval consistently outperforms the baseline, particularly as the budget increases. The use of pre-trained embeddings (PE-EmbPT) shows promising results across benchmarks.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_16_2.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "This figure displays the effectiveness of different variations of PromptEval (PE) against the \"avg\" baseline strategy in estimating the performance distribution of LLMs across various prompt templates.  The x-axis represents the evaluation budget (number of evaluations), and the y-axis shows the Wasserstein-1 distance, which measures the difference between the true performance distribution and the estimated distribution. The figure shows results for three benchmarks: MMLU, LMentry, and BBH.  Each line represents a different method: \"avg\" (simple average of scores), PE-Rasch (a basic IRT model), PE-discrete (using discrete prompt features), PE-EmbPT (using pre-trained embeddings for prompts), and PE-EmbFT (using fine-tuned embeddings for prompts).  The shaded area around each line represents the standard deviation of the estimation errors across multiple runs.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_17_1.jpg", "caption": "Figure 2: Performance distribution estimation errors measured with Wasserstein-1 distance on three benchmarks.", "description": "This figure displays the effectiveness of different variations of PromptEval (PE) against a baseline strategy ('avg') in estimating the performance distribution of LLMs across various prompt templates.  The x-axis represents the evaluation budget (in multiples of a single-prompt evaluation), and the y-axis represents the Wasserstein-1 distance, a measure of the error in estimating the performance distribution.  The figure includes results for three benchmarks: MMLU, BIG-bench Hard (BBH), and LMentry. Each line represents a different method for estimating the distribution, with PE-Rasch, PE-discrete, PE-EmbPT, and PE-EmbFT representing variations of PromptEval, showcasing the use of different covariates.", "section": "5 Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_17_2.jpg", "caption": "Figure 12: Prompt templates consistently lead the judge to assign higher (or lower) scores across models.", "description": "This heatmap visualizes the consistency of prompt templates in influencing the judge's scores across different LLMs. Each row represents a prompt template, and each column represents an LLM. The color intensity indicates the score assigned by the judge (GPT-4), ranging from lower scores (darker colors) to higher scores (brighter colors). The figure demonstrates that certain templates consistently elicit either higher or lower scores across all LLMs, suggesting a level of predictability in prompt influence.", "section": "6.1 Estimating the distribution of scores for the LLM-as-a-judge framework"}, {"figure_path": "jzkpwcj200/figures/figures_18_1.jpg", "caption": "Figure 5: Best-prompt identification.", "description": "The figure shows the comparison of regret between PromptEval and TRIPLE-GSE for best-prompt identification task across three different benchmarks. PromptEval consistently shows lower regret compared to TRIPLE-GSE, indicating its effectiveness in identifying the best prompt efficiently. The different line styles represent different variations of PromptEval and TRIPLE-GSE, each using different types of covariates. The x-axis represents the budget, and the y-axis represents the regret.", "section": "6.2 Best-prompt identification"}, {"figure_path": "jzkpwcj200/figures/figures_18_2.jpg", "caption": "Figure 5: Best-prompt identification.", "description": "This figure compares the regret of PromptEval against the baseline TRIPLE-GSE [Shi et al., 2024] with a logistic regression performance predictor using one-hot encoding, discrete covariates, and pre-trained embeddings for covariates. It shows that using PromptEval for best-prompt identification results in lower regret, i.e., the performance of the best template minus the performance of the chosen template.", "section": "6.2 Best-prompt identification"}, {"figure_path": "jzkpwcj200/figures/figures_19_1.jpg", "caption": "Figure 3: Performance quantile estimation errors for varying quantiles (columns) and benchmarks (rows).", "description": "The figure presents the performance of different variations of PromptEval in estimating various quantiles (5th, 25th, 50th, 75th, and 95th) of the performance distribution across prompt templates.  It displays the estimation error for three benchmark datasets (MMLU, LMentry, and BBH) across different evaluation budgets.  Each point represents the average error across multiple trials, and error bars indicate variability. The figure allows for comparison of different PromptEval variations (including those utilizing different prompt embedding methods) and a baseline average method.", "section": "Assessing multi-prompt evaluation strategies"}, {"figure_path": "jzkpwcj200/figures/figures_21_1.jpg", "caption": "Figure 16: MMLU accuracy (all 57 subjects).", "description": "This figure shows the distribution of MMLU accuracy scores across 57 subjects for 15 different LLMs.  Each distribution represents the accuracy of a single LLM across all subjects.  The purpose is to visualize the performance consistency of each LLM across various subjects in MMLU. The x-axis represents the MMLU accuracy, and the y-axis shows the density of the distribution.", "section": "7 Analysis of prompt sensitivity on MMLU"}, {"figure_path": "jzkpwcj200/figures/figures_21_2.jpg", "caption": "Figure 1: Average estimation error for performance quantiles across 100 templates given a limited budget (in multiples of one-template MMLU evaluations).", "description": "This figure shows the average estimation error for different performance quantiles (5th, 25th, 50th, 75th, 95th) when estimating performance across 100 different prompt templates using a limited evaluation budget. The budget is represented as multiples of a single-prompt evaluation.  The plot illustrates that PromptEval can accurately estimate performance quantiles even with a limited budget, outperforming the baseline method, particularly for higher quantiles.", "section": "Abstract"}, {"figure_path": "jzkpwcj200/figures/figures_22_1.jpg", "caption": "Figure 3: Performance quantile estimation errors for varying quantiles (columns) and benchmarks (rows).", "description": "This figure presents the results of experiments evaluating the performance of different PromptEval variations in estimating performance quantiles across various benchmarks (MMLU, BBH, LMentry).  The plots show the average estimation errors for different quantiles (5th, 25th, 50th, 75th, 95th) across multiple tasks and LLMs, for varying evaluation budgets.  The error bars represent the average error across the LLMs.  The figure illustrates the effectiveness of PromptEval in accurately estimating performance quantiles, even with limited evaluation budgets, particularly for the median (50th percentile).", "section": "5 Assessing multi-prompt evaluation strategies"}]