[{"type": "text", "text": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guy Bar-Shalom\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yam Eitan\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science Technion - Israel Institute of Technology guy.b@campus.technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering Technion - Israel Institute of Technology yameitan1997@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Fabrizio Frasca ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haggai Maron ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering Technion - Israel Institute of Technology fabrizio.frasca.effe@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering Technion - Israel Institute of Technology NVIDIA Research haggaimaron@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing. ", "page_idx": 0}, {"type": "text", "text": "Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subgraph GNNs [4, 12, 39, 8, 27, 29, 38, 3] have recently emerged as a promising direction in graph neural network research, addressing the expressiveness limitations of Message Passing Neural Networks (MPNNs) [24, 35, 25]. In essence, a Subgraph GNN operates on a graph by transforming it into a collection of subgraphs, generated based on a specific selection policy. Examples of such policies include removing a single node from the original graph or simply marking a node without changing the graph\u2019s original connectivity [26]. The model then processes these subgraphs using an equivariant architecture, aggregates the derived representations, and makes graph- or nodelevel predictions. The growing popularity of Subgraph GNNs stems not only from their enhanced expressive capabilities over MPNNs but also from their impressive empirical results, as notably demonstrated on well-known molecular benchmarks [38, 12, 3]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Unfortunately, Subgraph GNNs are hindered by substantial computational costs as they necessitate message-passing operations across all subgraphs within the bag. Typically, the number of subgraphs is the number of nodes in the graph, $n$ \u2014 for bounded degree graphs, this results in a time complexity scaling quadratically $(\\mathcal{O}(n^{2}\\bar{)})$ , in contrast to the linear complexity of a standard MPNN. This significant computational burden makes Subgraph GNNs impractical for large graphs, hindering their applicability to important tasks and widely used datasets. To overcome this challenge, various studies have explored methodologies that process only a subset of subgraphs from the bag. These methods range from simple random sampling techniques [8, 4, 40, 3] to more advanced strategies that learn to select the most relevant subset of the bag to process [5, 20, 29]. However, while random sampling of subgraphs yields subpar performance, more sophisticated learnable selection strategies also have significant limitations. Primarily, they rely on training-time discrete sampling which complicates the optimization process, as evidenced by the high number of epochs required to train them [20, 5, 29]. As a result, these methods often allow only a very small bag size, yielding only modest performance improvements compared to random sampling and standard MPNNs. ", "page_idx": 1}, {"type": "text", "text": "Our approach. The goal of this paper is to devise a Subgraph GNN architecture that can flexibly generate and process variable-sized bags, and deliver strong experimental results while sidestepping intricate and lengthy training protocols. Specifically, our approach aims to overcome the common limitation of restricting usage to a very small set of subgraphs. ", "page_idx": 1}, {"type": "text", "text": "Our proposed method builds upon and extends an observation made by Bar-Shalom et al. [3], who draw an analogy between using Subgraph GNNs and performing message-passing operations over a larger \u201cproduct graph\u201d. Specifically, it was shown that when considering the maximally expressive (nodebased) Subgraph GNN suggested by $[38]^{2}$ , the bag of subgraphs and its update rules can be obtained by transforming a graph ", "page_idx": 1}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/0290840ea7efc56d0bc8d7d1bedd776ba71a7a649d107431e0205bf6892ad0f8.jpg", "img_caption": ["Figure 1: Product graph construction. Left: Transforming of the graph into a coarse graph; Right: Cartesian product of the coarsened graph with the original graph. The vertical axis corresponds to the subgraph dimension (super-nodes), while the horizontal axis corresponds to the node dimension (nodes). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "through the graph cartesian product of the original graph with itself, i.e., $G\\boxed{G}G$ , and then processing the resulting graph using a standard MPNN. In our approach, we propose to modify the first term of the product and replace it with a coarsened version of the original graph, denoted $\\mathcal T(G)$ , obtained by mapping nodes to super-nodes (e.g., by applying graph clustering, see Figure 1(left)), making the resulting product graph $\\tau(G)\\boxed{G}G$ significantly smaller. This construction is illustrated in Figure 1(right). This process effectively associates each subgraph \u2013 a row in Figure 1(right) \u2013 with a set of nodes produced by the coarsening function $\\tau$ . Different choices of $\\tau$ allow for both flexible bag sizes and a simple, meaningful selection of the subgraphs. ", "page_idx": 1}, {"type": "text", "text": "While performing message passing on $\\tau(G)\\boxed{G}G$ serves as the core update rule in our architecture, we augment our message passing operations with another set of operations derived from the symmetry structure of the resulting node feature tensor, which we call symmetry-based updates. Specifically, our node feature tensor is indexed by pairs $(S,v)$ where $S$ is a super-node and $v$ is an original node. Accordingly, $\\mathcal{X}$ is a $T\\times n\\times d$ tensor, where $d$ is the feature dimension, and $T$ is the number of super-nodes (a constant hyper-parameter). As super-nodes are sets of nodes, $\\mathcal{X}$ can also be viewed as a (very) sparse $2^{n}\\times n\\times d$ tensor where $2^{n}$ is the number of all subsets of the vertex set. Since the symmetric group $S_{n}$ acts naturally on this representation, we use it to develop symmetry based updates. ", "page_idx": 1}, {"type": "text", "text": "Interestingly, we find that this node feature tensor, $\\mathcal{X}$ , adheres to a specific set of symmetries, which, to the best of our knowledge, is yet unstudied in the context of machine learning: applying a permutation $\\sigma\\,\\in\\,S_{n}$ to the nodes in $S$ and to $v$ results in an equivalent representation of our node feature tensor. We formally define the symmetries of this object and characterize all the affine equivariant operations in this space. We incorporate these operations into our message-passing by encoding the parameter-sharing schemes [30] as additional edge features. These additional update rules significantly improve experimental results. We note that our symmetry analysis may be useful for processing bags derived from other high-order generation policies [29, 20] by treating tuples of nodes as sets. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Inspired by these symmetries and traditional binary-based [4] and shortest path-based [38] nodemarking strategies, we propose four natural marking strategies for our framework. Interestingly, unlike the full-bag scenario, they vary in expressiveness, with the shortest path-based technique being the most expressive. ", "page_idx": 2}, {"type": "text", "text": "The flexibility and effectiveness of our full framework are illustrated in Figure 2, depicting detailed experimental results on the popular ZINC-12K dataset [31]. Our method demonstrates a significant performance boost over baseline models in the small bag setting (for which they are designed), while achieving results that compare favourably to stateof-the-art Subgraph GNNs in the full bag setting. Additionally, we can obtain results in-between these two regimes. ", "page_idx": 2}, {"type": "text", "text": "Contributions. The main contributions of this paper are: (1) the development of a novel, flexible Subgraph GNN framework that enables meaningful construction and processing of bags of subgraphs of any size; (2) a ", "page_idx": 2}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/6ca30f90ea7a144623410c7aa5d03c07f647eebdbbb184bba61f159016fde8ed.jpg", "img_caption": ["Figure 2: The performance landscape of Subgraph GNNs with varying number of subgraphs: Our method leads in the lower bag-size set, outperforming other approaches in nearly all cases. Additionally, our method matches the performance of state-of-theart Subgraph GNNs in the full-bag setting. The full mean absolute error (MAE) scores along with standard deviations are available in Table 9 in the appendix. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "characterization of all affine invariant/equivariant layers defined on our node feature tensors; (3) a theoretical analysis of our framework, including the expressivity benefits of our node-marking strategy; and (4) a comprehensive experimental evaluation demonstrating the advantages of the new approach across both small and large bag sizes, achieving state-of-the-art results, often by a significant margin. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Subgraph GNNs. Subgraph GNNs [39, 8, 27, 4, 40, 26, 12, 29, 17, 38, 3] represent a graph as a collection of subgraphs, obtained by a predefined generation policy. For example, each subgraph can be generated by marking exactly one node in the original graph (see inset 3) \u2013 an approach commonly referred to as node marking [26]; this marked node is considered the root node in its subgraph. Several recent papers focused on scaling these methods to larger graphs, starting with basic random selection of subgraphs from the bag, and extending beyond with more sophisticated techniques that aim to learn how to select subgraphs. To elaborate, [5] introduced Policy-Learn (PL), an approach based on two models, where the first model predicts a distribution over the nodes of the original graph, and the second model processes bags of subgraphs sampled from this distribution. MAG-GNN [20] employs a similar approach utilizing Reinforcement Learning. ", "page_idx": 2}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/b85bb862c3a9ff8e3df9a9ec0dc91debebf7d9b3a9b7845018751278a0655987.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Similarly to our approach, this method permits high-order policies by associating subgraphs with tuples rather than individual nodes, allowing for the marking of several nodes within a subgraph. ", "page_idx": 2}, {"type": "text", "text": "However, as mentioned before, these approaches involve discrete sampling while training, making them very hard to train (1000-4000 epochs vs. ${\\sim}400$ epochs of state-of-the-art methods [3, 38] on the ZINC-12K dataset), and limiting their usage to very small bags. Finally, we mention another high-order method, OSAN, introduced by [29], which learns a distribution over tuples that represent subgraphs with multiple node markings. In contrast to these previous approaches, we suggest a simpler and more effective way to select subgraphs and also show how to leverage the resulting symmetry structure to augment our message-passing operations. ", "page_idx": 3}, {"type": "text", "text": "Symmetries in graph learning. Many previous works have analyzed and utilized the symmetry structure that arises from graph learning setups [22, 23, 18, 2]. Specifically relevant to our paper is the work of [22] that characterized basic equivariant linear layers for graphs, the work of [1] that characterizes equivariant maps for many other types of incidence tensors that arise in graph learning, and the works [4, 12] that leveraged group symmetries for designing Subgraph GNNs in a principled way. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation. Let $\\mathcal{G}$ be a family of undirected graphs, and consider a graph $G=(V,E)$ within this family. The adjacency matrix $A\\in\\mathbb{R}^{n\\times n}$ defines the connectivity of the graph4, while the feature matrix $X\\,\\in\\,\\bar{\\mathbb{R}^{n\\times d}}$ represents the node features. Here, $V$ and $E$ represent the sets of nodes and edges, respectively, with $|V|=n$ indicating the number of nodes. We use the notation $v_{1}\\sim_{A}v_{2}$ to denote that $v_{1}$ and $v_{2}$ are neighboring nodes according to the adjacency $A$ . Additionally, we define $[n]:=\\{1,2,\\dots n\\}$ , and $\\mathcal{P}([\\bar{n]})$ as the power set of $[n]$ . ", "page_idx": 3}, {"type": "text", "text": "Subgraph GNNs as graph products. In a recent work, [3] demonstrated that various types of update rules used by current Subgraph GNNs can be simulated by employing the Cartesian graph product between the original graph and another graph, and running standard message passing over that newly constructed product graph. Formally, the cartesian product of two graphs $G_{1}$ $\\boldsymbol{n}_{1}$ nodes) and $G_{2}$ $\\boldsymbol{n}_{2}$ nodes), denoted by $G_{1}\\boxed{G_{2}}$ , forms a graph with vertex set $V(G_{1})\\times V(G_{2})$ . Two vertices $(u_{1},u_{2})$ and $(v_{1},v_{2})$ are adjacent if either $u_{1}=v_{1}$ and $u_{2}$ is adjacent to $v_{2}$ in $G_{2}$ , or $u_{2}=v_{2}$ and $u_{1}$ is adjacent to $v_{1}$ in $G_{1}$ . We denote by $\\mathcal{A}\\in\\mathbb{R}^{n_{1}\\cdot n_{2}\\times n_{1}\\cdot n_{2}}$ and $\\mathcal{X}\\in\\mathbb{R}^{n_{1}\\cdot n_{2}\\times d}$ the adjacency and node feature matrices of the product graph; in general, we use calligraphic letters to denote the adjacency and feature matrices of product graphs, while capital English letters are used for those of the original graphs. In particular, for the graph cartesian product, $G_{1}\\boxed{G_{2}}$ , the following holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{G_{1}\\bigsqcup G_{2}}=A_{1}\\otimes I+I\\otimes A_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For a detailed definition of the cartesian product of graphs, please refer to Definition A.1. As a concrete example for the analogy between Subgraph GNNs and the Cartesian product of graphs, we refer to a result by [3], which states that the maximally expressive node-based Subgraph GNN architecture GNN-SSWL $^+$ [38], can be simulated by an MPNN on the Cartesian product of the original graph with itself, denoted as $G\\boxed{G}G$ . As we shall see, our framework utilizes a cartesian product of the original graph and a coarsened version of it, as illustrated in Figure 1 (right). ", "page_idx": 3}, {"type": "text", "text": "Equivariance. A function $L:U\\to W$ is called equivariant if it commutes with the group action. More formally, given a group element, $g\\in\\mathbb{G}$ , the function $L$ should satisfy $L(g\\cdot v)=g\\cdot\\bar{L}(v)$ for all $v\\in U$ and $g\\in\\mathbb{G}$ . $L$ is said to be invariant if $L(g\\cdot v)=L(v)$ . ", "page_idx": 3}, {"type": "text", "text": "4 Coarsening-based Subgraph GNN ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview. This section introduces the Coarsening-based Subgraph GNN (CS-GNN) framework. The main idea is to select and process subgraphs in a principled and flexible manner through the following approach: (1) coarsen the original graph via a coarsening function, $\\tau$ \u2013 see Figure 1(left); (2) Obtain the product graph \u2013 Figure 1(right) defined by the combination of two adjacencies, $A_{\\mathcal{T}(G)}$ (red edges), $A_{G}$ (grey edges), which arise from the graph Cartesian product operation (details follow); (3) leveraging the symmetry of this product graph to develop symmetry-based updates, described by $A_{\\mathrm{Equiv}}$ (this part is not visualized in Figure 1). The general update of our suggested layer takes the following form 4, ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{X}}^{t+1}(S,v)=f^{t}\\Big({\\mathcal{X}}(S,v)^{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the superscript t indicates the layer index. In what follows, we further elaborate on these three steps (in Sections 4.1 to 4.2). ", "page_idx": 4}, {"type": "text", "text": "We note that each connectivity in Equation (2) is processed using a distinct MPNN, and after stacking of those layers, we apply a pooling layer5 to obtain a graph representation; that is, $\\rho(\\mathcal{X}^{\\mathrm{T}})=$ $\\begin{array}{r}{\\mathrm{MLP}^{\\mathrm{T}}\\bigg(\\stackrel{}{\\sum}_{S}\\bigg(\\sum_{v=1}^{n}\\dot{\\chi}^{\\mathrm{T}}(S,v)\\bigg)\\bigg)}\\end{array}$ ; T denotes the final layer. ", "page_idx": 4}, {"type": "text", "text": "For more specific implementation details, we refer to Appendix F. ", "page_idx": 4}, {"type": "text", "text": "4.1 Construction of the coarse product graph ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As mentioned before, a maximally expressive node-based Subgraph GNN can be realized via the Cartesian product of the original graph with itself $G\\square G$ . In this work, we extend this concept by allowing the left operand in the product to be the coarsened version of $G$ , denoted as $\\tau(G)$ , as defined next. This idea is illustrated in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "Graph coarsening. Consider a graph $G=(V,E)$ with $n$ nodes and an adjacency matrix $A$ . Graph coarsening is defined by the function $\\tau:\\mathcal{G}\\rightarrow\\mathcal{G}$ , which maps $G$ to a new graph $\\dot{\\mathcal{T}}(G)=(V^{\\mathcal{T}},\\dot{E^{\\mathcal{T}}})$ with an adjacency matrix $A^{\\mathcal{T}}\\in\\mathbb{R}^{2^{n}\\times2^{n}}$ and a feature matrix $X^{\\mathcal{T}}\\in\\mathbb{R}^{2^{n}\\times d}$ . Here, $V^{\\mathcal{T}}$ , the vertex set of the new graph represents super-nodes \u2013 defined as subsets of $[n]$ . Additionally, we require that nodes in $\\bar{V}^{T}$ induce a partition over the nodes of the original graph6. The connectivity $\\dot{E}^{\\mathcal{T}}$ is extremely sparse and induced from the original graph\u2019s connectivity via the following rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA^{T}(S_{1},S_{2})={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}\\exists v\\in S_{1},\\exists v\\in S_{2}{\\mathrm{~s.t.~}}A(v,u)=1,}\\\\ {0}&{{\\mathrm{otherwise}},}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To clarify, in our running example (Figure 1), it holds that $A^{\\mathcal{T}}(\\{a,b,c,d\\},\\{e\\})\\;=\\;1$ , while $A^{\\mathcal{T}}(\\{e\\},\\dot{\\{f\\}})=0$ . For a more formal definition, refer to Definition A.3. ", "page_idx": 4}, {"type": "text", "text": "More specifically, our implementation of the graph coarsening function $\\tau$ employs spectral cluster$\\mathrm{ing^{7}}$ [33] to partition the graph into $T$ clusters, which in our framework controls the size of the bag. This results in a coarsened graph with fewer nodes and edges than $G$ . We highlight and stress that the space complexity of this sparse graph, $\\tau(G)$ , is upper bounded by that of the original graph $G$ (we do not store $2^{n}$ nodes). ", "page_idx": 4}, {"type": "text", "text": "Defining the (coarse) product graph $\\tau(G)\\sqcup G$ . We define the connectivity of the product graph, see Figure 1(right), by applying the cartesian product between the coarsened graph, $\\bar{\\mathcal{T}}(G)$ , and the original graph, $G$ . The product graph is denoted by $\\tau(G)\\boxed{G}G$ , and is represented by the matrices $\\bar{A_{\\mathcal{T}(G)}}\\bar{\\Omega_{G}}\\in\\bar{\\mathbb{R}}^{(2^{n}\\times n)\\times(\\bar{2}^{n}\\times n)}$ and $\\mathcal{X}\\in\\mathbb{R}^{2^{n}\\times n\\times d8}$ , where by recalling Equation (1), we obtain, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\mathcal{T}(G)\\Omega G}=\\overbrace{A^{\\mathcal{T}}\\otimes I}^{\\triangleq_{A_{\\mathcal{T}(G)}}}+\\overbrace{I\\otimes A}^{\\triangleq_{A_{G}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The connectivity in this product graph induces the horizontal $(\\mathcal{A}_{G})$ and vertical updates $(\\mathcal{A}_{\\mathcal{T}(G)})$ in Equation (2), visualized in Figure 1(right) via grey and red edges, respectively. ", "page_idx": 4}, {"type": "text", "text": "4.2 Symmetry-based updates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous subsection, we used a combination of a coarsening function and the graph Cartesian product to derive the two induced connectivities $\\mathcal{A}_{G},\\mathcal{A}_{\\mathcal{T}(G)}$ of our product graph. We use these connectivities to to perform message-passing on our product graph (see Equation (2)). ", "page_idx": 5}, {"type": "text", "text": "Inspired by recent literature on Subgraph GNNs [12, 3, 38], which incorporates and analyzes additional non-local updates arising from various symmetries (e.g., updating a node\u2019s representation via all nodes in its subgraphs), this section aims to identify potential new updates that can be utilized over our product graph. To that end, we study the symmetry structure of the node feature tensor in our product graph, ${\\mathcal{X}}(S,v)$ .The new updates described below will result in the third term in Equation (2), dubbed Symmetry-based updates $(\\mathcal{A}_{\\mathrm{Equiv}})$ . For better clarity in this derivation, we change the notation from nodes $(v)$ to indices $(i)$ . ", "page_idx": 5}, {"type": "text", "text": "4.2.1 Symmetries of our product graph ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since the order of nodes in the original graph $G$ is arbitrary, each layer in our architecture must exhibit equivariance to any induced changes in the product graph. This requires maintaining equivariance to permutations of nodes in both the original graph and its transformation $\\tau(G)$ . As a result, recalling that $\\mathcal{A}\\in\\mathbb{R}^{(2^{n}\\times n)\\times(2^{n}\\times n)}$ and $\\mathcal{X}\\in\\mathbb{R}^{2^{n}\\times n\\times d}$ represent the adjacency and feature matrices of the product graph, the symmetries of the product graph are defined by an action of the symmetric group $S_{n}$ . Formally, a permutation $\\sigma\\in S_{n}$ acts on the adjacency and feature matrices by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(\\sigma\\cdot A)\\bigl(S_{1},i_{1},S_{2},i_{2}\\bigr)=A\\bigl(\\sigma^{-1}(S_{1}),\\sigma^{-1}(i_{1}),\\sigma^{-1}(S_{2}),\\sigma^{-1}(i_{2})\\bigr),}}\\\\ {{(\\sigma\\cdot\\mathcal{X})(S,i)=\\mathcal{X}\\bigl(\\sigma^{-1}(S),\\sigma^{-1}(i)\\bigr),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we define the action of $\\sigma\\ \\in\\ S_{n}$ on a set ${\\cal S}\\;=\\;\\{i_{1},i_{2},...\\,,i_{k}\\}$ of size $k$ as: $\\sigma\\,\\cdot\\,S\\,\\,:=\\,\\,$ $\\{\\sigma^{-1}(i_{1}),\\sigma^{-1}(i_{2}),\\ldots,\\sigma^{-1}(i_{k})\\}:=\\sigma^{-1}(S)$ . ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Derivation of linear equivariant layers for the node feature tensor ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now characterize the linear equivariant layers with respect to the symmetry defined above, focusing on Equation (6). We adopt a similar notation to [22], and assume for simplicity that the number of feature channels is $d=1$ (extension to multiple features is straightforward [22]). In addition, our analysis considers the case where $V^{\\mathcal{T}}$ encompasses all potential super-nodes formed by subsets of $[n]$ (i.e we use the sparse coarsened adjacency9). ", "page_idx": 5}, {"type": "text", "text": "Our main tool is the characterization of linear equivariant layers for permutation symmetries as parameter-sharing schemes [34, 30, 22]. In a nutshell, this characterization states that the parameter vectors of the biases, invariant layers, and equivariant layers can be expressed as a learned weighted sum of basis tensors, where the basis tensors are indicators of the orbits induced by the group action on the respective index spaces. We focus here on presenting the final results and summarize them in Proposition 4.1 at the end of this subsection. Detailed discussion and derivations are available in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Equivariant bias and invariant layers. The bias vectors of the linear layers in our space are in $\\mathbb{R}^{\\bar{2^{n}}\\times n}$ . As shown in Figure 3(right), the set of orbits induced by the action of $S_{n}$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\mathcal{P}([n])\\times[n])/S_{n}:=\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\gamma^{k^{+}}$ corresponds to all pairs $(S,i)\\in\\mathcal{P}([n])\\times[n]$ with $|S|=k$ and $i\\not\\in S$ , and $\\gamma^{k^{-}}$ to all pairs with $|S|=k$ and $i\\in S$ . ", "page_idx": 5}, {"type": "text", "text": "As stated in [34, 30, 22], the tensor set $\\left\\{\\mathbf{B}_{S,i}^{\\gamma}\\right\\}_{\\gamma\\in({\\mathcal{P}}([n])\\times[n])/S_{n}}$ where: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{B}_{S,i}^{\\gamma}=\\left\\{{1,\\quad\\mathrm{if~}(S,i)\\in\\gamma};\\begin{array}{l l}{}\\\\ {}\\\\ {}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "are a basis of the space of bias vectors of the invariant linear layers induced by the action of $S_{n}$ . ", "page_idx": 5}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/5f872aeb02198eedbd2c3b41827a2ee1e411d46bd998a1be14d35b2f6dc30321.jpg", "img_caption": ["Figure 3: Visualization via heatmaps (different colors correspond to different parameters) of the parameter-sharing scheme determined by symmetries for a graph with $n=6$ nodes, zooming-in on the block which corresponds to sets of size two. Left: Visualization of the weight matrix for the equivariant basis BS\u03931,i1;S2,i2 (a total of 35 parameters in the block). Right: Visualization of the bias vector for the invariant basis $\\mathbf{B}_{S,i}^{\\gamma}$ (a total of 2 parameters in the block). Symmetry-based updates reduce parameters more effectively than previously proposed linear equivariant layers by treating indices as unordered tuples (see Appendix E.3 for a discussion). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Weight matrices. Following similar reasoning, consider elements $(S_{1},i_{1},S_{2},i_{2})\\;\\in\\;({\\mathcal P}([n])\\;\\times$ $[n]\\ \\bar{\\times}\\ \\mathcal{P}([n])\\times[n])$ . In Appendix $\\boldsymbol{\\mathrm E}$ we characterize the orbits of $S_{n}$ in this space as a partition in which each partition set is defined according to six conditions. Some of these conditions include the sizes of $S_{1},\\;S_{2}$ and $S_{1}\\cap S_{2}$ , which remain invariant under permutations. Given an orbit, $\\Gamma\\in(\\mathcal{P}([n])\\times[n]\\times\\mathcal{P}([n])\\times[n])/S_{n}$ , we define a basis tensor, $\\mathbf{B}^{\\bar{\\Gamma}}\\in\\mathbb{R}^{2^{n}\\times n\\times2^{n}\\times n}$ by setting: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{B}_{S_{1},i_{1};S_{2},i_{2}}^{\\Gamma}=\\left\\{1,\\begin{array}{r l}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A visualization of the two basis vectors in Equations (8) and (9), is available in Figure 3. The following (informal) proposition summarizes the results in this section (the proof is given in Appendix G), ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1 (Basis of Invariant (Equivariant) Layers). The tensors ${\\bf B}^{\\gamma}\\ ({\\bf B}^{\\Gamma})$ in Equation (8) (Equation (9)) form an orthogonal basis (in the standard inner product) of the invariant layers and biases (Equivariant layers \u2013 weight matrix) . ", "page_idx": 6}, {"type": "text", "text": "4.2.3 Incorporating symmetry-based updates in our framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the previous subsection, we derived all possible linear invariant and equivariant operations that respect the symmetries of our product graph. We now use this derivation to define the symmetry-based updates in Equation (2), which correspond to the construction of $A_{\\mathrm{Equiv}}$ and the application of an MPNN. ", "page_idx": 6}, {"type": "text", "text": "To begin, we note that any linear equivariant layer can be realized through an MPNN [13] applied to a fully connected graph with appropriate edge features. ", "page_idx": 6}, {"type": "text", "text": "This is formally stated in Lemma F.1, the main idea is to encode the parameters on the edges of this graph (see visualization inset). Thus, the natural ", "page_idx": 6}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/2eabdd8f481c322b89663b4d7bd5339d491d157632396713c9e5f7d96c2aa3ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "construction of $A_{\\mathrm{Equiv}}$ corresponds to a fully connected graph, with appropriate edge features derived from the parameter-sharing scheme we have developed. ", "page_idx": 6}, {"type": "text", "text": "However, one of our main goals and guidelines in developing our flexible framework is to maintain efficiency, and to align with the (node-based) maximally expressive GNN, namely GNN-SSWL $^+$ [38, 3], for the case of a trivial coarsening function, ${\\mathcal{T}}(G)\\,=\\,G$ (which correspond to the full-bag setting). To achieve this, we opt for a sparser choice by using only a subset of the basis vectors (defined in Equation (9)) to construct $A_{\\mathrm{Equiv}}$ . Specifically, the matrix $A_{\\mathrm{Equiv}}$ corresponding to the chosen subset of basis vectors is visualized inset \u2013 the parameter-sharing scheme is represented by edges with matching colors. To clarify, the nodes $(S,v)$ that satisfy $v\\,\\in\\,S$ \u201csend messages\u201d (i.e., broadcast their representation) to all the nodes $(S^{\\prime},v^{\\prime})$ such that $\\boldsymbol{v}\\,=\\,\\boldsymbol{v}^{\\prime}$ . A more formal discussion regarding our implementation of those symmetry based updates is given in Appendix F.4. ", "page_idx": 6}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/f595dea89e5ec23795214c68a51080fae0d73164ec29b19073c9bd3165a2936b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Maintaining sparsity. While the updates above are defined over the sparse representation of the coarse product graph, in practice we use its dense representation, treating it as a graph over the set of nodes $V\\stackrel{\\smile}{\\times}\\dot{V}^{T}$ , which requires space complexity $\\mathcal{O}(T\\cdot|V|)$ . The update rules above are adapted to this representation simply by masking all nodes $(S,v)$ in the sparse representation such that $S\\notin V^{\\mathcal{T}}$ . We note the models using the resulting update rule remain invariant to the action of $S_{n}$ . See discussion in [1]. ", "page_idx": 7}, {"type": "text", "text": "4.3 Marking Strategies and Theoretical Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "One of the key components of subgraph architectures is their marking strategy. Two widely used approaches in node-based subgraph architectures are binary-based node marking [4] and distancebased marking [38], which were proven to be equally expressive in the full-bag setup [38]. Empirically, distance-based marking has been demonstrated to outperform other strategies across several standard benchmarks. In this section, our aim is to develop and theoretically justify an appropriate marking strategy, specifically tailored to the structure of our product graph. We present and discuss here our main results, and refer to Appendix C for a more formal discussion. ", "page_idx": 7}, {"type": "text", "text": "Building on existing marking strategies and considering the unique structure of our product graph, we propose two natural extensions to both the binary node marking [4] and distance-based marking strategies [38]. Extending binary node marking, we first suggest Simple Marking $(\\pi_{S})$ , where an element $(S,v)$ is assigned a binary feature that indicates whether node $v$ belongs to subgraph $S$ $(v\\in S)$ . The second extension, Node + Size Marking $(\\pi_{S S})$ , builds on the simple marking by assigning an additional feature that encodes the size of the super-node $S$ . ", "page_idx": 7}, {"type": "text", "text": "For distance-based strategies, we propose Minimum Distance $(\\pi_{M D})$ , where each element $(S,v)$ is assigned the smallest (minimal) shortest path distance (SPD) from node $v$ to any node $u\\in S$ . Finally, Learned Distance Function $\\left(\\pi_{L D}\\right)$ extends this further by assigning to each element $(S,v)$ the output of a permutation-invariant learned function, which takes the set of SPDs between node $v$ and the nodes in $S$ as input. ", "page_idx": 7}, {"type": "text", "text": "Surprisingly, unlike the node-based full-bag case, we find that these marking strategies are not all equally expressive. We conveniently gather the first three strategies as $\\Pi=\\{\\pi_{S},\\pi_{S S},\\pi_{M D}\\}$ and summarize the relation between all variants as follows: ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2 (Informal \u2013 Expressivity of marking strategies.). (i) Strategies in $\\Pi$ are all equally expressive, independently of the transformation function $\\tau$ . (ii) The strategy $\\pi_{L D}$ is at least as expressive as strategies in \u03a0. Additionally, there exists transformation functions s.t. it is strictly more expressive than all of them. ", "page_idx": 7}, {"type": "text", "text": "The above is formally stated in Propositions C.1 and C.2, and more thoroughly discussed in Appendix C. In light of the above proposition, we instatiate the learned distance function $\\pi_{L D}$ strategy when implementing our model, as follows, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{X}_{S,v}\\gets\\sum_{u\\in S}z_{d_{G}(v,u)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $d_{G}(v,u)$ denotes the shortest path distance between nodes $v$ and $u$ in the original $G^{10}$ . ", "page_idx": 7}, {"type": "text", "text": "Coarsening Function and Expressivity. We investigate whether our CS-GNN framework offers more expressiveness compared to directly integrating information between the coarsened graph and the original graph. ", "page_idx": 7}, {"type": "text", "text": "The two propositions below illustrate that a simple, straight forward integration of the coarsen graph with the original graph (this integration is referred to as the sum graph \u2013 formally defined in Definition D.2), and further processing it via standard message-passing, results in a less expressive architecture. Furthermore, when certain coarsening functions are employed within the CS-GNN framework, our resulting architecture becomes strictly more expressive than conventional node-based subgraph GNNs. These results suggest that the interplay between the coarsening function and the subgraph layers we have developed enhances the model\u2019s overall performance. We summarize this informally below and provide a more formal discussion in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Proposition 4.3 (Informal \u2013 CS-GNN goes beyond coarsening). For any transformation function $\\tau$ , CS-GNN can implement message-passing on the sum graph, hence being at least as expressive. Also, there exist transformations $\\tau$ \u2019s s.t. CS-GNN is strictly more expressive than that. ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.4 (Informal \u2013 CS-GNN vs node based subgraphs). There exist transformations $\\tau$ \u2019s s.t.   \nour CS-GNN model using $\\tau$ as its coarsening function is strictly more expressive than GNN-SSWL $^{+}$ . ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We experimented extensively over seven different datasets to answer the following questions: $(Q I)$ Can CS-GNN outperform efficient Subgraph GNNs operating on small bags? (Q2) Does the additional symmetry-based updates boost performance? (Q3) Does CS-GNN offer a good solution in settings where full-bag Subgraph GNNs cannot be applied? $(Q4)$ Does CS-GNN in the full-bag setting validate its theory and match state-of-the-art full-bag Subgraph GNNs? ", "page_idx": 8}, {"type": "text", "text": "In the following sections, we present our main results and refer to Appendix F for additional experiments and details. ", "page_idx": 8}, {"type": "text", "text": "Baselines. For each task, we include several baselines. The RANDOM baseline corresponds to random subgraph selection. We report the best performing random baseline from all prior work [5, 20, 29, 3]. The other two (nonrandom) baselines are: (1) LEARNED [5, 20, 29], which represents methods that learn the specific subgraphs to be used; and (2) FULL [38, 3], which corresponds to full-bag Subgraph GNNs. ", "page_idx": 8}, {"type": "text", "text": "ZINC. We experimented with both the ZINC- $12\\mathrm{K}$ and ZINC-FULL datasets [31, 14, 10], adhering to a $500k$ parameter budget as prescribed. As shown in Table 1, ", "page_idx": 8}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/80a1ddcd67f02ce1e5ea751626fb36596dacf694c0b982789bfe40312bbb1664.jpg", "table_caption": ["Table 1: Results on ZINC-12K dataset. Top two results are reported as First and Second. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "CS-GNN outperforms all efficient baselines by a significant margin, with at least a $+0.008$ MAE improvement for bag sizes $T\\in\\{3,4,5\\}$ . Additionally, in the full-bag setting, our method recovers state-of-the-art results. The results for ZINC-FULL are available in Table 8 in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "OGB. We tested our framework on several datasets from the OGB benchmark collection [16]. Table 4 shows the performance of our method compared to both efficient and full-bag Subgraph GNNs. Our CS-GNN outperforms all baselines across all datasets for bag sizes $T\\in\\{2,5\\}$ , except for the MOLHIV dataset with $T=2$ , where PL achieves the best results and our method ranks second. In the full-bag setting, CS-GNN is slightly outperformed by the top-performing Subgraph GNNs but still offers comparable results. ", "page_idx": 8}, {"type": "text", "text": "Peptides. We experimented on the PEPTIDESFUNC and PEPTIDES-STRUCT datasets [9] \u2013 which full-bag Subgraph GNNs already struggle to process \u2013 evaluating CS-GNN\u2019s ability to scale to larger graphs. The results are summarized in Table 2. CS-GNN outperforms all MPNN variants, even when incorporating structural encodings such as GATEDGCN+RWSE. Additionally, our method surpasses the ran$\\mathrm{dom}^{11}$ baseline on both datasets. ", "page_idx": 8}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/ac1cf486b76ddd67c1d5c73a14ba3d57775df36c330f364ae92dcd5b345cc077.jpg", "table_caption": ["Table 2: Results on PEPTIDES dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/a498a38b826bcae770f49bcbdf233538772f0f27c9234ac7cf969b3213b058f1.jpg", "table_caption": ["Table 3: Ablation study. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Ablation study \u2013 symmetry-based updates. We assessed the impact of the symmetry-based update on the performance of CS-GNN. Specifically, we ask, do the symmetry-based updates significantly contribute to the performance of CS-GNN? To evaluate this, we conducted several experiments using the ZINC-12K dataset across various bag sizes, $T\\,\\in\\,\\{2,3,\\bar{4},5\\}$ , comparing CS-GNN with and without the symmetry-based update. The results are summarized in Table 3. It is clear that the symmetry-based updates play a key role in the performance of CS-GNN. For a bag size of $T=2$ , the inclusion of the symmetry-based update improves the MAE by a significant 0.034. For other bag sizes, the improvements range from 0.005 to 0.016, clearly demonstrating the benefits of including the symmetry-based updates. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Discussion. In what follows, we address research questions Q1 to Q4. (A1) Tables 1, 2 and 4 clearly demonstrate that we outperform efficient Subgraph GNNs (which operate on a small bag) in 10 out of 12 dataset and bag size combinations. (A2) Our ablation study on the ZINC-12K dataset, as shown in Table 3, clearly demonstrates the beneftis of the symmetry-based updates across all the considered bag sizes. (A3) Table 2 demonstrates that CS-GNN provides an effective solution when the full-bag setting cannot be applied, outperforming all baselines. (A4) On the ZINC-12K dataset (see Table 1), CS-GNN achieves state-of-the-art results compared to Subgraph GNNs. On the OGBG datasets (see Table 4), our performance ", "page_idx": 9}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/414452dff9d75fa785493e3a558de295e2d9ef34471f04e5625e2fe67009c847.jpg", "table_caption": ["Table 4: Results on OGB datasets. The top two results are reported as First and Second. "], "table_footnote": ["is comparable to these top-performing Subgraph GNNs. "], "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we employed graph coarsenings and leveraged the insightful connection between Subgraph GNNs and the graph Cartesian product to devise CS-GNN, a novel and flexible Subgraph GNN that can effectively generate and process any desired bag size. Several directions for future research remain open. Firstly, we experimented with spectral clustering based coarsening, but other strategies are possible and are interesting to explore. Secondly, in our symmetry-based updates, we have only considered a portion of the whole equivariant basis we derived: evaluating the impact of other basis elements deserve further attention, both theoretically and in practice. Finally, whether Higher-Order Subgraph GNNs can benefti from our developed parameter-sharing scheme remains an intriguing open question. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our method operates over a product graph. Although we provide control over the size of this product graph, achieving better performance requires a larger bag size. This can become a complexity bottleneck, particularly when the original graph is large. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors are grateful to Beatrice Bevilacqua, for helpful discussions, and constructive conversations about the experiments. HM is the Robert J. Shillman Fellow and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and an equipment grant (ISF 532/23). FF is funded by the Andrew and Erna Finci Viterbi Post-Doctoral Fellowship; FF partially performed this work while visiting the Machine Learning Research Unit at TU Wien led by Prof. Thomas G\u00e4rtner. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marjan Albooyeh, Daniele Bertolini, and Siamak Ravanbakhsh. Incidence networks for geometric deep learning. arXiv preprint arXiv:1905.11460, 2019.   \n[2] Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. arXiv preprint arXiv:2006.15646, 2020.   \n[3] Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron. Subgraphormer: Unifying subgraph GNNs and graph transformers via graph products. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id $\\cdot$ 6djDWVTUEq.   \n[4] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. International Conference on Learning Representations, 2022. [5] Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, and Haggai Maron. Efficient subgraph gnns by learning effective selection policies. International Conference on Learning Representations, 2024.   \n[6] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www. wandb.com/. Software available from wandb.com.   \n[7] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.   \n[8] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[9] Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326\u201322340, 2022.   \n[10] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):1\u201348, 2023.   \n[11] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.   \n[12] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. Advances in Neural Information Processing Systems, 35:31376\u201331390, 2022.   \n[13] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \n[14] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[15] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n[16] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[17] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with $\\mathrm{i^{2}}$ -gnns. In The Eleventh International Conference on Learning Representations, 2022.   \n[18] Nicolas Keriven and Gabriel Peyr\u00e9. Universal invariant and equivariant graph neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \n[19] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2016.   \n[20] Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang. Maggnn: Reinforcement learning boosted graph neural network. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, 2021.   \n[22] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. arXiv preprint arXiv:1812.09902, 2018.   \n[23] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. Advances in neural information processing systems, 32, 2019.   \n[24] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 4602\u20134609, 2019.   \n[25] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. arXiv preprint arXiv:2112.09992, 2021.   \n[26] P\u00e1l Andr\u00e1s Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In International Conference on Machine Learning, pages 17323\u201317345. PMLR, 2022.   \n[27] P\u00e1l Andr\u00e1s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. Advances in Neural Information Processing Systems, 34:21997\u201322009, 2021.   \n[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[29] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. Advances in Neural Information Processing Systems, 35: 21030\u201321045, 2022.   \n[30] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing. In International conference on machine learning, pages 2892\u20132901. PMLR, 2017.   \n[31] Teague Sterling and John J Irwin. Zinc 15\u2013ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324\u20132337, 2015.   \n[32] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. International Conference on Learning Representations, 2017.   \n[33] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395\u2013416, 2007.   \n[34] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete applied mathematics, 69(1-2):33\u201360, 1996.   \n[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? International Conference on Learning Representations, 2018.   \n[36] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. Advances in Neural Information Processing Systems, 32, 2019.   \n[37] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017.   \n[38] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. International Conference on Machine Learning, 2023.   \n[39] Muhan Zhang and Pan Li. Nested graph neural networks. In Advances in Neural Information Processing Systems, volume 34, 2021.   \n[40] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 In Appendix A, we provide some basic definitions that will be used in later sections of the paper.   \n\u2022 In Appendix B we discuss some theoretical aspects of our model implementation, and its relation to Equation (2).   \n\u2022 In Appendix C we define four natural general node marking policies and analyze their theoretical effects on our model, as well as their relation to some node-based node marking policies. Finally, we provide a principled derivation of one of these policies using the natural symmetry of our base object.   \n\u2022 In Appendix D.1 we compare our model to node-based subgraph GNNs, which are the most widely used variant of subgraph GNNs. Additionally, we demonstrate that different choices of coarsening functions can recover various existing subgraph GNN designs.   \n\u2022 In Appendix D.2 we demonstrate how our model can leverage the information provided by the coarsening function in an effective way, comparing its expressivity to a natural baseline which also leverages the coarsening function. We show that for all coarsening functions, we are at least as expressive as the baseline and that for some coarsening functions, our model is strictly more expressive.   \n\u2022 In Appendix E we delve deeper into the characterization of all linear maps $L:\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}\\rightarrow$ RP([n])\u00d7[n] that are equivariant to the action of the symmetric group.   \n\u2022 In Appendix $\\boldsymbol{\\mathrm{F}}$ we provide experimental details to reproduce the results in Section 5, as well as a comprehensive set of ablation studies.   \n\u2022 In Appendix G we provide detailed proofs to all propositions in this paper. ", "page_idx": 13}, {"type": "text", "text": "A Basic Definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We devote this section to formally defining the key concepts of this paper, as well as introducing new useful notation. We start by defining the two principle components of our pipeline, the cartesian product graph and the coarsening function: ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Cartesian Product Graph). Given two graphs $G_{1}$ and $G_{2}$ , their Cartesian product $G_{1}\\boxed{G_{2}}$ is defined as: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The vertex set $V(G_{1}\\varOmega G_{2})=V(G_{1})\\times V(G_{2}).$ .   \n\u2022 Vertices $(u_{1},u_{2})$ and $(v_{1},v_{2})$ in $G_{1}\\boxed{G_{2}}$ are adjacent $i f$ : \u2013 $u_{1}=v_{1}$ and $u_{2}$ is adjacent to $v_{2}$ in $G_{2}$ , or \u2013 $u_{2}=v_{2}$ and $u_{1}$ is adjacent to $v_{1}$ in $G_{1}$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Coarsening Function). A Coarsening function $\\tau(\\cdot)$ is defined as a function that, given a graph $G=(V,E)$ with vertex set $V=[n]$ and adjacency matrix $A\\in\\mathbb{R}^{n\\times n}$ , takes $A$ as input and returns a set of \"super-nodes\" $T(A)\\subseteq{\\mathcal{P}}{\\dot{(}}[{\\dot{n}}])$ . The function $\\tau(\\cdot)$ is considered equivariant $i f,$ for any permutation $\\sigma\\in S_{n}$ , the following condition holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(\\sigma\\cdot A)=\\sigma\\cdot{\\mathcal{T}}(A).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\sigma\\cdot A$ , and $\\sigma\\cdot\\mathcal{T}(A)$ represent the group action of the symmetric group $S_{n}$ on $\\mathbb{R}^{n\\times n}$ , and $\\mathcal P([n])$ respectively. ", "page_idx": 13}, {"type": "text", "text": "A coarsening function allows us to naturally define a graph structure on the \"super-nodes\" obtained from a given graph in the following way: ", "page_idx": 13}, {"type": "text", "text": "Definition A.3 (Coarsened Graph). Given a coarsening function $\\tau(\\cdot)$ and a graph $G=(V,E)$ with vertex set $V=\\left\\lceil n\\right\\rceil$ , adjacency matrix $A\\in\\mathbb{R}^{n\\times n}$ , we abuse notation and define the coarsened graph $\\mathcal{T}(G)=(V^{\\mathcal{T}},\\dot{E}^{\\mathcal{T}})$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nV^{\\mathcal{T}}={\\mathcal{T}}(A)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\nE^{T}=\\{\\{S,S^{\\prime}\\}\\mid S,S^{\\prime}\\in{\\mathcal{T}}(A),\\;\\exists i\\in S,i^{\\prime}\\in S^{\\prime}\\;s.t.\\;A_{i,i^{\\prime}}=1\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The adjacency matrix of the coarsened graph can be expressed in two ways. The dense representation $A_{d e n s e}^{\\mathcal{T}}\\in\\mathbb{R}^{|V^{\\mathcal{T}}|\\times|V^{\\mathcal{T}}|}$ is defined by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nA_{d e n s e}^{\\mathcal{T}}(S,S^{\\prime})=\\left\\{1\\quad\\{S,S^{\\prime}\\}\\in E^{\\mathcal{T}}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The sparse representation $A_{s p a r s e}^{\\mathcal{T}}\\in\\mathbb{R}^{\\mathcal{P}([n])\\times\\mathcal{P}([n])}$ is defined by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nA_{s p a r s e}^{\\mathcal{T}}(S,S^{\\prime})=\\left\\{{1\\atop0}\\ \\ {S,S^{\\prime}\\in V^{T},\\{S,S^{\\prime}\\}\\in E^{T}}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We note that if the coarsened graph $\\mathcal T(G)$ has a corresponding node feature map $\\mathcal{X}:V^{\\mathcal{T}}\\to\\mathbb{R}^{d}$ , it also has sparse and dense vector representations defined similarly. Though the dense representation seems more natural, the sparse representation is also useful, as the symmetric group $S_{n}$ acts on it by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma\\cdot A_{\\mathrm{sparse}}^{\\mathcal{T}}(S,S^{\\prime})=A_{\\mathrm{sparse}}^{\\mathcal{T}}(\\sigma^{-1}(S),\\sigma^{-1}(S^{\\prime})).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When the type of representation is clear from context, we abuse notation and write $A^{\\mathcal{T}}$ . Note also that in the above discussion, we have used the term \"node feature map\". Throughout this paper, in order to denote the node features of a graph $G=(V,E)$ with $|V|=n$ , we use both the vector representation $X\\in\\mathbb{R}^{n\\times d}$ and the map representation $\\mathcal{X}:V\\rightarrow\\mathbb{R}^{d}$ interchangeably. Now, recalling that our pipeline is defined to create and update a node feature map ${\\mathcal{X}}(S,v)$ supported on the nodes of the product graph $G\\square\\tau(G)$ , we define a general node marking policy, the following way: ", "page_idx": 14}, {"type": "text", "text": "Definition A.4 (General Node Marking Policy). $A$ general node marking policy $\\pi(\\cdot,\\cdot)$ , is a function which takes as input a graph $G=(V,E)$ , and a coarsening function $\\tau(\\cdot)$ , and returns a node feature map $\\mathcal{X}:V^{T}\\times V\\rightarrow\\mathcal{R}^{d}$ . ", "page_idx": 14}, {"type": "text", "text": "In Appendix C We provide four different node marking policies, and analyze the effect on our pipeline. We now move on to define the general way in which we update a given node feature map on the product graph. ", "page_idx": 14}, {"type": "text", "text": "Definition A.5 (General CS-GNNLayer Update). Given a graph $G\\,=\\,(V,E)$ and a coarsening function $\\tau(\\cdot)$ , let $\\mathcal{X}^{t}(S,v):V\\times V^{T}\\rightarrow\\mathcal{R}^{d}$ denote the node feature map at layer $t$ . The general CS-GNNlayer update is defined by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal X}^{t+1}(S,v)=f^{t}\\bigg({\\mathcal X}^{t}(S,v),}\\\\ &{\\qquad\\qquad\\qquad\\quad\\,a g_{1}^{t}\\mathbb{f}\\big({\\mathcal X}^{t}(S,v^{\\prime}),e_{v,v^{\\prime}}\\big)\\mid v^{\\prime}\\sim_{G}v\\mathbb{j},}\\\\ &{\\qquad\\qquad\\quad\\,a g g_{2}^{t}\\mathbb{f}\\big({\\mathcal X}^{t}(S^{\\prime},v),\\tilde{e}_{S,S^{\\prime}}\\big)\\mid{\\mathcal S}^{\\prime}\\sim_{G}{\\mathcal T}\\;S\\mathbb{j},}\\\\ &{\\qquad\\quad\\,a g g_{3}^{t}\\mathbb{f}\\big({\\mathcal X}^{t}(S^{\\prime},v),z(S,v,S^{\\prime},v)\\big)\\mid S^{\\prime}\\in V^{T}{\\mathcal S}.t.\\ v\\in S^{\\prime}\\mathbb{j},}\\\\ &{\\qquad\\qquad\\quad\\,a g g_{4}^{t}\\mathbb{f}\\big({\\mathcal X}^{t}(S,v^{\\prime}),z(S,v,S,v^{\\prime})\\big)\\mid v^{\\prime}\\in V s.t.\\;v^{\\prime}\\in S\\mathbb{j}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $f^{t}$ is an arbitrary (parameterized) continuous function, $a g g_{i}^{t}$ , $i\\,=\\,1,\\ldots4$ are learnable permutation invariant aggregation functions, $e_{v,v^{\\prime}},\\tilde{e}_{S,S^{\\prime}}$ are the (optional) edge features of $G$ and $\\tau(G)$ respectively and the function $z:\\mathcal{P}([n])\\times[n]\\times\\mathcal{P}([n])\\times[n]\\to\\mathbb{R}^{d}$ maps each tuple of indices $\\mathbf{v}=(S,v,S^{\\prime},v^{\\prime})$ to a vector uniquely encoding the orbit of $\\mathbf{v}$ under the action of $S_{n}$ as described in 73. ", "page_idx": 14}, {"type": "text", "text": "We note that for brevity, the notation used in the main body of the paper omits the aggregation functions $\\arg_{1}^{t},\\dots,\\arg_{4}^{t}$ and the edge features from the formulation of some of the layer updates. However, we explicitly state each component of the update, as we heavily utilize them in later proofs. We also note that this update is different than the general layer update presented in Equation (2), as it doesn\u2019t use all global updates characterized in 9. The reason for this is that some of the global updates have an asymptotic runtime of $\\tilde{\\mathcal{O}}(n^{2})$ where $n$ is the number of nodes in the input graph. As our goal was to create models that improve on the scalability of standard subgraph GNNs which have an asymptotic runtime of $\\tilde{\\mathcal{O}}(n^{2})$ , We decided to discard some of the general global updates and keep only the ones that are induced by the last two entries in equation 15 which all have a linear runtime. After a stacking of the layers in Equation (15), we employ the following pooling procedure on the final node feature map $\\chi^{\\bar{T}}$ : ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho(\\mathcal{X}^{T})=\\mathbb{M}\\mathrm{LP}_{2}\\left(\\sum_{S\\in V^{T}}\\left(\\mathbb{M}\\mathrm{LP}_{1}\\big(\\sum_{v\\in V}\\mathcal{X}^{T}(S,v)\\big)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we define the set of all functions that can be expressed by our model: ", "page_idx": 15}, {"type": "text", "text": "Definition A.6 (Expressivity of Family of Graph Functions). Let $\\mathcal{F}$ be a family of graph functions, we say that $\\mathcal{F}$ can express a graph function $g(\\cdot)$ if for every finite family of graphs $\\mathcal{G}$ there exists a function $f\\in\\mathcal F$ such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(G)=g(G)\\quad\\forall G\\in{\\mathcal{G}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $\\mathcal{G}$ is a finite family of graphs if all possible values of node/edge features of the graphs in $\\mathcal{G}$ form a finite set, and the maximal size of the graphs within $\\mathcal{G}$ is bounded. ", "page_idx": 15}, {"type": "text", "text": "Definition A.7 (Family of Functions Expressed By CS-GNN). Let $\\pi$ be a general node marking policy and $\\tau$ be a coarsening function. Define $S(\\tau,\\pi)$ to be the family of graph functions, which when given input graph $G=(V,E)$ , first compute $\\mathcal{X}^{0}(S,v)$ using $\\pi(G,{\\mathcal{T}})$ , then update this node feature map by stacking $T$ layers of the form $^{l5}$ , and finally pooling $\\mathcal{X}^{0}(S,v)$ using equation $I6$ . We define $C S{-}G N N({\\mathcal{T}},{\\pi})$ to be the set of all functions that can be expressed by ${\\cal S}(\\tau,\\pi)$ . ", "page_idx": 15}, {"type": "text", "text": "B Theoretical Validation of Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide implementation details of our model and prove that they enable us to recover the conceptual framework of the model discussed thus far. First, we note that in Section 4.2, we characterized all equivariant linear maps $L:\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}\\rightarrow\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}$ in order to incorporate them into our layer update. Given the high dimensionality of the space of all such linear maps, and in order to save parameters, we demonstrate that it is possible to integrate these layers into our layer update by adding edge features to a standard MPNN model. This is formalized in the following proposition: ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1 (Parameter Sharing as MPNN). Let $B_{1},\\ldots B_{k}:\\mathbb{R}^{n\\times n}$ be orthogonal matrices with entries restricted to 0 or $^{\\,l}$ , and let $W_{1},\\dots W_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}}$ denote a sequence of weight matrices. Define $\\textstyle B_{+}=\\sum_{i=1}^{k}B_{i}$ and choose $z_{1},\\dots z_{k}\\in\\mathbb{R}^{d^{*}}$ to be a set of unique vectors representing an encoding of the  index set. The function that represents an update via parameter sharing: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(X)=\\sum_{i=1}^{k}B_{i}X W_{i},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "can be implemented on any finite family of graphs $\\mathcal{G}$ , by $a$ stack of MPNN layers of the following form [13], ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{m_{v}^{l}=\\displaystyle\\sum_{u\\in N_{B_{+}}(v)}M^{l}(X_{u}^{l},e_{u,v}),}}\\\\ {{\\displaystyle X_{v}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $U^{l},M^{l}$ are multilayer perceptrons $(M L P s,$ . The inputs to this MPNN are the adjacency matrix $B_{+}$ , node feature vector $X$ , and edge features \u2013 the feature of edge $(u,v)$ is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne_{u,v}=\\sum_{i=1}^{k}z_{i}\\cdot B_{i}(u,v).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $B_{i}(u,v)$ denotes the $(u,v)$ entry to matrix $B_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "The proof is given in Appendix G. The analysis in Section 4.2 demonstrates that the basis of the space of all equivariant linear maps $L:\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}\\rightarrow\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}$ satisfies the conditions of Lemma F.1. Additionally, we notice that some of the equivariant linear functions have an asymptotic runtime of $\\tilde{\\mathcal{O}}(n^{2})$ where $n$ is the number of nodes in the input graph. As our main goal is to construct a more scalable alternative to node-based subgraph GNNs, which also have a runtime of $\\tilde{\\mathcal{O}}(n^{2})$ , we limit ourselves to a subset of the basis for which all maps run in linear time. This is implemented by adding edge features to the adjacency matrices $A_{P_{1}}$ and $A_{P_{2}}$ , defined later in this section. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "We now move on to discussing our specific implementation of the general layer update from Definition A.5. ", "page_idx": 16}, {"type": "text", "text": "Given a graph $G=(V,E)$ and a coarsening function $\\tau$ , we aim to implement this general layer update by combining several standard message passing updates on the product graph $G\\square T(\\stackrel{\\cdot}{G})$ . In the next two definitions, we define the adjacency matrices supported on the node set $V\\times\\dot{V}^{T}$ , which serve as the foundation for these message passing procedures, and formalize the procedures themselves. ", "page_idx": 16}, {"type": "text", "text": "Definition B.1 (Adjacency Matrices on Product Graph). Let $G=(V,E)$ be a graph with adjacency matrix $A$ and node feature vector $X$ , and let $\\boldsymbol{\\mathcal{T}}(\\cdot)$ be a coarsening function. We define the following four adjacency matrices on the vertex set $V^{T}\\times\\dot{V}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{{\\cal A}_{G}(S,v,S^{\\prime},v^{\\prime})=\\left\\{\\begin{array}{c c}{{1}}&{{v\\sim_{G}v^{\\prime},\\;S=S^{\\prime}}}\\\\ {{0}}&{{o t h e r w i s e.}}\\end{array}\\right.}}\\\\ {{{\\cal A}_{T(G)}(S,v,S^{\\prime},v^{\\prime})=\\left\\{\\begin{array}{c c}{{1}}&{{S\\sim_{T(G)}S^{\\prime},\\;v=v^{\\prime}}}\\\\ {{0}}&{{o t h e r w i s e.}}\\end{array}\\right.}}\\\\ {{{\\cal A}_{P_{1}}(S,v,S^{\\prime},v^{\\prime})=\\left\\{\\begin{array}{c c}{{1}}&{{v\\in S^{\\prime},\\;v=v^{\\prime}}}\\\\ {{0}}&{{o t h e r w i s e.}}\\end{array}\\right.}}\\\\ {{{\\cal A}_{P_{2}}(S,v,S^{\\prime},v^{\\prime})=\\left\\{\\begin{array}{c c}{{1}}&{{v^{\\prime}\\in S,\\;S^{\\prime}=S}}\\\\ {{0}}&{{o t h e r w i s e.}}\\end{array}\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given edge features $\\{e_{v,v^{\\prime}}\\mid v\\sim_{G}v^{\\prime}\\}$ and $\\{\\tilde{e}_{S,S^{\\prime}}\\mid s\\sim_{T(G)}s^{\\prime}\\}$ corresponding to the graphs $G$ and $\\tau(G)$ , respectively, we can trivially define the edge features corresponding to $A_{G}$ and $A_{G^{7}}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{G}(S,v,S^{\\prime},v^{\\prime})=e_{v,v^{\\prime}},}\\\\ {e_{T(G)}(S,v,S^{\\prime},v^{\\prime})=\\tilde{e}_{S,S^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, for $i=1,2$ , we define the edge features corresponding to adjacency matrices $A_{P_{i}}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\ne_{P_{i}}(S,v,S^{\\prime},v^{\\prime})=z(S,v,S^{\\prime},v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the function $z:\\mathcal{P}([n])\\times[n]\\times\\mathcal{P}([n])\\times[n]\\to\\mathbb{R}^{d}$ maps each tuple $\\mathbf{v}=(S,v,S^{\\prime},v^{\\prime})$ to $a$ vector uniquely encoding the orbit of v under the action of $S_{n}$ as described in Equation 73. ", "page_idx": 16}, {"type": "text", "text": "Definition B.2 (CS-GNN Update Implementation). Given a graph $G=(V,E)$ , and a coarsening function $\\tau(\\cdot)$ , let $A_{1}\\dots A_{4}$ enumerate the set of adjacency matrices $\\{A_{G},A_{{\\bar{T}}(G)},A_{P_{1}},A_{P_{2}}\\}$ . We define a CS-GNN layer update in the following way: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{X}_{i}^{t}(S,v)=U_{i}^{t}\\left((1+\\epsilon_{i}^{t})\\cdot\\mathcal{X}^{t}(S,v)+\\sum_{(S^{\\prime},v^{\\prime})\\sim A_{i}(S,v)}M^{t}(\\mathcal{X}^{t}(S^{\\prime},v^{\\prime})+e_{i}(S,v,S^{\\prime},v^{\\prime}))\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\chi^{t+1}(S,v)=U_{f i n}^{t}\\left(\\sum_{i=1}^{4}\\chi_{i}^{t}(S,v)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here $\\mathcal{X}^{t}(S,v)$ and $\\mathcal{X}^{t{+}1}(S,v)$ denote the node feature maps of the product graph at layers $t$ and $t+1$ , respectively. $e^{1}(S,v,S^{\\prime},v^{\\prime}),\\ldots,e^{4}(S,v,S^{\\prime},v^{\\prime})$ denote the edge features associated with adjacency matrices $A_{1},\\ldots,A_{4}$ . $\\epsilon_{1}^{t},\\hdots,\\epsilon_{4}^{t}$ represent learnable parameters in $\\mathbb{R}$ , and $U_{1}^{t},\\dots,U_{4}^{t}$ , $U_{\\hbar n}^{i},\\,M^{t}$ all refer to multilayer perceptrons. ", "page_idx": 16}, {"type": "text", "text": "The next proposition states that using the layer update defined in equations 29 and 30 is enough to efficiently recover the general layer update defined in equation 15. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.1 (Equivalence of General Layer and Implemented Layer). Let $\\tau(\\cdot)$ be a coarsening function, \u03c0 be a generalized node marking policy, and $\\mathcal{G}$ be a finite family of graphs. Applying a stack of $t$ general layer updates as defined in Equation 15 to the node feature map ${\\mathcal{X}}(S,v)$ induced by $\\pi(G,\\tau)$ , can be effectively implemented by applying a stack of t layer updates specified in Equations 29 and $30$ to ${\\mathcal{X}}(S,v)$ . Additionally, the depths of all MLPs that appear in 29 and $30$ can be bounded by 4. ", "page_idx": 17}, {"type": "text", "text": "C Node Marking Policies \u2013 Theoretical Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we define and analyze various general node marking policies, starting with four natural choices. ", "page_idx": 17}, {"type": "text", "text": "Definition C.1 (Four General Node Marking policies). Let $G=(V,E)$ be a graph with adjacency matrix $A\\in\\mathbb{R}^{n\\times n}$ and node feature vector $X\\in\\mathbb{R}^{n\\times d}$ , and let $\\tau(\\cdot)$ be a coarsening function. All of the following node marking policies take the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi(G,\\mathcal{T})=\\mathcal{X}(S,v)=[X_{u},b_{\\pi}(S,v)],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $[\\cdot,\\cdot]$ denotes the concatenation operator. We focus on four choices for $b_{\\pi}(S,v)$ : ", "page_idx": 17}, {"type": "text", "text": "1. Simple Node Marking: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nb_{\\pi}(S,v)=\\left\\{1\\begin{array}{c l}{{i f v\\in S,}}\\\\ {{0}}&{{i f v\\notin S.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote this node marking policy by $\\pi_{S}$ . ", "page_idx": 17}, {"type": "text", "text": "2. Node $^+$ Size Marking: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nb_{\\pi}(S,v)={\\left\\{\\begin{array}{l l}{(1,|S|)}&{i f v\\in S,}\\\\ {(0,|S|)}&{i f v\\notin S.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote this node marking policy by $\\pi_{S S}$ . ", "page_idx": 17}, {"type": "text", "text": "3. Minimum Distance: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nb_{\\pi}(S,v)=\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{G}(v,v^{\\prime})$ is the shortest path distance between nodes $v$ and $v^{\\prime}$ in the original graph.   \nWe denote this node marking policy by $\\pi_{M D}$ . ", "page_idx": 17}, {"type": "text", "text": "4. Learned Distance Function: ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nb_{\\pi}(S,v)=\\phi(\\{d_{G}(v,v^{\\prime})\\mid v^{\\prime}\\in S\\})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\phi(\\cdot)$ is a learned permutation-invariant function. We denote this node marking policy by \u03c0LD. ", "page_idx": 17}, {"type": "text", "text": "We note that when using the identity coarsening function ${\\mathcal{T}}(G)=G$ , our general node marking policies output node feature maps supported on the product $V\\times V$ . Thus, they can be compared to node marking policies used in node-based subgraph GNNs. In fact, in this case, both $\\pi_{\\mathrm{S}}$ and $\\pi_{\\mathrm{SS}}$ reduce to classical node-based node marking, while $\\pi_{\\mathrm{MD}}$ and $\\pi_{\\mathrm{LD}}$ reduce to distance encoding. The definitions of these can be found in [38]. Interestingly, even though in the case of node-based subgraph GNNSs, both distance encoding and node marking were proven to be maximally expressive [38], in our case for some choices of $\\tau$ , $\\pi_{\\mathrm{LD}}$ is strictly more expressive than the other three choices. The exact effect of each generalized node marking policy on the expressivity of our model is explored in the following two propositions. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1 (Equal Expressivity of Node Marking Policies). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nC S\\!-\\!G N N(\\mathcal{T},\\pi_{S})=C S\\!-\\!G N N(\\mathcal{T},\\pi_{S S})=C S\\!-\\!G N N(\\mathcal{T},\\pi_{M D}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition C.2 (Expressivity of Learned Distance Policy). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nC S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{S})\\subseteq C S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{L D}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In addition, for some choices of $\\tau(\\cdot)$ the containment is strict. ", "page_idx": 18}, {"type": "text", "text": "The proofs of both propositions can be found in Appendix G. Finally, we provide a principled approach to deriving a generalized node marking policy based on symmetry invariance, and prove its equivalence to $\\pi_{\\mathrm{SS}}$ . Given a graph $G=(V,E)$ with $\\dot{V}=[n]$ , adjacency matrix $A$ , and node feature vector $X\\,\\in\\,\\mathbb{R}^{n\\times d}$ , along with a coarsening function $\\tau(\\cdot)$ , We define an action of the symmetric group $S_{n}$ on the space $\\mathbb{R}^{\\bar{\\mathcal{P}}([n])\\times[n]}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma\\cdot\\mathcal{X}(S,v)=\\mathcal{X}(\\sigma^{-1}(S),\\sigma^{-1}(v))\\quad\\mathrm{for}\\,\\sigma\\in S_{n},\\mathcal{X}\\in\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, for each orbit $\\gamma\\in(\\mathcal{P}([n])\\times[n])/S_{n}$ , we define $\\mathbf{1}_{\\gamma}\\in\\mathbb{R}^{\\mathcal{P}([n])\\times[n]}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{1}_{\\gamma}(S,v)=\\left\\{1\\begin{array}{l l}{\\,\\,\\,(S,v)\\in\\gamma,}\\\\ {0}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Choosing some enumeration of the orbit set $(\\mathcal{P}([n])\\times[n])/S_{n}=\\{\\gamma_{1},\\ldots,\\gamma_{k}\\}$ , We now define the invariant generalized node marking policy $\\pi_{\\mathrm{inv}}$ by first setting: ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{\\pi_{\\mathrm{inv}}}^{\\mathrm{sparse}}(S,v):{\\mathcal{P}}([n])\\times[n]\\rightarrow\\mathbb{R}^{k}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{\\pi_{\\mathrm{inv}}}:V^{T}\\times V\\to\\mathbb{R}^{k}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{b_{\\pi_{\\mathrm{inv}}}^{\\mathrm{sparse}}(S,v)=[\\mathbf{1}_{\\gamma_{1}}(S,v),\\dots,\\mathbf{1}_{\\gamma_{k}}(S,v)]\\qquad\\qquad}&{S\\in\\mathcal{P}(V),\\ v\\in V,}\\\\ &{\\ b_{\\pi_{\\mathrm{inv}}}(S,v)=b_{\\pi_{\\mathrm{inv}}}^{\\mathrm{sparse}}(S,v)\\qquad\\qquad}&{S\\in V^{T},\\ v\\in V.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we define the node feature map induced by $\\pi_{\\mathrm{inv}}$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\chi^{\\pi_{\\mathrm{inv}}}(S,v)=[X_{v},b_{\\pi_{\\mathrm{inv}}}(S,v)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Interestingly, $\\pi_{\\mathrm{inv}}$ , derived solely from the group action of $S_{n}$ on $\\textstyle P([n])\\times[n]$ , is equivalent to the generalized node marking policy $\\pi_{\\mathrm{SS}}$ . This is stated more rigorously in the following proposition: ", "page_idx": 18}, {"type": "text", "text": "Proposition C.3 (Node $^+$ Size Marking as Invariant Marking). Given a graph $G=(V,E)$ with node feature vector $X\\in\\mathbb{R}^{n\\times d}$ , and a coarsening function $\\tau(\\cdot)$ , let $\\mathcal{X}^{\\pi_{S S}},\\mathcal{X}^{\\pi_{i m}}$ be the node feature maps induced by $\\pi_{S S}$ and $\\pi_{i n\\nu}$ respectively. Recall that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\chi^{\\pi_{s s}}(S,v)=[X_{v},b_{\\pi_{s s}}(S,v)],}\\\\ {\\chi^{\\pi_{i n v}}(S,v)=[X_{v},b_{\\pi_{i n v}}(S,v)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following now holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{\\pi_{i n v}}(S,v)=O H E(b_{\\pi_{S S}}(S,v))\\quad\\forall S\\in V^{T},\\;\\forall v\\in V.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, OHE denotes a one-hot encoder, independent of the choice of both $G$ and $\\tau$ . ", "page_idx": 18}, {"type": "text", "text": "The proof of proposition C.3 can be found in Appendix G. ", "page_idx": 18}, {"type": "text", "text": "D Expressive Power of CS-GNN ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Recovering Subgraph GNNs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we demonstrate that by choosing suitable coarsening functions, our architecture can replicate various previous subgraph GNN designs. We begin by focusing on node-based models, which are the most widely used type. We define a variant of these models which was proven in [38] to be maximally expressive, and show that our approach can recover it. ", "page_idx": 18}, {"type": "text", "text": "Definition D.1 (Maximally Expressive Subgraph GNN). We define MSGNN(\u03c0NM) as the set of all functions expressible by the following procedure: ", "page_idx": 18}, {"type": "text", "text": "1. Node Marking: The representation of tuple $(u,v)\\in V\\times V$ is initially given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\chi^{0}(u,v)=\\left\\{1\\begin{array}{c l}{{\\,\\,\\,\\,i f u=v,}}\\\\ {{\\,\\,\\,\\,i f u\\not=v.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2. Update: The representation of tuple $(u,v)$ is updated according to: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\chi^{t+1}(u,v)=f^{t}\\bigg(\\mathcal{X}^{t}(u,v),\\mathcal{X}^{t}(u,u),\\mathcal{X}^{t}(v,v),\\quad}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{}&{\\qquad\\qquad\\qquad a g g_{1}^{t}\\mathbb{f}(\\mathcal{X}^{t}(u,v^{\\prime}),e_{v,v^{\\prime}})\\mid v^{\\prime}\\sim v\\mathbb{j},\\quad}\\\\ &{}&{\\qquad\\qquad\\qquad a g g_{2}^{t}\\mathbb{f}(\\mathcal{X}^{t}(v,u^{\\prime}),e_{u,u^{\\prime}})\\mid u^{\\prime}\\sim u\\mathbb{j}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "3. Pooling: The final node feature vector $\\chi^{T}(u,v)$ is pooled according to: ", "page_idx": 19}, {"type": "equation", "text": "$$\nM L P_{2}\\left(\\sum_{u\\in V}M L P_{1}\\left(\\sum_{v\\in V}\\chi^{T}(u,v)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, for any $t\\in[T]$ , $f^{t}$ is any continuous (parameterized) functions, $a g g_{1}^{t},,a g g_{2}^{t}$ are any continuous (parameterized) permutation-invariant functions and $M L P_{1},M L P_{2}$ are multilayer preceptrons. ", "page_idx": 19}, {"type": "text", "text": "Proposition D.1 (CS-GNN Can Implement MSGNN). Let $\\tau(\\cdot)$ be the identity coarsening function defined by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\quad\\forall G=(V,E).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\nC S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{S})=M S G N\\!N(\\pi_{N M}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The proof of proposition D.1 can be found in Appendix G. We observe that, similarly, by selecting the coarsening function: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}(G)=E\\quad\\forall G=(V,E),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "one can recover edge-based subgraph GNNs. An example of such a model is presented in [4] (DSGNN), where it was proven capable of distinguishing between two 3-WL indistinguishable graphs, despite having an asymptotic runtime of $\\tilde{\\mathcal{O}}(\\tilde{m^{2}})$ , where $m$ is the number of edges in the input graph. This demonstrates our model\u2019s ability to achieve expressivity improvements while maintaining a (relatively) low asymptotic runtime by exploiting the graph\u2019s sparsity through the coarsening function. Finally, we note that by selecting the coarsening function: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{S\\in{\\mathcal{P}}(V)\\mid|S|=k\\}\\quad G=(V,E),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can recover an unordered variant of the $k$ -OSAN model presented in [29]. ", "page_idx": 19}, {"type": "text", "text": "D.2 Comparison to Natural Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we demonstrate how our model can leverage the information provided by the coarsening function $\\tau(\\cdot)$ in an effective way. First, we define a baseline model that incorporates $\\tau$ in a straightforward manner. We then prove that, for any $\\tau(\\cdot)$ , our model is at least as expressive as this baseline. Additionally, we show that for certain choices of $\\tau(\\cdot)$ , our model exhibits strictly greater expressivity. To construct the baseline model, we first provide the following definition: ", "page_idx": 19}, {"type": "text", "text": "Definition D.2 (Coarsened Sum Graph). Given a graph $G=(V,E)$ and a coarsening function $\\tau(\\cdot)$ , we define the coarsened sum graph $G_{+}^{\\mathcal{T}}=(V_{+}^{\\mathcal{T}},\\check{E}_{+}^{\\mathcal{T}})$ by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;V_{+}^{\\mathcal{T}}=V\\cup V^{\\mathcal{T}}.}\\\\ &{\\bullet\\;E_{+}^{\\mathcal{T}}=E\\cup E^{\\mathcal{T}}\\cup\\{\\{S,v\\}\\mid S\\in V^{\\mathcal{T}},\\;v\\in V\\;v\\in S\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If graph $G$ had a node feature vector $X\\in\\mathbb{R}^{n\\times d}$ , we define the node feature vector of $G_{+}^{T}$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{v}=\\left\\{\\!\\!\\begin{array}{l l}{{X_{v},1]}}&{{v\\in V}}\\\\ {{0_{d+1}}}&{{v\\in V^{T}}}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here we concatenated a 1 to the end of node features of $V$ to distinguish them from the nodes of $V^{\\mathcal{T}}$ ", "page_idx": 19}, {"type": "text", "text": "The connectivity of the sum graph (for our running example Figure 1) is visualized inset. ", "page_idx": 20}, {"type": "text", "text": "We now define our baseline model: ", "page_idx": 20}, {"type": "text", "text": "Definition D.3 (Coarse MPNN). Let $\\tau(\\cdot)$ be a coarsening function. Define $M P N N_{+}({\\mathcal{T}})$ as the set of all functions which can be expressed by the following procedure: ", "page_idx": 20}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/64c1d77229f2c98e55c4bab17312cf0f7b6568feb0653bd4a124ae800f1bbab8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "1. Preprocessing: We first construct the sum graph $G_{+}^{\\mathcal{T}}$ of the input graph $G$ , along with a node feature map $\\mathcal{X}^{0}:V_{+}^{\\mathcal{T}}\\rightarrow\\mathbb{R}^{d}$ defined according to equation $53$ . ", "page_idx": 20}, {"type": "text", "text": "2. Update: The representation of node $v\\in V_{+}^{\\mathcal{T}}$ is updated according to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{F o r\\;v\\in V:}&{\\chi^{t+1}(v)=f_{V}^{t}\\left(\\chi^{t}(v),a g g_{1}^{t}\\{\\mathbb{f}\\left(\\mathcal{X}^{t}(u),e_{u,v}\\right)\\mid u\\sim_{G}v\\}\\right),}\\\\ &{\\quad a g g_{2}^{t}\\{\\mathbb{f}^{t}(S)\\mid S\\in V^{T},v\\in S\\}\\}\\,,}\\\\ {F o r\\;S\\in V^{T}:}&{\\chi^{t+1}(S)=f_{V}^{t}\\tau\\left(\\mathcal{X}^{t}(S),a g g_{1}^{t}\\{\\mathbb{f}\\left(\\mathcal{X}^{t}(S^{\\prime}),e_{S,S^{\\prime}}\\right)\\mid S^{\\prime}\\sim_{T(G)}S\\},\\right.}\\\\ &{\\quad\\left.a g g_{2}^{t}\\mathbb{f}\\mathcal{X}^{t}(v)\\mid v\\in V,v\\in S\\right\\}\\mathbb{J}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "3. Pooling: The final node feature vector $\\chi^{T}(\\cdot)$ is pooled according ", "page_idx": 20}, {"type": "text", "text": "to: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM L P\\left(\\sum_{v\\in V_{+}^{\\mathcal{T}}}\\chi^{T}(v)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, for $t~\\in~[T],~f_{V}^{t},~f_{V^{T}}^{t}$ are continuous (parameterized) functions and , $a g g_{1}^{t},a g g_{2}^{t}T$ are continuous (parameterized) permutation invariant functions. Finally, we notice that for the trivial coarsening function defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{\\varnothing}(G)=\\varnothing,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the update in Equation (54) devolves into a standard MPNN update, as defined in [13] and so we define: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM P N N=M P N N_{+}(\\mathcal{T}_{\\varnothing}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In essence, given an input graph $G=(V,E)$ , the $\\mathbf{MPNN}_{+}(\\mathcal{T})$ pipeline first constructs the coarsened graph $\\tau(G)$ . It then adds edges between each super-node $S\\in\\dot{V}^{T}$ and the nodes it is comprised of (i.e., any $v\\in S$ ). This is followed by a standard message passing procedure on the graph. The following two propositions suggest that this simple approach to incorporating $\\tau$ into a GNN pipeline is less powerful than our model. ", "page_idx": 20}, {"type": "text", "text": "Proposition D.2 (CS-GNN Is at Least as Expressive as Coarse MPNN ). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM P N N\\subseteq M P N N_{+}({\\mathcal{T}})\\subseteq C S\u2013G N N({\\mathcal{T}},\\pi_{S})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proposition D.3 (CS-GNN Can Be More Expressive Than $\\mathrm{{MPNN+}}$ ). Let $\\tau(\\cdot)$ be the identity coarsening function defined by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\quad G=(V,E).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The following holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM P N N=M P N N_{+}({\\mathcal T}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus: ", "page_idx": 20}, {"type": "equation", "text": "$$\nM P N N_{+}({\\mathcal T})\\subset C S{\\cdot}G N N({\\mathcal T},\\pi_{S}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where this containment is strict. ", "page_idx": 20}, {"type": "text", "text": "The proofs to the last two propositions can be found in Appendix G. Proposition D.3 demonstrates that CS-GNNis strictly more expressive than $\\mathbf{MPNN}_{+}$ when using the identity coarsening function. However, this result extends to more complex coarsening functions as well. We briefly discuss one such example. Let $\\tau(\\cdot)$ be the coarsening function defined by: ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{\\triangle}(G)=\\{v_{1},v_{2},v_{3}\\mid G[v_{1},v_{2},v_{3}]\\cong\\triangle\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e. for an input graph $G$ , the set of super-nodes is composed of all triplets of nodes whose induced subgraph is isomorphic to a triangle. To see that CS-GNN is strictly more expressive then $\\mathbf{MPNN}_{+}$ when using $\\tau_{\\triangle}(\\cdot)$ , we look at the two graphs $G$ and $H$ depicted in Figure 4. In the figure, we see the two original graphs, $G$ and $H$ , their corresponding sum graphs $G_{+}^{\\tau_{\\triangle}}$ and $H_{+}^{\\tau_{\\triangle}}$ , and a subgraph of their corresponging product graphs $G\\square\\tau_{\\triangle}(G)$ and $H\\square\\tau_{\\triangle}(H)$ induced by the sets $\\{(S_{0},\\bar{v})\\ |\\ v\\in V_{G}\\}$ and $\\{(S_{0},\\bar{v})\\mid\\bar{v}\\in V_{H}\\}$ respectively (this can be thought of as looking at a single subgraph from the bag of subgraphs induced by CS-GNN). One can clearly see that both the original graphs and their respective sum graphs are 1-WL indistinguishable. On the other hand, the subgraphs induced by our method are 1-WL distinguishable. Since for both $G$ and $H$ the \"bag of subgraphs\" induced by CS-GNN is composed of 6 isomorphic copies of the same graph, this would imply that our method can distinguish between $G$ and $H$ , making it strictly mor expressive then $\\mathbf{MPNN}_{+}$ . ", "page_idx": 21}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/58fc05b9c0f683098c87bd83929caba4fbb7277bfd71f548a217b10ac479c624.jpg", "img_caption": ["Figure 4: Rows 1 and 3 depict two 1-WL indistinguishable graphs> Rows 2 and 4 depict the sum graph of each of these graphs, as well as one subgraph of their product graphs induced by all node, super-node tuples whose super-node is fixed. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "We conclude this section with the following proposition, showing there exists coarsening functions which, when combined with CS-GNN, results in an architecture that is strictly more expressive then node-based subgraph GNNs. ", "page_idx": 22}, {"type": "text", "text": "Proposition D.4 (CS-GNN can be strictly more expressive then node-based subgraph GNNs). Let $\\tau$ be the coarsening function defined by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\cup E\\quad G=(V,E).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The following holds: ", "page_idx": 22}, {"type": "text", "text": "1. Let $G_{1},G_{2}$ be a pair of graphs such that there exists a node-based subgraph GNN model $M$ where $M(G_{1})\\neq M(G_{2})$ . There exists a CS-GNNmodel $M^{\\prime}$ which uses $\\tau$ such that $M^{\\prime}(G_{1})\\neq\\dot{M}^{\\prime}(\\dot{G}_{2})$ .   \n2. There exists a pair of graphs $G_{1},G_{2}$ such that for any subgraph GNN model M it holds that $M(G_{1})\\;=\\;M(G_{2}),$ , but there exists a CS-GNNmodel $M^{\\prime}$ which uses $\\tau$ such that $M^{\\prime}(G_{1})\\neq M^{\\prime}(G_{2})$ . ", "page_idx": 22}, {"type": "text", "text": "This proposition is proved in Appendix G. ", "page_idx": 22}, {"type": "text", "text": "E Linear Invariant (Equivariant) Layer \u2013 Extended Section ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We introduce some key notation. In the matrix $\\mathcal{X}$ , the $i$ -th row corresponds to the $i$ -th subset $S$ arranged in the lexicographic order of all subsets of $[n]$ , namely, $[\\{0\\},\\{0,1\\},\\{0,2\\},\\ldots,\\{0,1,2,\\ldots,n\\}]$ . Each $i$ -th position in this sequence aligns with the $i$ -th row index in $\\mathcal{X}$ . It follows, that the standard basis for such matrices in $\\mathbb{R}^{2^{n}\\times n}$ is expressed as $\\mathbf{e}^{(S)}\\cdot\\mathbf{e}^{(i)^{T}}$ , where $\\mathbf{e}^{(S)}$ is a 1-hot vector, with the value 1 positioned according to $S$ in the lexicographic order. For a matrix $X\\in\\mathbb{R}^{a\\times b}$ , the operation of vectorization, denoted by $\\operatorname{vec}(X)$ , transforms $X$ into a single column vector in $\\mathbb{R}^{a b\\times1}$ by sequentially stacking its columns; in the context of $\\mathcal{X}$ , the basis vectors of those vectors are $\\mathbf{e}^{(i)}\\overset{\\cdot}{\\otimes}\\mathbf{e}^{(\\bar{S})}$ . The inverse process, reshaping a vectorized matrix back to its original format, is denoted as $\\left[\\operatorname{vec}(X)\\right]=X$ . We also denote an arbitrary permutation by $\\sigma\\in S_{n}$ . The actions of permutations on vectors, whether indexed by sets or individual indices, are represented by ${\\bf P}_{S}\\in\\mathrm{GL}(2^{n})$ and $\\mathbf{P}_{\\mathcal{T}}\\in\\mathrm{GL}(n)$ , respectively. This framework acknowledges $S_{n}$ as a subgroup of the larger permutation group $S_{2^{n}}$ , which permutes all $2^{n}$ positions in a given vector $\\mathbf{v}_{S}\\in\\mathbb{R}^{2^{n}}$ . ", "page_idx": 22}, {"type": "text", "text": "Let $\\mathbf{L}\\in\\mathbb{R}^{1\\times2^{n}\\cdot n}$ be the matrix representation of a general linear operator $\\mathcal{L}:\\mathbb{R}^{2^{n}\\times n}\\rightarrow\\mathbb{R}$ in the standard basis. The operator $\\mathcal{L}$ is order-invariant iff ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\mathcal{X}\\mathbf{P}_{\\mathcal{Z}})=\\mathbf{L}\\operatorname{vec}(\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, let $\\mathbf{L}\\,\\in\\,\\mathbb{R}^{2^{n}\\cdot n\\times2^{n}\\cdot n}$ denote the matrix for $\\mathcal{L}\\,:\\,\\mathbb{R}^{2^{n}\\times n}\\,\\rightarrow\\,\\mathbb{R}^{2^{n}\\times n}$ . The operator $\\mathcal{L}$ is order-equivariant if and only if ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\boldsymbol{\\mathcal{X}}\\mathbf{P}_{\\mathcal{Z}})]=\\mathbf{P}_{S}^{T}[\\mathbf{L}\\operatorname{vec}(\\boldsymbol{\\mathcal{X}})]\\mathbf{P}_{\\mathcal{Z}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using properties of the Kronecker product (see Appendices E.1 and E.2 for details), we derive the following conditions for invariant and equivariant linear layers: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Invariant~}\\mathbf{L}:}&{\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\,\\mathrm{vec}(\\mathbf{L})=\\mathrm{vec}(\\mathbf{L}),}\\\\ {\\mathrm{Equivariant~}\\mathbf{L}:}&{\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\otimes\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\,\\mathrm{vec}(\\mathbf{L})=\\mathrm{vec}(\\mathbf{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Solving Equations (66) and (67). Let $\\sigma\\in S_{n}$ denote a permutation corresponding to the permutation matrix $\\mathbf{P}$ . Let ${\\bf P}\\star{\\bf L}$ denote the tensor that results from expressing $\\mathbf{L}$ after renumbering the nodes in $V^{\\mathcal{T}},V$ according to the permutation $\\sigma$ . Explicitly, for $\\mathbf{L}\\in\\mathbb{R}^{2^{n}\\times n}$ , the $(\\sigma(S),\\sigma(i))$ -entry of ${\\bf P}\\star{\\bf L}$ equals to the $(S,i)$ -entry of $\\mathbf{L}$ . The matrix that corresponds to the operator $\\mathbf{P}\\star$ in the standard basis, $\\mathbf{e}^{(i)}\\otimes\\mathbf{e}^{(S)}$ is the kronecker product $\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}$ . Since $\\operatorname{vec}(\\mathbf{L})$ is exactly the coordinate vector of the tensor $\\mathbf{L}$ in the standard basis we have, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbf{P}\\star\\mathbf{L})=\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\operatorname{vec}(\\mathbf{L}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "following the same logic, the following holds for the equivariant case, where $\\mathbf{L}\\in\\mathbb{R}^{2^{n}\\cdot n\\times2^{n}\\cdot n}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\mathbf{P}\\star\\mathbf{L})=\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\otimes\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\,\\mathrm{vec}(\\mathbf{L}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Given Equations (66) and (68) and Equations (67) and (69), it holds that we should focus on solving, for both cases where $\\mathbf{L}\\in\\mathbb{R}^{2^{n}\\times n}$ and $\\mathbf{L}\\in\\mathbb{R}^{2^{n}\\times n\\times2^{n}\\times n}$ , corresponding to the bias term, and linear term. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Bias. To this end, let us define an equivalence relation in the index space of a tensor in $\\mathbb{R}^{2^{n}\\times n}$ . Given a pair $(S,i)\\in\\mathcal{P}([n])\\times[n]$ , we define $\\gamma^{k^{+}}$ to correspond to all pairs $(S,i)$ such that $|S|=k$ and $i\\not\\in S$ . Similarly, $\\gamma^{k^{-}}$ corresponds to all pairs $(S,i)$ such that $|S|=k$ and $i\\in S$ . We denote this equivalence relation as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\mathcal{P}([n])\\times[n])/_{\\sim}\\triangleq\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each set-equivalence class $\\gamma\\,\\in\\,({\\mathcal{P}}([n])\\times[n])_{\\sim}$ , we define a basis tensor, $\\mathbf{B}^{\\gamma}\\,\\in\\,\\mathbb{R}^{2^{n}\\times n}$ by setting: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{B}_{S,i}^{\\gamma}=\\left\\{{1,\\quad\\mathrm{if~}(S,i)\\in\\gamma};\\begin{array}{l l}{}\\\\ {}\\\\ {}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Following similar reasoning, consider elements $(S_{1},i_{1},S_{2},i_{2})\\,\\in\\,({\\mathcal{P}}([n])\\,\\times\\,[n]\\,\\times\\,{\\mathcal{P}}([n])\\,\\times\\,[n])$ . We define a partition according to six conditions: the relationship between $i_{1}$ and $i_{2}$ , denoted as $i_{1}\\leftrightarrow i_{2}$ , which is determines by the condition: $i_{1}\\,=\\,i_{2}$ or $i_{1}\\neq i_{2}$ ; the cardinalities of $S_{1}$ and $S_{2}$ , denoted as $k_{1}$ and $k_{2}$ , respectively; the size of the intersection $S_{1}\\cap S_{2}$ , denoted as $k^{\\cap}$ ; the membership of $i_{l}$ in $S_{l}$ for $l\\in\\{1,2\\}$ , denoted as $\\delta_{\\mathrm{same}}\\in\\{1,2,3,4\\}$ ; and the membership of $i_{l_{1}}$ in $S_{l_{2}}$ for distinct $l_{1},l_{2}\\in\\{1,2\\}$ , denoted as $\\delta_{\\mathrm{diff}}\\in\\{1,2,3,4\\}$ . The equivalence relation thus defined can be represented as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\mathcal{P}([n])\\times[n]\\times\\mathcal{P}([n])\\times[n]\\right)/_{\\sim}\\triangleq\\{\\Gamma^{\\leftrightarrow;k_{1};k_{2};k^{\\cap};\\delta_{\\mathrm{same}};\\delta_{\\mathrm{diff}}}\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each set-equivalence class $\\Gamma\\,\\in\\,({\\mathcal{P}}([n])\\,\\times\\,[n]\\,\\times\\,{\\mathcal{P}}([n])\\,\\times\\,[n])/_{\\sim}$ , we define a basis tensor, $\\mathbf{B}^{\\Gamma}\\in\\mathbb{R}^{2^{n}\\times n\\times\\bar{2}^{n}\\times n}$ by setting: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{B}_{S_{1},i_{1};S_{2},i_{2}}^{\\Gamma}=\\left\\{1,\\begin{array}{r l}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following two proposition summarizes the results in this section, ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1 $(\\gamma\\,(\\Gamma)$ are orbits). The sets $\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}$ and $\\left\\{\\Gamma^{\\leftrightarrow;k_{1};k_{2};k^{\\cap};\\delta_{s a m e};\\delta_{d i f f}}\\right\\}$ are the orbits of $S_{n}$ on the index space $\\left.\\mathcal{P}([n])\\times[n]\\right)$ and $({\\mathcal{P}}([n])\\times[n]\\times({\\mathcal{P}}([n])\\times[n])$ , respectively. ", "page_idx": 23}, {"type": "text", "text": "Proposition E.1 (Basis of Invariant (Equivariant) Layer). The tensors ${\\bf B}^{\\gamma}\\,({\\bf B}^{\\Gamma})$ in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equation (66) (Equation (67)). ", "page_idx": 23}, {"type": "text", "text": "The proofs are given in Appendix G. ", "page_idx": 23}, {"type": "text", "text": "E.1 Full Derivation of Equation (66). ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our goal is to transition from the equation, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\boldsymbol{\\mathcal{X}}\\mathbf{P}_{\\mathcal{T}})=\\mathbf{L}\\operatorname{vec}(\\boldsymbol{\\mathcal{X}})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "to the form, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\,\\mathrm{vec}(\\mathbf{L})=\\mathrm{vec}(\\mathbf{L})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We introduce the following property of the Kronecker product, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbf{ABC})=(\\mathbf{C}^{T}\\otimes\\mathbf{A})\\operatorname{vec}(\\mathbf{B}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using Equation (75) on the left side of Equation (64), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{LP}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{S}^{T}\\operatorname{vec}(\\mathcal{X})=\\mathbf{L}\\operatorname{vec}(\\mathcal{X}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since this should be true for any $\\mathcal{X}\\in\\mathbb{R}^{2^{n}\\times n}$ , we derive ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{L}\\mathbf{P}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{\\mathcal{S}}^{T}=\\mathbf{L}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying the transpose operation on both sides, and noting that $(\\mathbf{P}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{\\mathcal{S}}^{T})^{T}=\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}\\mathbf{L}^{T}=\\mathbf{L}^{T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recalling that $\\mathbf{L}\\in\\mathbb{R}^{1\\times2^{n}\\cdot n}$ , and thus $\\mathbf{L}^{T}\\in\\mathbb{R}^{2^{n}\\cdot n\\times1}$ , we find that $\\mathbf{L}^{T}=\\operatorname{vec}(\\mathbf{L})$ . Substituting this back into the previous equation we achieve Equation (66). ", "page_idx": 23}, {"type": "text", "text": "E.2 Full Derivation of Equation (67). ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our goal is to transition from the equation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n[\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\boldsymbol{\\mathcal{X}}\\mathbf{P}_{Z})]=\\mathbf{P}_{S}^{T}[\\mathbf{L}\\operatorname{vec}(\\boldsymbol{\\mathcal{X}})]\\mathbf{P}_{Z}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "to the form, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{S}\\otimes\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{S}\\operatorname{vec}(\\mathbf{L})=\\operatorname{vec}(\\mathbf{L}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying the property in Equation (75), after the reverse operation of the vectorization, namely, ", "page_idx": 24}, {"type": "equation", "text": "$$\n[\\operatorname{vec}(\\mathbf{ABC})]=[(\\mathbf{C}^{T}\\otimes\\mathbf{A})\\operatorname{vec}(\\mathbf{B})]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "on the right hand side of Equation (65), for ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}\\triangleq\\mathbf{P}_{S}^{T};}\\\\ &{\\mathbf{B}\\triangleq[\\mathbf{L}\\ensuremath{\\operatorname{vec}}(\\chi)];}\\\\ &{\\mathbf{C}\\triangleq\\mathbf{P}_{\\mathbb{Z}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we obtain, ", "page_idx": 24}, {"type": "equation", "text": "$$\n[\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\mathcal{X}\\mathbf{P}_{\\mathbb{Z}})]=[\\mathbf{P}_{\\mathbb{Z}}^{T}\\otimes\\mathbf{P}_{S}^{T}\\mathbf{L}\\operatorname{vec}(\\mathcal{X})].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, by omitting the revere-vectorization operation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\boldsymbol{\\mathcal{X}}\\mathbf{P}_{\\mathbb{Z}})=\\mathbf{P}_{\\mathbb{Z}}^{T}\\otimes\\mathbf{P}_{S}^{T}\\mathbf{L}\\operatorname{vec}(\\boldsymbol{\\mathcal{X}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Noting that $(\\mathbf{P}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{\\mathcal{S}}^{T})^{-1}=\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}}$ , and multiplying by this inverse both sides (from the left), we obtain, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{S}\\mathbf{L}\\operatorname{vec}(\\mathbf{P}_{S}^{T}\\mathcal{X}\\mathbf{P}_{\\mathcal{Z}})=\\mathbf{L}\\operatorname{vec}(\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying, again, the property in Equation (75), we obtain, ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bf P}_{\\mathcal{Z}}\\otimes{\\bf P}_{S}{\\bf L}{\\bf P}_{\\mathcal{Z}}^{T}\\otimes{\\bf P}_{S}^{T}\\,\\mathrm{vec}(\\mathcal{X})={\\bf L}\\,\\mathrm{vec}(\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since this should be true for any $\\mathcal{X}\\in\\mathbb{R}^{2^{n}\\times n}$ , we derive, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathcal{Z}}\\otimes\\mathbf{P}_{S}\\mathbf{L}\\mathbf{P}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{S}^{T}=\\mathrm{vec}(\\mathbf{L}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Again, applying Equation (75) on the left side, where, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}\\triangleq\\mathbf{P}_{\\mathcal{T}}\\otimes\\mathbf{P}_{\\mathcal{S}};}\\\\ &{\\mathbf{B}\\triangleq\\mathbf{L};}\\\\ &{\\mathbf{C}\\triangleq\\mathbf{P}_{\\mathcal{T}}^{T}\\otimes\\mathbf{P}_{\\mathcal{S}}^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we get the following equality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathbb{Z}}\\otimes\\mathbf{P}_{S}\\mathbf{L}\\mathbf{P}_{\\mathbb{Z}}^{T}\\otimes\\mathbf{P}_{S}^{T}=\\mathbf{P}_{\\mathbb{Z}}\\otimes\\mathbf{P}_{S}\\otimes\\mathbf{P}_{\\mathbb{Z}}\\otimes\\mathbf{P}_{S}\\operatorname{vec}(\\mathbf{L}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By substituting this to the left side of Equation (87) we obtain Equation (67). ", "page_idx": 24}, {"type": "text", "text": "E.3 Comparative Parameter Reduction in Linear Equivariant Layers ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To demonstrate the effectiveness of our parameter-sharing scheme, which results from considering unordered tuples rather than ordered tuples, we present the following comparison. 3-IGNs [22] are structurally similar to our approach, with the main difference being that they consider indices as ordered tuples, while we consider them as sets. Both approaches use a total of six indices, as shown in the visualized block in Figure 3, making 3-IGNs a natural comparator. By leveraging our scheme, we reduce the number of parameters from 203 (the number of parameters in 3-IGNs) to just 35! ", "page_idx": 24}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/8be0548173e259f937df32897b0c6fb430ebd3e6db55f6acc0352d0317950198.jpg", "table_caption": ["Table 5: Overview of the graph learning datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "F Extended Experimental Section ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Dataset Description ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we overview the eight different datasets considered; this is summarized in Table 5. ", "page_idx": 25}, {"type": "text", "text": "ZINC-12K and ZINC-FULL Datasets [31, 14, 10]. The ZINC-12K dataset includes 12,000 molecular graphs sourced from the ZINC database, a compilation of commercially available chemical compounds. These molecular graphs vary in size, ranging from 9 to 37 nodes, where each node represents a heavy atom, covering 28 different atom types. Edges represent chemical bonds and there are three types of bonds. The main goal when using this dataset is to perform regression analysis on the constrained solubility (logP) of the molecules. The dataset is divided into training, validation, and test sets with 10,000, 1,000, and 1,000 molecular graphs respectively. The full version, ZINC-FULL, comprises approximately 250,000 molecular graphs, ranging from 9 to 37 nodes and 16 to 84 edges per graph. These graphs also represent heavy atoms, with 28 distinct atom types, and the edges indicate bonds between these atoms, with four types of bonds present. ", "page_idx": 25}, {"type": "text", "text": "OGBG-MOLHIV, OGBG-MOLBACE, OGBG-MOLESOL Datasets [16]. These datasets are used for molecular property prediction and have been adopted by the Open Graph Benchmark (OGB, MIT License) from MoleculeNet. They use a standardized featurization for nodes (atoms) and edges (bonds), capturing various chemophysical properties. ", "page_idx": 25}, {"type": "text", "text": "PEPTIDES-FUNC and PEPTIDES-STRUCT Datasets [9]. The PEPTIDES-FUNC and PEPTIDESSTRUCT datasets consist of atomic graphs representing peptides released with the Long Range Graph Benchmark (LRGB, MIT License). In PEPTIDES-FUNC, the task is to perform multi-label graph classification into ten non-exclusive peptide functional classes. Conversely, PEPTIDES-STRUCT is focused on graph regression to predict eleven three-dimensional structural properties of the peptides. ", "page_idx": 25}, {"type": "text", "text": "We note that for all datasets, we used the random splits provided by the public benchmarks. ", "page_idx": 25}, {"type": "text", "text": "F.2 Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Implementation Details. Our implementation of Equation (2) is given by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{X}^{(l+1)}=\\mathrm{MLP}\\left(\\sum_{i=1}^{3}\\tt M P N N^{(l+1,{i})}\\left(\\mathcal{X},\\mathcal{A}_{i}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathcal{A}_{1}=\\mathcal{A}_{G}$ , $\\mathcal{A}_{2}=\\mathcal{A}_{\\mathcal{T}(G)}$ , and $\\mathcal{A}_{3}=\\mathcal{A}_{\\mathrm{Equiv}}$ . ", "page_idx": 25}, {"type": "text", "text": "For all considered datasets, namely, ZINC-12K, ZINC-FULL, OGBG-MOLHIV, OGBG-MOLBACE, and OGBG-MOLESOL, except for the PEPTIDES-FUNC and PEPTIDES-STRUC datasets, we use a GINE [15] base encoder. Given an adjacency matrix $\\boldsymbol{\\mathcal{A}}$ , and defining $e_{(S^{\\prime},v^{\\prime}),(S,v)}$ to denote the edge features from node $(S^{\\prime},v^{\\prime})$ to node $(S,v)$ , it takes the following form: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{X}(S,v)=\\mathtt{M L P}\\Bigg((1+\\epsilon)\\cdot\\mathcal{X}(S,v)+\\sum_{(S^{\\prime},v^{\\prime})\\sim_{A}(S,v)}\\mathtt{R e L U}\\big(\\mathcal{X}(S^{\\prime},v^{\\prime})+e_{(S^{\\prime},v^{\\prime}),(S,v)}\\big)\\Bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We note that for the symmetry-based updates, we switch the ReLU to an $\\mathtt{M L P}^{12}$ to align with the theoretical analyses13 (Appendix B), stating that we can implement the equivariant update developed in Section 4.2. A more thorough discussion regarding the implementation of the symmetry-based updates is given in Appendix F.4. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "When experimenting with the PEPTIDES-FUNC and PEPTIDES-STRUC datasets, we employ GAT [32] as our underlying MPNN to ensure a fair comparison with the random baseline\u2014the random variant of Subgraphormer $^+$ PE [3]. To clarify, we consider the random variant of Subgraphormer $^+$ PE as a natural random baseline since it incorporates the information in the eigenvectors of the Laplacian (which we also do via the coarsening function). To maintain a fair comparison, we use a single vote for this random baseline, and maintained the same hyperparameters. ", "page_idx": 26}, {"type": "text", "text": "Our experiments were conducted using the PyTorch [28] and PyTorch Geometric [11] frameworks (resp. BSD and MIT Licenses), using a single NVIDIA L40 GPU, and for every considered experiment, we show the mean $\\pm$ std. of 3 runs with different random seeds. Hyperparameter tuning was performed utilizing the Weight and Biases framework [6] \u2013 see Appendix F.3. All our MLPs feature a single hidden layer equipped with a ReLU non-linearity function. For the encoding of atom numbers and bonds, we utilized learnable embeddings indexed by their respective numbers. ", "page_idx": 26}, {"type": "text", "text": "In the case of the OGBG-MOLHIV, OGBG-MOLESOL, OGBG-MOLBACE datasets, we follow Frasca et al. [12], therefore adding a residual connection between different layers. Additionally, for those datasets (except OGBG-MOLHIV), we used linear layers instead of MLPs inside the GIN layers. Moreover, for these four datasets, and for the PEPTIDES datasets, the following pooling mechanism was employed ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho(\\mathcal{X})=\\mathbb{M}\\mathrm{LP}\\left(\\sum_{S}\\left(\\frac{1}{n}\\sum_{v=1}^{n}\\mathcal{X}(s,v)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the PEPTIDES datasets, we also used a residual connection between layers. ", "page_idx": 26}, {"type": "text", "text": "F.3 HyperParameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we detail the hyperparameter search conducted for our experiments. Besides standard hyperparameters such as learning rate and dropout, our specific hyperparameters are: ", "page_idx": 26}, {"type": "text", "text": ". Laplacian Dimension: This refers to the number of columns used in the matrix $U$ , where $L\\doteq U^{T}\\lambda U$ , for the spectral clustering in the coarsening function. ", "page_idx": 26}, {"type": "text", "text": "2. SPD Dimension: This represents the number of indices used in the node marking equation. To clarify, since $|S|$ might be large, we opt for using the first $k$ indices that satisfy $i\\in S$ , sorted according to the SPD distance. ", "page_idx": 26}, {"type": "text", "text": "SPD Dimension. For the Laplacian dimension, we chose a fixed value of 10 for all bag sizes for both ZINC-12K and ZINC-FULL datasets. For OGBG-MOLHIV, we used a fixed value of 1, since the value 10 did not perform well. For the PEPTIDES datasets, we also used the value 1. For the OGBG-MOLESOL and OGBG-MOLBACE datasets, we searched over the two values $\\{1,2\\}$ . ", "page_idx": 26}, {"type": "text", "text": "Laplacian Dimension. For the Laplacian dimension, we searched over the values $\\{1,2\\}$ for all datasets. ", "page_idx": 26}, {"type": "text", "text": "Standard Hyperparameters. For ZINC-12K, we used a weight decay of 0.0003 for all bag sizes, except for the full bag size, for which we used 0.0001. ", "page_idx": 26}, {"type": "text", "text": "All of the hyperparameter search configurations are presented in Table 6, and the selected hyperparameters are presented in Table 7. ", "page_idx": 26}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/05334f78319ffbd437e99f19ed3b0964645ca4584adb02633ac58b64eb634863.jpg", "table_caption": ["Table 6: Hyperparameters search for CS-GNN. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/98b637e1ec5cd5dec57a95ab032fffddd46da309ef7939a99c6bb039cd14bae4.jpg", "table_caption": ["Table 7: Chosen Hyperparameters for CS-GNN "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Optimizers and Schedulers. For the ZINC-12K and ZINC-FULL datasets, we employ the Adam optimizer paired with a ReduceLROnPlateau scheduler,factor set to 0.5, patience at $\\mathrm{\\overline{{40^{14}}}}$ , and a minimum learning rate of 0. For the OGBG-MOLHIV dataset, we utilized the ASAM optimizer [21] without a scheduler. For both OGBG-MOLESOL and OGBG-MOLBACE, we employed a constant learning rate without any scheduler. Lastly, for the PEPTIDES-FUNC and PEPTIDES-STRUCT datasets, the AdamW optimizer was chosen in conjunction with a cosine annealing scheduler, incorporating 10 warmup epochs. ", "page_idx": 27}, {"type": "text", "text": "F.4 Implementation of Linear Equivariant and Invariant layers \u2013 Extended Section ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, in a more formal discussion, we specify how to integrate those invariant and equivariant layers to our proposed architecture. We start by drawing an analogy between parameter sharing in linear layers and the operation of an MPNN on a fully connected graph with edge features in the following lemma, ", "page_idx": 27}, {"type": "text", "text": "Lemma F.1 (Parameter Sharing as MPNN). Let $B_{1},...\\,B_{k}:\\mathbb{R}^{n\\times n}$ be orthogonal matrices with entries restricted to $\\boldsymbol{O}$ or $^{\\,l}$ , and let $W_{1},\\dots W_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}}$ denote a sequence of weight matrices. Define $\\textstyle B_{+}=\\sum_{i=1}^{k}B_{i}$ Tahned  fcuhnocotisoe $\\boldsymbol{z}_{1},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\boldsymbol{z}_{k}\\in\\mathbb{R}^{d^{*}}$ nttso  abne  ua psdeat toef  vuian ipqaure avmeectteorr ss hreaprirnesge:nting an encoding ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(X)=\\sum_{i=1}^{k}B_{i}X W_{i},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "can be implemented by a stack of MPNN layers of the following form $[I3J_{:}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{m_{u}^{l}=\\displaystyle\\sum_{v\\in N_{B_{+}}(u)}M^{l}(X_{v}^{l},e_{u,v}),,}}\\\\ {{\\qquad\\qquad\\qquad}}\\\\ {{X_{u}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $U^{l},M^{l}$ are multilayer preceptrons $(M L P s,$ ). The inputs to this MPNN are the adjacency matrix $B_{+}$ , node feature vector $X$ , and edge features \u2013 the feature of edge $(u,v)$ is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\ne_{u,v}=\\sum_{i=1}^{k}z_{i}\\cdot B_{i}(u,v).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $B_{i}(u,v)$ denotes the $(u,v)$ entry to matrix $B_{i}$ . ", "page_idx": 27}, {"type": "text", "text": "The proof is given in Appendix G. ", "page_idx": 27}, {"type": "text", "text": "Table 8: Comparison over the ZINC-FULL molecular dataset under $500k$ parameter budget. The best performing method is highlighted in blue, while the second best is highlighted in red. ", "page_idx": 28}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/168e5e00135dc8a8e9bba2151bb6f6c722bcb8fdec84ab7691e93a991ac63b49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Thus, our implementation for the global update is as follows, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{X}(S,i)=\\mathtt{M L P}\\left((1+\\epsilon)\\cdot\\mathcal{X}(S,i)+\\sum_{(S^{\\prime},i^{\\prime})\\sim\\mathcal{A}_{\\mathtt{E q i v}}(S,i)}\\mathtt{M L P}\\Big(\\mathcal{X}(S^{\\prime},i^{\\prime})+e_{(S^{\\prime},i^{\\prime}),(S,i)}\\Big)\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{e_{(S^{\\prime},i^{\\prime}),(S,i)}=\\sum_{\\Gamma}z_{\\Gamma}\\cdot\\mathbf{B}_{S,i;S^{\\prime},i^{\\prime}}^{\\Gamma}}\\end{array}$ and $z_{\\Gamma}$ are orthogonal 1-hot vectors for different $\\Gamma$ \u2019s. The connectivity $A_{E q u i v}$ is such that $\\mathcal{A}_{E q u i v}(S,v,S^{\\prime},v^{\\prime})$ contains the value one iff $v\\,\\in\\,S,v\\,=\\,v^{\\prime}$ This corresponds to choosing only several $\\Gamma$ \u2019s in the partition, and since each $\\Gamma$ is invariant to the permutation, this choice still maintains equivariance. ", "page_idx": 28}, {"type": "text", "text": "F.5 Additional Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "ZINC-FULL. Below, we present our results on the ZINC-FULL dataset for a bag size of $T=4$ and the full-bag. For the bag size $T=4$ , we benchmark against MAG-GNN [20], which in their experiments used the best out of the bag sizes $T\\in\\{2,3,4\\}$ ; however, they did not specify which one performed the best. The results are summarized in Table 8. ", "page_idx": 28}, {"type": "text", "text": "ZINC-12K \u2013 additional results. We present all the results from Figure 2, along with some additional ones, in Table 9. ", "page_idx": 28}, {"type": "text", "text": "Runtime comparison. We compare the training time and prediction performance on the ZINC-12K dataset. For all methods, we report the training and inference times on the entire training and test sets, respectively, using a batch size of 128. Our experiments were conducted using an NVIDIA L40 GPU, while for the baselines, we used the timing reported in [5], which utilized an RTX A6000 GPU. The runtime comparison is presented in Table 10. ", "page_idx": 28}, {"type": "text", "text": "F.6 ZINC12K Product Graph Visualization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this subsection, we visualize the product graph derived from the first graph in the ZINC12K dataset. Specifically, we present the right part of Figure 1, for the case of the real-world graphs in the $Z_{\\mathrm{INC}}12\\mathrm{K}$ dataset. We perform this visualization for different cluster sizes, $T\\in\\{2,3,4,5,8,12\\}$ , which also define the bag size, hence the notation $T$ . The nodes in the product graph, $\\tau(G)\\boxed{G}G$ , are $(S,v)$ , where $S$ is the coarsened graph node (again a tuple), and $v$ is the node index (of a node from the original graph). For better clarity, we color the nodes $(S,v)$ with $v\\in S$ using different colors, while reserving the gray color exclusively for nodes $(S,v)$ where $v\\not\\in S$ . The product graphs are visualized in Figures 5 to 10 below. ", "page_idx": 28}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/739d8b3a337d4f8f56b7a2d6b1f2149b14e3f55d00a9a4d4376102e678f89da9.jpg", "table_caption": ["Table 9: Test results on the ZINC-12K molecular dataset under $500k$ parameter budget. The top two results are reported as First and Second. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "9cFyqhjEHC/tmp/ac3794b76a2fe5e79e4594155dca21cb2ee8eba2a94ba6c291e794d210c06462.jpg", "table_caption": ["Table 10: Run time comparison over the ZINC-12K dataset. Time taken at train for one epoch and at inference on the test set. All values are in milliseconds. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/f1dfcdbe7bbab522f315cdb1781d3ca84ed6023d5dab2d0c8c32c9fdda9b1999.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 5: $T=2$ . ", "page_idx": 30}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/cf7765b4aa2d695e09f7c7d77f83950a0414ec1e441618fba3e89aa228eb87ed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 6: $T=3$ . ", "page_idx": 30}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/a8477019588afae8c3d4da4e8e17335561e0320348b4a656078b7bb4b1751b03.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/da95d7aa0e4d7fdda071804d3d743a22906a9aa24e20388d9af794a7ac226754.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 8: $T=5$ . ", "page_idx": 31}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/a4414c320d16726625ef284032f07b7ee992c1054f7e6941eadf1bd2a50b0351.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 9: $T=8$ . ", "page_idx": 31}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/77aab03ba4ac308358213c7f3756c68ed6f21757179393faac777c8fb0138e78.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 10: $T=12$ . ", "page_idx": 31}, {"type": "text", "text": "G Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "G.1 Proofs of Appendix B ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We first state the memorization theorem, proven in [36] , which will be heavily used in a lot of the proofs in this section. ", "page_idx": 32}, {"type": "text", "text": "Theorem G.1 (Memorization Theorem). Consider a dataset $\\{x_{j},y_{j}\\}_{j=1}^{N}\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d_{y}}$ , with each $x_{j}$ being distinct and every $y_{j}\\in\\{0,1\\}^{d_{y}}$ . There exists a $^{4}$ -layer fully connected ReLU neural network $f_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d_{y}}$ that perfectly maps each $x_{j}$ to its corresponding $y_{j}$ , i.e., $f_{\\theta}(x_{j})=y_{j}$ for all $j$ . ", "page_idx": 32}, {"type": "text", "text": "We now restate and prove the propositions and lemmas of Appendix B. ", "page_idx": 32}, {"type": "text", "text": "Lemma B.1 (Parameter Sharing as MPNN). Let $B_{1},\\ldots B_{k}:\\mathbb{R}^{n\\times n}$ be orthogonal matrices with entries restricted to $\\boldsymbol{O}$ or $^{\\,l}$ , and let $W_{1},\\dots W_{k}\\in\\mathbb{R}^{d\\times d^{\\prime}}$ denote a sequence of weight matrices. Define $\\textstyle B_{+}=\\sum_{i=1}^{k}B_{i}$ and choose $z_{1},\\dots z_{k}\\in\\mathbb{R}^{d^{*}}$ to be a set of unique vectors representing an encoding of the index set. The function that represents an update via parameter sharing: ", "page_idx": 32}, {"type": "equation", "text": "$$\nf(X)=\\sum_{i=1}^{k}B_{i}X W_{i},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "can be implemented on any finite family of graphs $\\mathcal{G}$ , by a stack of MPNN layers of the following form [13], ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{m_{v}^{l}=\\displaystyle\\sum_{u\\in N_{B_{+}}(v)}M^{l}(X_{u}^{l},e_{u,v}),}}\\\\ {{\\displaystyle X_{v}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $U^{l},M^{l}$ are multilayer perceptrons $(M L P s,$ ). The inputs to this MPNN are the adjacency matrix $B_{+}$ , node feature vector $X$ , and edge features \u2013 the feature of edge $(u,v)$ is given by: ", "page_idx": 32}, {"type": "equation", "text": "$$\ne_{u,v}=\\sum_{i=1}^{k}z_{i}\\cdot B_{i}(u,v).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here, $B_{i}(u,v)$ denotes the $(u,v)$ entry to matrix $B_{i}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Since we are concerned only with input graphs $G$ from a finite family of graphs (where \"finite\" means that the maximal graph size is bounded and all possible node and edge feature values come from a finite set), we assume that for any $v\\in[n],i\\in[k]$ , both the input feature vectors $X_{v}\\in\\mathbb{R}^{d}$ and the encoding vectors $z_{i}\\in\\mathbb{R}^{d^{*}}$ are one-hot encoded. We aim to show that under these assumptions, any function $f(\\cdot)$ of the form 95 can be realized through a single-layer update detailed in Equations 97 , 96, where $M$ is a 4 layer MLP , and $U$ is a single linear layer. The proof involves the following steps: ", "page_idx": 32}, {"type": "text", "text": "1. Compute $[B_{1}X,\\ldots,B_{k}X]$ using the message function $M$ . ", "page_idx": 32}, {"type": "text", "text": "2. Compute $f(X)$ using the update function $U$ . ", "page_idx": 32}, {"type": "text", "text": "Step 1: We notice that for every $i\\in[k],v\\in[n]$ we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n(B_{i}X)_{v}=\\sum_{B_{i}(v,u)=1}X_{u}=\\sum_{u\\in N_{B_{+}}(v)}X_{u}\\cdot\\mathbf{1}_{z_{i}}(e_{u,v}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here $\\mathbf{1}_{z_{i}}$ is the indicator function of the set $\\{z_{i}\\}$ . We notice that since $X_{u}$ and $z_{i}$ are one-hot encoded, there is a finite set of possible values for the pair $(X_{u},e_{u,v})$ . In addition, the function: ", "page_idx": 32}, {"type": "equation", "text": "$$\ne n c(X_{u},e_{u,v})=[X_{u}\\cdot\\mathbf{1}_{z_{1}}(e_{u,v}),\\dots,X_{u}\\cdot\\mathbf{1}_{z_{k}}(e_{u,v})]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "outputs vectors in the set $\\{0,1\\}^{d\\times k}$ . Thus, employing the memorization theorem G.1, we define a dataset $\\{x_{j},y_{j}\\}_{j=1}^{N}$ by taking the $x_{j}{\\mathbf s}$ to be all possible (distinct) values of $(X_{u},e_{u,v})$ with each ", "page_idx": 32}, {"type": "text", "text": "corresponding $y_{i}$ being the output enc $\\left(x_{i}\\right)$ . We note that there are finitely many such values as both $X_{u}$ and $e_{u,v}$ are one-hot encoded. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network $M$ such that: ", "page_idx": 33}, {"type": "equation", "text": "$$\nM(X_{u},e_{u,v})=e n c(X_{u},e_{u,v}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and so, equation 100 implies: ", "page_idx": 33}, {"type": "equation", "text": "$$\nm_{v}=\\sum_{u\\in N_{B_{+}}(v)}M(X_{u},e_{u,v})=[(B_{1}X)_{v},\\ldots,(B_{k}X)_{v}].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Step 2: Define $P_{i}:\\mathbb{R}^{k\\times d}\\rightarrow\\mathbb{R}^{d}$ as the projection operator, extracting coordinates $d\\cdot i+1$ through $d\\cdot(i+1)$ from its input vector: ", "page_idx": 33}, {"type": "equation", "text": "$$\nP_{i}(V)=V|_{d\\cdot i+1:d\\cdot(i+1)}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We define the update function to be the following linear map: ", "page_idx": 33}, {"type": "equation", "text": "$$\nU(X_{v},m_{v})=\\sum_{i=1}^{k}P_{i}(m_{v})W_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining equations 103 and 105 we get: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{X}_{v}=U(X_{v},m_{v})=\\sum_{i=1}^{k}(B_{i}X)_{v}\\cdot W_{i}=f(X)_{v}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proposition B.1 (Equivalence of General Layer and Implemented Layer). Let $\\tau(\\cdot)$ be a coarsening function, $\\pi$ be a generalized node marking policy, and $\\mathcal{G}$ be a finite family of graphs. Applying a stack of $t$ general layer updates as defined in Equation 15 to the node feature map ${\\mathcal{X}}(S,v)$ induced by $\\pi(G,\\tau)$ , can be effectively implemented by applying a stack of $t$ layer updates specified in Equations 29 and 30 to ${\\mathcal{X}}(S,v)$ . Additionally, the depths of all MLPs that appear in 29 and 30 can be bounded by 4. ", "page_idx": 33}, {"type": "text", "text": "Proof. For convenience, let us first restate the general layer update: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{X}^{t+1}(S,v)=f^{t}\\bigg(\\mathcal{X}^{t}(S,v),}\\\\ &{\\qquad\\qquad\\quad\\mathrm{\\arg}t\\,[\\,\\{\\left(\\mathcal{X}^{t}(S,v^{\\prime}),e_{v,v^{\\prime}}\\right)\\mid v^{\\prime}\\sim_{G}v\\},}\\\\ &{\\qquad\\qquad\\quad\\mathrm{\\arg}t\\,[\\,\\left(\\mathcal{X}^{t}(S^{\\prime},v),\\tilde{e}_{S,S^{\\prime}}\\right)\\mid S^{\\prime}\\sim_{G^{\\prime}}S\\,]\\,,}\\\\ &{\\qquad\\quad\\mathrm{\\arg}t\\,[\\,\\left(\\mathcal{X}^{t}(S^{\\prime},v),z(S,v,S^{\\prime},v)\\right)\\mid S^{\\prime}\\in V^{T}{\\mathrm{s.t.}}\\,v\\in S^{\\prime}\\mathbb{J}\\bigg),}\\\\ &{\\qquad\\quad\\mathrm{\\arg}t\\,[\\,\\left(\\mathcal{X}^{t}(S,v^{\\prime}),z(S,v,S,v^{\\prime})\\right)\\mid v^{\\prime}\\in V{\\mathrm{s.t.}}\\,v^{\\prime}\\in S\\mathbb{J}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "as well as the two step implemented layer update: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{X}_{i}^{t}(S,v)=U_{i}^{t}\\left((1+\\epsilon_{i}^{t})\\cdot\\mathcal{X}^{t}(S,v)+\\sum_{(S^{\\prime},v^{\\prime})\\sim A_{i}(S,v)}M^{t}(\\mathcal{X}^{t}(S^{\\prime},v^{\\prime})+e_{i}(S,v,S^{\\prime},v^{\\prime}))\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{X}^{t+1}(S,v)=\\mathbf{U}_{\\mathrm{fn}}^{t}\\left(\\sum_{i=1}^{4}\\mathcal{X}_{i}^{t}(S,v)\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We aim to demonstrate that any general layer, which updates the node feature map $\\mathcal{X}^{t}(S,v)$ at layer $t$ to node feature map $\\bar{\\mathcal{X}}^{t+1}(\\bar{S_{,}}\\bar{v})$ at layer $t+1$ as described in equation 15, can be effectively implemented using the layer update processes outlined in equations 29 and 30. ", "page_idx": 33}, {"type": "text", "text": "As we are concerned only with input graphs belonging to the finite graph family $\\mathcal{G}$ (where \"finite\" indicates that the maximal graph size is bounded and all node and edge features have a finite set of possible values), we assume that the values of the node feature map $\\mathcal{X}^{t}(S,v)$ and the edge feature vectors $e_{i}(S,v,S^{\\prime},v^{\\prime})$ are represented as one-hot vectors in $\\mathbb{R}^{k}$ . We also assume that the parameterized functions $f^{t}$ and $\\mathrm{agg}_{1}^{t},\\dotsc\\mathrm{agg}_{4}^{t}$ , which are applied in Equation 15 outputs one-hot vectors. Finally, we assume that there exists integers $d,d^{*}$ , such that the node feature map values are supported on coordinates $1,\\cdot\\cdot d$ , the edge feature vectors are supported on coordinates $\\bar{d+1},\\ldots{d+}$ $d^{\\ast}$ , and coordinates $d+d^{*}+1,\\ldots k$ are used as extra memory space, with: ", "page_idx": 34}, {"type": "equation", "text": "$$\nk>d\\times d^{*}+d+d^{*}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We note that the last assumption can be easily achieved using padding. The proof involves the following steps: ", "page_idx": 34}, {"type": "text", "text": "1. For $i=1,\\dots,4$ , Use the term: ", "page_idx": 34}, {"type": "equation", "text": "$$\nm_{i}^{t}=\\sum_{(S^{\\prime},v^{\\prime})\\sim_{A_{i}}(S,v)}M^{t}(\\mathcal{X}^{t}(S^{\\prime},v^{\\prime})+e_{i}(S,v,S^{\\prime},v^{\\prime}))\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "to uniquely encode: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\{(\\mathcal{X}^{t}(S^{\\prime},v^{\\prime}),e_{i}(S,v,S^{\\prime},v^{\\prime}))\\mid(S^{\\prime},v^{\\prime})\\sim_{A_{i}}(S,v)\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "2. Use the term: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{X}_{*}^{t}=\\sum_{i=1}^{4}\\mathcal{X}_{i}^{t}(S,v)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "to uniquely encode the input of $f^{t}$ as a whole. ", "page_idx": 34}, {"type": "text", "text": "3. Implement the parameterized function $f^{t}$ . ", "page_idx": 34}, {"type": "text", "text": "Step 1: Since we assume that node feature map values and edge feature vectors are supported on orthogonal sub-spaces of $\\mathbb{R}^{k}$ , the term: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{X}^{t}(S,v)+e_{i}(S,v,S^{\\prime},v^{\\prime})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "uniquely encodes the value of the tuple: ", "page_idx": 34}, {"type": "equation", "text": "$$\n(\\mathcal{X}^{t}(S,v),e_{i}(S,v,S^{\\prime},v^{\\prime})).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\mathcal{X}^{t}(S,v)$ is a one-hot encoded vector with $d$ possible values, while $e_{i}(S,v,S^{\\prime},v^{\\prime})$ is a one-hot encoded vector with $d^{\\ast}$ possible values, their sum has $d\\cdot d^{*}$ possible values. Thus there exists a function: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{enc}:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{k}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which encodes each such possible value as a one-hot vector in $\\mathbb{R}^{k}$ supported on the last $k-d-d^{*}$ coordinates (this is possible because of equation 107). Now, employing theorem G.1, we define the $x_{j}\\mathbf{s}$ as all possible (distinct) values of 111, with each corresponding $y_{j}$ being the output enc $\\left(x_{j}\\right)$ . The theorem now tells us that there exists a 4-layer fully connected ReLU neural network capable of implementing the function enc(\u00b7). We choose $M^{t}$ to be this network. Now since $m_{i}^{t}$ , defined in equation 108 is a sum of one-hot encoded vectors, it effectively counts the number of each possible value in the set 109. This proves step 1. ", "page_idx": 34}, {"type": "text", "text": "Step 2: First, we note that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\frac{\\{\\mathbb J}\\left({\\mathcal X}^{t}\\left(S,v^{\\prime}\\right),e_{v,v^{\\prime}}\\right)\\mid v^{\\prime}\\sim{_G{\\mathcal V}})\\!\\!\\mid}{0}}\\\\ &{\\ \\ \\ =\\left\\{{\\mathbb J}\\left({\\mathcal X}^{t}(S^{\\prime},v^{\\prime}),e_{G}(S,v,S^{\\prime},v^{\\prime})\\right)\\mid(S,v)\\sim_{A_{G}}(S^{\\prime},v^{\\prime})\\right\\}}\\\\ &{\\ \\ \\ }\\\\ &{\\ \\ \\frac{{\\mathbb J}\\left({\\mathcal X}^{t}\\left(S^{\\prime},v\\right),e_{s^{\\prime},s}\\right)\\mid S^{\\prime}\\sim{_{T(G)}}S\\Vert}{0}}\\\\ &{\\ =\\left\\{{\\mathbb J}\\left({\\mathcal X}^{t}(S^{\\prime},v^{\\prime}),e_{T(G)}(S,v,S^{\\prime},v^{\\prime})\\right)\\mid(S,v)\\sim_{A_{T(G)}}(S^{\\prime},v^{\\prime})\\right\\}}\\\\ &{\\ \\ \\ \\ \\{{\\mathbb J}\\left({\\mathcal X}^{t}(S^{\\prime},v),z(S,v,S^{\\prime},v^{\\prime})\\right)\\mid v\\in S^{\\prime}\\right\\}}\\\\ &{\\ \\ =\\left\\{{\\mathbb J}\\left({\\mathcal X}^{t}(S^{\\prime},v^{\\prime}),e_{P_{1}}(S,v,S^{\\prime},v^{\\prime})\\right)\\mid(S,v)\\sim_{A_{P_{1}}}(S^{\\prime},v^{\\prime})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\left(\\mathcal{X}^{t}(S,v^{\\prime}),z(S,v,S^{\\prime},v^{\\prime})\\right)\\mid v^{\\prime}\\in S\\right\\}}\\\\ &{=\\left\\{\\left(\\mathcal{X}^{t}(S^{\\prime},v^{\\prime}),e_{P_{2}}(S,v,S^{\\prime},v^{\\prime})\\right)\\mid(S,v)\\sim_{A_{P_{2}}}(S^{\\prime},v^{\\prime})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, since $m_{i}^{t}$ and $\\mathcal{X}^{t}(S,v)$ are supported on orthogonal sub-spaces of $\\mathbb{R}^{k}$ , the sum $\\mathcal{X}^{t}(S,v)+m_{i}^{t}$ uniquely encodes the value of: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(\\mathcal{X}^{t}(S,v),\\{(\\mathcal{X}^{t}(s,v),e_{i}(S,v,S^{\\prime},v^{\\prime}))\\mid(S,v)\\sim_{A_{i}}(S,^{\\prime}v^{\\prime})\\}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, we choose $\\epsilon_{1}^{t},\\dots,e p s i l o n_{4}^{t}$ to be all zeroes. To compute the aggregation functions $\\arg_{1}^{t},\\dots,\\arg_{4}^{t}$ using these unique encodings, and to avoid repetition of the value $\\mathcal{X}^{t}(S,v)$ , we define auxiliary functions $\\widetilde{\\bf{a g}}\\mathbf{g}_{i}^{t}:\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{k_{i}}$ for $i=1,\\dots,4$ as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{a}\\tilde{\\mathbf{g}}_{1}^{t}(\\mathcal{X}^{t}(S,v)+m_{1}^{t})=\\big(\\mathcal{X}^{t}(S,v),\\mathrm{agg}_{1}^{t}\\mathfrak{g}(\\mathcal{X}^{t}(S,v^{\\prime}),e_{1}(S,v,S^{\\prime},v^{\\prime}))\\mid(S,v)\\sim_{A_{1}}(S,^{\\prime}v^{\\prime})\\mathbb{\\mathbb{J}}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and for $i>1$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathtt{a}\\tilde{\\mathtt{g}}\\mathrm{g}_{i}^{t}(\\mathcal{X}^{t}(S,v)+m_{i}^{t})=\\mathtt{a}\\mathrm{g}\\mathrm{g}_{i}^{l}\\mathbb{f}(\\mathcal{X}^{t}(s,v),e_{i}(S,v,S^{\\prime},v^{\\prime}))\\mid(S,v)\\sim_{A_{i}}(S,'v^{\\prime})\\mathbb{j}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here, since we avoided repeating the value of $\\mathcal{X}^{t}(S,v)$ by only adding it to the output of $\\mathrm{a}\\tilde{\\mathrm{g}}\\mathrm{g}_{1}^{t}(\\cdot)$ , the expression: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(\\mathbf{a}\\mathbf{\\tilde{g}}\\mathbf{g}_{1}^{t}(\\mathcal{X}^{t}(S,v)+m_{1}^{t}),\\ldots,\\mathbf{a}\\mathbf{\\tilde{g}}\\mathbf{g}_{4}^{t}(\\mathcal{X}^{t}(S,v)+m_{4}^{t})\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "is exactly equal to the input of $f^{t}$ . In addition, since the function $\\mathrm{agg}_{i}^{t}$ outputs one-hot encoded vectors, and the vector $\\mathcal{X}^{t}(S,v)$ is one-hot encoded, the output of $a\\tilde{\\mathbf{g}}\\mathbf{g}_{i}^{t}$ is always within the set $\\{0,1\\}^{k_{i}}$ . Now for any input vector $X\\in\\mathbb{R}^{k}$ define: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}^{t}(X)=\\big(\\mathsf{a}\\tilde{\\mathsf{g}}\\mathsf{g}_{1}^{t}(X),\\ 0_{k2},\\ 0_{k3},\\ 0_{k4}\\big).}\\\\ &{V_{2}^{t}(X)=\\big(0_{k_{1}},\\ \\mathsf{a}\\tilde{\\mathsf{g}}\\mathsf{g}_{2}^{t}(X),\\ 0_{k3},\\ 0_{k4}\\big).}\\\\ &{V_{3}^{t}(X)=\\big(0_{k_{1}},\\ 0_{k_{2}},\\ \\mathsf{a}\\tilde{\\mathsf{g}}\\mathsf{g}_{3}^{t}(X),\\ 0_{k4}\\big).}\\\\ &{V_{4}^{t}(X)=\\big(0_{k_{1}},\\ 0_{k2},\\ 0_{k3},\\ \\mathsf{a}\\tilde{\\mathsf{g}}\\mathsf{g}_{4}^{t}(X)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We note that since the output of $\\mathrm{agg}_{i}^{t}$ is always within the set $\\{0,1\\}^{k_{i}}$ , the outputs of $V_{i}^{t}$ is always within $\\{0,1\\}^{k_{1}+\\cdots+k_{4}}$ . Now for $i=1,\\dots4$ , employing theorem G.1 we define a dataset $\\{x_{j},y_{j}\\}_{j=1}^{N}$ by taking the $x_{j}{\\mathbf s}$ as all possible (distinct) values of $\\mathcal{X}^{t}(S,v)+m_{i}^{t}$ , with each corresponding $y_{j}$ being the output $V_{i}^{t}(x_{j})$ . We note that there are finitely many such values as both $\\bar{\\mathcal{X}}^{t}(\\bar{S_{}},v)$ and $m_{i}^{t}$ are one-hot encoded vectors. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function $V_{i}^{t}(\\cdot)$ . We choose $U_{i}^{\\bar{t}}$ to be this network. Equations 121 - 124 now give us: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{4}\\chi_{i}^{t}(S,v)=\\left(\\mathtt{a}\\tilde{\\mathtt{g}}\\mathtt{g}_{1}^{t}(\\mathcal{X}^{t}(S,v)+m_{1}^{t}),\\dots,\\mathtt{a}\\tilde{\\mathtt{g}}\\mathtt{g}_{4}^{t}(\\mathcal{X}^{t}(S,v)+m_{4}^{t})\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which as stated before, is exactly the input to $f^{t}$ . This proves step 2. ", "page_idx": 35}, {"type": "text", "text": "Step 3: We employ theorem G.1 for one final time, defining a dataset $\\{x_{j},y_{j}\\}_{j=1}^{N}$ by taking the $x_{j}\\mathbf{s}$ as all possible(distinct) values of: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{4}\\chi_{i}^{t}(S,v)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(which we showed is a unique encoding to the input of $f^{t}(\\cdot))$ , with each corresponding $y_{j}$ being the output $f^{t}(x_{j})$ . We note that Given the finite nature of our graph set, there are finitely many such values. Recalling that $f^{t}(\\cdot)$ outputs one-hot encoded vectors, The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function $f^{t}(\\cdot)$ . We choose $U_{\\mathrm{{fin}}}^{\\bar{t}}$ to be this network. This completes the proof. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "G.2 Proofs of Appendix C ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proposition C.1 (Equal Expressivity of Node Marking Policies). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 36}, {"type": "equation", "text": "$$\nC S\\!-\\!G N N(\\mathcal{T},\\pi_{S})=C S\\!-\\!G N N(\\mathcal{T},\\pi_{S S})=C S\\!-\\!G N N(\\mathcal{T},\\pi_{M D}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $\\Pi=\\{\\pi_{\\mathrm{S}},\\pi_{\\mathrm{SS}},\\pi_{\\mathrm{MD}}\\}$ be the set of all relevant node initialization policies, and assume for simplicity that our input graphs have no node features (the proof can be easily adjusted to account for the general case). For each $\\pi\\in\\Pi$ , let ${\\mathcal{X}}^{\\pi}(S,v)$ denote the node feature map induced by general node marking policy $\\pi$ , as per Definition C.1. We notice it is enough to prove for each $\\pi_{1},\\pi_{2}\\in\\Pi$ that $\\mathcal{X}^{\\pi_{1}}(S,v)$ can be implemented by updating ${\\mathcal{X}}^{\\pi_{2}}(S,v)$ using a stack of $T$ layers of type 54. Thus, we prove the following four cases: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Node $^+$ Size Marking $\\Rightarrow{}$ Simple Node Marking.   \n\u2022 Minimum Distance $\\Rightarrow{}$ Simple Node Marking.   \n\u2022 Simple Node Marking $\\Rightarrow{}$ Node $^+$ Size Marking.   \n\u2022 Simple Node Marking $\\Rightarrow{}$ Minimum Distance. ", "page_idx": 36}, {"type": "text", "text": "Node $^+$ Size Marking $\\Rightarrow{}$ Simple Node Marking: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this case, we aim to update the node feature map: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\chi^{\\pi_{S S}}(S,v)={\\binom{(1,|S|)}{(0,|S|)}}\\quad v\\in S\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We notice that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\langle(1,0),\\,\\chi^{\\pi_{S}}(S,v)\\rangle,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ denotes the standard inner product in $\\mathbb{R}^{2}$ . Using a CS-GNN update as per equation 15, with the update function: ", "page_idx": 36}, {"type": "equation", "text": "$$\nf^{1}(\\mathcal{X}^{0}(S,v),\\cdot,\\cdot,\\cdot)=\\langle(1,0),\\mathcal{X}^{0}(S,v)\\rangle,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $f(a,\\cdot,\\cdot,\\cdot,\\cdot)$ indicates that the function $f$ depends solely on the parameter $a$ , we obtain: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{1}(S,v)=f^{1}(\\chi^{0}(S,v),\\cdot,\\cdot,\\cdot)=\\chi^{\\pi_{S}}(S,v).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that for any coarsening function $\\tau(\\cdot)$ , the following holds: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})\\subseteq\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{SS}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Minimum Distance $\\Rightarrow{}$ Simple Node Marking: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this case, we aim to update the node feature map: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{X}^{0}(S,v)=\\mathcal{X}^{\\pi_{\\mathrm{MD}}}(S,v)=\\operatorname*{min}_{v\\in s}d_{G}(u,v)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We notice that: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{\\mathrm{{S}}}(S,v)=g(\\chi^{0}(S,v))\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ is any continuous function such that: ", "page_idx": 36}, {"type": "text", "text": "Using a CS-GNN update as per equation 15, with the update function: ", "page_idx": 36}, {"type": "equation", "text": "$$\nf^{1}(\\mathcal{X}^{0}(S,v),\\cdot,\\cdot,\\cdot)=g(\\mathcal{X}^{0}(S,v)),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "we obtain: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{1}(S,v)=f^{1}(\\chi^{0}(S,v),\\cdot,\\cdot,\\cdot)=\\chi^{\\pi s}(S,v).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that for any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})\\subseteq\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{MD}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Simple Node Marking $\\Rightarrow{}$ Node $^+$ Size Marking: In this case, we aim to update the node feature map: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\chi^{\\pi_{S}}(S,v)={\\binom{1}{0}}\\quad v\\in S\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We notice that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{v^{\\prime}\\in S}\\chi^{0}(S,v^{\\prime})=|S|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using a CS-GNN update as per Equation (15), with aggregation function: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\log_{4}^{l}\\{\\mathbb{f}(\\mathcal{X}^{0}(S,v^{\\prime}),z(S,v,S,v^{\\prime}))\\mid v^{\\prime}\\in S\\}=\\sum_{v^{\\prime}\\in S}\\mathcal{X}^{0}(S,v^{\\prime}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and update function: ", "page_idx": 37}, {"type": "equation", "text": "$$\nf^{1}\\left(\\mathcal{X}^{0}(S,v),\\cdot,\\cdot,\\cdot,\\sum_{v^{\\prime}\\in S}\\mathcal{X}^{0}(S,v^{\\prime})\\right)=\\left(\\mathcal{X}^{0}(S,v),\\sum_{v^{\\prime}\\in S}\\mathcal{X}^{0}(S,v^{\\prime})\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\chi^{1}(S,v)=f^{1}\\left(\\mathcal{X}^{0}(S,v),\\cdot,\\cdot,\\cdot,\\sum_{v^{\\prime}\\in S}\\mathcal{X}^{0}(S,v^{\\prime})\\right)=\\chi^{\\pi_{S S}}(S,v).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This implies that for any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{SS}})\\subseteq\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Simple Node Marking $\\Rightarrow{}$ Minimum Distance: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this case, we aim to update the node feature map: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\chi^{\\pi_{S}}(S,v)={\\left\\{\\begin{array}{l l}{1}&{v\\in S}\\\\ {0}&{v\\not\\in S.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We shall prove that ${\\mathcal{X}}^{\\pi_{\\mathrm{MD}}}$ can be expressed by updating $\\mathcal{X}^{0}(S,v)$ with a stack of CS-GNN layers. We do this by inductively showing that this procedure can express the following auxiliary node feature maps: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{X}_{*}^{t}(S,v)=\\left\\{\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})+1\\right.\\!\\!\\begin{array}{l l}{{\\left.\\!\\!\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})\\leq t\\right.}}\\\\ {\\left.\\!\\!\\operatorname{otherwise}.\\!\\!\\!}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We notice first that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\chi_{*}^{0}(S,v).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now for the induction step, assume that there exists a stack of $t$ CS-GNN layers such that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{X}^{t}(S,v)=\\mathcal{X}_{*}^{t}(S,v).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We observe that equation: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})=t+1\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "holds if and only if the following two conditions are met: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})>t\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\exists u\\in N_{G}(v){\\mathrm{~s.t.~}}\\operatorname*{min}_{u^{\\prime}\\in S}d_{G}(u,u^{\\prime})=t.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Equations 143 imply: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{v^{\\prime}\\in S}d_{G}(v,v^{\\prime})>t\\Leftrightarrow\\mathcal{X}^{t}(S,v)=0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In addition, since the node feature map $\\mathcal{X}^{t}=\\mathcal{X}_{*}^{t}$ is bounded by $t+1$ , Equation (143) implies: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\exists u\\in N_{G}(v){\\mathrm{~s.t.~}}\\operatorname*{min}_{u^{\\prime}\\in S}d_{G}(u,u^{\\prime})=t\\Leftrightarrow\\operatorname*{max}\\{\\mathcal{X}^{t}(s,u)\\mid v\\sim_{G}u\\}=t+1.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now, let $g_{t}:\\mathbb{R}^{2}\\,\\rightarrow\\,\\mathbb{R}$ be any continuous function such that for every pair of natural numbers $a,b\\in\\mathbb{N}$ : ", "page_idx": 38}, {"type": "text", "text": "Equations 146 - 150 imply: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{X}_{*}^{t+1}(S,v)=g_{t}(\\mathcal{X}^{t}(S,v),\\operatorname*{max}\\{\\mathcal{X}^{t}(s,u)\\mid v\\sim_{G}u\\}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using a CS-GNN update as per Equation (15), with aggregation function: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname{agg}_{1}^{t}\\{\\!\\{(\\mathcal{X}^{t}(S,v^{\\prime}),e_{v,v^{\\prime}})\\;|\\;v^{\\prime}\\sim_{G}v\\}\\!\\}=\\operatorname*{max}_{v^{\\prime}\\sim_{G}v}\\mathcal{X}^{t}(S,v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and update function: ", "page_idx": 38}, {"type": "equation", "text": "$$\nf^{t}(\\mathcal{X}^{t}(S,v),\\operatorname*{max}_{v^{\\prime}\\sim_{G}v}\\mathcal{X}^{t}(S,v^{\\prime}),\\cdot,\\cdot,\\cdot)=g_{t}(\\mathcal{X}^{t}(S,v),\\operatorname*{max}_{v^{\\prime}\\sim_{G}v}\\mathcal{X}^{t}(S,v^{\\prime}))\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "we obtain: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{X}^{t+1}(S,v)=f^{t}(\\mathcal{X}^{t}(S,v),\\operatorname*{max}_{v^{\\prime}\\sim_{G}v}\\mathcal{X}^{t}(S,v^{\\prime}),\\cdot,\\cdot,\\cdot)=\\mathcal{X}_{*}^{t+1}(S,v).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This completes the induction step. Now, let $\\mathcal{G}$ be a finite family of graphs, whose maximal vertex size is $n$ . We notice that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\chi^{\\pi_{\\mathrm{MD}}}(S,v)=\\chi_{*}^{n}(S,v)-1,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Which implies that there exists a stack of $n$ CS-GNN layers such that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\chi^{\\pi_{\\mathrm{S}}}(S,v)\\quad\\mathrm{and}\\quad\\chi^{n}(S,v)=\\chi^{\\pi_{\\mathrm{MD}}}(S,v).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This implies: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{MD}})\\subseteq\\operatorname{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 38}, {"type": "image", "img_path": "9cFyqhjEHC/tmp/8bd8736d6d75a1fb98288eeea47243bf236f9a06d12495a4f05a151575475a98.jpg", "img_caption": ["Figure 11: Graphs $\\mathrm{G}$ and $\\mathrm{H}$ defined in the proof of Proposition C.2. In each graph, the circle marks the single super-node induced by $\\tau$ , while the number next to each node $u$ is the maximal SPD between $u$ and the nodes that compose the super-node. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Proposition C.2 (Expressivity of Learned Distance Policy). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 39}, {"type": "equation", "text": "$$\nC S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{S})\\subseteq C S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{L D}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In addition, for some choices of $\\tau(\\cdot)$ the containment is strict. ", "page_idx": 39}, {"type": "text", "text": "Proof. First, since we are concerned with input graphs belonging to a finite graph family $\\mathcal{G}$ , the learned function $\\phi(\\cdot)$ implemented by an MLP can express any continuous function on $\\mathcal{G}$ . This follows from Theorem G.1 (see the proof of Proposition B.1 for details). By choosing $\\phi=\\operatorname*{min}(\\cdot)$ in equation 35, it is clear that for any coarsening function $\\tau(\\cdot)$ we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CS}\\mathrm{-}\\mathrm{GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})=\\mathrm{CS}\\mathrm{-}\\mathrm{GNN}(\\mathcal{T},\\pi_{\\mathrm{MD}})\\subseteq\\mathrm{CS}\\mathrm{-}\\mathrm{GNN}(\\mathcal{T},\\pi_{\\mathrm{LD}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now construct a coarsening function $\\tau(\\cdot)$ along with two graphs, $G$ and $H$ , and demonstrate that there exists a function in ${\\mathrm{CS-GNN}}({\\mathcal{T}},{\\pi}_{\\mathrm{LD}})$ that can separate $G$ and $H$ . However, every function in $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ cannot separate the two. ", "page_idx": 39}, {"type": "text", "text": "For an input graph $G=(V,E)$ define: ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{u\\in V\\mid\\deg_{G}(u)=3\\}\\}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "i.e., $\\tau(\\cdot)$ returns a single super-node composed of all nodes with degree 3. Now, define $G=(V_{G},E_{G})$ as the graph obtained by connecting two cycles of size four by adding an edge between a single node from each cycle. Additionally, define $\\dot{H}\\overset{\\cdot}{=}(V_{H},E_{H})$ as the graph formed by joining two cycles of size five along one of their edges. See Figure 11 for an illustration of the two graphs. By choosing $\\phi=\\operatorname*{max}(\\cdot)$ in equation 35 a quick calculation shows that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{S\\in V_{T(G)}}\\sum_{v\\in V_{G}}\\chi^{\\pi_{\\mathrm{LD}}}(S,v)=16,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "while: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{S\\in V_{T(H)}}\\sum_{v\\in V_{H}}\\chi^{\\pi_{\\mathrm{LD}}}(S,v)=14.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Refer to Figure 11 for more details. Observe that: ", "page_idx": 39}, {"type": "equation", "text": "$$\nf(G)=\\sum_{s\\in V_{T(H)}}\\sum_{u\\in V_{H}}\\chi^{\\pi_{\\mathrm{LD}}}(S,v)\\in\\mathbf{CS-}\\mathbf{GNN}(T,\\pi_{\\mathrm{LD}})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus it is enough to show that: ", "page_idx": 39}, {"type": "equation", "text": "$$\nf(G)=f(H),\\quad\\forall f\\in\\mathbf{CS-}\\mathbf{GNN}(\\mathcal{T},\\pi_{\\mathrm{S}}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To achieve this, we use the layer update as per Definition B.2, which was demonstrated in Proposition B.1 to be equivalent to the general equivariant message passing update in Definition A.5. First, we observe that the graphs $G$ and $H$ are WL-indistinguishable. We then observe that since $|V^{\\mathcal{T}}|=1$ , the graphs induced by the adjacency matrices $A_{G}$ and $A_{H}$ in Definition B.1 are isomorphic to the original graphs $G$ and $H$ , respectively, and therefore they are also WL-indistinguishable. Additionally, we notice that the graphs induced by the adjacency matrices $A_{\\mathcal{T}(G)}$ and $A_{\\mathcal{T}(H)}$ in Definition B.1 are both isomorphic to the fully disconnected graph with 8 nodes, making them WL-indistinguishable as well. We also observe that there exists a bijection $\\sigma:V_{G}\\rightarrow V_{H}$ that maps all nodes of degree 3 in $G$ to all nodes of degree 3 in $H$ . The definition of $\\tau(\\cdot)$ implies that $\\sigma$ is an isomorphism between the adjacency matrices $A_{P_{i}}$ corresponding to $G$ and $H$ , where $i=1,2$ . Finally, we notice that for both $G$ , and $H$ , the node feature map induced by $\\pi_{\\mathrm{S}}$ satisfies: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{X}^{\\pi_{\\mathtt{S}}}(S,v)=\\deg(v)-2.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This node feature map can be easily implemented by the layer update in definition B.2 and so it can be ignored. Since all four graphs corresponding to $G$ that are induced by the adjacency matrices in Definition B.1, are WL-indistinguishable from their counterpart corresponding to $H$ , and equation 29 in definition B.2 is an MPNN update, which is incapable of distinguishing graphs that are WL-indistinguishable, we see that equation 163 holds, concluding the proof. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Proposition C.3 (Node $^+$ Size Marking as Invariant Marking). Given a graph $G=(V,E)$ with node feature vector $X\\in\\mathbb{R}^{n\\times d}$ , and a coarsening function $\\tau(\\cdot)$ , let $\\mathcal{X}^{\\pi_{S S}},\\mathcal{X}^{\\pi_{i m}}$ be the node feature maps induced by $\\pi_{S S}$ and $\\pi_{i n\\nu}$ respectively. Recall that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{\\pi_{s s}}(S,v)=[X_{v},b_{\\pi_{s s}}(S,v)],}\\\\ &{\\chi^{\\pi_{i n v}}(S,v)=[X_{v},b_{\\pi_{i n v}}(S,v)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The following now holds: ", "page_idx": 40}, {"type": "equation", "text": "$$\nb_{\\pi_{i n v}}(S,v)=O H E(b_{\\pi_{S S}}(S,v))\\quad\\forall S\\in V^{T},\\;\\forall v\\in V.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Here, OHE denotes a one-hot encoder, independent of the choice of both $G$ and $\\tau$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $G=(V,E)$ be a graph with $V=[n]$ , and let $\\tau(\\cdot)$ be a coarsening function. Recall that the maps $b_{\\pi\\mathrm{ss}}(\\cdot,\\cdot)$ and $b_{\\pi_{\\mathrm{inv}}}(\\cdot,\\cdot)$ are both independent of the connectivity of $G$ and are defined as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\nb_{\\pi_{\\mathrm{SS}}}(S,v)=\\left\\{\\left(1,|S|\\right)\\quad v\\in S,\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\nb_{\\pi_{\\mathrm{inv}}}(S,v)=[\\mathbf{1}_{\\gamma_{1}}(S,v),\\ldots,\\mathbf{1}_{\\gamma_{k}}(S,v)].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Here, $v~\\in~[n]$ $|,\\;S\\;\\in\\;{\\mathcal{T}}([n])\\;\\subseteq\\;{\\mathcal{P}}([n]),\\;\\gamma_{1},...\\,,\\gamma$ is any enumeration of the set of all orbits $(\\mathcal{P}([n])\\,\\times\\,[\\dot{n]})\\big/S_{n}$ , and ${\\mathbf{1}}_{\\gamma_{i}}$ denotes the indicator function of orbit $\\gamma_{i}$ . Since any tuple $(S,v)\\in$ $\\textstyle P([n])\\times[n]$ belongs to exactly one orbit $\\gamma_{i}$ , we note that the right hand side of Equation (166) is a one-hot encoded vector. Thus, it suffices to show that for every $v,v^{\\prime}\\in[n]$ and $S,S^{\\prime}\\in\\mathcal{P}([n])$ , we have: ", "page_idx": 40}, {"type": "equation", "text": "$$\nb_{\\pi_{\\mathrm{SS}}}(S,v)=b_{\\pi_{\\mathrm{SS}}}(S,^{\\prime}v^{\\prime})\\Leftrightarrow b_{\\pi_{\\mathrm{inv}}}(S,v)=b_{\\pi_{\\mathrm{inv}}}(S,^{\\prime}v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This is equivalent to: ", "page_idx": 40}, {"type": "equation", "text": "$$\n(\\mathcal{P}([n])\\times[n])/S_{n}=\\left\\{\\{(S,v)\\mid|S|=i,\\mathbf{1}_{S}(v)=j\\}\\mid i\\in[n],j\\in\\{0,1\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Essentially, this means that each orbit corresponds to a choice of the size of $s$ and whether $v\\in S$ or not. To conclude the proof, it remains to show that for any two pairs $(S,v),(S,^{\\prime}v^{\\prime})\\in\\mathcal{P}([n])\\times[n]$ , there exists a permutation $\\sigma\\in S_{n}$ such that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma\\boldsymbol{\\cdot}(S,v)=(S,^{\\prime}v^{\\prime})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "if and only if ", "page_idx": 40}, {"type": "equation", "text": "$$\n|S|=|S^{\\prime}|\\;\\mathrm{and}\\;{\\bf1}_{S}(v)={\\bf1}_{S^{\\prime}}(v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Assume first that $\\sigma\\cdot(S,v)=(S^{\\prime},v^{\\prime})$ , then $\\sigma^{-1}(S)=S^{\\prime}$ and since $\\sigma$ is a bijection, $\\left|S\\right|=\\left|S^{\\prime}\\right|$ . In addition $\\sigma^{-1}(v)=v^{\\prime}$ thus: ", "page_idx": 40}, {"type": "equation", "text": "$$\nv\\in S\\Leftrightarrow v^{\\prime}=\\sigma^{-1}(v)\\in\\sigma^{-1}(S)=S^{\\prime}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Assume now that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{|S|=|S^{\\prime}|}}\\\\ {{{\\bf1}_{S}(v)={\\bf1}_{S^{\\prime}}(v^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "It follows that for some $r,m\\in[n]$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n|S\\setminus\\{v\\}|=|S^{\\prime}\\setminus\\{v^{\\prime}\\}|=r\\quad{\\mathrm{and}}\\quad|[n]\\setminus(S\\cup\\{v\\})|=|[n]\\setminus(S^{\\prime}\\cup\\{v^{\\prime}\\})|=m\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Write: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S\\setminus\\{v\\}=\\{i_{1},\\dotsc,i_{r}\\},\\quad S^{\\prime}\\setminus\\{v^{\\prime}\\}=\\{i_{1}^{\\prime},\\dotsc,i_{r}^{\\prime}\\},}\\\\ &{[n]\\setminus(S\\cup\\{v\\})=\\{j_{1},\\dotsc j_{m}\\},\\quad[n]\\setminus(S^{\\prime}\\cup\\{v^{\\prime}\\})=\\{j_{1}^{\\prime},\\dotsc j_{m}^{\\prime}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and define: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma(x)=\\left\\{\\!\\!\\begin{array}{l l}{v^{\\prime}}&{x=v}\\\\ {i_{l}^{\\prime}}&{x=i_{l},l\\in[r]}\\\\ {j_{l}^{\\prime}}&{x=j_{l},l\\in[m]}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We now have: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma\\cdot(S,v)=(S^{\\prime},v^{\\prime}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 40}, {"type": "text", "text": "G.3 Proofs of Appendix D.1 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proposition D.1 (CS-GNN Can Implement MSGNN). Let $\\tau(\\cdot)$ be the identity coarsening function defined by: ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\quad\\forall G=(V,E).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The following holds: ", "page_idx": 41}, {"type": "equation", "text": "$$\nC S\\!\\cdot\\!G N\\!N(\\mathcal{T},\\pi_{S})=M S G N\\!N(\\pi_{N M}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Abusing notation, for a given graph $G=(V,E)$ we write $T(G)=G$ , $V^{T}=V$ . First, we observe that: ", "page_idx": 41}, {"type": "equation", "text": "$$\nv\\in\\{u\\}\\Leftrightarrow u=v,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This implies that the initial node feature map $\\mathcal{X}^{0}(u,v)$ induced by $\\pi_{\\mathrm{S}}$ is equivalent to the standard node marking described in equation 46. Additionally, we note that the pooling procedures for both models, as described in equations 16 and 55, are identical. Therefore, it is sufficient to show that the CS-GNN and MSGNN layer updates described in equations 15 and 47 respectively are also identical. For this purpose, let $\\mathcal{X}^{t}(\\bar{v},u)$ be a node feature map supported on the set $V\\times V$ . The inputs to the MSGNN layer are the following: ", "page_idx": 41}, {"type": "text", "text": "1. $\\mathcal{X}^{t}(u,v)$ .   \n2. $\\mathcal{X}^{t}(u,u)$ .   \n3. $\\mathcal{X}^{t}(v,v)$ .   \n4. $\\arg_{1}^{t}\\{\\!\\!\\{(\\mathcal{X}^{t}(u,v^{\\prime}),e_{v,v^{\\prime}})\\mid v^{\\prime}\\sim v\\}\\!\\!\\}.$ .   \n5. $\\arg^{t}_{2}\\{\\!\\!\\!\\{(\\mathcal{X}^{t}(u^{\\prime},v),e_{u,u^{\\prime}})\\mid u^{\\prime}\\sim u\\}\\!\\!\\}.$ ", "page_idx": 41}, {"type": "text", "text": "The inputs to the CS-GNN layer are the following: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{agg}_{1}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(S,v^{\\prime}),e_{v,v^{\\prime}})\\mid v^{\\prime}\\sim_{G}v\\}\\Rightarrow\\mathrm{agg}_{1}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(u,v^{\\prime}),e_{v,v^{\\prime}})\\mid v^{\\prime}\\sim v\\}.}\\\\ &{\\mathrm{agg}_{2}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(S^{\\prime},v),\\tilde{e}_{S,S^{\\prime}})\\mid S^{\\prime}\\sim_{G}\\sigma\\}\\Rightarrow\\mathrm{agg}_{2}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(u,u^{\\prime}),e_{u,v^{\\prime}})\\mid v^{\\prime}\\sim v\\}.}\\\\ &{\\mathrm{agg}_{3}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(S^{\\prime},v),z(S,v,S^{\\prime},v))\\mid\\forall s^{\\prime}\\in V^{T}\\mathrm{s.t.}\\ v\\in S^{\\prime}\\}\\Rightarrow\\{(X^{t}(v,v),z(u,v,v))\\}.}\\\\ &{\\mathrm{agg}_{4}^{t}\\{\\mathbb{I}(\\mathcal{X}^{t}(S,v^{\\prime}),z(S,v,S,v^{\\prime}))\\mid\\forall u^{\\prime}\\in V\\mathrm{s.t.}\\ v^{\\prime}\\in S\\}\\Rightarrow\\{(X^{t}(u,u),z(u,v,u,u))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The terms $z(u,v,v,v)$ and $z(u,v,u,u)$ appearing in the last two input terms of the CS-GNN layer uniquely encode the orbit tuples $(u,v,v,v)$ and $(u,v,u,u)$ belong to respectively. Since these orbits depend solely on whether $u\\,=\\,v$ , these values are equivalent to the node marking feature map $\\chi^{0}(u,v)$ . Therefore, these terms can be ignored. Observing the two lists above, we see that the inputs to both update layers are identical (ignoring the $z(\\cdot)$ terms), Thus, as both updates act on these inputs in the same way, the updates themselves are identical. and so ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MSGNN}(\\pi_{\\mathrm{NM}})=\\mathrm{CS}\\mathrm{-}\\mathrm{GNN}(\\mathcal{T},\\pi_{\\mathrm{S}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "G.4 Proofs of Appendix D.2 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proposition D.2 (CS-GNN Is at Least as Expressive as Coarse MPNN ). For any coarsening function $\\tau(\\cdot)$ the following holds: ", "page_idx": 41}, {"type": "equation", "text": "$$\nM P N N\\subseteq M P N N_{+}({\\mathcal{T}})\\subseteq C S\u2013G N N({\\mathcal{T}},\\pi_{S})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. For convenience, let us first restate the CS-GNN layer update: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr X}^{t+1}(S,v)=f^{t}\\bigg({\\mathscr X}^{t}(S,v),}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{agg}_{1}^{t}\\mathbb{I}({\\mathscr X}^{t}(S,v^{\\prime}),e_{v,v^{\\prime}})\\mid v^{\\prime}\\sim_{G}v\\mathbb{J},}\\\\ &{\\qquad\\qquad\\quad\\mathrm{agg}_{2}^{t}\\mathbb{I}({\\mathscr X}^{t}(S^{\\prime},v),\\tilde{e}_{S,S^{\\prime}})\\mid S^{\\prime}\\sim_{G^{T}}S\\mathbb{J},}\\\\ &{\\qquad\\quad\\mathrm{agg}_{3}^{t}\\mathbb{I}({\\mathscr X}^{t}(S^{\\prime},v),z(S,v,S^{\\prime},v))\\mid s^{\\prime}\\in V^{T}{\\mathrm{s.t.~}}v\\in S^{\\prime}\\mathbb{J},}\\\\ &{\\qquad\\qquad\\quad\\mathrm{agg}_{4}^{t}\\mathbb{I}({\\mathscr X}^{t}(S,v^{\\prime}),z(S,v,S,v^{\\prime}))\\mid u^{\\prime}\\in V{\\mathrm{s.t.~}}v^{\\prime}\\in S\\mathbb{J}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as well as the $\\mathbf{MPNN}_{+}$ layer update: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{For}\\;v\\in V:}&{\\mathcal{X}^{t+1}(v)=f_{V}^{t}\\left(\\mathcal{X}^{t}(v),\\mathbf{agg}_{1}^{t}\\{\\left\\{\\left(\\mathcal{X}^{t}(v^{\\prime}),e_{v,v^{\\prime}}\\right)\\mid v\\sim_{G}v^{\\prime}\\right\\}\\right\\},}\\\\ &{\\quad\\mathrm{agg}_{2}^{t}\\mathbb{I}\\mathcal{X}^{t}(S)\\mid S\\in V^{T},v\\in S\\mathbb{J}\\}\\right),}\\\\ {\\mathrm{For}\\;S\\in V^{T}:}&{\\mathcal{X}^{t+1}(S)=f_{V^{T}}^{t}\\left(\\mathcal{X}^{t}(S),\\mathbf{agg}_{1}^{t}\\{\\left\\{\\left(\\mathcal{X}^{t}(S^{\\prime}),e_{S,S^{\\prime}}\\right)\\mid S\\sim_{G^{T}}S^{\\prime}\\right\\}\\},}\\\\ &{\\quad\\mathrm{agg}_{2}^{t}\\{\\mathcal{X}^{t}(v)\\mid v\\in V,v\\in S\\}\\}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We note that by setting $f_{V^{\\tau}}^{t}$ to be a constant zero and choosing $f_{V}^{t}$ to be any continuous function that depends only on its first two arguments, the update in equation 54 becomes a standard MPNN layer. This proves: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbf{MPNN}\\subseteq\\mathbf{MPNN}_{+}(T).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Next, we prove the following 2 Lemmas: ", "page_idx": 42}, {"type": "text", "text": "Lemma G.1. Given a graph $G=(V,E)$ such that $V=[n]$ with node feature vector $X\\in\\mathbb{R}^{n\\times d}$ , and a coarsening function $\\tau(\\cdot)$ , there exists a $C S\\mathrm{-}G N N(\\mathcal{T},\\pi_{S})$ layer such that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\chi^{1}(S,v)=[0_{d+1},X_{v},1]=[\\tilde{\\chi}^{0}(S),\\tilde{\\chi}^{0}(v)].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here $[\\cdot,\\cdot]$ denotes concatenation and $\\tilde{\\mathcal{X}}^{0}(\\cdot)$ denotes the initial node feature map of the coarsened sum graph $G_{+}^{\\tau}$ . ", "page_idx": 42}, {"type": "text", "text": "Lemma G.2. Let $\\tilde{\\mathcal{X}}^{t}(\\cdot)$ denote the node feature maps of $G_{+}^{T}$ at layers $t$ of a stack of $M P N N_{+}({\\mathcal{T}})$ layers. There exists a stack of $t+1$ CS-GNN $\\left(\\tau,\\pi_{S}\\right)$ layers such that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{X}^{t+1}(S,v)=[\\tilde{\\mathcal{X}}^{t}(S),\\tilde{\\mathcal{X}}^{t}(v)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "proof of Lemma G.1. Recall that the initial node feature map of $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ is given by: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\chi^{0}(S,v)=\\left\\{\\!\\!\\!\\begin{array}{l l}{{\\left[X_{v},1\\right]}}&{{v\\in S}}\\\\ {{\\left[X_{v},0\\right]}}&{{v\\notin S.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In addition, the initial node feature map of $\\mathbf{MPNN}_{+}(\\mathcal{T})$ is given by: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tilde{X}^{0}(v)=\\left\\{\\!\\!\\!\\begin{array}{l l}{{X_{v},1]}}&{{v\\in V}}\\\\ {{0_{d+1}}}&{{v\\in V^{\\top}.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, we choose a layer update as described in equation 15 with: ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\mathcal{X}}^{1}(S,v)=f^{0}({\\mathcal{X}}^{0}(S,v),\\cdot,\\cdot,\\cdot,\\cdot)=[0_{d+1},{\\mathcal{X}}^{0}(S,v)_{1:d},1]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here, $f(a,\\cdot,\\cdot,\\cdot)$ denotes that the function depends only on the parameter $a$ , and $X_{a:b}$ indicates that only the coordinates $a$ through $b$ of the vector $X$ are taken. This gives us: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{X}^{1}(S,v)=[\\tilde{\\mathcal{X}}^{0}(S),\\tilde{\\mathcal{X}}^{0}(v)].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "proof of Lemma $G.2$ . We prove this Lemma by induction on $t$ . We note that Lemma G.1 provides the base case $t=0$ . Assume now that for a given stack of $t+1\\;\\mathrm{MPNN}_{+}(\\mathcal{T})$ layer updates, with corresponding node feature maps: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{X}}^{i}:V_{+}^{\\mathcal{T}}\\rightarrow\\mathbb{R}^{d_{i}}\\quad i=1\\ldots,t+1,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "there exists a stack of $t+1\\,\\mathrm{CS}\\mathrm{-}\\mathrm{GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ layers with node feature maps: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{X}^{i}:V^{T}\\times V\\rightarrow\\mathbb{R}^{2d_{i}}\\quad i=1,\\dots,t+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "such that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\chi^{t+1}(S,v)=[\\tilde{\\mathcal{X}}^{t}(S),\\tilde{\\mathcal{X}}^{t}(v)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We shall show that there exists a single additional $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ layer update such that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\chi^{t+2}(S,v)=[\\tilde{\\chi}^{t+1}(S),\\tilde{X}^{t+1}(v)].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For that purpose we define the following $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ update (abusing notation, the left hand side refers to components of the $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ update at layer $t+1$ , while the right hand side refers to components of the $\\mathbf{MPNN}_{+}(\\mathcal{T})$ update at layer $t$ ): ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{agg}_{1}^{t+1}=\\mathbf{agg}_{1\\mid1:d_{t}}^{t},}\\\\ &{\\mathbf{agg}_{2}^{t+1}=\\mathbf{agg}_{1\\mid d_{t}+1:2d_{t}}^{t},}\\\\ &{\\mathbf{agg}_{3}^{t+1}=\\mathbf{agg}_{2\\mid1:d_{t}}^{t},}\\\\ &{\\mathbf{agg}_{4}^{t+1}=\\mathbf{agg}_{2}^{t}\\mid_{d_{t}+1:2d_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\nf^{t+1}(a,b,c,d,e)=[f_{V}^{t}(a_{1:d_{t}},b,d),f_{V^{\\tau}}^{t}(a_{d_{t}+1:2d_{t}},c,e)].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here the operation $\\mathrm{agg}|_{a:b}$ initially projects all vectors in the input multi-set onto coordinates $a$ through $b$ , and subsequently passes them to the function agg. equations 190 , 191 guarantee that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mathcal{X}}^{t+2}(S,\\boldsymbol{v})_{1:d_{t+1}}=\\boldsymbol{f}_{\\boldsymbol{V}}^{t}\\left(\\boldsymbol{\\mathcal{X}}^{t}(S,\\boldsymbol{v})_{1:d_{t}},\\right.}\\\\ &{\\left.\\begin{array}{r l}&{\\underline{{\\mathrm{age}}}_{1}^{t}\\boldsymbol{\\mathrm{f}}(\\boldsymbol{S},\\boldsymbol{v}^{\\prime})_{1:d_{t}}\\ \\vert\\ \\boldsymbol{v}\\sim\\boldsymbol{\\mathcal{C}}^{\\prime}\\boldsymbol{\\mathbb{J}}\\right),}\\\\ &{\\underline{{\\mathrm{age}}}_{2}^{t}\\boldsymbol{\\mathrm{f}}(\\boldsymbol{S}^{\\prime},\\boldsymbol{v})_{1:d_{t}}\\ \\vert\\ \\boldsymbol{v}\\in\\boldsymbol{S}^{\\prime}\\boldsymbol{\\mathbb{J}}\\right)}\\\\ &{=\\boldsymbol{\\bar{X}}^{t+1}(\\boldsymbol{v}),}\\end{array}}\\\\ &{\\boldsymbol{\\mathcal{X}}^{t+2}(S,\\boldsymbol{v})_{d_{t+1}+1:2d_{t+1}}=\\boldsymbol{f}_{\\boldsymbol{V}}^{t}\\left(\\boldsymbol{\\mathcal{X}}^{t}(S,\\boldsymbol{v})_{d_{t}+1:2d_{t}},\\right.}\\\\ &{\\left.\\begin{array}{r l}&{\\underline{{\\mathrm{age}}}_{1}^{t}\\boldsymbol{\\mathrm{f}}(\\boldsymbol{S}^{\\prime},\\boldsymbol{v})_{d_{t}+1:2d_{t}}\\ \\vert\\ \\boldsymbol{S}^{\\prime}\\sim\\boldsymbol{\\mathcal{T}}(\\boldsymbol{G})\\ S\\boldsymbol{\\mathbb{J}},}\\\\ &{\\underline{{\\mathrm{age}}}_{2}^{t}\\boldsymbol{\\mathrm{f}}(\\boldsymbol{S},\\boldsymbol{v}^{\\prime})_{d_{t}+1:2d_{t}}\\ \\vert\\ \\boldsymbol{v}^{\\prime}\\in\\boldsymbol{S}\\boldsymbol{\\mathbb{J}}\\right)}\\\\ &{=\\boldsymbol{\\bar{X}}^{t+1}(\\boldsymbol{S}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This proves the Lemma. ", "page_idx": 43}, {"type": "text", "text": "Now, for a given finite family of graphs $\\mathcal{G}$ and a function $f\\in\\operatorname{MPNN}_{+}(\\mathcal{T})$ , there exists a stack of $T$ $\\mathbf{MPNN}_{+}(\\mathcal{T})$ layers such that: ", "page_idx": 43}, {"type": "equation", "text": "$$\nf(G)=U\\left(\\sum_{v\\in V_{+}^{\\mathcal{T}}}\\tilde{\\chi}^{T}(v)\\right)\\quad\\forall G\\in\\mathcal{G}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here, $\\tilde{\\mathcal{X}}^{T}:V_{+}^{T}\\rightarrow\\mathbb{R}^{d_{T}}$ denotes the final node feature map, and $U$ is an MLP. Lemma G.2 now tells us that there exists a stack of $T+1\\,\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ layers such that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\chi^{T+1}(S,v)=[\\tilde{\\chi}^{T}(S),\\tilde{\\chi}^{T}(v)].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Similarly to Lemma G.1, we use one additional layer to pad $\\mathcal{X}^{T+1}(S,v)$ as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{X}^{T+2}(S,v)=[\\tilde{\\mathcal{X}}^{T}(S),\\tilde{\\mathcal{X}}^{T}(v),1].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We notice that: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{s\\in V^{T}}\\mathcal{X}^{T+2}(S,v)=\\left[\\displaystyle\\sum_{S\\in V^{T}}\\tilde{X}^{T}(S),\\;\\sum_{S\\in V^{T}}\\tilde{X}^{T}(v),\\;\\sum_{S\\in V^{T}}1\\right]}\\\\ &{}&{\\quad=\\left[\\displaystyle\\sum_{S\\in V^{T}}\\tilde{X}^{T}(S),\\;|V^{T}|\\cdot\\tilde{X}^{T}(v),\\;|V^{T}|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, in order to get rid of the $|V^{\\mathcal{T}}|$ term, We define: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathrm{MLP}_{1}(a,b,c)=[a,\\frac{1}{c}\\cdot b,1],\\quad a,b\\in\\mathbb{R}^{d_{L}},c>0.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We note that since we are restricted to a finite family of input graphs, the use of an MLP in equation 200 can be justified using Theorem G.1 (see the proof of Proposition B.1 for a detailed explanation). Equations 196 and 200 imply: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathrm{MLP}_{1}\\left(\\sum_{s\\in V^{\\tau}}\\chi^{T+2}(S,v)\\right)=\\left[\\sum_{S\\in V^{\\tau}}\\tilde{X}^{T}(S),\\;\\tilde{X}^{T}(v),\\;1\\right]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, similarly to equation 196: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{v\\in V}\\mathrm{MLP}_{1}\\left(\\sum_{S\\in V^{\\prime}}\\chi^{T+2}(S,v)\\right)=\\left[|V|\\cdot\\sum_{S\\in V^{\\prime}}\\tilde{X}^{T}(S),\\ \\sum_{v\\in V}\\tilde{X}^{T}(v),\\ |V|\\right]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "And so, in order to get rid of the $|V|$ term, We define: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{MLP}_{2}(a,b,c)=U(a\\cdot\\frac{1}{c}+b,1),\\quad a,b\\in\\mathbb{R}^{d_{T}},c>0.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus for all $G\\in{\\mathcal{G}}$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{MLP}_{2}\\left(\\displaystyle\\sum_{v\\in V}\\mathrm{MLP}_{1}\\left(\\displaystyle\\sum_{S\\in V^{T}}\\chi^{T+2}(S,v)\\right)\\right)}\\\\ &{=\\mathrm{MLP}_{2}\\left(\\left[\\Big\\lvert\\{V\\}\\cdot\\displaystyle\\sum_{S\\in V^{T}}\\tilde{X}^{T}(S),\\displaystyle\\sum_{v\\in V}\\tilde{X}^{T}(v),\\ \\lvert V\\Big\\rvert\\right]\\right)}\\\\ &{=U\\left(\\displaystyle\\sum_{v\\in V_{+}^{T}}\\tilde{X}^{T}(v)\\right)}\\\\ &{=f(G).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and so $f\\in\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ . This proves: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{MPNN}_{+}(T)\\subseteq\\mathbf{CS-GNN}(T,\\pi_{\\mathrm{S}}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proposition D.3 (CS-GNN Can Be More Expressive Than $\\mathrm{{MPNN+1}}$ ). Let $\\tau(\\cdot)$ be the identity coarsening function defined by: ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\quad G=(V,E).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The following holds: ", "page_idx": 44}, {"type": "equation", "text": "$$\nM P N N=M P N N_{+}({\\mathcal T}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus: ", "page_idx": 44}, {"type": "equation", "text": "$$\nM P N N_{+}({\\mathcal T})\\subset C S{\\cdot}G N N({\\mathcal T},\\pi_{S}),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where this containment is strict. ", "page_idx": 44}, {"type": "text", "text": "Proof. First, using the notation $\\tilde{v}$ to mark the single element set $\\{v\\}\\,\\in\\,V^{\\mathcal{T}}$ , We notice that the $\\mathrm{MPNN}_{+}(\\mathcal{T})$ layer update described in equation 54, becomes: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{For}\\;v\\in V:}&{\\mathcal{X}^{t+1}(v)=f_{V}^{t}\\biggl(\\mathcal{X}^{t}(v),\\mathcal{X}^{t}(\\tilde{v}),\\mathrm{agg}^{t}\\,\\mathbb{\\{\\left(\\mathcal{X}^{t}(v^{\\prime}),e_{v,v^{\\prime}}\\right)\\mid\\,v^{\\prime}\\sim_{G}v\\}},\\biggr),}\\\\ {\\mathrm{For}\\;\\tilde{v}\\in V^{T}:}&{\\mathcal{X}^{t+1}(\\tilde{v})=f_{V^{T}}^{t}\\biggl(\\mathcal{X}^{t}(\\tilde{v}),\\mathcal{X}^{t}(v),\\mathrm{agg}^{t}\\,\\mathbb{\\{\\left(\\mathcal{X}^{t}(\\tilde{v}^{\\prime}),e_{\\tilde{v},\\tilde{v}^{\\prime}}\\right)\\mid v\\sim_{G}v^{\\prime}\\}}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, for a given finite family of graphs $\\mathcal{G}$ and a function $f\\in\\operatorname{MPNN}_{+}(\\mathcal{T})$ , there exists a stack of $T$ $\\mathbf{MPNN}_{+}(\\mathcal{T})$ layers such that: ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(G)=U\\left(\\sum_{v\\in V_{+}^{\\mathcal{T}}}\\chi^{T}(v)\\right)\\quad\\forall G\\in\\mathcal{G}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, $\\mathcal{X}^{T}:V_{+}^{T}\\to\\mathbb{R}^{d}$ denotes the final node feature map, and $U$ is an MPL. We now prove by induction on $t$ that there exists a stack of $t$ standard MPNN layers, with corresponding node feature map $X^{t}:V\\rightarrow\\mathbb{R}^{2d_{t}}$ such that : ", "page_idx": 45}, {"type": "equation", "text": "$$\nX^{t}(v)=[\\mathcal{X}^{t}(v),\\mathcal{X}^{t}(\\tilde{v})].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, $[\\cdot,\\cdot]$ stands for concatenation. We assume for simplicity that the input graph $G$ does not have node features, though the proof can be easily adapted for the more general case. We notice that for the base case $t=0$ , equation 53 in definition D.2 implies: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathcal{X}^{0}(v)=\\left\\{1\\begin{array}{l l}{v\\in V,}\\\\ {0}&{v\\in V^{T}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, we define: ", "page_idx": 45}, {"type": "equation", "text": "$$\nX^{0}(v)=(1,0).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This satisfies Equation (205), establishing the base case of the induction. Assume now that Equation (205) holds for some $t\\in[T]$ . Let $\\bar{\\mathrm{agg}^{t}},f_{V}^{t},f_{V^{\\tau}}^{t}$ be the components of layer $t$ , as in equation 203. We define: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbf{a}\\tilde{\\mathbf{g}}^{t}=[\\mathbf{a}\\mathbf{g}\\mathbf{g}^{t}|_{1:d_{t}},\\mathbf{a}\\mathbf{g}\\mathbf{g}^{t}|_{d_{t}+1:2d_{t}}].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here the operation $\\mathrm{agg}|_{a:b}$ initially projects all vectors in the input multi-set onto coordinates $a$ through $b$ , and subsequently passes them to the function agg. ", "page_idx": 45}, {"type": "text", "text": "Additionally, let $d^{\\ast}$ denote the dimension of the output of the function $\\mathrm{agg^{t}}$ . We define: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{f}^{t}(a,b)=\\left[f_{V}^{t}\\left(a|_{1:d_{t}},a|_{d_{t}+1:2d_{t}},b|_{1:d^{*}}\\right),f_{V}^{t}\\left(a|_{d_{t}+1:2d_{t}},a|_{1:d_{t}},b|_{d^{*}+1:2d^{*}}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Finally, we update our node feature map $X^{t}$ using a standard MPNN update according to: ", "page_idx": 45}, {"type": "equation", "text": "$$\nX^{t+1}(v)=\\tilde{f}^{l}\\left(X^{t}(v),\\{\\left(X^{t}(v^{\\prime}),e_{v,v^{\\prime}}\\right)\\mid v^{\\prime}\\sim_{G}v\\}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "equations 203, 205 and 210 now guarantee that: ", "page_idx": 45}, {"type": "equation", "text": "$$\nX^{t+1}(v)=[\\mathcal{X}^{t}(v),\\mathcal{X}^{t+1}(\\tilde{v})].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This concludes the inductive proof. We now define: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{MLP}(x)=U(x|_{1:d_{T}})+U(x|_{d_{T}+1:2d_{T}}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This gives us: ", "page_idx": 45}, {"type": "equation", "text": "$$\nU\\biggl(\\sum_{v\\in V_{+}^{\\mathcal{T}}}\\chi^{T}(v)\\biggr)=\\mathbf{MLP}\\biggl(\\sum_{v\\in V}X^{T}(v)\\biggr)=f(G).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We have thus proven that $f\\in\\operatorname{MPNN}$ and so: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{MPNN}_{+}(\\mathcal{N})\\subseteq\\mathrm{MPNN}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining this result with Proposition D.2, we obtain: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbf{MPNN}=\\mathbf{MPNN}_{+}(\\mathcal{T}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Finally, since Proposition D.1 tells us that $\\mathrm{CS-GNN}(\\mathcal{T},\\pi_{\\mathrm{S}})$ has the same implementation power as the maximally expressive node policy subgraph architecture MSGNN, which is proven to be strictly more expressive than the standard MPNN, we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbf{MPNN}_{+}(T)\\subset\\mathbf{CS}\\mathbf{-GNN}(T,\\pi_{\\mathrm{S}}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proposition D.4 (CS-GNN can be strictly more expressive then node-based subgraph GNNs). Let $\\tau$ be the coarsening function defined by: ", "page_idx": 46}, {"type": "equation", "text": "$$\n{\\mathcal{T}}(G)=\\{\\{v\\}\\mid v\\in V\\}\\cup E\\quad G=(V,E).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The following holds: ", "page_idx": 46}, {"type": "text", "text": "1. Let $G_{1},G_{2}$ be a pair of graphs such that there exists a node-based subgraph GNN model $M$ where $M(G_{1})\\neq M(G_{2})$ . There exists a CS-GNNmodel $M^{\\prime}$ which uses $\\tau$ such that $M^{\\prime}(G_{1})\\neq M^{\\prime}(\\dot{G}_{2})$ .   \n2. There exists a pair of graphs $G_{1},G_{2}$ such that for any subgraph GNN model M it holds that $M(G_{1})\\,=\\,M(G_{2})$ , but there exists a CS-GNNmodel $M^{\\prime}$ which uses $\\tau$ such that $M^{\\prime}(G_{1})\\neq M^{\\prime}(G_{2})$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. First, notice that the super-nodes produced by $\\tau$ are either of size 1, in which case they correspond to nodes, or they are of size two, in which case they correspond to edges. Since an CS-GNNmodel processes feature maps $\\mathcal{X}^{t}(S,v)$ where in the initial layer the sset size of $S$ is encoded in $\\chi^{t}(\\bar{S_{}},v)$ , we can easily use the CS-GNNupdate in Definition A.5 to ignore all values of $\\mathcal{X}^{t}(S,v)$ were $|S|=2$ (This can be done by using $f^{\\bar{t}},\\arg_{1}^{t},\\dots\\arg_{1}^{t}$ in Definition A.5 to zero out these values at each update). This means CS-GNNusing $\\tau$ is able to simulate an CS-GNNupdate with the identity coarsening function, which was shown in Proposition D.1 to be as expressive as $\\mathrm{GNN-SSWL+}$ (Definition D.1) which is a maximally expressive node-based subgraph GNN, thus proving part (1) of the proposition. To prove part (2), notice that using the same reasoning as before, an CS-GNNmodel using $\\tau$ as a coarsening function cal implement an CS-GNNmodel using the edge coarsening function: ", "page_idx": 46}, {"type": "equation", "text": "$$\nT^{\\prime}(G)=E\\quad G=(V,E).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "An CS-GNNmodel with the identity coarsening function can be interpreted as a $\\mathrm{GNN-SSWL+}$ model. Similarly, an CS-GNNmodel using the edge coarsening function $\\mathcal{T}^{\\prime}$ generalizes the $\\mathrm{GNN-SSWL+}$ framework by extending it from node-based subgraph GNNs to edge-based subgraph GNNs. In fact, the same proof in [38, 12] showing that GNN-SSWL $^+$ is at least as expressive as a DSS subgraph GNN using the node deletion policy (see [4] for a definition of the DSS subgraph GNN), can be used to show that CS-GNNusing the edge coarsening function $\\mathcal{T}^{\\prime}$ is at least as expressive as a DSS subgraph GNN with an edge deletion policy. The latter model was shown in [4] to be able to separate a pair of 3-WL indistinguishable graphs. In contrast, node-based subgraph GNNs were shown in [12] to not be able to separate any pair of 3-WL indistinguishable graphs. Thus, there exists a pair of graphs which CS-GNNusing $\\tau$ can separate while node-based subgraph GNNs cant, proving part (2) of the proposition. ", "page_idx": 46}, {"type": "text", "text": "G.5 Proofs of Appendix E ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Lemma E.1 $(\\gamma\\,(\\Gamma)$ are orbits). The sets $\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}$ and $\\left\\{\\Gamma^{\\leftrightarrow;k_{1};k_{2};k^{\\cap};\\delta_{s a m e};\\delta_{d i f f}}\\right\\}$ are the orbits of $S_{n}$ on the index space $\\left(\\mathcal P([n])\\times[n]\\right)$ and $({\\mathcal{P}}([n])\\times[n]\\times({\\mathcal{P}}([n])\\times[n])$ , respectively. ", "page_idx": 46}, {"type": "text", "text": "Proof. We will prove this lemma for $\\gamma$ . The proof for $\\Gamma$ follows similar reasoning; we also refer the reader to [22] for a general proof. ", "page_idx": 46}, {"type": "text", "text": "We will prove this lemma through the following three steps. ", "page_idx": 46}, {"type": "text", "text": "(1). Given indices $(S,i)\\in\\mathcal{P}([n])\\times[n]$ , there exists $\\gamma\\in({\\mathcal{P}}([n])\\times[n])_{\\sim}$ such that $(S,i)\\in\\gamma$ . ", "page_idx": 46}, {"type": "text", "text": "(2). Given indices $(S,i)\\in\\gamma$ , for any $\\sigma\\in S_{n}$ , it holds that $(\\sigma^{-1}(S),\\sigma^{-1}(i))\\in\\gamma$ . ", "page_idx": 46}, {"type": "text", "text": "(3). Given $(S,i)\\in\\gamma$ and $(S^{\\prime},i^{\\prime})\\,\\in\\,\\gamma$ (the same $\\gamma$ ), it holds that there exists a $\\sigma\\,\\in\\,S_{n}$ such that $\\sigma\\cdot(S,i)=\\stackrel{.}{(}S^{\\prime},i^{\\prime})$ . ", "page_idx": 47}, {"type": "text", "text": "We prove in what follows. ", "page_idx": 47}, {"type": "text", "text": "(1). Given indices $(S,i)\\in\\mathcal{P}([n])\\times[n]$ , w.l.o.g. we assume that $|S|=k$ , thus if $i\\in S\\,(i\\notin S)$ it holds that $(S,i)\\in\\gamma^{k^{-}}$ $\\left((S,i)\\in\\gamma^{k^{+}}\\right)$ , recall Equation (71). ", "page_idx": 47}, {"type": "text", "text": "(2). Given indices $(S,i)\\in\\gamma$ , note that any permutation $\\sigma\\in S_{n}$ does not change the cardinality of $S$ nor the inclusion (or exclusion) of $i$ in $S$ . Recalling Equation (71), we complete this step. ", "page_idx": 47}, {"type": "text", "text": "(3). Given that $(S,i)\\in\\gamma$ and $(S^{\\prime},i^{\\prime})\\in\\gamma$ , and recalling Equation (71), we note that $\\left|S\\right|=\\left|S^{\\prime}\\right|$ and that either both $i\\in S$ and $i^{\\prime}\\in S^{\\prime}$ , or both $i\\not\\in S$ and $i^{\\prime}\\,\\bar{\\notin}\\,S^{\\bar{\\prime}}$ . ", "page_idx": 47}, {"type": "text", "text": "(3.1). In (3.1) we focus on the case where $i\\not\\in S$ and $i^{\\prime}\\notin S^{\\prime}$ . Let $S\\,=\\,\\{i_{1},\\ldots,i_{k}\\}$ and $S^{\\prime}=$ $\\{i_{1}^{\\prime},\\ldots,i_{k}^{\\prime}\\}$ . Then, we have $(\\{i_{1},\\ldots,i_{k}\\},j)$ and $\\big(\\{i_{1}^{\\prime},\\dots,i_{k}^{\\prime}\\},j^{\\prime}\\big)$ . Define $\\sigma\\ \\in\\ S_{n}$ such that $\\sigma(i_{l})=i_{l}^{\\prime}$ for $l\\in[k]$ , and $\\sigma(j)=j^{\\prime}$ . Since $(\\{i_{1},\\ldots,i_{k}\\},j)$ consists of $k+1$ distinct indices and $(\\{i_{1}^{\\prime},\\ldots,\\bar{i}_{k}^{\\prime}\\},j^{\\prime})$ also consists of $k+1$ distinct indices, this is a valid $\\sigma\\in S_{n}$ . ", "page_idx": 47}, {"type": "text", "text": "(3.2). Here, we focus on the case where $i\\in S$ and $i^{\\prime}\\in S^{\\prime}$ . This proof is similar to (3.1), but without considering the indices $j$ and $j^{\\prime}$ , as they are included in $S$ and $S^{\\prime}$ , respectively. ", "page_idx": 47}, {"type": "text", "text": "Proposition E.1 (Basis of Invariant (Equivariant) Layer). The tensors ${\\bf B}^{\\gamma}\\,({\\bf B}^{\\Gamma})$ in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equation (66) (Equation (67)). ", "page_idx": 47}, {"type": "text", "text": "Proof. We prove this proposition for the invariant case. The equivariant case is proved similarly \u2013 we also refer the reader for [22] for a general proof. We will prove this in three steps, ", "page_idx": 47}, {"type": "text", "text": "(1). For any $\\gamma\\in({\\mathcal{P}}([n])\\times[n])_{\\sim}$ it holds that $\\mathbf{B}_{S,i}^{\\gamma}$ solves Equation (66). ", "page_idx": 47}, {"type": "text", "text": "(2). Given a solution $\\mathbf{L}$ to Equation (66), it is a linear combination of the basis elements. ", "page_idx": 47}, {"type": "text", "text": "(3). We show that the basis vectors are orthogonal and thus linearly independent. ", "page_idx": 47}, {"type": "text", "text": "We prove in what follows. ", "page_idx": 47}, {"type": "text", "text": "(1). Given $\\gamma\\,\\in\\,({\\mathcal{P}}([n])\\times[n])_{\\sim}$ , we need to show that $\\mathbf{B}_{S,i}^{\\gamma}=\\mathbf{B}_{\\sigma^{-1}(S),\\sigma^{-1}(i)}^{\\gamma}$ . Since any $\\gamma\\in$ $(\\mathcal P([n])\\times[n])_{\\sim}$ is an orbit in the index space (recall Lemma E.1), and $\\mathbf{B}_{S,i}^{\\gamma}$ are indicator vectors of the orbits this always holds. ", "page_idx": 47}, {"type": "text", "text": "(2). Given a solution $\\mathbf{L}$ to Equation (66), it must hold that $\\mathbf{L}_{S,i}=\\mathbf{L}_{\\sigma^{-1}(S),\\sigma^{-1}(i)}$ . Since the set $\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}$ corresponds to the orbits in the index space with respect to $S_{n}$ , $\\mathbf{L}$ should have the same values over the index space of these orbits. Let\u2019s define these values as $\\alpha^{\\gamma}$ for each $\\gamma\\in\\{\\gamma^{k^{*}}:k=1,\\ldots,n;*\\in\\{+,-\\}\\}$ . Thus, we obtain that $\\begin{array}{r}{\\mathbf{L}^{\\prime}=\\sum_{\\gamma\\in(\\mathcal{P}([n])\\times[n])\\sim}\\alpha^{\\gamma}\\cdot\\mathbf{B}^{\\gamma}}\\end{array}$ , since $\\mathbf{B}^{\\gamma}$ are simply indicator vectors of the orbits. This completes this step. ", "page_idx": 47}, {"type": "text", "text": "(3). Once again, since the basis elements are indicator vectors of disjoint orbits we obtain their orthogonality, and thus linearly independent. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 48}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 48}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 48}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 48}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 48}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 48}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: The abstract spells out all the main contributions in the present paper, both theoretical and empirical ones. These are extensively discussed and recapitulated in the Introduction Section 1 (see paragraphs \u201cOur approach\u201d and \u201cContributions\u201d). The scope of the paper is well defined in the first periods of the abstract and comprehensively articulated in the first two paragraphs of the Introduction Section 1. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Please refer to paragraph \u201cLimitations\u201d in Section 6. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Please refer to Appendices B to E and F.4, which include precise and contextualized statements of all theoretical results and derivations, and to Appendix G for proofs thereof. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, and Appendix F.5 for a series of complementary results. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: The code to reproduce our results can be found in the following GitHub repository: https://github.com/BarSGuy/Efficient-Subgraph-GNNs. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, including the utilized training procedures. The results for baselines approaches are reported according to what stated in Section 5 and Appendix F.2. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our results in Section 5 are reported in terms of mean and standard deviation calculated over different model initializations (i.e., by setting different random seeds prior to code execution). Table 9 reports error bars for the results illustrated in Figure 2. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: The hardware employed to obtain all experimental results, as well as a runtime comparison, are described in Appendix F.2 (see \u201cImplementation Details\u201d). ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We have made sure to comply to the Code of Ethics and to preserve our anonymity. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The models we developed are not generative, hence not posing risks of malicious use as for what concerns fabricating misleading or otherwise fake information, online profiles and media. Additionally, although our approach improves the efficiency of certain Graph Neural Networks, the models we developed are not scalable enough to apply and impact (online) social networks: represented as graphs, their scale is way beyond that considered in our experiments. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 52}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: Our model(s) do not have a high risk of misuse. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: As explicitly mentioned in the main corpus of the paper, the inset figure in Section 2 is taken with permission by the original authors. Creators of datasets employed in this study, as well as the benchmark frameworks used are properly referenced and cited. For these last we report license information in Appendix F.1, also reported for code assets Appendix F.2. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not release new assets. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 54}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]