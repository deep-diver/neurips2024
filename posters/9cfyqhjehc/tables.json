[{"figure_path": "9cFyqhjEHC/tables/tables_8_1.jpg", "caption": "Table 1: Results on ZINC-12K dataset. Top two results are reported as First and Second.", "description": "This table presents the results of the proposed CS-GNN model and several baseline models on the ZINC-12K dataset. The results are shown for different bag sizes (T), indicating the number of subgraphs considered. The table also includes the results of state-of-the-art full-bag Subgraph GNN models for comparison.", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_8_2.jpg", "caption": "Table 2: Results on PEPTIDES dataset. Top two results are reported as First and Second.", "description": "This table presents the results of experiments conducted on two PEPTIDES datasets: PEPTIDES-FUNC (multi-label graph classification) and PEPTIDES-STRUCT (multi-task regression).  The table compares the performance of the proposed CS-GNN method against several baseline models, including GCN, GIN, GatedGCN, and GatedGCN+RWSE.  The results are shown in terms of Average Precision (AP) for PEPTIDES-FUNC and Mean Absolute Error (MAE) for PEPTIDES-STRUCT.  The CS-GNN model outperforms all baseline models on both datasets, demonstrating its effectiveness in handling large graph datasets.", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_9_1.jpg", "caption": "Table 3: Ablation study.", "description": "This table presents the ablation study results for the impact of symmetry-based updates on the performance of the CS-GNN model. It shows the mean absolute error (MAE) achieved on the ZINC-12K dataset for different bag sizes (T) with and without the symmetry-based updates. The results demonstrate that the symmetry-based updates significantly improve the model's performance across all bag sizes.", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_9_2.jpg", "caption": "Table 4: Results on OGB datasets. The top two results are reported as First and Second.", "description": "This table presents the results of experiments conducted on seven datasets from the Open Graph Benchmark (OGB) collection.  The table compares the performance of the proposed CS-GNN model against several baseline methods across different bag sizes (number of subgraphs processed) and the full bag setting. For each dataset and bag size, it shows the performance metrics (ROC-AUC for MOLHIV, MOLBACE, and RMSE for MOLESOL). The results highlight the flexibility and performance gains achieved by the CS-GNN model.", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_25_1.jpg", "caption": "Table 5: Overview of the graph learning datasets.", "description": "This table presents eight graph datasets used for evaluating the proposed model, along with key characteristics such as the number of graphs, average number of nodes and edges, directionality (directed or undirected), prediction task (regression, classification), and the metric used for evaluation (e.g., Mean Absolute Error, AUROC, Root Mean Squared Error).  The datasets represent diverse applications of graph learning and include various sizes and complexities.", "section": "Extended Experimental Section"}, {"figure_path": "9cFyqhjEHC/tables/tables_26_1.jpg", "caption": "Table 7: Chosen Hyperparameters for CS-GNN.", "description": "This table presents the chosen hyperparameters for the CS-GNN model across various datasets and bag sizes.  It includes hyperparameters like learning rate, embedding size, number of layers, epochs, batch size, dropout rate, Laplacian dimension, and SPD dimension, showing the specific values selected for each experiment.  These settings are crucial for reproducibility of the results presented in the paper.", "section": "F.3 HyperParameters"}, {"figure_path": "9cFyqhjEHC/tables/tables_27_1.jpg", "caption": "Table 7: Chosen Hyperparameters for CS-GNN.", "description": "This table presents the chosen hyperparameters for the CS-GNN model across various datasets and bag sizes.  The hyperparameters include the number of layers, learning rate, embedding size, number of epochs, batch size, dropout rate, Laplacian dimension, and SPD dimension.  The choices of hyperparameters were made through a hyperparameter search process, details of which can be found in Appendix F.3 of the paper. Note that the Laplacian dimension and SPD dimension parameters relate to the coarsening process used to control the number of subgraphs processed.", "section": "F Extended Experimental Section"}, {"figure_path": "9cFyqhjEHC/tables/tables_28_1.jpg", "caption": "Table 8: Comparison over the ZINC-FULL molecular dataset under 500k parameter budget. The best performing method is highlighted in blue, while the second best is highlighted in red.", "description": "This table compares the performance of different models on the ZINC-FULL dataset, using a parameter budget of 500k. The models are evaluated based on their Mean Absolute Error (MAE). The best-performing model for each bag size (T) is highlighted in blue, and the second best in red.  The table helps illustrate the relative performance of different Subgraph GNNs, specifically comparing the proposed method against state-of-the-art techniques, both in settings where a small subset of subgraphs is used and the full set is used.", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_29_1.jpg", "caption": "Table 1: Results on ZINC-12K dataset. Top two results are reported as First and Second.", "description": "This table presents the results of the proposed CS-GNN model and several baseline methods on the ZINC-12K dataset for different bag sizes (number of subgraphs).  The results show Mean Absolute Error (MAE) values for each model, indicating its performance in predicting molecular properties.  The table highlights that CS-GNN outperforms other methods, especially in smaller bag sizes, demonstrating its effectiveness in handling various numbers of subgraphs. ", "section": "5 Experiments"}, {"figure_path": "9cFyqhjEHC/tables/tables_29_2.jpg", "caption": "Table 10: Run time comparison over the ZINC-12K dataset. Time taken at train for one epoch and at inference on the test set. All values are in milliseconds.", "description": "This table presents a runtime comparison of different methods on the ZINC-12K dataset.  For each method, it shows the training time for a single epoch and the inference time on the test set, both measured in milliseconds.  The Mean Absolute Error (MAE) is also included for context.", "section": "5 Experiments"}]