[{"figure_path": "lckAdnVzsT/figures/figures_0_1.jpg", "caption": "Figure 1: Given a single RGB image of an indoor scene, our model reconstructs the 3D scene by jointly estimating object arrangements and shapes in a globally consistent manner. Our novel diffusion-based 3D scene reconstruction approach achieves highly accurate predictions by utilizing a novel generative scene prior that captures scene context and inter-object relationships, and by employing an efficient surface alignment loss formulation for joint pose- and shape-synthesis.", "description": "This figure illustrates the overall process of the proposed method. Given a single RGB image as input, the system first detects 2D bounding boxes of objects.  Then, a 3D scene diffusion model jointly estimates the 3D poses and shapes of all objects within the scene, considering both individual object characteristics and their inter-object relationships. Finally, a shape decoding process generates a globally consistent 3D scene reconstruction.", "section": "1 Introduction"}, {"figure_path": "lckAdnVzsT/figures/figures_3_1.jpg", "caption": "Figure 2: Scene Prior and Surface Alignment Loss Overview. (Left) We propose a novel way to model scene priors (Sec. 3.5) by modeling the scene context and the relationships between all objects during the denoising process. (Right) For additional supervision and joint training, we use a surface alignment loss (Sec. 3.6) between a given ground truth depth map and point samples directly drawn from the intermediate shape representation \u00f4\u2081 and transformed to camera space with the predicted object pose pi\u00b7", "description": "This figure illustrates two key components of the proposed 3D scene reconstruction method: the scene prior and the surface alignment loss. The scene prior models the relationships between objects in the scene to improve reconstruction accuracy. The surface alignment loss uses point samples from intermediate shape predictions and ground truth depth maps to improve training, even with limited annotations.", "section": "3 Method"}, {"figure_path": "lckAdnVzsT/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative comparison of 3D scene reconstruction on SUN RGB-D [62]. While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.", "description": "This figure compares the 3D scene reconstruction results of three different methods: Total3D, Im3D, and the proposed method.  The input RGB image is shown alongside the 3D reconstructions generated by each method. The figure highlights the superior quality and coherence of the proposed method's 3D scene reconstructions compared to the baselines, which tend to produce noisy, incomplete, or spatially inconsistent results.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_8_2.jpg", "caption": "Figure 3: Qualitative comparison of 3D scene reconstruction on SUN RGB-D [62]. While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.", "description": "This figure compares the 3D scene reconstruction results of three different methods (Total3D, Im3D, and the proposed method) on the SUN RGB-D dataset.  The input RGB images are shown alongside the 3D reconstructions generated by each method. The figure highlights that the proposed method produces more realistic and accurate 3D scene reconstructions compared to the baseline methods, particularly in terms of object arrangements and shape details. The baseline methods often suffer from intersecting objects or missing parts of objects due to inaccuracies in pose estimation and shape prediction, while the proposed method shows significantly improved results with more coherent scene structures and higher-quality object shapes.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_9_1.jpg", "caption": "Figure 5: Unconditional results. Injecting \u00d8 as a condition to our conditional diffusion model, i.e., effectively disabling the conditioning mechanism, results in high-quality and diverse results.", "description": "This figure shows the results of running the model without any conditioning image.  The model generates high-quality and diverse 3D models of various furniture items, demonstrating the model's ability to learn a rich and varied representation of shapes in a scene without the guidance of a specific input image.  This showcases the effectiveness of the learned scene prior in capturing the essence of indoor scenes.", "section": "4.5 Ablations Studies"}, {"figure_path": "lckAdnVzsT/figures/figures_14_1.jpg", "caption": "Figure 3: Qualitative comparison of 3D scene reconstruction on SUN RGB-D [62]. While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.", "description": "This figure compares the results of three different methods for 3D scene reconstruction from a single RGB image on the SUN RGB-D dataset. The methods compared are Total3D, Im3D, and the authors' proposed method. The figure shows that the authors' method produces more accurate and realistic 3D scene reconstructions compared to the baselines, especially in terms of object arrangement and shape details. The baselines often produce noisy or incomplete shapes, intersecting objects, or misplaced objects. In contrast, the authors' method is able to generate plausible object arrangements and high-quality shape reconstructions.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_15_1.jpg", "caption": "Figure 3: Qualitative comparison of 3D scene reconstruction on SUN RGB-D [62]. While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.", "description": "This figure compares the results of 3D scene reconstruction on the SUN RGB-D dataset using three different methods: Total3D, Im3D, and the proposed method.  The input RGB image is shown alongside the 3D reconstructions generated by each method.  The ground truth is also provided for comparison.  The figure highlights that the proposed method generates more plausible and realistic 3D scene reconstructions than the baselines, which often suffer from noisy shapes, incomplete objects, and unrealistic object placements.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_18_1.jpg", "caption": "Figure 3: Qualitative comparison of 3D scene reconstruction on SUN RGB-D [62]. While the baselines often produce noisy or incomplete shape reconstruction of intersecting or misplaced objects, our method produces plausible object arrangements as well as high-quality shape reconstructions.", "description": "This figure compares the results of three different methods for 3D scene reconstruction from a single RGB image on the SUN RGB-D dataset.  The input image is shown on the left. The other three columns show the results produced by Total3D, Im3D, and the authors' proposed method.  The figure illustrates that the authors' method generates significantly more accurate and realistic 3D scene reconstructions compared to the baselines, avoiding issues like intersecting or floating objects.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_19_1.jpg", "caption": "Figure 9: Qualitative comparison of 3D shape reconstruction on the Pix3D [64]. While InstPIFu often produces noisy surfaces, our image-conditional 3D diffusion model synthesizes high-quality shapes that closely match the target geometries.", "description": "This figure compares the 3D shape reconstruction results of the proposed method with those of InstPIFu on the Pix3D dataset.  It shows that the proposed diffusion model produces high-quality shapes, unlike InstPIFu, which tends to produce noisy surfaces. The comparison highlights the superior shape reconstruction capabilities of the proposed approach, especially in terms of detail and accuracy in matching the ground truth shapes.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_19_2.jpg", "caption": "Figure 1: Given a single RGB image of an indoor scene, our model reconstructs the 3D scene by jointly estimating object arrangements and shapes in a globally consistent manner. Our novel diffusion-based 3D scene reconstruction approach achieves highly accurate predictions by utilizing a novel generative scene prior that captures scene context and inter-object relationships, and by employing an efficient surface alignment loss formulation for joint pose- and shape-synthesis.", "description": "This figure illustrates the overall pipeline of the proposed method. It takes a single RGB image as input and produces a complete 3D reconstruction of the scene. The process involves jointly estimating the 3D poses and shapes of all the objects in the scene using a diffusion model. A novel generative scene prior is used to capture the scene context and inter-object relationships. An efficient surface alignment loss is employed to ensure accurate predictions. The figure shows an example of an indoor scene, an intermediate scene representation, and the resulting 3D scene reconstruction.", "section": "1 Introduction"}, {"figure_path": "lckAdnVzsT/figures/figures_20_1.jpg", "caption": "Figure 11: Comparison with retrieval baseline method ROCA [17] on frames from ScanNet [11]. While ROCA cannot always retrieve a matching mode from the shape database, such as the desk in the first row, our diffusion-based reconstruction approach reconstructs accurate shapes and poses.", "description": "This figure compares the results of 3D scene reconstruction using the proposed diffusion-based method and a retrieval-based method (ROCA) on three different scenes from the ScanNet dataset.  The input RGB images are shown on the left. The middle column displays reconstructions from ROCA, a retrieval-based approach that selects shapes from a database. The right column presents the results from the proposed diffusion model. The figure highlights that while ROCA struggles to find appropriate matches from its database (particularly evident in the desk example), the diffusion model generates more accurate and complete 3D reconstructions that better align with the input images.", "section": "4.4 Comparison to State of the Art"}, {"figure_path": "lckAdnVzsT/figures/figures_20_2.jpg", "caption": "Figure 2: Scene Prior and Surface Alignment Loss Overview. (Left) We propose a novel way to model scene priors (Sec. 3.5) by modeling the scene context and the relationships between all objects during the denoising process. (Right) For additional supervision and joint training, we use a surface alignment loss (Sec. 3.6) between a given ground truth depth map and point samples directly drawn from the intermediate shape representation \u00f4\u2081 and transformed to camera space with the predicted object pose pi.", "description": "This figure illustrates two key components of the proposed method: the scene prior and the surface alignment loss.  The left side shows how the model considers the relationships between objects within a scene during the denoising process, improving scene coherence. The right side details the surface alignment loss, which uses point samples from intermediate shape predictions and ground truth depth maps to provide additional training supervision, especially useful when full ground truth is unavailable.", "section": "3 Method"}]