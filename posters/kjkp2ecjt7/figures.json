[{"figure_path": "kJkp2ECJT7/figures/figures_0_1.jpg", "caption": "Figure 1: FleVRS is a single model trained to support standard, promptable and open-vocabulary fine-grained visual relationship segmentation (<subject mask, relationship categories, object mask>). It can take images only or images with structured prompts as inputs, and segment all existing relationships or the ones subject to the text prompts.", "description": "This figure showcases the FleVRS model's ability to perform visual relationship segmentation in three different modes: standard, promptable, and open-vocabulary.  The standard mode segments all relationships present in an image without any prompts.  In the promptable mode, user-provided prompts guide the segmentation, allowing users to focus on specific relationships. Lastly, the open-vocabulary mode allows the model to identify relationships with previously unseen objects or predicates.", "section": "1 Introduction"}, {"figure_path": "kJkp2ECJT7/figures/figures_2_1.jpg", "caption": "Figure 1: FleVRS is a single model trained to support standard, promptable and open-vocabulary fine-grained visual relationship segmentation (<subject mask, relationship categories, object mask>). It can take images only or images with structured prompts as inputs, and segment all existing relationships or the ones subject to the text prompts.", "description": "This figure shows example results of the proposed FleVRS model on different visual relationship segmentation tasks.  The top row demonstrates standard HOI (Human-Object Interaction) segmentation, where the model segments objects and relationships without any textual prompt. The middle row shows promptable HOI segmentation, where a textual prompt specifies the relationship of interest. The bottom row shows promptable panoptic scene graph generation, where a textual prompt helps to ground various types of visual relationships.", "section": "Introduction"}, {"figure_path": "kJkp2ECJT7/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of FleVRS. In standard VRS, without textual queries, the latent queries perform self- and cross-attention within the relationship decoder to output a triplet for each query. For promptable VRS, the decoder additionally incorporates textual queries Qt, concatenated with Q. This setup similarly predicts triplets, each based on Q outputs aligned with features from the optional textual prompt Qt.", "description": "This figure shows the architecture of FleVRS, a flexible one-stage framework for visual relationship segmentation.  The model consists of an image encoder, a pixel decoder, a textual encoder (used for promptable VRS), and a relationship decoder.  The relationship decoder uses latent queries (for standard VRS) or latent queries combined with textual queries (derived from textual prompts, for promptable VRS) to predict triplets which include subject and object masks and classes, along with predicate class. This shows how the model integrates standard, promptable, and open-vocabulary visual relationship segmentation in a single framework.", "section": "3 Method"}, {"figure_path": "kJkp2ECJT7/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative results of promptable VRS on HICO-DET [4] test set. We show visualizations of subject and object masks and relationship category outputs, given three types of text prompts. In (c), we show the predicted predicates in bold characters. Unseen objects and predicates are denoted in red characters.", "description": "This figure demonstrates the qualitative results of the proposed FleVRS model on the HICO-DET test set for promptable visual relationship segmentation (VRS). It showcases the model's ability to accurately segment subject and object masks and predict relationship categories based on three different types of textual prompts: (a) specifying the object and predicate, (b) specifying the subject and predicate, and (c) specifying the subject and object.  The use of bold and red text highlights predicted predicates and unseen objects/predicates, respectively, showcasing the model's flexibility in handling various prompt formats and its capability for open-vocabulary relationship segmentation.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative results of promptable VRS on HICO-DET [4] test set. We show visualizations of subject and object masks and relationship category outputs, given three types of text prompts. In (c), we show the predicted predicates in bold characters. Unseen objects and predicates are denoted in red characters.", "description": "This figure shows example results of the model's promptable visual relationship segmentation (VRS) capabilities on the HICO-DET dataset.  It demonstrates the model's ability to segment and classify relationships based on three types of text prompts:\n\n1.  Providing only the predicate, asking the model to identify the subject and object. \n2.  Providing the subject and predicate, asking the model to identify the object.\n3. Providing the subject and object, asking the model to identify the predicate. \n\nThe use of red characters highlights instances where the model successfully identifies unseen objects or predicates, showcasing the model's generalization to novel concepts.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative results of promptable VRS on HICO-DET [4] test set. We show visualizations of subject and object masks and relationship category outputs, given three types of text prompts. In (c), we show the predicted predicates in bold characters. Unseen objects and predicates are denoted in red characters.", "description": "This figure shows qualitative results of the proposed FleVRS model on the HICO-DET test set for promptable visual relationship segmentation (VRS).  It demonstrates the model's ability to segment relationships based on different types of text prompts (missing subject, missing object, or missing predicate).  The use of bold and red text highlights the model's performance on unseen object and predicate categories. This showcases the model's flexibility and capacity for handling novel relationships.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/figures/figures_21_1.jpg", "caption": "Figure 1: FleVRS is a single model trained to support standard, promptable and open-vocabulary fine-grained visual relationship segmentation (<subject mask, relationship categories, object mask>). It can take images only or images with structured prompts as inputs, and segment all existing relationships or the ones subject to the text prompts.", "description": "The figure showcases the FleVRS model's capabilities in three scenarios: standard, promptable, and open-vocabulary visual relationship segmentation.  In the standard setting, it segments all relationships present in an image. In the promptable setting, it segments relationships based on textual prompts provided as input. Finally, the open-vocabulary setting demonstrates the model's ability to segment relationships involving unseen object and predicate categories not present in its training data.", "section": "Introduction"}, {"figure_path": "kJkp2ECJT7/figures/figures_22_1.jpg", "caption": "Figure 2: Examples of converting HOI detection boxes to masks. We filter out low-quality masks during training by computing IoU between the mask and box.", "description": "This figure shows examples of how the authors convert Human-Object Interaction (HOI) detection bounding boxes into segmentation masks.  The process uses the Segment Anything Model (SAM) to generate masks, and then filters out low-quality masks by calculating the Intersection over Union (IoU) between the generated mask and the original bounding box.  Only masks with sufficient overlap (above a certain IoU threshold) are retained for training. This step helps to improve the quality of the training data and reduce noise caused by inaccurate bounding box annotations.", "section": "2 Related Work"}]