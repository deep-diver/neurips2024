[{"figure_path": "js74ZCddxG/figures/figures_3_1.jpg", "caption": "Figure 1: Overall framework", "description": "This figure illustrates the overall framework of the robust federated learning algorithm proposed in the paper. It's a four-round algorithm involving interactions between user devices and a server.  Each round involves specific computations, including normalization and quantization of local updates, generation and exchange of secret shares, robust aggregation, and model updates. The figure depicts the flow of information and computations in each round, highlighting the use of secure aggregation techniques to maintain privacy while enhancing robustness against poisoning attacks.", "section": "4 Framework"}, {"figure_path": "js74ZCddxG/figures/figures_5_1.jpg", "caption": "Figure 2: Cosine similarity computation on packed secret sharing", "description": "The figure illustrates two different approaches to computing cosine similarity using packed Shamir secret sharing. The naive approach directly sends local shares, resulting in the server reconstructing a partial dot product, which reveals more information than intended.  The proposed method, called dot product aggregation protocol, uses a more secure approach that ensures the server only reconstructs the final dot product, thus protecting user privacy.  This highlights the increased information leakage risk of a naive approach and the improved privacy protection of the proposed technique.", "section": "4 Framework"}, {"figure_path": "js74ZCddxG/figures/figures_9_1.jpg", "caption": "Figure 3: Per-iteration communication (left two) and computation cost (right two).", "description": "This figure shows a comparison of communication and computation costs between RFLPA and BREA. The left two graphs show the communication cost, broken down by the number of users and model dimensions.  The right two graphs show the computation cost, broken down by the number of users for both the server and individual users.  The results clearly demonstrate that RFLPA significantly reduces both communication and computation overhead compared to BREA.", "section": "6 Experiments"}, {"figure_path": "js74ZCddxG/figures/figures_22_1.jpg", "caption": "Figure 3: Per-iteration communication (left two) and computation cost (right two).", "description": "This figure shows the communication and computation overhead comparison between RFLPA and BREA. The left two graphs illustrate communication costs under varying client sizes and model dimensions.  The right two graphs present computation costs for users and the server under varying client sizes.  RFLPA demonstrates significantly reduced overhead compared to BREA in all scenarios.", "section": "6 Experiments"}, {"figure_path": "js74ZCddxG/figures/figures_22_2.jpg", "caption": "Figure 5: Original and inferred image under RFLPA.", "description": "This figure compares the original images from the CIFAR-10 dataset with the images reconstructed by the Deep Leakage from Gradients (DLG) attack under the RFLPA framework. The reconstructed images are noisy and bear little resemblance to the originals, demonstrating the effectiveness of RFLPA in protecting the privacy of the training data against inference attacks.", "section": "K.6.2 Inference Attacks"}, {"figure_path": "js74ZCddxG/figures/figures_24_1.jpg", "caption": "Figure 3: Per-iteration communication (left two) and computation cost (right two).", "description": "This figure shows the communication and computation overhead of RFLPA and BREA.  The left two graphs compare per-iteration communication cost (in MB) for both algorithms across different numbers of participating users (top) and varying model dimensions (bottom). The right two graphs compare the per-iteration computation cost (in ms) for both algorithms under the same conditions as the communication cost comparisons. These graphs illustrate that RFLPA significantly reduces both communication and computation overhead compared to BREA.", "section": "6 Experiments"}]