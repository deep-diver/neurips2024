[{"figure_path": "js74ZCddxG/tables/tables_7_1.jpg", "caption": "Table 1: Complexity summary of RFLPA and BERA", "description": "This table compares the computational and communication complexities of the proposed RFLPA framework and the BERA framework.  For both frameworks, complexities are broken down for server-side and user-side operations, and expressed using Big O notation in terms of model dimension (M) and number of selected clients (N).  The table highlights the significant reduction in computational and communication overhead achieved by RFLPA compared to BERA.", "section": "5.1 Complexity Analysis"}, {"figure_path": "js74ZCddxG/tables/tables_8_1.jpg", "caption": "Table 2: Accuracy under different proportions of attackers. The values denote the mean \u00b1 standard deviation of the performance.", "description": "This table presents the accuracy results of different federated learning frameworks (FedAvg, Bulyan, Trim-mean, LDP, CDP, BREA, and RFLPA) under two types of poisoning attacks: gradient manipulation and label flipping.  It shows the accuracy for each framework at different proportions of malicious users (0%, 10%, 20%, and 30%) across three datasets (MNIST, Fashion-MNIST, and CIFAR-10). The mean and standard deviation of the accuracy are provided.", "section": "6.2 Experiment Results"}, {"figure_path": "js74ZCddxG/tables/tables_13_1.jpg", "caption": "Table 3: Notation table.", "description": "This table lists notations used in the paper.  It includes notations for model parameters, datasets, learning rates, client sizes, packed secret shares, partial cosine similarities, gradient norms, trust scores, and various matrix operations used within the algorithms. The table is crucial for understanding the mathematical representation and operations described within the paper.", "section": "A Notation Table"}, {"figure_path": "js74ZCddxG/tables/tables_14_1.jpg", "caption": "Table 4: Comparison between Byzantine-robust aggregation rules.", "description": "This table compares four different Byzantine-robust aggregation rules: KRUM, Bulyan, Trim-mean, and FLTrust.  For each rule, it lists the computation complexity, whether prior knowledge of the number of poisoners is required, the maximum number of poisoners tolerated, and whether the rule is compatible with Shamir Secret Sharing (SSS).  The table highlights that FLTrust offers advantages due to its lower computation complexity, lack of prior knowledge requirement, high tolerance for poisoners, and SSS compatibility.", "section": "C Comparison between Byzantine-robust aggregation rules"}, {"figure_path": "js74ZCddxG/tables/tables_21_1.jpg", "caption": "Table 5: Corse-grained comparison among Aggregation Frameworks. \u201c/\u201d denotes non-applicable. ELSA improves on RoFL regarding the the efficiency.", "description": "This table compares several federated learning frameworks based on four key aspects: robustness against malicious users, privacy protection against the server, collusion threshold during model training, and the multi-party computation (MPC) techniques used.  It highlights the strengths and weaknesses of each framework in terms of its ability to defend against attacks while preserving user privacy, and the computational complexity associated with each approach.", "section": "K.4 Comparison among Aggregation Frameworks"}, {"figure_path": "js74ZCddxG/tables/tables_22_1.jpg", "caption": "Table 6: Accuracies on CIFAR-10 under varying proportions of attackers. For backdoor attacks, the values are presented as overall accuracy (backdoor accuracy).", "description": "This table compares the accuracies of FedAvg and RFLPA on the CIFAR-10 dataset under different proportions (10%, 20%, 30%) of attackers.  It includes results for three types of attacks: KRUM attack (a poisoning attack that targets the model's robustness), BadNets (a backdoor attack that introduces malicious behavior), and Scaling attack (another type of backdoor attack). For BadNets and Scaling attacks, the table shows both the overall accuracy and the accuracy on the targeted subset of data.", "section": "6.2 Experiment Results"}, {"figure_path": "js74ZCddxG/tables/tables_23_1.jpg", "caption": "Table 7: Accuracies on NLP dataset under different proportions of attackers.", "description": "This table presents the accuracy results of three different federated learning frameworks (FedAvg, BREA, and RFLPA) on two natural language processing (NLP) datasets (RTE and WNLI) under varying proportions of malicious attackers (0%, 10%, 20%, and 30%).  The results show the robustness of each framework against poisoning attacks by showing how their accuracy changes as the percentage of malicious attackers increases. This allows for a comparison of the resilience of different federated learning approaches to adversarial attacks in NLP tasks.", "section": "K.7 Performance on Diverse Dataset"}, {"figure_path": "js74ZCddxG/tables/tables_23_2.jpg", "caption": "Table 8: Accuracy on CIFAR-100 dataset under gradient manipulation attack.", "description": "This table presents the accuracy of the RFLPA and FedAvg models on the CIFAR-100 dataset under gradient manipulation attacks with varying proportions of attackers (0%, 10%, 20%, and 30%).  The results show the mean \u00b1 standard deviation of the accuracy for each condition. It demonstrates the robustness of RFLPA against these attacks, especially when compared to FedAvg.", "section": "6 Experiments"}, {"figure_path": "js74ZCddxG/tables/tables_23_3.jpg", "caption": "Table 9: Computation cost (in minutes) with varying client size.", "description": "This table compares the computation time (in minutes) for RFLPA and three HE-based methods (PEFL, PBFL, ShieldFL) with varying client sizes (100, 200, 300, 400). It shows the per-user cost and the server cost for each method and client size.  The results highlight the significant difference in computation time between RFLPA and the HE-based methods, demonstrating the efficiency advantage of RFLPA.", "section": "K.8 Overhead Analysis"}, {"figure_path": "js74ZCddxG/tables/tables_24_1.jpg", "caption": "Table 10: Communication cost (in MB) per client with varying client size with MNIST classifier (1.6M parameters). RFLPA (KRUM) replaces the aggregation rule with KRUM in RFLPA.", "description": "This table presents the communication overhead (in MB) per client for different client sizes (300, 400, 500, 600) when using three different methods: RFLPA, BREA, and RFLPA (KRUM). RFLPA (KRUM) is a variant of RFLPA where the aggregation rule is replaced with KRUM.  The results show the communication cost of RFLPA and its KRUM variant are significantly lower than BREA.", "section": "K.8 Overhead Analysis"}, {"figure_path": "js74ZCddxG/tables/tables_24_2.jpg", "caption": "Table 9: Computation cost (in minutes) with varying client size.", "description": "This table presents the computation cost for RFLPA, PEFL, PBFL, and ShieldFL with varying client sizes (100, 200, 300, 400).  It shows the per-user computation cost and the server computation cost for each algorithm.  The table highlights the significant difference in computation time between RFLPA and the HE-based methods (PEFL, PBFL, ShieldFL), demonstrating RFLPA's efficiency.", "section": "K.8 Overhead Analysis"}, {"figure_path": "js74ZCddxG/tables/tables_25_1.jpg", "caption": "Table 2: Accuracy under different proportions of attackers. The values denote the mean \u00b1 standard deviation of the performance.", "description": "This table shows the accuracy of different federated learning frameworks (FedAvg, Bulyan, Trim-mean, LDP, CDP, BREA, and RFLPA) under gradient manipulation and label flipping attacks with varying proportions of malicious users (0%, 10%, 20%, and 30%).  The results are presented for three datasets: MNIST, Fashion-MNIST, and CIFAR-10.  The table highlights the robustness of RFLPA against poisoning attacks compared to other methods.  The mean and standard deviation of the accuracy are reported for each method and attack condition.", "section": "6.2 Experiment Results"}, {"figure_path": "js74ZCddxG/tables/tables_25_2.jpg", "caption": "Table 2: Accuracy under different proportions of attackers. The values denote the mean \u00b1 standard deviation of the performance.", "description": "This table shows the accuracy of different federated learning frameworks (FedAvg, Bulyan, Trim-mean, LDP, CDP, BREA, and RFLPA) under two types of poisoning attacks (gradient manipulation and label flipping) with varying proportions of malicious users (0%, 10%, 20%, and 30%).  The results are presented for three datasets: MNIST, Fashion-MNIST, and CIFAR-10. The table highlights the robustness of each method against poisoning attacks.", "section": "6.2 Experiment Results"}]