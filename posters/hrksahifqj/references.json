{"references": [{"fullname_first_author": "J. Achiam", "paper_title": "Constrained policy optimization", "publication_date": "2017-00-00", "reason": "This paper introduces a constrained policy optimization method that is a foundational approach for many other constrained reinforcement learning methods, including some that are directly compared in this paper."}, {"fullname_first_author": "Y. Liu", "paper_title": "Ipo: Interior-point policy optimization under constraints", "publication_date": "2020-00-00", "reason": "This paper presents a novel method for constrained policy optimization that is directly compared to the proposed method in this paper, making it an important baseline and a key point of comparison."}, {"fullname_first_author": "T.-H. Pham", "paper_title": "Optlayer-practical constrained optimization for deep reinforcement learning in the real world", "publication_date": "2018-00-00", "reason": "This paper is a key reference due to its focus on practical constrained optimization, with a comparison to the proposed method providing a real-world context for evaluation."}, {"fullname_first_author": "J. Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-00-00", "reason": "This is a highly influential paper in reinforcement learning that provides the foundation for the Proximal Policy Optimization (PPO) algorithm used in this paper, making it a crucial reference for understanding the optimization technique used."}, {"fullname_first_author": "L. Yang", "paper_title": "Cup: A conservative update policy algorithm for safe reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper is an important reference because it introduces another recent approach to safe reinforcement learning that is directly compared to the proposed method in this paper."}]}