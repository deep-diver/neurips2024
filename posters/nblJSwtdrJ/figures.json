[{"figure_path": "nblJSwtdrJ/figures/figures_0_1.jpg", "caption": "Figure 1: Demonstration of train-once-for-all personalization scenario. Users have text descriptions of the desired personalized models.", "description": "This figure illustrates the train-once-for-all personalization process. A central server hosts a model, \"Tina,\" trained to generate personalized models based on text descriptions provided by users.  The figure shows three example user requests, each requesting a different type of classifier (dog-cat, cat breeds, vehicle).  Tina processes these requests and generates the corresponding specialized classifier models. The output displays sample classifications for each model, demonstrating the personalized outputs of the generated classifiers.", "section": "1 Introduction"}, {"figure_path": "nblJSwtdrJ/figures/figures_2_1.jpg", "caption": "Figure 2: Description of the training and testing data for Tina. p-Model is short for personalized models. The blue blocks are for training, and the green blocks are for testing.", "description": "This figure illustrates the data partitioning strategy used for training and evaluating the Tina model.  The blue blocks represent the training data, which consists of personalized models (p-Models) generated from training tasks, paired with their corresponding task descriptions. These p-Models are used to train Tina, the text-conditioned neural network diffusion model.  The green blocks represent the testing data, divided into in-distribution and out-of-distribution sets.  In-distribution testing evaluates Tina's ability to generate p-Models for tasks similar to those seen during training. Out-of-distribution testing evaluates its generalization ability on unseen tasks.  The diagram shows how p-Models derived from different tasks are grouped for training and then tested against seen and unseen tasks, showcasing the model's in-distribution and out-of-distribution generalization capabilities.", "section": "2.1.3 Dataset Preparation and Description"}, {"figure_path": "nblJSwtdrJ/figures/figures_3_1.jpg", "caption": "Figure 3: Framework overview of Tina.", "description": "This figure shows the framework of Tina, a text-conditioned neural network diffusion model for train-once-for-all personalization.  The training process (a) involves using a CLIP text encoder to encode task descriptions, adding noise to p-Model parameters, and using a diffusion transformer to predict the denoised parameters. The test process (b) takes user text prompts or image prompts, uses the CLIP encoder, adds random noise to the parameters, and uses the diffusion transformer to generate the personalized p-Model.  The figure highlights the use of padding, tokenization, and augmentation steps in both training and testing.", "section": "2.2 Proposed Tina: Text-conditioned Neural Network Diffusion Model"}, {"figure_path": "nblJSwtdrJ/figures/figures_6_1.jpg", "caption": "Figure 4: Tina capability analysis w.r.t. different parameterization and training schemes. (a) Scaling the parameters of DiT in Tina. CNN-5K (14K) means the p-Model is a CNN with 5K (14K) parameters. From 152M (hidden size 32) to 789M (hidden size 2048), scaling helps in the emergence of intelligence. (b) Parameter inheritance from pretrained G.pt helps speed up training in the early. (c) Training Tina with image-prompted data versus text-prompted data. The text-prompted has faster convergence.", "description": "This figure presents three subfigures showing different aspects of Tina's capabilities. Subfigure (a) demonstrates the impact of scaling the parameters of the diffusion transformer (DiT) in Tina on the accuracy of generating personalized models (p-Models). Subfigure (b) illustrates the effect of parameter inheritance from a pre-trained GPT model on Tina's training efficiency. Subfigure (c) compares the training performance of Tina using image prompts versus text prompts.", "section": "3.3 In-depth Analysis of Tina"}, {"figure_path": "nblJSwtdrJ/figures/figures_6_2.jpg", "caption": "Figure 2: Description of the training and testing data for Tina. p-Model is short for personalized models. The blue blocks are for training, and the green blocks are for testing.", "description": "This figure demonstrates the data partitioning strategy for training and testing Tina.  The blue blocks represent the training data, which includes samples of personalized models (p-Models) created for various seen tasks and their corresponding task descriptions. The green blocks show the testing data, further divided into in-distribution (light green) and out-of-distribution (dark green) sets. In-distribution tests assess Tina's ability to generate personalized models for tasks similar to those in its training data. Out-of-distribution tests assess its generalization capabilities by evaluating the performance of models generated for previously unseen tasks.", "section": "2.1.3 Dataset Preparation and Description"}, {"figure_path": "nblJSwtdrJ/figures/figures_7_1.jpg", "caption": "Figure 3: Framework overview of Tina.", "description": "The figure provides a visual representation of the Tina model's framework, illustrating its architecture and workflow during both training and testing phases.  It highlights the key components, such as the Diffusion Transformer, CLIP text and image encoders, and the process of generating personalized models from text or image prompts.  The training phase involves adding noise to a pre-trained model, embedding the task description, and using the diffusion transformer to predict the denoised model parameters. The testing phase uses the generated model to perform the intended task, with user input provided as text or image prompts.", "section": "2.2 Proposed Tina: Text-conditioned Neural Network Diffusion Model"}]