[{"figure_path": "MDgn9aazo0/tables/tables_3_1.jpg", "caption": "Table 1: Averaged performance gain from channel identity information (\u2206L(%)) and Pearson Correlation Coefficients (PCC) between {\u2206Lij}i,j and {SIM(Xi, Xj)}i,j. The values are averaged across all test samples.", "description": "This table presents the results of a toy experiment designed to explore the relationship between channel similarity and model performance.  Four different time series models (TSMixer, DLinear, PatchTST, TimesNet) with varying channel strategies (CD and CI) were trained.  The performance was measured using MSE loss on test sets where channels were either kept in their original order or randomly shuffled. \u2206L(%) represents the average performance gain (in terms of MSE loss) due to using the original channel order instead of randomly shuffling channels.  The PCC (Pearson Correlation Coefficient) indicates the correlation between the performance gain and the calculated similarity between channel pairs using a radial basis function kernel. The negative PCC values indicate that the more similar the channels are, the less the model performance is affected by shuffling channels, suggesting the importance of channel identity.", "section": "4.1 Motivation for Channel Similarity"}, {"figure_path": "MDgn9aazo0/tables/tables_5_1.jpg", "caption": "Table 2: The statistics of datasets in long-term forecasting. Horizon is {96, 192, 336, 720}.", "description": "This table presents the characteristics of nine datasets used for long-term time series forecasting experiments in the paper.  For each dataset, it lists the number of channels (variates), the length of the time series (number of data points), the frequency of data sampling, and the forecasting horizons used in the experiments.  The datasets cover diverse domains including weather, traffic, electricity, and illness data, offering a comprehensive benchmark for evaluating the proposed method.", "section": "5.1 Experimental Setup"}, {"figure_path": "MDgn9aazo0/tables/tables_5_2.jpg", "caption": "Table 3: Dataset details of M4 and Stock in short-term forecasting.", "description": "This table presents the length and horizon of the time series datasets used for short-term forecasting.  The M4 dataset is broken down by the frequency of the time series (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly), showing the number of data points (length) and the prediction time range (horizon) for each.  A new Stock dataset is also included, which has 10,000 time series and a horizon of 7 or 24, depending on the experiment setup.", "section": "5.1 Experimental Setup"}, {"figure_path": "MDgn9aazo0/tables/tables_6_1.jpg", "caption": "Table 4: Long-term forecasting results on 9 real-world datasets in terms of MSE and MAE, the lower the better. The forecasting horizons are {96, 192, 336, 720}. The better performance in each setting is shown in bold. The best results for each row are underlined. The last column shows the average percentage of MSE/MAE improvement of CCM over four base models.", "description": "This table presents the results of long-term time series forecasting experiments on nine real-world datasets using four different base models (TSMixer, DLinear, PatchTST, TimesNet) with and without the Channel Clustering Module (CCM).  The table shows the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each model and dataset at four different forecasting horizons (96, 192, 336, and 720 time steps).  The best performing model for each row is underlined and the percentage improvement achieved by CCM over the four base models is given in the last column. Lower MSE and MAE values indicate better forecasting performance.", "section": "5.2 Long-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_6_2.jpg", "caption": "Table 5: Comparison between CCM and existing regularization method for improved performance on CI/CD strategies in terms of MSE metric. The best results are highlighted in bold.", "description": "This table compares the performance of the proposed Channel Clustering Module (CCM) against a previously published regularization method (PRReg) for improving the performance of Channel-Independent (CI) and Channel-Dependent (CD) models in time series forecasting.  The metric used is Mean Squared Error (MSE), and lower values indicate better performance. The table shows that for various datasets (ETTh1, ETTm1, Weather, ILI, Electricity), CCM generally outperforms or matches PRReg in terms of MSE reduction across different model types (linear and transformer).", "section": "5.3 Short-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_7_1.jpg", "caption": "Table 4: Long-term forecasting results on 9 real-world datasets in terms of MSE and MAE, the lower the better. The forecasting horizons are {96, 192, 336, 720}. The better performance in each setting is shown in bold. The best results for each row are underlined. The last column shows the average percentage of MSE/MAE improvement of CCM over four base models.", "description": "This table presents the results of long-term forecasting experiments conducted on nine real-world datasets using four different time series forecasting models: TSMixer, DLinear, PatchTST, and TimesNet.  Each model was evaluated with and without the Channel Clustering Module (CCM). The table displays the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each model and dataset across four different forecasting horizons (96, 192, 336, and 720). The best performing model for each horizon and dataset is highlighted in bold, and the best overall result for each row is underlined. The final column shows the average percentage improvement in MSE and MAE achieved by CCM across all models and datasets.", "section": "5.2 Long-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_7_2.jpg", "caption": "Table 7: Zero-shot forecasting results on ETT datasets. The forecasting horizon is {96, 720}. The best value in each row is underlined.", "description": "This table presents the results of zero-shot forecasting experiments conducted on the ETT datasets.  The model was trained on one subset of the ETT data and then tested on unseen subsets (ETTh1 to ETTh2, ETTh1 to ETTm1, etc.). Two different forecasting horizons were used (96 and 720).  The table shows the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each model and generalization task. The best performing model for each row is underlined.  The purpose is to demonstrate the effectiveness of the proposed method (CCM) in scenarios where the model must generalize to unseen data.", "section": "5.4 Zero-shot Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_15_1.jpg", "caption": "Table 8: Complexity of similarity computation", "description": "This table shows the time complexity of different similarity computation approaches used in the paper. The time complexity is expressed in Big O notation and depends on the length (H) of the time series. Euclidean distance has a time complexity of O(H), while Edit Distance, Dynamic Time Warping (DTW), Longest Common Subsequence (LCSS), and Cross-correlation (CCor) all have a time complexity of O(H^2).", "section": "A Definitions"}, {"figure_path": "MDgn9aazo0/tables/tables_17_1.jpg", "caption": "Table 9: The statistics of dataset in long-term and short-term forecasting tasks", "description": "This table presents the characteristics of the datasets used in the paper's experiments. It shows the dataset name, number of channels, forecasting horizon (the number of future time steps to predict), the length of the time series, the frequency of data points (e.g., hourly, daily, etc.), and the domain from which the data originates.  It's divided into long-term and short-term forecasting tasks, which reflects the different prediction horizons used in the respective experiments.", "section": "5.1 Experimental Setup"}, {"figure_path": "MDgn9aazo0/tables/tables_18_1.jpg", "caption": "Table 10: Experiment configuration.", "description": "This table shows the hyperparameters used in the experiments for different datasets.  It includes the number of clusters (K) used in the Channel Clustering Module (CCM), the regularization parameter (\u03b2), the number of linear layers in the MLP for channel embedding, the hidden dimension of the embedding, and the number of layers in each of the four base time series models (TSMixer, DLinear, PatchTST, TimesNet). The values are optimized for each dataset.", "section": "5.1 Experimental Setup"}, {"figure_path": "MDgn9aazo0/tables/tables_18_2.jpg", "caption": "Table 11: Full Results on Comparison between CCM and existing regularization method for enhanced performance on CI/CD strategies in terms of MSE metric. The best results are highlighted in bold.", "description": "This table compares the performance of the proposed Channel Clustering Module (CCM) against the Channel Independent (CI) and Channel Dependent (CD) strategies, as well as a previously proposed regularization method (PRReg). The results, measured by MSE (Mean Squared Error), are presented for multiple datasets and different model types (Linear and Transformer). The table shows CCM's ability to improve performance on both CI and CD baselines.", "section": "C.4 Comparison between CCM and Other Approach"}, {"figure_path": "MDgn9aazo0/tables/tables_19_1.jpg", "caption": "Table 12: Multivariate intrinsic similarity for long-term forecasting datasets", "description": "This table shows the average Pearson correlation coefficient (r) among channels for each of the nine datasets used in the long-term forecasting experiments.  The correlation coefficient is a measure of the linear relationship between channels, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation).  A value of 0 indicates no linear correlation.  This table provides insight into the inherent relationships between channels within each dataset, which influences the effectiveness of different channel strategies (Channel-Independent vs. Channel-Dependent).", "section": "5.2 Long-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_19_2.jpg", "caption": "Table 13: Intrinsic similarity for short-term forecasting datasets", "description": "This table presents the average Pearson correlation coefficients (r) between time series in different M4 sub-datasets (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly).  It shows the degree of multivariate correlation across channels in the datasets, which is used to assess the influence of intrinsic similarity on forecasting performance.  Higher correlation indicates stronger inherent relationships between time series within the dataset.", "section": "5.3 Short-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_20_1.jpg", "caption": "Table 14: Standard deviation of Table 2 on long-term forecasting benchmarks. The forecasting horizon is 96.", "description": "This table shows the standard deviation of the MSE and MAE values reported in Table 2 for nine real-world datasets used in long-term forecasting experiments.  The forecasting horizon for all experiments was 96 time steps. The table provides a measure of the variability in the model's performance across multiple runs with different random seeds for each dataset and model configuration.  It helps to understand the reliability of the results presented in Table 2.", "section": "5.2 Long-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_20_2.jpg", "caption": "Table 6: Short-term forecasting results on M4 dataset in terms of SMAPE, MASE, and OWA, and stock dataset in terms of MSE and MAE. The lower the better. The forecasting horizon is {7, 24} for the stock dataset. The better performance in each setting is shown in bold.", "description": "This table presents the results of short-term forecasting experiments using four different time series models (TSMixer, DLinear, PatchTST, TimesNet) with and without the Channel Clustering Module (CCM).  The models are evaluated on the M4 dataset (with yearly, quarterly, monthly, weekly, daily, and hourly sub-datasets) and a new stock dataset.  The metrics used are SMAPE, MASE, and OWA for the M4 dataset, and MSE and MAE for the stock dataset.  The table shows the performance of each model for different forecasting horizons (7 and 24 for the stock dataset), highlighting the best-performing model in each case.", "section": "5.3 Short-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_21_1.jpg", "caption": "Table 16: Standard deviation of Table 6 on stock dataset", "description": "This table presents the standard deviations of the MSE and MAE metrics reported in Table 6 for the stock dataset.  It shows the variability in performance across multiple runs for different models (TSMixer, DLinear, PatchTST, TimesNet) with and without the Channel Clustering Module (CCM), for both forecasting horizons (7 and 24). The low standard deviations indicate consistent performance across runs.", "section": "5.3 Short-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_21_2.jpg", "caption": "Table 1: Averaged performance gain from channel identity information (\u2206L(%)) and Pearson Correlation Coefficients (PCC) between {\u2206Lij}i,j and {SIM(Xi, Xj)}i,j. The values are averaged across all test samples.", "description": "This table presents the results of a toy experiment designed to motivate the concept of channel similarity.  The experiment randomly shuffles channels within batches during training, removing channel identity information. The table shows the average performance drop (\u2206L(%)) for four different time series models, each using either a channel-dependent (CD) or channel-independent (CI) strategy.  It also presents the Pearson Correlation Coefficient (PCC) between the performance drop and the channel similarity (SIM(Xi, Xj)) calculated using radial basis function kernels. The negative PCC values indicate an anticorrelation between channel similarity and the impact of shuffling, supporting the hypothesis that the model's reliance on channel identity information is inversely proportional to the similarity between channels.", "section": "4.1 Motivation for Channel Similarity"}, {"figure_path": "MDgn9aazo0/tables/tables_22_1.jpg", "caption": "Table 18: Short-term forecasting on stock dataset with different look-back window length in {14,21,28}. The forecasting length is 7. The best results with the same base model are underlined. Bold means CCM successfully enhances forecasting performance over the base model.", "description": "This table presents the results of short-term stock price forecasting experiments using different look-back window lengths (14, 21, 28).  The forecasting horizon is 7 days.  The table compares the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for four different time series forecasting models (TSMixer, DLinear, PatchTST, TimesNet), both with and without the Channel Clustering Module (CCM). The best-performing model for each configuration is underlined, and bold values indicate cases where CCM improved performance compared to the baseline model.  The \"Imp.\" column shows the percentage improvement achieved by CCM.  The results highlight the impact of different look-back window lengths on forecasting accuracy and the effectiveness of CCM in improving performance.", "section": "5.3 Short-term Forecasting Results"}, {"figure_path": "MDgn9aazo0/tables/tables_22_2.jpg", "caption": "Table 5: Comparison between CCM and existing regularization method for improved performance on CI/CD strategies in terms of MSE metric. The best results are highlighted in bold.", "description": "This table compares the performance of the proposed Channel Clustering Module (CCM) against a previously proposed regularization method (PRReg) for improving the performance of Channel-Independent (CI) and Channel-Dependent (CD) models in time series forecasting.  The results are presented in terms of Mean Squared Error (MSE), a common metric for evaluating forecasting accuracy. Lower MSE values indicate better performance. The table shows that CCM generally outperforms PRReg across different model types (Linear and Transformer) and datasets (ETTh1, ETTm1, Weather, ILI, Electricity).", "section": "5.3 Short-term Forecasting Results"}]