[{"figure_path": "ugXKInqDCC/figures/figures_0_1.jpg", "caption": "Figure 1: AdaFlow is a fast imitation learning policy. It adaptively adjust the number of simulation steps when generating actions. For low-variance states, it functions as a one-step action generator. For high-variance states, it employs more steps to ensure accurate action generation. This adaptive approach enables AdaFlow to achieve an average generation speed close to one step per task completion.", "description": "This figure compares the performance of AdaFlow and Diffusion Policy.  It shows that AdaFlow can generate actions much faster than Diffusion Policy, especially in low-variance states where it only requires a single step. In high-variance states, AdaFlow still significantly outperforms Diffusion Policy by requiring fewer simulation steps.  The color bar visually represents the state variance, illustrating how the algorithm adapts the number of steps to the variance of the state. This adaptive strategy allows AdaFlow to maintain high speed across different states.", "section": "Abstract"}, {"figure_path": "ugXKInqDCC/figures/figures_1_1.jpg", "caption": "Figure 2: Illustrating the computation adaptivity of AdaFlow (orange) on simple regression task. In the upper portion of the image, we use Diffusion Policy (DDIM) and AdaFlow to predict y given x, with deterministic y = 0 when x < 0, and bimodal y = \u00b1x when x > 0. Both DDIM and AdaFlow fit the demonstration data well. However, the simulated ODE trajectory learned by Diffusion-Policy with DDIM (red) is not straight no matter what x is. By contrast, the simulated ODE trajectory learned by AdaFlow with fixed step (blue) is a straight line when the prediction is deterministic (x < 0), which means the generation can be exactly done by one-step Euler discretization. At the bottom, we show that AdaFlow can adaptively adjust the number of simulation steps based on the x value according to the estimated variance at x.", "description": "This figure demonstrates AdaFlow's adaptive computation.  The top shows the prediction of y from x using AdaFlow and Diffusion Policy. AdaFlow generates a straight line when the prediction is deterministic, demonstrating its one-step generation capability.  The bottom shows how AdaFlow adjusts the number of simulation steps based on the variance at x, achieving efficiency while maintaining accuracy.", "section": "2 Illustrating the computation adaptivity"}, {"figure_path": "ugXKInqDCC/figures/figures_6_1.jpg", "caption": "Figure 3: Generated trajectories. We visualize the trajectories generated by different policies, with the agent's starting point fixed.", "description": "This figure compares the trajectories generated by four different methods: Diffusion Policy with 1 and 20 function evaluations, behavioral cloning, and AdaFlow.  It shows that AdaFlow generates trajectories that are both diverse and efficient, while Diffusion Policy with only 1 function evaluation is less successful and less diverse in its results.  Behavioral cloning's trajectories are relatively constrained.", "section": "4.2 Navigating a 2D Maze"}, {"figure_path": "ugXKInqDCC/figures/figures_7_1.jpg", "caption": "Figure 4: LIBERO tasks. We visualize the demonstrated trajectories of the robot's end effector.", "description": "This figure shows six different manipulation tasks from the LIBERO benchmark dataset.  Each subfigure shows a single task, visualizing the robot's end effector movements. The trajectories illustrate the complexity and diversity of the actions involved, highlighting the challenges in robotic manipulation and the ability of the model to execute them effectively.", "section": "4.3 Robot Manipulation Tasks"}, {"figure_path": "ugXKInqDCC/figures/figures_8_1.jpg", "caption": "Figure 5: Predicted variance. We visualize the variance predicted by AdaFlow. The variance is computed on states from the expert's demonstration and averaged over all simulation steps (e.g., t from 0 to 1). Then we normalize the variance to [0, 1] by the largest variance found at all states.", "description": "This figure compares the ground truth variance with the variance predicted by AdaFlow across different states during robot manipulation tasks.  The top row shows the ground truth variance, visualized as a colormap on the robot arm's trajectory during a series of actions.  The bottom row displays AdaFlow's variance prediction for the same task, using similar color coding.  It demonstrates AdaFlow's ability to accurately estimate the variance in states with complex action distributions (high variance), while maintaining efficiency in states with more straightforward deterministic actions (low variance).", "section": "4.4 Ablation Study"}, {"figure_path": "ugXKInqDCC/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation studies on AdaFlow.", "description": "This figure presents ablation studies on AdaFlow, comparing its performance with baselines (BC and Diffusion Policy) across different metrics.  The top panel shows success rate (SR) against the number of function evaluations (NFE), demonstrating AdaFlow's efficiency in achieving high success rates even with low NFE. The bottom panel illustrates the training efficiency by displaying success rate over epochs, highlighting AdaFlow's faster learning speed compared to Diffusion Policy.", "section": "4.4 Ablation Study"}, {"figure_path": "ugXKInqDCC/figures/figures_13_1.jpg", "caption": "Figure 7: Trajectories of 100 demonstrations for each maze.", "description": "This figure visualizes 100 demonstration trajectories for four different mazes.  Two mazes are single-task, meaning the agent always starts and ends in the same location, and two mazes are multi-task, where the start and end locations vary. The trajectories show the paths taken by an expert agent to navigate these mazes, illustrating the diverse and complex behaviors involved in maze navigation. These demonstrations were used to train the AdaFlow model. The color intensity represents the speed of the agent, with brighter colors denoting higher speed.", "section": "A.4 Visualization of Tasks"}, {"figure_path": "ugXKInqDCC/figures/figures_14_1.jpg", "caption": "Figure 7: Trajectories of 100 demonstrations for each maze.", "description": "This figure visualizes 100 demonstration trajectories for four different maze environments (Maze1, Maze2, Maze3, Maze4).  Each maze presents a unique navigational challenge with varying complexity and layout.  The trajectories showcase the diverse paths taken by an expert agent to navigate each maze, highlighting the multi-modality and complexity of the task that an imitation learning algorithm needs to capture.", "section": "A.4 Visualization of Tasks"}, {"figure_path": "ugXKInqDCC/figures/figures_16_1.jpg", "caption": "Figure 9: Generated trajectories. We visualize the trajectories generated by standard Rectified Flow and AdaFlow, with the agent's starting point remaining fixed. 0", "description": "This figure compares the trajectories generated by standard Rectified Flow and AdaFlow in a maze navigation task.  The visualization highlights the difference in path planning and action generation between the two methods.  Standard Rectified Flow, even with multiple steps (NFE=5), struggles to produce diverse and efficient paths. In contrast, AdaFlow efficiently generates diverse and effective trajectories with a significantly lower number of function evaluations (NFE=1.12). The figure underscores AdaFlow's improved efficiency and ability to produce diverse and effective behaviors.", "section": "A.10 Comparison with standard Rectified Flow"}]