[{"figure_path": "yDjojeIWO9/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of UMI-GRAT towards SAM and its downstream tasks. The UMI-GRAT can mislead various downstream models by solely utilizing information from the open-sourced SAM.", "description": "This figure illustrates the UMI-GRAT (Universal Meta-Initialized and Gradient Robust Adversarial Attack) method. It shows how imperceptible adversarial perturbations, generated only using information from the open-source SAM (Segment Anything Model), can mislead various downstream models fine-tuned on different tasks (medical, shadow, camouflage image segmentation). The attack's effectiveness comes from the UMI algorithm, which extracts the inherent vulnerabilities in the foundation model, and the gradient robust loss, which enhances robustness against gradient-based noise and improves transferability. This showcases the risk of using open-sourced models without considering potential adversarial attacks.", "section": "1 Introduction"}, {"figure_path": "yDjojeIWO9/figures/figures_5_1.jpg", "caption": "Figure 2: The data flow of our UMI-GRAT, consisting of an offline learning process of UMI and a real-time gradient robust adversarial attack.", "description": "This figure illustrates the two-stage process of the proposed UMI-GRAT method. The first stage involves an offline learning process for universal meta-initialization (UMI). This process aims to extract the intrinsic vulnerability of the foundation model (SAM) by training on natural image datasets. The extracted vulnerability is represented as a universal meta-initialization perturbation (\u03b4). In the second stage, a real-time gradient robust adversarial attack is performed. The UMI (\u03b4) is adapted to a specific input (x<sub>t</sub>) and is used to initialize the adversarial perturbation. Then the adversarial perturbation is updated by maximizing a gradient robust loss (L<sub>GR</sub>) that is designed to mitigate the deviation caused by gradient disparity between the surrogate model (open-sourced SAM) and the victim model (fine-tuned downstream model).  Gradient noise augmentation is used to enhance robustness.", "section": "5 Implementation of the proposed MUI-GRAT"}, {"figure_path": "yDjojeIWO9/figures/figures_6_1.jpg", "caption": "Figure 3: The cosine similarity of white-box generated perturbations on surrogate and victim models.", "description": "This figure shows the cosine similarity between the gradients of white-box generated adversarial perturbations on both the surrogate model (open-source SAM) and the victim models (fine-tuned downstream models) across different attack methods (MI-FGSM, MI-FGSM+GR, and ILPD).  The x-axis represents the iteration step (t) of the attack, and the y-axis represents the cosine similarity.  Higher cosine similarity indicates higher alignment between the gradients, suggesting better transferability of adversarial examples. The figure demonstrates that MI-FGSM+GR achieves higher cosine similarity compared to other methods, implying that its adversarial examples transfer better to victim models. The 'opt' labels point to the optimal cosine similarity for each attack method.", "section": "Experimental Results"}, {"figure_path": "yDjojeIWO9/figures/figures_8_1.jpg", "caption": "Figure 2: The data flow of our UMI-GRAT, consisting of an offline learning process of UMI and a real-time gradient robust adversarial attack.", "description": "This figure illustrates the two-stage process of the proposed UMI-GRAT method. The first stage is an offline learning process for Universal Meta Initialization (UMI) where the model learns to extract the intrinsic vulnerability from the foundation model.  The second stage is a real-time gradient robust adversarial attack. Here, the UMI is adapted to the specific task and a gradient robust loss is used to generate adversarial perturbations that are robust to the variations between the surrogate and victim models. The figure shows the data flow for both offline UMI learning and real-time attack using the extracted UMI.", "section": "5 Implementation of the proposed MUI-GRAT"}, {"figure_path": "yDjojeIWO9/figures/figures_16_1.jpg", "caption": "Figure A.5: The visualized adversarial attack results in camouflaged object segmentation task.", "description": "This figure visualizes the results of adversarial attacks on camouflaged object segmentation. For three different datasets (COD-10K, CHAME, CAMO), it shows the original image, the image with adversarial noise added, the ground truth segmentation mask, and the segmentation mask produced by the model when given the adversarial image.  The goal is to illustrate how imperceptible adversarial noise can cause significant errors in the model's segmentation output.", "section": "A.5 Visualization of the adversarial examples and the prediction"}, {"figure_path": "yDjojeIWO9/figures/figures_16_2.jpg", "caption": "Figure 2: The data flow of our UMI-GRAT, consisting of an offline learning process of UMI and a real-time gradient robust adversarial attack.", "description": "This figure illustrates the two-stage process of the proposed UMI-GRAT attack method.  The first stage (left panel) is an offline process where Universal Meta-Initialization (UMI) is performed. This involves using natural images to train the image encoder of the open-sourced SAM to find an optimal adversarial perturbation that is robust to downstream model variations. The second stage (right panel) is a real-time gradient robust adversarial attack. The pre-trained UMI from the first stage is used to initialize the adversarial perturbation, then the perturbation is further optimized using a gradient-robust loss function to mitigate the deviation that can occur between the surrogate (open-sourced SAM) and victim (downstream) models. Gradient noise augmentation helps enhance the robustness of the adversarial example, improving transferability to different downstream tasks. ", "section": "5 Implementation of the proposed MUI-GRAT"}]