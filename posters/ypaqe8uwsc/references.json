{"references": [{"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduces federated learning, a core concept in the paper's approach to distributed learning."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper is foundational to the offline reinforcement learning component of FEDORA, introducing a key algorithm used in the proposed method."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper provides datasets used for evaluating FEDORA, offering a benchmark for comparing the performance of federated offline reinforcement learning algorithms."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-00-00", "reason": "This paper introduces an improved actor-critic method used by FEDORA, which is crucial for handling the complexities of continuous control environments."}, {"fullname_first_author": "Tianhe Yu", "paper_title": "Conservative data sharing for multi-task offline reinforcement learning", "publication_date": "2021-00-00", "reason": "This paper addresses data heterogeneity in offline RL, a key challenge that FEDORA also tackles, offering insights into handling diverse datasets from different sources."}]}