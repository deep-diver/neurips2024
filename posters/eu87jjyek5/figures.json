[{"figure_path": "eU87jJyEK5/figures/figures_5_1.jpg", "caption": "Figure 1: The overall framework for SpeAr. The whole training process consists of two stages.", "description": "The figure illustrates the SpeAr model's training process, which is divided into two stages. Stage 1 uses an unsupervised spectral contrastive loss (Luscl) to learn initial node embeddings and class prototypes from the graph data and class semantic vectors (CSVs).  The class prototypes are initialized using CSVs. In Stage 2, a supervised spectral contrastive loss (Lscl) refines these embeddings and prototypes using both labeled and unlabeled data. This iterative process improves node representation and class separation, ultimately enhancing the model's zero-shot node classification performance.  The colored nodes indicate seen classes in the first stage and both seen and unseen classes in the second stage.", "section": "4 Experiment"}, {"figure_path": "eU87jJyEK5/figures/figures_7_1.jpg", "caption": "Figure 1: The overall framework for SpeAr. The whole training process consists of two stages.", "description": "The figure illustrates the two-stage training process of the SpeAr model. Stage 1 involves unsupervised spectral contrastive loss (Luscl) to initialize class prototypes using semantic vectors (CSVs).  The graph neural network (GNN) generates node embeddings.  Stage 2 uses supervised spectral contrastive loss (Lscl) to refine class prototypes iteratively based on updated node embeddings, leading to improved classification accuracy.  The figure shows the flow of information and the processes of initialization and update steps of the two stages.  Different colors of nodes indicate different node states. ", "section": "4 Experiment"}, {"figure_path": "eU87jJyEK5/figures/figures_8_1.jpg", "caption": "Figure 3: The effects of \u03b1 and \u03b2 on Cora, Citeseer, and C-M10M.", "description": "This figure shows the impact of parameters \u03b1 and \u03b2 on the classification accuracy across three different datasets: Cora, Citeseer, and C-M10M.  Each subplot represents a dataset. The x-axis represents the value of \u03b1, and the y-axis represents the value of \u03b2. The height of each bar represents the classification accuracy achieved with the given parameter combination. The visualization helps to understand the relative contribution of labeled and unlabeled samples in the spectral contrastive loss function (Lscl).", "section": "4.4 Parametric Analysis"}, {"figure_path": "eU87jJyEK5/figures/figures_8_2.jpg", "caption": "Figure 4: An example of the SpeAr's effectiveness in mitigating prediction bias on Cora dataset.", "description": "The radar chart visualizes the performance of different zero-shot node classification models (DGPN, DBiGCN, and SpeAr) across various categories, showcasing SpeAr's superior ability to mitigate prediction bias by achieving higher recall in multiple categories compared to other models.", "section": "4.5 Further Analysis"}, {"figure_path": "eU87jJyEK5/figures/figures_14_1.jpg", "caption": "Figure 5: An example of the SpeAr model's effectiveness in mitigating prediction bias on Citeseer.", "description": "This figure compares the performance of three different zero-shot node classification methods (DGPN, DBIGCN, and SpeAr) on the Citeseer dataset. Each radar chart visualizes the recall (a measure of how well the model identifies nodes belonging to a specific class) for three different classes: Artificial Intelligence, Human-Computer Interaction, and Database. The SpeAr model exhibits substantially better recall across all three classes, particularly for classes with low recall in other methods (Artificial Intelligence and Human-Computer Interaction).  This demonstrates the model's improved ability to correctly classify nodes from unseen classes and to mitigate prediction bias.", "section": "6.2.2 Discussion on Different CSVS"}]