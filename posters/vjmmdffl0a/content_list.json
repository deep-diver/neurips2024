[{"type": "text", "text": "The Benefits of Balance: From Information Projections to Variance Reduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lang Liu\u2217 Ronak Mehta\u2217 Soumik Pal Zaid Harchaoui ", "page_idx": 0}, {"type": "text", "text": "University of Washington ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data balancing across multiple modalities and sources appears in various forms in foundation models in machine learning and AI, e.g. in CLIP and DINO. We show that data balancing across modalities and sources actually offers an unsuspected benefit: variance reduction. We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators. Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have shown remarkable success at learning task-specific representations of data when provided supervision from massive amounts of labeled training examples. Recent trends, however, have shifted toward task-agnostic, universal representations that may be easily fine-tuned or even have zero-shot capabilities out of the box. Supervised learning, stricto sensu, is too limited a framework for these billion-parameter, data-hungry models, and a question at the heart of modern machine learning is learning from unlabeled, partially labeled, or weakly labeled data. ", "page_idx": 0}, {"type": "text", "text": "This need has paved the way for the current generation of self-supervised learning (SSL) approaches that circumvent the need for large amounts of \u201cstrong\u201d labels. In SSL, a model is trained on a generic pseudo-task suited for unlabeled data, such as relating image-caption pairs or augmentations of the same image. Despite modern foundation models such as DINO [Caron et al., 2021] and CLIP [Radford et al., 2021] being trained in this fashion, many aspects of SSL remain mysterious. ", "page_idx": 0}, {"type": "text", "text": "In particular, the training process of self-supervised models often transcends the rules of the standard empirical risk minimization (ERM) toolkit. ERM combines two well-understood techniques: minibatch sampling and gradient-based optimization using backpropagation. On the other hand, SSL adds clever, yet less-understood techniques to the training pipeline. To illustrate this, consider a minibatch $(X_{1},Y_{1}),\\ldots,(X_{n},Y_{n})$ of training examples observed in $\\mathcal X\\times\\mathcal X$ (e.g. feature-label or image-caption pairs) and let $P_{n}$ be the empirical distribution of the minibatch. For a model parameterized by $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ with loss function $h_{\\theta}$ , a standard stochastic learning algorithm involves computing the minibatch loss ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{n}}\\left[h_{\\theta}(X,Y)\\right]=\\frac{1}{n}\\sum_{i=1}^{n}h_{\\theta}(X_{i},Y_{i})\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "and backpropagating through it to produce a minibatch stochastic gradient estimate. The algorithm then proceeds with the stochastic gradient training or a variant thereof (e.g., Adam). Self-supervised methods often modify this recipe by intervening on the optimization algorithm in a minibatch-specific way. For example, SwaV [Caron et al., 2020] passes the minibatch examples through the model\u2019s encoder and clusters output vectors to generate pseudo-labels for a prediction task. ", "page_idx": 0}, {"type": "text", "text": "In teacher-student architectures such as BYOL [Grill et al., 2020] and DINO [Caron et al., 2021], the minibatch is passed through two networks, where the \u201cstudent\u201d network is updated via backpropagation and the \u201cteacher\u201d network is updated by cloning the student\u2019s weights in regular intervals. In CLIP [Radford et al., 2021], a model optimizes the sum of two cross-entropy losses, where the predicted class probabilities on example $i$ is generated by comparison to all other elements of the minibatch. While introducing such interventions into the procedure has clearly proven useful practically, it remains conceptually unclear what exactly is being optimized by the learning algorithm. ", "page_idx": 1}, {"type": "text", "text": "In this work, we aim to gain a better theoretical understanding of the objectives and algorithms underlying these empirically effective recipes. In particular, we want to shed a theoretical light on their marginal benefits over traditional learning methods. In this work, we show that such recipes often enjoy an unsuspected benefit: reducing the variance of the empirical minibatch objective. ", "page_idx": 1}, {"type": "text", "text": "To be more concrete, we formalize the broad steps of the training algorithms above as follows. Let $Z_{1},\\ldots,Z_{n}$ be a minibatch containing data points of arbitrary type (e.g. unlabeled images). Possibly using a model parameterized by $\\theta$ , this original data source is mapped to another minibatch $(X_{1},Y_{1}),\\ldots,(X_{n},Y_{n})$ of derived pairs in $\\mathcal{X}\\times\\mathcal{Y}$ . For example, in SwaV, each $Z_{i}$ is an image with vector representation $X_{i}$ and pseudo-label $Y_{i}$ based on clustering. In CLIP, each $Z_{i}$ is an image-caption pair and $X_{i}$ and $Y_{i}$ are the vector representations of its image and text components, respectively. We may also use the parameterized model to compute a probability distribution $P_{n,\\theta}$ over $\\mathcal X\\times\\mathcal X$ , and view each training step above as exactly or approximately optimizing the objective ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{n,\\theta}}\\left[h_{\\theta}(X,Y)\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This reduces to empirical risk minimization on the minibatch objective (1) when $Z=(X,Y)$ (each data point is originally observed in $\\mathcal{X}\\times\\mathcal{Y}$ ) and $P_{n,\\theta}=P_{n}$ (the empirical distribution of the data is used, regardless of the model). Beyond this setting, one specific example of $P_{n,\\theta}$ has been applied across various families of self-supervised learning (as detailed in Sec. 2), which we refer to as data balancing or simply balancing, the primary subject of this work. ", "page_idx": 1}, {"type": "text", "text": "For a probability measure $Q$ on $\\mathcal X\\times\\mathcal X$ , let $Q_{X}$ and $Q_{Y}$ be the respective marginals on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ and let $Q_{X\\mid Y}$ and $Q_{Y\\mid X}$ denote the respective conditional distributions. Given an initial measure $P_{n}^{(0)}$ over $\\mathcal X\\times\\mathcal X$ which may depend on the model2, and fixed marginal distributions $P_{X}$ on $\\mathcal{X}$ and $P_{Y}$ on $\\boldsymbol{\\wp}$ , balancing refers to generating the sequence $P_{n}^{(0)},\\ldots,P_{n}^{(k)}$ by applying the operations ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ=Q_{X}\\cdot Q_{Y|X}\\mapsto P_{X}\\cdot Q_{Y|X}\\quad{\\mathrm{and}}\\quad Q=Q_{Y}\\cdot Q_{X|Y}\\mapsto P_{Y}\\cdot Q_{X|Y},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "in an alternating fashion. After enough iterations, the resulting probability measure approximately marginalizes to $P_{X}$ and $P_{Y}$ in each variable. When $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are finite with $|{\\boldsymbol{\\mathcal{X}}}|=m$ and $|\\mathcal{Y}|=\\bar{l}$ , these operations reduce to rescaling the rows of an $(m\\times l)$ -matrix by $P_{X}/Q_{X}$ or its columns by $P_{Y}/Q{\\bar{Y}}$ . This algorithm has a decades-old history and is known in other contexts as Sinkhorn-Knopp matrix scaling [Sinkhorn, 1967], iterative proportional or biproportional ftiting [Johnston and Pattie, 1993], and raking-ratio estimation [Thompson, 2000]. The marginals $P_{X}$ and $P_{Y}$ can represent auxiliary information or inductive bias from users, such as the desire for balanced clusters. ", "page_idx": 1}, {"type": "text", "text": "As we describe in detail in Sec. 2, both self-labeling and contrastive approaches in SSL implicitly use $P_{n,\\theta}=P_{n}^{(k)}$ in (2) for different choices of $P_{n}^{(0)}$ . In other words, they embed a learnable balancing operation in their objectives, where the objective (as in (2)) or the stochastic gradient estimate depends linearly on $P_{n,\\theta}$ . A natural question to consider is: if the marginals one uses accurately represent the ones of the true probability measure $P$ governing the data, are balanced quantities \u201cbetter behaved\u201d than their unbalanced counterparts? If so, in what way? ", "page_idx": 1}, {"type": "text", "text": "Inspired by this question, we fix the model parameter $\\theta$ (thus dropping the subscript from the quantities above) and analyze the fluctuations of the unbalanced and balanced objectives. The formal problem statement is as follows. Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be two finite sample spaces on which there is an unknown probability measure $P$ with known marginals $(P_{X},P_{Y})$ . We observe independent data $(X_{1},Y_{1}),\\ldots,(X_{n},Y_{n})\\sim P$ , defining the empirical measure $\\begin{array}{r}{P_{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{(X_{i},Y_{i})}}\\end{array}$ . Let $P_{n}^{(0)}=P_{n}$ and $P_{n}^{(k)}$ denote the output of $k\\geq1$ iterations of data balancing (see Sec. 3 for the precise definition). Finally, letting $h:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ be a function of interest, we define the population parameter $\\varphi$ and its $k$ -step balanced estimator $\\varphi_{n}^{(k)}$ by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\varphi:=\\mathbb{E}_{P}\\left[h(X,Y)\\right]\\quad\\mathrm{and}\\quad\\varphi_{n}^{(k)}:=\\mathbb{E}_{P_{n}^{(k)}}\\left[h(X,Y)\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our goal is to establish theoretical guarantees on the mean squared error (MSE) $\\mathbb{E}_{P}[(\\varphi_{n}^{(k)}-\\varphi)^{2}]$ of estimating $\\varphi$ using $\\varphi_{n}^{(k)}$ , with an informative dependence on the sample size $n$ , number of iterations $k$ , tdairrgeectt  emsatirgmiantaolsr $(P_{X},P_{Y})$ t, haen ed mtepsitr ifcuanlc tmioena $h$ .r eW in=1 h(Xi, Yi) ,d  aisn  tiot s qcuoamntpifayri tshoen  teoff tehcet of the auxiliary information $(P_{X},P_{Y})$ . Our analysis uncovers two surprising facts. Firstly, while originally proposed for a different purpose, balancing reduces the variance of the empirical estimate. Secondly, while the balancing iterations are nonlinear operations on the input measure, the variance reduction can be precisely quantified using the spectral decay of two linear Markov operators: the conditional means given $X$ and $Y$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Contributions. In Sec. 2, we clarify the mathematical connection between the classical data balancing methods and the modern representation learning techniques mentioned above. In Sec. 3, we prove a new upper bound on the MSE of the balancing estimator $\\varphi_{n}^{(k)}$ . The bound decomposes into an $O(n^{-1})$ first-order variance term and an $O(n^{-3/2})$ second-order term. The first-order term is shown to have a strict improvement over the empirical measure baseline with a fine-grained dependence on the spectra of two particular Markov operators. Our proof technique relies on a recursion decomposition for balancing-based estimators, which may be of independent interest. In Sec. 4, we illustrate how insights from our analysis can be practically applied to CLIP-type objectives and evaluation setups. ", "page_idx": 2}, {"type": "text", "text": "2 Data Balancing in Practice ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now expand on the examples of data balancing applications mentioned in Sec. 1. To demonstrate a precise connection to (2), we describe how a collection of training examples $Z_{1},\\ldots,Z_{n}$ observed in an original data space $\\mathcal{Z}$ (e.g. grayscale images) is mapped to a probability measure $P_{n,\\theta}$ . Using the framework introduced in Sec. 1, this amounts to specifying four components: 1) the map from the original data into the derived sample spaces $\\mathcal{X}$ and $\\mathcal{V},2$ ) the initial measure $P_{n}^{(0)}$ , 3) the function $h$ , and 4) the target marginals $(P_{X},P_{Y})$ for this measure to fit. From that point, the iterations of (3) porf oduce P (1 ), . . $P_{n}^{(1)},\\ldots,P_{n}^{(k)}$ wone  tshete $P_{n,\\theta}:=P_{n}^{(k)}$ . eFteorr asweh eofn  pcroenssetnrtuacttiionng, iadse  dtehsec rdiebpeedn idne n(4c)e. $P_{n}^{(k)}=P_{n,\\theta}$ $h\\equiv h_{\\theta}$ $\\theta$ $\\varphi_{n}^{(k)}$ Note that the sample spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ do not necessarily correspond to feature-label pairs in each example below; each example given below corresponds to a different choice of $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ (see Fig. 1). ", "page_idx": 2}, {"type": "text", "text": "Example 1: Self-Supervised Learning. Balancing is used in discriminative clustering and selfsupervised clustering; see [Jones et al., 2022, Asano et al., 2020, Caron et al., 2020] for variations on this theme. We describe the swapped prediction task of Caron et al. [2020] for concreteness but emphasize that clustering of this form is used as an intermediate step (or as the task itself) in many SSL pseudo-tasks. At a high level, this approach involves passing elements of a minibatch through two encoders to generate vector representations. These representations are then clustered separately, and the features from one encoder predict the cluster label from the other encoders. Denote the encoders $f_{\\theta_{s}}:\\mathcal{Z}\\to\\mathbb{R}^{r}$ and $f_{\\theta_{t}}:\\mathcal{Z}\\to\\mathbb{R}^{r}$ , colloquially known as the student and teacher networks, respectively. Here, we let $\\{Z_{i}\\}_{i=1}^{n}$ be a minibatch of $n$ images, with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{X}=\\{Z_{1},\\dotsc,Z_{n}\\}\\quad\\mathrm{and}\\quad\\mathcal{Y}=\\{1,\\dotsc,l\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $m=n$ and the elements of $\\boldsymbol{\\wp}$ index learnable cluster representation vectors $c_{1},\\ldots,c_{l}\\in\\mathbb{R}^{r}$ . Thus, we consider the overall parameter vector to be $\\theta:=(\\theta_{s},\\theta_{t},c_{1},\\ldots,c_{l})$ . Given temperature hyperparameters $\\epsilon,\\tau>0$ , the initial measure and loss function are given by the expressions ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{n}^{\\scriptscriptstyle(0)}(x,y)\\propto e^{f_{\\theta_{s}}(x)^{\\top}c_{y}/\\epsilon}\\quad\\mathrm{and}\\quad h(x,y)=\\log\\frac{e^{f_{\\theta_{t}}(x)^{\\top}c_{y}/\\tau}}{\\sum_{y^{\\prime}=1}^{l}e^{f_{\\theta_{t}}(x)^{\\top}c_{y^{\\prime}}/\\tau}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Directly optimizing $\\begin{array}{r}{\\sum_{x,y}P_{n}^{(0)}\\underline{{(x,y)}}h(x,y)}\\end{array}$ without any constraints would lead to collapse, so it is balanced before optimized. The target marginals $P_{X}$ and $P_{Y}$ are given by the discrete uniform measures on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ . This formulation is often derived by solving an optimal transport problem with the Sinkhorn-Knopp algorithm to assign soft cluster labels, the iterative solution result from this procedure is precisely $P_{n}^{(k)}$ . The intuition behind the choice of uniform marginal $P_{X}$ is that each data point has an equal amount of mass to allot to each cluster, whereas $P_{Y}$ captures that the cluster sizes are equal. The number of iterations $k$ is selected based on optimization considerations. ", "page_idx": 2}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/6617053dd2133572b3bb290e616760437056a8a0d2835bfa4be85c0ae3887c5f.jpg", "img_caption": ["Figure 1: Data Balancing Examples: Each panel shows a possible distribution $Q$ on different choices of $(\\mathcal{X},\\mathcal{Y})$ . The orange histograms are the target marginal $P_{Y}$ . Left: $Q(x,y)$ is the affinity of an image $x$ for cluster $y$ . Center: $Q(x,y)$ is the similarity of an image $x$ to a text caption $y$ . Right: $Q(x,y)$ is the proportion of substring matches between a text caption $x$ and a keyword $y$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Example 2: Contrastive Learning. Contrastive Language-Image Pre-Training [Radford et al., 2021], or CLIP, is an architecture with an image encoder and a text encoder that map to a joint embedding space. Trained using image-caption pairs, the loss promotes representations such that images and text that are paired in the minibatch are close, whereas those that are not paired are far. The latter aspect (promoting dissimilarity of unpaired images/text) is what prevents collapse in this framework. Our interpretation of the CLIP objective as an implicit data balancing procedure is novel, to our knowledge. Under this interpretation, we demonstrate that the objective is in fact a nonlinear function of $P_{n,\\theta}$ , whereas its gradient will have a linear form similar to (2). In this case, each $Z_{i}=(X_{i},Y_{i})$ , where $X_{i}$ is an image and $Y_{i}$ is an associated caption. We have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\{X_{1},\\ldots,X_{n}\\}\\quad\\mathrm{and}\\quad\\mathcal{Y}=\\{Y_{1},\\ldots,Y_{n}\\}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "so that $m=n$ . Consider an image encoder $f_{\\theta_{I}}:\\mathcal{X}\\mapsto\\mathbb{R}^{r}$ and text encoder $f_{\\theta_{T}}:\\mathcal{D}\\mapsto\\mathbb{R}^{r}$ with parameter vector $\\boldsymbol{\\theta}=\\left(\\boldsymbol{\\theta}_{I},\\boldsymbol{\\theta}_{T}\\right)$ . The initial, unnormalized measure and the (in this case, vector-valued) function $h$ are chosen based on these encoded representations: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{n}^{(0)}(x,y)\\propto e^{f_{\\theta_{I}}(x)^{\\top}f_{\\theta_{T}}(y)}\\quad\\mathrm{and}\\quad h(x,y)=\\nabla_{\\theta}(f_{\\theta_{I}}(x)^{\\top}f_{\\theta_{T}}(y)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While we usually interpret $h$ as a loss function, we will show below that the CLIP loss depends nonlinearly on $P_{n,\\theta}$ , while the gradient has a linear dependence. If we believe, as in Example 1, that the target marginals $(P_{X},P_{Y})$ of the images and the text should be roughly uniform, we can apply the balancing iterations (3) with the target marginals being the uniform distributions over $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , respectively. Because there is no preference for starting the iterations with the $\\mathcal{X}$ or $\\boldsymbol{\\wp}$ dimension $R_{n}^{(1)}$ rwepe remsaeyn tc oonnes isduecr hb iottehr aotirodne riinn gths.e $\\mathcal{X}$ t $Q_{n}^{(1)}$ nbsieo on.n eT ihteerna ttihoe no orifg binalaal nCciLnIgP  ionb tjhece $\\boldsymbol{\\wp}$ ed $L_{n}^{\\mathrm{CLIP}}$ iocna na nbde recovered (up to an additive constant) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{{\\cal L}_{n}^{\\mathrm{CLIP}}:=-\\frac{1}{2}\\sum_{i=1}^{n}\\left[\\log\\frac{P_{n}^{(0)}(X_{i},Y_{i})}{\\sum_{x}P_{n}^{(0)}(x,Y_{i})}+\\log\\frac{P_{n}^{(0)}(X_{i},Y_{i})}{\\sum_{y}P_{n}^{(0)}(X_{i},y)}\\right]}\\\\ &{}&{=-\\frac{1}{2}\\sum_{i=1}^{n}[\\log Q_{n}^{(1)}(X_{i},Y_{i})+\\log R_{n}^{(1)}(X_{i},Y_{i})]-\\log n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The measure $\\begin{array}{r}{P_{n,\\theta}=\\frac{1}{2}Q_{n}^{(1)}\\!+\\!\\frac{1}{2}R_{n}^{(1)}}\\end{array}$ is constructed in this case by averaging the outputs of one iteration of balancing under each modality. Taking the gradient of (6) with respect to $\\theta$ (whose dependence is contained in $(Q_{n}^{(1)},R_{n}^{(1)}))$ ) recovers the expression for $h$ in (5). The objective is often interpreted as an average of cross-entropy loss terms, each representing the prediction of one modality\u2019s original pair from the other. In our formulation, $L_{n}^{\\mathrm{CLIP}}$ can also be viewed as an average negative log-likelihood under the $Q_{n}^{(1)}$ and $R_{n}^{(1)}$ . It is also of interest to study the effect of using $Q_{n}^{(k)}$ and $R_{n}^{(k)}$ for $k\\geq0$ in general, as we show in Sec. 4. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Example 3: Metadata Curation. Here, we consider taking $\\mathcal{M}_{n}$ to be an entire training set, as opposed to a particular mini-batch. At the billion-parameter scale, dataset design can be the primary factor that differentiates performance between foundation models [Fang et al., 2013, Xu et al., 2024, Gadre et al., 2023]. One general approach used in both the original CLIP dataset [Radford et al., 2021] and an open-source replication $[\\mathrm{Xu}$ et al., 2024] is metadata curation, wherein a text dataset $\\mathcal{M}_{n}$ (possibly captions for images) is synthesized using a list of keywords $\\{y_{1},\\ldots,y_{l}\\}$ so that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{X}=\\left\\{Z_{1},\\dots,Z_{n}\\right\\},\\quad\\mathcal{Y}=\\left\\{y_{1},\\dots,y_{l}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "meaning that $m=n$ . The keywords are matched to texts within $\\mathcal{X}$ via substring matching. While the approach of $\\mathrm{Xu}$ et al. [2024] (dubbed MetaCLIP) pools all matched keywords on every text to measure the \u201cdistribution\u201d of keywords, we consider a version in which each text $Z_{i}$ can only be labeled with a single keyword $y_{j}$ . This allows for a true joint probability measure on $\\mathcal X\\times\\mathcal X$ . The marginal distribution of observed keywords is initially long-tailed (see Fig. 4) (e.g., \u201cthe\u201d will match many more texts than \u201cxylophone\u201d). In both Radford et al. [2021] and $\\mathrm{Xu}$ et al. [2024], the data are resampled so that this distribution of keywords over matches is closer to uniformity, i.e. keywords with many matches have their associated texts downsampled during the dataset creation process. While the probability measure may not be computed explicitly (due to scale), this adjustment of the keyword distribution can be viewed as a single iteration of balancing (3) applied to the $\\boldsymbol{\\wp}$ marginal. For tasks such as language modeling, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{n}^{(0)}(x,y)=P_{n}(x,y)\\quad\\mathrm{and}\\quad h(x,y)=\\ell(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\ell(x)$ denotes the loss of a model evaluated at a single text $x\\in\\mathscr{X}$ (notice that the keyword is not used). We elucidate this connection by applying direct balancing on a subset of the ImageNet-Captions dataset in Sec. 4, observing the effect on downstream model performance. ", "page_idx": 4}, {"type": "text", "text": "Motivated by these scenarios, we address the statistical problem outlined in Sec. 1 by analyzing balancing-based estimators. We then return to examples mentioned above in Sec. 4, illustrating how the theoretical analysis can be translated to algorithmic variants. ", "page_idx": 4}, {"type": "text", "text": "3 Theoretical Analysis of Variance Reduction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present theoretical guarantees on the mean squared error (MSE) of the data-balanced estimator $\\varphi_{n}^{(k)}$ and highlight relevant points in the proofs. For readers\u2019 convenience, a notation table (Tab. 1) is in Appx. A. We first give context on the main innovations of the analysis and then outline its high-level steps. These innovations include relating the nonlinear iterations of balancing over probability measures to linear operators on a vector space and using a singular value decomposition of these operators to quantify their effect after a finite number of iterations. Furthermore, by scaling the number of iterations appropriately, we can characterize the estimator using the limit of balancing iterations, which is an object of interest in applications including optimal transport. ", "page_idx": 4}, {"type": "text", "text": "Preliminaries. Recall the setting introduced in Sec. 1, in which we consider sample spaces $(\\mathcal{X},\\mathcal{Y})$ , along with true and unknown joint distribution $P$ on $\\mathcal X\\times\\mathcal X$ with known marginals $(P_{X},P_{Y})$ . For ease of presentation, we assume that $|{\\mathcal{X}}|=|{\\mathcal{Y}}|=m$ , although the arguments do not rely on equal support sizes. We make the following assumption throughout, which is usually satisfied by the desired marginals $P_{X}$ and $P_{Y}$ , such as in the uniform cases discussed in Sec. 2: the target marginals $P_{X}\\!\\left(x\\right)>0$ and $P_{Y}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ . We define $P_{n}^{(0)}=P_{n}$ as the empirical measure and for $k\\geq1$ construct ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{n}^{(k)}(x,y):=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}\\cdot P_{n}^{(k-1)}(x,y)}&{\\ k\\ \\mathrm{odd}}\\\\ {\\frac{P_{Y}(y)}{P_{n,Y}^{(k-1)}(y)}\\cdot P_{n}^{(k-1)}(x,y)}&{\\ k\\ \\mathrm{even}}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By direct computation, we see that the iterations in (8) are equivalent to applying (3) for $k$ odd and even, respectively. See Fig. 2 (left) for a visualization of this procedure. The iterations are ", "page_idx": 4}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/b2a75cf33ed66b3ab5c54deee6622364ccd7690d88259a56888ca157d3887f9a.jpg", "img_caption": ["Figure 2: Data Balancing. Nonlinear and linear operators associated to each iteration of (8). Left: Visualization of the exact iterations of (8) in the space of probability measures. The blue set contains joint distributions with $\\mathcal{X}$ -marginal equal to $P_{X}$ , whereas the orange set contains joint distributions with $\\boldsymbol{\\wp}$ -marginal equal to $P_{Y}$ . Right: Visualization of $\\mathbf{L}^{2}(P)$ , the operators defining (11), and the singular values given in (13). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "well-defined for all $k$ under the event that $\\operatorname{Supp}(P_{n,X})=\\operatorname{Supp}(P_{X})$ and $\\operatorname{Supp}(P_{n,Y})=\\operatorname{Supp}(P_{Y})$ , i.e., all observed row counts and column counts are non-empty.3 ", "page_idx": 5}, {"type": "text", "text": "To provide background, the scheme of alternating the operators (8) is often seen as an iterative algorithm to solve the problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q\\in\\Pi(P_{X},P_{Y})}\\mathrm{KL}(Q||P_{n}^{(0)}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Pi(P_{X},P_{Y})$ denotes the set of probability measures on $\\mathcal X\\times\\mathcal Y$ that marginalize to $P_{X}$ and $P_{Y}$ in each variable and $\\operatorname{KL}(\\cdot\\|\\cdot)$ denotes the Kullback-Leibler divergence. The iterations (8) can be derived from the alternating minimization subproblems ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{n}^{(k)}(x,y):=\\left\\{\\arg\\operatorname*{min}_{\\{Q:Q_{X}=P_{X}\\}}\\mathrm{KL}(Q||P_{n}^{(k-1)})\\right.\\quad k\\left.\\mathrm{odd}\\right.\\quad}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which inspires the viewpoint of balancing as alternating information projections. As we show in Appx. C, the iterations of (8) can equivalently be defined using the KL, reverse KL, or $\\chi^{2}$ -divergences. This viewpoint is relevant as previously, efforts have been made (e.g. in Bickel et al. [1998]) to analyze the variance reduction afforded by the solution to (9) directly. However, quantifying the variance reduction (in terms of properties of $P$ ) using this approach is challenging, as there is no closed-form expression for the solution of (9). A key mathematical outcome of our analysis is that the closed-form expressions of the projections (8) can be used to compute the reduction in mean squared error at each iteration. Thus, by letting $k\\rightarrow\\infty$ (scaled appropriately against $n$ ), we can determine the reduction for the solution of (9) for large $n$ . This is the subject of the upcoming Cor. 2. ", "page_idx": 5}, {"type": "text", "text": "From Information Projections to Orthogonal Projections. First, we will show that the variance reduction resulting from each nonlinear iteration of (8) is associated with a linear operator applied to $h$ . Thus, instead of analyzing the alternating information projections over probability measures, we may use familiar tools to understand alternating orthogonal projections in a vector space. To define them, we first let $\\mathbf{L}^{2}(P)$ to be the set of functions $h:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ satisfying $\\mathbb{E}_{P}\\left[h^{\\hat{2}}(X,Y)\\right]<\\infty$ . Even though $\\mathcal X\\times\\mathcal Y$ is finite, working within $\\mathbf{L}^{2}(P)$ will be analytically convenient. Let $\\mathbf{L}^{2}(P_{X})$ be the subspace of $\\mathbf{L}^{2}(P)$ containing functions that only depend on the first argument $x\\,\\in\\,{\\mathcal{X}}$ and define $\\hat{\\mathbf{L}}^{2}(P_{Y})$ analogously. These are the solid-colored subspaces in Fig. 2 (right). Next, let $\\mu_{X}:\\mathbf{L}^{2}(P)\\to\\mathbf{L}^{2}(P_{X})$ and $\\mu_{Y}^{\\cdot}:{\\bf L}^{2}(P)\\rightarrow{\\bf L}^{2}(P_{Y})$ be defined as, for any $h\\in\\mathbf{L}^{2}(\\bar{P})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{X}h=\\operatorname{arg\\,min}_{f\\in\\mathbf{L}^{2}(P_{X})}\\mathbb{E}_{P}\\left[(h(X,Y)-f(X))^{2}\\right]\\implies[\\mu_{X}h](x,y):=\\mathbb{E}_{P}\\left[h(X,Y)|X\\right](x)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The operator $\\mu_{X}$ is an orthogonal projection onto $\\mathbf{L}^{2}(P_{X})$ . The orthogonal projection operator $\\mu_{Y}$ onto $\\mathbf{L}^{2}(\\dot{P}_{Y})$ is defined analogously. We may also define the conditional debiasing operators $\\mathcal{C}_{X}=I\\!-\\!\\mu_{X}$ and $\\mathcal{C}_{Y}=I\\!-\\!\\mu_{Y}$ , which each project onto the orthogonal complements of $\\overset{\\cdot}{\\mathbf{L}^{2}}(\\dot{P}_{X})$ and $\\mathbf{L}^{2}(P_{Y})$ , visualized as subspaces with dotted border in Fig. 2 (right). To understand the importance of the conditional mean and debiasing operators, we give a recursive formula that forms the backbone of our analysis. Define $\\mu_{k}=\\mu_{X}$ for $k$ odd and $\\mu_{k}=\\mu_{Y}$ for $k$ even, and define $\\mathcal{C}_{k}$ similarly. Thus, by using the notation $Q(h):=\\mathbb{E}_{Q}[h(X,Y)]$ , we have by linearity of expectation that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[P_{n}^{(k)}-P](h)=[P_{n}^{(k)}-P](\\mathcal{C}_{k}h)+\\overbrace{[P_{n}^{(k)}-P](\\mu_{k}h)}^{=0}}\\\\ &{\\qquad\\qquad\\qquad=[P_{n}^{(k-1)}-P](\\mathcal{C}_{k}h)+[P_{n}^{(k)}-P_{n}^{(k-1)}](\\mathcal{C}_{k}h)}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{[P_{n}^{(0)}-P](\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)}_{\\mathrm{first\\-order\\,\\,term}}+\\underbrace{\\sum_{\\ell=1}^{k}[P_{n}^{(\\ell)}-P_{n}^{(\\ell-1)}](\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)}_{\\mathrm{higher\\-order\\,\\,terms}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To justify the first line, we discuss the case when $k$ is odd. Notice that $\\mu_{X}h$ is only a function of $X$ , so its expectation only depends on $P_{X}$ that is equal to Pn(k,)X by (8). The last line follows by unrolling the previous step $k-1$ times. This recursive expansion is proven formally in Prop. 15 in Appx. D. Given the expansion, the mean squared error can be computed by taking the expectation of squared (10). We show that the second moment of the first-order term in (10) is equal to $\\bar{\\sigma}_{k}^{2}/n$ where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sigma_{0}^{2}:=\\mathbb{V}\\mathrm{ar}(h)\\;\\mathrm{and}\\;\\sigma_{k}^{2}:=\\mathbb{V}\\mathrm{ar}(\\mathscr{C}_{1}\\ldots\\mathscr{C}_{k}h)\\;\\mathrm{for}\\;k\\geq1,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and all other terms are $O(n^{-3/2})$ . Thus, by exactly computing the constant in the dominating term, we may quantify the asymptotic variance reduction. Our first main result concerns the higher-order terms and shows that it is indeed dominated by the first-order term. Note that the empirical mean $\\begin{array}{r}{\\varphi_{n}^{(0)}=\\frac{1}{n}\\sum_{i=1}^{n}h(X_{i},Y_{i})}\\end{array}$ is unbiased, and so its MSE is equal to $\\sigma_{0}^{2}/n$ . Define in addition ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\star}:=\\operatorname*{min}\\{\\operatorname*{min}_{x}P_{X}(x),\\operatorname*{min}_{y}P_{Y}(y)\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which measures the non-uniformity of the target marginals. We have that $p_{\\star}$ is positive because both $P_{X}$ and $P_{Y}$ are positive. We now state the first main result. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. For a sequence of data-balancing estimators $\\big(\\varphi_{n}^{(k)}\\big)k\\{\\varphi_{n}^{(k)}\\}$ as defined in (4), there exists an absolute constant $C>0$ and distribution dependent constant $s\\in[0,1)$ and such the following holds for $\\sigma_{g a p}^{2}=\\sigma_{0}^{2}-\\sigma_{k}^{2}$ : For $n\\geq C[\\log_{2}(2n/p_{\\star})+m\\log{(n+1)}]/p_{\\star}^{2}$ and $k\\geq1$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[(\\varphi_{n}^{(k)}-\\varphi)^{2}\\right]\\le\\frac{\\sigma_{0}^{2}-\\sigma_{g a p}^{2}}{n}+O\\left(\\frac{s^{k}}{n}\\right)+\\tilde{O}\\left(\\frac{k^{6}}{n^{3/2}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The quantities $\\sigma_{\\mathrm{gap}}^{2}$ and $s$ are quantified toward the end of this section and are dependent on eigendecays of the conditional mean operators for each variable under $P$ . Furthermore, $\\sigma_{\\mathrm{gap}}^{2}>0$ except for the pathological case of $\\mu_{X}h$ being a constant function. Showing Thm. 1 boils down to showing that the higher-order term in (10) is $\\bar{O(n^{-1})}$ with high probability. Using the expression (8) and assuming that $\\ell\\geq1$ is odd, we see that ", "page_idx": 6}, {"type": "equation", "text": "$$\n[P_{n}^{(\\ell)}-P_{n}^{(\\ell-1)}](\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)=\\sum_{x,y}\\left[\\frac{P_{X}(x)}{P_{n,X}^{(\\ell-1)}(x)}-1\\right]\\cdot[\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h](x,y)P_{n}^{(\\ell-1)}(x,y).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first (blue) term in the product quantifies the disagreement between the $\\mathcal{X}$ -marginal of $P_{n}^{(\\ell-1)}$ and the true marginal, which can be bounded in terms of $\\mathrm{KL}(P_{n,X}^{(0)}||P_{X})$ and is shown to be $O(n^{-1/2})$ with high probability via techniques from information theory. The second (orange) term can be unrolled recursively in a similar fashion to (10) itself, which will consequently be $O(n^{-1/2})$ as well; this is the most technical part of the analysis (see Appx. D.3). Our analysis also yields a bound for the sensitivity of balancing to misspecified marginals; see Appx. D.5. ", "page_idx": 6}, {"type": "text", "text": "Given Thm. 1, a natural next step is to quantify the gap between $\\sigma_{0}^{2}$ and $\\sigma_{k}^{2}$ , which requires finergrained properties of $\\mathcal{C}_{X}$ and $\\mathcal{C}_{Y}$ . Notably, we show that as $k\\rightarrow\\infty$ , $\\sigma_{k}^{2}$ approaches a limiting value. Thus, via (12), by using $k=o(n^{1/12})$ obtains asymptotic variance of the solution to (9). This contrasts with Albertus and Berthet [2019], in which the dependence of a quantity similar to (12) is exponential in $k$ , meaning that $k=o(\\log(n))$ is required for convergence under this argument. ", "page_idx": 6}, {"type": "text", "text": "From Orthogonal Projections to Variance Reduction. We now clarify what is precisely meant by the \u201cspectrum\u201d of the conditional mean operators $\\mu_{X}$ and $\\mu_{Y}$ . As proven using a singular value decomposition (Prop. 3) in Appx. B.1, there exists a basis $\\{\\alpha_{j}\\}_{j=1}^{m}$ of $\\mathbf{L}^{2}(P_{X})$ , a basis $\\{\\beta_{j}\\}_{j=1}^{m}$ of $\\mathbf{L}^{2}(P_{Y})$ , and real values $\\left\\{s_{j}\\right\\}_{j=1}^{m}$ , that satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{Y}\\alpha_{j}=s_{j}\\beta_{j}\\;\\mathrm{and}\\;\\mu_{X}\\beta_{j}=s_{j}\\alpha_{j}\\;\\mathrm{for}\\;j\\in\\left\\{1,.\\dots,m\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore, $\\alpha_{1}=\\mathbf{1}_{\\mathcal{X}}$ and $\\beta_{1}=\\mathbf{1}_{\\mathcal{X}}$ leading to the equality $\\left\\langle f,\\alpha_{1}\\right\\rangle_{\\mathbf{L}^{2}(P_{X})}=\\mathbb{E}_{P_{X}}\\left[f(X)\\right]$ . Finally, $s_{1}=1$ and $s_{j}$ is non-negative and non-increasing in $j$ . For a concrete example, consider $m=2$ , in which case $P$ can be written as a matrix in $\\mathbb{R}^{2\\times2}$ and elements of $\\mathbf{L}^{2}(P_{X})$ and $\\mathbf{L}^{2}(P_{X})$ are vectors in $\\mathbb{R}^{2}$ . Then, in the case of uniform marginals, we can verify directly that (13) can be satisfied by setting ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha_{1}=\\beta_{1}=\\left[\\!\\!\\begin{array}{l}{1}\\\\ {1}\\end{array}\\!\\!\\right],\\alpha_{2}=\\beta_{2}=\\left[\\!\\!\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\!\\!\\right],\\mathrm{~and~}P=\\frac{1}{4}\\left[\\!\\!\\begin{array}{l l}{1+s}&{1-s}\\\\ {1-s}&{1+s}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for $s\\ =s_{2}$ (the second largest singular value). Thus, as $s\\rightarrow1$ , the distribution becomes \u201cfully dependent\u201d as $Y$ and $X$ are completely determined by one another. As $s\\rightarrow0$ , $P$ approaches the product measure. Geometrically, because $\\alpha_{1}=\\beta_{1}$ , we know that the angle $a$ between the subspaces $\\mathbf{\\dot{L}}^{2}(P_{X})$ and $\\mathbf{L}^{2}(P_{Y})$ is given by the angle between $\\alpha_{2}$ and $\\beta_{2}$ . By computing their inner product in $\\mathbf{L}^{2}(P)$ , we have that $\\langle\\alpha_{2},\\beta_{2}\\rangle_{\\mathbf{L}^{2}(P)}=\\left\\langle P,\\alpha_{2}\\beta_{2}^{\\top}\\right\\rangle=s=\\cos a.$ Thus, $s=0$ indicates orthogonality of these subspaces, alluding to the independence of $X$ and $Y$ . ", "page_idx": 7}, {"type": "text", "text": "Returning to $m\\geq2$ , we consider the following as a sufficient condition for variance reduction: the operators $\\mu_{X}$ and $\\mu_{Y}$ have a positive spectral gap, i.e., $s_{2}<s_{1}$ . Note that this assumption is satisfied when $P(x,y)>0$ for all $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ by the Perron\u2013Frobenius Theorem [Horn and Johnson, 2013, Chapter 8]. Using the intuition from Fig. 2, this rules out pathological cases such $Y$ being a deterministic function of $X$ , for instances Under the spectral gap condition, the singular values $\\left\\{s_{j}\\right\\}_{j=2}^{m}$ that are strictly less than 1 will determine a geometric rate of decay in variance given in Cor. 2. The left and right singular functions $\\alpha_{j}:\\mathcal{X}\\rightarrow\\mathbb{R}$ and $\\beta_{j}:y\\rightarrow\\mathbb{R}$ will define a useful coordinate system to represent projections of $h$ when analyzing $\\varphi_{n}^{(\\bar{k})}$ . ", "page_idx": 7}, {"type": "text", "text": "Indeed, let $\\bar{h}-P(h)$ be the centered test function. Because $\\mu_{X}{\\bar{h}}\\in{\\bf L}^{2}(P_{X})$ and $\\mu_{Y}\\bar{h}\\in{\\bf L}^{2}({P}_{Y})$ , we may decompose this function on the two bases to write ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mu_{X}{\\bar{h}}=\\sum_{j=1}^{m}u_{j}\\alpha_{j}\\quad{\\mathrm{and}}\\quad\\mu_{Y}{\\bar{h}}=\\sum_{j=1}^{m}v_{j}\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Cor. 2 below relates the (normalized) variance $\\sigma_{k}^{2}$ of the first-order term to the one of the sample mean $\\varphi_{n}^{(0)}$ . In fact, it shows that the variance reduction $\\sigma_{0}^{2}-\\sigma_{k}^{2}$ decays geometrically to the quantity ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{gap}}^{2}:=\\sum_{j=2}^{m}\\left[u_{j}^{2}+\\frac{(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For simplicity, we only present the result for $k$ even, i.e., $\\sigma_{2t}^{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Corollary 2. The variance reduction achieved by $t+1$ iterations of the ${\\mathcal{C}}_{Y}{\\mathcal{C}}_{X}$ operator can be quantified as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sigma_{0}^{2}-\\sigma_{2(t+1)}^{2}=\\sigma_{g a p}^{2}-\\sum_{j=2}^{m}\\frac{s_{j}^{2}(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}s_{j}^{4t}=\\sum_{j=2}^{m}\\left[u_{j}^{2}+(1-s_{j}^{4t+2})\\frac{(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Intuitively, the operators $\\mathcal{C}_{X}$ and $\\mathcal{C}_{Y}$ are the main sources of the variance reduction via orthogonality. Since $\\alpha_{1}=\\mathbf{1}_{\\mathcal{X}}$ , we can see that the reduction will always be strictly positive as long as $\\mu_{X}\\bar{h}$ is not a constant function. Finally, using $s:=s_{2}\\geq s_{j}$ for $j\\geq2$ gives the second term in Thm. 1. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Illustrations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We illustrate how data balancing manifests in the motivating examples mentioned in Sec. 2 with experiments with CLIP-type models. We focus here on zero-shot image classification tasks. Details on these experiments, and additional ones including linear probing and zero-shot retrieval, as well as an empirical investigation of the sensitivity to misspecified marginals, are all contained in Appx. E. Code to reproduce the data and experiments can be found at https://github.com/ronakdm/balancing. ", "page_idx": 7}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/def12392de3bcf7437d8da209c6f3cbfa0da2b45625366cc8dda4a19c357576e.jpg", "img_caption": ["Figure 3: Zero-Shot Classification Performance across Embeddings, Batch Sizes, and Objectives. The three vertical panels describe different choices of the text encoder $f_{\\theta_{T}}$ which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Within each vertical panel, examples include batch sizes $m=128$ and $m=512$ . Rows indicate various evaluation datasets from CIFAR-10, CIFAR-100, and STL-10. The $y$ -axis of each plot indicates average per-class recall, whereas the $x$ -axis indicates training iterations at the given batch size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Model, Datasets, and Evaluation. Throughout, we consider training variants of CLIP models (see Sec. 2), which require a dataset of image-caption pairs. For the training set, we use the ImageNetCaptions dataset [Fang et al., 2013], which pairs images from ImageNet [Deng et al., 2009] that were taken from Flickr with their original captions. In the notation of Sec. 2, the model is specified by selecting an image encoder $f_{\\theta_{I}}$ and a text encoder $f_{\\theta_{T}}$ . In all cases, we use a fixed image/text encoder as a base vector representation and compose it with a trainable feed-forward neural network, i.e., $f_{\\theta}=f_{\\theta}^{\\mathrm{head}}\\circ f^{\\mathrm{base}}$ . We fix the base image encoder as CLIP ViT-B/32 architecture pre-trained on LAION-2B [Schuhmann et al., 2022], and vary the base text encoder across embedding models of varying quality: GPT-2 [Radford et al., 2019], BERT [Devlin et al., 2019], and CLIP-based encodings. When two CLIP encoders are used for the base image/text vector representation, they are taken from separate CLIP models (i.e. the base representations are not dependent). We evaluate models based on zero-shot classification performance using the standard CLIP inference procedure: for any image $x$ , a label $c\\in\\{1,\\ldots,\\bar{C}\\}$ is predicted by associating to each $c$ a natural language prompt $y_{c}$ , and predicting the scores $\\boldsymbol{s}(\\boldsymbol{x})=(s_{1}(\\boldsymbol{x}),\\dots,s_{C}(\\boldsymbol{x}))$ , with ", "page_idx": 8}, {"type": "equation", "text": "$$\ns_{c}(x)=\\frac{e^{\\left\\langle f_{\\theta_{I}}(x),f_{\\theta_{T}}(y_{c})\\right\\rangle/\\tau}}{\\sum_{c^{\\prime}=1}^{C}e^{\\left\\langle f_{\\theta_{I}}(x),f_{\\theta_{T}}(y_{c^{\\prime}})\\right\\rangle/\\tau}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for a temperature $\\tau>0$ . Multiple prompting strategies can be used depending on the evaluation dataset, for which we average embeddings before applying (16). We use the public CLIP Benchmark repository, using the datasets CIFAR-10, CIFAR-100, STL-10, with their default caption sets. ", "page_idx": 8}, {"type": "text", "text": "Data Balancing Effects. Fig. 3 shows the zero-shot classification performance (in terms of average per-class recall) of variants depending on whether the contrastive learning objective from Sec. 2 is used or not. One iteration of balancing already leads to improvement in terms of downstream performance. Multiple balancing iterations lead to further improvements. See Appx. E for more details on this experiment, and for analogous ones with linear probing and zero-shot retrieval. ", "page_idx": 8}, {"type": "text", "text": "Fig. 4 then shows how balancing can be used to adjust an entire pre-training set to given marginals based on metadata, as described in Sec. 2 in the metadata curation example. After balancing, the target marginal has less than 2 orders of difference. In terms of downstream performance, data balancing leads to some improvement in the smaller batch regime $m=512$ ) when curating the dataset. See Appx. E for more details on this experiment. ", "page_idx": 8}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/b5c310e4c5b233c936e0ab7452676e961014b019b1d4b4c646d2294a1a5c9d16.jpg", "img_caption": ["Figure 4: Balancing and Metadata Curation. Depiction of balancing and metadata curation (Example 3 in Sec. 2) on ImageNet-Captions dataset, in which $\\mathcal{X}$ represents image-caption pairs and $\\boldsymbol{\\wp}$ represents keywords. Left: Observed marginal $P_{n,Y}$ (orange) and $P_{Y}$ (blue), which are sorted by order of increasing probability. Right: Zero-shot evaluation of an embedding model trained using the standard CLIP loss original versus the balanced training set. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Related Work Self-supervised learning has witnessed a surge of recent interest as datasets and computing hardware allow for larger, more capable models (see Balestriero et al. [2023] and references therein). While we highlight in this paper the connections between data balancing and contrastive learning [Radford et al., 2021], we acknowledge that data balancing can also be related to \u201dselfdistillation\u201d approaches more broadly [Grill et al., 2020, Chen and He, 2021, Oquab et al., 2024]. ", "page_idx": 9}, {"type": "text", "text": "Historical motivations for data balancing include census or survey data, in which $P_{n}$ is a crosstabulation of (a limited number of) paired observations and the target marginals were estimated from large amounts of unpaired observations [Deming and Stephan, 1940, Ireland and Kullback, 1968]. This situation is not unlike the present day\u2014yet at a different scale\u2014in which the amount of unstructured single-modality data (such as images) still dwarfs the amount of high-quality multimodal data [Gadre et al., 2023]. Bickel et al. [1991] proved classical asymptotic results on balancing estimators. Linear operators similar to the ones we use in Sec. 3 also appear in their analysis. More recently, Albertus and Berthet [2019] studied such estimators from an asymptotic empirical process viewpoint. Our theoretical results significantly improve on those from Albertus and Berthet [2019] primarily in the dependence of the number of iterations $k$ on the sample size $n$ to achieve convergence guarantees (from logarithmic to polynomial). ", "page_idx": 9}, {"type": "text", "text": "Matrix scaling is a popular algorithm to solve entropy-regularized optimal transport (EOT). We refer to Peyre\u00b4 and Cuturi [2019] for a survey. See also Courty et al. [2017], Shen et al. [2018], Peng et al. [2019] for interesting methods based on EOT in machine learning. Entropy-regularized optimal transport was one of the original inspirations for SSL techniques such as SwaV (see Sec. 2). While EOT is itself a deterministic optimization problem, a related statistical problem is the large-sample limits of EOT solutions when the marginal measures are estimated from data [Mena and Niles-Weed, 2019, Genevay et al., 2019, Klatt et al., 2020]. We emphasize that, while this line of work shares the matrix scaling algorithm with our setting, the statistical problem is entirely distinct; in statistical EOT, the target marginal distributions are computed from observations of independent, unpaired data, and the initial measure can be computed from the cost function. In our setting, the data are dependent, forming the random initial measure $P_{n}$ , whereas $P_{X}$ and $P_{Y}$ are fixed auxiliary information. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We showed how several disparate techniques used towards the training of foundation models are instances of a data balancing algorithm, which has the unsuspected benefit of reducing the variance of learning objectives involving multiple sources of data. We proved a new non-asymptotic bound on the mean-squared error of balanced estimators as they adjust to the given marginals. We also highlight the key roles of conditional expectation operators in quantifying that variance reduction effect. Finally, we translated the marginal balancing interpretation of several training practices for foundation models into algorithmic variants that warrant further investigation. Exploring variants incorporating prior information on the data sources is also an interesting venue for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements The authors are grateful to G. Ilharco, M. Wortsman, K. Pillutla, L. Schmidt, and J. Wellner for fruitful discussions related to this work. This work was supported by NSF DMS-2023166, CCF-2019844, DMS-2134012, PIMS 20240827-PRN01, NIH, and IARPA 2022- 22072200003. Part of this work was done while L. Liu was with the University of Washington, and while R. Mehta and Z. Harchaoui were visiting the Simons Institute for the Theory of Computing. ", "page_idx": 10}, {"type": "text", "text": "Broader Impact While this paper is of a theoretical nature, the web-scale pre-training sets used to train foundation models can affect not only the biases of the models themselves but also the behavior of individuals who interact with them. In the case of representation learning, unrefined Internet data may lead to non-uniform performance among protected attributes such as gender, age, etc. For generative models, individuals of all ages may be influenced by harmful images or textual output. Studying the relationship between the balancing procedures considered in this paper and more holistic model evaluations presents a valuable direction for follow-up work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. Albertus and P. Berthet. Auxiliary information: The raking-ratio empirical process. Electronic Journal of Statistics, 13(1), 2019.   \nY. Asano, C. Rupprecht, and A. Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020.   \nR. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A cookbook of self-supervised learning. arXiv preprint, 2023.   \nP. J. Bickel, Y. Ritov, and J. A. Wellner. Efficient estimation of linear functionals of a probability measure $P$ with known marginal distributions. The Annals of Statistics, 1991.   \nP. J. Bickel, C. A. Klaassen, Y. Ritov, and J. A. Wellner. Efficient and Adaptive Estimation for Semiparametric Models. Springer, 1 edition, 1998.   \nM. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.   \nM. Caron, H. Touvron, I. Misra, H. J\u00b4egou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.   \nX. Chen and K. He. Exploring simple Siamese representation learning. In CVPR, 2021.   \nN. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In NeurIPS, 2017.   \nT. M. Cover. Elements of Information Theory. John Wiley & Sons, 1999.   \nW. E. Deming and F. F. Stephan. On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. Annals of Mathematical Statistics, 11, 1940.   \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.   \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019.   \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007.   \nA. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language-image pre-training (CLIP). In ICML, 2013.   \nS. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. M. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt. DataComp: In search of the next generation of multimodal datasets. In NeurIPS, 2023.   \nA. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyre\u00b4. Sample complexity of Sinkhorn divergences. In AISTATS, 2019.   \nI. Gohberg, S. Goldberg, and M. Kaashoek. Classes of Linear Operators Vol. 1. Springer, 1990.   \nJ.-B. Grill, F. Strub, F. Altche\u00b4, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, B. Piot, k. kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.   \nM. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 2013.   \nR. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 2013.   \nC. T. Ireland and S. Kullback. Contingency tables with given marginals. Biometrika, 1968.   \nR. J. Johnston and C. J. Pattie. Entropy-maximizing and the iterative proportional fitting procedure. The Professional Geographer, 45, 1993.   \nC. Jones, V. Roulet, and Z. Harchaoui. Discriminative clustering with representation learning with any ratio of labeled to unlabeled data. Statistics and Computing, 2022.   \nD. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.   \nM. Klatt, C. Tameling, and A. Munk. Empirical regularized optimal transport: Statistical theory and applications. SIAM Journal on Mathematics of Data Science, 2020.   \nT.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dolla\u00b4r. Microsoft COCO: Common Objects in Context, 2015.   \nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-Grained Visual Classification of Aircraft. Technical report, University of Oxford, 2013.   \nG. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: Sample complexity and the central limit theorem. In NeurIPS, 2019.   \nM. Nutz. Introduction to Entropic Optimal Transport. Lecture notes, Columbia University, 2021.   \nM. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024.   \nX. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019.   \nG. Peyr\u00b4e and M. Cuturi. Computational Optimal Transport: With Applications to Data Science. Foundations and Trends in Machine Learning, 11, 2019.   \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019.   \nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \nC. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022.   \nJ. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. In AAAI, 2018.   \nR. Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. American Mathematical Monthly, 74(4), 1967.   \nM. E. Thompson. Theory of Sample Surveys. Chapman & Hall, 2000.   \nH. Xu, S. Xie, X. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying CLIP data. In ICLR, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Notation 15 ", "page_idx": 13}, {"type": "text", "text": "B Linear Operators and Variance Reduction 15 ", "page_idx": 13}, {"type": "text", "text": "B.1 Singular Value Decomposition 15   \nB.2 Proof of Main Results 16 ", "page_idx": 13}, {"type": "text", "text": "C From Information Projections to Data Balancing 19 ", "page_idx": 13}, {"type": "text", "text": "C.1 Balancing as Information Projections 19   \nC.2 Proof of Main Results 22 ", "page_idx": 13}, {"type": "text", "text": "D Statistical Analysis of Balancing Estimators 24 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Recursion of Estimation Error . 25   \nD.2 Technical Tools & Intermediate Results . 26   \nD.3 Analysis of Higher-Order Term . 27   \nD.4 Proof of Main Results . 29   \nD.5 Misspecified Marginal Distributions . 35 ", "page_idx": 13}, {"type": "text", "text": "E Experimental Details 44 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Datasets 44   \nE.2 Model Specification and Hyperparameters 44   \nE.3 Compute Environment . . 45   \nE.4 CLIP and Multi-CLIP 45   \nE.5 Metadata Curation 45   \nE.6 Additional Experiments 45 ", "page_idx": 13}, {"type": "text", "text": "F NeurIPS Paper Checklist 50 ", "page_idx": 13}, {"type": "table", "img_path": "vJMMdFfL0A/tmp/dbdc129976436f110a50d54d3a01be6eed20fc58e16840ee0665736682fc2ade.jpg", "table_caption": ["Table 1: Notation used throughout the paper. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Linear Operators and Variance Reduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section is dedicated to establishing the variance reduction result in Cor. 2 by employing properties of the Markov operators introduced in Sec. 3. In the first part, we establish Prop. 3, the singular value decomposition that defines the quantities appearing in Cor. 2. In the second part, we quantify the difference between $\\sigma_{0}^{2}$ and $\\sigma_{k}^{2}$ for even and odd iterations of $k$ . ", "page_idx": 14}, {"type": "text", "text": "B.1 Singular Value Decomposition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall the conditional mean operators $\\mu_{X}$ and $\\mu_{Y}$ from Sec. 3, ", "page_idx": 14}, {"type": "equation", "text": "$$\n[\\mu_{X}h](x):=\\mathbb{E}\\left[h(X,Y)|X\\right](x){\\mathrm{~and~}}[\\mu_{Y}h](y):=\\mathbb{E}\\left[h(X,Y)|Y\\right](y).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with the corresponding debiasing (a.k.a. centering) operators defined by $\\mathcal{C}_{X}=I-\\mu_{X}$ and $\\mathcal{C}_{Y}=$ I \u2212\u00b5Y . ", "page_idx": 14}, {"type": "text", "text": "Proposition 3. There exists a basis $\\{\\alpha_{j}\\}_{j=1}^{m}$ of $\\mathbf{L}^{2}(P_{X})$ , a basis $\\{\\beta_{j}\\}_{j=1}^{m}\\,o f{\\bf L}^{2}(P_{Y}),$ , and real values $\\left\\{s_{j}\\right\\}_{j=1}^{m}$ , which satisfy: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{Y}\\alpha_{j}=s_{j}\\beta_{j}\\,a n d\\,\\mu_{X}\\beta_{j}=s_{j}\\alpha_{j}\\,f o r\\,j\\in\\left\\{1,\\dots,m\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\alpha_{1}=\\mathbf{1}_{\\mathcal{X}},\\,\\beta_{1}=\\mathbf{1}_{\\mathcal{Y}},\\,s_{1}=1$ and $s_{j}$ is non-negative and non-increasing in $j$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. When $\\mu_{X}$ is restricted to $\\mathbf{L}^{2}(P_{Y})$ and $\\mu_{Y}$ is restricted to $\\mathbf{L}^{2}(P_{X})$ , these operators are in fact adjoint in $\\mathbf{L}^{2}(\\dot{P})$ , as by the tower property we have the relation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle f,\\mu_{X}g\\rangle_{{\\bf L}^{2}(P_{X})}=\\mathbb{E}\\left[f(X)\\mathbb{E}\\left[g(Y)\\vert X\\right]\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[f(X)\\vert Y\\right]g(Y)\\right]=\\langle\\mu_{Y}f,g\\rangle_{{\\bf L}^{2}(P_{Y})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\mu_{Y}:{\\bf L}^{2}(P_{X})\\rightarrow{\\bf L}^{2}(P_{Y})$ is a compact linear operator, by Gohberg et al. [1990, Section IV.1 Theorem 1.1] and Gohberg et al. [1990, Section IV.1 Corollary 1.2], we have that $\\mu_{Y}$ admits a singular value decomposition satisfying (17). Next, we show that $s_{1}\\leq1$ and that ${\\bf1}_{\\mathcal{X}}$ is an eigenvector of $\\mu_{X}\\mu_{Y}:{\\bf L}^{2}\\dot{(}P_{X})\\,\\to\\,{\\bf L}^{2}(\\dot{P}_{X})\\,$ with eigenvalue 1, which confirms that $s_{1}\\,=\\,1$ and $\\alpha_{1}\\,=\\,{\\bf1}_{\\mathcal{X}}$ by the definition of singular values (arguing symmetrically achieves $\\beta_{1}\\,=\\,{\\bf1}_{\\mathcal{Y}})$ ). By the variational representation of singular values [Gohberg et al., 1990, Section IV.1 Equation (2)], we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f:\\|f\\|_{\\mathbf{L}^{2}(P_{X})}=1}\\left\\|\\mu_{Y}f\\right\\|_{\\mathbf{L}^{2}(P_{Y})}=s_{1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider any $f\\mathrm{~\\boldmath~\\in~}\\,{\\bf L}^{2}(P_{X})$ such that $\\|f\\|_{\\mathbf{L}^{2}(P_{X})}~=~1$ . Define the conditional probability $P_{X|Y}(x|y)=P(x,y)/P_{Y}(y)$ which is well-defined by assumption. Then, by the Cauchy-Schwarz inequality in ${\\bf L}^{2}(P_{X|Y})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mu_{Y}f\\|_{\\mathbf{L}^{2}(P_{Y})}^{2}=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\left(\\sum_{x\\in\\mathcal{X}}f(x)P_{X|Y}(x|y)\\right)^{2}P_{Y}(y)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\sum_{x\\in\\mathcal{X}}f^{2}(x)P_{X|Y}(x|y)P_{Y}(y)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}f^{2}(x)\\sum_{y\\in\\mathcal{Y}}P(x,y)}\\\\ &{\\qquad=\\|f\\|_{\\mathbf{L}^{2}(P_{X})}^{2}=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This proves that $s_{1}~\\leq~1$ . For equality, notice that $\\mu_{X}\\mu_{Y}\\,\\mathbf{1}_{X}\\,=\\,\\mu_{X}\\,\\mathbf{1}_{\\mathcal{Y}}\\,=\\,\\mathbf{1}_{X}$ , completing the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Main Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "From Prop. 3, we establish two bases $\\{\\alpha_{j}\\}_{j=1}^{m}$ and $\\{\\beta_{j}\\}_{j=1}^{m}$ of $\\mathbf{L}^{2}(P_{X})$ and $\\mathbf{L}^{2}(P_{Y})$ , respectively. These bases span the range of the operators $\\mu_{X}$ and $\\mu_{Y}$ . We will consider the repeated application of the operator ${\\mathcal{C}}_{Y}{\\mathcal{C}}_{X}$ , a sequence of two centering operations on some function $h\\in\\mathbf{L}^{2}(P)$ , and compare ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[((C_{Y}\\mathcal{C}_{X})^{t}\\bar{h})^{2}\\right]\\mathrm{~against~}\\mathbb{E}\\left[\\bar{h}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\bar{h}=h-\\mathbb{E}_{P}\\left[h\\right]$ . We establish the main result by measuring the reduction in variance from a single application, in terms of the coordinates of the function of interest on each of the two subspaces. We will then observe how these coordinates change iteration-to-iteration to give the final result. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. For any $h\\in\\mathbf{L}^{2}(P)$ such that $\\mathbb{E}_{P}\\left[h\\right]=0,$ , let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{X}h=\\sum_{j=1}^{m}u_{j}\\alpha_{j}\\;a n d\\,\\mu_{Y}h=\\sum_{j=1}^{m}v_{j}\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(\\mathcal C_{Y}\\mathcal C_{X}h)^{2}\\right]=\\mathbb{E}\\left[h^{2}\\right]-\\sum_{j=2}^{m}u_{j}^{2}-\\sum_{j=2}^{m}(v_{j}-s_{j}u_{j})^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By orthogonality, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathscr{C}_{Y}\\mathscr{C}_{X}h)^{2}\\right]=\\mathbb{E}\\left[((I-\\mu_{Y})\\mathscr{C}_{X}h)^{2}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[(\\mathscr{C}_{X}h)^{2}\\right]-2\\mathbb{E}\\left[(\\mathscr{C}_{X}h)(\\mu_{Y}\\mathscr{C}_{X}h)\\right]+\\mathbb{E}\\left[(\\mu_{Y}\\mathscr{C}_{X}h)^{2}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[(\\mathscr{C}_{X}h)^{2}\\right]-2P_{Y}((\\mu_{Y}\\mathscr{C}_{X}h)^{2})+P_{Y}((\\mu_{Y}\\mathscr{C}_{X}h)^{2})}\\\\ &{\\qquad\\quad=\\mathbb{E}\\left[(\\mathscr{C}_{X}h)^{2}\\right]-P_{Y}((\\mu_{Y}\\mathscr{C}_{X}h)^{2})}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[h^{2}\\right]-P_{X}((\\mu_{X}h)^{2})-P_{Y}((\\mu_{Y}\\mathscr{C}_{X}h)^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $P(h)=0$ , it holds by the tower property of conditional expectation that $P_{X}\\!\\left(\\mu_{X}h\\right)=0$ , which implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\nu_{1}=\\langle\\mu_{X}h,\\alpha_{1}\\rangle_{\\mathbf{L}^{2}(P_{X})}=0\\implies P_{X}((\\mu_{X}h)^{2})=\\sum_{j=2}^{m}u_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the second term, observe that $P_{X}({\\mathcal{C}}_{X}h)\\ =\\ 0$ , so it holds by the tower property that $P_{Y}(\\mu_{Y}\\mathcal{C}_{X}h)=0$ , so ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{Y}((\\mu_{Y}\\mathcal{C}_{X}h)^{2})=\\sum_{j=2}^{m}\\left(\\langle\\mu_{Y}\\mathcal{C}_{X}h,\\beta_{j}\\rangle_{\\mathbf{L}^{2}(P_{Y})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we compute the term in the square by applying Prop. 3: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\langle\\mu_{Y}\\mathcal{C}_{X}h,\\beta_{j}\\rangle_{{\\bf L}^{2}(P_{Y})}=\\langle\\mu_{Y}h,\\beta_{j}\\rangle_{{\\bf L}^{2}(P_{Y})}-\\langle\\mu_{Y}\\mu_{X}h,\\beta_{j}\\rangle_{{\\bf L}^{2}(P_{Y})}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=v_{j}-\\left\\langle\\mu_{Y}\\displaystyle\\sum_{k=1}^{m}u_{k}\\alpha_{k},\\beta_{j}\\right\\rangle_{{\\bf L}^{2}(P_{Y})}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=v_{j}-\\left\\langle\\displaystyle\\sum_{k=1}^{m}u_{k}s_{k}\\beta_{k},\\beta_{j}\\right\\rangle_{{\\bf L}^{2}(P_{Y})}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\mathrm{L}^{2}(P_{Y})}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=v_{j}-s_{j}u_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "Lem. 4 ensures that we have a reduction on each iteration, with a formula that depends on the coordinates of the function on each subspace. Because these coordinates change every iteration, we track them in the next lemma. Define $\\dot{h}_{0}=\\bar{h}$ and $h_{t+1}=(\\mathcal{C}_{Y}\\mathcal{C}_{X})h_{t}$ , along with the constants $\\{u_{t,j}\\}_{j=1}^{m}$ and $\\{v_{t,j}\\}_{j=1}^{m}$ given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{X}h_{t}=\\sum_{j=1}^{m}u_{t,j}\\alpha_{j}{\\mathrm{~and~}}\\mu_{Y}h_{t}=\\sum_{j=1}^{m}v_{t,j}\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have the following. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5. For all $t\\geq0$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{t+1,j}=s_{j}^{2}u_{t,j}-s_{j}v_{t,j},}\\\\ &{v_{t+1,j}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Fix any $j\\in[m]$ , and use Prop. 3 to write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle u_{t+1,j}=\\langle\\mu_{X}\\mathcal{C}_{Y}\\mathcal{C}_{X}h_{t},\\alpha_{j}\\rangle_{{\\mathbf L}^{2}(P_{X})}}\\ ~}\\\\ {{\\displaystyle~~~~~~~=\\langle\\mu_{X}(I-\\mu_{X}-\\mu_{Y}+\\mu_{Y}\\mu_{X})h_{t},\\alpha_{j}\\rangle_{{\\mathbf L}^{2}(P_{X})}}\\ ~}\\\\ {{\\displaystyle~~~~~=\\langle\\mu_{X}\\mu_{Y}\\mu_{X}h_{t},\\alpha_{j}\\rangle_{{\\mathbf L}^{2}(P_{X})}-\\langle\\mu_{X}\\mu_{Y}h_{t},\\alpha_{j}\\rangle_{{\\mathbf L}^{2}(P_{X})}}\\ ~}\\\\ {{\\displaystyle~~~~~=\\left\\langle\\mu_{X}\\mu_{Y}\\sum_{k=1}^{m}u_{t,k}\\alpha_{k},\\alpha_{j}\\right\\rangle_{{\\mathbf L}^{2}(P_{X})}-\\left\\langle\\mu_{X}\\sum_{k=1}^{m}v_{t,k}\\beta_{k},\\alpha_{j}\\right\\rangle_{{\\mathbf L}^{2}(P_{X})}}\\ ~}\\\\ {{\\displaystyle~~~~~=s_{j}^{2}u_{t,j}-s_{j}v_{t,j},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which proves the first part of the claim. For the second part, note that $\\mu_{Y}{\\mathcal{C}}_{Y}\\;\\;=\\;\\;0$ , so $\\langle\\mu_{Y}\\mathcal{C}_{Y}\\mathcal{C}_{X}h_{t},\\alpha_{j}\\rangle_{\\mathbf{L}^{2}(P_{Y})}=0$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Using Lem. 4 and Lem. 5, we can simply accumulate the reduction incurred on every iteration. ", "page_idx": 16}, {"type": "text", "text": "Proposition 6. Define the constants $(u_{j})_{j=1}^{m}$ and $(v_{j})_{j=1}^{m}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{X}{\\bar{h}}=\\sum_{j=1}^{m}u_{j}\\alpha_{j}\\;a n d\\,\\mu_{Y}{\\bar{h}}=\\sum_{j=1}^{m}v_{j}\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we may quantify the variance reduction achieved by $t+1$ iterations of the ${\\mathcal{C}}_{Y}{\\mathcal{C}}_{X}$ operator as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbb{E}\\left[\\bar{h}^{2}\\right]-\\mathbb{E}\\left[((\\mathcal{C}_{Y}\\mathcal{C}_{X})^{t+1}\\bar{h})^{2}\\right]=\\displaystyle\\sum_{j=2}^{m}\\left\\{u_{j}^{2}+(v_{j}-s_{j}u_{j})^{2}\\left[1+\\frac{s_{j}^{2}(1-s_{j}^{4t})}{1-s_{j}^{2}}\\right]\\right\\}}&{}\\\\ {\\displaystyle\\qquad\\qquad\\to\\sum_{j=2}^{m}\\left[u_{j}^{2}+\\frac{(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}\\right]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as $t\\to\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Apply Lem. 4 $(t+1)$ -times so that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[((\\mathcal{C}_{Y}\\mathcal{C}_{X})^{t+1}\\bar{h})^{2}\\right]=\\mathbb{E}\\left[\\bar{h}^{2}\\right]-\\displaystyle\\sum_{j=2}^{m}\\sum_{\\tau=0}^{t}\\left[(1+s_{j}^{2})u_{\\tau,j}^{2}+v_{\\tau,j}^{2}-2s_{j}u_{\\tau,j}v_{\\tau,j}\\right]}\\\\ {=\\mathbb{E}\\left[\\bar{h}^{2}\\right]-\\displaystyle\\sum_{j=2}^{m}\\left[v_{0,j}^{2}-2s_{j}u_{0,j}v_{0,j}+\\displaystyle\\sum_{\\tau=0}^{t}(1+s_{j}^{2})u_{\\tau,j}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as by Lem. 5, we have that $v_{\\tau,j}=0$ for $\\tau>0$ . Next, we unroll the definition of $u_{\\tau,j}$ so that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{\\tau,j}=s_{j}^{2}u_{\\tau-1,j}-s_{j}v_{\\tau-1,j}}\\\\ &{\\qquad=s_{j}^{2}(s_{j}^{2}u_{\\tau-2,j}-s_{j}v_{\\tau-2,j})-s_{j}v_{\\tau-1,j}}\\\\ &{\\qquad=s_{j}^{2\\tau-2}(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $\\tau>0$ , yielding ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\hat{h}^{2}\\right]-\\mathbb{E}\\left[((\\mathcal{C}_{Y}\\mathcal{C}_{X})^{t+1}\\hat{h})^{2}\\right]}\\\\ {\\displaystyle=\\sum_{j=2}^{m}\\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+(1+s_{j}^{2})(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})^{2}\\underset{\\tau=1}{\\overset{t}{\\sum}}(s_{j}^{4})^{\\tau-1}\\right]}\\\\ {\\displaystyle=\\sum_{j=2}^{m}\\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+(1+s_{j}^{2})(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})^{2}\\underset{\\tau=0}{\\overset{t}{\\sum}}(s_{j}^{4})^{\\tau}\\right]}\\\\ {\\displaystyle=\\sum_{j=2}^{m}\\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+\\frac{s_{j}^{2}\\left(1+s_{j}^{2}\\right)(v_{0,j}-s_{j}u_{0,j})^{2}\\left(1-s_{j}^{4}\\right)}{1-s_{j}^{4}}\\right]}\\\\ {\\displaystyle=\\sum_{j=2}^{m}\\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+\\frac{s_{j}^{2}\\left(v_{0,j}-s_{j}u_{0,j}\\right)^{2}\\left(1-s_{j}^{4}\\right)}{1-s_{j}^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Substitute $u_{0,j}=u_{j}$ and $v_{0,j}=v_{j}$ to complete the proof. ", "page_idx": 17}, {"type": "text", "text": "We also present the corresponding result for $k$ odd. The proof follows similarly by repeated application of the operator ${\\mathcal{C}}_{Y}{\\mathcal{C}}_{X}$ . However, the iterations will be compared to $\\sigma_{1}^{2}\\stackrel{\\bullet}{=}\\overline{{\\mathbb{E}_{P}}}^{\\bullet}[(\\mathcal{C}_{X}\\bar{h})^{\\underline{{\\bullet}}}]$ , as we consider $\\mathcal{C}_{X}\\bar{h}$ as the \u201cfirst\u201d iteration to this process. ", "page_idx": 17}, {"type": "text", "text": "Proposition 7. Define the constants $(u_{j})_{j=1}^{m}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{Y}{\\mathcal{C}}_{X}{\\bar{h}}=\\sum_{j=1}^{m}u_{j}\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we may quantify the variance reduction achieved by $t+1$ iterations of the $\\mathcal{C}_{X}\\mathcal{C}_{Y}$ operator as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathcal{C}_{X}\\bar{h})^{2}\\right]-\\mathbb{E}\\left[((\\mathcal{C}_{X}\\mathcal{C}_{Y})^{t+1}\\mathcal{C}_{X}\\bar{h})^{2}\\right]=\\displaystyle\\sum_{j=2}^{m}\\left\\{u_{j}^{2}+(s_{j}u_{j})^{2}\\left[1+\\frac{s_{j}^{2}(1-s_{j}^{4t})}{1-s_{j}^{2}}\\right]\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\to\\displaystyle\\sum_{j=2}^{m}\\left(\\frac{1+s_{j}^{2}}{1-s_{j}^{2}}\\right)u_{j}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as $t\\to\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "In order to have full monotonicity, we also need that $\\sigma_{0}^{2}\\geq\\sigma_{1}^{2}$ . This follows by orthogonality, as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma_{0}^{2}=\\mathbb{E}\\left[\\bar{h}^{2}\\right]=\\mathbb{E}\\left[(\\mathcal{C}_{X}\\bar{h})^{2}\\right]+\\mathbb{E}\\left[(\\mu_{X}\\bar{h})^{2}\\right]=\\sigma_{1}^{2}+\\mathbb{E}\\left[(\\mu_{X}\\bar{h})^{2}\\right]\\geq\\sigma_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we can combine Prop. 7 and (18) to fully quantify the relationship between $\\sigma_{0}^{2}$ and $\\sigma_{k}^{2}$ for $k$ odd. ", "page_idx": 18}, {"type": "text", "text": "C From Information Projections to Data Balancing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section is dedicated to deriving three representations of the balancing procedure as projections in various statistical divergences, as shown in Fig. 2. ", "page_idx": 18}, {"type": "text", "text": "We consider two sets of probability measures denoted by $\\Pi_{X}\\ =\\ \\{Q:Q_{X}=P_{X}\\}$ and $\\Pi_{Y}\\ =$ $\\{Q:Q_{Y}=P_{Y}\\}$ . The marginal matching steps are written as projections in terms of a statistical divergence $D$ (precisely, an $f$ -divergence) in the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{P_{X}}{P_{n,X}^{(k-1)}}\\otimes P_{n}^{(k-1)}=\\underset{Q\\in\\Pi_{X}}{\\operatorname{arg\\,min}}\\,D(Q\\|P_{n}^{(k-1)}),\\quad\\frac{P_{Y}}{P_{n,Y}^{(k-1)}}\\otimes R=\\underset{Q\\in\\Pi_{Y}}{\\operatorname{arg\\,min}}\\,D(Q\\|P_{n}^{(k-1)}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We provide the derivations for three common choices of $D$ : Kullback-Leibler (KL), reverse $\\mathrm{KL}$ , and $\\chi^{2}$ . Using this viewpoint, and simply assuming the positivity of the marginal measures $P_{X}$ and $P_{Y}$ , we derive an upper bound in Prop. 14 that is constant in $k$ . This is an improvement over the recent work of Albertus and Berthet [2019], in which they show an upper bound that scales exponentially in $k$ . ", "page_idx": 18}, {"type": "text", "text": "The KL representation will be used in the proof of Prop. 14, which (recalling the sequence $(P_{n}^{(k)})_{k\\geq1}$ from (8)), controls the error between Pn(k,)Y and PY for k odd and Pn(k,)X and PX for k even. ", "page_idx": 18}, {"type": "text", "text": "C.1 Balancing as Information Projections ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1.1 Projection in KL-Divergence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 8. Assume that $P_{X}\\ll R_{X}$ and $P_{Y}\\ll R_{Y}$ , and define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{\\star}:=\\underset{Q\\in\\Pi_{X}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(Q||R),\\quad P^{\\star}:=\\underset{Q\\in\\Pi_{Y}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(Q||R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ^{\\star}(x,y)={\\binom{P_{X}(x)R_{Y|X}(y|x)}{0}}\\quad i f R_{X}(x)>0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\nP^{\\star}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{P_{Y}(y)R_{X}(x|y)}&{\\mathrm{~if~}R_{Y}(y)>0}\\\\ {0}&{\\mathrm{~if~}R_{Y}(y)=0}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. In the case that $Q(x,y)=0$ , we apply the convention that $0\\log0=0$ . Consider the case $Q^{\\star}$ , the projection of $R$ onto $\\Pi_{X}$ . Write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(Q||R)=\\displaystyle\\sum_{\\substack{x\\in\\mathcal{X}}}Q(x,y)\\log\\frac{Q_{Y,X}(y)\\circ\\mathcal{F}(x^{\\perp})Q(x^{\\perp})}{R_{Y,X}^{\\perp}(y|\\mathcal{X}^{\\perp})R(x^{\\perp})}}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}Q_{X}(x)\\left[\\sum_{\\substack{y\\in\\mathcal{Y}}}Q_{Y|X}(y|x)\\log\\frac{Q_{Y|X}(y)\\circ\\mathcal{F}(x^{\\perp})Q_{X}(x^{\\perp})}{R_{Y,X}^{\\perp}(y|\\mathcal{X}^{\\perp})R(x^{\\perp})}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}Q_{X}(x)\\left[\\sum_{\\substack{y\\in\\mathcal{Y}}}Q_{Y|X}(y|x)\\log\\frac{Q_{Y|X}(y|x)}{R_{Y,X}^{\\perp}(y|\\mathcal{X}^{\\perp})}+\\sum_{\\substack{y\\in\\mathcal{Y}}}Q_{Y|X}(y|x)\\log\\frac{Q_{X}(x)}{R_{X}^{\\perp}(x)}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}Q_{X}(x)\\left[\\sum_{\\substack{y\\in\\mathcal{Y}}}Q_{Y|X}(y|x)\\log\\frac{Q_{Y|X}(y|x)}{R_{Y,X}^{\\perp}(y|\\mathcal{X}^{\\perp})}\\right]+\\displaystyle\\sum_{x\\in\\mathcal{X}}Q_{X}(x)\\log\\frac{Q_{X}(x)}{R_{X}^{\\perp}(x)}}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}Q_{X}(x)\\mathrm{KL}(Q_{Y|X}(\\cdot|x)|R_{Y|X}(\\cdot|x))+\\mathrm{KL}(Q_{X}||R_{X})}\\\\ &{\\qquad=\\displaystyle\\sum_{x\\in\\mathcal{X}}P_{X}(x)\\mathrm{KL}(Q_{Y|X}(\\cdot|x)|R_{Y|X}(\\cdot|x))+\\mathrm{KL}(P_{X}||R_{X}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last line is due to the marginal constraint $Q\\in\\Pi_{X}$ . For the above to be well defined, we need that $P_{X}\\ll R_{X}$ so that $\\mathrm{KL}(P_{X}\\|R_{X})<+\\infty$ . The above is minimized when $Q_{Y|X}(y|x)=$ $R_{Y|X}(y|x)$ for all $(x,y)\\,\\in\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}$ such that $Q_{X}(x)\\;=\\;P_{X}(x)\\;>\\;0$ . The case of $P^{\\star}$ follows analogously when using that $P_{Y}\\ll R_{Y}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.1.2 Projection in Reverse KL-Divergence ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 9. Assume that $P_{Y}\\ll R_{X}$ and $P_{Y}\\ll R_{Y}$ , and define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{\\star}:=\\underset{Q\\in\\Pi_{X}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(R\\|Q),\\quad P^{\\star}:=\\underset{Q\\in\\Pi_{Y}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(R\\|Q).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ^{\\star}(x,y)={\\binom{P_{X}(x)R_{Y|X}(y|x)}{0}}\\quad i f R_{X}(x)>0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\nP^{\\star}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{P_{Y}(y)R_{X}(x|y)}&{\\mathrm{~if~}R_{Y}(y)>0}\\\\ {0}&{\\mathrm{~if~}R_{Y}(y)=0}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. In the case that $R(x,y)=0$ , we apply the convention that $0\\log0=0$ . Note that minimizing $\\mathrm{KL}(R\\|Q)$ over $Q$ is equivalent to minimizing $\\begin{array}{r}{-\\sum_{x,y}R(x,y)\\log\\breve{Q(x,y)}}\\end{array}$ (i.e. the cross entropy). Consider the case $Q^{\\star}$ , the projection of $R$ onto $\\Pi_{X}$ . Because $R\\ll Q$ for $\\mathrm{KL}(R\\|Q)<+\\infty$ to hold, we have that $R(x)>0\\implies Q(x)>0$ , so that $Q_{Y|X}(y|x)$ is well-defined. Write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-\\sum_{x,y}R(x,y)\\log Q(x,y)}}\\\\ {{\\displaystyle=-\\sum_{x\\in\\mathcal{X}}R_{X}(x)\\log Q_{X}(x)-\\sum_{x\\in\\mathcal{X}}R(x)\\sum_{y\\in\\mathcal{Y}}R_{Y|X}(y|x)\\log Q_{Y|X}(y|x)}}\\\\ {{\\displaystyle=-\\sum_{x\\in\\mathcal{X}}R_{X}(x)\\log P_{X}(x)+\\sum_{x\\in\\mathcal{X}}R_{X}(x)\\left[-\\sum_{y\\in\\mathcal{Y}}R_{Y|X}(y|x)\\log Q_{Y|X}(y|x)\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second first term does not depend on $Q$ due to the marginal constraint $Q\\in\\Pi_{X}$ . The second term is the expectation of the cross entropy from $R_{Y\\mid X}$ to $Q_{Y\\mid X}$ over $R_{X}$ , which is minimized if $R_{Y\\mid X}=Q_{Y\\mid X}$ . We have specified $Q_{Y\\mid X}$ and $Q_{X}$ , completing the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.1.3 Projection in $\\chi^{2}$ -Divergence ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let 1 denote the function that is identically equal to 1. Consider the following optimization problem, which is the subject of the subsequent lemmas: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\xi\\in A_{X}}\\left\\|\\mathbf{1}-\\xi\\right\\|_{\\mathbf{L}^{2}(R)}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{X}:=\\left\\{f:{\\mathcal{X}}\\times{\\mathcal{Y}}\\to\\mathbb{R}{\\mathrm{~satisfying~}}\\sum_{y\\in{\\mathcal{Y}}}f(x,y)R(x,y)=P_{X}(x){\\mathrm{~for~any~}}x\\in{\\mathcal{X}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 10. Assume that $P_{X}\\ll R_{X}$ , and define The problem (25) is feasible, and its solution can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\xi^{\\star}=\\mathcal{C}_{X}^{R}({\\bf1}-f)+f\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $f\\in\\mathbf{L}^{2}(R)$ , where the linear operator $\\mathcal{C}_{X}^{R}$ is specified by ", "page_idx": 20}, {"type": "equation", "text": "$$\n[\\mathcal{C}_{X}^{R}g](x,y)=g(x,y)-\\sum_{y^{\\prime}\\in\\mathcal{Y}}g(x,y^{\\prime})R_{Y|X}(y^{\\prime}|x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. First, we establish feasibility by letting ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x,y):=\\left\\{{P_{X}(x)/R_{X}(x)\\atop1}\\right.\\quad{\\mathrm{if~}}R_{X}(x)>0\\atop{\\mathrm{otherwise}}{\\mathrm{~\\Delta~}}^{}\\quad.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This function does not depend on the second input $y$ . Because we assumed that $P_{X}\\ll R_{X}$ , we have that the terms of $f(x,y)$ for which $R_{X}(x)=0$ do not affect whether $\\begin{array}{r}{\\sum_{y\\in\\mathcal{Y}}f(x,y)R(x,y)=P_{X}(x)}\\end{array}$ , because $P_{X}\\!\\left(x\\right)=0$ in these cases. In the remainder of this proof, we will show that (25) is an affine projection problem, and find its solution by converting it to a subspace projection problem. Indeed, consider $f_{1},\\dots,f_{r}\\in\\mathcal{A}_{X}$ , and $\\alpha_{1},\\ldots,\\alpha_{r}\\in\\mathbb{R}$ such that $\\textstyle\\sum_{j=1}^{r}\\alpha_{j}^{\\dot{\\mathbf{\\alpha}}}=1$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{Y}}\\left[\\sum_{j=1}^{r}\\alpha_{j}f_{j}(x,y)\\right]\\cdot R(x,y)=\\sum_{j=1}^{r}\\alpha_{j}\\left[\\sum_{y\\in\\mathcal{Y}}f_{j}(x,y)R(x,y)\\right]=P_{X}(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "indicating that $\\textstyle\\sum_{j=1}^{r}\\alpha_{j}f_{j}(x,y)\\in{\\mathcal{A}}_{X}$ and $A_{X}$ is an affine subset of $\\mathbf{L}^{2}(R)$ . Define ", "page_idx": 20}, {"type": "equation", "text": "$$\nS_{X}:=\\left\\{g:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}{\\mathrm{~satisfying~}}\\sum_{y\\in\\mathcal{Y}}g(x,y)R(x,y)=0{\\mathrm{~for~any~}}x\\in\\mathcal{X}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, for any $f\\in A_{X}$ , we have that $g\\in S_{X}$ if and only if $g+f\\in A_{X}$ . Taking any $f\\in A_{X}$ , letting $\\phi^{\\star}$ be the solution of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi\\in S_{X}}\\big\\|\\mathbf{1}-f-\\phi\\big\\|_{\\mathbf{L}^{2}(R)}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we will have that $\\phi^{\\star}+f$ will be the solution of (25). The remainder of the proof is showing that $\\phi^{\\star}=\\mathcal{C}_{X}^{R}({\\bf1}-f)$ . ", "page_idx": 20}, {"type": "text", "text": "First, define the operator $\\mu_{X}^{R}$ by $\\begin{array}{r}{[\\mu_{X}g](x,y)=\\sum_{y^{\\prime}\\in\\mathcal{Y}}g(x,y^{\\prime})R_{Y|X}(y^{\\prime}|x)}\\end{array}$ , and note (by factoring out $R_{X}(x))$ that $g\\,\\in\\,S_{X}$ if and only if $\\mu_{X}^{R}g\\,=\\,0$ . In addition, $\\mu_{X}^{R}g$ is linear and idempotent as $\\mu_{X}^{R}\\mu_{X}^{R}g=\\mu_{X}^{R}g$ , so it is a projection operator in $\\mathbf{L}^{2}(R)$ . Thus, $\\mathcal{S}_{X}$ is the orthogonal complement of $\\mathrm{range}(\\mu_{X}^{R})$ , and the solution of (26) is given by $(I\\!-\\!\\mu_{X}^{R})({\\bf1}-f)=\\mathcal{C}_{X}^{R}({\\bf1}-f)$ , because $\\mathcal{C}_{X}^{R}=I\\!-\\!\\mu_{X}^{R}$ . The claim is proved. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma 11. Assume that $P_{X}\\ll R_{X}$ . Define ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ^{\\star}:=\\operatorname*{arg\\,min}_{Q\\in\\Pi_{X}}\\chi^{2}(Q||R).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and let $\\xi^{\\star}$ be the solution of problem (25). Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ^{\\star}(x,y)=\\xi^{\\star}(x,y)R(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{P_{X}(x)R_{Y|X}(y|x)}&{i f R_{X}(x)>0}\\\\ {0}&{i f R_{X}(x)=0}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. First, by reparametrizing the problem (27) as finding $\\xi$ such that $Q(x,y)=\\xi(x,y)R(x,y)$ , we can compute its solution by solving ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\xi\\in A_{X},\\xi\\geq0}\\left\\|\\mathbf{1}-\\xi\\right\\|_{\\mathbf{L}^{2}(R)}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that we also have a non-negativity constraint, as opposed to (25). If $\\xi^{\\star}$ solves (25) and happens to be non-negative, then we have that $\\xi^{\\star}$ solves (29) as well and the first equality of (28) is satisfied by definition. We show the second equality of (28) by direct computation, which also establishes the non-negativity of $\\xi^{\\star}$ simultaneously. ", "page_idx": 21}, {"type": "text", "text": "Apply Lem. 10 with ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x,y):=\\left\\{{P_{X}(x)/R_{X}(x)\\atop1}\\right.\\quad{\\mathrm{if~}}R_{X}(x)>0\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\xi^{\\star}(x,y)=\\mathcal{C}_{X}^{R}\\left({\\bf1}-f\\right)(x,y)+f(x,y)}}\\\\ {{\\displaystyle\\qquad=\\left[\\sum_{z\\in\\mathcal{Y}}f(x,z)R_{Y\\mid X}(z\\vert x)-f(x,y)\\right]+f(x,y)}}\\\\ {{\\displaystyle\\qquad=f(x,y^{\\prime})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $y^{\\prime}\\,\\in\\,\\mathcal{V}$ . Thus, the likelihood ratio of $Q^{\\star}$ with respect to $R$ is a marginal reweighting. Accordingly, ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ^{\\star}(x,y)=\\xi^{\\star}(x,y)R(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{P_{X}(x)R_{Y|X}(y|x)}&{\\mathrm{~if~}R_{X}(x)>0}\\\\ {0}&{\\mathrm{~if~}R_{X}(x)=0}\\end{array}\\!\\!\\right.,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "completing the proof. ", "page_idx": 21}, {"type": "text", "text": "Proposition 12. Assume that $P_{X}\\ll R_{X}$ and $P_{Y}\\ll R_{Y}$ . Define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{\\star}:=\\underset{Q\\in\\Pi_{X}}{\\arg\\operatorname*{min}}\\,\\chi^{2}(Q||R),\\quad P^{\\star}:=\\underset{Q\\in\\Pi_{Y}}{\\arg\\operatorname*{min}}\\,\\chi^{2}(Q||R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q^{\\star}(x,y)=\\left\\{\\begin{array}{l l}{P_{X}(x)R_{Y|X}(y|x)}&{i f R_{X}(x)>0}\\\\ {0}&{i f R_{X}(x)=0}\\end{array}\\right.}\\\\ {P^{\\star}(x,y)=\\left\\{\\begin{array}{l l}{P_{Y}(y)R_{X|Y}(x|y)}&{i f R_{Y}(y)>0}\\\\ {0}&{i f R_{Y}(y)=0}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The first equality of (31) follows by the claim of Lem. 11. The second equality follows by repeating the argument of Lem. 10 and Lem. 11 with $(X,x)$ and $(Y,y)$ swapped. ", "page_idx": 21}, {"type": "text", "text": "C.2 Proof of Main Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We may now control the errors of the ratio of marginals using the projection interpretation established in the previous sections. Recall the event $\\boldsymbol{S}$ as defined in Tab. 1. The following result, the monotonicity of the marginal violation terms in terms of KL, will be useful in the bound. ", "page_idx": 21}, {"type": "text", "text": "Proposition 13. [Nutz, 2021, Proposition 6.10] Under the event $\\boldsymbol{S}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{KL}(P_{n,X}^{(0)}||P_{X})\\geq\\mathrm{KL}(P_{Y}||P_{n,Y}^{(1)})\\geq\\mathrm{KL}(P_{n,X}^{(2)}||P_{X})\\geq..\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We give the following result for $\\mathcal{X}$ and the analogous claim holds on $\\boldsymbol{\\wp}$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 14. Assume that $P_{n,X}(x)>0$ for all $x\\in\\mathscr{X}$ . It holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\right|\\leq\\left\\{\\operatorname*{max}\\{n-1,1\\}\\right.\\quad\\left.i f k=1\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In addition, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\right|\\leq\\left\\{\\frac{1}{\\frac{1}{p_{\\star}^{2}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}}\\right.\\quad i f k=1\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, when $\\mathrm{KL}(P_{n,X}\\|P_{X})\\leq p_{\\star}^{2}/2$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\bigg|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\bigg|\\leq\\frac{2}{p_{\\star}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We first show that $P_{n}^{(k-1)}(x)\\geq1/n$ for $k=1$ and $P_{n}^{(k-1)}(x)\\geq p_{\\star}^{2}$ for $k>1$ . In the case that $k=1$ , the result follows directly from the event $\\boldsymbol{S}$ . For $k>1$ such that $k$ is odd, we have that for $x\\in\\mathscr{X}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{n,X}^{(k-1)}(x)=\\displaystyle\\sum_{y\\in\\mathcal{Y}}P_{n}^{(k-1)}(x,y)=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\frac{P_{Y}(y)}{P_{n,Y}^{(k-2)}(y)}P_{n}^{(k-2)}(x,y)}\\\\ &{\\qquad\\qquad\\geq p_{\\star}\\displaystyle\\sum_{y\\in\\mathcal{Y}}P_{n}^{(k-2)}(x,y)=p_{\\star}P_{n,X}^{(k-2)}(x)=p_{\\star}P_{X}(x)\\geq p_{\\star}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The result for $k$ even can be proven similarly. We now proceed to prove the inequalities given in the statement, which will rely on the lower bound above. ", "page_idx": 22}, {"type": "text", "text": "Proving the first inequality. Then, for any $x\\in\\mathscr{X}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\right|=\\operatorname*{max}\\left\\{\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1,1-\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}\\right\\}\\leq\\left\\{\\operatorname*{max}\\{n-1,1\\}\\atop\\operatorname*{max}\\{1/p_{\\star}^{2}-1,1\\}\\right.\\quad\\mathrm{if}\\;k>1\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is the desired result for the first inequality. ", "page_idx": 22}, {"type": "text", "text": "Proving the second and third inequalities. Consider an odd $k\\geq1$ . By the definition of total variation distance, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\\right|\\leq\\mathrm{TV}(P_{n,X}^{(k-1)},P_{X}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to Pinsker\u2019s inequality, we have that $\\begin{array}{r}{\\operatorname{TV}(P_{n,X}^{(k-1)},P_{X})\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}^{(k-1)}\\|P_{X})}}\\end{array}$ , and so we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\\right|\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}^{(k-1)}\\|P_{X})}\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}^{(0)}\\|P_{X})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows by the monotonicity of Sinkhorn iterations given in Prop. 13. We apply the lower bounds to write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\right|\\leq\\left\\{\\frac{1}{\\frac{1}{p_{\\star}^{2}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}}\\right.\\quad\\mathrm{if}\\;k=1\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, when $\\begin{array}{r}{\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}\\leq p_{\\star}/2}\\end{array}$ , we have that $\\begin{array}{r}{\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\\right|\\leq p_{\\star}/2}\\end{array}$ and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathcal{X}}P_{n,X}^{(k-1)}(x)\\geq\\operatorname*{min}_{x\\in\\mathcal{X}}P_{X}(x)-\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|P_{n,X}^{(k-1)}(x)-P_{X}(x)\\right|\\geq\\frac{p_{\\star}}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\bigg|\\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\\bigg|\\leq\\frac{\\operatorname*{max}_{x\\in\\mathcal{X}}\\Big|P_{n,X}^{(k-1)}(x)-P_{X}(x)\\Big|}{\\operatorname*{min}_{x\\in\\mathcal{X}}P_{n,X}^{(k-1)}(x)}\\leq\\frac{2}{p_{\\star}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, for $k$ even, set $k=2t$ for $t\\geq0$ . We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in\\mathcal{Y}}\\Big|P_{n,Y}^{(2t-1)}(y)-P_{Y}(y)\\Big|\\leq\\mathrm{TV}(P_{n,Y}^{(2t-1)},P_{Y})\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{Y}\\|P_{n,Y}^{(2t-1)})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Invoke Prop. 13 once again to achieve ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{Y}\\|P_{n,Y}^{(2t-1)})}\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which completes the proof. ", "page_idx": 23}, {"type": "text", "text": "D Statistical Analysis of Balancing Estimators ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section contains the proof of the main result, namely Thm. 1. We first introduce some additional notation and then give a broad outline of the proof for readability. Let the expectation of a function $h$ under a probability measure $Q$ on $\\mathcal X\\times\\mathcal X$ by denoted by ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ(h)=\\sum_{x\\in\\mathcal{X},y\\in\\mathcal{Y}}h(x,y)Q(x,y)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\varphi_{n}^{(k)}=P_{n}^{(k)}(h),\\quad\\varphi=P(h),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h)=\\sqrt{n}[P_{n}^{(k)}-P](h)=\\sqrt{n}(P_{n}^{(k)}(h)-P(h)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recalling in addition that $\\mathcal{C}_{k}=\\mathcal{C}_{X}$ for $k$ odd and $\\mathcal{C}_{k}=\\mathcal{C}_{Y}$ for $k$ even. Finally, the event ", "page_idx": 23}, {"type": "equation", "text": "$$\nS:=\\left\\{{\\mathrm{Supp}}(P_{n,X})={\\mathrm{Supp}}(P_{X}){\\mathrm{~and~}}{\\mathrm{Supp}}(P_{n,Y})={\\mathrm{Supp}}(P_{Y})\\right\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is used as a condition in many results. ", "page_idx": 23}, {"type": "text", "text": "Proof Outline. We first establish that the recursion formula ", "page_idx": 23}, {"type": "equation", "text": "$$\n[P_{n}^{(k)}-P](h)=[P_{n}^{(k-1)}-P](\\mathcal{C}_{k}h)+V_{n}^{(k-1)}(\\mathcal{C}_{k}h)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds in Prop. 15. Applying this result repeatedly to the balanced sequence $(P_{n}^{(k)})_{k\\geq1}$ and unrolling the recursion, we see that when $k$ is odd, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[P_{n}^{(k)}-P](h)=[P_{n}^{(k-1)}-P](\\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\\mathcal{C}_{X}h)}\\\\ &{\\qquad\\qquad\\qquad=[P_{n}^{(k-2)}-P](\\mathcal{C}_{Y}\\mathcal{C}_{X}h)+V_{n}^{(k-2)}(\\mathcal{C}_{Y}\\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\\mathcal{C}_{X}h)}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{[P_{n}^{(0)}-P](\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)}_{\\mathrm{first-order\\,term}}+\\underbrace{\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}(\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)}_{\\mathrm{higher-order\\,term}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Additionally, let $h_{\\ell,k}:=\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h$ , so that the first-order term can be written as $P_{n}^{(0)}(h_{1,k})-P(h_{1,k})$ higher-order term can also be written as $\\begin{array}{r}{\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}\\big(h_{\\ell,k}\\big)}\\end{array}$ . Because our original goal is to upper bound the mean squared error, we use the expansion above to write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left|P_{n}^{(k)}(h)-P(h)\\right|^{2}\\leq\\mathbb{E}\\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\\right|^{2}}\\\\ &{\\quad+\\,2\\mathbb{E}\\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\\right|\\left|\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}(h_{\\ell,k})\\right|+\\mathbb{E}\\left|\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}(h_{\\ell,k})\\right|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Regarding the first term, we have that $\\mathbb{E}\\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\\right|^{2}=\\sigma_{k}^{2}/n.$ , which is the dominant term in Thm. 1. Thus, the remaining challenge of the proof will be to upper bound the cross term and squared term and show its dependence on $n$ . The dominant term of these two will be the cross term, as we will essentially show that $|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})|$ is $O(n^{-1/2})$ with high probability and that $|\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}\\big(h_{\\ell,k}\\big)|$ is in fact $O(n^{-1})$ with high probability. As stated in Sec. 3, a key intermedi ate result in controlling the higher-order term is Prop. 14, whose proof is given in Appx. C. The remaining subsections walk through these steps in detail. ", "page_idx": 23}, {"type": "text", "text": "D.1 Recursion of Estimation Error ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first recall that the sequence $(P_{n}^{(k)})_{k\\geq1}$ can be computed with the following formula: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{n}^{(0)}(x,y):=P_{n}(x,y)\\mathrm{~and~}P_{n}^{(k)}(x,y):=\\left\\{\\begin{array}{l l}{\\frac{P_{X}}{P_{n,X}^{(k-1)}}(x)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~odd~}}\\\\ {\\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~even}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Prop. 15 establishes the conditions under which these steps are well-defined (i.e. $P_{n,X}^{(k-1)}(x)>0$ and $P_{n,Y}^{(k-1)}(y)>0)$ . Let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{n}^{(k-1)}(h)=\\left\\{\\begin{array}{l l}{\\sum_{x,y}\\left(\\frac{P_{X}}{P_{n,X}^{(k-1)}}(x)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~}k\\mathrm{~odd}}\\\\ {\\sum_{x,y}\\left(\\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~}k\\mathrm{~even.}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition 15. Let $(P_{n}^{(k)})_{k\\geq1}$ , be a sequence computed according to (8). These iterations are well-defined under the event $\\boldsymbol{S}$ , and for $\\mathbb{G}_{n}^{(k)}$ defined in (44), it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h)=\\mathbb{G}_{n}^{(k-1)}(h)+\\sqrt{n}V_{n}^{(k-1)}(h).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h)=\\mathbb{G}_{n}^{(k-1)}(\\mathscr{C}_{k}h)+\\sqrt{n}V_{n}^{(k-1)}(\\mathscr{C}_{k}h).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. First, assume that $P_{n,X}^{(k-1)}(x)>0$ and $P_{n,Y}^{(k-1)}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ so that we may establish the recursion, which we will show by induction toward the end of the proof. ", "page_idx": 24}, {"type": "text", "text": "Consider the following steps in the case that $k$ is odd: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{n}^{(\\boldsymbol{k})}(h)}\\\\ &{=\\displaystyle\\sum_{\\boldsymbol{x},\\boldsymbol{y}}h(\\boldsymbol{x},\\boldsymbol{y})P_{n}^{(\\boldsymbol{k})}(\\boldsymbol{x},\\boldsymbol{y})=\\sum_{\\boldsymbol{x},\\boldsymbol{y}}h(\\boldsymbol{x},\\boldsymbol{y})\\frac{P_{X}}{P_{n,X}^{(\\boldsymbol{k}-1)}}(\\boldsymbol{x})P_{n}^{(\\boldsymbol{k}-1)}(\\boldsymbol{x},\\boldsymbol{y})}\\\\ &{=\\displaystyle\\sum_{\\boldsymbol{x},\\boldsymbol{y}}1\\cdot h(\\boldsymbol{x},\\boldsymbol{y})P_{n}^{(\\boldsymbol{k}-1)}(\\boldsymbol{x},\\boldsymbol{y})+\\displaystyle\\sum_{\\boldsymbol{x},\\boldsymbol{y}}\\left[\\frac{P_{X}}{P_{n,X}^{(\\boldsymbol{k}-1)}}(\\boldsymbol{x})-1\\right]\\cdot h(\\boldsymbol{x},\\boldsymbol{y})P_{n}^{(\\boldsymbol{k}-1)}(\\boldsymbol{x},\\boldsymbol{y})}\\\\ &{=P_{n}^{(\\boldsymbol{k}-1)}(h)+V_{n}^{(\\boldsymbol{k}-1)}(h),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we substituted ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{n}^{(k-1)}(h)=\\left\\{\\begin{array}{l l}{\\sum_{x,y}\\left(\\frac{P_{X}}{P_{n,X}^{(k-1)}}(x)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~odd}}\\\\ {\\sum_{x,y}\\left(\\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~even}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Arguing analogously for $k$ even and subtracting $P(h)$ on both sides, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n[P_{n}^{(k)}-P](h)=[P_{n}^{(k-1)}-P](h)+V_{n}^{(k-1)}(h).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We refer to this as the \u201cuncentered\u201d recursion., which proves (39). ", "page_idx": 24}, {"type": "text", "text": "We can then establish the following \u201ccentered\u201d recursion using the following decomposition in the case of $k$ odd. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{[P_{n}^{(k)}-P](h)}\\\\ &{=[P_{n}^{(k)}-P](\\mathcal{C}_{X}h)+[P_{n}^{(k)}-P](\\mu_{X}h)}&{h=\\mathcal{C}_{X}h+\\mu_{X}h}\\\\ &{=[P_{n}^{(k-1)}-P](\\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\\mathcal{C}_{X}h)+[P_{n}^{(k)}-P](\\mu_{X}h)}&{\\mathrm{~apply~}(42)\\,\\mathrm{to}\\,\\mathcal{C}_{X}h}\\\\ &{=[P_{n}^{(k-1)}-P](\\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\\mathcal{C}_{X}h).}&{P_{n}^{(k)}(\\mu_{X}h)=P(\\mu_{X}h)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The last line follows because $\\mu_{X}h$ is only a function on $\\mathcal{X}$ , and due to the definition of the marginal rebalancing iterations, $P_{n,X}^{(k)}=P_{X}$ . This gives the desired formula by substituting (34). ", "page_idx": 24}, {"type": "text", "text": "We proceed to show that the iterations are well-defined. We will in fact show that $P_{n,X}^{(k-1)}(x)>0$ and $P_{n,Y}^{(k-1)}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ . For $k=1$ , $P_{n,X}^{(0)}(x)=P_{n,X}(x)>0$ and $P_{n,Y}^{(0)}(y)=$ $P_{n,Y}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ this holds under the event $\\boldsymbol{S}$ by assumption. We argue by induction that this holds for all $k>1$ . Assume that the claim is true for $\\{1,\\ldots,\\bar{k}-1\\}$ , and that $k$ is even. Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P_{n,X}^{(k-1)}(x)=P_{X}(x)>0,}\\\\ {P_{n,Y}^{(k-1)}(y)=\\displaystyle\\sum_{x\\in\\mathcal{X}}P_{n}^{(k-1)}(x,y)=\\displaystyle\\sum_{x\\in\\mathcal{X}}\\frac{P_{X}}{P_{n,X}^{(k-2)}}(x)P_{n}^{(k-2)}(x,y)}\\\\ {\\geq\\displaystyle\\operatorname*{min}_{x\\in\\mathcal{X}}\\frac{P_{X}}{P_{n,X}^{(k-2)}}(x)\\cdot P_{n,Y}^{(k-2)}(y)>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as $P_{n,X}^{(k-2)}(x)>0$ and $P_{n,Y}^{(k-2)}(y)>0$ by the inductive hypothesis. Arguing analogously for $k$ odd achieves the claim. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D.2 Technical Tools & Intermediate Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Having established the backbone of the argument, we collect in this subsection some useful tools that are used in the remainder of the proofs. ", "page_idx": 25}, {"type": "text", "text": "The following result follows from the method of types in information theory, and will be helpful in deriving the dependence of the higher-order term on $n$ . ", "page_idx": 25}, {"type": "text", "text": "Theorem 16. [Cover, 1999, Theorem 11.2.1] Let \u03bd be a discrete probability measure supported on m atoms. Let $U_{1},\\ldots,U_{n}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\nu$ and $\\nu_{n}$ be the associated empirical measure. Then, we have for any $\\epsilon>0$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\mathrm{KL}(\\nu_{n}\\|\\nu)\\geq\\epsilon\\right)\\leq2^{-n\\left(\\epsilon-m\\frac{\\log(n+1)}{n}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We then provide a result that counts the number of terms that appear when repeatedly centering via the operators $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}$ . This formalizes the pattern ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{X}=I-\\mu_{X}}}\\\\ {{\\displaystyle c_{Y}c_{X}=I-\\mu_{X}-\\mu_{Y}+\\mu_{Y}\\mu_{X}}}\\\\ {{\\displaystyle c_{X}c_{Y}c_{X}=I-\\mu_{X}-\\mu_{Y}+\\mu_{Y}\\mu_{X}+\\mu_{X}\\mu_{Y}-\\mu_{X}\\mu_{Y}\\mu_{X},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and so on. This will be useful when bounding $h_{\\ell,k}$ uniformly. ", "page_idx": 25}, {"type": "text", "text": "Lemma 17. For any $k\\geq1$ and $\\ell\\in\\{1,\\ldots,k\\}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}=I-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-1)/2}(\\mu_{X}\\mu_{Y})^{\\tau}\\mu_{X}-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-1)/2}(\\mu_{Y}\\mu_{X})^{\\tau}\\mu_{Y}}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{\\tau=1}^{(k-\\ell)/2}(\\mu_{X}\\mu_{Y})^{\\tau}+\\displaystyle\\sum_{\\tau=1}^{(k-\\ell)/2}(\\mu_{Y}\\mu_{X})^{\\tau}+(-1)^{k-\\ell+1}\\mu_{\\ell}\\ldots\\mu_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the sum $\\textstyle\\sum_{\\tau=i}^{j}$ is $\\boldsymbol{O}$ when $i>j$ and is $\\textstyle\\sum_{\\tau=i}^{\\lfloor j\\rfloor}$ when $j$ is not an integer by convention. ", "page_idx": 25}, {"type": "text", "text": "Proof. We prove the claim by backward induction on $\\ell$ , for the case that $k$ is odd. In the case $\\ell=k$ , the claim holds because $\\mathcal{C}_{k}=I-\\mu_{k}$ . Next, for any $\\ell<k$ , assume that the stated result holds for ", "page_idx": 25}, {"type": "text", "text": "$\\{\\ell+1,\\ldots,k\\}$ . Then, if $\\ell$ is also odd (so that $\\mu_{\\ell}=\\mu_{X}.$ ), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C_{\\ell}\\ldots C_{k}=C_{\\ell}C_{\\ell+1}\\ldots C_{k}}\\\\ &{\\qquad\\qquad\\underset{\\tau=0}{\\overset{(k-\\ell-2)/2}{\\sum}}\\mu_{X}-\\underset{\\tau=0}{\\overset{(k-\\ell-2)/2}{\\sum}}\\mu_{Y}}\\\\ &{\\qquad\\qquad\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\big(\\mu_{X}\\mu_{Y}\\big)^{\\tau}\\mu_{X}-\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\big(\\mu_{Y}\\mu_{X}\\big)^{\\tau}\\mu_{Y}}\\\\ &{\\qquad+\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\big(\\mu_{X}\\mu_{Y}\\big)^{\\tau}+\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\big(\\mu_{Y}\\mu_{X}\\big)^{\\tau}+\\mu_{Y}\\underset{k-\\ell\\in\\mathrm{terms}}{\\overset{(k-\\ell-1)/2}{\\sum}}\\mu_{X}}\\\\ &{\\qquad-\\ \\mu_{X}+\\underset{\\tau=0}{\\overset{(k-\\ell-2)/2}{\\sum}}\\big(\\mu_{X}\\mu_{Y}\\big)^{\\tau}\\mu_{X}+\\underset{\\tau=0}{\\overset{(k-\\ell-2)/2}{\\sum}}\\mu_{X}\\big(\\mu_{Y}\\mu_{X}\\big)^{\\tau}\\mu_{Y}}\\\\ &{\\qquad+\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\big(\\mu_{X}\\mu_{Y}\\big)^{\\tau}-\\underset{\\tau=1}{\\overset{(k-\\ell-1)/2}{\\sum}}\\mu_{X}\\big(\\mu_{Y}\\mu_{X}\\big)^{\\tau}-\\big(\\mu_{X}\\mu_{Y}\\big)^{(k-\\ell)/2}\\mu_{X}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The red terms and blue terms cancel out to zero. This leaves ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}=I-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-2)/2}\\left(\\mu_{X}\\mu_{Y}\\right)^{\\tau}\\mu_{X}-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-2)/2}\\left(\\mu_{Y}\\mu_{X}\\right)^{\\tau}\\mu_{Y}}\\\\ &{\\quad\\qquad\\qquad+\\displaystyle\\sum_{\\tau=1}^{(k-\\ell-1)/2}\\left(\\mu_{Y}\\mu_{X}\\right)^{\\tau}+\\left(\\mu_{Y}\\mu_{X}\\right)^{(k-\\ell)/2}}\\\\ &{\\quad\\qquad\\qquad+\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-2)/2}\\mu_{X}(\\mu_{Y}\\mu_{X})^{\\tau}\\mu_{Y}+(-1)^{k-\\ell+1}\\mu_{\\ell}\\ldots\\mu_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "wherein we combine the red terms and re-index the blue terms to get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}=I-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-2)/2}(\\mu_{X}\\mu_{Y})^{\\tau}\\mu_{X}-\\displaystyle\\sum_{\\tau=0}^{(k-\\ell-2)/2}(\\mu_{Y}\\mu_{X})^{\\tau}\\mu_{Y}}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{\\tau=1}^{(k-\\ell)/2}(\\mu_{Y}\\mu_{X})^{\\tau}+\\displaystyle\\sum_{\\tau=1}^{(k-\\ell)/2}(\\mu_{X}\\mu_{Y})^{\\tau}+(-1)^{k-\\ell+1}\\mu_{\\ell}\\ldots\\mu_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, because $k-\\ell$ is even when $k$ is odd and $\\ell$ is odd, we can set the upper bound of the first two sums to $(k-\\ell-1)/2$ without changing the number of terms. This proves the desired result. The result can be proved similarly when $\\ell$ is even. As a result, we have proved the claim for any odd $\\boldsymbol{\\mathrm{k}}$ and $\\ell\\leq k$ . Similar arguments can be used for the case of $k$ even and $\\ell\\leq k$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.3 Analysis of Higher-Order Term ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Returning to the outline at the start of this section, we may now bound the higher-order remainder term in (36), namely ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}\\big(h_{\\ell,k}\\big)=\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}\\big(\\mathcal{C}_{\\ell}\\cdot..\\,.\\mathcal{C}_{k}h\\big),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "depends on controlling the quantity $V_{n}^{(k-1)}$ in the summation, which we recall for convenience: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{n}^{(k-1)}(h)=\\left\\{\\begin{array}{l l}{\\sum_{x,y}\\left(\\frac{P_{X}}{P_{n,X}^{(k-1)}}(x)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~odd}}\\\\ {\\sum_{x,y}\\left(\\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)-1\\right)h(x,y)P_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~even}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Because we have established uniform control over the functions $P_{X}/P_{n,X}^{(k-1)}-1$ and $P_{Y}/P_{n,Y}^{(k-1)}-1$ via Prop. 14 in Appx. C we can now bound the full remainder in Prop. 20. ", "page_idx": 26}, {"type": "text", "text": "We also make use of the following intermediate result, which controls how large the $\\ell_{\\infty}$ -norm of the function $h$ can grow after centering. ", "page_idx": 26}, {"type": "text", "text": "Proof. Apply Lem. 17 and the triangle inequality, so that we only need to count the number of terms that appear in the sums, adding 2 for the first and last term in the expression. We subtract 1 from the total, as one of either $(k-\\ell)/2$ or $(k-\\ell+1)/2$ will be a fraction. This yields $2(k-\\ell+1)$ terms total, the desired result. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "We upper bound the sum in Prop. 20. To do so, we introduce some notation. Consider $B_{1}$ and $B_{2}$ defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\nB_{1}:=M_{1}\\quad\\mathrm{~and~}\\quad B_{2}:=\\operatorname*{max}_{2\\leq\\ell\\leq k}M_{\\ell}\\quad\\mathrm{for}\\quad M_{\\ell}:=\\left\\{\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{P_{X}(x)}{P_{n,X}^{(\\ell-1)}(x)}-1\\right|\\quad\\ell\\mathrm{~odd}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $k\\geq1$ . We also enumerate the sample spaces as $\\mathcal{X}=\\{x_{1},\\ldots,x_{m}\\}$ and $\\mathcal{Y}=\\{y_{1},\\ldots,y_{m}\\}$ , and define the function ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{1}_{j k}(x,y):={\\textstyle\\left\\{\\begin{array}{l l}{\\mathbb{1}\\left\\{x=x_{j}\\right\\}}&{k{\\mathrm{~odd}}}\\\\ {\\mathbb{1}\\left\\{y=y_{j}\\right\\}}&{k{\\mathrm{~even}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This is an indicator function on the $j$ -th element of either $\\mathcal{X}$ or $\\boldsymbol{\\wp}$ depending on whether $k$ is odd or even. Finally, for any function $h$ , use (under the event $\\boldsymbol{S}$ ) recall the empirical process notation ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h):=\\sqrt{n}\\left(P_{n}^{(k)}(h)-P(h)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using this notation, we can rewrite the recursion in terms of the quantity $\\mathbb{G}_{n}^{(k)}(h)$ itself. This is established in the following lemma. ", "page_idx": 27}, {"type": "text", "text": "Lemma 19. For $k$ odd, it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h)=\\mathbb{G}_{n}^{(k-1)}(\\mathcal{C}_{X}h)+\\sum_{j=1}^{m}\\left[\\frac{P_{X}(x_{j})}{P_{n,X}^{(k-1)}(x_{j})}-1\\right]\\mathbb{G}_{n}^{(k-1)}(\\mathcal{C}_{X}h\\,\\mathbf{1}_{j k}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "whereas for $k$ even, it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{G}_{n}^{(k)}(h)=\\mathbb{G}_{n}^{(k-1)}(\\mathcal{C}_{Y}h)+\\sum_{j=1}^{m}\\left[\\frac{P_{Y}(y_{j})}{P_{n,Y}^{(k-1)}(y_{j})}-1\\right]\\mathbb{G}_{n}^{(k-1)}(\\mathcal{C}_{Y}h\\,\\mathbf{1}_{j k}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We give the proof for $k$ odd. By (40) from Prop. 15 and by the definition of $\\mathbb{G}_{n}^{(k)}(h)$ , we need only show that $P(\\bar{C_{X}}h\\mathbf{1}_{j k})=0$ . Indeed, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathcal{C}_{X}h\\mathbf{1}_{j k}\\vert X\\right](x)=\\left\\{\\begin{array}{l l}{\\mathbb{E}\\left[\\mathcal{C}_{X}h\\vert X\\right](x_{j})}&{\\mathrm{~if~}x=x_{j}}\\\\ {0}&{\\mathrm{~if~}x\\ne x_{j}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "But $\\mathbb{E}\\left[{\\mathcal C}_{X}h|X\\right](x_{j})\\ =\\ 0$ by definition of $\\mathcal{C}_{X}$ . Taking an expectation over $P_{X}$ gives that $P(\\mathcal{C}_{X}h\\mathbf{1}_{j k})=0$ , which implies the desired result. The proof for $k$ even follows symmetrically. ", "page_idx": 27}, {"type": "text", "text": "The higher-order term in (36), can be bounded using Prop. 20. ", "page_idx": 27}, {"type": "text", "text": "Proposition 20. For any $k\\geq1$ , the following holds under the event $\\boldsymbol{S}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sqrt{n}\\sum_{\\ell=1}^{k}|V_{n}^{(\\ell-1)}(\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)|\\leq\\sum_{j=1}^{m}\\Bigg(B_{1}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\mathbf{1}_{j\\ell})\\right|+B_{2}\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\mathbf{1}_{j\\ell})|\\Bigg)}}\\\\ &{}&{\\quad+\\left.m B_{2}\\left\\|h\\right\\|_{\\infty}\\sqrt{n}k(k-1)[B_{1}+B_{2}(k+1)/3].~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. First, for any $\\ell\\in\\{1,\\ldots,k\\}$ , recall the notation $h_{\\ell,k}:=\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h$ . By (39) from Prop. 15 and by Lem. 19, we have that for $\\ell$ odd, ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\sqrt{n}}V_{n}^{(\\ell-1)}{\\big(}h_{\\ell,k}{\\big)}=\\sum_{j=1}^{m}\\left[{\\frac{P_{X}}{P_{n,X}^{(\\ell-1)}}}(x_{j})-1\\right]\\mathbb{G}_{n}^{(\\ell-1)}{\\big(}h_{\\ell,k}\\mathbf{1}_{j\\ell}{\\big)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the statement above, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{n}\\,\\vert V_{n}^{(\\ell-1)}(h_{\\ell,k})\\vert\\leq M_{\\ell}\\sum_{j=1}^{m}\\vert(\\mathbb{G}_{n}^{(\\ell-1)}(h_{\\ell,k}\\,\\mathbf{1}_{j\\ell})\\vert\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The bound above holds for $\\ell$ even as well. Then, using the (39) from Prop. 15 again, we have that for $\\ell\\geq2$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n[P_{n}^{(\\ell-1)}-P](h_{\\ell,k}\\mathbf{1}_{j\\ell})=[P_{n}^{(\\ell-2)}-P](h_{\\ell,k}\\mathbf{1}_{j\\ell})+V_{n}^{(\\ell-2)}(h_{\\ell,k}\\mathbf{1}_{j\\ell})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\mathbb{G}_{n}^{(\\ell-1)}(h_{\\ell,k}{\\bf1}_{j\\ell})|\\le|\\mathbb{G}_{n}^{(\\ell-2)}(h_{\\ell,k}{\\bf1}_{j\\ell})|+\\sqrt{n}\\,|V_{n}^{(\\ell-2)}(h_{\\ell,k}{\\bf1}_{j\\ell})|}&{}\\\\ {\\le|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}{\\bf1}_{j\\ell})|+\\sqrt{n}\\,|V_{n}^{(0)}(h_{\\ell,k}{\\bf1}_{j\\ell})|+\\dots+\\sqrt{n}\\,|V_{n}^{(\\ell-2)}(h_{\\ell,k}{\\bf1}_{j\\ell})|}&{}\\\\ {\\le|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}{\\bf1}_{j\\ell})|+M_{1}\\sqrt{n}\\,P_{n}^{(0)}(|h_{\\ell,k}|\\,{\\bf1}_{j\\ell})+\\dots+M_{\\ell}\\sqrt{n}\\,P_{n}^{(\\ell-2)}(|h_{\\ell,k}|\\,{\\bf1}_{j\\ell})}&{}\\\\ {\\le|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}{\\bf1}_{j\\ell})|+2\\,\\|h\\|_{\\infty}\\,\\sqrt{n}\\,[B_{1}+B_{2}(\\ell-1)]\\,(k-\\ell+1),\\qquad\\qquad(4)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "by Lem. 18 and $M_{1}\\leq B_{1}$ and $M_{\\ell}\\le B_{2}$ for $\\ell\\geq2$ . Summing these bounds, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n_{i}}\\sum_{t=1}^{\\infty}\\left|\\mathrm{tr}_{i}(t,x_{i})\\right|}\\\\ &{\\leq M_{1}\\frac{1}{\\rho_{1}}\\left|(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i}))\\right|+\\underset{\\ell\\leq2}{\\sum}M_{\\ell}\\frac{1}{\\rho_{1}}\\left|(\\xi_{t}^{\\alpha_{i}-1}(h_{i},1,t_{i}))\\right|}\\\\ &{\\leq B_{1}\\frac{1}{\\rho_{1}}\\left|(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i}))\\right|+B_{2}\\sum_{t=2}^{\\infty}\\left|\\xi_{t}^{\\alpha_{i}-1}(h_{i},1,t_{i})\\right|}\\\\ &{\\leq B_{1}\\frac{1}{\\rho_{1}}\\left|(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i}))\\right|+B_{2}\\sum_{t=2}^{\\infty}\\left|\\xi_{t}^{\\alpha_{i}-1}(h_{i},1,t_{i})\\right|}\\\\ &{\\leq B_{1}\\frac{1}{\\rho_{1}}\\left|(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i}))\\right|+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times2}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left|(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i}))+2|h|_{i}\\times\\sqrt{n}|\\left|\\bar{h}_{i}+B_{2}(t-1)\\right|(k-t+1))\\quad\\mathrm{apply}\\;(u,1,t_{i})}\\\\ &{=\\frac{\\displaystyle1}{\\rho_{1}}\\left(\\Delta_{1}\\frac{1}{\\rho_{1}}\\left(\\mathrm{tr}_{i}(h_{i},1,t_{i})\\right)+B_{1}\\frac{1}{\\rho_{1}^{2}}\\left(\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i})\\right)\\right)+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\quad\\int_{0}^{\\infty}\\left(B_{1}\\right|\\xi_{t}^{\\alpha_{i}}(h_{i},1,t_{i})\\right)+B\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "because $|{\\boldsymbol{\\mathcal{X}}}|=m$ . We sum the last term: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\ell=2}^{k}\\left[B_{1}+B_{2}(\\ell-1)\\right]\\left(k-\\ell+1\\right)=B_{1}\\sum_{\\ell=1}^{k-1}(k-\\ell)+B_{2}\\sum_{\\ell=1}^{k-1}\\ell(k-\\ell)}}\\\\ &{}&{=\\frac{k(k-1)}{2}\\left[B_{1}+B_{2}(k+1)/3\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "completing the proof. ", "page_idx": 28}, {"type": "text", "text": "D.4 Proof of Main Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We can now show the main result of this section: the bound on the mean squared error of the rebalanced estimator. Recall the event ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\mathcal{S}}:=\\{{\\mathrm{Supp}}(P_{n,X})={\\mathrm{Supp}}(P_{X}){\\mathrm{~and~}}{\\mathrm{Supp}}(P_{n,Y})={\\mathrm{Supp}}(P_{Y})\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as introduced in (35). To remind the reader of the high-level steps of the proof, we may decompose the error on the event $\\boldsymbol{S}$ we used the estimator ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\varphi}_{n}^{(k)}:=\\varphi_{n}^{(k)}\\mathbb{1}_{\\mathcal{S}}+\\varphi_{n}^{(0)}\\mathbb{1}_{\\mathcal{S}^{c}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so we decompose on the event $\\boldsymbol{S}$ to write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\left(\\tilde{P_{n}}^{(k)}(h)-P(h)\\right)^{2}\\right]=\\mathbb{E}_{P}\\left[\\left(P_{n}(h)-P(h)\\right)^{2}\\mathbf{1}_{S^{c}}\\right]+\\mathbb{E}_{P}\\left[\\left(P_{n}^{(k)}(h)-P(h)\\right)^{2}\\mathbf{1}_{S}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, we use the upcoming Prop. 21 to bound the first term, which will in turn require showing that $\\boldsymbol{S}$ occurs with high probability. As for the second term, we will apply Prop. 15 and the derivation (36) to write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\left(P_{n}^{(k)}(h)-P(h)\\right)^{2}\\mathbb{1}_{S}\\right]=\\mathbb{E}_{P}\\left[T_{1}^{2}\\mathbb{1}_{S}\\right]+2\\mathbb{E}_{P}\\left[T_{1}T_{2}\\mathbb{1}_{S}\\right]+\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for ", "page_idx": 29}, {"type": "equation", "text": "$$\nT_{1}:=[P_{n}^{(0)}-P]({\\mathcal{C}}_{1}\\ldots{\\mathcal{C}}_{k}h){\\mathrm{~and~}}T_{2}:=\\sum_{\\ell=1}^{k}V_{n}^{(\\ell-1)}({\\mathcal{C}}_{\\ell}\\ldots{\\mathcal{C}}_{k}h).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By definition, we have that $\\mathbb{E}_{P}\\left[T_{1}^{2}\\mathbb{1}s\\right]\\le\\mathbb{E}_{P}\\left[T_{1}^{2}\\right]=\\sigma_{k}^{2}/n$ . It then remains to bound the cross term $\\mathbb{E}_{P}\\left[T_{1}T_{2}\\mathbb{1}_{S}\\right]$ and squared term $\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S}\\right]$ . This is accomplished by Lem. 23 and Lem. 22, respectively. ", "page_idx": 29}, {"type": "text", "text": "Proposition 21. It holds that $P(S^{c})\\leq2m(1-p_{\\star})^{n}$ . Moreover, for any $\\delta\\in(0,1)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\left(P_{n}(h)-P(h)\\right)^{2}\\mathbf{1}_{S^{c}}\\right]\\le4\\left\\|h\\right\\|_{\\infty}^{2}\\operatorname*{min}\\left\\{2m(1-p_{\\star})^{n},\\delta\\right\\}+\\frac{2\\log(2/\\delta)}{n}\\left\\|h\\right\\|_{\\infty}^{2}2m(1-p_{\\star})^{n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Define $\\mathcal{F}_{X}:=\\{\\operatorname{Supp}(P_{n,X})\\neq\\operatorname{Supp}(P_{X})\\}$ and $\\mathcal{F}_{Y}:=\\{\\operatorname{Supp}(P_{n,Y})\\neq\\operatorname{Supp}(P_{Y})\\}$ , so that $S^{c}=\\mathcal{F}_{X}\\cup\\mathcal{F}_{Y}$ . We first control the probability of ${\\mathcal{F}}_{X}$ . Let $F_{j}:=\\{P_{n,X}(x_{j})=0\\}$ for $j\\in[m]$ . We then obtain $\\mathcal{F}_{X}=\\cup_{j=1}^{m}F_{j}$ , which implies by the union bound that ", "page_idx": 29}, {"type": "equation", "text": "$$\nP(\\mathcal F_{X})\\leq\\sum_{j=1}^{m}P(F_{j})=\\sum_{j=1}^{m}(1-P_{X}(x_{j}))^{n}\\leq m(1-p_{\\star})^{n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, we have that $P(\\mathcal{F}_{Y})\\leq m(1-p_{\\star})^{n}$ and thus $P(S^{c})\\leq2m(1-p_{\\star})^{n}$ , which gives the first claim. ", "page_idx": 29}, {"type": "text", "text": "To control the expectation, consider any $\\delta>0$ , and define the event ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\delta}:=\\left\\{\\left|P_{n}^{(0)}(h)-P(h)\\right|\\leq\\sqrt{\\frac{2\\log\\left(2/\\delta\\right)}{n}}\\left\\|h\\right\\|_{\\infty}\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Hoeffding\u2019s inequality, it holds that $P(\\mathcal{E}_{\\delta})\\geq1-\\delta$ . Furthermore, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbb{1}_{S^{c}}(P_{n}^{(0)}(h)-P(h))^{2}]=\\mathbb{E}[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{\\delta}^{c}}(P_{n}^{(0)}(h)-P(h))^{2}]+\\mathbb{E}[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{\\delta}}(P_{n}^{(0)}(h)-P(h))^{2}]}\\\\ &{\\phantom{=}\\leq4\\left\\lVert h\\right\\rVert_{\\infty}^{2}\\mathbb{E}[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{\\delta}^{c}}]+\\frac{2\\log{(2/\\delta)}}{n}\\left\\lVert h\\right\\rVert_{\\infty}^{2}\\mathbb{E}[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{\\delta}}]}\\\\ &{\\phantom{=}\\leq4\\left\\lVert h\\right\\rVert_{\\infty}^{2}\\operatorname*{min}\\{P(S^{c}),P(\\mathcal{E}_{\\delta}^{c})\\}+\\frac{2\\log{(2/\\delta)}}{n}\\left\\lVert h\\right\\rVert_{\\infty}^{2}P(S^{c})}\\\\ &{\\phantom{=4}4\\left\\lVert h\\right\\rVert_{\\infty}^{2}\\operatorname*{min}\\{2m(1-p_{\\star})^{n},\\delta\\}+\\frac{2\\log{(2/\\delta)}}{n}\\left\\lVert h\\right\\rVert_{\\infty}^{2}2m(1-p_{\\star})^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In order to bound the terms appearing in (49), we introduce the events $\\mathcal{E}_{1}^{\\delta},\\mathcal{E}_{2}^{\\delta}$ , and $\\mathcal{E}_{3}^{\\delta}$ , defined by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{E}_{1}^{\\delta}:=\\left\\{\\operatorname*{max}\\left\\{\\mathrm{KL}(P_{n,X}\\|P_{X}),\\mathrm{KL}(P_{n,Y}\\|P_{Y})\\right\\}\\leq\\displaystyle\\frac{1}{n}\\log_{2}\\displaystyle\\frac{2}{\\delta}+m\\frac{\\log(n+1)}{n}\\right\\}}\\\\ {\\mathcal{F}_{\\ell}^{\\delta}:=\\left\\{|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,\\mathbf{1}_{j\\ell})|\\leq\\sqrt{2\\log(2m k/\\delta)}2(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}\\right\\},\\quad\\ell=1,\\dots,k,\\,j=1,\\dots,m}\\\\ {\\mathcal{E}_{2}^{\\delta}:=\\displaystyle\\prod_{\\ell=1}^{k}\\mathcal{F}_{\\ell}^{\\delta}}\\\\ {\\mathcal{E}_{3}^{\\delta}:=\\left\\{|\\mathbb{G}_{n}^{(0)}(h_{1,k})|\\leq\\sqrt{2\\log(2/\\delta)}2k\\left\\|h\\right\\|_{\\infty}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The events are constructed such that $\\mathbb{P}(\\mathcal{E}_{1}^{\\delta})\\geq1-\\delta,\\mathbb{P}(\\mathcal{E}_{2}^{\\delta})\\geq1-\\delta$ , and $\\mathbb{P}(\\mathcal{E}_{3}^{\\delta})\\geq1-\\delta$ , as we used in the upcoming proofs of Lem. 23, Lem. 22, and Thm. 24. ", "page_idx": 29}, {"type": "text", "text": "Lemma 22 (Squared term bound). Let $T_{2}$ be defined as in (50). For any $\\delta\\,>\\,0$ , assuming that $n\\geq2[\\log_{2}(2/\\bar{\\delta})+m\\log(n+1)]/p_{\\star}^{2}$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}s\\right]\\le\\displaystyle\\frac{2\\left\\|h\\right\\|_{\\infty}^{2}m^{2}k^{2}}{p_{\\star}^{2}}\\left[\\log_{2}(2/\\delta)+m\\log(n+1)\\right]^{2-1\\{k=1\\}}\\~\\times}\\\\ &{\\left[\\left(4n+\\displaystyle\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\displaystyle\\frac{k+1}{p_{\\star}^{2}}\\right)\\right)^{2}\\delta+\\displaystyle\\frac{8}{n^{2}}\\left(\\sqrt{2\\log\\displaystyle\\frac{2m k}{\\delta}}(k+1)+\\displaystyle\\frac{(k-1)(k+4)}{p_{\\star}^{2}}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The following computations are done under the event $\\boldsymbol{S}$ . First, apply Prop. 20 to write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{|T_{2}|\\leq\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{j=1}^{m}\\left(B_{1}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,\\mathbf{1}_{j\\ell})\\right|+B_{2}\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,\\mathbf{1}_{j\\ell})|\\right)\\,\\,+}}\\\\ {{m B_{2}\\left\\|h\\right\\|_{\\infty}k(k-1)[B_{1}+B_{2}(k+1)/3].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We decompose on the event $\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ . Note that by Thm. 16, we have that $\\mathbb{P}(\\mathcal{E}_{1}^{\\delta})\\,\\geq\\,1\\,-\\,\\delta$ . It follows from Hoeffding\u2019s inequality, the union bound, and boundedness of $\\left\\|h_{\\ell,k}\\,\\mathbf{1}_{j\\ell}\\right\\|$ by Lem. 18 that $\\mathbb{P}(\\mathcal{E}_{2}^{\\delta})\\geq1-\\delta$ As a result, $\\mathbb{P}(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})\\geq1-2\\delta$ . ", "page_idx": 30}, {"type": "text", "text": "Bound $\\left|T_{2}\\right|$ under the event $S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})$ . In this case, we apply (32) from Prop. 14 to get $B_{1}\\leq n$ and $B_{2}\\,\\leq\\,\\mathrm{i}/p_{\\star}^{2}$ , along with the universal bounds from Lem. 18: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{\\sqrt{n}}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\left\\|h_{1,k}\\right\\|_{\\infty}\\leq4k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{\\ell=2}^{k}\\left|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\sum_{\\ell=2}^{k}\\left\\|h_{\\ell,k}\\right\\|_{\\infty}\\leq\\sum_{\\ell=2}^{k}4(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}=2k(k-1)\\left\\|h\\right\\|_{\\infty}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "so that by plugging into (51), ", "page_idx": 30}, {"type": "equation", "text": "$$\n|T_{2}|\\leq\\|h\\|_{\\infty}\\,m k\\left[4n+\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\frac{k+1}{3p_{\\star}^{2}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and in turn, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S\\backslash(\\xi_{1}^{\\delta}\\cap\\xi_{2}^{\\delta})}\\right]\\leq2\\left\\|h\\right\\|_{\\infty}^{2}m^{2}k^{2}\\left[4n+\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\frac{k+1}{3p_{\\star}^{2}}\\right)\\right]^{2}\\delta.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bound $\\left|{{T_{2}}}\\right|$ under the event $S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ . In this case, we may use that $n\\,\\geq\\,2[\\log_{2}(2/\\delta)\\,+$ $m\\log(n+1)]/p_{\\star}^{2}$ apply (33) from Prop. 14 to get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{B_{1},B_{2}\\right\\}\\leq\\frac{2}{p_{\\star}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}\\leq\\frac{1}{p_{\\star}\\sqrt{n}}\\sqrt{2\\log_{2}(2/\\delta)+2m\\log(n+1)}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the bounds based on $\\mathcal{E}_{2}^{\\delta}$ which give ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})|\\leq\\sqrt{2\\log\\frac{2m k}{\\delta}}2k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})|\\leq\\sum_{\\ell=2}^{k}\\sqrt{2\\log\\frac{2m k}{\\delta}}2(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}\\leq\\sqrt{2\\log\\frac{2m k}{\\delta}}k(k-1)\\left\\|h\\right\\|_{\\infty},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By plugging into (51), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|T_{2}|\\le\\displaystyle\\frac{2m\\left\\|h\\right\\|_{\\infty}\\sqrt{2\\log(2m k/\\delta)\\left[2\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]}}{n p_{\\star}}k(k+1)+}\\\\ &{\\qquad\\frac{m\\left\\|h\\right\\|_{\\infty}\\left[2\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]}{3n p_{\\star}^{2}}k(k-1)(k+4)}\\\\ &{\\qquad\\le\\displaystyle\\frac{4m k\\left\\|h\\right\\|_{\\infty}\\left[\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]^{1-1\\{k=1\\}/2}}{n p_{\\star}^{2}}\\ \\times}\\\\ &{\\qquad\\qquad\\Big[p_{\\star}\\sqrt{2\\log\\left(2m k/\\delta\\right)}(k+1)+(k-1)(k+4)\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In turn, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S\\setminus(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})}\\right]\\le\\frac{16\\left\\|h\\right\\|_{\\infty}^{2}m^{2}k^{2}\\left[\\log_{2}(2/\\delta)+m\\log(n+1)\\right]^{2-1\\left\\{k=1\\right\\}}}{n^{2}p_{\\star}^{4}}\\times}\\\\ {\\left[p_{\\star}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)\\right]^{2}.\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining together both (57) and (52) and using that $[\\log_{2}(2/\\delta)+2m\\log(n+1)]\\geq1$ , we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}s\\right]\\le\\displaystyle\\frac{2\\left\\lVert h\\right\\rVert_{\\infty}^{2}m^{2}k^{2}}{p_{\\star}^{2}}\\left[\\log_{2}(2/\\delta)+m\\log(n+1)\\right]^{2-1\\left\\{k=1\\right\\}}\\~\\times}\\\\ &{\\left[\\left(4n+\\displaystyle\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\displaystyle\\frac{k+1}{p_{\\star}^{2}}\\right)\\right)^{2}\\delta+\\displaystyle\\frac{8}{n^{2}}\\left(\\sqrt{2\\log(2m k/\\delta)}(k+1)+\\displaystyle\\frac{(k-1)(k+4)}{p_{\\star}^{2}}\\right)^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "the result as desired. ", "page_idx": 31}, {"type": "text", "text": "Lemma 23 (Cross term bound). Let $T_{1}$ and $T_{2}$ be defined as in (50). For any $\\delta>0$ , assuming that $n\\geq2[\\log_{2}(2/\\delta)+m\\log(n+1)]/p_{\\star}^{2}$ , we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad^{\\mathbb{D}_{P}}\\lfloor\\iota1^{L}2^{\\mathbb{1}}S\\rfloor}\\\\ &{\\le\\frac{2m k^{2}\\,\\left\\|h\\right\\|_{\\infty}^{2}\\sqrt{2\\log(2/\\delta)}\\,\\big[\\log_{2}(2/\\delta)+2m\\log(n+1)\\big]^{1-\\mathbb{1}\\{k=1\\}/2}}{p_{\\star}^{2}}\\times}\\\\ &{\\left[\\frac{p_{\\star}\\sqrt{2\\log{(2m k/\\delta)}}(k+1)+(k-1)(k+4)}{n^{3/2}}+6\\left(4n p_{\\star}^{2}+(k-1)\\left(n+2+\\frac{k+1}{p_{\\star}^{2}}\\right)\\right)\\delta\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. The following computations are done under the event $\\boldsymbol{S}$ . First, apply Prop. 20 to write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle|T_{1}T_{2}|\\le\\frac{1}{\\sqrt{n}}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k})\\right|\\left[\\frac{1}{\\sqrt{n}}\\sum_{j=1}^{m}\\left(B_{1}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})\\right|+B_{2}\\sum_{\\ell=2}^{k}\\left|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})\\right|\\right)\\right.\\,+}}\\\\ {{\\displaystyle m B_{2}\\left\\|h\\right\\|_{\\infty}k(k-1)[B_{1}+B_{2}(k+1)/3]\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We decompose on the event $\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta}$ . Note that by Thm. 16 and that $n\\ge\\log_{2}(2/\\delta)\\!+\\!m\\log(n\\!+\\!1)$ , we have that $\\mathbb{P}(\\mathcal{E}_{1}^{\\delta})~\\geq~1\\,-\\,\\delta$ . It follows by Hoeffding\u2019s inequality and the union bound that $\\mathbb{P}(\\mathcal{E}_{2}^{\\delta})\\geq1-\\delta$ . Similarly, we also have by Hoeffding\u2019s inequality that $\\mathbb{P}(\\mathcal{E}_{3}^{\\delta})\\geq1-\\delta$ . As a result, $\\mathbb{P}(\\mathcal{E}_{1}^{\\bar{\\delta}}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta})\\geq1-3\\delta$ . ", "page_idx": 31}, {"type": "text", "text": "Bound $|T_{1}T_{2}|$ under the event $S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta})$ . In this case, we apply (32) from Prop. 14 to get $B_{1}\\leq n$ and $\\dot{B}_{2}\\leq1/p_{\\star}^{2}$ , along with the universal bounds from Lem. 18: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{\\sqrt n}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k})\\right|\\leq2\\left\\|h_{1,k}\\right\\|_{\\infty}\\leq4k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\frac{1}{\\sqrt n}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\left\\|h_{1,k}\\right\\|_{\\infty}\\leq4k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\frac{1}{\\sqrt n}\\sum_{\\ell=2}^{k}\\left|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\sum_{\\ell=2}^{k}\\left\\|h_{\\ell,k}\\right\\|_{\\infty}\\leq\\sum_{\\ell=2}^{k}4(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}=2k(k-1)\\left\\|h\\right\\|_{\\infty},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "so that by plugging into (58), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|T_{1}T_{2}\\right|\\leq4k^{2}\\left\\|h\\right\\|_{\\infty}^{2}m\\left[4n+\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\frac{k+1}{3p_{\\star}^{2}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and in turn, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[T_{1}T_{2}\\mathbb{1}_{S\\setminus(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta})}\\right]\\le\\frac{12k^{2}\\left\\lVert h\\right\\rVert_{\\infty}^{2}m}{p_{\\star}^{2}}\\left[4n p_{\\star}^{2}+(k-1)\\left(n+2+\\frac{k+1}{3p_{\\star}^{2}}\\right)\\right]\\delta.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Bound $|T_{1}T_{2}|$ under the event $S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta}$ . In this case, we may use that $n\\geq2[\\log_{2}(2/\\delta)+$ $m\\log(n+1)\\big]/p_{\\star}^{2}$ apply (33) from Prop. 14 to get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{B_{1},B_{2}\\right\\}\\leq\\frac{2}{p_{\\star}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}\\leq\\frac{1}{\\sqrt{n}}\\frac{1}{p_{\\star}}\\sqrt{2\\log_{2}(2/\\delta)+2m\\log(n+1)}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and the bounds based on $\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta}$ which give ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{|\\mathbb{G}_{n}^{(0)}(h_{1,k})|\\leq\\sqrt{2\\log(2/\\delta)}2k\\left\\|h\\right\\|_{\\infty}}\\\\ {|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j}\\ell)|\\leq\\sqrt{2\\log(2m k/\\delta)}2k\\left\\|h\\right\\|_{\\infty}}\\\\ {\\displaystyle\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})|\\leq\\displaystyle\\sum_{\\ell=2}^{k}\\sqrt{2\\log\\frac{2m k}{\\delta}}2(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}\\leq\\sqrt{2\\log\\frac{2m k}{\\delta}}k(k-1)\\left\\|h\\right\\|_{\\infty},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By plugging into (58), ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|T_{2}|\\le\\frac{m\\|h\\|_{\\infty}\\sqrt{2\\log(2m k/\\delta)}\\left[2\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]}{n p_{*}}k(k+1)+}\\\\ &{\\qquad\\xrightarrow{m\\|h\\|_{\\infty}\\,\\lceil2\\log_{2}(2/\\delta)+2m\\log(n+1)\\rceil}k(k-1)(k+4)}\\\\ &{\\qquad\\le\\frac{m k\\,\\|h\\|_{\\infty}\\,\\lceil\\log_{2}(2/\\delta)+\\,2m\\log(n+1)\\rceil^{1-1(k-1)/2}}{n p_{*}^{2}}\\,\\times}\\\\ &{\\qquad\\qquad\\bigg[p_{*}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)\\bigg]}\\\\ &{\\qquad\\textstyle\\prod_{T}T_{2}|\\le\\frac{2m k^{2}\\,\\|h\\|_{\\infty}^{2}\\,\\sqrt{2\\log(2/\\delta)}\\,\\bigl[\\log_{2}(2/\\delta)+2m\\log(n+1)\\bigr]^{1-1(k-1)/2}}{n^{3/2}p_{*}^{2}}\\,\\times}\\\\ &{\\qquad\\qquad\\bigg[p_{*}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)\\bigg]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In turn, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S\\setminus(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta})}\\right]\\le\\frac{2m k^{2}\\,\\|h\\|_{\\infty}^{2}\\,\\sqrt{2\\log(2/\\delta)}\\,[\\log_{2}(2/\\delta)+2m\\log(n+1)]^{1-1\\{k=1\\}/2}}{n^{3/2}p_{\\star}^{2}}\\times}\\\\ {\\quad\\left[p_{\\star}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)\\right],\\qquad\\qquad\\qquad(60,20)\\le\\epsilon\\le0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining together both (60) and (59) and using that $[\\log_{2}(2/\\delta)+2m\\log(n+1)]\\geq1$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}_{P}\\left[T_{1}T_{2}\\mathbb{1}s\\right]}\\\\ &{\\leq\\frac{2m k^{2}\\,\\|h\\|_{\\infty}^{2}\\,\\sqrt{2\\log(2/\\delta)}\\,[\\log_{2}(2/\\delta)+2m\\log(n+1)]^{1-\\mathbf{1}\\{k=1\\}/2}}{p_{\\star}^{2}}\\,\\times}\\\\ &{\\,\\left[\\frac{p_{\\star}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)}{n^{3/2}}+6\\left(4n p_{\\star}^{2}+(k-1)\\left(n+2+\\frac{k+1}{p_{\\star}^{2}}\\right)\\right)\\delta\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the result as desired. ", "page_idx": 32}, {"type": "text", "text": "We now combine the previous results to prove Thm. 24. ", "page_idx": 32}, {"type": "text", "text": "Theorem 24. For a sequence of rebalanced distributions $(\\tilde{P}_{n}{}^{(k)})_{k\\geq1}$ , there exists an absolute constant such that when $n\\ge C[\\log_{2}(2n/p_{\\star})+m\\log{(n+1)}]/p_{\\star}^{2}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[(\\tilde{P_{n}}^{(k)}(h)-P(h))^{2}]\\le\\frac{\\sigma_{k}^{2}}{n}+\\frac{C B}{n^{3/2}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\nB=\\frac{\\sqrt{\\log\\left(2n/p_{\\star}\\right)}m^{2}k^{4}\\left\\lVert h\\right\\rVert_{\\infty}^{2}}{p_{\\star}^{2}}\\;\\left(\\log_{2}\\frac{2n}{p_{\\star}}+m\\log\\left(n+1\\right)\\right)^{2-1\\left\\{k\\right\\}}\\;\\left(\\log\\frac{2m k n}{p_{\\star}}+\\frac{(k-1)^{2}}{p_{\\star}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We apply the decomposition (48), and subsequently handle the second term using bounds on the terms in (49). Set $\\delta=p_{\\star}^{\\bar{4}}/n^{4}$ . We apply Lem. 22 and Lem. 23 with this choice of $\\delta$ , so that there exists an absolute constants $\\tilde{C}$ , $C_{1}$ , and $C_{2}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[T_{1}T_{2}\\mathbb{1}_{S}\\right]\\le C_{1}\\displaystyle\\frac{\\|h\\|_{\\infty}^{2}\\,m^{2}k^{3}\\sqrt{\\log(2n/p_{\\star})}}{n^{3/2}p_{\\star}^{2}}\\left[\\log_{2}(2n/p_{\\star})+m\\log(n+1)\\right]^{1-1\\{k=1\\}/2}\\,\\times}\\\\ &{\\qquad\\qquad\\qquad\\bigg(\\log\\displaystyle\\frac{2m n k}{p_{\\star}}+\\frac{k-1}{p_{\\star}^{2}}\\bigg)}\\\\ &{\\mathbb{E}_{P}\\left[T_{2}^{2}\\mathbb{1}_{S}\\right]\\le C_{2}\\displaystyle\\frac{\\|h\\|_{\\infty}^{2}\\,m^{2}k^{4}}{n^{2}p_{\\star}^{2}}\\left[\\log_{2}(2n/p_{\\star})+m\\log(n+1)\\right]^{2-1\\{k=1\\}}\\,\\times}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\bigg(\\log\\displaystyle\\frac{2m n k}{p_{\\star}}+\\frac{(k-1)^{2}}{p_{\\star}^{2}}\\bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "when $n\\geq\\tilde{C}[\\log_{2}(2n/p_{\\star})+m\\log{(n+1)}]/p_{\\star}^{2}$ . This then implies that there is an absolute constant $C_{3}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left(\\tilde{P_{n}}^{(k)}(h)-P(h)\\right)^{2}\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[\\left(P_{n}^{(0)}(h)-P(h)\\right)^{2}\\mathbb{1}_{S^{c}}\\right]+\\frac{\\sigma_{k}^{2}}{n}\\,+}\\\\ &{\\,\\,\\,\\frac{C_{3}\\,\\|h\\|_{\\infty}^{2}\\,m^{2}k^{4}\\sqrt{\\log(2n/p_{\\star})}}{n^{3/2}p_{\\star}^{2}}\\left[\\log_{2}\\frac{2n}{p_{\\star}}+m\\log(n+1)\\right]^{2-1\\{k=1\\}}\\left(\\log\\frac{2m n k}{p_{\\star}}+\\frac{(k-1)^{2}}{p_{\\star}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, we apply Prop. 21 with the same choice of $\\delta$ . Because $2[\\log_{2}(2/\\delta)\\!+\\!m\\log(n\\!+\\!1)]\\geq\\log(m/\\delta)$ and $-\\log(\\bar{1}\\,-\\,p_{\\star}\\bar{)}\\,\\geq\\,p_{\\star}\\,\\geq\\,p_{\\star}^{2}$ , we have that $n\\,\\geq\\,\\log(\\delta/m)/\\log(1-p_{\\star})$ , which implies that $m(1-p_{\\star})^{n}\\leq\\delta$ . Combining with the display above, we have that there exists an absolute constant $C>0$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left(\\tilde{P_{n}}^{(k)}(h)-P(h)\\right)^{2}\\right]\\leq\\frac{\\sigma_{k}^{2}}{n}+\\frac{C\\left\\|h\\right\\|_{\\infty}^{2}m^{2}k^{4}\\sqrt{\\log(2n/p_{\\star})}}{n^{3/2}p_{\\star}^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left[\\log_{2}(2/\\delta)+m\\log(n+1)\\right]^{2-1\\{k=1\\}}\\left(\\log\\frac{2m n k}{p_{\\star}}+\\frac{(k-1)^{2}}{p_{\\star}^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is the claimed result. ", "page_idx": 33}, {"type": "text", "text": "While not shown in the main text, similar techniques to those used above can also control the bias of $\\tilde{P_{n}}^{(k)}(h)$ as in Thm. 25. Interestingly, this bias is of order $O(n^{-2})$ which confirms the intuition that even thought $\\tilde{P_{n}}^{(k)}(h)$ may be biased, the dominant term is the variance. ", "page_idx": 33}, {"type": "text", "text": "Theorem 25. For a sequence of rebalanced distributions $\\left(P^{(k)}\\right)_{k\\geq1}$ , there exists an absolute constant $C>0$ such that when $n\\geq C[\\log_{2}(2n/p_{\\star})+m\\log{(n+1)}]/p_{\\star}^{2}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{P}[\\tilde{P_{n}}^{(k)}(h)-P(h)]\\right|^{2}\\leq\\frac{C B}{n^{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $B$ is as defined in Thm. 24. ", "page_idx": 33}, {"type": "text", "text": "Proof. First, apply the decomposition (48) so that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\mathbb E_{P}\\left[\\tilde{P_{n}}^{(k)}(h)-P(h)\\right]\\right|\\leq\\left|\\mathbb E_{P}\\left[\\left(P_{n}(h)-P(h)\\right)\\mathbf{1}_{S^{c}}\\right]\\right|+\\left|\\mathbb E_{P}\\left[\\left(P_{n}^{(k)}(h)-P(h)\\right)\\mathbb{1}_{S}\\right]\\right|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By using the argument of Prop. 21, we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[P_{n}(h)-P(h)\\right]\\mathbf{1}_{S^{c}}|\\le2\\left\\|h\\right\\|_{\\infty}\\operatorname*{min}\\left\\{2m(1-p_{*})^{n},\\delta\\right\\}+\\sqrt{\\frac{2\\log(2/\\delta)}{n}}\\left\\|h\\right\\|_{\\infty}2m(1-p_{*})^{n}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, by the recursion formula Equation (36), we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\sqrt n\\left\\vert\\mathbb E_{P}\\left[\\left(P_{n}^{(k)}(h)-P(h)\\right)\\mathbb1_{S}\\right]\\right\\vert}\\\\ &{=\\left\\vert\\mathbb E_{P}\\left[\\mathbb G_{n}^{(k)}(h)\\mathbb1_{S}\\right]\\right\\vert=\\left\\vert\\mathbb E_{P}\\left[\\left(1-\\mathbb1_{S^{c}})\\mathbb G_{n}^{(0)}(\\mathcal C_{1}\\ldots\\mathcal C_{k}h)+\\sqrt n\\mathbb1_{S}\\underset{\\ell=1}{\\overset{k}{\\sum}}V_{n}^{(\\ell-1)}(\\mathcal C_{\\ell}\\ldots\\mathcal C_{k}h)\\right]\\right\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Because $\\mathbb{G}_{n}^{(0)}({\\mathcal{C}}_{1}\\ldots{\\mathcal{C}}_{k}h)$ has zero mean, it follows that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left|\\mathbb{E}_{P}\\left[\\left(P_{n}^{(k)}(h)-P(h)\\right)\\mathbb{1}_{S}\\right]\\right|\\leq\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S^{c}}\\mathbb{G}_{n}^{(0)}(\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)\\right]\\right|+\\sqrt{n}\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S}T_{2}\\right]\\right|\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We \u221ahave by Hoeffding\u2019s inequality that $\\mathbb{P}(\\mathcal{E}_{3}^{\\delta})\\geq1-\\delta$ , and that by Lem. 18 that $\\mathbb{G}_{n}^{(0)}\\big(\\mathcal{C}_{1}\\,.\\,.\\,.\\,\\mathcal{C}_{k}h\\big)\\leq$ $4k\\sqrt{n}\\left\\|h\\right\\|_{\\infty}$ universally. As a result, applying Prop. 21 once again, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S^{c}}\\mathbb{G}_{n}^{(0)}(\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)\\right]|}\\\\ &{\\le\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{n}^{\\delta}}\\mathbb{G}_{n}^{(0)}(\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)\\right]\\right|+\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S^{c}}\\mathbb{1}_{\\mathcal{E}_{n}^{\\delta}}\\mathbb{G}_{n}^{(0)}(\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)\\right]\\right|}\\\\ &{\\le4k\\sqrt{n}\\left\\|h\\right\\|_{\\infty}\\operatorname*{min}\\left\\{2m(1-p_{\\star})^{n},\\delta\\right\\}+\\sqrt{2\\log(2/\\delta)}2k\\left\\|h\\right\\|_{\\infty}2m(1-p_{\\star})^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using a similar argument to Lem. 22, we have that under $S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})$ (which occurs with probability no more than $2\\delta$ ), ", "page_idx": 34}, {"type": "equation", "text": "$$\n|T_{2}|\\leq\\|h\\|_{\\infty}\\,m k\\left[4n+\\frac{k-1}{p_{\\star}^{2}}\\left(n+2+\\frac{k+1}{3p_{\\star}^{2}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and that under $S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ (which occurs with probability at least $1-2\\delta)$ ), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|T_{2}\\right|\\leq\\frac{4m k\\,\\|h\\|_{\\infty}\\,\\left[\\log_{2}\\left(2/\\delta\\right)+2m\\log\\left(n+1\\right)\\right]^{1-1\\,\\left\\{k=1\\right\\}/2}}{n p_{\\star}^{2}}\\qquad}\\\\ {\\left[p_{\\star}\\sqrt{2\\log(2m k/\\delta)}(k+1)+(k-1)(k+4)\\right].\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Applying the decompositio $\\begin{array}{r}{\\mathfrak{n}\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S}T_{2}\\right]\\right|\\leq\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})}T_{2}\\right]\\right|+\\left|\\mathbb{E}_{P}\\left[\\mathbb{1}_{S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}}T_{2}\\right]\\right|}\\end{array}$ and setting $\\begin{array}{r}{\\delta=\\frac{p_{\\star}^{2}}{n^{2}}}\\end{array}$ achieves the desired result. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "D.5 Misspecified Marginal Distributions ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We now adapt the main results to cases in which the marginal distributions $(P_{X},P_{Y})$ are misspecified, in that the user is provided marginal distributions $(\\hat{P}_{X,\\epsilon},\\hat{P}_{Y,\\epsilon})$ which satisfy the following structure. ", "page_idx": 34}, {"type": "text", "text": "Assumption 1. There exist fixed probability mass functions ${\\hat{P}}_{X}$ and $\\hat{P}_{Y}$ for some $\\epsilon\\in[0,1)$ , ", "page_idx": 34}, {"type": "text", "text": "We also have the existence of the positive quantity ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{p}_{\\star}:=\\operatorname*{min}\\{\\operatorname*{min}_{x}\\hat{P}_{X}(x),\\operatorname*{min}_{y}\\hat{P}_{Y}(y)\\}>0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Given the existence of $\\hat{p}_{\\star}>0$ , we may also define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{p}_{{\\star},\\epsilon}=\\operatorname*{min}\\{\\operatorname*{min}_{x}\\hat{P}_{X,\\epsilon}(x),\\operatorname*{min}_{y}\\hat{P}_{Y,\\epsilon}(y)\\}\\geq\\epsilon\\hat{p}_{\\star}+(1-\\epsilon)p_{\\star}>0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To be precise, the iterations of balancing follow $\\hat{P}_{n}^{(0)}=P_{n}$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{P}_{n}^{(k)}(x,y):=\\left\\{\\begin{array}{l l}{\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}\\cdot\\hat{P}_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~odd}}\\\\ {\\frac{\\hat{P}_{Y,\\epsilon}(y)}{\\hat{P}_{n,Y}^{(k-1)}(y)}\\cdot\\hat{P}_{n}^{(k-1)}(x,y)}&{\\mathrm{~k~even}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We start by deriving a result similar to Prop. 15. Since $\\epsilon<1$ , the (possibly misspecified) target marginals $\\hat{P}_{X,\\epsilon}(x)>0$ and $\\hat{P}_{Y,\\epsilon}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ . Define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{V}_{n}^{(k-1)}(h):=\\left\\{\\sum_{x,y}\\left(\\frac{\\hat{P}_{X,\\epsilon}}{\\hat{P}_{n,X}^{(k-1)}}(x)-1\\right)h(x,y)\\hat{P}_{n}^{(k-1)}(x,y)\\quad\\mathrm{~k~odd~}\\quad\\epsilon(x,y)\\hat{P}_{n}^{(k-1)}(x,y)\\quad\\mathrm{~f~o~r~}\\quad x\\in[0,1],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{G}}_{n}^{(k)}(h):=\\sqrt{n}\\left(\\hat{P}_{n}^{(k)}(h)-P(h)\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The format of this section will be to derive analogous results to the building blocks of the previous section. From that point, the computations from Appx. D.4 will achieve the desired result. For the sake of comparison to Thm. 1 we consider error terms containing $\\epsilon$ only by their dependence on $(\\epsilon,k,n,\\hat{p}_{\\star,\\epsilon})$ . ", "page_idx": 35}, {"type": "text", "text": "D.5.1 Intermediate Results ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proposition 26. Let $\\big(\\hat{P}_{n}^{(k)}\\big)_{k\\geq1}$ be a sequence computed according to (63). Define ", "page_idx": 35}, {"type": "equation", "text": "$$\nc^{2}=\\operatorname*{max}\\left\\{\\chi^{2}(\\hat{P}_{X}\\|P_{X}),\\chi^{2}(\\hat{P}_{Y}\\|P_{Y})\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "These iterations are well-defined under the event $\\boldsymbol{S}$ , and for $\\mathbb{G}_{n}^{(k)}$ defined in (44), it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{G}}_{n}^{(k)}(h)=\\hat{\\mathbb{G}}_{n}^{(k-1)}(h)+\\sqrt{n}\\hat{V}_{n}^{(k-1)}(h)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{G}}_{n}^{(k)}(h)=\\hat{\\mathbb{G}}_{n}^{(k-1)}(\\mathcal{C}_{k}h)+\\sqrt{n}\\hat{V}_{n}^{(k-1)}(\\mathcal{C}_{k}h)+\\left\\{\\sqrt{n}[\\hat{P}_{X,\\epsilon}-P_{X}](\\mu_{X}h)\\quad\\psi k\\,o d d\\right.}\\\\ {\\left.\\sqrt{n}[\\hat{P}_{Y,\\epsilon}-P_{Y}](\\mu_{Y}h)\\quad\\psi k\\,e\\nu e n\\cdot\\psi_{k}^{(k-1)}(\\mu_{X}h)\\right\\}_{Z_{n}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{\\mathbb{G}}_{n}^{(k)}(h)\\right|\\leq\\left|\\hat{\\mathbb{G}}_{n}^{(k-1)}(\\mathcal{C}_{k}h)\\right|+\\sqrt{n}\\left|\\hat{V}_{n}^{(k-1)}(\\mathcal{C}_{k}h)\\right|+c\\left\\|h\\right\\|_{\\mathrm{L}^{2}(P)}\\sqrt{n\\epsilon}}\\\\ &{\\qquad\\qquad=\\left|\\hat{\\mathbb{G}}_{n}^{(k-1)}(\\mathcal{C}_{k}h)\\right|+\\sqrt{n}\\left|\\hat{V}_{n}^{(k-1)}(\\mathcal{C}_{k}h)\\right|+O\\left(\\sqrt{n\\epsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. The proof that $\\hat{P}_{n,X}^{(k-1)}(x)>0$ and $\\hat{P}_{n,Y}^{(k-1)}(y)>0$ for all $x\\in\\mathscr{X}$ and $y\\in\\mathcal{V}$ follows the exact same steps as in the proof of Prop. 15. We take this for granted and establish the recursion. ", "page_idx": 35}, {"type": "text", "text": "Consider the following steps in the case that $k$ is odd: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{P}_{n}^{(k)}(h)=\\displaystyle\\sum_{x,y}h(x,y)\\hat{P}_{n}^{(k)}(x,y)=\\displaystyle\\sum_{x,y}h(x,y)\\frac{\\hat{P}_{X,\\epsilon}}{\\hat{P}_{n,X}^{(k-1)}}(x)\\hat{P}_{n}^{(k-1)}(x,y)}\\\\ &{\\qquad=\\displaystyle\\sum_{x,y}1\\cdot h(x,y)\\hat{P}_{n}^{(k-1)}(x,y)+\\displaystyle\\sum_{x,y}\\left[\\frac{\\hat{P}_{X,\\epsilon}}{\\hat{P}_{n,X}^{(k-1)}}(x)-1\\right]\\cdot h(x,y)\\hat{P}_{n}^{(k-1)}(x,y)}\\\\ &{\\qquad=\\hat{P}_{n}^{(k-1)}(h)+\\hat{V}_{n}^{(k-1)}(h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Subtracting $P(h)$ on both sides, we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n[\\hat{P}_{n}^{(k)}-P](h)=[\\hat{P}_{n}^{(k-1)}-P](h)+\\hat{V}_{n}^{(k-1)}(h).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This proves the uncentered recursion formula given in (65). We then show the centered version. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\hat{P}_{n}^{(k)}-P](h)}\\\\ &{=[\\hat{P}_{n}^{(k)}-P](\\mathcal{C}_{X}h)+[\\hat{P}_{n}^{(k)}-P](\\mu_{X}h)}\\\\ &{=[\\hat{P}_{n}^{(k)}-P](\\mathcal{C}_{X}h)+[\\hat{P}_{X,\\epsilon}-P_{X}](\\mu_{X}h)}\\\\ &{=[\\hat{P}_{n}^{(k-1)}-P](\\mathcal{C}_{X}h)+\\hat{V}_{n}^{(k-1)}(\\mathcal{C}_{X}h)+[\\hat{P}_{X,\\epsilon}-P_{X}](\\mu_{X}h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, we bound the additional error term. By the Cauchy-Schwarz inequality, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\hat{P}_{X,\\epsilon}-P_{X}](\\mu_{X}h)\\leq\\left\\|\\frac{\\hat{P}_{X,\\epsilon}}{P_{X}}-\\mathbf{1}\\right\\|_{\\mathbf{L}^{2}(P_{X})}\\cdot\\left\\|\\mu_{X}h\\right\\|_{\\mathbf{L}^{2}(P_{X})}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\chi^{2}(\\hat{P}_{X,\\epsilon}\\|P_{X})}\\cdot\\left\\|\\mu_{X}h\\right\\|_{\\mathbf{L}^{2}(P_{X})}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\chi^{2}(\\hat{P}_{X,\\epsilon}\\|P_{X})}\\cdot\\left\\|h\\right\\|_{\\mathbf{L}^{2}(P)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as $\\mu_{X}$ is an orthogonal projection in $\\mathbf{L}^{2}(P)$ . Using convexity of $f$ -divergences, we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\chi^{2}(\\hat{P}_{X,\\epsilon}\\|P_{X})\\leq\\epsilon\\chi^{2}(\\hat{P}_{X}\\|P_{X})+(1-\\epsilon)\\chi^{2}(P_{X}\\|P_{X})=\\epsilon\\chi^{2}(\\hat{P}_{X}\\|P_{X}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This achieves the desired result. ", "page_idx": 36}, {"type": "text", "text": "Using similar ideas, we then prove an analog of Lem. 19. Lemma 27. For $k$ odd, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sqrt{n}\\hat{V}_{n}^{(k-1)}(\\mathcal C_{X}h)=\\sum_{j=1}^{m}\\left(\\frac{\\hat{P}_{X,\\epsilon}}{\\hat{P}_{n,X}^{(k-1)}}(x_{j})-1\\right)\\hat{\\mathbb{G}}_{n}^{(k-1)}(\\mathcal C_{X}h\\,\\mathbf{1}_{j k}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "whereas for $k$ even, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sqrt{n}\\hat{V}_{n}^{(k-1)}(\\mathcal{C}_{Y}h)=\\sum_{j=1}^{m}\\left(\\frac{\\hat{P}_{Y,\\epsilon}}{\\hat{P}_{n,Y}^{(k-1)}}(x_{j})-1\\right)\\hat{\\mathbb{G}}_{n}^{(k-1)}(\\mathcal{C}_{Y}h\\,\\mathbf{1}_{j k}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. We give the proof for $k$ odd. We claim that we need only show that $P(\\mathcal{C}_{X}h\\mathbf{1}_{j k})=0$ . This would show that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sqrt{n}\\dot{V}_{n}^{(k-1)}(C_{X}h)=\\displaystyle\\sum_{s,y}\\left(\\frac{\\widehat{P}_{X,c}}{\\widehat{P}_{n,X}^{(k-1)}}(x)-1\\right)\\big[\\mathcal{E}_{X}h\\big](x,y)\\widehat{P}_{n}^{(k-1)}(x,y)}\\\\ &{=\\sqrt{n}\\displaystyle\\sum_{j=1}^{n}x_{j}\\left(\\frac{\\widehat{P}_{X,c}}{\\widehat{P}_{n,X}^{(k-1)}}(x)-1\\right)\\big[\\mathcal{E}_{X}h\\,\\mathbf{1}_{j,k}\\big](x,y)\\widehat{P}_{n}^{(k-1)}(x,y)}\\\\ &{=\\sqrt{n}\\displaystyle\\sum_{j=1}^{m}x_{j}\\left(\\frac{\\widehat{P}_{X,c}}{\\widehat{P}_{n,X}^{(k-1)}}(x_{j})-1\\right)\\big[\\mathcal{E}_{X}h\\,\\mathbf{1}_{j,k}\\big](x,y)\\widehat{P}_{n}^{(k-1)}(x,y)}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\left(\\frac{\\widehat{P}_{X,c}}{\\widehat{P}_{n,X}^{(k-1)}}(x_{j})-1\\right)\\sqrt{n}\\sum_{x_{j}}\\big[\\mathcal{E}_{X}h\\,\\mathbf{1}_{j,k}(x,y)\\widehat{P}_{n}^{(k-1)}(x,y)}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\left(\\frac{\\widehat{P}_{X,c}}{\\widehat{P}_{n,X}^{(k-1)}}(x_{j})-1\\right)\\widehat{\\mathcal{E}}_{n}^{(k-1)}(\\mathcal{E}_{X}h\\,\\mathbf{1}_{j,k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $P(\\mathcal{C}_{X}h\\mathbf{1}_{j k})=0$ is employed in the last step. Now the result follows from (66) in Prop. 26 and the definition of $\\hat{\\mathbb{G}}_{n}^{(k)}(h)$ . To prove the claim, as in Lem. 19, write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathcal{C}_{X}h\\mathbf{1}_{j k}\\vert X\\right](x)=\\left\\{\\begin{array}{l l}{\\mathbb{E}\\left[\\mathcal{C}_{X}h\\vert X\\right](x_{j})}&{\\mathrm{~if~}x=x_{j}}\\\\ {0}&{\\mathrm{~if~}x\\ne x_{j}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "But $\\mathbb{E}\\left[{\\mathcal C}_{X}h|X\\right](x_{j})\\ =\\ 0$ by definition of $\\mathcal{C}_{X}$ . Taking an expectation over $P_{X}$ gives that $P(\\mathcal{C}_{X}h\\mathbf{1}_{j k})=0$ , which implies the desired result. The proof for $k$ even follows symmetrically. ", "page_idx": 36}, {"type": "text", "text": "For the remainder of the argument, we see that (67) can be unrolled so that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\hat{\\mathbb{G}}_{n}^{(k)}(h)\\right|\\leq\\underbrace{|\\mathbb{G}_{n}^{(0)}(\\mathcal{C}_{1}\\ldots\\mathcal{C}_{k}h)|}_{\\mathrm{first\\-order\\term}}+\\underbrace{\\sqrt{n}\\sum_{\\ell=1}^{k}\\left|\\hat{V}_{n}^{(\\ell-1)}(\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)\\right|}_{\\ell=1}+\\underbrace{O(k\\sqrt{n\\epsilon})}_{\\mathrm{misspecification}}\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we use that $\\mathbb{G}_{n}^{(0)}=\\hat{\\mathbb{G}}_{n}^{(0)}$ . ", "page_idx": 36}, {"type": "text", "text": "Next, we need to bound $\\left|\\hat{V}_{n}^{(\\ell-1)}(\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h)\\right|$ , in particular accounting for the marginal violation term.   \nWe follow similar steps as in the analysis of the higher-order term in Appx. D.3. ", "page_idx": 36}, {"type": "text", "text": "Proposition 28. Assume that $P_{n,X}(x)>0$ for all $x\\in\\mathscr{X}$ . It holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\right|\\leq\\left\\{\\operatorname*{max}\\{n-1,1\\}\\right.\\quad\\left.i f k=1\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In addition, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\bigg|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\bigg|\\leq\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(n\\sqrt{\\log\\frac{1}{1-\\epsilon}}\\right)+n\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}}&{i f k=1}\\\\ {\\mathcal{O}\\left(\\frac{1}{\\hat{p}_{*,\\epsilon}^{2}}\\sqrt{\\log\\frac{1}{1-\\epsilon}}\\right)+\\frac{1}{\\hat{p}_{*,\\epsilon}^{2}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}}&{i f k>1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, when $\\begin{array}{r}{\\mathrm{KL}(P_{n,X}\\|P_{X})\\leq\\frac{\\hat{p}_{\\star,\\epsilon}^{2}}{8}}\\end{array}$ and $\\begin{array}{r}{\\epsilon\\leq1-\\exp\\left(-\\frac{\\hat{p}_{\\star,\\epsilon}^{2}}{8}\\right)}\\end{array}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\in\\mathcal{X}}{\\operatorname*{max}}\\left|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\right|\\leq O\\left(\\frac{1}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\log\\frac{1}{1-\\epsilon}}\\right)+\\frac{2}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. First, observe that $\\hat{P}_{n,X}^{(0)}(x)=P_{n,X}^{(0)}(x)\\geq1/n$ under the event $\\boldsymbol{S}$ . For $k>1$ such that $k$ is odd, we have that for $x\\in\\mathscr{X}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{P}_{n,X}^{(\\kappa-1)}(x)=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\hat{P}_{n}^{(\\kappa-1)}(x,y)=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\frac{\\hat{P}_{Y,\\epsilon}(y)}{\\hat{P}_{n,Y}^{(k-2)}(y)}\\hat{P}_{n}^{(\\kappa-2)}(x,y)}\\\\ &{\\qquad\\qquad\\geq\\hat{p}_{\\star,\\epsilon}\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\hat{P}_{n}^{(\\ k-2)}(x,y)=\\hat{p}_{\\star,\\epsilon}\\hat{P}_{n,X}^{(\\kappa-2)}(x)=\\hat{p}_{\\star,\\epsilon}\\hat{P}_{X,\\epsilon}(x)\\geq\\hat{p}_{\\star,\\epsilon}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The result for $k$ even can be proven similarly. We now prove the inequalities listed in the statement using on the lower bounds above. ", "page_idx": 37}, {"type": "text", "text": "Proving the first inequality. For any $x\\in\\mathscr{X}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\right|=\\operatorname*{max}\\left\\{\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1,1-\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}\\right\\}\\leq\\left\\{\\operatorname*{max}\\{n-1,1\\}\\atop{\\operatorname*{max}\\{1/\\hat{p}_{\\star,\\epsilon}^{2}-1,1\\}}\\right.\\quad\\mathrm{if}\\;k>1\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which is the desired result. ", "page_idx": 37}, {"type": "text", "text": "Proving the second and third inequalities. Consider an odd $k\\geq1$ . By the definition of total variation distance, it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\hat{P}_{X,\\epsilon}(x)-\\hat{P}_{n,X}^{(k-1)}(x)\\right|\\le\\mathrm{TV}(\\hat{P}_{n,X}^{(k-1)},\\hat{P}_{X,\\epsilon}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "According to Pinsker\u2019s inequality, we have that $\\begin{array}{r}{\\mathrm{TV}(\\hat{P}_{n,X}^{(k-1)},\\hat{P}_{X,\\epsilon})\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(\\hat{P}_{n,X}^{(k-1)}\\|\\hat{P}_{X,\\epsilon})}}\\end{array}$ , and so we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\Big|\\hat{P}_{X,\\epsilon}(x)-\\hat{P}_{n,X}^{(k-1)}(x)\\Big|\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(\\hat{P}_{n,X}^{(k-1)}\\|\\hat{P}_{X,\\epsilon})}\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}^{(0)}\\|\\hat{P}_{X,\\epsilon})},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality follows by the monotonicity of Sinkhorn iterations given in Prop. 13. Notice that the remaining term is $\\mathrm{KL}(\\dot{P_{n,X}^{(0)}}\\|\\hat{P}_{X,\\epsilon})\\,=\\,\\mathrm{KL}(P_{n,X}\\|\\hat{P}_{X,\\epsilon})$ , which may not decay to zero as $n\\to\\infty$ . Because $\\epsilon<1$ , write ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(P_{n,X}\\|\\hat{P}_{X,\\epsilon})=\\displaystyle\\sum_{x\\in\\mathcal{X}}P_{n,X}(x)\\log\\displaystyle\\frac{P_{n,X}(x)}{(1-\\epsilon)P_{X}(x)+\\epsilon\\hat{P}_{X}(x)}}\\\\ &{\\phantom{\\mathrm{KL}}\\leq\\displaystyle\\sum_{x\\in\\mathcal{X}}P_{n,X}(x)\\log\\displaystyle\\frac{P_{n,X}(x)}{(1-\\epsilon)P_{X}(x)}}\\\\ &{=\\mathrm{KL}(P_{n,X}\\|P_{X})+\\log\\displaystyle\\frac{1}{1-\\epsilon}}\\\\ &{\\Longrightarrow\\displaystyle\\sqrt{\\frac{1}{2}\\mathrm{KL}(P_{n,X}\\|\\hat{P}_{X,\\epsilon})}\\leq\\sqrt{\\displaystyle\\frac{1}{2}\\mathrm{KL}(P_{n,X}\\|P_{X})+\\sqrt{\\displaystyle\\frac{1}{2}\\log\\displaystyle\\frac{1}{1-\\epsilon}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can then apply the lower bounds ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\right|\\leq\\left\\{\\begin{array}{l l}{n\\left(\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}+\\sqrt{\\frac{1}{2}\\log\\frac{1}{1-\\epsilon}}\\right)}&{\\mathrm{if~}k=1}\\\\ {\\frac{1}{\\hat{p}_{\\star,\\epsilon}^{2}}\\left(\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}+\\sqrt{\\frac{1}{2}\\log\\frac{1}{1-\\epsilon}}\\right)}&{\\mathrm{if~}k>1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, combining the arguments above, we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\in\\mathcal{X}}{\\operatorname*{max}}\\left|\\hat{P}_{X,\\epsilon}(x)-\\hat{P}_{n,X}^{(k-1)}(x)\\right|\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}+\\sqrt{\\frac{1}{2}\\log\\frac{1}{1-\\epsilon}}}\\\\ &{\\phantom{\\sum_{x\\in\\mathcal{X}}\\left|\\hat{P}_{X,\\epsilon}(x)-\\hat{P}_{n,X}^{(k-1)}(x)\\right|}\\leq\\frac{\\hat{p}_{\\star,\\epsilon}}{4}+\\frac{\\hat{p}_{\\star,\\epsilon}}{4}=\\frac{\\hat{p}_{\\star,\\epsilon}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last step invoked the assumption that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(P_{n,X}\\|P_{X})\\leq\\frac{\\hat{p}_{\\star,\\epsilon}^{2}}{8}\\quad\\mathrm{and}\\quad\\epsilon\\leq1-\\exp\\left(-\\frac{\\hat{p}_{\\star,\\epsilon}^{2}}{8}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This means that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathcal{X}}\\hat{P}_{n,X}^{(k-1)}(x)\\geq\\operatorname*{min}_{x\\in\\mathcal{X}}\\hat{P}_{X,\\epsilon}(x)-\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\hat{P}_{n,X}^{(k-1)}(x)-\\hat{P}_{X,\\epsilon}(x)\\right|\\geq\\frac{\\hat{p}_{\\star,\\epsilon}}{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\bigg|\\frac{\\hat{P}_{X,\\epsilon}(x)}{\\hat{P}_{n,X}^{(k-1)}(x)}-1\\bigg|\\leq\\frac{\\operatorname*{max}_{x\\in\\mathcal{X}}\\Big|\\hat{P}_{n,X}^{(k-1)}(x)-\\hat{P}_{X,\\epsilon}(x)\\Big|}{\\operatorname*{min}_{x\\in\\mathcal{X}}\\hat{P}_{n,X}^{(k-1)}(x)}\\leq\\frac{2}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|\\hat{P}_{X,\\epsilon})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now, for $k$ even, set $k=2t$ for $t\\geq0$ . We have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in\\mathcal{Y}}\\Big|\\hat{P}_{n,Y}^{(2t-1)}(y)-\\hat{P}_{Y,\\epsilon}(y)\\Big|\\leq\\mathrm{TV}(\\hat{P}_{n,Y}^{(2t-1)},\\hat{P}_{Y,\\epsilon})\\leq\\sqrt{\\frac{1}{2}\\operatorname{KL}(\\hat{P}_{Y,\\epsilon}\\|\\hat{P}_{n,Y}^{(2t-1)})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Invoke Prop. 13 once again to achieve ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{2}\\operatorname{KL}(\\hat{P}_{Y,\\epsilon}\\|\\hat{P}_{n,Y}^{(2t-1)})}\\le\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|\\hat{P}_{X,\\epsilon})}\\le\\sqrt{\\frac{1}{2}\\operatorname{KL}(P_{n,X}\\|P_{X})}+\\sqrt{\\frac{1}{2}\\log\\frac{1}{1-\\epsilon}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which completes the proof. ", "page_idx": 38}, {"type": "text", "text": "Proceeding with similar steps, define the quantities ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{B}_{1}:=\\hat{M}_{1}\\quad\\mathrm{~and~}\\quad\\hat{B}_{2}:=\\operatorname*{max}_{2\\leq\\ell\\leq k}\\hat{M}_{\\ell}\\quad\\mathrm{~for~}\\quad\\hat{M}_{\\ell}:=\\left\\{\\begin{array}{l l}{\\operatorname*{max}_{x\\in\\mathcal{X}}\\left|\\frac{\\hat{P}_{X,\\ell}(x)}{\\hat{P}_{n,X}^{\\ell-1}(x)}-1\\right|}&{\\ell\\mathrm{~odd}}\\\\ {\\operatorname*{max}_{y\\in\\mathcal{Y}}\\left|\\frac{\\hat{P}_{Y,\\ell}(y)}{\\hat{P}_{n,Y}^{\\ell-1}(y)}-1\\right|}&{\\ell\\mathrm{~even}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We must now establish an analog of Prop. 20. ", "page_idx": 38}, {"type": "text", "text": "Proposition 29. For any $k\\geq1$ , the following holds under the event $\\boldsymbol{S}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sqrt{n}\\sum_{\\ell=1}^{k}\\Big|\\hat{V}_{n}^{(\\ell-1)}(\\mathcal C_{\\ell}\\ldots\\mathcal C_{k}h)\\Big|\\leq\\sum_{j=1}^{m}\\Bigg(\\hat{B}_{1}\\,|\\mathbb{G}_{n}^{(0)}(h_{1,k}{\\bf1}_{j\\ell})|+\\hat{B}_{2}\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}{\\bf1}_{j\\ell})|\\Bigg)}}\\\\ &{}&{\\quad+\\,m\\hat{B}_{2}\\,\\|h\\|_{\\infty}\\,\\sqrt{n}k(k-1)[\\hat{B}_{1}+\\hat{B}_{2}(k+1)/3].~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. This proof largely follows the argument of Prop. 20, while accounting for the misspecified marginal error. Using again the notation $h_{\\ell,k}:=\\mathcal{C}_{\\ell}\\ldots\\mathcal{C}_{k}h$ , it follows from Lem. 27 that, for odd $\\ell$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sqrt{n}\\hat{V}_{n}^{(\\varepsilon-1)}(h_{\\ell,k})=\\sum_{j=1}^{m}\\left[\\frac{\\hat{P}_{X,\\epsilon}}{\\hat{P}_{n,X}^{(\\varepsilon-1)}}(x_{j})-1\\right]\\hat{\\mathbb{G}}_{n}^{(\\varepsilon-1)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\leq\\hat{M}_{\\ell}\\sum_{j=1}^{m}\\left|\\hat{\\mathbb{G}}_{n}^{(\\varepsilon-1)}(h_{\\ell,k}\\,{\\mathbf{1}}_{j\\ell})\\right|.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The bound above holds for $\\ell$ even as well. Then, using (65) from Prop. 26 along with the triangle inequality, we have that for $\\ell\\geq2$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|[\\hat{P}_{n}^{(\\ell-1)}-P](h_{\\ell,k}\\mathbf{1}_{j\\ell})\\right|\\leq\\left|\\hat{P}_{n}^{(\\ell-2)}-P](h_{\\ell,k}\\mathbf{1}_{j\\ell})\\right|+\\left|\\hat{V}_{n}^{(\\ell-2)}(h_{\\ell,k}\\mathbf{1}_{j\\ell})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{\\mathbb{G}}_{n}^{(\\ell-1)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|}\\\\ &{\\leq\\left|\\hat{\\mathbb{G}}_{n}^{(\\ell-2)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|+\\sqrt{n}\\left|\\hat{V}_{n}^{(\\ell-2)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|}\\\\ &{\\leq\\left|\\hat{\\mathbb{G}}_{n}^{(0)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|+\\sqrt{n}\\left|\\hat{V}_{n}^{(0)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|+\\ldots+\\sqrt{n}\\left|\\hat{V}_{n}^{(\\ell-2)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|}\\\\ &{\\leq\\left|\\hat{\\mathbb{G}}_{n}^{(0)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|+\\hat{M}_{1}\\sqrt{n}\\hat{P}_{n}^{(0)}(|h_{\\ell,k}|{\\mathbf{1}}_{j\\ell})+\\ldots+\\hat{M}_{\\ell}\\sqrt{n}\\hat{P}_{n}^{(\\ell-2)}(|h_{\\ell,k}|{\\mathbf{1}}_{j\\ell})}\\\\ &{\\leq\\left|\\hat{\\mathbb{G}}_{n}^{(0)}(h_{\\ell,k}{\\mathbf{1}}_{j\\ell})\\right|+2\\left\\|h\\right\\|_{\\infty}\\sqrt{n}\\left[\\hat{B}_{1}+\\hat{B}_{2}(\\ell-1)\\right](k-\\ell+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "by Lem. 18 and $\\hat{M}_{1}\\leq\\hat{B}_{1}$ and $\\hat{M}_{\\ell}\\,\\leq\\,\\hat{B}_{2}$ for $\\ell\\geq2$ . The bound above holds trivially for $\\ell=1$ . Summing these bounds over $\\ell$ and $j$ , we have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\displaystyle\\sum_{t=1}^{k}\\left|\\hat{c}_{u}^{t}-\\hat{\\nu}_{u}^{t}\\right|(h_{\\delta})\\right|}\\\\ &{\\phantom{\\hat{c}_{u}^{t}\\displaystyle\\sum_{t=1}^{k}\\left|\\hat{c}_{v}^{t}\\hat{w}_{t,1}\\hat{u}_{t,1}\\hat{y}_{t}\\right|+\\underbrace{1}_{\\displaystyle{\\sum_{t=1}^{k}\\hat{u}_{t}}\\hat{u}_{t}^{t}\\displaystyle\\sum_{\\rho=1}^{k}\\left|\\hat{c}_{u}^{t,\\prime}-\\hat{\\nu}_{u}^{t}\\right|(h_{\\delta},1_{\\rho})}_{\\displaystyle{t=1}^{k}\\displaystyle\\sum_{t=1}^{k}\\Big|\\hat{c}_{u}^{t,\\prime}-\\hat{\\nu}_{u}^{t}\\Big|(h_{\\delta},1_{\\rho})}}\\\\ &{\\leq\\delta_{1}\\displaystyle\\sum_{t=1}^{k}\\left|\\hat{c}_{u}^{t}\\hat{w}_{t,1}\\hat{u}_{t,1}\\right|+\\delta_{2}\\displaystyle\\sum_{t=2}^{k}\\sum_{t=1}^{k}\\left|\\hat{c}_{u}^{t,\\prime}-\\hat{\\nu}_{u}^{t}\\right|(h_{\\delta},1_{\\rho})}\\\\ &{\\leq\\delta_{1}\\displaystyle\\sum_{t=1}^{k}\\left|\\hat{c}_{u}^{t}\\hat{w}_{t,1}\\hat{u}_{t,1}\\right|}\\\\ &{\\phantom{\\hat{c}_{u}^{t}\\displaystyle\\sum_{t=1}^{k}\\hat{w}_{t,2}}\\left(\\hat{b}_{u}^{t}\\delta_{1,1}h_{\\delta}\\right)}\\\\ &{\\phantom{\\hat{b}_{\\rho}^{t}\\displaystyle\\sum_{t=1}^{k}\\hat{w}_{t,2}}\\displaystyle\\sum_{t=1}^{k}\\left(\\hat{b}_{u}^{t}\\delta_{1,1}h_{\\rho}\\right)+2\\left|1\\right|h_{\\delta}\\sqrt{n}\\left[\\hat{b}_{t}+\\hat{b}_{2}(t-1)\\right](k-\\ell+1)\\right)\\mathrm{~~apby~ot~}}\\\\ &{=\\displaystyle\\sum_{t=1}^{k}\\left(\\hat{b}_{1}\\lvert\\hat{c}_{u}^{t}(h_{1,1,2})\\rvert+\\hat{b}_{2}\\displaystyle\\sum_{t=2}^{k}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "because $|{\\boldsymbol{\\mathcal{X}}}|=m$ . We sum the last term: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\ell=2}^{k}\\left[\\hat{B}_{1}+\\hat{B}_{2}(\\ell-1)\\right](k-\\ell+1)=\\hat{B}_{1}\\sum_{\\ell=1}^{k-1}(k-\\ell)+\\hat{B}_{2}\\sum_{\\ell=1}^{k-1}\\ell(k-\\ell)}}\\\\ &{}&{=\\frac{k(k-1)}{2}\\left[\\hat{B}_{1}+\\hat{B}_{2}(k+1)/3\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which completes the proof. ", "page_idx": 39}, {"type": "text", "text": "D.5.2 Mean Squared Error Bound ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Ultimately, we wish to construct an upper bound for ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\left(\\hat{P}_{n}^{(k)}(h)-P(h)\\right)^{2}\\mathbb{1}_{S}\\right]+\\mathbb{E}_{P}\\left[\\left(P_{n}(h)-P(h)\\right)^{2}\\mathbb{1}_{S^{c}}\\right],\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "as the method returns $P_{n}(h)$ when $\\boldsymbol{S}$ is not satisfied. The first term will be controlled by intermediate tools developed above. The second term that includes $S^{c}$ is no different from the one analyzed in Prop. 21. We handle the second term first. Recall from Prop. 21 that for any $\\delta\\in(0,1)$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left(P_{n}(h)-P(h)\\right)^{2}\\mathbf{1}_{S^{c}}\\right]\\leq}\\\\ &{\\quad4\\left\\|h\\right\\|_{\\infty}^{2}\\operatorname*{min}\\left\\{2m(1-p_{\\star})^{n},\\delta\\right\\}+\\frac{2\\log(2/\\delta)}{n}\\left\\|h\\right\\|_{\\infty}^{2}2m(1-p_{\\star})^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Repeat the argument from the proof of Thm. 24: because $2[\\log_{2}(2/\\delta)+m\\log(n+1)]\\geq\\log(m/\\delta)$ and $-\\log(1-p_{\\star})\\geq p_{\\star}\\geq p_{\\star}^{2}$ , we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\nn\\geq2\\lceil\\log_{2}(2/\\delta)+m\\log{(n+1)}\\rceil/p_{\\star}^{2}\\implies n\\geq\\log(\\delta/m)/\\log(1-p_{\\star}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This in turn implies that $m(1-p_{\\star})^{n}\\leq\\delta$ , and gives as a condition on the sample size $n$ . Further in the analysis, we will set $\\delta=(\\hat{p}_{\\star,\\epsilon}/n)^{4}$ , so right-hand side of (75) can then be upper bounded further, resulting in ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P}\\left[\\left(P_{n}(h)-P(h)\\right)^{2}\\mathbf{1}_{S^{c}}\\right]\\leq4\\left\\|h\\right\\|_{\\infty}^{2}\\delta\\left(2+\\displaystyle\\frac{\\log(2/\\delta)}{n}\\right)=\\tilde{O}\\left(\\frac{\\hat{p}_{\\star,\\epsilon}^{4}}{n^{4}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "a higher-order term compared to other components of the bound. ", "page_idx": 40}, {"type": "text", "text": "Next, we must control the left-hand side of (74). We perform the decomposition based on (69): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left(\\hat{P}_{n}^{(\\kappa)}(h)-P(h)\\right)^{2}\\mathbb{1}_{S}\\right]}\\\\ &{\\leq\\mathbb{E}_{P}\\left[T_{1}^{2}\\mathbb{1}_{S}\\right]+2\\mathbb{E}_{P}\\left[\\left|T_{1}\\hat{T}_{2}\\right|\\mathbb{1}_{S}\\right]+\\mathbb{E}_{P}\\left[\\hat{T}_{2}^{2}\\mathbb{1}_{S}\\right]}\\\\ &{\\quad+\\left.O(k\\sqrt{\\epsilon})\\cdot\\mathbb{E}_{P}\\left[\\left(\\left|T_{1}\\right|+\\left|\\hat{T}_{2}\\right|\\right)\\mathbb{1}_{S}\\right]+O(k^{2}\\epsilon)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for ", "page_idx": 40}, {"type": "equation", "text": "$$\nT_{1}:=[P_{n}-P]({\\mathcal{C}}_{1}\\ldots{\\mathcal{C}}_{k}h){\\mathrm{~and~}}{\\hat{T}}_{2}:=\\sum_{\\ell=1}^{k}\\left|{\\hat{V}}_{n}^{(\\ell-1)}({\\mathcal{C}}_{\\ell}\\ldots{\\mathcal{C}}_{k}h)\\right|.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Recall the events $\\mathcal{E}_{1}^{\\delta}$ and $\\mathcal{E}_{2}^{\\delta}$ and $\\mathcal{E}_{3}^{\\delta}$ from Appx. D.4. To perform this computation efficiently, we will split the bounds on each term into two components. In particular, we will show that ", "page_idx": 40}, {"type": "text", "text": "\u2022 Under the event $S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}:\\left|\\hat{T}_{2}\\right|\\leq T_{2}+E_{2},$ \u2022 Under the event $S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}):\\left|\\hat{T}_{2}\\right|\\leq T_{2}^{c}+E_{2}^{c},$ , \u2022 Under the event $S\\cap\\mathcal{E}_{3}^{\\delta}:|T_{1}|\\leq\\tau_{1}$ , \u2022 Under the event $S\\backslash\\mathcal{E}_{3}^{\\delta}:|T_{1}|\\leq T_{1}^{c}$ , ", "page_idx": 40}, {"type": "text", "text": "where any term denoted with $\\Lleftarrow\\Lleftarrow\\Lleftarrow$ will represent all error terms that include $\\epsilon$ and will be written in big- $O$ notation. There are no errors for the bounds on $T_{1}$ , as this term does not depend on the misspecified marginals. The idea is that for the ${\\sqrt[\\object Object]{_{2}}}^{,}$ terms we may reuse the bounds derived in Appx. D.4 by simply replacing $p_{\\star}$ with $\\hat{p}_{\\star,\\epsilon}$ . This is due to the fact that the dependence of the analogous terms from Appx. D.4 depend on $p_{\\star}$ only through Prop. 14; similarly, the corresponding terms in this section depend on $\\hat{p}_{\\star,\\epsilon}$ through Prop. 28. We return to the terms in (77) and (78). ", "page_idx": 40}, {"type": "text", "text": "Decomposing on $\\mathcal{E}_{3}^{\\delta}$ will result in a bound of the form ", "page_idx": 40}, {"type": "equation", "text": "$$\nO(k\\sqrt{\\epsilon})\\cdot\\mathbb{E}_{P}\\left[|T_{1}|\\mathbb{1}s\\right]\\leq O(k\\sqrt{\\epsilon})\\cdot(\\delta T_{1}^{c}+\\mathcal{T}_{1})\\,.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Decomposing on $\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ will result in a bounds of the form ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\mathbb{E}_{P}\\left[\\hat{T}_{2}^{2}\\mathbb{1}_{S}\\right]\\leq2\\delta(T_{2}^{c})^{2}+\\mathcal{T}_{2}^{2}+\\tilde{O}\\left(\\delta\\left((E_{2}^{c})^{2}+E_{2}^{c}\\mathcal{T}_{2}^{c}\\right)+\\left(E_{2}^{2}+E_{2}\\mathcal{T}_{2}\\right)\\right)}\\\\ {O(k\\sqrt{\\epsilon})\\cdot\\mathbb{E}_{P}\\left[|\\hat{T}_{2}|\\mathbb{1}_{S}\\right]\\leq O(k\\sqrt{\\epsilon})\\cdot\\left(\\delta\\left(\\mathcal{T}_{2}^{c}+E_{2}^{c}\\right)+\\mathcal{T}_{2}+E_{2}\\right).\\qquad\\qquad}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Finally, decomposing on $\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}\\cap\\mathcal{E}_{3}^{\\delta}$ will result in a bound of the form ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\left|T_{1}\\hat{T}_{2}\\right|\\mathbb{1}_{S}\\right]\\leq3\\delta{\\mathcal{T}_{1}^{c}}\\mathcal{T}_{2}^{c}+\\mathcal{T}_{1}\\mathcal{T}_{2}+\\tilde{O}\\left(\\delta{\\mathcal{T}_{1}^{c}}E_{2}^{c}+\\mathcal{T}_{1}E_{2}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The leading terms $2\\delta(\\mathcal{T}_{2}^{c})^{2}+\\mathcal{T}_{2}^{2}$ and $3\\delta T_{1}^{c}T_{2}^{c}+T_{1}T_{2}$ from both bounds should have the exact same form as the terms in Lem. 22 and Lem. 23, with $p_{\\star}$ replaced by $\\hat{p}_{\\star,\\epsilon}$ , thus retaining the same dependence on $(n,k)$ . By setting $\\delta=\\hat{p}_{\\star,\\epsilon}^{4}/n^{4}$ , we will achieve a similar result to Thm. 24, i.e., that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\left[\\left(\\hat{P}_{n}^{(k)}(h)-P(h)\\right)^{2}\\mathbb{1}_{\\mathcal{S}}\\right]}\\\\ &{\\leq\\frac{\\sigma_{k}^{2}}{n}+\\tilde{O}\\left(\\frac{k^{6}}{n^{3/2}}\\right)}\\\\ &{+\\tilde{O}\\Big((\\hat{p}_{\\star,\\epsilon}/n)^{4}\\,(E_{2}^{c}(E_{2}^{c}+T_{2}^{c}))+E_{2}\\,(E_{2}+T_{2})+(\\hat{p}_{\\star,\\epsilon}/n)^{4}\\mathcal{T}_{1}^{c}E_{2}^{c}+\\mathcal{T}_{1}E_{2}\\Big).}\\\\ &{+\\,\\tilde{O}\\left(k\\sqrt{\\epsilon}\\left((\\hat{p}_{\\star,\\epsilon}/n)^{4}\\mathcal{T}_{1}^{c}+\\mathcal{T}_{1}+(\\hat{p}_{\\star,\\epsilon}/n)^{4}\\,(\\mathcal{T}_{2}^{c}+E_{2}^{c})+\\mathcal{T}_{2}+E_{2}\\right)+k^{2}\\epsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "It remains to quantify the $\\tilde{O}$ terms by computing the order of the 6 constants $(T_{2},E_{2},T_{2}^{c},E_{2}^{c},T_{1},T_{1}^{c})$ .   \nWe follow similar steps to Lem. 22 and Lem. 23 to achieve this. ", "page_idx": 41}, {"type": "text", "text": "Lemma 30. For $\\delta\\;=\\;(\\hat{p}_{\\star,\\epsilon}/n)^{4}$ , assume that $n\\,\\geq\\,8[\\log_{2}(2/\\delta)+m\\log(n+1)]/\\hat{p}_{\\star,\\epsilon}^{2}$ and $\\epsilon\\,\\leq$ $1-\\exp\\left(-\\frac{\\hat{p}_{\\star,\\epsilon}^{2}}{8}\\right)$ . Then, it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{2}^{c}=\\tilde{O}\\left(\\frac{k^{2}}{\\hat{p}_{\\star,\\epsilon}^{2}}\\left(n+\\frac{k}{\\hat{p}_{\\star,\\epsilon}^{2}}\\right)\\right),\\quad E_{2}^{c}=0}\\\\ &{\\mathcal{T}_{2}=\\tilde{O}\\left(\\frac{k^{3}}{n\\hat{p}_{\\star,\\epsilon}^{2}}\\right),\\quad E_{2}=\\tilde{O}\\left(\\frac{k^{3}}{\\hat{p}_{\\star,\\epsilon}^{2}}\\left(\\sqrt{\\frac{1}{n}\\log\\frac{1}{1-\\epsilon}}+\\log\\frac{1}{1-\\epsilon}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. The following computations are done under the event $\\boldsymbol{S}$ . First, apply Prop. 29 to write ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\sqrt{n}\\left\\vert\\hat{T}_{2}\\right\\vert\\leq\\displaystyle\\sum_{j=1}^{m}\\left(\\hat{B}_{1}\\left\\vert\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,\\mathbf{1}_{j\\ell})\\right\\vert+\\hat{B}_{2}\\displaystyle\\sum_{\\ell=2}^{k}\\left\\vert\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,\\mathbf{1}_{j\\ell})\\right\\vert\\right)}\\\\ {\\displaystyle~~~~~~~~~~~~~~+m\\hat{B}_{2}\\left\\Vert h\\right\\Vert_{\\infty}k(k-1)[\\hat{B}_{1}+\\hat{B}_{2}(k+1)/3].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We decompose on the event $\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ . ", "page_idx": 41}, {"type": "text", "text": "Bound $\\left|T_{2}\\right|$ under the event $S\\backslash(\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta})$ . In this case, we apply (70) from Prop. 28 to get $\\hat{B}_{1}\\leq n$ and $\\hat{B}_{2}\\leq1/\\hat{p}_{\\star,\\epsilon}^{2}$ , along with the universal bounds from Lem. 18: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{\\sqrt{n}}\\left|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\left\\|h_{1,k}\\right\\|_{\\infty}\\leq4k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{\\ell=2}^{k}\\left|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})\\right|\\leq2\\sum_{\\ell=2}^{k}\\left\\|h_{\\ell,k}\\right\\|_{\\infty}\\leq\\sum_{\\ell=2}^{k}4(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}=2k(k-1)\\left\\|h\\right\\|_{\\infty}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "so that by plugging into (82), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\hat{T}_{2}\\right|\\leq\\underbrace{\\|h\\|_{\\infty}\\,m k\\left[4n+\\frac{k-1}{\\hat{p}_{\\star,\\epsilon}^{2}}\\left(n+2+\\frac{k+1}{3\\hat{p}_{\\star,\\epsilon}^{2}}\\right)\\right]}_{T_{2}^{c}}+\\underbrace{0}_{E_{2}^{c}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Bound $\\left|{{T_{2}}}\\right|$ under the event $S\\cap\\mathcal{E}_{1}^{\\delta}\\cap\\mathcal{E}_{2}^{\\delta}$ . In this case, we may use that $n\\,\\ge\\,8/\\hat{p}_{\\star,\\epsilon}^{2}$ (because $[\\log_{2}(2/\\delta)+m\\log(n+1)]\\geq1$ for $\\delta\\in(0,1)$ ) and apply (71) from Prop. 28 to get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\left\\{\\hat{B}_{1},\\hat{B}_{2}\\right\\}\\leq O\\left(\\frac{1}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\log\\frac{1}{1-\\epsilon}}\\right)+\\frac{2}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\frac{2\\log_{2}(2/\\delta)+2m\\log(n+1)}{2n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The bounds based on $\\mathcal{E}_{2}^{\\delta}$ give ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,{\\bf1}_{j\\ell})|\\leq\\sqrt{2\\log\\frac{2m k}{\\delta}}2k\\left\\|h\\right\\|_{\\infty}}}\\\\ {{\\displaystyle\\sum_{\\ell=2}^{k}|\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,{\\bf1}_{j\\ell})|\\leq\\sum_{\\ell=2}^{k}\\sqrt{2\\log\\frac{2m k}{\\delta}}2(k-\\ell+1)\\left\\|h\\right\\|_{\\infty}\\leq\\sqrt{2\\log\\frac{2m k}{\\delta}}k(k-1)\\left\\|h\\right\\|_{\\infty}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By plugging into (82), we can reuse the steps in the bound from (56) (for all terms without $\\epsilon$ ) to write ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\hat{T}_{2}\\right|\\leq\\frac{4m k\\,\\|h\\|_{\\infty}\\,\\left[\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]^{1-1\\,\\{k=1\\}/2}}{n\\hat{p}_{\\star,\\epsilon}^{2}}\\times}\\\\ {\\left[\\hat{p}_{\\star,\\epsilon}\\sqrt{2\\log\\left(2m k/\\delta\\right)}(k+1)+(k-1)(k+4)\\right]+E_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "so that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{2}=\\frac{4m k\\,\\|h\\|_{\\infty}\\,\\left[\\log_{2}(2/\\delta)+2m\\log(n+1)\\right]^{1-\\mathbb{1}\\left\\{k=1\\right\\}/2}}{n\\hat{p}_{\\star,\\epsilon}^{2}}}\\\\ {\\times\\left[\\hat{p}_{\\star,\\epsilon}\\sqrt{2\\log\\left(2m k/\\delta\\right)}(k+1)+(k-1)(k+4)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We compute $E_{2}$ by using that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{max}\\left\\{\\hat{B}_{1},\\hat{B}_{2}\\right\\}\\leq O\\left(\\frac{1}{\\hat{p}_{\\star,\\epsilon}}\\sqrt{\\log\\frac{1}{1-\\epsilon}}\\right)+\\tilde{O}\\left(\\frac{1}{\\hat{p}_{\\star,\\epsilon}\\sqrt{n}}\\right)}\\\\ &{\\quad\\vert\\mathbb{G}_{n}^{(0)}(h_{1,k}\\,\\mathbf{1}_{j\\ell})\\vert\\leq\\tilde{O}\\left(k\\right)}\\\\ &{\\displaystyle\\sum_{\\ell=2}^{k}\\vert\\mathbb{G}_{n}^{(0)}(h_{\\ell,k}\\,\\mathbf{1}_{j\\ell})\\vert\\leq\\tilde{O}\\left(k^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which gives ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{2}=\\tilde{O}\\left(\\frac{k^{3}}{\\hat{p}_{\\star,\\epsilon}^{2}}\\left(\\sqrt{\\frac{1}{n}\\log\\frac{1}{1-\\epsilon}}+\\log\\frac{1}{1-\\epsilon}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We now make the corresponding argument for the term $T_{1}$ ", "page_idx": 42}, {"type": "text", "text": "Lemma 31. For $\\delta=(\\hat{p}_{\\star,\\epsilon}/n)^{4}$ , it holds that ", "page_idx": 42}, {"type": "equation", "text": "$$\nT_{1}^{c}=\\tilde{O}(k),\\quad T_{1}=\\tilde{O}\\left(\\frac{k}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. The following computations are done under the event $\\boldsymbol{S}$ . ", "page_idx": 42}, {"type": "text", "text": "Bound $\\left|T_{1}\\right|$ under the event $S\\backslash\\mathcal{E}_{3}^{\\delta}$ . Here we simply apply a universal bound on the empirical process term: ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{n}}}\\left\\vert\\mathbb{G}_{n}^{\\scriptscriptstyle(0)}(h_{1,k})\\right\\vert\\leq2\\left\\|h_{1,k}\\right\\|_{\\infty}\\leq4k\\left\\|h\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "so that ${\\cal T}_{1}^{c}=4k\\left\\|h\\right\\|_{\\infty}$ ", "page_idx": 42}, {"type": "text", "text": "Bound $\\left|T_{1}\\right|$ under the event $S\\cap\\mathcal{E}_{3}^{\\delta}$ . Now, we may use the definition of the event $\\mathcal{E}_{3}^{\\delta}$ to achieve ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{n}}\\left\\vert\\mathbb{G}_{n}^{\\mathrm{(0)}}(h_{1,k})\\right\\vert\\leq\\sqrt{\\frac{2\\log(2/\\delta)}{n}}2k\\left\\|h\\right\\|_{\\infty}=\\mathcal{T}_{1}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Knowing that $E_{2}^{c}=0$ , we simplify (80) and (80) to read ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{O}\\Big(E_{2}\\left(E_{2}+\\mathcal{T}_{2}+\\mathcal{T}_{1}\\right)\\Big)}\\\\ &{\\tilde{O}\\left(k\\sqrt{\\epsilon}\\left((\\hat{p}_{\\star,\\epsilon}/n)^{4}\\mathcal{T}_{1}^{c}+\\mathcal{T}_{1}+(\\hat{p}_{\\star,\\epsilon}/n)^{4}\\mathcal{T}_{2}^{c}+\\mathcal{T}_{2}+E_{2}\\right)+k^{2}\\epsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We now combine the bounds from the previous two lemmas to compute (80) and (81) to state the main result. ", "page_idx": 43}, {"type": "text", "text": "Theorem 32. Let Asm. 1 be true with error $\\epsilon\\,\\in\\,[0,1)$ . For a sequence of rebalanced distributions $\\big(\\hat{P}_{n}^{(k)}\\big)_{k\\geq1}$ , there exists an absolute constant $C>0$ such that when $n\\geq C[\\log_{2}(2n/\\hat{p}_{\\star,\\epsilon})+$ $m\\log{(n+1)}]/\\operatorname*{min}{\\left\\{p_{\\star},\\hat{p}_{\\star,\\epsilon}\\right\\}}^{2}$ , we have that ", "page_idx": 43}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/0ef0e1c2d72768e54d4c967062f04f9d0d4f39ef8e3009bbb5501567148e53f2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We provide the full experimental details of the experimental results from Sec. 4. We report additional evaluations on downstream tasks with linear probing and zero-shot retrieval. Finally, we give illustrations of the sensitivity to misspecified marginals, and of the convergence to the given marginals. ", "page_idx": 43}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Pre-Training Data. The pre-training data was taken from the public ImageNet-Captions dataset [Fang et al., 2013]. We subset the dataset by selecting the 250 classes that were most frequent in the dataset, resulting in 174,594 images and associated Flickr captions. The exact images used and their associated captions are given in the code supplement. ", "page_idx": 43}, {"type": "text", "text": "Evaluation Data. We perform zero-shot classification (as described in Sec. 4), zero-shot retrieval, and linear probing with various image classification and image-caption datasets. We used the default class captions (for classification) and default linear probing parameters from the CLIP Benchmark repo. The datasets (test splits) used were: ", "page_idx": 43}, {"type": "text", "text": "\u2022 CIFAR-10: 10,000 colored natural images labeled with one of 10 classes.   \n\u2022 CIFAR-100: 10,000 colored natural images labeled with one of 100 classes.   \n\u2022 STL-10: 80,000 colored natural images labelled with one of 10 classes.   \n\u2022 MS-COCO: 41,000 colored natural images with associated captions.   \n\u2022 Flickr8k: 8,000 colored natural images with associated captions.   \n\u2022 Rendered SST2: 1,821 images of typed natural language with sentiment label (2 classes).   \n\u2022 VOC2007: 4,952 colored natural images labelled with one of 20 classes.   \n\u2022 FGVC Aircraft: 34,000 colored natural images labelled with one of 102 classes. ", "page_idx": 43}, {"type": "text", "text": "Evaluation scripts using the various embeddings models (described below) are provided. ", "page_idx": 43}, {"type": "text", "text": "E.2 Model Specification and Hyperparameters ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Architecture and Implementation. The models considered CLIP models [Radford et al., 2021], and are specified by pairs of encoders $(f_{\\theta},g_{\\theta})$ , representing images and text, respectively. The encoders decompose into $f_{\\theta}=f_{\\theta}^{\\mathrm{head}}\\circ f_{\\theta}^{\\mathrm{base}}$ (similarly for $g_{\\theta}$ ) where $\\bar{f}_{\\theta}^{\\mathrm{base}}$ denotes a base image encoder and $f_{\\theta}^{\\mathrm{head}}$ denotes a trainable head model. The head models are feed-forward networks with two hidden layers, 256 hidden units, and 128-dimensional output representations. Their input dimensions may be 512 or 768, depending on whether a CLIP model or BERT/GPT-2 model is used as the base. For the image base/foundation models, we use the open-source OpenCLIP implementation of the ViT-B/32 model with the laion2b s34b b $7\\,9\\,\\mathrm{k}$ model tag. For the text encoder, we use the encoder of the variant of the ViT-B/32 with tag datacomp xl s13b b90k. For the other text encoders the Huggingface implementations of GPT-2 and BERT were used. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "Optimizer. For optimization, models were trained with stochastic gradient descent (SGD) with the learning rate tuned along the grid $\\left\\{1^{-3},3^{-3},1^{-2},3^{-2},1^{-1}\\right\\}$ and a fixed weight decay parameter of 0.01. Momentum-variants such as Adam [Kingma and Ba, 2015] were not used to isolate the effect varying losses as described in Sec. 4. ", "page_idx": 44}, {"type": "text", "text": "E.3 Compute Environment ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Experiments were run on a CPU/GPU workstation 12 virtual cores, 126G of memory, and four NVIDIA TITAN Xp GPUs with 12G memory each. The code was written in Python 3 and we use PyTorch for automatic differentiation. The OpenCLIP and CLIP Benchmark repos were used for zero-shot evaluation. ", "page_idx": 44}, {"type": "text", "text": "E.4 CLIP and Multi-CLIP ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We considered in the contrastive learning example from Sec. 2 \u2013 see (6) in particular \u2013 a variant of the CLIP objective in which either zero, or one, or more than one balancing iterations are performed (see (6)), via optimizing ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\cal L}_{n}^{(k)}=-\\frac{1}{2}\\sum_{i=1}^{n}\\left[\\log Q_{n}^{(k)}(X_{i},Y_{i})+\\log R_{n}^{(k)}(X_{i},Y_{i})\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This contrasts the single-iteration variant $L_{n}^{(1)}$ which in fact reduces to the original CLIP loss. Because these iterations are applied in the objective, backpropagation occurs through each iteration. ", "page_idx": 44}, {"type": "text", "text": "In Fig. 3, we plot the zero-shot classification performance (in terms of average per-class recall) of the variants trained on $L_{n}^{(0)}$ (the normalized initial measure, No Balancing), $L_{n}^{(1)}$ (the original CLIP loss, CLIP balancing), and $L_{n}^{(2)}$ (the two-iteration CLIP loss, Multi-CLIP balancing). We also vary the quality of the text encoder $f_{\\theta_{T}}$ , observing an overall accuracy trend of GPT- $2\\prec\\mathrm{BERT}\\prec\\mathrm{CLIP}$ across variants, which is to be expected given the base representation quality of each model. Interestingly, there is an improvement stemming from performing multiple balancing iterations across choices of the text embedding, the batch size $m$ , and the evaluation dataset. ", "page_idx": 44}, {"type": "text", "text": "E.5 Metadata Curation ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We considered in the metadata curation example from Sec. 2 how to use balancing to adjust the entire pre-training set, in the spirit of $\\mathrm{\\DeltaXu}$ et al. [2024]. The target marginal $P_{Y}$ is selected by choosing a threshold for which frequent keywords have their probability mass truncated, and the probability measure is normalized to sum to one. In Fig. 4, we show the observed marginal $P_{n,Y}$ and the target marginal $P_{Y}$ sorted in increasing order (left). The original marginal on $\\boldsymbol{\\wp}$ has approximately 5 orders of magnitude of difference between the most and least probable keyword. After balancing, the target marginal has less than 2 orders of difference. To see how this affects downstream performance, we plot the zero-shot classification accuracy over training iterations in Fig. 4 (right) when using the original dataset (orange) and using the metadata-balanced dataset (blue). We observe moderate improvement especially in the small batch regime $m=512$ ) when curating the dataset. ", "page_idx": 44}, {"type": "text", "text": "E.6 Additional Experiments ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we provide 1) a synthetic data example that helps elucidate the role of the spectral decomposition introduced in Sec. 3, and 2) additional evaluations on downstream tasks such as zero-shot retrieval and linear probing. For the latter, we maintain the experimental settings as used in the zero-shot classification example from Sec. 4 (Fig. 3). That is, we train variants of CLIP models (see Sec. 2) on the ImageNet-Captions dataset [Fang et al., 2013]. As before, we use a fixed image/text encoder as a base vector representation and compose it with a trainable feed-forward neural network, i.e., $f_{\\theta}\\,=\\,f_{\\theta}^{\\mathrm{head}}\\,\\circ\\,f^{\\mathrm{base}}$ , for $\\theta\\,=\\,\\theta_{I}$ (images) or $\\theta\\,=\\,\\theta_{T}$ (text). For the base text embeddings, we maintain three levels of model quality: GPT-2 [Radford et al., 2019], BERT [Devlin et al., 2019], and CLIP-based encodings. ", "page_idx": 44}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/2a1c65f9a83b35233b3ee04a9fc87b3282a948217713d277927c0ca24aa6ba52.jpg", "img_caption": ["Marginal Fitting Techniques under Misspecifiation ", "Figure 5: Baseline Comparisons across Dependence and Misspecification Levels. Each line refers tfro oa mc o(8m5b)i, noarti tohne  obf aalna necstiinmg aetisotin mmateothr hfeo er )l  apnrod baa bnioliitsye  mleevaeslu roen $P_{n}$ e,  tphreo evsitdiemda tmora $P_{n}^{\\mathrm{IPWI}}$ $P_{n}^{(k)}$ $k=8$ (see (84)). The $y$ -axis shows the mean squared error of estimating a linear functional. The $x$ -axis represents the dependence level $s=s_{2}$ (i.e. the leading singular value other than $s_{1}=1$ ). "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "Baseline Comparisons. We present a synthetic data example to understand the role of the singular values $s_{2},\\ldots,s_{m}$ and compare our approach to simple baselines that make use of $(P_{X},P_{Y})$ . We also consider misspecification of these target marginals, in that they are chosen by the user but are not the marginal distributions of the data-generating distribution $P$ . First, while one can verify by hand that (14) is a distribution for which $s_{2}=s$ , we construct a more general example for $m\\geq2$ . We leave the construction to the end of this example. For controllable misspecification, we define $\\epsilon\\in[0,0.5]$ to be the misspecification level, so that the corrupted target marginals are set to be ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{P}_{X}:=(1-\\epsilon)P_{X}+\\epsilon\\hat{P}_{X}\\mathrm{~and~}\\tilde{P}_{Y}:=(1-\\epsilon)P_{Y}+\\epsilon\\hat{P}_{Y},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where ${\\hat{P}}_{X}$ and $\\hat{P}_{Y}$ are drawn independently and randomly from the Dirichlet $\\left(\\mathbf{1}_{m}\\right)$ distribution (i.e. uniformly over the probability simplex on $m$ atoms). Finally, other than the empirical measure $P_{n}$ , we define one additional baseline; the importance weighted independently $(I P W I)$ estimator is defined as ", "page_idx": 45}, {"type": "equation", "text": "$$\nP_{n}^{\\mathrm{IPWI}}(x,y)=\\frac{\\Tilde{P}_{X}(x)}{P_{n,X}(x)}\\frac{\\Tilde{P}_{Y}(y)}{P_{n,Y}(y)}P_{n}(x,y).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This estimator simply reweighs all cells of the empirical measure by the likelihood ratio from each observed marginal to the target marginal. Note that the result may not even be a probability measure, as it may not sum to one. Observe the comparative performance in (see Fig. 5). We notice in particular that the naive $P_{n}^{\\mathrm{IPWI}}$ is outperformed by empirical measure uniformly over $s$ , as by applying both reweightings simultaneously, the estimator does not satisfy either marginal constraint. On the other hand, under the maximum amount of target marginal corruption $\\epsilon\\,=\\,0.5)$ , the balancing-based estimator suffers an approximately half-order of magnitude in MSE. When $s\\approx1$ , the MSE of the balancing estimator decreases significantly. We hypothesize that this is because the data sources $X$ and $Y$ are nearly a function of one another, and if this function is estimable to high precision by a small amount of data, then a single marginal can identify the entire joint distribution via pushforward calculations. That being said, it is important to note that the quantities $u_{j}$ and $v_{j}$ in (15) also depend on $s$ , so it is difficult to control the singular values without controlling the respective bases. ", "page_idx": 45}, {"type": "text", "text": "As for the construction of the probability mass function and test function, let $\\mathbf{I}_{m}$ and ${\\mathbf{1}}_{m\\times m}$ denote the identity matrix and matrix of ones in $\\mathbb{R}^{m\\times m}$ . For any $s\\,\\in\\,(0,1)$ and $m\\,\\geq\\,1$ , consider the ", "page_idx": 45}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/8950b43e4463b41432b93fc81d5b9740649e67b034e5def0c6e361e682985f55.jpg", "img_caption": ["Zeroshot Retrieval Performance (Recall $@$ 5) ", "Figure 6: Zero-Shot Retrieval Performance across Embeddings and Objectives. The three vertical panels describe different choices of the text encoder $f_{\\theta_{T}}$ which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various datasets, either MS-COCO or Flickr8k. evaluated under recall at $K=5$ for image and text retrieval, respectively. The $y$ -axis of each plot indicates the metric (see (86)) for either image or text retrieval, whereas the $x$ -axis indicates training iterations at batch size 512. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "probability mass matrix $P$ given by ", "page_idx": 46}, {"type": "equation", "text": "$$\nP=\\frac{1}{m}\\left[\\frac{1}{m}\\,\\mathbf{1}_{m\\times m}+s\\left(\\mathbf{I}_{m}-\\frac{1}{m}\\,\\mathbf{1}_{m\\times m}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The eigenvalues of the first matrix in the squared brackets are $(1,0,\\dots,0)$ , as it is a rank 1 matrix for which ${\\mathbf{1}}_{m}$ is an eigenvector. The second matrix in the square brackets is the centering matrix (the projection matrix that subtracts the mean of a vector\u2019s components from the entire vector). Multiplied by $s$ , it has eigenvalues $(0,s,\\ldots,s)$ where 0 is associated to the eigenvector ${\\mathbf{1}}_{m}$ . Thus, the matrix in its entirety has eigenvalues $(1/m,s/m,\\ldots,s/m)$ , where the scaling factor ensures that $P$ sums to one. The relation (13) holds for this choice of $P$ and uniform marginals, with $s_{2}=...=s_{m}=s$ . Thus, by tuning $s$ , we may control the level of dependence between $X$ and $Y$ . Finally, because $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are finite, we can also specify the test function $h$ via an $m\\times m$ table indexed by $i$ (meaning $x_{i}$ ) and $j$ (meaning $y_{j}$ ). We let $h(x_{i},y_{j})=|Z_{i j}|$ where the $Z_{i j}$ are independently drawn from a standard normal distribution. The resulting mean squared error is estimated with 200 seeds at $n=300$ . ", "page_idx": 46}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/fa83f23c99604ef242af78c9c42acf9ec9df2206fb7f88848e0884348db86042.jpg", "img_caption": ["CLIP Text Embeddings BERT Text Embeddings GPT-2 Text Embeddings ", "Figure 7: Linear Probe Performance across Embeddings and Objectives. The three vertical panels describe different choices of the text encoder $f_{\\theta_{T}}$ which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various evaluation datasets from Rendered SST2, VOC2007, and FGVC Aircraft. The $y$ -axis of each plot indicates average per-class recall, whereas the $x$ -axis indicates training iterations at batch size 512. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Zero-Shot Retrieval. In this evaluation, we assess the ability of the learned representations to match queries from one modality to their counterparts in another modality. We are given a test sets $\\mathcal{X}_{\\mathrm{test}}=\\mathsf{\\bar{\\{}}}(x_{1},\\ldots,x_{M})$ of images and $\\mathcal{V}_{\\mathrm{test}}=\\{y_{1},\\bar{\\cdot}\\cdot\\cdot,y_{N}\\}$ of texts in natural language. We are also given a matrix of annotations $A\\in\\left\\{0,1\\right\\}^{M\\times N}$ where $A_{i j}=1$ if and only if $y_{j}$ is a \u201crelevant\u201d caption for image $x_{i}$ (and vice versa). Given a particular query $\\bar{y}\\in\\mathcal{Y}_{\\mathrm{test}}$ , we define the top- $K$ neighborhood of $y$ as ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\cal N}_{K}(y;\\theta)=\\operatorname*{arg\\,max}_{S\\subseteq[M]:|S|=K}\\sum_{i\\in S}\\,\\langle\\,f_{\\theta_{I}}(x_{i}),f_{\\theta_{T}}(y)\\rangle,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "i.e. the images in the test set that have the closest embeddings under the given model. Then, we may define the average recall at $K$ for image retrieval metric as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{AverageRecall}_{K}(\\theta):=\\frac{1}{N}\\sum_{j=1}^{N}\\frac{\\sum_{i\\in\\mathcal{N}_{K}(y_{j};\\theta)}A_{i j}}{\\sum_{i^{\\prime}\\in[M]}A_{i^{\\prime}j}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "In words, the metric evaluates the retrieval system\u2019s ability to detect relevant items in the dataset, in this case by comparing closeness of the image-text representations. We can analogously define the average recall at $K$ for text retrieval metric by swapping the role of $x$ and $y$ above. The results for both retrieval metrics on the MS-COCO [Lin et al., 2015] and Flickr8k [Hodosh et al., 2013] benchmarks are given in Fig. 6. ", "page_idx": 47}, {"type": "text", "text": "Linear Probe. Here, we evaluate the quality of the model\u2019s encoders by fine-tuning a single linear layer on top of the learned representations for a classification task. In the case of linear probing via image classification, we use only the image encoder $f_{\\theta_{I}}$ . We are given a training set $\\left\\{(x_{1},\\bar{c_{1}}),\\dots,(\\bar{x_{N}},c_{N})\\right\\}$ of image-label pairs, where each $c_{i}\\;\\in\\;\\{\\bar{1},\\ldots,C\\}$ . We fix the model parameter $\\theta_{I}$ and solve the regularized multinomal cross entropy (MCE) objective ", "page_idx": 47}, {"type": "image", "img_path": "vJMMdFfL0A/tmp/541e41239a9e831e95a6d8a2b9f505071cc586ef423885e68bdcf586760e5f90.jpg", "img_caption": ["Figure 8: Empirical Marginals of CLIP Contrast Matrix. Depiction of the probability measures $Q_{n}^{(k)}$ and $R_{n}^{(k)}$ as described in (83) from Sec. 2. The orange bars correspond to the observed marginal after fitting to the target uniform distribution on the given iteration. Left: $Q_{n}^{(0)}$ and $R_{n}^{(0)}$ , where neither marginal is set to uniform. Center: ${Q}_{n}^{(1)}$ and $R_{n}^{(1)}$ , which corresponds to the original CLIP loss. Right: ${Q}_{n}^{(2)}$ and $R_{n}^{(2)}$ , which correspond to two iterations of the balancing procedure within the loss. The blue bars are slightly non-uniform. "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W\\in\\mathbb{R}^{C\\times r}}\\left[L_{\\mathrm{MCE}}(W):=-\\frac{1}{N}\\sum_{i=1}^{N}[\\mathrm{LogSoftmax}(W f_{\\theta_{I}}(x_{i}))]_{c_{i}}+\\frac{\\lambda}{2}\\left\\lVert W\\right\\rVert_{F}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\lambda~>~0$ is a regularization parameter, $\\|\\cdot\\|_{F}$ denotes the Frobenius norm on $\\mathbb{R}^{C\\times r}$ and LogSoftmax : $\\mathbb{R}^{C}\\,\\rightarrow\\,\\mathbb{R}^{C}$ is given by $\\begin{array}{r}{\\mathrm{LogSoftmax}(z)\\,=\\,z\\,-\\,\\log\\sum_{j=1}^{C}\\exp(z_{j})}\\end{array}$ . This results in a classifier ", "page_idx": 48}, {"type": "equation", "text": "$$\ng(x):=\\underset{j\\in[C]}{\\arg\\operatorname*{max}}[W f_{\\theta_{I}}(x)]_{j},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which can then be evaluated using standard accuracy metrics on a held-out test set. The image classification results for the Rendered SST2 [Radford et al., 2021], VOC2007 [Everingham et al., 2007], and FGVC Aircraft [Maji et al., 2013] benchmarks are given in Fig. 7. ", "page_idx": 48}, {"type": "text", "text": "Empirical Marginals in CLIP Balancing. To further clarify how the iterative balancing procedure is baked into the CLIP losses, recall from (83) that the objectives decompose into two terms, which depend on $Q_{n}^{(k)}$ and $R_{n}^{(k)}$ which differ only based on whether balancing to fit $P_{Y}$ or to fit $P_{X}$ is applied first, respectively. Thus, for any model parameterized by $\\theta$ and any number of iterations $k$ , there are four marginal distributions of interest: $Q_{\\theta,X}^{(k)},Q_{\\theta,Y}^{(k)},\\tilde{R_{\\theta,X}^{(k)}}$ , and $\\bar{R}_{{\\theta},Y}^{(k)}$ . Based on the order of iterations, we have that Q(\u03b81,)Y = R $Q_{\\theta,Y}^{(1)}=R_{\\theta,Y}^{(2)}=P_{Y}$ , and $R_{\\theta,X}^{(1)}=Q_{\\theta,X}^{(2)}=P_{X}$ . This is illustrated in Fig. 8. We see that after only a few iterations, both marginal distributions converge to the uniform distribution. ", "page_idx": 48}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Theoretical claims, the focus of this paper, are supported with proofs. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We clarify that the setting studied has some dissimilarities with practice in Sec. 2. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is done for all theoretical statements. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 50}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: Code is provided to reproduce the main results and an extensive experimental details section is written. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: This is written in the public repo provided in Sec. 4. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 51}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Yes, we even give a list of the specific images of ImageNet used to train the multimodal models. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 51}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Our evaluation metrics are shown with all seeds and their mean plotted in the corresponding figures. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: Please see Appx. E. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 52}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: NA ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 52}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The work is primarily theoretical and retrospective. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: We do not release general-purpose models, only small-scale illustrative ones. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 53}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We site all software and models used in the study in Appx. E. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 53}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 54}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We provide a list of notebooks and scripts to illustrate our method and connect it to existing software repositories such as OpenCLIP. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 54}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: NA ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 54}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: NA ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]