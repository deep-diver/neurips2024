[{"figure_path": "vJMMdFfL0A/figures/figures_3_1.jpg", "caption": "Figure 1: Data Balancing Examples: Each panel shows a possible distribution Q on different choices of (X, Y). The orange histograms are the target marginal Py. Left: Q(x, y) is the affinity of an image x for cluster y. Center: Q(x, y) is the similarity of an image x to a text caption y. Right: Q(x, y) is the proportion of substring matches between a text caption x and a keyword y.", "description": "This figure illustrates three different applications of data balancing.  The left panel shows self-supervised clustering, where data points (images) are mapped to features (X) and pseudo-labels (Y, representing cluster assignments). The center panel depicts contrastive learning, mapping image-text pairs to image features (X) and text features (Y). The right panel demonstrates metadata curation, mapping text data to text features (X) and keyword matches (Y). In each case, data balancing aims to adjust the probability distribution over features and labels to match the target marginals (represented by the orange histograms), improving the model's performance.", "section": "Data Balancing in Practice"}, {"figure_path": "vJMMdFfL0A/figures/figures_5_1.jpg", "caption": "Figure 2: Data Balancing. Nonlinear and linear operators associated to each iteration of (8). Left: Visualization of the exact iterations of (8) in the space of probability measures. The blue set contains joint distributions with X-marginal equal to Px, whereas the orange set contains joint distributions with Y-marginal equal to Py. Right: Visualization of L\u00b2(P), the operators defining (11), and the singular values given in (13).", "description": "The figure visually represents the data balancing process. The left panel shows the nonlinear iterations in the space of probability measures, illustrating how the process alternates between projecting onto the set of distributions with X-marginal equal to Px (blue) and the set of distributions with Y-marginal equal to Py (orange).  The right panel illustrates the corresponding linear operators (\u03bcx, \u03bcy) in the Hilbert space L\u00b2(P), showing their singular value decomposition and the relationship between the singular values and the angle between the subspaces L\u00b2(Px) and L\u00b2(Py). This visualization helps understand the variance reduction achieved through data balancing.", "section": "B Linear Operators and Variance Reduction"}, {"figure_path": "vJMMdFfL0A/figures/figures_8_1.jpg", "caption": "Figure 3: Zero-Shot Classification Performance across Embeddings, Batch Sizes, and Objectives. The three vertical panels describe different choices of the text encoder  which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Within each vertical panel, examples include batch sizes m = 128 and m = 512. Rows indicate various evaluation datasets from CIFAR-10, CIFAR-100, and STL-10. The y-axis of each plot indicates average per-class recall, whereas the x-axis indicates training iterations at the given batch size.", "description": "This figure shows the results of zero-shot image classification experiments using different text encoders (GPT-2, BERT, CLIP), batch sizes (128, 512), and datasets (CIFAR-10, CIFAR-100, STL-10).  The plots show the average per-class recall (y-axis) as a function of training iterations (x-axis).  It demonstrates how data balancing impacts zero-shot classification performance.", "section": "Numerical Illustrations"}, {"figure_path": "vJMMdFfL0A/figures/figures_9_1.jpg", "caption": "Figure 4: Balancing and Metadata Curation. Depiction of balancing and metadata curation (Example 3 in Sec. 2) on ImageNet-Captions dataset, in which X represents image-caption pairs and Y represents keywords. Left: Observed marginal Pn,Y (orange) and Py (blue), which are sorted by order of increasing probability. Right: Zero-shot evaluation of an embedding model trained using the standard CLIP loss original versus the balanced training set.", "description": "This figure shows the effect of data balancing on metadata curation for image caption pairs. The left panel displays the original and balanced distributions of keywords (metadata). The right panel demonstrates the zero-shot classification accuracy on CIFAR-100 and STL-10 datasets using models trained on both the original and balanced datasets.  The results show that balancing leads to a more uniform keyword distribution and can improve the performance of the model.", "section": "Numerical Illustrations"}, {"figure_path": "vJMMdFfL0A/figures/figures_43_1.jpg", "caption": "Figure 5: Baseline Comparisons across Dependence and Misspecification Levels. Each line refers to a combination of an estimation method (the empirical probability measure Pn, the estimator PIPWI from (85), or the balancing estimator P(k) for k = 8) and a noise level on the provided marginals (see (84)). The y-axis shows the mean squared error of estimating a linear functional. The x-axis represents the dependence level s = s2 (i.e. the leading singular value other than s\u2081 = 1).", "description": "This figure compares three methods for estimating a linear functional from data with misspecified marginal distributions.  The methods are the empirical measure, an importance weighted estimator, and a data balancing estimator. The x-axis shows the dependence level between the two variables (higher values meaning more dependence). The y-axis shows the mean squared error. The different colored lines show the results with different levels of noise added to the marginal distributions provided to the estimators. The figure shows how the balancing estimator outperforms the other methods.", "section": "E.6 Additional Experiments"}, {"figure_path": "vJMMdFfL0A/figures/figures_45_1.jpg", "caption": "Figure 5: Baseline Comparisons across Dependence and Misspecification Levels. Each line refers to a combination of an estimation method (the empirical probability measure Pn, the estimator PIPWI from (85), or the balancing estimator P(k) for k = 8) and a noise level on the provided marginals (see (84)). The y-axis shows the mean squared error of estimating a linear functional. The x-axis represents the dependence level s = s2 (i.e. the leading singular value other than s\u2081 = 1).", "description": "This figure compares three methods for estimating a linear functional of a probability distribution: the empirical measure, an importance weighted estimator, and the balanced estimator.  It shows how the mean squared error (MSE) of these estimators varies with the dependence between two random variables (represented by the leading singular value 's') and the amount of noise in the provided marginal distributions (represented by '\u03b5'). The results demonstrate that the balanced estimator generally outperforms the other two, especially when the variables are highly dependent.", "section": "E.6 Additional Experiments"}, {"figure_path": "vJMMdFfL0A/figures/figures_46_1.jpg", "caption": "Figure 6: Zero-Shot Retrieval Performance across Embeddings and Objectives. The three vertical panels describe different choices of the text encoder for which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various datasets, either MS-COCO or Flickr8k, evaluated under recall at K = 5 for image and text retrieval, respectively. The y-axis of each plot indicates the metric (see (86)) for either image or text retrieval, whereas the x-axis indicates training iterations at batch size 512.", "description": "This figure shows the results of zero-shot retrieval experiments using different text encoders (GPT-2, BERT, CLIP) and data balancing techniques.  The performance is measured by recall@5 for both image and text retrieval tasks across different datasets (MS-COCO and Flickr8k). The x-axis represents the number of training iterations, and the y-axis shows the recall@5 metric.  Different colored lines represent different data balancing approaches (No Balancing, CLIP Balancing, Multi-CLIP).", "section": "Numerical Illustrations"}, {"figure_path": "vJMMdFfL0A/figures/figures_47_1.jpg", "caption": "Figure 7: Linear Probe Performance across Embeddings and Objectives. The three vertical panels describe different choices of the text encoder  for which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various evaluation datasets from Rendered SST2, VOC2007, and FGVC Aircraft. The y-axis of each plot indicates average per-class recall, whereas the x-axis indicates training iterations at batch size 512.", "description": "This figure shows the performance of linear probing on three different datasets (Rendered SST2, VOC2007, and FGVC Aircraft).  The results are shown for three different text encoders (GPT-2, BERT, and CLIP). The x-axis represents the number of training steps, and the y-axis represents the average per-class recall.  It demonstrates how the quality of the text encoder and the number of balancing iterations affect the performance of the linear probing task.", "section": "Numerical Illustrations"}, {"figure_path": "vJMMdFfL0A/figures/figures_48_1.jpg", "caption": "Figure 8: Empirical Marginals of CLIP Contrast Matrix. Depiction of the probability measures Q(k) and R(k) as described in (83) from Sec. 2. The orange bars correspond to the observed marginal after fitting to the target uniform distribution on the given iteration. Left: Q(0) and R(0), where neither marginal is set to uniform. Center: Q(1) and R(1), which corresponds to the original CLIP loss. Right: Q(2) and R(2), which correspond to two iterations of the balancing procedure within the loss. The blue bars are slightly non-uniform.", "description": "This figure shows the empirical marginal distributions obtained from the CLIP contrast matrix at different iterations of the balancing procedure. The left panel shows the initial distributions, which are not uniform. The middle panel shows the distributions after one iteration, which are closer to uniform. The right panel shows the distributions after two iterations, which are nearly uniform.  This illustrates how the balancing procedure helps to reduce the variance of the empirical objective function.", "section": "Empirical Marginals in CLIP Balancing"}]