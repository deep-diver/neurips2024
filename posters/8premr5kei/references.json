{"references": [{"fullname_first_author": "Lei Jimmy Ba", "paper_title": "Layer Normalization", "publication_date": "2016-07-06", "reason": "This paper introduces Layer Normalization, a crucial technique used in the Vision Transformer architecture, which is central to the paper's method."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2021-00-00", "reason": "This is a foundational paper introducing Vision Transformers (ViTs), the core model architecture upon which the paper's work is based."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a model that directly connects visual and textual data, providing a relevant baseline for comparison with the proposed approach."}, {"fullname_first_author": "Zifeng Wang", "paper_title": "Learning to Prompt for Continual Learning", "publication_date": "2022-00-00", "reason": "This paper introduces the concept of prompt tuning for continual learning, a direct precursor to the work presented in this paper."}, {"fullname_first_author": "Shipeng Wang", "paper_title": "Training Networks in Null Space of Feature Covariance for Continual Learning", "publication_date": "2021-00-00", "reason": "This paper introduces the concept of orthogonal projection for continual learning, which is theoretically linked to and practically influences the approach proposed in this paper."}]}