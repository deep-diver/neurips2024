[{"figure_path": "8pRemr5kEi/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the forward propagation in a ViT layer. Residual connections are omitted. The red crosses indicate the rows of attention map or the output prompts can be neglected.", "description": "This figure illustrates the forward propagation process within a single layer of a Vision Transformer (ViT) network, specifically focusing on how visual prompts are incorporated. The process is broken down into several stages: Layer Normalization (LN) of both the input image tokens and the prompts, the multi-head self-attention mechanism (including the Affinity and Aggregation stages), and finally, another Layer Normalization and Multi-Layer Perceptron (MLP).  The figure highlights that only the image tokens are used in the subsequent calculations, while the prompt's contribution to the final output is represented as a 'red cross' symbol, which means they can be discarded during fine-tuning of this layer.", "section": "3 Preliminaries"}, {"figure_path": "8pRemr5kEi/figures/figures_7_1.jpg", "caption": "Figure 2: Task-by-task accuracy changing curves of VPT-Seq and VPT-NSP2 on two benchmarks.", "description": "This figure shows the task-by-task accuracy curves for both the sequential fine-tuning baseline (VPT-Seq) and the proposed Null-Space Projection for Prompts (VPT-NSP2) method.  The plots illustrate the performance on two benchmark datasets: 10-split CIFAR-100 and 10-split ImageNet-R.  The x-axis represents the task number, while the y-axis shows the accuracy. This visualization demonstrates the effectiveness of VPT-NSP2 in mitigating catastrophic forgetting, as its accuracy remains consistently higher across tasks compared to the VPT-Seq baseline.", "section": "5.2 Main Results"}, {"figure_path": "8pRemr5kEi/figures/figures_7_2.jpg", "caption": "Figure 3: Results of utilizing different pre-training datasets and paradigms. The blue and yellow bars represent accuracy and forgetting, respectively. The upward arrows indicate the accuracy increasing from VPT-Seq to VPT-NSP2, whereas the downward arrows denote the reduction in forgetting.", "description": "This figure presents the results of experiments using different pre-trained models and datasets.  The models were fine-tuned using the proposed method (VPT-NSP2) and a baseline sequential fine-tuning method (VPT-Seq). The results are shown for four datasets: 10-split and 20-split CIFAR-100, 10-split ImageNet-R, and 10-split DomainNet.  Each bar graph represents a dataset, with different pre-training methods shown on the x-axis. The blue bars represent the accuracy, while the yellow bars show the amount of forgetting that occurred during training. Upward-pointing arrows highlight improvements in accuracy with the proposed method compared to the baseline, while downward-pointing arrows indicate reductions in forgetting.", "section": "5 Experiments"}, {"figure_path": "8pRemr5kEi/figures/figures_8_1.jpg", "caption": "Figure 2: Task-by-task accuracy changing curves of VPT-Seq and VPT-NSP2 on two benchmarks.", "description": "This figure shows the task-by-task accuracy curves for both the sequential fine-tuning baseline (VPT-Seq) and the proposed Null-Space Projection for Prompts (VPT-NSP2) method.  The plots visually demonstrate the effectiveness of VPT-NSP2 in maintaining high accuracy on previously learned tasks while learning new ones. The downward-pointing arrows highlight the significant reduction in forgetting achieved by VPT-NSP2 compared to VPT-Seq.  Two different benchmark datasets are presented: 10-split CIFAR-100 and 10-split ImageNet-R.", "section": "5.2 Main Results"}, {"figure_path": "8pRemr5kEi/figures/figures_13_1.jpg", "caption": "Figure 1: Illustration of the forward propagation in a ViT layer. Residual connections are omitted. The red crosses indicate the rows of attention map or the output prompts can be neglected.", "description": "This figure illustrates the forward propagation process within a single ViT layer, focusing on how input image tokens and prompts interact. It shows the steps involved: Layer Normalization (LN), linear transformations (qkv), self-attention (including affinity, softmax, and aggregation), and finally the MLP layer.  Crucially, it highlights that certain elements (rows of the attention map related to prompts as queries, and output prompts) can be effectively ignored during training because they don't directly influence the image tokens. This simplification is key to understanding the proposed approach in the paper.", "section": "3 Preliminaries"}]