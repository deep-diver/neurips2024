{"references": [{"fullname_first_author": "Jimmy Lei Ba", "paper_title": "Layer normalization", "publication_date": "2016-07-06", "reason": "This paper introduces layer normalization, a crucial technique used for stabilizing training in deep neural networks and improving performance, which is directly used in this paper's model architecture."}, {"fullname_first_author": "Yinghao Huang", "paper_title": "Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time", "publication_date": "2018-00-00", "reason": "This paper is a seminal work on sparse inertial pose estimation that introduces the use of neural networks for this task, which directly inspired the work in this paper."}, {"fullname_first_author": "Timo Von Marcard", "paper_title": "Sparse inertial poser: Automatic 3d human pose estimation from sparse imus", "publication_date": "2017-00-00", "reason": "This paper is among the pioneering works that first demonstrated that recovering full-body motion from sparse inertial data was feasible and introduced the concept of sparse inertial pose estimation."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, the core model architecture used in this paper's approach for fixed-length sequential data. "}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "This paper introduced BERT, a highly influential pre-training method for transformers that demonstrates the power of transfer learning, which is used in this paper's training methodology"}]}