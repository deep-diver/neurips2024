[{"type": "text", "text": "No-Regret Learning for Fair Multi-Agent Social Welfare Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mengxiao Zhang ", "page_idx": 0}, {"type": "text", "text": "Ramiro Deo-Campo Vuong ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Iowa mengxiao-zhang@uiowa.edu ", "page_idx": 0}, {"type": "text", "text": "Cornell University ramdcv@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Haipeng Luo University of Southern California haipengl@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of online multi-agent Nash social welfare (NsW) maximization. While previous works of Hossain et al. [2021], Jones et al. [2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\\sqrt{T}$ -regret is possible after $T$ rounds, their fairness measure is the product of all agents\u2019 rewards, instead of their Nsw (that is, their geometric mean). Given the fundamental role of Nsw in the fairness literature, it is more than natural to ask whether no-regret fair learning with NsW as the objective is possible. In this work, we provide a complete answer to this question in various settings. Specifically, in stochastic $N$ -agent $K$ -armed bandits, we develop an algorithm with $\\hat{\\tilde{\\mathcal{O}}}(K^{\\frac{2}{N}}T^{\\frac{\\Bar{N}-1}{N}})$ ", "page_idx": 0}, {"type": "text", "text": "regret and prove that the dependence on $T$ is tight, making it a sharp contrast to the $\\sqrt{T}$ -regret bounds of Hossain et al. [2021], Jones et al. [2023]. We then consider a more challenging version of the problem with adversarial rewards. Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret. To circumvent such negative results, we further consider a setting with full-information feedback and design two algorithms with $\\sqrt{T}$ -regret: the first one has no dependence on $N$ at all and is applicable to not just NsW but a broad class of welfare functions, while the second one has better dependence on $K$ and is preferable when $N$ is small. Finally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study online multi-agent Nash social welfare (NsW) maximization, which is a generalization of the classic multi-armed bandit (MAB) problem [Thompson, 1933, Lai and Robbins, 1985]. Different from MAB, in which the learner makes her decisions sequentially in order to maximize her own reward, in online multi-agent NsW maximization, the learner's decision affects multiple agents and the goal is to maximize the NSW over all the agents. Specifically, NSW is defined as the geometric mean of the expected utilities over all agents [Moulin, 2004], which can be viewed as a measure of fairness among the agents. This problem includes many important real-life applications such as resource allocation [Jones et al., 2023], where the learner needs to guarantee fair allocations among multiple agents. We refer the readers to [Hossain et al., 2021, Jones et al., 2023] for more applications of NsW maximization. ", "page_idx": 0}, {"type": "text", "text": "Recent work by Hossain et al. [2021], Jones et al. [2023] studies a similar problem but with $\\mathrm{NSW}_{\\mathrm{prod}}$ as the objective, a variant of NsW that is defined as the product of the utilities over agents instead of their geometric mean. While the optimal strategy is the same if the utility for each agent is stationary, this is not the case with a non-stationary environment. Moreover, $\\mathrm{NSW}_{\\mathrm{prod}}$ is homogeneous of degree $N$ instead of degree 1, where $N$ is the number of agents, meaning that $\\mathrm{NSW}_{\\mathrm{prod}}$ is more sensitive to the scale of the utility. Specifically, if the utilities of each agent are scaled by 2, then NsW is scaled by 2 as well, but $\\mathrm{NSW}_{\\mathrm{prod}}$ is scaled by $2^{N}$ . Therefore, it is arguably more reasonable to consider regret with respect to Nsw, which has not been studied before (to our knowledge) and is the main objective of our work. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "From a technical perspective, however, due to the lack of Lipschitzness, NsW poses much more challenges in regret minimization compared to $\\mathrm{NSW}_{\\mathrm{prod}}$ . For example, one cannot directly apply the algorithm for Lipschitz bandits [Kleinberg et al., 2019] to our problem, while it is directly applicable to $\\mathrm{NSW}_{\\mathrm{prod}}$ as mentioned in [Hossain et al., 2021, Jones et al., 2023]. Despite such challenges, we manage to provide complete answers to this problem in various setting. Specifically, our contributions are listed below (where $T,N$ ,and $K$ denote the number of rounds, agents, and arms/actions respectively): ", "page_idx": 1}, {"type": "text", "text": "\u00b7 (Section 3) We first study the stochastic bandit setting, where the utility matrix at each round is i.i.d. drawn from an unknown distribution, and the learner can only observe the utilities (for different agents) of the action she picked. In this case, we develop an algorithm with $\\widetilde{\\mathcal{O}}(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}})$ regret.1 While our algorithm is also naturally based on the Upper Confidence Bound (UCB) algorithm as in Hossain et al. [2021], Jones et al. [2023], we show that a novel analysis with Bernstein-type confidence intervals is important for handling the lack of Lipschitzness of NsW. Moreover, we prove a lower bound of order $\\widetilde\\Omega(\\frac{1}{N^{3}}\\cdot K^{\\frac{1}{N}}T^{\\frac{N-1}{N}})$ , showing that the dependence on $T$ is tight. This is in sharp contrast to the $\\sqrt{T}$ -regret bound of Hossain et al. [2021], Jones et al. [2023] and demonstrates the dificulty of learning with NSW compared to $\\mathrm{NSW}_{\\mathrm{prod}}$ ", "page_idx": 1}, {"type": "text", "text": "\u00b7 (Section 4.1) We then consider a more challenging setting where the utility matrix at each round can be adversarially chosen by the environment. Somewhat surprisingly, we show that no algorithm can achieve sublinear regret in this case, despite NSW being concave and the vast literature on bandit online maximization with concave utility functions (the subtlety lies in the slightly different feedback model). In fact, the same impossibility result also holds for $\\mathrm{NSW}_{\\mathrm{prod}}$ asweshow. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 (Section 4.2) To bypass such impossibility, we further consider this adversarial setting under richer feedback, where the learner observes the full utility matrix after her decision (the so-called fullinformation feedback). Contrary to the bandit feedback setting, learning is not only possible now but can also be much faster despite having adversarial utilities. Specifically, we design two different algorithms with $\\sqrt{T}$ -regret. The first algorithm is based on Follow-the-Regularized-Leader (FTRL) with the log-barrier regularizer, which achieves $\\mathcal{O}(\\sqrt{K T\\log T})$ regret (Section 4.2.1). Notably, this algorithm does not have any dependence on the number of agents $N$ and can also be generalized to a broader class of social welfare functions. The second algorithm is based on FTRL with a Tsallis entropy regularizer, which achieves $\\widetilde{\\mathcal{O}}(K^{\\frac{1}{2}-\\frac{1}{N}}\\sqrt{N T})$ regret and is thus more favorable when $K$ is much larger than $N$ (Section 4.2.2). Finally, we also show that improved logarithmic regret is possible as long as at each round there exists at least one agent who is indifferent about the learner's choice of arm (Section 4.2.3). ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Hossain et al. [2021], Jones et al. [2023] are most related to our work. Hossain et al. [2021] is the first to consider designing no-regret algorithms under $\\mathrm{NSW}_{\\mathrm{prod}}$ for the stochastic multi-agent multi-armed bandit problem. Specifically, they propose two algorithms. The first one is based on $\\varepsilon$ greedy and achieves ${\\mathcal{O}}(T^{{\\frac{2}{3}}})$ regret efficiently, and the second one is based on UCB and achieves $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret inefficiently. Jones et al. [2023] improves these results by providing a better UCB-based algorithm that is efficient and achieves the same $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret. To the best of our knowledge, there are no previous results for regret minimization over Ns w under this particular setup. ", "page_idx": 1}, {"type": "text", "text": "However, several other models of fairness have been introduced in (single-agent or multi-agent) multi-armed bandits, some using NsW as well. These models differ in whether they aim to be fair among different objectives, different arms, different agents, different rounds, or others. Most related to this paper is multi-objective bandits, in which the learner tries to increase different and possibly competing objectives in a fair manner. For example, Drugan and Nowe [2013] introduces the multi-objective stochastic bandit problem and offers a regret measure to explore Pareto Optimal solutions, and Busa-Fekete et al. [2017] investigates the same setting using the Generalized Gini Index in their regret measure to promote fairness over objectives. Their regret measure closely resembles the one we use, except they apply some social welfare function (SWF) to the cumulative expected utility of agents over all rounds as opposed to the expected utility of agents each round. On the other hand, some other works study fairness among different rounds which incentivizes the learner to perform well consistently over all rounds [Barman et al., 2023, Sawarni et al., 2024]. Besides, there are other models that measure fairness in different ways, including how often each arm is pulled [Joseph et al. 2016, Liu et al., 2017, Gillen et al., 2018, Chen et al., 2020] and how the regret is allocated across different groups [Baek and Farias, 2021]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Kaneko and Nakamura [1979] axiomatically derives the Nsw function. It is a fundamental and widely-adopted fairness measure and is especially popular for the task of fairly allocating goods. Caragiannis et al. [2019] justifies the fairness of NsW by showing that its maximum solution ensures some desirable envy-free property. This result prompted the design of approximation algorithms for the problem of allocating indivisible goods by maximizing NSW, which is known to be NP-hard even for simple valuation functions [Barman et al., 2018, Cole and Gkatzelis, 2015, Garg et al., 2023, Li and Vondrak, 2021]. ", "page_idx": 2}, {"type": "text", "text": "There is a vast literature on the multi-armed bandit problem; see the book by Lattimore and Szepesvari [2020] for extensive discussions. The standard algorithm for the stochastic setting is UCB [Lai and Robbins, 1985, Auer et al., 2002a], while the standard algorithm for the adversarial setting is FTRL or the closely related Online Mirror Descent (OMD) algorithm [Auer et al., 2002b, Audibert and Bubeck, 2010, Abernethy et al., 2015]. For FTRL/OMD, the log-barrier or Tsallis entropy regularizers have been extensively studied in recent years due to some of their surprising properties (e.g., [Foster et al., 2016, Wei and Luo, 2018, Zimmert and Seldin, 2019, Lee et al., 2020]). They are rarely used in the full-information setting as far as we know, but our analysis reveals that they are useful even in such settings, especially for dealing with the lack of Lipschitzness of NsW. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "General Notation.  Throughout this paper, we denote the set $\\{1,2,\\ldots,n\\}$ by $[n]$ for any positive integer $n$ . For a matrix $M\\in\\mathbb{R}^{m\\times n}$ , we denote the $i$ -th row vector of $M$ by $M_{i,:}\\in\\mathbb{R}^{n}$ , the $j$ -th columnvectorof $M$ by $M_{:,j}\\in\\mathbb{R}^{m}$ , and the $(i,j)$ -th entry of $M$ by $M_{i,j}$ . We say $M\\succeq0$ if $M$ is a positive semi-definite matrix. The $(K-1)$ -dimensional simplex is denoted as $\\Delta_{K}$ , and its clipped version with a parameter $\\delta>0$ is denoted as $\\Delta_{K,\\delta}=\\{p\\in\\Delta_{K}\\mid p_{i}\\geq\\delta,\\forall i\\in[K]\\}$ . We use 0 and 1 to denote the all-zero and all-one vector in an appropriate dimension. For two random variables $X$ and $Y$ , we use $X\\ {\\stackrel{d}{=}}\\ Y$ to say $X$ is equivalent to $Y$ in distribution. ", "page_idx": 2}, {"type": "text", "text": "For a twice differentiable function $f$ , we use $\\nabla f(\\cdot)$ and $\\nabla^{2}f(\\cdot)$ to denote its gradient and Hessian. For concave functions that are not differentiable, $\\nabla f(\\cdot)$ denotes a super-gradient. Throughout the paper, we study functions of the form $f(u^{\\top}p)$ for $u\\in[0,1]^{m\\times n}$ and $p\\in\\Delta_{m}$ . In such cases, the gradient, super-gradient, or hessian are all with respect to $p$ unless denoted otherwise (for example, we write $\\bar{\\nabla_{u}}^{f}(\\bar{u}^{\\top}p)$ , with an explicit subscript $u$ , to denote the gradient with respect to $u$ ", "page_idx": 2}, {"type": "text", "text": "Social Welfare Functions A social welfare function (SWF) $f:[0,1]^{N}\\,\\rightarrow\\,[0,1]$ measures the desirability of the agents\u2019 expected utilities. Specifically, for two different vectors of expected utilities $\\mu,\\mu^{\\prime}\\in[0,1]^{N}$ \uff0c $f(\\bar{\\mu})>f(\\bar{\\mu^{\\prime}})$ means that $\\mu$ is a fairer alternative than $\\mu^{\\prime}$ . In each setting we explore, each action by the learner yields some expected utility for each of the $N$ agents, and the learner's goal is maximize some SWF applied to these $N$ expected utilities. ", "page_idx": 2}, {"type": "text", "text": "Nash Social Welfare (NsW) For the majority of this paper, we focus on a specific type of SWF, namely the Nash Social Welfare (NSW) function [Nash, 1950, Kaneko and Nakamura, 1979]. Specifically, for $\\mu\\in[0,1]^{N}$ , NSW is defined as the geometric mean of the $N$ coordinates: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{NSW}(\\mu)=\\prod_{n\\in[N]}\\mu_{n}^{1/N}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As mentioned, Hossain et al. [2021], Jones et al. [2023] considered a closely related variant that is simply the product of the coordinates: $\\begin{array}{r}{\\mathrm{NSW}_{\\mathrm{prod}}(\\mu)\\,=\\,\\prod_{n\\in[N]}\\mu_{n}}\\end{array}$ . It is clear that NsW has a better scaling property since it is homogeneous: scaling each $\\mu_{n}$ by a constant $c$ also scales $\\mathrm{NSW}(\\mu)$ by $c$ , but it scales $\\mathrm{NSW}_{\\mathrm{prod}}(\\mu)$ by $c^{N}$ . This makes $\\mathrm{NSW}_{\\mathrm{prod}}$ an unnatural learning objective, which motivates us to use NSW as our choice of SWF. Learning with NSW, however, brings extra challenges since it is not Lipschitz in the small-utility regime (while $\\mathrm{NSW}_{\\mathrm{prod}}$ is Lipschitz over the entire $[0,1]^{\\overline{{N}}})$ We shall see in subsequent sections how we address such challenges. ", "page_idx": 3}, {"type": "text", "text": "We remark that while our main focus is regret minimization with respect to NsW, some of our results alsoapplyto $\\mathrm{NSW}_{\\mathrm{prod}}$ or more general classes of SWFs (as will become clear later). ", "page_idx": 3}, {"type": "text", "text": "Problem Setup. The $N$ -agent $K$ -armed social welfare optimization problem we consider is defined as follows (with $N\\geq2$ and $K\\geq2$ throughout). Ahead of time, with the knowledge of the learner's algorithm, the environment decides $T$ utility matrices $u_{1},\\ldots,u_{T}\\in[0,1]^{K\\times N}$ ,.where $u_{t,i,n}$ is the utility of agent $n$ if arm/action $i$ is selected at round $t$ . Then, the learner interacts with the environment for $T$ rounds: at each round $t$ , the learner decides a distribution $p_{t}\\in\\Delta_{K}$ and then samples an action $i_{t}\\sim p_{t}$ . In the full-information feedback setting, the learner observes the full utility matrix $u_{t}$ after her decision, and in the bandit feedback setting, the learner only observes $u_{t,i_{t},n}$ for each agent $n\\in[N]$ , that is, the utilities of the selected action. ", "page_idx": 3}, {"type": "text", "text": "We consider two different types of environments, the stochastic one and the adversarial one, with a slight difference in their regret definition. In the stochastic environment, there exists a mean utility matrix $u\\in[0,1]^{K\\times N}$ such that at each round $t$ $u_{t}$ is an i.i.d. random variable with mean $u$ Fix an SWF $f$ . The social welfare of a strategy $p\\in\\Delta_{K}$ is defined as $f(u^{\\top}p)$ , which is with respect to the agents\u2019 expected utilities over the randomness of both the learner's and the environment's. The regret is then defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{sto}}=T\\cdot\\operatorname*{max}_{p\\in\\Delta_{K}}f(u^{\\top}p)-\\mathbb{E}\\left[\\sum_{t=1}^{T}f(u^{\\top}p_{t})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is the difference between the total social welfare of the optimal strategy and that of the learner.   \nWhen $f$ ischosentobe $\\mathrm{NSW}_{\\mathrm{prod}}$ , Eq. (2) reduces to the regret notion considered in Hossain et al.   \n[2021], Jones et al. [2023]. ", "page_idx": 3}, {"type": "text", "text": "On the other hand, in the adversarial environment, we do not make any distributional assumption on the utility matrices and allow them to be selected arbitrarily. The social welfare of a strategy $p\\in\\Delta_{K}$ for time $t$ is defined as $f(u_{t}^{\\top}p)$ , and the overall regret of the learner is correspondingly defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathrm{Reg}}_{\\mathrm{adv}}=\\operatorname*{max}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}f(u_{t}^{\\top}p)-\\mathbb{E}\\left[\\sum_{t=1}^{T}f(u_{t}^{\\top}p_{t})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In both Eq. (2) and Eq. (3), the expectation is taken with respect to the randomness of the algorithm. ", "page_idx": 3}, {"type": "text", "text": "Social welfare of expected utilities versus expected social welfare of realized utilities.  One might wonder why we measure fairness using the social welfare of expected utilities (e.g., $f(u^{\\top}p))$ instead of the expected social welfare of realized utilities (e.g., $\\mathbb{E}_{i\\sim p}[f(u^{\\top}e_{i})])$ . This is because the former is arguably more meaningful as a fairness measure. To see this, consider $f=\\mathrm{NSW}$ or $f=\\mathrm{NSW_{prod}}$ and a setting with 2 agents, 2 arms, and $u$ being the identity matrix. Then, in terms of $f(u^{\\top}p)$ , the uniform distribution is the best policy (which makes sense from a fairness viewpoint), while in terms of $\\mathbb{E}_{i\\sim p}[f(u^{\\top}e_{i})]$ , all distributions achieve the same value of O, implying that all polices are as fair, which is clearly undesired. ", "page_idx": 3}, {"type": "text", "text": "Connection to Bandit Convex optimization. When taking $f\\,=\\,\\mathrm{NSW}$ (our main focus) and considering the bandit feedback setting, our problem is seemingly an instance of the heavily-studied Bandit Convex optimization (BCO) problem, since $-\\mathrm{NSW}$ is convex. However, there is a slight but critical difference in the feedback model: a BCO algorithm would require observing $f(u_{t}^{\\top}\\tilde{p}_{t})$ ,or equivalently $u_{t}^{\\top}p_{t}$ , at the end of each round $t$ while in our problem the learner only observes $u_{t,i_{t},:}$ much more realistic scenario. Even though they have the same expectation, due to the non-linearity of NSW, this slight difference in the feedback turns out to cause a huge difference in terms of learning \u2014 the minimax regret for BCO is known to be $\\Theta({\\sqrt{T}})$ , while in our problem (with bandit feedback), ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 UCB for $N$ -agent $K$ -armed NSW maximization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input:warm-upphaselength $N_{0}>0$   \nInitialization: $\\widehat{u}_{1,i,n}=1$ for all $n\\in[N],i\\in[K].\\ N_{1,i}=0$ for all $i\\in[K]$ \uff1a   \nfor $t=1,2,\\ldots,T$ do if $t\\le K N_{0}$ then select $\\begin{array}{r}{i_{t}=\\left\\lceil\\frac{t}{N_{0}}\\right\\rceil}\\end{array}$ else calculate $p_{t}=\\operatorname{argmax}_{p\\in\\Delta_{K}}\\operatorname{NSW}(\\widehat{u}_{t}^{\\top}p)$ and select $i_{t}\\sim p_{t}$ \uff1b Observe $u_{t,i_{t},n}$ for all $n\\in[N]$ \uff1a Updatecounters $N_{t+1,i_{t}}=N_{t,i_{t}}+1$ and $N_{t+1,i}=N_{t,i}$ for $i\\neq i_{t}$ Update upper confidence utility matrix: $\\widehat{u}_{t+1,i,n}=\\bar{u}_{t,i,n}+4\\sqrt{\\frac{\\bar{u}_{t,i,n}\\log(N K T^{2})}{N_{t+1,i}}}+\\frac{8\\log(N K T^{2})}{N_{t+1,i}},$ (4) for ll $n\\in[N]$ $i\\in[K]$ where $\\begin{array}{r}{\\bar{u}_{t,i,n}=\\frac{1}{N_{t+1,i}}\\sum_{\\tau\\leq t}u_{\\tau,i,n}\\mathbb{1}\\{i_{\\tau}=i\\}.}\\end{array}$   \nend ", "page_idx": 4}, {"type": "text", "text": "as we will soon show, the regret is either $\\Theta(T^{\\frac{N-1}{N}})$ in the stochastic setting or even $\\Omega(T)$ in the adversarial setting. Therefore, in a sense our problem is much more difficult than BCO. For more details on BCO, we refer the reader to a recent survey by Lattimore [2024]. ", "page_idx": 4}, {"type": "text", "text": "3  Stochastic Environments with Bandit Feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we consider regret minimization over $f=\\mathrm{NSW}$ with bandit feedback in the stochastic setting, where the utility matrix $u_{t}$ at each round $t\\in[T]$ is i.i.d. drawn from a distribution with mean $u$ . Again, this is the same setup as Hossain et al. [202i], Jones et al. [2023] except that $\\mathrm{NSW}_{\\mathrm{prod}}$ is replaced with Nsw. ", "page_idx": 4}, {"type": "text", "text": "3.1 Upper Bound: a Refined Analysis of UCB with a Bernstein-Type Confidence Set ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by describing our algorithm and its regret guarantee, followed by discussion on what the key ideas are and how the algorithm/analysis is different from previous work. Specifically, our algorithm, shown in Algorithm 1, is based on the classic UCB algorithm. It starts by picking each action for $N_{0}=\\widetilde{\\mathcal{O}}(1)$ rounds. After this warm-up phase, at each time $t$ the algorithm picks the optimal strategy $p_{t}$ that maximizes the NsW with respect to some upper confidence utility matrix $\\widehat{\\boldsymbol{u}}_{t}$ . After sampling an action $i_{t}\\sim p_{t}$ , the algorithm observes the utility of each agent for action $i_{t}$ and then updates the upper confidence utility matrix $\\widehat{u}_{t+1}$ as the empirical average utility plus a certain Bernstein-type confidence width (Eq. (4)). ", "page_idx": 4}, {"type": "text", "text": "The following theorem shows that Algorithm 1 guarantees $\\widetilde{\\mathcal{O}}(K^{\\frac{2}{N}}T^{\\frac{K-1}{K}})$ expected regret (with $f$ ,in the definition of $\\mathrm{Reg}_{\\mathrm{sto}}$ , set to NSW; the same below unless stated otherwise). The fullproof can be found in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Wit $h\\;N_{0}=1+18\\log\\,K T,A l g o r i t h m\\;I\\;g u a r a n t e e s\\,\\mathbb{E}\\left[\\mathrm{Reg}_{\\mathrm{sto}}\\right]=\\widetilde{\\mathcal{O}}(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}+K).$ ", "page_idx": 4}, {"type": "text", "text": "Other than replacing $\\mathrm{NSW}_{\\mathrm{prod}}$ with NsW, our algorithm differs from that of Jones et al. [2023] in the form of the confidence width, and the analysis sketch below explains why we need this change. Specifically, for either $f=\\mathrm{NSW_{prod}}$ or $f=\\mathrm{NSW}$ , standard analysis of UCB states that the regret is bounded by $\\begin{array}{r}{\\sum_{t=1}^{T}\\left|f(\\widehat{u}_{t}^{\\top}p_{t})-f(u^{\\top}p_{t})\\right|}\\end{array}$ . When $f$ .is $\\mathrm{NSW}_{\\mathrm{prod}}$ , a Lipschitz function, Hossain et al. [2021, Lemma 3] shows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left|\\mathrm{NSW}_{\\mathrm{prod}}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}_{\\mathrm{prod}}(u^{\\top}p_{t})\\right|\\leq\\sum_{n=1}^{N}\\sum_{i=1}^{K}p_{t,i}\\left|\\widehat{u}_{t,i,n}-u_{i,n}\\right|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the rest of the analysis follows by direct calculations. However, when $f$ is NsW, a non-Lipschitz function, we cannot expect something similar to Eq. (5) anymore. Indeed, direct calculation shows that the Lipschitzconstant of $\\mathrm{NSW}(u^{\\top}p)$ with respect to $u_{:,n}$ equals to $\\begin{array}{r}{\\Theta\\left(\\sum_{n=1}^{N}\\left\\langle p,u_{:,n}\\right\\rangle^{-\\frac{N-1}{N}}\\right)}\\end{array}$ which can be arbitrarily large when $\\langle p,u_{:,n}\\rangle$ is close to O for some $n\\in[N]$ and $N\\geq2$ ", "page_idx": 4}, {"type": "text", "text": "To handle this issue, we require a more careful analysis. Specifically, using Freedman's inequality, we know that with a high probability, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{u}_{t,i,n}\\in\\left[u_{i,n},u_{i,n}+8\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{N_{t,i}}}+\\widetilde{\\mathcal{O}}\\left(1/N_{t,i}\\right)\\right]\\subseteq\\left[u_{i,n},2u_{i,n}+\\widetilde{\\mathcal{O}}\\left(1/N_{t,i}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With the help of Eq. (6), we consider two different cases at each round $t$ . The first case is that there exists certain $n\\in[N]$ such that $\\langle p_{t},u_{:,n}\\rangle\\leq\\sigma$ for some $\\sigma>0$ to be chosen later. In this case, we use Eq. (6) to show ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigl|\\mathrm{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\bigr|\\leq\\mathcal{O}\\left(\\mathrm{NSW}(u^{\\top}p_{t})\\right)+\\tilde{\\mathcal{O}}\\left(\\bigg(\\displaystyle\\sum_{i=1}^{K}\\frac{p_{t,i}}{N_{t,i}}\\bigg)^{\\frac{1}{N}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sigma^{\\frac{1}{N}}+\\tilde{\\mathcal{O}}\\left(\\bigg(\\displaystyle\\sum_{i=1}^{K}\\frac{p_{t,i}}{N_{t,i}}\\bigg)^{\\frac{1}{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first inequality uses Eq. (6) and the second inequality is because $\\mathrm{NSW}(\\boldsymbol{u}^{\\top}\\boldsymbol{p}_{t})\\leq\\left\\langle\\boldsymbol{p}_{t},\\boldsymbol{u}_{n}\\right\\rangle^{\\frac{1}{N}}$ for any $n\\in[N]$ . For the second term in Eq. (7), a standard analysis shows that it is upper bounded by $\\widetilde{\\mathcal{O}}\\big(K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}\\big)$ ", "page_idx": 5}, {"type": "text", "text": "Now we consider the case where $\\langle p_{t},u_{:,n}\\rangle\\geq\\sigma$ for all $n\\in[N]$ . In this case, via a decomposition lemma (Lemma C.1), we show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\Big|\\leq\\sum_{n=1}^{N}\\left[\\langle p_{t},\\widehat{u}_{t,:,n}\\rangle^{\\frac{1}{N}}-\\langle p_{t},u_{:,n}\\rangle^{\\frac{1}{N}}\\right]=\\mathcal{O}\\left(\\sum_{n=1}^{N}\\frac{\\langle p_{t},\\widehat{u}_{t,:,n}-u_{:,n}\\rangle}{N\\,\\langle p_{t},u_{:,n}\\rangle^{\\frac{N-1}{N}}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To bound Eq. (8), we use Eq. (6) again: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\langle p_{t},\\widehat{u}_{t,:,n}-u_{:,n}\\rangle}{\\langle p_{t},u_{:,n}\\rangle^{\\frac{N-1}{N}}}\\leq\\mathcal{O}\\left(\\frac{1}{\\langle p_{t},u_{:,n}\\rangle^{\\frac{N-1}{N}-\\frac{1}{2}}}\\sum_{i=1}^{K}\\sqrt{\\frac{p_{t,i}}{N_{t,i}}}\\right)\\leq\\mathcal{O}\\left(\\sigma^{\\frac{1}{2}-\\frac{N-1}{N}}\\sum_{i=1}^{K}\\sqrt{\\frac{p_{t,i}}{N_{t,i}}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the last inequality is due to the condition $\\langle p_{t},u_{:,n}\\rangle\\geq\\sigma$ for all $n\\in[N]$ . Finally, combining Eq. (7), Eq. (8), Eq. (9), followed by direct calculations, we show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{Reg}_{\\mathrm{sto}}\\right]\\leq\\sum_{t=1}^{T}\\left|\\mathbf{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathbf{NSW}(u^{\\top}p_{t})\\right|\\leq\\widetilde{\\mathcal{O}}\\left(T\\sigma^{\\frac{1}{N}}+K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}+\\sigma^{\\frac{1}{2}-\\frac{N-1}{N}}K\\sqrt{T}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Picking the optimal choice of $\\sigma$ finishes the proof. ", "page_idx": 5}, {"type": "text", "text": "We now highlight the importance of using a Bernstein-type confidence width in Eq. (4): if the standard Hoeffding-tpe confidence widthis used instead, then one can oly obtain $\\begin{array}{r l}{\\widehat{u}_{t,i,n}-u_{i,n}\\le\\mathcal{O}(\\sqrt{\\frac{1}{N_{t,i}}})}\\end{array}$ and consequently, Eq. (8) can only be bounded by $\\mathcal{O}\\left(\\sigma^{-\\frac{N-1}{N}}\\sqrt{K T}\\right)$ after taking summation over $t\\in[T]$ . This eventually leads to a worse regret bound of $\\widetilde{\\mathcal{O}}(K^{\\frac{1}{2N}}T^{\\frac{2N-1}{2N}})$ ", "page_idx": 5}, {"type": "text", "text": "3.2 Lower Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we prove an $\\widetilde\\Omega(T^{\\frac{N-1}{N}})$ lower bound for this setting. This not only shows that the regret bound we achieve via Algorithm 1 is tight in $T$ , but also highlights the difference and difficulty of learning with NSW compared to learning with $\\mathrm{NSW}_{\\mathrm{prod}}$ , since in the latter case, $\\Theta({\\sqrt{T}})$ regret is minimax optimal [Hossain et al., 2021, Jones et al., 2023]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. In the bandit feedback setting, for any algorithm, there exists a stochastic environment in which the expected regret (with respect to NSW) of this algorithm is $\\begin{array}{r}{\\Omega\\left(\\frac{(\\log K)^{3}}{N^{3}}\\cdot K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}\\right)f\\!o r}\\end{array}$ $N\\geq\\log K$ andsufficientlylarge $T$ ", "page_idx": 5}, {"type": "text", "text": "We defer the full proof to Appendix A.2 and discuss the hard instance used in the proof below. First, the mean utility vector $u_{:,n}$ for each agent $n\\geq2$ is a constant vector 1. This makes the problem equivalent toa on-agent problm,bt with $\\left\\langle p,u_{:,1}\\right\\rangle^{1/N}$ as the reward, instead of $\\langle p,u_{:,1}\\rangle$ as in standard stochastic $K$ -armed bandits. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Then, for the first agent, different from the standard $K$ -armed bandits, where the hardest instance is to hide one arm with a slightly better expected reward of $\\textstyle{\\frac{1}{2}}+{\\sqrt{K/T}}$ among other $K-1$ arms with expected reward of exactly $\\frac{1}{2}$ 2 we hide one arm with expected reward $K/T$ among other $K-1$ arms with exactly O reward (so overall the rewards are shifted towards O but with a smaller gap between the best arm and the others). By standard information theory arguments, within $T$ rounds the learner cannot distinguish the best arm from the others. Therefore, the best strategy she can apply is to pick a uniform distribution over actions, suffering $\\Omega((1-K^{-\\frac{1}{N}})\\cdot(K/T)^{\\frac{1}{N}})^{-}\\widetilde\\Omega(K^{\\frac{1}{N}}\\bar{T^{-\\frac{1}{N}}})$ regret per round and leading to $\\widetilde\\Omega(K^{\\frac{1}{N}}T^{\\frac{N-1}{N}})$ regret overall. ", "page_idx": 6}, {"type": "text", "text": "4   Adversarial Environments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now that we have a complete answer for the stochastic setting, we move on to consider the adversarial casewhereeach $u_{t}$ is chosen arbitrarily, a multi-agent generalization of the expert problem (fullinformation feedback) [Freund and Schapire, 1997] and the adversarial multi-armed bandit problem (bandit feedback) [Auer et al., 2002b]. There are no prior studies on this problem, be it with $\\bar{f}=\\mathrm{NSW}$ Or $f=\\mathrm{NSW_{prod}}$ , as far as we know. ", "page_idx": 6}, {"type": "text", "text": "4.1 Impossibility Results with Bandit Feedback ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We start by considering the bandit feedback setting. As mentioned in Section 2, even though NSW is a concave function, our problem is not an instance of Bandit Convex Optimization, since we can only observe $u_{t,i_{t},}$ : instead of $\\mathrm{NSW}(u_{t}^{\\top}p_{t})$ at the end of round $t$ . Somewhat surprisingly, this slight difference in the feedback in fact makes a sharp separation in learnability \u2014 while $\\mathcal{O}(\\sqrt{T})$ regret is achievable in BCO, we prove that $o(T)$ regret is impossible in our problem. ", "page_idx": 6}, {"type": "text", "text": "Before showing the theorem and its proof, we first give high level ideas on the construction of the hard environments. Specifically, we consider the environment with 2 agents, 2 arms, and binary utility matrix $u_{t}\\in\\{0,1\\}^{2\\times2}$ Similar to the hard instance in the stochastic environment, we set $u_{t,:,2}=\\mathbf{1}$ reducing the problem to a single-agent one. For the first agent, we let $u_{t,:,1}$ at each round $t$ be i.i.d. drawn from a stationary distribution over the 4 binary utility vectors $\\{(0,0),(0,1),(1,0),(1,1)\\}$ Then, we construct two different distributions, $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ over these 4 binary utility vectors satisfying that: 1) the distribution of the learner's observation is identical for $\\mathcal{E}$ and $\\mathcal{E}^{\\prime};2\\$ the optimal strategy for $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ are significantly different. The first property guarantees that no algorithm can distinguish these two environments, while the second property ensures that there is no one single strategy that can perform well in both environments. Formally, we prove the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. In the bandit feedback setting, for any algorithm, there exists an adversarial environmentsuchthat $\\mathbb{E}[\\mathbf{Reg}_{\\mathrm{adv}}]=\\Omega(T)$ for $f=\\mathrm{NSW}$ ", "page_idx": 6}, {"type": "text", "text": "Proof. As sketched earlier, we consider two different environments with 2 agents, 2 arms, and binary utility matrices $u_{t}~\\in~\\{0,1\\}^{2\\times2}$ \uff0c\uff0c $t~\\in~[T]$ . In both environments, we have $u_{t,:,2}~=~\\mathbf{1}$ Next, we construct two different distributions from which $u_{t,:,1}$ is potentially drawn from, $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ , over $\\{(0,0),(0,1),(1,0),(1,1)\\}$ : Specifically, $\\mathcal{E}$ is characterized by $(q_{00},q_{01},q_{10},q_{11})=$ $\\left({\\frac{4}{10}},\\,{\\frac{2}{10}},\\,{\\frac{1}{10}},\\,{\\frac{3}{10}}\\right)$ , where $q_{x y}$ is the probability of the vector $(x,y)$ in $\\mathcal{E};\\;\\mathcal{E}^{\\prime}$ is characterized by $\\bigl(q_{00}^{\\prime},q_{01}^{\\prime},q_{10}^{\\prime},q_{11}^{\\prime}\\bigr)\\;=\\;\\bigl({\\textstyle\\frac{3}{10}},{\\textstyle\\frac{3}{10}},{\\textstyle\\frac{2}{10}},{\\textstyle\\frac{2}{10}}\\bigr)$ where $q_{x y}^{\\prime}$ isthe probabityof vetor $(x,y)$ ${\\mathcal{E}}^{\\prime}$ With a slight abuse of notation, we write $u\\sim\\mathcal{E}$ for a matrix $u\\in\\{0,1\\}^{2\\times2}$ if $u_{:,1}$ is drawn from $\\mathcal{E}$ and $u_{;,2}=1$ ; the same for ${\\mathcal{E}}^{\\prime}$ ", "page_idx": 6}, {"type": "text", "text": "We argue that the learner's observations are equivalent in distribution in $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ , since the marginal distributions of the utility of each action are the same. Specifically, ", "page_idx": 6}, {"type": "text", "text": "\u00b7 When action 1 is chosen, the distributions of the learner's observation in both $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ are a Bernoulli random variable with mean Q1o + Q11 = Q1o + Q11 = 10; ", "page_idx": 6}, {"type": "text", "text": "\u00b7 When action 2 is chosen, the distributions of the learner's observation in both $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ are a Bernoullirandom variable with mean qo1 + Q11 = 91 + Q11 = 1\u00b7- ", "page_idx": 7}, {"type": "text", "text": "Direct calculation shows $\\begin{array}{r l r}{p_{\\star}\\!}&{=}&{\\!\\arg\\!\\operatorname*{max}_{p\\in\\Delta_{K}}\\mathbb{E}_{u\\sim\\mathcal{E}}\\left[\\mathrm{NSW}(u^{\\top}p)\\right]\\;\\;=\\;\\;\\left(\\frac{q_{10}^{2}}{q_{01}^{2}+q_{10}^{2}},\\frac{q_{01}^{2}}{q_{01}^{2}+q_{10}^{2}}\\right)\\;\\;=\\;}\\end{array}$ (0.2, 0.8) and $\\begin{array}{r}{p_{\\star}^{\\prime}=\\,\\mathrm{argmax}_{p\\in\\Delta_{K}}\\,\\mathbb{E}_{u\\sim\\mathcal{E}^{\\prime}}\\left[\\mathrm{NSW}(u^{\\top}p)\\right]\\,=\\,\\left(\\frac{q_{10}^{\\prime2}}{q_{10}^{\\prime2}+q_{01}^{\\prime2}},\\frac{q_{01}^{\\prime2}}{q_{10}^{\\prime2}+q_{01}^{\\prime2}}\\right)\\,=\\,(\\frac{4}{13},\\frac{9}{13}),}\\end{array}$ which are constant apart from each other. Pick a threshold value $\\begin{array}{r}{\\theta=\\frac{33}{130}\\in(0.2,\\frac{4}{13})}\\end{array}$ Direct calculation shows that for a strategy $p$ with $p_{1}\\geq\\theta$ , we have $\\mathbb{E}_{u\\sim\\mathcal{E}}[\\mathrm{NSW}(u^{\\top}p_{\\star})-\\mathrm{NSW}(u^{\\top}p)]\\ge\\Delta$ where $\\begin{array}{r}{\\Delta=\\frac{1}{500}}\\end{array}$ $p$ with $p_{1}<\\theta$ we have $\\mathbb{E}_{u\\sim\\mathcal{E}^{\\prime}}[\\mathrm{NSW}(u^{\\top}p_{\\star})-\\mathrm{NSW}(u^{\\top}p)]\\ge\\Delta$ as well. Now, given an algorithm, let $\\alpha_{\\mathcal{E}}$ be the probability that the number of rounds $p_{t,1}\\geq\\theta$ is larger than $\\textstyle{\\frac{T}{2}}$ under environment $\\mathcal{E}$ and $\\bar{\\alpha}_{\\mathcal{E}^{\\prime}}$ be the probability of the complement of this event under environment ${\\mathcal{E}}^{\\prime}$ . We have, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{E}}[\\mathrm{Reg}_{\\mathrm{adv}}]\\ge\\mathbb{E}_{\\mathcal{E}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{\\star})-\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right]\\ge\\frac{\\alpha_{\\mathcal{E}}T\\Delta}{2},}\\\\ &{\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathrm{Reg}_{\\mathrm{adv}}]\\ge\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{\\star}^{\\prime})-\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right]\\ge\\frac{\\bar{\\alpha}_{\\mathcal{E}^{\\prime}}T\\Delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Finally, since the feedback for the algorithm is the same in distribution in these two environments, we know $\\alpha_{\\mathcal{E}}+\\bar{\\alpha}_{\\mathcal{E}^{\\prime}}=1$ ,and thus ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathbb{E}_{\\mathcal{E}}[\\mathbf{Reg}_{\\mathrm{adv}}],\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{Reg}_{\\mathrm{adv}}]\\}\\ge\\frac{\\mathbb{E}_{\\mathcal{E}}[\\mathbf{Reg}_{\\mathrm{adv}}]+\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathbf{Reg}_{\\mathrm{adv}}]}{2}\\ge\\frac{(\\alpha_{\\mathcal{E}}+\\bar{\\alpha}_{\\mathcal{E}^{\\prime}})T\\Delta}{4}=\\Omega(T),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 7}, {"type": "text", "text": "In fact, by a similar but more involved construction (that actually requires using two agents in a non-trivial way), the same impossibility result also holds for $f=\\mathrm{NSW_{prod}}$ ; see Appendix B.1. We remark that non-linearity of $f$ in these results plays an important role in the hard instance construction, since otherwise, the optimal strategy for $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ will be the same as they both induce the same marginal distributions. ", "page_idx": 7}, {"type": "text", "text": "4.2Full-Information Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To sidestep the impossibility result due to the bandit feedback, we shift our focus to the fullinformation feedback model, where the learner observes the entirety of the utility matrix $u_{t}$ at the end of round $t$ . As mentioned, this corresponds to a multi-agent generalization of the well-known expert problem [Freund and Schapire, 1997]. We propose several algorithms for this setting, showing that the richer feedback not only makes learning possible but also leads to much lower regret. ", "page_idx": 7}, {"type": "text", "text": "4.2.1 FTRL with Log-Barrier Regularizer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When $f$ is concave, our problem is in fact also an instance of the well-known Online Convex Optimization (OCO) [Zinkevich, 2003]. However, standard OCO algorithms such as Online Gradient Descent, an instance of the more general Follow-the-Regularized-Leader algorithm with a $\\ell_{2}$ regularizer, require the utility function to also be Lipschitz and thus cannot be directly applied to learning NSW. Nevertheless, we will show that using a different regularizer that induces more stability than the $\\ell_{2}$ regularizer can resolve this issue. ", "page_idx": 7}, {"type": "text", "text": "More specifically, the FTRL algorithm is shown in Algorithm 2, which predicts at time $t$ the distribution $\\begin{array}{r}{p_{t}=\\operatorname{argmin}_{p\\in\\Delta_{K}}\\overbar{\\langle p,-\\sum_{s=1}^{t-1}}\\nabla f(u_{s}^{\\top}p_{s})\\rangle+\\overbar{\\frac{1}{\\eta}}\\psi(p)}\\end{array}$ for some learning rate $\\eta$ and some strongly convex regularizer $\\psi$ . Standard analysis shows that the regret of FTRL contains two terms: the regularization penalty term that is of order $1/\\eta$ and the stability term that is of order $\\begin{array}{r}{\\eta\\sum_{t}\\|\\nabla f(u_{t}^{\\top}p_{t})\\|_{\\nabla^{-2}\\psi(p_{t})}^{2}}\\end{array}$ where we use the notation $||\\boldsymbol{a}||_{M}=\\sqrt{\\boldsymbol{a}^{\\top}M\\boldsymbol{a}}$ To deal with the lack the Lipschitzness, that is, the potentially large $\\nabla f(u_{t}^{\\top}p_{t})$ , we need to find a regularizer $\\psi$ so that the induced local norm $\\|\\nabla f(u_{t}^{\\top}p_{t})\\|_{\\nabla^{-2}\\psi(p_{t})}$ is always reasonably small despite $\\nabla f(u_{t}^{\\top}p_{t})$ being large (in $\\ell_{2}$ norm for example). ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 FTRL for $N$ -agent $K$ -armed SWF maximization with full-info feedback ", "page_idx": 8}, {"type": "text", "text": "Inputs: a SWF $f$ , a learning rate $\\eta>0$ , and a strongly convex regularizer $\\overline{{\\psi:\\Delta_{K}\\rightarrow\\mathbb{R}}}$   \nfor $t=1,2,\\ldots,T$ do Play $\\begin{array}{r}{p_{t}=\\arg\\!\\operatorname*{min}_{p\\in\\Delta_{K}}\\langle p,-\\sum_{s=1}^{t-1}\\nabla f(u_{s}^{\\top}p_{s})\\rangle+\\frac{1}{\\eta}\\psi(p).}\\end{array}$ Observe ut.   \nend ", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{\\psi(p)=-\\sum_{i=1}^{K}\\log p_{i}}\\end{array}$ induces a small local norm not just for NsW, but also for a broad family of SWFs as long as they are concave and Pareto optimal \u2014 an SWF $f:[0,1]^{N}\\to[0,1]$ is Pareto optimal if for two utility vectors $x$ and $y$ such that $x_{n}\\geq y_{n}$ for all $i\\in[N]$ , we have $f\\!\\left(x\\right)\\geq f\\!\\left(y\\right)$ . NSW is clearly in this family, and there are many other standard fairness measures that fall into this class; see Appendix B.2.1. For any SWF in this family, we prove the following regret bound, which remarkably has no dependence on the number of agents $N$ at all. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. For any $f\\,:\\,[0,1]^{N}\\,\\,\\rightarrow\\,\\,[0,1]$ that is concave and Pareto optimal, Algorithm 2 with the log-barrier regularizer $\\begin{array}{r}{\\psi(p)\\,=\\,-\\sum_{i=1}^{K}\\log p_{i}}\\end{array}$ and $\\begin{array}{r}{\\eta=\\sqrt{\\frac{K\\log T}{T}}}\\end{array}$ guarantees ${\\tt R e g}_{\\mathrm{adv}}=$ $\\mathcal{O}(\\sqrt{K T\\log T})$ ", "page_idx": 8}, {"type": "text", "text": "Proof Sktch.Usingthe conecreform of $\\psi$ itis lea thathe local norm $\\|\\nabla f(u_{t}^{\\top}p_{t})\\|_{\\nabla^{-2}\\psi(p_{t})}^{2}$ simplifies to $\\begin{array}{r}{\\sum_{i=1}^{K}p_{t,i}^{2}[\\nabla f(u_{t}^{\\top}p_{t})]_{i}^{2}\\leq\\left\\langle p_{t},\\nabla f(u_{t}^{\\top}p_{t})\\right\\rangle^{2}}\\end{array}$ where the inequality is due to $[\\nabla f(u_{t}^{\\top}p_{t})]_{i}\\geq$ O implied by Pareto optimality. Furthermore, by concavity, we have $\\left\\langle p_{t},\\nabla f(u_{t}^{\\top}p_{t})\\right\\rangle\\leq f(u_{t}^{\\top}p_{t})-$ $f(0)\\leq1$ , and thus the local norm at most 1. The rest of the proof is by direct calculation. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "4.2.2 FTRL with Tsallis Entropy Regularizer ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In fact, when $f=\\mathrm{NSW}$ , using the special structure of the welfare function, we find yet another regularizer that ensures a small $\\mathcal{O}(N)$ local norm, with the benefit of having smaller dependence on $K$ for the penalty term. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.3. For $f=\\mathrm{NSW},$ Algorithm 2 with the Talis entropy regularizer $\\begin{array}{r}{\\psi(p)=\\frac{1-\\sum_{i=1}^{K}p_{i}^{\\beta}}{1-\\beta}}\\end{array}$ $\\begin{array}{r}{\\beta=\\frac{2}{N}}\\end{array}$ , and the optimal choice of  guarantees $\\mathrm{Reg_{adv}}=\\widetilde{\\mathcal{O}}(K^{\\frac{1}{2}-\\frac{1}{N}}\\sqrt{N T})$ ", "page_idx": 8}, {"type": "text", "text": "The proof is more involved and is deferred to Appendix B.2.3. While the regret in Theorem 4.3 suffers polynomial dependence on $N$ , it has better dependence on $K$ compared to Theorem 4.2, and is thus more preferable when $K$ is much larger than $N$ ", "page_idx": 8}, {"type": "text", "text": "4.2.3 Logarithmic Regret for a Special Case ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we discuss a special case with $f=\\mathrm{NSW}$ where logarithmic regret is possible. This is based on a simple observation that when there is one agent who is indifferent about the learner's choice (that is, the agent's utility is the same for all arms for this round), then $-\\mathrm{NSW}$ is not only convex, but also exp-concave, a stronger curvature property. Therefore, by applying known results, specifically the EWOO algorithm [Hazan et al., 2007], we achieve the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4. Fix $f=\\mathrm{NSW}$ Suppose that for each time $t$ there is a set of agents $A_{t}\\subseteq[N]$ such that $\\left|A_{t}\\right|\\geq M$ and $u_{t,:,n}=c_{t,n}{\\bf1}$ with $c_{t,n}\\geq0$ for each agent $n\\in A_{t}$ . Then the EWOO algorithm guarantes $\\begin{array}{r}{{\\mathrm{Reg}}_{\\mathrm{adv}}={\\mathcal{O}}\\left({\\frac{N-M}{M}}\\cdot K\\log T\\right)}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "The proof, which verifies the exp-concavity of $-\\mathrm{NSW}$ in this special case, can be found in Appendix B.2.4. We note that the reason that we apply EWOO instead of Online Newton Step, another algorithm discussed in [Hazan et al., 2007] for exp-concave losses, is that the latter requires Lipschizness (which, again, is not satisfied by NSW). ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, motivated by recent research on social welfare maximization for the problem of multiagent multi-armed bandits, we consider a variant with the arguably more natural version of Nash social welfare as the objective function, and develop multiple algorithms and regret upper/lower bounds in different settings (stochastic versus adversarial and full-information versus bandit feedback). Our results show a sharp separation between our problem and previous settings, including the heavily studied Bandit Convex Optimization problem. ", "page_idx": 9}, {"type": "text", "text": "There are many interesting future directions. First, in the stochastic bandit setting, we have only shown the tight dependence on $T$ ,sowhat about $K$ and $N?$ Second, is there a more general strategy/analysis that works for different social welfare functions (similar to our result in Theorem 4.2)? Taking one step further, similar to the recent research on \u201comniprediction\" [Gopalan et al., 2022], is there one single algorithm that works for a class of social welfare functions simultaneously? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "HL and MZ are supported by NSF Award IS-1943607. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \nAlekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In Conference on Learning Theory, pages 12-38. PMLR, 2017.   \nJean-Yves Audibert and Sebastien Bubeck. Regret bounds and minimax policies under partial monitoring. The Journal of Machine Learning Research, 11:2785-2836, 2010.   \nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235-256, 2002a.   \nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002b.   \nJackie Baek and Vivek Farias. Fair exploration via axiomatic bargaining. Advances in Neural Information Processing Systems, 34:22034-22045, 2021.   \nSiddharth Barman, Sanath Kumar Krishnamurthy, and Rohit Vaish. Finding fair and efficient allocations. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 557-574,2018.   \nSiddharth Barman, Arindam Khan, Arnab Maiti, and Ayush Sawarni. Fairness and welfare quantification for regret in multi-armed bandits. AAAI'23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i6.25829.   \nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 19-26. JMLR Workshop and Conference Proceedings, 2011.   \nRobert Busa-Fekete, Balazs Szorenyi, Paul Weng, and Shie Mannor. Multi-objective bandits: Optimizing the generalized gini index. In International Conference on Machine Learning, pages 625-634. PMLR, 2017.   \nIoannis Caragiannis, David Kurokawa, Herve Moulin, Ariel D. Procaccia, Nisarg Shah, and Junxing Wang. The unreasonable fairness of maximum nash welfare. ACM Trans. Econ. Comput., 7(3), sep 2019. ISSN 2167-8375. doi: 10.1145/3355902.   \nViolet Xinying Chen and J.N. Hooker. A guide to formulating fairness in an optimization model. Annals of Operation Research, 326:581-619, 2023.   \nYifang Chen, Alex Cullar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis. Fair contextual multi-armed bandits: Theory and experiments. In Conference on Uncertainty in Artificial Intelligence, pages 181-190. PMLR, 2020.   \nRichard Cole and Vasilis Gkatzelis. Approximating the nash social welfare with indivisible items. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 371-380, 2015.   \nMadalina M Drugan and Ann Nowe. Designing multi-objective multi-armed bandits algorithms: A study. In The 2013 international joint conference on neural networks (IJCNN), pages 1-8. IEEE, 2013.   \nDylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. Advances in Neural Information Processing Systems, 29, 2016.   \nYoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.   \nJugal Garg, Pooja Kulkarni, and Rucha Kulkarni. Approximating nash social welfare under submodular valuations through (un) matchings. ACM Transactions on Algorithms, 19(4):1-25, 2023.   \nStephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an unknown fairness metric. Advances in neural information processing systems, 31, 2018.   \nParikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022), volume 215, pages 79:1-79:21, 2022.   \nElad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169-192, 2007.   \nElad Hazan et al. Introduction to online convex optimization. Foundations and Trends? in Optimization, 2(3-4):157-325, 2016.   \nSafwan Hossain, Evi Micha, and Nisarg Shah. Fair algorithms for multi-agent multi-armed bandits. Advances in Neural Information Processing Systems, 34:24005-24017, 2021.   \nMatthew Jones, Huy Nguyen, and Thy Nguyen. An efficient algorithm for fair multi-agent multiarmed bandit with low regret. InProceedings of the AAAl Conference on Artificial Inteligence, volume 37, pages 8159-8167, 2023.   \nMathew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \nMamoru Kaneko and Kenjiro Nakamura. The nash social welfare function. Econometrica, 47: 423-435, 1979.   \nRobert Klinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Joural of the ACM (JACM), 66(4):1-77, 2019.   \nTze Leung Lai and Herbert Robbins. Asymptotically effcient adaptive allocation rules. Advances in applied mathematics, 6(1):4-22, 1985.   \nTor Lattimore. Bandit convex optimisation. arXiv preprint arXiv:2402.06535, 2024.   \nTor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \nChung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. Advances in neural information processing systems, 33:15522-15533,2020.   \nWenzheng Li and Jan Vondrak. Estimating the nash social welfare for coverage and other submodular valuations. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1119-1130. SIAM, 2021.   \nYang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875, 2017.   \nHaipeng Luo. Lecture note 13, Introduction to Online Learning. 2017. URL https : / / haipeng-luo.net/courses/CscI699/lecture13.pdf. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Herve Moulin. Fair division and collective welfare. MIT press, 2004 ", "page_idx": 11}, {"type": "text", "text": "John F Nash. The bargaining problem. Econometrica, 18(2):155-162, 1950. ", "page_idx": 11}, {"type": "text", "text": "Ayush Sawarni, Soumyabrata Pal, and Siddharth Barman. Nash regret guarantees for linear bandits. Advances in Neural Information Processing Systems, 36, 2024.   \nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933.   \nChen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning Theory, pages 1263-1291. PMLR, 2018.   \nJulian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 467-475. PMLR, 2019.   \nMartin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03),pages 928-936, 2003. ", "page_idx": 11}, {"type": "text", "text": "A Omitted Details in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide the omitted proofs for the results in Section 3. In Appendix A.1, we provide the proof for Theorem 3.1 and in Appendix A.2, we provide the proof for Theorem 3.2. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To prove Theorem 3.1, we first consider the following two events. ", "page_idx": 12}, {"type": "text", "text": "Event 1. For all $t\\in\\{K N_{0}+1,\\ldots,T\\}$ and $i\\in[K],$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nN_{t,i}\\geq N_{0}+\\frac{1}{2}\\sum_{\\tau=K N_{0}}^{t}p_{\\tau,i}-18\\log(K T),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $N_{t,i}$ 's and $p_{t,i}$ 's are defined in Algorithm $^{\\,l}$ ", "page_idx": 12}, {"type": "text", "text": "Event 2. For all $t\\in\\{K N_{0}+1,\\ldots,T\\}$ $i\\in[K]$ and $n\\in[N]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nu_{i,n}\\leq\\widehat{u}_{t,i,n}\\leq u_{i,n}+8\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{15\\log(N K T^{2})}{N_{t,i}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $N_{t,i}$ 's are defined in Algorithm $^{\\,l}$ ", "page_idx": 12}, {"type": "text", "text": "As we prove in Lemma A.1 and Lemma A.2, Event 1 and Event 2 hold with probability at least $\\textstyle1-{\\frac{1}{T}}$ Now we prove Theorem 3.1. For convenience, we restate the theorem as follows. ", "page_idx": 12}, {"type": "text", "text": "Theorem3.1.With $N_{0}=1+18\\log K T,$ AlgorithmIguaranteesI $\\mathfrak{T}[\\mathrm{Reg}_{\\mathrm{sto}}]=\\widetilde{\\mathcal{O}}(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}+K).$ ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $p^{\\star}\\,=\\,\\mathrm{argmax}_{p\\in\\Delta_{K}}\\,\\mathbf{NSW}(u^{\\top}p)$ According to a standard regret decomposition for UCB-type algorithms, we know that $\\mathbf{Reg}_{\\mathrm{sto}}$ can be upper bounded as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}\\left[\\underset{t=K N_{0}+1}{\\sum}\\left(\\mathrm{NSW}(u^{\\top}p^{*})-\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p^{*})\\right)\\;\\Bigg|\\mathrm{Event~1~and~Event~2~hold}\\Bigg]+K N_{0}+2}\\\\ &{\\qquad+\\mathbb{E}\\left[\\underset{t=K N_{0}+1}{\\sum}\\left(\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p^{*})-\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p_{t})\\right)\\;\\Bigg|\\mathrm{Event~1~and~Event~2~hold}\\Bigg]}\\\\ &{\\qquad+\\mathbb{E}\\left[\\underset{t=K N_{0}+1}{\\sum}\\left(\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\right)\\;\\Bigg|\\mathrm{Event~1~and~Event~2~hold}\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\left[\\underset{t=1}{\\sum}\\left(\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p^{*})-\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p_{t})\\right)\\Bigg]+\\mathbb{E}\\left[\\underset{t=1}{\\sum}\\left(\\mathrm{NSW}(\\hat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\right)\\Bigg]+K N_{0}+2}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In the following, we bound the first term ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=K N_{0}+1}^{T}\\left(\\mathrm{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\right)~\\Big|~\\mathrm{Event~1~and~Event~2~hold}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As discussed in Section 3.1, we consider two cases. First, consider the set of rounds $\\mathcal{T}_{\\sigma}$ such that for all $t\\in\\mathcal T_{\\sigma}$ there exists at least one $n\\in[N]$ such that $\\langle p_{t},u_{:,n}\\rangle\\leq\\sigma$ for some $\\sigma$ that we will specify later. Denote such $n$ to be $n_{t}$ (if there are multiple such $n$ 's, we pick an arbitrary one). According to Event 2, we know that for all $i\\in[K]$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n{{t_{i,n}}_{t}}\\le{{\\widehat{u}}_{t,i,n_{t}}}\\le{{u}_{i,n_{t}}}+8\\sqrt{\\frac{{{u}_{i,n_{t}}}\\log(N K T^{2})}{{{N}_{t,i}}}}+\\frac{15\\log(N K T^{2})}{{{N}_{t,i}}}\\le2{{u}_{i,n_{t}}}+\\mathcal{O}\\left(\\frac{\\log(N K T^{2})}{{{N}_{t,i}}}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality is because of AM-GM inequality. Therefore, we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\langle p_{t},\\widehat{u}_{t,:,n_{t}}\\right\\rangle\\leq2\\left\\langle p_{t},u_{:,n_{t}}\\right\\rangle+\\widetilde{\\mathcal{O}}\\left(\\sum_{j=1}^{K}\\frac{p_{t,j}}{N_{t,j}}\\right)\\leq2\\sigma+\\widetilde{\\mathcal{O}}\\left(\\sum_{j=1}^{K}\\frac{p_{t,j}}{N_{t,j}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now consider $\\begin{array}{r}{\\sum_{t\\in\\mathcal{T}_{\\sigma}}\\left(\\mathrm{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\right)}\\end{array}$ . Direct calculation shows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\mathrm{since\\,\\,NSW}(u^{\\top}p_{t})\\geq0)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in T_{\\alpha}}\\left(\\mathrm{NSW}(\\widehat{u}_{t}^{T}p_{t})-\\mathrm{NSW}(u^{T}p_{t})\\right)}\\\\ &{\\le\\displaystyle\\sum_{t\\in T_{\\alpha}}\\mathbf{NSW}(\\widehat{u}_{t}^{T}p_{t})}\\\\ &{\\le\\displaystyle\\sum_{t\\in T_{\\alpha}}\\left(p_{t},\\widehat{u}_{t,\\cdot\\,n_{t}}\\right)^{\\star}}\\\\ &{\\leq2|T_{\\alpha}|\\cdot\\sigma^{\\star}+\\displaystyle\\sum_{t=1}^{T}\\tilde{\\sigma}\\left(\\left(\\displaystyle\\sum_{j=1}^{K}\\frac{p_{t,j}}{N_{t,j}}\\right)^{\\star}\\right)}\\\\ &{\\leq2|T_{\\alpha}|\\cdot\\sigma^{\\star}+T^{\\star}\\frac{1}{\\tilde{\\sigma}}\\left(\\left(\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{j=1}^{N_{t,j}}\\frac{p_{t,j}}{N_{t,j}}\\right)^{\\star}\\right)}\\\\ &{\\leq2T_{\\alpha}\\cdot\\sigma^{\\star}+\\displaystyle\\tilde{\\sigma}\\left(K^{\\star}\\cdot T^{\\star\\,n_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now consider the regret within $t\\in\\{N K_{0}+1,\\ldots,T\\}\\backslash T_{\\sigma}$ in which we have $\\langle p_{t},u_{:,n}\\rangle\\geq\\sigma$ for all $n\\in[N]$ . In this case, we bound $\\begin{array}{r}{\\sum_{t\\notin\\mathcal{T}_{\\sigma}}\\left(\\mathrm{NSW}(\\widehat{u}_{t}^{\\top}p_{t})-\\mathrm{NSW}(u^{\\top}p_{t})\\right)}\\end{array}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{t\\in\\mathbb{Z}}{\\sum}\\left(\\mathrm{NSW}(\\hat{\\boldsymbol{u}}_{t}^{\\top}\\boldsymbol{p}_{t})-\\mathrm{NSW}(\\boldsymbol{u}^{\\top}\\boldsymbol{p}_{t})\\right)}\\\\ &{\\le\\underset{t\\in\\mathbb{Z}}{\\sum}\\underset{n\\in\\mathbb{N}}{\\sum}\\left[\\left\\langle\\rho_{t},\\hat{u}_{t_{i},n}\\right\\rangle^{\\frac{1}{n}}-\\left\\langle\\rho_{t},u_{t_{i},n}\\right\\rangle^{\\frac{1}{n}}\\right]}&{\\mathrm{(using~Lemma~C.1~and~Event~2)}}\\\\ &{=\\underset{t\\in\\mathbb{Z}}{\\sum}\\underset{n\\in\\mathbb{N}}{\\sum}(\\boldsymbol{v}_{t}^{\\top}\\boldsymbol{\\hat{N}})\\frac{\\left\\langle\\rho_{t},\\hat{u}_{t_{i},n}-u_{t,n}\\right\\rangle}{\\sum_{k=0}^{N-1}\\left\\langle\\rho_{t},\\hat{u}_{t_{i},n}\\right\\rangle^{\\frac{1}{N}}\\left\\langle\\rho_{t},u_{t,n}\\right\\rangle^{\\frac{N-1-k}{N}}}}\\\\ &{\\le\\underset{t\\in\\mathbb{Z}}{\\sum}\\sum_{n\\in\\mathbb{N}}\\frac{\\left\\langle\\rho_{t},\\hat{u}_{t_{i},n}-u_{t,n}\\right\\rangle}{N\\left\\langle\\rho_{t},u_{t_{i},n}\\right\\rangle^{\\frac{1}{N}-1}}\\quad\\mathrm{(since~\\hat{u}}_{t_{i},n}\\ge u_{i,n}\\mathrm{~for~all~}i,t,n~\\mathrm{based~on~Event~2)}}\\\\ &{\\le\\underset{t\\in\\mathbb{Z}}{\\sum}\\sum_{n\\in\\mathbb{N}}\\frac{\\sum_{t\\in\\mathbb{N}}^{N}\\mu_{t,j}\\left(\\mathbf{s}\\sqrt{\\frac{u_{t,n}\\log(N K T^{2})}{N\\cdot t}}+\\frac{\\mathbf{sin}(N K T^{2})}{N_{t,j}}\\right)}{N\\left\\langle\\rho_{t},u_{t,n}\\right\\rangle^{\\frac{N-1}{N}}}\\quad\\mathrm{(using~\\ensuremath{Event}~2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\sum_{t\\notin{\\cal T}_{o}}\\sum_{n\\in[N]}\\left(\\frac{\\sum_{j=1}^{K}8\\sqrt{\\frac{p_{t,j}\\log(N K T^{2})}{N_{t,j}}}}{N\\,\\langle p_{t},u_{;n}\\rangle^{\\frac{N-1}{N}-\\frac{1}{2}}}+\\frac{\\sum_{j=1}^{K}\\frac{8p_{t,j}\\log(N K T^{2})}{N_{t,j}}}{N\\,\\langle p_{t},u_{;n}\\rangle^{\\frac{N-1}{N}}}\\right)}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(\\frac{1}{N\\sigma^{\\frac{N-1}{N}-\\frac{1}{2}}}\\sum_{t\\in{\\cal T}}\\sum_{n\\in[N]}^{K}\\sqrt{\\frac{p_{t,j}}{N_{t,j}}}+\\frac{1}{N\\sigma^{\\frac{N-1}{N}}}\\sum_{t=1}^{T}\\displaystyle\\sum_{n\\in[N]}^{K}\\sum_{j=1}^{p_{t,j}}\\frac{p_{t,j}}{N_{t,j}}\\right)}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(\\sigma^{\\frac{1}{2}-\\frac{N-1}{N}}K\\sqrt{T}+K\\cdot\\sigma^{-\\frac{N-1}{N}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality is because Lemma A.3. Combining the regret for both parts, we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathbf{Reg}_{\\mathrm{sto}}]\\leq\\tilde{\\mathcal{O}}\\left(K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}+T\\cdot\\sigma^{\\frac{1}{N}}+\\sigma^{\\frac{1}{2}-\\frac{N-1}{N}}K\\sqrt{T}+K\\sigma^{-\\frac{N-1}{N}}+K\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Picking the optimal $\\sigma$ leads to the expected regret bounded by $\\mathbb{E}[{\\mathrm{Reg}}_{\\mathrm{sto}}]\\leq{\\widetilde O}\\left(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}+K\\right)$ ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Event 1 happens with probability at least $\\textstyle1-{\\frac{1}{T}}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. According to Algorithm 1, we know that $N_{(K N_{0}+1),i}=N_{0}$ for each $i\\in[K]$ . Consider the case when $t\\geq K N_{0}+1$ . According to Freedman's inequality (Lemma C.3), we have with probability at least $1-\\delta$ , for a fixed $t\\geq K N_{0}+1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=K N_{0}+1}^{t}\\mathbb{1}\\{i_{\\tau}=i\\}\\geq\\displaystyle\\sum_{\\tau=K N_{0}+1}^{t}p_{\\tau,i}-2\\sqrt{\\sum_{\\tau=K N_{0}}^{t}p_{\\tau,i}\\log(1/\\delta)}-\\log(1/\\delta)}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\sum_{\\tau=K N_{0}+1}^{t}p_{\\tau,i}-9\\log(1/\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we know that with probability at least $1-\\delta$ , for a fixed $t\\geq K N_{0}+1$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\nN_{t,i}=N_{0}+\\sum_{\\tau=K N_{0}+1}^{t}\\mathbb{1}\\{i_{\\tau}=i\\}\\geq N_{0}+\\frac{1}{2}\\sum_{\\tau=K N_{0}+1}^{t}p_{\\tau,i}-9\\log(1/\\delta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Picking $\\begin{array}{r}{\\delta\\,=\\,\\frac{1}{K T^{2}}}\\end{array}$ and aking a union boundoverll $i\\,\\in\\,[K]$ and $K N_{0}+1\\,\\leq\\,t\\,\\leq\\,T$ reach he ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Event 2 happens with probability at least $\\textstyle1-{\\frac{1}{T}}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. According to Freedman's inequality Lemma C.3, applying a union bound over $t\\,\\in\\,[T]$ $i\\in[K]$ , and $n\\in[N]$ , we know that with probability at least $1-\\delta$ , for all $t\\in[T],\\,i\\in[K]$ ,and $n\\in[N]$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\bar{u}_{t,i,n}-u_{i,n}\\right|\\leq2\\sqrt{\\frac{u_{i,n}\\log(N K T/\\delta)}{N_{t,i}}}+\\frac{\\log(N K T/\\delta)}{N_{t,i}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Solving the inequality with respect to $u_{i,n}$ , we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{u_{i,n}}\\le\\sqrt{\\frac{\\log(N K T/\\delta)}{N_{t,i}}}+\\sqrt{\\bar{u}_{t,i,n}+\\frac{2\\log(N K T/\\delta)}{N_{t,i}}}}\\\\ &{\\qquad\\le\\sqrt{2\\bar{u}_{t,i,n}+\\frac{6\\log(N K T/\\delta)}{N_{t,i}}},\\qquad\\qquad\\qquad\\qquad(\\mathrm{using~AM\\-GM~ine}\\:)}\\\\ &{\\qquad\\bar{u}_{t,i,n}\\le\\left(\\sqrt{u_{i,n}}+\\sqrt{\\frac{\\log(N K T/\\delta)}{N_{t,i}}}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "quality) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\leq2u_{i,n}+\\frac{2\\log(N K T/\\delta)}{N_{t,i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(using AM-GM inequality) ", "page_idx": 15}, {"type": "text", "text": "Using the above inequality and picking $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ , we can lower bound $\\widehat{u}_{t,i,n}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widehat{u}_{t,i,n}=\\bar{u}_{t,i,n}+4\\sqrt{\\frac{\\bar{u}_{t,i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{8\\log(N K T^{2})}{N_{t,i}}}\\\\ {\\geq\\bar{u}_{t,i,n}+4\\sqrt{\\frac{\\log(N K T^{2})}{N_{t,i}}}\\left(\\frac{u_{i,n}}{2}-\\frac{3\\log(N K T^{2})}{N_{t,i}}\\right)+\\frac{8\\log(N K T^{2})}{N_{t,i}}}\\\\ {\\geq\\bar{u}_{t,i,n}+4\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{2N_{t,i}}}+\\frac{(8-4\\sqrt{3})\\log(N K T^{2})}{N_{t,i}}}\\\\ {\\geq\\bar{u}_{t,i,n}+2\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{\\log(N K T^{2})}{N_{t,i}}}\\\\ {\\geq\\bar{u}_{t,i,n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality uses Eq. (10). To upper bound $\\widehat{u}_{t,i,n}$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{u}_{t,i,n}=\\bar{u}_{t,i,n}+4\\sqrt{\\frac{\\overline{{u}}_{t,i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{8\\log(N K T^{2})}{N_{t,i}}}\\\\ &{\\qquad\\leq u_{i,n}+2\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{\\log(N K T^{2})}{N_{t,i}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathrm{using~Eq.~(10)})}\\\\ &{\\qquad\\qquad+4\\sqrt{\\frac{\\log(N K T^{2})}{N_{t,i}}\\left(2u_{i,n}+\\frac{2\\log(N K T^{2})}{N_{t,i}}\\right)}+\\frac{8\\log(N K T^{2})}{N_{t,i}}}\\\\ &{\\qquad\\qquad\\leq u_{i,n}+8\\sqrt{\\frac{u_{i,n}\\log(N K T^{2})}{N_{t,i}}}+\\frac{15\\log(N K T^{2})}{N_{t,i}}.\\qquad\\qquad\\mathrm{(using~AM-GM~inequality)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the lower and the upper bound finishes the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Under Event $^{\\,l}$ ,Algorithm $^{\\,l}$ guarantees that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=K N_{0}+1}^{t}\\frac{p_{\\tau,i}}{N_{\\tau,i}}\\leq\\mathcal{O}\\left(\\log T\\right),}\\\\ &{\\displaystyle\\sum_{\\tau=K N_{0}+1}^{t}\\sqrt{\\frac{p_{\\tau,i}}{N_{\\tau,i}}}\\leq\\mathcal{O}\\left(\\sqrt{T\\log T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $i\\in[K]$ and $K N_{0}+1\\le t\\le T$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Since Event 1 holds, we know that $\\begin{array}{r}{N_{t,i}\\geq\\frac{1}{2}\\sum_{\\tau=K N_{0}+1}^{t}p_{\\tau,i}+1}\\end{array}$ =KNo+1Pr, + 1 holds for all i E [K] and $\\tau\\geq K N_{0}+1$ based on the choice of $N_{0}\\,=\\,18\\,\\mathrm{{l}o g}\\,K T+\\mathbb{I}$ : Therefore, we know that for each $t\\geq K N_{0}+1$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=K N_{0}+1}^{t}\\frac{p_{\\tau,i}}{N_{\\tau,i}}\\leq\\sum_{\\tau=K N_{0}+1}^{t}\\frac{2p_{\\tau,i}}{\\sum_{\\tau^{\\prime}=K N_{0}+1}^{\\tau}p_{\\tau^{\\prime},i}+2}}}\\\\ &{}&{\\leq2\\int_{0}^{\\sum_{\\tau=K N_{0}+1}^{t}p_{\\tau,i}}\\frac{1}{x+2}d x\\leq2\\log(T+2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As for the term t=KNo+1 V $\\begin{array}{r}{\\sum_{\\tau=K N_{0}+1}^{t}\\sqrt{\\frac{p_{\\tau,i}}{N_{\\tau,i}}}}\\end{array}$ N, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\tau=K N_{0}+1}^{t}\\sqrt{\\frac{p_{\\tau,i}}{N_{\\tau,i}}}\\leq\\sqrt{\\left(t-K N_{0}\\right)\\sum_{\\tau=K N_{0}+1}^{t}\\frac{p_{\\tau,i}}{N_{\\tau,i}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Cauchy-Schwarz inequality) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq{\\mathcal{O}}({\\sqrt{T\\log T}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Omitted Details in Section 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 3.2. In the bandit feedback setting, for any algorithm, there exists a stochastic environment in which the expected regret (with respect to NSW) of this algorithm is $\\begin{array}{r}{\\Omega\\left(\\frac{(\\log K)^{3}}{N^{3}}\\cdot K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}\\right)f\\!c}\\end{array}$ $N\\geq\\log K$ andsufficientlylarge $T$ ", "page_idx": 16}, {"type": "text", "text": "Proo. Consider the environment $\\mathcal{E}$ that picks $u$ uniformly from $\\{u^{(1)},\\ldots,u^{(K)}\\}$ ,where $u_{:,1}^{(i)}\\,=$ $\\boldsymbol{\\varepsilon}\\cdot\\mathbf{e}_{i}\\in\\mathbb{R}^{K}$ and $u_{:,j}^{(i)}=\\mathbf{1}$ for all $j\\in\\{2,3,\\dots,N\\}$ Here, $\\varepsilon\\in(0,\\frac{1}{9}]$ is some constat to e seied later. Denote $u^{(0)}$ to be the environment where $u_{:,n}=\\mathbf{0}$ for all $n\\in[N]$ . At each round $t$ $u_{t,i,n}$ is an i.i.d. Bernoulli random variable with mean $u_{i,n}$ . For notational convenience, we use $\\mathbb{E}_{i}[\\cdot]$ when we take expectation over the environment $u^{(i)}$ for $i\\in\\{0\\}\\cup[K]$ . Let $n_{i}$ be the number of rounds that action $i$ is selected over the total horizon $T$ for all $i\\in[K]$ . Therefore, the expected regret with respect to environment $\\mathcal{E}$ (a uniform distribution over $u^{(i)}$ \uff0c $i\\in[K])$ is lower bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\mathcal{E}}[\\mathrm{Reg}]=\\frac{1}{K}\\sum_{i=1}^{K}\\mathbb{E}_{i}\\left[\\varepsilon^{\\frac{1}{\\hbar}}\\sum_{t=1}^{T}(1-p_{t,i}^{\\frac{1}{\\hbar}})\\right]}\\qquad}&{}\\\\ &{\\geq T\\varepsilon^{\\frac{1}{\\hbar}}-\\frac{\\varepsilon^{\\frac{1}{\\hbar}}T^{\\frac{N-1}{\\hbar}}}{K}\\sum_{i=1}^{K}\\mathbb{E}_{i}\\left[\\left(\\sum_{t=1}^{T}p_{t,i}\\right)^{\\frac{1}{\\hbar}}\\right]}\\\\ &{\\geq T\\varepsilon^{\\frac{1}{\\hbar}}-\\frac{\\varepsilon^{\\frac{1}{\\hbar}}T^{\\frac{N-1}{\\hbar}}}{K}\\sum_{i=1}^{K}\\left(\\mathbb{E}_{i}\\left[\\displaystyle\\sum_{t=1}^{T}p_{t,i}\\right]\\right)^{\\frac{1}{\\hbar}}}\\\\ &{\\geq T\\varepsilon^{\\frac{1}{\\hbar}}-K^{-\\frac{1}{\\hbar}}\\varepsilon^{\\frac{1}{\\hbar}}T^{\\frac{N-1}{\\hbar}}\\left(\\sum_{i=1}^{K}\\mathbb{E}_{i}\\left[\\displaystyle\\sum_{t=1}^{T}p_{t,i}\\right]\\right)^{\\frac{1}{\\hbar}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Holder's inequality) ", "page_idx": 16}, {"type": "text", "text": "where the last inequality is again due to Holder's inequality. Let $\\mathbf{Ber}(\\alpha)$ be the Bernoulli distribution with mean $\\alpha$ . Combining Exercise 14.4 of [Lattimore and Szepesvari, 2020], Pinsker's inequality, and Lemma 15.1 of [Lattimore and Szepesvari, 2020], we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{i}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]=\\mathbb{E}_{i}\\left[n_{i}\\right]\\leq\\mathbb{E}_{0}\\left[n_{i}\\right]+T\\sqrt{\\frac{1}{2}\\mathbb{E}_{0}[n_{i}]\\mathbb{K}[\\mathbf{Ber}(0)|\\mathbf{Ber}(\\varepsilon))}}}\\\\ &{}&{\\leq\\mathbb{E}_{0}\\left[n_{i}\\right]+T\\sqrt{\\frac{1}{2}\\mathbb{E}_{0}[n_{i}]\\log\\left(1+\\frac{\\varepsilon}{1-\\varepsilon}\\right)}}\\\\ &{}&{\\leq\\mathbb{E}_{0}\\left[n_{i}\\right]+T\\sqrt{\\mathbb{E}_{0}[n_{i}]\\frac{\\varepsilon}{2(1-\\varepsilon)}}\\quad\\quad\\quad\\quad\\quad\\mathrm{(using~log(1+x)\\leq\\varepsilon)}}\\\\ &{}&{\\leq\\mathbb{E}_{0}\\left[n_{i}\\right]+\\frac{3T}{4}\\sqrt{\\mathbb{E}_{0}[n_{i}]\\varepsilon}}\\\\ &{}&{=\\mathbb{E}_{0}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]+\\frac{3T}{4}\\sqrt{\\mathbb{E}_{0}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]\\varepsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is because $\\varepsilon\\leq{\\frac{1}{9}}$ ", "page_idx": 16}, {"type": "text", "text": "Taking summation over all $i\\in[K]$ , we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in[K]}\\mathbb{E}_{i}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{i\\in[K]}\\mathbb{E}_{0}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]+\\frac{3}{4}T\\sum_{i=1}^{K}\\sqrt{\\mathbb{E}_{0}\\left[\\sum_{t=1}^{T}p_{t,i}\\right]\\varepsilon}}\\\\ {\\displaystyle\\leq T+\\frac{3T}{4}\\sqrt{K\\varepsilon\\mathbb{E}_{0}\\left[\\sum_{i=1}^{K}\\sum_{t=1}^{T}p_{t,i}\\right]}}\\\\ {\\displaystyle=T+\\frac{3T}{4}\\sqrt{K T\\varepsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality is due to Cauchy-Schwarz inequality. ", "page_idx": 17}, {"type": "text", "text": "Applying Eq. (13) to Eq. (11), we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}\\xi[\\mathrm{Keg}]}\\\\ &{\\ge T\\varepsilon^{\\frac{1}{N}}-K^{-\\frac{1}{N}}\\varepsilon^{\\frac{1}{N}}T^{\\frac{N-1}{N}}\\left(T+\\frac{3T}{4}\\sqrt{K T\\varepsilon}\\right)^{\\frac{1}{N}}}\\\\ &{\\ge\\left(1-K^{-\\frac{1}{N}}\\right)T\\varepsilon^{\\frac{1}{N}}-K^{-\\frac{1}{2N}}\\varepsilon^{\\frac{3}{2N}}T^{\\frac{2N+1}{2N}}}&&{\\mathrm{(using~}(a+b)^{\\frac{1}{N}}\\le a^{\\frac{1}{N}}+b^{\\frac{1}{N}})}\\\\ &{\\ge\\frac{\\log K}{2N}T\\varepsilon^{\\frac{1}{N}}-K^{-\\frac{1}{2N}}\\varepsilon^{\\frac{3}{2N}}T^{\\frac{2N+1}{2N}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality is according to Lemma C.2 with $\\begin{array}{r}{x\\,=\\,\\frac{1}{N}}\\end{array}$ and $\\alpha\\,=\\,{\\textstyle{\\frac{1}{K}}}$ , meaning that $\\begin{array}{r}{N\\left(1-\\frac{1}{K}^{\\frac{1}{N}}\\right)\\ge\\frac{\\log K}{2}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Picking e = (log K)2N.K. , we know that ", "page_idx": 17}, {"type": "equation", "text": "$$\nK^{-\\frac{1}{2N}}\\varepsilon^{\\frac{3}{2N}}T^{\\frac{2N+1}{2N}}=\\frac{\\varepsilon^{\\frac{1}{N}}T\\log K}{4N}=\\Omega\\left(\\frac{(\\log K)^{3}}{N^{3}}\\cdot K^{\\frac{1}{N}}T^{\\frac{N-1}{N}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the aove alltogether, we kow that E[Re] \u22652 (%K) KT). .Therefore, there exists one environment amongu(), [K] suchthat E[Re] \u2265( KT which finishes the proof. ", "page_idx": 17}, {"type": "text", "text": "B Omitted Details in Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Omitted Details in Section 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we prove that that in the adversarial environment, it is also impossible to achieve sublinearregretwhen $f=\\mathrm{NSW_{prod}}$ . The hard instance construction shares a similar spirit to the one for $f=\\mathrm{NSW}$ shown in Theorem 4.1. ", "page_idx": 17}, {"type": "text", "text": "Theorem B.1. In the bandit feedback setting, for any algorithm, there exists an adversarial environment suchthat $\\mathbb{E}[\\mathbf{Reg}_{\\mathrm{adv}}]=\\Omega(T)$ for $f=\\mathrm{NSW_{prod}}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We consider the learning environment with two agents and two arms. The agents utilities are binary, meaning that $u\\in\\{0,1\\}^{2\\times2}$ . We construct two distributions $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ with support $\\{0,1\\}^{2\\times2}$ To define environment $\\mathcal{E}$ we use $q_{w x y z}$ for any $w,x,y,z\\in\\{0,1\\}$ to denote the probability that $u_{1,:}=(w,x)$ and $u_{2,:}=(y,z)$ when $\\bar{u}\\sim\\mathcal{E}$ . For simplicity of notation, the binary number $w x y z$ will be written in decimal form (i.e. $q_{8}=\\mathrm{Pr}_{u\\sim\\mathcal{E}}[u_{1,:}^{\\quad}=(1,0),u_{2,:}=(0,0)],$ . For environment $\\mathcal{E}$ \uff0c we assign the probabilities ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{i}=\\frac{1}{16}\\quad\\mathrm{for~}i\\in\\{0,\\dots,15\\}\\setminus\\{0,2,4,6\\}\\qquad\\quad(q_{0},q_{2},q_{4},q_{6})=\\left(\\frac{1}{8},0,0,\\frac{1}{8}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for environment ${\\mathcal{E}}^{\\prime}$ , we use $q_{w x y z}^{\\prime}$ for any $w,x,z,y\\in\\{0,1\\}$ to denote the probability that $u_{1,:}^{\\prime}=(w,x)$ and $u_{2,:}^{\\prime}=(y,z)$ when $u^{\\prime}\\sim\\mathcal{E}^{\\prime}$ . Again, we will write the binary number $w x y z$ in decimal form for ease of notation. To environment ${\\mathcal{E}}^{\\prime}$ we assign probabilities ", "page_idx": 17}, {"type": "equation", "text": "$$\nq_{i}^{\\prime}=\\frac{1}{16}\\quad\\mathrm{for~}i\\in\\{0,\\dots,15\\}\\setminus\\{1,3,5,7\\}\\qquad\\quad(q_{1}^{\\prime},q_{3}^{\\prime},q_{5}^{\\prime},q_{7}^{\\prime})=\\left(0,\\frac{1}{8},\\frac{1}{8},0\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we argue that the learner's observations are equivalent in distribution in $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ ,sincethe marginal distribution of every possible observation of each action is the same. Specifically, ", "page_idx": 18}, {"type": "text", "text": "\u00b7. When action 1 is played, the learner's observation $\\psi_{1,}$ : in both $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ are given by the following marginal distribution. ", "page_idx": 18}, {"type": "text", "text": "- The probability of observation $(0,0)$ $\\begin{array}{r l}&{\\colon q_{0}+q_{1}+q_{2}+q_{3}=q_{0}^{\\prime}+q_{1}^{\\prime}+q_{2}^{\\prime}+q_{3}^{\\prime}=\\frac{1}{4}}\\\\ &{\\colon q_{4}+q_{5}+q_{6}+q_{7}=q_{4}^{\\prime}+q_{5}^{\\prime}+q_{6}^{\\prime}+q_{7}^{\\prime}=\\frac{1}{4}}\\\\ &{\\colon q_{8}+q_{9}+q_{10}+q_{11}=q_{8}^{\\prime}+q_{9}^{\\prime}+q_{10}^{\\prime}+q_{11}^{\\prime}=\\frac{1}{4}}\\\\ &{\\colon q_{12}+q_{13}+q_{14}+q_{15}=q_{12}^{\\prime}+q_{13}^{\\prime}+q_{14}^{\\prime}+q_{15}^{\\prime}=\\frac{1}{4}}\\end{array}$   \n- The probability of observation $(0,1)$ .is   \n- The probability of observation $(1,0)$   \n- The probability of observation $(1,1)$ ", "page_idx": 18}, {"type": "text", "text": "\u00b7 When action 2 is played, the learner's observation $u_{2,}$ : in both $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ are given by the following marginal distribution. ", "page_idx": 18}, {"type": "text", "text": "- The probability of observation $(0,0)$ .is $\\begin{array}{l}{{q_{0}+q_{4}+q_{8}+q_{12}=q_{0}^{\\prime}+q_{4}^{\\prime}+q_{8}^{\\prime}+q_{12}^{\\prime}=\\frac{1}{4}}}\\\\ {{q_{1}+q_{5}+q_{9}+q_{13}=q_{1}^{\\prime}+q_{5}^{\\prime}+q_{9}^{\\prime}+q_{13}^{\\prime}=\\frac{1}{4}}}\\\\ {{q_{2}+q_{6}+q_{10}+q_{14}=q_{2}^{\\prime}+q_{6}^{\\prime}+q_{10}^{\\prime}+q_{14}^{\\prime}=\\frac{1}{4}}}\\\\ {{q_{3}+q_{7}+q_{11}+q_{15}=q_{3}^{\\prime}+q_{7}^{\\prime}+q_{11}^{\\prime}+q_{15}^{\\prime}=\\frac{1}{4}}}\\end{array}$   \n- The probability of observation $(0,1)$ .s   \n- The probability of observation $(1,0)$ .s   \n- The probability of observation $(1,1)$ .is ", "page_idx": 18}, {"type": "text", "text": "Direct calculation shows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{u\\sim\\mathcal{E}}\\left[\\mathrm{NSW}_{\\mathrm{prod}}(u^{\\top}p)\\right]}\\\\ &{=(q_{5}-q_{6}-q_{9}+q_{10})p_{1}^{2}+(q_{6}-q_{7}-2q_{5}+q_{9}+q_{11}-q_{13}+q_{14})p_{1}+(q_{5}+q_{7}+q_{13}+q_{15})}\\\\ &{=-\\cfrac{1}{16}p_{1}^{2}+\\cfrac{1}{16}p_{1}+\\cfrac{1}{4},}\\\\ &{\\mathbb{E}_{u\\sim\\mathcal{E}^{\\prime}}\\left[\\mathrm{NSW}_{\\mathrm{prod}}(u^{\\top}p)\\right]}\\\\ &{=(q_{5}^{\\prime}-q_{6}^{\\prime}-q_{9}^{\\prime}+q_{10}^{\\prime})p_{1}^{2}+(q_{6}^{\\prime}-q_{7}^{\\prime}-2q_{5}^{\\prime}+q_{9}^{\\prime}+q_{11}^{\\prime}-q_{13}^{\\prime}+q_{14}^{\\prime})p_{1}+(q_{5}^{\\prime}+q_{7}^{\\prime}+q_{13}^{\\prime}+q_{15}^{\\prime})}\\\\ &{=\\cfrac{1}{16}p_{1}^{2}-\\cfrac{1}{16}p_{1}+\\cfrac{1}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we compute the learner's best strategy for environments $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ by direct calculation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\star}=\\underset{p\\in\\Delta_{2}}{\\mathrm{argmax}}\\,\\mathbb{E}_{u\\sim\\mathcal{E}}\\left[\\mathrm{NSW}_{\\mathrm{prod}}(u^{\\top}p)\\right]=\\underset{p\\in\\Delta_{2}}{\\mathrm{argmax}}\\left[\\frac{1}{4}+\\frac{1}{16}p_{1}-\\frac{1}{16}p_{1}^{2}\\right]=\\left(\\frac{1}{2},\\frac{1}{2}\\right)}\\\\ &{p_{\\star}^{\\prime}=\\underset{p\\in\\Delta_{2}}{\\mathrm{argmax}}\\,\\mathbb{E}_{u\\sim\\mathcal{E}^{\\prime}}\\left[\\mathrm{NSW}_{\\mathrm{prod}}(u^{\\top}p)\\right]=\\underset{p\\in\\Delta_{2}}{\\mathrm{argmax}}\\left[\\frac{1}{4}-\\frac{1}{16}p_{1}+\\frac{1}{16}p_{1}^{2}\\right]=\\{(0,1),(1,0)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, consider a distribution $p\\in\\Delta_{2}$ such that $p_{1}\\in\\left[0,\\frac{1}{4}\\right]\\cup\\left[\\frac{3}{4},1\\right]$ . For such $p$ , direct calculation shows that $\\mathbb{E}_{u\\sim\\mathcal{E}}$ $\\left[\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}^{\\top}\\boldsymbol{p}_{\\star})-\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}^{\\top}\\boldsymbol{p})\\right]^{-}\\bar{\\Delta}$ ,where $\\begin{array}{r}{\\bar{\\Delta}=\\frac{1}{256}}\\end{array}$ . On the other hand, for a strategy $p\\in\\Delta_{2}$ with $\\begin{array}{r}{p_{1}\\,\\in\\,\\left(\\frac{1}{4},\\frac{3}{4}\\right)}\\end{array}$ , we have $\\mathbb{E}_{u\\sim\\mathcal{E}^{\\prime}}$ $\\left[\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}^{\\top}\\boldsymbol{p}_{\\star}^{\\prime})-\\bar{\\mathrm{NSW}}_{\\mathrm{prod}}(\\boldsymbol{u}^{\\top}\\boldsymbol{p})\\right]>\\Delta$ as well. Given any algorithm, let $\\alpha_{\\mathcal{E}}$ be the probability that the number of rounds $p_{t,1}\\in\\left[0,\\textstyle{\\frac{1}{4}}\\right]\\cup\\left[\\textstyle{\\frac{3}{4}},1\\right]$ is larger than $\\textstyle{\\frac{T}{2}}$ under environment $\\mathcal{E}$ Let $\\bar{\\alpha}\\varepsilon^{\\prime}$ be the probability of the complement of this event under environment ${\\mathcal{E}}^{\\prime}$ . By definition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{E}}[\\mathrm{Reg}_{\\mathrm{adv}}]\\ge\\mathbb{E}_{\\mathcal{E}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{\\star})-\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right]\\ge\\frac{\\alpha_{\\mathcal{E}}T\\Delta}{2}}\\\\ &{\\mathbb{E}_{\\mathcal{E}^{\\prime}}[\\mathrm{Reg}_{\\mathrm{adv}}]\\ge\\mathbb{E}_{\\mathcal{E}^{\\prime}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{\\star}^{\\prime})-\\displaystyle\\sum_{t=1}^{T}\\mathrm{NSW}_{\\mathrm{prod}}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right]\\ge\\frac{\\bar{\\alpha}_{\\mathcal{E}^{\\prime}}T\\Delta}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the feedback for the algorithm is the same in distribution, we have $\\alpha_{\\mathcal{E}}+\\bar{\\alpha}_{\\mathcal{E}^{\\prime}}=1$ Thus,we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\mathbb E_{\\varepsilon}[{\\bf R e g}_{\\mathrm{adv}}],\\mathbb E_{\\varepsilon^{\\prime}}[{\\bf R e g}_{\\mathrm{adv}}]\\}\\ge\\frac{\\mathbb E_{\\varepsilon}[{\\bf R e g}_{\\mathrm{adv}}]+\\mathbb E_{\\varepsilon^{\\prime}}[{\\bf R e g}_{a d v}]}{2}\\ge\\frac{(\\alpha_{\\varepsilon}+\\bar{\\alpha}_{\\varepsilon^{\\prime}})T\\Delta}{4}=\\Omega(T).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.2Omitted Details in Section 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.2.1 Concave and Pareto Optimal SWFs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Formally, for a function $f:[0,1]^{N}\\mapsto[0,1]$ , concavity and Pareto Optimality are defined as: ", "page_idx": 19}, {"type": "text", "text": "\u00b7 Concavity: $f(\\alpha x+(1-\\alpha)y)\\leq\\alpha f(x)+(1-\\alpha)f(y)$ for any $\\alpha\\in[0,1]$ and $x,y\\in[0,1]^{N}$ \u00b7 Pareto optimality: for any $x,y\\in[0,1]^{N}$ \uff0c $x_{n}\\geq y_{n}$ for all $n\\in[N]$ implies $f\\!\\left(x\\right)\\geq f\\!\\left(y\\right)$ ", "page_idx": 19}, {"type": "text", "text": "Pareto optimality is a \u201cfundamental property\u201d in social choice theory because it ensures that a SWF prefers alternatives that strictly more efficient: everyone is no worse off [Kaneko and Nakamura, 1979]. Concavity appears less in social choice literature. However, it promotes equity by modeling diminishing levels of desirability with the increase of a single agent's utility. ", "page_idx": 19}, {"type": "text", "text": "In the following, we provide examples of SWFs that satisfy concavity and Pareto optimality. Each of the following SWFs are parameterized by fixed weights $w\\in\\Delta_{K}$ ", "page_idx": 19}, {"type": "text", "text": "\u00b7 Utilitarian SWF: $f(u)=\\langle w,u\\rangle$   \n\u00b7 Generalized Gini Index (GGI): $\\begin{array}{r}{f(u)=\\operatorname*{min}_{\\pi\\in\\mathbb{S}_{N}}\\left\\langle w_{\\pi},u\\right\\rangle}\\end{array}$ , where $\\mathbb{S}_{N}$ is the set of permutations over $N$ items and $w_{\\pi}$ is weights $w$ permuted according to $\\pi\\in\\mathbb{S}_{N}$   \n. Weighted NSW: $\\begin{array}{r}{f(u)=\\prod_{n\\in N}u_{n}^{w_{n}}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "The last notable fact about the class of concave and Pareto Optimal SWFs is that it is closed under convex combinations. Specifically, for two concave and Pareto Optimal functions $f,g:[0,1]^{N}\\rightarrow$ $[0,1]$ ,thefunction $h(\\cdot)\\ {\\stackrel{.}{=}}\\ \\lambda f(\\cdot)\\ {\\stackrel{.}{+}}\\ (1-\\lambda)g(\\cdot)$ forany $\\lambda\\in[0,1\\bar{]}$ is concave and Pareto Optimal. [Chen and Hooker, 2023] discusses how such convex combinations can be used to combine a SWF prioritizing effciency and another prioritizing equity to derive a different SWF that prioritizes a balance between efficiency and equity. ", "page_idx": 19}, {"type": "text", "text": "B.2.2 Omitted Details in Section 4.2.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove Theorem 4.2, which shows that $\\mathcal{O}(\\sqrt{K T\\log T})$ regret is achievable for all concave and Pareto optimal SWFs. ", "page_idx": 19}, {"type": "text", "text": "Theorem 4.2. For any $f\\,:\\,[0,1]^{N}\\,\\,\\rightarrow\\,\\,[0,1]$ that is concave and Pareto optimal, Algorithm 2 with the log-barrier regularizer $\\begin{array}{r}{\\psi(p)\\,=\\,-\\sum_{i=1}^{K}\\log p_{i}}\\end{array}$ and $\\begin{array}{r}{\\eta=\\sqrt{\\frac{K\\log T}{T}}}\\end{array}$ guarantees ${\\tt R e g}_{\\mathrm{adv}}=$ $\\mathcal{O}(\\sqrt{K T\\log T})$ ", "page_idx": 19}, {"type": "text", "text": "Proof. Using the concavity of $f$ , we can upper bound $\\mathtt{R e g}_{\\mathrm{adv}}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{Reg_{adv}}=\\operatorname*{max}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}f({u_{t}^{\\top}}p)-\\displaystyle\\sum_{t=1}^{T}f({u_{t}^{\\top}}p_{t})}}\\\\ {\\displaystyle\\le\\operatorname*{max}_{p\\in\\Delta_{K}}\\sum_{\\bar{\\kappa},\\bar{\\kappa}_{T}}^{T}\\langle-\\nabla f({u_{t}^{\\top}}p_{t}),p_{t}-p\\rangle+\\displaystyle\\operatorname*{max}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}f({u_{t}^{\\top}}p)-\\displaystyle\\operatorname*{max}_{p\\in\\Delta_{\\kappa,\\bar{\\kappa}_{T}}}\\sum_{t=1}^{T}f({u_{t}^{\\top}}p),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{{\\Delta}_{K,\\frac{1}{K T}}=\\{p\\in\\Delta_{K}\\;\\vert\\;p_{i}\\geq\\frac{1}{K T},\\forall i\\in[K]\\}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "To bound Term (1), according to a standard analysis of FTRL/OMD with log-barrer regularizer (e.g. Lemma 12 of [Agarwal et al., 2017]), we know that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{T e r m}\\left(1\\right)\\leq\\operatorname*{max}_{p\\in\\Delta_{K,\\frac{1}{K T}}}\\frac{D_{\\psi}(p,p_{1})}{\\eta}+\\eta\\sum_{t=1}^{T}\\sum_{i=1}^{K}p_{t,i}^{2}\\cdot\\left[\\nabla f(u_{t}^{\\top}p_{t})\\right]_{i}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D_{\\psi}(p,q)\\triangleq\\psi(p)-\\psi(q)-\\langle\\nabla\\psi(q),p-q\\rangle$ is the Bregman divergence between $p$ and $q$ with respect to $\\psi$ . To further bound the right-hand side, note that $\\textstyle p_{1}={\\frac{1}{K}}\\cdot\\mathbf{1}$ . Direct calculation shows that for any p E K,' ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\psi}(p,p_{1})=\\sum_{i=1}^{K}\\left(\\frac{p_{i}}{p_{1,i}}-1-\\log\\left(\\frac{p_{i}}{p_{1,i}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\sum_{i=1}^{K}\\log\\left(\\frac{1}{K p_{i}}\\right)}\\\\ {\\displaystyle\\leq K\\log\\left(\\frac{1}{K\\cdot\\frac{1}{K T}}\\right)}\\\\ {\\displaystyle\\leq K\\log T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the Pareto optimality property of $f$ and the positivity of the utility matrix $u_{t}$ , we know that $[\\nabla f(u_{t}^{\\top}p)]_{i}\\geq0$ meaning that $\\begin{array}{r}{\\dot{\\sum_{i=1}^{K}p_{t,i}^{2}}\\left[\\nabla f(u_{t}^{\\top}\\bar{p_{t}})\\right]_{i}^{2}\\leq\\dot{\\langle p_{t},\\nabla f(u_{t}^{\\top}\\bar{p_{t}})\\rangle}^{2}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Moreover, using the concavity property of $f$ we know that $\\left\\langle p_{t},\\nabla f(u_{t}^{\\top}p_{t})\\right\\rangle\\leq f(u_{t}^{\\top}p_{t})-f(u_{t}^{\\top}\\mathbf{0})=$ $f(u_{t}^{\\top}p_{t})\\leq1$ . Combining the above two inequalities means that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{i=1}^{K}p_{t,i}^{2}\\left[\\nabla f(u_{t}^{\\top}p_{t})\\right]_{i}^{2}\\leq T.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Eq. (15) and Eq. (16), we can upper bound Term (1) as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\left(1\\right)\\leq\\frac{K\\log T}{\\eta}+\\eta T.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote the optimal distribution $\\begin{array}{r}{p^{\\star}=\\operatorname*{argmax}_{p\\in\\Delta_{K}}\\sum_{t=1}^{T}f(u_{t}^{\\top}p)}\\end{array}$ Recallthat $\\textstyle p_{1}={\\frac{1}{K}}\\cdot\\mathbf{1}$ .Now we upper bound Term (2) as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T e r m}\\left(2\\right)=\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p^{*})-\\displaystyle\\operatorname*{max}_{p\\in\\Delta_{K,\\widetilde{\\gamma}_{T}}}\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p)}\\\\ &{\\phantom{T e r m}\\leq\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p^{*})-\\displaystyle\\sum_{t=1}^{T}f\\left(u_{t}^{\\top}\\left(\\left(1-\\frac{1}{T}\\right)p^{*}+\\frac{1}{T}\\cdot p_{1}\\right)\\right)}\\\\ &{\\phantom{T e r m}\\leq\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p^{*})-\\displaystyle\\sum_{t=1}^{T}\\left[\\left(1-\\frac{1}{T}\\right)\\cdot f(u_{t}^{\\top}p^{*})+\\displaystyle\\frac{1}{T}\\cdot f(u_{t}^{\\top}p_{1})\\right]\\phantom{T e r m}\\leq\\displaystyle\\sum_{t=1}^{T}f(\\mathrm{such}_{t}^{\\top}p^{*})}\\\\ &{\\phantom{T e r m}\\leq\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p^{*})-\\displaystyle\\sum_{t=1}^{T}\\left[\\left(1-\\frac{1}{T}\\right)\\cdot f(u_{t}^{\\top}p^{*})+\\displaystyle\\frac{1}{T}\\cdot f(u_{t}^{\\top}p_{1})\\right]\\phantom{T e r m}(\\mathrm{Comeativ})}\\\\ &{\\phantom{T e r m}\\leq\\displaystyle\\frac{1}{T}\\cdot\\displaystyle\\sum_{t=1}^{T}f(u_{t}^{\\top}p^{*})}\\\\ &{\\phantom{T e r m}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Eq. (17) and Eq. (18), and choosing $\\begin{array}{r}{\\eta=\\sqrt{\\frac{K\\log T}{T}}}\\end{array}$ finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "B.2.3 Omitted Details in Section 4.2.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present the omitted proof for Theorem 4.3, which shows a better dependency on $K$ compared with Theorem 4.2. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4.3. For $f=\\mathrm{NSW}_{\\mathrm{\\ell}}$ Algorithm 2 with the Tsallis entropy regularizer $\\begin{array}{r}{\\psi(p)=\\frac{1-\\sum_{i=1}^{K}p_{i}^{\\beta}}{1-\\beta}}\\end{array}$ $\\begin{array}{r}{\\beta=\\frac{2}{N}}\\end{array}$ , and the optimal choice of $\\eta$ guarantees $\\mathrm{Reg_{adv}}=\\widetilde{\\mathcal{O}}(K^{\\frac{1}{2}-\\frac{1}{N}}\\sqrt{N T})$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Consider the case when $N\\geq3$ . Direct calculation shows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left[\\nabla f(u^{\\top}p)\\right]_{i}\\leq\\frac{1}{N}\\sum_{n=1}^{N}\\frac{u_{i,n}}{\\langle p,u_{:,n}\\rangle^{1-\\frac{1}{N}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the concavity of $f$ and a standard analysis of FTRL with Tsallis entropy (e.g., [Luo, 2017, Theorem 1]), we know that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{\\mathrm{adv}}=\\sum_{t=1}^{T}\\big(f(u_{t}^{\\top}p^{\\star})-f(u_{t}^{\\top}p_{t})\\big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\ E}\\left(|\\nabla u_{t}^{\\tau}|\\right),p^{*}-p_{t}\\right)}\\\\ &{\\leq\\frac{K^{1-\\beta}-1}{\\eta(1-\\beta)}+\\frac{\\eta}{\\beta}\\displaystyle\\sum_{i=1}^{T}\\sum_{l=i}^{K}\\mathbb{E}\\left(\\nabla f(u_{t}^{\\tau}|\\rho_{l}^{\\tau})\\right)_{i}^{2}}\\\\ &{=\\frac{K^{1-\\beta}-1}{\\eta(1-\\beta)}+\\frac{\\eta}{N^{2}\\beta}\\displaystyle\\sum_{i=1}^{T}\\sum_{i=1}^{K}\\left(\\sum_{i=1}^{N}\\frac{p_{i}^{\\tau}\\frac{\\beta}{\\beta}}{(\\sum_{j=1}^{N}u_{i,j_{n}},\\ p_{i,j})^{1-\\beta}}\\right)^{2}}\\\\ &{\\leq\\frac{K^{1-\\beta}-1}{\\eta(1-\\beta)}+\\frac{\\eta}{N^{2}\\mu_{1}^{2}\\ln(1-\\ln(1))}\\displaystyle\\sum_{i=1}^{T}\\sum_{i=1}^{K}\\frac{p_{i}^{\\tau}\\frac{\\beta}{\\beta}}{(\\sum_{j=1}^{N}u_{i,j_{n}},\\ p_{i,j})^{2-\\beta}}\\quad\\mathrm{(cusiv.~schwarz~inequality)}}\\\\ &{\\leq\\frac{K^{1-\\beta}-1}{\\eta(1-\\beta)}+\\frac{\\eta}{N\\beta}\\displaystyle\\sum_{i=1}^{T}\\sum_{i=1}^{N}\\sum_{j=1}^{K}\\frac{p_{i}^{\\tau}\\frac{\\beta}{\\beta}}{p_{i,j}\\frac{\\beta}{\\beta}}u_{i,j_{n}}^{\\tau}}\\\\ &{\\leq\\frac{K^{1-\\beta}-1}{\\eta(1-\\beta)}+\\frac{\\eta}{N\\beta}\\displaystyle\\sum_{i=1}^{T}\\sum_{n=1}^{N}\\sum_{j=1}^{K}\\mu_{1,j_{n}}^{2-\\beta}u_{i,n}^{\\tau}}\\\\ &{\\leq\\frac{K^{1-\\beta}}{\\eta(1-\\beta)}+\\frac{\\eta}{N\\beta}\\displaystyle\\sum_{i=1}^{T}\\sum_{k=1}^{N}\\sum_{j=1}^{K}p_{i,j_{n}}^{\\tau}\\frac{\\beta}{\\beta}u_{i,n}^{2-\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Picking $\\begin{array}{r}{\\beta=\\frac{2}{N}}\\end{array}$ , the first term can be upper bounded by $\\frac{3K^{1-\\frac{2}{N}}}{\\eta}$ and the second term can be upper bounded by $\\begin{array}{r}{\\frac{\\eta T}{\\beta}=\\frac{\\eta N T}{2}}\\end{array}$ . Further picking the optimal choice of $\\eta$ finishes the proof. ", "page_idx": 21}, {"type": "text", "text": "When N = 2 and \u03b2 = 2 = 1, the regularier (e) = 1-DP becomes the negative Shannon entropye $\\begin{array}{r}{\\psi(p)=\\sum_{i=1}^{K}p_{i}\\log p_{i}}\\end{array}$ Usingthe conavity of $f$ and folowing standard analysisofFTR L with Shannon entropy regularizer (e.g., [Hazan et al., 2016, Theorem 5.2]), we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{\\mathrm{shr}}=\\displaystyle\\sum_{t=1}^{T}\\left(f(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}^{*})-f_{t}(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right)}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}(\\nabla f(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t}),\\boldsymbol{p}^{*}-\\boldsymbol{p}_{t})}\\\\ &{\\le\\displaystyle\\frac{\\mathrm{i}\\hat{\\boldsymbol{\\eta}}(\\boldsymbol{p}^{*})-\\hat{\\boldsymbol{\\eta}}(\\boldsymbol{p}_{1})}{\\eta}+2\\eta\\sum_{t=1}^{T}\\sum_{t=1}^{K}p_{t,t}\\left[\\nabla f(\\boldsymbol{u}_{t}^{\\top}\\boldsymbol{p}_{t})\\right]_{t}^{2}\\left(\\mathbb{b}\\ y\\left[\\mathrm{Hazon~etal},2016,\\mathrm{Theorem}~5.2\\right]\\right)}\\\\ &{=\\displaystyle\\frac{\\log K}{\\eta}+2\\eta\\sum_{t=1}^{T}\\sum_{t=1}^{T}p_{t,t}\\left(\\frac{u_{t,\\lambda}}{2\\sqrt{\\eta}\\langle t_{1},u_{t}\\rangle_{1}}+\\frac{u_{t,\\lambda}}{2\\sqrt{\\eta}\\langle t_{1},u_{t,\\lambda}\\rangle_{2}}\\right)^{2}}\\\\ &{\\le\\displaystyle\\frac{\\log K}{\\eta}+\\eta\\sum_{t=1}^{T}\\left(\\sum_{t=1}^{K}p_{t,t}u_{t,\\lambda}^{2}+\\sum_{t=1}^{K}p_{t,t}u_{t,\\lambda}^{2}\\right)}\\\\ &{\\le\\displaystyle\\frac{\\log K}{\\eta}+2\\eta^{T}L}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Picking  = \u2190 $\\eta=\\sqrt{\\frac{\\log K}{T}}$ shows that ${\\mathrm{Reg}}_{\\mathrm{adv}}={\\mathcal{O}}\\left({\\sqrt{T\\log K}}\\right)={\\widetilde{\\mathcal{O}}}({\\sqrt{T}})\\,{\\mathrm{for}}\\,N=2.$ ", "page_idx": 21}, {"type": "text", "text": "B.2.4 Omitted Details in Section 4.2.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we show the proof for Theorem 4.4, which shows that logarithmic regret is achievable when there is at least one agent who is indifferent about the learner's choice. ", "page_idx": 21}, {"type": "text", "text": "Theorem 4.4. Fix $f=\\mathrm{NSW}$ Suppose that for each time $t$ there is a set of agents $A_{t}\\subseteq[N]$ such that $\\left|A_{t}\\right|\\geq M$ and $u_{t,:,n}=c_{t,n}{\\bf1}$ with $c_{t,n}\\geq0$ for each agent $n\\in A_{t}$ . Then the EWOO algorithm guarantes $\\begin{array}{r}{{\\mathrm{Reg}}_{\\mathrm{adv}}={\\mathcal{O}}\\left({\\frac{N-M}{M}}\\cdot K\\log T\\right)}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. To show that EWOO algorithm achieves logarithmic regret, we need to show that $f_{t}({\\boldsymbol{p}})\\triangleq$ $-\\mathrm{NSW}(u_{t}^{\\top}p)$ is $\\alpha$ -exp-concave for some $\\alpha>0$ for all $t\\in[T]$ , meaning that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla^{2}f_{t}(p)-\\alpha\\nabla f_{t}(p)\\nabla f_{t}(p)^{\\top}\\succeq0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $A_{t}\\ \\subseteq\\ [N]$ be the set of agents with $u_{t,:,n}\\,=\\,c_{t,n}\\cdot{\\bf1}$ for all $n\\,\\in\\,A$ on round $t\\,\\in\\,[T]$ . It is guaranteed that $\\left|A_{t}\\right|\\geq M$ for all $t\\in[T]$ . Denote $B_{t}=[N]\\setminus A_{t}$ . Direct calculation shows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f_{t}(p)=\\frac{\\prod_{m\\in A_{t}}c_{t,m}^{\\frac{3}{N}}}{N}\\sum_{\\substack{n\\in B_{t}\\,\\left\\langle\\,\\rho,n,\\,\\iota,n\\right\\rangle}}^{\\mathcal{I}_{t}(p)}u_{t,n},}\\\\ &{\\nabla^{2}f_{t}(p)=\\frac{\\prod_{m\\in A_{t}}c_{t,m}^{\\frac{3}{N}}}{N}\\sum_{\\substack{i\\in B_{t}\\,}}^{\\pi}\\frac{u_{t,:,n}\\nabla f_{t}(p)^{\\top}\\,\\left\\langle\\,p,u_{t,:,n}\\right\\rangle-f_{t}(p)u_{t,:,n}u_{t,:,n}^{\\top}}{\\langle p,u_{t,:,n}\\rangle^{2}}}\\\\ &{\\qquad=\\frac{f_{t}(p)\\prod_{m\\in A_{t}}c_{t,m}^{\\frac{3}{N}}}{N^{2}}\\left(\\sum_{\\substack{i\\in B_{t}\\,\\left\\langle\\,p,u_{t,:,n}\\right\\rangle}}^{\\frac{\\mathcal{U}_{t}}{N}\\,\\frac{u_{t,:,n}}{\\langle p,u_{t,:,n}\\rangle}}\\right)\\left(\\sum_{\\substack{n\\in B_{t}\\,\\left\\langle p,u_{t,:,n}\\right\\rangle}}^{\\frac{u_{t,:,n}}{\\langle p,u_{t,:,n}}\\right)^{\\top}}\\right.}\\\\ &{\\qquad\\qquad\\left.-\\frac{f_{t}(p)\\Pi_{m\\in A_{t}\\,c_{t,m}^{\\frac{1}{N}}}}{N}\\sum_{\\substack{i\\in B_{t}\\,\\left\\langle\\,p,u_{t,:,n}\\right\\rangle}}^{\\frac{\\mathcal{U}_{t}}{N}\\,\\frac{u_{t,:,n}u_{t,:,n}^{\\top}}{\\langle p,u_{t,:,n}\\rangle^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For notational convenience,lt $\\lambda_{t}=\\Pi_{m\\in A_{t}}c_{t,m}^{\\frac{1}{N}}\\leq1$ Picking $\\begin{array}{r}{\\alpha=\\frac{M}{N-M}}\\end{array}$ N-M, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}f(\\bar{p})-\\bar{\\theta}(X)\\nabla^{2}f(\\bar{p})}\\\\ &{=\\frac{\\bar{\\theta}\\int_{\\bar{p}}\\bar{\\theta}(X)}{X}\\left(\\frac{\\bar{p}}{\\omega_{0}\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})}\\left(\\sum_{s_{0}=0}^{\\infty}\\frac{u_{s_{0}}}{\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})}\\right)^{\\top}-\\frac{\\bar{h}_{0}\\bar{p}_{0}}{X}\\right)\\nabla\\frac{\\bar{\\theta}(X)\\cdot\\bar{\\theta}(X)}{\\bar{\\theta}(\\bar{p},\\bar{t}_{0})\\omega_{0}}}\\\\ &{\\qquad-\\frac{\\bar{\\theta}(X)\\bar{\\theta}(X)}{X}\\left(\\frac{\\bar{p}}{\\omega_{0}\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})}\\left(\\sum_{s_{0}=0}^{\\infty}\\frac{u_{s_{0}}}{\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})}\\right)^{\\top}\\right.}\\\\ &{=\\frac{-\\bar{\\lambda}\\bar{f}(\\bar{p})}{X}\\left(\\frac{\\bar{\\theta}(X)\\cdot\\bar{\\theta}(X)}{\\sqrt{\\theta}(\\bar{p},\\bar{t}_{0})\\omega_{0}^{2}}-\\frac{\\bar{\\lambda}(1-\\bar{\\theta}(X))}{X}\\left(\\sum_{s_{0}=0}^{\\infty}\\frac{u_{s_{0}}}{\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})\\omega_{0}}\\right)^{\\top}\\left(\\sum_{s_{0}=0}^{\\infty}\\frac{u_{s_{0}}}{\\bar{p}_{0}(\\bar{p},\\bar{t}_{0})\\omega_{0}}\\right)^{\\top}\\Bigg]}\\\\ &{\\quad\\times\\frac{-\\bar{\\lambda}\\bar{f}(\\bar{p})}{X}\\left[\\frac{\\bar{\\theta}(X)\\cdot\\bar{\\theta}(X)}{\\sqrt{\\theta}(\\bar{p},\\bar{t}_{0})\\omega_{0}^{2}}-\\frac{\\bar{\\theta}(X)}{X}\\bar{\\theta}(X)\\left(\\sum_{s_{0}=0}^{\\infty}\\frac{u_{s_{0}}}{\\bar{p}_{0}(\\bar{p},\\bar{t \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(Cauchy-Schwarz inequality) ", "page_idx": 22}, {"type": "text", "text": "where the first inequality is because $\\lambda_{t}\\ \\leq\\ 1$ and $f_{t}({\\boldsymbol{p}})\\;\\leq\\;0$ ; the second inequality is because $f_{t}(p)\\geq-1$ ; the third inequality is because $\\begin{array}{r}{|B_{t}|\\leq\\frac{1}{N-M}}\\end{array}$ . This shows that the $f_{t}(\\boldsymbol{p})$ .s (N-M)-expconcave. Therefore, according to Theorem 4.4 of [Hazan et al., 2016], we know that the EwOO algorithm guarantees that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f_{t}(p_{t})-f_{t}(p^{\\star}))\\leq\\left({\\frac{N-M}{M}}\\right)\\cdot K\\log T+{\\frac{2(N-M)}{M}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C Auxiliary Lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we include several auxiliary lemmas that are useful in the analysis. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.1. Let $a_{1},\\ldots,a_{n},b_{1},\\ldots b_{n}\\,\\in\\,[0,1],$ where $a_{i}\\geq b_{i}$ for all $i\\,\\in\\,[n]$ Then, $\\textstyle\\prod_{i=1}^{n}a_{i}\\,-$ $\\begin{array}{r}{\\prod_{i=1}^{n}b_{i}\\leq\\sum_{i=1}^{n}(a_{i}-b_{i})}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Direct calculation shows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\prod_{i=1}^{n}a_{i}-\\displaystyle\\prod_{i=1}^{n}b_{i}=\\displaystyle\\sum_{j=1}^{n}\\left(\\prod_{i=1}^{j}a_{i}\\displaystyle\\prod_{i=j+1}^{n}b_{i}-\\displaystyle\\prod_{i=1}^{j-1}a_{i}\\displaystyle\\prod_{i=j}^{n}b_{i}\\right)}&{}\\\\ {\\displaystyle=\\sum_{j=1}^{n}\\left((a_{j}-b_{j})\\displaystyle\\prod_{i=1}^{j-1}a_{i}\\displaystyle\\prod_{i=j+1}^{n}b_{i}\\right)}&{}\\\\ {\\displaystyle\\leq\\sum_{j=1}^{n}(a_{j}-b_{j}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma C.2. For all $x\\in(0,1)$ and $\\alpha\\in(0,1)$ satisfying $1+x\\log\\alpha\\geq0,$ we have $\\begin{array}{r}{\\frac{1-\\alpha^{x}}{x}\\geq-\\frac{\\log\\alpha}{2}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $y=\\log\\alpha<0$ We know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{1-\\alpha^{x}}{x}\\geq-\\frac{\\log\\alpha}{2}}}\\\\ {{\\Longleftrightarrow1-e^{x y}\\geq-\\frac{x y}{2}}}\\\\ {{\\Longleftrightarrow e^{x y}\\leq1+\\frac{x y}{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is true since $e^{u}\\leq1+\\frac{u}{2}$ for all $u\\in[-1,0]$ and $x y=x\\log\\alpha\\geq-1$ ", "page_idx": 23}, {"type": "text", "text": "Lemma C.3 (Theorem 1 in [Beygelzimer et al., 2011]). Let $X_{1},\\ldots,X_{T}\\in[-B,B].$ for some $B>0$ be a martingale difference sequence and with $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}_{t}[X_{t}^{2}]\\leq V}\\end{array}$ for some fixed quantity $V>0$ We have for all $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}X_{t}\\leq\\operatorname*{min}_{\\lambda\\in[0,1/B]}\\left(\\lambda V+\\frac{\\log(1/\\delta)}{\\lambda}\\right)\\leq2\\sqrt{V\\log(1/\\delta)}+B\\log(1/\\delta).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See abstract and Section 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Section 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Section 3, Section 4, and the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / / nips . CC / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code ofEthics https: / /neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPs Code of Ethics. The research conducted in this paper conforms with it in every respect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work is mostly theoretical, and we do not foresee any negative ethical or societal outcomes. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of adataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]