[{"figure_path": "qfCQ54ZTX1/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the LLM4EA framework. LLM4EA utilizes active sampling to select important entities based on feedback from an EA model. It also includes a label refiner to effectively train the base EA model using noisy pseudo-labels. Feedback from the EA model updates the selection policy.", "description": "This figure illustrates the overall architecture of the LLM4EA framework. The framework consists of four main components: an active sampling module, an LLM annotator, an unsupervised label refiner, and a base entity alignment (EA) model. The active sampling module selects important entities to annotate using feedback from the EA model, optimizing the usage of LLM queries. The LLM annotator generates pseudo-labels for the selected entities, which are refined by the unsupervised label refiner. The refined labels are then used to train the base EA model. The output of the EA model is used to update the active sampling policy in an iterative manner, forming a cycle to progressively refine the alignment results.", "section": "3 Entity alignment with noisy annotations from LLMs"}, {"figure_path": "qfCQ54ZTX1/figures/figures_7_1.jpg", "caption": "Figure 2: Performance-cost comparison between GPT-3.5 and GPT-4 as the annotator, evaluated by MRR. We increase the budget for GPT-3.5 to evaluate its performance. [n\u00d7] denotes using n\u00d7 of the default query budget. Each experiment is repeated three times to show mean and standard deviation.", "description": "This figure shows the mean reciprocal rank (MRR) achieved by using either GPT-3.5 or GPT-4 as the annotator for generating pseudo-labels for entity alignment across four different datasets.  The x-axis represents the query budget (increased for GPT-3.5 to compare performance at different costs), and the y-axis represents the MRR.  Error bars show the standard deviation across three repeated experiments for each condition.  The figure demonstrates the trade-off between cost and performance with different LLMs and demonstrates that GPT-4 is more efficient for entity alignment than GPT-3.5 at the default budget.", "section": "4.2 Results"}, {"figure_path": "qfCQ54ZTX1/figures/figures_7_2.jpg", "caption": "Figure 3: Analysis of the Label Refinement. We illustrate the evolution of the true positive rate (TPR) (left) and recall (middle) for refined labels across four datasets. Furthermore, we assess the robustness of the label refinement process by examining the TPR of refined labels against varying initial TPRs within the D-W-15K dataset (right), with initial pseudo-labels synthesized at different TPR levels.", "description": "This figure analyzes the performance of the label refinement module in LLM4EA. The left and middle subfigures show how TPR and recall evolve across iterations for four datasets, demonstrating the refinement process's effectiveness in improving label accuracy. The right subfigure shows the robustness of the refinement process by testing it with initial pseudo-labels of varying quality (initial TPRs), highlighting its ability to consistently improve TPR.", "section": "4.2.2 Effect of the label refiner"}, {"figure_path": "qfCQ54ZTX1/figures/figures_8_1.jpg", "caption": "Figure 4: Performance of entity alignment across four datasets with varying active sampling iterations, under a fixed query budget.", "description": "This figure visualizes the results of experiments evaluating the performance of entity alignment across four different datasets.  The x-axis represents the number of active sampling iterations, while the y-axis shows the Mean Reciprocal Rank (MRR), a metric assessing the accuracy of the entity alignment. Each dataset's results are presented in a separate subplot. The plots show how the MRR changes as the number of iterations increases, allowing for observation of the relationship between the number of iterations performed in the active sampling process and the accuracy of the model's alignment predictions. Error bars are included to indicate the variability across the repeated experiments.", "section": "4.2.4 Pareto frontier of runtime overhead against performance"}]