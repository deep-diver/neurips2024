[{"figure_path": "erwatqQ4p8/figures/figures_3_1.jpg", "caption": "Figure 1: An illustrative depiction of the relationship between self-attention and MoE. Each output vector of a head in the MSA layer can be viewed as the output of a MoE model. These MoE models share the same set of experts encoded in the value matrix. Each entry in the attention matrix corresponds to a score function within this architecture.", "description": "This figure illustrates the connection between the self-attention mechanism in Vision Transformers and the Mixture of Experts (MoE) model.  It shows how each output vector from a self-attention head can be interpreted as the output of an MoE model. The experts in this MoE are represented by the rows of the value matrix in the self-attention, and each element of the attention matrix represents a score function used for gating in the MoE. This visual representation helps to understand how the self-attention block implicitly incorporates an MoE architecture.", "section": "Mixture of Experts Meets Attention"}, {"figure_path": "erwatqQ4p8/figures/figures_4_1.jpg", "caption": "Figure 2: Left: An illustrative depiction of prefix tuning as the introduction of new experts into pre-trained MoE models. Right: Visualization of NoRGa implementation, integrating non-linear activation and residual connections into the prefix tuning attention matrix.", "description": "The figure illustrates the concept of prefix tuning as introducing new experts to a pre-trained Mixture of Experts (MoE) model.  The left side shows how prefix tuning adds new experts (represented in the value matrix) to the existing experts of the pre-trained model, while the right side details the proposed NoRGa mechanism, highlighting how it enhances the gating function of prefix tuning through non-linear activation and residual connections. NoRGa improves the model's ability to adapt to new tasks and enhance the efficiency of parameter estimations.", "section": "Connection between Prefix Tuning and Mixture of Experts"}, {"figure_path": "erwatqQ4p8/figures/figures_30_1.jpg", "caption": "Figure 2: Left: An illustrative depiction of prefix tuning as the introduction of new experts into pre-trained MoE models. Right: Visualization of NoRGa implementation, integrating non-linear activation and residual connections into the prefix tuning attention matrix.", "description": "The figure shows two illustrations. The left one illustrates prefix tuning as introducing new experts to pre-trained Mixture of Experts (MoE) models. The right one visualizes the implementation of Non-linear Residual Gates (NoRGa), which integrates non-linear activation and residual connections into the prefix tuning attention matrix. NoRGa is proposed to address the suboptimal sample efficiency of the original prefix tuning by incorporating non-linearity and residual connection.", "section": "Connection between Prefix Tuning and Mixture of Experts"}]