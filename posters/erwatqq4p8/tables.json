[{"figure_path": "erwatqQ4p8/tables/tables_8_1.jpg", "caption": "Table 1: Overall performance comparison on Split CIFAR-100 and Split ImageNet-R. We present Final Average Accuracy (FA), Cumulative Average Accuracy (CA), and Average Forgetting Measure (FM) of all methods under different pre-trained models.", "description": "This table compares the performance of different continual learning methods (L2P, DualPrompt, S-Prompt, CODA-Prompt, HiDe-Prompt, and NoRGa) on two benchmark datasets (Split CIFAR-100 and Split ImageNet-R).  The comparison is done using various pre-trained models (Sup-21K, iBOT-21K, iBOT-1K, DINO-1K, and MoCo-1K). The table shows the final average accuracy (FA), cumulative average accuracy (CA), and average forgetting measure (FM) for each method and pre-trained model combination.  The results highlight the effectiveness of the proposed NoRGa method in achieving state-of-the-art performance on both datasets.", "section": "5 Experiments"}, {"figure_path": "erwatqQ4p8/tables/tables_9_1.jpg", "caption": "Table 2: Final average accuracy (FA) on Split CUB-200 and 5-Datasets.", "description": "This table presents the final average accuracy (FA) achieved by different continual learning methods on two benchmark datasets: Split CUB-200 (a fine-grained image classification dataset) and 5-Datasets (a dataset combining five different datasets with varying characteristics).  The results are broken down by the pre-trained model used (Sup-21K and iBOT-21K), showing the performance of each method across different data scenarios and pre-training techniques.  This helps in understanding the generalization capabilities of each method across diverse datasets.", "section": "5 Experiments"}, {"figure_path": "erwatqQ4p8/tables/tables_9_2.jpg", "caption": "Table 3: Ablation study of different activation functions, measured by final average accuracy (FA).", "description": "This table presents the ablation study results of using different activation functions (tanh, sigmoid, and GELU) within the NoRGa method. The final average accuracy (FA) metric is used to evaluate the performance on Split CIFAR-100 and Split CUB-200 datasets with both Sup-21k and iBOT-21k pre-trained models.  The results show the impact of the activation function on the model's performance in a continual learning setting.", "section": "4 Non-linear Residual Gate Meets Prefix Tuning"}, {"figure_path": "erwatqQ4p8/tables/tables_28_1.jpg", "caption": "Table 4: Performance comparison in task-incremental learning setting. Here we present Final Average Accuracy (FA).", "description": "This table presents the final average accuracy (FA) achieved by different methods in a task-incremental learning setting.  Task-incremental learning differs from class-incremental learning in that the model knows the task identity at test time.  The table compares the performance of HiDe-Prompt and NoRGa (with different activation functions) on two datasets (Split CIFAR-100 and Split CUB-200), using two different pre-trained models (Sup-21K and iBOT-21K). The results show that NoRGa generally outperforms HiDe-Prompt, achieving higher accuracy across the different datasets and pre-trained models.", "section": "5 Experiments"}, {"figure_path": "erwatqQ4p8/tables/tables_28_2.jpg", "caption": "Table 2: Final average accuracy (FA) on Split CUB-200 and 5-Datasets.", "description": "This table shows the final average accuracy (FA) achieved by different continual learning methods on two benchmark datasets: Split CUB-200 (a fine-grained image classification dataset) and 5-Datasets (a dataset designed to test continual learning performance with large inter-task differences).  The results are broken down by the pre-trained model used (Sup-21K and iBOT-21K).  The table helps demonstrate the effectiveness of the NoRGa method compared to other state-of-the-art continual learning approaches, showing that NoRGa maintains higher accuracy and exhibits less catastrophic forgetting across a range of tasks and pre-trained models.  ", "section": "5 Experiments"}, {"figure_path": "erwatqQ4p8/tables/tables_29_1.jpg", "caption": "Table 6: Performance comparison of pre-trained model-based continual learning methods using ViT-B/16 with Sup-21K weights. Here we present Final Average Accuracy (FA).", "description": "This table compares the final average accuracy (FA) of several pre-trained model-based continual learning methods on the Split CIFAR-100 and Split CUB-200 datasets.  The methods compared include ADAM with various parameter-efficient fine-tuning techniques (VPT-D, SSF, Adapter) and RanPAC, a recent state-of-the-art method. The results show that the proposed NoRGa method significantly outperforms all other methods.", "section": "Experiments"}, {"figure_path": "erwatqQ4p8/tables/tables_29_2.jpg", "caption": "Table 7: Ablation study of different activation functions, measured by final average accuracy (FA).", "description": "This ablation study analyzes the impact of different activation functions (tanh, sigmoid, GELU) on the NoRGa model's final average accuracy (FA).  The experiment measures the performance on the Split CIFAR-100 and Split CUB-200 datasets using Sup-21K and iBOT-21K pre-trained models to determine the optimal activation function for the NoRGa model.", "section": "4 Non-linear Residual Gate Meets Prefix Tuning"}, {"figure_path": "erwatqQ4p8/tables/tables_30_1.jpg", "caption": "Table 1: Overall performance comparison on Split CIFAR-100 and Split ImageNet-R. We present Final Average Accuracy (FA), Cumulative Average Accuracy (CA), and Average Forgetting Measure (FM) of all methods under different pre-trained models.", "description": "This table compares the performance of different continual learning methods (L2P, DualPrompt, S-Prompt, CODA-Prompt, HiDe-Prompt, and NoRGa) on two benchmark datasets (Split CIFAR-100 and Split ImageNet-R).  It shows the final average accuracy (FA), cumulative average accuracy (CA), and average forgetting measure (FM) for each method, using various pre-trained models (Sup-21K, iBOT-21K, iBOT-1K, DINO-1K, and MoCo-1K).  The results demonstrate the relative effectiveness of each method in preventing catastrophic forgetting and maintaining high accuracy across multiple tasks.", "section": "5 Experiments"}]