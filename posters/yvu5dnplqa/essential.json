{"importance": "This paper is crucial for researchers working on **instruction tuning** and **large language models (LLMs)**.  It presents a novel, **cost-effective method** for creating high-quality instruction datasets by mining existing web data. This addresses the limitations of current methods that rely on expensive human annotation or GPT-4 distillation, opening new avenues for research on more scalable and diverse LLM training. The **state-of-the-art performance** achieved by the proposed MAmmoTH2 models on various benchmarks highlights the potential of this approach for advancing LLM capabilities.", "summary": "MAmmoTH2: Harvesting 10M web instructions for enhanced LLM reasoning!", "takeaways": ["A novel method for creating large-scale, high-quality instruction datasets by mining web data.", "The proposed MAmmoTH2 models significantly improve LLM reasoning performance on various benchmarks.", "The approach is cost-effective, eliminating the need for expensive human annotation or GPT-4 distillation."], "tldr": "Current instruction tuning for LLMs relies on either expensive human annotation or GPT-4 distillation, both of which have limitations in scale and diversity.  This often leads to biased and less generalizable models. This paper tackles this issue by proposing a new paradigm for enhancing LLM reasoning.  Existing instruction data is scarce and costly, limiting the scalability and diversity of training.\nThe proposed solution involves a three-step pipeline: recalling relevant documents from a pre-training web corpus, extracting instruction-response pairs, and refining these pairs using open-source LLMs.  Fine-tuning base LLMs on the resulting 10-million instruction dataset (WEBINSTRUCT) creates significantly improved models (MAmmoTH2). The results demonstrate the effectiveness and cost-efficiency of this method, achieving state-of-the-art performance on several benchmarks. This offers a new approach for building better instruction tuning data for LLMs.", "affiliation": "Carnegie Mellon University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "yVu5dnPlqA/podcast.wav"}