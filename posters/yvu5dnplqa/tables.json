[{"figure_path": "yVu5dnPlqA/tables/tables_4_1.jpg", "caption": "Table 1: The list of existing supervise-fine-tuning (SFT) and continue-training (CT) datasets. SFT datasets are primarily from academic NLP sources or synthesized by GPT-3.5/4 using seed data. CT datasets are larger but nosier. Our dataset falls between these two types.", "description": "This table compares various existing datasets used for training language models, categorized as either Supervised Fine-Tuning (SFT) or Continued Training (CT).  SFT datasets are generally smaller, higher quality, and often created using human-annotated data or GPT-3.5/4 generation.  CT datasets are much larger but contain more noise. The table highlights the size, domain focus, format, and creation method of each dataset.  It also positions the WEBINSTRUCT dataset (developed in this paper) in relation to these other datasets, showing that it falls between SFT and CT methods, achieving a good balance between size and data quality.", "section": "2 WEBINSTRUCT"}, {"figure_path": "yVu5dnPlqA/tables/tables_5_1.jpg", "caption": "Table 2: Main results on reasoning datasets. Models without the '-Instruct' suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).", "description": "This table presents the main results of the experiments conducted on seven reasoning benchmark datasets.  It compares the performance of various language models, categorized by their parameter size (7B, 8B, or >20B). The table shows each model's performance on each benchmark and highlights the improvement achieved by the models trained using WEBINSTRUCT, both alone and in combination with additional public instruction datasets. Few-shot Chain-of-Thought (CoT) prompting was used for all model evaluations.  The table clearly indicates improvements made by the models trained on the proposed WEBINSTRUCT dataset, demonstrating their effectiveness in enhancing reasoning capabilities.", "section": "4.1 Experimental Results on Reasoning Benchmarks"}, {"figure_path": "yVu5dnPlqA/tables/tables_7_1.jpg", "caption": "Table 2: Main results on reasoning datasets. Models without the '-Instruct' suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).", "description": "This table presents the main experimental results on seven reasoning benchmarks.  It compares the performance of various language models, both with and without instruction tuning, across different parameter sizes (7B and 8B).  The table highlights the performance gains achieved by the MAmmoTH2 models, particularly when trained using WEBINSTRUCT, a novel web-derived dataset.  Few-shot chain-of-thought prompting was used for evaluation.", "section": "4.1 Experimental Results on Reasoning Benchmarks"}, {"figure_path": "yVu5dnPlqA/tables/tables_7_2.jpg", "caption": "Table 4: Comparison of the two data-refining LLMs. We train the three models with the same steps.", "description": "This table compares the performance of three Mistral-7B models trained on data refined by different LLMs.  One model was trained on data refined by Mixtral-22B\u00d78, another on data refined by Qwen-72B, and the third on a merged dataset combining data refined by both Mixtral and Qwen. The table shows the results on GSM8K, MATH, MMLU-STEM, TheoremQA, and ARC-C benchmarks, demonstrating the impact of different refinement methods on model performance.", "section": "5.2 Comparison of Two Refined Models"}, {"figure_path": "yVu5dnPlqA/tables/tables_8_1.jpg", "caption": "Table 5: Impact of different data domains and sources on model performance. All models are trained with identical steps; Base denotes the base model's performance.", "description": "This table presents the results of training Mistral 7B language model on subsets of WEBINSTRUCT categorized by domain (Math, Science, Education, Forum, Other) and data source (Education, Forum). The performance is evaluated on five benchmark datasets: GSM8K, MATH, MMLU-STEM, TheoremQA, and ARC-C.  It shows how different domains and data sources affect the model's performance on these tasks.", "section": "2 WEBINSTRUCT"}, {"figure_path": "yVu5dnPlqA/tables/tables_9_1.jpg", "caption": "Table 2: Main results on reasoning datasets. Models without the '-Instruct' suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).", "description": "This table presents the main results of the reasoning benchmarks comparing different language models.  It shows the performance of various models (both base models and those fine-tuned with WEBINSTRUCT) on seven reasoning benchmarks.  The table is divided into two sections based on model parameter size (20B-110B and 7B-8B).  Few-shot Chain-of-Thought (CoT) prompting was used for all models, with the number of shots specified for each benchmark.  Results are taken from official sources when available, otherwise, the authors' own evaluation was used.  Underscored scores indicate the best baseline performance among comparable models. Finally, the table highlights performance improvements achieved through fine-tuning with WEBINSTRUCT and additional datasets.", "section": "4.1 Experimental Results on Reasoning Benchmarks"}, {"figure_path": "yVu5dnPlqA/tables/tables_17_1.jpg", "caption": "Table 2: Main results on reasoning datasets. Models without the '-Instruct' suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).", "description": "This table presents the main experimental results on several reasoning benchmarks.  It compares the performance of various language models, both with and without instruction tuning, across different sizes (7B, 8B, 34B parameters).  The models are evaluated using few-shot chain-of-thought prompting on several datasets. The table highlights the performance gains achieved by models trained using the WEBINSTRUCT dataset.  Results are also shown for models further fine-tuned with additional instruction datasets.", "section": "4.1 Experimental Results on Reasoning Benchmarks"}, {"figure_path": "yVu5dnPlqA/tables/tables_17_2.jpg", "caption": "Table 2: Main results on reasoning datasets. Models without the '-Instruct' suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).", "description": "This table presents the main experimental results on seven reasoning benchmarks, comparing the performance of various language models.  It shows the performance of models trained solely with WEBINSTRUCT, compared to those trained with additional instruction datasets and the base models.  Models are categorized by their parameter size (7B or 8B, and 20B-110B). Few-shot chain-of-thought (CoT) prompting is used for all evaluations.  The results highlight the performance gains achieved by training models using WEBINSTRUCT, even exceeding state-of-the-art performance in several cases.", "section": "4.1 Experimental Results on Reasoning Benchmarks"}]