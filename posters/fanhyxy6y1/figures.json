[{"figure_path": "FaNhyXY6Y1/figures/figures_1_1.jpg", "caption": "Figure 1: Artemis' ability in video-based dialogue. Notably, Artemis excels particularly in video-based referring, outperforming the existing MLLMs including Merlin [57] and Video-LLaVA [31] lacking comprehensiveness and Osprey [59] suffering hallucination.", "description": "This figure showcases Artemis' performance on video-based referring tasks compared to other state-of-the-art Multimodal Large Language Models (MLLMs).  It demonstrates Artemis's superior ability to accurately and comprehensively answer complex questions about video content, highlighting its ability to avoid hallucinations and produce more complete descriptions than other models. The examples in the figure illustrate various aspects of video understanding including object behavior, movement and scene description.", "section": "1 Introduction"}, {"figure_path": "FaNhyXY6Y1/figures/figures_3_1.jpg", "caption": "Figure 2: Left: the overall framework of Artemis, where an MLLM receives a text prompt together with spatial, temporal, and target-specific video features, and produces the answer. Right: the RoI tracking and selection mechanism to generate target-specific features. We use different IDs to show the clustering result. This figure is best viewed in color.", "description": "This figure illustrates the architecture of Artemis, a multimodal large language model designed for video-based referential understanding. The left panel shows the overall framework, where the model takes a user query and video features (spatial, temporal, and target-specific) as input and generates a textual answer.  The right panel details the RoI (Region of Interest) tracking and selection process used to extract the target-specific video features.  This involves tracking a bounding box around the target object across frames, selecting key frames with relevant features, and then using a clustering method to obtain a compact representation. The different IDs in the right panel show the results of the clustering process. ", "section": "3 Artemis: A Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_6_1.jpg", "caption": "Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected Rols. Red and green texts indicate incorrect and correct answers, respectively. This figure is best viewed in color.", "description": "This figure compares the performance of Artemis and Merlin on video-based referring tasks.  It highlights Artemis's advantage in not requiring semantic class information about the target region, which is unlike Merlin. The figure shows examples where Artemis correctly answers the question about a target region's actions in a video, while Merlin provides incorrect or less comprehensive answers.  The visual components of the figure illustrate the process; input regions are marked in orange, tracked regions of interest (ROIs) in blue, and selected ROIs are highlighted with yellow stars.  Correct and incorrect answers are noted in green and red, respectively.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_6_2.jpg", "caption": "Figure 4: RoI manipulation increases the informativeness and diversity of RoIs. See Appendix D for details.", "description": "This figure shows the results of two experiments evaluating the impact of RoI tracking and selection methods on the informativeness and diversity of RoIs. The top panel shows that adding RoI tracking increases the information entropy of the RoIs, indicating that more information is captured. The bottom panel shows that using K-means clustering to select RoIs results in higher inter-frame RoI difference compared to using uniform or random sampling, suggesting that the selected RoIs are more diverse. Overall, this figure demonstrates that RoI tracking and selection improves the quality of the video features used for referential understanding.", "section": "3.3 RoI Tracking and Selection"}, {"figure_path": "FaNhyXY6Y1/figures/figures_7_1.jpg", "caption": "Figure 5: How RoI tracking and selection gradually improves the quality of video-based referring. In each example, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and green and yellow stars label the uniformly sampled and K-means selected RoIs, respectively. Red and green texts highlight the incorrect and correct outputs. This figure is best viewed in color.", "description": "This figure shows an ablation study comparing three different RoI selection methods: only using the queried RoI, tracking and uniformly choosing 4 RoIs, and tracking & clustering to choose 4 RoIs.  For each method, the figure displays example videos with the input <region> highlighted in orange, tracked RoIs in blue, and selected RoIs marked with yellow stars.  The corresponding generated captions are given, highlighting the improved accuracy and comprehensiveness of the description when using tracking and clustering.", "section": "3.3 RoI Tracking and Selection"}, {"figure_path": "FaNhyXY6Y1/figures/figures_8_1.jpg", "caption": "Figure 6: An example of multi-round, video-based referring by integrating Artemis with GroundingDINO [34].", "description": "This figure demonstrates Artemis's integration with GroundingDINO for multi-round video-based referring. The example shows a series of questions and answers, where Artemis provides the initial answer (referring to a specific region in the video), and GroundingDINO then locates the referenced entity in the video and presents it as further input to answer subsequent questions. This showcases the collaborative nature of these models in handling complex video understanding tasks.", "section": "4.2 Artemis Is a Building Block for Complex Video Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_14_1.jpg", "caption": "Figure 8: Some examples of VideoRef45K.", "description": "This figure shows six example video clips from the VideoRef45K dataset. Each clip contains a sequence of frames showing an object performing an action, along with bounding boxes highlighting the object's position in each frame.  The accompanying text provides a concise description of the object's activity in the clip.  The purpose is to illustrate the diversity of actions and object types represented in the dataset, which consists of 45K video question-answer pairs.", "section": "3.1 Problem Formulation and Data Preparation"}, {"figure_path": "FaNhyXY6Y1/figures/figures_16_1.jpg", "caption": "Figure 9: A comparison of video-based referring between image-based MLLMs and Artemis. GPT-3.5-Turbo is used to integrate the 5 independent outputs from the image-based MLLMs.", "description": "This figure compares the performance of Artemis with other image-based MLLMs (Osprey and Ferret) on a video-based referring task.  The image-based models process the video frame by frame using image-based referring methods and then use GPT-3.5-Turbo to combine these results into an overall video description.  The figure highlights Artemis's superior ability to understand and describe the actions and behavior within a video compared to the combined image-based MLLM approach, demonstrating that Artemis is more effective at video-based referring than a frame-wise image-based approach followed by text summarization.  The examples provided in the figure shows differences in the comprehensiveness and accuracy of the descriptions generated by each method.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_17_1.jpg", "caption": "Figure 10: Attention map between RoI tokens and temporal tokens.", "description": "This figure shows the attention map between RoI tokens and temporal tokens. The attention map visualizes the relationships between the RoI features extracted from different frames of the video and the temporal features representing the entire video. It helps demonstrate how the model integrates spatial and temporal information to understand the video content. Warmer colors indicate stronger attention weights, suggesting stronger relationships between those features.", "section": "3.2 Overall Framework and Visual Features"}, {"figure_path": "FaNhyXY6Y1/figures/figures_17_2.jpg", "caption": "Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected Rols. Red and green texts indicate incorrect and correct answers, respectively. This figure is best viewed in color.", "description": "This figure compares Artemis and Merlin's performance on video-based referring tasks.  It highlights Artemis's ability to answer questions without requiring explicit semantic class labels for the target region, unlike Merlin. It demonstrates Artemis's superior performance by showing its ability to identify and track the relevant regions of interest within the video and accurately answer questions regarding the target's actions.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_18_1.jpg", "caption": "Figure 12: The example of long video understanding generated by Artemis.", "description": "This figure shows an example of Artemis performing long video understanding. The video is divided into four segments, and Artemis produces a description for each segment.  These individual descriptions are then combined by Artemis to generate a comprehensive summary of the entire video's events. The example showcases Artemis' ability to not only understand individual actions within short video segments but also to combine these into a coherent narrative summarizing the overall events of a longer video.", "section": "4.2 Artemis Is a Building Block for Complex Video Understanding"}, {"figure_path": "FaNhyXY6Y1/figures/figures_18_2.jpg", "caption": "Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected Rols. Red and green texts indicate incorrect and correct answers, respectively. This figure is best viewed in color.", "description": "This figure compares the performance of Artemis and Merlin on video-based referring tasks.  It highlights Artemis's ability to successfully answer questions about a target region in a video without requiring explicit semantic labels, unlike Merlin which does need this extra information.  Each row shows an example, with the orange rectangle showing the initial target region; blue rectangles track the target throughout the video; yellow stars highlight selected keyframes; and red and green text indicates whether the model was correct (green) or incorrect (red).", "section": "4 Experiments"}, {"figure_path": "FaNhyXY6Y1/figures/figures_19_1.jpg", "caption": "Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected Rols. Red and green texts indicate incorrect and correct answers, respectively. This figure is best viewed in color.", "description": "This figure compares the performance of Artemis and Merlin on video-based referring tasks.  It highlights that Artemis does not require the semantic class of the target region as input, unlike Merlin.  The figure shows example questions, model responses, and visual representations (bounding boxes) to illustrate the model's ability to identify and describe target actions within a video.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}]