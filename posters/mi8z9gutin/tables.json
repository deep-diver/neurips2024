[{"figure_path": "MI8Z9gutIn/tables/tables_6_1.jpg", "caption": "Table 1: The performance (testing accuracy %) comparison among various bilevel optimization methods on the data condensation task over three datasets. All the datasets are condensed using a 3-layer ConvNet. IPC: image(s) per class. Ratio (%): the ratio of condensed examples to the whole training set.", "description": "This table presents a comparison of the testing accuracy achieved by different bilevel optimization methods on three datasets (MNIST, CIFAR-10, and CIFAR-100) after performing data condensation.  The table shows the results for various methods, including TRGU, Hessian-Free, Neumann, the proposed (FG)\u00b2U, RGU, and the results achieved using the whole datasets. Different image per class (IPC) ratios are considered. The table demonstrates the effectiveness of (FG)\u00b2U in achieving high accuracy, especially in large-scale scenarios, where other methods suffer from memory limitations or approximation errors.", "section": "Data Condensation"}, {"figure_path": "MI8Z9gutIn/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of the online adaptation performance. The reported evaluation metrics include the exact match (EM) and F1 scores. For vanilla CaMeLS [25], RGU is conducted with unrolled depth 6, using DistilGPT2 as the base model. We present both the results reported by [66] and those from our implementation (denoted as impl.). For CaMeLS + (FG)\u00b2U, we select unrolled depths from {24, 48}, and the base model from {DistilGPT2, GPT2}. We report the results for the combination that yields the best F1 score. Additional details and ablation studies are documented in Appendix G.1.", "description": "This table compares the performance of three different methods for online adaptation of language models: CaMeLS + RGU (results from the original paper), CaMeLS + RGU (our implementation), and CaMeLS + (FG)\u00b2U (our proposed method).  The comparison is done across three different language models (DistilGPT2, GPT2-Large, and GPT2-XL) and two datasets (StreamingQA and SQUAD-Seq).  The evaluation metrics used are Exact Match (EM) and F1 score.  The table highlights that our method, (FG)\u00b2U, consistently achieves better results than the other two.", "section": "Meta Learning Online Adaptation of Language Models"}, {"figure_path": "MI8Z9gutIn/tables/tables_23_1.jpg", "caption": "Table 1: The performance (testing accuracy %) comparison among various bilevel optimization methods on the data condensation task over three datasets. All the datasets are condensed using a 3-layer ConvNet. IPC: image(s) per class. Ratio (%): the ratio of condensed examples to the whole training set.", "description": "This table compares the performance of various bilevel optimization methods on three different datasets (MNIST, CIFAR-10, and CIFAR-100) for a data condensation task. The performance is measured by testing accuracy.  Different image per class (IPC) ratios and overall dataset ratios are tested. The table shows that (FG)\u00b2U outperforms other methods.", "section": "4 Experiments"}, {"figure_path": "MI8Z9gutIn/tables/tables_23_2.jpg", "caption": "Table 2: Comparison of the online adaptation performance. The reported evaluation metrics include the exact match (EM) and F1 scores. For vanilla CaMeLS [25], RGU is conducted with unrolled depth 6, using DistilGPT2 as the base model. We present both the results reported by [66] and those from our implementation (denoted as impl.). For CaMeLS + (FG)\u00b2U, we select unrolled depths from {24, 48}, and the base model from {DistilGPT2, GPT2}. We report the results for the combination that yields the best F1 score. Additional details and ablation studies are documented in Appendix G.1.", "description": "This table compares the performance of different methods for online adaptation of language models.  It shows the exact match (EM) and F1 scores for the CaMeLS approach using RGU (with DistilGPT2) and the proposed (FG)\u00b2U method (with DistilGPT2 and GPT2 models) on two datasets. Different unrolled depths are tested for (FG)\u00b2U, demonstrating its ability to handle larger models and depths.", "section": "Meta Learning Online Adaptation of Language Models"}, {"figure_path": "MI8Z9gutIn/tables/tables_24_1.jpg", "caption": "Table 1: The performance (testing accuracy %) comparison among various bilevel optimization methods on the data condensation task over three datasets. All the datasets are condensed using a 3-layer ConvNet. IPC: image(s) per class. Ratio (%): the ratio of condensed examples to the whole training set.", "description": "This table compares the performance of different bilevel optimization methods on three image datasets (MNIST, CIFAR-10, CIFAR-100) using a data condensation technique. The performance is measured by testing accuracy, and it's shown for different image per class (IPC) ratios, which represent the proportion of condensed examples to the total number of examples in the training dataset. The results are compared to using the whole dataset for training (WHOLE).", "section": "Data Condensation"}, {"figure_path": "MI8Z9gutIn/tables/tables_24_2.jpg", "caption": "Table 2: Comparison of the online adaptation performance. The reported evaluation metrics include the exact match (EM) and F1 scores. For vanilla CaMeLS [25], RGU is conducted with unrolled depth 6, using DistilGPT2 as the base model. We present both the results reported by [66] and those from our implementation (denoted as impl.). For CaMeLS + (FG)\u00b2U, we select unrolled depths from {24, 48}, and the base model from {DistilGPT2, GPT2}. We report the results for the combination that yields the best F1 score. Additional details and ablation studies are documented in Appendix G.1.", "description": "This table compares the performance of the CaMeLS model using RGU and (FG)\u00b2U for online adaptation of language models on two datasets, StreamingQA and SQUAD-Seq. It shows the Exact Match (EM) and F1 scores for different model sizes and unrolled depths. The results demonstrate that (FG)\u00b2U achieves better performance than RGU, especially for larger models, overcoming the memory limitations of RGU.", "section": "Meta Learning Online Adaptation of Language Models"}, {"figure_path": "MI8Z9gutIn/tables/tables_24_3.jpg", "caption": "Table 2: Comparison of the online adaptation performance. The reported evaluation metrics include the exact match (EM) and F1 scores. For vanilla CaMeLS [25], RGU is conducted with unrolled depth 6, using DistilGPT2 as the base model. We present both the results reported by [66] and those from our implementation (denoted as impl.). For CaMeLS + (FG)\u00b2U, we select unrolled depths from {24, 48}, and the base model from {DistilGPT2, GPT2}. We report the results for the combination that yields the best F1 score. Additional details and ablation studies are documented in Appendix G.1.", "description": "This table compares the performance of different methods for online adaptation of language models on two datasets: StreamingQA and SQUAD-Seq.  It shows the exact match (EM) and F1 scores for CaMeLS with RGU (results from the original paper and the authors' implementation) and CaMeLS with (FG)\u00b2U.  The table highlights (FG)\u00b2U's improved performance, especially when using larger language models and increased unrolled depth, overcoming the memory limitations of RGU.", "section": "Meta Learning Online Adaptation of Language Models"}]