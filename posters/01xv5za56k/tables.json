[{"figure_path": "01XV5Za56k/tables/tables_7_1.jpg", "caption": "Table 1: Calibration testing thresholds (smallest passed on half of 100 runs).", "description": "This table shows the smallest calibration threshold (epsilon) at which the calibration testers (smCE, dCE, ConvECE) correctly identify the calibrated distribution in at least half of the 100 experimental runs. The column index n represents the number of samples used in the experiments. The ground truth calibration error is 0.01.", "section": "Experiments"}, {"figure_path": "01XV5Za56k/tables/tables_8_1.jpg", "caption": "Table 2: Empirical smCE on postprocessed DenseNet40 predictions (median over 20 runs)", "description": "This table presents the empirical smooth calibration error (smCE) for three different distributions obtained by post-processing the predictions of a DenseNet40 model trained on the CIFAR-100 dataset.  The three distributions are: Dbase (original model predictions), Diso (predictions after isotonic regression), and Dtemp (predictions after temperature scaling). The median smCE across 20 runs is reported for each distribution.  The results show that temperature scaling leads to lower smooth calibration error than the original model and isotonic regression.", "section": "3 Experiments"}, {"figure_path": "01XV5Za56k/tables/tables_9_1.jpg", "caption": "Table 3: Runtimes (in seconds) for computing the value of (3), using various solvers", "description": "This table compares the runtime performance of four different solvers for computing the value of equation (3) in the paper, which is a linear program representing the smooth calibration error.  The solvers compared are CVXPY's linear program solver, Gurobi's minimum-cost flow solver, a custom dynamic programming solver from Corollary 1 in the paper, and the custom solver further optimized with PyPy.  The table shows how the runtime of each solver scales with increasing sample size (n), demonstrating the efficiency gains achieved by the custom dynamic programming solver, especially when combined with PyPy.", "section": "3 Experiments"}, {"figure_path": "01XV5Za56k/tables/tables_19_1.jpg", "caption": "Table 1: Calibration testing thresholds (smallest passed on half of 100 runs).", "description": "This table presents the results of calibration testing experiments using three different calibration measures: smCE, dCE, and ConvECE.  The goal was to determine the smallest threshold epsilon for each measure that correctly identifies a miscalibrated distribution (i.e., a distribution with non-zero distance to calibration) in at least half of 100 trials.  The number of samples (n) was varied, and the ground truth distance to calibration (0.01) is also shown. The results indicate the relative performance of these calibration measures for calibration testing, highlighting the reliability of smCE and dCE compared to ConvECE.", "section": "3 Experiments"}, {"figure_path": "01XV5Za56k/tables/tables_20_1.jpg", "caption": "Table 1: Calibration testing thresholds (smallest passed on half of 100 runs).", "description": "This table presents the results of calibration testing experiments using three different calibration measures: smooth calibration error (smCE), lower distance to calibration (dCE), and convolved ECE (ConvECE). For various sample sizes (n), the table shows the smallest threshold (\u03b5) such that a majority of 100 runs of an \u03b5-tester reported \"yes\", indicating that the distribution is calibrated. This is compared to the ground truth calibration error of 0.01. The table demonstrates that smCE and dCE testers are more reliable estimators of the ground truth calibration error than ConvECE.", "section": "Experiments"}]