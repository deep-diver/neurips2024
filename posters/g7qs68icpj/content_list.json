[{"type": "text", "text": "Nimbus: Secure and Efficient Two-Party Inference for Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhengyi $\\mathbf{Li}^{1,}$ , Kang $\\mathbf{Yang}^{3},$ ,, $\\mathbf{Jin}\\,\\mathbf{Tan}^{4}$ , Wen-jie $\\mathbf{L}\\mathbf{u}^{4}$ , Haoqi $\\mathbf{W}\\mathbf{u}^{4}$ , Xiao Wang5, Yu $\\mathbf{Y}\\mathbf{u}^{1,2}$ , Derun Zhao4, Yancheng Zheng4, Minyi Guo1,2, Jingwen Leng1,2,\u2020 1Shanghai Jiao Tong University, 2Shanghai Qizhi Institute, 3State Key Laboratory of Cryptology 4Ant Group, 5Northwestern University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer models have gained significant attention due to their power in machine learning tasks. Their extensive deployment has raised concerns about the potential leakage of sensitive information during inference. However, when being applied to Transformers, existing approaches based on secure two-party computation (2PC) bring about efficiency limitations in two folds: (1) resource-intensive matrix multiplications in linear layers, and (2) complex non-linear activation functions like GELU and Softmax. This work presents a new two-party inference framework Nimbus for Transformer models. For the linear layer, we propose a new 2PC paradigm along with an encoding approach to securely compute matrix multiplications based on an outer-product insight, which achieves $2.9\\times\\sim12.5\\times$ performance improvements compared to the state-of-the-art (SOTA) protocol. For the non-linear layer, through a new observation of utilizing the input distribution, we propose an approach of low-degree polynomial approximation for GELU and Softmax, which improves the performance of the SOTA polynomial approximation by $2.9\\times\\sim4.0\\times$ , where the average accuracy loss of our approach is $0.08\\%$ compared to the non-2PC inference without privacy. Compared with the SOTA two-party inference, Nimbus improves the end-to-end performance of BERTbase inference by $2.7\\times\\sim4.7\\times$ across different network settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer models [36] bring about significant advancements in various machine learning tasks, such as language understanding [19], vision tasks [6], and chatting bot [21]. As Transformer models handle increasingly sensitive data and tasks, privacy becomes a major concern for deployment. For example, one hospital (client) wants to use the model from another organization (server) to enhance its diagnostic capabilities. This raises privacy concerns for both parties: either the hospital has to upload its private data, or the organization needs to send its proprietary model to the hospital. ", "page_idx": 0}, {"type": "text", "text": "Recently, several works [14, 26, 16, 29], building upon secure two-party computation (2PC), realize secure two-party inference in a privacy-preserving way. These works proceed by having the client and server jointly execute inference over the \u201cencrypted\u201d input and model, using cryptographic techniques including homomorphic encryption (HE) [7], additive secret sharing, etc. The client learns nothing about the model except for inference results and keeps the server unknown for the client\u2019s input. ", "page_idx": 0}, {"type": "text", "text": "Privacy protection comes with substantial computation and communication costs due to expensive cryptographic operations. While existing secure two-party inference for convolution neural networks could be completed in a few minutes [8, 23, 18, 2, 34, 32, 17], it is more challenging to make secure inference on Transformer models practical, due to heavy matrix multiplications in linear layers and complex non-linear layers. To amortize the overhead of HE in linear layers, many works [17, 14, 26] adopt window encoding to simulate the inner product. However, such an encoding approach brings about a sparse format of HE ciphertexts, leading to redundant communication and computation. The efficiency bottleneck of non-linear layers is to securely compute GELU and exponential (included in Softmax). Prior works [5, 26] use piecewise polynomials to approximate the two non-linear functions. However, high-degree polynomials and large fixed-point precision are used to maintain the accuracy, which causes large communication costs and rounds. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This work proposes a new secure two-party inference framework Nimbus for Transformer models to address the above efficiency bottlenecks. Specifically, our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a Client-Side Outer Product (COP) protocol to facilitate linear layers. Our COP protocol incorporates two key innovations. First, the static nature of model weights allows the server to send encrypted weights to the client during the setup stage, thereby eliminating input communication during the online stage. Second, removing input communication enables us to design a novel row-wise encoding scheme that achieves homomorphic matrix multiplication via the outer product. Such encoding further enhances the efficiency of homomorphic matrix multiplication and yields compact output ciphertexts for communication. \u2022 For non-linear layers, we present a new observation that their input distribution exhibits regular patterns. Unlike prior approximations that assumed a uniform input distribution, our approach reduces the approximation budget allocated to seldom-occurring input values. This enables us to use lower-degree polynomials and fewer pieces to approximate non-linear functions. Additionally, low-degree polynomials demonstrate lower sensitivity to fixed-point errors, allowing us to adopt a smaller ring for greater efficiency. We also propose a new protocol that enables free conversion between the small and large rings. Consequently, our approach achieves improved performance for non-linear layers while incurring only an average accuracy loss of $0.08\\%$ . \u2022 We evaluate the performance of Nimbus using the popular Transformer model BERTbase under both LAN and WAN settings. Compared with the SOTA work BumbleBee [26], we improve the performance of securely computing matrix multiplication (resp., GELU and Softmax) by $2.9\\times\\sim12.5\\times$ (resp., $2.9\\times\\sim4.0\\times)$ ). Combining all the optimizations, we improve the end-to-end performance of secure two-party inference by $2.7\\times\\sim5.9\\times$ and reduce the communication cost by $60\\%$ . The code is available at: https://github.com/secretflow/spu. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We present the necessary background, including the threat model, cryptographic building blocks, and secure Transformer inference. ", "page_idx": 1}, {"type": "text", "text": "2.1 Threat Model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our protocol works in the two-party setting where the client $C$ holds an input and the server $S$ holds a model. Our protocol is secure in the presence of a semi-honest adversary who could passively corrupt either the client or the server, where the adversary follows the protocol specification but may try to learn more information than allowed. Semi-honest adversary is a common assumption for privacypreserving machine learning and has been used in most two-party protocols [18, 34, 17, 14]. As in all prior two-party inference protocols, the client is only allowed to learn the model\u2019s architecture and inference result while the server gains no information about the client\u2019s input. ", "page_idx": 1}, {"type": "text", "text": "2.2 Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We use upper-case bold letters to represent matrices like W for model weights and $\\mathbf{X}$ for activations. For a matrix W, we use $\\mathbf{W}_{i}$ to denote the $i$ -th row of W and $\\mathbf{W}_{i,j}$ to denote the entry in the $i$ -th row and $j$ -th column of $\\mathbf{W}$ . For an integer $n$ , we write $[n]=\\{0,\\bar{1},\\cdot\\cdot\\cdot,n-1\\}$ . For an additive secret sharing $\\langle\\cdot\\rangle$ (defined in Section 2.3), we use $\\langle\\cdot\\rangle_{c}$ (resp., $\\langle\\cdot\\rangle_{s},$ ) to denote a share held by a client (resp., a server). We denote by $\\mathbb{[M]}$ the homomorphic encryption (HE) ciphertexts on matrix/vector M where it may consist of multiple ciphertexts. We use $\\mathbb{Z}_{2^{\\ell}}=\\mathbb{Z}\\cap[0,2^{\\ell})$ to denote a ring with all entries modulo $2^{\\ell}$ . For a power-of-two integer $N$ , we use $\\mathbb{A}_{N,2^{\\ell}}=\\mathbb{Z}_{2^{\\ell}}[X]/\\left(X^{N}+1\\right)$ to denote a set of polynomials over a ring $\\mathbb{Z}_{2^{\\ell}}$ . Besides, we use lower-case letters with a \u201chat\u201d symbol such as $\\hat{a}$ to represent polynomials of degree $N-1$ in $\\mathbb{A}_{N,2^{\\ell}}$ , where $\\hat{a}[j]$ denotes the $j$ -th coefficient of polynomial $\\hat{a}$ . Note that a polynomial $\\hat{a}$ can encode at most $N$ elements over $\\mathbb{Z}_{2^{\\ell}}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 Building Blocks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our framework is built upon multi-party computation (MPC) techniques, including additive secret sharings and homomorphic encryption (HE). The building block of oblivious transfer (OT) and the sub-protocols used in non-linear layers can be found in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Additive Secret Sharings. In the two-party setting, an additive secret sharing over a ring $\\mathbb{Z}_{2^{\\ell}}$ is defined as: for a value $x\\in\\mathbb{Z}_{2^{\\ell}}$ , two random shares $\\langle x\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}$ and $\\langle x\\rangle_{s}\\in\\mathbb{Z}_{2^{\\ell}}$ are sampled uniformly such that $x=\\langle{x}\\rangle_{c}+\\langle{x}\\rangle_{s}$ mod $2^{\\ell}$ , where $\\langle x\\rangle_{c}$ is held by a client and $\\langle x\\rangle_{s}$ is held by a server. ", "page_idx": 2}, {"type": "text", "text": "Homomorphic Encryption. We adopt the lattice-based additive HE scheme [26] (building upon [33]). HE allows one party to perform computations on the encrypted data of the other party without the need for the decryption key. The HE scheme encodes a plaintext vector $\\mathbf{m}\\in(\\mathbb{Z}_{2^{\\ell}})^{\\bar{N}}$ into a plaintext polynomial $\\hat{m}\\in\\mathbb{A}_{N,2^{\\ell}}$ , and then $\\hat{m}$ is encrypted to a ciphertext $\\mathbb{\\left[m\\right]}=(\\hat{b},\\hat{a})\\in\\mathbb{A}_{N,q}^{2}$ where $q$ is a ciphertext modulus. Given a ciphertext $[\\mathbf{m}]$ and a circuit $f$ including only linear operations, one can homomorphically compute another ciphertext $\\mathbb{I}^{f}(\\mathbf{m})\\mathbb{I}$ . We refer the reader to Appendix B.1 for details of the HE scheme and its homomorphic operations. ", "page_idx": 2}, {"type": "text", "text": "Conversion between Floating-point Numbers and Ring Elements. As 2PC and HE usually operate over rings, the floating-point numbers used in Transformers need to be converted into fixed-point numbers in a ring. Given a scale $s\\,\\in\\,\\mathbb{N}$ (i.e., the length of the fractional part) and a ring $\\mathbb{Z}_{2^{\\ell}}$ , a floating-point number $x\\in\\mathbb R$ is converted to the approximated fixed-point number by computing $\\tilde{x}:=\\lfloor x\\cdot2^{s}\\rfloor$ mod $2^{\\ell}$ , and $\\tilde{x}$ can be converted back to the approximated $x$ by setting $\\tilde{x}/2^{s}$ . ", "page_idx": 2}, {"type": "text", "text": "2.4 Secure Two-Party Transformer Inference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The details of the Transformer architecture are described in Appendix A. To securely evaluate the model, the input and output of all layers are in the form of additive secret sharing, enabling the arbitrary linkage of different layers despite specific protocols. This work optimizes the protocol of the linear layers, including $\\mathsf{L i n e a r_{q k v}}$ , Linearo, $\\mathsf{L i n e a r}_{\\mathsf{h}_{1}}$ , and $\\mathsf{L i n e a r}_{\\mathsf{h}_{2}}$ . We also optimize the protocols for non-linear layers Softmax and GELU. The activation multiplication in the attention and layer normalization are relatively fast following SOTA studies [28, 26]. We do not give special optimizations and leave them as future works. ", "page_idx": 2}, {"type": "text", "text": "3 Secure Computation of Linear Layers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first analyze the efficiency problems of the prior solution in Section 3.1. Then, Section 3.2 presents our client-side outer product (COP) protocol with row-wise encoding. In Section 3.3, we optimize the memory occupation of our COP protocol. ", "page_idx": 2}, {"type": "text", "text": "3.1 Prior Solution: Server-side Inner Product Protocol ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The starting point of this work is the protocol so-called server-side inner product (SIP) [17, 14, 26], as shhelodw bn yi nt hFei gcluireen t2 (aan).d  Tsehrev ienr.p uTths eo fs etrhve elri naelsaor  lhaoyledrs  atrhe ea dwdeiitgivhet ss $\\mathbf{W}\\in\\mathbb{Z}_{2^{\\ell}}^{m\\times n}$ .s $\\langle\\mathbf{X}\\rangle_{C},\\langle\\mathbf{X}\\rangle_{S}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{x}=\\pi_{\\mathrm{L}}(\\mathbf{X}):\\hat{x}[i m n+(m-1)-j]=\\mathbf{X}_{i,j},\\;\\mathrm{for}\\;i\\in[k],j\\in[m]}\\\\ &{\\hat{w}=\\pi_{\\mathrm{R}}(\\mathbf{W}):\\hat{w}[j m+i]=\\mathbf{W}_{i,j},\\;\\mathrm{for}\\;i\\in[m],j\\in[n]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The values of two activation shares and server weights are encoded into polynomials over $\\mathbb{A}_{N,2^{\\ell}}$ using encoding functions $\\pi_{\\mathrm{L}}~:~\\mathbb{Z}_{2^{\\ell}}^{k\\times m}~\\to~\\mathbb{A}_{N,2^{\\ell}}$ and $\\pi_{\\mathrm{R}}~:~\\mathbb{Z}_{2^{\\ell}}^{m\\times n}~\\to~\\mathbb{A}_{N,2^{\\ell}}$ , as shown in Equation (1). The coefficients of the polynomials and $\\hat{w}$ that are not defined are set to 0. ", "page_idx": 2}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/772693572b8a9f7ff9e2a7f08127da0bca69205d3538a6a9d312289890935fae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Two rows represent the client and server operations, respectively. The inefficient parts that are accelerated are marked by dashed boundaries. The input communication is shifted as a one-time setup, and the output ciphertexts are compact. The expensive NTT/INTT operations at the online stage are also reduced. ", "page_idx": 3}, {"type": "text", "text": "Some of coefficients of polynomial $\\hat{z}=\\hat{x}\\cdot\\hat{w}\\in$ $\\mathbb{A}_{N,2^{\\ell}}$ gives the result of matrix multiplication Z = X \u00b7 W \u2208Z2k\u2113\u00d7n, as illustrated in Figure 1. If $k m n>N$ , the encoding function would use coefficients with degrees exceeding $N$ . The input matrices $\\mathbf{X}$ and $\\mathbf{W}$ need to be partitioned into smaller windows with respective dimensions $k_{w}\\times m_{w}$ and $m_{w}\\times n_{w}$ , which results in multiple windows of the output matrix $\\mathbf{Z}$ with ", "page_idx": 3}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/7f485aa2299c5fa458525922ed9cd2bded08fafb1d8372cc8c4889d123dc6c3a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: An example of the window encoding of the matrix multiplication using $N=16$ and $\\ell=5$ . ", "page_idx": 3}, {"type": "text", "text": "dimension $k_{w}\\times n_{w}$ . Therefore, we refer to this encoding approach as window encoding. ", "page_idx": 3}, {"type": "text", "text": "Then, the client encrypts her plaintext polynomials as HE ciphertexts $[\\langle\\mathbf{X}\\rangle_{c}]$ , and sends them to the server. After receiving $[[\\langle\\mathbf{X}\\rangle_{c}]]$ , the server computes the HE ciphertexts $\\mathbb{I}^{\\mathbf{X}}]$ by homomorphically adding $[[\\mathbf{\\langleX\\rangle}_{c}]+\\mathbf{\\langleX\\rangle}_{s}^{\\bullet}$ . Next, the server homomorphically computes $\\bar{\\mathbf{W}}\\cdot\\bar{\\mathbb{[X]}}-\\mathbf{R}$ to obtain HE ciphertexts $\\left[\\mathbf{WX}-\\mathbf{R}\\right]$ , where $\\mathbf{R}$ is randomly generated to mask $\\mathbf{Y}\\,=\\,\\mathbf{W}\\mathbf{X}$ and keeps as the server\u2019s output shares $\\langle\\bar{\\mathbf{Y}}\\rangle_{s}$ . Finally, the server sends $\\left[\\mathbf{WX}-\\mathbf{R}\\right]$ to the client who decrypts the HE ciphertexts into $\\mathbf{WX}{\\mathrm{-}}\\mathbf{R}$ that is used as the client\u2019s shares $\\langle\\mathbf{Y}\\rangle_{c}$ . Note that number theoretic transform (NTT) [30] is employed to weight plaintext polynomial and activation ciphertext polynomial so that their multiplication complexity is reduced from $O(N^{2})$ to $O(N\\log N)$ . ", "page_idx": 3}, {"type": "text", "text": "Analysis of Communication and Computation Costs. To simulate the inner product, the window encoding produces a sparse output (e.g. the even-degree terms of $\\hat{z}$ in Figure 1). The sparse polynomials are treated as dense after encryption, leading to inefficient communication and computation marked by the dashed boundary in F\u221aigure 2(a). First, the computation includes unnecessary zero terms. Second, Iron shows at least 2\u221akNmn ciphertexts are transmitted [14]. Then, BumbleBee [26] proposes a packing approach that trades computation for less communication, but the overall latency is similar. ", "page_idx": 3}, {"type": "text", "text": "3.2 Client-side Outer Product Protocol ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To solve the efficiency problems as described above, we propose an alternative client-side outer product $(C O P)$ protocol. The COP protocol includes two key insights. First, the static nature of model weights allows the server to send encrypted weights to the client at the setup stage, which can eliminate input communication at the online stage. Second, this elimination of input communication enables us to design a new row-wise encoding that realizes homomorphic matrix multiplication through the outer product. Our encoding further results in compact output ciphertext for communication and enhances the efficiency of HE matrix multiplication. The formal protocol is described in Appendix C.1. ", "page_idx": 3}, {"type": "text", "text": "COP Protocol. In Figure 2(b), we describe the COP protocol for secure matrix multiplication, where the dashed boundary shows the optimizations of computation and communication over prior works. In the setup stage, the server encodes the model weights W in a row-wise fashion and sends the HE ciphertexts $\\left[\\mathbf{W}\\right]$ on these weights to the client. The client stores the HE ciphertexts $\\left[\\mathbf{W}\\right]$ in the disk, which enables these ciphertexts to be reused for multiple queries by loading them into memory. In the execution stage, for additive secret sharings $\\langle\\mathbf{X}\\rangle_{c}$ , $\\langle\\mathbf{X}\\rangle_{s}$ held by the client and server respectively, the client homomorphically computes $\\langle\\mathbf{X}\\rangle_{c}\\cdot\\[\\mathbf{W}]$ to obtain HE ciphertexts $[\\mathbf{W}\\langle\\mathbf{X}\\rangle_{c}]$ , and the server locally computes $\\bar{\\bf W}\\cdot\\langle{\\bf X}\\rangle_{s}$ in plaintext. Then, the client samples a random matrix $\\mathbf{R}$ (used as its ", "page_idx": 3}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/73eea3ae88b7683a3b2ffdfe7b19e9189e575e547ca1fc6cd9fdeffb5a04947c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Illustration of our matrix multiplication. Left: Functionality of the matrix multiplication using row-wise encoding. Middle: Computing the first row of the output through the scalar-poly product. Right: Packing two ciphertexts using aright shift for less number of output ciphertext. ", "page_idx": 4}, {"type": "text", "text": "output shares $\\langle\\mathbf{Y}\\rangle_{c}=\\mathbf{R}$ where $\\mathbf{Y}\\,=\\,\\mathbf{W}\\mathbf{X})$ ), and homomorphically computes $[\\mathbf{W}\\langle\\mathbf{X}\\rangle_{c}]-\\mathbf{R}$ to obtain HE ciphertexts $[\\mathbf{W}\\langle\\mathbf{X}\\rangle_{c}-\\mathbf{R}]$ which is sent to the server. Finally, the server decrypts these ciphertexts to obtain ${\\mathbf{W}}\\langle{\\mathbf{X}}\\rangle_{c}-{\\mathbf{R}}$ , and then sets its output shares as $\\langle\\mathbf{Y}\\rangle_{s}=\\mathbf{W}\\langle\\mathbf{X}\\rangle_{c}-\\mathbf{R}+\\mathbf{W}\\langle\\mathbf{X}\\rangle_{s}$ . As a result, two parties hold additive secret sharings $(\\langle\\bar{\\mathbf{Y}}\\rangle_{c},\\langle\\mathbf{Y}\\rangle_{s})$ . Below, we explain the key process for computing $\\langle\\mathbf{\\bar{X}}\\rangle_{c}\\cdot\\[\\mathbf{W}]$ using the row-wise encoding approach. ", "page_idx": 4}, {"type": "text", "text": "Row-wise Encoding. Assigning the client to perform the plaintext-ciphertext multiplication has a direct benefti in terms of saving the communication of inputs. More importantly, such saving enables us to design a new encoding approach for the weights and activations. First, we use the following function to encode each row of the weight matrix, i.e., $\\mathbf{W}_{i}$ for $i\\in[m]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}_{i}=\\pi_{\\mathrm{R}}(\\mathbf{W}_{i})\\mathrm{~s.t.~}\\hat{w}_{i}[j]=\\mathbf{W}_{i,j},\\;\\mathrm{for}\\;j\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The output dimensions $n$ of Transformers are generally less than $N$ , so the last $N-n$ coefficients are set as zero, indicated by the blank squares in Figure 3. Row-wise encoding corresponds to an extreme case of prior window encoding (i.e., setting $n_{w}=n_{z}$ ), which may cause a large number of ciphertexts on inputs. This problem is solved by eliminating the input communication in our COP protocol. Second, for the activations, the client no longer encodes his share into polynomials but directly performs multiplication between the plaintext activation shares and ciphertext on weights. ", "page_idx": 4}, {"type": "text", "text": "Efficient Computation and Compact Output Ciphertexts. Our encoding realizes secure matrix multiplication through outer product, which achieves more efficient homomorphic computation and compact HE ciphertexts on the output. We illustrate the computation of the first row of the output matrix $\\mathbf{Z}_{1}$ in Figure 3. Following the spirit of the outer product, each scalar-polynomial multiplication produces a partial sum of $\\mathbf{Z}_{1}$ and their accumulation produces the final output. The scalar-polynomial multiplication has same complexity as the prior poly-poly multiplication in the NTT space. But we reduce the online NTT/INTT operation, and thus reduce the computation cost. For the output communication, if $n$ is smaller than the polynomial degree $N$ , the output ciphertext still leaves blank when communicating. The row-wise encoding makes valid coefficients and zeros separate in the output ciphertext instead of in an interleaved fashion. This enables packing the output ciphertexts through a free operation, right shift. Right shift coefficients for $s$ steps in a ciphertext can be done by multiplying the ciphertext with a plaintext polynomial with only a $s$ -order term. The right figure of Figure 3 shows the right shift packing. The second ciphertext is shifted to the right four slots and added with the first ciphertext. Then, all slots of the output are utilized for output communication. ", "page_idx": 4}, {"type": "text", "text": "Complexity Analysis. Through analysis in Appendix D, Table 1 compares the computation complexity and numbers of communicated ciphertexts. Communication: Our COP protocol eliminates the input communication, and output communication is the minimal case of priorkwknnw when kwnw = N since values are densely arranged in the output ciphertext. Computation: The SIP protocol requires polynomial-polynomial multiplication. It applies NTT with $O(N\\log N)$ complexity to the plaintext polynomial of weights, and ciphertext polynomials of inputs and outputs. Then, the complexity of all poly-poly multiplications in the NTT space is $O(k m n)$ . In our protocol, the NTT is only applied when the server decrypts the output ciphertext. The client\u2019s scalar-polynomial multiplication saves time spent on the NTT by directly performing the multiplication with a complexity $O(k m N)$ , which is comparable to the previous multiplication in the NTT space, especially when $n$ is close to $N$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Memory Impact of the COP Protocol ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our COP protocol, the client stores the encrypted model weights in the disk so that the ciphertexts can be reused. At the online stage, the encrypted weights are loaded into memory for secure matrix multiplication. In this way, the client executes an efficient outer product rather than the original encryption and decryption. The feasibility of such workload reallocation is due to the difference between the \"client\" in MPC and the traditional client. Due to the symmetric-computation characteristic of MPC as well as the expensive NTT cost brought about by homomorphic encryption and decryption, existing secure-inference frameworks, e.g., [17, 14, 5, 26, 29, 42], require the client to be equipped with similar resources as the server, including a powerful CPU (e.g., 64 vCPUs) and a large memory (e.g., 128 GB) [42, 26, 29]. For clients in MPC, disk usage does not pose a significant issue as storage resources are inexpensive. The CPU usage is also not an issue as the analysis and experiments in Appendix F.2 indicate that the client\u2019s computational overhead remains similar as the prior SIP protocol. However, we notice that keeping encrypted weights instead of plaintext weights may introduce additional overhead to memory usage, which we address in the next paragraph. ", "page_idx": 4}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/ec723ff3108e91a6359c3d0f2c73e13007f23d9c360423d74f3378b03cb1272d.jpg", "table_caption": ["Table 1: Comparison of the computation and communication for multiplication of two matrices with dimension $k\\times m$ and $m\\times n$ . $k_{w},m_{w},n_{w}$ are the window size corresponding to matrix dimensions. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Asynchronous Weight Loading. Our COP protocol allows the weights to be encrypted at the setup stage and stored in the client disk. Different from prior SIP protocol that keeps the model weight shares in memory, loading all encrypted model weights in memory may become a burden since the size of ciphertext is at least four times larger than the secret shares[26]. To reduce the additional usage of memory, we let the client only keep the encrypted weights w.r.t. the current layer in memory (e.g., either $180\\,\\mathrm{MB}$ or $720\\,\\mathrm{MB}$ for Transformer model $\\mathrm{BERT_{base}}$ ). The encrypted weight of the subsequent layer is loaded asynchronously with the communication of output ciphertexts of the linear layer and the secure computation of the following non-linear layer, which involves large-size communication and multiple rounds of interaction. Moreover, the network bandwidth is hundreds of times smaller than the disk bandwidth. For example, as shown in Appendix F.2, loading the encrypted weights of one layer in $\\mathrm{BERT_{base}}$ from the disk to memory can be accomplished in tens of milliseconds, while the communication between the client and server requires several seconds. Therefore, the loading time of the encrypted weights can overlap with the communication process. ", "page_idx": 5}, {"type": "text", "text": "4 Secure Computation of Non-Linear Functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Prior Solution: Piecewise Polynomial Approximation of Non-Linear Functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For Transformers, the main efficiency bottleneck in non-linear layers is to securely compute functions exponential and GELU [5, 28, 26]. These works approximate the non-linear functions through piecewise polynomial approximation, which can be securely computed by executing two-party addition, multiplication, and comparison operations. To maintain the accuracy, these works adopt four-piece polynomials with degree 6 for GELU and two-piece Taylor series with Taylor expansion degree 6 for exponential. The approximation of high-degree polynomials inherently imposes a large overhead for securely computing the powers of values. Additionally, such an approximation requires computations to be conducted over a large ring $\\mathbb{Z}_{64}$ and with a large scale $s=18$ [14, 5, 26]. This is brought about by the fact that computing the powers of values with high degrees leads to the accumulation of fixed-point errors and the potential overflow problem. ", "page_idx": 5}, {"type": "text", "text": "4.2 Simpler Piecewise Polynomial and Smaller Rings by Distribution-aware Approximation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We aim to use simpler piecewise polynomials to fti non-linear functions and reduce the size of rings without sacrificing accuracy. Inspired by the finding that activation distribution exhibits a regular pattern across training and test data [24, 40], our insight for enabling simpler polynomials is to assign the approximation budget according to the input distribution instead of treating all input values with equal importance. Figure 4 illustrates patterns of the input distribution using the $\\mathrm{BERT_{base}}$ \u2019s nonlinear functions at the $4_{t h}$ encoder. As an example, consider the input distribution of the GELU function. The probability peak centers around $-3$ and values greater than zero occur with less than $10\\%$ probability. A wise strategy should leave more budget to the high-probability ranges. Compared with prior research directly minimizing the approximation error of the original function, assuming a uniform input distribution, our strategy is supposed to generate more effective approximations. Additionally, we want to note that the fitted polynomial does not leak the input distribution of the data as the client remains oblivious to the fitted polynomial during secure inference. ", "page_idx": 5}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/e23a161c56c80ea6dc7b5e9804ec0b2c8e36f55f8ca243fe6995a1ed7ea78f82.jpg", "img_caption": ["Figure 4: The input distribution of non-linear functions. The y-axis indicates the occurrence counts. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Distribution-aware Splitting and Fitting. Prior works typically split the input range and fit each interval based on the non-linearity of the curve. We further include the consideration of the input distribution for these two processes. For intervals with low non-linearity or input probability, we split them out and assign constant or linear polynomials to fit. The other intervals with both high non-linearity and input probability are fitted by quadratic or cubic polynomials. When fitting each piece of the non-linear function $f(x)$ by the polynomial $f^{\\prime}(x)$ , we minimize the expected loss that integrates the inputs\u2019 probability density $p(x)$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f^{\\prime}(x)}\\int_{l}^{h}p(x)\\left[f(x)-f^{\\prime}(x)\\right]^{2}d x,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $l$ and $h$ are the lower bound and upper bound. $p(x)$ is the probability density function obtained by summarizing a batch of training data. Unlike prior works using fixed breakpoints $l$ and $h$ , we initialize each breakpoint with a starting value and search around it to better fti non-linear functions at different depths. This is because although the activation distributions are broadly similar, they may shift slightly across varying model depths and the breakpoints should be adjusted accordingly. We refer to the Appendix C.2 for the splitting and fitting algorithm. The detailed protocols for securely evaluating nonlinear functions are provided in Appendix C.1. Next, we elaborate on the specific design for fitting exponential and GELU functions. ", "page_idx": 6}, {"type": "text", "text": "Exponential. The exponential is used in the Softmax. Given an input vector $\\mathbf{x}$ , the $i$ -th element of Softmax is computed as $\\frac{\\exp(x_{i}-\\operatorname*{max}\\{\\mathbf{x}\\})}{\\sum_{j}\\exp(x_{j}-\\operatorname*{max}\\{\\mathbf{x}\\})}$ . Input values subtracted from maximal values result in \u00b7 maximal zero. The exponential curve exhibits two distinct patterns: a long smooth tail on the left and a sharp increase on the right. Prior works adopt a two-piece approximation by breakpoint -13. Instead, we initial breakpoints around $^{-4}$ for varying depths. As the right interval spans a smaller range, it adopts a cubic polynomial $P^{3}(x)$ instead of the Taylor series with expansion degree six [5, 26]. Values less than $^{-4}$ are less occurred and the curve is smooth, and a linear function is enough to fit. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\exp(x)\\approx\\left\\{{0\\atop P^{3}(x)}\\right.\\ \\ X<T_{\\mathrm{exp}}}\\\\ {\\left.\\exp\\left(x\\right)\\ \\ N>\\right.}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "GELU. The GELU curve nearing the zero exhibits pronounced non-linearity. Prior works [5, 26] assign two polynomials for intervals $[-5,-1.97]$ and $[-1.97,3]$ with degree three and six. We merge these two intervals by one and shrink the range to $[T_{1},T_{2}]=[-2.1,0.2]$ . This is because the values beyond this interval present either less non-linearity or fewer occurrence probabilities, and using constant or linear polynomials is enough. As the middle interval becomes narrow, we find a square polynomial $P^{2}(x)$ is enough. The specific breakpoints $T_{1}$ and $T_{2}$ change for different depths. ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathrm{GELU}}(x)\\approx\\left\\{{\\begin{array}{l l}{0}&{x\\leq T_{1}}\\\\ {P^{2}(x)}&{T_{1}<x\\leq T_{2}}\\\\ {x}&{x>T_{2}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/d873472f306a498c239fde193601672dc66c1f938a3c33686f48107630fe6194.jpg", "table_caption": ["Table 2: Accuracy comparison of floating-point (FP) baseline, BumbleBee, Nimbus (without finetuning), and Nimbus\u2020 (with finetuning). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Free Ring Conversion by Fusion with Truncation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our low-degree polynomials reduce the errors of operation on fixed-point numbers and potential overflow problems. This enables smaller ring $\\mathbb{Z}_{32}$ and precision $s=12$ for computing Softmax and GELU functions, instead of the original standard ring $\\mathbb{Z}_{64}$ and precision of $s=18$ . However, since other operations still require the larger ring to preserve the accuracy, another challenge is to convert secret shares between differently sized rings. The process of downcasting from a larger to a smaller ring can be performed locally, incurring negligible cost [31, 39]. Upcasting from a smaller to a larger ring necessitates addressing the wrap-around of shares, requiring communication among parties. Interestingly, we notice the situations demanding to upcast are always after a truncation operation that inherently computes the wrapping, which can be repurposed for the upcast to avoid additional costs. We propose a novel protocol that fuses the upcast with the truncation. We defer the protocol and the correctness proof to the Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5 Performance Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental Setup. We follow similar configurations used in prior works [26]. Except optimized non-linear functions using ring $\\mathbb{Z}_{2^{32}}$ and precision $s=12$ , other operations follow standard $\\mathbb{Z}_{2^{64}}$ and $s=18$ for the secret sharing. We use $N=8192$ for the HE encryption. The performances are evaluated on two nodes with 64 vCPUs and 128 GB memory. We use Linux Traffic Control (tc) to simulate LAN and WAN network settings, where the bandwidth and the ping latency are (3Gbps, 1ms) and (400Mbps, 10ms), respectively. ", "page_idx": 7}, {"type": "text", "text": "Baselines. The baselines include Iron [14] and BumbleBee [26]. Our implementation follows the open-sourced code of BumbleBee on SecretFlow [28]. As Iron is not open-sourced, we implement Iron following their protocol using the SecretFlow library for a fair comparison. For the linear layer, Iron uses window encoding described in Section 3.1, and BumbleBee further compresses the output ciphertext. For non-linear functions, Iron evaluates them via integrating underlying protocols. Later works [5] use piecewise polynomial approximation. BumbleBee further integrates cryptographic optimizations to make a stronger baseline. In the Appendix F.4, we also compare our work with those that use rough approximations to trade off accuracy for efficiency [22, 29]. ", "page_idx": 7}, {"type": "text", "text": "Model and Datasets. Our method is evaluated on widely used Transformer model $\\mathrm{BERT_{base}}$ [19] from HuggingFace [38]. When evaluating the performance, we use 128 as a mild average number of the input sequence length. To evaluate the accuracy of our non-linear approximation, we test it on eight datasets from widely used GLUE benchmark [37]. To obtain the input distribution of non-linear functions, we randomly sample sentences from the training dataset until the total token count reaches 512. This number is chosen because further increasing the number of sampled tokens yields no significant changes in the input distribution. ", "page_idx": 7}, {"type": "text", "text": "5.1 Accuracy Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 2 reports the accuracy of floating-point plaintext, BumbleBee, and our approximation across 8 tasks in the GLUE benchmark[37]. The precise approximation of BumbleBee causes small errors due to the truncation error of the fixed-point value computation. Without fine-tuning, Nimbus decreases accuracy in a small range and the average loss is around $0.6\\%$ . Such loss can be easily reduced to $0.08\\%$ through a lightweight fine-tuning Nimbus\u2020. This demonstrates the effectiveness of our approximation. We also compare accuracy and efficiency with works that compromise accuracy in Appendix F.4, including MPCFormer [22] and BOLT [29]. ", "page_idx": 7}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/166458b6bd591ff1880979ff920264d6d52ab4dc2bc69e0176cf3b22c2804e5d.jpg", "img_caption": ["Figure 5: The L2-Norm of output error between oracle non-linear functions and approximations. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/a9a9e432ddb070e16ac9e8d803f0d6106b5be851533a0c815028abda4acc77d2.jpg", "img_caption": ["Figure 6: The end-to-end latency of a Transformer block of $\\mathrm{BERT}_{\\mathrm{base}}$ and its breakdown. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 5 presents the output error of exponential and GELU functions to further explain the effectiveness of our approximation. The error is summarized using a batch of test data on a certain layer. On the standard ring $\\mathbb{Z}_{2^{64}}$ and precision $s=18$ , our L2-norm errors are around 0.005 and are close to the loss-free approximation of BumbleBee. When reducing to ring $\\mathbb{Z}_{2^{32}}$ and $s=12$ , BumbleBee encounters higher errors. This increase is attributed to the more pronounced fixed-point errors that arise when evaluating high-degree polynomials. Moreover, the destructive overflow occurs for precision greater than 10 bits, as the sharp divergence of green and red curves of GELU function. Nimbus has a steady fixed-point error and is not prone to overflow thanks to the low-degree polynomial. This enables moving to the smaller ring with a minor impact on the accuracy. ", "page_idx": 8}, {"type": "text", "text": "5.2 Efficiency Comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As the main body of the Transformer model are identical Transformer blocks, we present the end-toend latency of one Transformer block under LAN and WAN network settings in Figure 6. Besides the optimized parts of this paper, we unify the unoptimized activation matrix multiplication $(Q K^{T}\\&P V)$ and LayerNorm (LN) using the BumbleBee\u2019s latency. The latencies of Iron are shorter than those reported in their paper due to SecretFlow integrating many SOTA optimizations for the building blocks, such as OT [41] and inverse square root [25]. Combining all our optimizations, the overall runtime is $4.8\\times\\sim5.9\\times$ faster than Iron and $2.7\\times\\sim4.7\\times$ faster than BumbleBee. Comprehensive results on varying Transformer size and input sequence length are listed in Appendix F.1. In the following, we provide a detailed analysis of linear and non-linear layers. ", "page_idx": 8}, {"type": "text", "text": "For linear layers, our method is efficient in both computation and communication. Therefore, we achieve obvious speedup in both LAN and WAN settings. Compared with stronger BumbleBee, we have $7.2\\times\\sim12.5\\times$ in the LAN setting and $2.9\\times\\sim4.0\\times$ in the WAN setting. More speedup for the LAN setting indicates we accelerate the computation more than the communication. For non-linear functions, our method reduces both the communication size and rounds, so that we obtain similar speedup for both the LAN and WAN settings. Compared with stronger baseline BumbleBee, the GELU is $3.4\\times$ faster in the LAN setting and $3.3\\times$ faster than the WAN setting. The Softmax is $4.0\\times$ faster in the LAN setting and $2.9\\times$ faster than the WAN setting. ", "page_idx": 8}, {"type": "text", "text": "5.3 Communication Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Then, we compare the communication cost and the number of rounds of linear layers, Softmax, and GELU in Table 3. The data is summarized using $\\mathrm{BERT}_{\\mathrm{base}}$ and sequence length 128. For different types of linear layers, our protocol only requires half the number of communication rounds. ", "page_idx": 8}, {"type": "text", "text": "Our total communication size of linear layers is reduced to only $11.51\\%$ of that of Iron. Although BumbleBee takes extra \"automorphism\" operation to compress the output ciphertext, our communication is still only $65\\%$ compared with BumbleBee since we also eliminate the communication of the input ciphertext. As for the nonlinear layers, compared with stronger baseline BumbleBee, we have fewer rounds and $3\\times$ less communication due to simpler piecewise polynomial approximations and the smaller ring si ", "page_idx": 9}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/35f7cc7d5f2500644a0df5f5275391b7389130856df103ac665574ae107796ca.jpg", "table_caption": ["Table 3: Communication cost (megabytes) and rounds comparison on one Transformer block. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Privacy-preserving Neural Network Inference. Due to the rapidly growing concerns about data privacy in DNN-based applications, significant efforts have been made to design efficient cryptographic protocols for DNN models [8, 18, 32, 43, 31]. Early works focus on the convolutional neural network (CNN) models. Cryptonets [8] proposed one of the first protocols for 2PC HE-based private neural network inference. Later works [18, 34, 17] are hybrid 2PC neural network inference protocols combining HE for matrix multiplications and multi-party computation for non-linear functions. ", "page_idx": 9}, {"type": "text", "text": "Private Transformers. Several works have investigated two-party secure inference for the Transformer model. For linear layers, Iron [14] builds upon Cheetah [17] by generalizing the original encoding of matrix-vector multiplication to matrix-matrix multiplication. Both Cheetah and Iron leave blanks in the input and output ciphertexts. BumbleBee [26] utilizes the \"automorphism\" operation to compress multiple output ciphertexts, which trades computation for communication. A recent work BOLT [29] adopts SIMD encoding to homomorphically evaluate the linear layer, which also trades computation for the compact output ciphertext. All existing works adopt the server-side inner product protocol. In contrast, this work proposes the client-side outer product protocol that eliminates the input ciphertext communication. The proposed protocol also allows a novel encoding approach that facilitates more efficient homomorphic computation and output communication. Other works [1, 5] consider 3PC inference for Transformers, which rely on different settings and cryptographic primitives from this work. ", "page_idx": 9}, {"type": "text", "text": "For the non-linear layers, some studies, such as THE-X [3] and MPCFormer [22], evaluate transformer models using cryptographic friendly replacements for non-linear layers, such as using Softmax $\\approx$ (ix([xi][i+]c+)c2)2 and GELU(x) \u2248x82 + x4 + 12. However, such aggressive approximations lead to noticeable accuracy loss, even when employing knowledge distillation to mitigate the decline in accuracy. Other methods, such as look-up tables for faithful approximation [31, 13, 29], are computationally expensive to maintain model accuracy. Later works, including PUMA [5] and BumbleBee [26], utilize piecewise polynomial approximation, which does not result in an accuracy drop but is also relatively costly to compute. In contrast, this work is inspired by insights from the input distribution used in the Transformer model [9, 10, 11, 12, 40, 24]. We propose fitting the non-linear functions according to their input distribution, allowing for lower-degree polynomials and fewer polynomial pieces without sacrificing accuracy. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a privacy-preserving, accurate, and efficient two-party inference framework Nimbus for Transformers. We present an efficient protocol of secure matrix multiplication using the COP approach, achieving significantly better computation and communication efficiencies. We use a distribution-aware polynomial approximation for non-linear layers, allowing a simpler approximation with less communication and rounds. These optimizations significantly improve the performance, advancing a step towards the practical use of secure Transformer inference. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China grants (62222210, 62102037, 61932019, 92270201, and 62125204). Yu Yu also acknowledges the support from the XPLORER PRIZE. This work was also supported by Ant Group Research Intern Program and we thank all members of the SecretFlow team for their support throughout this project. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yoshimasa Akimoto, Kazuto Fukuchi, Youhei Akimoto, and Jun Sakuma. Privformer: Privacypreserving transformer with mpc. In 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P), pages 392\u2013410. IEEE, 2023.   \n[2] Alon Brutzkus, Ran Gilad-Bachrach, and Oren Elisha. Low latency privacy preserving inference. In International Conference on Machine Learning, pages 812\u2013821. PMLR, 2019.   \n[3] Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei. The-x: Privacy-preserving transformer inference with homomorphic encryption. arXiv preprint arXiv:2206.00216, 2022.   \n[4] Vanya Cohen and Aaron Gokaslan. Opengpt-2: Open language models and implications of generated text. XRDS: Crossroads, The ACM Magazine for Students, 27(1):26\u201330, 2020.   \n[5] Ye Dong, Wen-jie Lu, Yancheng Zheng, Haoqi Wu, Derun Zhao, Jin Tan, Zhicong Huang, Cheng Hong, Tao Wei, and Wenguang Cheng. Puma: Secure inference of llama-7b in five minutes. arXiv preprint arXiv:2307.12533, 2023.   \n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[7] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the 41th Annual ACM Symposium on Theory of Computing - STOC\u201909, page 169\u2013178. ACM, 2009.   \n[8] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International conference on machine learning, pages 201\u2013210. PMLR, 2016.   \n[9] Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo. Transkimmer: Transformer learns to layer-wise skim. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275\u20137286, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[10] Yue Guan, Zhengyi Li, Zhouhan Lin, Yuhao Zhu, Jingwen Leng, and Minyi Guo. Block-skim: Efficient question answering for transformer. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 10710\u201310719, 2022.   \n[11] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1\u201315, 2023.   \n[12] Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 1414\u20131433. IEEE, 2022.   \n[13] Kanav Gupta, Neha Jawalkar, Ananta Mukherjee, Nishanth Chandran, Divya Gupta, Ashish Panwar, and Rahul Sharma. Sigma: secure gpt inference with function secret sharing. Cryptology ePrint Archive, 2023.   \n[14] Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, and Tianwei Zhang. Iron: Private inference on transformers. Advances in Neural Information Processing Systems, 35:15718\u201315731, 2022.   \n[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[16] Xiaoyang Hou, Jian Liu, Jingyu Li, Yuhan Li, Wen-jie Lu, Cheng Hong, and Kui Ren. Ciphergpt: Secure two-party gpt inference. Cryptology ePrint Archive, 2023.   \n[17] Zhicong Huang, Wen-jie Lu, Cheng Hong, and Jiansheng Ding. Cheetah: Lean and fast secure {two-party} deep neural network inference. In 31st USENIX Security Symposium (USENIX Security 22), pages 809\u2013826, 2022.   \n[18] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. {GAZELLE}: A low latency framework for secure neural network inference. In 27th USENIX Security Symposium (USENIX Security 18), pages 1651\u20131669, 2018.   \n[19] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2, 2019.   \n[20] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens van der Maaten. Crypten: Secure multi-party computation meets machine learning. Advances in Neural Information Processing Systems, 34:4961\u20134973, 2021.   \n[21] Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Belouadi, Daniil Larionov, Vivian Fresen, and Steffen Eger. Chatgpt: A meta-analysis after 2.5 months. arXiv preprint arXiv:2302.13795, 2023.   \n[22] Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P Xing, and Hao Zhang. Mpcformer: fast, performant and private transformer inference with mpc. arXiv preprint arXiv:2211.01452, 2022.   \n[23] Jian Liu, Mika Juuti, Yao Lu, and Nadarajah Asokan. Oblivious neural network predictions via minionn transformations. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, pages 619\u2013631, 2017.   \n[24] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137\u201322176. PMLR, 2023.   \n[25] Wen-jie Lu, Yixuan Fang, Zhicong Huang, Cheng Hong, Chaochao Chen, Hunter Qu, Yajin Zhou, and Kui Ren. Faster secure multiparty computation of adaptive gradient descent. In Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice, pages 47\u201349, 2020.   \n[26] Wen-jie Lu, Zhicong Huang, Zhen Gu, Jingyu Li, Jian Liu, Kui Ren, Cheng Hong, Tao Wei, and WenGuang Chen. Bumblebee: Secure two-party inference framework for large transformers. Cryptology ePrint Archive, 2023.   \n[27] Vadim Lyubashevsky, Chris Peikert, and Oded Regev. On ideal lattices and learning with errors over rings. In Advances in Cryptology\u2013EUROCRYPT 2010: 29th Annual International Conference on the Theory and Applications of Cryptographic Techniques, French Riviera, May 30\u2013June 3, 2010. Proceedings 29, pages 1\u201323. Springer, 2010.   \n[28] Junming Ma, Yancheng Zheng, Jun Feng, Derun Zhao, Haoqi Wu, Wenjing Fang, Jin Tan, Chaofan Yu, Benyu Zhang, and Lei Wang. {SecretFlow-SPU}: A performant and {UserFriendly} framework for {Privacy-Preserving} machine learning. In 2023 USENIX Annual Technical Conference (USENIX ATC 23), pages 17\u201333, 2023.   \n[29] Qi Pang, Jinhao Zhu, Helen M\u00f6llering, Wenting Zheng, and Thomas Schneider. Bolt: Privacypreserving, accurate and efficient inference for transformers. In 2024 IEEE Symposium on Security and Privacy (SP), pages 130\u2013130. IEEE Computer Society, 2024.   \n[30] John M Pollard. The fast fourier transform in a finite field. Mathematics of computation, 25(114):365\u2013374, 1971.   \n[31] Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya Gupta, Rahul Sharma, Nishanth Chandran, and Aseem Rastogi. Sirnn: A math library for secure rnn inference. In 2021 IEEE Symposium on Security and Privacy (SP), pages 1003\u20131020. IEEE, 2021.   \n[32] Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul Sharma. Cryptflow2: Practical 2-party secure inference. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 325\u2013342, 2020.   \n[33] Deevashwer Rathee, Thomas Schneider, and K. K. Shukla. Improved multiplication triple generation over rings via RLWE-based AHE. In Cryptology and Network Security, volume 11829 of LNCS, pages 347\u2013359. Springer International Publishing, 2019.   \n[34] Wenting Zheng Srinivasan, PMRL Akshayaram, and Popa Raluca Ada. Delphi: A cryptographic inference service for neural networks. In Proc. 29th USENIX Secur. Symp, pages 2505\u20132522, 2019.   \n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[37] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[39] Haoqi Wu, Wenjing Fang, Yancheng Zheng, Junming Ma, Jin Tan, Yinggui Wang, and Lei Wang. Ditto: Quantization-aware secure inference of transformers upon mpc, 2024.   \n[40] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023.   \n[41] Kang Yang, Chenkai Weng, Xiao Lan, Jiang Zhang, and Xiao Wang. Ferret: Fast extension for correlated ot with small communication. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 1607\u20131626, 2020.   \n[42] Wenxuan Zeng, Meng Li, Wenjie Xiong, Tong Tong, Wen-jie Lu, Jin Tan, Runsheng Wang, and Ru Huang. Mpcvit: Searching for accurate and efficient mpc-friendly vision transformer with heterogeneous attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5052\u20135063, 2023.   \n[43] Wenting Zheng, Ryan Deng, Weikeng Chen, Raluca Ada Popa, Aurojit Panda, and Ion Stoica. Cerebro: a platform for {Multi-Party} cryptographic collaborative learning. In 30th USENIX Security Symposium (USENIX Security 21), pages 2723\u20132740, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/54ea655154372c62e67294d7eafc9cb84110a2ef801ec4cb0696519c587aebd2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 7: The illustration of the Transformer-based model and the latency breakdown of its private evaluation. ", "page_idx": 13}, {"type": "text", "text": "A Background of Transformer Models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We focus on Transformer [36] DNN models, such as popular models BERT [19], GPT-2 [4] and LLaMA [35]. These models are stacked with Transformer blocks, each consisting of an attention module and a feed-forward network (FFN) module. ", "page_idx": 13}, {"type": "text", "text": "Attention Module. The attention module starts with three independent linear layers Linea $\\mathbf{\\dot{\\omega}}_{q k v}$ that project the input to three activation tensors: $\\mathbf{Q},\\mathbf{K}$ , and $\\mathbf{V}$ . The multi-head attention mechanism splits them into and computes the self-attention of all heads in parallel through ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{Attention}(Q,K,V)=\\mathsf{S o f t m a x}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)V,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $d_{k}$ is the hidden dimension of the key activation. The outputs of different heads are concatenated and fed into another linear layer Linear $^o$ , with one residue connection and one normalization layer to generate the final output of the attention module. ", "page_idx": 13}, {"type": "text", "text": "FFN Module. The FFN module is composed of two linear layers and one activation layer ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{FFN}(\\mathbf{X})=\\operatorname{Linear}_{h_{2}}(\\mathsf{G E L U}(\\mathsf{L i n e a r}_{h_{1}}(\\mathbf{X}))),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where GELU [15] is the activation function. Similar to the attention module, its output needs a residue connection and a normalization layer. ", "page_idx": 13}, {"type": "text", "text": "Task Head. After all the transformer blocks are evaluated, the output is fed into a task-specific head for classification, regression, or token generation. ", "page_idx": 13}, {"type": "text", "text": "B More Details of Cryptographic Building Blocks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We give a more detailed description of the cryptographic building blocks as a supplement to the paper.   \nWe follow those notations used in the paper. ", "page_idx": 13}, {"type": "text", "text": "B.1 Lattice-based Additive Homomorphic Encryption ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Homomorphic encryption (HE) enables computation on the encrypted data without knowing the decryption key. This work uses an HE scheme based on Ring Learning-with-Error (RLWE) [27]. The RLWE scheme is defined by a set of public parameters $\\{N,q,t\\}$ , where $N$ is the polynomial degree, $t$ is the modulus of the plaintext, and $q$ is the modulus of the ciphertext. ", "page_idx": 13}, {"type": "text", "text": "\u2022 KeyGen. Generate the RLWE key pair $(s k,p k)$ where the secret key $s k\\in\\mathbb{A}_{N,q}$ and the public key pk \u2208A2N,q.   \n\u2022 Encryption. An RLWE ciphertext is given as a polynomial tuple $(\\hat{b},\\hat{a})\\in\\mathbb{A}_{N,q}^{2}$ . Given a vector $\\mathbf{m}$ which is encoded as $\\hat{m}\\in\\mathbb{A}_{N,t}$ , We write $\\mathbb{[m]}=\\mathsf{E n c}(\\hat{m})$ to denote the encryption of $\\hat{m}$ under a key $p k$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Decryption. Given an RLWE ciphertext $\\mathbb{\\left[m\\right]}=(\\hat{b},\\hat{a})\\in\\mathbb{A}_{N,q}^{2}$ . We write $\\hat{m}=\\mathsf{D e c}(\\mathbb{I}\\mathbf{m}\\mathbb{I})$ to denote the decryption under a secret key $s k$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 Addition ( \u229e). Given two RLWE ciphertexts $\\mathbb{[m}_{0}\\mathbb{]}=\\left(\\hat{b}_{0},\\hat{a}_{0}\\right)$ and $\\mathbb{[m}_{1}\\]\\,=\\,\\left(\\widehat{b}_{1},\\widehat{a}_{1}\\right)$ that respectively encrypts $\\hat{m}_{0},\\hat{m}_{1}\\in\\mathbb{A}_{N,t}$ under a same key, the operation $[\\mathbf{m}_{0}]\\boxplus[\\mathbf{m}_{1}]$ computes the RLWE tuple $\\left(\\hat{b}_{0}+\\hat{b}_{1},\\hat{a}_{0}+\\hat{a}_{1}\\right)\\in\\mathbb{A}_{N,q}^{2}$ which can be decrypted to $\\hat{m}_{0}+$ $\\hat{m}_{1}$ mod $\\mathbb{A}_{N,t}$ .   \n\u2022 Plaintext-ciphertext polynomial Multiplication $(\\boxtimes)$ . Given an RLWE ciphertext $\\left\\[\\mathbf{m}\\right]=$ $({\\hat{b}},{\\hat{a}})$ that encrypts $\\hat{m}\\,\\in\\,\\mathbb{A}_{N,t}$ , and a plain polynomial ${\\hat{c}}\\in\\mathbb{A}_{N,t}$ , the operation $\\hat{c}\\,\\mathbb{boxtimes\\,[m]}$ computes the tuple $(\\hat{b}\\cdot\\hat{c},\\hat{a}\\cdot\\hat{c})\\in\\mathbb{A}_{N,q}^{2}$ which can be decrypted to $\\boldsymbol{\\hat{c}}\\cdot\\boldsymbol{\\hat{m}}$ mod $\\mathbb{A}_{N,t}$ . \u2022 Plaintext-ciphertext scalar-polynomial Multiplication $\\left(\\otimes\\right)$ . Given an RLWE ciphertext $[\\mathbf{m}]=(\\hat{b},\\hat{a})$ that encrypts $\\hat{m}\\in\\mathbb{A}_{N,t}$ , and a scalar $c$ , the operation $c\\otimes[\\![\\mathbf{m}]\\!]$ computes the tuple (c \u00b7 b\u02c6, c \u00b7 \u02c6a) \u2208A2N,q which can be decrypted to $c\\cdot\\hat{m}$ mod $\\mathbb{A}_{N,t}$ .   \n\u2022 Right Shift. Given an RLWE ciphertext $\\mathbb{[m]}=(\\hat{b},\\hat{a})$ that encrypts $\\hat{m}\\in\\mathbb{A}_{N,t}$ , and a plain polynomial $\\hat{c}\\in\\mathbb{A}_{N,t}$ with a single $s$ -order term, the right shift operation RShift $([\\mathbf{m}],s)$ computes the tuple $(\\hat{b}\\cdot\\hat{c},\\hat{a}\\cdot\\hat{c})\\in\\mathbb{A}_{N,q}^{2}$ which can be decrypted to negacyclicly right shift $\\hat{m}$ for $s$ terms. This can be implemented through a simple rearrange of the coefficients of the ciphertext, which is a free operation. ", "page_idx": 14}, {"type": "text", "text": "B.2 Oblivious Transfer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "OT lets a sender input two messages $m_{0},m_{1}$ and a receiver input a bit $b$ , and then the receiver obtains the message $m_{b}$ . For security, the sender is unknown for $b$ and the receiver does not learn $m_{1-b}$ . We adopt OT to construct secure two-party computation (2PC) protocols of some non-linear operations such as comparison. We instantiate OT with the communication-efficient Ferret protocol [41]. ", "page_idx": 14}, {"type": "text", "text": "B.3 Sub-Protocols for Non-linear Layers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our protocol for non-linear layers calls the following functionalities in a black-box way to compute element-wise multiplication, comparison, Boolean to Arithmetic share (B2A), and wrap. These functionalities can be securely realized using the known protocols. We use $\\langle\\cdot\\rangle^{\\ell}$ to denote arithmetic additive sharings over a ring $\\mathbb{Z}_{2^{\\ell}}$ and $\\langle\\cdot\\rangle^{B}$ to denote Boolean additive sharings over a binary field $\\mathbb{F}_{2}$ . Secret sharing without superscript indicates using the default ring size (e.g., $\\mathbb{Z}_{2^{64}}$ for the linear layers and $\\mathbb{Z}_{2^{32}}$ for the exponetial and GELU functions). ", "page_idx": 14}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/6cf1e293089ebe3bb317dad39aa9a2fb7cb5544fa39e85bc372cc2b2b0d5026c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Detailed Protocols and Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Protocols ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present our detailed protocol of matrix multiplication of Section 3 in Algorithm 1. The detailed protocols of securely evaluating GELU and exponential in Section 4 are presented in Algorithm 2 and Algorithm 3. ", "page_idx": 14}, {"type": "text", "text": "Security Proof of the Matrix Multiplication Protocol The proposed client-side outer product protocol directly builds upon secure building blocks. It guarantees the same security as the traditional server-side inner product protocol. In the presence of a semi-honest adversary, we provide a brief proof idea below. The notations follow those in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "Specifically, the model weights are encrypted by the server and then sent to the client, where the ciphertexts are denoted by $\\left[\\mathbf{W}\\right]$ . According to the security property of the HE scheme, the ciphertexts reveal no information about these model weights. For secure matrix multiplication, the input matrix $\\mathbf{X}$ has been shared as $(\\langle\\mathbf{X}\\rangle_{c},\\langle\\mathbf{X}\\rangle_{s})$ using additive secret sharing. The client samples a matrix of random shares $\\mathbf{R}$ , and then homomorphically computes a ciphertext $\\begin{array}{r}{\\[\\^{-}\\!\\mathbf{X}\\rangle_{c}\\!*\\!\\mathbf{W}\\!-\\!\\mathbf{R}]\\stackrel{{}=}-\\langle\\mathbf{X}\\rangle_{c}\\!*\\![\\mathbf{W}]\\!-\\!\\mathbf{R}}\\end{array}$ . Due to the circuit-privacy property of the HE scheme, in the client view, the ciphertext $[\\langle\\mathbf{X}\\rangle_{c^{*}}\\mathbf{W}{-}\\mathbf{R}]$ does not reveal information on $\\mathbf{W}$ . This ciphertext is sent to the server, who decrypts it to $\\langle\\mathbf{X}\\rangle_{c}*\\mathbf{W}-\\mathbf{R}$ . Because of the random mask $\\mathbf{R}$ , the server also learns nothing about the client share $\\langle\\mathbf{X}\\rangle_{c}$ . In the proof of security, the simulator can simulate the HE ciphertexts using \"dummy\" ciphertexts on zero, and the adversary\u2019s view between the real-world execution and ideal-world execution is proven to be computationally indistinguishable by reducing it to the circuit-privacy security of the HE scheme. ", "page_idx": 15}, {"type": "text", "text": "Security Proof of Non-linear Function Protocols This work does not modify the protocol for evaluating piecewise polynomials but improves the generation method for these polynomials, ensuring that security remains consistent with previous work. We highlight the training data information that are utilized for ftiting non-linear functions is not leaked. This is because the secure evaluation of the piecewise polynomial prevents the client from learning the coefficients and comparison thresholds, as indicated in Algorithm 2 and Algorithm 3. Except for the usage of ${\\bf b}_{0}$ , which requires the primitive ${\\mathcal{F}}_{\\mathrm{mul}}$ , all other coefficients are used through addition, which can be performed locally on the server side. For the comparison threshold, the server can subtract the threshold from its share and compare the resulting shares with zero. As a result, the client learns nothing about the piecewise polynomial. A possible improvement on the efficiency is to make ${\\bf b}_{0}$ public, thereby saving one round of communication. The only leakage of ${\\bf b}_{0}$ does not necessarily lead to leakage of the meaningful information and can provide approximately an $8\\%$ speedup. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Secure Matrix Multiplication Protocol of Nimbus ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Parties: $C$ is the client. $S$ is the server owning the model. ", "page_idx": 15}, {"type": "text", "text": "Inp ,  aacntdi vsaeticorent  skheayr $\\bar{\\langle\\mathbf{X}\\rangle_{c}}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ . The server holds activation share $\\langle\\mathbf{X}\\rangle_{s}\\in$ $\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ $\\mathbf{W}\\in\\mathbb{Z}_{2^{\\ell}}^{m\\times n}$ $s k$ ", "page_idx": 15}, {"type": "text", "text": "Output: Sharing $\\langle\\bar{\\mathbf{Y}}\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times n}$ and $\\langle\\mathbf{Y}\\rangle_{s}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times n}$ such that $\\mathbf{Y}=W X$ mod $2^{\\ell}$ . {Setup phase} ", "page_idx": 15}, {"type": "text", "text": "1: Server $S$ partitions the matrix $\\mathbf{W}$ into rows $\\mathbf{W}_{\\beta}\\,\\in\\,\\mathbb{Z}_{\\ell}^{1\\times n}$ . Then $S$ encodes each row as a polynomial $\\hat{w}_{\\beta}=\\pi_{w}\\left(\\mathbf{W}_{\\beta}\\right)$ for $\\beta\\in[m]$ . After that $S$ sends $\\mathbb{[W_{\\beta}]}=\\mathsf{E n c}(\\hat{w}_{\\beta})$ for $\\beta\\in[m]$ to the client $C$ . ", "page_idx": 15}, {"type": "text", "text": "2: The client computes the scalar-polynomial multiplication to obtain a vector of output ciphertexts $\\mathbf{c}=[\\mathbb{[c}_{0}\\mathbb{]},\\mathbb{[c}_{1}\\mathbb{]}\\cdot\\cdot\\cdot[\\mathbf{c}_{k-1}\\mathbb{]}]$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{c}[\\alpha]=\\mathbb{E}_{\\beta\\in[m]}\\left(x_{\\alpha,\\beta}\\otimes\\mathbb{[W_{\\beta}]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\alpha\\in[k]$ . The ${\\bf c}[\\alpha]$ denotes the $\\alpha$ -th element of the vector $\\mathbf{c}$ . ", "page_idx": 15}, {"type": "text", "text": "3: To compress the the $k$ ciphertexts vector of $\\mathbf{c}$ into $k/\\lfloor N/n\\rfloor$ ciphertexts, The client applies right shift on ciphertexts of c. For example ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{c}}[\\theta]=\\mathrm{RShift}(\\mathbf{c}\\left[\\theta\\cdot\\lfloor N/n\\rfloor\\right],0)+\\mathrm{RShift}(\\mathbf{c}\\left[\\theta\\cdot\\lfloor N/n\\rfloor+1\\right],k)+\\cdot\\cdot\\cdot}\\\\ &{\\quad\\quad+\\mathrm{RShift}(\\mathbf{c}\\left[\\theta\\cdot\\lfloor N/n\\rfloor+\\lfloor N/n\\rfloor-1\\right],k*(\\lfloor N/n\\rfloor-1))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\theta\\in[k/\\lfloor N/n\\rfloor]$ . Pad with zeros if $k$ cannot be exactly divided by $\\lfloor N/n\\rfloor$ . ", "page_idx": 15}, {"type": "text", "text": "4: The client $C$ generates a random polynomial vector $\\mathbf{r}=[\\hat{r}_{0},\\hat{r}_{0},\\cdot\\cdot\\cdot\\hat{r}_{k/\\lfloor N/n\\rfloor-1}]$ to mask the ciphertext. The client sends $\\tilde{\\mathbf{c}}[\\theta]\\,\\boxed{\\neg}\\,\\mathbf{r}[\\theta]$ to the server for all $\\theta$ , which are then decrypted by server to obtain ${\\mathbf{W}}\\langle{\\mathbf{X}}\\rangle_{c}-{\\mathbf{R}}$ . The client keeps $\\mathbf{r}$ , which is $\\mathbf{R}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times n}$ . ", "page_idx": 15}, {"type": "text", "text": "5: The server locally computes $\\mathbf{W}\\langle\\mathbf{X}\\rangle_{s}$ and outputs $\\langle\\mathbf{Y}\\rangle_{s}=\\bar{\\mathbf{W}}\\langle\\mathbf{X}\\rangle_{s}+\\mathbf{W}\\langle\\mathbf{X}\\rangle_{c}-\\mathbf{R}$ . The client outputs $\\langle\\mathbf{Y}\\rangle_{c}=\\dot{\\mathbf{R}}$ . ", "page_idx": 15}, {"type": "text", "text": "Parties: $C$ is the client. $S$ is the server owning the model. The polynomial $P^{2}(x)$ with coefficients $\\{b_{0},b_{1},b_{2}\\}$ from Equation 5.   \nInput: The client holds activation share $\\langle\\mathbf{X}\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ Z2k\u2113\u00d7mand the server holds activation share \u27e8X\u27e9s \u2208 $\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ .   \nOutput: Sharing $\\langle\\mathbf{Y}\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ and $\\langle\\mathbf{Y}\\rangle_{s}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ such that $\\mathbf{Y}=G E L U(\\mathbf{X})$ .   \n1: Two parties locally compute $\\langle\\mathbf{A}_{1}\\rangle=\\mathcal{F}_{\\mathrm{mul}}\\left(\\langle\\mathbf{b}_{0}\\rangle,\\langle\\mathbf{X}\\rangle\\right)+b_{1}$ . Then two parties jointly compute $\\langle\\mathbf{A}_{2}\\rangle=\\mathcal{F}_{\\mathrm{mul}}\\left(\\langle\\mathbf{A}_{1}\\rangle,\\langle\\mathbf{X}\\rangle\\right)+\\dot{b}_{2}$ . The truncations are implicitly called. 2: Jointly compute the comparisons for interval selection ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{b}_{0}\\rangle^{B}=\\mathcal{F}_{l e s s}(\\langle\\mathbf{X}\\rangle,T_{1})\\;\\;\\;\\;\\mathsf{s}\\mathbf{b}_{0}=\\mathbf{1}\\{\\mathbf{X}<T_{1}\\}}\\\\ {\\langle\\mathbf{b}_{1}\\rangle^{B}=\\mathcal{F}_{l e s s}(T_{2},\\langle\\mathbf{X}\\rangle)\\;\\;\\;\\mathsf{s}\\mathbf{b}_{1}=\\mathbf{1}\\{T_{2}<\\mathbf{X}\\}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "${\\bf1}\\{P\\}$ is 1 when the condition $P$ is true and 0 otherwise. Two parties locally set $\\langle\\mathbf{z}_{0}\\rangle^{B}=\\langle\\mathbf{b}_{0}\\rangle^{B}$ , $\\dot{\\langle\\mathbf{z}_{1}\\rangle}^{B}=\\langle\\mathbf{b}_{0}\\rangle^{B}$ xor $\\langle\\mathbf{b}_{1}\\rangle^{B}$ xor $l$ , $\\langle\\mathbf{z}_{2}\\rangle^{B}=\\langle\\mathbf{b}_{2}\\rangle^{B}$ , where $l$ is the party index. In this way, two parties have ${\\bf z}_{0}=1\\{{\\bf X}<T_{1}\\}$ , ${\\bf z}_{1}=1\\{T_{1}\\leq{\\bf X}<T_{2}\\}$ , and $\\mathbf{z}_{2}=\\mathbf{\\bar{1}}\\{\\bar{T}_{2}\\leq\\mathbf{X}\\}$ . 3: Jointly compute the multiplexing $\\langle\\mathbf{Y}\\rangle=\\langle\\mathbf{z}_{0}\\rangle^{B}\\cdot0+\\langle\\mathbf{z}_{1}\\rangle^{B}\\cdot\\langle A_{2}\\rangle+\\langle\\mathbf{z}_{2}\\rangle^{B}\\cdot\\langle\\mathbf{X}\\rangle$ as the output share of each party. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Secure Exponential Protocol of Nimbus ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Parties: $C$ is the client. $S$ is the server owning the model. The polynomial $P^{3}(x)$ with coefficients   \nInput: The client holds activation share $\\{b_{0},b_{1},b_{2},b_{3}\\}$ from Equation 4. $\\langle\\mathbf{X}\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ and the server holds activation share $\\langle\\mathbf{X}\\rangle_{s}\\in$ $\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ .   \nOutput: Sharing $\\langle\\mathbf{Y}\\rangle_{c}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ and $\\langle\\mathbf{Y}\\rangle_{s}\\in\\mathbb{Z}_{2^{\\ell}}^{k\\times m}$ such that $\\mathbf{Y}=e x p(\\mathbf{X})$ . 1: Two parties locally compute $\\langle\\mathbf{A}_{1}\\rangle=\\mathcal{F}_{\\mathrm{mul}}\\left(\\langle\\mathbf{b}_{0}\\rangle,\\langle\\mathbf{X}\\rangle\\right)+b_{1}$ . Then two parties jointly compute $\\langle\\mathbf{A}_{2}\\rangle=\\mathcal{F}_{\\mathrm{mul}}\\,(\\langle\\mathbf{A}_{1}\\rangle,\\langle\\mathbf{X}\\rangle)\\!+\\!b_{2}$ and $\\left\\langle\\mathbf{A}_{3}\\right\\rangle=\\mathcal{F}_{\\mathrm{mul}}\\left(\\left\\langle\\mathbf{A}_{2}\\right\\rangle,\\left\\langle\\mathbf{X}\\right\\rangle\\right)\\!+\\!b_{3}$ . The truncations are implicitly called. 2: Jointly compute the comparisons for interval selection ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\mathbf{z}_{0}\\rangle^{B}={\\mathcal{F}}_{l e s s}(\\langle\\mathbf{X}\\rangle,T_{e x p})\\;\\;\\;\\triangleright\\mathbf{z}_{0}=\\mathbf{1}\\{\\mathbf{X}<T_{e x p}\\}\\;\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "${\\bf1}\\{P\\}$ is 1 when the condition $P$ is true and 0 otherwise. ", "page_idx": 16}, {"type": "text", "text": "3: Jointly compute the multiplexing $\\langle\\mathbf{Y}\\rangle=(1-\\langle z_{0}\\rangle^{B})\\cdot0+\\langle z_{0}\\rangle^{B}\\cdot\\langle\\mathbf{A}_{3}\\rangle$ as the output share of each party. ", "page_idx": 16}, {"type": "text", "text": "C.2 Fitting Algorithm for Non-linear Approximation. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the algorithm used to search the interval breakpoint of the piecewise polynomial. We use the exponential with only one breakpoint as an example to explain. A similar algorithm can be easily generated to the GELU with two breakpoints. ", "page_idx": 16}, {"type": "text", "text": "The first step generates the breakpoint candidate set $S$ given the initial breakpoint $T$ . One can choose the search range and step according to the needs (Line 1). Then, for each breakpoint candidate, the input range is separated into two intervals (Lines 3-4). We fit both intervals using Equation 3. The required input distribution $p(x)$ can be drawn from a batch of data from the training dataset. The corresponding loss is accumulated for all intervals (Lines 5-9). Then, we update the optimal piecewise approximation (lines 10-13). Finally, the optimal approximation $f^{\\prime}(x)$ is returned. ", "page_idx": 16}, {"type": "text", "text": "D Complexity Analysis of Linear-Layer Protocol of Nimbus ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section analyzes the computation and communication complexities listed in Table 1. We first analyze the number of HE ciphertexts to be communicated. The SIP protocol requires $\\frac{k m}{k_{w}m_{w}}+\\frac{k n}{k_{w}n_{w}}$ for the communication of input and output, as we have explained in Section 3.1. Our COP protocol removes the overhead of sending the input $\\frac{k m}{k_{w}m_{w}}$ . The scalar-polynomial product produces $k$ output ciphertext, which we pack as $k/\\lfloor N/n\\rfloor$ . ", "page_idx": 16}, {"type": "text", "text": "Input: Activation function $f(x)$ , initial value of the interval breakpoint $T$ , input distribution $p(x)$ , and function template of $f^{\\prime}(x)$ .   \nOutput: The approximated function $f^{\\prime}(x)$ ;   \n1: Generate breakpoint candidates set $S$ around $T$ .   \n2: Set best_ $l o s s\\gets\\infty$ and $f^{\\prime}(x)\\gets N o n e$ .   \n3: for $s\\in S$ do   \n4: Partition the input range into two intervals using $s$ .   \n5: $L_{t o t a l}=0$ .   \n6: for each interval $i$ do   \n7: Fit a polynomial of given degree using Equation 3 and obtain the corresponding loss $L_{i}$ . 8: Compute total loss ${\\cal L}_{t o t a l}+={\\cal L}_{i}$ .   \n9: end for   \n10: if $L_{t o t a l}<b e s t\\_l o s s$ then   \n11: best_ $\\mathit{l o s s}\\gets\\mathit{L}_{t o t a l}$   \n12: $f^{\\prime}(x)\\gets$ current approximation   \n13: end if   \n14: end for   \nOutput: $f^{\\prime}(x)$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 5 Secure Fused Truncation and Upcast. ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/6c41c357ab54beb97e04a2a31b23c2ae7a397d46a192059e2e0dbf392366954d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Then, we explain the computation complexity. The server in the SIP protocol needs to apply NTT to weight $O\\big(\\frac{m n}{m_{w}n_{w}}N\\log N\\big)$ and the dyadic product wkmmwnnw \u2217N = kmn. In our scheme, the server only decrypts the $k/\\lfloor N/n\\rfloor$ output ciphertexts with complexity $O\\left((k/\\lfloor N/n\\rfloor)N\\log N\\right)$ . The client of the SIP protocol needs to perform NTT when encrypting the activation and INTT when decrypting the output, which requires $\\begin{array}{r}{{\\cal O}\\left((\\frac{k m}{k_{w}m_{w}}+\\frac{k n}{k_{w}n_{w}})N l o g N\\right)}\\end{array}$ complexity. In our protocol, the client can directly multiply her activation share with the ciphertext on model weights. Our method has $O(N)$ complexity for each plaintext-ciphertext scalar-polynomial multiplication and $k m$ times product with total complexity $O(k m N)$ . ", "page_idx": 17}, {"type": "text", "text": "E Correctness of Truncation-upcast Fusion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The protocol that computes truncation and upcast is in Algorithm 5. We show the correctness of it through the following derivation. Let $\\langle x\\rangle_{i}^{\\ell^{\\star}}(i\\in0,1)$ denote the secret share held by the client and server on the ring $\\mathbb{Z}_{2^{\\ell}}$ . The second line is drawn from the truncating secret shares on the ring $2^{\\ell}$ . $w$ is a boolean value indicates the wrap of $\\langle x\\rangle_{i}^{\\ell}$ over ring size $2^{\\ell}$ and $w^{\\prime}$ is the carry bits of the lower $s$ bits. The carry bit $w^{\\prime}$ is either zero or one and can be safely ignored in the inference while the $w\\cdot2^{\\ell-s}$ is a significant error that needs to be carefully handled. The third line holds since $\\begin{array}{r}{\\sum_{i=0}^{1}\\langle x\\rangle_{i}^{\\ell}/2^{s}-w\\cdot2^{\\ell-s}+\\hat{w}}\\end{array}$ falls within the larger ring $2^{\\ell^{\\prime}}$ . The fourth line is the modulo expansion of the wrap $w$ on a ring with $k$ bits, where $v$ is a boolean value that indicates the wrap of the $\\langle w\\rangle^{k}$ . Through a proper choice of $k\\,\\geq\\,\\ell^{\\prime}-\\ell+s$ to promote the boolean share $\\langle w\\rangle^{B}$ , its wrap can be eliminate by modulo $2^{\\ell^{\\prime}}$ . The final line indicates the overhead of truncation and upcast is the same as truncation alone, which only requires computing the $\\langle w\\rangle^{k}=\\langle w\\rangle^{\\ell^{\\prime}-\\ell+s}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{x}^{\\ell^{\\prime}}=x^{\\ell}/2^{s}}\\\\ &{=\\displaystyle\\sum_{i=0}^{1}(x)_{i}^{\\ell}/2^{s}-w\\cdot2^{\\ell-s}+\\hat{w}}\\\\ &{=\\left(\\displaystyle\\sum_{i=0}^{1}\\langle x\\rangle_{i}^{\\ell}/2^{s}-w\\cdot2^{\\ell-s}+\\hat{w}\\right)\\bmod2^{\\ell^{\\prime}}}\\\\ &{=\\displaystyle\\sum_{i=0}^{1}\\langle x\\rangle_{i}^{\\ell}/2^{s}\\bmod2^{\\ell^{\\prime}}-\\left(\\displaystyle\\sum_{i=0}^{1}\\langle w\\rangle_{i}^{k}-v^{B}\\cdot2^{k}\\right)\\cdot2^{l-s}\\bmod2^{\\ell^{\\prime}}+\\hat{w}\\bmod2^{\\ell^{\\prime}}}\\\\ &{\\overset{k\\geq\\ell-\\ell+s}{=}\\displaystyle\\sum_{i=0}^{1}\\langle x\\rangle_{i}^{\\ell}/2^{s}\\bmod2^{\\ell^{\\prime}}-\\left(\\displaystyle\\sum_{i=0}^{1}\\langle w\\rangle_{i}^{k}\\right)\\cdot2^{l-s}\\bmod2^{\\ell^{\\prime}}+\\hat{w}\\bmod2^{\\ell^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "F More Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Comprehensive Performance Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We present a comprehensive end-to-end latency comparison of a transformer block in Figure 8, Figure 9 and Figure 10. We present three model sizes: 768, 1024, and 2048. The input sequence lengths include 1, 32, and 128. Two network conditions are considered: 3000Gbps, 1ms (LAN) and 400Mbps, 10ms (WAN). The sequence lengths of 32 and 128 correspond to the classification Transformer model or the prefill phase of the generative Transformer model. The sequence length 1 can be viewed as the performance during the generation phase of the generative Transformer model. Overall, for the stronger baseline BumbleBee, our method outperforms by a magnitude of $1.9\\times$ to $7.6\\times$ on sequence lengths 32 and 128. The speedup is minor on sequence 1 with $1.2\\times$ to $2.1\\times$ . For the linear layers, the speedup ranges from $2.47\\times$ to $12.09\\times$ , and the non-linear layers range from $2\\times$ to $3.9\\times$ In the following, we provide a detailed analysis of the speedup in linear and non-linear layers under varying conditions. ", "page_idx": 18}, {"type": "text", "text": "Linear Layers. Our method is efficient in both computation and communication. Therefore, we achieve apparent speedup in both LAN and WAN settings. Our method obtains more speedup for large input sequences and hidden dimensions. This is because the computation time of NTT is more dominant for large-size matrix multiplication and input sequences. When the communication speedup is similar, our method has more speedup when the computation time speedup is more significant. Our method has more speedup for the LAN than WAN, where latency is mainly composed by the computation. As our method computes much faster, our latency benefits more from the improved network condition. ", "page_idx": 18}, {"type": "text", "text": "Non-Linear Layers. Our method is $4\\times$ to $10\\times$ faster than Iron and $2.0\\times$ to $3.9\\times$ faster than the stronger baseline BumbleBee. Our speedup on LAN and WAN are similar as our method improves the communication size and the communication rounds. The speedup is also similar for varying sequence lengths and hidden sizes. This is because our optimization lies in a lower degree of approximated polynomials and is not correlated with the input size. ", "page_idx": 18}, {"type": "text", "text": "F.2 Detailed Client Burden Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section provides evidence to support Section 3.3, including the client computation time and the asynchronous weight loading. ", "page_idx": 18}, {"type": "text", "text": "Client Computation Time Comparison. The client\u2019s computation cost for our COP protocol is similar to the previous SIP protocol. This is based on a counter-intuitive workload distribution of the HE multiplication. Due to the introduction of the NTT, the server in SIP protocol only computes the dyadic product with $O(N)$ complexity, and the more expensive NTT with complexity $O(N\\log N)$ is performed by the client when encrypting and decrypting, as listed in Table 1. Our COP protocol makes the client perform a more efficient scalar-vector product with complexity $O(k m N)$ . Since directly comparing computation complexity is hard to draw a conclusion due to the choice of the window size, we profile the client\u2019s computation time to compare the computation workload. ", "page_idx": 18}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/2a9c133758803f571f0cac5e9f813aa900b5642550dc7b00773560e62c6c30dc.jpg", "img_caption": ["Figure 8: Under sequence length 1, the end-to-end speedup and breakdown of varying hidden dimensions and network conditions. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/b8a36f5a2108bc7297f13e7c25060bdc3bbd48496bef53bb8f187265c55201f7.jpg", "img_caption": ["Figure 9: Under sequence length 32, the end-to-end speedup and breakdown of varying hidden dimensions and network conditions. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/a5fdcf33f27da84c6f1d0fd9413e2e0bedde11e7c670a6bab4aad47723223118.jpg", "img_caption": ["Figure 10: Under sequence length 128, the end-to-end speedup and breakdown of varying hidden dimensions and network conditions. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/671c1bb6fd4ad9239ec1e033d8cd3f87228a365d0b690bee33b977e049aaad56.jpg", "img_caption": ["Figure 11: Under different sequence lengths and hidden sizes, we present comprehensive experiments of client computation time of different methods. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Under different sequence lengths and hidden sizes, Figure 11 presents comprehensive experiments of client computation time of different methods. Across varying model sizes and input sequence lengths, Nimbus only takes around $20\\%$ to $30\\%$ compared with Iron and $0.7\\times$ to $2.7\\times$ compared with BumbleBee. The BumbleBee has less client computation because the compression used in BumbleBee allows the client to encrypt her activation shares to less number of ciphertexts and receive fewer output ciphertexts, thereby requiring fewer NTT operations. In the worst case of the Linear $\\mathrm{~\\,~\\,~}^{\\cdot}h2$ , the client computation is around $5.0\\times$ longer than the BumbleBee. But still, on the total Transformer block, the extra overhead is only $2.7\\times$ . This extra computation ratio can be further shrunk given the base that the client needs to perform a large amount of computation of non-linear layers. Therefore, we believe this is acceptable for a powerful client in MPC. ", "page_idx": 20}, {"type": "text", "text": "Asynchronous Weights Loading. For varying hidden dimensions, Table 4 lists the encrypted weight size and corresponding loading time from disk to memory, which usually takes less than 1 second. For the hidden size 768, which is the size of $\\mathrm{BERT}_{\\mathrm{base}}$ mainly considered in this paper. The loading time is only $90~\\mathrm{{ms}}$ and $370~\\mathrm{ms}$ , which can be easily overlapped by later communication. Therefore, we can only keep a limited number of encrypted weights in the memory, such as weights of four linear layers of a Transformer block. Then, we swap the later weights during the execution. ", "page_idx": 20}, {"type": "text", "text": "Table 4: The size of the encrypted weights measured by megabytes (MB) and the corresponding loading time measured by seconds (s). ", "page_idx": 21}, {"type": "image", "img_path": "G7QS68ICPJ/tmp/fb559292437ee7cb002e433399276decef77e0a72177e03a48c285a4ef74fe0d.jpg", "img_caption": ["Figure 12: The execution $^+$ setup speedup over BumbleBee under different queries. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.3 Amortized Overhead of the encrypted weights. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our linear protocol replaces the input communication with a one-time setup communication of sending encrypted weights. Although our method focuses on the overhead of the execution phase, we are also interested in how many queries can make the amortized setup overhead negligible. For $\\mathrm{BERT}_{\\mathrm{base}}$ , using our row-wise encoding, the encrypted weight size of four types of layer within a Transformer block is 180MB, 180MB, 180MB, and 720MB. We incorporate the one-time overhead of transmitting these weights into our analysis and amortize it across the number of queries. Figure 12 shows the speedup of amortized Nimbus over BumbleBee. The dot lines indicate the speedup without considering the setup overhead. There are two critical points that we are interested in. At around three queries, the amortized overhead begins to surpass the BumbleBee, leading to a speedup greater than 1. At around 40 queries, the maximal speedup is achieved. ", "page_idx": 21}, {"type": "text", "text": "F.4 More Accuracy and Efficiency Comparisons of Non-linear Approximations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Besides BumbleBee [26], this section compares non-linear approximation with other state-of-the-art (SOTA) works, including MPCFormer [22] and BOLT [29]. ", "page_idx": 21}, {"type": "text", "text": "Accuracy Comparison The details of the accuracy are listed in Table 5. The accuracy of MPCFormer is reproduced using the open-sourced code of them. MPCFormer\u2019s idea is to adopt aggressive approximation to tailor the efficiency of the MPC but sacrifice the inference accuracy. For example, they let $G E L U(x)=0.125x^{2}+0.{\\dot{2}}5x+0.5$ . MPCFormer can efficiently compute the Softmax and GELU. However, even using knowledge distillation to recover the accuracy, they still incur $2.26\\%$ accuracy on average. BOLT optimizes $\\mathsf{e x}$ ponential and GELU using integer-only approximation and piecewise polynomial approximation, respectively. Similar to the BumbleBee, their approximation regards all input values as equal importance. Therefore, they require a relatively high-degree polynomial to approximate the original function. Since BOLT does not opensource the codes used to fine-tune the model, we use accuracy numbers reported in their paper. Their approximations are relatively accurate and incur $0.44\\%$ average accuracy loss after fine-tuning. ", "page_idx": 21}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/09a9036f70e1375489f405854e8fbdc403e73784bf6885e78934d2cb6cb44c9a.jpg", "table_caption": ["Table 5: Accuracy comparison of floating-point (FP) baseline, BumbleBee, MPCFormer (reproduced using Quad $+$ 2ReLU), $\\mathsf{B O L T^{*}}$ (accuracy relative change from the original paper), Nimbus (with finetuning). We report the relative change in accuracy. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "G7QS68ICPJ/tmp/0ae83871c92bf0f551d8c9739a5a1e280bf40343482cdd166263233710e5c247.jpg", "table_caption": ["Table 6: Efficiency comparison of BumbleBee, MPCFormer (Quad $+2.$ ReLU), BOLT, Nimbus "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Efficiency Comparison The efficiency comparisons for LAN and WAN settings are listed in Table 6. We utilize the open-sourced code of BOLT to reproduce their latency. MPCFormer is originally implemented on the Crypten framework [20], which uses secret sharing as its underlying cryptographic primitive instead of homomorphic encryption. To facilitate a fair comparison of the effectiveness of non-linear approximation, we re-implement their polynomial approximation using SecretFlow as the backend. ", "page_idx": 22}, {"type": "text", "text": "MPCFormer\u2019s GELU does not require comparison and is extremely fast. They use ReLU to replace the exponential in the Softmax, which is also fast. The same as the ReLU substitution of MPCFormer, Nimbus\u2019s approximation of the exponential also has one comparison, and the additional three multiplications are relatively quick to compute. Furthermore, our approach allows for computation on a smaller ring, which makes us faster than MPCFormer\u2019s Softmax. Our method is approximately $10\\times$ faster than BOLT in all cases. In addition to the advantages of our simpler approximation, other factors also contribute to the slowness of BOLT. First, BOLT\u2019s linear layer is evaluated in the field, necessitating extra operations to convert field elements to ring elements. Second, our simpler approximation reduces the fixed-point error during computation, enabling computation on a smaller ring. Since other layers still require computations on a larger ring, we also propose a truncation-upcast fusion protocol to eliminate the overhead of upcasting ring elements from a smaller ring to a larger one. In contrast, BOLT requires additional communication for this upcasting process. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstraction and introduction reflect necessary contributions and experiments ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The potential trade-off is discussed within the paper in a separate Section 3.3. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The necessary assumption and proof are include in the Appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have listed detail configuration of experiments in Section 5. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code to reproduce the experimental results are provided on Github. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is not the core contribution of this work. But still, we follow the standard method of prior work as we have mentioned in the Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The experiments are conducted many times and report average results. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The specific configurations are included in the Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the potential positive societal impacts of using privacy-preserving inference in the Introduction section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The answer NA means that the paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]