[{"figure_path": "G7QS68ICPJ/figures/figures_3_1.jpg", "caption": "Figure 2: Two rows represent the client and server operations, respectively. The inefficient parts that are accelerated are marked by dashed boundaries. The input communication is shifted as a one-time setup, and the output ciphertexts are compact. The expensive NTT/INTT operations at the online stage are also reduced.", "description": "This figure compares two different protocols for secure matrix multiplication in the context of two-party computation: the Server-side Inner Product (SIP) protocol and the Client-side Outer Product (COP) protocol.  The SIP protocol involves multiple rounds of communication and computationally expensive operations like Number Theoretic Transform (NTT) and Inverse NTT (INTT) in both the online and setup phases. The COP protocol, by contrast, leverages the static nature of model weights to shift the computationally expensive steps to the setup phase and reduce communication overhead during the online phase.  This results in improved efficiency and latency for secure linear layer computations in transformer models.", "section": "3 Secure Computation of Linear Layers"}, {"figure_path": "G7QS68ICPJ/figures/figures_3_2.jpg", "caption": "Figure 1: An example of the window encoding of the matrix multiplication using N = 16 and l = 5.", "description": "This figure illustrates the concept of window encoding for matrix multiplication within the context of secure two-party computation using polynomials.  The left side shows a simple matrix multiplication example. The right side demonstrates how this multiplication is represented using polynomials in the ring A<sub>16,2<sup>5</sup></sub>. The polynomial representation efficiently encodes the matrix multiplication but results in sparse polynomials with many zero coefficients.  This sparsity is a key aspect that the paper aims to improve upon.", "section": "2.4 Secure Two-Party Transformer Inference"}, {"figure_path": "G7QS68ICPJ/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of our matrix multiplication. Left: Functionality of the matrix multiplication using row-wise encoding. Middle: Computing the first row of the output through the scalar-poly product. Right: Packing two ciphertexts using a right shift for less number of output ciphertext.", "description": "This figure illustrates the client-side outer product (COP) protocol for secure matrix multiplication.  The left panel shows how the row-wise encoding works. The middle panel focuses on the computation of the first row of the output matrix, highlighting the use of scalar-polynomial multiplication. The right panel demonstrates how the output ciphertexts are packed using a right shift to reduce communication overhead.", "section": "3.2 Client-side Outer Product Protocol"}, {"figure_path": "G7QS68ICPJ/figures/figures_6_1.jpg", "caption": "Figure 4: The input distribution of non-linear functions. The y-axis indicates the occurrence counts.", "description": "This figure shows the distribution of input values for GELU and Softmax functions in the 4th encoder of the BERTbase model.  The x-axis represents the input values, and the y-axis represents the number of times each input value occurred.  The GELU distribution is roughly bell-shaped, while the Softmax distribution is heavily skewed towards lower values with a sharp increase near zero. This observation of non-uniform input distribution is exploited in the paper to improve the efficiency of non-linear layer approximations.", "section": "4 Secure Computation of Non-Linear Functions"}, {"figure_path": "G7QS68ICPJ/figures/figures_8_1.jpg", "caption": "Figure 5: The L2-Norm of output error between oracle non-linear functions and approximations.", "description": "This figure shows the L2-norm of the output error between the original non-linear functions (exponential and GELU) and their polynomial approximations for different fixed-point precisions.  The results are shown separately for two different ring sizes (Z232 and Z264) and for both Nimbus and BumbleBee methods. This visualization helps to understand the trade-off between accuracy and efficiency when choosing the fixed-point precision and ring size for secure computation of non-linear functions in transformer models.", "section": "5.2 Efficiency Comparison"}, {"figure_path": "G7QS68ICPJ/figures/figures_8_2.jpg", "caption": "Figure 6: The end-to-end latency of a Transformer block of BERTbase and its breakdown", "description": "This figure shows a performance comparison of different secure two-party inference frameworks (Iron, BumbleBee, and Nimbus) on a Transformer block of the BERTbase model.  It breaks down the total end-to-end latency into the time spent on different layers (Linear_qkv, Linear_o, Linear_h1, Linear_h2, Softmax, GELU, QKT&PV, and LayerNorm), under two different network settings (3Gbps, 1ms and 400Mbps, 10ms). The bar chart visually represents the latency for each component, allowing for a direct comparison of the efficiency of each framework across all layers and network conditions.", "section": "5 Performance Evaluation"}, {"figure_path": "G7QS68ICPJ/figures/figures_13_1.jpg", "caption": "Figure 7: The illustration of the Transformer-based model and the latency breakdown of its private evaluation.", "description": "This figure illustrates the architecture of a Transformer-based model and provides a breakdown of the latency for each component during private inference.  The model consists of an embedding layer, an encoder (with multiple repeated blocks), and a task head. Each encoder block includes a multi-head self-attention mechanism and a feed-forward network (FFN). The self-attention module uses three linear layers (Linear_qkv) to compute query, key, and value tensors, followed by a softmax function, a matrix multiplication of query and key (QKT),  and another linear layer (Linear_o). The FFN module consists of two linear layers (Linear_h1 and Linear_h2), with GELU activation between them. Both the self-attention and FFN modules include layer normalization (LayerNorm). The latency is broken down into separate components of the various steps during the inference process, which is useful for performance analysis and optimization.", "section": "A Background of Transformer Models"}, {"figure_path": "G7QS68ICPJ/figures/figures_19_1.jpg", "caption": "Figure 6: The end-to-end latency of a Transformer block of BERTbase and its breakdown.", "description": "This figure shows a detailed breakdown of the end-to-end latency for a single Transformer block in the BERTbase model.  It compares the performance of three different secure two-party computation (2PC) protocols: Iron, BumbleBee, and Nimbus. The breakdown shows the latency contribution of each layer (Linearqkv, Linear, Linearn, Softmax, GELU, QKT&PV, LN) and the total latency. Two network settings are considered: a LAN (3Gbps, 1ms) and a WAN (400Mbps, 10ms) environment. The figure highlights the significant performance improvements achieved by Nimbus compared to the other two protocols, especially in the linear layers.", "section": "5 Performance Evaluation"}, {"figure_path": "G7QS68ICPJ/figures/figures_19_2.jpg", "caption": "Figure 6: The end-to-end latency of a Transformer block of BERTbase and its breakdown.", "description": "This figure displays a breakdown of the end-to-end latency for a single Transformer block within the BERTbase model.  The breakdown shows the time spent on different components, including linear layers (Linearqkv, Linearo, Linearh1, Linearh2), non-linear layers (Softmax, GELU), attention and layer normalization (QKT&PV, LN), and the total time. The comparison is done under different network settings (3Gbps, 1ms and 400Mbps, 10ms).  The results show the relative performance of three different methods (Iron, BumbleBee, and Nimbus).", "section": "5 Performance Evaluation"}, {"figure_path": "G7QS68ICPJ/figures/figures_20_1.jpg", "caption": "Figure 6: The end-to-end latency of a Transformer block of BERTbase and its breakdown.", "description": "This figure shows the end-to-end latency breakdown for a single Transformer block within the BERTbase model under different network conditions (LAN and WAN).  It compares the performance of three different secure two-party inference methods: Iron, BumbleBee, and Nimbus. The breakdown shows the time spent in different parts of the Transformer block, such as linear layers, non-linear layers (Softmax and GELU), attention mechanisms (QKT&PV), and layer normalization (LN).  The results demonstrate the significant performance improvements achieved by the Nimbus framework.", "section": "5 Performance Evaluation"}, {"figure_path": "G7QS68ICPJ/figures/figures_20_2.jpg", "caption": "Figure 11: Under different sequence lengths and hidden sizes, we present comprehensive experiments of client computation time of different methods.", "description": "This figure compares the client computation time of Iron, BumbleBee, and Nimbus across various model sizes (hidden dimensions of 768, 1024, and 2048) and input sequence lengths (32 and 128).  The results show that Nimbus significantly reduces the client-side computation time compared to Iron and BumbleBee, demonstrating its efficiency improvement.", "section": "F.2 Detailed Client Burden Analysis"}, {"figure_path": "G7QS68ICPJ/figures/figures_21_1.jpg", "caption": "Figure 12: The execution+setup speedup over BumbleBee under different queries.", "description": "This figure shows the speedup achieved by Nimbus over BumbleBee when amortized setup time is considered. The x-axis represents the number of queries, and the y-axis represents the speedup.  Two lines are plotted, one for LAN and one for WAN network settings.  The figure demonstrates how, as the number of queries increases, the amortized overhead of Nimbus\u2019s one-time setup communication becomes less significant, leading to a greater speedup compared to BumbleBee, especially in LAN environment.  The point where the speedup crosses above 1 indicates when the amortized setup cost is outweighed by Nimbus's computational efficiency.", "section": "F.3 Amortized Overhead of the encrypted weights"}]