{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the models discussed in the current paper."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "This paper introduced BERT, a popular Transformer-based model used extensively for benchmarking and evaluation in the field."}, {"fullname_first_author": "Zhicong Huang", "paper_title": "Cheetah: Lean and fast secure two-party deep neural network inference", "publication_date": "2022-10-01", "reason": "This paper significantly advanced the state-of-the-art in secure two-party computation for deep neural networks, setting a baseline for comparison in the current work."}, {"fullname_first_author": "Meng Hao", "paper_title": "Iron: Private inference on transformers", "publication_date": "2022-12-01", "reason": "This paper presented a direct predecessor to the current work, and many of its ideas and techniques are discussed and compared to in detail."}, {"fullname_first_author": "Wen-jie Lu", "paper_title": "Bumblebee: Secure two-party inference framework for large transformers", "publication_date": "2023-05-01", "reason": "This paper presents another very recent state-of-the-art work directly compared to the current work, offering the latest techniques and performance benchmarks for secure two-party Transformer inference."}]}