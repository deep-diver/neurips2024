{"importance": "This paper is crucial for researchers in interpretable machine learning and related fields because it introduces a novel method for making black-box models more intervenable, addresses the limitations of existing concept bottleneck models, and provides a formal measure of intervenability.  This opens new avenues for research, particularly in high-stakes decision-making applications where human-model interaction is critical, such as healthcare. The findings challenge existing assumptions about the interpretability of black-box models and suggest new directions for improving their explainability and usability.", "summary": "This paper presents a novel method to make black box neural networks intervenable using only a small validation set with concept labels, improving the effectiveness of concept-based interventions.", "takeaways": ["A novel method for performing concept-based interventions on pre-trained black-box neural networks is introduced.", "Intervenability is formalised as a measure for intervention effectiveness and leveraged for fine-tuning black boxes.", "The proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions, even surpassing concept bottleneck models in some cases."], "tldr": "Many real-world applications need **interpretable models** that allow human users to intervene and influence the prediction process. However, achieving this with complex, **black-box models** poses significant challenges. Existing methods, such as concept bottleneck models, often require extensive data annotation or limit intervention capabilities. This paper tackles these challenges by proposing a novel approach for making black-box models more intervenable.  The core problem is that black box models are not designed for interpretability, and methods to make them interpretable often require significant extra labeled data, or only work in limited contexts. \nThe authors introduce a simple procedure for performing instance-specific interventions directly on pre-trained black-box models using a small validation set with concept labels. The method involves training a 'probe' to map the model's internal representations to concept values.  Then, the method edits these representations to align with the desired concept values, resulting in an updated prediction.  The effectiveness of this approach is evaluated using a new formal measure of intervenability. Importantly, the paper shows how fine-tuning the black-box model, using intervenability as a loss, improves intervention effectiveness and often yields better-calibrated predictions. This is empirically demonstrated on various benchmarks, including chest X-ray classification, showcasing the practical utility of the proposed approach. The methods prove effective even when using vision-language-model based concept annotations instead of human-annotated ones.", "affiliation": "ETH Zurich", "categories": {"main_category": "AI Theory", "sub_category": "Interpretability"}, "podcast_path": "KYHma7hzjr/podcast.wav"}