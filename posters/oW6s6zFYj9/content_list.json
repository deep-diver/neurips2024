[{"type": "text", "text": "Stepwise Weighted Spike Coding for Deep Spiking Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological   \n2 neurons and are expected to play a key role in the advancement of neural computing   \n3 and artificial intelligence. The efficiency of SNNs is often determined by the   \n4 neural coding schemes. Existing coding schemes either cause huge delays and   \n5 energy consumption or necessitate intricate neuron models and training techniques.   \n6 To address these issues, we propose a novel Stepwise Weighted Spike (SWS)   \n7 coding scheme to enhance the encoding of information in spikes. This approach   \n8 compresses the spikes by weighting the significance of the spike in each step of   \n9 neural computation, achieving high performance and low energy consumption. A   \n10 Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for   \n11 supporting SWS-based computing, aimed at minimizing the residual error resulting   \n12 from stepwise weighting in neural computation. Our experimental results show   \n13 that the SWS coding scheme outperforms the existing neural coding schemes in   \n14 very deep SNNs, and significantly reduces operations and latency. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Spiking Neural Networks (SNNs) are known as the third generation of neural network models   \n17 inspired by the biological structures and functions in the brain [32]. Unlike traditional Artificial   \n18 Neural Networks (ANNs) that use continuous activation functions, SNNs incorporate discrete spiking   \n19 events, enabling them to capture temporal dynamics and process information in a manner that closely   \n20 mimics the brain\u2019s functioning [31]. This event-driven paradigm aligns with the brain\u2019s energy  \n21 efficient computation and has the potential for more efficient and lower-power computing systems.   \n22 [33].   \n23 Various coding schemes have been proposed to describe neural activities, including rate coding and   \n24 temporal coding [9]. Rate coding counts the number of spikes fired within a broad time window   \n25 [23, 3, 18, 6], which effectively mitigates the impact of short-term interference on the signal. It was   \n26 widely accepted in the early days and typically outperformed temporal coding [11, 34, 4, 29, 20].   \n27 However, the rate coding scheme disregards the information in the temporal domain of the input   \n28 spike sequence and requires many pulses to represent the input signal value, making it an inefficient   \n29 coding method that negates the low-power benefits of SNN. Due to the functional similarity to the   \n30 biological neural network, spiking neural networks can embrace the sparsity found in biology and   \n31 are highly compatible with temporal coding [31, 33, 27, 28, 21, 15]. Temporal coding relies on   \n32 the specific timing or patterns of input spikes, allowing for greater information capacity in a single   \n33 pulse. However, it requires a large number of time steps to provide fine-grained timing, which   \n34 increases inference latency. Its sensitivity to variations in spike timing also makes it more vulnerable   \n35 to temporal jitter or delays [25, 24]. Additionally, decoding temporal-coded information usually   \n36 requires more complex neuron models [30, 36] and training methodologies [17, 26].   \n37 In the study of the temporal information dynamics of spikes, Kim et al. [16] discovered a phenomenon   \n38 of temporal information concentration in SNNs. It is found that after training, information becomes   \n39 highly concentrated in the first few timesteps. Based on this observation, we hypothesize that, from   \n40 the perspective of the postsynaptic neuron, the first arriving spikes contain more information and   \n41 require stronger responses. Consequently, we propose a mechanism whereby the neuron augments   \n42 its own membrane potential with a specific coefficient prior to processing the subsequent input.   \n43 This enhancement serves to increase the importance of preceding pulses on neurons, which is why   \n44 the spikes are designated as Stepwise Weighted Spikes (SWS). Nevertheless, the amplification of   \n45 the membrane potential makes it difficult for neurons to reduce its value through traditional \"soft   \n46 reset\" (i.e. subtracted by an amount equal to the firing threshold), which can result in residual errors   \n47 after neuron firing. To address this issue, we make the membrane potential reduced by a magnitude   \n48 exceeding the threshold after firing. As a result, the membrane potential has both positive and negative   \n49 residual values, which will generate both positive and negative spikes. This neuron is designated as a   \n50 Ternary Self-Amplifying (TSA) neuron. To further reduce the error caused by the weighting process,   \n51 a silent period is incorporated into the TSA neuron, allowing it to receive more input information   \n52 before firing. We perform the classification tasks with SWS-based SNN on MNIST, CIFAR10, and   \n53 ImageNet. The results show that the SWS coding scheme can achieve better performance with much   \n54 fewer coding and computing steps. Even in very deep SNN, SWS coding scheme still performs well   \n55 and achieves similar accuracy to the ANN with the same structure. Our major contributions to this   \n56 paper can be summarized as follows:   \n57 \u2022 We propose the SWS coding scheme, which enables easy implementation of SNNs with   \n58 low energy consumption and high accuracy. The stepwise weighting process enhances   \n59 the information-carrying capacity of the preceding pulses, greatly reducing the number   \n60 of coding spikes. Negative pulses are introduced in SWS coding to ensure an accurate   \n61 information transmission.   \n62 \u2022 A novel TSA neuron model is proposed. TSA neuron progressively weights the input by   \n63 augmenting its residual membrane potential before receiving the subsequent spike. The   \n64 introduction of negative residual membrane potential and negative thresholds enhances the   \n65 accuracy of the model\u2019s output.   \n66 \u2022 A silent period is added to TSA neuron to markedly improve accuracy at minimal latency   \n67 cost. By adjusting the silent period step and coding step, SWS-based SNNs can exhibit   \n68 performance advantages in different aspects, improving the flexibility of applications. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 SNNs use spike sequences to convey information, making the encoding of real data into pulses a   \n71 crucial step. Currently, the mainstream schemes of neural coding are rate coding and temporal coding   \n72 [9, 33, 32]. Rate coding represents different activities with the number of spikes emitted within   \n73 a specific time window. Due to its simplicity, rate coding is commonly used in deep learning of   \n74 SNNs. However, it distributes information uniformly across a large number of spikes, resulting in an   \n75 inefficient transmission process that increases network latency and energy consumption. Numerous   \n76 researchers have proposed solutions to optimize inference latency in rate coding. Han et al.[11]   \n77 proposed a \"soft reset\" spiking neuron model that retains a residual membrane potential after firing   \n78 to better mimic the ReLU functionality. They demonstrated near lossless ANN-SNN conversion by   \n79 using 2-8 times fewer inference time steps. Still, a delay of thousands of steps is required in large   \n80 datasets or deep networks. In [14], Hu et al. reduced the encode time steps by converting a quantized   \n81 low-precision ANN to a rate-coded SNN. They also proposed a layer-wise fine-tuning mechanism   \n82 to minimize the inference latency. However, their neuron model and the subsequent fine-tuning   \n83 algorithm are relatively complex. Furthermore, in deeper neural networks such as ResNet56, a $1.5\\%$   \n84 drop in accuracy can be observed. The above rate encoding solutions are limited because they do not   \n85 consider the significance of each spike.   \n86 In [15], Kim et al. proposed phase coding, which assigns different weights to spikes based on their   \n87 time phase. However, the transmission amount of information is bounded by the global phase, which   \n88 causes inefficiency in hidden layers, resulting in a latency of up to three thousand steps for a 32-layer   \n89 network. Burst coding [21] attempts to overcome this issue by introducing burst spikes, which   \n90 utilize Inter-Spike Interval (ISI). Burst spikes are capable of conveying more information quickly and   \n91 accurately by inducing Post-Synaptic Potential (PSP) dramatically. Nevertheless, it is still deficient   \n92 in terms of latency and efficiency. Rueckauer and Liu [27] proposed an efficient temporal encoding   \n93 scheme where the analog activation values of the ANN neurons are represented by the inverse Time  \n94 To-First-Spike (TTFS) in the SNN neurons. Their new spiking network model generates 7-10 times   \n95 fewer pulses by utilizing temporal information carried by a single spike. However, as pointed out   \n96 in [10], TTFS coding scheme incurs expensive memory access and computational overhead, which   \n97 diminishes the benefti of reduced pulse count. Furthermore, TTFS necessitates a large number of time   \n98 steps to differentiate between various time points, which also increases network latency. Han and   \n99 Roy [10] proposed the Temporal-Switch-Coding (TSC) scheme, in which each input image pixel is   \n100 represented by two spikes, and its intensity is proportional to the timing between the two pulses. Their   \n101 results showed a reduction in energy expenditure. However, TSC coding requires a large number of   \n102 time steps to provide distinguishable time intervals, rendering it an ineffective approach to addressing   \n103 the issue of the long latency.   \n104 Overall, rate coding employs a large number of pulses to encode information, which results in a   \n105 considerable energy overhead and inference delays. On the other hand, temporal coding allows for   \n106 greater information capacity in a single spike, but this does not reduce the computing latency as a   \n107 precise time point or period can be identified only with a sufficient number of time steps. Therefore,   \n108 new neural coding schemes should be developed. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "oW6s6zFYj9/tmp/2a2b992c557d7056156ff23927efbb428acaf6e587654dbb5f54720905c4bb3e.jpg", "table_caption": ["Table 1: Common symbols and their meanings in this paper. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "109 3 Stepwise weighted spike coding scheme ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110 3.1 Stepwise weighting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 The spike train $S_{i}^{l}(t)$ of the $i^{t h}$ neuron in the $l^{t h}$ layer can be expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{i}^{l}(t)=\\sum_{t_{i}^{l,(f)}\\in F_{i}^{l}}\\theta^{l}\\delta(t-t_{i}^{l,(f)})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 where $\\delta(t)$ is the Dirac delta function, \u03b8lis the spike amplitude of the lthlayer, which is usually set   \n113 to the same value as the firing threshold. $f$ is the index of the spike in the sequence, and $F_{i}^{l}$ denotes a   \n114 set of spike times which satisfies the firing condition: ", "page_idx": 2}, {"type": "equation", "text": "$$\nt_{i}^{l,(f)}:u_{i}^{l}(t_{i}^{l,(f)})\\geq V_{t h}^{l}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "115 where $u_{i}^{l}(t)$ denotes the membrane potential and $V_{t h}^{l}$ denotes the firing threshold of the neurons in   \n116 the $l^{t h}$ layer.   \n117 Our basic idea is to amplify the membrane potential before the receipt of the subsequent input, which   \n118 amplifies and prolongs the impact of the preceding input spikes on membrane potential, emulating the   \n119 phenomenon of information concentration identified in [16]. For clarity, the meanings of important   \n120 symbols are provided in table 1. The action of a neuron in SWS-SNN can be described as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nu_{j}^{l}(t)=\\beta u_{j}^{l}(t-1)+z_{j}^{l}(t)-S_{j}^{l}(t)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $\\beta$ is the amplification factor which should be greater than one, $z_{j}^{l}(t)$ denotes the PSP (i.e.   \n122 integrated inputs): ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{j}^{l}(t)=\\sum_{i}\\omega_{i j}^{l}S_{i}^{l-1}(t)+b_{j}^{l}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where $\\omega_{i j}$ is the synaptic weight and $b_{j}^{l}$ is the bias. Begin with the initial value $u_{j}^{l}(0)\\,=\\,0$ and   \n124 iteratively apply eq. (3) for each subsequent value until $u_{j}^{l}(n)$ and substitute eq. (1) and eq. (4) into it, ", "page_idx": 2}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/e1c4fc9b0e1d835970d503798891401117f18287eca72bbbb857cca3ed6b76ff.jpg", "img_caption": ["Figure 1: (a) Illustration of the stepwise weighting process. The meanings of the symbol $z_{j}^{l}(t),u_{j}^{l}(t)$ and $S_{j}^{l}(t)$ can be found in table 1. The blue dotted line represents the membrane potential prior to the spike firing, and the black exponential function-like dotted line is employed to illustrate the trend of membrane potential amplification. (b) A $V_{t h}^{l}$ equal to $\\theta^{l}$ results in residual errors, leaving a lot of information unencoded. (c) $V_{t h}^{l}$ is set to ${\\textstyle\\frac{1}{2}}\\theta^{l}$ , which increases the possibility to fire spikes early to better limit the residual. (d) Use negative spikes to correct the excessively emitted information. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "125 eq. (3) can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nu_{j}^{l}(n)=\\beta^{n}u_{j}^{l}(0)+\\sum_{\\tau=1}^{n}\\beta^{n-\\tau}z_{j}^{l}(\\tau)=\\sum_{t_{i}^{l-1,(f)}}\\sum_{i}\\sum_{\\tau=1}^{n}\\beta^{n-\\tau}\\omega_{i j}^{l}\\theta^{l-1}\\delta(\\tau-t_{i}^{l-1,(f)})+\\beta^{n-\\tau}b_{j}^{l}\\left(\\tau+t_{i}^{l-1}\\right)\\delta(\\tau-t_{i}^{l}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 Note that $S_{j}^{l}(t)$ is set to zero for simplicity. From eq. (5), it can be seen that the stepwise augment of   \n127 the membrane potential results in the spike input at time $t_{i}^{l-1,(f)}$ )encoding the value \u03b8l\u22121\u03b2n\u2212tli\u22121,(f).   \n128 This process is thus referred to as stepwise weighting, and $\\beta^{n-t_{i}^{l-1,(f)}}$ serves as the weight. The   \n129 earlier the input pulse, the greater its ability to carry information. This solves the problem of excessive   \n130 encoding steps in previous schemes, allowing faster information transmission. ", "page_idx": 3}, {"type": "text", "text": "131 3.2 Residual error ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "132 Stepwise weighting effectively assigns more weight to earlier arriving pulses, but it also makes spike   \n133 generation more tricky. To ensure that input information is efficiently encoded and transmitted to   \n134 the next layer, the residual membrane potential should be minimized after neural computation is   \n135 completed. The stepwise weighting, however, amplifies the residual potential from the previous   \n136 time step. If $z_{j}^{l}(t)$ remains high in subsequent steps, reducing the membrane potential becomes   \n137 challenging, as shown in fig. 1(b). This vicious cycle ultimately leads to a persistently high membrane   \n138 potential, indicating that a substantial amount of information remains unencoded.   \n139 We refer to this phenomenon as residual error. One contributing factor is that the threshold is set   \n140 too high, resulting in a pulse being emitted only when the membrane potential exceeds the value $\\theta^{l}$ .   \n141 While this prevents excessive information transmission, it results in missed opportunities to bring   \n142 down $u_{j}^{l}(t)$ by firing a spike.   \n143 To address this issue, we propose setting the firing threshold $V_{t h}^{l}$ to ${\\frac{1}{2}}\\theta^{l}$ . This adjustment facilitates   \n144 pulse generation and reduces the residual membrane potential. After the neuron firing, the membrane   \n145 potential is subtracted by $\\theta^{l}$ , which leads to the emergence of a negative residual that will be stepwise   \n146 weighted over time. The coefficient $1/2$ is selected as it is capable of controlling both positive and   \n147 negative residuals within a narrow and balanced range. A negative threshold $-V_{t h}^{\\overline{{l}}}$ is introduced into   \n148 the neuron model, which initiates a negative spike when the membrane potential falls below this   \n149 threshold. This mechanism allows the excessively emitted information to be corrected by the negative   \n150 spike, as shown in fig. 1(d). Given the above characteristics, we designate this neuron model as a   \n151 TSA neuron. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/0e906415671535ad1566ac32c12bb50ed849661ea7c656649319e00e4530532f.jpg", "img_caption": ["Figure 2: (a) Uncertainty in the input distribution leads to residual errors. (b) The silent period allows more information to be known when firing pulses. $T_{s}$ is set to 1 here. $V_{t h}^{l}$ is amplified by $\\beta^{T_{s}}$ , and the original threshold is represented by a gray solid line. The orange dashed line represents the amount of membrane potential reduction after firing. (c) The silent period also avoids some unnecessary spikes and increases sparsity. Without the silent period, since $\\bar{u_{j}^{l}}(1)$ exceeds the original threshold, a pulse will be generated at $t=1$ , which will later be corrected by another negative spike. (d) The impact of the silent period on network latency. The output spike sequences corresponding to different inputs are drawn in blocks of different colors. The pulses drawn in the spike sequence are for illustrative purposes only. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "152 3.3 Silent period ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "153 Another contributing factor to residual error is the imbalanced distribution of $z_{j}^{l}(t)$ . A burst input of   \n154 $z_{j}^{l}(t)$ at time point $\\tau$ results in a sharp rise in membrane potential, making it difficult for subsequent   \n155 spikes to reduce it, as shown in fig. 2(a).   \n156 This can be addressed by incorporating a silent period $T_{s}$ into the TSA neuron model. The neurons   \n157 only integrates input and performs stepwise weighting, but are not allowed to fire in the first $T_{s}$   \n158 steps. This enables the acquisition of more known information before spike generation, resulting   \n159 in increased accuracy, as illustrated in fig. 2(b). Since the preceding input information has been   \n160 amplified by $\\beta^{T_{s}}$ after the silent period, $\\bar{V}_{t h}^{l}$ also needs to be adjusted accordingly, which is set to   \n161 $\\frac{\\beta^{T s}}{2}\\theta^{l}$ . Similarly, after firing, the membrane potential should be subtracted by $\\theta^{l}\\beta^{T_{s}}$ . Note that the   \n162 fired spike amplitude remains unchanged, that is, $\\theta^{l}$ .   \n163 The impact of the silent period on network latency is shown in fig. 2(d). The output results for   \n164 different input sequences are distinguished by blocks of different colors. It can be observed that   \n165 as network depth increases, the silent period accumulates, leading to a higher output latency. The   \n166 inference latency of SWS-SNN can be calculated as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{i n f}=T_{c}+T_{s}\\cdot L_{T S A}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 where $T_{i n f}$ is the inference delay, $T_{c}$ is the coding time steps, $T_{s}$ is the length of the silent period and   \n168 $L_{T S A}$ is the number of TSA neuron layers. The neuron model in other coding schemes yields a zero   \n169 $T_{s}$ , leading to an output delay equal to the coding time step, which is consistent with the definition in   \n170 the previous scheme. From fig. 2(d), it can be seen that different input sequences are processed in a   \n171 pipeline-like manner, and the value of $T_{c}+T_{s}$ determines the throughput rate of SWS-SNN. ", "page_idx": 5}, {"type": "text", "text": "172 3.4 Input encoding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "173 According to eq. (5), the value that can be losslessly encoded under the SWS coding scheme can be   \n174 expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{j}=\\sum_{\\tau=1}^{T_{c}}a_{j}^{\\tau}\\cdot\\theta^{0}\\beta^{T_{c}-\\tau}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "175 where $A_{j}$ denotes the encoded value. $a_{j}^{\\tau}\\in\\{-1,0,1\\}$ indicates the type of the output spike at time $\\tau$ :   \n176 1 for a positive pulse, $-1$ for a negative pulse and 0 for no pulse. $T_{c}$ denoted the time steps used for   \n177 encoding. The weight $\\beta^{T_{c}-\\tau}$ results from the stepwise weighting process described in section 3.1. $\\theta^{0}$   \n178 denotes the spike amplitude of the input encoding layer, which can be assigned an appropriate value   \n179 based on the range to be encoded.   \n180 According to eq. (7), given a fixed $T_{c}$ and $\\theta^{0}$ , the distribution of $A_{j}$ is determined by $\\beta$ . Setting $\\beta$ to   \n181 2 is reasonable, as it ensures $A_{j}$ is evenly distributed within the codable range. Compared to rate   \n182 coding, which necessitates $2^{T_{c}}$ coding steps to encode the same range with same precision, SWS   \n183 coding significantly enhances coding efficiency. Note that with the introduction of negative pulses,   \n184 setting $\\beta$ to 3 can also achieve a uniform distribution of $A_{j}$ and offers even more values for accurate   \n185 encoding compared to $\\beta\\,=\\,2$ .1 When $\\beta$ is less than 2, the distribution of $A_{j}$ becomes denser at   \n186 smaller values, which may be suitable for encoding data that follows a similar distribution.   \n187 For static image classification tasks, the pixel value $p_{j}$ can be encoded by applying a constant input   \n188 $z_{j}^{0}(t)$ to the TSA neuron. Considering the stepwise weighting process, we can write: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\np_{j}=\\sum_{\\tau=1}^{T_{c}}\\left|z_{j}^{0}\\right|\\beta^{T_{c}-\\tau}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "189 where $\\left|z_{j}^{0}\\right|$ denotes the amplitude of the constant input $z_{j}^{0}(t)$ . Solve for $\\left|z_{j}^{0}\\right|$ and we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{j}^{0}(t)=\\sum_{\\sigma=1}^{T_{c}}\\frac{p_{j}}{\\sum_{\\tau=1}^{T_{c}}\\beta^{T_{c}-\\tau}}\\cdot\\delta(t-\\sigma)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "190 Given that $z_{j}^{0}(t)$ is a constant at each step, $T_{s}$ can be set to 0 for the encoding layer. However, the   \n191 neuron must await $T_{s}$ time steps after the completion of an encoding. This allows neurons in the   \n192 subsequent layer to complete the previous neural computing before receiving the next encoded input. ", "page_idx": 5}, {"type": "text", "text": "193 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "194 In this section, we convert quantized ANNs to SWS-based $\\mathrm{SNNs}^{2}$ and conduct experiments on   \n195 MNIST, CIFAR10, and ImageNet. Firstly, an overview of SWS-SNN\u2019s performance across various   \n196 datasets is provided. Subsequently, the network\u2019s inference latency and energy consumption is   \n197 compared with other spike coding schemes. Finally, an ablation study is conducted to investigate the   \n198 impact of lowered thresholds and silent periods on reducing residuals and enhancing accuracy.   \n199 ANNs used for conversion are all quantized to 8 bits. $\\beta$ is set to 2 in the experiments to ensure that   \n200 codable values are evenly distributed. Compared to $\\beta=3$ , a smaller amplification factor reduces the   \n201 impact of residual errors, resulting in more accurate output. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "oW6s6zFYj9/tmp/be1c1db7cfcff3b19c5f7328d35106f2f13c751d786f01cb9847dbbc6d61db29.jpg", "table_caption": ["Table 2: Performance on CIFAR10 and ImageNet. "], "table_footnote": ["\u2020 \u2206Acc = AccSNN \u2212AccANN "], "page_idx": 6}, {"type": "text", "text": "202 4.1 Overall performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "203 For simple classification tasks such as CIFAR10, our proposed SWS coding scheme has a faster   \n204 inference speed than other ANN-SNN models while achieving similar classification accuracy, or has   \n205 higher classification accuracy than direct learning at similar inference speeds. For example, ResNet18   \n206 with SWS improves throughput seven times over [20] while simultaneously improving accuracy.   \n207 Although the network in [5] has a slightly higher throughput, its accuracy is $1.17\\%$ lower than our   \n208 scheme. To fully test the potential of our proposed coding scheme, we conducted experiments on   \n209 ImageNet using networks with various structures. The experimental results demonstrate that SWS   \n210 coding has distinct advantages on extremely deep SNNs. Our SWS-based ResNet50 and ResNeXt101   \n211 achieved over $80\\%$ accuracy on ImageNet with only eight coding steps. The model in [12] achieves   \n212 an almost lossless conversion with eight time steps. However, their method has to adjust the resting   \n213 potential of neurons layer by layer, and the calibration effect for deeper networks is unclear. In [14],   \n214 the original ANN needs to be quantized to 3 bits, resulting in a larger conversion loss. Directly trained   \n215 SNNs typically achieve higher throughput, but their accuracy still requires improvement. In addition,   \n216 the SWS coding scheme is easy to implement. No further fine-tuning is required after the conversion. ", "page_idx": 6}, {"type": "text", "text": "217 4.2 Accuracy vs. latency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "218 The comparison of latency results between SWS-SNN and other ANN-converted SNNs[1, 11, 10,   \n219 4, 18, 2, 7] is illustrated in fig. 3. The latency of the network is calculated with eq. (6). In the   \n220 counterpart models, the variation of delay is mainly caused by the changes in $T_{c}$ . In contrast,   \n221 $T_{s}$ determines latency in deep SWS-SNNs. Therefore, SWS-SNN has an upper limit on latency:   \n222 $T_{i n f}^{m a x}=T_{c}(1+L_{T S\\dot{A}})$ , which causes our curve to terminate earlier in fig. 3.   \n223 To ensure a fair comparison, we represent the ANN accuracy of each counterpart with dotted lines of   \n224 the same color. The experimental results indicate that SWS-SNN can achieve optimal performance   \n225 with minimal latency. Specifically, SWS-based VGG-16 can converge to the ANN performance   \n226 in the shortest time on CIFAR10 and reduce the inference latency on ImageNet by more than one   \n227 order. Even though the silent period accumulates when the network gets deeper, the results in fig. 3(c)   \n228 demonstrate that our scheme still achieves the fastest inference speed with the highest accuracy in a   \n229 34-layer network. Note that $T_{s}$ is set to the same value for each TSA layer for simplicity, resulting in   \n230 discontinuous $T_{i n f}$ values. This causes a sharp drop in accuracy at smaller delays. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/baee5135803c05e2281d0758fd6c94e30a0c7a4e563775faa5ca7690e543f47f.jpg", "img_caption": ["Figure 3: Latency versus accuracy. The ANN accuracy of each compared SNN is marked by dotted lines of the same colour. (a) VGG-16 on CIFAR10. (b) VGG-16 on ImageNet. (c) ResNet34 on ImageNet. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/729926383bcbcb3b88cc7b26f42d0297deec31279c7525b49e2b8919a73dbcbd.jpg", "img_caption": ["Figure 4: (a) Accuracy versus OPF with different combinations of $T_{c}$ and $T_{s}$ . (b) Comparison of accuracy and energy consumption of SWS-SNN with other SNNs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "231 4.3 Operation counting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "232 To compare the energy consumption of SWS-SNN with SNNs under other encoding schemes, we   \n233 adopt the method as in [29, 27, 28] to count operations: ", "page_idx": 7}, {"type": "equation", "text": "$$\nO P F=(T_{c}+T_{s})N_{T S A}+\\sum_{l=1}^{L_{T S A}}\\sum_{\\tau=T_{s}l+1}^{T_{s}l+T_{c}}f_{o u t}^{l}n^{l}(\\tau)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "234 where $O P F$ (Operations Per Frame) denotes the number of operations for the classification of one   \n235 frame, $T_{c}$ and $T_{s}$ denotes the coding steps and the length of the silent period, respectively. $L_{\\mathit{T S A}}$   \n236 denotes the number of TSA layers, f olut d enotes the fan-out of neurons in layer $l$ , $\\dot{n}^{l}(t)$ denotes the   \n237 number of spikes fired in layer $l$ at time $\\tau$ and $N_{T S A}$ denotes the number of TSA neurons. The   \n238 first term on the right-hand side of the equation arises from the TSA\u2019s requirement to amplify the   \n239 membrane potential. Note that due to the accumulation of $T_{s}$ over the network depth, the time period   \n240 for counting $n^{l}(t)$ varies with $l$ .   \n241 Experiments were conducted on MNIST using LeNet-5. We varied the silent periods and adjusted   \n242 the coding steps to study their effects on OPF. The results are presented in fig. 4(a). As indicated in   \n243 eq. (10), reducing $T_{c}$ lowers energy overhead. This presents a trade-off between energy consumption   \n244 and inference accuracy, as fewer coding steps also reduce the number of values that can be accurately   \n245 encoded. A larger $T_{s}$ requires TSA neurons to perform more operations to amplify the membrane   \n246 potential. On the other hand, it reduces the number of unnecessary pulse emissions. Overall, silent   \n247 period has a negligible impact on OPF.   \n248 In fig. 4(b), the energy consumption of SWS-based SNN is compared with that of other SNNs. The   \n249 experimental results demonstrate that our coding scheme can achieve a favorable balance between   \n250 accuracy and energy consumption. The SWS coding scheme is superior to rate coding and temporal   \n251 pattern coding in that it requires fewer operations and achieves higher accuracy. In TTFS encoding,   \n252 each neuron fires at most one spike at a time, theoretically demanding the least OPF. With $T_{c}=4$ ,   \n253 SWS-SNN can achieve significantly higher accuracy with minimal increase in OPF. Note that if   \n254 the ANN is quantized to a lower number of bits (e.g., 4 bits), the error caused by the reduced $T_{c}$   \n255 can actually be compensated by the quantization algorithm, which can potentially result in a higher   \n256 performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/6ed4f383b1879cf2a35a29eafaaeffb5618cd57d8e30ce3596bbc5b5c2881376.jpg", "img_caption": ["Figure 5: (a) The probability density of the residuals with/without a lowered $V_{t h}^{l}$ and a silent period. (b) Inference accuracy of SWS-ResNet18 on CIFAR10 with/without a lowered $V_{t h}^{l}$ and a silent period. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "257 4.4 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "258 In section 3.2 and section 3.3, we proposed reducing the firing threshold and introducing a silent   \n259 period to mitigate residual error. To assess the impact of these two adjustments, we conducted   \n260 experiments on CIFAR10 using ResNet18. After the neural computation, the residuals (absolute   \n261 values) of the TSA neurons were analyzed. We first scaled the residuals by $1/\\beta^{T_{s}}$ to counteract the   \n262 effect of membrane potential amplification caused by the silent period, and then normalized them in   \n263 units of $\\theta^{l}$ . The probability density of the residuals is shown in fig. 5(a).   \n264 The results demonstrate that lowering $V_{t h}^{l}$ shifts the residual distribution from around $0.5\\theta^{l}$ to   \n265 approximately $0.25\\theta^{l}$ , corresponding to the quantization errors (i.e. rounding errors) under their   \n266 respective thresholds. The addition of silent periods further concentrates the distribution and reduces   \n267 large deviations. As can be seen from the green curve in fig. 5(a), setting $T_{s}$ to 2 and $V_{t h}^{l}$ to $\\theta^{l}/2$   \n268 makes the residuals almost all distributed around the quantization error. Compared to the red curve   \n269 (without a lowered $V_{t h}^{l}$ or a silent period), the residuals are greatly reduced, which fully proves the   \n270 effectiveness of lowering the threshold and adding a silent period. The inference results on CIFAR10   \n271 is shown in fig. 5(b). When setting $V_{t h}^{l}$ to $\\theta^{l}$ and $T_{s}$ to zero, the network\u2019s output is almost random.   \n272 Lowering the threshold and adding a silent period improve the accuracy to $\\mathrm{35.41\\%}$ and $84.21\\%$ ,   \n273 respectively. Ultimately, the combination of both adjustments enabled SWS-ResNet18 to achieve an   \n274 accuracy of $95.68\\%$ on CIFAR10. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "275 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "276 In this work, we have proposed a novel SWS spike coding scheme. The stepwise weighting process   \n277 enhances the information-carrying capacity of the preceding pulses, greatly reducing the number of   \n278 time steps for encoding. Combined with a silent period, our proposed TSA neuron model solves the   \n279 problem of residual errors and achieves fast and accurate information transmission. Our experimental   \n280 results have demonstrated that SWS coding is highly effective in extremely deep SNNs and achieves   \n281 state-of-the-art accuracy. The SWS coding scheme is also highly flexible and can adapt to various   \n282 needs. ", "page_idx": 8}, {"type": "text", "text": "283 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "284 [1] Bu, T., Ding, J., Yu, Z., Huang, T.: Optimized potential initialization for low-latency spiking   \n285 neural networks (2022)   \n286 [2] Bu, T., Fang, W., Ding, J., Dai, P., Yu, Z., Huang, T.: Optimal ann-snn conversion for high  \n287 accuracy and ultra-low-latency spiking neural networks (2023)   \n288 [3] Cao, Y., Chen, Y., Khosla, D.: Spiking deep convolutional neural networks for   \n289 energy-efficient object recognition. International Journal of Computer Vision 113(1), 54\u2013   \n290 66 (May 2015). https://doi.org/10.1007/s11263-014-0788-3, https://doi.org/10.1007/   \n291 s11263-014-0788-3   \n292 [4] Deng, S., Gu, S.: Optimal conversion of conventional artificial neural networks to spiking neural   \n293 networks (2021)   \n294 [5] Deng, S., Li, Y., Zhang, S., Gu, S.: Temporal efficient training of spiking neural network via   \n295 gradient re-weighting (2022)   \n296 [6] Diehl, P.U., Neil, D., Binas, J., Cook, M., Liu, S.C., Pfeiffer, M.: Fast-classifying,   \n297 high-accuracy spiking deep networks through weight and threshold balancing. In:   \n298 2015 International Joint Conference on Neural Networks (IJCNN). pp. 1\u20138 (2015).   \n299 https://doi.org/10.1109/IJCNN.2015.7280696   \n300 [7] Ding, J., Yu, Z., Tian, Y., Huang, T.: Optimal ann-snn conversion for fast and accurate inference   \n301 in deep spiking neural networks (2021)   \n302 [8] Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., Tian, Y.: Deep residual learning in   \n303 spiking neural networks (2022)   \n304 [9] Guo, W., Fouda, M.E., Eltawil, A.M., Salama, K.N.: Neural coding in spiking neural net  \n305 works: A comparative study for robust neuromorphic systems. Frontiers in Neuroscience   \n306 15 (2021). https://doi.org/10.3389/fnins.2021.638474, https://www.frontiersin.org/   \n307 journals/neuroscience/articles/10.3389/fnins.2021.638474   \n308 [10] Han, B., Roy, K.: Deep spiking neural network: Energy efficiency through time based coding.   \n309 In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision \u2013 ECCV 2020. pp.   \n310 388\u2013404. Springer International Publishing, Cham (2020)   \n311 [11] Han, B., Srinivasan, G., Roy, K.: Rmp-snn: Residual membrane potential neuron for enabling   \n312 deeper high-accuracy and low-latency spiking neural network (2020)   \n313 [12] Hao, Z., Ding, J., Bu, T., Huang, T., Yu, Z.: Bridging the gap between anns and snns by   \n314 calibrating offset spikes (2023)   \n315 [13] Hu, Y., Tang, H., Pan, G.: Spiking deep residual networks. IEEE Transac  \n316 tions on Neural Networks and Learning Systems 34(8), 5200\u20135205 (2023).   \n317 https://doi.org/10.1109/TNNLS.2021.3119238   \n318 [14] Hu, Y., Zheng, Q., Jiang, X., Pan, G.: Fast-snn: Fast spiking neural network by converting   \n319 quantized ann. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(12), 14546\u2013   \n320 14562 (2023). https://doi.org/10.1109/TPAMI.2023.3275769   \n321 [15] Kim, J., Kim, H., Huh, S., Lee, J., Choi, K.: Deep neural networks with weighted spikes. Neuro  \n322 computing 311, 373\u2013386 (2018). https://doi.org/https://doi.org/10.1016/j.neucom.2018.05.087,   \n323 https://www.sciencedirect.com/science/article/pii/S0925231218306726   \n324 [16] Kim, Y., Li, Y., Park, H., Venkatesha, Y., Hambitzer, A., Panda, P.: Exploring temporal   \n325 information dynamics in spiking neural networks (2022)   \n326 [17] Lee, C., Sarwar, S.S., Panda, P., Srinivasan, G., Roy, K.: Enabling spike-based back  \n327 propagation for training deep neural network architectures. Frontiers in Neuroscience   \n328 14 (Feb 2020). https://doi.org/10.3389/fnins.2020.00119, http://dx.doi.org/10.3389/   \n329 fnins.2020.00119   \n330 [18] Li, Y., Deng, S., Dong, X., Gong, R., Gu, S.: A free lunch from ann: Towards efficient, accurate   \n331 spiking neural networks calibration (2021)   \n332 [19] Meng, Q., Xiao, M., Yan, S., Wang, Y., Lin, Z., Luo, Z.Q.: Training high-performance low  \n333 latency spiking neural networks by differentiation on spike representation (2023)   \n334 [20] Meng, Q., Yan, S., Xiao, M., Wang, Y., Lin, Z., Luo, Z.Q.: Training much deeper spiking   \n335 neural networks with a small number of time-steps. Neural Networks 153, 254\u2013268 (2022).   \n336 https://doi.org/https://doi.org/10.1016/j.neunet.2022.06.001, https://www.sciencedirect.   \n337 com/science/article/pii/S0893608022002064   \n338 [21] Park, S., Kim, S., Choe, H., Yoon, S.: Fast and efficient information transmission with burst   \n339 spikes in deep spiking neural networks (2019)   \n340 [22] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,   \n341 Gimelshein, N., Antiga, L., Desmaison, A., K\u00f6pf, A., Yang, E., DeVito, Z., Raison, M., Tejani,   \n342 A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style,   \n343 high-performance deep learning library (2019)   \n344 [23] P\u00e9rez-Carrasco, J.A., Zhao, B., Serrano, C., Acha, B., Serrano-Gotarredona, T., Chen, S.,   \n345 Linares-Barranco, B.: Mapping from frame-driven to frame-free event-driven vision sys  \n346 tems by low-rate rate coding and coincidence processing\u2013application to feedforward convnets.   \n347 IEEE Transactions on Pattern Analysis and Machine Intelligence 35(11), 2706\u20132719 (2013).   \n348 https://doi.org/10.1109/TPAMI.2013.71   \n349 [24] Querlioz, D., Bichler, O., Dollfus, P., Gamrat, C.: Immunity to device variations in a spiking   \n350 neural network with memristive nanodevices. IEEE Transactions on Nanotechnology 12(3),   \n351 288\u2013295 (2013). https://doi.org/10.1109/TNANO.2013.2250995   \n352 [25] Querlioz, D., Bichler, O., Gamrat, C.: Simulation of a memristor-based spiking neural network   \n353 immune to device variations. In: The 2011 International Joint Conference on Neural Networks.   \n354 pp. 1775\u20131781 (2011). https://doi.org/10.1109/IJCNN.2011.6033439   \n355 [26] Rathi, N., Srinivasan, G., Panda, P., Roy, K.: Enabling deep spiking neural networks with hybrid   \n356 conversion and spike timing dependent backpropagation (2020)   \n357 [27] Rueckauer, B., Liu, S.C.: Conversion of analog to spiking neural networks using sparse temporal   \n358 coding. In: 2018 IEEE International Symposium on Circuits and Systems (ISCAS). pp. 1\u20135   \n359 (2018). https://doi.org/10.1109/ISCAS.2018.8351295   \n360 [28] Rueckauer, B., Liu, S.C.: Temporal pattern coding in deep spiking neural networks.   \n361 In: 2021 International Joint Conference on Neural Networks (IJCNN). pp. 1\u20138 (2021).   \n362 https://doi.org/10.1109/IJCNN52387.2021.9533837   \n363 [29] Rueckauer, B., Lungu, I.A., Hu, Y., Pfeiffer, M., Liu, S.C.: Conversion of continuous-valued   \n364 deep networks to efficient event-driven networks for image classification. Frontiers in Neuro  \n365 science 11 (2017). https://doi.org/10.3389/fnins.2017.00682   \n366 [30] St\u00f6ckl, C., Maass, W.: Optimized spiking neurons can classify images with high accu  \n367 racy through temporal coding with two spikes. Nature Machine Intelligence 3(3), 230\u2013   \n368 238 (Mar 2021). https://doi.org/10.1038/s42256-021-00311-4, https://doi.org/10.1038/   \n369 s42256-021-00311-4   \n370 [31] Taherkhani, A., Belatreche, A., Li, Y., Cosma, G., Maguire, L.P., McGinnity, T.: A re  \n371 view of learning in biologically plausible spiking neural networks. Neural Networks 122,   \n372 253\u2013272 (2020). https://doi.org/https://doi.org/10.1016/j.neunet.2019.09.036, https://www.   \n373 sciencedirect.com/science/article/pii/S0893608019303181   \n374 [32] Wang, X., Lin, X., Dang, X.: Supervised learning in spiking neural networks:   \n375 A review of algorithms and evaluations. Neural Networks 125, 258\u2013280 (2020).   \n376 https://doi.org/https://doi.org/10.1016/j.neunet.2020.02.011, https://www.sciencedirect.   \n377 com/science/article/pii/S0893608020300563   \n378 [33] Yamazaki, K., Vo-Ho, V.K., Bulsara, D., Le, N.: Spiking neural networks and their applications:   \n379 A review. Brain Sci 12(7) (Jun 2022)   \n380 [34] Yan, Z., Zhou, J., Wong, W.: Near lossless transfer learning for spiking neural networks. In:   \n381 AAAI Conference on Artificial Intelligence (2021), https://api.semanticscholar.org/   \n382 CorpusID:235349069   \n383 [35] Zheng, H., Wu, Y., Deng, L., Hu, Y., Li, G.: Going deeper with directly-trained larger spiking   \n384 neural networks (2020)   \n385 [36] Zhou, S., LI, X., Chen, Y., Chandrasekaran, S.T., Sanyal, A.: Temporal-coded deep spiking   \n386 neural network with easy training and robust performance (2021) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "387 A Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "388 A.1 Convert quantized ANNs to SWS-SNNs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "389 A pretrained ANN was first obtained from torchvision, which is part of the PyTorch[22] project, and   \n390 then quantized into $n$ bits following the Quantization-Aware Training (QAT) Workflow provided by   \n391 PyTorch (8 bits in the actual experiment, with $n$ bits used here for generality). The quantized ANN   \n392 can be characterized by the parameters listed in table 3, and the basic idea of the conversion process is   \n393 illustrated in fig. 6(a). The activations of the quantized ANN can be mapped to an integer $Q$ between   \n394 $[0,2^{n}-1]$ using a scaling factor $C$ and a zero point $Z$ . With the same weight and bias between $Q_{i}^{l}$   \n395 and $Q_{o}^{l}$ , the TSA layer can generate $S^{l}$ , which encodes $Q_{o}^{l}$ , provided that $S^{l-1}$ encodes $Q_{i}^{l}$ and no   \n396 residual error occurs. In the actual SNN, the pulse amplitude $\\theta^{l}$ is normalized to 1. Therefore, the   \n397 bias need to be further scaled to derive the final weight $\\mathbf{\\Bar{\\boldsymbol{W}}}^{l}$ and bias $b^{l}$ for the SWS-SNN. ", "page_idx": 11}, {"type": "table", "img_path": "oW6s6zFYj9/tmp/28f7e8f592744d4c7c8e822153d60d6dfd9244624f27135b2cd03b03a119c0a2.jpg", "table_caption": ["Table 3: The notations and meanings of parameters in the quantized network. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "398 The derivation is as follows. After QAT, we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{W}^{l}\\hat{X}_{i}^{l}+\\hat{b}^{l}=\\hat{X}_{o}^{l}}\\\\ {Q_{i}^{l}=\\cfrac{\\hat{X}_{i}^{l}}{C_{i}^{l}}+Z_{i}^{l},}\\\\ {Q_{o}^{l}=\\cfrac{\\hat{X}_{o}^{l}}{C_{o}^{l}}+Z_{o}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "399 where $Q_{i}^{l},Q_{o}^{l}$ represent the integers to which the quantized input and output are mapped, respectively.   \n400 Substitute eq. (12) and eq. (13) into eq. (11), and we can write: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\hat{W}^{l}(Q_{i}^{l}-Z_{i}^{l})C_{i}^{l}+\\hat{b}^{l}=(Q_{o}^{l}-Z_{o}^{l})C_{o}^{l},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "401 which gives: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{o}^{l}=\\hat{W}^{l}\\frac{C_{i}^{l}}{C_{o}^{l}}Q_{i}^{l}+\\frac{\\hat{b}^{l}}{C_{o}^{l}}+Z_{o}^{l}-\\frac{\\hat{W}^{l}Z_{i}^{l}C_{i}^{l}}{C_{o}^{l}}}\\\\ {=\\hat{W}^{l}Q_{i}^{l}+\\tilde{b}^{l},\\qquad\\qquad\\qquad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "image", "img_path": "oW6s6zFYj9/tmp/0890c67256ffbf969e5c93e85eb3c5e1b67d1c032dfb45162f6afe7c2dbb2f17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 6: (a) Convert quantized ANNs to SWS-SNNs. $Q_{i}^{l}$ and $Q_{o}^{l}$ represent the integers to which $\\hat{X}_{i}^{l}$ and $\\hat{X}_{o}^{l}$ are mapped, respectively. $\\Tilde{W}^{l}$ and $\\tilde{b}^{l}$ denotes the weight and bias to get $Q_{o}^{l}$ from $Q_{i}^{l}$ . $W^{l}$ and $b^{l}$ denotes the weight and bias in SWS-SNN. The process of transferring weights and biases from the quantized ANN to SWS-SNN is indicated by white arrows. The core of the conversion is that the distribution of the integer $Q_{o}^{l}$ is known and can be easily encoded by $S^{l}$ . (b) Process the input pixels to encode by pulses with an amplitude of 1. $\\bar{P}$ denotes the original pixel value, $P$ denotes the mapped value and $\\bar{\\tilde{P}}$ denotes the value after scaled by $1/\\theta^{0}$ . ", "page_idx": 12}, {"type": "text", "text": "402 where ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\tilde{W}^{l}=\\hat{W}^{l}\\displaystyle\\frac{C_{i}^{l}}{C_{o}^{l}},}}\\\\ {{\\tilde{b}^{l}=\\displaystyle\\frac{\\hat{b}^{l}}{C_{o}^{l}}+Z_{o}^{l}-\\displaystyle\\frac{\\hat{W}^{l}Z_{i}^{l}C_{i}^{l}}{C_{o}^{l}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "403 As seen in eq. (15), with the weight and bias set to $\\tilde{W}^{l}$ and $\\tilde{b}^{l}$ respectively, the layer outputs $Q_{o}^{l}$ when   \n404 receiving $Q_{i}^{l}$ . The pulse amplitude $\\theta^{l}$ can be set to any value as long as the codable range calculated   \n405 by eq. (7) covers $[0,2^{n}-1]$ . Then we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\nW^{l}=\\tilde{W}^{l}\\frac{\\theta^{l-1}}{\\theta^{l}}=\\hat{W}^{l}\\frac{C_{i}^{l}}{C_{o}^{l}}\\frac{\\theta^{l-1}}{\\theta^{l}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "406 Considering the membrane potential amplification, $b^{l}$ can be calculated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nb^{l}=\\frac{1}{\\sum_{\\tau=1}^{T_{c}}\\beta^{T_{c}-\\tau}}\\tilde{b}^{l}=\\frac{1}{\\sum_{\\tau=1}^{T_{c}}\\beta^{T_{c}-\\tau}}(\\frac{\\hat{b}^{l}}{C_{o}^{l}}+Z_{o}^{l}-\\frac{\\hat{W}^{l}Z_{i}^{l}C_{i}^{l}}{C_{o}^{l}})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "407 Once the $T_{c},\\beta$ and $\\theta^{l}$ $\\theta^{l-1}$ is given by the previous layer) have been determined, all values on the   \n408 right side of eq. (18) and eq. (19) are known. Consequently, $W_{l}$ and $b^{l}$ in the SWS-SNN can be   \n409 readily calculated from the weight and bias of the quantized ANN.   \n410 After configuring the weights and biases as described above, the input pixel must be encoded into a   \n411 pulse sequence with an amplitude of 1 as well. This process is illustrated in fig. 6(b). First, map the   \n412 pixel value to $[0,2^{n}-1]$ using $C_{i}^{0}$ and $Z_{i}^{0}$ obtained from QAT. Assuming this range can be encoded   \n413 by SWSs with an amplitude of $\\theta^{0}$ , scaling the pixel value by $1/\\theta^{0}$ allows the use of a sequence with   \n414 $\\theta^{\\bar{0}}=1$ for encoding. Finally, encode the scaled pixels following section 3.4, and the required input   \n415 spike sequence is obtained. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "416 A.2 Details for QAT ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "417 QAT is the quantization method that typically results in the highest accuracy. We basically follows   \n418 the workflow provided by PyTorch. The default QAT quantization configuration is chosen to specify   \n419 the kind of fake-quantization inserted after weights and activations. We choose Stochastic Gradient   \n420 Descent (SGD) optimizer in QAT, with the value of momentum set to 0.9 and the learning rate set to   \n421 $1\\times10^{-4}$ since the weights only need to be fine-tuned. QAT is done for 12 epochs and 20 batches in   \n422 each epoch. We freeze the batch norm mean and variance estimates after three epochs and freeze the   \n423 quantizer parameters (scaling factor and zero point) after another two epochs. ", "page_idx": 12}, {"type": "text", "text": "424 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "429 Justification: Stepwise weighting enhances the encoding of information in spikes, as is   \n430 proved in eq. (5) in section 3.1. Our proposed SWS coding scheme achieves high perfor  \n431 mance and low energy consumption, which is supported by our experimental results in   \n432 section 4. The TSA neuron model effectively minimizes the residual error, which can be   \n433 proved from the ablation study in section 4.4. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The inclusion of silent periods can lead to increased latency, as noted in section 3.3, which is a limitation we\u2019ve found so far. However, our experimental results demonstrate that our delay performance still surpasses that of other SNNs. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "477 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: The membrane potential amplification enhances the information-carrying capacity of the preceding pulses and is proved in eq. (5). ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "94 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We set specific random number seeds when conducting experiments to ensure that all the results of section 4 are reproducible. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 14}, {"type": "text", "text": "530 In the case of closed-source models, it may be that access to the model is limited in   \n531 some way (e.g., to registered users), but it should be possible for other researchers   \n532 to have some path to reproducing or verifying the results.   \n533 5. Open access to data and code   \n534 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n535 tions to faithfully reproduce the main experimental results, as described in supplemental   \n536 material?   \n537 Answer: [No]   \n538 Justification: Code will be released when the paper is accepted.   \n539 Guidelines:   \n540 \u2022 The answer NA means that paper does not include experiments requiring code.   \n541 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n542 public/guides/CodeSubmissionPolicy) for more details.   \n543 \u2022 While we encourage the release of code and data, we understand that this might not be   \n544 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n545 including code, unless this is central to the contribution (e.g., for a new open-source   \n546 benchmark).   \n547 \u2022 The instructions should contain the exact command and environment needed to run to   \n548 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n549 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n550 \u2022 The authors should provide instructions on data access and preparation, including how   \n551 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n552 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n553 proposed method and baselines. If only a subset of experiments are reproducible, they   \n554 should state which ones are omitted from the script and why.   \n555 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n556 versions (if applicable).   \n557 \u2022 Providing as much information as possible in supplemental material (appended to the   \n558 paper) is recommended, but including URLs to data and code is permitted.   \n559 6. Experimental Setting/Details   \n560 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n561 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n562 results?   \n563 Answer: [Yes]   \n564 Justification: The details for acquiring different delays and the OPF calculation method   \n565 are provided in section 4.2 and section 4.3, respectively. The parameters used during QAT   \n566 training is outlined in appendix A.2.   \n567 Guidelines:   \n568 \u2022 The answer NA means that the paper does not include experiments.   \n569 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n570 that is necessary to appreciate the results and make sense of them.   \n571 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n572 material.   \n573 7. Experiment Statistical Significance   \n574 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n575 information about the statistical significance of the experiments?   \n576 Answer: [No]   \n577 Justification: We believe it is not necessary to include error bars in the results because each   \n578 experimental result itself is already the average of a large number of tests (E.g., the test   \n579 accuracy for an epoch is averaged over $N u m\\_o f\\_b a t c h e s\\times B a t c h\\_s i z e$ input images,   \n580 and is therefore very close to each other in every test epoch).   \n581 Guidelines:   \n582 \u2022 The answer NA means that the paper does not include experiments.   \n583 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n584 dence intervals, or statistical significance tests, at least for the experiments that support   \n585 the main claims of the paper.   \n586 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n587 example, train/test split, initialization, random drawing of some parameter, or overall   \n588 run with given experimental conditions).   \n589 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n590 call to a library function, bootstrap, etc.)   \n591 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n592 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n593 of the mean.   \n594 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n595 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n596 of Normality of errors is not verified.   \n597 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n598 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n599 error rates).   \n600 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n601 they were calculated and reference the corresponding figures or tables in the text.   \n602 8. Experiments Compute Resources   \n603 Question: For each experiment, does the paper provide sufficient information on the com  \n604 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n605 the experiments?   \n606 Answer: [No]   \n607 Justification: We found it difficult to quantify the computing resources used in every   \n608 experiments.   \n609 Guidelines:   \n610 \u2022 The answer NA means that the paper does not include experiments.   \n611 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n612 or cloud provider, including relevant memory and storage.   \n613 \u2022 The paper should provide the amount of compute required for each of the individual   \n614 experimental runs as well as estimate the total compute.   \n615 \u2022 The paper should disclose whether the full research project required more compute   \n616 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n617 didn\u2019t make it into the paper).   \n618 9. Code Of Ethics   \n619 Question: Does the research conducted in the paper conform, in every respect, with the   \n620 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n621 Answer: [Yes]   \n622 Justification: We have read the NeurIPS Code of Ethics and the research conducted in this   \n623 paper conforms with it.   \n624 Guidelines:   \n625 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n626 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n627 deviation from the Code of Ethics.   \n628 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n629 eration due to laws or regulations in their jurisdiction).   \n630 10. Broader Impacts   \n631 Question: Does the paper discuss both potential positive societal impacts and negative   \n632 societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "634 Justification: There is no societal impact of the work performed.   \n635 Guidelines:   \n636 \u2022 The answer NA means that there is no societal impact of the work performed.   \n637 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n638 impact or why the paper does not address societal impact.   \n639 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n640 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n641 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n642 groups), privacy considerations, and security considerations.   \n643 \u2022 The conference expects that many papers will be foundational research and not tied   \n644 to particular applications, let alone deployments. However, if there is a direct path to   \n645 any negative applications, the authors should point it out. For example, it is legitimate   \n646 to point out that an improvement in the quality of generative models could be used to   \n647 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n648 that a generic algorithm for optimizing neural networks could enable people to train   \n649 models that generate Deepfakes faster.   \n650 \u2022 The authors should consider possible harms that could arise when the technology is   \n651 being used as intended and functioning correctly, harms that could arise when the   \n652 technology is being used as intended but gives incorrect results, and harms following   \n653 from (intentional or unintentional) misuse of the technology.   \n654 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n655 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n656 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n657 feedback over time, improving the efficiency and accessibility of ML).   \n658 11. Safeguards   \n659 Question: Does the paper describe safeguards that have been put in place for responsible   \n660 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n661 image generators, or scraped datasets)?   \n662 Answer: [NA]   \n663 Justification: The paper poses no such risks.   \n664 Guidelines:   \n665 \u2022 The answer NA means that the paper poses no such risks.   \n666 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n667 necessary safeguards to allow for controlled use of the model, for example by requiring   \n668 that users adhere to usage guidelines or restrictions to access the model or implementing   \n669 safety filters.   \n670 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n671 should describe how they avoided releasing unsafe images.   \n672 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n673 not require this, but we encourage authors to take this into account and make a best   \n674 faith effort.   \n675 12. Licenses for existing assets   \n676 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n677 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n678 properly respected?   \n679 Answer: [Yes]   \n680 Justification: The pretrained ANN model and the QAT workflow is provided by PyTorch   \n681 and we cited the original paper in appendix A.1 as [22].   \n682 Guidelines:   \n683 \u2022 The answer NA means that the paper does not use existing assets.   \n684 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n685 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n686 URL.   \n687 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n688 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n689 service of that source should be provided.   \n690 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n691 package should be provided. For popular datasets, paperswithcode.com/datasets   \n692 has curated licenses for some datasets. Their licensing guide can help determine the   \n693 license of a dataset.   \n694 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n695 the derived asset (if it has changed) should be provided.   \n696 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n697 the asset\u2019s creators.   \n698 13. New Assets   \n699 Question: Are new assets introduced in the paper well documented and is the documentation   \n700 provided alongside the assets?   \n701 Answer: [NA]   \n702 Justification: The paper does not release new assets.   \n703 Guidelines:   \n704 \u2022 The answer NA means that the paper does not release new assets.   \n705 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n706 submissions via structured templates. This includes details about training, license,   \n707 limitations, etc.   \n708 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n709 asset is used.   \n710 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n711 create an anonymized URL or include an anonymized zip file.   \n712 14. Crowdsourcing and Research with Human Subjects   \n713 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n714 include the full text of instructions given to participants and screenshots, if applicable, as   \n715 well as details about compensation (if any)?   \n716 Answer: [NA]   \n717 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n720 human subjects.   \n721 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n722 tion of the paper involves human subjects, then as much detail as possible should be   \n723 included in the main paper.   \n724 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n725 or other labor should be paid at least the minimum wage in the country of the data   \n726 collector.   \n727 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n728 Subjects   \n729 Question: Does the paper describe potential risks incurred by study participants, whether   \n730 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n731 approvals (or an equivalent approval/review based on the requirements of your country or   \n732 institution) were obtained?   \n733 Answer: [NA]   \n734 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n735 Guidelines:   \n736 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n737 human subjects. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]