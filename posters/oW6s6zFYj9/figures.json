[{"figure_path": "oW6s6zFYj9/figures/figures_3_1.jpg", "caption": "Figure 1: (a) Illustration of the stepwise weighting process. The meanings of the symbol z}(t), u(t) and S}(t) can be found in table 1. The blue dotted line represents the membrane potential prior to the spike firing, and the black exponential function-like dotted line is employed to illustrate the trend of membrane potential amplification. (b) A Vth equal to \u03b8\u00b9 results in residual errors, leaving a lot of information unencoded. (c) Vth is set to 10, which increases the possibility to fire spikes early to better limit the residual. (d) Use negative spikes to correct the excessively emitted information.", "description": "This figure illustrates the stepwise weighting process and its effect on the membrane potential. (a) shows how the membrane potential is amplified before receiving input, (b) shows how residual errors accumulate when the threshold is set equal to the spike amplitude, (c) shows how limiting the residual improves accuracy, and (d) shows how negative spikes correct residual errors.", "section": "3 Stepwise weighted spike coding scheme"}, {"figure_path": "oW6s6zFYj9/figures/figures_4_1.jpg", "caption": "Figure 1: (a) Illustration of the stepwise weighting process. The meanings of the symbol z<sup>l</sup><sub>i</sub>(t), u<sup>l</sup><sub>i</sub>(t) and S<sup>l</sup><sub>i</sub>(t) can be found in table 1. The blue dotted line represents the membrane potential prior to the spike firing, and the black exponential function-like dotted line is employed to illustrate the trend of membrane potential amplification. (b) A V<sup>l</sup><sub>th</sub> equal to \u03b8<sup>l</sup> results in residual errors, leaving a lot of information unencoded. (c) V<sup>l</sup><sub>th</sub> is set to 10, which increases the possibility to fire spikes early to better limit the residual. (d) Use negative spikes to correct the excessively emitted information.", "description": "This figure illustrates the stepwise weighting process in the proposed SWS coding scheme. It shows how amplifying the membrane potential before receiving input (a) helps to weight earlier spikes more significantly. However, simply amplifying can lead to residual errors (b). Limiting the residual through a lower threshold (c) and using negative spikes for correction (d) are shown to improve the accuracy of information encoding.  The figure uses plots of membrane potential and spike trains to visualize the process.", "section": "3 Stepwise weighted spike coding scheme"}, {"figure_path": "oW6s6zFYj9/figures/figures_7_1.jpg", "caption": "Figure 3: Latency versus accuracy. The ANN accuracy of each compared SNN is marked by dotted lines of the same colour. (a) VGG-16 on CIFAR10. (b) VGG-16 on ImageNet. (c) ResNet34 on ImageNet.", "description": "This figure compares the latency and accuracy of different spiking neural networks (SNNs) on three different datasets: CIFAR10, ImageNet, and ResNet34.  The x-axis represents latency, and the y-axis represents accuracy. Each line represents a different SNN architecture or conversion method, and the dotted line shows the accuracy of the original ANN model. The figure demonstrates how the proposed SWS-SNN (in blue) achieves better accuracy at lower latency compared to other methods.", "section": "4.2 Accuracy vs. latency"}, {"figure_path": "oW6s6zFYj9/figures/figures_7_2.jpg", "caption": "Figure 3: Latency versus accuracy. The ANN accuracy of each compared SNN is marked by dotted lines of the same colour. (a) VGG-16 on CIFAR10. (b) VGG-16 on ImageNet. (c) ResNet34 on ImageNet.", "description": "This figure compares the latency and accuracy of the proposed SWS-SNN method against other state-of-the-art ANN-converted SNN methods.  The x-axis represents latency, and the y-axis represents accuracy. Separate plots are shown for different network architectures (VGG-16 and ResNet34) and datasets (CIFAR10 and ImageNet). The dotted lines indicate the accuracy of the original ANN models for each architecture to provide a benchmark for comparison. This visualization highlights the trade-off between latency and accuracy, and demonstrates that the SWS-SNN method achieves competitive accuracy with lower latency compared to alternative methods.", "section": "4.2 Accuracy vs. latency"}, {"figure_path": "oW6s6zFYj9/figures/figures_8_1.jpg", "caption": "Figure 5: (a) The probability density of the residuals with/without a lowered  Vth and a silent period. (b) Inference accuracy of SWS-ResNet18 on CIFAR10 with/without a lowered  Vth and a silent period.", "description": "This figure shows the impact of lowering the firing threshold (Vth) and adding a silent period (Ts) on the residual error and accuracy of the SWS-ResNet18 model.  (a) shows the probability density distribution of the residuals, indicating a significant reduction in residuals with both modifications. (b) shows a dramatic increase in the CIFAR-10 accuracy after applying both adjustments. ", "section": "4.4 Ablation study"}, {"figure_path": "oW6s6zFYj9/figures/figures_12_1.jpg", "caption": "Figure 6: (a) Convert quantized ANNs to SWS-SNNs. Q and Q represent the integers to which X and X are mapped, respectively. W and denotes the weight and bias to get Q from Q. W and b denotes the weight and bias in SWS-SNN. The process of transferring weights and biases from the quantized ANN to SWS-SNN is indicated by white arrows. The core of the conversion is that the distribution of the integer Q is known and can be easily encoded by S. (b) Process the input pixels to encode by pulses with an amplitude of 1. P denotes the original pixel value, P denotes the mapped value and P denotes the value after scaled by 1/\u03b8\u00ba.", "description": "This figure illustrates the conversion process from a quantized ANN to an SWS-SNN.  Part (a) shows the process of transferring weights and biases from the quantized ANN to the SWS-SNN.  The key is that the distribution of the quantized output (Q) is known and easily encoded by the spike train (S). Part (b) shows how the input pixels are encoded into spike trains with an amplitude of 1.", "section": "A.1 Convert quantized ANNs to SWS-SNNS"}]