[{"figure_path": "xvYI7TCiU6/tables/tables_13_1.jpg", "caption": "Table 1: Common hyperparameters in MA-Mujoco", "description": "This table lists the common hyperparameters used in the MA-Mujoco experiments described in the paper.  These hyperparameters are consistent across different tasks within the MA-Mujoco environment.  The settings include the activation function used in the neural networks, batch size for training updates, discount factor (gamma), gain, number of PPO epochs, episode length, and number of rollout threads.", "section": "B.1 Detailed experimental Settings"}, {"figure_path": "xvYI7TCiU6/tables/tables_13_2.jpg", "caption": "Table 2: Different hyperparameters in MA-Mujoco", "description": "This table lists the hyperparameters used in the Multi-Agent MuJoCo experiments.  It shows the hidden layer architecture, learning rates for the actor and critic networks, the clipping parameter for PPO, the coefficient for balancing inter-agent and intra-agent divergence, and the parameter for the Cauchy-Schwarz divergence.  The hyperparameters are specified for each of the different MuJoCo tasks.", "section": "B.1 Detailed experimental Settings"}, {"figure_path": "xvYI7TCiU6/tables/tables_14_1.jpg", "caption": "Table 3: Common hyperparameters in Bi-DexHands", "description": "This table lists the common hyperparameters used in the Bi-DexHands experiments.  It includes settings for the activation function, batch size, gamma, gain, PPO epochs, episode length, number of rollout threads, hidden layer architecture, clipping range, actor learning rate, and critic learning rate. These hyperparameters are consistent across different scenarios within the Bi-DexHands environment, ensuring consistency and facilitating comparison between experiments.", "section": "B.1 Detailed experimental Settings"}, {"figure_path": "xvYI7TCiU6/tables/tables_14_2.jpg", "caption": "Table 4: Different hyperparameters in Bi-DexHands", "description": "This table lists the different hyperparameter settings used for the various tasks within the Bi-DexHands environment.  Specifically, it shows the values of lambda (\u03bb) and sigma (\u03c3) which are parameters of the Mutual Policy Divergence Maximization (Mutual PDM) method used in the MADPO algorithm.  Different tasks require different parameter tuning, as indicated in this table.", "section": "B.1 Detailed experimental Settings"}, {"figure_path": "xvYI7TCiU6/tables/tables_17_1.jpg", "caption": "Table 5: Wall time comparison.", "description": "This table shows the training time of different algorithms (A2PO, HAPPO, HATRPO, MAPPO, and MADPO) on various multi-agent tasks in both MuJoCo and Bi-DexHands environments.  The results are presented in hours and minutes, indicating the computational cost of each method for reaching a certain level of training progress. Note that training steps are also reported for each task.", "section": "5 Experiments"}]