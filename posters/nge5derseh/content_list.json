[{"type": "text", "text": "On the Power of Decision Trees in Auto-Regressive Language Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yulu Gan Massachusetts Institute of Technology yulu@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Tomer Galanti Texas A&M University galanti@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Tomaso Poggio Massachusetts Institute of Technology tp@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Eran Malach Harvard University eran.malach@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Originally proposed for handling time series data, Auto-regressive Decision Trees (ARDTs) have not yet been explored for language modeling. This paper explores both the theoretical and practical applications of ARDTs in this new context. We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging \u201cchainof-thought\u201d computations. Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power. Empirically, we train ARDTs on simple language generation tasks, showing that they can learn to generate coherent and grammatically correct text on par with a smaller Transformer model. Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks. This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, Large Language Models (LLMs) have achieved outstanding results in tasks such as natural language understanding, coding, and mathematical reasoning. LLMs predominantly utilize the Transformer architecture Vaswani et al. (2023), establishing it as the standard in this field. However, recent initiatives (Gu & Dao, 2023; Sun et al., 2023; Ma et al., 2023; De et al., 2024) have begun to challenge the dominance of Transformers. These alternatives, while not yet matching Transformer performance, offer advantages in terms of inference time efficiency. Moreover, some works are revisiting traditional non-neural network models for language modeling, such as classical symbolic models (Wong et al., 2023). These developments indicate a shift towards diverse, efficient, and interpretable language modeling methodologies. ", "page_idx": 0}, {"type": "text", "text": "Tree-based models, particularly favored for handling tabular data (Grinsztajn et al., 2022), continue to hold significant importance. While tree-based methods are mostly used for classification and regression tasks, Auto-regressive Decision Trees (ARDTs) (Meek et al., 2002) have been studied for time-series prediction, offering a simpler and more interpretable alternative to complex nonlinear approaches. Although the ARDT approach was not originally designed for language tasks, it has demonstrated considerable promise in various time-series datasets, outperforming traditional autoregressive models while maintaining ease of interpretation. Motivated by these results, our study seeks to explore the potential of ARDTs for language prediction tasks, assessing whether they could serve as a viable, interpretable alternative to complex, resource-intensive language models. ", "page_idx": 0}, {"type": "image", "img_path": "nge5deRsEH/tmp/d0eeda83f67a1e7d966674a09a976ac770019fc2ac399ae173da6b1f401e10a0.jpg", "img_caption": ["Figure 1: (a) An example of story continuation generated by our Auto-Regressive Decision Trees. We use decision trees and, remarkably, attain results comparable to Transformer-based models in terms of linguistic fluency. (b) The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To understand the power of ARDTs, we first conduct theoretical studies demonstrating that ARDTs, using decision trees as next-token predictors, can compute more complex functions than traditional decision trees. We explore the classes of functions ARDTs can compute, showing their ability to simulate functions computed by automata, Turing machines, or sparse circuits through intermediate \u201cchain-of-thought\u201d computations. We provide bounds on the size, depth, and run-time (measured by the number of intermediate tokens) required for ARDTs to simulate these function classes. Our findings highlight the surprising computational capabilities of ARDTs, underscoring their potential as a powerful and interpretable alternative for language prediction tasks requiring complex function computations. ", "page_idx": 1}, {"type": "text", "text": "Our experimental results further demonstrate the practical utility of ARDTs in language generation tasks. Utilizing standard auto-regressive inference methods, these models generate output sequences token-by-token, appending each new token to the input of the subsequent iteration. When trained on the TinyStories dataset Eldan & Li (2023), ARDTs produce coherent and grammatically accurate text (see in Fig 1). Notably, decision tree ensembles with approximately 0.3 million parameters outperform a Transformer model with around 1 million parameters on the same Tinystories dataset, highlighting their efficiency despite a smaller size. We discuss our approach to training interpretable decision trees, which enhances the transparency of the decision-making process in language generation. Furthermore, we assess the ability of tree-based models to execute various logical reasoning tasks. Notably, tree ensembles built on top of transformer embeddings and trained on specific downstream tasks perform comparably to larger general models like InstructGPT Ouyang et al. (2022) and PaLM-540B Chowdhery et al. (2022), under the conditions of these particular tasks. ", "page_idx": 1}, {"type": "text", "text": "Our contribution can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We extend the application of ARDTs to language prediction tasks, adopting a novel approach that capitalizes on their inherent simplicity and interpretability. This aims to broaden the architectural diversity in language model development.   \n\u2022 Through theoretical analysis, we demonstrate that ARDTs can compute a broader array of complex functions than previously recognized, including the simulation of automata, Turing machines, and sparse circuits. These theoretical findings deepen our understanding of ARDTs\u2019 computational capabilities.   \n\u2022 Our experimental results offer empirical evidence that ARDTs are capable of generating coherent and grammatically correct text, perform well compared to more complex models like small Transformers, and demonstrate solid reasoning abilities. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Decision Trees. Tree based models have been widely used for solving different classification and regression tasks in machine learning (Navada et al., 2011). The ID3 algorithm was introduced by Quinlan (1986), and has been widely used for decision tree learning, along with the CART (Breiman et al., 1984; Lewis, 2000) algorithm. Decision tree ensembles, such as random forests (Breiman, 2001) and gradient boosted trees (Friedman, 2002), are also very popular. Despite continuous advancements in deep learning, decision tree ensembles still outperform neural network based models on tabular datasets (Shwartz-Ziv & Armon, 2022). Different from traditional decision trees, we use auto-regressive decision trees to perform language prediction tasks more efficiently. ", "page_idx": 2}, {"type": "text", "text": "Learning Theory for Decision Trees. There are a few theoretical works studying the power of decision trees in solving machine learning problems. The work of Brutzkus et al. (2020) shows that the ID3 algorithm can learn sparse functions in some setting. Kearns & Mansour (1996) show that decision trees are equivalent to boosting methods for amplifying the performance of weak learners on the distribution. Other works focus on other aspects of decision tree learnability (Rivest, 1987; Blum, 1992; Ehrenfeucht & Haussler, 1989; Bshouty & Burroughs, 2003). We note that from the approximation point of view, decision trees can be regarded as splines with free knots. For instance, piecewise constant hierarchical splines functions, similar to neural networks with threshold activation can also be seen as decision trees. Note that ReLU networks can be viewed as piecewise hierarchical linear splines (Anselmi et al., 2015; Yarotsky, 2016), and so decision trees can represent ReLU networks (see Aytekin (2022)), though possibly with an exponential number of parameters. We note that none of the works mentioned above studies the theory of auto-regressive decision trees, which is a novel contribution of our paper. ", "page_idx": 2}, {"type": "text", "text": "Decision Trees for Language. Despite gaining popularity in several fields of machine learning, tree based models are not widely used for language generation. Past works have utilized auto-regressive decision trees for time-series analysis (Meek et al., 2002), or use trees for basic language modeling (Potamianos & Jelinek, 1998). Decision trees were also used in parsing (Magerman, 1995; Heeman, 1999; Nallapati & Allan, 2002), modeling syntax (Filimonov, 2011) and language identification (Hakkinen & Tian, 2001). ", "page_idx": 2}, {"type": "text", "text": "3 Theory ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To explore the capabilities of ARDTs, we initially undertake theoretical studies demonstrating that using decision trees as next-token predictors enables ARDTs to process significantly more complex functions than \u201cstandard\u201d decision trees. Firstly, we define the theoretical setting of our analysis in Section 3.1. We then examine the various classes of functions that an ARDT can compute, as discussed in Sections 3.2, 3.3, and 3.4. Here, the computation involves the ARDT receiving an input sequence, such as a question, generating a series of intermediate tokens that describe the thought process, and finally producing the output token. Specifically, we demonstrate that functions computed by Automata, Turing machines, or sparse circuits can be emulated by an ARDT using these intermediate \u201cchain-of-thought\u201d computations. Additionally, we provide bounds on the size, depth, and runtime (measured by the number of intermediate tokens) required for ARDTs to simulate these classes of interest. Our findings affirm that ARDTs, by leveraging decision trees for next-token prediction, can handle far more complex functions than \u201cstandard\u201d decision trees. ", "page_idx": 2}, {"type": "text", "text": "Comment 1. The results in this section are representation results. That is, we study which functions can, in theory, be represented by auto-regressive decision trees. We do not provide any formal results on whether such functions can be learned from data. The question of how decision trees can be trained to produce \u201cchain-of-thought\u201d responses to input questions is beyond the scope of this work. ", "page_idx": 2}, {"type": "text", "text": "3.1 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We adapt the standard definition of a decision tree, as described by Quinlan (1986), to include modifications that allow for the processing of vector sequences of arbitrary lengths. Firstly, we establish a vocabulary $\\mathbb{D}$ , which serves as our token dictionary. Next, we define an input embedding $\\Psi:\\mathbb{D}\\rightarrow\\mathbb{R}^{d}$ . For any sequence of tokens $s\\in\\mathbb{D}^{n}$ , $\\Psi(s)\\in\\mathbb{R}^{n\\times d}$ represents the embedding applied individually to each token. The space comprising sequences of d-dimensional vectors is denoted by $\\mathcal{X}=\\mathbb{R}^{*\\times d}$ . Subsequently, we define a decision tree $\\tau$ that receives an input $x\\in\\mathcal{X}$ and outputs a token $y\\in\\mathbb{D}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In our experiments, detailed in Section 4, we apply a weighted-average operator to the word vectors of the sequence, where the average vectors are used as an input to the decision trees. For the theoretical analysis we study a different approach for using decision trees over vector sequences, where instead of averaging word vectors we \u201cconcatenate\u201d them. That is, the decision tree is applied to the $L$ most recent words, in a \u201csliding window\u201d fashion. We note that experimentally we observed that both the \u201csliding-window\u201d and the weighted-average approach produced similar results, and use the weighted-average technique in our experiments for computational reasons. ", "page_idx": 3}, {"type": "text", "text": "We start by defining a decision tree $\\tau$ that gets inputs of a fixed length $L$ , namely $\\mathcal{T}:\\mathbb{R}^{L\\times d}\\rightarrow\\mathbb{D}$ We refer to the value $L$ as the context length of $\\tau$ , and this value will correspond to the maximal length of a sequence that affects the computation of the tree. In this case, we treat the input $\\pmb{x}\\in\\mathbb{R}^{L\\times d}$ as a vector, and let $\\tau$ be a standard decision tree operating on vectors of size $L\\cdot d$ . Namely, $\\tau$ is defined by a binary tree, where each node corresponds to an input feature $x_{i,j}$ and some threshold $\\tau\\in\\mathbb{R}$ . Each leaf corresponds to some output token $y\\in\\mathbb{D}$ . The output of the tree $\\tau$ is computed by starting at the root, and for each internal node with feature $x_{i,j}$ and threshold $\\tau$ , moving to the right node if $x_{i,j}\\ge\\tau$ and otherwise moving to the left node. When reaching a leaf, we output the value $y\\in\\mathbb{D}$ corresponding to the leaf. The size of the tree $\\tau$ is the number of leaves in the tree, and its depth is the maximum length of a path from root to leaf. Note that the runtime of computing the output of $\\tau$ corresponds to the depth of the tree. ", "page_idx": 3}, {"type": "text", "text": "Now, given some tree over length- $L$ inputs $\\mathcal{T}:\\mathbb{R}^{L\\times d}\\rightarrow\\mathbb{D}$ , we apply $\\tau$ to an input of arbitrary length $x\\in\\mathcal{X}$ using the following simple rule: if $\\textbf{\\em x}$ has length shorter than $L$ , we pad it to length $L$ by prepending the input, adding additional padding $(\\mathrm{\\langlePAD\\rangle})$ ) tokens at the beginning; if $\\textbf{\\em x}$ is longer than $L$ , we apply $\\tau$ only to the last $L$ tokens in $\\textbf{\\em x}$ . This induces a decision tree with arbitrary length inputs $\\tau:\\mathcal{X}\\rightarrow\\mathbb{D}$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, we use the tree $\\tau$ as a next-token predictor function, applied over some input using autoregressive computation. That is, we define a sequence-to-sequence predictor $T^{\\mathrm{AR}}:\\mathbb{D}^{*}\\,\\stackrel{\\bullet}{\\to}\\,\\mathbb{D}^{*}$ induced from the tree $\\tau$ as follows: for every input $\\textbf{\\textit{s}}\\in\\mathbb{D}^{n}$ , recursively define $s_{n+i+1}\\;=$ $T(\\Psi(s_{1},\\ldots,s_{n+i}))$ , and let $T^{\\mathrm{AR}}(s_{1},\\ldots,s_{n})=\\overline{{(s_{n+1},s_{n+2},\\ldots)}}$ . We call $\\tau^{\\mathrm{{\\check{A}}R}}$ an auto-regressive decision tree (ARDT). ", "page_idx": 3}, {"type": "text", "text": "In the rest of this section, we will analyze the capacity of ARDTs to simulate some function classes. Following Malach (2023), we give the following definition: ", "page_idx": 3}, {"type": "table", "img_path": "nge5deRsEH/tmp/8e4cd34b9a0d836e4e1f5f918988951be9361b22572e943b9e4df6143b24ef30.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "In other words, we say that the tree $\\mathcal{T}^{\\mathrm{AR}}$ can compute the function $f$ , if given some input sequence $\\pmb{s}$ , it generates $T$ tokens followed by the correct output $f(s)$ . That is, we allow the tree to use $T$ intermediate tokens as \u201cchain-of-thought\u201d before outputting the correct answer. ", "page_idx": 3}, {"type": "text", "text": "3.2 Simulating Automata ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An automaton $\\boldsymbol{\\mathcal{A}}$ is defined over an alphabet $\\Sigma$ , using a set of states $Q$ , an initial state $q_{0}\\in Q$ and a transition function $\\delta:Q\\times\\Sigma\\rightarrow Q$ . We always assume that $|\\Sigma|\\ge2$ and $|Q|\\ge2$ . The automaton $\\boldsymbol{\\mathcal{A}}$ gets an input string $\\pmb{x}\\in\\Sigma^{*}$ , and computes an output state $\\pmb{\\mathcal{A}}(\\pmb{\\mathscr{x}})\\in\\mathcal{Q}$ by starting at state $q_{0}$ and at each iteration $i$ transitioning to the next state based on the $i$ -th token $x_{i}$ , namely $q_{i}=\\delta(q_{i-1},x_{i})$ . The automaton then returns the state reached at the final iteration. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{F}_{n}^{\\mathrm{Aut}}$ is the class of all functions computed by automata over strings of length $n$ . Namely, $\\mathcal{F}_{n}^{\\mathrm{Aut}}$ is the class of functions $f:\\Sigma^{n}\\to Q$ s.t. for all $f\\in\\mathcal{F}_{n}^{\\mathrm{Aut}}$ there exists an automaton $\\boldsymbol{\\mathcal{A}}$ s.t. ${\\ddot{A(\\mathbf{\\emx})}}=f(\\mathbf{\\emx})$ for all $\\pmb{x}\\in\\Sigma^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "The class of functions computed by Automata has been well-studied from the early days computer science theory (Hopcroft et al., 2001), and has various important connections to language problems. This class of functions is also interesting in the context of reasoning tasks for language modeling. For example, the Web-of-Lies and Navigate problems in the Big-Bench Hard dataset (Srivastava et al., 2023) can be solved by finite state Automata. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "nge5deRsEH/tmp/339e6aab2442912174ca4b8bd366e4e4012bf3c002e04e7b0149baa46b735c64.jpg", "table_caption": ["We show that ARDTs can simulate Automata: "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Note that ARDTs simulate Automata very efficiently: the total run-time of the ARDT guaranteed by Theorem 3 is $O(n\\log|\\mathbb{D}|)$ , which corresponds to the time it takes to read all the input bits. In this sense, no algorithm can simulate Automata significantly faster than ARDT. ", "page_idx": 4}, {"type": "text", "text": "In the proof, we construct an ARDT that, at every iteration $i$ , outputs the state of the Automaton at step $i$ (denoted $q_{i}$ ). The state at step $i+1$ is only a function of the $i$ -th state, given by the most recent token generated by the model; and the $i$ -th input, which is always given by looking back $n+1$ tokens. Therefore, a simple tree, applied as a sliding-window over the input, can compute the transition matrix to find the next state. The full proof is given in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Next, we show that the above result implies a separation between ARDTs and standard decision trees. Specifically, we show that if we use a decision-tree over the input to directly predict the final output of the Automata, without outputting intermediate states, then the size of the decision tree must be exponential in the length of the input: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. There exists some $f\\in\\mathcal{F}_{n}^{\\mathrm{Aut}}$ s.t. any decision tree that computes $f$ has size $\\geq\\Omega(2^{n})$ . ", "page_idx": 4}, {"type": "text", "text": "This shows that the fact that ARDTs can perform intermediate computations auto-regressively (e.g., perform chain-of-thought) significantly improves their efficiency1. To prove the result, we show that computing the parity of a sequence of bits (i.e., whether the number of bits is even or odd) requires a tree of exponential size, but can be easily computed by a simple 2-state Automaton. ", "page_idx": 4}, {"type": "text", "text": "Proof of Theorem 4. Consider the binary alphabet $\\Sigma=\\{0,1\\}$ and the state set $Q=\\{{\\mathrm{even}},{\\mathrm{odd}}\\}$ , with $|\\Sigma|=2$ and $|Q|=2$ . We define a function $f:\\Sigma^{n}\\to Q$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\mathbf{x})={\\left\\{\\begin{array}{l l}{\\mathrm{even}}&{{\\mathrm{if}}\\,\\sum x_{i}\\quad{\\mathrm{mod}}\\,2=0,}\\\\ {\\mathrm{odd}}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The function $f$ describes the parity of the sum of bits in $\\mathbf{x}$ and can be efficiently computed by an automaton that toggles between states even and odd upon encountering a 1. ", "page_idx": 4}, {"type": "text", "text": "Suppose a decision tree $\\tau$ computes $f$ . We claim that the size of $\\tau$ must be at least $2^{n}$ . Assume for contradiction that $\\tau$ has fewer than $2^{n}$ leaves. Since $\\tau$ is a decision tree, we assume that all its leaves are reachable by some input ${\\bf x}\\in\\{0,1\\}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Consider a leaf $l$ of $\\tau$ reached by some input $\\mathbf{x}$ , at a depth less than $n$ . This implies that there exists at least one bit index $j\\in[n]$ such that no decision node in $\\tau$ queries $x_{j}$ on the path to $l$ . Define $\\mathbf{x}^{\\prime}\\in\\{0,1\\}^{n}$ by flipping $x_{j}$ in $\\mathbf{x}$ , while keeping all other bits unchanged: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{i}^{\\prime}=\\left\\{{\\overset{x_{i}}{\\operatorname{\\mu}}}\\quad{\\mathrm{~if~}}i\\neq j,\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $\\mathbf{x}^{\\prime}$ alters $\\mathbf{x}$ only at the unqueried index $j$ , it follows the same path in $\\tau$ and reaches the same leaf $l$ . Therefore, $\\mathcal{T}(\\mathbf{x})\\overset{\\cdot}{=}\\mathcal{T}(\\mathbf{x}^{\\prime})$ . However, the definition of $f$ guarantees $f(\\mathbf{x})\\neq f(\\mathbf{x}^{\\prime})$ as their parities are different, leading to a contradiction. Thus $\\tau$ cannot compute $f$ with fewer than $2^{n}$ leaves. ", "page_idx": 4}, {"type": "text", "text": "3.3 Simulating Turing Machines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A Turing machine $\\mathcal{M}$ is defined over an alphabet $\\Sigma$ , using a space set $Q$ , initial state $q_{0}\\in Q$ and transition function $\\delta:Q\\times\\Sigma\\rightarrow Q\\times\\Sigma\\times\\hat{\\{}\\langle\\mathrm{[LEFT)}\\,,\\langle\\mathrm{RI\\bar{G}H\\hat{T}\\rangle}\\}$ . The Turing machine has a tape, where each cell contains a symbol from $\\Sigma$ . The head of the Turing machine is initialized at the leftmost cell on the tape in state $q_{0}\\in Q$ . At each iteration of the machine, it reads the symbol $s\\in\\Sigma$ and given the head state $q\\in Q$ uses $\\delta(q,s)$ to determined the new state of the head, the symbol to write under the head, and whether to move the head left or right on the tape. ", "page_idx": 5}, {"type": "text", "text": "In our setting, we consider Turing machines with fixed memory $M$ , i.e. Turing machines with access to a tape with $M$ cells. In particular, this means that the Turing machine $\\mathcal{M}$ operate on inputs with $<M$ tokens. At the initial step, the input is written on the tape. If the input size is shorter than $M$ , we add empty tokens $\\{\\varnothing\\}\\in{\\bar{\\Sigma}}$ after the input sequence. We consider Turing machines with fixed runtime $T$ , namely we let the machine run for $T$ iterations and then halt it. The output of the machine is the rightmost symbol on the tape after $T$ iterations. So, we define $\\mathcal{M}:\\Sigma^{M}\\xrightarrow{}\\dot{\\Sigma}$ to be the function computed by the machine after $T$ steps. We denote by FMTu,rTingthe class of functions computed by Turing machines with memory of size $M$ and runtime $T$ . ", "page_idx": 5}, {"type": "text", "text": "Comment 5. Turing machines are typically defined with infinite number of tape cells, and are allowed to run arbitrarily long before halting. However, for every given input length, any computable function always uses a fixed memory and run-time (which depend on the input length). ", "page_idx": 5}, {"type": "text", "text": "We now show any Turing machine with fixed memory and run-time can be simulated by an ARDT: ", "page_idx": 5}, {"type": "image", "img_path": "nge5deRsEH/tmp/9fd3d334fd2cda69eed0ebe79c314f81faa3a90ec252894eee00d92003e0ff6c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To prove the result, we show that an ARDT can compute the state of the Turing machine at each iteration. Specifically, we encode the state of the machine as a sequence of tokens from $\\mathbb{D}$ , where we put a token $q\\in Q\\subseteq\\mathbb{D}$ indicating the state of the head before the token that the head reads. This way, the transition between states is a function that only depends locally on the tokens surrounding the position of the head, where all other (non-state) tokens can be copied as-is from one state to the next. Similarly to the proof in the previous section, this operation can be realized by a small sliding-window tree. The full proof is given in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "3.4 Simulating Sparse Circuits ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A circuit $\\mathcal{C}$ over some alphabet $\\Sigma$ is defined as a directed-acyclic-graph (DAG), with $n$ input nodes and one output node. Each internal (non-input) node with $k$ incoming edges corresponds to some function $g:\\^{\\bullet}\\Sigma^{k}\\rightarrow\\Sigma$ computed by the node over its incoming inputs. For some input $\\pmb{x}\\in\\Sigma^{n}$ , the output of the circuit $\\mathcal{C}$ is the value of the output node, when setting the input nodes of $\\mathcal{C}$ to $x_{1},\\ldots,x_{n}$ The size of the circuit $\\mathcal{C}$ is the number of nodes in the computational graph. We say that $\\mathcal{C}$ is $k$ -sparse, if the maximal in-degree of every node in the graph is k. Denote by FNCi,rkc uitthe class of functions computed by $k$ -sparse circuits of size $N$ . ", "page_idx": 5}, {"type": "text", "text": "We note that sparse circuits are an extension of sparse Boolean circuits, and so can represent Turing machines with bounded memory (Arora & Barak, 2009). In this sense, this class is \u201cequivalent\u201d to the class of functions computed by Turing machines. However, some functions may be more efficient to compute using sparse circuits, and so it is interesting to understand how ARDTs can directly simulate sparse circuits, as demonstrated in the following theorem: ", "page_idx": 5}, {"type": "image", "img_path": "nge5deRsEH/tmp/f0ebb622f5dbbd7ac3b187e47eed49fef4d81d28f7bec8a4da62cebdc82df72c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "2We introduce a new separator token $\\langle\\mathrm{SEP}\\rangle$ , that is used during the generation of the ARDT, but is not part of the alphabet or state set of the Turing machine. ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem 7. Consider a $k$ -sparse circuit $\\mathcal{C}$ with $N$ total nodes, where $N-n$ are internal nodes. Let $g_{1},\\dotsc,g_{N-n}\\ :\\ \\Sigma^{k}\\ \\stackrel{}{\\rightarrow}^{^{}}\\Sigma^{}$ be the functions computed at the internal nodes, ordered topologically so that each function depends only on the inputs or the results of preceding nodes. Let $g_{N-n}$ denote the function computed by the output node. ", "page_idx": 6}, {"type": "text", "text": "Define $f_{i}:\\Sigma^{n+i-1}\\to\\Sigma$ as the output of the $i$ -th node in this ordering, considering all inputs and outputs from previous nodes. Each $f_{i}$ is effectively a $k$ -Junta. By Lemma 10, there exists a decision tree $\\mathcal{T}_{i}$ of size $O\\left(\\lvert\\mathbb{D}\\rvert^{k}\\right)$ such that $\\mathcal{T}_{i}(\\Psi(\\pmb{x}))=f_{i}(\\pmb{x})$ for all $\\pmb{x}\\in\\dot{\\Sigma}^{n+i-1}$ . ", "page_idx": 6}, {"type": "text", "text": "To accommodate inputs $\\pmb{x}\\in\\Sigma^{N}$ , we modify each tree $\\mathcal{T}_{i}$ to ignore the first $N-n-i+1$ inputs.   \nThis adaptation does not affect the size of the tree. ", "page_idx": 6}, {"type": "text", "text": "Let $z=\\Psi(\\langle\\mathrm{PAD}\\rangle)\\in\\{0,1\\}^{d}$ . Construct a tree as follows: begin with the rightmost branch of the tree, using functions $h_{1,1},\\ldots,h_{1,d},\\ldots,h_{N-n,1},\\ldots,h_{N-n,d}$ . For each node $i\\in[N-n]$ and each bit $j\\in[d]$ , define: ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{i,j}=\\left\\{1\\{\\Psi(\\pmb{x})_{i,j}\\geq1\\}\\quad\\mathrm{if}\\,\\,z_{j}=1,\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Attach tree $\\tau_{N-n-i+1}$ at each left node $(i,j)$ . ", "page_idx": 6}, {"type": "text", "text": "Observe that during the $i$ -th iteration, the string begins with $N-n-i\\langle\\mathrm{PAD}\\rangle$ tokens, allowing $\\mathcal{T}_{i}$ to process the pertinent part of the input. After $N-n$ iterations, the constructed tree calculates the output token as specified by $\\mathcal{C}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we experimentally validate the capabilities of ARDTs as demonstrated in the previous section and prove their language modeling potential. In Section 4.2, we first train a model based on ARDTs and test its ability to continue stories on Tinystories Eldan & Li (2023), which involves extending narratives similar to a finite state automaton. ARDTs generate coherent text that builds on existing stories, also requiring the interpretation of complex contexts and emotions. This showcases the effectiveness of sparse circuits in managing significant yet limited inputs. ", "page_idx": 6}, {"type": "text", "text": "Additionally, in Section 4.3, we assess the model\u2019s reasoning abilities on the Big-Bench-Hard Suzgun et al. (2022) dataset, where tasks often involve evaluating the truthfulness of propositions, effectively emulating a Turing machine as it processes inputs to determine a definitive outcome (true or false). ", "page_idx": 6}, {"type": "text", "text": "4.1 Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To align with the theory section, we designed our experiments to closely mirror the theoretical settings as closely as possible. We here provide a detailed description of our implementation of Auto-regressive Decision Trees (ARDTs) for next-token prediction tasks. Our objective is to utilize ARDTs as a language model that receives a sequence of input tokens $x_{1},\\ldots,x_{n}$ and predicts the subsequent token $x_{n+1}$ . Initially, we employ a Word2Vec embedding Mikolov et al. (2013), denoted by $\\Psi$ , to convert the sequence tokens into word embeddings $\\Psi(x_{1}),\\ldots,\\Psi(x_{n}),\\Psi(x_{n+1})\\in\\mathbb{R}^{100}$ . We then compute a weighted average of these embeddings with exponential decay, prioritizing the most recent tokens: $\\begin{array}{r}{\\overline{{\\pmb{v}}}=\\sum_{i=1}^{n}\\alpha^{n-i+1}\\Psi(\\boldsymbol{x}_{i})}\\end{array}$ , where $\\alpha\\in(0,1)$ . Using XGBoost Chen & Guestrin (2016), we train an ense mble of decision trees, $\\tau$ , which takes the input vector $\\overline{{\\pmb{v}}}$ and predicts the embedding of the next token $\\Psi(x_{n+1})$ , aiming to minimize the mean squared error (MSE) loss. We train this model using sequences of varying lengths sampled from our dataset. During inference, the model generates text auto-regressively. At each step, it receives the current sequence $\\overline{{\\pmb{v}}}$ , outputs the predicted embedding of the next token $\\hat{\\pmb u}=\\mathcal{T}(\\overline{{\\pmb v}})$ , and identifies the token whose embedding is closest to this prediction, i.e., $\\hat{x}=\\arg\\operatorname*{min}_{x}\\|\\Psi(x)-\\hat{\\pmb u}\\|_{2}$ . This token is then used as the next token in the sequence. The input vector is updated with the new token using ${\\overline{{\\pmb{v}}}}\\,\\leftarrow\\,\\alpha{\\overline{{\\pmb{v}}}}+\\Psi({\\hat{x}})$ , and the process repeats for the next iteration. Figure 2 illustrates the training and inference pipeline. ", "page_idx": 6}, {"type": "text", "text": "Comment 8. We note that the setting described above deviates from the theory setting. 1) While the theoretical analysis focuses on the representational power of a single auto-regressive decision tree, the experiments utilize ensembles of decision trees. Notably, tree ensembles are more expressive, which suggests that our positive findings should also extend to these ensembles. 2) For simplicity, our theoretical study examines trees that generate a single output token in each iteration, rather than producing a word vector, which is the approach used in the experiments. 3) The decision trees discussed theoretically operate on concatenated token vectors within a sliding window, in contrast to the use of vector averages in the experimental setting. ", "page_idx": 6}, {"type": "image", "img_path": "nge5deRsEH/tmp/a1c93b02fb851f289f4d16a7fb2435055f1668ac4c25ab31ed078dc66fa26a54.jpg", "img_caption": ["Figure 2: The Pipeline of Our Method. (a) Training. First, we employ a Word2Vec model to convert words into embeddings. Next, we utilize a sliding window approach to construct a dataset for training decision trees. Within this window, we performed a weighted average calculation, and the following token after the window was used as the label. (b) Inference. We use our trained Decision Trees for the purpose of next-token prediction. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 The Ability to Generate Coherent Stories ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We test ARDTs\u2019 ability to generate stories with the TinyStories Eldan & Li (2023) dataset, which is a widely-used high-quality synthetic dataset of short stories that contain words that a 3 to 4-year-old child can understand, generated by GPT-3.5 and GPT-4. Details can be found in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "For experiments conducted on TinyStories, we strictly follow Eldan & Li (2023) and employ the multidimensional score provided by GPT-4, as detailed in Appendix B.5. ", "page_idx": 7}, {"type": "text", "text": "For baselines to compare with ARDTs, we selected several Transformer-based models. These include two small Transformers trained on the TinyStories dataset (TinyStories-1M and TinyStories33M Eldan & Li (2023)), as well as GPT-4 OpenAI et al. (2023), to illustrate the performance differences between non-neural network methods and the Transformer architecture. ", "page_idx": 7}, {"type": "text", "text": "For our evaluation, we provide the models with 100 story beginnings (refer to examples in Appendix B.4), each consisting of fewer than 6 words, generated by GPT-4. We use these beginnings as inputs to the model, allowing the it to perform next token prediction, ultimately generating outputs of 20 words. For the ground truth row in Table 1, we grade complete stories from the dataset. ", "page_idx": 7}, {"type": "text", "text": "As shown in Table 1, ARDTs achieved performance comparable to GPT-4 and TinyStories-33M on four metrics: grammar, creativity, consistency, and plot. Our model outperforms TinyStories1M, a Transformer-based model with 1M parameters, despite being smaller in size. These results demonstrate that although tree-based models are generally considered inferior to large neural networks, surprisingly, they can compete with small Transformers when trained on the TinyStories dataset. ", "page_idx": 7}, {"type": "text", "text": "4.3 Evaluating ARDTs in Language Reasoning Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now explore the potential of using decision trees for logical reasoning tasks using the Big-BenchHard dataset. The Big-Bench-Hard dataset, detailed in Appendix B.2, contains 23 challenging reasoning tasks from the BIG-Bench benchmark. We selected four representative reasoning tasks for evaluation, with examples provided in Appendix B.2. ", "page_idx": 7}, {"type": "table", "img_path": "nge5deRsEH/tmp/de607b2e820b09b7e88c307c8cc786ef4be13e7db138065f19db1f61e125c341.jpg", "table_caption": ["Table 1: Experiment Results on TinyStories: The results show that an auto-regressive tree can achieve better performance as the GPT-Neo architecture and exhibit competitive performance compared to both GPT-4 and TinyStories-33M. "], "table_footnote": ["\u2217For our decision trees, we report the total number of tree nodes in the ensemble as the parameter count. $^\\dagger$ To minimize the impact of inconsistency on our results and enhance the robustness of our evaluation metrics, we calculated the average scores from ten assessments for each of the 100 stories. Each story was evaluated ten times using the same prompt provided to GPT-4. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Each task involves training a separate decision tree ensemble. These ensembles utilize a weighted average of input word embeddings, as described in Section 4.1, using the word embedding layer from a pre-trained GPT-2 model trained on WebText. Each model is trained with 200 examples and tested on 50 examples. We also experiment with decision trees trained on top of a pre-trained GPT-2 Transformer model, where the output vectors from GPT-2 serve as input features for the decision trees, combining GPT-2\u2019s advanced language understanding with the analytical capabilities of decision trees. ", "page_idx": 8}, {"type": "text", "text": "For establishing baselines, we follow the methodology of Suzgun et al. (2022) and use accuracy as the metric. InstructGPT, Codex, and PaLM 540B are used as baselines. ", "page_idx": 8}, {"type": "text", "text": "As presented in Table 2, our model demonstrates substantial effectiveness in reasoning tasks, with performance comparable to state-of-the-art methods. For instance, we observe improvements of $7.4\\%$ in Boolean Expression tasks, $2\\%$ in Navigate tasks, and $7.8\\%$ in Sports Understanding tasks. Moreover, we find that further enhancements are possible by integrating decision trees with the GPT-2 Transformer, underscoring the significant impact of word embeddings on performance. However, his paper focuses on highlighting the potential of the ARDTs architecture, not word embeddings. Our results show that the ARDTs model has strong reasoning abilities. ", "page_idx": 8}, {"type": "table", "img_path": "nge5deRsEH/tmp/a8d1e89c3845cc57486fef8a9d362f9df4edf1d31dff05c0989426126053b42a.jpg", "table_caption": ["Table 2: Experimental Results on BIG-Bench-Hard. Lin: Linear Embedding; GPT: GPT-2 Embedding. The results demonstrate that ARDTs possess good reasoning capabilities. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The findings in this paper demonstrate that tree-based models have potential in language generation. Although they do not yet match the performance of large language models, they possess certain advantages that make them valuable for studying the emergence of intelligence on a smaller scale. Decision trees are easier to interpret (see Appendix C for more on interpretability using ARDTs), simpler to understand and analyze mathematically, and fast to train. Moreover, unlike standard neural networks, the inference time for decision trees typically increases logarithmically with their size: a tree with depth $d$ can have $2^{d}$ nodes but only requires traversing $O(d)$ nodes per input. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "This paper serves as a preliminary exploration into using ARDTs for language modeling tasks. We aim to inspire further research that integrates tree-based models into current language model pipelines, leveraging their unique strengths to enhance language generation capabilities. We believe incorporating tree-structured models into hybrid models with Transformers could be a promising direction for future research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Anselmi, F., Rosasco, L., Tan, C., and Poggio, T. Deep convolutional networks are hierarchical kernel machines. Center for Brains, Minds and Machines (CBMM) Memo No. 035, 2015. ", "page_idx": 9}, {"type": "text", "text": "Arora, S. and Barak, B. Computational complexity: a modern approach. Cambridge University Press, 2009. ", "page_idx": 9}, {"type": "text", "text": "Aytekin, C. Neural networks are decision trees, 2022. ", "page_idx": 9}, {"type": "text", "text": "Bird, S., Klein, E., and Loper, E. Natural language processing with Python: analyzing text with the natural language toolkit. \u201d O\u2019Reilly Media, Inc.\u201d, 2009. ", "page_idx": 9}, {"type": "text", "text": "Blum, A. Rank-r decision trees are a subclass of r-decision lists. Information Processing Letters, 42 (4):183\u2013185, 1992. ", "page_idx": 9}, {"type": "text", "text": "Breiman, L. Random forests. Machine learning, 45:5\u201332, 2001. ", "page_idx": 9}, {"type": "text", "text": "Breiman, L., Friedman, J., Olshen, R., and Stone, C. Classification and Regression Trees. Wadsworth Publishing, 1984. ", "page_idx": 9}, {"type": "text", "text": "Brutzkus, A., Daniely, A., and Malach, E. Id3 learns juntas for smoothed product distributions. In Conference on Learning Theory, pp. 902\u2013915. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "Bshouty, N. H. and Burroughs, L. On the proper learning of axis-parallel concepts. The Journal of Machine Learning Research, 4:157\u2013176, 2003. ", "page_idx": 9}, {"type": "text", "text": "Chen, T. and Guestrin, C. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pp. 785\u2013 794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785. ", "page_idx": 9}, {"type": "text", "text": "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022. ", "page_idx": 9}, {"type": "text", "text": "De, S., Smith, S. L., Fernando, A., Botev, A., Cristian-Muraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., Desjardins, G., Doucet, A., Budden, D., Teh, Y. W., Pascanu, R., Freitas, N. D., and Gulcehre, C. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. ", "page_idx": 9}, {"type": "text", "text": "Ehrenfeucht, A. and Haussler, D. Learning decision trees from random examples. Information and Computation, 82(3):231\u2013246, 1989. ", "page_idx": 9}, {"type": "text", "text": "Eldan, R. and Li, Y. Tinystories: How small can language models be and still speak coherent english?, 2023. ", "page_idx": 9}, {"type": "text", "text": "Filimonov, D. Decision tree-based syntactic language modeling. University of Maryland, College Park, 2011. ", "page_idx": 9}, {"type": "text", "text": "Friedman, J. H. Stochastic gradient boosting. Computational statistics & data analysis, 38(4): 367\u2013378, 2002.   \nGrinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507\u2013520, 2022.   \nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.   \nHakkinen, J. and Tian, J. N-gram and decision tree based language identification for written words. In IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU\u201901., pp. 335\u2013338. IEEE, 2001.   \nHeeman, P. A. Pos tags and decision trees for language modeling. In 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, 1999.   \nHopcroft, J. E., Motwani, R., and Ullman, J. D. Introduction to automata theory, languages, and computation. Acm Sigact News, 32(1):60\u201365, 2001.   \nKearns, M. and Mansour, Y. On the boosting ability of top-down decision tree learning algorithms. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pp. 459\u2013468, 1996.   \nLewis, R. J. An introduction to classification and regression tree (cart) analysis. In Annual meeting of the society for academic emergency medicine in San Francisco, California, volume 14. Citeseer, 2000.   \nMa, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L. Mega: Moving average equipped gated attention, 2023.   \nMagerman, D. M. Statistical decision-tree models for parsing. arXiv preprint cmp-lg/9504030, 1995.   \nMalach, E. Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979, 2023.   \nMeek, C., Chickering, D. M., and Heckerman, D. Autoregressive tree models for time-series analysis. In Proceedings of the 2002 SIAM International Conference on Data Mining, pp. 229\u2013244. SIAM, 2002.   \nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word representations in vector space, 2013.   \nNallapati, R. and Allan, J. Capturing term dependencies using a sentence tree based language model. In Proceedings of CIKM, volume 2, pp. 383\u2013390. Citeseer, 2002.   \nNavada, A., Ansari, A. N., Patil, S., and Sonkamble, B. A. Overview of use of decision tree algorithms in machine learning. In 2011 IEEE control and system graduate research colloquium, pp. 37\u201342. IEEE, 2011.   \nOpenAI, :, Achiam, J., and Steven Adler, e. a. Gpt-4 technical report, 2023.   \nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.   \nPoggio, T. Compositional sparsity: a framework for ml. Center for Brains, Minds and Machines (CBMM) Memo No. 138, 2022.   \nPotamianos, G. and Jelinek, F. A study of n-gram and decision tree letter language modeling methods. Speech Communication, 24(3):171\u2013192, 1998.   \nQuinlan, J. R. Induction of decision trees. Machine learning, 1:81\u2013106, 1986. ", "page_idx": 10}, {"type": "text", "text": "Rivest, R. L. Learning decision lists. Machine learning, 2:229\u2013246, 1987. ", "page_idx": 10}, {"type": "text", "text": "Shwartz-Ziv, R. and Armon, A. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022. ", "page_idx": 11}, {"type": "text", "text": "Srivastava, A., Rastogi, A., and Abhishek Rao, e. a. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.   \nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models, 2023.   \nSuzgun, M., Scales, N., Scha\u00a8rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. ", "page_idx": 11}, {"type": "text", "text": "van der Maaten, L. Barnes-hut-sne, 2013. ", "page_idx": 11}, {"type": "text", "text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023.   \nWong, L., Grand, G., Lew, A. K., Goodman, N. D., Mansinghka, V. K., Andreas, J., and Tenenbaum, J. B. From word models to world models: Translating from natural language to the probabilistic language of thought, 2023.   \nYarotsky, D. Error bounds for approximations with deep relu networks. CoRR, abs/1610.01145, 2016. URL http://arxiv.org/abs/1610.01145. ", "page_idx": 11}, {"type": "text", "text": "A Additional Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For any $\\mathbb{D}$ , let $d=\\lceil\\log(\\lvert\\mathbb{D}\\rvert)\\rceil+1$ and let $\\Psi:\\mathbb{D}\\rightarrow\\{0,1\\}^{d}$ be a one-to-one mapping of tokens to Boolean vectors, s.t. $\\Psi_{1}(s)=1$ for all $s\\in\\mathbb{D}$ . ", "page_idx": 12}, {"type": "text", "text": "Definition 9. A function $f:\\mathbb{D}^{L}\\,\\rightarrow\\,\\mathbb{D}$ is called $k$ -Junta if there exists a set of separate indexes $i_{1},\\ldots,i_{k}\\in[L]$ and function $g:\\mathbb{D}^{k}\\rightarrow\\mathbb{D}$ s.t. $f(\\pmb{x})=g(x_{i_{1}},\\ldots,x_{i_{k}})$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma 10. For every $k$ -Junta $f:\\mathbb{D}^{L}\\,\\rightarrow\\,\\mathbb{D}_{}$ , there exists a tree $\\tau$ of size $O\\left(\\left|\\mathbb{D}\\right|^{k}\\right)$ and depth $O(k\\log|\\mathbb{D}|)$ s.t. $\\mathcal T(\\Psi(\\pmb{x}))=f(\\pmb{x})$ for all $\\mathbf{\\Delta}x\\in\\mathbb{D}^{L}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\tau$ the perfect binary tree of depth $d k$ , where each level of the tree corresponds to a pair $(j,l)\\in[k]\\times[d]$ , and all the nodes at the level implement the condition $\\Psi_{l}(x_{i_{j}})\\geq1$ . Observe that in this construction, each leaf correspond to a specific choice of values for $\\Psi(x_{i_{1}}),\\ldots,\\Psi(x_{i_{k}})$ , and we can set its output to be $g(x_{i_{1}},\\ldots,x_{i_{k}})$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 3. Let $\\boldsymbol{\\mathcal{A}}$ be some automaton, defined by transition function $\\delta:Q\\!\\times\\!\\Sigma\\rightarrow Q$ , and we can arbitrarily extend it to $\\delta:\\mathbb{D}^{2}\\rightarrow\\mathbb{D}$ s.t. $\\delta(x,\\langle\\mathrm{PAD}\\rangle)\\overset{\\cdot}{=}q_{0}$ for all $x\\in\\mathbb{D}$ . Then, from Theorem 10 there exists some tree $\\tau$ of size $O(\\left|\\mathbb{D}\\right|^{2})$ s.t. for all $\\mathbf{\\Delta}x\\in\\mathbb{D}^{L}$ it holds that $\\tau(\\Psi(\\pmb{x}))=\\delta(x_{L},x_{L-n})$ . ", "page_idx": 12}, {"type": "text", "text": "We prove by induction that for all $i\\in[n]$ it holds that $T_{i}^{\\mathrm{AR}}({\\pmb x})=q_{i}$ , where $q_{i}$ is the state of the automaton $\\boldsymbol{\\mathcal{A}}$ at iteration $i$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 Let $\\begin{array}{r l r l}{z}&{{}}&{\\in}&{{}\\quad\\mathbb{R}^{L,d}}\\end{array}$ be the padded output of $\\Psi(x)$ , i.e. $\\begin{array}{r l r l}{z}&{{}\\quad}&{}&{{}=}\\end{array}$ $[\\Psi(\\langle\\mathrm{PAD}\\rangle),...\\,,\\Psi(\\langle\\mathrm{PAD}\\rangle),\\Psi(x_{1}),\\dot{..}.\\,,\\Psi(x_{n})]$ . Note that since $x_{L-n}\\;\\;=\\;\\;\\langle\\mathrm{PAD}\\rangle$ we have $T_{1}^{\\mathrm{AR}}(\\pmb{x})=T(\\pmb{z})=\\delta(x_{L},\\langle\\mathrm{PAD}\\rangle)=q_{1}$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 Assume that $T_{1:i-1}^{\\mathrm{AR}}(\\pmb{x})=\\left(q_{1},\\ldots,q_{i-1}\\right)$ . Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{i}^{\\mathrm{AR}}(x)=\\mathcal{T}(\\Psi(\\langle\\mathrm{PAD}\\rangle\\,,\\dots,\\langle\\mathrm{PAD}\\rangle\\,,x_{1},\\dots,x_{n},q_{1},\\dots,q_{i-1}))}\\\\ &{\\qquad\\qquad=\\delta(q_{i-1},x_{i})=q_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, the required follows. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem $6$ . We encode the state of the Turing machine by a string $\\pmb{\\mathscr{s}}\\in\\mathbb{D}^{M+1}$ as follows: if the head is in state $q\\in Q$ and at position $i\\in[M]$ , and the memory is $m_{1},\\ldots,m_{M}\\in\\Sigma.$ , we set $\\pmb{s}=(m_{1},\\pmb{\\dots},m_{i-1},q,m_{i},\\pmb{\\dots},m_{M})$ . That is, we add a token indicating the state of the head before the cell where the head is located. Let $\\delta:Q\\times\\Sigma\\rightarrow Q\\times\\Sigma\\times\\{\\langle\\mathrm{LEFT}\\rangle\\;,\\langle\\mathrm{RIGHT}\\rangle\\}$ be the transition function of the Turing machine. We define the following function $g:\\mathbb{D}^{4}\\to\\mathbb{D}^{4}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\ng(s)=\\left\\{\\begin{array}{l l}{x_{2}}&{\\mathrm{if~}x_{1},x_{2},x_{3}\\notin Q}\\\\ {q}&{\\mathrm{if~}x_{1}\\in Q\\mathrm{~and~}\\delta(x_{1},x_{2})=(q,\\alpha,\\langle\\mathrm{RIGHT}\\rangle)}\\\\ {\\alpha}&{\\mathrm{if~}x_{1}\\in Q\\mathrm{~and~}\\delta(x_{1},x_{2})=(q,\\alpha,\\langle\\mathrm{LEFT}\\rangle)}\\\\ {\\alpha}&{\\mathrm{if~}x_{2}\\in Q\\mathrm{~and~}\\delta(x_{2},x_{3})=(q,\\alpha,\\langle\\mathrm{RIGHT}\\rangle)}\\\\ {x_{1}}&{\\mathrm{if~}x_{2}\\in Q\\mathrm{~and~}\\delta(x_{2},x_{3})=(q,\\alpha,\\langle\\mathrm{LEFT}\\rangle)}\\\\ {x_{2}}&{\\mathrm{if~}x_{3}\\in Q\\mathrm{~and~}\\delta(x_{3},x_{4})=(q,\\alpha,\\langle\\mathrm{RIGHT}\\rangle)}\\\\ {q}&{\\mathrm{if~}x_{3}\\in Q\\mathrm{~and~}\\delta(x_{3},x_{4})=(q,\\alpha,\\langle\\mathrm{LEFT}\\rangle)}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Observe that the function $\\boldsymbol{f}:\\mathbb{D}^{M+1}\\to\\mathbb{D}^{M+1}$ s.t. $f_{i}(\\pmb{\\mathscr{s}})=g\\big(\\mathscr{s}_{i-1},\\mathscr{s}_{i},\\mathscr{s}_{i+1},\\mathscr{s}_{i+2}\\big)$ exactly defines the transition between the encoded states of the Turing machine. Namely, if the state of the machine at iteration $i$ is $\\pmb{s}$ , then the state at iteration $i+1$ is $f(s)$ . We slightly modify $g$ to handle the generation of the first iteration, as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\tilde{g}(s)=\\left\\{\\begin{array}{l l}{\\langle\\mathrm{SEP}\\rangle}&{x_{1}=\\langle\\mathrm{PAD}\\rangle\\mathrm{~and~}x_{2}=\\langle\\mathrm{PAD}\\rangle\\mathrm{~and~}x_{3}=\\langle\\mathrm{PAD}\\rangle}\\\\ {q_{0}}&{x_{1}=\\langle\\mathrm{PAD}\\rangle\\mathrm{~and~}x_{2}=\\langle\\mathrm{PAD}\\rangle\\mathrm{~and~}x_{3}\\neq\\langle\\mathrm{PAD}\\rangle}\\\\ {\\langle\\mathrm{SEP}\\rangle}&{x_{2}=\\langle\\mathrm{SEP}\\rangle}\\\\ {g(s)}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now, from Lemma 10 there exists a tree $\\tau$ of size $O(|\\mathbb{D}|^{4})$ s.t. $\\mathcal{T}(\\Psi(\\pmb{x}))=\\tilde{g}(x_{1},x_{2},x_{3},x_{4})$ . ", "page_idx": 13}, {"type": "text", "text": "Let $\\pmb{s}_{1},\\dots,\\pmb{s}_{T}\\in\\mathbb{D}^{M+1}$ the encodings of the state of the Turing machine at iterations $1,\\cdot\\cdot\\cdot,T$ . Let $\\mathbf{\\Delta}x\\in\\mathbb{D}^{L}$ be the encoding of the input, staring with $\\langle\\mathrm{PAD}\\rangle$ tokens, followed by one $\\langle{\\mathrm{BOS}}\\rangle$ token and the input string. Denote the output of the ARDT $\\mathcal{T}^{\\mathrm{AR}}$ after $T\\cdot(M+2)$ given the input $\\textbf{\\em x}$ , where we split the output into chunks of size $M+2$ by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{T}^{\\mathrm{AR}}(\\pmb{x})=(\\boldsymbol{z}_{1},\\dots,\\boldsymbol{z}_{T})\\in\\mathbb{D}^{T\\cdot(M+2)},\\ \\boldsymbol{z}_{i}\\in\\mathbb{D}^{M+2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Claim: For all $i\\in[T]$ , it holds that $z_{i}=(\\left<\\mathrm{SEP}\\right>,\\pmb{s}_{i})$ . ", "page_idx": 13}, {"type": "text", "text": "Prove: We prove by induction on $i$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 For $i=1$ , notice that the input begins with 3 $\\langle{\\mathrm{PAD}}\\rangle$ tokens, followed by the input tokens $x_{1},\\ldots,x_{M}$ , and therefore by definition of $\\tilde{g}$ we get $z_{1}\\,=\\,(\\langle\\mathrm{SEP}\\rangle\\,,q_{0},x_{1},...\\,,x_{M})\\,=$ $\\left(\\langle\\mathrm{SEP}\\rangle\\,,s_{1}\\right)$ .   \n\u2022 Assume the required holds for $i$ . First, observe that $z_{i+1,1}=T(\\Psi(s_{i-1,M+1},\\langle\\mathrm{SEP}\\rangle\\,,s_{i,1},\\dots,s_{i,M+1}))=\\langle\\mathrm{SEP}\\rangle$ Now, assume that $z_{i+1,1:j}=(\\left<\\mathrm{SEP}\\right>,s_{i+1,1},\\ldots,s_{i+1,j-1})$ . Therefore $\\begin{array}{r l}&{z_{i+1,j+1}={\\mathcal{T}}(\\Psi(s_{i,j-1},s_{i,j},s_{i,j+1},\\ldots,s_{i,M+1},\\langle\\mathrm{SEP}\\rangle\\,,s_{i+1,1},\\ldots,s_{i+1,j-1}))}\\\\ &{\\qquad\\qquad=g(s_{i,j-1},s_{i,j},s_{i,j+1},s_{i,j+2})=s_{i+1,j}}\\end{array}$ and by induction we get $z_{i+1}=\\left(\\left\\langle\\mathrm{SEP}\\right\\rangle,s_{i+1}\\right)$ ", "page_idx": 13}, {"type": "text", "text": "Therefore, $\\tau$ outputs the final token of iteration $T$ after $T(M+2)$ steps of auto-regression, which proves the theorem. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B Additional Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Hardware & Computational Cost ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our experiments were conducted on a single NVIDIA A100 GPU. For the Tiny Stories experiments, the training process took approximately 1 hour, and it required about 1 second to generate 20 words during the inference phase. ", "page_idx": 13}, {"type": "text", "text": "B.2 Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Tiny Stories. As shown in Tab. 3, the training and validation datasets of Tiny Stories contain 147,273 and 21,990 stories, respectively. We use NLTK Bird et al. (2009) as the tokenizer to obtain 420,351,665 and 4,329,963 tokens from the training dataset. In the training dataset and validation dataset, the number of words in the vocabulary is 27,455 and 11,273, respectively. ", "page_idx": 13}, {"type": "text", "text": "BIG-Bench-Hard is a dataset contains the selection of 23 difficult tasks from the BIG-Bench. These tasks are identified by their resistance to being outperformed by prior language model evaluations when compared to the average human evaluator. The BIG-Bench-Hard tasks often demand complex, multi-step reasoning, and the use of few-shot prompting without CoT, as previously utilized in BIG-Bench evaluations Srivastava et al. (2023), significantly underrepresents the true potential and performance of language models. ", "page_idx": 13}, {"type": "text", "text": "Four representative reasoning tasks we select for evaluate our ARDTs: ", "page_idx": 13}, {"type": "text", "text": "(1) Boolean Expressions. Example: not (True) and (True). Answer: False. ", "page_idx": 13}, {"type": "text", "text": "(2) Navigate. Example: If you follow these instructions, will you return to the starting point? Instructions: Turn left. Take 5 steps. Turn right. Answer: No. ", "page_idx": 13}, {"type": "text", "text": "(3) Web-of-Lies. Example: Delbert tells the truth. Delfina says Delbert lies. Antwan says Delfina tells the truth. Does Delfina tell the truth? Answer: No. ", "page_idx": 13}, {"type": "text", "text": "(4) Sports Understanding. Example: Is the following sentence plausible? \u201dElias Lindholm beat the buzzer.\u201d Answer: No. ", "page_idx": 13}, {"type": "table", "img_path": "nge5deRsEH/tmp/d4ce7d0ee5d72b6c57e41d90e497e4c4e68bca09ccf8ad51a6481fe463522b5e.jpg", "table_caption": ["Table 3: Basic Information about the Tinystories Dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.3 Details about the Visualization of the Decision Trees ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To enable visualization that treats words as features, as shown in Algorithm 1, we map word embeddings into a lower-dimensional space. This process utilizes three primary inputs: word embeddings $W$ in an $N\\times100$ matrix, where $N$ represents the number of words and 100 the dimensionality of each embedding; cluster centers $C$ in a $20\\times100$ matrix, indicating 20 clusters within the 100-dimensional embedding space; and a mapping matrix $M$ sized $100\\times20$ , designed to reduce the embeddings\u2019 dimensionality to 20. The algorithm begins with an orthogonalization procedure, applying QR decomposition to the transpose of $C\\,C^{T})$ and returning the first 20 columns of $Q^{T}$ , thereby establishing an orthogonal basis for the cluster space. It then projects the word embeddings $W$ into this lower-dimensional space by multiplying them with the mapping matrix $M$ By iterating over each word embedding in $W$ , the algorithm applies this projection and ultimately returns a set of transformed embeddings $\\{E_{1},\\allowbreak\\cdot\\cdot,E_{N}\\}$ , where each $E_{i}$ provides a lower-dimensional representation of the corresponding word embedding. This approach allows us to treat each vector value as individual words, facilitating a more intuitive understanding of the data. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Map Word Embeddings to Lower Dimensional Space ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "nge5deRsEH/tmp/95afec5bf82ee0b41eaa0e4ad5b872e27dc5e525195944e2f484a4fdb3dc45ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.4 The Story Beginnings Used in Our Evaluation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 6, we provide 10 examples over 100 story beginnings we used for our experiments on Tiny Stories. ", "page_idx": 14}, {"type": "text", "text": "B.5 An Example on Multidimensional Scores ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As an example, we took the story that appears in the example above and cut it after the word \u201dloud\u201d. We generated a completion of stories using our ARDTs and baselines, then we prompted GPT-4 to first provide a verbal assessment and then grade the completion. The transcript of interaction is as follows, with the parts produced by GPT-4 in italic font: ", "page_idx": 14}, {"type": "table", "img_path": "nge5deRsEH/tmp/f935e37cb0615f50326fb9e6c45e8c9a614800dbfbdc37545dc76f5eeaecbde7.jpg", "table_caption": ["Table 4: Generated Chain-of-Thought data for boolean expression. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "nge5deRsEH/tmp/3590280302c03f0e6cd23a3d9e24022d2861fc012e507449ffaf262011106457.jpg", "table_caption": ["Table 5: Generated Chain-of-Thought data for navigation. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "nge5deRsEH/tmp/e4ada827c844dbe5e800f3c480b08280252328d6ceac26987e4ddae52330e315.jpg", "table_caption": ["Table 6: Examples of the story beginnings generated by GPT-4. In the inference phase, we fed these story beginnings generated by GPT-4 into the ARDTs to continue writting the stories. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "the following exercise, the student is given a beginning of a story. The student needs to complete it into a full story. The exercise tests the student\u2019s language abilities and creativity. The symbol $^{\\ast\\ast\\ast\\ast}$ marks the separator between the prescribed beginning and the student\u2019s completion: ", "page_idx": 16}, {"type": "text", "text": "Once upon a time, $^{***}$ there was a little girl named Lily. She loved to play outside in th sunshine. One day, she saw a big tree in the garden. She was so excited about it. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Please provide your general assessment about the part written by the student (the one after the $^{***}$ symbol). Is it grammatically correct? Is it consistent with the beginning of the story? Pay special attention to whether the student manages to complete the sentence which is split in the middle by the separator $^{***}$ . ", "page_idx": 16}, {"type": "text", "text": "This is a charming and whimsical story about a little girl named Lily who feels excited upon discovering a big tree in the garden. ", "page_idx": 16}, {"type": "text", "text": "The grammar is generally correct. ", "page_idx": 16}, {"type": "text", "text": "Overall, the student\u2019s completion of the story demonstrates adequate language abilities and creativity. However, adding more details about Lily\u2019s exploration, as well as her thoughts and actions after discovering the tree, could make the story more engaging and vivid. ", "page_idx": 16}, {"type": "text", "text": "Now, grade the student\u2019s completion in terms of grammar, creativity, consistency with the story\u2019s beginning and whether the plot makes sense. Moreover, please provide your best guess of what the age of the student might be, as reflected from the completion. Choose from possible age groups: A: 3 or under. B: 4-5. C: 6-7. D: 8-9. E: 10-12. F: 13-16. ", "page_idx": 16}, {"type": "text", "text": "Grammar: 9/10, Creativity: 4/10, Consistency: 7/10, Age group: E (10-12) ", "page_idx": 16}, {"type": "text", "text": "C Interpretability ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "One advantage of using decision trees over complex neural networks is the Interpretability of the computational process. Unlike neural networks, which rely on abstract vector operations in highdimensional space, decision trees implement relatively simple logic, computing their output based on splitting criteria that may be easily displayed to, and interpreted by, humans. That said, recall that our decision trees operate on aggregations of word vector embeddings, which make interpretability harder to achieve. Specifically, each splitting rule of the decision tree is based on the value of a single coordinate, which does not necessarily have an interpretable semantic value when using rotation-invariant word embedding methods such as Word2Vec. ", "page_idx": 16}, {"type": "image", "img_path": "nge5deRsEH/tmp/336820129680cddf3c59b1aa9ff7743d852210aeedcf5c47a14363e854e43faf.jpg", "img_caption": ["Figure 3: t-SNE van der Maaten (2013) visualization of 20 cluster centers. We selected 20 cluster centers and display 4 words closest to the cluster centers. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In order to generate decision trees with meaningful splitting rules, we modify the word embedding such that single coordinates have specific semantic values. To achieve this, we begin by clustering all the word vectors from the dataset (over 16K words) into 20 clusters using K-means. We then choose one representative word for each cluster, by taking the word that is closest to the center of the cluster in the embedding space (see Figure 3 for an illustration of the clusters and representative words). Now, these words (represented as vectors) form a basis for a new 20-dimensional embedding space, which is a linear subspace of the original 100-dimensional space of Word2Vec. We use these basis words to compute the new word embedding, by projecting each vector from the original space into this subspace, and representating the projection as a linear combination of the basis words. Mathematically, if $x_{1},\\ldots,x_{k}$ are the basis words, we define our new embedding $\\Phi$ into $\\mathbb{R}^{k}$ by: $\\begin{array}{r}{\\Phi(x)=\\arg\\operatorname*{min}_{z\\in\\mathbb{R}^{k}}\\left\\|\\sum_{i}z_{i}\\Psi(x_{i})-\\Psi(x)\\right\\|_{2}}\\end{array}$ . Observe that each basis word $x_{i}$ is mapped by $\\Phi$ to a unit vector $e_{i}$ . Intuitively, the $i$ -th coordinate of the embedding $\\Phi$ now represents words that are semantically similar to the word $x_{i}$ . Now, splitting rules based on the coordinate $i$ can be interpreted as \u201ctesting\u201d whether a word similar to $x_{i}$ appears in the sentence. ", "page_idx": 16}, {"type": "image", "img_path": "nge5deRsEH/tmp/e59e7e1860ebcd9fd5cee7ac6e83f803169181b3499d341ac4682a73fd19f683.jpg", "img_caption": ["Figure 4: Track the decision-making process within the decision trees. We use \u2019Lily and Tom loved to play together, and they found\u2019 as an the input prompt and generate the next word using our ARDTs. We visualize part of the process within the decision tree. Specifically, we visualized 31 nodes of the first decision tree. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "nge5deRsEH/tmp/96b8b7f19775d34b18b26d86be3fde825f664b6e5bad7868ffd60f9faed01b3f.jpg", "img_caption": ["Figure 5: Feature Importance. We present the feature importance of the top 20 words most closely associated with each cluster, based on their average gain. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "We visualize one of the decision trees trained on the Tiny Stories Dataset using the new \u201cinterpretable\u201d embedding $\\Phi$ in Figure 1. Note that, unlike complex neural network architectures, which carry out opaque computations, the decision process of the ARDT with the new embedding appears to be semantically meaningful. For example, observe that the word Lily appears for three times as the most relevant word during node splits. Considering Lily is a frequently occurring name in the Tiny Stories dataset, it\u2019s frequent appearance in the tree can be deemed reasonable. We further analyze the importance of different features by plotting their importance score. We plot the importance of each cluster, represented by a single word, in Figure 5. We assess the importance of each cluster by calculating its average gain during every split within the model. ", "page_idx": 18}, {"type": "text", "text": "In Figure 4, we use the input sentence \u201cLily and Tom loved to play together and they found\u201d as an example to visualize part of the decision-making process of the first decision tree in the ensemble. We note that each feature corresponds to a single cluster, represented by a single word, e.g. the feature $f_{2}$ corresponds to the word \u201cLily\u201d. That is, the word \u201cLily\u201d will be mapped to the unit vector $e_{2}=(0,1,0,\\ldots,0)$ . Note that most words (besides the 20 words used as a basis for the embedding), will be mapped to a linear combination of the basis words, and so can also affect (positively or negatively) the value of the feature $f_{2}$ . Since the input vector is a weighted-average of the embedding of all words, the decision when splitting on the feature $f_{2}$ may be affected by multiple words in the sentence. ", "page_idx": 18}]