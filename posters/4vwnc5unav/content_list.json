[{"type": "text", "text": "The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Elizabeth Collins-Woodfin Inbar Seroussi McGill University Tel-Aviv University elizabeth.collins-woodfin@mail.mcgill.ca inbarser@tauex.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Bego\u00f1a Garc\u00eda Malaxechebarr\u00eda University of Washington begogar9@uw.edu ", "page_idx": 0}, {"type": "text", "text": "Andrew W. Mackenzie McGill University andrew.mackenzie@mail.mcgill.ca ", "page_idx": 0}, {"type": "text", "text": "Elliot Paquette McGill University elliot.paquette@mcgill.ca ", "page_idx": 0}, {"type": "text", "text": "Courtney Paquette\u2217 McGill University & Google DeepMind courtney.paquette@mcgill.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates \u2013 an idealized exact line search and AdaGrad-Norm \u2013 on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provide our code for evaluation at https://github.com/amackenzie1/highline2024. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In deterministic optimization, adaptive stepsize strategies, such as line search (see [40], therein), AdaGrad-Norm [59], Polyak stepsize [48], and others were developed to provide stability and improve efficiency and adaptivity to unknown parameters. While the practical benefits for deterministic optimization problems are well-documented, much of our understanding of adaptive learning rate strategies for stochastic algorithms are still in their infancy. ", "page_idx": 0}, {"type": "text", "text": "There are many adaptive learning rate strategies used in machine learning with many design goals. Some are known to adapt to stochastic gradient descent (SGD) gradient noise while others are robust to hyper-parameters (e.g., [4, 63]). Theoretical results for adaptive algorithms tend to focus on guaranteeing minimax-optimal rates, but this theory is not engineered to provide realistic performance comparisons; indeed many adaptive algorithms are minimax-optimal, and so more precise statements are needed to distinguish them. For instance, the exact learning rates (or rate schedules) to which these strategies converge are unknown, nor their dependence on the geometry of the problem. Moreover, we often do not know how these adaptive stepsizes compare with well-tuned constant or decaying fixed learning rate SGD, which can be viewed as a cost associated with selecting the adaptive strategy in comparison to tuning by hand. ", "page_idx": 0}, {"type": "image", "img_path": "4VWnC5unAV/tmp/d4c2faa5cf0fda67d777a3cad8b10a5cb52f9e22f9f0adc6e95ba0fe223a9882.jpg", "img_caption": ["Figure 1: Concentration of learning rate and risk for AdaGrad-Norm on least squares with label noise $\\omega=1$ (left) and logistic regression with no noise (right). As dimension increases, both risk and learning rate concentrate around a deterministic limit (red) described by our ODE in Theorem 2.1. The initial risk increase (left) suggests the learning rate started too high, but AdaGrad-Norm adapts. Our ODEs predict this behavior. See Sec. H for simulation details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we develop a framework for analyzing the exact dynamics of the risk and adaptive learning rate strategies for a wide class of optimization problems that we call high-dimensional linear (high line) composite functions. In this class, the objective function takes the form of an expected risk $\\mathcal{R}:\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}\\mathbb{R}$ over high-dimensional data $(\\boldsymbol{a},\\dot{\\epsilon})\\sim\\mathcal{D}\\subset\\mathbb{R}^{d}\\times\\mathbb{R}$ of a function $f\\,:\\,\\mathbb{R}^{3}\\to\\mathbb{R}$ composed with the linear functions $\\langle X,a\\rangle$ , $\\langle X^{\\star},a\\rangle$ . That is, we seek to solve ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{d}}\\Big\\{\\mathcal{R}(X)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{a,\\epsilon}[f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle,\\epsilon)]\\quad\\mathrm{for}\\quad(a,\\epsilon)\\sim\\mathcal{D},X^{\\star}\\in\\mathbb{R}^{d}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We suppose $a\\sim\\mathcal{N}(0,K)$ where $K\\in\\mathbb{R}^{d\\times d}$ is the covariance matrix. We train (1) using (one-pass) stochastic gradient descent with adaptive learning rates, ${\\mathfrak{g}}_{k}$ ( $\\mathrm{SGD+AL}_{\\mathrm{,}}$ ). Our main goal is to give a framework for better2 performance analysis of these adaptive methods. We then illustrate this framework by considering two adaptive learning rate algorithms on the least squares problem3, the results of which appear in Table 1: exact line-search (idealistic) (Sec. 3) and AdaGrad-Norm (Sec. 4). We expect other losses and adaptive learning rates can be studied using this approach. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. Performance analysis framework. We provide an equivalence of ${\\mathcal R}(X_{k})$ and learning rate ${\\mathfrak{g}}_{k}$ under $\\mathrm{SGD+AL}$ to deterministic functions $\\mathcal{R}(t)$ and $\\gamma_{t}$ via solving a deterministic system of ODEs (see Section 2), which we then analyze to show how the covariance spectrum influences the optimization. See Figure 1. As the dimension $d$ of the problem grows, the learning curves of ${\\mathcal{R}}(X_{k})$ become closer to $\\mathcal{R}(t)$ and the curves concentrate around $\\mathcal{R}(t)$ with probability better than any inverse power of $d$ (See Theorem 2.1). ", "page_idx": 1}, {"type": "text", "text": "Greed can be arbitrarily bad in the presence of strong anisotropy (that is, $T r(K)/d\\ll T r(K^{2})/d\\}$ . Our analysis reveals that exact line search, which is to say optimally decreasing the risk at each step, can run arbitrarily slower than the best fixed learning rate for SGD on a least squares problem when $\\lambda_{\\operatorname*{min}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\lambda_{\\operatorname*{min}}(K)>C>0$ . The best fixed stepsize (least squares problem) is $(\\operatorname{Tr}(K)/d)^{-1}$ or the inverse of the average eigenvalue, see Polyak stepsize [48]. Line search, on the other hand, converges to a fixed stepsize of order $\\lambda_{\\mathrm{min}}/(\\mathrm{Tr}(\\dot{K^{2}})/d)$ . It can be that $\\lambda_{\\operatorname*{min}}/(\\mathrm{Tr}(K^{2})/d)\\ll(\\mathrm{Tr}(K)/d)^{-1}$ making exact line search substantially underperform Polyak stepsize. We further explore this and, in the case where $d$ -eigenvalues of $K$ take only two values $\\lambda_{1}>\\lambda_{2}>0$ , we give an exact expression as a function of $\\lambda_{1}$ and $\\lambda_{2}$ for the limiting behavior of $\\gamma_{t}$ as $t\\to\\infty$ (See Fig. 5). ", "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of adaptive learning rates results on the least squares problem. We summarize our results for line search and AdaGrad-Norm under various assumptions on the covariance matrix $K$ . We denote $\\lambda_{\\operatorname*{min}}$ the smallest non-zero eigenvalue of $K$ and $\\frac{\\mathrm{Tr}(K)}{d}$ the average eigenvalue. Power $\\operatorname{law}(\\delta,\\beta)$ assumes the eigenvalues of $K$ , $\\{\\lambda_{i}\\}_{i=1}^{d}$ , follow a power law distribution, that is, for $0<$ $\\beta<1$ , $\\lambda_{i}\\sim(1-\\beta)\\lambda^{-\\beta}\\mathbf{1}_{(0,1)}$ for all $1\\leq i\\leq d$ and $\\langle X_{0}-X^{\\star},\\omega_{i}\\rangle^{2}\\sim\\lambda_{i}^{-\\delta}$ where $\\{\\omega_{i}\\}_{i=1}^{d}$ are eigenvectors of $K$ (see Prop 4.4). For \u2217(see Prop. 4.2), requires a good initialization on $b,\\eta$ . ", "page_idx": 2}, {"type": "table", "img_path": "4VWnC5unAV/tmp/525573234a0be19a1a4bcf19d24da5e2bfd1954eb94a194770d32630792b945a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "AdaGrad-Norm selects the optimal step-size, provided it has a warm start. In the absence of label noise and when the smallest eigenvalue of $K$ satisfies $\\lambda_{\\operatorname*{min}}>C>0$ , the learning rate converges to a deterministic constant that depends on the average condition number (like in Polyak) and scales itenrvmerss eolfy  thwei tch $\\frac{\\mathrm{Tr}(K)}{d}\\|X_{0}-X^{\\star}\\|^{2}$ .n oThwelreedfgoer eo ift s,  abuutt opmaaytsi caa lpley ntahlte yo ipnt itmhea l cfoixnestda sntt,e pnsaizmee ilny ${\\mathrm{Tr}}(K)$   \n$\\lVert X_{0}-X^{\\star}\\rVert^{2}$ . If one knew $\\lVert X_{0}-X^{\\star}\\rVert^{2}$ then by tuning the parameters of AdaGrad-Norm one might achieve performance consistent with Polyak; this also motivates more sophisticated adaptive algorithms such as DoG [29] and D-Adaptation [18], which adaptively compensate and/or estimate $\\|\\bar{X}_{0}-X^{\\star}\\|^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "AdaGrad-Norm can use overly pessimistic decaying schedules on hard problems. Consider power law behavior for the spectrum of $K$ and the signal $X^{\\star}$ . This is a natural setting as power law distributions have been observed in many datasets [60]. Here the learning rate and asymptotic convergence of $K$ undergo a phase transition. For power laws corresponding to easier optimization problems, the learning rate goes to a constant and the risk decays at $t^{-\\alpha_{1}}$ . For harder problems, the learning rate decays like $t^{-\\eta_{1}}$ and the risk decays at a different sublinear rate $t^{-\\alpha_{2}}$ . See Table 1 and Sec. 4 for details. ", "page_idx": 2}, {"type": "text", "text": "Notation. Define $\\mathbb{R}_{+}=[0,\\infty)$ . We say an event holds with overwhelming probability, $w.o.p.$ ., if there is a function $\\omega:\\mathbb{N}\\rightarrow\\mathbb{R}$ with $\\omega(d)/\\log d\\to\\infty$ so that the event holds with probability at least $1-e^{-\\omega(d)}$ . We let $\\mathbf{1}_{A}(x)$ be the indicator function of the set $A$ where it is 1 if $x\\in A$ and 0 otherwise. For a matrix $A\\in\\mathbb{R}^{m\\times d}$ , we use $\\|A\\|$ to denote the Frobenius norm and $\\|A\\|_{\\mathrm{op}}$ to denote the operator-2 norm. If unspecified, we assume that the norm is the Frobenius norm. For normed vector spaces $A,\\,B$ with norms $\\|\\cdot\\|_{\\mathcal{A}}$ and ${\\big\\|}\\cdot{\\big\\|}_{B}$ , respectively, and for $\\alpha\\geq0$ , we say a function $F:A\\rightarrow B$ is $\\alpha$ -pseudo-Lipschitz with constant $L$ if for any $A,{\\hat{A}}\\in A$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F(A)-F(\\hat{A})\\|_{\\mathcal{B}}\\leq L\\|A-\\hat{A}\\|_{A}(1+\\|A\\|_{A}^{\\alpha}+\\|\\hat{A}\\|_{A}^{\\alpha}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We write $f(t)\\asymp g(t)$ if there exist absolute constants $C,c>0$ such that $c\\cdot g(t)\\leq f(t)\\leq C\\cdot g(t)$ for all $t$ . If the constants depend on parameters, e.g., $\\alpha$ , then we write $\\asymp_{\\alpha}$ . ", "page_idx": 2}, {"type": "text", "text": "Related work. Some notable adaptive learning rates in the literature are AdaGrad-Norm [32, 59, 61], RMSprop [28], stochastic line search, stochastic Polyak stepsize [35], and more recently DoG [29] and D-Adaptation [18]. In this work, we introduce a framework for analyzing these algorithms, and we strongly believe it can be used to analyze many more of these adaptive algorithms. We highlight below a nonexhaustive list of related work. ", "page_idx": 2}, {"type": "text", "text": "AdaGrad-Norm. AdaGrad, introduced by [19, 36], updates the learning rate at each iteration using the stochastic gradient information. The single stepsize version [32, 59, 61], that depends on the norm of the gradient, (see Table 2 for the updates), has been shown to be robust to input parameters [34]. Several works have shown worst-case convergence guarantees [21, 33, 57, 59]. A linear rate of $O(\\exp(-\\kappa T))$ is possible for $\\mu$ -strongly convex, $L$ -smooth functions ( $\\kappa$ is the condition number $\\mu/L)$ ). In [62] (similar idea in [61]), the authors show for strongly convex, smooth stochastic objectives (with additional assumptions) that the AdaGrad-Norm learning rate exhibits a two stage behavior \u2013 a burn in phase and then when it reaches the smoothness constant it self-stablizes. ", "page_idx": 3}, {"type": "text", "text": "Stochastic line search and Polyak stepsizes. Recently there has been renewed interest in studying stochastic line search [20, 42, 54] and stochastic Polyak stepsize (and their variants) [7, 26, 27, 30, 35, 39, 41, 49]. Much of this research focuses on worst-case convergence guarantees for strongly convex and smooth functions (see e.g., [35]) and designing practical algorithms. In [53], the authors provide a bound on the learning rate for Armijo line search in the finite sum setting with a rate of $L_{\\mathrm{max}}/\\mathrm{avg}$ . $\\mu$ where avg. $\\mu$ is the avg. strong convexity and $L_{\\mathrm{max}}$ is the max. Lipschitz constant of the individual functions. In this work, we consider a slightly different problem. We work with the population loss and we note that the analogue to $L_{\\mathrm{max}}$ for us would require that the samples $a$ satisfy $\\|a a^{T}\\|_{\\mathrm{op}}\\leq L_{\\operatorname*{max}}$ for all $a$ ; this fails to hold for $a\\sim\\mathcal{N}(0,K)$ . Moreover, $L_{\\mathrm{max}}$ could be much worse than $\\mathbb{E}\\left[\\Vert a a^{T}\\Vert_{\\mathrm{op}}\\right]$ . ", "page_idx": 3}, {"type": "text", "text": "Deterministic dynamics of stochastic algorithms in high-dimensions. The literature on deterministic dynamics for isotropic Gaussian data has a long history [9, 10, 50, 51]. These results have been rigorously proven and extended to other models under the isotropic Gaussian assumption [1, 2, 6, 16, 17, 23, 58]. Extensions to multi-pass SGD with small mini-batches [46] as well as momentum [31] have also been studied. Other high-dimensional limits leading to a different class of dynamics also exist [11\u201313, 22, 37]. Recently, significant contributions have been made in understanding the effects of a non-identity data covariance matrix on the training dynamics [5, 14, 15, 24, 25, 64]. The non-identity covariance modifies the optimization landscape and affects convergence properties, as discussed in [15]. This work extends the findings of [15] to stochastic adaptive algorithms, exploring the effect of non-identity covariance within these algorithms. Notably, Theorem 1.1 from [15] is restricted to deterministic learning rate schedules, limiting its applicability in many practical scenarios. In contrast, our Theorem 2.1 accommodates stochastic adaptive learning rates, aligning with widely used algorithms in practice. ", "page_idx": 3}, {"type": "text", "text": "1.1 Model Set-up ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We suppose that a sequence of independent samples $\\{(a_{k},y_{k})\\}$ drawn from a distribution $\\mathcal{D}\\subset\\mathbb{R}^{d}\\times\\mathbb{R}$ is provided where $y_{k}$ is the target. The target $y_{k}$ is a function of some random label noise $\\epsilon_{k}\\in\\mathbb{R}$ and the input feature $a_{k}$ dotted with a ground truth signal $X^{\\star}\\in\\mathbb{R}^{d}$ , $\\langle a_{k},X^{\\star}\\rangle$ . Therefore, the distribution of the data is only determined by the input feature and the noise, i.e., the pair $(a,\\epsilon)$ . In particular, we assume $(a,\\epsilon)$ follows a distributional assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Data and label noise). The samples $(a,\\epsilon)\\,\\sim\\,\\mathcal{D}$ are normally distributed: $\\epsilon\\mathrm{~\\sim~}$ $\\mathcal{N}(0,\\omega^{2})$ where $\\omega\\in\\mathbb{R},$ , and $a\\sim\\mathcal{N}(0,K)$ , with a covariance matrix $K\\in\\mathbb{R}^{d\\times\\dot{d}}$ that is bounded in operator norm independent of $d$ ; i.e., $\\|K\\|_{o p}\\leq C$ . Furthermore, a and $\\epsilon$ are independent. ", "page_idx": 3}, {"type": "text", "text": "For $a,X,X^{\\star}\\,\\in\\,\\mathbb{R}^{d}$ , $\\epsilon\\,\\in\\,\\mathbb{R}$ , and a function $f:\\mathbb{R}^{3}\\,\\rightarrow\\,\\mathbb{R}$ , we seek to minimize an expected risk function $\\mathcal{R}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , which we refer to as the high-dimensional linear composite4, of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(X)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{a,\\epsilon}[\\Psi(X;a,\\epsilon)]\\quad\\mathrm{for}\\quad(a,\\epsilon)\\sim\\mathcal{D},\\quad\\mathrm{and}\\quad\\Psi(X;a,\\epsilon)=f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle,\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In what follows, we use the matrix $W=[X|X^{\\star}]\\in\\mathbb{R}^{d\\times2}$ that concatenates $X$ and $X^{\\star}$ , and we shall let $B=B(W)=W^{T}K W$ be the covariance matrix of the Gaussian vector $(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle)$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Pseudo-lipschitz $f$ ). The function $f:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}$ is $\\alpha$ -pseudo-Lipschitz with $\\alpha\\leq1$ . ", "page_idx": 3}, {"type": "text", "text": "By assumption, ${\\mathcal{R}}(X)$ involves an expectation over the correlated Gaussians $\\langle a,X\\rangle$ and $\\langle a,X^{\\star}\\rangle$ . We can express this as $\\mathcal{R}(X)\\stackrel{\\mathrm{def}}{=}h(B)$ for some well-behaved function $h:\\mathbb{R}^{2\\times2}\\rightarrow\\mathbb{R}$ . ", "page_idx": 3}, {"type": "table", "img_path": "4VWnC5unAV/tmp/959aa5c72f555d53b0860c59e54c559b36c929f3a3825a01b945e7ed6c759975.jpg", "table_caption": ["Table 2: Two adaptive learning rates considered in detail. The stochastic adaptive learning rate, ${\\mathfrak{g}}_{k}$ , is the learning rate directly used in the update for SGD whereas the deterministic, $\\gamma_{t}$ , is the deterministic equivalent of ${\\mathfrak{g}}_{k}$ after scaling. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Assumption 3 (Risk representation). There exists a function $h:\\mathbb{R}^{2\\times2}\\rightarrow\\mathbb{R}$ such that $h(B)={\\mathcal{R}}(X)$ is differentiable and satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{X}\\mathcal{R}(X)=\\mathbb{E}_{a,\\epsilon}\\nabla_{X}\\Psi(X;a,\\epsilon).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, $h$ is continuously differentiable and its derivative $\\nabla h$ is $\\alpha$ -pseudo-Lipschitz for some $0\\leq\\alpha\\leq1,$ , with constant $L(\\nabla h)$ . ", "page_idx": 4}, {"type": "text", "text": "The final assumption is the well-behavior of the Fisher information matrix of the gradients. The first coordinate of $f$ is special, as the optimizer must be able to differentiate it. Thus, we treat $f(x,x^{\\star},\\epsilon)$ as a function of a single variable with two parameters: $f(x,x^{\\star},\\epsilon)\\,=\\,f(x;x^{\\star},\\epsilon)$ and denote the (almost everywhere) derivative with respect to the first variable as $f^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 (Fisher matrix). Define $I(B)\\stackrel{d e f}{=}\\mathbb{E}_{a,\\epsilon}[(f^{\\prime}(\\langle a,X\\rangle;\\langle a,X^{\\star}\\rangle,\\epsilon))^{2}]$ where the function $I:\\mathbb{R}^{2\\times2}\\rightarrow\\mathbb{R}$ . Furthermore, $I$ is $\\alpha$ -pseudo-Lipschitz with constant $L(I)$ for some $\\alpha\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "A large class of natural regression problems fti within this framework, such as logistic regression and least squares (see [15, Appendix B]). We also note that Assumptions 3 and 4 are nearly satisfied for $L$ -smooth objectives $f$ (see Lemma B.1), and a version of the main theorem holds under just this assumption (albeit with a weaker conclusion). ", "page_idx": 4}, {"type": "text", "text": "1.2 Algorithmic set-up ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We apply one-pass or streaming SGD with an adaptive learning rate ${\\mathfrak{g}}_{k}$ $\\mathrm{SGD+AL}_{\\mathrm{,}}$ ) to solve $\\operatorname*{min}_{X\\in\\mathbb{R}^{d}}\\mathcal{R}(X)$ , (2). Let $X_{0}\\in\\bar{\\mathbb{R}}^{d}$ be an initial vector (random or non-random). Then $\\mathrm{SGD+AL}$ iterates by selecting a new data point $(a_{k+1},\\epsilon_{k+1})$ such that $a_{k+1}\\sim\\mathcal{N}(0,K)$ and $\\epsilon_{k+1}\\sim\\mathcal{N}(0,\\omega^{2})$ and makes the update ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-{\\frac{\\mathfrak{L}_{k}}{d}}\\cdot\\nabla_{X}\\Psi(X_{k};a_{k+1},\\epsilon_{k+1})=X_{k}-{\\frac{\\mathfrak{L}_{k}}{d}}f^{\\prime}(\\langle a_{k+1},X_{k}\\rangle;\\langle a_{k+1},X^{\\star}\\rangle,\\epsilon_{k+1})a_{k+1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathfrak{g}}_{k}>0$ is a learning rate (see assumptions below)5. To perform our analysis, we place the following assumption on the initialization $X_{0}$ and signal $X^{\\star}$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 5 (Initialization and signal). The initialization point $X_{0}$ and the signal $X^{\\star}$ are bounded independent of $d_{\\cdot}$ , that is, $\\operatorname*{max}\\{\\|X_{0}^{-}\\|,\\|X^{\\star}\\|\\}\\leq C$ for some $C$ independent of $d$ . ", "page_idx": 4}, {"type": "text", "text": "Adaptive learning rate. Our analysis requires some mild assumptions on the learning rate. To this end, we define a learning rate function $\\gamma:\\mathbb{R}_{+}\\times D([0,\\infty))\\times D([0,\\infty))\\times D([0,\\infty))\\to\\mathbb{R}_{+}$ by6 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(N_{k}(t),G_{k}(t),Q_{k}(t))\\stackrel{\\mathrm{def}}{=}{\\bf1}_{\\{t<k\\}}\\Big((W_{t})^{T}W_{t},\\frac{1}{d}\\|\\nabla_{X}\\Psi(X_{t};a_{t+1},\\epsilon_{t+1})\\|^{2},\\mathcal{R}(X_{t})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this definition, for functions taking integer arguments, we extend them to real-valued inputs by first taking the floor function of its argument. Note that the adaptive learning rates can depend on the whole history of stochastic iterates $(N_{k})$ , gradients $\\left(G_{k}\\right)$ , and risk $\\left(Q_{k}\\right)$ via this definition. ", "page_idx": 5}, {"type": "text", "text": "We also define a conditional expectation version of $G_{k}$ where the flitration $\\mathcal{F}_{k}=\\sigma(X^{\\star},X_{0},\\ldots,X_{k})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{G}_{k}(t)\\overset{\\mathrm{def}}{=}\\mathbf{1}_{\\{t<k\\}}(\\cdot)\\frac{1}{d}\\mathbb{E}[\\|\\nabla_{X}\\Psi(X_{t};a_{t+1},\\epsilon_{t+1})\\|^{2}|\\mathcal{F}_{t}]\\quad\\mathrm{for}\\;t\\ge0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With this, we impose the following learning rate condition. ", "page_idx": 5}, {"type": "text", "text": "Assumption 6 (Learning rate). The learning rate function $\\gamma\\,:\\,\\mathbb{R}_{+}\\times D([0,\\infty))\\times D([0,\\infty))\\times$ $D([0,\\stackrel{\\bar{\\infty}}{\\infty}))\\;\\rightarrow\\;\\mathbb{R}$ is $\\alpha$ -pseudo-Lipschitz with constant $L(\\gamma)$ (independent of $d$ ) in $D([0,\\infty))\\ \\times$ $D([0,\\infty))\\times D([0,\\infty))$ . Moreover, for some constant $C=C(\\gamma)>0$ independent of $d$ and $\\delta>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[|\\gamma(k,f,G_{k}(d\\times\\cdot),q)-\\gamma(k,f,\\mathcal{G}_{k}(d\\times\\cdot),q)|\\,|\\,\\mathcal{F}_{k}\\right]\\leq C d^{-\\delta}(1+\\|f\\|_{\\infty}^{\\alpha}+\\|q\\|_{\\infty}^{\\alpha})\\quad w.o.p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, $\\gamma$ is bounded, i.e., there exists a constant $\\hat{C}=\\hat{C}(\\gamma)>0$ independent of $d$ so that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(k,f,g,q)\\leq\\hat{C}(1+\\|f\\|_{\\infty}^{\\alpha}+\\|q\\|_{\\infty}^{\\alpha}+\\|g\\|_{\\infty}^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The inequality (5) ensures that the learning rate concentrates around the mean behavior of the stochastic gradients. Many well-known adaptive stepsizes satisfy (4) and Assumption 6 including AdaGrad-Norm, DoG, D-Adaptation, and RMSProp (see Table 2, Sec. A, and Sec. C.3). ", "page_idx": 5}, {"type": "text", "text": "2 Deterministic dynamics for SGD with adaptive learning rates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Intuition for deriving dynamics: The risk ${\\mathcal{R}}(X)$ and Fisher matrix can be evaluated solely in terms of the covariance matrix $B$ . Thus, to know the evolution of the risk over time, it would suffice to know the evolution of $B$ . Alas, except in the isotropic case where $K$ is a multiple of the identity, the evolution of $B$ is not autonomous (i.e., its time evolution depends on other unknown variables). However, if we let $\\left({\\lambda_{i}},{\\omega_{i}}\\right)$ be the eigenvalues and corresponding orthonormal eigenvectors of $K$ , we can consider projections $\\overset{\\./}{V_{i}}(X_{k})=\\overset{\\.}{d}\\cdot W_{k}^{T}\\omega_{i}\\omega_{i}^{T}W_{k}$ , and it turns out that these behave autonomously. ", "page_idx": 5}, {"type": "text", "text": "Example: Least Squares. One canonical example of (2) is least squares, where we aim to recover the target $X^{\\star}$ given noisy observations $\\langle a,X^{\\star}\\rangle+\\epsilon$ . In this case, the least squares problem is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{d}}\\Big\\{\\mathcal{R}(X)=\\frac{1}{2}\\mathbb{E}_{a,\\epsilon}[(\\langle a,X-X^{\\star}\\rangle-\\epsilon)^{2}]=\\frac{1}{2}\\omega^{2}+\\frac{1}{2}(X-X^{\\star})^{T}K(X-X^{\\star})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The pair of functions $h$ (Assumption 3) and $I$ (Assumption 4) can be evaluated simply: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(B(W))=\\frac12I(B(W))=\\frac12(X-X^{\\star})^{T}K(X-X^{\\star})+\\frac12\\omega^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The deterministic dynamics for the risk $\\mathcal{R}(t)$ in this case can be simplified to: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(t)=\\frac{1}{2}(X_{0}-X^{\\star})^{T}K e^{-2K\\int_{0}^{t}\\gamma_{s}\\;\\mathrm{d}s}(X_{0}-X^{\\star})+\\frac{1}{2}\\omega^{2}+\\frac{1}{d}\\int_{0}^{t}\\gamma_{s}^{2}\\Upsilon\\!\\Gamma(K^{2}e^{-2K\\int_{s}^{t}\\gamma_{\\tau}\\,\\mathrm{d}\\tau})\\mathcal{R}(s)\\;\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This is a convolution Volterra equation with a convergence threshold of $\\begin{array}{r}{\\gamma_{t}<\\frac{2d}{\\mathrm{Tr}K}}\\end{array}$ [14, 44, 46, 47]. ", "page_idx": 5}, {"type": "text", "text": "In the noiseless label case (i.e., \u03f5 = 0), the risk is given by R(t) = 21d $\\begin{array}{r}{\\mathcal{R}(t)=\\frac{1}{2d}\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{D}_{i}^{2}(t)}\\end{array}$ id=1 \u03bbiDi2 (t). Using the ODEs in (9), we get the following deterministic equivalent ODE for the $\\mathcal{D}_{i}^{2}$ \u2019s: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}_{i}^{2}(t)=-2\\gamma_{t}\\lambda_{i}\\mathcal{D}_{i}^{2}(t)+2\\gamma_{t}^{2}\\lambda_{i}\\mathcal{R}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We will perform a deep analysis of the dynamics of the learning rate on least squares (7), which will generalize to settings where the outer function $f$ is strongly convex (see D.1). ", "page_idx": 5}, {"type": "text", "text": "Deterministic dynamics. To derive deterministic dynamics, we make the following change to continuous time by setting ", "page_idx": 5}, {"type": "text", "text": "This time change is necessary, as when we scale the size of the problem, more time is needed to solve the underlying problem. This scaling law scales SGD so all training dynamics live on the same ", "page_idx": 5}, {"type": "text", "text": "space. One can solve a smaller $d$ problem and scale it to recover the training dynamics of the larger problem.7 ", "page_idx": 6}, {"type": "text", "text": "We now introduce a coupled system of differential equations, which will allow us to model the behaviour of our learning algorithms. For the ith $\\left({\\lambda_{i}},{\\omega_{i}}\\right)$ -eigenvalue/eigenvector of $K$ , set ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{i}(t)\\,\\stackrel{\\mathrm{def}}{=}\\,\\bigg[\\mathcal{V}_{11,i}(t)\\,}&{\\mathcal{V}_{12,i}(t)\\bigg]\\mathrm{~\\and~averaging~over~}i,\\,\\mathcal{B}(t)\\stackrel{\\mathrm{def}}{=}\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{V}_{i}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The $\\mathcal{V}_{i}(t)$ and $\\mathcal{B}(t)$ are deterministic continuous analogues of $V_{i}(X_{t d})$ and $B(X_{t d})$ respectively. Define the following continuous analogues ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla h(\\mathcal{B}(t))\\stackrel{\\mathrm{def}}{=}\\left[H_{2,t}\\quad H_{3,t}\\right],\\;\\mathcal{N}(t)\\stackrel{\\mathrm{def}}{=}\\frac{1}{d}\\sum_{i=1}^{d}\\mathcal{H}_{i}(t),\\;\\mathcal{R}(t)\\stackrel{\\mathrm{def}}{=}h(\\mathcal{B}(t)),\\;\\;\\mathcal{I}(t)\\stackrel{\\mathrm{def}}{=}I(\\mathcal{B}(t)),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$\\begin{array}{r}{\\gamma_{t}\\overset{\\mathrm{def}}{=}\\gamma(t,1_{\\{\\cdot\\leq t\\}}\\mathcal{N}(\\cdot),\\frac{\\mathrm{Tr}(K)}{d}1_{\\{\\cdot\\leq t\\}}\\mathcal{I}(\\cdot),1_{\\{\\cdot\\leq t\\}}\\mathcal{R}(\\cdot)).}\\end{array}$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We now introduce a system of coupled ODEs for each $\\left({\\lambda_{i}},{\\omega_{i}}\\right)$ -eigenvalue/eigenvector pair of $K$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathcal{V}_{11,i}(t)=-2\\lambda_{i}\\gamma_{t}\\left(\\mathcal{V}_{11,i}(t)H_{1,t}+H_{1,t}\\mathcal{V}_{11,i}(t)+\\mathcal{V}_{12,i}(t)H_{2,t}+H_{2,t}\\mathcal{V}_{12,i}(t)\\right)+\\lambda_{i}\\gamma_{t}^{2}\\mathcal{I}(t),}\\\\ &{\\mathrm{d}\\mathcal{V}_{12,i}(t)=-2\\lambda_{i}\\gamma_{t}\\left(H_{1,t}\\mathcal{V}_{12,i}(t)+H_{2,t}\\mathcal{V}_{22,i}(t)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with the initialization of $\\mathcal{V}_{i}(0)$ given by ${V_{i}}(X_{0})$ . We finally state the deterministic dynamics for the risk and learning rate. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.1. Under Assumptions 1, 2, 3, 4, 5, 6, then for any $\\varepsilon\\in(0,\\frac12)$ and any $T>0$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\|\\left(\\stackrel{\\mathcal{R}(X_{\\lfloor t d\\rfloor})}{\\mathfrak{g}_{\\lfloor t d\\rfloor}}\\right)-\\left(\\stackrel{\\mathcal{R}(t)}{\\gamma_{t}}\\right)\\|<d^{-\\varepsilon},\\quad w.o.p.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The same statements hold comparing $W_{t d}^{T}W_{t d}$ to $\\mathcal{N}(t)$ and $W_{t d}^{T}K W_{t d}$ to $\\mathcal{B}(t)$ . ", "page_idx": 6}, {"type": "text", "text": "In fact, we can derive deterministic dynamics for a large class of statistics which are linear combinations of $\\mathcal{V}(t)$ and functions thereof (See Theorem B.1, and Corollary B.1). ", "page_idx": 6}, {"type": "text", "text": "One important corollary is a deterministic limit for the distance to optimality, $D^{2}(X_{k})=\\|X_{k}\\!-\\!X^{\\star}\\|^{2}$ , which is a quadratic form of $W_{k}^{T}W_{k}$ and hence covered by Thm. 2.1. The equivalent deterministic dynamics are ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{D}^{2}(t)=\\frac{1}{d}\\sum_{i=1}^{d}\\mathcal{D}_{i}^{2}(t)=\\frac{1}{d}\\sum_{i=1}^{d}(\\mathcal{V}_{11,i}(t)-2\\mathcal{V}_{12,i}(t)+\\mathcal{V}_{22,i}(t)),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{D}_{i}^{2}(t)$ corresponds $D_{i}^{2}(X_{k})\\stackrel{\\mathrm{def}}{=}d\\times(\\langle X_{k}-X^{\\star},\\omega_{i}\\rangle)^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "3 Idealized Exact Line Search and Polyak Stepsize ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we consider two classical idealized algorithms \u2013 exact line search and Polyak stepsize. In deterministic optimization, these learning rate strategies are chosen so that the function value (exact line search) or distance to optimality (Polyak) produces the largest decrease in function value (resp. distance to optimality) at the next iteration. For stochastic algorithms, we can ask this to hold for the deterministic equivalent to the risk $\\mathcal{R}(t)$ (resp. distance to optimality, $\\mathcal{D}(t))$ since we know that SGD is close to these deterministic equivalents. Thus, the question is: what choice of learning rate decreases the $\\mathcal{R}(t)$ (exact line search) and/or $\\mathcal{D}(t)$ (Polyak stepsize)? We will restrict to least squares in this section \u2013 see Appendix F.1 and F.2 for general functions as well as proofs for least squares. These are idealized algorithms because we can not implement them as they require distributional knowledge of $a$ or $X^{\\star}$ . Despite this, they provide a basis for more practical algorithms. ", "page_idx": 6}, {"type": "image", "img_path": "4VWnC5unAV/tmp/aa108afbc8f816f850f210e92c763c77be8b85b8cf86f078f8e62ba57c2ffc5f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Comparison for Exact Line Search and Polyak Stepsize on a noiseless least squares problem. The left plot illustrates the convergence of the risk function, while the right plot depicts the convergence of the quotient \u03b3t/ 1\u03bb mTirn((KK2)) for Polyak stepsize and exact line search. Both plots highlight the implication of equation (13) in high-dimensional settings, where a broader spectrum of $K$ results in $\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\mathrm{Tr}(K^{2})}\\ll\\frac{1}{\\frac{1}{d}\\mathrm{Tr}(K)}}\\end{array}$ , indicating slower risk convergence and poorer performance of exact line search (unmarked) as it deviates from the Polyak stepsize (circle markers) . The gray shaded region demonstrates that equation (13) is satisfied. See Appendix $\\mathrm{H}$ for simulation details. ", "page_idx": 7}, {"type": "text", "text": "Polyak Stepsize. A natural threshold to consider is the largest learning rate such that $\\mathrm{d}\\mathcal{D}(t)<0$ , which we denote by $\\bar{\\gamma}_{t}^{\\mathcal{D}}$ . Using the least squares ODE (8), this is precisely ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\gamma}_{t}^{\\mathcal{D}}=\\frac{\\left(2\\mathcal{R}(t)-\\omega^{2}\\right)}{\\frac{\\mathrm{Tr}(K)}{d}\\mathcal{R}(t)}\\quad\\mathrm{and}\\quad\\bar{\\mathfrak{g}}_{k}^{\\mathcal{D}}=\\frac{\\left(2\\mathcal{R}(X_{k})-\\omega^{2}\\right)}{\\frac{\\mathrm{Tr}(K)}{d}\\mathcal{R}(X_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Without label noise, (12) simplifies to $\\begin{array}{r}{\\bar{\\gamma}_{t}^{\\mathcal{D}}=\\bar{\\mathfrak{g}}_{k}^{\\mathcal{D}}=\\frac{2}{\\operatorname{Tr}(K)/d}}\\end{array}$ , the exact threshold for convergence of least squares. ", "page_idx": 7}, {"type": "text", "text": "A greedy stepsize strategy would maximize the decrease in the distance to optimality at each iteration, denoted by us as Polyak stepsize, $\\gamma_{t}^{\\mathrm{Polyak}}\\in\\arg\\operatorname*{min}_{\\gamma}\\mathrm{d}\\mathcal{D}(t)$ . In the case of least squares, this is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{t}^{\\mathrm{Polyak}}=\\frac{1}{2}\\bar{\\gamma}_{t}^{\\mathrm{2D}}\\quad\\mathrm{and}\\quad\\mathfrak{g}_{k}^{\\mathrm{Polyak}}=\\frac{1}{2}\\bar{\\mathfrak{g}}_{k}^{\\mathrm{2D}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The latter yields the optimal fixed learning rate (up to absolute constant factors) for a noiseless target on a least squares problem [35, 43]).8 ", "page_idx": 7}, {"type": "text", "text": "Exact Line Search. In the context of risk, using (8) and noting that $\\begin{array}{r}{\\mathcal{R}(t)=\\frac{1}{2d}\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{D}_{i}^{2}(t)}\\end{array}$ , we can find $\\gamma_{t}^{\\mathrm{line}}\\in\\arg\\operatorname*{min}\\mathrm{d}\\mathcal{R}(t)$ ; i.e., the greedy learning rate that decreases the risk the most in the next iteration. We call this exact line search. Expressions for the learning rates are given in Table 2, (c.f. Appendix F.1 for general losses). Because these come from ODEs, we can use ODE theory to give exact limiting values for the deterministic equivalent of ${\\mathfrak{g}}_{k}^{\\mathrm{line}}$ ", "page_idx": 7}, {"type": "text", "text": "Proposition 3.1. [Limiting learning rate; line search on noiseless least squares] Consider the noiseless $\\omega=0$ ) least squares problem (7) . Then the learning rate is always lower bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}\\leq\\gamma_{t}^{\\operatorname*{line}}\\quad f o r\\,a l l\\,t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, suppose $K$ has only two distinct eigenvalues $\\lambda_{1}>\\lambda_{2}>0,$ , i.e., $K$ has $d/2$ eigenvalues equal to $\\lambda_{1}$ eigenvalues and $d/2$ eigenvalues equal to $\\lambda_{2}$ . Then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}\\leq\\underset{t\\to\\infty}{\\operatorname*{lim}}\\gamma_{t}^{\\operatorname*{line}}\\leq\\frac{2\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For a proof and explicit formula for $\\mathrm{lim}_{t\\rightarrow\\infty}\\,\\gamma_{t}^{\\mathrm{line}}$ \u03b3tline, see Section F.2. Hence, being greedy for the risk in a sufficiently anisotropic setting will badly underperform Polyak stepsize (see Fig. 2). ", "page_idx": 7}, {"type": "image", "img_path": "4VWnC5unAV/tmp/c43be3bcbca0c8db323024e1b7fa7bf62cb2a8b62926ec2fc6b7e87b4639c763.jpg", "img_caption": ["Figure 3: Quantities effecting AdaGrad-Norm learning rate. (left): Effect of noise $\\omega=1.0)$ ) on risk (left axis) and learning rate (right axis). Depicted is leaasrynminpgt ortaitce so it approaches 1. (Center, right): Noiseless least squares $(\\omega=0)$ ). As predicted in Prop. 4.2, $\\operatorname*{lim}_{t\\to\\infty}\\gamma_{t}$ depends on avg. eig. of $K$ $(\\operatorname{Tr}(K)/d)$ and $\\|X_{0}-X^{\\star}\\|^{2}$ but not $\\kappa=\\lambda_{\\mathrm{max}}/\\lambda_{\\mathrm{min}}$ . See Appendix H for simulation details. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 AdaGrad-Norm analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we analyze the behavior of AdaGrad-Norm learning rate in the least squares setting (see Sec. D for general strongly convex functions). In the presence of additive noise, the AdaGradNorm learning rate decays like $t^{-1/2}$ , regardless of the data covariance $K$ . In contrast, the model with no noise exhibits a learning rate that depends on the spectrum of $K$ , as illustrated in Figure 3. The learning rate is bounded below by a constant when $\\lambda_{\\operatorname*{min}}(K)>0$ is fixed as $d\\to\\infty$ , and we quantify this lower bound. If the limiting spectral measure of $K$ has unbounded density near 0 (e.g. power law spectrum), then the learning rate can approach zero and we quantify the rate of this convergence in the least squares setting as a function of spectral parameters. ", "page_idx": 8}, {"type": "text", "text": "For least squares with additive noise, the learning rate asymptotic $\\begin{array}{r}{\\gamma_{t}\\asymp\\eta/(b^{2}+\\frac{\\omega^{2}}{d}\\mathrm{Tr}(K)t)^{(1/2)}}\\end{array}$ is the fastest decay that AdaGrad-Norm can exhibit. In contrast, the propositions below concern the noiseless case where, for various covariance examples, the decay rate of $\\gamma_{t}$ changes. This is tightly connected to whether the risk is integrable or not. In the simple case of identity covariance, we obtain a closed formula for the trajectory of the integral of the risk and therefore also the learning rate. ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.1. In the case of identity covariance $'K=I_{d})$ , the risk solves the differential equation ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{R}(t)=\\frac{\\eta^{2}\\mathcal{R}(t)}{b^{2}+2\\int_{0}^{t}\\mathcal{R}(s)\\,\\mathrm{d}s}-\\frac{2\\eta\\mathcal{R}(t)}{\\sqrt{b^{2}+2\\int_{0}^{t}\\mathcal{R}(s)\\,\\mathrm{d}s}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The solution $\\textstyle\\int_{0}^{t}{\\mathcal{R}}(s)\\,\\mathrm{d}s$ approaches (from below) a positive constant which yields a computable lower bound to which $\\gamma_{t}$ will converge. Generalizing this to a broader class of covariance matrices, we get the next proposition, which captures the dependence of $\\gamma_{t}$ on ${\\mathrm{Tr}}(K)$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.2. Suppose $\\begin{array}{r}{\\frac{1}{d}\\operatorname{Tr}(K)\\,\\leq\\,b/\\eta,}\\end{array}$ , and that $\\begin{array}{r}{\\int_{0}^{\\infty}\\mathcal{R}(s)\\gamma_{s}\\,\\mathrm{d}s\\,<\\,\\infty}\\end{array}$ with $\\gamma_{s}$ as in Table 2 (AdaGrad-Norm for least squares), then $\\begin{array}{r}{\\gamma_{t}\\asymp\\frac{1}{\\frac{b}{\\eta}+\\frac{\\eta^{2}}{4d}\\operatorname{Tr}(K)\\mathcal{D}^{2}(0)}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "An analog of Proposition 4.2 for the strongly convex setting appears in Sec. D (see Prop. D.1). We now consider two cases in which, as $d\\to\\infty$ , there are eigenvalues of $K$ arbitrarily close to 0. ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.3. Assume that, for some $C>0$ , the number of eigenvalues of $K$ below $C$ is $o(d)$ , and that $\\langle X^{\\star},\\omega_{i}\\rangle=O(d^{-1/2}).$ for all $i$ , (i.e. $X^{\\star}$ is not concentrated in any eigenvector direction). Then, with the initialization $X_{0}=0$ , there exists some $\\tilde{\\gamma}>0$ such that $\\gamma_{t}>\\tilde{\\gamma}$ for all $t>0$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.4. Let $K$ have a spectrum that converges as $d\\rightarrow\\infty$ to the power law measure $\\rho(\\lambda)=(1-\\beta)\\lambda^{-\\beta}\\mathbf{1}_{(0,1)}$ , for some $\\beta<1^{9}$ , and suppose that $\\mathscr{D}_{i}^{2}(0)\\sim\\lambda_{i}^{-\\delta}$ for $\\delta\\geq0$ . Then: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bullet\\ F o r\\,1<\\beta+\\delta<2,\\,\\gamma_{t}\\asymp_{\\delta,\\beta}t_{\\delta,\\beta}^{-1+\\frac{1}{\\beta+\\delta}},\\,a n d\\,\\mathcal{R}(t)\\asymp_{\\delta,\\beta}t^{-\\frac{2}{\\beta+\\delta}+1}f o r\\,a l l\\ t\\geq1.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "9Our result can be compared to existing findings for SGD under power-law distributions in [8, 52, 55]. While these works explore similar assumptions regarding the covariance matrix spectrum, they do not address the high-dimensional regime with diverging $\\operatorname{Tr}(K)$ , focusing primarily on $\\beta>1$ . ", "page_idx": 8}, {"type": "image", "img_path": "4VWnC5unAV/tmp/d61b6e948c73fadd46f83125716beb7e3c73ac47cb17d21f8cba12a2ad9859e6.jpg", "img_caption": ["Figure 4: Power law covariance in AdaGrad Norm on a least squares problem. Ran exact predictions (ODE) for the risk and learning rate (solid lines). Dashed lines give the predictions from Prop. 4.4 which match experimental results exactly. Phase transition as $\\delta+\\beta$ varies. When $\\delta+\\beta<1$ (green), the learning rate $(r i g h t)$ is constant as $t\\to\\infty$ . In contrast, when $2>\\delta+\\beta>1$ (purple), the learning rate decreases at a rate $t^{-1+1/(\\beta+\\delta)}$ with $\\delta+\\beta=1$ (white) where the change occurs. Same phase transition occurs in the sublinear rate of the risk decay (left) (see Prop. 4.4). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "This proposition shows non-trivial decay of the learning rate is dictated by the residuals (distance to optimality at initialization) and the spectrum of $K$ . We note that $\\delta=0$ corresponds to uniform contribution of each mode (e.g. $X_{0}$ normally distributed). As the eigenmodes of the residuals become more localized, the decay of the learning rate is closer to the behaviour in the presence of additive noise. Furthermore, the scaling behaviour of the loss is affected by the structure of the AdaGrad-Norm algorithm (see Fig. 4). Lastly, constant stepsize SGD yields $\\begin{array}{r}{\\mathcal{R}(t)\\asymp t^{\\beta+\\delta-2}}\\end{array}$ , with no transition occurring at $\\beta+\\delta=1$ . ", "page_idx": 9}, {"type": "text", "text": "Proofs of the above propositions, in a slightly more general setting, are deferred to Sec. D. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work studies stochastic adaptive optimization algorithms when data size and parameter size are large, allowing for nonconvex and nonlinear risk functions, as well as data with general covariance structure. The theory shows a concentration of the risk, the learning rate and other key functions to a deterministic limit, which is described by a set of ODEs. The theory is then used to derive the asymptotic behavior of the AdaGrad-Norm and idealized exact line search on strongly convex and least square problems, revealing the influence of the covariance matrix structure on the optimization. A potential extension of this work would be to study other adaptive algorithms such as D-adaptation, DOG, and RMSprop which are covered by the theory. Studying the asymptotic behavior of the risk and the learning rate may improve our understanding of the performance and scalability of these algorithms on more realistic data. Another important application of the theory would be to analyze the ODEs presented here on nonconvex problems. ", "page_idx": 9}, {"type": "text", "text": "The current form of the theory is limited to Gaussian data, though many parts of the proof can be extended easily beyond Gaussian data. The main ODE comparison theorem is also only tuned for analyzing problem setups where the trace of the covariance is on the order of the ambient dimension; when the trace of the covariance is much smaller than ambient dimension, other stepsize scalings of SGD are needed. In addition, the analysis is limited to the streaming stochastic adaptive methods. We conjecture that a similar deterministic equivalent holds also for multi-pass algorithms at least for convex problems. This has already been shown in the least square problem for SGD with a fixed deterministic learning rate [43, 45]. Lastly, numerical simulations on real datasets (e.g., CIFAR- $.5\\mathrm{m}$ ) suggests that the predicted risk derived by our theory matches the empirical risk of multipass SGD beyond Gaussian data (see for example Figure 6). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "E. Collins-Woodfin was supported by Fonds de recherche du Qu\u00e9bec \u2013 Nature et technologies (FRQNT) postdoctoral training scholarship and Centre de recherches math\u00e9matiques (CRM) Applied math postdoctoral fellowship. Research of B. Garc\u00eda Malaxechebarr\u00eda was in part funded by NSF DMS 2023166 (NSF TRIPODS II). Research by E. Paquette was supported by a Discovery Grant from the Natural Science and Engineering Council (NSERC). C. Paquette is a Canadian Institute for Advanced Research (CIFAR) AI chair, Quebec AI Institute (MILA) and a Sloan Research Fellow in Computer Science (2024). C. Paquette was supported by a Discovery Grant from the Natural Science and Engineering Research Council (NSERC) of Canada, NSERC CREATE grant Interdisciplinary Math and Artificial Intelligence Program (INTER-MATH-AI), Google research grant, and Fonds de recherche du Qu\u00e9bec \u2013 Nature et technologies (FRQNT) New University Researcher\u2019s Start-Up Program. Additional revenues related to this work: C. Paquette has $20\\%$ part-time employment at Google DeepMind. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luca Arnaboldi, Florent Krzakala, Bruno Loureiro, and Ludovic Stephan. Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. arXiv preprint arXiv:2305.18502, 2023.   \n[2] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From highdimensional and mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks. arXiv preprint arXiv:2302.05882, 2023.   \n[3] Krishna B Athreya, Peter E Ney, and PE Ney. Branching processes. Courier Corporation, 2004.   \n[4] Amit Attia and Tomer Koren. SGD with adagrad stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance. In International Conference on Machine Learning, pages 1147\u20131171. PMLR, 2023.   \n[5] Krishnakumar Balasubramanian, Promit Ghosal, and Ye He. High-dimensional scaling limits and fluctuations of online least-squares SGD with smooth covariance. arXiv preprint arXiv:2304.00707, 2023.   \n[6] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. In Advances in Neural Information Processing Systems, volume 35, pages 25349\u201325362, New York, 2022. Curran Associates, Inc.   \n[7] Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Training neural networks for and by interpolation. In International conference on machine learning, pages 799\u2013809. PMLR, 2020.   \n[8] R. Berthier, F. Bach, and P. Gaillard. Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[9] Michael Biehl and Peter Riegler. On-line learning with a perceptron. Europhysics Letters, 28 (7):525, 1994.   \n[10] Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. Journal of Physics A: Mathematical and general, 28(3):643, 1995.   \n[11] B. Bordelon and C. Pehlevan. Learning Curves for SGD on Structured Features. In International Conference on Learning Representations (ICLR), 2022.   \n[12] Michael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of first order methods with random data. arXiv preprint arXiv:2112.07572, 2021.   \n[13] Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global convergence guarantees for iterative nonconvex optimization with random data. Ann. Statist., 51(1):179\u2013210, 2023. ISSN 0090-5364,2168-8966. doi: 10.1214/22-aos2246. URL https: //doi.org/10.1214/22-aos2246.   \n[14] Elizabeth Collins-Woodfin and Elliot Paquette. High-dimensional limit of one-pass SGD on least squares. Electronic Communications in Probability, 29:1\u201315, 2024. doi: 10.1214/23-ECP571.   \n[15] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi. Hitting the high-dimensional notes: An ODE for SGD learning dynamics on GLMs and multi-index models. arXiv preprint arXiv:2308.08977, 2023.   \n[16] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models. In Advances in Neural Information Processing Systems, volume 36, pages 752\u2013 784, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 02763667a5761ff92bb15d8751bcd223-Paper-Conference.pdf.   \n[17] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov\u00e1, and Florent Krzakala. The beneftis of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024.   \n[18] Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. In International Conference on Machine Learning, pages 7449\u20137479. PMLR, 2023.   \n[19] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \n[20] Darina Dvinskikh, Aleksandr Ogaltsov, Alexander Gasnikov, Pavel Dvurechensky, Alexander Tyurin, and Vladimir Spokoiny. Adaptive gradient descent for convex and non-convex stochastic optimization. arXiv preprint arXiv:1911.08380, 2019.   \n[21] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform smoothness: A stopped analysis of adaptive SGD. In The Thirty Sixth Annual Conference on Learning Theory, pages 89\u2013160. PMLR, 2023.   \n[22] C\u00e9dric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborov\u00e1. Rigorous dynamical mean-field theory for stochastic gradient descent methods. SIAM Journal on Mathematics of Data Science, 6(2):400\u2013427, 2024. doi: 10.1137/23M1594388. URL https://doi.org/10.1137/23M1594388.   \n[23] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov\u00e1. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. Advances in neural information processing systems, 32, 2019.   \n[24] Sebastian Goldt, Marc M\u00e9zard, Florent Krzakala, and Lenka Zdeborov\u00e1. Modeling the influence of data structure on learning in neural networks: The hidden manifold model. Physical Review X, 10(4):041044, 2020.   \n[25] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. The gaussian equivalence of generative models for learning with shallow neural networks. In Mathematical and Scientific Machine Learning, pages 426\u2013471, New York, New York, USA, 2022. PMLR.   \n[26] Robert M Gower, Aaron Defazio, and Michael Rabbat. Stochastic polyak stepsize with a moving target. arXiv preprint arXiv:2106.11851, 2021.   \n[27] Elad Hazan and Sham Kakade. Revisiting the polyak step size. arXiv preprint arXiv:1905.00313, 2019.   \n[28] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012.   \n[29] Maor Ivgi, Oliver Hinder, and Yair Carmon. DoG is SGD\u2019s best friend: A parameter-free dynamic step size schedule. arXiv preprint arXiv:2302.12022, 2023.   \n[30] Xiaowen Jiang and Sebastian U Stich. Adaptive SGD with polyak stepsize and line-search: Robust convergence and variance reduction. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Kiwon Lee, Andrew N. Cheng, Courtney Paquette, and Elliot Paquette. Trajectory of MiniBatch Momentum: Batch Size Saturation and Convergence in High Dimensions. To Appear in NeurIPS 2022, art. arXiv:2206.01029, June 2022.   \n[32] Kfir Levy. Online to offilne conversions, universality and adaptive minibatch sizes. Advances in Neural Information Processing Systems, 30, 2017.   \n[33] Kfir Y Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and acceleration. Advances in neural information processing systems, 31, 2018.   \n[34] Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 983\u2013992, 2019.   \n[35] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics, pages 1306\u20131314. PMLR, 2021.   \n[36] H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. arXiv preprint arXiv:1002.4908, 2010.   \n[37] F. Mignacco, F. Krzakala, P. Urbani, and L. Zdeborov\u00e1. Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification. In Advances in Neural Information Processing Systems, volume 33, pages 9540\u20139550, 2020.   \n[38] P. Nakkiran, B. Neyshabur, and H. Sedghi. The Deep Bootstrap Framework: Good Online Learners are Good Offilne Generalizers. In International Conference on Learning Representations (ICLR), 2021.   \n[39] Angelia Nedic\u00b4 and Dimitri Bertsekas. Convergence rate of incremental subgradient algorithms. Stochastic optimization: algorithms and applications, pages 223\u2013264, 2001.   \n[40] J. Nocedal and S. J. Wright. Numerical Optimization. Springer New York, 2 edition, 2006.   \n[41] Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of SGD with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. Advances in Neural Information Processing Systems, 35:26943\u201326954, 2022.   \n[42] C. Paquette and K. Scheinberg. A stochastic line search method with expected complexity analysis. SIAM J. Optim., 30(1):349\u2013376, 2020. ISSN 1052-6234. doi: 10.1137/18M1216250. URL https://doi.org/10.1137/18M1216250.   \n[43] C. Paquette, K. Lee, F. Pedregosa, and E. Paquette. SGD in the Large: Average-case Analysis, Asymptotics, and Stepsize Criticality. In Proceedings of Thirty Fourth Conference on Learning Theory (COLT), volume 134, pages 3548\u20133626, 2021.   \n[44] Courtney Paquette and Elliot Paquette. Dynamics of stochastic momentum methods on largescale, quadratic models. In Advances in Neural Information Processing Systems, volume 34, pages 9229\u20139240, 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 4cf0ed8641cfcbbf46784e620a0316fb-Paper.pdf.   \n[45] Courtney Paquette and Elliot Paquette. High-dimensional optimization. SIAM Views and News, 20:16pp, December 2022.   \n[46] Courtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Homogenization of SGD in high-dimensions: Exact dynamics and generalization properties. arXiv e-prints, art. arXiv:2205.07069, May 2022.   \n[47] Courtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Implicit regularization or implicit conditioning? exact risk trajectories of sgd in high dimensions. In Advances in Neural Information Processing Systems, volume 35, pages 35984\u201335999, New York, 2022. Curran Associates, Inc.   \n[49] Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning. Advances in neural information processing systems, 31, 2018.   \n[50] David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. In Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995.   \n[51] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks. Physical Review Letters, 74(21):4337, 1995.   \n[52] A. Varre, L. Pillaud-Vivien, and N. Flammarion. Last iterate convergence of SGD for LeastSquares in the Interpolation regime. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[53] S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 3732\u20133745, 2019.   \n[54] Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien. Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search). arXiv preprint arXiv:2006.06835, 2020.   \n[55] M. Velikanov, D. Kuznedelev, and D. Yarotsky. A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta. In International Conference on Learning Representations (ICLR), 2023.   \n[56] R. Vershynin. High-dimensional probability: An introduction with applications in data science. Cambridge University Press, Cambridge, UK, 2018. doi: 10.1017/9781108231596. URL https://doi.org/10.1017/9781108231596.   \n[57] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual Conference on Learning Theory, pages 161\u2013190. PMLR, 2023.   \n[58] Chuang Wang, Hong Hu, and Yue Lu. A solvable high-dimensional model of GAN. In Advances in Neural Information Processing Systems, volume 32, New York, 2019. Curran Associates, Inc.   \n[59] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. The Journal of Machine Learning Research, 21(1):9047\u20139076, 2020.   \n[60] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In International Conference on Machine Learning, pages 23549\u201323588. PMLR, 2022.   \n[61] Xiaoxia Wu, Rachel Ward, and L\u00e9on Bottou. Wngrad: Learn the learning rate in gradient descent. arXiv preprint arXiv:1803.02865, 2018.   \n[62] Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient descent. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1475\u20131485, 2020.   \n[63] Junchi Yang, Xiang Li, Ilyas Fatkhullin, and Niao He. Two sides of one coin: the limits of untuned SGD and the power of adaptive methods. Advances in Neural Information Processing Systems, 36, 2024.   \n[64] Yuki Yoshida and Masato Okada. Data-dependence of plateau phenomenon in learning with neural network\u2014statistical mechanical analysis. In Advances in Neural Information Processing Systems, volume 32, New York, 2019. Curran Associates, Inc. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Supplementary material ", "page_idx": 14}, {"type": "text", "text": "Broader Impact Statement. The work presented in this paper is foundational research and it is not tied to any particular application. The set-up is on a simple well-studied high-dimensional linear composites (e.g., least squares, logistic regression, phase retrieval) with synthetic data and solved using known algorithms, e.g., AdaGrad-Norm. We present deterministic dynamics for the training loss and adaptive stepsizes. The results are theoretical and we do not anticipate any direct ethical and societal issues. We believe the results will be used by machine learning practitioners and we encourage them to use it to build a more just, prosperous world. ", "page_idx": 14}, {"type": "text", "text": "A SGD adaptive learning rate algorithms and stepsizes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we write down the explicit update rules for 2 different adaptive stochastic gradient descent algorithms. ", "page_idx": 14}, {"type": "text", "text": "Example: AdaGrad-Norm. We begin with AdaGrad-Norm (see Algorithm 1). Note by unraveling the recursion, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{g}_{k}=\\frac{\\eta}{\\sqrt{b^{2}+\\frac{1}{d^{2}}\\sum_{j=0}^{k}\\|\\nabla_{X}\\Psi(X_{j};a_{j+1},\\epsilon_{j+1})\\|^{2}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with the deterministic equivalent (see Section 2 and also C.3) for this learning rate being ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{\\eta}{\\sqrt{b^{2}+\\frac{\\mathrm{Tr}(K)}{d}\\int_{0}^{t}I(\\mathcal{B}(s))\\ \\mathrm{d}s}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the case of the least squares problem, the quantity $I(\\mathcal{B}(t))$ is explicit and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{\\eta}{\\sqrt{b^{2}+\\frac{2\\mathrm{Tr}(K)}{d}\\int_{0}^{t}\\mathcal{R}(s)\\ \\mathrm{d}s}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 AdaGrad-Norm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Require: Initialize $\\eta>0$ , $X_{0}\\in\\mathbb{R}^{d}$ , $b\\in\\mathbb{R}$ and set $b_{0}=b\\times d$ ", "page_idx": 14}, {"type": "text", "text": "Generate new sample $a_{k}\\sim\\mathcal{N}(0,K)$ , $\\epsilon_{k}\\sim\\mathcal{N}(0,\\omega^{2})$ ;   \n$b_{k}^{2}\\leftarrow b_{k-1}^{2}+\\|\\nabla_{\\bar{X}}\\Psi(X_{k-1};a_{k},\\epsilon_{k})\\|^{2}$ ;   \n$\\begin{array}{r}{\\mathfrak{g}_{k-1}=d\\times\\frac{\\eta}{|b_{k}|}}\\end{array}$ ; ", "page_idx": 14}, {"type": "text", "text": "\u25b7updating learning rate ", "page_idx": 14}, {"type": "text", "text": "$\\triangleright$ updating step with stochastic gradient ", "page_idx": 14}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Example: RMSprop-Norm We consider the \"normed\" version of RMSprop, that is, where there is only one learning rate parameter. ", "page_idx": 14}, {"type": "text", "text": "We consider Algorithm 2 where we put a factor of the learning into the exponential moving average for RMSprop. The deterministic equivalent for ${\\mathfrak{g}}_{k}$ for Alg. 2 (see Section 2) is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{\\eta}{\\sqrt{b^{2}e^{-\\alpha t}+\\frac{\\mathrm{Tr}(K)}{d}\\int_{0}^{t}e^{-\\alpha(t-s)}I(\\mathcal{B}(s))\\ \\mathrm{d}s}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the case of the least squares problem, the quantity $I(\\mathcal{B}(t))$ is explicit and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{\\eta}{\\sqrt{b^{2}e^{-\\alpha t}+\\frac{2\\mathrm{Tr}(K)}{d}\\int_{0}^{t}e^{-\\alpha(t-s)}\\mathcal{R}(s)\\ \\mathrm{d}s}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 RMSprop-Norm, $\\alpha$ Exponential Moving Average ", "page_idx": 15}, {"type": "text", "text": "Require: Initialize $\\eta>0$ , $X_{0}\\in\\mathbb{R}^{d}$ , $b\\in\\mathbb{R}$ and set $b_{0}=d\\times b$ , $\\alpha>0$ exponential moving avg. $\\begin{array}{r}{\\mathfrak{g}_{-1}=d\\times\\frac{\\eta}{b_{0}}}\\end{array}$ ; for $k=1,2,\\hdots,\\mathbf{d}$ o Generate new sample $a_{k}\\sim\\mathcal{N}(0,K)$ , $\\epsilon_{k}\\sim\\mathcal{N}(0,\\omega^{2})$ ; $b_{k}^{2}\\gets\\alpha\\cdot b_{k-1}^{2}+(\\bar{1}-\\alpha)\\|\\nabla_{X}\\Psi(X_{k-1};a_{k},\\epsilon_{k})\\|^{2}$ ; $\\begin{array}{r}{\\mathfrak{g}_{k-1}=d\\times\\frac{\\eta}{|b_{k}|}}\\end{array}$ ; \u25b7updating learning rate $\\begin{array}{r}{X_{k}\\leftarrow X_{k-1}-\\frac{\\mathfrak{g}_{k-1}}{d}\\nabla_{X}\\Psi(X_{k-1};a_{k},\\epsilon_{k})}\\end{array}$ ; $\\triangleright$ updating step with stochastic gradient ", "page_idx": 15}, {"type": "text", "text": "B The Dynamical nexus ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove the main theorem on concentration of the risk curves and learning rates. We shall set some notation. In what follows, we again use $W\\,=\\,[X|X^{\\star}]\\,\\in\\,\\mathbb{R}^{d\\times2}$ . We also use $W^{+}=[W|X_{0}]=[X|X^{\\star}|X_{0}]$ . ", "page_idx": 15}, {"type": "text", "text": "We shall also use the shorthand $r\\;=\\;\\langle a,W\\rangle$ , and $x\\ =\\ \\langle a,X\\rangle$ so that $f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle;\\epsilon)\\;=$ $f(\\langle a,W\\rangle;\\epsilon)=f(r;\\epsilon)$ . ", "page_idx": 15}, {"type": "text", "text": "We shall let $B=B(X)=W^{T}K W$ be the covariance matrix of the Gaussian vector $r$ . We also write $f^{\\prime}$ for the $\\partial_{x}f$ . ", "page_idx": 15}, {"type": "text", "text": "B.1 Discussion of the assumptions on $f$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we show how the assumptions we put on $h$ and $I$ are almost satisfied for $L$ -smooth $f$ . We say that $f$ is $L$ -smooth if: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(r_{1},\\epsilon_{1})-\\nabla f(r_{2},\\epsilon_{2})\\|\\leq L\\sqrt{(\\|r_{1}-r_{2}\\|^{2}+\\|\\epsilon_{1}-\\epsilon_{2}\\|^{2})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which we note implies $f$ is $\\alpha$ -pseudo Lipschitz with $\\alpha=1$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. 1. There exists a function $h:\\mathbb{R}^{2\\times2}\\,\\rightarrow\\,\\mathbb{R}$ such that $h(B(X))\\;=\\;\\mathcal{R}(X)$ is differentiable and satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{X}\\mathcal{R}(X)=\\mathbb{E}_{a,\\epsilon}\\nabla_{X}\\Psi(X;a,\\epsilon).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, $h$ is continuously differentiable on $\\{B:\\operatorname*{det}B\\neq0\\}$ and its derivative $\\nabla h$ satisfies an estimate ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla h(B_{1})-\\nabla h(B_{2})\\|\\leq(\\sqrt{2}+1)L(f)\\operatorname*{min}\\{\\|B_{1}^{-1}\\|_{o p},\\|B_{2}^{-1}\\|_{o p}\\}\\|B_{1}-B_{2}\\|_{F}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. The function $I(B)=\\mathbb{E}_{a,\\epsilon}[(f^{\\prime}(\\langle a,X\\rangle;\\langle a,X^{\\star}\\rangle,\\epsilon))^{2}]$ satisfies an estimate ", "page_idx": 15}, {"type": "equation", "text": "$$\n|I(B_{1})-I(B_{2})|\\leq L(f)\\sqrt{I(B_{1})+I(B_{2})}\\operatorname*{min}\\{\\|B_{1}^{-1}\\|_{o p},\\|B_{2}^{-1}\\|_{o p}\\}\\|B_{1}-B_{2}\\|_{F}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. To derive the existence of $h$ , note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}(X)=\\mathbb{E}(\\mathbb{E}(f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle,\\epsilon)|\\epsilon))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is an expectation of a Gaussian vector $r=(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle)$ . This vector can be expressed as an image of an iid Gaussian vector $z$ by representing $r=\\sqrt{B}z$ , and hence we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nh(B)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbb{E}(\\mathbb{E}(f({\\sqrt{B}}z,\\epsilon)|\\epsilon)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the function $f$ is absolutely continuous with a Lipschitz gradient, we can differentiate under the integral sign and conclude ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{X}\\mathcal{R}(X)=\\nabla_{X}\\operatorname{\\mathbb{E}}f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle,\\epsilon)=\\operatorname{\\mathbb{E}}\\nabla_{X}f(\\langle a,X\\rangle,\\langle a,X^{\\star}\\rangle,\\epsilon).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the differe\u221antiability of $h$ , suppose for the moment that $f$ is $C^{2}$ with bounded second derivatives.10 Setting $Q={\\sqrt{B}}$ the positive semi-definite square root of $B$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{Q_{i j}}h(Q^{2})=\\mathbb{E}(\\mathbb{E}(\\partial_{Q_{i j}}f(Q z,\\epsilon)|\\epsilon)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then using the chain rule, and setting $\\partial_{i}f$ to be the $i$ -th partial derivative of $f$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{Q_{i j}}h(Q^{2})=\\mathbb{E}(\\mathbb{E}(z_{j}\\partial_{i}f(Q z,\\epsilon)|\\epsilon))=\\mathbb{E}(\\mathbb{E}([Q_{i j}\\partial_{i}+Q_{j j}\\partial_{j}]\\partial_{i}f(Q z,\\epsilon)|\\epsilon))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have applied Stein\u2019s Lemma. We conclude when $\\operatorname*{det}Q\\neq0$ by the implicit function theorem that $h$ is differentiable and we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{Q_{i j}}h(Q^{2})=\\sum\\partial_{k l}h\\partial_{Q_{i j}}(Q^{2})_{k l}=\\sum_{l}(\\partial_{i l}h)Q_{j l}+\\sum_{k}(\\partial_{k j}h)Q_{i k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a matrix equation, this can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n(D h)Q+Q(D h)=J Q\\quad\\mathrm{where}\\quad J_{k l}=\\mathbb{E}(\\mathbb{E}((\\partial_{k}\\partial_{l}f)(Q z,\\epsilon)|\\epsilon)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is a linear equation in $D h$ . When $Q\\succ0$ , we can define ", "page_idx": 16}, {"type": "equation", "text": "$$\nA=\\int_{0}^{\\infty}e^{-t Q}(J Q)e^{-t Q}\\,\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and note ", "page_idx": 16}, {"type": "equation", "text": "$$\nA Q+Q A=-\\int_{0}^{\\infty}{\\frac{\\mathrm{d}}{\\mathrm{d}t}}\\left(e^{-t Q}(J Q)e^{-t Q}\\right)\\mathrm{d}t=J Q.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, the mapping $\\begin{array}{r}{M\\mapsto\\int_{0}^{\\infty}e^{-t Q}M e^{-t Q}\\,\\mathrm{d}t}\\end{array}$ defines a two-sided inverse for $M\\mapsto M Q+Q M,$ and so ${\\cal D}h={\\cal A}$ . Note that by symmetry of $J,Q$ , and $D h$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ Q=(D h)Q+Q(D h)=Q J,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\n(D h)Q+Q(D h)=\\frac{1}{2}(J Q+Q J),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and so taking inverses on both sides, $D h=J,$ . ", "page_idx": 16}, {"type": "text", "text": "Undoing Stein\u2019s Lemma, we have $Q(D h)=(D h)Q=M$ , where $M_{i j}=\\mathbb{E}(\\mathbb{E}(z_{j}\\partial_{i}f(Q z,\\epsilon)|\\epsilon))$ . From $L$ -smoothness of $f$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|M(Q_{1})-M(Q_{2})\\|\\leq L\\,\\mathbb{E}(\\|z\\|\\|Q_{1}z-Q_{2}z\\|)\\leq\\sqrt{2}L\\|Q_{1}-Q_{2}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|D h(Q_{1}^{2})-D h(Q_{2}^{2})\\|=\\|Q_{1}^{-1}M(Q_{1})-Q_{2}^{-1}M(Q_{2})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|Q_{1}^{-1}\\|_{\\partial p}\\|M(Q_{1})-Q_{1}Q_{2}^{-1}M(Q_{2})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|Q_{1}^{-1}\\|_{\\partial p}\\left(\\|M(Q_{1})-M(Q_{2})\\|+\\|(Q_{2}-Q_{1})Q_{2}^{-1}M(Q_{2})\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note $Q_{2}^{-1}M(Q_{2})=(D h)(Q_{2}^{2})$ is bounded by $L(f)$ , and so we arrive at ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|D h(Q_{1}^{2})-D h(Q_{2}^{2})\\|\\leq(\\sqrt{2}+1)L(f)\\|Q_{1}^{-1}\\|_{o p}\\|Q_{1}-Q_{2}\\|_{F}}\\\\ {\\leq(\\sqrt{2}+1)L(f)\\|Q_{1}^{-2}\\|_{o p}\\|Q_{1}^{2}-Q_{2}^{2}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We note the bound is symmetric in $Q_{1}$ and $Q_{2}$ , and by density of $C^{2}$ in space of $C^{1,l i p}$ , this holds for $L$ -smooth $f$ . This concludes the estimates for the derivative of $h$ . ", "page_idx": 16}, {"type": "text", "text": "For the Fisher matrix, $I(B)$ , from $L$ -smoothness, we have again with $Q={\\sqrt{B}}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nI(Q^{2})=\\mathbb{E}(\\mathbb{E}((\\partial_{1}f(Q z,\\epsilon))^{2}|\\epsilon)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n|I(Q_{1}^{2})-I(Q_{2}^{2})|\\leq\\left|\\mathbb{E}(\\mathbb{E}((\\partial_{1}f(Q_{1}z,\\epsilon))^{2}-(\\partial_{1}f(Q_{2}z,\\epsilon))^{2}|\\epsilon))\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Cauchy-Schwarz and using the $L$ -smoothness of $f$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n|I(Q_{1}^{2})-I(Q_{2}^{2})|\\leq\\sqrt{I(Q_{1}^{2})+I(Q_{2}^{2})}\\times L(f)\\|Q_{1}-Q_{2}\\|_{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This lemma shows that an $L$ -smooth function nearly satisfies Assumption 3 and 4 provided that $\\|B^{-1}\\|_{\\mathrm{op}}$ is bounded. Therefore, our concentration result Theorem B.1 and its Corollaries will hold provided we add a stopping time. Fix $M>0$ and let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hbar_{M}(B)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname*{inf}\\{t>0\\,:\\,\\|B^{-1}\\|_{\\mathrm{op}}>M\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then the concentration of the risk under SGD to a deterministic function, Theorem B.1, holds with $t$ replaced with $t\\wedge\\hbar_{M}(B)\\wedge\\hbar_{M}(\\mathcal{B})$ . The corollaries of Theorem B.1 also follow under this added stopping time. ", "page_idx": 16}, {"type": "text", "text": "In the next section, we prove this concentration theorem, Theorem B.1. ", "page_idx": 16}, {"type": "text", "text": "B.2 Integro-differential equation for $\\mathcal{S}(t,z)$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A goal of this paper is to show that quadratic statistics $\\varphi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ applied to SGD converge to a deterministic function. This argument hinges on understanding the deterministic dynamics of one important statistic, defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\nS(W,z)=W^{\\top}R(z;K)W,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "applied to $W_{\\lfloor t d\\rfloor}$ (SGD updates). Here $W=[X|X^{\\star}]$ and $R(z;K)=(K-z I_{d})^{-1}$ for $z\\in\\mathbb{C}$ is the resolvent of the matrix $K$ . The statistic $S(W,z)$ is valuable because it encodes many other important quantities including $W^{\\top}q(K)W$ for all polynomials $q$ . We show that $S(W_{\\lfloor t d\\rfloor},z)$ , is close to a deterministic function $(t,z)\\mapsto\\mathcal{S}(t,z)$ which satisfies an integro-differential equation. ", "page_idx": 17}, {"type": "text", "text": "To introduce the integro-differential equation, recall by Assumptions 3 and 4 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(X)=h\\circ B(W)\\quad\\mathrm{and}\\quad\\mathbb{E}_{\\mathit{a},\\epsilon}[f^{\\prime}(\\mathfrak{a}^{\\top}W)^{2}]=I\\circ B(W)\\quad\\mathrm{with}\\quad B(W)=W^{\\top}K W,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\alpha$ -pseudo-Lipschitz functions $h:\\,\\mathbb{R}^{2\\times2}\\to\\mathbb{R}$ differentiable and $I\\,:\\,\\mathbb{R}^{2\\times2}\\,\\rightarrow\\,\\mathbb{R}$ . It will be useful, throughout the remaining paper, to express $\\nabla h$ explicitly as a $2\\times2$ matrix, that is, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla h\\cong\\left[\\frac{\\nabla h_{11}\\mid\\nabla h_{12}}{\\nabla h_{21}\\mid\\nabla h_{22}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With these recollections, the integro-differential equation is defined below. ", "page_idx": 17}, {"type": "text", "text": "Integro-Differential Equation for $\\mathcal{S}(t,z)$ . For any contour $\\Omega\\subset\\mathbb{C}$ enclosing the eigenvalues of $K$ , we have an expression for the derivative of $\\mathcal{S}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\mathcal{S}}(t,\\cdot)={\\mathcal{F}}(z,{\\mathcal{S}}(t,\\cdot))\\ \\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}(z,\\mathcal{S}(t,\\cdot))\\overset{\\mathrm{def}}{=}-2\\gamma_{t}\\bigg(\\Big(\\displaystyle\\frac{-1}{2\\pi i}\\oint_{\\Omega}\\mathcal{S}(t,z)\\ \\mathrm{d}z\\Big)H(\\mathcal{B}(t))}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\ H^{T}(\\mathcal{B}(t))\\Big(\\displaystyle\\frac{-1}{2\\pi i}\\oint_{\\Omega}\\mathcal{S}(t,z)\\ \\mathrm{d}z\\Big)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad+\\ \\displaystyle\\frac{\\gamma_{t}^{2}}{d}\\left[\\frac{\\mathrm{Tr}(K R(z;K))I(\\mathcal{B}(t))\\ \\big|0}{0}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\gamma_{t}(\\mathcal{S}(t,z)(2z H(\\mathcal{B}(t)))+(2z H^{T}(\\mathcal{B}(t)))\\mathcal{S}(t,z)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The functions $h:\\,\\mathbb{R}^{2\\times2}\\,\\rightarrow\\,\\mathbb{R}$ and $I\\ :\\ \\mathbb{R}^{2\\times2}\\ \\to\\ \\mathbb{R}$ are defined in Assumption 3 and Assumption 4, respectively. ", "page_idx": 17}, {"type": "text", "text": "We first note that there is an actual solution to the integro-differential equation. This solution is the same as the ODEs defined in the introduction (see (9)) and proved in [15, Lemma 4.1]. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2 (Equivalence to coupled ODEs.). The unique solution of (21) with initial condition (22) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{S}(t,z)=\\frac{1}{d}\\sum_{i=1}^{d}\\frac{1}{\\lambda_{i}-z}\\mathcal{V}_{i}(t).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this section, we will be working with approximate solutions to the integro-differential equation (20) (see below for specifics). For working with these solutions, we introduce some notation. We shall always work on a fixed contour $\\Omega$ surrounding the spectrum of $K$ , given by $\\Omega\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{z:|z|=\\operatorname*{max}\\left\\{1,2\\|K\\|_{\\mathrm{op}}\\right\\}\\right\\}$ . We note that this contour is always distance at least $\\frac{1}{2}$ from the spectrum of $K$ . We define a norm, ${\\big\\|}\\cdot{\\big\\|}_{\\Omega}$ , on a continuous function $A:\\mathbb{C}\\to\\mathbb{R}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|A\\|_{\\Omega}=\\operatorname*{max}_{z\\in\\Omega}\\|A(z)\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition B.1 $((\\varepsilon,M,T)$ -approximate solution to the integro-differential equation). For constants $M,T,\\varepsilon\\,>\\,0$ , we call a continuous function $\\S\\ :\\ [0,\\infty)\\stackrel{\\bullet}{\\times}\\mathbb{C}\\to\\mathbb{R}^{2\\times2}$ an $\\bar{(\\varepsilon,M,T)}$ -approximate solution of (20) if with ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\hat{\\tau}}_{M}(\\vartheta)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname*{inf}{\\biggl\\{}t\\geq0\\,:\\,\\|\\vartheta(t,\\cdot)\\|_{\\Omega}>M{\\biggr\\}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq(\\hat{\\tau}_{M}\\wedge T)}\\left\\|\\mathbb{S}(t,\\cdot)-S(0,\\cdot)-\\int_{0}^{t}\\mathcal{F}(\\cdot,\\mathbb{S}(s,\\cdot))\\ \\mathrm{d}s\\right\\|_{\\Omega}\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\mathcal{S}(0,\\cdot)=W_{0}^{\\top}R(\\cdot,K)W_{0}$ , where $W_{0}=[X_{0}|X^{\\star}]$ is the initialization of SGD. ", "page_idx": 18}, {"type": "text", "text": "We suppress the S in the notation for $\\hat{\\tau}_{M}$ , that is, $\\hat{\\tau}_{M}=\\hat{\\tau}_{M}(\\mathcal{S})$ , when the function S is clear from the context. ", "page_idx": 18}, {"type": "text", "text": "We are now ready to state and prove one of our main results. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.1 (Concentration of SGD and deterministic function $\\mathcal{S}(t,z)\\rangle$ ). Suppose the risk function ${\\mathcal{R}}(X)$ (2) satisfies Assumptions 2, 3, and 4. Suppose the learning rate satisfies Assumption 6, and the initialization $X_{0}$ and hidden parameters $X^{\\star}$ satisfy Assumption 5. Moreover the data $a\\sim\\mathcal{N}(0,K)$ and label noise $\\epsilon$ satisfy Assumption $^{\\,l}$ . Let $\\{W_{\\lfloor t d\\rfloor}\\}$ be generated from the iterates of SGD. Then there is an $\\varepsilon>0$ so that for any $T,M>0$ and $d$ sufficiently large, with overwhelming probability ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\hat{\\tau}_{M}(S(W,\\cdot))\\wedge\\hat{\\tau}_{M}(\\mathcal{S})}\\lVert S(W_{\\lfloor t d\\rfloor},\\cdot)-\\mathcal{S}(t,\\cdot)\\rVert_{\\Omega}\\leq d^{-\\varepsilon},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the deterministic function $\\mathcal{S}(t,z)$ solves the integro-differential equation (20). ", "page_idx": 18}, {"type": "text", "text": "Proof. By Proposition C.1, for any $M$ and $T$ , we can find a $\\tilde{\\varepsilon}>0$ such that the function $S(W_{t d},z)$ is an $(d^{\\bar{-}\\tilde{\\varepsilon}},M,T)$ -approximate solution. (For the deterministic function $\\mathcal{S}$ , it is an $(0,M,T)$ - approximate solution by definition.) We now apply the stability result, [15, Prop. 4.1], to conclude that there exists a $\\varepsilon>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\hat{\\tau}_{M}}\\Vert\\mathcal{S}(t,z)-S(W_{t d},z)\\Vert_{\\Omega}\\leq d^{-\\varepsilon},\\quad w.o.p,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\hat{\\tau}_{M}$ is shorthand for $\\hat{\\tau}_{M}(S(W,\\cdot))\\wedge\\hat{\\tau}_{M}(\\mathcal{S})$ . The result immediately follows. ", "page_idx": 18}, {"type": "text", "text": "Corollary B.1. Suppose the assumptions of Theorem B.1 hold. Let $f$ be an $\\alpha$ -pseudo-Lipschitz function with $\\alpha\\leq1$ and let $q$ be a polynomial. Set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varphi(X)\\stackrel{d e f}{=}f(W^{T}q(K)W),\\quad\\phi(t)\\stackrel{d e f}{=}f\\left(\\frac{-1}{2\\pi i}\\oint_{\\Omega}q(z)\\mathcal{S}(t,z)\\ \\mathrm{d}z\\right),\\quad w h e r e\\ \\mathcal{S}(t,z)\\ s o l v e s\\ (20).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then there is an $\\varepsilon>0$ such that for $d$ sufficiently large, with overwhelming probability, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}|\\varphi\\left(X_{t d}\\right)-\\phi(t)|\\leq d^{-\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. This is basically equivalent to [15, Corollary 4.2]. The only difference is that [15, Corollary 4.2] requires the boundedness of $\\mathcal{N}$ ; however, since our function $f$ is $\\alpha$ -pseudo-Lipschitz with $\\alpha\\leq1$ , this boundedness follows from [15, Proposition 1.2], and the rest of the proof is identical to the one in [15]. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Remark B.1. The learning rate ${\\mathfrak{g}}_{k}$ , technically, is not a function of $W^{T}q(K)W$ . However, Assumption 6 ensures that the learning rate concentrates around a function $W^{T}q(K)W$ . Therefore, Corollary B.1 applies to the learning rate. ", "page_idx": 18}, {"type": "text", "text": "C SGD-AL is an approximate solution ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We introduce a rescaling of time to relate the $k$ -th iteration of SGD to the continuous time parameter $t$ in the differential equation through the relationship $k=\\lfloor t d\\rfloor$ . Thus, when $t=1$ , SGD has done exactly $d$ updates. Since the parameter $t$ is continuous and the iteration counter $k$ (integer) discrete, to simplify the discussion below, we extend $k$ to continuous values through the floor operation, $X_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ X_{\\lfloor k\\rfloor}$ . Using the continuous parameter $t$ , the iterates are related by $X_{t d}=X_{\\lfloor t d\\rfloor}$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "The paper [15] provides a net argument showing that we do not need to work with every $z$ on the contour $\\Omega$ defining the integro-differential equation, but only polynomially many in $d$ . Recall that $\\Omega=\\{z:|z|=\\operatorname*{max}\\{2\\|K\\|_{\\mathrm{op}},1\\}\\}$ . For a fixed $\\xi>0$ , we say that $\\Omega_{\\xi}$ is a ${\\dot{d}}^{-\\xi}$ -mesh of $\\Omega$ if $\\Omega_{\\xi}\\subset\\Omega$ and for every $z\\in\\Omega$ there exists a $\\bar{z}\\in\\Omega_{\\xi}$ such that $|z-{\\bar{z}}|<d^{-\\xi}$ . We can achieve this with $\\Omega_{\\xi}$ having cardinality, $|\\Omega_{\\xi}|=C(|\\Omega|)d^{\\xi}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.1 (Net argument, [15], Lemma 5.1). $F i x\\,T,M>0$ and let $\\xi>0$ . Suppose $\\Omega_{\\xi}$ is a $d^{-\\xi}$ mesh of $\\Omega$ with $|\\Omega_{\\xi}|=C\\cdot d^{\\xi}$ and positive $C>0$ . Let the function $S(t,z)=S(W_{t d},z)$ satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq(\\widehat{\\tau}_{M}\\wedge T)}\\|S(t,\\cdot)-S(0,\\cdot)-\\int_{0}^{t}\\mathcal{F}(\\cdot,S(s,\\cdot))\\ \\mathrm{d}s\\|_{\\Omega_{\\xi}}\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ${\\hat{\\tau}}_{M}\\;=\\;\\operatorname*{inf}\\{t\\;\\geq\\;0\\;:\\;\\;\\|S(t,\\cdot)\\|_{\\Omega}\\;>\\;M\\}$ . Then $S$ is a $(\\varepsilon+C(M,T,\\|K\\|_{o p})d^{-\\xi},M,T)$ - approximate solution to the integro-differential equation, that is, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq(\\hat{\\tau}_{M}\\wedge T)}\\|S(t,\\cdot)-S(0,\\cdot)-\\int_{0}^{t}\\mathcal{F}(\\cdot,S(s,\\cdot))\\ \\mathrm{d}s\\|_{\\Omega}\\leq\\varepsilon+C\\cdot d^{-\\xi},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C=C(M,T,\\|K\\|_{o p},L(I),L(h))$ is a positive constant. ", "page_idx": 19}, {"type": "text", "text": "(We prove in Section C.1 that $S(t,z)$ does indeed satisfy inequality (26).) We also cite the following lemma, which relates two stopping times used throughout this paper. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2 (Stopping time, [15], Lemma 4.2). For a constant $C$ depending on $\\|K\\|_{o p;}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nC\\leq\\frac{\\|S(W_{t d},\\cdot)\\|_{\\Omega}}{\\|W_{t d}\\|^{2}}\\leq2.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark C.1. Fix $M>0$ and define the stopping time on $\\|W_{t d}\\|,\\,\\vartheta=\\vartheta_{M},$ , by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\vartheta_{M}(W_{t d})\\stackrel{d e f}{=}\\operatorname*{inf}\\left\\{t\\geq0:\\|W_{t d}\\|^{2}>M\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Due to the previous lemma, any stopping time $\\hat{\\tau}_{M}$ defined on $\\|S(t,\\cdot)\\|_{\\Omega}$ corresponds to a stopping time $\\vartheta$ on $\\|W_{t d}\\|$ , that is, for $c=C^{-1}$ , $\\hat{\\tau}_{M}\\leq\\vartheta_{c M}$ . ", "page_idx": 19}, {"type": "text", "text": "C.1 SGD-AL is an approximated solution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition C.1 (SGD-AL is an approximate solution). Fix a $T,M>0$ and $0<\\varepsilon<\\delta/8,$ , where $\\delta$ is defined in Assumption $^{6}$ . Then $S(W_{t d},z)$ is a $(d^{-\\varepsilon},M,T)$ -approximate solution w.o.p., that is, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq(T\\wedge\\tau_{M})}\\|S(W_{t d},z)-S(W_{0},z)-\\int_{0}^{t}\\mathcal{F}(z,S(W_{s d},z))\\ \\mathrm{d}s\\|_{\\Omega}\\leq d^{-\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again, the proof is very similar to [15, Prop. 5.2]. The one difference is that the martingales and error terms are slightly more involved, because of the non-deterministic stepsize we are using. The remainder of this section, along with section C.2, fills in the details of bounding these lower-order terms, so that the proof can proceed as in [15]. ", "page_idx": 19}, {"type": "text", "text": "C.1.1 Shorthand notation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the following sections, we will be using various versions of the stepsize $\\gamma$ . In order to simplify notation, we set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(G_{k})=\\gamma(k,N_{k}(d\\times\\cdot),G_{k}(d\\times\\cdot),Q_{k}(d\\times\\cdot)),}\\\\ &{\\gamma(\\mathcal{G}_{k})=\\gamma(k,N_{k}(d\\times\\cdot),\\mathcal{G}_{k}(d\\times\\cdot),Q_{k}(d\\times\\cdot)),}\\\\ &{\\gamma(B_{k})=\\gamma(k,N_{k}(d\\times\\cdot),\\mathrm{Tr}(K)I(B_{k}(d\\times\\cdot))/d,Q_{k}(d\\times\\cdot)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Further, setting $\\Delta_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ f^{\\prime}(r_{k})a_{k+1}$ , define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\mathrm{\\boldmath~\\Xi~}}(k)\\stackrel{\\mathrm{def}}{=}\\Delta_{k}^{\\top}\\nabla^{2}\\varphi(X_{k})\\Delta_{k}/d,\\ I_{2}(k)\\stackrel{\\mathrm{def}}{=}\\mathrm{Tr}(\\nabla^{2}\\varphi(X_{k})K){\\mathbb E}\\left[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}\\right]/d,\\ I_{3}(k)\\stackrel{\\mathrm{def}}{=}\\nabla\\varphi(X_{k})^{\\top}\\Delta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The normalization here (dividing by $d$ ) is chosen so that the $I$ terms are all $O(1)$ ; this is formally shown in Lemma C.5. ", "page_idx": 19}, {"type": "text", "text": "C.1.2 SGD-AL under the statistic ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We follow the approach in [15, Section 5.3] to rewrite the SGD adaptive learning rate update rule as an integral equation. Considering a quadratic function $\\varphi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and performing Taylor expansion, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varphi(X_{k+1})=\\varphi(X_{k})-{\\frac{\\gamma(G_{k})}{d}}\\nabla\\varphi(X_{k})^{\\top}\\Delta_{k}+{\\frac{\\gamma(G_{k})^{2}}{2d^{2}}}\\Delta_{k}^{\\top}\\nabla^{2}\\varphi(X_{k})\\Delta_{k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We will now relate this equation to its expectation by performing a Doob decomposition, involving the following martingale increments and error terms: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\Delta\\mathcal{M}_{k}^{\\mathrm{grad}}(\\varphi)\\stackrel{\\mathrm{def}}{=}\\frac{1}{d}\\left(-\\gamma(G_{k})I_{3}(k)+\\mathbb{E}\\left[\\gamma(G_{k})I_{3}(k)\\,\\big|\\,\\mathcal{F}_{k}\\right]\\right),}\\\\ &{\\,\\Delta\\mathcal{M}_{k}^{\\mathrm{Hess}}(\\varphi)\\stackrel{\\mathrm{def}}{=}\\frac{1}{2d}\\left(\\gamma(G_{k})^{2}I_{1}(k)-\\mathbb{E}\\left[\\gamma(G_{k})^{2}I_{1}(k)\\,\\big|\\,\\mathcal{F}_{k}\\right]\\right),}\\\\ &{\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{Hess}}(\\varphi)\\,\\big|\\,\\mathcal{F}_{k}\\right]\\stackrel{\\mathrm{def}}{=}\\frac{1}{2d}\\left(\\mathbb{E}\\,\\left[\\gamma(G_{k})^{2}I_{1}(k)\\,\\big|\\,\\mathcal{F}_{k}\\right]-\\gamma(B_{k})^{2}I_{2}(k)\\right),}\\\\ &{\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{grad}}(\\varphi)\\,\\big|\\,\\mathcal{F}_{k}\\right]\\stackrel{\\mathrm{def}}{=}\\frac{1}{d}\\left(-\\mathbb{E}\\,\\left[\\gamma(G_{k})I_{3}(k)\\,\\big|\\,\\mathcal{F}_{k}\\right]+\\gamma(B_{k})\\nabla\\varphi(X_{k})^{\\top}\\nabla\\mathcal{R}(X_{k})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can then write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\varphi}(X_{k+1})=\\boldsymbol{\\varphi}(X_{k})-\\frac{\\gamma(B_{k})}{d}{\\boldsymbol{\\nabla}}\\boldsymbol{\\varphi}(X_{k})^{\\top}{\\boldsymbol{\\nabla}}\\boldsymbol{\\mathcal{R}}(X_{k})+\\frac{\\gamma(B_{k})^{2}}{2d^{2}}\\operatorname{Tr}({\\boldsymbol{\\nabla}}^{2}\\boldsymbol{\\varphi}(X_{k}){\\boldsymbol{K}})\\mathbb{E}\\left[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad+\\,\\Delta\\mathcal{M}_{k}^{\\mathrm{grad}}(\\boldsymbol{\\varphi})+\\Delta\\mathcal{M}_{k}^{\\mathrm{Hess}}(\\boldsymbol{\\varphi})+\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{Hess}}(\\boldsymbol{\\varphi})\\,|\\,\\mathcal{F}_{k}\\right]+\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{grad}}(\\boldsymbol{\\varphi})\\,|\\,\\mathcal{F}_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Extending $X_{k}$ into continuous time by defining $X_{t}=X_{\\lfloor t\\rfloor}$ , we sum up (integrate). For this, we introduce the forward difference ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\Delta\\varphi)(X_{j})\\stackrel{\\mathrm{def}}{=}\\varphi(X_{j+1})-\\varphi(X_{j}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "giving us ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varphi(X_{t d})=\\varphi(X_{0})+\\sum_{j=0}^{\\lfloor t d\\rfloor-1}(\\Delta\\varphi)(X_{j})\\overset{\\mathrm{def}}{=}\\varphi(X_{0})+\\int_{0}^{t}d\\cdot(\\Delta\\varphi)(X_{s d})\\,\\,\\mathrm{d}s+\\xi_{t d},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\left|\\xi_{t d}\\right|=\\bigg|\\int_{(\\lfloor t d\\rfloor-1)/d}^{t}d\\cdot\\Delta\\varphi(X_{s d})\\ \\mathrm{d}s\\bigg|\\leq\\operatorname*{max}_{0\\leq j\\leq\\lceil t d\\rceil}\\{\\left|\\Delta\\varphi(X_{j})\\right|\\}.$ . With this, we obtain the Doob decomposition for SGD-AL: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\varphi(X_{t d})=\\varphi(X_{0})-\\int_{0}^{t}\\gamma(B_{s d})\\nabla\\varphi(X_{s d})^{\\top}\\nabla{\\mathcal R}(X_{s d})\\,\\,\\mathrm{d}s}\\\\ &{\\displaystyle\\qquad\\qquad+\\,\\frac{1}{2d}\\int_{0}^{t}\\gamma(B_{s d})^{2}\\,\\mathrm{Tr}(K\\nabla^{2}\\varphi(X_{s d}))\\mathbb{E}\\left[f^{\\prime}(r_{s d})^{2}\\,\\big|\\,{\\mathcal F}_{s d}\\right]\\,\\mathrm{d}s}\\\\ &{\\displaystyle\\qquad\\qquad\\downarrow\\lfloor t d\\rfloor-1\\,\\,\\xi_{j}^{\\mathrm{adl}}(\\varphi),}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\downarrow\\bigcup_{j=0}^{t}\\xi_{j}^{\\mathrm{al}}(\\varphi),}\\\\ {\\mathrm{with}}&{\\displaystyle\\mathcal E_{j}^{\\mathrm{al}}(\\varphi)=\\Delta{\\mathcal M}_{j}^{\\mathrm{grad}}(\\varphi)+\\Delta{\\mathcal M}_{j}^{\\mathrm{Hes}}(\\varphi)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\big|\\mathbb{E}\\big[\\xi_{j}^{\\mathrm{Hes}}(\\varphi)\\,\\big|\\,{\\mathcal F}_{j}\\big]+\\mathbb{E}\\left[\\xi_{j}^{\\mathrm{grad}}(\\varphi)\\,\\big|\\,{\\mathcal F}_{j}\\right]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\,\\xi_{j}t(\\varphi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From here, we can proceed as in [15, Section 5.3] to show that SGD-AL is an $(\\varepsilon,M,T)$ -approximated solution. ", "page_idx": 20}, {"type": "text", "text": "C.1.3 $S(W_{t d},z)$ is an approximate solution ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Proposition $C.I$ . The appropriate stepsize, as a function of $W_{t d}$ , is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\gamma(t d,N_{t d},\\mathrm{Tr}(K)I(B_{t d})/d,Q_{t d}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(Note that $N,I$ and $Q$ can all be found as functions of $S(W_{t d},\\cdot)$ using contour integration.) It is shown in the proof of [15, Proposition 5.2] that given the analogue of (33) for deterministic stepsize, $S(W_{t d},\\cdot)$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\nS\\left(W_{t d},z\\right)=S\\left(W_{0},z\\right)+\\int_{0}^{t}\\mathcal{F}\\left(z,S\\left(W_{s d},z\\right)\\right)\\mathrm{d}s+\\sum_{i=0}^{\\lfloor t d\\rfloor-1}\\mathcal{E}_{j}^{\\mathrm{all}}(S).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The only terms of (33) that differ in our case are the martingale and error terms. Thus to show that $S(W_{t d},\\cdot)$ is an approximate solution of the integro-differential equation (20) all we need is to pbroeuvnido utshley . mWaret itnhguasl ehsa vaen dt heartr foor rt earllm t,ained in $\\bar{\\mathcal{E}}_{j}^{\\mathrm{all}}$ . Let $\\Omega=\\{z:\\bar{|}z|=\\operatorname*{max}\\{1,2\\|K\\|_{\\mathrm{op}}\\}\\}$ , as $z\\in\\Omega$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\hat{\\tau}_{M}}\\bigg|S(W_{t d},z)-S(W_{0},z)-\\int_{0}^{t}\\mathcal{F}(z,S(W_{s d},z))\\ \\mathrm{d}s\\bigg|\\,\\leq\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\hat{\\tau}_{M}}\\|\\mathcal{E}_{t d}^{\\mathrm{all}}(S(\\cdot,z))\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, fix a constant $\\xi>0$ . Let $\\Omega_{\\xi}\\subset\\Omega$ such that there exists a $\\bar{z}\\in\\Omega_{\\xi}$ such that $|z-{\\bar{z}}|\\leq d^{-\\xi}$ and the cardinality of $\\Omega_{\\xi}$ , $|\\Omega_{\\xi}|=C d^{\\xi}$ where $C>0$ can depend on $\\|K\\|_{\\mathrm{op}}$ . For all $z\\in\\Omega$ , we note that $\\hat{\\tau}_{M}\\leq\\vartheta_{c M}$ (see Lemma C.2). Consequently, we evaluate the error with the stopped process $W_{t d}^{\\vartheta}\\stackrel{\\mathrm{def}}{=}W_{d(t\\wedge\\vartheta)}$ instead of using $\\hat{\\tau}_{M}$ . By Proposition C.2, the proof of which we have deferred to Section C.2, we have, for any $\\hat{\\delta}>0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in\\Omega_{\\xi}}\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta_{c M}}\\|\\mathcal{E}_{d t}^{\\mathrm{all}}(S(\\cdot,z))\\|\\leq d^{-\\delta/4+\\hat{\\delta}}\\quad\\mathrm{w.o.p.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We deduce that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\hat{\\tau}_{M}}\\|S(W_{t d},z)-S(W_{0},z)-\\int_{0}^{t}\\mathcal{F}(z,S(W_{s d},z))\\ \\mathrm{d}s\\|_{\\Omega_{\\xi}}\\leq d^{\\hat{\\delta}-\\delta/4}\\quad\\mathrm{w.o.p.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "An application of the net argument, Lemma C.1, finishes the proof after setting $\\hat{\\delta}~=~\\delta/8$ and $\\xi=\\delta/8$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.2 Error bounds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All the martingale and error terms (34) go to 0 as $d$ grows. Formally, ", "page_idx": 21}, {"type": "text", "text": "Proposition C.2. Let the function $f$ be defined as in Assumption 2. Let the statistic $S:[0,\\infty)\\times\\mathbb{C}\\to$ $\\mathbb{R}^{2\\times2}$ be defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\nS(t,z)=W_{\\lfloor t d\\rfloor}^{\\top}R(z;K)W_{\\lfloor t d\\rfloor},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $W=[X|X^{\\star}]$ . Then, for any $z\\in\\Omega$ and $T,M,\\zeta>0,$ , with overwhelming probability, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta}\\left\\|\\mathcal{E}_{d t}^{a l l}(S(\\cdot,z))\\right\\|\\leq d^{-\\delta/4+\\zeta},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where to suppress notation we use $\\vartheta$ as shorthand for $\\vartheta_{c M}$ , and $c$ is the constant from Lemma C.2. ", "page_idx": 21}, {"type": "text", "text": "Proof. This follows from combining Propositions C.3, C.4, C.5, C.6, and C.7. ", "page_idx": 21}, {"type": "text", "text": "The remainder of this subsection is devoted to proving these supporting propositions; throughout these proofs we will work with the stopping time $\\vartheta$ as defined in the proposition above. ", "page_idx": 21}, {"type": "text", "text": "C.2.1 Bounds on the lower order terms in the gradient and hessian ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition C.3 (Hessian error term). Let $f$ and $S$ be defined as in Assumption 2 and (37). Then, for any $z\\in\\Omega,\\,T>0$ and $\\zeta>0$ , with overwhelming probability, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta}\\sum_{k=0}^{\\lfloor t d\\rfloor-1}\\left\\|\\mathbb{E}\\left[\\mathcal{E}_{k}^{H e s s}(S(\\cdot,z))\\mid\\mathcal{F}_{k}\\right]\\right\\|\\leq d^{-\\delta/4+\\zeta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For arbitrary $z\\in\\Omega$ and $k\\leq(T\\wedge\\vartheta)d-1$ , set $\\varphi(X)=S_{i j}(W,z)$ to be the $i j$ -th entry of the matrix $S(W,z)$ . Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2d\\mathbb{E}[\\mathcal{E}_{k}^{\\mathrm{Hess}}(\\varphi)\\,|\\,\\mathcal{F}_{k}]=\\mathbb{E}\\left[\\gamma(G_{k})^{2}I_{1}(k)\\,|\\,\\mathcal{F}_{k}\\right]-\\gamma(B_{k})^{2}I_{2}(k)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}[(\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2})I_{1}(k)\\,|\\,\\mathcal{F}_{k}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,(\\gamma(\\mathcal{G}_{k})^{2}-\\gamma(B_{k})^{2})\\,\\mathbb{E}[I_{1}(k)\\,|\\,\\mathcal{F}_{k}]+\\gamma(B_{k})^{2}\\,\\mathbb{E}[(I_{1}(k)-I_{2}(k)\\,|\\,\\mathcal{F}_{k}]}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{E}_{1}+\\mathcal{E}_{2}+\\mathcal{E}_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We look at $|\\mathcal{E}_{1}|$ first. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{E}_{1}|=\\left|\\mathbb{E}\\left[(\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2})I_{1}(k)\\mid\\mathcal{F}_{k}\\right]\\right|}\\\\ &{\\quad\\quad\\leq\\mathbb{E}\\,\\left[\\left|(\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2})\\right|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\,\\left[\\left|I_{1}(k)\\,|^{2}\\,\\mathcal{F}_{k}\\right|\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\leq\\mathbb{E}\\,\\left[|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})|^{\\frac{7}{2}}\\,|\\,\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|^{\\frac{1}{2}}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\,\\left[\\left|I_{1}(k)\\,|^{2}\\,\\mathcal{F}_{k}\\right|\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\leq\\mathbb{E}\\,\\left[|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})|^{\\gamma}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}\\cdot\\mathbb{E}\\,\\left[\\left|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})\\right|\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}\\cdot\\mathbb{E}\\,\\left[\\left|I_{1}(k)\\right|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the first term, we use (6). We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{L}\\left[|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})|^{\\top}\\;|\\;\\mathcal{F}_{k}\\right]\\le\\hat{C}(\\gamma)\\cdot\\mathbb{E}\\;\\left[|2+2\\|N_{k}\\|_{\\infty}^{\\alpha}+2\\|Q_{k}\\|_{\\infty}^{\\alpha}+\\|G_{k}\\|_{\\infty}^{\\alpha}+\\|\\mathcal{G}_{k}\\|_{\\infty}^{\\alpha}|^{\\top}\\;|\\;\\mathcal{F}_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "All the terms inside the expectation, apart from $\\|G_{k}\\|_{\\infty}^{\\alpha}$ , are deterministic with respect to $\\mathcal{F}_{k}$ and bounded by a constant independent of $d$ (see Lemma C.6). Since we know from Lemma C.6 that for any $\\varepsilon>0$ , all moments of $\\|G_{k}\\|_{\\infty}$ are bounded by $d^{\\varepsilon}$ w.o.p., we conclude ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})\\right|^{7}\\,|\\,\\mathcal{F}_{k}\\right]\\le d^{\\varepsilon}\\quad\\mathrm{w.o.p.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the second term, we use (5). Again, since $\\|N_{k}\\|_{\\infty}$ and $\\|Q_{k}\\|_{\\infty}$ are bounded due to our stopping time, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}\\leq d^{-\\delta/4}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last term, E $\\left[\\left|I_{1}(k)\\right|^{2}\\,\\left|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}$ , is also bounded by a constant (see Lemma C.5), and all together, we find that $|\\mathcal{E}_{1}|\\leq d^{\\varepsilon-\\delta/4}$ with overwhelming probability. ", "page_idx": 22}, {"type": "text", "text": "Now let us consider $|\\mathcal{E}_{2}|$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2}|=|(\\gamma(\\mathcal{G}_{k})^{2}-\\gamma(B_{k})^{2})\\mathbb{E}[I_{1}(k)\\,|\\,\\mathcal{F}_{k}]|=|\\gamma(\\mathcal{G}_{k})+\\gamma(B_{k})|\\cdot|\\gamma(\\mathcal{G}_{k})-\\gamma(B_{k})|\\cdot|\\mathbb{E}[I_{1}(k)\\,|\\,\\mathcal{F}_{k}]|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first term is bounded by (6), since $\\mathcal{G}_{k}$ and $\\mathrm{Tr}(K)I(B_{k})/d$ are bounded independent of $d$ ; the second term is bounded $\\stackrel{\\cdot}{C}d^{-1}$ by Lemma C.9, and the last term is bounded by a constant by Lemma C.5. ", "page_idx": 22}, {"type": "text", "text": "Finally, consider $\\left|\\mathcal{E}_{3}\\right|$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathcal{E}_{3}|=\\gamma(B_{k})^{2}\\cdot|\\mathbb{E}\\left[\\left(I_{1}(k)-I_{2}(k)\\,|\\,\\mathcal{F}_{k}\\right]|\\,.\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By (6), the first term is bounded by $\\hat{C}(\\gamma)^{2}(1+\\|N_{k}\\|_{\\infty}^{\\alpha}+\\|Q_{k}\\|_{\\infty}^{\\alpha}+\\|\\operatorname{Tr}(K)I(B_{k})/d\\|_{\\infty}^{\\alpha})^{2}$ . All of these terms are bounded by a constant independent of $d$ (because of the stopping time.) The second term satisfies the assumptions of Lemma C.8 with $H=\\nabla^{2}\\varphi(X_{k})$ , and is thus bounded by $C d^{-1}$ . All together, ", "page_idx": 22}, {"type": "equation", "text": "$$\n2d\\mathbb{E}[\\mathcal{E}_{k}^{\\mathrm{Hess}}(\\varphi)\\,|\\,\\mathcal{F}_{k}]\\le d^{-\\delta/4+\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Summing up to $k=T d$ and dividing through by $2d$ , we obtain the desired bound. ", "page_idx": 22}, {"type": "text", "text": "Proposition C.4 (Gradient error term). Let $f$ and $S$ be defined as in Assumption 2 and (37). Then, for any $z\\in\\Omega,\\,\\zeta>0$ and $T>0$ , with overwhelming probability, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta}\\sum_{k=0}^{\\lfloor t d\\rfloor-1}\\left\\|\\mathbb{E}\\left[\\mathcal{E}_{k}^{g r a d}(S(\\cdot,z))\\mid\\mathcal{F}_{k}\\right]\\right\\|\\leq d^{-\\delta/4+\\zeta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{grad}}\\,|\\,\\mathcal{F}_{k}\\right]=-\\mathbb{E}\\left[\\gamma(G_{k})\\langle\\nabla\\varphi(X_{k}),\\Delta_{k}\\rangle\\,|\\,\\mathcal{F}_{k}\\right]+\\gamma(B_{k})\\langle\\nabla\\varphi(X_{k}),\\nabla R(X_{k})\\rangle}\\\\ &{\\qquad\\qquad\\qquad=-\\mathbb{E}\\left[(\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k}))I_{3}(k)\\,|\\,\\mathcal{F}_{k}\\right]-(\\gamma(\\mathcal{G}_{k})-\\gamma(B_{k})\\mathbb{E}\\left[I_{3}(k)\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{E}_{1}+\\mathcal{E}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We then have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{E}_{1}|\\leq\\mathbb{E}\\,\\left[\\left|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})\\right|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\,\\left[\\left|I_{3}(k)\\right|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad\\leq\\mathbb{E}\\,\\left[\\left|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})\\right|^{3}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}\\cdot\\mathbb{E}\\,\\left[\\left|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})\\right|\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}\\cdot\\mathbb{E}\\,\\left[\\left|I_{3}(k)\\right|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Just as in the Hessian argument, (6) lets us bound $\\mathbb{E}\\,\\left[|\\gamma(G_{k})+\\gamma(\\mathcal{G}_{k})|^{3}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{4}}$ by $d^{\\varepsilon}$ w.o.p., (5) lets us bound $\\mathbb{E}\\;[|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|\\;|\\;\\mathcal{F}_{k}]^{\\frac{1}{4}}$ by $d^{-\\delta/4}$ w.o.p., and Lemma C.5 lets us bound E $\\stackrel{!}{\\underset{\\r{i}}{\\left[\\left|I_{3}(k)\\right|^{2}\\;|\\;\\mathcal{F}_{k}\\right]}^{\\frac{1}{2}}}$ by a constant, giving an overall bound of $|\\mathcal{E}_{1}|\\leq d^{-\\delta/4+\\varepsilon}$ . ", "page_idx": 23}, {"type": "text", "text": "By the same argument as in the Hessian case, $|\\mathcal{E}_{2}|$ is bounded by $C d^{-1}$ ; in conclusion, ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\mathbb{E}\\left[\\mathcal{E}_{k}^{\\mathrm{grad}}\\,|\\,\\mathcal{F}_{k}\\right]\\le d^{\\varepsilon-\\delta/4}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing and dividing through by $d$ , we obtain the desired result with $\\zeta=\\varepsilon$ . ", "page_idx": 23}, {"type": "text", "text": "Proposition C.5 (Gradient martingale). Let $f$ and $S$ be defined as in Assumption 2 and (37). Then, for any $z\\in\\Omega,\\,\\zeta>0$ and $T>0$ , with overwhelming probability, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta}\\left\\|\\mathcal{M}_{\\lfloor d t\\rfloor}^{\\mathrm{grad}}(S(\\cdot,z))\\right\\|\\leq d^{-1/2+\\zeta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For notational convenience, set $\\Delta\\mathcal{M}_{k}=\\Delta\\mathcal{M}_{d(k/d\\wedge\\vartheta)}^{\\mathrm{grad}}.$ , and $F_{k}=-\\gamma(G_{k})I_{3}(k)/d$ , so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathcal{M}_{k}=F_{k}-\\mathbb{E}[F_{k}\\,|\\,\\mathcal{F}_{k}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set $F_{k}^{\\beta}=\\mathbf{Proj}_{\\beta}(F_{k})$ , that is, ensuring $F_{k}$ stays in $[-\\beta,\\beta]$ . Then $F_{k}^{\\beta}-\\mathbb{E}[F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]$ is in $[-2\\beta,2\\beta]$ , and so for the martingale $\\mathcal{M}_{k}^{\\beta}$ with increments $\\Delta\\mathcal{M}_{k}^{\\beta}=F_{k}^{\\beta}-\\mathbb{E}[F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]$ , Azuma\u2019s inequality tells us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mathcal{M}_{k}^{\\beta}|\\geq t\\right)\\leq2\\exp\\left(\\frac{-t^{2}}{2\\sum_{i=0}^{k}(2\\beta)^{2}}\\right)\\leq2\\exp\\left(\\frac{-t^{2}}{2T d(2\\beta)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set $\\beta=d^{-1+\\zeta/2}$ and $t=d^{-1/2+\\zeta}$ ; this becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mathcal{M}_{k}^{\\beta}|\\geq d^{-1/2+\\zeta}\\right)\\leq2\\exp\\left(\\frac{-d^{\\zeta}}{8T}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "However, $\\mathcal{M}_{k}^{\\beta}$ is not quite the martingale we started with: there is still an error term, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{M}_{k}-\\mathcal{M}_{k}^{\\beta}|=\\displaystyle\\left|\\sum_{i=0}^{k}(F_{k}-{\\mathbb{E}}[F_{k}\\,|\\,\\mathcal{F}_{k}])-(F_{k}^{\\beta}-{\\mathbb{E}}[F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}])\\right|}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{i=0}^{k}\\left|F_{k}-F_{k}^{\\beta}\\right|+\\left|{\\mathbb{E}}[F_{k}-F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We bound this term in overwhelming probability. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(F_{k}-F_{k}^{\\beta}\\neq0\\right)=\\mathbb{P}\\left(|F_{k}|>\\beta\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{P}\\left(|\\gamma(G_{k})I_{3}(k)/d|>d^{-1+\\zeta/2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\left(\\gamma(G_{k})\\geq d^{\\zeta/4}\\right)+\\mathbb{P}\\left(|I_{3}(k)|\\geq d^{\\zeta/4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second term is superpolynomially small by Lemma C.5; the first term is superpolynomially small by (6) and (C.6). ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}[F_{k}-F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]\\right|=\\left|\\mathbb{E}[(F_{k}-F_{k}^{\\beta})\\mathbf{1}_{\\{|F_{k}|>\\beta\\}}\\,|\\,\\mathcal{F}_{k}]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}[(F_{k}-F_{k}^{\\beta})^{2}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{k}|>\\beta\\}}^{2}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leq4\\,\\mathbb{E}[F_{k}^{2}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{k}|>\\beta\\}}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leq4d^{-1}\\,\\mathbb{E}[\\gamma(G_{k})^{4}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{4}}\\cdot\\mathbb{E}[I_{3}(k)^{4}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{4}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{k}|>\\beta\\}}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As before, the first and second expectations are bounded by constants, and the last expectation is just the probability that $|F_{k}|>\\beta$ , which we have already shown is superpolynomially small. So with overwhelming probability, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\mathcal{M}_{k}-\\mathcal{M}_{k}^{\\beta}|=\\left|\\sum_{i=0}^{k}(F_{k}-{\\mathbb{E}}[F_{k}\\,|\\,\\mathcal{F}_{k}])-(F_{k}^{\\beta}-{\\mathbb{E}}[F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}])\\right|\\leq d^{-1/2+\\zeta}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(any power of $d$ would have worked). Combining the error term and the projected martingale, we find that, with overwhelming probability, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{M}_{k}|\\leq d^{-1/2+\\zeta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can now take the maximum over $k$ from 0 to $T d$ using a union bound; this does not affect the overwhelming probability statement. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proposition C.6 (Hessian martingale). Let $f$ and $S$ be defined as in Assumption 2 and (37). Then, for any $z\\in\\Omega,\\,\\zeta>0$ and $T>0$ , with overwhelming probability, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T\\wedge\\vartheta}\\left\\|\\mathcal{M}_{\\lfloor t d\\rfloor}^{\\mathrm{Hess}}(S(\\cdot,z))\\right\\|\\leq d^{-1/2+\\zeta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The proof here is basically identical to the previous one. Again, set $F_{k}=\\gamma(G_{k})^{2}I_{1}(k)/d$ and $F_{k}^{\\beta}=\\mathbf{Proj}_{\\beta}(F_{k})$ , with their associated martingales being $\\mathcal{M}_{k}=F_{k}-\\mathbb{E}[F_{k}\\,|\\,\\mathcal{F}_{k}]$ and $\\mathcal{M}_{k}^{\\beta}=$ $F_{k}^{\\beta}-\\mathbb{E}[F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]$ . As before, Azuma\u2019s inequality, with $\\beta=d^{-1+\\zeta/2}$ , gives us ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{M}_{k}^{\\beta}\\geq d^{-1/2+\\zeta})\\leq2\\exp\\left(-\\frac{d^{\\zeta}}{8T}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The error term is also quite similar: ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\mathcal{M}_{k}-\\mathcal{M}_{k}^{\\beta}|\\leq\\sum_{i=0}^{k}|F_{k}-F_{k}^{\\beta}|+|\\,\\mathbb{E}[F_{k}-F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(F_{k}-F_{k}^{\\beta}\\neq0)\\le\\mathbb{P}(\\gamma(G_{k})^{2}\\le d^{\\zeta/4})+\\mathbb{P}(|I_{2}(k)|\\le d^{\\zeta/4}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "both of which are superpolynomially small by (6) and Lemma C.5. For the expectation, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\mathbb{E}[F_{k}-F_{k}^{\\beta}\\,|\\,\\mathcal{F}_{k}]|\\le4d^{-1}\\,\\mathbb{E}[\\gamma(G_{k})^{8}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{4}}\\cdot\\mathbb{E}[I_{1}(k)^{4}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{4}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{k}|>\\beta\\}}\\,|\\,\\mathcal{F}_{k}]^{\\frac{1}{2}};\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "this product is superpolynomially small by (6), Lemma C.6, and Lemma C.5. Overall, we have, with overwhelming probability, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{M}_{k}|\\leq d^{-1/2+\\zeta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking the supremum, we obtain the desired result. ", "page_idx": 24}, {"type": "text", "text": "Proposition C.7 (Integral error term). Let $f$ and $S$ be defined as in Assumption 2 and (37). Then, for $z\\in\\Omega$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\xi_{t d}(S(\\cdot,z))|\\leq d^{-1/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have, as above, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\xi_{t d}|=\\bigg|\\int_{(\\lfloor t d\\rfloor-1)/d}^{t}d\\cdot\\Delta\\varphi(X_{s d})\\ \\mathrm{d}s\\bigg|}\\\\ &{\\quad\\quad\\leq\\underset{0\\leq j\\leq\\lceil t d\\rceil}{\\operatorname*{max}}\\{|\\Delta\\varphi(X_{j})|\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is bounded by $d^{-1/2}$ w.o.p. by the boundedness of $I_{1},I_{2},I_{3}$ , and $\\gamma(B_{k})$ . ", "page_idx": 24}, {"type": "text", "text": "C.2.2 General bounds ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we make use of the subgaussian norm $\\|\\cdot\\|_{\\psi_{2}}$ of a random variable (see [56] for details.) When it exists, this norm is defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|X\\|_{\\psi_{2}}\\asymp\\operatorname*{inf}\\left\\{V>0:\\forall t>0,\\ \\mathbb{P}(|X|>t)\\leq2e^{-t^{2}/V^{2}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, Gaussian random variables have a well-defined subgaussian norm. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.3 ([15], Lemma 5.3). There exist constants $c,C>0$ such that ", "page_idx": 25}, {"type": "text", "text": "$c\\|W\\|^{2}\\leq\\|S(W,z)\\|_{\\Omega}\\leq C\\|W\\|^{2},\\quad\\|\\nabla_{X}S(W,z)\\|_{\\Omega}\\leq C\\|W\\|,\\quad a n d\\quad\\|\\nabla_{X}^{2}S(W,z)\\|_{\\Omega}\\leq C.$ Lemma C.4 (Preliminary bounds). With $f$ and $\\Delta_{k}$ defined as above, for $\\varepsilon>0$ and $\\lambda\\geq0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f^{\\prime}(r_{k})\\leq d^{\\varepsilon}}&{w.o.p.\\ a n d\\quad\\mathbb{E}[|f^{\\prime}(r_{k})|^{\\lambda}\\,|\\,\\mathcal{F}_{k}]\\leq C(\\lambda),}\\\\ {\\frac{\\|\\Delta_{k}\\|^{2}}{d}\\leq d^{\\varepsilon}}&{w.o.p.\\ a n d\\quad\\mathbb{E}\\left[\\left(\\frac{\\|\\Delta_{k}\\|^{2}}{d}\\right)^{\\lambda}\\mid\\mathcal{F}_{k}\\right]\\leq C(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of (39) in Lemma C.4. By [15, Lemma 3.4], if function $f$ is $\\alpha$ -pseudo-Lipschitz with Lipschitz constant $L(f)$ (as in (2)) and the noise $\\epsilon$ is independent of $a$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f^{\\prime}(r)|\\leq C(\\alpha)(L(f))(1+|r|+|\\epsilon|)^{\\operatorname*{max}\\{1,\\alpha\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f^{\\prime}(r_{k})|\\leq C(\\alpha)(L(f))(1+|r_{k}|+|\\epsilon|)^{\\operatorname*{max}\\{1,\\alpha\\}}}\\\\ &{\\qquad\\qquad\\leq C(\\alpha)(L(f))(1+|X_{k}^{\\top}a_{k+1}|+|\\epsilon|)^{\\operatorname*{max}\\{1,\\alpha\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, since $a_{k+1}$ is Gaussian, we can write $a_{k+1}=\\sqrt{K}v_{k}$ , for a standard normal $v_{k}$ . Then we see that $X_{k}^{\\top}a_{k+1}=X_{k}^{\\top}\\sqrt{K}v_{k}$ is a single-variable Gaussian, with variance $|X_{k}^{\\top}K X_{k}|\\leq\\|X_{k}\\|^{2}\\!\\cdot\\!\\|K\\|_{\\mathrm{op}}$ (bounded independently of $d$ because of the stopping time on $X_{k}$ ). Similarly, $\\epsilon$ is Gaussian and independent of $a_{k+1}$ , so the expression (41) is bounded w.o.p. by $d^{\\varepsilon}$ , and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(C(\\alpha)(L(f))(1+|X_{k}^{\\top}a_{k+1}|+|\\epsilon|)^{\\operatorname*{max}\\{1,\\alpha\\}}\\right)^{\\lambda}\\mid\\mathcal{F}_{k}\\right]\\leq C(\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some constant $C(\\lambda)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of (40) in Lemma C.4. We can write $a_{k+1}=\\sqrt{K}v_{k}$ , where $v_{k}$ is a standard $d$ -dimensional normal vector. Then, by Hanson-Wright, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|\\left\\|a_{k+1}\\right\\|^{2}-\\mathbb{E}[\\left|\\left|a_{k+1}\\right|]^{2}\\,|\\,\\mathcal{F}_{k}]\\right|\\geq d\\right)=\\mathbb{P}\\left(\\left|v_{k}^{\\top}K v_{k}-\\mathbb{E}[v_{k}^{\\top}K v_{k}\\,|\\,\\mathcal{F}_{k}]\\right|\\geq d\\right)}\\\\ &{\\qquad\\qquad\\leq2\\exp\\left(-\\frac{c d^{2}}{\\|K\\|_{F}^{2}+\\|K\\|_{\\log}d}\\right)}\\\\ &{\\qquad\\qquad\\leq2\\exp\\left(-\\frac{c d^{2}}{d(\\|K\\|_{\\log}+\\|K\\|_{\\log}^{2})}\\right)}\\\\ &{\\qquad\\qquad\\leq2\\exp\\left(-C d\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, note that $\\mathbb{E}[v_{k}^{\\top}K v_{k}\\,|\\,\\mathcal{F}_{k}]=\\mathrm{Tr}(K)\\le d\\|K\\|_{\\mathrm{op}}$ . Together, we get that $\\|a_{k+1}\\|^{2}\\leq d^{1+\\epsilon}$ with overwhelming probability. Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{\\|\\Delta_{k}\\|^{2}}{d}}={\\frac{\\|f^{\\prime}(r_{k})a_{k+1}\\|^{2}}{d}}={\\frac{\\|a_{k+1}\\|^{2}f^{\\prime}(r_{k})^{2}}{d}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is bounded by $d^{2\\varepsilon}$ w.o.p. Now for the expectation: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\frac{\\|\\Delta_{k}\\|^{2}}{d}\\right)^{\\lambda}\\mid\\mathcal{F}_{k}\\right]\\leq\\mathbb{E}\\left[\\left(\\frac{\\|\\sqrt{K}v_{k}\\|^{2}}{d}\\right)^{2\\lambda}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\left[f^{\\prime}(r_{k})^{4\\lambda}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\left[\\left(\\frac{\\|K\\|_{\\infty}\\cdot\\|v_{k}\\|^{2}}{d}\\right)^{2\\lambda}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\left[f^{\\prime}(r_{k})^{4\\lambda}\\mid\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the first term, we have ", "page_idx": 26}, {"type": "text", "text": "(Jensen\u2019s inequality) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\frac{\\|K\\|_{\\infty}\\cdot\\|v_{k}\\|^{2}}{d}\\right)^{2\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]=\\|K\\|_{\\infty}^{2\\lambda}\\cdot\\mathbb{E}\\left[\\left(\\frac{\\|v_{k}\\|^{2}}{d}\\right)^{2\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|K\\|_{\\infty}^{2\\lambda}\\cdot\\frac{1}{d}\\displaystyle\\sum_{i=0}^{d-1}\\mathbb{E}\\left[\\left(\\|v_{k}^{i}\\|^{2}\\right)^{2\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|K\\|_{\\infty}^{2\\lambda}\\cdot\\mathbb{E}\\left[\\|v_{k}^{0}\\|^{4\\lambda}\\,|\\,\\mathcal{F}_{k}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we are using the notation $v_{k}^{i}$ to refer to the ith component of the vector $v_{k}$ . Now, since $v_{k}^{0}$ is just a standard Gaussian, all of its moments are bounded. The second term in (42) is bounded by a constant by (39), as desired. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma C.5 (Gradient and Hessian bounds). Setting ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}(k)\\overset{d e f}{=}\\Delta_{k}^{\\top}\\nabla^{2}\\varphi(X_{k})\\Delta_{k}/d,\\quad I_{2}(k)\\overset{d e f}{=}\\mathrm{Tr}(\\nabla^{2}\\varphi(X_{k})K)\\mathbb{E}\\left[f^{\\prime}(r_{k})^{2}\\mid\\mathcal{F}_{k}\\right]/d,}\\\\ &{\\qquad\\qquad\\qquad\\qquad I_{3}(k)\\overset{d e f}{=}\\nabla\\varphi(X_{k})^{\\top}\\Delta_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any $\\varepsilon>0$ and $\\lambda\\geq0$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|I_{1}(k)|\\leq d^{\\varepsilon}}&{w.o.p.\\ a n d\\quad\\mathbb{E}\\left[|I_{1}(k)|^{\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]\\leq C(\\lambda),}\\\\ {|I_{2}(k)|\\leq C,}\\\\ {|I_{3}(k)|\\leq d^{\\varepsilon}}&{w.o.p.\\ a n d\\quad\\mathbb{E}\\left[|I_{3}(k)|^{\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]\\leq C(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of (43) in Lemma C.5. Using the fact that $\\|\\nabla^{2}\\varphi(X_{k})\\|_{\\mathrm{op}}\\leq\\|S(W_{k},\\cdot)\\|_{\\Omega}.$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\vert\\Delta_{k}^{\\top}\\nabla^{2}\\varphi(X_{k})\\Delta_{k}\\vert}{d}\\leq\\frac{\\vert\\vert S(W_{k},\\cdot)\\vert\\vert_{\\Omega}\\vert\\vert\\Delta_{k}\\vert\\vert^{2}}{d}}&{}\\\\ {\\leq\\frac{C\\vert\\vert W_{k}\\vert\\vert^{2}\\vert\\vert\\Delta_{k}\\vert\\vert^{2}}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, $\\|W_{k}\\|$ is bounded by the stopping time. From Lemma C.4, $\\frac{\\|\\Delta_{k}\\|^{2}}{d}$ is bounded by $d^{\\varepsilon}$ w.o.p., and every moment of this expression is bounded independent of $d$ , as desired. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Proof of(44) in Lemma C.5. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\left|\\mathrm{Tr}\\left(\\nabla^{2}\\varphi(X_{k})K\\right)\\mathbb{E}\\left[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}\\right]\\right|}{d}\\leq\\frac{d\\|\\nabla^{2}\\varphi(X_{k})K\\|_{\\mathrm{op}}\\cdot\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]}{d}}&{}\\\\ {\\leq\\|\\nabla^{2}\\varphi(X_{k})\\|_{\\mathrm{op}}\\cdot\\|K\\|_{\\mathrm{op}}\\cdot\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]}&{}\\\\ {\\leq C M^{2}\\,\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}].}&{\\qquad\\qquad(\\mathrm{Le})^{2}\\,\\|\\mathcal{F}_{k}\\|_{\\mathrm{op}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Lemma C.4, $\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]$ is bounded by a constant independent of $d$ , as desired. ", "page_idx": 26}, {"type": "text", "text": "Proof of (45) in Lemma C.5. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\nabla\\varphi(X_{k})^{\\top}\\Delta_{k}|\\leq|\\nabla\\varphi(X_{k})^{\\top}a_{k+1}|\\cdot|f^{\\prime}(r_{k})|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Lemma C.3, $\\|\\nabla\\varphi(X_{k})\\|\\leq C\\|W_{k}\\|\\leq C M$ (since we are working under a stopping time), and so $\\nabla\\varphi(X_{k})^{\\top}a_{k+1}$ is subgaussian (and thus bounded by $d^{\\varepsilon}$ w.o.p.). By (39), $f^{\\prime}(r_{k})$ is bounded by $d^{\\varepsilon}$ w.o.p., and so their product is bounded by $d^{2\\varepsilon}$ w.o.p., as desired. Now for the expectation: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[|\\nabla\\varphi(X_{k})^{\\top}\\Delta_{k}|\\,|\\,\\mathcal{F}_{k}\\right]\\le\\mathbb{E}\\left[|\\nabla\\varphi(X_{k})^{\\top}a_{k+1}|\\cdot|f^{\\prime}(r_{k})|\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\mathbb{E}\\left[|\\nabla\\varphi(X_{k})^{\\top}a_{k+1}|^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}\\cdot\\mathbb{E}\\left[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}\\right]^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first term is bounded by a constant independent of $d$ , since subgaussian moments are bounded. The second term is bounded by Lemma C.4, completing the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma C.6 (Infinity norm bounds). For $G_{k}$ $\\mathrm{\\Delta}_{{\\sf k},\\mathrm{\\Delta}N_{k},\\mathrm{\\Delta}Q_{k}}$ as defined in 1.2, we have, for any $\\varepsilon,\\lambda>0$ , there exists $C>0$ such that, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|G_{k}\\|_{\\infty}\\leq d^{\\varepsilon}}&{w.o.p.\\ a n d\\quad\\mathbb{E}[\\|G_{k}\\|_{\\infty}^{\\lambda}\\,|\\,\\mathcal{F}_{k}]\\leq d^{\\varepsilon}\\quad w.o.p.,}\\\\ {\\|N_{k}\\|_{\\infty}\\leq C,\\quad\\|Q_{k}\\|_{\\infty}\\leq C,\\quad\\|\\mathcal{G}_{k}\\|_{\\infty}\\leq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The first line, (46),  follows from (40). For the first inequality, $\\begin{array}{r}{\\|G_{k}\\|_{\\infty}=\\operatorname*{max}_{0\\leq j\\leq k}\\frac{\\|\\Delta_{j}\\|^{2}}{d}}\\end{array}$ which are all bounded by with overwhelming probability. A union bound tells us that the maximum is also bounded by $d^{\\varepsilon}$ w.o.p.. For the second inequality, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|G_{k}\\right|_{\\infty}^{\\lambda}\\big|\\,\\mathcal{F}_{k}\\right]\\leq\\mathbb{E}\\left[\\left(\\frac{\\|\\Delta_{k}\\|^{2}}{d}\\right)^{\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]+\\mathbb{E}\\left[\\operatorname*{max}_{0\\leq j\\leq k-1}\\left(\\frac{\\|\\Delta_{j}\\|^{2}}{d}\\right)^{\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\left(\\frac{\\|\\Delta_{k}\\|^{2}}{d}\\right)^{\\lambda}\\,|\\,\\mathcal{F}_{k}\\right]+\\operatorname*{max}_{0\\leq j\\leq k-1}\\left(\\frac{\\|\\Delta_{j}\\|^{2}}{d}\\right)^{\\lambda}}\\\\ &{\\qquad\\qquad\\leq d^{\\varepsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "as desired. The second line is more straightforward: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|N_{k}\\|_{\\infty}=\\operatorname*{max}_{0\\leq j\\leq k}\\|(W_{j}^{+})^{\\top}W_{j}^{+}\\|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, $\\|X^{\\star}\\|$ and $\\|X_{0}\\|$ are bounded independent of $d$ , and $\\|X_{j}\\|$ is bounded by $c M$ (because of the stopping time we are using.) Thus the maximum over $j$ of their inner products are bounded by a constant. The same thing holds for $\\|Q_{k}\\|_{\\infty}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q_{k}\\|_{\\infty}=\\underset{0\\leq j\\leq k}{\\operatorname*{max}}\\,\\mathcal{R}(X_{j})}\\\\ &{\\qquad\\qquad=\\underset{0\\leq j\\leq k}{\\operatorname*{max}}\\,h({W_{j}^{\\top}}K W_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since the derivative of $h$ is pseudo-Lipschitz, $h$ is continuous, and thus bounded for bounded arguments. And indeed, the argument to $h$ is bounded: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|W_{j}^{\\top}K W_{j}\\|\\leq\\|W_{j}\\|^{2}\\|K\\|_{\\mathrm{op}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "both of which are bounded independent of $d$ . Finally, a similar argument applies to $\\mathcal{G}_{k}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathcal{G}_{k}\\|_{\\infty}=\\operatorname*{max}_{0\\leq j\\leq k}\\mathbb{E}\\left[\\frac{\\|\\Delta_{j}\\|^{2}}{d}\\,|\\,\\mathcal{F}_{j}\\right]\\leq\\operatorname*{max}_{0\\leq j\\leq k}C=C\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by Lemma C.4. ", "page_idx": 27}, {"type": "text", "text": "We now prove a concentration result that closely follows [15, Proposition 5.6]. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.7 ([15], Lemma 5.2). Suppose $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ is distributed $\\mathcal{N}(0,I_{d})$ and $U\\ \\in\\ R^{d\\times2}$ has orthonormal columns. Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\boldsymbol{v}\\,|\\,\\boldsymbol{U}^{\\top}\\boldsymbol{v}\\sim\\boldsymbol{v}-\\boldsymbol{U}(\\boldsymbol{U}^{\\top}\\boldsymbol{v})+\\boldsymbol{U}\\boldsymbol{U}^{\\top}\\boldsymbol{v},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $v-U\\left(U^{T}v\\right)\\sim N\\left(0,I_{d}-U U^{T}\\right)a n d U U^{T}v\\sim N\\left(0,U U^{T}\\right)w i t h\\,v-U\\left(U^{T}v\\right)$ independent of $U U^{T}v$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma C.8. For a matrix $H=H_{k}$ with bounded operator norm, or $\\|H\\|_{o p}<C$ and $\\mathbb{E}[H_{k}\\,|\\,\\mathcal{F}_{k}]=$ $H_{k}$ , set $q(a)=a^{\\top}H a$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}[q(a_{k+1})f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]-\\mathrm{Tr}(K H)\\,\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]\\right|\\leq C(H).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that the $H$ used here is not the same as the matrix used in the integro-differential equation. ", "page_idx": 27}, {"type": "text", "text": "Proof. Many of the computations in this proof are taken directly from [15], but we repeat them here for completeness. We have $\\mathscr{F}_{k}\\,=\\,\\sigma(\\{W_{i}\\}_{i=0}^{k})$ ; set $\\hat{\\mathcal{F}}_{k}\\,=\\,\\sigma(\\{W_{i}\\}_{i=0}^{k},\\{r_{i}\\}_{i=0}^{k})$ . A simple calculation shows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[q(a_{k+1})f^{\\prime}(r_{k})^{2}\\,|\\,\\hat{\\mathcal{F}}_{k}]=\\mathbb{E}[q(a_{k+1}-\\mathbb{E}[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}])\\,|\\,\\hat{\\mathcal{F}}_{k}]\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,q(\\mathbb{E}[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}])\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To compute the conditional mean $\\mathbb{E}[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}]$ and covariance $(a_{k+1}\\,-\\,\\mathbb{E}[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}])(a_{k+1}\\,-\\,$ $\\mathbb{E}[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}])^{\\top}$ , we use Lemma C.7. By Assumption 1, we can write $a_{k+1}~=~\\sqrt{K}v_{k}$ , for $v_{k}\\sim\\mathcal{N}(0,I_{d})$ . ", "page_idx": 28}, {"type": "text", "text": "Now we perform a QR-decomposition on ${\\sqrt{K}}W_{k}\\ \\ {\\stackrel{\\mathrm{def}}{=}}\\ \\ Q_{k}R_{k}$ where $Q_{k}\\ \\in\\ \\mathbb{R}^{d\\times2}$ with orthonormal columns and $R_{k}\\,\\in\\,\\mathbb{R}^{2\\times2}$ is upper triangular (and invertible). Set $\\Pi_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ Q_{k}Q_{k}^{T}$ . In distribution, ", "page_idx": 28}, {"type": "equation", "text": "$$\na_{k+1}\\,|\\,a_{k+1}^{\\top}W_{k}\\overset{\\mathrm{d}}{=}\\sqrt{K}v_{k}\\,|\\,R_{k}^{T}Q_{k}^{T}v_{k}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As $R_{k}$ is invertible, by Lemma C.7, ", "page_idx": 28}, {"type": "equation", "text": "$$\na_{k+1}\\,|\\,a_{k+1}^{\\top}W_{k}\\overset{\\mathrm{d}}{=}\\sqrt{K}v_{k}\\,|\\,Q_{k}^{T}v_{k}\\overset{\\mathrm{d}}{=}\\sqrt{K}\\big(v_{k}-\\Pi_{k}v_{k}\\big)+\\sqrt{K}\\Pi_{k}v_{k}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We note that $(I_{d}-\\Pi_{k})v_{k}\\sim N(0,I_{d}-\\Pi_{k})$ and $\\Pi_{k}v_{k}\\sim N(0,\\Pi_{k})$ with $(I_{d}-\\Pi_{k})v_{k}$ independent of $\\Pi_{k}v_{k}$ . From this, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[a_{k+1}\\,|\\,\\hat{\\mathcal{F}}_{k}\\right]=\\sqrt{K}\\Pi_{k}v_{k},\\quad\\mathrm{where}\\ v_{k}\\sim N(0,I_{d}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover the conditional covariance of $a_{k+1}$ is precisely ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\big(\\mathbb{E}\\left[\\left(a_{k+1}-\\mathbb{E}\\left[a_{k+1}\\,\\big|\\,\\hat{\\mathcal{F}}_{k}\\right]\\right)\\!\\right)\\!(a_{k+1}-\\mathbb{E}\\left[a_{k+1}\\,\\big|\\,\\hat{\\mathcal{F}}_{k}\\right])^{\\top}\\,\\big|\\hat{\\mathcal{F}}_{k}\\big]\\big)}\\\\ &{}&{=\\sqrt{K}(I_{d}-\\Pi_{k})\\sqrt{K},\\quad\\mathrm{where~}\\Pi_{k}=Q_{k}Q_{k}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next, using that $\\mathbb{E}[H_{k}\\,|\\,\\mathcal{F}_{k}]=H_{k}$ , we expand (49) to get the leading order behavior ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[q(a_{k+1})f^{\\prime}(r_{k})^{2}\\,|\\,\\hat{\\mathcal{F}}_{k}\\right]=\\mathrm{Tr}(H K)\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}]}\\\\ &{\\phantom{=}-\\mathrm{Tr}(H\\sqrt{K}\\Pi_{k}\\sqrt{K})\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}]}\\\\ &{\\phantom{=}+q(\\sqrt{K}\\Pi_{k}v_{k})\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}]\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the expectation with respect to $\\mathcal{F}_{k}$ , we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[q(a_{k+1})f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}\\right]-\\operatorname{Tr}(H K)\\,\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]=\\mathbb{E}[\\mathcal{E}_{k}\\,|\\,\\mathcal{F}_{k}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the error $\\mathcal{E}_{k}$ is defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k}=-\\;\\mathrm{Tr}(H\\sqrt{K}\\Pi_{k}\\sqrt{K})\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}]}\\\\ {+\\;q(\\sqrt{K}\\Pi_{k}v_{k})\\,\\mathbb{E}_{\\epsilon}[f^{\\prime}(r_{k})^{2}].\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The proof now turns to bounding the expectation of this error quantity. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lvert\\operatorname{Tr}(H\\sqrt{K}\\Pi_{k}\\sqrt{K})\\operatorname{\\mathbb{E}}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]\\rvert=\\lvert\\operatorname{Tr}(H\\sqrt{K}\\Pi_{k}\\sqrt{K})\\rvert\\cdot\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\lVert H\\rVert_{\\infty}\\lVert K\\rVert_{\\infty}\\lvert\\operatorname{Tr}(\\Pi_{k})\\rvert\\cdot\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\lVert H\\rVert_{\\infty}\\lVert K\\rVert_{\\infty}\\cdot\\operatorname{rank}(Q_{k})\\,\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\lVert H\\rVert_{\\infty}\\lVert K\\rVert_{\\infty}\\,\\mathbb{E}[f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal{F}_{k}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By (39), the expectation is bounded by a constant, so this term is overall bounded by a constant. We move on to the next term in the error: ", "page_idx": 28}, {"type": "equation", "text": "$$\nq(\\sqrt{K}\\Pi_{k}\\boldsymbol{v}_{k})f^{\\prime}(\\boldsymbol{r}_{k})^{2}\\leq\\|H\\|_{\\mathrm{op}}\\|K\\|_{\\mathrm{op}}\\|\\Pi_{k}\\boldsymbol{v}_{k}\\|^{2}f^{\\prime}(\\boldsymbol{r}_{k})^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking expectations and using Cauchy Schwarz, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb E[q(\\sqrt{K}\\Pi_{k}v_{k})f^{\\prime}(r_{k})^{2}\\,|\\,\\mathcal F_{k}]\\le\\|H\\|_{\\mathrm{op}}\\|K\\|_{\\mathrm{op}}\\cdot\\sqrt{\\mathbb E}[\\|\\Pi_{k}v_{k}\\|^{4}\\,|\\,\\mathcal F_{k}]\\cdot\\sqrt{\\mathbb E[f^{\\prime}(r_{k})^{4}\\,|\\,\\mathcal F_{k}]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The first expectation is $\\mathbb{E}[\\|\\Pi_{k}v_{k}\\|^{2}\\,|\\,\\mathcal{F}_{k}]=\\|\\Pi_{k}\\|_{\\mathrm{F}}^{4}=8$ , and the second is bounded by (39) as before. We thus conclude that $\\mathbb{E}[\\mathcal{E}_{k}\\,|\\,\\mathcal{F}_{k}]$ is bounded by a constant depending on $\\|H\\|_{\\mathrm{op}}$ , completing the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma C.9. There is a constant $C$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\gamma(\\mathcal{G}_{k})-\\gamma(B_{k})|\\leq C d^{-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Using the Lipschitz condition on the stepsize, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma(\\mathcal{G}_{k})-\\gamma(B_{k})|}}\\\\ &{\\leq\\|\\mathcal{G}_{k}-\\mathrm{T}(K)I(B_{k})/d\\|_{\\infty}\\times(1+2\\|N_{k}\\|_{\\infty}^{\\alpha}+\\|\\mathcal{G}_{k}\\|_{\\infty}^{\\alpha}+\\|\\,\\mathrm{Tr}(K)I(B_{k})/d\\|_{\\infty}^{\\alpha}+2\\|Q_{k}\\|_{\\infty}^{\\alpha})}}\\\\ &{\\leq C\\|\\mathcal{G}_{k}-\\mathrm{Tr}(K)I(B_{k})/d\\|_{\\infty}}\\\\ &{\\leq C d^{-1}\\operatorname*{max}_{0\\leq j\\leq k}\\left\\|\\mathbb{E}[a_{j+1}^{\\top}a_{j+1}f^{\\prime}(r_{j})^{2}\\,|\\,\\mathcal{F}_{j}]-\\mathrm{Tr}(K)\\,\\mathbb{E}[f^{\\prime}(r_{j})^{2}\\,|\\,\\mathcal{F}_{j}]\\right\\|}\\\\ &{\\leq C d^{-1},}&{\\mathrm{(Lemma~C.8)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "as desired. ", "page_idx": 29}, {"type": "text", "text": "C.3 Specific learning rates ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we confirm that AdaGrad-Norm satisfies Assumption 6. In the notation of Assumption 6, we have, for AdaGrad-Norm, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma(t d,f,g,q)=\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}g(s)~\\mathrm{d}s}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that this reduces to the discrete stepsize if we plug in $g=G_{k}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(t d,f,G_{k}(d\\times\\cdot),q)=\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}G_{k}(d s)\\;\\mathrm{d}s}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}\\Big(1_{\\{d s\\leq k\\}}\\frac{1}{d}\\sum_{i=0}^{k}\\|\\nabla_{X}\\Psi(X_{i};a_{i+1},\\epsilon_{i+1})\\|^{2}\\mathbf{1}_{\\{i,i+1\\}}(d s)\\Big)\\;\\mathrm{d}s}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}\\Big(1_{\\{u\\leq k\\}}\\frac{1}{d^{2}}\\sum_{i=0}^{k}\\|\\nabla_{X}\\Psi(X_{i};a_{i+1},\\epsilon_{i+1})\\|^{2}\\mathbf{1}_{\\{i,i+1\\}}(u)\\Big)\\;\\mathrm{d}u}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\eta}{\\sqrt{b^{2}+\\frac{1}{d^{2}}\\sum_{i=0}^{k}\\|\\nabla_{X}\\Psi(X_{i};a_{i+1},\\epsilon_{i+1})\\|^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is exactly the discrete version of the AdaGrad-Norm stepsize. ", "page_idx": 29}, {"type": "text", "text": "Proposition C.8 (Lipschitz). For functions $f,g,q$ such that $f(d s)=g(d s)=q(d s)=0$ for $s>t$ , the AdaGrad stepsize $\\gamma$ is Lipschitz. That is, ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\gamma(t d,f(d\\times\\cdot),g(d\\times\\cdot),q(d\\times\\cdot))-\\gamma(t d,\\hat{f}(d\\times\\cdot),\\hat{g}(d\\times\\cdot),\\hat{q}(d\\times\\cdot))|\\leq C(t,\\gamma)(\\|g-\\hat{g}\\|_{\\infty}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Remark C.2. This is a stronger condition than the $\\alpha$ -pseudo Lipschitz one in Assumption $^{6}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. To show this, we look at the derivative of the AdaGrad stepsize function. Setting $F(x)=$ \u221ab2+x, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n|F^{\\prime}(x)|=\\frac{\\eta}{2(b^{2}+x)^{3/2}}\\leq\\frac{\\eta}{2b^{3}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $x\\in[0,\\infty)$ . We thus have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\gamma(H,f(d\\times\\cdot),g(d\\times\\cdot),q(d\\times\\cdot))-\\gamma(H,f(d\\times\\cdot),\\hat{g}(d\\times\\cdot),\\hat{q}(d\\times\\cdot))|}\\\\ &{=\\left|\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}g(d s)\\;\\mathrm{d}s}}-\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{\\infty}\\hat{g}(d s)\\;\\mathrm{d}s}}\\right|}\\\\ &{=\\left|F\\left(\\int_{0}^{\\infty}g(d s)\\;\\mathrm{d}s\\right)-F\\left(\\int_{0}^{\\infty}\\hat{g}(d s)\\;\\mathrm{d}s\\right)\\right|}\\\\ &{\\leq\\frac{\\eta}{2b^{3}}\\left|\\int_{0}^{\\infty}g(d s)\\;\\mathrm{d}s-\\int_{0}^{\\infty}\\hat{g}(d s)\\;\\mathrm{d}s\\right|}\\\\ &{\\leq\\frac{\\eta}{2b^{3}}\\left|\\int_{0}^{t}g(d s)\\;\\mathrm{d}s-\\int_{0}^{t}\\hat{g}(d s)\\;\\mathrm{d}s\\right|}\\\\ &{\\leq\\frac{\\eta}{2b^{3}}\\left(t-\\Vert g-\\hat{g}\\Vert_{\\infty}\\right)}\\\\ &{\\leq\\frac{\\eta}{2b^{3}}\\cdot\\Vert g-\\hat{g}\\Vert_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we were able to replace the $\\infty$ with a $t$ because $g(d s)=0$ for $s>t$ . We have thus obtained a Lipschitz constant 2\u03b7bt3 depending only on t. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Next we show that the AdaGrad-Norm is bounded. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.9 (Boundedness). Suppose $\\gamma$ is AdaGrad-Norm. Then (6), as part of Assumption $^{\\sc6}$ holds. ", "page_idx": 30}, {"type": "text", "text": "Proof. This is immediate: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma(t d,f,g,q)=\\frac{\\eta}{\\sqrt{b^{2}+\\int_{0}^{t}g(s)\\,d s}}\\leq\\frac{\\eta}{b}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It remains to show that AdaGrad-Norm satisfies (5) in Assumption 6. ", "page_idx": 30}, {"type": "text", "text": "Proposition C.10 (Concentration). Suppose $\\gamma$ is AdaGrad-Norm, with $G_{k}$ and $\\mathcal{G}_{k}$ being defined as before. Then Equation (5), as part of Assumption $^{6}$ , holds: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|\\,|\\,\\mathcal{F}_{k}]\\leq C d^{-\\delta}(1+\\|f\\|_{\\infty}^{\\alpha}+\\|q\\|_{\\infty}^{\\alpha}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Looking to remove the square roots, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|\\leq|\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2}|^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For AdaGrad-Norm, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2}\\right|=\\eta^{2}\\left|\\frac{1}{b^{2}+\\frac{1}{d^{2}}\\sum_{j=0}^{k}\\|\\Delta_{j}\\|^{2}}-\\frac{1}{b^{2}+\\frac{1}{d^{2}}\\sum_{j=0}^{k}\\mathbb{E}\\left[\\|\\Delta_{j}\\|^{2}\\mid\\mathcal{F}_{j}\\right]}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\eta^{2}}{d^{2}b^{4}}\\cdot\\left|\\displaystyle\\sum_{j=0}^{k}(\\mathbb{E}\\left[\\|\\Delta_{j}\\|^{2}\\mid\\mathcal{F}_{j}\\right]-\\|\\Delta_{j}\\|^{2})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now bound the sum above. Set $F_{i}=\\|\\boldsymbol{\\Delta}_{i}\\|^{2}/d,\\,F_{i}^{\\beta}=\\mathrm{Proj}_{\\beta}(F_{i}),\\boldsymbol{\\Delta}\\boldsymbol{\\mathcal{M}}_{i}=F_{i}-\\mathbb{E}[F_{i}\\,|\\,\\mathcal{F}_{i}]$ , and $\\Delta\\mathcal{M}_{i}^{\\beta}=F_{i}^{\\beta}-\\mathbb{E}[F_{i}^{\\beta}\\,|\\,\\mathcal{F}_{i}]$ . Then $|\\Delta\\mathcal{M}_{i}^{\\beta}|\\in[-2\\beta,2\\beta]$ , so Azuma\u2019s inequality gives us ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\left(|\\mathcal{M}_{k}^{\\beta}|\\geq t\\right)\\leq2\\exp\\left(-\\frac{-t^{2}}{2\\sum_{i=0}^{k}(2\\beta)^{2}}\\right),}}\\\\ &{}&{\\mathbb{P}\\left(|\\mathcal{M}_{k}^{\\beta}|\\geq d^{1/2+\\varepsilon}\\right)\\leq2\\exp\\left(-\\frac{-d^{1+2\\varepsilon}}{2T d(2d^{\\varepsilon/2})^{2}}\\right)=\\exp\\left(-\\frac{d^{\\varepsilon}}{8T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we set $\\beta=d^{\\varepsilon/2}$ . This is close to the bound we want: the error is ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\mathcal{M}_{k}-\\mathcal{M}_{k}^{\\beta}|\\leq\\sum_{i=0}^{k}|F_{i}-F_{i}^{\\beta}|+|\\,\\mathbb{E}[F_{i}-F_{i}^{\\beta}\\,|\\,\\mathcal{F}_{i}]|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(F_{i}-F_{i}^{\\beta}\\neq0)=\\mathbb{P}(|F_{i}|>\\beta)=\\mathbb{P}\\left(\\frac{\\|\\Delta_{i}\\|^{2}}{d}>d^{\\varepsilon/2}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which superpolynomially small by (40). The expectation is similar: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbb{E}[F_{i}-F_{i}^{\\beta}\\,|\\,\\mathcal{F}_{i}]|=|\\,\\mathbb{E}[(F_{i}-F_{i}^{\\beta})\\mathbf{1}_{\\{|F_{i}|>\\beta\\}}\\,|\\,\\mathcal{F}_{i}]|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}[|F_{i}-F_{i}^{\\beta}|^{2}\\,|\\,\\mathcal{F}_{i}]^{\\frac{1}{2}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{i}|>\\beta\\}}\\,|\\,\\mathcal{F}_{i}]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq4\\,\\mathbb{E}[|F_{i}|^{2}\\,|\\,\\mathcal{F}_{i}]^{\\frac{1}{2}}\\cdot\\mathbb{E}[\\mathbf{1}_{\\{|F_{i}|>\\beta\\}}\\,|\\,\\mathcal{F}_{i}]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The first expectation is bounded by a constant independent of $d$ by (40), and the second expectation is superpolynomially small by the same argument as above. We then have ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\mathcal{M}_{k}-\\mathcal{M}_{k}^{\\beta}|\\leq d^{1/2+\\varepsilon}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with overwhelming probability (note that this would be true for any power of $d$ , by the definition of superpolynomially small.) We thus conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\mathcal{M}_{k}|\\le d^{1/2+\\varepsilon}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with overwhelming probability. Multiplying by $d$ , we find that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=0}^{k}(\\mathbb{E}\\left[\\|\\Delta_{j}\\|^{2}\\,|\\,\\mathcal{F}_{j}\\right]-\\|\\Delta_{j}\\|^{2})\\right|\\leq d^{3/2+\\varepsilon}\\quad\\mathrm{w.o.p.}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging this back into (57), we find that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\big\\vert\\gamma(G_{k})^{2}-\\gamma(\\mathcal{G}_{k})^{2}\\big\\vert\\le\\frac{\\eta^{2}}{d^{2}b^{4}}d^{3/2+\\varepsilon}}}\\\\ &{}&{\\le C d^{-1/2+\\varepsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with overwhelming probability, and so, taking the square root, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})\\right|\\leq C d^{-1/4+\\varepsilon/2}\\quad\\mathrm{w.o.p,}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is less than $d^{-1/4+\\varepsilon}$ as $d$ grows (we replaced the constant with an extra factor of $d^{\\varepsilon/2}$ .) Controlling the expectation via the boundedness of $\\gamma$ , we find that with $\\delta=1/8$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[|\\gamma(G_{k})-\\gamma(\\mathcal{G}_{k})|\\,|\\,\\mathcal{F}_{k}]\\leq d^{-\\delta}\\quad\\mathrm{w.o.p.},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as desired. ", "page_idx": 31}, {"type": "text", "text": "D Proofs for AdaGrad-Norm analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section we provide proofs of the propositions related to AdaGrad-Norm in the least squares setting as well as the more general strongly convex setting. Statements of the propositions for least squares examples are found in Section 4. ", "page_idx": 31}, {"type": "text", "text": "D.1 Strongly convex setting ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In order to derive the limiting learning rate in this case, we need the following assumption and some standard definitions of strong convexity. ", "page_idx": 31}, {"type": "text", "text": "Assumption 7 (Risk and loss minimizer). Suppose that ", "page_idx": 31}, {"type": "equation", "text": "$$\nX^{\\star}\\in a r g\\,m i n_{X}\\bigl\\{\\mathcal{R}(X)=\\mathbb{E}_{\\mathit{a},\\epsilon}\\bigl[f\\bigl(\\langle X,a\\rangle,\\langle X^{\\star},a\\rangle\\bigr),\\epsilon\\bigr]\\bigr\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "exists and has norm bounded independent of $d$ . Then one has, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\langle X^{\\star},a\\rangle\\in a r g\\,m i n_{x}\\{f(x,\\langle X^{\\star},a\\rangle,\\epsilon)\\},\\ \\ \\ \\ \\ \\,f o r\\,a l m o s t\\,s u r e l y\\ a\\sim{\\mathcal N}(0,K)\\ a n d\\ \\ \\ ,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "While at first, this assumption seems quite strong, in fact, in a typical student-teacher setup when label noise is 0 (i.e., $\\epsilon=0$ ), where the targets have the same model as the outputs, the assumption is satisfied. Our goal here is not to be exhaustive, but simply to illustrate that our framework admits a nontrivial and useful analysis and which gives nontrivial conclusions for the optimization theory of these problems. ", "page_idx": 32}, {"type": "text", "text": "Definition D.1 ( $\\hat{L}$ -smoothness of outer function $f$ ). A function $f\\,:\\,\\mathbb{R}^{3}\\,\\rightarrow\\,\\mathbb{R}$ that is $C^{1}$ -smooth (in the first variable) is called $\\hat{L}(f)$ -smooth if the following quadratic upper bound holds for any $x,\\hat{x},y,z\\in\\mathbb{R}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\hat{x},y,z)\\leq f(x,y,z)+\\langle f^{\\prime}(x,y,z),\\hat{x}-x\\rangle+\\frac{\\hat{L}(f)}{2}|\\hat{x}-x|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that if $\\textstyle f^{\\prime}={\\frac{\\partial}{\\partial x}}f(x,y,z)$ is $\\hat{L}(f)$ -Lipschitz, i.e., $|f^{\\prime}(x,y,z)-f^{\\prime}(\\hat{x},y,z)|\\leq\\hat{L}(f)|x-\\hat{x}|$ , then the inequality (58) holds with constant $\\hat{L}$ . Suppose $x^{\\star}\\in\\arg\\operatorname*{min}_{x}\\{f(x,y,z)\\}$ exists. An immediate consequence of (58) is that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2\\hat{L}(f)}|f^{\\prime}(x,y,z)|^{2}\\leq f(x,y,z)-f(x^{\\star},y,z)\\leq\\frac{\\hat{L}(f)}{2}|x-x^{\\star}|^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Definition D.2 (Restricted Secant Inequality). $A$ function $f\\,:\\,\\mathbb{R}^{3}\\,\\rightarrow\\,\\mathbb{R}$ that is $C^{1}$ -smooth (in the first variable) satisfies the $(\\mu,\\theta)$ \u2013restricted secant inequality $(R S I)$ if, for any $x~\\in~\\mathbb{R}$ and $x^{\\star}\\in a r g\\,m i n_{x}\\{f(x)\\}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle x-x^{\\star},f^{\\prime}(x)\\rangle\\geq\\binom{\\mu|x-x^{\\star}|^{2}}{0,}\\begin{array}{l l}{i f\\operatorname*{max}\\{|x^{\\star}|^{2},|x-x^{\\star}|^{2}\\}\\leq\\theta,}\\\\ {o t h e r w i s e.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If $f$ satisfies the above for $\\theta=\\infty$ , then we say $f$ satisfies the $\\mu$ \u2013RSI. ", "page_idx": 32}, {"type": "text", "text": "Proposition D.1. Let the outer function $f\\,:\\,\\mathbb{R}^{3}\\,\\rightarrow\\,\\mathbb{R}$ be a $\\hat{L}(f)$ -smooth function satisfying the RSI condition with $\\hat{\\mu}(f)$ with respect to $x\\in\\mathbb R$ . Suppose $X^{\\star}\\in a r g m i n_{X}\\{\\mathcal{R}(X)\\}$ exists bounded, independent of d and Assumption 7 holds and that \u03b30 = \u03b7b =(L\u02c6(f)2)2\u00b5\u02c6 (1f)Tr(K)\u03b6, for some $\\zeta\\in(0,1)$ , and that $\\begin{array}{r}{\\int_{0}^{\\infty}\\mathcal{R}(s)\\gamma_{s}\\,\\mathrm{d}s<\\infty}\\end{array}$ with $\\gamma_{s}$ as in Table 2 (AdaGrad-Norm, general formula), then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\gamma_{\\infty}\\geq\\frac{\\gamma_{0}\\eta^{2}}{1+\\frac{\\zeta}{1-\\zeta}\\mathcal{D}^{2}(0)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Given the Eq. (87) for the distance to optimality, with $(\\boldsymbol{x},\\boldsymbol{x}^{\\star})\\sim\\mathcal{N}(\\boldsymbol{0},\\mathcal{B})$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}^{2}(t)=-2\\gamma_{t}\\mathbb{E}_{a,\\epsilon}[\\langle x-x^{\\star},f^{\\prime}(x,x^{\\star})\\rangle]+\\frac{\\gamma_{t}^{2}}{d}\\mathrm{Tr}(K)\\mathbb{E}_{\\,a,\\epsilon}[(f^{\\prime}(x,x^{\\star}))^{2}]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the RSI (with constant $\\hat{\\mu}(f))$ condition on $f$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a,\\epsilon}\\big[\\langle x-x^{\\star},f^{\\prime}(x,x^{\\star})\\rangle\\big]\\geq\\hat{\\mu}(f)\\mathbb{E}_{a,\\epsilon}[(x-x^{\\star})^{2}]=2\\hat{\\mu}(f)\\mathcal{R}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $x=\\langle X,a\\rangle$ and $x^{\\star}=\\langle X^{\\star},a\\rangle$ and we note that $x$ has $t$ -dependence due to the $t$ -dependence in $\\mathcal{B}$ . By $\\hat{L}(f)$ -smoothness, ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\frac{1}{2\\hat{L}(f)}}(f^{\\prime}(x))^{2}\\leq{\\frac{\\hat{L}(f)}{2}}(x-x^{\\star})^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2(\\hat{L}(f))^{2}}\\mathbb{E}_{a,\\epsilon}\\big[(f^{\\prime}(x,x^{\\star})^{2}\\big]\\leq\\frac{1}{2}\\mathbb{E}_{a,\\epsilon}\\big[(x-x^{\\star})^{2}\\big]=\\mathcal{R}(t).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus by (60) and (61), we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}^{2}(t)\\leq-\\gamma_{t}\\left(4\\hat{\\mu}(f)-2(\\hat{L}(f))^{2}\\frac{1}{d}\\operatorname{Tr}(K)\\gamma_{t}\\right)\\mathcal{R}(t)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Which then yield: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{D}^{2}(t)\\leq\\mathcal{D}^{2}(0)-2\\left(2\\hat{\\mu}(f)-(\\hat{L}(f))^{2}\\frac{1}{d}\\,\\mathrm{Tr}(K)\\gamma_{0}\\right)\\int_{0}^{t}\\mathcal{R}(s)\\gamma_{s}\\,\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Changing variables $\\begin{array}{r}{u\\ =\\ \\Gamma(t)\\ =\\ \\int_{0}^{t}\\gamma_{s}\\,\\mathrm{d}s}\\end{array}$ , we have that $\\begin{array}{r l r}{\\int_{0}^{\\infty}\\mathcal{R}(t)\\gamma_{t}\\,\\mathrm{d}t}&{{}\\!=}&{\\int_{0}^{\\infty}r(u)\\,\\mathrm{d}u\\ =}\\end{array}$ $\\left\\Vert\\boldsymbol{r}\\right\\Vert_{1}$ . Rearranging the term in the above equation and taking $t~\\rightarrow~\\infty$ . We obtain: $\\lVert r\\rVert_{1}\\,\\leq$ (2\u00b5\u02c6(f)\u2212(L\u02c6(f))2 d1 Tr(K)\u03b30), given that $\\frac{2\\hat{\\mu}(f)}{(\\hat{L}(f))^{2}\\frac{1}{d}\\,\\mathrm{Tr}(K)}~~>~~\\gamma_{0}$ . Using Lemma D.1, with $i(v)\\;=\\;$ $I(\\mathcal{B}(\\Gamma^{-1}(v)))=\\mathbb{E}_{\\mathit{a,\\epsilon}}\\big[(f^{\\prime}(x,x^{\\star})^{2}\\big]$ instead of the risk ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\infty}=\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{2d}\\mathrm{Tr}(K)\\int_{0}^{\\infty}i(v)\\,\\mathrm{d}v}\\ge\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{d}\\,\\mathrm{Tr}(K)(\\hat{L}(f))^{2}\\int_{0}^{\\infty}r(v)\\,\\mathrm{d}v}}\\\\ &{\\quad\\ge\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{d}\\,\\mathrm{Tr}(K)\\frac{(\\hat{L}(f))^{2}{\\mathcal Q}^{2}(0)}{\\left(2\\hat{\\mu}(f)-(\\hat{L}(f))^{2}\\frac{1}{d}\\,\\mathrm{Tr}(K)\\gamma_{0}\\right)}}=\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{2\\hat{\\mu}(f)(1-\\zeta)}{\\mathcal Q}^{2}(0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first inequality is by Eq. 61, and the last transition is by taking the initial learning rate to be (L\u02c6(f)2)\u00b5\u02c62 (1f)Tr(K)\u03b6, for \u03b6 \u2208(0, 1). \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Lemma D.1. Given $\\gamma_{t}$ as in Table 2 (AdaGrad-Norm), defining $g(u)=\\gamma(\\Gamma^{-1}(u))$ , with $\\boldsymbol{\\Gamma}(t)=$ $\\begin{array}{r}{\\int_{0}^{t}\\gamma_{s}\\,\\mathrm{d}s.}\\end{array}$ , then \u03b7b + 21d Tr(K\u03b7) 0u i(v) dv with i(v) = I(B(\u0393\u22121(v))). ", "page_idx": 33}, {"type": "text", "text": "Proof. Taking the square of both sides of the $\\gamma_{t}$ equation in Table 2 (AdaGrad-Norm), changing variables to $u=\\Gamma(t)$ and rearranging the terms: ", "page_idx": 33}, {"type": "equation", "text": "$$\nb^{2}+\\frac{\\mathrm{Tr}(K)}{d}\\int_{0}^{u}\\frac{i(v)}{g(v)}~\\mathrm{d}v=\\frac{\\eta^{2}}{g(u)^{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "such that $i(v)=I({\\mathcal{B}}(\\Gamma^{-1}(v)))$ . Taking derivative with respect to $u$ , rearranging terms and integrating leads to the desired result. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "D.2 Least squares setting ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To study the effect of the structured covariance matrix and cases in which the problem is not strongly convex, we will focus on the linear least square problem. In this setting, the continuum limit of the risk for the AdaGrad-Norm algorithm has the form of a convolutional integral Volterra equation, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)=F(\\Gamma(t))+\\int_{0}^{t}\\gamma_{s}^{2}K(\\Gamma(t)-\\Gamma(s))\\mathcal{R}(s)\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{\\Gamma(t):=\\int_{0}^{t}\\gamma_{s}\\,\\mathrm{d}s}\\end{array}$ with, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F(x)\\stackrel{\\mathrm{def}}{=}\\displaystyle\\frac{1}{2d}\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{D}_{i}^{2}(0)e^{-2\\lambda_{i}x},}}\\\\ {{\\phantom{\\sum}_{\\displaystyle{F}(x)\\stackrel{\\mathrm{def}}{=}\\frac{1}{d}\\sum_{i=1}^{d}\\lambda_{i}^{2}e^{-2\\lambda_{i}x}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the following we consider three cases, a strongly convex risk in which the spectrum of the eigenvalues is bounded from below (section D.2.1). A case in which the spectrum is not bounded from below as $d\\to\\infty$ , but the number of eigenvalues below some fixed threshold is $o(d)$ (section D.2.2). Finally, power law spectrum supported on $[0,1]$ with $d\\to\\infty$ (section D.2.3). ", "page_idx": 33}, {"type": "text", "text": "D.2.1 Proofs for case of fixed $d$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proof of Proposition 4.2. Define the composite functions $r(u)~=~\\mathcal{R}(\\Gamma^{-1}(u))$ , and $g(u)\\;\\;=\\;\\;$ $\\gamma(\\Gamma^{-1}(u))$ . Integrating the formula for the risk: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{t}r(u)\\,\\mathrm{d}u=\\int_{0}^{t}F(u)\\,\\mathrm{d}u+\\int_{0}^{t}\\int_{0}^{\\Gamma^{-1}(u)}\\gamma_{s}^{2}K(u-\\Gamma(s))\\mathcal{R}(s)\\,\\mathrm{d}s\\,\\mathrm{d}u}}\\\\ &{}&{\\qquad=\\int_{0}^{t}F(u)\\,\\mathrm{d}u+\\int_{0}^{t}\\int_{0}^{u}K(u-x)r(x)g(x)\\,\\mathrm{d}x\\,\\mathrm{d}u}\\\\ &{}&{\\qquad\\le\\int_{0}^{t}F(u)\\,\\mathrm{d}u+\\gamma_{0}\\int_{0}^{t}r(x)\\int_{x}^{t}K(u-x)\\,\\mathrm{d}u\\,\\mathrm{d}x}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking $t\\to\\infty$ , we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|r\\|_{1}\\leq\\|F\\|_{1}+\\gamma_{0}\\|K\\|_{1}\\|r\\|_{1}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using $\\|K\\|_{1}\\;=\\;\\textstyle\\int_{0}^{\\infty}K(x)\\,\\mathrm{d}x\\;<\\;\\gamma_{0}^{-1}$ , and noting that by Eq. (66), and Eq. (65), we have that $\\begin{array}{r}{\\|F\\|_{1}=\\frac{1}{4}\\mathcal{D}^{2}(0)}\\end{array}$ , and $\\begin{array}{r}{\\|\\boldsymbol{K}\\|_{1}=\\frac{1}{2d}\\operatorname{Tr}(\\boldsymbol{K})}\\end{array}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|r\\|_{1}\\leq\\frac{\\|F\\|_{1}}{1-\\gamma_{0}\\|K\\|_{1}}=\\frac{\\frac{1}{4}\\mathcal{D}^{2}(0)}{1-\\frac{\\gamma_{0}}{2d}\\operatorname{Tr}(K)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "On the hand following Lemma D.3, $\\begin{array}{r}{\\frac{1}{4}\\mathcal{D}^{2}(0)(1+\\frac{\\gamma_{0}}{2d}\\operatorname{Tr}(K))\\leq\\|r\\|_{1}}\\end{array}$ . Therefore, $\\begin{array}{r}{\\|r\\|_{1}\\asymp\\frac{1}{4}\\mathcal{D}^{2}(0)}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Next, rewriting the $\\gamma_{t}$ equation in Table 2 (AdaGrad-Norm for least squares) in terms of $g(u)$ (Lemma D.1), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\ng(u)=\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{d}\\operatorname{Tr}(K)\\int_{0}^{u}r(x)\\,\\mathrm{d}x}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking $u\\rightarrow\\infty$ , and using $\\begin{array}{r}{\\|r\\|_{1}\\asymp\\frac{1}{4}\\mathcal{D}^{2}(0)}\\end{array}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\gamma_{\\infty}=g(\\infty)=\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{d}\\operatorname{Tr}(K)\\lVert r\\rVert_{1}}\\asymp\\frac{\\eta^{2}}{\\frac{b}{\\eta}+\\frac{1}{4d}\\operatorname{Tr}(K)\\mathcal{D}^{2}(0)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This then completes the proof. ", "page_idx": 34}, {"type": "text", "text": "Remark D.1. We note that, on the Least square problem $\\hat{L}(f)=\\hat{\\mu}(f)=1,$ , therefore, the bound in Proposition D.1 yields $\\frac{\\eta^{2}}{\\frac{b}{\\eta}\\!+\\!\\frac{1}{2(1\\!-\\!\\zeta)}\\,\\frac{1}{d}\\,\\mathrm{Tr}(K)\\mathcal{D}^{2}(0)}$ ", "page_idx": 34}, {"type": "text", "text": "Proof of Proposition 4.1. Using the equation for the distance to optimality (Eq. 8), we can derive an equation for the integral of the risk (with no target noise) which we denote by $\\begin{array}{r}{g(t)=\\int_{0}^{t}\\mathcal{R}(s)\\,\\mathrm{d}s}\\end{array}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\ng^{\\prime\\prime}(t)=-\\gamma_{t}\\sum_{i}\\lambda_{i}^{2}\\mathcal{D}_{i}^{2}(t)+\\gamma_{t}^{2}\\frac{\\mathrm{Tr}(K^{2})}{d}g^{\\prime}(t).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $K=I_{d}$ , this equation simplifies, ", "page_idx": 34}, {"type": "equation", "text": "$$\ng^{\\prime\\prime}(t)=-2\\gamma_{t}g^{\\prime}(t)+\\gamma_{t}^{2}{\\frac{\\mathrm{Tr}(K^{2})}{d}}g^{\\prime}(t).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Plugging in the equation for the AdaGrad-Norm learning rate (Table 2) leads to the desired result. We note that by using the equation for the learning rate, one can also derive a close equation for the learning rate itself. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "D.2.2 Vanishingly few eigenvalues near 0 as $d\\to\\infty$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We now consider the case where, as $d\\rightarrow\\infty$ , there are eigenvalues of $K$ arbitrarily close to 0. In Proposition 4.2 we saw a constant lower bound on $\\gamma_{t}$ when $d$ is fixed (and thus there are finitely many eigenvalues within any fixed distance of 0). This can be extended to the case where we have some $C>0$ such that the number of eigenvalues of $K$ below $C$ is $o(d)$ (see Proposition 4.3). ", "page_idx": 34}, {"type": "text", "text": "Proof of Proposition 4.3. Following the structure of the loss, after some time the risk starts to decrease, and therefore $\\mathcal{R}(t)\\leq R_{0}$ for and $t\\geq0$ . Using these observations, we obtain a preliminary lower bound of $\\gamma_{t}>C_{1}t^{-1/2}$ (for $t>0$ ), which enables us to deduce that $\\mathcal{R}(t)$ is integrable and finally obtain a constant lower bound for $\\gamma_{t}$ . The details of this are below. ", "page_idx": 34}, {"type": "text", "text": "For $t\\geq0$ and some $C_{1}>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\frac{\\eta}{\\sqrt{b^{2}+\\frac{2}{d}\\operatorname{Tr}(K)\\int_{0}^{t}\\mathcal{R}(s)d s}}\\ge\\frac{\\eta}{\\sqrt{b^{2}+\\frac{2}{d}\\operatorname{Tr}(K)R_{0}t}}\\ge C_{1}t^{-1/2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, to show that the risk is integrable, we divide the matrix $K$ into two parts $K_{+}$ , and $K_{-}$ , such that the eigenvalues of $K_{+}$ are greater than some $\\alpha_{s}>0$ and the eigenvalues of $K_{-}$ are smaller than $\\alpha_{s}$ where $\\alpha_{s}$ is a decreasing function of $s$ to be determined later. We then have that, following Eq. (8), and the definition of the risk $\\begin{array}{r}{\\mathcal{R}(t)=\\frac{1}{2d}\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{D}_{i}^{2}(t)}\\end{array}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{R}(t)=\\mathcal{R}(0)-\\frac{1}{d}\\sum_{i=1}^{d}\\lambda_{i}^{2}\\int_{0}^{t}\\gamma_{s}\\mathcal{D}_{i}(s)\\mathrm{d}s+\\frac{1}{d}\\int_{0}^{t}\\gamma_{s}^{2}\\,\\mathrm{Tr}(K^{2})\\cdot\\mathcal{R}(s)\\mathrm{d}s}\\\\ {\\displaystyle\\leq\\mathcal{R}(0)-\\int_{0}^{t}\\gamma_{s}(2\\alpha_{s}-\\gamma_{s}\\frac{1}{d}\\,\\mathrm{Tr}(K^{2}))\\cdot\\mathcal{R}(s)\\mathrm{d}s+2\\int_{0}^{t}\\gamma_{s}\\mathcal{R}_{2}(s)\\,\\mathrm{d}s}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with $\\begin{array}{r}{\\mathcal{R}_{2}(s)=\\frac{1}{2d}\\sum_{i:\\lambda_{i}\\leq\\alpha_{s}}\\lambda_{i}\\mathcal{D}_{i}^{2}(s)}\\end{array}$ . Next, choosing $\\begin{array}{r}{\\alpha_{s}=\\gamma_{s}\\frac{1}{d}\\operatorname{Tr}(K^{2})}\\end{array}$ , we show that the last term is of order $o_{d}(1)$ . By Lemma $\\mathbf{D.2}\\forall i$ , $\\mathcal{D}_{i}^{2}(t)\\leq\\operatorname*{max}\\left(\\gamma_{t_{1}}\\mathcal{R}(t_{1}),\\mathcal{D}_{i}^{2}(0)\\right)=c_{0}$ where the bound $c_{0}$ comes from the assumption $\\langle X^{\\star},\\omega_{i}\\rangle=O(d^{-1/2})$ and the initialization $X_{0}=0$ . Therefore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n2\\int_{0}^{t}\\gamma_{s}\\mathcal{R}_{2}(s)\\,\\mathrm{d}s\\leq\\frac{1}{d^{2}}\\,\\mathrm{Tr}(K^{2})c_{0}\\int_{0}^{t}\\gamma_{s}N_{s}\\,\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\begin{array}{r}{N_{s}\\,=\\,\\sum_{i=1}^{d}1_{\\lambda_{i}\\le\\gamma_{s}\\frac{1}{d}\\operatorname{Tr}(K^{2})}}\\end{array}$ . This implies that, if $\\gamma_{s}N_{s}\\,=\\,o(d)$ , then $\\begin{array}{r l r}{~}&{{}\\mathrm{2}\\int_{0}^{t}\\gamma_{s}\\mathcal{R}_{2}(s)\\,\\mathrm{d}s\\,=}\\end{array}$ $o_{d}(1)$ , provided that $d$ is taken to be large before $t$ . ", "page_idx": 35}, {"type": "text", "text": "We then have that up to $o_{d}(1)$ constant, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)\\leq\\mathcal{R}(0)-\\frac{1}{d}\\operatorname{Tr}(K^{2})\\int_{0}^{t}\\gamma_{s}^{2}\\cdot\\mathcal{R}(s)\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using Gronwall\u2019s inequality, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(t)\\le\\mathcal{R}(0)e^{-\\frac{1}{d}\\operatorname{Tr}(K^{2})\\int_{0}^{t}\\gamma_{s}^{2}\\,\\mathrm{d}s}\\le\\mathcal{R}(0)e^{-\\frac{1}{d}\\operatorname{Tr}(K^{2})C_{1}^{2}t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the last transition we used the lower bound on the learning rate derived in Eq. (71). Thus, the risk is integrable, i.e. there is some $C_{3}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\mathscr{R}(s)\\,\\mathrm{d}s\\leq\\frac{\\mathscr{R}(0)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})C_{1}^{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for all $t>0$ . Finally, we plug this into the formula for $\\gamma_{t}$ and conclude that, for all $t>0$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma_{t}\\ge\\frac{\\eta}{\\sqrt{b^{2}+\\frac{\\frac{1}{d}\\operatorname{Tr}(K)\\mathcal{R}(0)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})C_{1}^{2}}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma D.2. Assume that the risk is bounded and attains its maximum at time $t_{1}$ . Then, for each $i$ , we have $\\mathcal{D}_{i}^{2}(t)\\leq\\operatorname*{max}(\\gamma_{t_{1}}\\mathcal{R}(t_{1}),\\mathcal{D}_{i}^{2}(0))$ for all $t\\geq0$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Case 1: Suppose that $\\mathcal{D}_{i}^{2}(0)\\,\\leq\\,\\gamma_{0}\\mathcal{R}(0)$ . Then, by equation (8), $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathscr{D}_{i}^{2}(0)\\,\\geq\\,0$ . However, since $\\mathcal{D}_{i}^{2}(t),\\mathcal{R}(t)$ are continuous, this equation implies that $\\mathcal{D}_{i}^{2}(t)\\,\\leq\\,\\gamma_{t}\\mathcal{R}(t)$ for all $t$ and thus $\\mathcal{D}_{i}^{2}(t)\\leq\\gamma_{t_{1}}\\mathcal{R}(t_{1})$ for all $t$ . ", "page_idx": 35}, {"type": "text", "text": "Case 2: Suppose that $\\mathcal{D}_{i}^{2}(0)>\\gamma_{0}\\mathcal{R}(0)$ . Then, by equation (8), $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathscr{D}_{i}^{2}(0)<0$ . If $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathscr{D}_{i}^{2}(t)<0}\\end{array}$ for all $t$ , then $\\mathcal{D}_{i}^{2}(t)\\leq\\mathcal{D}_{i}^{2}(0)$ for all $t$ . If at some point $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}_{i}^{2}(t)>0}\\end{array}$ , this implies $\\mathcal{D}_{i}^{2}(t)\\leq\\gamma_{t}\\mathcal{R}(t)$ and we are in Case 1. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "In the next section, we consider cases in which the risk is not integrable, an example of such case is when the spectrum of $K$ is supported on the interval $[0,1]$ or has power-law behavior near 0. ", "page_idx": 35}, {"type": "text", "text": "D.2.3 Power law behavior at $d\\to\\infty$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Non-asymptotic bound for the Convolutional Volterra In this section, we use the convolutional Volterra structure of the risk (Eq. (64)) to derive non-asymptotic bounds on the risk, which will be useful in Section D.2.3 to derive the asymptotic behavior of the risk and the learning rate under power law assumption on the spectrum of the covariance matrix and the discrepancy from the target at initialization. ", "page_idx": 36}, {"type": "text", "text": "Lemma D.3. Let $\\begin{array}{r}{\\Gamma(t):=\\int_{0}^{t}\\gamma_{s}}\\end{array}$ ds and let ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)=F(\\Gamma(t))+\\int_{0}^{t}\\gamma_{s}^{2}K(\\Gamma(t)-\\Gamma(s))\\mathcal{R}(s)\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\gamma_{t},\\kappa$ are monotonically decreasing, with $\\|\\kappa\\|_{1}<\\infty$ . Then all $t_{\\perp}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)\\geq F(\\Gamma(t))+\\int_{0}^{t}\\gamma_{s}^{2}K(\\Gamma(t)-\\Gamma(s))F(\\Gamma(s))\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If in addition, there exist $\\epsilon>0$ and $T>0$ such that, for all $t>T$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{0}^{t}K(s)K(t-s)\\,\\mathrm{d}s\\leq2(1+\\epsilon)\\|K\\|_{1}K(t)\\quad a n d\\quad2\\|K\\|_{1}(1+\\epsilon)\\gamma_{0}<1\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "then for all $t$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)\\leq F(\\Gamma(t))+C\\int_{0}^{t}\\gamma_{s}^{2}K(\\Gamma(t)-\\Gamma(s))F(\\Gamma(s))\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for ", "page_idx": 36}, {"type": "equation", "text": "$$\nC=\\left(\\frac{K(0)}{K(T)(2\\epsilon+1)}+2\\right)\\frac{1}{1-2\\gamma(0)\\|K\\|_{1}(1+\\epsilon)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. The lower bound holds trivially, using $\\mathcal{R}(s)\\geq F(\\Gamma(s))$ . For the upper bound, we start with the following change of variables: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{R}(t)=F(\\Gamma(t))+\\int_{0}^{\\Gamma(t)}g(u)\\mathcal{K}(\\Gamma(t)-u))\\mathcal{R}(u)\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $g(u)=\\gamma_{\\Gamma^{-1}(u)}$ . Let us define the convolution map ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{G}(f)(\\Gamma)=\\mathcal{K}*(g f)(\\Gamma)=\\int_{0}^{\\Gamma}\\mathcal{K}(\\Gamma-u)g(u)f(u)\\,\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Next we show that this map is contracting and in particular, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}^{2}(f)=\\mathcal{G}(\\mathcal{G}(f))(t)=\\displaystyle\\int_{0}^{t}K(t-s)\\mathcal{G}(f)(s)g(s)\\,\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{t}K(t-s)\\int_{0}^{s}K(s-u)g(u)f(u)\\,\\mathrm{d}u g(s)\\,\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{t}\\left(\\int_{u}^{t}K(t-s)K(s-u)g(s)\\,\\mathrm{d}s\\right)g(u)f(u)\\,\\mathrm{d}u}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{t}K^{*2}(t-u)g(u)^{2}f(u)\\,\\mathrm{d}u}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the third transition is since $u<s<t$ . The last transition is by change of variables and the assumption that $\\gamma_{t}$ is a monotone decreasing function. Consecutive application of the convolution map will then yield by induction, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{G}^{j}(f)(t)\\leq\\int_{0}^{t}{\\mathcal{K}^{*(j)}(t-u)g(u)^{j}f(u)\\,\\mathrm{d}u}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, expanding the loss and using the above upper bound, and denote by $q=2(1+\\varepsilon)\\|\\kappa\\|_{1}\\gamma_{0}$ such that $q<1$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{R}(t)=F(t)+\\sum_{j=1}^{\\infty}{g^{j}(F)(t)}}\\\\ {\\displaystyle\\leq F(t)+\\sum_{j=1}^{\\infty}\\int_{0}^{t}{K^{*}(t)(t-u)g(u)^{j}F(u)\\,\\mathrm{d}u}}\\\\ {\\displaystyle\\leq F(t)+\\left(\\sum_{j=0}^{\\infty}(2\\|K\\|_{1}\\gamma_{0}(1+\\varepsilon))^{j}-1\\right)C_{1}\\int_{0}^{t}{K(t-u)g(u)F(u)\\,\\mathrm{d}u}}\\\\ {\\displaystyle\\leq F(t)+\\frac{q}{1-q}C_{1}(K*(g F))(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the third transition is by Lemma D.4, with C1 =K(T K)((20\u03f5)+1) , which then completes the proof. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Lemma D.4 (Lemma IV.4.7 in [3]). Suppose $\\kappa$ is monotonically decreasing, with $\\|\\kappa\\|_{1}<\\infty,$ , and that there exists $T>0$ such that $\\forall t\\geq T$ , and $\\epsilon\\geq0$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\mathcal{K}(s)\\mathcal{K}(t-s)\\,\\mathrm{d}s\\leq2(1+\\epsilon)\\|\\mathcal{K}\\|_{1}\\mathcal{K}(t).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq0}\\frac{K^{*n}(t)}{K(t)}\\leq(2\\|K\\|_{1}(1+\\epsilon))^{n-1}\\left(\\frac{K(0)}{K(T)(2\\epsilon+1)}+1\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Define $\\begin{array}{r}{\\alpha_{n}=\\operatorname*{sup}_{t\\geq0}\\frac{\\kappa^{*n}(t)}{\\mathcal{K}(t)(2\\|\\mathcal{K}\\|_{1})^{n-1}}}\\end{array}$ , trivially $\\alpha_{1}=1$ . Consider the $n+1$ convolution, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{K^{*(n+1)}(t)}{K(t)(2\\|K\\|_{1})^{n}}=\\frac{1}{K(t)}\\int_{0}^{t}\\frac{K(s)K^{*n}(t-s)}{(2\\|K\\|_{1})^{n}}\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By the assumption of the Lemma, we know that there exists some $T>0$ such that for $\\forall t\\geq T$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\frac{\\d\\mathcal{K}(s)\\mathcal{K}(t-s)}{2\\|\\mathcal{K}\\|_{1}}\\,\\mathrm{d}s\\leq(1+\\epsilon)\\mathcal{K}(t).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, if $t\\geq T$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{K(t)}\\int_{0}^{t}\\frac{K(s)K^{*n}(t-s)}{(2\\|K\\|_{1})^{n}}\\,\\mathrm{d}s}}\\\\ &{=\\int_{0}^{t}\\frac{K(s)K(t-s)}{2\\|K\\|_{1}}\\frac{K^{*n}(t-s)}{K(t-s)(2\\|K\\|_{1})^{n-1}}\\,\\mathrm{d}s\\leq\\alpha_{n}(1+\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "On the other hand, if $t<T$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{K(t)}\\int_{0}^{t}\\frac{K(s)K^{\\ast n}(t-s)}{(2\\|K\\|_{1})^{n}}\\,\\mathrm{d}s\\leq\\frac{K(0)}{K(T)}\\frac{\\|K^{\\ast n}(t)\\|_{1}}{(2\\|K\\|_{1})^{n}}\\leq\\frac{K(0)}{K(T)2^{n}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Taking supremum in Eq. (82), and combining the results of Eq. (85), and Eq. (84), we obtain that, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\alpha_{n+1}\\leq\\frac{\\kappa(0)}{\\mathcal{K}(T)2^{n}}+\\alpha_{n}(1+\\epsilon)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Solving the above recursion equation, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{n}\\leq\\displaystyle\\frac{K(0)}{K(T)}\\displaystyle\\sum_{k=0}^{n-2}\\displaystyle\\frac{1}{2^{n-k-1}}(1+\\epsilon)^{k}+(1+\\epsilon)^{n-1}=\\displaystyle\\frac{K(0)}{K(T)2^{n-1}}\\displaystyle\\frac{1-(2(1+\\epsilon))^{n-1}}{1-2(1+\\epsilon)}+(1+\\epsilon)^{n-1}}\\\\ &{\\phantom{\\alpha_{n}\\geq\\alpha_{n}+}\\leq(1+\\epsilon)^{n-1}\\left(\\displaystyle\\frac{K(0)}{K(T)(2\\epsilon+1)}+1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "rearranging the terms we arrived at the required result. ", "page_idx": 37}, {"type": "text", "text": "Asymptotic analysis of the risk Here, we consider a family of models with $d\\to\\infty$ , for which the following power law asymptotics assumption is satisfied: ", "page_idx": 38}, {"type": "text", "text": "Assumption 8. $F(x)\\asymp x^{-\\kappa_{1}}$ and $K(x)\\asymp x^{-\\kappa_{2}}$ for $x\\geq1$ with \u03ba1 \u22650, \u03ba2 > 1 ", "page_idx": 38}, {"type": "text", "text": "Corollary D.1 apply Lemma D.3 in the setting for which $F$ , and $\\kappa$ has a power law behavior asymptotically. It shows that the risk will then be dominated by $F$ only. Corollary D.2 shows the behavior of the learning rate in this setting. Finally, Lemma D.5 shows that Assumption 8 is a consequence of a power law spectrum near zero on the eigenvalues of the covariance matrix and a power law assumption on the projected discrepancy at initialization. ", "page_idx": 38}, {"type": "text", "text": "Corollary D.1. Suppose Assumption 8 is satisfied, then $\\mathcal{R}(t)\\asymp F(\\Gamma(t))$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. Define $g(u)=\\gamma_{\\Gamma^{-1}(u)}$ and $r(u)=\\mathcal{R}(\\Gamma^{-1}(u))$ and observe that $g(u)$ is a decreasing function. Then, from the upper bound in Lemma D.3, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(u)\\leq F(u)+C\\displaystyle\\int_{0}^{u}g(v)K(u-v)F(v)\\,\\mathrm{d}v}\\\\ &{\\qquad=F(u)+C\\left(\\displaystyle\\int_{0}^{u/2}g(v)K(u-v)F(v)\\,\\mathrm{d}v+\\displaystyle\\int_{u/2}^{u}g(v)K(u-v)F(v)\\,\\mathrm{d}v\\right)}\\\\ &{\\qquad\\leq F(u)+C_{1}g(0)\\left(\\left(\\frac{u}{2}\\right)^{-\\kappa_{2}}\\displaystyle\\int_{0}^{u/2}F(v)\\,\\mathrm{d}v+\\left(\\frac{u}{2}\\right)^{-\\kappa_{1}}\\displaystyle\\int_{u/2}^{u}K(u-v)\\,\\mathrm{d}v\\right)}\\\\ &{\\qquad\\leq F(u)+C_{2}(u^{-\\kappa_{2}+1-\\kappa_{1}}+u^{-\\kappa_{1}}\\|K\\|)}\\\\ &{\\qquad=O(F(u)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining this upper bound with the lower bound from Lemma D.3 and that $\\kappa_{2}>1$ , we conclude that $r(u)\\breve{\\asymp}F(u)^{\\tilde{}}$ and $\\mathcal{R}(t)\\asymp F(\\Gamma(t))$ . \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Next, we derive the asymptotics of $\\gamma_{t}$ . There are three different cases, depending on whether the risk is integrable, which translates to a threshold with respect to the parameter $\\kappa_{1}$ . ", "page_idx": 38}, {"type": "text", "text": "Corollary D.2. Suppose Assumption 8 then the following asymptotics for the learning rate hold: ", "page_idx": 38}, {"type": "text", "text": "\u2022 For $\\kappa_{1}>1$ , there exists $\\tilde{\\gamma}$ such that $\\gamma_{t}\\geq\\tilde{\\gamma}$ and $\\mathcal{R}(t)\\asymp t^{-\\kappa_{1}}$ for all $t\\geq0$ .   \n\u2022 For $\\kappa_{1}<1$ , $\\gamma_{t}\\asymp t^{-(1-\\kappa_{1})/(2-\\kappa_{1})}$ and $\\mathcal{R}(t)\\asymp t^{-\\frac{\\kappa_{1}}{2-\\kappa_{1}}}$ for all $t\\geq1$ .   \n\u2022 For \u03ba1 = 1, $\\begin{array}{r}{\\gamma_{t}\\asymp\\frac{1}{\\log(t+1)}~}\\end{array}$ and $\\begin{array}{r}{\\mathcal{R}(t)\\asymp(\\frac{t}{\\log(t+1)})^{-\\kappa_{1}}}\\end{array}$ for all $t\\geq1$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. Using the notations $g(u)$ and $r(u)$ defined above along with the change of variable $u=\\Gamma(t)$ , we get $\\begin{array}{r}{\\int_{0}^{t}\\mathcal{R}(s)\\,\\mathrm{d}s=\\int_{0}^{u}\\frac{r(v)}{g(v)}\\,\\mathrm{d}v}\\end{array}$ . Combining this with Corollary D.1 and the formula for $\\gamma_{t}$ we get ", "page_idx": 38}, {"type": "equation", "text": "$$\ng(u)\\asymp\\frac{\\eta}{\\sqrt{b^{2}+\\frac{2}{d}\\mathrm{Tr}(K)\\int_{0}^{u}\\frac{(1+v)^{-\\kappa_{1}}}{g(v)}\\,\\mathrm{d}v}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\begin{array}{r l r}{I(u)\\!\\!}&{{}=}&{\\!\\!b^{2}\\;+\\;\\frac{2}{d}\\mathrm{Tr}(K)\\int_{0}^{u}\\frac{(1+v)^{-\\kappa_{1}}}{g(v)}\\,\\mathrm{d}v}\\end{array}$ 1dv and observe that g(u) \u224d \u221aI1(u) and $\\begin{array}{r l}{I^{\\prime}(u)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{2}{d}\\mathrm{Tr}(K)\\frac{(1+u)^{-\\kappa_{1}}}{g(u)}}\\end{array}$ . Thus, $I(u)$ satisfies $\\begin{array}{r}{\\frac{I^{\\prime}(u)}{\\sqrt{I(u)}}\\asymp(1+u)^{-\\kappa_{1}}}\\end{array}$ so we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sqrt{I(u)}-\\sqrt{I(0)}\\asymp\\int_{0}^{u}(1+v)^{-\\kappa_{1}}\\,\\mathrm{d}v.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the case of $\\kappa_{1}>1$ , this implies $\\begin{array}{r}{\\sqrt{I(u)}\\leq\\sqrt{I(0)}+C\\int(1+v)^{-\\kappa_{1}}\\,\\mathrm{d}v}\\end{array}$ . This upper bound on $I(u)$ gives a corresponding lower bound on $g(u)$ and thus a lower bound on $\\gamma_{t}$ . ", "page_idx": 38}, {"type": "text", "text": "In the case of $\\kappa_{1}<1$ , we have $\\sqrt{I(u)}-\\sqrt{I(0)}\\asymp(1+v)^{1-\\kappa_{1}}$ so, for $u$ sufficiently large, $g(u)\\asymp$ $(1+u)^{\\kappa_{1}-1}$ . To recover the asymptotic for $\\gamma_{t}$ , we observe that $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}u}\\Gamma^{-1}(u)=\\frac{1}{g(u)}\\asymp(1+u)^{1-\\kappa_{1}}}\\end{array}$ Integrating both sides and changing back to $t$ variables, we get $t\\asymp(1+\\Gamma(t))^{2-\\kappa_{1}}$ (or equivalently ", "page_idx": 38}, {"type": "text", "text": "$1+\\Gamma(t)\\asymp t^{1/(2-\\kappa_{1})})$ . Finally, plugging this into the formula for $\\gamma_{t}$ and applying Corollary D.1, we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma_{t}\\asymp\\frac{\\eta}{\\sqrt{b^{2}+\\frac{2}{d}\\mathrm{Tr}(K)\\int_{0}^{t}F(\\Gamma(s))\\,\\mathrm{d}s}}\\asymp(1+t)^{-(1-\\kappa_{1})/(2-\\kappa_{1})}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In the case of $\\,\\kappa_{1}=1$ , we follow a similar procedure as for $\\kappa_{1}<1$ to show that $t\\asymp\\Gamma(t)\\log(\\Gamma(t))$ for sufficiently large $t$ . This implies $\\Gamma(t)\\asymp t/\\log(t)$ which gives the desired result after integration. The decay rate of the risk is then immediate using Corollary D.1. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Lemma D.5. Let $K$ have a spectrum that converges as $d\\to\\infty$ to the power law measure $\\rho(\\lambda)=$ $C\\lambda^{-\\beta}\\mathbf{1}_{(0,\\lambda_{\\operatorname*{max}})}$ , with $\\begin{array}{r}{C^{-1}=\\frac{\\lambda_{\\mathrm{max}}^{1-\\beta}}{1-\\beta}}\\end{array}$ \u03bb1m\u2212a\u03b2x for some \u03b2 < 1, and \u03bbmax > 0, and suppose that Di2 (0) \u223c\u03bbi\u2212 \u03b4, then $F(t)\\asymp t^{-\\kappa_{1}}$ , and $K(t)\\asymp t^{-\\kappa_{2}}$ , with $\\kappa_{1}=2-\\beta-\\delta$ , and $\\kappa_{2}=3-\\beta$ . In addition, $K(t)\\asymp t^{-\\kappa_{2}}$ , satisfies Eq. (80). ", "page_idx": 39}, {"type": "text", "text": "Proof. Following the definition in Eq. (66), and Eq. (65) ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{F(x)=\\displaystyle\\frac{1-\\beta}{2\\lambda_{\\operatorname*{max}}^{1-\\beta}}\\int_{0}^{\\lambda_{\\operatorname*{max}}}\\lambda^{1-\\beta-\\delta}e^{-2\\lambda x}\\,\\mathrm{d}\\lambda}\\\\ {\\displaystyle\\qquad=\\frac{1-\\beta}{2\\lambda_{\\operatorname*{max}}^{1-\\beta}(2x)^{2-\\beta-\\delta}}\\int_{0}^{2\\lambda_{\\operatorname*{max}}x}y^{1-\\beta-\\delta}e^{-y}\\,\\mathrm{d}y=\\frac{1-\\beta}{\\lambda_{\\operatorname*{max}}^{1-\\beta}2^{3-\\beta-\\delta}}\\frac{\\gamma(2-\\beta-\\delta,2\\lambda_{\\operatorname*{max}}x)}{x^{2-\\beta-\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Similarly for $\\kappa$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{K}(x)=\\frac{1-\\beta}{\\lambda_{\\operatorname*{max}}^{1-\\beta}}\\int_{0}^{\\lambda_{\\operatorname*{max}}}\\lambda^{2-\\beta}e^{-2\\lambda x}\\,\\mathrm{d}\\lambda=\\frac{1-\\beta}{\\lambda_{\\operatorname*{max}}^{1-\\beta}2^{3-\\beta}}\\frac{\\gamma(3-\\beta,2\\lambda_{\\operatorname*{max}}x)}{x^{3-\\beta}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with $\\begin{array}{r}{\\gamma(s,z)=\\int_{0}^{z}x^{s-1}e^{-x}\\,\\mathrm{d}x}\\end{array}$ is the incomplete gamma function. For large $z$ , $\\gamma(s,z)\\asymp\\Gamma(s)$ , the complete gamma function. We therefore obtain $\\kappa_{1}=2-\\beta-\\delta$ , and $\\kappa_{2}=3-\\beta$ . Next, we show that $K(x)\\asymp x^{-\\kappa_{2}}$ satisfies Eq. (80), ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{t}K(s)K(t-s)\\,\\mathrm{d}s\\le\\displaystyle\\int_{0}^{t/2}K(t)K(t-s)\\,\\mathrm{d}s+\\displaystyle\\int_{t/2}^{t}K(t)K(t-s)\\,\\mathrm{d}s}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\le K(t/2)\\left(\\displaystyle\\int_{0}^{t/2}K(s)\\,\\mathrm{d}s+\\displaystyle\\int_{t/2}^{t}K(t-s)\\,\\mathrm{d}s\\right)\\le2K(t/2)\\|K\\|_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "by the power-law assumption for $t>T,\\mathcal{K}(t/2)\\asymp\\mathcal{K}(t)$ which then complete the proof. ", "page_idx": 39}, {"type": "text", "text": "Proof of Proposition 4.4. The proof is an immediate application of Corollary D.2 with, $\\kappa_{1}=2\\!-\\!\\beta\\!-\\!\\delta$ as implied by Lemma D.5. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Remark D.2. This includes the case $\\beta=0$ , which is the uniform measure on $[0,\\lambda_{\\mathrm{max}}]$ . ", "page_idx": 39}, {"type": "text", "text": "E Polyak Stepsize ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The distance to optimality of SGD is measured say by $D^{2}(X)\\,=\\,\\|X\\,-\\,X^{\\star}\\|^{2}$ . Let us consider the deterministic equivalent for the distance to optimality $\\mathcal{D}^{2}(t)$ in (11). Fixing $T\\,>\\,0$ and any $\\varepsilon\\in(0,1/2)$ , we have by Theorem 2.1 (see also corollary B.1 which show concentration for large class of statistics) that $\\begin{array}{r}{\\operatorname*{sup}_{0\\leq t\\leq T}|\\|X_{\\lfloor t d\\rfloor}-X^{\\star}\\|^{2}-\\mathcal{D}^{2}(\\dot{t})|\\leq d^{-\\varepsilon}}\\end{array}$ , w.o.p. In this way, if we want to guarantee that the distance to optimality of SGD decreases, we need $\\mathrm{d}\\mathscr{D}^{2}(t)<0$ with the maximum decrease being $\\operatorname*{min}_{\\gamma_{t}}\\mathrm{d}\\mathcal{D}^{2}(t)$ . ", "page_idx": 39}, {"type": "text", "text": "As it turns out, the evolution of $\\mathcal{D}^{2}$ is particular simple, as it solves the differential equation (derived from the ODE in (9)) ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}^{2}(t)=-2\\gamma_{t}A(\\mathcal{B}(t))+\\frac{\\gamma_{t}^{2}}{d}\\mathrm{Tr}(K)I(\\mathcal{B}(t)),\\quad\\left\\{\\begin{array}{l l}{A(\\mathcal{B})=\\mathbb{E}_{a,\\epsilon}[\\langle x-x^{\\star},f^{\\prime}(x\\oplus x^{\\star})\\rangle],}\\\\ {I(\\mathcal{B})=\\mathbb{E}_{a,\\epsilon}[f^{\\prime}(x\\oplus x^{\\star})^{2}],\\quad\\mathrm{where}}\\\\ {(x\\oplus x^{\\star})\\sim N(0,\\mathcal{B}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "image", "img_path": "4VWnC5unAV/tmp/9025645890bf4a89b4cba3565031767d6423a6572a9c1a2c2306027bb7d72e0d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 5: Convergence in Exact Line Search on a noiseless least squares problem. The plot on the left illustrates the convergence of the risk function, while the center and right plots depict the convergence of the quotient $\\frac{\\bar{\\mathcal{D}}_{\\lambda_{2}}(t)}{\\bar{\\mathcal{D}}_{\\lambda_{1}}(t)}$ and the learning rate $\\gamma_{t}$ , respectively. Further details and formulas for the limiting behavior can be found in the Appendix F.2. See Appendix H for simulation details. ", "page_idx": 40}, {"type": "text", "text": "The distance to optimality threshold, $\\bar{\\gamma}_{t}^{\\mathcal{D}}$ , occurs precisely when $\\mathrm{d}\\mathscr{D}^{2}<0$ . This choice of $\\gamma$ makes the ODE for the distance to optimality stable. By translating the relevant deterministic quantities in $\\bar{\\gamma}_{t}^{\\mathcal{D}}$ back to SGD quantities, we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\bar{\\mathfrak{g}}_{k}^{\\mathcal{O}}}{\\bar{\\mathfrak{g}}_{k}^{\\mathcal{O}}}\\,\\frac{2\\langle X_{k}-X^{\\star},\\nabla\\mathcal{R}(X_{k})\\rangle}{\\frac{\\mathbb{T}(K)}{d}\\mathbb{E}_{a,\\mathfrak{c}}[f^{\\prime}(\\langle X_{k},a\\rangle;\\langle X^{\\star},a\\rangle,\\epsilon)^{2}]}\\mathrm{~with~the~deterministic~equiv.~}\\bar{\\gamma}_{t}^{\\mathcal{O}}=\\frac{2A(\\mathcal{B}(t))}{\\frac{\\mathbb{T}(K)}{d}I(\\mathcal{B}(t))}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "A greedy learning rate that maximizes the decrease at each iteration is simply given by ${\\mathfrak{g}}_{t}^{\\mathrm{Polyak}}\\in$ arg min $\\dot{\\mathrm{d}}\\mathcal{D}^{2}(t)$ . This has a closed form and we call this Polyak stepsize11. Again translating this back to SGD, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{ak~learning~rate}\\quad\\mathfrak{g}_{k}^{\\mathrm{Polyak}}=\\frac{1}{2}\\bar{\\mathfrak{g}}_{k}^{\\mathcal{D}}\\quad\\mathrm{and}\\quad\\mathrm{deterministic~equivalent}\\quad\\gamma_{t}^{\\mathrm{Polyak}}=\\frac{1}{2}\\bar{\\gamma}_{t}^{\\mathcal{D}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In this context, the Polyak learning rate is impractical because we do not known $X^{\\star}$ . In spite of this, we can learn some things about this learning rate as it is the natural extension of Polyak learning rate to SGD. ", "page_idx": 40}, {"type": "text", "text": "The quantities $A({\\mathcal{B}})$ and $I({\\mathcal{B}})$ in (88) and (89) only depend on the low-dimensional function $f$ and thus do not carry any covariance $K$ or $d$ dependence. Moreover, under additional assumptions on the function such as (strong) convexity, we can bound from below $A({\\mathcal{B}})/I({\\mathcal{B}})$ . Thus, in terms covariance $K$ and $d$ , the Polyak stepsize $\\begin{array}{r}{\\mathfrak{g}_{k}^{\\mathrm{Polyak}}\\asymp\\frac{1}{\\mathrm{Tr}(K)/(d)}=\\frac{1}{\\mathrm{avg.~eig~of~}K}}\\end{array}$ . ", "page_idx": 40}, {"type": "text", "text": "In the case of least squares (see (7)), we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathfrak{g}_{k}^{\\mathrm{Polyak}}=\\frac{2\\mathcal{R}(X_{k})-\\omega^{2}}{\\frac{2\\mathrm{Tr}(K)}{d}\\mathcal{R}(X_{k})}\\mathrm{~and~on~a~noiseless~least~squares,~~~}\\mathfrak{g}_{k}^{\\mathrm{Polyak}}=\\frac{1}{\\frac{\\mathrm{Tr}(K)}{d}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The latter gives the best fixed learning rate for a noiseless target on a LS problem (as noted in [35, 43]). ", "page_idx": 40}, {"type": "text", "text": "F Line Search ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "F.1 General Line Search ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Naturally, one can ask a similar question as in Polyak in the context of line search (i.e., decreasing risk at each iteration of SGD). First, by the structure of the risk (Assumption 3 and 4), ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{R}(X)\\|^{2}=m(W^{T}K^{2}W)\\quad\\mathrm{and}\\quad\\operatorname{Tr}(\\nabla^{2}\\mathcal{R}(X)K)=v(K).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore using (9), we have that the deterministic equivalent for $\\|\\nabla\\mathcal{R}(X)\\|^{2}$ is $\\begin{array}{r l}{\\mathcal{M}(t)}&{{}=}\\end{array}$ $\\frac{1}{2}\\sum_{i=1}^{d}m(\\mathscr{V}_{i}(t)\\lambda_{i}^{2})$ . In this case, the deterministic equivalent for the risk $\\mathcal{R}$ satisfies the following ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathcal{R}=-\\gamma_{t}\\mathcal{M}(t)\\,\\mathrm{d}t+\\frac{\\gamma_{t}^{2}}{d}v(K)I(\\mathcal{B}(t)).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From this, we get an immediate learning rate (stability) threshold for the risk, that is, $\\bar{\\mathfrak{g}}_{k}^{\\mathcal{R}}$ is the largest learning rate for which SGD is guaranteed to decrease at each iteration, i.e., when the deterministic equivalent of $\\mathcal{R}$ satisfies $\\mathrm{d}\\mathcal{R}<0$ or equivalently after translating relevant terms into SGD quantities ", "page_idx": 41}, {"type": "text", "text": "risk threshold $\\bar{\\mathfrak{g}}_{k}^{\\mathcal{R}}=\\frac{\\|\\nabla\\mathcal{R}(X_{k})\\|^{2}}{\\frac{\\operatorname{T}(K\\nabla^{2}\\mathcal{R}(X_{k}))}{d}I(W_{k}^{T}K W_{k})}\\mathrm{~and~deterministic~equiv}\\quad\\bar{\\gamma}_{t}^{\\mathcal{R}}=\\frac{\\mathcal{M}(t)}{\\frac{v(K)}{d}I(\\mathcal{B}(t))}.$ ", "page_idx": 41}, {"type": "text", "text": "The greediest approach, which we call exact line search, would choose the learning rate such that \u03b3tline\u2208arg min\u03b3 dR. In this case, we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{g}_{k}^{\\mathrm{line}}=\\frac{1}{2}\\mathfrak{g}_{k}^{\\mathcal{R}}\\quad\\mathrm{and}\\quad\\mathrm{deterministic~equiv}\\quad\\gamma_{t}^{\\mathrm{line}}=\\frac{1}{2}\\gamma_{t}^{\\mathcal{R}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "F.2 Line Search on least squares ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section, we provide a proof of Proposition 3.1, but, we show more than this including the exact limiting value for $\\gamma_{t}$ . ", "page_idx": 41}, {"type": "text", "text": "Proposition F.1. Consider the noiseless $\\omega=0$ ) least squares problem (7) . Then the learning rate is always lower bounded by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}\\leq\\gamma_{t}^{\\operatorname*{line}}\\quad f o r\\,a l l\\,t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover, suppose $K$ has only two distinct eigenvalues $\\lambda_{1}>\\lambda_{2}>0,$ , i.e., $K$ has $d/2$ eigenvalues equal to $\\lambda_{1}$ eigenvalues and $d/2$ eigenvalues equal to $\\lambda_{2}$ . In this context, the exact limiting value of \u03b3tlineis given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\gamma_{t}^{\\mathrm{line}}=\\frac{2\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}x\\right)}{\\left(\\lambda_{1}+\\lambda_{2}x\\right)\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\right)},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where x is the positive real root of the second-degree polynomial ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{P}(x)=\\lambda_{1}\\lambda_{2}(x+1)(\\lambda_{2}x-\\lambda_{1})+(\\lambda_{2}-\\lambda_{1})^{3}x.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This leads to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}\\leq\\underset{t\\to\\infty}{\\operatorname*{lim}}\\gamma_{t}^{\\operatorname*{line}}\\leq\\frac{2\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. We establish the inequality ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\frac{\\lambda_{\\operatorname*{min}}(K)}{\\frac{1}{d}\\operatorname{Tr}(K^{2})}}\\leq\\gamma_{t}^{\\mathrm{line}}\\quad{\\mathrm{for~all~}}t\\geq0\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "by observing ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\lambda_{i}^{2}\\mathcal{D}_{i}^{2}(t)\\geq2\\lambda_{\\operatorname*{min}}(K)\\frac{1}{2d}\\sum_{i=1}^{d}\\lambda_{i}\\mathcal{D}_{i}^{2}(t)=2\\lambda_{\\operatorname*{min}}(K)\\mathcal{R}(t).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now let us consider $K\\sim{\\textstyle\\frac{1}{2}}\\lambda_{1}+{\\textstyle\\frac{1}{2}}\\lambda_{2}$ for $\\lambda_{1}>\\lambda_{2}>0$ . ", "page_idx": 41}, {"type": "text", "text": "We define $\\begin{array}{r}{\\mathcal{D}_{\\lambda}(t)\\stackrel{\\mathrm{def}}{=}\\sum_{\\lambda_{i}=\\lambda}^{d}\\mathcal{D}_{i}^{2}(t)}\\end{array}$ . Utilizing the ODEs in (9), we derive ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}_{\\lambda}(t)=-2\\gamma_{t}\\lambda\\mathcal{D}_{\\lambda}(t)+2\\gamma_{t}^{2}\\lambda\\times|\\{\\lambda=\\lambda_{i}\\}_{i=1}^{d}|\\times\\mathcal{R}(t)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for each distinct eigenvalue $\\lambda$ of $K$ . Here $\\vert\\{\\lambda=\\lambda_{i}\\}_{i=1}^{d}\\vert$ is the number of eigenvalues of $K$ that are equal to $\\lambda$ . It immediately follows by our construction of $K$ that $\\vert\\{\\lambda=\\lambda_{i}\\}_{i=1}^{d}\\vert=\\frac{d}{2}$ . Thus, we establish the following system of ODEs ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}_{\\lambda_{1}}(t)=-2\\gamma_{t}\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)+d\\gamma_{t}^{2}\\lambda_{1}\\mathcal{R}(t)\\right.}\\\\ {\\left.\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{D}_{\\lambda_{2}}(t)=-2\\gamma_{t}\\lambda_{2}\\mathcal{D}_{\\lambda_{2}}(t)+d\\gamma_{t}^{2}\\lambda_{2}\\mathcal{R}(t)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{R}(t)=\\frac{1}{2d}\\left(\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)}\\end{array}$ and $\\begin{array}{r}{\\gamma_{t}^{\\mathrm{line}}=\\frac{2\\left(\\lambda_{1}^{2}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}^{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)}{\\left(\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\right)}.}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Since $\\mathcal{D}_{\\lambda_{2}}(t)~\\ge~0$ and $\\lambda_{1}~>~\\lambda_{2}~>~0$ , we infer that $\\begin{array}{r}{\\mathcal{R}(t)\\;=\\;\\frac{1}{2d}\\left(\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)\\;\\geq}\\end{array}$ $\\begin{array}{r}{\\frac{1}{2d}\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)\\,\\geq\\,0}\\end{array}$ . The structure of the exact line search algorithm ensures $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathcal{R}(t)\\,=\\,0,}\\end{array}$ hence $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathcal{D}_{\\lambda_{1}}(t)=0}\\end{array}$ . Similarly, we deduce $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathcal{D}_{\\lambda_{2}}(t)=0}\\end{array}$ . ", "page_idx": 42}, {"type": "text", "text": "By applying L\u2019H\u00f4pital\u2019s rule and substituting the expressions for $\\gamma_{t}^{\\mathrm{line}}$ and $\\mathcal{R}(t)$ in terms of $\\mathcal{D}_{\\lambda_{1}}(t)$ and $\\mathcal{D}_{\\lambda_{2}}(t)$ , we derive ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial_{\\lambda_{2}}(t)}{\\partial x_{1}(t)}=\\operatorname*{lim}_{k=0}\\frac{\\mathrm{d}\\tilde{\\mathcal{G}}_{\\lambda_{2}}(t)}{\\mathrm{d}y_{1}(t)}}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{-2\\gamma_{k}\\lambda_{2}}{-2\\gamma_{k}\\lambda_{1}}\\mathcal{G}_{\\lambda_{3}}(t)+d\\gamma_{k}^{2}\\lambda_{2}\\mathcal{H}(t)}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{-2\\lambda_{2}\\mathcal{G}_{\\lambda_{3}}(t)+d\\gamma_{k}\\lambda_{2}\\mathcal{H}(t)}{-2\\gamma_{k}\\lambda_{1}}}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{-2\\lambda_{2}\\mathcal{G}_{\\lambda_{3}}(t)+d\\gamma_{k}\\lambda_{2}\\mathcal{H}(t)}{-2\\gamma_{k}\\lambda_{1}}}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{\\gamma_{k}\\lambda_{2}}{\\gamma_{k}\\lambda_{2}}\\mathcal{G}_{\\lambda_{1}}(t)+\\lambda_{2}\\mathcal{G}_{\\lambda_{3}}(t)\\left(\\gamma_{k}\\frac{\\lambda_{2}}{\\lambda_{1}}-2\\right)}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{\\gamma_{k}\\lambda_{2}}{\\gamma_{k}\\lambda_{2}}\\mathcal{G}_{\\lambda_{3}}(t)+\\lambda_{3}\\mathcal{G}_{\\lambda_{1}}(t)\\left(\\gamma_{k}\\frac{\\lambda_{1}}{\\lambda_{1}}-2\\right)}\\\\ &{\\quad=\\operatorname*{lim}_{k=\\infty}\\frac{\\tilde{\\mathcal{D}}_{\\lambda_{1}}(t)^{2}\\lambda_{1}}{\\mathcal{D}_{\\lambda_{1}}(t)^{2}(-\\lambda_{1}\\lambda_{2})}+\\mathcal{D}_{\\lambda_{1}}(t)\\mathcal{G}_{\\lambda_{2}}(t)(-\\lambda_{1}\\lambda_{2}+\\lambda_{1}^{2}\\lambda_{2}^{2}-2\\lambda_{1}\\lambda_{2}^{3})+\\mathcal{D}_{\\lambda_{2}}(t)^{2}(-\\lambda_{2}^{4}-2\\lambda_{1}^{2}\\lambda_{2}^{2})}\\\\ &{\\\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, limt\u2192\u221eD\u03bb\u03bb2(t) is the positive real root of the second-degree polynomial ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{P}(x)=\\lambda_{1}\\lambda_{2}(x+1)(\\lambda_{2}x-\\lambda_{1})+(\\lambda_{2}-\\lambda_{1})^{3}x.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Solving for $x>0$ , we derive the explicit formula ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{\\mathcal{D}_{\\lambda_{2}}(t)}{\\mathcal{D}_{\\lambda_{1}}(t)}}\\\\ &{=\\frac{\\lambda_{1}^{3}-2\\lambda_{1}^{2}\\lambda_{2}+2\\lambda_{1}\\lambda_{2}^{2}-\\lambda_{2}^{3}+\\sqrt{\\lambda_{1}^{6}-4\\lambda_{1}^{5}\\lambda_{2}+8\\lambda_{1}^{4}\\lambda_{2}^{2}-6\\lambda_{1}^{3}\\lambda_{2}^{3}+8\\lambda_{1}^{2}\\lambda_{2}^{4}-4\\lambda_{1}\\lambda_{2}^{5}+\\lambda_{2}^{6}}{2\\lambda_{1}\\lambda_{2}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Given ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\gamma_{t}^{\\mathrm{line}}=\\frac{2\\left(\\lambda_{1}^{2}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}^{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)}{\\left(\\lambda_{1}\\mathcal{D}_{\\lambda_{1}}(t)+\\lambda_{2}\\mathcal{D}_{\\lambda_{2}}(t)\\right)(\\lambda_{1}^{2}+\\lambda_{2}^{2})}=\\frac{2\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\frac{\\mathcal{D}_{\\lambda_{2}}(t)}{\\mathcal{D}_{\\lambda_{1}}(t)}\\right)}{\\left(\\lambda_{1}+\\lambda_{2}\\frac{\\mathcal{D}_{\\lambda_{2}}(t)}{\\mathcal{D}_{\\lambda_{1}}(t)}\\right)(\\lambda_{1}^{2}+\\lambda_{2}^{2})},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\gamma_{t}^{\\mathrm{line}}=\\frac{2\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\operatorname*{lim}_{t\\to\\infty}\\frac{\\mathcal{D}_{\\lambda_{2}}(t)}{\\mathcal{D}_{\\lambda_{1}}(t)}\\right)}{\\left(\\lambda_{1}+\\lambda_{2}\\operatorname*{lim}_{t\\to\\infty}\\frac{\\mathcal{D}_{\\lambda_{2}}(t)}{\\mathcal{D}_{\\lambda_{1}}(t)}\\right)\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By substituting (98), we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}\\gamma_{t}^{\\mathrm{line}}}\\\\ &{=\\frac{\\lambda_{1}^{3}+2\\lambda_{1}^{2}\\lambda_{2}+2\\lambda_{1}\\lambda_{2}^{2}+\\lambda_{2}^{3}-\\sqrt{\\lambda_{1}^{6}-4\\lambda_{1}^{5}\\lambda_{2}+8\\lambda_{1}^{4}\\lambda_{2}^{2}-6\\lambda_{1}^{3}\\lambda_{2}^{3}+8\\lambda_{1}^{2}\\lambda_{2}^{4}-4\\lambda_{1}\\lambda_{2}^{5}+\\lambda_{2}^{6}}}{\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "A direct calculation reveals that \u03bb1 > \u03bb2 > 0 implies limt\u2192\u221e\u03b3tline $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\gamma_{t}^{\\mathrm{line}}\\le\\frac{2\\lambda_{\\mathrm{min}}(K)}{\\frac{1}{d}\\mathrm{Tr}(K^{2})}}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Remark F.1. For the scenario where $K$ has an arbitrary number n of distinct eigenvalues, equation (13) remains valid. The proof parallels the one outlined above. However, in this case, the expression for $\\scriptstyle\\operatorname*{lim}_{k\\to\\infty}{\\mathfrak{g}}_{k}$ is given by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}{\\mathfrak{g}}_{k}={\\frac{n\\left(\\lambda_{1}^{2}+\\lambda_{2}^{2}x_{1}+\\cdot\\cdot\\cdot+\\lambda_{n}^{2}x_{n-1}\\right)}{\\left(\\lambda_{1}+\\lambda_{2}x_{1}+\\cdot\\cdot\\cdot\\lambda_{n}x_{n-1}\\right)\\left(\\lambda_{1}^{2}+\\cdot\\cdot\\cdot+\\lambda_{n}^{2}\\right)}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $x_{1},\\ldots,x_{n-1}>0$ satisfy $a$ more intricate coupled system of $n-1$ equations. ", "page_idx": 42}, {"type": "text", "text": "G Examples ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Any single index model with $\\alpha$ -pseudo Lipschitz $(\\alpha\\leq1)$ ) activation function is covered by our $\\mathrm{SGD+AL}$ theory. In this section, we provide key learning problems within this family of models. ", "page_idx": 43}, {"type": "text", "text": "G.1 Binary logistic regression ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We consider a binary logistic regression problem with $\\epsilon=0$ where we are trying to classify two classes. We will follow a Student-Teacher model, in which there exists a true vector $X^{\\star}$ to be the true direction such that possible labels are, $\\begin{array}{r}{y=\\frac{\\exp(\\langle X^{\\star},a\\rangle)}{\\exp(\\langle X^{\\star},a\\rangle)+1}}\\end{array}$ . or $1-y$ . In order to classify the data we minimize the KL-divergence between the label $y$ and our estimate defined by the above formula, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathcal{R}(X)=\\mathbb{E}_{a}\\bigg[-\\langle X,a\\rangle\\cdot\\frac{\\exp(\\langle X^{\\star},a\\rangle)}{\\exp(\\langle X^{\\star},a\\rangle)+1}+\\log\\big(\\exp(\\langle X,a\\rangle)+1\\big)\\bigg].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "To study the ODE dynamics of SGD in Eq. (9) one needs the deterministic risk $h(B)$ , and ${\\cal I}(B)=$ $\\mathbb{E}_{a}[f^{\\prime}(\\langle X,a\\rangle,\\langle X^{\\star},\\overset{\\cdot}{a}\\rangle)^{2}]$ , with $B=W^{T}\\dot{K}W$ . Following the computation in Appendix $\\mathrm{D}$ example D.4 in [15] we obtain that ", "page_idx": 43}, {"type": "equation", "text": "$$\nh(B)=-B_{21}\\mathbb{E}_{z}\\bigg[\\frac{\\exp(\\sqrt{B_{22}}\\cdot z)}{(1+\\exp(\\sqrt{B_{22}}\\cdot z))^{2}}\\bigg]+\\mathbb{E}_{w}\\big[\\log(\\exp(w\\sqrt{B_{11}})+1)\\big],\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $z,w\\sim\\mathcal{N}(0,1)$ . The $I$ function can also be computed explicitly by solving the following Gaussian integral, where we define $\\begin{array}{r}{g(x)\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\exp(x)}{1+\\exp(y)}}}\\end{array}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\nI(B)=\\frac{1}{2\\pi\\sqrt{\\operatorname*{det}(B)}}\\int_{\\mathbb R^{2}}(g(x)-g(y))^{2}\\exp\\bigg(-\\frac{1}{2}\\binom{x}{y}^{T}B^{-1}\\binom{x}{y}\\bigg)\\ \\mathrm{d}x\\,\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We note that, the logistic regression is $(\\mu,\\theta)$ \u2013RSI with $\\begin{array}{r}{\\mu=\\frac{1}{\\ell e^{\\sqrt{4\\theta}}}}\\end{array}$ see section 2.2 in [15]. Its Lipschitz constant is $\\hat{L}(f)=1$ . Using Proposition D.1 one can derive a lower bound on the limiting learning of AdaGrad Norm. ", "page_idx": 43}, {"type": "text", "text": "For more details and more examples, see [15]. ", "page_idx": 43}, {"type": "text", "text": "G.2 CIFAR 5m ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Finally, we include an example that uses real-world data, that is, the CIFAR $5\\mathrm{m}$ dataset [38]. Our theory does not explicitly deal with non-Gaussian distributions, but we find that the theoretical risk curves generalize cleanly to that case. ", "page_idx": 43}, {"type": "text", "text": "As we are now working with discrete data points rather than a distribution, the learning setup, while closely analogous to what was presented earlier, has some slight differences. ", "page_idx": 43}, {"type": "text", "text": "We start with a subset of the data consisting of $n$ grayscale images, each of which is $32\\times32$ pixels, that is, $A\\in\\mathbb{R}^{n\\times1024}$ . We fill a vector $b\\in\\mathbb{R}^{n}$ with the corresponding labels (0 for an image of a plane, 1 for an image of a car.) We then randomly choose a matrix $W\\in\\breve{\\mathbb{R}}^{1024\\times\\dot{d}}$ with i.i.d. Gaussian entries to generate the features $F=\\operatorname{relu}(A W)$ . We want to use least squares to predict the label from the features, i.e., find ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{X\\in\\mathbb{R}^{d}}\\left\\{\\mathcal{R}(X):=\\frac{1}{2n}\\left\\|F X-b\\right\\|^{2}=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(f_{i}\\cdot X-b_{i}\\right)^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $f_{i}$ is the ith row of $F$ . The SGD we now consider is ", "page_idx": 43}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-\\gamma_{k}\\,{\\big(}f_{i_{k+1}}\\cdot X-b_{i_{k+1}}{\\big)}\\,f_{i_{k+1}},\\quad{\\big\\{}i_{k}{\\big\\}}{\\mathrm{~iid~}}\\operatorname{Unif}(\\{1,2,\\cdot\\cdot\\cdot\\cdot,n\\}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\gamma_{k}$ is the usual AdaGrad-Norm stepsize, as in (15). Our empirical covariance matrix $K$ (remembering that $f_{i}$ is a row vector) is then ", "page_idx": 43}, {"type": "equation", "text": "$$\nK=\\mathbb{E}_{\\mathbf{\\mu}_{i}\\in[n],j\\in[n]}\\left[f_{i}^{\\top}f_{j}\\right]=\\frac{1}{n}F^{\\top}F.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We now use (64), with the AdaGrad-Norm stepsize, to numerically simulate the SGD loss, which we then compare to the actual loss. Our theory matches empirical results very closely. ", "page_idx": 43}, {"type": "image", "img_path": "4VWnC5unAV/tmp/54cb6ce46fa3c6ec01262241b8a502a0724b106fa6908c3f75c50e2ebf8eb75c.jpg", "img_caption": ["Figure 6: Predicting the training dynamics on a real dataset, CIFAR- $.5\\mathrm{m}$ [38], using multi-pass AdaGrad-Norm. This suggests the theory extends beyond Gaussian data and one-pass. Note that the curves look significantly different for different $n$ ; smaller values of $n$ lead to an overparametrized problem, allowing least squares to memorize datapoints, whereas for larger $n$ , least squares must learn a general function mapping images of cars and airplanes to their respective labels. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "H Numerical simulation details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Here we provide more details for the figures that appear in the main paper. ", "page_idx": 44}, {"type": "text", "text": "Figure 1: Concentration learning rate and risk for AdaGrad-Norm on a least squares problem with label noise $\\omega=1$ (left) and on a logistic regression problem with no label noise (right). For logistic, see Section G. 30 runs of AdaGrad-Norm with parameters $b=1$ and $\\eta=1$ for each $d$ ; $X^{\\star}\\sim\\mathcal{N}(0,I_{d}/d)$ , $X_{0}=0$ , and $K=I_{d}$ . The shaded region represents a $90\\%$ confidence interval for the SGD runs. As the dimension increases, the risk and stepsize both concentrate around a deterministic limit (red). The deterministic limit is described by an ODE in Theorem 2.1. The initial loss increase in the least squares problem suggesting that the learning rate was initially too high, but AdaGrad-Norm naturally adapts and still the loss converges. Our ODEs predict this behavior. ", "page_idx": 44}, {"type": "text", "text": "Figure 2: Comparison for Exact Line Search and Polyak Stepsize on a noiseless least squares problem. The left plot illustrates the convergence of the risk function, while the right plot depicts the convergence of the quotient / 1\u03bbmTirn((KK2)) for Polyak stepsize and exact line search. Both ODE theory and SGD results are presented, showing a close agreement between the two approaches. The covariance matrix $K$ is generated such that the eigenvalues follow the expression $\\lambda_{i}(K)\\,=$ ${\\sqrt{\\frac{d}{\\sum_{i=1}^{d}\\bigl({\\frac{i}{d+1}}\\bigr)^{-2/s}}}}\\cdot\\,\\biggl({\\frac{i}{d{+}1}}\\biggr)^{-1/s}\\,,\\quad i\\,=\\,1,\\dots,d$ , where $s\\ >\\ 2$ is a constant. As $s$ approaches 2, the spectrum becomes more spread out, resulting in larger values of $\\textstyle{\\frac{1}{d}}\\operatorname{Tr}(K^{2})$ . Larger values of $s$ correspond to smaller spreads in the spectrum. Additionally, $\\mathrm{Tr}(K)\\stackrel{\\ast}{/}d=1$ for all $s$ . Both plots highlight the implication of equation (13) in high-dimensional settings, where a broader spectrum of $K$ results in $\\begin{array}{r}{\\bar{\\lambda}_{\\mathrm{min}}(K)\\ll\\frac{1}{\\frac{1}{d}\\mathrm{Tr}(K^{2})}}\\end{array}$ , indicating slower risk convergence and poorer performance of exact line search (unmarked) as it deviates from the Polyak stepsize (circle markers). The gray shaded region demonstrates that equation (13) is satisfied. ", "page_idx": 44}, {"type": "text", "text": "Figure 3: Quantities effecting AdaGrad-Norm learning rate. (left): The effect of adding noise to the targets $\\(\\omega=1.0)$ ) to the risk (left axis) and learning rate (right axis). Ran AdaGrad$\\mathrm{Norm}(b=1.0,\\eta=2.5)$ on least squares problem with $d=500$ . $X_{0}$ , $X^{\\star}\\sim\\mathcal{N}(0,I_{d}/d)$ . A single run of the SGD (solid line purple) matches exactly the prediction (ODE, teal). The shaded region represents 10 runs of SGD with $90\\%$ confidence interval. The learning rate decays at the exact predicted rate of $\\frac{\\eta}{\\sqrt{b^{2}+\\frac{\\operatorname{Tr}K\\omega^{2}}{d}t}}$ . Depicted is $\\frac{\\mathrm{learning~rate}}{\\mathrm{asymptotic}}$ so it approaches 1. (center, right): Noiseless least squares setting $\\omega=0$ ). (center): Prop. 4.2 predicts the avg. eig of $K$ $(\\operatorname{Tr}(K)/d)$ as compared with $\\lambda_{\\mathrm{max}}$ effects the $\\scriptstyle\\operatorname*{lim}_{k\\to\\infty}{\\mathfrak{g}}_{k}$ . Indeed, this is true. We varied the $\\kappa=\\lambda_{\\mathrm{max}}/\\lambda_{\\mathrm{min}}$ while keeping the $\\mathrm{Tr}(K)/d$ and all other parameters fixed. All the learning rates behave identically verifying our theory about the effect of $\\mathrm{Tr}(K)/d$ on learning rates. (right): Varying the learning rate of AdaGrad norm by $\\|X_{0}-X^{\\star}\\|^{2}$ ; our predictions (dashed) match and we see the inverse relationship predicted by Prop. 4.2. See Appendix D for details. Additionally, we did the following. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "\u2022 Center plot: AdaGrad with $b=0.5$ , $\\eta\\,=\\,2.5$ is run on the least squares problem with $d=1000$ and $\\begin{array}{r}{X_{0},X^{\\star}\\sim\\frac{1}{\\sqrt{d}}{\\mathcal N}(0,I)}\\end{array}$ . The covariance matrix $K$ is generated so that the eigenvalues are ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda_{i}(K)=\\sqrt{\\frac{d}{\\sum_{i=1}^{d}\\left(\\frac{i}{d+1}\\right)^{-2/s}}}\\cdot\\left(\\frac{i}{d+1}\\right)^{-1/s},\\quad i=1,\\dots,d.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The constant $s>2$ . When $s$ is near 2, the spectrum is more spread out, i.e., $\\begin{array}{r}{\\kappa=\\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}}}\\end{array}$ large. Larger values of $s$ mean smaller the spreads. Moreover $\\operatorname{Tr}(K)/d=1$ for all $s$ . In the simulations, we used $s\\in\\{2.1,3.0,3.5,4.0,5.5\\}$ and recorded the condition number $\\kappa$ . ", "page_idx": 45}, {"type": "text", "text": "\u2022 Right plot: Ran AdaGrad with $b=0.5$ , $\\eta=2.5$ on the least squares problem with $d=1000$ . $X^{\\star}=0$ and $X_{0}\\sim\\sqrt{\\textstyle{\\frac{p}{d}}}{\\mathcal{N}}(0,I)$ where $p\\in\\{1,2,4,8,16\\}$ . In this way, $\\|X_{0}-X^{\\star}\\|^{2}=p$ . ", "page_idx": 45}, {"type": "text", "text": "Figure 4: Power law covariance in AdaGrad Norm on a least squares problem. Generated covariance $K$ such that the density of eigenvalues are $(1-\\beta)\\lambda^{-\\beta}$ where $\\beta=0.2$ and set $X_{0}=0$ . Choose $(X_{i}^{\\star})_{i=1}^{d}=(\\lambda_{i}^{-\\delta/2})_{i=1}^{d}$ where $\\lambda_{i}$ is the $i$ -th eigenvalue of $K$ and we vary $\\delta\\in(0,1.8)$ so that $0<\\delta+\\beta\\leq2$ . Setting of Prop. 4.4. ", "page_idx": 45}, {"type": "text", "text": "Figure 5: Convergence in Exact Line Search on a noiseless least squares problem. The plot on the left illustrates the convergence of the risk function, while the center and right plots depict the convergence of the quotient $\\frac{\\mathbf{\\bar{\\mathcal{D}}}_{\\lambda_{2}}(t)}{\\mathbf{\\bar{\\mathcal{D}}}_{\\lambda_{1}}(t)}$ and the learning rate $\\gamma_{t}$ , respectively. Predictions from ODE theory are compared with results obtained from SGD, demonstrating close agreement between the two approaches. Initialization was performed randomly, with $X_{0}\\stackrel{!}{\\sim}\\mathcal{N}(0,\\bar{I_{d}}/d)$ and $\\begin{array}{r}{X^{\\star}\\sim\\frac{1}{\\sqrt{d}}{\\bf1}}\\end{array}$ , where $d=400$ . The covariance matrix $K$ has two distinct eigenvalues $\\lambda_{1}\\,=\\,1\\,>\\,\\lambda_{2}\\,>\\,0$ , and was constructed by specifying the spectrum, with $\\lambda_{i}$ sampled from a discrete uniform distribution $\\mathcal{U}\\{1,\\lambda_{2}\\}$ for $i=1,\\dots,d=400$ , and setting $K=\\operatorname{diag}(\\lambda_{i}:i=1,\\dots,400)$ . Further details and formulas for the limiting behavior can be found in the Appendix F.2. ", "page_idx": 45}, {"type": "text", "text": "Figure 6 Convergence on CIFAR 5m [38]. We train a classifier to distinguish between images of airplanes and cars. Fix $d=2000$ . Then for multiple values of $n$ , we run AdaGrad-Norm with initialization $X_{0}\\,=\\,0$ , $b=0.1$ and $\\eta=5$ , randomly sampling a datapoint from $F$ at every step. Details of the setup can be found in Appendix G.2. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The abstract and introduction introduce the class of problems we consider in this work. We do not exaggerate our claims and state precisely the limitations of our work, namely \u2019high-dimensional linear composite setting\u2019 with normally distributed data in Section 1.1. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We clearly state all the assumptions on the model and algorithm in Section 1.1. When the theory switches to the least squares problem, we indicate clearly in the theorem statements. For example, see AdaGrad (Section 4) and Line Search (Section 3). We are very careful in the statements of our theorems/propositions/lemmas/corollaries to include all the assumptions needed to prove the result. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper contains 2 sections detailing the exact set-up of the problem class (Section 1.1) as well as the algorithmic set-up (Section 1.2). All the proofs for the main results are provided in the appendix and we are careful to include all the necessary assumptions to prove these results. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Either in the captions or in Section H, we state all the parameters, covariances of the data used, initialization, etc that are needed to reproduce the results. We also intend to release the code and make it available via github. We also remark that the numerical simulations are relatively simple to reproduce as they are generated using synthetic data and the algorithms are already in use. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: In the captions of the figures and/or in Section H, we have provided all the details to reproduce the experiments. The data is synthetic. We also intend to release the code. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Either in the captions of the figures or in Section H, we state all the parameters, covariance of the data, initialization, etc that are needed to reproduce the results. We also intend to release the code and make it available via github. We remark that the numerical simulations are relatively simple to reproduce as they are generated using synthetic data and the algorithms are already in use. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: In the figures (especially Figure 1), we provided statistical error bars (shaded region). Part of our theory is that the training dynamics of SGD with adaptive learning rate in high-dimensions concentrate. We then predict these training dynamics exactly. In this sense, we have used statistical tests to illustrate our theory. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The computing resources are minimal in our case as the models are just least squares and logistic regression with synthetic data. We explicitly state all the relevant details for another person to run the experiments. We remark that due to the simplicity of our simulations, no experiment required extensive compute capabilities. ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have reviewed the code of ethics. We have made our utmost attempt to adhere to the guidelines provided by NeurIPS. We do not use any human subjects nor any datasets (privacy is not a concern). We did our best to cite all the relevent related work. Given that our work is in the foundational research and, in addition, the theory side, it is difficult to mitigate all the risks as the downstream effects of theory are long. The model is completely synthetic using the standard SGD algorithm; thus we don\u2019t, to the best of our knowledge, anticipate any risks. ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not anticipate any negative societal impact of the work. The work is a purely theoretical result on foundational research and it is not tied to any particular application. The work uses only synthetic data, standard algorithms (e.g., SGD, AdaGradNorm) that have already been used in ML/AI, and models such as least squares and logistics that are textbook. We have included a paragraph at the beginning of our appendix justifying this answer to the broader impacts of our work. ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper poses no such risks. The work is purely theoretical and uses only synthetic data generated from a normal distribution. The models employed are standard statistical model (e.g., least squares and logistic regression) which are textbook problems. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We acknowledge via citations that the algorithms we study have been introduced before by others. We are not using any datasets or other assets beyond numpy and thus do not need to name any license or cite any dataset. ", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We do not intend to release any new assets. Neither the model nor the algorithms are new. We are simply analyzing known algorithms and models in this paper. We do intend to release code for reproducibility. In particular, we want readers to be able to generate the deterministic ODEs. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects.   \nThe work is purely theoretical on a simple model. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The work does not involve crowdsourcing nor research with human subjects.   \nThe data used in this work is generated synthetically. ", "page_idx": 48}]