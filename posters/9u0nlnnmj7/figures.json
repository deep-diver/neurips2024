[{"figure_path": "9U0nLnNMJ7/figures/figures_0_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure illustrates the cost-effectiveness and performance gains achieved by using the MINITRON approach.  The x-axis represents the cost to train the model (in trillions of tokens), while the y-axis represents the MMLU (Massive Multitask Language Understanding) score, a metric measuring a model's performance across various language tasks. The figure shows that MINITRON models (green circles), derived from a larger pretrained model via pruning and knowledge distillation, significantly outperform comparable models trained from scratch (orange circles) while requiring far fewer training tokens (40x less in one instance). The dashed green line traces the compression path, starting with a larger model and ending with smaller MINITRON models, highlighting the efficiency of the method.", "section": "Experiments and Results"}, {"figure_path": "9U0nLnNMJ7/figures/figures_2_1.jpg", "caption": "Figure 2: High-level overview of our proposed iterative pruning and distillation approach to train a family of smaller LLMs. On a pretrained LLM, we first evaluate importance of neurons, rank them, trim the least important neurons and distill the knowledge from the original LLM to the pruned model. The original model is replaced with the distilled model for the next iteration of compression.", "description": "This figure illustrates the iterative process of pruning and knowledge distillation used to create a family of smaller LLMs from a larger, pre-trained model.  It shows the steps involved: 1. Starting with a trained LLM. 2. Estimating the importance of different components (neurons, heads, embeddings) within the model. 3. Ranking these components by importance. 4. Trimming (removing) the least important components. 5. Performing knowledge distillation to transfer knowledge from the original model to the pruned model. Steps 2-5 are repeated iteratively to progressively reduce the model size while retaining performance.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of our neural architecture search algorithm. We perform a search on multiple axes: number of layers, attention head count, MLP and embedding dimensions to arrive at a set of feasible architectures meeting the parameter budget. RT refers to retraining.", "description": "This figure illustrates the neural architecture search algorithm used to find optimal compressed architectures for LLMs.  The process starts with a defined search space encompassing various parameters (number of layers, attention heads, MLP expansion factor, and embedding dimensions).  The algorithm then enumerates all possible architectures within the specified parameter budget. These candidates undergo lightweight retraining, and their performance is evaluated. Finally, a best-performing architecture is selected and further refined through full retraining.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/figures/figures_4_2.jpg", "caption": "Figure 4: Overview of Distillation. A student model with N layers is distilled from a teacher model with M layers. The student learns by minimizing a combination of embedding output loss, logit loss and transformer encoder specific losses mapped across student block S and teacher block T.", "description": "This figure illustrates the knowledge distillation process used in the paper to retrain smaller language models (LLMs). It shows a teacher model (larger LLM) and a student model (smaller LLM). The student model learns to mimic the teacher model's output and intermediate states by minimizing various loss functions. These losses include the differences between the teacher and student's embeddings, MLP inputs, encoder block outputs, LM head outputs, and logits.", "section": "3 Retraining"}, {"figure_path": "9U0nLnNMJ7/figures/figures_7_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure compares the training cost and MMLU performance of MINITRON models (resulting models after compression) with other state-of-the-art language models. The x-axis represents the training cost measured in trillions of tokens. The y-axis shows the MMLU scores (%).  The figure highlights that MINITRON models achieve comparable or better performance than other models while requiring significantly less training data (up to 40 times less). For example, MINITRON 8B shows 9% better MMLU performance than Nemotron-4 15B while being 40 times cheaper to train.", "section": "Experiments and Results"}, {"figure_path": "9U0nLnNMJ7/figures/figures_15_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure demonstrates the cost-effectiveness of the MINITRON approach.  By pruning and retraining a large language model (15B parameters), the authors created smaller models (8B and 4B parameters). The figure highlights that training these smaller models using MINITRON requires significantly fewer training tokens (up to 40x less) compared to training them from scratch.  Despite this reduction in training cost, the smaller MINITRON models achieve comparable or even better performance on various benchmarks (as measured by MMLU scores) than similarly sized models trained from scratch.", "section": "Abstract"}, {"figure_path": "9U0nLnNMJ7/figures/figures_16_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure shows the results of the MINITRON model compression technique.  The x-axis represents the cost to train a model (in trillions of tokens), and the y-axis represents the MMLU score (%). The plot compares MINITRON models of different sizes (4B and 8B) to other state-of-the-art models such as Gemma 7B, Llama-3 8B, and Mistral 7B.  It demonstrates that MINITRON achieves comparable or even better results with significantly lower training costs, representing a 40x reduction in training cost. This highlights the efficiency of MINITRON in compressing large language models.", "section": "Introduction"}, {"figure_path": "9U0nLnNMJ7/figures/figures_18_1.jpg", "caption": "Figure 7: Accuracy on MMLU, HellaSwag and HumanEval benchmarks for iterative vs one-shot depth pruning and retraining strategy. One shot pruning and retraining outperforms the iterative approach.", "description": "This figure compares the performance of iterative and one-shot pruning and retraining strategies on three downstream tasks: MMLU, HellaSwag, and HumanEval.  The x-axis represents the number of layers remaining after pruning, while the y-axis shows the accuracy achieved on each task.  Multiple lines are presented showing the accuracy with different retraining token budgets, showing that the one-shot strategy generally outperforms the iterative strategy across all three tasks.", "section": "4.2 Obtaining the Best Pruned Model"}, {"figure_path": "9U0nLnNMJ7/figures/figures_19_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure demonstrates the cost-effectiveness of the MINITRON approach.  It compares the training cost (in trillions of tokens) to achieve various model sizes (Minitron models shown in different colors) versus training from scratch. The chart shows that MINITRON models (derived from a larger pretrained model through pruning and retraining) are significantly cheaper to train (40x less) than training from scratch, while simultaneously exhibiting improved performance (indicated by the higher MMLU scores).", "section": "Introduction"}, {"figure_path": "9U0nLnNMJ7/figures/figures_20_1.jpg", "caption": "Figure 1: Results for MINITRON. Compression results in significant reduction of training costs for additional models (40\u00d7) while producing better results.", "description": "This figure shows the results of applying the MINITRON compression technique to a family of LLMs. The x-axis represents the cost to train the models (in trillions of tokens), and the y-axis represents the MMLU score (a measure of the models' performance on various language modeling tasks). The figure demonstrates that compression significantly reduces the training cost (by a factor of 40x) while yielding comparable or even better results compared to training larger models from scratch.  The chart also compares MINITRON models with other models from the field.", "section": "Introduction"}]