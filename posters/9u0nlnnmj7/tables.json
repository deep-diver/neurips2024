[{"figure_path": "9U0nLnNMJ7/tables/tables_1_1.jpg", "caption": "Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using ~1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.", "description": "This table demonstrates the impact of different pruning strategies on a 15B parameter language model before and after retraining with a small amount of data (~1.8B tokens).  The model is pruned to 8B parameters, and the results compare the change in distillation loss (KL divergence) and LM validation loss. The results highlight that pruning width (attention, MLP, embedding layers) is more effective than pruning depth, but only after retraining.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/tables/tables_5_1.jpg", "caption": "Table 2: Performance of our pruned MINITRON 8B model compared to multiple baselines: the original Nemotron-4 15B, the previous generation Nemotron-3 8B, and multiple community models. MINITRON 8B uses 40x fewer tokens than Nemotron-3 8B. All evaluations run by us, except for entries marked with *, which we report from the corresponding papers.", "description": "This table compares the performance of the MINITRON 8B model against several baselines, including previous generations of the Nemotron model and other comparable models from the research community.  The key finding is that MINITRON 8B achieves comparable or better performance while using significantly fewer training tokens (40x less than Nemotron-3 8B).", "section": "4 Experiments and Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_5_2.jpg", "caption": "Table 3: Performance of MINITRON 4B model compared to similarly-sized community models. All evaluations run by us, except for entries marked with *, which we report from the corresponding papers. We only compare to base models without SFT and DPO, therefore Phi-3 is excluded.", "description": "This table compares the performance of the MINITRON 4B model against other models of similar size from the research community.  The metrics used are across various common benchmarks for evaluating large language models. The table highlights that MINITRON 4B, despite using significantly fewer training tokens (94B vs 1.1T-3T), performs comparably or even better than several other models on several tasks.", "section": "Experiments and Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_7_1.jpg", "caption": "Table 4: Performance of MINITRON models w.r.t recent state-of-the-art models obtained through depth/width pruning. Top and bottom halves show results for MINITRON 8B and 4B, respectively.", "description": "This table compares the performance of MINITRON 8B and 4B models against other state-of-the-art models that used depth or width pruning techniques.  It highlights MINITRON's competitive performance, particularly its superior results compared to models of similar size and the significant improvement in accuracy it achieves over those models.", "section": "4 Experiments and Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_7_2.jpg", "caption": "Table 5: Evaluation results on MT-Bench.", "description": "This table presents a comparison of the MINITRON 4B-instruct model's performance on the MT-Bench benchmark against several other models, including Phi-2, Qwen-1.5 Chat, Gemma-2B-IT, StableLM 2 Chat, and TinyLlama v1.0 Chat.  The comparison highlights MINITRON 4B-instruct's performance relative to other models of similar size, indicating its competitive performance across various instruction-following tasks.", "section": "4.1 Main Pruning Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_8_1.jpg", "caption": "Table 7: Evaluation results on ChatRAG-Bench.", "description": "This table presents the performance comparison of MINITRON 4B-instruct and Gemma-2B-IT models on the ChatRAG-Bench benchmark.  The average score across all tasks for MINITRON 4B-instruct is 41.11, which is higher than that of Gemma-2B-IT (33.31). This demonstrates that MINITRON 4B-instruct, created using pruning and knowledge distillation techniques, achieves better performance on instruction-following and role-playing tasks compared to Gemma-2B-IT.", "section": "4.1 Main Pruning Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_8_2.jpg", "caption": "Table 8: Evaluation results on BFCL v2.", "description": "This table presents the average performance of MINITRON 4B-instruct and other comparable models on the Berkeley Function Calling Leaderboard (BFCL v2).  MINITRON 4B-instruct demonstrates superior performance compared to Gemma-2B-IT and Llama-3-8B-instruct.", "section": "4.1 Main Pruning Results"}, {"figure_path": "9U0nLnNMJ7/tables/tables_9_1.jpg", "caption": "Table 9: MINITRON 8B and 4B search space.", "description": "This table specifies the search space for the hyperparameters of MINITRON 8B and 4B models.  The search space includes the number of layers, the number of attention heads, the MLP expansion factor, and the embedding dimension.  Each hyperparameter has a range of possible values, indicating the different model configurations explored during the neural architecture search process.", "section": "2.3 Obtaining a Pruned Model"}, {"figure_path": "9U0nLnNMJ7/tables/tables_15_1.jpg", "caption": "Table 10: Architecture details of the uncompressed Nemotron and pruned MINITRON models. Vocabulary size is 256k for all models.", "description": "This table presents the architectural specifications of four language models: Nemotron-4 15B, Nemotron-3 8B, MINITRON 8B, and MINITRON 4B.  It details the number of layers, hidden size, attention heads, query groups, MLP hidden size, and total number of parameters for each model.  The table highlights the architectural differences between the original Nemotron models and their compressed MINITRON counterparts, showcasing the reduction in parameters achieved through pruning.", "section": "A.1 Pruned Architecture Details"}, {"figure_path": "9U0nLnNMJ7/tables/tables_15_2.jpg", "caption": "Table 11: Zero-shot performance of activation-based importance with different batch and sequence aggregation metrics. LM loss is reported on the validation set of the 8T and WikiText2 datasets.", "description": "This table presents the results of an experiment evaluating different aggregation functions for computing activation-based importance scores for structured pruning of LLMs.  The experiment compares various combinations of batch and sequence aggregation methods (mean, L2 norm, variance) and their impact on the language modeling (LM) loss for two datasets: 8T and WikiText2.  The results are shown as zero-shot LM loss (before retraining). This helps determine the best strategy for calculating importance scores during pruning, as different approaches can impact the overall model performance significantly.", "section": "2.2 Importance Analysis"}, {"figure_path": "9U0nLnNMJ7/tables/tables_15_3.jpg", "caption": "Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using ~1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.", "description": "This table demonstrates the performance of different pruning strategies on a language model before and after retraining.  It shows the change in distillation loss and the LM validation loss after applying various pruning methods (depth, width, attention, MLP, and embedding). The results highlight that width pruning is superior to depth pruning, but only after a lightweight retraining process.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/tables/tables_16_1.jpg", "caption": "Table 13: Comparison of retraining LM loss across different pruning strategies post retraining with 1.8B tokens. We explore depth only, width only, and a combination of both. Width only strategy though with the least parameter count outperforms the rest.", "description": "This table compares the Language Model (LM) validation loss after retraining with 1.8 billion tokens for four different pruning strategies applied to the MINITRON 8B model.  The strategies are: pruning only depth using perplexity (PPL) as the metric, pruning only depth using Block Importance (BI) as the metric, pruning only width (attention, MLP, and embedding dimensions), and combining both depth and width pruning.  The table shows that although the combined depth and width pruning results in a smaller model, the width-only pruning strategy achieves the lowest LM validation loss after retraining.", "section": "4.3 Retraining and Search"}, {"figure_path": "9U0nLnNMJ7/tables/tables_16_2.jpg", "caption": "Table 14: Accuracy comparison across different strategies to train a 4B model. Pruning the 15B model and distillation results in a gain of 4.8% on Hellaswag and 13.5% on MMLU compared to training from scratch with equivalent compute. Pruning an 8B model instead of a 15B model results in an additional gain of 1% and 4.6% on the benchmarks. * Indicates settings with iso-compute.", "description": "This table compares the performance of training a 4B model using different methods: random initialization, pruning a 15B model and retraining, and pruning a 15B or 8B model with knowledge distillation. It demonstrates the effectiveness of knowledge distillation for improving the accuracy of pruned models and shows the advantage of pruning a smaller model rather than a large one.", "section": "4.3 Retraining and Search"}, {"figure_path": "9U0nLnNMJ7/tables/tables_17_1.jpg", "caption": "Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using ~1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.", "description": "This table demonstrates the performance of different pruning strategies on a 15B parameter language model before and after retraining with a small amount of data (1.8B tokens).  It compares the impact of pruning different aspects of the model (depth, width of attention, MLP, and embedding layers) on the distillation loss (KL divergence) and final language modeling validation loss. The results show that width pruning generally outperforms depth pruning, but only after the retraining step.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/tables/tables_17_2.jpg", "caption": "Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using ~1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.", "description": "This table compares different pruning strategies (depth, width) applied to the Nemotron-4 15B model before and after retraining. The results show that width pruning generally outperforms depth pruning, but only when combined with lightweight retraining (using around 1.8 billion tokens). The table highlights the change in distillation loss and the final language model validation loss after retraining.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/tables/tables_17_3.jpg", "caption": "Table 1: Demonstration of how various pruning strategies perform before and after lightweight retraining using ~1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model.", "description": "This table demonstrates the performance of different pruning strategies (depth, width, attention, and MLP) on the Nemotron-4 15B language model before and after lightweight retraining with approximately 1.8 billion tokens.  It compares the change in distillation loss (KL divergence) and final language modeling validation loss for each pruning strategy.  The results show that width pruning generally outperforms depth pruning, especially after retraining.", "section": "2 Pruning Methodology"}, {"figure_path": "9U0nLnNMJ7/tables/tables_18_1.jpg", "caption": "Table 18: Ablation study for MINITRON 8B with and without the loss component  Lis , and increased retraining token count with  Llogits . Adding  Lis  performs on par with using  Llogits  alone.", "description": "This table presents the ablation study results for MINITRON 8B model, comparing the performance using different loss functions: Llogits + Lis and Llogits, with varying training token counts. It demonstrates that adding Lis to the Llogits loss function does not significantly improve performance compared to using Llogits alone, even with increased training tokens.", "section": "4.3 Retraining and Search"}, {"figure_path": "9U0nLnNMJ7/tables/tables_19_1.jpg", "caption": "Table 9: MINITRON 8B and 4B search space.", "description": "This table presents the search space used for finding optimal architecture configurations for MINITRON 8B and 4B models. It shows the range of values considered for the number of layers, attention heads, MLP expansion factor, and embedding dimensions.", "section": "2.3 Obtaining a Pruned Model"}, {"figure_path": "9U0nLnNMJ7/tables/tables_19_2.jpg", "caption": "Table 2: Performance of our pruned MINITRON 8B model compared to multiple baselines: the original Nemotron-4 15B, the previous generation Nemotron-3 8B, and multiple community models. MINITRON 8B uses 40x fewer tokens than Nemotron-3 8B. All evaluations run by us, except for entries marked with *, which we report from the corresponding papers.", "description": "This table compares the performance of the MINITRON 8B model against several baselines, including the original Nemotron-4 15B, Nemotron-3 8B, and various other community models.  Key metrics across multiple benchmarks (Knowledge, Logic, and Coding) are presented, highlighting MINITRON 8B's performance despite using significantly fewer training tokens (40x fewer than Nemotron-3 8B).  The asterisk (*) indicates results taken from other published papers.", "section": "4 Experiments and Results"}]