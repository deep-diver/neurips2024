[{"figure_path": "oYyEsVz6DX/figures/figures_1_1.jpg", "caption": "Fig. 1: Definition of the Machine Interpretability Score. A. We build on top of the established task definition for quantifying the per-unit interpretability via human psychophysics experiments [6]. The task measures how well participants understand the sensitivity of a unit by asking them to match strongly activating query images to strongly activating visual explanations of the unit. Red and blue squares illustrate the unit's minimally and maximally activating images; shaded and solid squares denote natural test images and explanations, respectively. See Fig. 9 for examples. B. Crucially, we remove the need for humans and fully automate the evaluation: We pass the explanations and query images through a feature encoder to compute pair-wise image similarities (DreamSim) before using a (hard-coded) binary classifier to solve the underlying task. Finally, the Machine Interpretability Score (MIS) is the average of the predicted probability of the correct choice over N tasks for the same unit. C. The MIS proves to be highly correlated with human interpretability ratings and allows fast evaluations of new hypotheses.", "description": "This figure illustrates the method used to create a machine interpretability score (MIS).  Panel A shows the established human psychophysics task to measure per-unit interpretability in a neural network. The task requires participants to match query images (strongly activating the unit) with visual explanations (also strongly activating the unit). Panel B describes the automated version replacing humans, using a feature encoder (DreamSim) and a classifier to predict the probability of a correct match across multiple tasks. The MIS is the average predicted probability. Panel C demonstrates that the automated MIS strongly correlates with human interpretability ratings.", "section": "3 Method"}, {"figure_path": "oYyEsVz6DX/figures/figures_4_1.jpg", "caption": "Fig. 2: Validation of the MIS. Our proposed Machine Interpretability Score (MIS) explains existing interpretability annotations (Human Interpretability Score, HIS) from IMI [50] well. (A) MIS Explains Interpretability Model Rankings. The MIS reproduces the ranking of models presented in IMI while being fully automated and not requiring any human labor, as evident by the strong correlation between MIS and HIS. Similar results are found for the interpretability afforded by another explanation method in Appx. E. (B) MIS Explains Per-unit Interpretability Annotations. The MIS also explains individual per-unit interpretability annotations. We show the calculated MIS and the recorded HIS for every unit in IMI and find a high correlation matching the noise ceiling at p = 0.80 (see Appx. C). (C) MIS Allows Detection of (Non-)Interpretable Units. We use the MIS to perform a causal intervention and determine the least (hardest) and most (easiest) interpretable units in a GoogLeNet and ResNet-50. Using the psychophysics setup of Zimmermann et al. [50], we measure their interpretability and compare them to randomly sampled units. Strikingly, the psychophysics results match the predicted properties: Units with the lowest MIS have significantly lower interpretability than random units, which have significantly lower interpretability than those with the highest MIS. Errorbars denote the 95% confidence interval.", "description": "This figure validates the Machine Interpretability Score (MIS) by comparing it to existing human interpretability scores (HIS).  Panel A shows that MIS accurately reproduces the model ranking from the IMI dataset, demonstrating its ability to predict model-level interpretability without human evaluation. Panel B shows a strong correlation between MIS and HIS at the unit level, indicating that MIS effectively predicts unit-level interpretability as well.  Panel C further validates MIS by showing that units selected as 'hardest' (lowest MIS) and 'easiest' (highest MIS) by MIS indeed demonstrate significantly lower and higher interpretability, respectively, in a human psychophysics study compared to random units.", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_5_1.jpg", "caption": "Fig. 3: Comparison of the Average Per-unit MIS for Models. We substantially extend the analysis of Zimmermann et al. [50] from a noisy average over a few units for a few models to all units of 835 models. The models are compared regarding their average per-unit interpretability (as judged by MIS); the shaded area depicts the 5th to 95th percentile over units. We see that all models fall into an intermediate performance regime, with stronger changes in interpretability at the tails of the model ranking. Models probed by Zimmermann et al. [50] are highlighted in red.", "description": "This figure displays the average per-unit machine interpretability score (MIS) for 835 different vision models.  The models are ranked by their average MIS. The shaded region shows the 5th to 95th percentile range of MIS values across all units within each model, highlighting the variability in interpretability among units within a single model. The figure demonstrates a substantial increase in the scale of the interpretability analysis compared to previous work, showing the average interpretability for a substantially larger number of models and units.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_6_1.jpg", "caption": "Fig. 4: (A) Relation Between ImageNet Accuracy and MIS. The average per-unit MIS of a model is anticorrelated with its ImageNet classification accuracy. Refer to Tab. 2 for a list of the Pareto-optimal models. (B) Distribution of per-unit MIS. Distribution of the per-unit MIS for 15 models, chosen based on the size of the error bar in Fig. 3: lowest (top row), medium (middle row), and highest variability (bottom row). While most models have an unimodal distribution, those with high variability have a second mode with lower MIS.", "description": "This figure shows the relationship between ImageNet accuracy and the average per-unit Machine Interpretability Score (MIS) across 835 models. Panel A displays a scatter plot showing a negative correlation between ImageNet top-1 accuracy and the average per-unit MIS.  Panel B presents kernel density estimations of the per-unit MIS distribution for 15 models selected to represent the range of variability observed in the data. Models with low variability exhibit a unimodal distribution, while models with high variability display a bimodal distribution, indicating the presence of both highly and lowly interpretable units within those models.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_6_2.jpg", "caption": "Fig. 3: Comparison of the Average Per-unit MIS for Models. We substantially extend the analysis of Zimmermann et al. [50] from a noisy average over a few units for a few models to all units of 835 models. The models are compared regarding their average per-unit interpretability (as judged by MIS); the shaded area depicts the 5th to 95th percentile over units. We see that all models fall into an intermediate performance regime, with stronger changes in interpretability at the tails of the model ranking. Models probed by Zimmermann et al. [50] are highlighted in red.", "description": "This figure compares the average per-unit Machine Interpretability Score (MIS) across 835 different computer vision models.  The x-axis represents the models sorted by their average MIS, and the y-axis shows the average per-unit MIS. The shaded area represents the 5th to 95th percentile range of MIS values across all units within each model. The authors highlight models previously analyzed in Zimmermann et al. [50] in red. The figure demonstrates a substantial extension of prior work, evaluating per-unit interpretability on a larger scale.  It shows that the models cluster within a medium range of interpretability, with more variation at the extremes of model ranking. This suggests that model architecture and training significantly impact interpretability.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_7_1.jpg", "caption": "Fig. 6: (A) Deeper Layers are More Interpretable. Average MIS per layer as a function of the relative depth of the layer within the network, grouped by layer types. For each type, the values are grouped into 30 bins of equal count based on the relative depth. The crosses depict the bin averages (correlations are calculated for those, too); for a visualization including the bins' variance see Fig. 23. (B) Wider Layers are More Interpretable. Average MIS per layer as a function of their relative width, grouped by layer types. The values are grouped into 5 bins. See Fig. 24 for visualizations of how the median, 5th, or 95th percentile of MIS depend on the layer width.", "description": "This figure analyzes the relationship between a layer's position and width within a neural network and its interpretability, as measured by the Machine Interpretability Score (MIS).  Panel (A) shows that deeper layers tend to be more interpretable, exhibiting an almost sinusoidal relationship with relative depth. Panel (B) indicates a positive correlation between a layer's width and its interpretability, although this effect is weaker for batch normalization layers.", "section": "4.2 Comparison of Layers"}, {"figure_path": "oYyEsVz6DX/figures/figures_8_1.jpg", "caption": "Figure 7: Interpretability During Training. For a ResNet-50 trained for 100 epochs on ImageNet, we track the MIS and accuracy after every epoch (epoch 0 refers to initialization). While the MIS improves drastically in the first epoch, it decays during the rest of the training (left). This results in an antiproportional relation between MIS and accuracy (right).", "description": "This figure shows the training dynamics of a ResNet-50 model on ImageNet.  The left panel plots the Machine Interpretability Score (MIS) against training epoch, illustrating a sharp increase in MIS during the first epoch, followed by a gradual decline throughout the remaining training process. The right panel depicts the relationship between MIS and ImageNet Top-1 accuracy, revealing an inverse correlation between the two metrics after the initial epoch. This suggests a trade-off between model interpretability and performance during training.", "section": "4.3 How Does the MIS Change During Training?"}, {"figure_path": "oYyEsVz6DX/figures/figures_8_2.jpg", "caption": "Fig. 8: Change of Interpretability per Layer During Training. To better understand the peak in interpretability after the first training epoch found in Fig. 7, we display the change in MIS during the first epoch, averaged over each layer. Layers are sorted by depth from left to right, and different colors encode different layer types. The change in interpretability appears moderately correlated with a layer's depth, such that deeper layers improve the strongest, whereas early layers show no improvement. For an extended visualization covering the full training, see Fig. 20.", "description": "This figure shows how the average per-unit Machine Interpretability Score (MIS) changes for each layer during the first training epoch.  The change in MIS is plotted against the layer's relative depth within the network (from early layers to late layers). Different colors represent different layer types.  The results reveal a moderate correlation between the change in interpretability and layer depth, with deeper layers showing greater improvement than earlier layers. A more detailed visualization of the change in MIS throughout the entire training process is available in Figure 20.", "section": "4.3 How Does the MIS Change During Training?"}, {"figure_path": "oYyEsVz6DX/figures/figures_14_1.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the 2-Alternative Forced Choice (2-AFC) task used in the paper.  The task is designed to measure how well a participant understands the sensitivity of a unit in a neural network. Each task consists of:\n\n1.  **Negative Explanations:** Images that minimally activate the unit.\n2.  **Positive Explanations:** Images that maximally activate the unit.\n3.  **Queries:** Two query images, one of which strongly activates the unit and one which weakly activates the unit.\n\nThe participant's job is to select which of the query images corresponds to the positive explanations, demonstrating their understanding of the unit's activation patterns.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_15_1.jpg", "caption": "Fig. 1: Definition of the Machine Interpretability Score. A. We build on top of the established task definition for quantifying the per-unit interpretability via human psychophysics experiments [6]. The task measures how well participants understand the sensitivity of a unit by asking them to match strongly activating query images to strongly activating visual explanations of the unit. Red and blue squares illustrate the unit's minimally and maximally activating images; shaded and solid squares denote natural test images and explanations, respectively. See Fig. 9 for examples. B. Crucially, we remove the need for humans and fully automate the evaluation: We pass the explanations and query images through a feature encoder to compute pair-wise image similarities (DreamSim) before using a (hard-coded) binary classifier to solve the underlying task. Finally, the Machine Interpretability Score (MIS) is the average of the predicted probability of the correct choice over N tasks for the same unit. C. The MIS proves to be highly correlated with human interpretability ratings and allows fast evaluations of new hypotheses.", "description": "This figure describes the Machine Interpretability Score (MIS) which is a fully automated method to quantify per-unit interpretability. Panel A shows the established task definition for human psychophysics experiments to measure per-unit interpretability.  Panel B shows how the proposed method automates this evaluation. Instead of humans, it uses a feature encoder and a binary classifier.  The MIS is the average of the predicted probability of a correct choice over several tasks. Panel C illustrates that the MIS correlates well with human interpretability ratings.", "section": "1 Introduction"}, {"figure_path": "oYyEsVz6DX/figures/figures_16_1.jpg", "caption": "Fig. 2: Validation of the MIS. Our proposed Machine Interpretability Score (MIS) explains existing interpretability annotations (Human Interpretability Score, HIS) from IMI [50] well. (A) MIS Explains Interpretability Model Rankings. The MIS reproduces the ranking of models presented in IMI while being fully automated and not requiring any human labor, as evident by the strong correlation between MIS and HIS. Similar results are found for the interpretability afforded by another explanation method in Appx. E. (B) MIS Explains Per-unit Interpretability Annotations. The MIS also explains individual per-unit interpretability annotations. We show the calculated MIS and the recorded HIS for every unit in IMI and find a high correlation matching the noise ceiling at p = 0.80 (see Appx. C). (C) MIS Allows Detection of (Non-) Interpretable Units. We use the MIS to perform a causal intervention and determine the least (hardest) and most (easiest) interpretable units in a GoogLeNet and ResNet-50. Using the psychophysics setup of Zimmermann et al. [50], we measure their interpretability and compare them to randomly sampled units. Strikingly, the psychophysics results match the predicted properties: Units with the lowest MIS have significantly lower interpretability than random units, which have significantly lower interpretability than those with the highest MIS. Errorbars denote the 95% confidence interval.", "description": "This figure validates the Machine Interpretability Score (MIS) by comparing it to existing Human Interpretability Scores (HIS).  Panel A shows that the MIS accurately reproduces the model ranking from a previous study, demonstrating its ability to predict model-level interpretability without human input. Panel B shows a strong correlation between MIS and HIS at the unit level, indicating that the MIS can also accurately predict the interpretability of individual units. Finally, Panel C demonstrates the predictive power of the MIS by showing that units with high MIS scores are significantly more interpretable than randomly selected units, while units with low MIS scores are significantly less interpretable, thereby demonstrating the method's predictive power in a causal intervention.", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_17_1.jpg", "caption": "Fig. 12: Best Perceptual Similarity Approaches Noise Ceiling. Considering the noise ceiling, caused by the inherent uncertainty of the HIS, the best perceptual similarity (DreamSim) shows an almost perfect performance. The black bar and shaded area show the mean correlation and standard deviation over 1000 simulations, respectively.", "description": "This figure compares three different perceptual similarity measures (LPIPS, DISTS, and DreamSim) used in calculating the Machine Interpretability Score (MIS).  It shows that DreamSim outperforms the others in terms of correlation with Human Interpretability Score (HIS), even when considering the noise ceiling (the theoretical maximum correlation due to limitations in human annotation). The noise ceiling is represented by the dashed horizontal line, and the bars show the average correlation with HIS along with the standard deviation over 1000 simulations.", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_17_2.jpg", "caption": "Fig. 11: LPIPS and DISTS Perform Similarly as DreamSim when Comparing Models. We compare DreamSim with two earlier perceptual similarity metrics, LPIPS and DISTS. All three lead to similar results on IMI (cf. Fig. 2A). See Fig. 13 for comparing these similarity functions on a per-unit level.", "description": "This figure compares the performance of three different perceptual similarity metrics (DreamSim, LPIPS, and DISTS) when used to compute the Machine Interpretability Score (MIS).  The metrics are evaluated based on how well the resulting MIS correlates with human interpretability scores from the IMI dataset.  The results show that DreamSim and LPIPS achieve similar overall results when comparing models, but DreamSim performs better at a finer-grained, per-unit level, as shown in a subsequent figure (Figure 13).", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_17_3.jpg", "caption": "Fig. 14: Convergence of MIS. We investigate how MIS changes depending on the number of tasks N that it is computed over. Here, we distinguish between two settings. In (a), we simulate that adding another task does not change the selection of query images and explanations in earlier tasks; in (b), this is not the case. While the former is easier to analyze due to a reduced level of randomness, note that the latter is the more relevant setting in practice. For both cases, we visualize the average absolute difference in MIS estimated for < 20 and N = 20 tasks.", "description": "This figure analyzes the impact of the number of tasks (N) used to calculate the Machine Interpretability Score (MIS) on its stability. Two scenarios are compared: one where adding tasks does not affect previously selected image-explanation pairs, and a more realistic scenario where adding new tasks influences the selection of all pairs. The plots show the average absolute difference in MIS when using fewer than 20 tasks compared to using all 20 tasks. The results indicate a convergence towards zero as the number of tasks increases, demonstrating the stability and reliability of the MIS estimation with sufficiently many tasks.", "section": "4 Results"}, {"figure_path": "oYyEsVz6DX/figures/figures_17_4.jpg", "caption": "Fig. 14: Convergence of MIS. We investigate how MIS changes depending on the number of tasks N that it is computed over. Here, we distinguish between two settings. In (a), we simulate that adding another task does not change the selection of query images and explanations in earlier tasks; in (b), this is not the case. While the former is easier to analyze due to a reduced level of randomness, note that the latter is the more relevant setting in practice. For both cases, we visualize the average absolute difference in MIS estimated for <20 and N = 20 tasks.", "description": "This figure shows how the Machine Interpretability Score (MIS) changes depending on the number of tasks used to compute it. Two scenarios are considered: one where adding a task doesn't affect previous tasks, and another where it does.  The graph plots the average absolute difference in MIS between using fewer tasks (1 to 19) and using the full 20 tasks.  The results show that the MIS converges to a stable value as more tasks are used, with convergence being slower in the more realistic scenario (where new tasks influence the selection of previous tasks).", "section": "4 Results"}, {"figure_path": "oYyEsVz6DX/figures/figures_18_1.jpg", "caption": "Fig. 2: Validation of the MIS. Our proposed Machine Interpretability Score (MIS) explains existing interpretability annotations (Human Interpretability Score, HIS) from IMI [50] well. (A) MIS Explains Interpretability Model Rankings. The MIS reproduces the ranking of models presented in IMI while being fully automated and not requiring any human labor, as evident by the strong correlation between MIS and HIS. Similar results are found for the interpretability afforded by another explanation method in Appx. E. (B) MIS Explains Per-unit Interpretability Annotations. The MIS also explains individual per-unit interpretability annotations. We show the calculated MIS and the recorded HIS for every unit in IMI and find a high correlation matching the noise ceiling at p = 0.80 (see Appx. C). (C) MIS Allows Detection of (Non-)Interpretable Units. We use the MIS to perform a causal intervention and determine the least (hardest) and most (easiest) interpretable units in a GoogLeNet and ResNet-50. Using the psychophysics setup of Zimmermann et al. [50], we measure their interpretability and compare them to randomly sampled units. Strikingly, the psychophysics results match the predicted properties: Units with the lowest MIS have significantly lower interpretability than random units, which have significantly lower interpretability than those with the highest MIS. Errorbars denote the 95% confidence interval.", "description": "This figure validates the Machine Interpretability Score (MIS) by comparing it to existing human interpretability annotations (HIS).  Panel A shows that MIS accurately reproduces model rankings from the IMI dataset, demonstrating its ability to predict model-level interpretability without human evaluation. Panel B demonstrates that MIS also predicts per-unit interpretability, showing a strong correlation with HIS. Panel C further validates MIS by conducting a causal intervention study, where the easiest and hardest interpretable units identified by MIS are confirmed through a psychophysics experiment, highlighting the measure's predictive power.", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_18_2.jpg", "caption": "Fig. 16: Ratio of Constant Units. We compute the ratio of units constant with respect to the input (over the training set of ImageNet-2012) for all models considered. While the ratio is low for most models, it becomes large for a few models.", "description": "This figure shows the ratio of constant units for each of the 835 models investigated in the paper. A unit is considered constant if the difference between its maximum and minimum activation across all images in the ImageNet-2012 training set is less than 10\u207b\u2078.  The x-axis represents the models sorted by the ratio of constant units, and the y-axis shows that ratio. The majority of models have a low ratio of constant units, but some models exhibit a significantly higher ratio.  This indicates the prevalence of inactive or constant units in a subset of the analyzed models.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_20_1.jpg", "caption": "Fig. 17: Comparable MIS for different SAE architectures. We compare two types of SAEs used by Bricken et al. [7] and Rajamanoharan et al. [39] (SAE and Gated SAE, respectively), in terms of their sparsity, reconstruction fidelity and interpretability. While Gated SAEs allow a more optimal tradeoff between fidelity and sparsity, they are comparably interpretable as standard SAEs. The SAEs overall MIS is in a similar regime as the original layer\u2019s (91.21%), while the sparsity is stronger than that of the original layer (\u2248 75).", "description": "This figure compares the performance of two different types of sparse autoencoders (SAEs) in terms of their sparsity, reconstruction fidelity, and interpretability, using the Machine Interpretability Score (MIS) as a metric.  It shows that while Gated SAEs offer a better balance between sparsity and reconstruction fidelity, both types achieve a similar level of interpretability as measured by MIS. Notably, the SAEs' interpretability remains comparable to the original layer\u2019s interpretability, despite exhibiting higher sparsity.", "section": "I Analyzing SAEs"}, {"figure_path": "oYyEsVz6DX/figures/figures_24_1.jpg", "caption": "Fig. 18: Differences Between Layer Types are Significant. We analyze and test for statistical significances in the differences in MIS between different layer types (see Fig. 5. The reported significance levels were computed using Conover's test over the per-model and per-layer-type means with Holm's correction for multiple comparisons.", "description": "This figure shows the statistical significance of the differences between different layer types' MIS.  The results are from a Conover's test that compares per-model and per-layer-type MIS means, with Holm's correction applied for multiple comparisons. The heatmap displays the significance levels (p-values) for each pairwise comparison between layer types. Darker colors indicate stronger statistical significance.", "section": "4.2 Comparison of Layers"}, {"figure_path": "oYyEsVz6DX/figures/figures_24_2.jpg", "caption": "Fig. 4: (A) Relation Between ImageNet Accuracy and MIS. The average per-unit MIS of a model is anticorrelated with its ImageNet classification accuracy. Refer to Tab. 2 for a list of the Pareto-optimal models. (B) Distribution of per-unit MIS. Distribution of the per-unit MIS for 15 models, chosen based on the size of the error bar in Fig. 3: lowest (top row), medium (middle row), and highest variability (bottom row). While most models have an unimodal distribution, those with high variability have a second mode with lower MIS.", "description": "This figure shows the relationship between ImageNet accuracy and the Machine Interpretability Score (MIS).  Panel A demonstrates a negative correlation: higher accuracy models tend to have lower average per-unit MIS. Panel B displays the distribution of per-unit MIS across 15 models, categorized by variability.  Models with low variability show unimodal distributions, whereas those with high variability exhibit bimodal distributions, indicating a subset of less interpretable units.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_24_3.jpg", "caption": "Fig. 20: Change of Interpretability per Layer During Training. Detailed version of Fig. 8.", "description": "This figure provides a more detailed visualization of the change in the machine interpretability score (MIS) during the training process of a ResNet-50 model on ImageNet-2012.  It expands on Figure 8 by showing the change in MIS for each layer across all training epochs.  The graph displays how the interpretability of different layer types (BatchNorm and Conv) changes over time, providing a more nuanced understanding of the learning dynamics and feature evolution within the network.  This detailed layer-by-layer breakdown gives insight into how different layer types contribute to overall model interpretability during training.", "section": "4.3 How Does the MIS Change During Training?"}, {"figure_path": "oYyEsVz6DX/figures/figures_25_1.jpg", "caption": "Fig. 3: Comparison of the Average Per-unit MIS for Models. We substantially extend the analysis of Zimmermann et al. [50] from a noisy average over a few units for a few models to all units of 835 models. The models are compared regarding their average per-unit interpretability (as judged by MIS); the shaded area depicts the 5th to 95th percentile over units. We see that all models fall into an intermediate performance regime, with stronger changes in interpretability at the tails of the model ranking. Models probed by Zimmermann et al. [50] are highlighted in red.", "description": "This figure shows the average per-unit machine interpretability score (MIS) for 835 different models.  The x-axis represents the models sorted by their average MIS. The y-axis shows the average per-unit MIS. The shaded area indicates the 5th to 95th percentile range of MIS across units within each model.  The figure highlights that while there is a range in average interpretability across models, most models fall within a similar range.  The models tested in previous work by Zimmermann et al. [50] are marked in red for comparison.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_25_2.jpg", "caption": "Fig. 3: Comparison of the Average Per-unit MIS for Models. We substantially extend the analysis of Zimmermann et al. [50] from a noisy average over a few units for a few models to all units of 835 models. The models are compared regarding their average per-unit interpretability (as judged by MIS); the shaded area depicts the 5th to 95th percentile over units. We see that all models fall into an intermediate performance regime, with stronger changes in interpretability at the tails of the model ranking. Models probed by Zimmermann et al. [50] are highlighted in red.", "description": "This figure shows the average per-unit Machine Interpretability Score (MIS) for 835 different models.  The models are ranked by their average MIS, which is a measure of how easily humans can understand the units (e.g. neurons or channels) in a vision model. The shaded region shows the 5th to 95th percentile of MIS across all units within each model, illustrating the variability in interpretability among units within the same model. The figure demonstrates that even though model rankings by interpretability correlates well with previous results using much smaller datasets, it also presents novel information on the wide variability of units' interpretability within a model.", "section": "4.2 Analyzing & Comparing Hundreds of Models"}, {"figure_path": "oYyEsVz6DX/figures/figures_25_3.jpg", "caption": "Fig. 6: (A) Deeper Layers are More Interpretable. Average MIS per layer as a function of the relative depth of the layer within the network, grouped by layer types. For each type, the values are grouped into 30 bins of equal count based on the relative depth. The crosses depict the bin averages (correlations are calculated for those, too); for a visualization including the bins' variance see Fig. 23. (B) Wider Layers are More Interpretable. Average MIS per layer as a function of their relative width, grouped by layer types. The values are grouped into 5 bins. See Fig. 24 for visualizations of how the median, 5th, or 95th percentile of MIS depend on the layer width.", "description": "This figure shows the relationship between the average per-unit machine interpretability score (MIS) and layer depth and width.  Panel A displays the average MIS for different layer types (convolutional, linear, batchnorm, layernorm, groupnorm) as a function of relative layer depth.  A sinusoidal pattern is observed across layer types, with an initial increase, a dip in the middle, and a final drop towards the end of the network.  Panel B presents a similar analysis but focuses on layer width and shows a relatively consistent and slight increase in MIS across different layer types with an increase in relative layer width.  This visualization indicates that deeper and wider layers tend to have higher interpretability scores.", "section": "4.2 Comparison of Layers"}, {"figure_path": "oYyEsVz6DX/figures/figures_26_1.jpg", "caption": "Fig. 24: Wider Layers are More Interpretable. Average MIS per layer as a function of the relative depth of the layer within the network, grouped by layer types. For each type, the values are grouped into 30 bins of equal count based on the relative depth. The crosses depict the bin averages (correlations are calculated for those, too); for a visualization including the bins' variance see Fig. 23. (B) Wider Layers are More Interpretable. Average MIS per layer as a function of their relative width, grouped by layer types. The values are grouped into 5 bins. See Fig. 24 for visualizations of how the median, 5th, or 95th percentile of MIS depend on the layer width.", "description": "This figure shows the relationship between the average per-unit machine interpretability score (MIS) and the relative layer depth/width for different layer types.  Panel (A) demonstrates that deeper layers generally have a higher average MIS. Panel (B) shows that wider layers also tend to have higher average MIS. This suggests a correlation between layer depth/width and model interpretability.", "section": "4.2.2 Comparison of Layers"}, {"figure_path": "oYyEsVz6DX/figures/figures_26_2.jpg", "caption": "Fig. 25: How do Dataset Exemplars for Units with Strong MIS Drop Change? To gain a better understanding of why the MIS of a ResNet50 drops during training after the first epoch, we display the least/most activating dataset exemplars of four units from the model after the first (left) and after the last (right) epoch. While the explanations after the first epoch seem to focus on easy-to-grasp visual features, the units on the right react to less clear-cut concepts. The units are among the units with the strongest MIS drop in the convolutional layers with the strongest MIS drop.", "description": "This figure shows the least and most activating dataset examples for four units of a ResNet50 model.  The left column displays examples from after the first training epoch, and the right column displays examples from after the last epoch.  The caption highlights that while the units initially respond strongly to easily understandable visual features (e.g., color), later in training they respond to more complex and less interpretable features, resulting in a decrease in their Machine Interpretability Score (MIS).  The units shown are those that experience the most significant drop in MIS during training.", "section": "4.3 How Does the MIS Change During Training?"}, {"figure_path": "oYyEsVz6DX/figures/figures_27_1.jpg", "caption": "Fig. 26: Visualization of Units for which MIS overestimates HIS. To showcase the shortcomings of the MIS, we visualize four units for which the MIS predicts an interpretability that is higher than the measured HIS in Fig. 2B. See Fig. 27 for the opposite direction. For each unit, we show the 20 most (right) and 20 least (left) activating dataset exemplars.", "description": "This figure shows four examples of units where the Machine Interpretability Score (MIS) overestimates the Human Interpretability Score (HIS).  Each unit's 20 most and 20 least activating images (visual explanations) are displayed. The goal is to illustrate instances where the automated MIS metric doesn't perfectly align with human perception of interpretability.", "section": "4.1 Validating the Machine Interpretability Score"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_1.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the 2-AFC (two-alternative forced choice) task used to evaluate the interpretability of units in a deep neural network. Each example shows two sets of images (positive and negative explanations) that represent the patterns a unit responds to strongly and weakly, respectively. Two additional query images are presented, and the task is to determine which query image matches the positive explanations better.  This task assesses human understanding of a unit's sensitivity by requiring participants to match strongly activating images to the strongly activating visual explanations.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_2.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the two-alternative forced-choice (2-AFC) task used in the paper to measure per-unit interpretability.  Each example displays sets of positive and negative visual explanations (images that strongly activate or deactivate the unit in question) alongside two query images. The task is to determine which query image best matches the positive explanations. This task is used to evaluate how well participants (either humans or a machine in the automated version) understand the sensitivity of a unit by assessing their ability to match strongly activating query images with strongly activating visual explanations.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_3.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the 2-AFC (two-alternative forced choice) task used to measure per-unit interpretability. Each task consists of three parts: (1) Negative explanations (minimally activating images for the unit), (2) Positive explanations (maximally activating images), and (3) Two query images (test images). The participant is asked to choose which query image better matches the positive explanations.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_4.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the two-alternative forced-choice (2-AFC) task used in the paper to evaluate the interpretability of units in a neural network.  Each task presents participants with two query images (center) and two sets of visual explanations: one set showing images that strongly activate the unit (positive explanations, right), and one set showing images that minimally activate the unit (negative explanations, left). Participants must choose which of the two query images better matches the positive explanations.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_5.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the 2-alternative forced choice (2-AFC) task used to evaluate per-unit interpretability.  Each example shows two sets of images: positive explanations (maximally activating images for a specific unit in a neural network) and negative explanations (minimally activating images).  In the center are two query images, one which strongly and one which weakly activates the unit. The task is for the participant (or the machine in the automated version) to determine which query image is the positive example (i.e., matches the positive explanations).", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_6.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the two-alternative forced-choice (2-AFC) task used in the paper to evaluate unit interpretability.  Each example shows sets of positive and negative visual explanations (images) for a specific unit in a GoogLeNet model. The positive explanations are images that maximally activate the unit, while the negative explanations are images that minimally activate it. Two query images (test images) are presented, and the task is for the participant (human or machine) to select the query image that best matches the positive explanations based on their perceived similarity.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_7.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the two-alternative forced-choice (2-AFC) task used to evaluate the per-unit interpretability of units in deep neural networks.  In each example, there are sets of \"positive\" and \"negative\" visual explanations (images representing strongly and weakly activating inputs to the unit, respectively) presented to the user alongside two query images. The task is to select the query image that best corresponds to the positive explanations. The figure highlights the challenge in determining which of the query images is the correct answer for the human participants in the original study, and how this task is automated using machine learning methods in the paper.", "section": "A Description of the 2-AFC Task"}, {"figure_path": "oYyEsVz6DX/figures/figures_28_8.jpg", "caption": "Fig. 9: Examples of the 2-AFC Task. For two different units of GoogLeNet one task each is shown. Every task contains a set of negative (left) and positive (right) visual explanations describing which visual feature the unit is sensitive to. In the center, two query images in the form of strongly and weakly activating dataset examples are shown, respectively. This means that each one of the two query images corresponds to the positive and the other to the negative explanations. The task is now to choose which query image corresponds to the positive ones.", "description": "This figure shows two examples of the 2-AFC (two-alternative forced-choice) task used to evaluate the per-unit interpretability of units in a convolutional neural network (CNN).  The task presents participants with two query images and two sets of visual explanations (positive and negative). The positive examples illustrate the patterns that maximally activate the unit. Conversely, the negative examples show patterns that minimally activate the unit. The goal is for participants to determine which query image corresponds to the positive explanations (i.e., which image elicits higher unit activation).  The figure highlights the importance of using both query images and explanations to assess a unit's sensitivity.", "section": "A Description of the 2-AFC Task"}]