[{"figure_path": "aGqldlOxxY/figures/figures_1_1.jpg", "caption": "Figure 1: UnSAM significantly surpasses the performance of the previous SOTA methods in unsupervised segmentation, and delivers impressive whole image and promptable segmentation results, rivaling the performance of the supervised SAM [21]. This comparative analysis features our unsupervised UnSAM, the supervised SAM, and an enhanced version, UnSAM+, across a variety of datasets. The top section displays raw images (row 1) alongside whole image segmentation outputs from UnSAM (row 3), and SAM (row 2). The bottom section highlights our promptable segmentation results using a point prompt (i.e., the star mark). The right panel quantitatively compares the performance across models, including metrics like Mask AR (%) and Point IoU.", "description": "This figure compares the performance of UnSAM (unsupervised), SAM (supervised), and UnSAM+ (lightly semi-supervised) on several datasets.  It shows examples of whole-image segmentations and segmentations based on a point prompt, and provides quantitative results using Mask AR and Point IoU.", "section": "1 Introduction"}, {"figure_path": "aGqldlOxxY/figures/figures_3_1.jpg", "caption": "Figure 2: Our divide-and-conquer pipeline for generating the \"ground-truth\" pseudo masks used for training UnSAM without human supervision begins with a top-down clustering approach (i.e., the divide stage), to extract initial semantic/instance-level masks using a Normalized Cuts [31]-based CutLER [39]. Subsequently, we refine these masks using a bottom-up clustering method (i.e., the conquer stage): within each mask, we iteratively merge semantically similar pixels into larger segments using various similarity thresholds. The resulting masks at different thresholds create a hierarchy. We zoom-in selected regions to visualize details.", "description": "This figure illustrates the two-stage pipeline used in UnSAM to generate pseudo masks for training. The first stage (divide) uses top-down clustering to extract initial semantic/instance-level masks.  The second stage (conquer) uses iterative bottom-up clustering to refine these masks, creating a hierarchy of masks with varying levels of granularity. This hierarchical structure represents the hierarchical structure of visual scenes.", "section": "4.1 Divide-and-Conquer for Hierarchical Image Segmentation"}, {"figure_path": "aGqldlOxxY/figures/figures_6_1.jpg", "caption": "Figure 3: Unsupervised pseudo-masks generated by our divide-and-conquer pipeline not only contain precise masks for coarse-grained instances (column 5), e.g., cameras and persons, but also capture fine-grained parts (column 3), e.g., digits and icons on a tiny camera monitor that are missed by SA-1B\u2019s [21] ground-truth labels.", "description": "This figure shows a comparison of the ground truth masks from the SA-1B dataset and the unsupervised pseudo-masks generated by the UnSAM model. The figure highlights that UnSAM is able to generate masks not only for coarse-grained objects, but also for fine-grained details that are often missed in the human-annotated ground truth masks. This demonstrates the ability of UnSAM to capture a wider range of granularity levels compared to previous methods.", "section": "4 UnSAM: Segment Anything without Supervision"}, {"figure_path": "aGqldlOxxY/figures/figures_7_1.jpg", "caption": "Figure A1: More visualizations on COCO [24]. From top to bottom are raw images, segmentation by SAM, segmentation by UnSAM, and segmentation by UnSAM+.", "description": "This figure shows a comparison of image segmentations produced by three different models: SAM (supervised), UnSAM (unsupervised), and UnSAM+ (lightly supervised) on the COCO dataset.  The top row displays a set of raw images from the COCO dataset. The second row displays the segmentation masks generated by the SAM model. The third row displays the segmentation masks generated by the UnSAM model. The bottom row displays the segmentation masks generated by the UnSAM+ model.  The goal is to visually demonstrate the differences in segmentation quality and detail between the three models. The color variations within each segmentation likely represent different identified objects or regions within the image.", "section": "A.5 More Visualizations"}, {"figure_path": "aGqldlOxxY/figures/figures_8_1.jpg", "caption": "Figure 5: UnSAM not only discovers more fine-grained masks than the previous state-of-the-art unsupervised segmentation method [6], but also provides segmentation masks with a wide range of granularity. We show qualitative comparisons between UnSAM (with 3 levels of granularity) and baseline models on SA-1B [21].", "description": "This figure compares the performance of UnSAM against previous state-of-the-art unsupervised segmentation methods.  It showcases UnSAM's ability to generate more fine-grained masks and a wider range of granularity levels compared to existing techniques, using examples from the SA-1B dataset.", "section": "5.3 Evaluation Results"}, {"figure_path": "aGqldlOxxY/figures/figures_8_2.jpg", "caption": "Figure 6: Qualitative comparisons of promptable image segmentation between the fully-supervised SAM [21], our unsupervised UnSAM, and the lightly semi-supervised UnSAM+. Both UnSAM and UnSAM+ consistently deliver high-quality, multi-granular segmentation masks in response to the point prompts (i.e., the star mark).", "description": "This figure displays a qualitative comparison of the promptable image segmentation results obtained using three different models: the fully-supervised SAM, the unsupervised UnSAM, and the lightly semi-supervised UnSAM+.  The comparison highlights the ability of UnSAM and UnSAM+ to generate high-quality, multi-granular segmentation masks from point prompts, which are represented by star marks in the images. The results showcase the performance of the proposed unsupervised and semi-supervised methods in comparison to the fully-supervised state-of-the-art.", "section": "4.3 UnSAM+: Improving Supervised SAM with Unsupervised Segmentation"}, {"figure_path": "aGqldlOxxY/figures/figures_9_1.jpg", "caption": "Figure 7: More visualizations on SA-1B [21]. From top to bottom are raw images, segmentation by SAM, segmentation by UnSAM, and segmentation by UnSAM+.", "description": "This figure displays a comparison of image segmentations across three different models: SAM (supervised), UnSAM (unsupervised), and UnSAM+ (lightly supervised).  For several images from the SA-1B dataset, it shows the original image alongside the segmentations produced by each model. This visual comparison highlights the differences in segmentation quality and granularity between the supervised and unsupervised approaches, showcasing UnSAM's ability to produce segmentations comparable to SAM, especially when augmented with a small amount of supervised data as in UnSAM+.", "section": "6 Summary"}, {"figure_path": "aGqldlOxxY/figures/figures_15_1.jpg", "caption": "Figure 1: UnSAM significantly surpasses the performance of the previous SOTA methods in unsupervised segmentation, and delivers impressive whole image and promptable segmentation results, rivaling the performance of the supervised SAM [21]. This comparative analysis features our unsupervised UnSAM, the supervised SAM, and an enhanced version, UnSAM+, across a variety of datasets. The top section displays raw images (row 1) alongside whole image segmentation outputs from UnSAM (row 3), and SAM (row 2). The bottom section highlights our promptable segmentation results using a point prompt (i.e., the star mark). The right panel quantitatively compares the performance across models, including metrics like Mask AR (%) and Point IoU.", "description": "This figure compares the performance of UnSAM (unsupervised), SAM (supervised), and UnSAM+ (lightly supervised) on several datasets.  The top half shows example images and their segmentations using SAM and UnSAM, illustrating UnSAM's ability to produce comparable results without human supervision. The bottom half presents quantitative results, showing that UnSAM significantly outperforms previous state-of-the-art unsupervised methods and that UnSAM+ even surpasses SAM's performance.", "section": "1 Introduction"}, {"figure_path": "aGqldlOxxY/figures/figures_16_1.jpg", "caption": "Figure A1: More visualizations on COCO [24]. From top to bottom are raw images, segmentation by SAM, segmentation by UnSAM, and segmentation by UnSAM+.", "description": "This figure provides more visual comparisons of the results from the SAM, UnSAM, and UnSAM+ models on the COCO dataset.  Each row shows four images, following the pattern of raw image, SAM segmentation, UnSAM segmentation, and UnSAM+ segmentation. The color-coded segmentations allow for a visual comparison of the models' performance in segmenting various objects in different images.", "section": "A.5 More Visualizations"}, {"figure_path": "aGqldlOxxY/figures/figures_17_1.jpg", "caption": "Figure A3: Failure cases of UnSAM. From left to right are raw images, segmentation by SAM, and segmentation by UnSAM.", "description": "This figure showcases instances where the unsupervised model, UnSAM, struggles to achieve accurate segmentation compared to the supervised model SAM. The examples highlight the challenges posed by dense fine-grained details and instances with similar textures, where UnSAM tends to miss objects or over-segment the scene. It illustrates limitations associated with unsupervised approaches in handling complex visual data compared to supervised models with access to labelled data.", "section": "A.6 Limitations"}]