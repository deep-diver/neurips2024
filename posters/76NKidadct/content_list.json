[{"type": "text", "text": "Improved Particle Approximation Error for Mean Field Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Atsushi Nitanda ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "CFAR and IHPC, Agency for Science, Technology and Research $\\operatorname{A\\!\\star\\!ST\\!A\\!R)}$ ), Singapore College of Computing and Data Science, Nanyang Technological University, Singapore atsushi_nitanda@cfar.a-star.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finiteparticle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constantfree particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we consider the following entropy-regularized mean-field optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mu)=F(\\mu)+\\lambda\\mathrm{Ent}(\\mu),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $F:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ is a convex functional on the space of probability distributions $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\begin{array}{r}{\\mathrm{Ent}(\\mu)=\\int\\mu(\\mathrm{d}x)\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}x}(x)}\\end{array}$ is a negative entropy. Especially we focus on the learning problem of mean-field neural networks, that is, $F(\\mu)$ is a risk of (infinitely wide) two-layer neural networks $\\mathbb{E}_{X\\sim\\mu}[h(X,\\cdot)]$ where $h(X,\\cdot)$ represents a single neuron with parameter $X$ . One advantage of this problem is that the convexity of $F$ with respect to $\\mu$ can be leveraged to analyze gradient-based methods for a finite-size two-layer neural network: $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}h(x^{i},\\bar{\\cdot})\\;(x^{i}\\in\\mathbb{R}^{d})}\\end{array}$ . This is achieved by translating the optimization dynamics of the finite-dimensional parameters $(x^{1},\\ldots,x^{N})\\in\\mathbb{R}^{d N}$ into the dynamics of $\\mu$ via the mean-field limit: $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{x^{i}}\\rightarrow\\mu}\\end{array}$ $\\left[N\\rightarrow\\infty\\right]$ ). This connection was pointed out by Nitanda and Suzuki (2017); Mei et al. (2018); Chizat and Bach (2018); Rotskoff and Vanden-Eijnden (2022); Sirignano and Spiliopoulos (2020b,a) in the case of $\\lambda=0$ , and used for showing the global convergence of the gradient flow for (1) by Mei et al. (2018); Chizat and Bach (2018). ", "page_idx": 0}, {"type": "text", "text": "One may consider adding Gaussian noise to the gradient descent to make the method more stable. Then, we arrive at the following mean-field Langevin dynamics (MFLD) (Hu et al., 2019; Mei et al., 2018) as a continuous-time representation under $N=\\infty$ of this noisy gradient descent. ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-\\nabla\\frac{\\delta F(\\mu_{t})}{\\delta\\mu}(X_{t})\\mathrm{d}t+\\sqrt{2\\lambda}\\mathrm{d}W_{t},\\;\\;\\;\\mu_{t}=\\mathrm{Law}(X_{t}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\{W_{t}\\}_{t\\ge0}$ is the $d$ -dimensional standard Brownian motion and $\\nabla\\frac{\\delta F(\\mu)}{\\delta\\mu}$ is the Wasserstein gradient that is the gradient of the first-variation \u03b4F\u03b4 (\u00b5\u00b5) of F. Even though several optimization methods (Nitanda et al., 2021; Oko et al., 2022; Chen et al., 2023) that can efficiently solve the above problem with polynomial computational complexity have been proposed, MFLD remains an interesting research subject because of the above connection to the noisy gradient descent. In fact, recent studies showed that MFLD globally converges to the optimal solution (Hu et al., 2019; Mei et al., 2018) thanks to noise perturbation and that its convergence rate is exponential in continuoustime under uniform log-Sobolev inequality (Nitanda et al., 2022; Chizat, 2022). ", "page_idx": 1}, {"type": "text", "text": "However, despite such remarkable progress, the above studies basically assume the mean-field limit: $N=\\infty$ . To analyze an implementable MFLD, we have to deal with discrete-time and finite-particle dynamics, i.e., noisy gradient descent: ", "page_idx": 1}, {"type": "equation", "text": "$$\nX_{k+1}^{i}=X_{k}^{i}-\\eta\\nabla{\\frac{\\delta F(\\mu\\mathbf{x}_{k})}{\\delta\\mu}}(X_{k}^{i})+\\sqrt{2\\lambda\\eta}\\xi_{k}^{i},\\;\\;\\;(i\\in\\{1,\\dots,N\\}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\xi_{k}^{i}\\,\\sim{\\mathcal{N}}(0,I_{d})\\,\\left(i\\,\\in\\,\\{1,\\dots,d\\}\\right)$ are i.i.d. standard normal random variables and $\\mu_{\\mathbf{X}_{k}}\\;=\\;$ $\\begin{array}{r}{{\\frac{1}{N}}\\sum_{i=1}^{N}\\delta_{X_{k}^{i}}}\\end{array}$ is an empirical measure. On the one hand, the convergence in the discrete-time setting has been proved by Nitanda et al. (2022) using the one-step interpolation argument for Langevin dynamcis (Vempala and Wibisono, 2019). On the other hand, approximation error induced by using finite-particle system $\\mathbf{X}_{k}\\,=\\,(X_{k}^{1},\\ldots,X_{k}^{N})$ has been studied in the literature of propagation of chaos (Sznitman, 1991). As for MFLD, Mei et al. (2018) suggested exponential blow-up of particle approximation error in time, but recent works (Chen et al., 2022; Suzuki et al., 2023a) proved uniformin-time propagation of chaos, saying that the gap between $N$ -particle system and its mean-field limit shrinks uniformly in time as $N\\rightarrow\\infty$ . Afterward, Suzuki et al. (2023b) established truly quantitative convergence guarantees for (3) by integrating the techniques developed in Nitanda et al. (2022); Chen et al. (2022). Furthermore, Kook et al. (2024) proved the sampling guarantee for the mean-field stationary distribution: $\\begin{array}{r}{\\mu_{*}=\\arg\\operatorname*{min}_{\\mathcal{P}_{2}(\\mathbb{R}^{d})}\\mathcal{L}(\\mu)}\\end{array}$ , building upon the uniform-in-time propagation of chaos. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we further improve the particle approximation error (Chen et al., 2022; Suzuki et al., 2023b) by alleviating the dependence on logarithmic Sobolev inequality (LSI) constants in their bounds. This improvement could exponentially reduce the required number of particles because LSI constant $\\alpha$ could exponentially deteriorate with the regularization coefficient, i.e., $\\alpha\\gtrsim\\exp(-\\Theta(1/\\lambda))$ . Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. Additionally, as the application, we demonstrate improved (i) convergence of MFLD, (ii) sampling guarantee for the mean-field stationary distribution $\\mu_{*}$ , and (iii) uniform-in-time Wasserstein propagation of chaos in terms of particle complexity. We summarize our contributions below. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate the particle approximation error $\\textstyle O\\big(\\frac{1}{N}\\big)$ (Theorem 1) regarding the objective gap. A significant difference from the existing approximation error $\\textstyle O({\\frac{\\lambda}{\\alpha N}})$ (Chen et al., 2022; Suzuki et al., 2023b) is that our bound is free from the LSI-constant. Therefore, the approximation error uniformly decreases as $N\\rightarrow\\infty$ regardless of the value of LSI-constant as well as $\\lambda$ . \u2022 As applications of Theorem 1, we derive the convergence rates of the finite-particle MFLDs (Theorem 2), sampling guarantee for $\\mu_{*}$ (Corollary 1), and uniform-in-time Wasserstein propagation of chaos (Corollary 2) with the approximation errors inherited from Theorem 1, which improve upon existing errors (Chen et al., 2022; Suzuki et al., 2023b; Kook et al., 2024). ", "page_idx": 1}, {"type": "text", "text": "Here, we mention the proof strategy of Theorem 1. Langevin dynamics (LD) is a special case of MFLD corresponding to the case where $F$ is a linear functional. It is well known that even with a single particle, we can simulate LD and the particle converges to the target Gibbs distribution. This means that the particle approximation error is due to the non-linearity of $F$ . Therefore, in our analysis, we carefully treat the non-linearity of $F$ and obtain an expression for the particle approximation error using the Bregman divergence induced by $F$ . Finally, we relate this divergence to the variance of an $N$ -particle neural network and show the error of $O(1/N)$ . This proof strategy is quite different from existing ones and is simple. Moreover, it leads to an improved approximation error as mentioned above. We refer the readers to Section 4 for details about the proof. ", "page_idx": 1}, {"type": "text", "text": "1.2 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote vectors and random variables on $\\mathbb{R}^{d}$ by lowercase and uppercase letters such as $x$ and $X$ , respectively, and boldface is used for $N$ -pairs of them like $\\mathbf{x}\\,\\,\\dot{=}\\,\\,(x^{1},\\ldots,x^{N})\\,\\in\\,\\mathbb{R}^{N d}$ and $\\mathbf{X}\\,=\\,(X^{1},\\ldots,X^{N})$ . $\\Vert\\cdot\\Vert_{2}$ denotes the Euclidean norm. Let $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be the set of probability distributions with finite second moment on $\\mathbb{R}^{d}$ . For probability distributions $\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , we define Kullback-Leibler (KL) divergence (a.k.a. relative entropy) by $\\begin{array}{r}{\\mathrm{KL}(\\mu\\lVert\\boldsymbol{\\nu})\\stackrel{\\mathrm{def}}{=}\\int\\mathrm{d}\\mu(\\boldsymbol{x})\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\nu}(\\boldsymbol{x})}\\end{array}$ . Ent denotes the negative entropy: $\\begin{array}{r}{\\mathrm{Ent}(\\mu)\\,=\\,\\int\\mu(\\mathrm{d}x)\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}x}(x)}\\end{array}$ . We denote by $\\textstyle{\\frac{\\mathrm{d}\\mu}{\\mathrm{d}x}}$ the density function of $\\mu$ with respect to the Lebesgue measure if it exists. We denote $\\langle f,m\\rangle=\\overbar{J}\\ f(x)m(\\mathrm{d}x)$ for a (singed) measure $m$ and integrable function $f$ on $\\mathbb{R}^{d}$ . Given $\\mathbf{x}=(x^{1},\\ldots,x^{N})\\in\\mathbb{R}^{N d}$ , we write an empirical measure supported on $\\mathbf{x}$ as $\\begin{array}{r}{\\mu_{\\mathbf{x}}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{x^{i}}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we explain a problem setting and give a brief overview of the recent progress of the mean-field Langevin dynamics. ", "page_idx": 2}, {"type": "text", "text": "2.1 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We say the functional $G:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ is differentiable when there exists a functional (referred to as a first variation): $\\begin{array}{r}{\\frac{\\delta G}{\\delta\\mu}:\\;\\mathcal{P}_{2}(\\mathbb{R}^{d})\\,\\times\\,\\mathbb{R}^{d}\\ni(\\mu,x)\\mapsto\\frac{\\delta G(\\mu)}{\\delta\\mu}(x)\\in\\mathbb{R}}\\end{array}$ such that for $\\forall\\mu,\\mu^{\\prime}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}G(\\mu+\\epsilon(\\mu^{\\prime}-\\mu))}{\\mathrm{d}\\epsilon}\\bigg\\vert_{\\epsilon=0}=\\int\\frac{\\delta G(\\mu)}{\\delta\\mu}(x)(\\mu^{\\prime}-\\mu)(\\mathrm{d}x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and say $G$ is convex when for $\\forall\\mu,\\mu^{\\prime}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nG(\\mu^{\\prime})\\geq G(\\mu)+\\int\\frac{\\delta G(\\mu)}{\\delta\\mu}(x)(\\mu^{\\prime}-\\mu)(\\mathrm{d}x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For a differentiable and convex functional $F_{0}:\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}$ and coefficients $\\lambda$ , $\\lambda^{\\prime}>0$ we consider the minimization of an entropy-regularized convex functional (Mei et al., 2018; Hu et al., 2019; Nitanda et al., 2022; Chizat, 2022; Chen et al., 2022; Suzuki et al., 2023b; Kook et al., 2024): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})}\\left\\{\\mathcal{L}(\\mu)=F_{0}(\\mu)+\\lambda^{\\prime}\\mathbb{E}_{X\\sim\\mu}[\\|X\\|_{2}^{2}]+\\lambda\\mathrm{Ent}(\\mu)\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We set $F(\\mu)=F_{0}(\\mu)+\\lambda^{\\prime}\\mathbb{E}_{\\mu}[\\|X\\|_{2}^{2}]$ . Note both $F$ and $\\mathcal{L}$ are differentiable convex functionals. In particular, we focus on the empirical risk $F_{0}$ of the mean-field neural networks, i.e., two-layer neural networks in the mean-field regime. The definition of this model is given in Section 3. Throughout the paper, we assume the existence of the solution $\\mu_{\\ast}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ of the problem (5) and make the following regularity assumption on the objective function, which is inherited from Chizat (2022); Nitanda et al. (2022); Chen et al. (2023). ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. There exists $M_{1},M_{2}>0$ such that for any $\\begin{array}{r}{\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\,x\\in\\mathbb{R}^{d},\\,\\left|\\nabla\\frac{\\delta F_{0}(\\mu)}{\\delta\\mu}(x)\\right|\\leq}\\end{array}$ $M_{1}$ and for any $\\mu,\\mu^{\\prime}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d}),\\,x,x^{\\prime}\\in\\mathbb{R}^{d}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bigg\\|\\nabla\\frac{\\delta F_{0}(\\mu)}{\\delta\\mu}(x)-\\nabla\\frac{\\delta F_{0}(\\mu^{\\prime})}{\\delta\\mu}(x^{\\prime})\\bigg\\|_{2}\\leq M_{2}\\left(W_{2}(\\mu,\\mu^{\\prime})+\\|x-x^{\\prime}\\|_{2}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, under Assumption 1, $\\mu_{*}$ uniquely exists and satisfies the optimality condition: $\\mu_{*}\\quad\\propto$ $\\mathrm{exp}\\left(-{\\textstyle{\\frac{1}{\\lambda}}}\\frac{\\delta F(\\mu_{*})}{\\delta\\mu}\\right)$ . We refer the readers to Chizat (2022); Hu et al. (2019); Mei et al. (2018) for details. ", "page_idx": 2}, {"type": "text", "text": "We introduce the proximal Gibbs distribution (Nitanda et al., 2022; Chizat, 2022), which plays a key role in showing the convergence of mean-field optimization methods (Nitanda et al., 2022; Chizat, 2022; Oko et al., 2022; Chen et al., 2023). ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Proximal Gibbs distribution). For $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , the proximal Gibbs distribution $\\hat{\\mu}$ associated with $\\mu$ is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\hat{\\mu}}{\\mathrm{d}x}(x)=\\frac{\\exp\\left(-\\frac{1}{\\lambda}\\frac{\\delta F(\\mu)}{\\delta\\mu}(x)\\right)}{Z(\\mu)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Z(\\mu)$ is the normalization constant and $\\mathrm{d}\\hat{\\mu}/\\mathrm{d}x$ is the density function w.r.t. Lebesgue measure. ", "page_idx": 3}, {"type": "text", "text": "We remark that $\\hat{\\mu}$ exists, that is $Z(\\mu)\\,<\\,\\infty$ , for any $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ because of the boundedness of $\\delta F_{0}/\\delta\\mu$ in Assumption 1 and that the optimality condition for the problem (5) can be simply written using $\\hat{\\mu}$ as follows: $\\mu_{*}\\ =\\ \\hat{\\mu}_{*}$ . Since the proximal Gibbs distribution $\\hat{\\mu}$ minimizes the linear approximation of $F$ at $\\mu$ : $\\begin{array}{r}{F(\\dot{\\mu})+\\int\\frac{\\delta F}{\\delta\\mu}(\\mu)(\\bar{x})(\\mu^{\\prime}-\\mu)(\\mathrm{d}x)+\\lambda\\mathrm{Ent}(\\mu^{\\prime})}\\end{array}$ with respect to $\\mu^{\\prime}$ , $\\hat{\\mu}$ can be regarded as a surrogate of the solution $\\mu_{*}$ . In the case where $F_{0}(\\mu)$ is a linear functional: $F_{0}(\\mu)=\\mathbb{E}_{\\mu}[f]$ $\\left(\\exists f:\\mathbb{R}^{d}\\to\\mathbb{R}\\right)$ , the proximal Gibbs distribution $\\hat{\\mu}$ coincides with $\\mu_{*}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Mean-field Langevin dynamics and finite-particle approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The mean field Langevin dynamics (MFLD) is one effective method for solving the problem (5). MFLD $\\{X_{t}\\}_{t\\ge0}$ is described by the following stochastic differential equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-\\nabla\\frac{\\delta F}{\\delta\\mu}(\\mu_{t})(X_{t})\\mathrm{d}t+\\sqrt{2\\lambda}\\mathrm{d}W_{t},\\;\\;\\;\\mu_{t}=\\mathrm{Law}(X_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{W_{t}\\}_{t\\ge0}$ is the $d$ -dimensional standard Brownian motion with $W_{0}=0$ . We refer the reader to Huang et al. (2021) for the existence of the unique solution of this equation under Assumption 1. Nitanda et al. (2022); Chizat (2022) showed the convergence of MFLD: $\\mathcal{L}(\\mu_{t})\\mathrm{~-~}\\mathcal{L}(\\bar{\\mu_{*}})\\leq$ $\\exp(-2\\alpha\\lambda t)(\\mathcal{L}(\\mu_{0})-\\mathcal{L}(\\mu_{*}))$ under the uniform log-Sobolev inequality $(L S I)$ : ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. There exists a constant $\\alpha\\,>\\,0$ such that for any $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , proximal Gibbs distribution $\\hat{\\mu}$ satisfies log-Sobolev inequality with $\\alpha$ , that is, for any smooth function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{\\mu}}[g^{2}\\log g^{2}]-\\mathbb{E}_{\\hat{\\mu}}[g^{2}]\\log\\mathbb{E}_{\\hat{\\mu}}[g^{2}]\\leq\\frac{2}{\\alpha}\\mathbb{E}_{\\hat{\\mu}}[\\|\\nabla g\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Because of the appearance of $\\mu_{t}$ in the drift term, MFLD is a distribution-dependent dynamics referred to as general McKean\u2013Vlasov (McKean Jr, 1966). This dependence makes the difference from the standard Langevin dynamics. Hence, we need multiple particles to approximately simulate MFLD (7) unlike the standard Langevin dynamics. We here introduce the finite-particle approximation of (7) described by the $N$ -tuple of stochastic differential equation $\\{\\mathbf{X}_{t}\\}_{t\\geq0}=\\{(X_{t}^{1},\\therefore\\cdot\\cdot,X_{t}^{N})\\}_{t\\geq0}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{i}=-\\nabla\\frac{\\delta F(\\mu\\mathbf{x}_{t})}{\\delta\\mu}(X_{t}^{i})\\mathrm{d}t+\\sqrt{2\\lambda}\\mathrm{d}W_{t}^{i},\\;\\;\\;(i\\in\\{1,\\dots,N\\}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu\\mathbf{x}_{t}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{X_{t}^{i}}}\\end{array}$ is an empirical measure supported on $\\mathbf{X}_{t}$ , $\\{W_{t}^{i}\\}_{t\\ge0}$ , $(i\\in\\{1,\\ldots,N\\})$ are independent standard Brownian motions, and the gradient in the first term in RHS is taken for the function: \u03b4F (\u03b4\u00b5\u00b5Xt)(\u00b7) : Rd \u2192R. We often denote F(x) = F(\u00b5x) when emphasizing F as a function of $\\mathbf{x}$ . Noticing $\\begin{array}{r}{N\\nabla_{x^{i}}F({\\mathbf x})=\\nabla\\frac{\\delta F(\\mu_{\\mathbf{x}})}{\\delta\\mu}(x^{i})}\\end{array}$ (Ch\u221aizat, 2022), we can identify the dynamics (8) as the Langevin dynamics $\\mathrm{d}\\mathbf{X}_{t}=-N\\nabla F(\\mathbf{X}_{t})\\mathrm{d}t+\\sqrt{2\\lambda}\\mathrm{d}\\mathbf{W}_{t}$ , where $\\{\\mathbf{W}_{t}\\}_{t\\ge0}$ is the standard Brownian motion on $\\mathbb{R}^{d N}$ , for sampling from the following Gibbs distribution $\\mu_{*}^{(N)}$ on $\\mathbb{R}^{d N}$ (Chen et al., 2022): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mu_{*}^{(N)}}{\\mathrm{d}\\mathbf{x}}(\\mathbf{x})\\propto\\exp\\left(-\\frac{N}{\\lambda}F(\\mathbf{x})\\right)=\\exp\\left(-\\frac{N}{\\lambda}F_{0}(\\mathbf{x})-\\frac{\\lambda^{\\prime}}{\\lambda}\\|\\mathbf{x}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, the dynamics (8) minimizes the entropy-regularized linear functional: $\\mu^{(N)}~\\in$ $\\mathcal{P}_{2}(\\mathbb{R}^{d N})$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}^{(N)}(\\mu^{(N)})=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}[F(\\mathbf{X})]+\\lambda\\mathrm{Ent}(\\mu^{(N)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\mu_{*}^{(N)}$ is the minimizer of $\\mathcal{L}^{(N)}$ . Therefore, two objective functions $\\mathcal{L}$ and $\\mathcal{L}^{(N)}$ are tied together through the two aspects of the dynamics (8); one is the finite-particle approximation of the MFLD (7) ", "page_idx": 3}, {"type": "text", "text": "for $\\mathcal{L}$ and the other is the optimization methods for $\\mathcal{L}^{(N)}$ . We then expect $\\mathscr{L}^{(N)}(\\mu_{*}^{(N)})/N$ converges to $\\mathcal{L}(\\mu_{*})$ as $N\\rightarrow\\infty$ . Such finite-particle approximation error between $\\mathscr{L}^{(N)}(\\mu_{*}^{(N)})/N$ and $\\mathcal{L}(\\mu_{*})$ has been studied in the literature of propagation of chaos. Especially, Chen et al. (2022) proved ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{N}\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\mu_{*}^{\\otimes N})\\leq\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{\\lambda C}{\\alpha N}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C>0$ is some constant and $\\mu_{*}^{\\otimes N}$ is an $N$ -product measure of $\\mu_{*}$ . Suzuki et al. (2023b) further studied MFLD in finite-particle and discrete-time setting defined below: given $k$ -th iteration $\\mathbf{X}_{k}=(X_{k}^{1},\\ldots,X_{k}^{N})$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{k+1}^{i}=X_{k}^{i}-\\eta\\nabla{\\frac{\\delta F(\\mu\\mathbf{x}_{k})}{\\delta\\mu}}(X_{k}^{i})+\\sqrt{2\\lambda\\eta}\\xi_{k}^{i},\\;\\;\\;(i\\in\\{1,\\dots,N\\}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xi_{k}^{i}\\sim\\mathcal{N}(0,I_{d})\\;(i\\in\\{1,\\ldots,N\\})$ are i.i.d. standard normal random variables. By extending the proof techniques developed by Chen et al. (2022), Suzuki et al. (2023b) proved the uniform-in-time propagation of chaos for MFLD (12); there exist constants $C_{1},C_{2}>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{k}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\exp{(-\\lambda\\alpha\\eta k/2)}\\left(\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})-\\mathcal{L}(\\mu_{*})\\right)+\\frac{(\\lambda\\eta+\\eta^{2})C_{1}}{\\lambda\\alpha}+\\frac{\\lambda C_{2}}{\\alpha N},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \u00b5(kN) = Law(Xk). The last two terms are due to time-discretization and finite-particle approximation, respectively. The finite-particle approximation error $\\textstyle O({\\frac{\\lambda}{\\alpha N}})$ appearing in (11), (13) means the deterioration as $\\alpha~\\to~0$ . Considering typical estimation $\\vec{\\alpha}\\stackrel{\\_}{\\sim}\\mathrm{~exp}(-\\Theta(1/\\lambda))$ (e.g., Theorem 1 in Suzuki et al. (2023b)) of LSI-constant using Holley and Stroock argument (Holley and Stroock, 1987) or Miclo\u2019s trick (Bardet et al., 2018), these bounds imply that the required number of particles increases exponentially as $\\lambda\\to0$ . ", "page_idx": 4}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present an LSI-constant free particle approximation error between $\\begin{array}{r}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})}\\end{array}$ and $\\mathcal{L}(\\mu_{*})$ for mean-field neural networks and apply it to the mean-field Langevin dynamics. ", "page_idx": 4}, {"type": "text", "text": "3.1 LSI-constant free particle approximation error for mean-field neural networks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We focus on the empirical risk minimization problem of mean-field neural networks. Let $h(x,\\cdot):$ $\\mathcal{Z}\\,\\rightarrow\\,\\mathbb{R}$ be a function parameterized by $x~\\in~\\mathbb{R}^{d}$ , where $\\mathcal{Z}$ is the data space. The mean-field model is obtained by integrating $h(x,\\cdot)$ with respect to the probability distribution $\\mu\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{d})$ over the parameter space: $h_{\\mu}(\\cdot)\\,=\\,\\mathbb{E}_{X\\sim\\mu}[h(X,\\cdot)]$ . Typically, $h$ is set as $h(x,z)\\,=\\,\\sigma(w^{\\top}z)$ or $h(x,z)\\,=\\,\\operatorname{tanh}(v\\sigma(w^{\\top}z))$ where $\\sigma$ is an activation function and $x\\ =\\ w$ or $\\boldsymbol{x}\\,=\\,(v,w)$ is the trainable parameter in each case. Given training examples $\\{(z_{j},y_{j})\\}_{j=1}^{n}\\subset\\mathcal{Z}\\times\\mathbb{R}$ and loss function $\\ell(\\cdot,\\cdot):\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ , we consider the empirical risk of the mean-field neural networks: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{0}(\\mu)=\\frac{1}{n}\\sum_{j=1}^{n}\\ell(h_{\\mu}(z_{j}),y_{j}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For our analysis, we make the following assumption which is satisfied in the common settings. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. $\\ell(\\cdot,y)$ is convex and $L$ -smooth, and $h(X,z)$ $(X\\sim\\mu_{*})$ has a finite-second moment; ", "page_idx": 4}, {"type": "text", "text": "\u2022 There exists $L>0$ such that for any $\\begin{array}{r}{a,b,y\\in\\mathcal{Y},\\ell(b,y)\\leq\\ell(a,y)+\\frac{\\partial\\ell(a,y)}{\\partial a}(b-a)+\\frac{L}{2}|b-a|^{2}.}\\end{array}$ \u2022 There exists $R>0$ such that for any $z\\in{\\mathcal{Z}}$ , $\\mathbb{E}_{X\\sim\\mu_{*}}[|h(X,z)|^{2}]\\leq R^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "We can directly verify this assumption for mean-field neural networks using a bounded activation function (Nitanda et al., 2022; Chizat, 2022; Chen et al., 2022; Suzuki et al., 2023b) and standard loss functions such as logistic loss and squared loss. The following is the main theorem that bounds $\\begin{array}{r}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\mathcal{L}(\\mu_{*})}\\end{array}$ . The proof is deferred to Section 4 and Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumptions $^{\\,I}$ and 3, it follows that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{N}\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\mu_{*}^{\\otimes N})\\leq\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{L R^{2}}{2N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A significant difference from the previous results (11), (13) with $k\\rightarrow\\infty$ , and Kook et al. (2024) is that our bound is free from the LSI-constant. Therefore, the approximation error uniformly decreases as $N\\rightarrow\\infty$ at the same rate regardless of the value of LSI-constant as well as $\\lambda$ . ", "page_idx": 5}, {"type": "text", "text": "As discussed in Section 4 later, the differences between $\\begin{array}{r}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})}\\end{array}$ and $\\mathcal{L}(\\mu_{*})$ is due to nonlinearity of the loss $\\ell$ . In fact, since $L=0$ for a linear loss function $\\ell$ , it follows that $\\begin{array}{r l}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})=}&{{}}\\end{array}$ $\\mathcal{L}(\\mu_{*})$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Application: mean-field Langevin dynamics in the finite-particle setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As an application of Theorem 1, we present the convergence analysis of the mean-field Langevin dynamics (MFLD) in the finite-particle settings (8) and (12), sampling guarantee for the mean-field stationary distribution $\\mu_{\\ast}\\in\\mathcal{P}_{2}(\\underline{{\\bar{\\mathbb{R}}}}^{d})$ , and uniform-in-time Wasserstein propagation of chaos. ", "page_idx": 5}, {"type": "text", "text": "3.2.1 Convergence of the mean-field Langevin dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our convergence theory assumes the logarithmic Sobolev inequality (LSI) on $\\mu_{*}^{(N)}$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 4. There exists a constant $\\bar{\\alpha}>0$ such that $\\mu_{*}^{(N)}$ satisfies log-Sobolev inequality with constant $\\bar{\\alpha}$ , that is, for any smooth function $g:\\mathbb{R}^{d N}\\rightarrow\\mathbb{R}$ , it follows that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu_{*}^{(N)}}\\big[g^{2}\\log g^{2}\\big]-\\mathbb{E}_{\\mu_{*}^{(N)}}\\big[g^{2}\\big]\\log\\mathbb{E}_{\\mu_{*}^{(N)}}\\big[g^{2}\\big]\\leq\\frac{2}{\\bar{\\alpha}}\\mathbb{E}_{\\mu_{*}^{(N)}}\\big[\\|\\nabla g\\|_{2}^{2}\\big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By setting g = d\u00b5(\u2217N) , Assumption 4 leads to KL(\u00b5(N)\u2225\u00b5(\u2217N)) \u2264 21\u03b1\u00afE\u00b5(N)    \u2207log dd\u00b5\u00b5((\u2217NN))   22 . For instance, using Holley and Stroock argument (Holley and Stroock, 1987) under the boundedness assumption $|F_{0}({\\bf x})|\\le B$ $\\mathbf{\\nabla}\\forall\\mathbf{x}\\in\\mathbb{R}^{d N}\\mathbf{\\Phi}$ , we can verify LSI on $\\mu_{*}^{(N)}$ with a constant $\\bar{\\alpha}$ that satisfies: $\\begin{array}{r}{\\bar{\\alpha}\\geq\\frac{2\\lambda^{\\prime}}{\\lambda}\\exp\\left(-\\frac{4\\dot{N}B}{\\lambda}\\right)}\\end{array}$ . For the detail, see Appendix B. ", "page_idx": 5}, {"type": "text", "text": "The following theorem demonstrates the convergence rates of $\\mathcal{L}^{(N)}(\\mu^{(N)})$ with the finite-particle MFLD in the continuous- and discrete-time settings. The first assertion is a direct consequence of Theorem 1 and the standard argument based on LSI for continuous-time Langevin dynamics. Whereas for the second assertion, we employ the one-step interpolation argument (Vempala and Wibisono, 2019) with some refinement to avoid the dependence on the dimensionality $d N$ where the dynamics (12) performs. The proof is given in Appendix A.2. We denote t(N)= Law(Xt) and \u00b5(kN)= Law(Xk) for continuous- and discrete-time dynamics (8) and (12), respectively. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Suppose Assumptions 1, 3, and 4 hold. Then, it follows that ", "page_idx": 5}, {"type": "text", "text": "1. MFLD (8) in finite-particle and continuous-time setting satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{t}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{L R^{2}}{2N}+\\exp(-2\\bar{\\alpha}\\lambda t)\\left(\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})-\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2. MFLD (12) with $\\eta\\lambda^{\\prime}<1/2$ in finite-particle and discrete-time setting satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{k}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{{\\cal L}R^{2}}{2N}+\\frac{\\delta_{\\eta}^{(N)}}{2\\bar{\\alpha}\\lambda}+\\exp(-\\bar{\\alpha}\\lambda\\eta k)\\left(\\displaystyle\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})-\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})\\right),}}\\\\ {{\\displaystyle w h e r e\\ \\delta_{\\eta}^{(N)}=16\\eta(M_{2}^{2}+\\lambda^{\\prime2})(\\eta M_{1}^{2}+\\lambda d)+64\\eta^{2}\\lambda^{\\prime2}(M_{2}^{2}+\\lambda^{\\prime2})\\left(\\frac{\\mathbb{E}[\\|{\\mathbf{X}}_{0}\\|_{2}^{2}]}{N}+\\frac{1}{\\lambda^{\\prime}}\\left(\\frac{M_{1}^{2}}{4\\lambda^{\\prime}}+\\lambda d\\right)\\right)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The term of $\\frac{L R^{2}}{2N}$ is the particle approximation error inherited from Theorem 1. Again our result shows the LSI-constant independence particle approximation error for MFLD unlike existing results (Chen et al., 2022; Suzuki et al., 2023b) where their error bounds $\\textstyle O({\\frac{\\lambda}{\\alpha N}})$ scale inversely with LSI-constant $\\alpha$ as seen in (11) and (13). Hence, the required number of particles to achieve $\\epsilon$ -accurate optimization: $\\begin{array}{r}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\epsilon}\\end{array}$ suggested by our result and Chen et al. (2022); Suzuki et al. (2023b) are $\\begin{array}{r}{N=O(\\frac{1}{\\epsilon})}\\end{array}$ and $\\begin{array}{r}{N=O(\\frac{\\lambda}{\\alpha\\epsilon})}\\end{array}$ , respectively. Whereas the iterations complexity of MFLD (12) is $\\begin{array}{r}{O\\big(\\frac{1}{\\bar{\\alpha}^{2}\\lambda\\epsilon}\\log\\frac{1}{\\epsilon}\\big)}\\end{array}$ which is same as that in Suzuki et al. (2023b) up to a difference in LSI constants $\\alpha$ or $\\bar{\\alpha}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2.2 Sampling guarantee for $\\mu_{*}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After running the finite-particle MFLD with a sufficient number of particles for a long time, each particle is expected to be distributed approximately according to $\\mu_{*}$ . In Corollary 1, we justify this sampling procedure for $\\mu_{*}$ as an application of Theorem 2. We set $\\begin{array}{r}{\\Delta_{0}^{(N)}=\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})\\stackrel{{}}{-}}\\end{array}$ $\\begin{array}{r}{\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})}\\end{array}$ and write the marginal distribution of $\\mu_{t}^{(N)}/\\mu_{k}^{(N)}$ on the first particle $x^{1}$ as $\\mu_{t,1}^{(N)}/\\mu_{k,1}^{(N)}$ ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Under the same conditions as in Theorem 2, we run MFLDs (8) and (12) with i.i.d. initial particles $\\mathbf{X}=(X_{0}^{1},\\ldots,X_{0}^{N})$ . Then, it follows that ", "page_idx": 6}, {"type": "text", "text": "1. MFLD (8) in finite-particle and continuous-time setting satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda\\mathrm{KL}(\\mu_{t,1}^{(N)}\\|\\mu_{*})\\leq\\mathcal{L}(\\mu_{t,1}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{L R^{2}}{2N}+\\exp(-2\\bar{\\alpha}\\lambda t)\\Delta_{0}^{(N)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2. MFLD (12) with $\\eta\\lambda^{\\prime}<1/2$ in finite-particle and discrete-time setting satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda\\mathrm{KL}(\\mu_{k,1}^{(N)}||\\mu_{*})\\leq\\mathcal{L}(\\mu_{k,1}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{L R^{2}}{2N}+\\frac{\\delta_{\\eta}^{(N)}}{2\\bar{\\alpha}\\lambda}+\\exp(-\\bar{\\alpha}\\lambda\\eta k)\\Delta_{0}^{(N)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. For any distribution $\\mu^{(N)}\\;\\in\\;\\mathcal{P}_{2}(\\mathbb{R}^{d N})$ whose marginal $\\mu_{i}^{(N)}$ on $i$ -th coordinate $x^{i}\\ (i\\ \\in$ $\\{1,\\ldots,N\\})$ are identical to each other, it follows that by the convexity of the objective function and the entropy sandwich (Nitanda et al., 2022; Chizat, 2022): $\\lambda\\mathrm{KL}(\\mu\\|\\dot{\\mu_{*}})\\leq\\mathcal{L}(\\mu)\\ddot{-}\\,\\mathcal{L}(\\mu_{*})\\,\\left(\\forall\\mu\\in\\mathcal{$ $\\mathcal{P}_{2}(\\mathbb{R}^{d}))$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda\\mathrm{KL}(\\mu_{1}^{(N)}\\|\\mu_{*})\\leq\\mathcal{L}(\\mu_{1}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu^{(N)})-\\mathcal{L}(\\mu_{*}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Because of i.i.d. initialization, the distributions of $\\mu_{t}^{(N)}/\\mu_{k}^{(N)}$ satisfies this property. That is, (16) with \u00b5(N) = \u00b5t(N) /\u00b5(kN)holds. Hence, Theorem 2 concludes the proof. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 shows the convergence of the objective $\\mathcal{L}(\\cdot)$ and KL-divergence $\\mathrm{KL}(\\cdot\\|\\mu_{*})$ which attain the minimum value at $\\mu=\\mu_{*}$ . For instance, we can deduce that the particle and iteration complexities to obtain $\\sqrt{\\mathrm{KL}(\\mu_{k,1}^{(N)}\\lVert\\mu_{*})}<\\epsilon$ by MFLD (12) are $\\begin{array}{r}{O\\big(\\frac{1}{\\lambda\\epsilon^{2}}\\big)}\\end{array}$ and $\\begin{array}{r}{O\\big(\\frac{1}{\\bar{\\alpha}^{2}\\lambda^{2}\\epsilon^{2}}\\log\\frac{1}{\\epsilon}\\big)}\\end{array}$ , respectively, whereas Kook et al. (2024) proved the following particle and iteration complexities: $O\\big(\\frac{1}{\\alpha\\lambda\\epsilon^{2}}\\big)$ and $O\\big(\\frac{1}{\\alpha^{2}\\lambda^{2}\\epsilon^{2}}\\big)$ . ", "page_idx": 6}, {"type": "text", "text": "3.2.3 Uniform-in-time Wasserstein propagation of chaos ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As another application of Theorem 2, we prove the uniform-in-time Wasserstein propagation of chaos for MFLDs (8) and (12), saying that the Wasserstein distance between finite-particle system and its mean-field limit shrinks uniformly in time as $N\\rightarrow\\infty$ . For the mean-field limit of (8) in the continuous-time setting, we refer to (7). For the discrete-time setting (12), we define its mean-field limit as follows; let $\\mu_{k}=\\operatorname{Law}(X_{k})$ be the distribution of the infinite-particle MFLD defined by ", "page_idx": 6}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-\\eta\\nabla\\frac{\\delta F(\\mu_{k})}{\\delta\\mu}(X_{k})+\\sqrt{2\\lambda\\eta}\\xi_{k},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\xi_{k}\\sim\\mathcal{N}(0,I_{d})$ . Now, the uniform-in-time Wasserstein propagation of chaos for MFLDs is given below. We set $\\begin{array}{r}{\\Delta_{0}^{(N)}=\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})-\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})}\\end{array}$ and $\\Delta_{0}=\\mathcal{L}(\\mu_{0})-\\mathcal{L}(\\mu_{*})$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 2. Suppose Assumptions 1, 2, 3, and $^{4}$ hold. Then, it follows that ", "page_idx": 6}, {"type": "text", "text": "1. discrepancy between continuous-time MFLDs (7) and (8) is uniformly bounded in time as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{N}W_{2}^{2}(\\mu_{t}^{(N)},\\mu_{t}^{\\otimes N})\\leq\\frac{4}{\\alpha\\lambda}\\left(\\frac{L R^{2}}{2N}+\\exp(-2\\bar{\\alpha}\\lambda t)\\Delta_{0}^{(N)}+\\exp(-2\\alpha\\lambda t)\\Delta_{0}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2. discrepancy between discrete-time MFLDs (17) and (12) is uniformly bounded in time as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{N}W_{2}^{2}(\\mu_{k}^{(N)},\\mu_{k}^{\\otimes N})\\leq\\frac{4}{\\alpha\\lambda}\\left(\\frac{L R^{2}}{2N}+\\frac{\\delta_{\\eta}^{(N)}}{2\\bar{\\alpha}\\lambda}+\\frac{\\delta_{\\eta}}{2\\alpha\\lambda}+\\exp(-\\bar{\\alpha}\\lambda\\eta k)\\Delta_{0}^{(N)}+\\exp(-2\\alpha\\lambda\\eta k)\\Delta_{0}\\right),}}\\\\ {{\\displaystyle w h e r e\\ \\delta_{\\eta}=8\\eta(M_{2}^{2}+\\lambda^{\\prime2})(2\\eta M_{1}^{2}+2\\lambda d)+32\\eta^{2}\\lambda^{\\prime2}(M_{2}^{2}+\\lambda^{\\prime2})\\left(\\mathbb{E}\\left[\\|X_{0}\\|_{2}^{2}\\right]+\\frac{1}{\\lambda^{\\prime}}\\left(\\frac{M_{1}^{2}}{4\\lambda^{\\prime}}+\\lambda d\\right)\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. We only prove the first assertion because the second can be proven similarly. We apply the triangle inequality to $W_{2}$ -distance as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\mu_{t}^{(N)},\\mu_{t}^{\\otimes N})\\leq2\\left(W_{2}^{2}(\\mu_{t}^{(N)},\\mu_{*}^{\\otimes N})+W_{2}^{2}(\\mu_{*}^{\\otimes N},\\mu_{t}^{\\otimes N})\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that LSI with the same constant is preserved under tensorization: $\\mu_{*}\\,\\to\\,\\mu_{*}^{\\otimes N}$ . Then, by Taragland\u2019s inequality (Otto and Villani, 2000), Proposition 1 with $\\mu=\\mu_{*}$ , and the entropy sandwich (Nitanda et al., 2022; Chizat, 2022): $\\lambda\\mathrm{KL}(\\mu_{t}\\|\\mu_{*})\\stackrel{*}{\\leq}\\mathcal{L}(\\mu_{t})-\\mathcal{L}(\\dot{\\mu}_{*})$ , we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\alpha}{2}W_{2}^{2}(\\mu_{t}^{(N)},\\mu_{*}^{\\otimes N})\\leq\\mathrm{KL}(\\mu_{t}^{(N)}\\|\\mu_{*}^{\\otimes N})\\leq\\frac{1}{\\lambda}(\\mathcal{L}^{(N)}(\\mu_{t}^{(N)})-N\\mathcal{L}(\\mu_{*})),}\\\\ &{\\displaystyle\\frac{\\alpha}{2}W_{2}^{2}(\\mu_{*}^{\\otimes N},\\mu_{t}^{\\otimes N})=\\frac{\\alpha}{2}N W_{2}^{2}(\\mu_{*},\\mu_{t})\\leq N\\mathrm{KL}(\\mu_{t}\\|\\mu_{*})\\leq\\frac{N}{\\lambda}(\\mathcal{L}(\\mu_{t})-\\mathcal{L}(\\mu_{*})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying the convergence rates of finite- and infinite-particle MFLDs (Theorem 2 and Nitanda et al. (2022)), we conclude the proof. For completeness, we include the auxiliary results used in the proof in Appendix B. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Corollary 2 uniformly controls the gap between $N$ -particle system $\\mu_{t}^{(N)}/\\mu_{k}^{(N)}$ and its mean-field limit $\\mu_{t}^{\\otimes N}/\\mu_{k}^{\\otimes N}$ . Again this result shows an improved particle approximation error $\\begin{array}{r}{O(\\frac{1}{\\alpha\\lambda N})}\\end{array}$ over $\\begin{array}{r}{O(\\frac{1}{\\alpha^{2}N})}\\end{array}$ (Chen et al., 2022; Suzuki et al., 2023b). Additionally, the propagation of chaos result in terms of TV-norm can be proven by using Pinsker\u2019s inequality instead of Talagrand\u2019s inequality in the proof. For the continuous-time MFLDs, we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\mathrm{TV}^{2}(\\mu_{t}^{(N)},\\mu_{t}^{\\otimes N})\\leq\\frac{1}{\\lambda}\\left(\\frac{L R^{2}}{2N}+\\exp(-2\\bar{\\alpha}\\lambda t)\\Delta_{0}^{(N)}+\\exp(-2\\alpha\\lambda t)\\Delta_{0}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and TV-norm counterpart for the discrete-time can be derived similary. ", "page_idx": 7}, {"type": "text", "text": "4 Proof outline and key results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide the proof sketch of Theorem 1. Our analysis carefully treats the nonlinearity of $F_{0}$ because the particle approximation errors, the gap between $\\mathcal{L}/\\mu_{*}$ and $\\mathscr{L}^{(N)}/\\mu_{*}^{(N)}$ , come from this non-linearity. In fact if $F_{0}$ is a linear functional: $F_{0}(\\mu)=\\mathbb{E}_{\\mu}[f]$ $\\left(\\exists f:\\mathbb{R}^{d}\\to\\mathbb{R}\\right)$ ), then $\\begin{array}{r}{\\mathcal{L}^{(N)}(\\mu^{(N)})=\\sum_{i=1}^{N}\\mathbb{E}_{X^{i}\\sim\\mu_{i}^{(N)}}[f(X^{i})+\\lambda^{\\prime}\\|X^{i}\\|_{2}^{2}]+\\lambda\\mathrm{Ent}(\\mu^{(N)})\\geq\\sum_{i=1}^{N}\\mathcal{L}(\\mu_{i}^{(N)})}\\end{array}$ , where $\\mu_{i}^{(N)}$ are marginal distributions on $X^{i}$ . This results in $\\mu_{*}^{(N)}=\\mu_{*}^{\\otimes N}$ and $\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})=N\\mathcal{L}(\\mu_{*})$ , and thus there is no approximation error by using finite-particles as also deduced from Theorem 1 with $L=0$ . Therefore, we should take into account the non-linearity of $F_{0}$ to tightly evaluate the gap between $\\mathcal{L}$ and $\\mathcal{L}^{(N)}$ . ", "page_idx": 7}, {"type": "text", "text": "To do so, we define Bregman divergence based on $F$ on $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ as follows; for any $\\mu$ $\\iota,\\,\\mu^{\\prime}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nB_{F}(\\mu,\\mu^{\\prime})=F(\\mu)-F(\\mu^{\\prime})-\\left\\langle\\frac{\\delta F(\\mu^{\\prime})}{\\delta\\mu},\\mu-\\mu^{\\prime}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$B_{F}$ measures the discrepancy between $\\mu$ and $\\mu^{\\prime}$ in light of the strength of the convexity. If $F$ is linear with respect to the distribution, $B_{F}=0$ clearly holds. By the convexity $F$ , we see $B_{F}(\\mu,\\mu^{\\prime})\\ge0$ . Moreover, we see the following relationship between $\\mu_{*}^{(N)}$ and $\\hat{\\mu}$ for any $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\mu_{*}^{(N)}}{\\mathrm{d}\\mathbf{x}}(\\mathbf{x})\\propto\\exp\\left(-\\frac{N}{\\lambda}F(\\mathbf{x})\\right)}\\\\ {\\displaystyle\\qquad\\qquad=\\exp\\left(-\\frac{N}{\\lambda}\\left(F(\\mu)+\\left\\langle\\frac{\\delta F(\\mu)}{\\delta\\mu},\\mu_{\\mathbf{x}}-\\mu\\right\\rangle+B_{F}(\\mu_{\\mathbf{x}},\\mu)\\right)\\right)}\\\\ {\\displaystyle\\qquad\\propto\\exp\\left(-\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\right)\\prod_{i=1}^{N}\\exp\\left(-\\frac{1}{\\lambda}\\frac{\\delta F(\\mu)}{\\delta\\mu}(x^{i})\\right)}\\\\ {\\displaystyle\\qquad\\propto\\exp\\left(-\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\right)\\frac{\\mathrm{d}\\hat{\\mu}^{\\otimes N}}{\\mathrm{d}\\mathbf{x}}(\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proximal Gibbs distribution $\\hat{\\mu}$ has been introduced in Nitanda et al. (2022) as a proxy for the solution $\\mu_{*}$ and it coincides with $\\mu_{*}$ when $F$ is a linear functional. The above equation (19) naturally reflects this property since it bridges the gap between $\\mu_{*}^{(N)}$ and $\\hat{\\mu}^{\\otimes N}$ using $B_{F}$ and leads to $\\mu_{*}^{(N)}=\\hat{\\mu}^{\\otimes N}$ for the linear functional $F$ . ", "page_idx": 8}, {"type": "text", "text": "Next, we provide key propositions whose proofs can be found in Appendix A. The following proposition expresses objective gaps $\\mathcal{L}^{(N)}(\\bar{\\mu}^{(N)})-N\\mathcal{L}(\\mu)$ and $\\mathcal{L}^{(N)}(\\bar{\\mu^{(N)}})-\\mathcal{L}^{(N)}(\\hat{\\mu}^{\\otimes N})$ using only divergences. ", "page_idx": 8}, {"type": "text", "text": "Proposition 1. For $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $\\mu^{(N)}\\in\\mathcal{P}_{2}(\\mathbb{R}^{d N})$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{(N)}(\\mu^{(N)})-N\\mathcal{L}(\\mu)=N\\mathbb{E}_{{\\mathbf{X}}\\sim\\mu^{(N)}}\\left[B_{F}(\\mu{\\mathbf{x}},\\mu)\\right]+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\otimes N})-\\lambda N\\mathrm{KL}(\\mu\\|\\hat{\\mu}),}\\\\ &{\\mathcal{L}^{(N)}(\\mu^{(N)})-\\mathcal{L}^{(N)}(\\hat{\\mu}^{\\otimes N})=N\\int B_{F}(\\mu{\\mathbf{x}},\\mu)(\\mu^{(N)}-\\hat{\\mu}^{\\otimes N})(\\mathrm{d}{\\mathbf{x}})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\otimes N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The following proposition shows that the KL-divergence between $\\mu_{*}^{(N)}$ and $\\hat{\\mu}^{\\otimes N}$ can be upperbounded by the Bregman divergence $B_{F}$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 2. For any $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\hat{\\mu}^{\\otimes N})\\leq\\frac{N}{\\lambda}\\int B_{F}(\\mu_{\\mathbf{x}},\\mu)(\\hat{\\mu}^{\\otimes N}-\\mu_{*}^{(N)})(\\mathrm{d}\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "By applying Eq. (22) to Eq. (20) with $\\mu=\\mu_{*}$ and $\\mu^{(N)}=\\mu_{*}^{(N)}$ , we obtain an important inequality: $\\begin{array}{r}{\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-N\\mathcal{L}(\\mu_{*})\\leq N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]}\\end{array}$ . Here, we give a finer result below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. For the minimizes $\\mu_{*}$ of $\\mathcal{L}$ and $\\mu_{*}^{(N)}$ of $\\mathcal{L}^{(N)}$ , it follows that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\mu_{*}^{\\otimes N})\\leq\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-N\\mathcal{L}(\\mu_{*})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{L}^{(N)}(\\mu_{*}^{\\otimes N})-N\\mathcal{L}(\\mu_{*})=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof. Proposition 1 with $\\mu=\\mu_{*}$ and $\\mu^{(N)}=\\mu_{*}^{(N)}$ lead to the following equalities: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-N\\mathcal{L}(\\mu_{*})=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{(N)}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]+\\lambda\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\mu_{*}^{\\otimes N}),}\\\\ &{\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\mathcal{L}^{(N)}(\\mu_{*}^{\\otimes N})=N\\int B_{F}(\\mu_{\\mathbf{x}},\\mu_{*})(\\mu_{*}^{(N)}-\\mu_{*}^{\\otimes N})(\\mathrm{d}\\mathbf{x})+\\lambda\\mathrm{KL}(\\mu_{*}^{(N)}\\|\\mu_{*}^{\\otimes N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The first inequality of the theorem is a direct consequence of Eq. (23) since $B_{F}\\geq0$ . The second inequality results from $\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})\\,\\leq\\,\\mathcal{L}^{(N)}(\\mu_{*}^{\\otimes N})$ . The last equality is obtained by subtracting Eq. (24) from Eq. (23). \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Intuitively, $\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]$ is small because the empirical distribution $\\mu\\mathbf{x}$ $(\\mathbf{X}\\,\\sim\\,\\mu_{*}^{\\otimes N})$ converges to $\\mu_{*}$ by law of large numbers. Indeed, more simply, we can relate this term to the variance of an $N$ -particle mean-field model $h_{\\mu\\mathbf{x}}(z)\\left(\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}\\right)$ , yielding a bound: $\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]\\le$ $\\textstyle{\\frac{L R^{2}}{2N}}$ (see the proof of Theorem 1 in Appendix A). Then, we arrive at Theorem 1. ", "page_idx": 8}, {"type": "text", "text": "Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provided an improved particle approximation error over Chen et al. (2022); Suzuki et al. (2023b) by alleviating the dependence on LSI constants in their bounds. Specifically, we established an LSI-constant-free particle approximation error concerning the objective gap. Additionally, we demonstrated improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution $\\mu_{*}$ , and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity. ", "page_idx": 8}, {"type": "text", "text": "A limitation of our result is that the iteration complexity still depends exponentially on the LSI constant. This hinders achieving polynomial complexity for MFLD. However, considering the difficulty of general non-convex optimization problems, this dependency may be unavoidable. Improving the iteration complexity for more specific problem settings is an important direction for future research. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative, the Centre for Frontier Artificial Intelligence Research, Institute of High Performance Computing, $\\mathrm{A\\!\\star\\!STAR}$ , and the College of Computing and Data Science at Nanyang Technological University. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author and do not reflect the views of National Research Foundation, Singapore, and Infocomm Media Development Authority. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Bakry, D. and \u00c9mery, M. (1985). Diffusions hypercontractives. S\u00e9minaire de probabilit\u00e9s de Strasbourg, 19:177\u2013206. ", "page_idx": 9}, {"type": "text", "text": "Bardet, J.-B., Gozlan, N., Malrieu, F., and Zitt, P.-A. (2018). Functional inequalities for gaussian convolutions of compactly supported measures: Explicit bounds and dimension dependence. Bernoulli, 24(1):333 \u2013 353.   \nChen, F., Ren, Z., and Wang, S. (2022). Uniform-in-time propagation of chaos for mean field langevin dynamics. arXiv preprint arXiv:2212.03050.   \nChen, F., Ren, Z., and Wang, S. (2023). Entropic fictitious play for mean field optimization problem. Journal of Machine Learning Research, 24(211):1\u201336.   \nChizat, L. (2022). Mean-field langevin dynamics: Exponential convergence and annealing. Transactions on Machine Learning Research.   \nChizat, L. and Bach, F. (2018). On the global convergence of gradient descent for over-parameterized models using optimal transport. In Advances in Neural Information Processing Systems 31, pages 3040\u20133050.   \nHolley, R. and Stroock, D. (1987). Logarithmic sobolev inequalities and stochastic ising models. Journal of statistical physics, 46(5-6):1159\u20131194.   \nHu, K., Ren, Z., Siska, D., and Szpruch, L. (2019). Mean-field langevin dynamics and energy landscape of neural networks. arXiv preprint arXiv:1905.07769.   \nHuang, X., Ren, P., and Wang, F.-Y. (2021). Distribution dependent stochastic differential equations. Frontiers of Mathematics in China, 16:257\u2013301.   \nKook, Y., Zhang, M. S., Chewi, S., Erdogdu, M. A., et al. (2024). Sampling from the mean-field stationary distribution. arXiv preprint arXiv:2402.07355.   \nMcKean Jr, H. P. (1966). A class of markov processes associated with nonlinear parabolic equations. Proceedings of the National Academy of Sciences, 56(6):1907\u20131911.   \nMei, S., Montanari, A., and Nguyen, P.-M. (2018). A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671.   \nNitanda, A. and Suzuki, T. (2017). Stochastic particle gradient descent for infinite ensembles. arXiv preprint arXiv:1712.05438.   \nNitanda, A., Wu, D., and Suzuki, T. (2021). Particle dual averaging: Optimization of mean field neural networks with global convergence rate analysis. In Advances in Neural Information Processing Systems 34, pages 19608\u201319621.   \nNitanda, A., Wu, D., and Suzuki, T. (2022). Convex analysis of the mean field langevin dynamics. In Proceedings of International Conference on Artificial Intelligence and Statistics 25, pages 9741\u20139757.   \nOko, K., Suzuki, T., Nitanda, A., and Wu, D. (2022). Particle stochastic dual coordinate ascent: Exponential convergent algorithm for mean field neural network optimization. In Proceedings of the 10th International Conference on Learning Representations.   \nOtto, F. and Villani, C. (2000). Generalization of an inequality by talagrand and links with the logarithmic sobolev inequality. Journal of Functional Analysis, 173(2):361\u2013400.   \nRotskoff, G. and Vanden-Eijnden, E. (2022). Trainability and accuracy of artificial neural networks: An interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):1889\u20131935.   \nSirignano, J. and Spiliopoulos, K. (2020a). Mean field analysis of neural networks: A central limit theorem. Stochastic Processes and their Applications, 130(3):1820\u20131852.   \nSirignano, J. and Spiliopoulos, K. (2020b). Mean field analysis of neural networks: A law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725\u2013752.   \nSuzuki, T., Nitanda, A., and Wu, D. (2023a). Uniform-in-time propagation of chaos for the mean field gradient langevin dynamics. In Proceedings of the 11th International Conference on Learning Representations.   \nSuzuki, T., Wu, D., and Nitanda, A. (2023b). Convergence of mean-field langevin dynamics: timespace discretization, stochastic gradient, and variance reduction. In Advances in Neural Information Processing Systems 36.   \nSznitman, A.-S. (1991). Topics in propagation of chaos. Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour XIX\u20141989, pages 165\u2013251.   \nVempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. In Advances in Neural Information Processing Systems 32, pages 8094\u20138106. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A Omitted Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Finite-particle approximation error ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section, we prove the LSI-constant-free particle approximation error (Theorem 1). First we give proofs of Propositions 1 and 2. ", "page_idx": 11}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . First, we prove Eq. (20) as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{(N)}(\\mu^{(N)})-N\\mathcal{L}(\\mu)}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[F(\\mu_{\\mathbf{X}})-F(\\mu)\\right]+\\lambda(\\mathrm{Ent}(\\mu^{(N)})-N\\mathrm{Ent}(\\mu))}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F(\\mu_{\\mathbf{X}},\\mu)+\\left\\langle\\frac{\\delta F}{\\delta\\mu}(\\mu),\\mu_{\\mathbf{X}}-\\mu\\right\\rangle\\right]+\\lambda(\\mathrm{Ent}(\\mu^{(N)})-N\\mathrm{Ent}(\\mu))}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F(\\mu_{\\mathbf{X}},\\mu)-\\lambda\\left\\langle\\log\\frac{\\mathrm{d}\\hat{\\mu}}{\\mathrm{d}x},\\mu_{\\mathbf{X}}-\\mu\\right\\rangle\\right]+\\lambda(\\mathrm{Ent}(\\mu^{(N)})-N\\mathrm{Ent}(\\mu))}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F(\\mu_{\\mathbf{X}},\\mu)-\\lambda\\left\\langle\\log\\frac{\\mathrm{d}\\hat{\\mu}}{\\mathrm{d}x},\\mu_{\\mathbf{X}}\\sim\\mu\\right\\rangle\\right]+\\lambda\\mathrm{Ent}(\\mu^{(N)})-\\lambda N\\mathrm{Ent}(\\mu))}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F(\\mu_{\\mathbf{X}},\\mu)-\\lambda\\left\\langle\\log\\frac{\\mathrm{d}\\hat{\\mu}}{\\mathrm{d}x},\\mu_{\\mathbf{X}}\\right\\rangle\\right]+\\lambda\\mathrm{Ent}(\\mu^{(N)})-\\lambda N\\mathrm{KL}(\\mu)\\|\\hat{\\mu}\\|}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F(\\mu_{\\mathbf{X}},\\mu)\\right]-\\lambda\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[\\displaystyle\\sum_{i=1}^{N}\\log\\frac{\\mathrm{d}\\hat{\\mu}}{\\mathrm{d}x}(X^{i})\\right]+\\lambda\\mathrm{Ent}(\\mu^{(N)})-\\lambda N\\mathrm{KL}(\\mu\\|\\hat{\\mu})}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu^{(N)}}\\left[B F_{\\mathbf{X}}(\\mu_{\\mathbf{X}},\\mu)\\right]+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Next, we prove Eq. (21) as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{(N)}(\\mu^{(N)})-\\mathcal{L}^{(N)}(\\hat{\\mu}^{\\mathcal{B}^{N}})}\\\\ &{=N\\int F(\\mathbf{x})(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda(\\mathrm{Ent}(\\mu^{(N)})-\\mathrm{Ent}(\\hat{\\mu}^{\\mathcal{B}^{N}}))}\\\\ &{=N\\int F(\\mathbf{x})(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\mathcal{B}^{N}})+\\lambda\\int\\log\\frac{\\mathrm{d}\\hat{\\mu}^{\\mathcal{B}^{N}}}{\\mathrm{d}\\mathbf{x}}(\\mathbf{x})(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathrm{dx})}\\\\ &{=N\\int F(\\mathbf{x})(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\mathcal{B}^{N}})-\\displaystyle\\int\\sum_{i=1}^{N}\\frac{\\delta F}{\\delta\\mu}\\langle\\mu\\rangle(x^{i})(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathrm{dx})}\\\\ &{=N\\int\\bigg(F(\\mathbf{x})-\\Big\\langle\\frac{\\delta F}{\\delta\\mu}(\\mu),\\mu_{\\mathbf{x}}\\Big\\rangle\\bigg)\\,(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\mathcal{B}^{N}})}\\\\ &{=N\\int\\bigg(F(\\mathbf{x})-F(\\mu)-\\Big\\langle\\frac{\\delta F}{\\delta\\mu}(\\mu),\\mu_{\\mathbf{x}}-\\mu\\Big\\rangle\\bigg)\\,(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\mathcal{B}^{N}})}\\\\ &{=N\\int\\bigg(F(\\mu_{\\mathbf{x}},\\mu)(\\mu^{(N)}-\\hat{\\mu}^{\\mathcal{B}^{N}})(\\mathbf{dx})+\\lambda\\mathrm{KL}(\\mu^{(N)}\\|\\hat{\\mu}^{\\mathcal{B}^{N}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof of Proposition 2. Let $Z_{F}(\\mu)$ be a normalizing factor in RHS of (19), that is, ", "page_idx": 11}, {"type": "equation", "text": "$$\nZ_{F}(\\mu)=\\int\\exp\\left(-\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\right)\\hat{\\mu}^{\\otimes N}(\\mathrm{d}\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "By the Jensen\u2019s inequality, we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\log Z_{F}(\\mu)\\geq-\\int\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\hat{\\mu}^{\\otimes N}(\\mathrm{d}\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mu_{*}^{(N)}||\\hat{\\mu}^{\\otimes N})=\\int\\mu_{*}^{(N)}(\\mathrm{d}\\mathbf{x})\\log\\frac{\\mathrm{d}\\mu_{*}^{(N)}}{\\mathrm{d}\\hat{\\mu}^{\\otimes N}}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\int\\mu_{*}^{(N)}(\\mathrm{d}\\mathbf{x})\\log\\frac{\\exp\\left(-\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\right)}{Z_{F}(\\mu)}}\\\\ &{=\\displaystyle-\\int\\frac{N}{\\lambda}B_{F}(\\mu_{\\mathbf{x}},\\mu)\\mu_{*}^{(N)}(\\mathrm{d}\\mathbf{x})-\\log Z_{F}(\\mu)}\\\\ &{\\leq\\displaystyle\\frac{N}{\\lambda}\\int B_{F}(\\mu_{\\mathbf{x}},\\mu)(\\hat{\\mu}^{\\otimes N}-\\mu_{*}^{(N)})(\\mathrm{d}\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now we are ready to prove Theorem 1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . As discussed in Section 4, the evaluation of $\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]$ completes the proof. For any function $G:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ so that the following integral is well defined, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\int\\left\\langle G,\\mu_{\\mathbf{x}}\\right\\rangle\\mu_{*}^{\\otimes N}(\\mathrm{d}\\mathbf{x})=\\int\\frac{1}{N}\\sum_{i=1}^{N}G(x^{i})\\prod_{i=1}^{N}\\mu_{*}(\\mathrm{d}x^{i})=\\int G(x)\\mu_{*}(\\mathrm{d}x)=\\left\\langle G,\\mu_{*}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Applying this equality with $\\begin{array}{r}{G(x)=\\frac{\\delta F}{\\delta\\mu}(\\mu_{*})(x),G(x)=\\|x\\|_{2}^{2}}\\end{array}$ , and $G(x)=h(x,z)$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\left\\langle\\frac{\\delta F}{\\delta\\mu}(\\mu_{*}),\\mu_{\\mathbf{x}}-\\mu_{*}\\right\\rangle\\mu_{*}^{\\otimes N}(\\mathrm{d}\\mathbf{x})=0,}\\\\ &{\\int\\left(\\mathbb{E}_{X\\sim\\mu_{\\mathbf{x}}}[\\|X\\|_{2}^{2}]-\\mathbb{E}_{X\\sim\\mu_{*}}[\\|X\\|_{2}^{2}]\\right)\\mu_{*}^{\\otimes N}(\\mathrm{d}\\mathbf{x})=0,}\\\\ &{\\int h_{\\mu_{\\mathbf{x}}}(z)\\mu_{*}^{\\otimes N}(\\mathrm{d}\\mathbf{x})=h_{\\mu_{*}}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, we can upper bound the Bregman divergence as follows. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta}}\\mathbf{\\mathcal{B}}^{n}\\left[\\boldsymbol{B}_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta,n}}\\Bigg[F(\\mu_{\\mathbf{X}})-F(\\mu_{*})-\\left\\langle\\frac{\\delta F}{\\delta\\mu}(\\mu_{*}),\\mu_{\\mathbf{X}}-\\mu_{*}\\right\\rangle\\Bigg]}\\\\ &{=N\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta,n}}\\left[F_{0}(\\mu_{\\mathbf{X}})-F_{0}(\\mu_{*})\\right]}\\\\ &{=\\frac{N}{n}\\sum_{j=1}^{N}\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta,n}}\\left[\\boldsymbol{\\ell}(h_{\\mu_{\\mathbf{X}}}(z_{j}),y_{j})-\\boldsymbol{\\ell}(h_{\\mu_{*}}(z_{j}),y_{j})\\right]}\\\\ &{\\leq\\frac{N}{n}\\sum_{j=1}^{N}\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta,n}}\\left[\\frac{\\partial\\boldsymbol{\\ell}(a,y_{j})}{\\partial a}\\biggr|_{a=h_{\\mu_{*}}(z_{j})}(h_{\\mu_{\\mathbf{X}}}(z_{j})-h_{\\mu_{*}}(z_{j}))+\\frac{L}{2}|h_{\\mu_{\\mathbf{X}}}(z_{j})-h_{\\mu_{*}}(z_{j})|^{2}\\right]}\\\\ &{=\\frac{L N}{2n}\\sum_{j=1}^{N}\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{0}^{\\delta N}}\\left[|h_{\\mu_{\\mathbf{X}}}(z_{j})-h_{\\mu_{*}}(z_{j})|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The last term is a variance of $h_{\\mu\\mathbf{x}}(z)\\left(z\\in\\mathcal{Z}\\right)$ ; hence we simply evaluate it as follows. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[\\left|h_{\\mu_{\\mathbf{X}}}(z)-h_{\\mu_{*}}(z)\\right|^{2}\\right]=\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[\\left|\\frac{1}{N}\\sum_{i=1}^{N}h(X_{i},z)-\\mathbb{E}_{X\\sim\\mu_{*}}[h(X,z)]\\right|^{2}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[\\frac{1}{N^{2}}\\sum_{i=1}^{N}|h(X_{i},z)-\\mathbb{E}_{X\\sim\\mu_{*}}[h(X,z)]|^{2}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\mathbb{E}_{\\mathbf{X}\\sim\\mu_{*}^{\\otimes N}}\\left[\\frac{1}{N^{2}}\\sum_{i=1}^{N}|h(X_{i},z)|^{2}\\right]}\\\\ &{\\leq\\frac{R^{2}}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\nN\\mathbb{E}_{{\\mathbf{X}}\\sim\\mu_{*}^{\\otimes N}}\\left[B_{F}(\\mu_{\\mathbf{X}},\\mu_{*})\\right]\\leq\\frac{L R^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining Theorem 3, we conclude ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\mathcal{L}(\\mu_{*})\\leq\\frac{L R^{2}}{2N}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Convergence of Mean-field Langevin dynamics in the discrete-setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove the convergence rate of MFLD (12). We first provide the following lemma which shows the uniform boundedness of the second moment of iterations. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. Under Assumption $^{\\,l}$ and $\\eta\\lambda^{\\prime}<1/2$ , we run discrete mean-field Langevin dynamics (12). Then we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|X_{k}^{i}\\|_{2}^{2}]\\leq\\mathbb{E}\\left[\\left\\|X_{0}^{i}\\right\\|_{2}^{2}\\right]+\\frac{1}{\\lambda^{\\prime}}\\left(\\frac{M_{1}^{2}}{4\\lambda^{\\prime}}+\\lambda d\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Using the inequality $\\begin{array}{r}{(a+b)^{2}\\le\\left(1+\\gamma\\right)a^{2}+\\left(1+\\frac{1}{\\gamma}\\right)b^{2}}\\end{array}$ with $\\begin{array}{r}{\\gamma=\\frac{2\\eta\\lambda^{\\prime}}{1-2\\eta\\lambda^{\\prime}}>0}\\end{array}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|X_{k+1}^{i}\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|X_{k}^{i}-\\eta\\nabla\\frac{\\delta F(t\\mathbf{x}_{k})}{\\delta\\mu}(X_{k}^{i})+\\sqrt{2\\lambda\\eta\\kappa}\\mathbb{E}_{k}^{i}\\right\\|_{2}^{2}\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\left\\|(1-2\\eta\\lambda^{\\prime})X_{k}^{i}-\\eta\\nabla\\frac{\\delta F(t\\mathbf{x}_{k})}{\\delta\\mu}(X_{k}^{i})+\\sqrt{2\\lambda\\eta\\kappa}\\mathbb{E}_{k}^{i}\\right\\|_{2}^{2}\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\left\\|(1-2\\eta\\lambda^{\\prime})X_{k}^{i}-\\eta\\nabla\\frac{\\delta F(t\\mathbf{x}_{k})}{\\delta\\mu}(X_{k}^{i})\\right\\|_{2}^{2}+2\\lambda\\eta\\left\\|\\xi_{k}^{i}\\right\\|_{2}^{2}\\right]}&{}\\\\ {=\\mathbb{E}\\left[\\left((1-2\\eta\\lambda^{\\prime})\\left\\|X_{k}^{i}\\right\\|_{2}+\\eta\\Lambda\\right)^{2}\\right]+2\\lambda\\eta\\,d}&{}\\\\ {\\leq(1+\\gamma)(1-2\\eta\\lambda^{\\prime})^{2}\\mathbb{E}\\left[\\left\\|X_{k}^{i}\\right\\|_{2}^{2}\\right]+\\left(1+\\frac{1}{\\gamma}\\right)\\eta^{2}M_{1}^{2}+2\\lambda\\eta d}\\\\ {=(1-2\\eta\\lambda^{\\prime})\\mathbb{E}\\left[\\left\\|X_{k}^{i}\\right\\|_{2}^{2}\\right]+\\eta\\left(\\frac{M_{1}^{2}}{2\\lambda^{\\prime}}+2\\lambda d\\right).}&{}\\\\ {\\mathbb{E}\\left[\\left\\|X_{k}^{i}\\right\\|_{2}^{2}\\right]\\leq(1-2\\eta\\lambda^{\\prime})^{2}\\mathbb{E}\\left[\\left\\|X_{k}^{i}\\right\\|_{2}^{2}\\right]+\\frac{1}{\\gamma}\\left(\\frac{M_{1}^{2}}{\\delta\\mu}+\\lambda d\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we prove Theorem 2 that is basically an extension of one-step interpolation argument in Vempala and Wibisono (2019). ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 2. The convergence rate in the continuous-time setting is a direct consequence of Theorem 1 and the convergence of the Langevin dynamics: $\\mathcal{L}^{(N)}(\\mu_{t}^{(N)})\\to\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})$ based on LSI (Vempala and Wibisono, 2019). ", "page_idx": 13}, {"type": "text", "text": "Next, we prove the convergence rate in the discrete-time setting. We consider the one-step interpolation for $k$ -th iteration: $\\begin{array}{r}{X_{k+1}^{i}=X_{k}^{i}-\\eta\\nabla\\frac{\\delta F(\\mu\\mathbf{x}_{k})}{\\delta\\mu}(X_{k}^{i})+\\sqrt{2\\lambda\\eta}\\xi_{k}^{i}}\\end{array}$ \u2207\u03b4F (\u03b4\u00b5\u00b5Xk )(Xki) + \u221a2\u03bb\u03b7\u03beik, (i \u2208{1, 2, . . . , d}). To do so, let us consider the following stochastic differential equation: for $i\\in\\{1,2,\\ldots,d\\}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}Y_{t}^{i}=-\\nabla\\frac{\\delta F({\\mu}_{\\mathbf{Y}_{0}})}{\\delta\\mu}(Y_{0}^{i})\\mathrm{d}t+\\sqrt{2\\lambda}\\mathrm{d}W_{t},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ${\\bf Y}_{0}=(Y_{0}^{1},\\ldots,Y_{0}^{d})=(X_{k}^{1},\\ldots,X_{k}^{d})$ and $W_{t}$ is the standard Brownian motion in $\\mathbb{R}^{d}$ with $W_{0}=0$ . Then, the following step (26) is the solution of this equation, starting from $\\mathbf{Y}_{0}$ , at time $t$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nY_{t}^{i}=Y_{0}^{i}-t\\nabla\\frac{\\delta F(\\mu\\mathbf{r}_{0})}{\\delta\\mu}(Y_{0}^{i})+\\sqrt{2\\lambda t}\\xi^{i},\\;(i\\in\\{1,2,\\ldots,d\\}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\xi^{i}\\sim\\mathcal{N}(0,I_{d})\\;(i\\in\\{1,\\ldots,d\\})$ are i.i.d. standard Gaussian random variables. ", "page_idx": 14}, {"type": "text", "text": "In this proof, we identify the probability distribution with its density function with respect to the Lebesgure measure for notational simplicity. For instance, we denote by $\\mu_{*}^{(N)}(\\mathbf{y})$ the density of $\\mu_{*}^{(N)}$ . We denote by $\\nu_{0t}(\\mathbf{y}_{0},\\mathbf{y}_{t})$ the joint probability distribution of $(\\mathbf{Y}_{0},\\mathbf{Y}_{t})$ for time $t$ , and by $\\nu_{t|0},\\,\\nu_{0|t}$ and $\\nu_{0},\\ \\nu_{t}$ the conditional and marginal distributions. Then, we see $\\nu_{0}\\,=\\,\\mu_{k}^{(N)}(=\\,\\mathrm{Law}(\\mathbf{X}_{k}))$ , \u03bd\u03b7 = \u00b5(kN+)1(= Law(Xk+1)) (i.e., Y\u03b7 d= Xk+1), and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu_{0t}(\\mathbf{y}_{0},\\mathbf{y}_{t})=\\nu_{0}(\\mathbf{y}_{0})\\nu_{t|0}(\\mathbf{y}_{t}|\\mathbf{y}_{0})=\\nu_{t}(\\mathbf{y}_{t})\\nu_{0|t}(\\mathbf{y}_{0}|\\mathbf{y}_{t}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The continuity equation of $\\nu_{t|0}$ conditioned on $\\mathbf{y}_{0}$ is given as follows (Vempala and Wibisono, 2019): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial\\nu_{t|0}(\\mathbf{y}|\\mathbf{y}_{0})}{\\partial t}=\\nabla\\cdot\\left(\\nu_{t|0}(\\mathbf{y}|\\mathbf{y}_{0})N\\nabla F(\\mathbf{y}_{0})\\right)+\\lambda\\Delta\\nu_{t|0}(\\mathbf{y}|\\mathbf{y}_{0}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we write $F(\\mathbf{y}_{0})=F(\\mu_{\\mathbf{y}_{0}})$ and hence $\\begin{array}{r}{N\\nabla_{y^{i}}F({\\mathbf y}_{0})=\\nabla\\frac{\\delta F(\\mu_{\\mathbf{y}_{0}})}{\\delta\\mu}(y_{0}^{i})}\\end{array}$ \u2207\u03b4F (\u03b4\u00b5\u00b5y0)(yi0). Therefore, we obtain the continuity equation of $\\nu_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\frac{\\partial\\nu_{t}(\\ensuremath{\\mathbf{y}})}{\\partial t}}=\\int\\ensuremath{\\frac{\\partial\\nu_{t}(\\ensuremath{\\mathbf{y}})}{\\partial t}}\\nu_{0}(\\ensuremath{\\mathbf{y}}_{0})\\ensuremath{\\mathrm{d}}\\ensuremath{\\mathbf{y}}_{0}}\\\\ &{\\qquad\\quad=\\int\\left(\\nabla\\cdot(\\nu_{0t}(\\ensuremath{\\mathbf{y}}_{0},\\ensuremath{\\mathbf{y}})N\\nabla F(\\ensuremath{\\mathbf{y}}_{0}))+\\lambda\\Delta\\nu_{0t}(\\ensuremath{\\mathbf{y}}_{0},\\ensuremath{\\mathbf{y}})\\right)\\ensuremath{\\mathrm{d}}\\ensuremath{\\mathbf{y}}_{0}}\\\\ &{\\qquad\\quad=\\nabla\\cdot\\left(\\nu_{t}(\\ensuremath{\\mathbf{y}})\\int\\nu_{0\\mid t}(\\ensuremath{\\mathbf{y}}_{0}\\vert\\ensuremath{\\mathbf{y}})N\\nabla F(\\ensuremath{\\mathbf{y}}_{0})\\ensuremath{\\mathrm{d}}\\ensuremath{\\mathbf{y}}_{0}\\right)+\\lambda\\Delta\\nu_{t}(\\ensuremath{\\mathbf{y}})}\\\\ &{\\qquad\\quad=\\nabla\\cdot\\left(\\nu_{t}(\\ensuremath{\\mathbf{y}})\\left(\\mathbb{E}_{\\ensuremath{\\mathbf{y}}_{0}\\mid t}\\left[N\\nabla F(\\ensuremath{\\mathbf{y}}_{0})\\middle\\vert\\mathbf{Y}_{t}=\\ensuremath{\\mathbf{y}}\\right]+\\lambda\\nabla\\log\\nu_{t}(\\ensuremath{\\mathbf{y}})\\right)\\right)}\\\\ &{\\qquad\\quad=\\lambda\\nabla\\cdot\\left(\\nu_{t}(\\ensuremath{\\mathbf{y}})\\nabla\\log{\\frac{\\nu_{t}}{\\mu_{*}^{(N)}}}(\\ensuremath{\\mathbf{y}})\\right)}\\\\ &{\\qquad\\quad+\\nabla\\cdot\\left(\\nu_{t}(\\ensuremath{\\mathbf{y}})\\left(\\mathbb{E}_{\\ensuremath{\\mathbf{y}}_{0}\\mid t}\\left[N\\nabla F(\\ensuremath{\\mathbf{y}}_{0})\\right]\\ensuremath{\\mathbf{Y}_{t}}=\\ensuremath{\\mathbf{y}}\\right]-N\\nabla F(\\ensuremath{\\mathbf{y}})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For simplicity, we write $\\delta_{t}(\\cdot)=\\mathbb{E}_{\\mathbf{Y}_{0}|}\\left[N\\nabla F(\\mathbf{Y}_{0})|\\mathbf{Y}_{t}=\\cdot\\right]-N\\nabla F(\\cdot)$ . By LSI inequality (Assumption 4) and Eq. (27), for $0\\leq t\\leq\\eta$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\hat{c}_{1}^{\\mathrm{SD}}}{\\hat{d}t}(n)=\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\hat{\\Pi}_{\\mathcal{F}}(y)}&{=\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}\\,}\\\\ &{\\quad-\\Delta\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\Pi_{\\mathcal{F}}\\left(-(v)(\\hat{\\mathcal{H}}\\times\\hat{\\mathcal{F}})\\right)\\,d y}\\\\ &{\\quad+\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\Pi_{\\mathcal{F}}\\left(-(v)(\\hat{\\mathcal{H}}\\times\\hat{\\mathcal{F}})\\right)\\,d y}\\\\ &{\\quad-\\Delta\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\Pi_{\\mathcal{F}}\\nabla u_{\\theta}\\,\\hat{\\mathcal{H}}(y)}\\\\ &{\\quad-\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)}\\\\ &{\\quad-\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)}\\\\ &{\\quad-\\Delta^{2}\\int_{0}^{+\\infty}\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}\\,\\left[\\Pi_{\\mathcal{F}}\\frac{\\hat{\\mathcal{H}}^{\\mathrm{SD}(n)}}{\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}}(y)\\right]\\,d y}\\\\ &{\\quad-\\int_{0}^{+\\infty}\\left(\\Pi_{\\mathcal{F}}\\right)\\sqrt{\\Pi_{\\mathcal{F}}}\\frac{\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}}{\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}}(y)\\,\\hat{\\mathcal{L}}_{n}^{\\mathrm{GSD}(n)}(y)\\,d y}\\\\ &{\\quad- \n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq-\\frac{\\lambda^{2}}{2}\\int\\nu_{t}(\\mathbf{y})\\left\\|\\nabla\\log\\frac{\\nu_{t}}{\\mu_{*}^{(N)}}(\\mathbf{y})\\right\\|_{2}^{2}\\mathrm{d}\\mathbf{y}+\\frac{N^{2}}{2}\\mathbb{E}_{(\\mathbf{Y}_{0},\\mathbf{Y})\\sim\\nu_{0t}}\\left[\\left\\|\\nabla F(\\mathbf{Y}_{0})-\\nabla F(\\mathbf{Y})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\leq-\\bar{\\alpha}\\lambda^{2}\\mathrm{KL}(\\nu_{t}\\|\\mu_{*}^{(N)})+\\frac{N^{2}}{2}\\mathbb{E}_{(\\mathbf{Y}_{0},\\mathbf{Y})\\sim\\nu_{0t}}\\left[\\left\\|\\nabla F(\\mathbf{Y}_{0})-\\nabla F(\\mathbf{Y})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle=-\\bar{\\alpha}\\lambda\\left(\\mathcal{L}^{(N)}(\\nu_{t})-\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})\\right)+\\frac{N^{2}}{2}\\mathbb{E}_{(\\mathbf{Y}_{0},\\mathbf{Y})\\sim\\nu_{0t}}\\left[\\left\\|\\nabla F(\\mathbf{Y}_{0})-\\nabla F(\\mathbf{Y})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we bound the last term as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{T}\\mathcal{R}_{n}\\times\\mathcal{V}_{n+1}\\sim\\left(\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\mathcal{R}_{n}\\right)}\\\\ &{=\\mathbb{E}_{\\displaystyle\\mathbb{R}_{n}\\times\\mathcal{V}_{n+1}}\\left[\\displaystyle\\sum_{i=1}^{n}\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\right]}\\\\ &{\\le\\mathbb{E}_{\\displaystyle\\mathbb{R}_{n}\\times\\mathcal{V}_{n+1}}\\left[\\displaystyle\\sum_{i=1}^{n}\\left|\\left(\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\displaystyle{\\mathrm{B}_{i}}^{(i)}-\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\right)(\\displaystyle{\\mathrm{P}}_{n}^{*})\\right]\\right]}\\\\ &{\\le\\mathbb{E}_{\\displaystyle\\mathbb{R}_{n}\\times\\mathcal{V}_{n+1}}\\left[\\displaystyle\\sum_{s=1}^{n}\\left|\\left(\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\displaystyle{\\mathrm{B}_{i}}^{(i)}\\displaystyle{\\mathrm{R}_{n}}^{(i)}-\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\displaystyle{\\mathrm{B}_{i}}^{(i)}\\right)(\\displaystyle{\\mathrm{P}}_{n}^{*})\\right]}\\\\ &{\\le\\mathbb{E}_{\\displaystyle\\mathrm{R}_{n}\\times\\mathcal{V}_{n+1}}\\left[\\displaystyle\\sum_{s=1}^{n}\\left|\\left(\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\displaystyle{\\mathrm{B}_{i}}^{(i)}-\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\right)\\right]}\\\\ &{\\le\\mathbb{E}_{\\displaystyle\\mathrm{R}_{n}\\times\\mathcal{V}_{n+1}}^{(i)+1}\\alpha^{2}\\mathbb{E}_{\\displaystyle\\mathbb{R}_{n}\\times\\mathcal{V}_{n+1}}\\left[\\displaystyle\\sum_{i=1}^{n}\\left|\\sum_{i=1}^{n}\\left|\\frac{\\displaystyle{\\mathrm{T}}}{\\displaystyle{\\mathrm{Si}}}\\right|^{n}\\right]}\\\\ &{\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore for any $t\\in[0,\\eta]$ , we see $N^{2}\\mathbb{E}_{(\\mathbf{Y}_{0},\\mathbf{Y})\\sim\\nu_{0t}}\\left[\\left\\lVert\\nabla F(\\mathbf{Y}_{0})-\\nabla F(\\mathbf{Y})\\right\\rVert_{2}^{2}\\right]\\,\\le\\,N\\delta_{\\eta}^{(N)}$ , where $\\begin{array}{r}{\\delta_{\\eta}^{(N)}=16\\eta(M_{2}^{2}+\\lambda^{\\prime2})(\\eta M_{1}^{2}+\\lambda d)+64\\eta^{2}\\lambda^{\\prime2}(M_{2}^{2}+\\bar{\\lambda}^{\\prime2})\\left(\\frac{1}{N}\\mathbb{E}\\left[\\left\\|\\mathbf{X}_{0}\\right\\|_{2}^{2}\\right]+\\frac{\\bar{1}}{\\lambda^{\\prime}}\\left(\\frac{M_{1}^{2}}{4\\lambda^{\\prime}}+\\lambda d\\right)\\right).}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Substituting this bound into Eq. (30), we get for $t\\in[0,\\eta]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left({\\mathcal L}^{(N)}(\\nu_{t})-{\\mathcal L}^{(N)}(\\mu_{*}^{(N)})-\\frac{N\\delta_{\\eta}^{(N)}}{2\\bar{\\alpha}\\lambda}\\right)\\leq-\\bar{\\alpha}\\lambda\\left({\\mathcal L}^{(N)}(\\nu_{t})-{\\mathcal L}(\\mu_{*}^{(N)})-\\frac{N\\delta_{\\eta}^{(N)}}{2\\bar{\\alpha}\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Noting $\\nu_{\\eta}=\\mu_{k+1}^{(N)}$ and $\\nu_{0}=\\mu_{k}^{(N)}$ , the Gronwall\u2019s inequality leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\tau}^{(N)}(\\mu_{k+1}^{(N)})-\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\frac{N\\delta^{(N)}\\eta}{2\\bar{\\alpha}\\lambda}\\leq\\exp(-\\bar{\\alpha}\\lambda\\eta)\\left(\\mathcal{L}^{(N)}(\\mu_{k}^{(N)})-\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\frac{N\\delta^{(N)}\\eta}{2\\bar{\\alpha}\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This inequality holds at every iteration of (25). Hence, we arrive at the desired result, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ddot{z}^{(N)}(\\mu_{k}^{(N)})-\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})\\leq\\frac{N\\delta^{(N)}\\eta}{2\\bar{\\alpha}\\lambda}+\\exp(-\\bar{\\alpha}\\lambda\\eta k)\\left(\\mathcal{L}^{(N)}(\\mu_{0}^{(N)})-\\mathcal{L}^{(N)}(\\mu_{*}^{(N)})-\\frac{N\\delta^{(N)}\\eta}{2\\bar{\\alpha}\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Auxiliary results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we showcase auxiliary results used in our theory. ", "page_idx": 16}, {"type": "text", "text": "LSI on \u00b5\u2217 $\\mu_{*}^{(N)}$ can be verified by using the following two lemmas. Lemma 2 says that the strong log-concave densities satisfy the LSI with a dimension-free constant. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2 (Bakry and \u00c9mery (1985)). Let $\\begin{array}{r}{\\frac{\\mathrm{d}\\mu(x)}{\\mathrm{d}x}\\propto\\exp(-f(x))}\\end{array}$ be a probability density, where $f:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is a smooth function. If there exists $c>0$ such that $\\nabla^{2}f\\,\\succeq\\,c I_{d},$ then $\\mu$ satisfies log-Sobolev inequality with constant c. ", "page_idx": 16}, {"type": "text", "text": "Additionally, the LSI is preserved under bounded perturbation as seen in Lemma 3. ", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Holley and Stroock (1987)). Let \u00b5 be a probability distribution on $\\mathbb{R}^{d}$ satisfying the logSobolev inequality with a constant $\\alpha$ . For a bounded function $B:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ , we define a probability distribution $\\mu_{B}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}\\mu_{B}(x)}{\\mathrm{d}x}}={\\frac{\\exp(B(x))}{\\mathbb{E}_{X\\sim\\mu}[\\exp(B(X))]}}{\\frac{\\mathrm{d}\\mu(x)}{\\mathrm{d}x}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, $\\mu_{B}$ satisfies the log-Sobolev inequality with a constant $\\alpha/\\exp(4\\|B\\|_{\\infty})$ . ", "page_idx": 16}, {"type": "text", "text": "Theorems 4 and 5 give convergence rates of the infinite-particle MFLDs in continuous- and discretetime settings. ", "page_idx": 16}, {"type": "text", "text": "Theorem 4 (Nitanda et al. (2022)). Under Assumptions $^{\\,I}$ and 2, we run the infinite-particle MFLD (7) in the continuous-time setting. Then, it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\mu_{t})-\\mathcal{L}(\\mu_{*})\\leq\\exp(-2\\alpha\\lambda t)\\left(\\mathcal{L}(\\mu_{0})-\\mathcal{L}(\\mu_{*})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For MFLD (17), we consider one-step interpolation: for $0\\leq t\\leq\\eta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nX_{k,t}=X_{k}-t\\nabla{\\frac{\\delta F(\\mu_{k})}{\\delta\\mu}}(X_{k})+{\\sqrt{2\\lambda t}}\\xi_{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{\\mu_{k},t}=\\mathbb{E}_{(X_{k},X_{k,t})}\\left[\\left\\|\\nabla\\frac{\\delta F(\\mu_{k})}{\\delta\\mu}(X_{k})-\\nabla\\frac{\\delta F(\\mu_{k,t})}{\\delta\\mu}(X_{k,t})\\right\\|_{2}^{2}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem 5 (Nitanda et al. (2022)). Under Assumptions $^{\\,I}$ and 2, we run the infinite-particle MFLD (17) in the discrete-time setting with the step size $\\eta$ . Suppose there exists a constant $\\delta_{\\eta}$ such that $\\delta_{\\mu_{k},t}\\le\\delta_{\\eta}$ for any $0<t\\leq\\eta$ . Then, it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mu_{k})-\\mathcal{L}(\\mu_{*})\\leq\\frac{\\delta_{\\eta}}{2\\alpha\\lambda}+\\exp(-\\alpha\\lambda\\eta k)\\left(\\mathcal{L}(\\mu_{0})-\\mathcal{L}(\\mu_{*})\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Under Assumption 1, we can evaluate $\\delta_{\\mu_{k},t}$ in a similar way as the proof of Theorem 2 and we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\delta_{\\eta}=8\\eta(M_{2}^{2}+\\lambda^{\\prime2})(2\\eta M_{1}^{2}+2\\lambda d)+32\\eta^{2}\\lambda^{\\prime2}(M_{2}^{2}+\\lambda^{\\prime2})\\left(\\mathbb{E}\\left[\\|X_{0}\\|_{2}^{2}\\right]+\\frac{1}{\\lambda^{\\prime}}\\left(\\frac{M_{1}^{2}}{4\\lambda^{\\prime}}+\\lambda d\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The next theorem gives a relationship between LSI and Talagrand\u2019s inequalities. ", "page_idx": 16}, {"type": "text", "text": "Theorem 6 (Otto and Villani (2000)). If a probability distribution $\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ satisfies the logSobolev inequality with constant $\\alpha>0$ , then $\\mu$ satisfies Talagrand\u2019s inequality with the same constant: for any $\\mu^{\\prime}\\in\\mathcal{P}_{2}(\\dot{\\mathbb{R}}^{d})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{2}W_{2}^{2}(\\mu^{\\prime},\\mu)\\leq\\mathrm{KL}(\\mu^{\\prime}||\\mu),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $W_{2}(\\mu^{\\prime},\\mu)$ denotes the 2-Wasserstein distance ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The contributions are clearly stated in Section 1 with the pointers to corresponding theorems and corollaries. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The limitation of the work is addressed in Conclusion and Discussion. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our theoretical assumptions are clearly stated. Omitted proofs are given in Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethic ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: This paper follows NeurIPS code of ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is no societal impact since our analysis targets existing methods. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]