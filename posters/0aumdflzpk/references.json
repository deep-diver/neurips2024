{"references": [{"fullname_first_author": "Christopher Berner", "paper_title": "Dota 2 with large scale deep reinforcement learning", "publication_date": "2019-12-06", "reason": "This paper is a seminal work in deep reinforcement learning applied to complex video games, setting the stage for many subsequent visual RL approaches."}, {"fullname_first_author": "David Bertoin", "paper_title": "Saliency-guided Q-networks for generalization in visual reinforcement learning", "publication_date": "2022-XX-XX", "reason": "This paper directly addresses the problem of generalization in visual RL using saliency maps, which is a core issue in the target paper."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-XX-XX", "reason": "Soft Actor-Critic (SAC) is a widely used off-policy RL algorithm and is the baseline algorithm used in the target paper."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Playing Atari with deep reinforcement learning", "publication_date": "2013-12-XX", "reason": "This paper is a foundational work in deep reinforcement learning using visual inputs, demonstrating the potential of deep learning for solving complex tasks from raw pixel data."}, {"fullname_first_author": "Denis Yarats", "paper_title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels", "publication_date": "2021-XX-XX", "reason": "This paper introduces important data augmentation techniques for improving the robustness of visual RL agents, which are directly compared and contrasted with the proposed methods in the target paper."}]}