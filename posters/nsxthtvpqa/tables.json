[{"figure_path": "NsxthTVpqA/tables/tables_3_1.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the results of Visual Question Answering experiments using three different Vision Language Models (VLMs): LLaVA-1.5, LLaVA-NeXT, and MGM/MGM-HD.  The experiments are conducted with and without the Contrastive Alignment (CAL) method proposed in the paper. The table shows the performance on various benchmarks, including Doc, Chart, and Text,  with and without OCR tokens, across different resolutions and model sizes.  Abbreviations for the benchmarks are provided. The authors' results are clearly marked.", "section": "3.2 Main Results"}, {"figure_path": "NsxthTVpqA/tables/tables_4_1.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the performance comparison of different Vision Language Models (VLMs) on various Visual Question Answering (VQA) benchmark datasets.  It showcases the impact of Contrastive Alignment (CAL), a method proposed in the paper, on improving the performance of these models. The table shows results for different LLMs (Large Language Models) at different resolutions (low and high) and with/without CAL.  Various metrics are used for evaluation across different VQA tasks.", "section": "3.2 Main Results"}, {"figure_path": "NsxthTVpqA/tables/tables_5_1.jpg", "caption": "Table 2: Image captioning and visual grounding benchmarks on LLaVA-1.5, LLaVA-NeXT, and MGM/MGM-HD2. Our results are marked with", "description": "This table presents the performance comparison of different Vision Language Models (VLMs) on image captioning and visual grounding benchmarks.  The VLMs are tested on COCO Caption, TextCaps, and RefCOCOg datasets.  The results show the performance improvement achieved by using Contrastive Alignment (CAL).  The table is split into two sections: low-resolution and high-resolution settings. Each row shows the performance of a specific VLM with and without CAL, along with the model size (LLM) and dataset used.", "section": "3.2 Main Results"}, {"figure_path": "NsxthTVpqA/tables/tables_6_1.jpg", "caption": "Table 3: Performance difference when CAL is applied at different training stages.", "description": "This table presents the performance difference observed when Contrastive Alignment (CAL) is applied at various stages during the training process of a Vision Language Model (VLM).  Specifically, it shows the results for different benchmarks (VQADoc, TextCaps, MMT, OCRB) when CAL is integrated only in the instruction tuning (IT) stage, only in the pre-training (PT) stage, and when it's included in both stages.  It demonstrates the relative contribution of each training stage and the cumulative impact when CAL is applied to both.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/tables/tables_6_2.jpg", "caption": "Table 4: Performance difference when applying different weights [\u03b1, \u03b2] for clamping.", "description": "This table presents the performance difference in various benchmarks (VQA Doc, VQA Chart, OCRB, Refcocogval) when different clamping weight ranges ([\u03b1, \u03b2]) are applied during the Contrastive Alignment (CAL) process. It shows how the choice of clamping weights affects the final results, with [1,5] showing the best overall performance compared to the baseline and other weight configurations.  The results highlight the sensitivity of CAL to the choice of hyperparameters and the importance of finding appropriate values for optimal performance.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/tables/tables_8_1.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the performance comparison of different Vision Language Models (VLMs) on various Visual Question Answering (VQA) benchmarks.  The VLMs tested include LLaVA-1.5, LLaVA-NeXT, and MGM/MGM-HD, both with and without the Contrastive Alignment (CAL) method. The results are categorized by resolution (low and high) and LLM used (Gemma-2B, Vicuna-7B, Vicuna-13B).  The table shows the performance improvements achieved by CAL on these benchmarks, using several metrics.", "section": "3.1 Experimental Setup"}, {"figure_path": "NsxthTVpqA/tables/tables_16_1.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the performance of different Vision Language Models (VLMs) on various Visual Question Answering (VQA) benchmarks.  It compares the performance of several state-of-the-art models (MGM, LLaVA-1.5, LLaVA-NeXT) both with and without the Contrastive Alignment (CAL) method proposed in the paper. The results are broken down by model size (Vicuna-7B, Vicuna-13B), resolution (low and high), and specific VQA sub-tasks (Doc, Chart, Text, etc.).  The table highlights the consistent improvements achieved by CAL across different model types and sizes.", "section": "3.2 Main Results"}, {"figure_path": "NsxthTVpqA/tables/tables_16_2.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the performance of several vision-language models (VLMs) on various visual question answering benchmarks.  It compares the baseline performance of three leading VLMs (LLaVA-1.5, LLaVA-NeXT, and MGM/MGM-HD) against their performance after applying the Contrastive Alignment (CAL) method.  The results are broken down by different LLM backbones (Gemma-2B, Vicuna-7B, Vicuna-13B), resolution settings (low and high), and benchmark types (Doc, Chart, Text, Text*, SQA, MMS, MMT, OCRB).  The \"Text*\" column indicates results where OCR tokens were not used in evaluation.  The table demonstrates the consistent improvement in performance across various VLMs and benchmarks after incorporating CAL.", "section": "3.1 Experimental Setup"}, {"figure_path": "NsxthTVpqA/tables/tables_17_1.jpg", "caption": "Table 8: Ablations for contrasting image conditions on Visual Question Answering benchmarks using LLaVA-NeXT/13B.", "description": "This table presents the ablation study on different image contrasting methods for visual question answering tasks using the LLaVA-NeXT/13B model.  It shows the performance variations of the model with and without CAL (Contrastive Alignment) under different image masking techniques (random patch masking and Gaussian blurring) and different masking ratios. The results are presented in terms of various metrics for different visual question answering benchmarks, including MMS, MMT, SQA, Text, Text*, Doc, Chart, and OCRB.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/tables/tables_17_2.jpg", "caption": "Table 9: Ablations for image contrasting conditions on image captioning and visual grounding benchmarks using LLaVA-NeXT/13B.", "description": "This table presents ablation studies on the impact of different image contrasting methods on the performance of LLaVA-NeXT-13B for image captioning and visual grounding tasks.  It shows how the model's performance varies when using different masking ratios (0.5, 0.7, 0.9) and Gaussian blurring (\u03c3=1 and \u03c3=10) during the contrasting process. The results are evaluated using COCO Caption, TextCaps, Refcocogval, and Refcocogtest metrics.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/tables/tables_18_1.jpg", "caption": "Table 10: Comparison of benchmarks with and without Average Pooling.", "description": "This table presents ablation study results on the impact of average pooling in the proposed CAL method. It compares the performance of CAL with and without average pooling on various benchmarks, including ChartVQA, DocVQA, SQA, COCO Caption, TextCaps, OCRB, and Refcocog_val. The results demonstrate that average pooling slightly improves performance across these benchmarks.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/tables/tables_18_2.jpg", "caption": "Table 11: Comparison of pre-trained models for CAL on LLaVA-Next-13B.", "description": "This table compares the performance of CAL using different pre-trained models on the LLaVA-Next-13B architecture.  It shows the results for three model variations: a baseline with original pre-training, CAL with original pre-training, and CAL with a baseline pre-trained model. Results are presented across various benchmark tasks including visual question answering (ChartVQA, DocVQA, SQA), image captioning (COCO Caption, TextCaps), and OCR-based tasks (OCRB, Refcocog).  This helps to analyze the impact of pre-training on the effectiveness of CAL.", "section": "A.5 Ablations for Pre-trained Model"}, {"figure_path": "NsxthTVpqA/tables/tables_18_3.jpg", "caption": "Table 1: Visual Question Answering benchmarks of CAL on leading methods including LLaVA-1.5, LLaVA-NeXT\u00b9, and MGM/MGM-HD. Our results are marked with VQA Text is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench).", "description": "This table presents the results of the Contrastive Alignment (CAL) method on various Visual Question Answering (VQA) benchmarks.  It compares the performance of several leading Vision Language Models (VLMs), including LLaVA-1.5, LLaVA-NeXT, and MGM/MGM-HD, both with and without the CAL method applied.  The table shows improvements in VQA scores across different model sizes and resolutions, highlighting the effectiveness of CAL in enhancing VQA performance.  Note that \"VQA Text*\" indicates results were evaluated without OCR tokens.", "section": "3 Experiments"}]