[{"figure_path": "Ku35qKpveg/figures/figures_1_1.jpg", "caption": "Figure 1: The proposed Dual-Perspective Activation (DPA) consists of three components: Pre-Activation Forward Memory (PreA-FM), Threshold Activation Unit (TAU), and Post-Activation Backward Memory (PostA-BM). DPA aims to efficiently identify irrelevant channels and apply channel denoising under the guidance of a joint criterion established online from both forward and backward propagation perspectives while preserving activation responses from relevant channels.", "description": "This figure illustrates the Dual-Perspective Activation (DPA) mechanism.  DPA consists of three main components working together:  Pre-Activation Forward Memory (PreA-FM) which tracks historical activation values; a Threshold Activation Unit (TAU) which processes the input signals; and a Post-Activation Backward Memory (PostA-BM) which tracks historical gradient statistics.  These components work together online to identify irrelevant channels.  Based on a joint criterion from forward and backward propagation, DPA applies channel denoising, preserving relevant channel activations.", "section": "3 Dual-Perspective Activation"}, {"figure_path": "Ku35qKpveg/figures/figures_2_1.jpg", "caption": "Figure 2: Category channel activation values (a) and channel activation gradients (b) are computed on the last block of ViT-Tiny. The activation layer used is ReLU. Each vector (left) is obtained by taking the average of 100 samples (right) randomly selected from its respective category. The results for five categories in CIFAR-100 are presented, with the values and gradients of the first 100 channels displayed. The horizontal axis represents the channel index, and the vertical axis represents the category index (left) / sample index (right). The brightness reflects the magnitude of the value / gradient. Here, we only focus on the sign of the gradient as the magnitude of the gradient is unstable. Therefore, the gradient of each sample is binarized to +1 and -1.", "description": "This figure shows the average activation values and gradients across channels for five different categories in the CIFAR-100 dataset, using the ViT-Tiny model with ReLU activation.  The left side of each subplot shows the average activation values/gradients across 100 samples for each category, while the right side shows the individual activation values/gradients for 100 samples from a single category. The visualization highlights the correlation between specific channels and different categories.  The brightness of the cells represents the magnitude of activation value or gradient.", "section": "2 Observations"}, {"figure_path": "Ku35qKpveg/figures/figures_2_2.jpg", "caption": "Figure 3: (a) Distributions of channel activation values in ViT-Tiny's last block are recorded by feeding samples from a given category in CIFAR-100. The activation layer used is ReLU. The horizontal axis represents the channel index, the vertical axis represents the channel activation value, and the area represents the value density. The distributions of the first 50 channels are shown. The red arrows point to potential irrelevant channels. (b) A confirmatory experiment is conducted on CIFAR-100 to compare the training accuracy between the original ViT-Tiny (baseline) and the ViT-Tiny with irrelevant channels manually removed for each category.", "description": "This figure shows the distribution of channel activation values for a given category in CIFAR-100 dataset using ReLU activation function in the last block of ViT-Tiny. Red arrows indicate potential irrelevant channels.  Subfigure (b) compares the training accuracy of original ViT-Tiny with that of a modified version where irrelevant channels were manually removed. The results demonstrate the negative impact of irrelevant channels on accuracy.", "section": "2 Observations"}, {"figure_path": "Ku35qKpveg/figures/figures_8_1.jpg", "caption": "Figure 4: Distributions of channel activation values in ViT-Tiny's last block are recorded by feeding samples from a given category in CIFAR-100. The proposed DPA is compared with ReLU. The horizontal axis represents the channel index, the vertical axis represents the channel activation value, and the area represents the value density. In the figure, each row displays the changes in response distributions for a specific category when transitioning from ReLU to the proposed DPA. The distributions of the first 50 channels are shown.", "description": "This figure compares the distribution of channel activation values between ReLU and DPA for four different categories in CIFAR-100 dataset. It visually demonstrates how DPA effectively suppresses activations from irrelevant channels, resulting in a sparser and more focused representation.", "section": "4.6 Activation Response Visualization"}, {"figure_path": "Ku35qKpveg/figures/figures_13_1.jpg", "caption": "Figure 5: Top-1 accuracy (%) w.r.t. the momentum m for updating the moving mean \u00b5 when training on CIFAR-100 with ViT-Tiny.", "description": "This figure shows the impact of the momentum (m) parameter used in updating the moving mean (\u00b5) on the Top-1 accuracy of the ViT-Tiny model trained on the CIFAR-100 dataset.  The x-axis represents different values of the momentum parameter, and the y-axis represents the resulting Top-1 accuracy.  The graph indicates that the model's performance is relatively insensitive to the choice of momentum within a wide range (approximately 0.2 to 0.99), with a peak accuracy around m = 0.9.  Extremely low values of m negatively affect performance.", "section": "A.1 Hyperparameter Impact Analysis"}, {"figure_path": "Ku35qKpveg/figures/figures_13_2.jpg", "caption": "Figure 6: Top-1 accuracy (%) w.r.t. the balanced parameter \u03bb for the channel loss Lch when training on CIFAR-100 with ViT-Tiny.", "description": "This figure shows the relationship between the balanced parameter \u03bb used in the channel loss calculation and the resulting top-1 accuracy achieved when training the ViT-Tiny model on the CIFAR-100 dataset.  The x-axis represents different values of \u03bb, while the y-axis shows the corresponding top-1 accuracy. The graph indicates an optimal range for \u03bb where accuracy is maximized, demonstrating a sensitivity of model performance to the choice of this hyperparameter.", "section": "A.1 Hyperparameter Impact Analysis"}, {"figure_path": "Ku35qKpveg/figures/figures_13_3.jpg", "caption": "Figure 7: Top-1 accuracy (%) w.r.t. the activation threshold \u03c4 for the Threshold Activation Unit (TAU) when training on CIFAR-100 with ViT-Tiny.", "description": "This figure shows the impact of the activation threshold (\u03c4) in the Threshold Activation Unit (TAU) on the model's accuracy.  The experiment was conducted on the CIFAR-100 dataset using the ViT-Tiny model.  It demonstrates that as the threshold \u03c4 increases, the accuracy decreases, suggesting that setting \u03c4 to 0 is optimal for this experiment.  The graph compares the accuracy with only TAU and the full DPA mechanism.", "section": "4.1 DPA on ViTs"}, {"figure_path": "Ku35qKpveg/figures/figures_14_1.jpg", "caption": "Figure 8: The performance of ViT-Tiny with ReLU and the proposed DPA on the CIFAR-10 dataset using weak data augmentations comprising only \"random horizontal flipping\" and \"normalization\"", "description": "This figure demonstrates the training and validation loss and accuracy curves for ViT-Tiny using ReLU and the proposed DPA activation function on the CIFAR-10 dataset. Only weak data augmentations (random horizontal flipping and normalization) were used.  The DPA model shows improved performance, particularly lower validation loss, suggesting better generalization and reduced overfitting.", "section": "A.2 Overfitting Test"}]