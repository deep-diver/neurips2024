[{"figure_path": "8K6ul0hgtC/figures/figures_8_1.jpg", "caption": "Figure 1: Training losses of PINNs solving (a) bi-harmonic equation and (b) Poisson equation.", "description": "This figure shows the training losses for PINNs solving the bi-harmonic and Poisson equations.  The plots illustrate how the training loss changes over epochs for different network widths (m) and different powers (p) of the ReLU activation function.  It visually demonstrates the relationship between the PDE order, network width, activation power, and convergence.  Each subplot represents a different value of p (the power of ReLU), and within each subplot are multiple lines showing the training loss for various network widths m.  The results support the authors' theoretical findings about the impact of these parameters on the convergence of PINNs.", "section": "5 Experiments"}, {"figure_path": "8K6ul0hgtC/figures/figures_9_1.jpg", "caption": "Figure 2: Loss curves of (a) effect of the power p of ReLUp and (b) comparison between PINNs with VS-PINNS.", "description": "This figure presents the experimental results that validate the theory. The figure is composed of two subfigures. Subfigure (a) shows the effect of the power p of the ReLU activation function on the training loss of PINNs. The results show that the convergence of loss is enhanced as p decreases, which supports the theoretical finding that the smaller p is, the more likely the gradient descent will converge. Subfigure (b) shows a comparison between PINNs and VS-PINNs for the second-order heat equation. The results show that the training loss for VS-PINNs converges more effectively than that of PINNs, which indicates that VS-PINNs, which optimize a loss function incorporating lower-order derivatives using networks with smaller p, facilitate convergence of GD.", "section": "Experiments"}, {"figure_path": "8K6ul0hgtC/figures/figures_49_1.jpg", "caption": "Figure 3: Loss curves of (a) effect of the power p of ReLUp and (b) comparison between PINNs with VS-PINNS.", "description": "This figure displays two subfigures. Subfigure (a) shows the impact of the power (p) of the ReLU activation function on the training loss of PINNs for a second-order heat equation.  It demonstrates that lower p values lead to faster convergence and better stability. Subfigure (b) compares the training loss curves of PINNs and VS-PINNs (Variable Splitting PINNs) for the same second-order heat equation. It illustrates that VS-PINNs converge significantly faster than standard PINNs, highlighting the effectiveness of the variable splitting technique in improving convergence.", "section": "5 Experiments"}, {"figure_path": "8K6ul0hgtC/figures/figures_50_1.jpg", "caption": "Figure 1: Training losses of PINNs solving (a) bi-harmonic equation and (b) Poisson equation.", "description": "This figure displays the training loss curves for PINNs trained on the bi-harmonic and Poisson equations.  The plots show how the training loss decreases with epochs (iterations) of training for different network widths (m) and different powers of the ReLU activation function (p).  Each subplot shows results for a given PDE (bi-harmonic or Poisson) with different activation functions and network widths. The shaded regions around the curves represent the variance observed over multiple runs. The figure aims to illustrate the impact of network width and the power of the ReLU activation function on the convergence behavior of PINNs, particularly as the order of the PDE increases.", "section": "Experiments"}, {"figure_path": "8K6ul0hgtC/figures/figures_50_2.jpg", "caption": "Figure 6: Loss of VS-PINNs trained by gradient descent with variant learning rates.", "description": "This figure shows the training loss curves of VS-PINNs trained using gradient descent with different learning rates for the elastic beam equation.  The results demonstrate the impact of varying learning rates on the convergence behavior, illustrating how optimal selection of the learning rate affects the speed and stability of convergence towards lower loss values.  Multiple runs with different random seeds are included to show the variation in results.", "section": "Experiments"}, {"figure_path": "8K6ul0hgtC/figures/figures_52_1.jpg", "caption": "Figure 6: Loss of VS-PINNs trained by gradient descent with variant learning rates.", "description": "This figure displays the training loss curves of VS-PINNs (Variable Splitting Physics-Informed Neural Networks) trained using gradient descent with different learning rates.  Two plots are shown: one for the initial 40,000 epochs of training and one for the total 400,000 epochs. Each plot shows three curves, representing different learning rates (lr = 0.01, lr = 0.001, lr = 0.0001). The figure illustrates how the learning rate affects the convergence speed and stability of the VS-PINN training process.  The results demonstrate the impact of the learning rate on the training loss, highlighting the need for careful selection of the learning rate to ensure efficient and stable convergence in VS-PINNs.", "section": "Experiments"}]