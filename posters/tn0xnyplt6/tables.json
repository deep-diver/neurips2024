[{"figure_path": "tN0xnYPLt6/tables/tables_1_1.jpg", "caption": "Table 1: LUT size estimation when storing 8bit data with output entities r = 4. Compared with the full size, the sampled operation effectively reduces the storage consumption of LUT. Furthermore, separable mapping strategy(SMS) and trained dynamic discretization mechanism (DDM) achieve an exaggerated reduction result.", "description": "This table compares the size of Lookup Tables (LUTs) for different receptive field (RF) sizes using various optimization techniques.  It shows the full size of LUT, the size after applying the SRLUT method, and the further reductions achieved with the Separable Mapping Strategy (SMS) and the combined SMS and Dynamic Discretization Mechanism (DDM).  The table highlights the significant storage reduction achieved by these methods, making LUT-based approaches more feasible for edge devices.", "section": "3 Method"}, {"figure_path": "tN0xnYPLt6/tables/tables_6_1.jpg", "caption": "Table 2: Quantitative comparisons on 5 standard SISR test sets for an upscaling factor of 4. The size of TinyLUTs are smaller than other LUT methods and achieves better PSNR and SSIM average values with a good margin. *: The storage overhead of weight parameters in the DNN. The inference latency evaluation environments are the same as [15, 14]", "description": "This table presents a quantitative comparison of TinyLUT and other state-of-the-art methods on five standard image super-resolution (SISR) datasets with an upscaling factor of 4.  The metrics used for comparison are Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).  The table also shows the storage requirements of each method and inference latency on two different hardware platforms (Xiaomi 11 and Raspberry Pi 4B).  The results demonstrate that TinyLUT achieves better accuracy with significantly lower memory consumption.", "section": "4.2 Evaluation on Image Super Resolution"}, {"figure_path": "tN0xnYPLt6/tables/tables_7_1.jpg", "caption": "Table 3: Quantitative comparisons on Set12 and BSD68. The size of TinyLUTs are smaller than other LUT methods and achieves better PSNR and SSIM values with a good margin. *: The storage overhead of weight parameters in DNN. The evaluation environments are the same as [48]", "description": "This table presents a quantitative comparison of different super-resolution methods on the Set12 and BSD68 datasets.  The metrics used are PSNR and SSIM.  The table highlights that TinyLUT achieves better results with significantly lower storage requirements compared to other Look-Up Table (LUT) based methods and even a Deep Convolutional Neural Network (DnCNN). The inference latency is also compared on two different devices.", "section": "4.3 Evaluation on Image Denoising"}, {"figure_path": "tN0xnYPLt6/tables/tables_7_2.jpg", "caption": "Table 2: Quantitative comparisons on 5 standard SISR test sets for an upscaling factor of 4. The size of TinyLUTs are smaller than other LUT methods and achieves better PSNR and SSIM average values with a good margin. *: The storage overhead of weight parameters in the DNN. The inference latency evaluation environments are the same as [15, 14]", "description": "This table presents a quantitative comparison of TinyLUT with other state-of-the-art LUT-based and DNN-based methods for single image super-resolution (SISR) tasks on five standard benchmark datasets (Set5, Set14, Urban100, BSD100, Manga109).  The comparison includes PSNR and SSIM values, storage consumption (in KB), and inference latency (runtime in ms) on two different edge devices (Xiaomi 11 and Raspberry 4B).  The results demonstrate that TinyLUT achieves superior performance (higher PSNR/SSIM) with significantly lower storage and latency compared to the alternatives.", "section": "4.2 Evaluation on Image Super Resolution"}, {"figure_path": "tN0xnYPLt6/tables/tables_8_1.jpg", "caption": "Table 4: The quantitative comparisons of PSNR-B on standard benchmark datasets for image deblocking under a quality factor of 10.", "description": "This table presents a comparison of PSNR-B (Peak Signal-to-Noise Ratio for deblocking) scores achieved by various methods on two standard benchmark datasets for image deblocking (Classic5 and LIVE1).  The methods compared include classical JPEG compression, two deep learning-based methods (SA-DCT and ARCNN), and several lookup table (LUT)-based approaches (SRLUT, MuLUT, SPF-LUT, SPF-LUT+DFC), and the authors' proposed TinyLUT-F method. The table shows that the TinyLUT-F method achieves comparable PSNR-B scores to the DNN (deep neural network) methods while having a significantly smaller model size.", "section": "4 Experiments"}, {"figure_path": "tN0xnYPLt6/tables/tables_8_2.jpg", "caption": "Table 5: Impact of SMS. Ablation studies on TinyLUT-S for 4\u00d7 SISR.", "description": "This table presents the ablation study results on the impact of the Separable Mapping Strategy (SMS) in TinyLUT-S model for image super-resolution (SISR) with an upscaling factor of 4. It compares the performance (PSNR in dB) on different benchmark datasets (Set5, Set14, Urban100, BSD100, Manga109) using three methods: Original (without SMS), Uniformly Sampled [15] (a previous method), and SMS (the proposed method). It also shows the LUT size for each method. The results demonstrate SMS's effectiveness in achieving significant storage reduction with minimal performance loss.", "section": "4.5 Ablation Studies"}, {"figure_path": "tN0xnYPLt6/tables/tables_9_1.jpg", "caption": "Table 6: Effects of DDM. Ablation studies for DDM with 4\u00d7 SISR.", "description": "This table presents the ablation study results on the impact of the Dynamic Discretization Mechanism (DDM) on the TinyLUT-S model for image super-resolution with a 4x upscaling factor. It compares the performance (PSNR values on Set5, Set14, Urban100, BSD100, and Manga109 datasets) and LUT size of the TinyLUT-S model using the full LUT and DDM.  The results demonstrate the effectiveness of DDM in reducing LUT size while maintaining comparable accuracy.", "section": "4.5 Ablation Studies"}]