[{"type": "text", "text": "On Affine Homotopy between Language Encoders ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Robin S. M. Chan1 Reda Boumasmoud1 Anej Svete1 Yuxin Ren2   \nQipeng Guo3 Zhijing Jin1,4 Shauli Ravfogel1 Mrinmaya Sachan1 Bernhard Sch\u00f6lkopf1,4 Mennatallah El-Assady1 Ryan Cotterell1 1ETH Z\u00fcrich 2Tsinghua University 3Fudan University 4Max Plank Institute for Intelligent Systems ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained language encoders\u2014functions that represent text as vectors\u2014are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be intrinsic, that is, task-independent, yet still be informative of extrinsic similarity\u2014the performance on downstream tasks. It is common to consider two encoders similar if they are homotopic, i.e., if they can be aligned through some transformation.1 In this spirit, we study the properties of affine alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them. ", "page_idx": 0}, {"type": "text", "text": "https://github.com/chanr0/affine-homotopy ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A common paradigm in modern natural language processing (NLP) is to pre-train a language encoder on a large swathe of natural language text. Then, a task-specific model is fit (fine-tuned) using the language encoder as the representation function of the text. More formally, a language encoder is a function $\\boldsymbol{h}\\colon\\Sigma^{*}\\to\\ensuremath{\\mathbb{R}}^{D}$ , i.e., a function that maps a string over an alphabet $\\Sigma$ to a finite-dimensional vector. Now, consider sentiment analysis as an informative example of a task. Suppose our goal is to classify a string $\\b{y}\\in\\b{\\Sigma}^{*}$ as one of three polarities $\\Pi=\\{\\odot,\\bar{\\odot},\\odot\\}$ . Then, the probability of $\\textit{\\textbf{y}}$ exhibiting a specific polarity is often given by a log-linear model, e.g., the probability of $\\odot$ is ", "page_idx": 0}, {"type": "equation", "text": "$$\np(\\boldsymbol{\\odot}\\mid\\boldsymbol{y})=\\mathrm{softmax}(\\mathbf{E}\\,h(\\boldsymbol{\\mathbf{y}})+\\mathbf{b})_{\\boldsymbol{\\odot}}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathbf{E}\\in\\mathbb{R}^{3\\times D}$ , $\\mathbf{b}\\in\\mathbb{R}^{3}$ and softmax: $\\mathbb{R}^{N}\\to\\Delta^{N-1}$ . Empirically, using a pre-trained encoder $^h$ leads to significantly better classifier performance than training a log-linear model from scratch. ", "page_idx": 0}, {"type": "text", "text": "In the context of the widespread deployment of language encoders, this paper tackles a natural question: Given two language encoders $^h$ and $\\textbf{\\textit{g}}$ , how can we judge to what extent they are similar? This question is of practical importance\u2014recent studies have shown that even small variations in the random seed used for training can result in significant performance differences on downstream tasks between models with the same architecture [13, 35] In this case, we say that two such language encoders exhibit an extrinsic difference, i.e., the difference between two encoders manifests itself when considering their performance on a downstream task. However, we also seek an intrinsic notion of similarity between two language encoders, i.e., a notion of similarity that is independent of any particular downstream task. Moreover, we may hope that a good notion of intrinsic similarity would allow us to construct a notion of extrinsic similarity that holds for all downstream tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing work studies language encoder similarity by evaluating whether two encoders produce similar representations for a finite dataset of strings [3, 20, 22, 42, inter alia], often by analyzing whether the representation sets can be approximately linearly aligned [22, 27]. More formally, two encoders are considered similar if there exists a matrix A such that $h(y)\\approx\\mathbf{A}\\,g(y)$ holds for strings $\\textit{\\textbf{y}}$ in some finite set $\\mathcal{D}\\subset\\Sigma^{*}$ .2 This assumes that examining finitely many outputs provides sufficient insight into encoder behavior. In contrast, we set out to study the relationships between language encoders, i.e., functions, themselves. This decision, rather than being just a technicality, allows us to derive a richer understanding of encoder relationships, revealing properties and insights that remain obscured under conventional finite-set analysis. Concretely, we ask what notions of similarity between encoders one could consider and what they imply for their relationships. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of the paper are of a theoretical nature. We first define an (extended) metric space on language encoders. We then extend this notion to account for transformations in a broad framework of $S$ -homotopy for a set of transformations $S$ , where $\\textbf{\\textit{g}}$ is $S$ -homotopic to $^h$ if $\\textbf{\\textit{g}}$ can be transformed into $^h$ through some transformation in $S$ . As a concrete application of the framework, we study affine homotopy\u2014the similarity of $^h$ and $\\psi\\circ\\mathbf g$ for affine transformations $\\psi$ . The notion of intrinsic similarity induced by such one-sided alignment is not symmetric and can be seen as the cost of transforming $\\textbf{\\textit{g}}$ into $^h$ . Nevertheless, we show it is informative of extrinsic similarity: If one encoder can be affinely mapped to another, we can guarantee that it also performs similarly on downstream tasks. We confirm this empirically by studying the intrinsic and extrinsic similarities of various pretrained encoders, where we observe a positive correlation between intrinsic and extrinsic similarity. Beyond measuring similarity, homotopy also allows us to define a form of hierarchy on the space of encoders, elucidating a structure in which some encoders are more informative than others. Such an order is also suggested by our experiments, where we find that certain encoders are easier to map to than others which shows in the rank of the learned representations and affects their transfer learning ability. ", "page_idx": 1}, {"type": "text", "text": "2 Language Encoders ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\Sigma$ be an alphabet\u2014a finite, non-empty set of symbols $y$ \u2014and $\\mathrm{EOS}\\notin\\Sigma$ a distinguished end-ofstring symbol. With $\\scriptstyle\\sum^{*}{\\overset{\\underset{\\mathrm{def}}{}}{=}}\\bigcup_{n=0}^{\\infty}\\sum^{n}$ we denote the Kleene closure of $\\Sigma$ , the set of all strings $\\textit{\\textbf{y}}$ . A language encoder is a fun c\u0164tion $h\\colon\\Sigma^{*}\\to V\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbb{R}^{D}$ that maps strings to real vectors.3 We write ${\\mathcal{E}}_{V}\\ {\\stackrel{\\mathrm{def}}{=}}\\ V^{\\Sigma^{*}}$ for the $\\mathbb{R}$ -vector space of language encoders, and ${\\mathcal{E}}_{b}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{h\\in{\\mathcal{E}}_{V}\\ |\\ h(\\Sigma^{*})$ is bounded $\\}\\subset$ ${\\mathcal{E}}_{V}$ for its sub-vector space of bounded encoders. ", "page_idx": 1}, {"type": "text", "text": "There are two common ways that language encoders are created [7]. The first is through autoregressive language modeling. A language model (LM) is a probability distribution over $\\Sigma^{*}$ .4 Autoregressive LMs are defined through the multiplication of conditional probability distributions $p_{h}(y_{t}\\mid\\bar{y}_{<t})$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{h}^{\\mathrm{LM}}({\\pmb y})=p_{h}(\\mathrm{EOS~}|\\{{\\pmb y}\\}\\prod_{t=1}^{T}p_{h}(y_{t}\\mid{\\pmb y}_{<t}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where each $p_{h}(\\cdot\\mid y_{<t})$ is a distribution over $\\Sigma\\cup\\{{\\mathrm{EOS}}\\}$ parametrized by a language encoder $^h$ ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{h}(y_{t}\\mid\\pmb{y}_{<t})\\stackrel{\\mathrm{def}}{=}\\mathrm{softmax}(\\mathbf{E}\\,h(\\pmb{y}_{<t}))_{y_{t}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{E}\\in\\mathbb{R}^{(|\\Sigma|+1)\\times D}$ . An autoregressive LM provides a simple manner to learn a language encoder from a dataset of strings $\\mathscr{D}=\\{\\pmb{y}^{(\\bar{n})}\\}_{n=1}^{N}$ by minimizing $\\mathcal{D}$ \u2019s negative log-likelihood. We may also learn a language encoder through masked language modeling (MLM), which defines the conditional probabilities based on both sides of the masked symbol\u2019s context ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{h}\\bigl(y_{t}\\mid y_{<t},y_{>t}\\bigr)\\stackrel{\\mathrm{def}}{=}\\mathrm{softmax}\\bigl(\\mathbf{E}\\,h\\bigl(\\mathbf{{y}}_{<t}\\circ[\\mathrm{MASK}]\\circ\\mathbf{{y}}_{>t}\\bigr)\\bigr)_{y_{t}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Maximizing the log-likelihood of a corpus under a language model derived from a language encoder $^h$ with a gradient-based algorithm only requires $^h$ to be a differentiable function of its parameters. Once a language encoder has been trained on a (large) corpus, its representations can be used on more fine-grained NLP tasks such as classification. The rationale for such transfer learning is that representations $\\boldsymbol{h}\\left(\\boldsymbol{y}\\right)$ stemming from a performant language model also contain information useful for other downstream tasks on natural language. An NLP practitioner might then implement a task-specific transformation of $\\boldsymbol{h}\\left(\\boldsymbol{y}\\right)$ . To tackle the problem that the tasks of interest are often less resource-abundant and to keep the training costs low, task-specific transformations are usually simple, often in the form of linear transformations of $\\boldsymbol{h}\\left(\\boldsymbol{y}\\right)$ , as in Eq. (1). ", "page_idx": 2}, {"type": "text", "text": "3 Measuring the Alignment of Langauge Encoders ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by introducing measures of affine alignment and hemi-metrics on ${\\mathcal{E}}_{V}$ ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries on Hemi-Metric Spaces ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language encoders compute representations for the infinitely many strings in $\\Sigma^{*}$ . In general, these representations might diverge towards $\\infty$ , making it necessary to talk about unbounded encoders, where it is convenient to allow distances and norms to take extended real numbers as values.5 ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1. An extended metric on a set $X$ is a map $d\\colon X\\to{\\overline{{\\mathbb{R}}}}_{+}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b.\\ \\forall x,y,z\\in X,\\quad d(x,y)\\leqslant d(x,z)+d(z,y);}\\\\ &{c.\\ \\forall x,y\\in X,\\quad d(x,y)=d(y,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, an extended norm is a map $\\|\\cdot\\|\\colon X\\to{\\overline{{\\mathbb{R}}}}_{+}$ that satisfies the norm axioms. Moreover, we will consider maps $d$ that do not satisfy the symmetry axiom. Lawvere [25] notes that symmetry is artificial and unnecessary for many of the main theorems involving metric spaces. In such situations, the quantity $d(x,y)$ can be interpreted as the cost of going from $x$ to $y$ . Occasionally, we want $d$ to capture that it costs more to go from $x$ to $y$ than to return, making asymmetry desirable. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.2. A hemi-metric6 or Lawvere-metric on a set $X$ is a map $d\\colon X\\to{\\overline{{\\mathbb{R}}}}_{+}$ such that ", "page_idx": 2}, {"type": "text", "text": "One of our main contributions is a formalization of measuring how far a language encoder $^h$ is from the set of all possible transformations of another encoder $\\textbf{\\emph{g}}$ \u2014for example, from all affine transformations of $\\textbf{\\textit{g}}$ . For this, we lift a hemi-metric over elements $x\\in X$ to subsets of $X$ , a crucial for the rest of the paper. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.3. Let $(X,d)$ be a hemi-metric space. For non-empty $E,E^{\\prime}\\subset X$ , we define ", "page_idx": 2}, {"type": "equation", "text": "$$\nd^{\\varkappa}(E,E^{\\prime})\\stackrel{\\mathrm{def}}{=}\\underset{x\\in E}{\\operatorname*{sup}}\\ \\underset{y\\in E^{\\prime}}{\\operatorname*{inf}}\\,d(x,y).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The map $d^{\\mathcal{H}}$ is called the Hausdorff\u2013Hoare map and is a hemi-metric on $\\mathcal{P}(X)\\backslash\\{\\mathcal{O}\\}$ , the power set of $X$ . When $E$ is a singleton set $\\{x\\}$ , we will, with a slight abuse of notation, write $\\boldsymbol{d}^{\\varkappa}(\\boldsymbol{x},E^{\\prime})$ to mean $d^{\\mathcal{H}}(\\{x\\},E^{\\prime})$ , defined $a s=\\operatorname*{inf}_{y\\in E^{\\prime}}d(x,y)$ .7 ", "page_idx": 2}, {"type": "text", "text": "We next introduce the hemi-metric recipe. It tells us how one can define a hemi-metric on a set $X$ by embedding $X$ into the power set of another space $Y$ where a hemi-metric already exists. After $X$ is embedded, one can use the Hausdorff\u2013Hoare map based on the hemi-metric from $Y$ to define a hemi-metric on $X$ through the images of $x\\in X$ . ", "page_idx": 2}, {"type": "text", "text": "Remark 3.1 (Hemi-Metric Recipe). Let $X$ be a set, $(Y,d)$ a hemi-metric space, and $S\\colon X\\ \\to$ ${\\mathcal{P}}(Y)\\backslash\\{\\emptyset\\},x\\mapsto E_{x}$ a function that assigns an $x\\in X$ a subset $E_{x}\\in{\\mathcal{P}}(Y)\\backslash\\{{\\mathcal{D}}\\}$ . Using Lem. $D.I$ , we can construct a hemi-metric on $X$ with $d_{S}^{\\varkappa}(x,y)\\,\\stackrel{\\mathrm{def}}{=}\\,d^{\\varkappa}(E_{x},E_{y})\\,=\\,d^{\\varkappa}(S(x),S(y))$ , and an extended pseudo-metric (a symmetric hemi-metric) with $d_{S}^{\\mathcal{H},\\;s y m}(x,y)=\\operatorname*{max}(d_{S}^{\\mathcal{H}}(x,y),d_{S}^{\\mathcal{H}}(y,x))$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 3.1 introduces a general recipe for defining hemi-metric spaces on function spaces\u2014 topological spaces whose elements are functions from a set to subsets of an extended-metric space. This naturally applies to the study of encoders and their transformations, which we call $S$ -homotopy, i.e., two encoders are $S$ -homotopic if one can be $S$ -deformed into the other. In this case, the set $E_{h}$ for $h\\in{\\mathcal{E}}_{V}$ corresponds to the set of encoders that $^h$ can be transformed into with mappings in $S$ . We could, for example, take $S$ as the set of all continuous maps, smooth maps, or multi-layer perceptrons. Our following discussion of affine maps, i.e., where $S=\\operatorname{Aff}(V)$ , is extrinsically motivated but can be understood as a specific instance of the more general framework of $S$ -homotopy. ", "page_idx": 3}, {"type": "text", "text": "3.2 A Norm and a Distance on ${\\mathcal{E}}_{V}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The hemi-metric recipe first requires us to define a (hemi-)metric on the individual elements. Given that all norms on the $\\mathbb{R}$ -vector space $V$ are equivalent [24, Proposition 2.2, $\\S X\\Pi$ , we fix in this paper a norm $|\\cdot|_{V}$ on $V$ . We introduce the maps $\\|\\cdot\\|_{\\infty}\\colon\\mathcal{E}_{V}\\to\\overline{{\\mathbb{R}}}_{+}$ and $d_{\\infty}(\\cdot,\\cdot)\\colon\\mathcal{E}_{V}\\times\\mathcal{E}_{V}\\rightarrow\\overline{{\\mathbb{R}}}_{+}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|h\\|_{\\infty}\\stackrel{\\mathrm{def}}{=}\\underset{y\\in\\Sigma^{*}}{\\operatorname*{sup}}|h(y)|_{V}\\qquad(6)\\qquad\\mathrm{and}\\qquad\\qquad d_{\\infty}(h,g)=\\|h-g\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\infty}$ is an extended norm on ${\\mathcal{E}}_{V}$ and $(\\mathcal{E}_{V},d_{\\infty})$ is a complete8 extended metric space.9 ", "page_idx": 3}, {"type": "text", "text": "Let $\\operatorname{GL}(V)$ be the set of invertible $D\\times D$ matrices. We write $\\|\\cdot\\|_{V}\\colon\\operatorname{GL}(V)\\to\\mathbb{R}_{+}$ for the subordinate $\\begin{array}{r}{\\|\\mathbf{A}\\|_{V}\\,=\\,\\operatorname*{sup}_{\\mathbf{v}\\in V\\backslash\\{0\\}}\\frac{|\\mathbf{A}\\mathbf{v}|_{V}}{|\\mathbf{v}|_{V}}}\\end{array}$ $V$ space10 and set $\\operatorname{Aff}(V)$ for the group of affine transformations of $V$ . An affine transformation $\\psi$ on $V$ is a map $\\mathbf{v}\\mapsto\\mathbf{A}\\mathbf{v}+\\mathbf{b}$ , for some invertible $\\mathbf{A}\\in\\operatorname{GL}(V)$ and $\\mathbf{b}\\in V$ . We call $\\psi_{\\mathrm{lin}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\bf A}$ the linear part of $\\psi$ and $t_{\\psi}\\colon{\\mathbf{v}}\\mapsto{\\mathbf{v}}\\!+\\!{\\mathbf{b}}$ its translation part. We denote with $\\mathcal{T}\\subset\\mathrm{Aff}(V)$ the subgroup of translations. Note that there is a natural left action of $\\operatorname{Aff}(V)$ on ${\\mathcal{E}}_{V}$ , i.e., $\\operatorname{Aff}(V)\\times{\\mathcal{E}}_{V}\\to{\\mathcal{E}}_{V},h\\mapsto\\psi\\circ h$ .11 ", "page_idx": 3}, {"type": "text", "text": "3.3 Affine Alignment of Language Encoders ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now use the general recipe from Remark 3.1 for affine alignment of language encoders\u2014affinely mapping from one encoder to another. For a subset $S\\subset\\operatorname{Aff}(V)$ we can define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{S}(\\pmb{h},\\pmb{g})\\stackrel{\\mathrm{def}}{=}d_{\\infty}^{\\varkappa}(\\pmb{h},S(\\pmb{g}))=\\operatorname*{inf}_{\\psi\\in S}\\|\\pmb{h}-\\psi\\circ\\pmb{g}\\|_{\\infty}}\\\\ &{\\quad\\quad\\|\\pmb{h}\\|_{S}\\stackrel{\\mathrm{def}}{=}d_{S}(0_{\\mathcal{E}_{V}},\\pmb{h}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S\\left(h\\right)\\stackrel{\\mathrm{def}}{=}\\left\\{s\\circ h\\mid s\\in S\\right\\}$ . In the notation of the hemi-metric recipe from Remark 3.1, we set $X=Y=\\mathcal{E}_{V}$ (we align an encoder with another encoder) and $d=d_{\\infty}$ , the uniform convergence distance (cf. $\\S3.2)$ . Further, we take $S\\,\\subseteq\\,\\mathrm{Aff}(V)\\,\\subset\\,{\\dot{V}}^{V}$ and define $S\\colon{\\mathcal{E}}_{V}\\,\\rightarrow\\,{\\mathcal{P}}\\left({\\mathcal{E}}_{V}\\right)$ , $^h\\mapsto$ $S\\left(h\\right)\\stackrel{\\mathrm{def}}{=}\\left\\{s\\circ h\\mid s\\in S\\right\\}$ . In words, $d_{S}(h,g)$ captures the notion of how well the encoder $\\textbf{\\textit{g}}$ can be $S_{}$ transformed into $^h$ . This is commonly called the alignment of $\\textbf{\\textit{g}}$ with $^h$ . $d_{S}(h,g)$ does not, however, necessarily tell us anything about how well $^h$ can be $S$ -transformed into $\\textbf{\\textit{g}}$ , resulting in asymmetry. ", "page_idx": 3}, {"type": "text", "text": "Remark 3.2. $d_{\\mathrm{Aff}(V)}$ defined in Eq. (8a) is not a metric on ${\\mathcal{E}}_{V}$ .12 Further, when $S=\\operatorname{Aff}(V)$ , the map $\\operatorname*{inf}_{\\psi,\\psi^{\\prime}\\in\\mathrm{Aff}(V)}\\|\\psi\\circ\\pmb{h}-\\psi^{\\prime}\\circ\\pmb{g}\\|_{\\infty}$ is trivially zero by Cor. D.1. ", "page_idx": 3}, {"type": "text", "text": "In the case of affine isometries $\\operatorname{Iso}(V)\\;=\\;\\{\\psi\\;\\in\\;\\operatorname{Aff}(V)\\;\\mid\\;\\psi_{\\mathrm{lin}}\\;\\in\\;\\operatorname{O}(V)\\}$ we show that the pair $(\\mathcal{E}_{V},d_{\\mathrm{Iso}(V)})$ constitutes an extended pseudo-metric space. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. The pair $(\\mathcal{E}_{V},d_{\\mathrm{Iso}(V)})$ is an extended pseudo-metric space. ", "page_idx": 4}, {"type": "text", "text": "4 Intrinsic Affine Homotopy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The notion of affine alignment allows us to introduce homotopic relations on ${\\mathcal{E}}_{V}$ . We first derive the affine intrinsic preorder $\\succsim_{\\mathrm{Aff}}$ on the space of encoders.13 ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.1. Let $(X,d)$ be a hemi-metric space. The relation $\\underline{{x}}\\succeq_{d}y$ iff $d(x,y)=0)$ q is a preorder14 and it will be called the specialization ordering of $d$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Goubault-Larrecq [17, Proposition 6.1.8]. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1 (Intrinsic Affine Preorder). For two encoders $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ , we define the relation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c c c}{h\\gtrsim_{\\mathrm{Aff}}g}&{i\\!\\!\\!/\\!\\!/\\!\\!/}&{d_{\\mathrm{Aff}(V)}\\!\\!\\!\\left(h,g\\right)=0.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 4.2. The relation $\\succ_{\\mathrm{Aff}}$ is a preorder on ${\\mathcal{E}}_{V}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Follows from $d_{\\mathrm{Aff}(V)}(\\psi\\circ h,g)\\leqslant\\|\\psi_{\\mathrm{lin}}\\|_{V}\\cdot d_{\\mathrm{Aff}(V)}({\\pmb h},{\\pmb g}),\\mathrm{see~App.~D.3.}$ ", "page_idx": 4}, {"type": "text", "text": "Intuitively, $\\succsim_{\\mathrm{Aff}}$ captures the order of encoders such that higher-positioned encoders in the order can be $S$ -transformed to the lower-positioned ones. To derive the implications of $\\succ_{\\mathrm{Aff}}$ we introduce the notion of an encoder rank. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.2 (Encoder Rank). For any $\\boldsymbol{h}\\in\\mathcal{E}_{V}$ let the encoder rank be $\\operatorname{rank}(h)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\dim_{\\mathbb{R}}(V_{h}),$ , where $V_{h}$ is the subvector space generated by the image of $^h$ . When $\\mathrm{rank}(\\pmb{h})=\\mathrm{dim}_{\\mathbb{R}}(V),$ , $^h$ is $a$ full rank encoder, else it is rank deficient. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. For $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nh\\gtrsim_{\\mathrm{Aff}}g\\Leftrightarrow h=\\psi(\\pi_{h}\\circ g)\\,f o r\\,s o m e\\ \\psi\\in\\mathrm{Aff}(V)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $\\pi_{h}$ is the orthogonal projection of $V$ onto $V_{h}$ . In particular, if $d_{\\mathrm{Aff}(V)}(h,g)\\;=\\;0$ then $\\mathrm{rank}(h)\\leqslant\\mathrm{rank}(g)$ . If in addtion, we know r $\\operatorname{ank}(\\pmb{g})=\\operatorname{rank}(\\pmb{h}),$ , then $\\textbf{\\textit{g}}$ must by an affine transformation of $\\textbf{\\textit{g}}$ , i.e., $\\pmb{h}=\\psi\\circ\\pmb{g}$ for some $\\psi\\in\\operatorname{Aff}(V)$ . ", "page_idx": 4}, {"type": "text", "text": "This allows us to state our first notion of language encoder similarity: intrinsic affine homotopy. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.3 (Exact Intrinsic Affine Homotopy). We say that two encoders $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ are exactly intrinsically affinely homotopic and write $h\\simeq_{\\mathrm{Aff}}g\\;i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\mathrm{Aff}(V)}({\\pmb h},{\\pmb g})=0\\,a n d\\,\\,\\mathrm{rank}({\\pmb h})=\\mathrm{rank}({\\pmb g}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any $\\boldsymbol{h},\\boldsymbol{g}\\in\\mathcal{E}_{V}$ , one can easily show that ", "page_idx": 4}, {"type": "equation", "text": "$$\nh\\simeq_{\\mathrm{Aff}}g\\Longleftrightarrow(g\\gtrsim_{\\mathrm{Aff}}h\\ a n d\\,h\\gtrsim_{\\mathrm{Aff}}g)\\Longleftrightarrow d_{\\mathrm{Aff}(V)}^{\\mathcal{H},\\ \\mathrm{sym}}(h,g)=0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which implies that $\\simeq\\!\\mathrm{Aff}$ is an equivalence relation on the set of language encoders ${\\mathcal{E}}_{V}$ . Intuitively, two encoders $^h$ and $\\textbf{\\textit{g}}$ are exactly intrinsically affinely homotopic, this means that both $\\textbf{\\textit{g}}$ can be affinely mapped to $^h$ , as well as the other way around. ", "page_idx": 4}, {"type": "text", "text": "5 Extrinsic Homotopy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In $\\S4$ , we explore methods for assessing how similar two language encoders are without reference to any downstream tasks. Here, we extend our discussion to the extrinsic homotopy of language encoders. Since language encoders are primarily used to generate representations for downstream tasks\u2014such as in transfer learning, illustrated by the sentiment analysis example in $\\S1$ \u2014we argue that the key criterion in the similarity of two encoders lies in how closely we can align predictions stemming from their representations.15 ", "page_idx": 4}, {"type": "text", "text": "Principle 5.1 (Extrinsic Homotopy). Two language encoders $^h$ and $\\textbf{\\textit{g}}$ are extrinsically homotopic if we can guarantee a similar performance on any downstream task $^h$ and $\\textbf{\\textit{g}}$ might be used for. ", "page_idx": 5}, {"type": "text", "text": "The rest of the section formalizes this intuitive notion and describes its relationship with intrinsic affine homotopy. Let $W$ be the vector space $\\mathbb{R}^{N}$ and set Af $(V,W)$ as the set of affine maps from $V$ to $W$ .16 We define $\\mathcal{E}_{\\Delta}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbf{M}\\mathrm{ap}(\\Sigma^{*},\\Delta^{N-1})$ and $\\mathcal{E}_{W}\\,=\\,\\mathbf{M}\\mathrm{ap}(\\Sigma^{*},W)$ . Lastly, we formalize the notion of a transfer learning task as constructing a classifier that uses a language encoder\u2019s string representations. Particularly, we set $\\mathcal{V}_{N}$ to be the family of log-linear models as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{N}\\colon\\mathcal{E}_{V}\\to\\mathcal{P}(\\mathcal{E}_{\\Delta^{N-1}})\\backslash\\{\\mathcal{Q}\\},\\quad h\\mapsto\\operatorname{softmax}_{\\lambda}(\\mathrm{Aff}_{V,W}(h)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{Aff}_{V,W}$ is the map ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Aff}_{V,W}\\colon\\mathcal{E}_{V}\\to\\mathcal{P}(\\mathcal{E}_{W})\\backslash\\{\\mathcal{Q}\\},\\quad h\\mapsto\\{\\psi\\circ h\\mid\\psi\\in\\mathrm{Aff}(V,W)\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ${\\mathrm{softmax}}_{\\lambda}\\colon\\mathbb{R}^{N}\\to\\Delta^{N-1}$ is defined for $\\lambda\\in\\mathbb{R}_{+}$ , $\\mathbf{x}\\in\\mathbb{R}^{N}$ , and $n\\in[N]$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{softmax}_{\\lambda}\\left(\\mathbf{x}\\right)_{n}=\\frac{\\exp\\left(\\lambda x_{n}\\right)}{\\sum_{n^{\\prime}=1}^{N}\\exp\\left(\\lambda x_{n^{\\prime}}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 5.1. Each $p_{\\psi}=\\mathrm{softmax}_{\\lambda}\\circ\\psi(\\pmb{h}(\\pmb{y}))$ can be seen as a \u201cprobability distribution\u201d over $[N]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{N}(h)=\\{p(\\circ\\mid y)\\colon[N]\\to[0,1],\\circ\\mapsto\\mathrm{softmax}_{\\lambda}\\circ\\psi(h(y))_{\\circ}\\mid\\psi\\in\\mathrm{Aff}(V,W)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Through our standard recipe from Remark 3.1, we can define the following hemi-metrics on ${\\mathcal{E}}_{V}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 5.1. For any two encoders $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ , we define17 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathrm{Aff}(V,W)}^{\\mathcal{H}}(\\pmb{h},\\pmb{g})\\overset{\\mathrm{def}}{=}d_{\\infty,W}^{\\mathcal{H}}(\\mathrm{Aff}_{V,W}(\\pmb{h}),\\mathrm{Aff}_{V,W}(\\pmb{g}))}\\\\ &{\\quad d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(\\pmb{h},\\pmb{g})\\overset{\\mathrm{def}}{=}d_{\\infty,\\Delta^{N-1}}^{\\mathcal{H}}(\\mathcal{V}_{N}(\\pmb{h}),\\mathcal{V}_{N}(\\pmb{g}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Notice that we use $d^{\\mathcal{H}}$ rather than $d$ in Def. 5.1 since we are interested in how closely we can bring $^h$ and $\\textbf{\\textit{g}}$ when we affinely transform both of them\u2014this corresponds to independently affinely transforming the encoders for the same transfer learning task. In particular, Eq. (17b) measures how different two encoders are on any transfer learning task, formalizing the notion of extrinsic homotopy (cf. Principle 5.1), captured by the following definition. ", "page_idx": 5}, {"type": "text", "text": "Definition 5.2 (Extrinsic Affine Preorder). An encoder $h\\in{\\mathcal{E}}_{V}$ is exactly extrinsically homotopic $t o^{18}\\;\\pmb{g}\\in\\mathcal{E}_{V}$ if $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)=0$ . ", "page_idx": 5}, {"type": "text", "text": "Analogously to Def. 4.1, we use $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)$ to define a preorder. ", "page_idx": 5}, {"type": "text", "text": "Definition 5.3 (Extrinsic Affine Preorder). For two encoders $\\boldsymbol{h},\\boldsymbol{g}\\in\\mathcal{E}_{V}$ , we define the relation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{h\\gtrsim_{\\mathrm{Ext}}g}&{{}i\\!f\\!\\!f\\quad d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)=0.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. The relation $\\pmb{h}\\succeq_{\\mathrm{Ext}}\\pmb{g}$ is a preorder on ${\\mathcal{E}}_{V}$ . ", "page_idx": 5}, {"type": "text", "text": "We now relate $d_{\\mathrm{Aff}}^{\\mathcal{H}}(V,W)^{\\big(h,g\\big)}$ and $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)$ from Def. 5.1, and $d_{\\mathrm{Aff}}(V)(h,g)$ from $\\S4$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.2. Let $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ . We have ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{\\infty,\\Delta^{N-1}}^{\\mathcal{H}}(\\mathrm{softmax}_{\\lambda}(\\psi\\circ h),\\mathcal{V}_{N}(g))\\leqslant c(\\lambda)\\|\\psi_{l i n}\\|d_{\\mathrm{Aff}(V)}(\\pmb{h},\\pmb{g}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{\\mathrm{Aff}(V)}(h,g)=0\\Rightarrow d_{\\mathrm{Aff}(V,W)}^{\\mathcal{H}}(h,g)=0\\Rightarrow d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lem. 5.2 shows that $\\gtrsim_{\\mathrm{Ext}}$ is finer than $\\succ_{\\mathrm{Aff}}$ . This means that the affine intrinsic preorder is contained in the extrinsic preorder, i.e., $h\\gtrsim_{\\mathrm{Aff}}g\\Rightarrow h\\gtrsim_{\\mathrm{Ext}}g$ . Lastly, we can show that $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)$ is upper bounded by the intrinsic hemi-metric $d_{\\mathrm{Aff}(V)}^{\\mathcal{H}}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1 $\\epsilon$ -Intrinsic $\\Rightarrow\\mathcal{O}(\\epsilon)$ -Extrinsic). Let $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ be two encoders. Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)\\leqslant c(\\lambda)\\,d_{\\mathrm{Aff}(V)}^{\\mathcal{H}}(h,g).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6 Linear Alignment Methods for Finite Representation Sets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$\\S\\S\\ 4$ and 5 introduce ways of comparing language encoders as functions, which holistically characterizes relationships between them. We now address a more practical concern: Given two language encoders $^h$ and $\\textbf{\\textit{g}}$ , how can we approximate their similarity in practice? Rather than comparing $\\boldsymbol{h}\\left(\\boldsymbol{y}\\right):\\boldsymbol{\\Sigma}^{*}\\rightarrow\\Bar{\\mathbb{R}^{D}}$ with $g\\left(\\pmb{y}\\right):\\dot{\\Sigma^{\\ast}}\\rightarrow\\mathbb{R}^{D}$ over the entire $\\Sigma^{*}$ ,19 we compare them over a finite set of strings $\\mathbf{\\boldsymbol{\\mathcal{V}}}\\,=\\,\\{\\mathbf{\\boldsymbol{y}}^{(n)}\\}_{n=1}^{N}$ . We combine $y$ \u2019s representations given by $^h$ and $\\textbf{\\textit{g}}$ into matrices $\\mathbf{H},\\mathbf{G}\\in\\mathbb{R}^{\\tilde{N}\\times D}$ , where we denote $\\mathbf{H}_{y,\\cdot}=h\\left(y\\right)$ and $\\mathbf{G}_{y,\\cdot}=\\pmb{g}\\left(\\pmb{y}\\right)$ . We can approximate the notions of similarity from $\\S3$ by optimizing over the affine maps ${\\mathrm{Aff}}(V)$ (for example, using gradient descent). Particularly, we approximate intrinsic similarity as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{d}_{\\mathrm{Aff}(V)}(\\mathbf{H},\\mathbf{G})\\stackrel{\\mathrm{def}}{=}\\operatorname*{inf}_{\\psi\\in\\mathrm{Aff}(V)}\\operatorname*{max}_{y\\in\\mathcal{Y}}\\big\\|\\mathbf{H}_{y,\\cdot}-\\psi\\circ\\mathbf{G}_{y,\\cdot}\\big\\|_{V},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and extrinsic similarity for some task-specific fixed $\\psi^{\\prime}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{d}_{\\psi^{\\prime}}(\\mathbf{H},\\mathbf{G})\\stackrel{\\mathrm{def}}{=}\\operatorname*{sup}_{\\psi\\in\\mathbb{A}\\mathbb{f}(V,W)}\\operatorname*{max}_{y\\in\\mathcal{Y}}\\|\\mathrm{softmax}(\\psi^{\\prime}\\circ\\mathbf{H}_{y,\\cdot})-\\mathrm{softmax}(\\psi\\circ\\mathbf{G}_{\\pmb{y},\\cdot})\\|_{W}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Unfortunately, the max over $\\boldsymbol{\\wp}$ makes the optimization in Eqs. (19) and (20) difficult. For simplicity, we turn to commonly used linear alignment methods, which we review for completeness. ", "page_idx": 6}, {"type": "text", "text": "Orthogonal Procrustes Problem. Rather than optimizing the infinity norm over $\\boldsymbol{\\wp}$ as Eqs. (19) and (20), the orthogonal Procrustes problem finds the orthogonal transformation minimizing the Frobenius norm [34] by solving argmin $\\mathbf{A}{\\in}\\mathrm{O}(V)$ $\\|\\mathbf{H}{-}\\mathbf{A}\\mathbf{G}\\|_{F}$ . Given the singular-value decomposition $\\mathbf{H}^{\\top}\\mathbf{G}=\\mathbf{U}\\Sigma\\mathbf{V}^{\\top}$ , the optimum is achieved by $\\mathbf{U}\\mathbf{V}^{\\top}$ .20 Since the argmin is over $\\mathrm{O}(V)$ , this defines an extended pseudo-metric space by Prop. 3.1. ", "page_idx": 6}, {"type": "text", "text": "Canonical Correlation Analysis (CCA). CCA [20] is a linear alignment method that finds the matrices A, $\\mathbf{B}$ that project $\\mathbf{H}$ and $\\mathbf{G}$ into subspaces maximizing their canonical correlation. Let $\\mathbf{A}_{\\cdot,j}$ and $\\mathbf{B}._{,j}$ be the $j$ th column vectors of $\\mathbf{A}$ and $\\mathbf{B}$ , respectively. The formulation is as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho_{j}=\\operatorname*{sup}_{\\mathbf{A},\\,j,\\mathbf{B},\\,j}\\mathrm{corr}(\\mathbf{H}\\mathbf{A}.._{j},\\mathbf{G}\\mathbf{B}._{,j})\\quad\\mathrm{s.t.}\\quad\\forall_{i<j}\\ \\mathbf{H}\\mathbf{A}.._{j}\\perp\\mathbf{H}\\mathbf{A}._{,i}\\ ,\\ \\forall i<j\\ \\mathbf{G}\\mathbf{B}._{,j}\\perp\\mathbf{G}\\mathbf{B}._{,i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The representation similarity is measured in terms of the goodness of CCA fti, e.g., the mean squared CCA correlation $\\begin{array}{r}{R_{\\mathrm{CCA}}^{2}=\\sum_{i=1}^{D}\\rho_{i}^{2}/D}\\end{array}$ . We can reformulate the CCA objective in Eq. (21) as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathbf{A},\\mathbf{B}}\\frac{1}{2}\\|\\mathbf{A}^{\\top}\\mathbf{H}-\\mathbf{B}^{\\top}\\mathbf{G}\\|_{F}^{2}\\qquad\\mathrm{s.t.}\\quad(\\mathbf{A}^{\\top}\\mathbf{H})(\\mathbf{A}^{\\top}\\mathbf{H})^{\\top}=(\\mathbf{B}^{\\top}\\mathbf{G})(\\mathbf{B}^{\\top}\\mathbf{G})^{\\top}=\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given the singular-value decomposition $\\mathbf{H}^{\\top}\\mathbf{G}\\,=\\,\\mathbf{U}\\Sigma\\mathbf{V}^{\\top}$ , the solution of Eq. (22) is $(\\hat{\\bf A},\\hat{\\bf B})\\;=\\;$ $((\\mathbf{H}\\mathbf{H}^{\\top})^{-\\frac{1}{2}}\\mathbf{U},(\\mathbf{G}\\mathbf{G}^{\\top})^{-\\frac{1}{2}}\\mathbf{V})$ , where $(\\mathbf{H}\\mathbf{H}^{\\top})^{-\\frac{1}{2}}$ and $(\\mathbf{G}\\mathbf{G}^{\\top})^{-\\frac{1}{2}}$ are whitening transforms of $\\mathbf{U}$ and $\\mathbf{V}$ . Assuming the data is whitened during pre-processing, CCA corresponds to linear alignment under an orthogonality constraint, equivalent to the orthogonal Procrustes problem; see also App. E. ", "page_idx": 6}, {"type": "text", "text": "CCA Extensions. Projection-weighted CCA (PWCCA) [30] also finds alignment matrices with CCA but applies weighting to correlation values $\\rho_{i}$ to report the goodness of fti. Given the canonical vectors $\\hat{\\bf A}$ , PWCCA reports $\\begin{array}{r}{\\bar{\\rho}_{\\mathrm{PW}}=\\sum_{i=1}^{D}\\alpha_{i}\\rho_{i}/\\dot{\\sum_{i}\\alpha_{i}}}\\end{array}$ , where $\\begin{array}{r}{\\alpha_{i}=\\sum_{j}|\\langle\\hat{\\mathbf{A}}_{\\cdot,i},\\mathbf{H}_{\\cdot,j}\\rangle|}\\end{array}$ .21 ", "page_idx": 6}, {"type": "image", "img_path": "FTpOwIaWUz/tmp/d0a1e16ecda1d877cd596e577086f6346c9f16ce527e6c69faf1d311dbac8c46.jpg", "img_caption": ["Figure 1: Asymmetry between ELECTRA (E), RoBERTa (R), and MULTIBERT encoders (M1-M25) across layers. For each pair of the encoders $\\mathcal{M}^{(i)}$ and $\\mathcal{M}^{(j)}$ , we generate training set embeddings $\\mathbf{H}^{(i)},\\mathbf{H}^{(j)}\\in\\mathbb{R}^{N\\times D}$ for SST-2, COLA, and MNLI. We then fti $\\mathbf{H}^{(i)}$ to $\\mathbf{H}^{(j)}$ with an affine map and report the goodness of fti through the max error L2 norm, i.e., an approximation of $d(\\mathbf{H}^{(j)},\\mathbf{H}^{(\\bar{i})})$ on row $i$ and column $j$ of the grid. Full results across GLUE tasks are shown in Figure 4. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Non-Alignment Methods. While not explicitly (linearly) aligning representations, CKA [22] evaluates the kernel similarity between representations. CKA computes the normalized Hilbert-Schmidt independence [18] between centered kernel matrices $\\mathbf{K}^{\\mathbf{H}}$ and ${\\bf K^{G}}$ where ${\\bf K}_{i j}^{\\mathbf{H}}\\;=\\;k({\\mathbf{H}}_{i,\\cdot},{\\mathbf{H}}_{j,\\cdot})$ , and $\\mathbf{K}_{i j}^{\\mathbf{G}}=k(\\mathbf{G}_{i,\\cdot},\\mathbf{G}_{j,\\cdot})$ for a kernel function $k$ , i.e., $\\mathrm{tr}(\\mathbf{K}^{\\mathbf{H}}\\mathbf{K}^{\\mathbf{G}})/\\sqrt{(\\mathrm{tr}(\\mathbf{K}^{\\mathbf{H}}\\mathbf{\\mathcal{T}}\\mathbf{K}^{\\mathbf{H}})\\mathrm{tr}(\\mathbf{K}^{\\mathbf{G}}\\mathbf{\\mathcal{T}}\\mathbf{K}^{\\mathbf{G}}))}$ . Linear CKA, where $k(\\mathbf{H}_{i,\\cdot},\\mathbf{H}_{j,\\cdot})=\\mathbf{H}_{i,\\cdot}^{\\top}\\mathbf{H}_{j,\\cdot}$ , is commonly used. ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now explore the practical implications of our theoretical results. We conduct experiments on ELECTRA [6], ROBERTA [28], and the 25 MULTIBERT [35] encoders, which are architecturally identical to BERT-BASE [11] models pre-trained with different seeds. We report results on the training sets of two GLUE benchmark classification tasks: SST-2 [38] and MRPC [14]. When reporting $d$ and $\\hat{d}_{\\psi^{\\prime}}$ from Eq. (19) and Eq. (20), we use the $L_{2}$ norm for simplicity and approximate dVpV,\u2206q as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{d}_{\\nu(V,\\Delta)}^{\\mu}(\\mathbf{H},\\mathbf{G})=\\operatorname*{sup}_{\\psi^{\\prime}\\in\\mathrm{Af}(V,W)}\\operatorname*{inf}_{\\psi\\in\\mathrm{Aff}(V,W)}\\operatorname*{max}_{y\\in\\mathcal{Y}}\\|\\mathrm{softmax}(\\psi^{\\prime}\\circ\\mathbf{H}_{y,\\cdot})-\\mathrm{softmax}(\\psi\\circ\\mathbf{G}_{y,\\cdot})\\|_{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The experimental setup and compute resources are further described in App. F. ", "page_idx": 7}, {"type": "text", "text": "The Intrinsic \u2018Preorder\u2019 of Encoders. We first investigate whether the asymmetry of $d_{\\mathrm{Aff}(V)}$ is measurable in the finite alphabet encoder representations. Figure 1 shows distinct vertical lines for both tasks indicating that there are encoders that are consistently easier to affinely map to $(^{\\rightarrow}\\mathcal{M})$ . This seems to be rather independent of which encoder we map from $(\\mathcal{M}^{\\rightarrow})$ . We further see that this trend is task-independent for early layers but diverges for later layers. ", "page_idx": 7}, {"type": "text", "text": "The Influence of Encoder Rank Deficiency. As discussed in $\\S4$ , the encoder rank plays a pivotal role in affine mappability; exact affine homotopy is only achievable between equal-rank encoders.22 With this in mind, we return to our findings from Figure 1 to evaluate whether the observed differences between encoders can be attributed to a difference in measurable rank. Due to the inaccuracies of computing the rank numerically, we approximate the encoder rank using the rank to precision $\\epsilon$ as the number of representation matrix singular values larger than some $\\bar{\\epsilon}\\in\\mathbb{R}$ .23 We find statistically significant ( $\\acute{p}$ -value $<0.05$ ) rank correlation with the median intrinsic distance $\\hat{d}_{\\mathrm{Aff}(V)}$ when mapping to the corresponding encoder for RTE $\\mathit{\\Delta}\\!\\!\\!\\!\\int\\rho=0.312)$ ), MRPC $\\zeta_{\\boldsymbol{\\rho}}=0.609)$ , and QQP $\\mathit{\\dot{\\varphi}}_{\\rho}=0.389)$ . We find no statistically significant correlations with the median distance when mapping from the corresponding encoder. This difference in encoder ranks could, therefore, partially explain the previously observed differences in affine mappability as some encoders seem to learn lower-rank representations. ", "page_idx": 7}, {"type": "image", "img_path": "FTpOwIaWUz/tmp/946e50d88ff8a41c3803231de3ad70bca89dbb1a44e5d2545517f7caf81f93bc.jpg", "img_caption": ["Figure 2: For ELECTRA (E), RoBERTa (R), and MULTIBERTs (M1-M25), we plot extrinsic $(\\hat{d}_{\\psi^{\\prime}})$ against intrinsic similarity $(\\hat{d}_{\\mathrm{Aff}(V)})$ across GLUE tasks. We group the points by how well we can map to each encoder $(^{\\rightarrow}\\mathcal{M})$ , and display the median, as well as the first and third quartiles as vertical and horizontal lines. We additionally show the linear regression from $\\hat{d}_{\\mathrm{Aff}(V)}$ to $\\bar{\\hat{d}}_{\\psi^{\\prime}}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "A Linear Bound on Extrinsic Similarity. Lem. 5.2 derives a relationship between affine intrinsic and extrinsic similarity. To evaluate its strength in practice, we measure Spearman\u2019s Rank Correlation $(\\rho)$ and Pearson Correlation (PCC) between intrinsic measures introduced in $\\S6$ and the extrinsic measures d\u02c6\u03c81 and d\u02c6VHpV,\u2206q. PCC measures the strength and direction of a linear relationship between two random variables, whereas Spearman\u2019s $\\rho$ additionally evaluates the variables\u2019 monotonic association. $\\hat{d}_{\\psi^{\\prime}}$ is computed by training a linear classifier $\\psi^{\\prime}\\in\\operatorname{Aff}(V)$ on the final MULTIBERT layer for each task. Further, we report d\u02c6VHpV,\u2206q as the maximum L2 loss for a large number of randomly generated24 classifiers $\\psi^{\\prime}$ on the final layer of each MULTIBERT encoder. We generate 100 such classifiers for a range of GLUE datasets.25 Table 1 show significant, large linear correlation prevalent in all linear alignment methods, whereas CKA\u2014a linear, non-alignment method\u2014does not capture extrinsic behavior as faithfully. Further, Figure 2 visualizes the linear relationship explicitly for all considered GLUE datasets. ", "page_idx": 8}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We set out to explore homotopic relationships between language encoders, augmenting existing work on the similarity of finite representation sets by holistically studying encoder functions. In particular, the general framework of $S$ -homotopy allows us to study any functional relationship between encoders, enabling the exploration of many types of encoder relationships. As a first step in this direction and a concrete example, $\\S4$ explores affine homotopy, discussing what it means to be able to align two models with affine transformations. Here, Hausdorff\u2013Hoare maps prove useful, as they allow us to measure a notion of (asymmetric) distance between a point\u2014an encoder\u2014and the set of all affine transformations of another encoder. Lem. 5.2 in $\\S5$ then connects the intrinsic, task-independent, similarly to extrinsic similarity\u2014the similarity of performance on downstream tasks. Concretely, it derives a linear relationship between the intrinsic and extrinsic dissimilarity for any fixed affine transformation $\\psi^{\\prime}$ (i.e., a fixed downstream task). Thm. 5.1 discusses a stronger bound, namely on the worst-case extrinsic dissimilarity among all downstream linear classifiers, i.e., among all possible tasks. Further, by accounting for the asymmetries of encoder relationships, we augment the work on similarity in proper metric spaces [3, 36, 42]. ", "page_idx": 8}, {"type": "table", "img_path": "FTpOwIaWUz/tmp/5e97ef74e3cea1b9c425e5bf1edb42076f1b69af7cbd0d8101e07b05bc9438d5.jpg", "table_caption": [], "table_footnote": ["bTaetblwee e1:n  Sinpterianrsmica nm\u2019se aRsaunrke sC ionrtrreoldatuicoend  Cino $\\S6$ icaienndt $(\\rho)$ and Pearson\u2019s Correlation Coefficient (PCC) extrinsic similarities $\\hat{d}_{\\psi^{\\prime}}$ and $\\hat{d}_{\\nu(V,\\Delta)}^{\\varkappa}$ d\u02c6VpV,\u2206q across various GLUE datasets. \\* indicates a $p$ -value $<0.01$ (assuming independence). "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Although encoders may not be affinely related in practice, empirical evidence in $\\S7$ suggests that notions of affine order still surface (cf. Tab. 1, Fig. 2), particularly as differently initialized BERTs exhibit variations in downstream task performance [29]. While other similarity measures, such as those used in seed specificity tests [12], are designed to remain invariant to initialization changes, our results indicate that intrinsic affine homotopy is appropriately sensitive to them. This sensitivity raises new questions about the landscape of pre-trained encoders; as seen in Fig. 1, asymmetry in intrinsic affine similarity among similarly pre-trained encoders impacts downstream performance, as corroborated by Lem. 5.2 and empirical results in Tab. 1. Differences in representation ranks may partly explain this asymmetry\u2014mapping between artificially generated rank-deficient encoders yields mostly symmetric affine distances (cf. Fig. 3). Another explanation might be that easy-to-learn encoders might be approximately linear combinations of others, making them easy to map to but not necessarily from. Overall, our findings highlight the need to account for directionality in encoder similarity measures to address the asymmetry inherent in this problem. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We discuss the structure of the space of language encoder in the framework of $S$ -homotopy\u2014the notion of aligning encoders with a chosen set of functions. We formalize affine alignment between encoders and show that it provides upper bounds on the differences in performance on downstream tasks. Experiments show our notion of intrinsic affine homotopy to be consistently predictive of downstream task behavior while revealing an asymmetric order in the space of encoders. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper presents foundational research about the similarity of language encoders. To the best of our knowledge, there are no ethical or negative societal implications to this work. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ryan Cotterell acknowledges support from the Swiss National Science Foundation (SNSF) as part of the \u201cThe Forgotten Role of Inductive Bias in Interpretability\u201d project. Anej Svete is supported by the ETH AI Center Doctoral Fellowship. Robin Chan acknowledges support from FYAYC. We thank Rapha\u00ebl Baur and Furui Cheng for helpful discussions and reviews of the current manuscript. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yamini Bansal, Preetum Nakkiran, and Boaz Barak. 2021. Revisiting model stitching to compare neural representations. In Advances in Neural Information Processing Systems, volume 34, pages 225\u2013236. Curran Associates, Inc.   \n[2] Saaid Baraty, Dan A. Simovici, and Catalin Zara. 2011. The impact of triangular inequality violations on medoid-based clustering. In Foundations of Intelligent Systems, pages 280\u2013289, Berlin, Heidelberg. Springer Berlin Heidelberg.   \n[3] Enric Boix-Adsera, Hannah Lawrence, George Stepaniants, and Philippe Rigollet. 2022. Gulp: a prediction-based metric between representations. In Advances in Neural Information Processing Systems, volume 35, pages 7115\u20137127. Curran Associates, Inc.   \n[4] Dmitri Burago, Yuri Burago, and Sergei Ivanov. 2001. A Course in Metric Geometry. American Mathematical Society, Providence, RI.   \n[5] C. Chang, W. Liao, Y. Chen, and L. Liou. 2016. A mathematical theory for clustering in metric spaces. IEEE Transactions on Network Science and Engineering, 3(01):2\u201316.   \n[6] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations.   \n[7] Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu, and Li Du. 2023. Formal aspects of language modeling. arXiv preprint arXiv:2311.04329.   \n[8] Adri\u00e1n Csisz\u00e1rik, P\u00e9ter K\u02ddor\u00f6si-Szab\u00f3, \u00c1kos Matszangosz, Gergely Papp, and D\u00e1niel Varga. 2021. Similarity and matching of neural network representations. In Advances in Neural Information Processing Systems, volume 34, pages 5656\u20135668. Curran Associates, Inc.   \n[9] Alexander D\u2019Amour, Katherine A. Heller, Dan I. Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin G. Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. 2020. Underspecification presents challenges for credibility in modern machine learning. Journal of Machine Learning Research, 23:226:1\u2013226:61.   \n[10] Sanjoy Dasgupta and Philip M. Long. 2005. Performance guarantees for hierarchical clustering. Journal of Computer and System Sciences, 70(4):555\u2013569. Special Issue on COLT 2002.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u2013 4186, Minneapolis, Minnesota. Association for Computational Linguistics.   \n[12] Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. 2021. Grounding representation similarity through statistical testing. In Advances in Neural Information Processing Systems, volume 34, pages 1556\u20131568. Curran Associates, Inc.   \n[13] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.   \n[14] William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).   \n[15] Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, and Ryan Cotterell. 2023. A measure-theoretic characterization of tight language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9744\u20139770, Toronto, Canada. Association for Computational Linguistics.   \n[16] Bolin Gao and Lacra Pavel. 2017. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 1704.00805.   \n[17] Jean Goubault-Larrecq. 2013. Non-Hausdorff Topology and Domain Theory: Selected Topics in Point-Set Topology. New Mathematical Monographs. Cambridge University Press.   \n[18] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch\u00f6lkopf. 2005. Measuring statistical dependence with Hilbert-Schmidt norms. In Algorithmic Learning Theory, pages 63\u201377, Berlin, Heidelberg. Springer Berlin Heidelberg.   \n[19] William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic word embeddings reveal statistical laws of semantic change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1489\u20131501, Berlin, Germany. Association for Computational Linguistics.   \n[20] David Roi Hardoon, S\u00e1ndor Szedm\u00e1k, and John Shawe-Taylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16:2639\u2013 2664.   \n[21] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. 2023. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329.   \n[22] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. 2019. Similarity of neural network representations revisited. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3519\u20133529. PMLR.   \n[23] Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. 2008. Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2.   \n[24] Serge Lang. 2002. Algebra. Springer New York.   \n[25] F. W. Lawvere. 2002. Metric spaces, generalized logic, and closed categories. Theory and Applications of Categories No. 1 (2002) pp 1-37.   \n[26] Karel Lenc and Andrea Vedaldi. 2019. Understanding image representations by measuring their equivariance and equivalence. International Journal of Computer Vision, 127(5):456\u2013476.   \n[27] Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John. 2015. Convergent learning: Do different neural networks learn the same representations? In Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015, volume 44 of Proceedings of Machine Learning Research, pages 196\u2013212, Montreal, Canada. PMLR.   \n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv, abs/1907.11692.   \n[29] R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2020. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217\u2013227, Online. Association for Computational Linguistics.   \n[30] Ari Morcos, Maithra Raghu, and Samy Bengio. 2018. Insights on representational similarity in neural networks with canonical correlation. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.   \n[31] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes: The Art of Scientific Computing, 3rd edition. Cambridge University Press.   \n[32] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. 2017. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.   \n[33] Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Bernhard Sch\u00f6lkopf, and Ryan Cotterell. 2023. All roads lead to Rome? Exploring the invariance of transformers\u2019 representations. arXiv preprint arXiv:2305.14555.   \n[34] Peter H. Sch\u00f6nemann. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1\u201310.   \n[35] Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander D\u2019Amour, Tal Linzen, Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das, et al. 2022. The MultiBERTs: BERT reproductions for robustness analysis. In International Conference on Learning Representations. OpenReview.net.   \n[36] Mahdiyar Shahbazi, Ali Shirali, Hamid Aghajan, and Hamed Nili. 2021. Using distance on the Riemannian manifold to compare representations in brain and in models. NeuroImage, 239:118271.   \n[37] C.G. Small. 2012. The Statistical Theory of Shape. Springer Series in Statistics. Springer New York.   \n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.   \n[39] Hrishikesh D. Vinod. 1976. Canonical ridge and econometrics of joint production. Journal of Econometrics, 4:147\u2013166.   \n[40] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.   \n[41] Fei Wang and Jimeng Sun. 2015. Survey on distance metric learning and dimensionality reduction in data mining. Data Mining and Knowledge Discovery, 29(2):534\u2013564.   \n[42] Alex H. Williams, Erin Kunz, Simon Kornblith, and Scott Linderman. 2021. Generalized shape metrics on neural representations. In Advances in Neural Information Processing Systems, volume 34, pages 4738\u20134750. Curran Associates, Inc.   \n[43] Peter N. Yianilos. 1993. Data structures and algorithms for nearest neighbor search in general metric spaces. In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201993, page 311\u2013321, USA. Society for Industrial and Applied Mathematics.   \n[44] Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob Steinhardt. 2021. Are larger pretrained language models uniformly better? Comparing performance at the instance level. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3813\u20133827, Online. Association for Computational Linguistics. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "FTpOwIaWUz/tmp/bff333105201ddfb23154b33e9ffb120929aaa7a7e5cc48da80e4e9e68598696.jpg", "table_caption": ["A Notation "], "table_footnote": ["Table 2: A summary of notation used in the paper. "], "page_idx": 13}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we address some of our work\u2019s limitations. ", "page_idx": 13}, {"type": "text", "text": "Non-Linear Encoder Relationships. This work focuses on affine similarity between encoders. As we find and discuss in $\\S4$ with the example of MULTIBERTs, language encoder representations may generally not be exactly affinely related. Nevertheless, understanding the affine-homotopy relationships on ${\\mathcal{E}}_{V}$ still helps us to make conclusions about practical findings as in $\\S7$ . ", "page_idx": 13}, {"type": "text", "text": "Linear Classifiers. Our work provides precise theoretical guarantees on the performance of linear classifiers applied to affinely-related encoders. In practice, task fine-tuning can take the form of more complex models, such as re-training entire pre-trained models. This work does not cover such more complex fine-tuning techniques. ", "page_idx": 13}, {"type": "text", "text": "Numerical Approximations. To bridge our theoretical findings on affine homotopy relationships in ${\\mathcal{E}}_{V}$ with their practical implementations in $\\S7$ , we concede several approximations. For instance, while $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}$ is valuable in analysis, optimizing Eq. (23) directly is computationally challenging and requires costly approximations. Similarly, in computing intrinsic distances across all representation layers in Fig. 1, we optimize for mean squared error (MSE) and evaluate the maximum loss instead of optimizing for it directly, which serves as an approximation of $d$ that results in more stable optimization given computational constraints. Finally, we address numerical inaccuracies encountered during singular value decomposition (SVD) computations in $\\S7$ , which we mitigate by tuning the rank according to precision $\\epsilon$ . ", "page_idx": 13}, {"type": "text", "text": "C Additional Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we complement our discussion in $\\S6$ and $\\S8$ with additional related work. ", "page_idx": 13}, {"type": "text", "text": "Representational and Functional Similarity. Our work is related to the ongoing efforts to quantify the similarity between neural networks. Much related work discusses similarity measures in terms of the invariance properties of neural networks [12, 22, inter alia]; see Klabunde et al. [21] for a recent comprehensive survey. Notably, Klabunde et al. [21] compile various representational [19, 23, 27, 32, 39, inter alia] and functional ways to measure similarity, which are related to our notions of intrinsic and extrinsic homotopy, respectively. Whereas our notion of intrinsic affine homotopy fits into the class of linear alignment-based measures [12, 27, 42, inter alia] as described in $\\S6$ , the notion of extrinsic similarity ftis into the broader category of performance-based functional measures [1, 8, 26]. Most relevantly, Boix-Adsera et al. [3] propose the GULP metric that provides a bound on the expected prediction dissimilarity for norm-one-bounded ridge regression. ", "page_idx": 14}, {"type": "text", "text": "Similarity Measures as Metrics. A line of work draws from statistical shape analysis [37] to motivate the development of similarity measures that are that conform to axioms of valid metrics [3, 36, 42]. Learning within proper metric spaces provides certain theoretical guarantees [2, 5, 10, 41, 43]. For example, Williams et al. [42] derive two families of generalized shape metrics, modifying existing dissimilarity measures to ensure they meet metric criteria. Notably, one of these generalized shape metrics is based on linear regression over the group of linear isometries, similar to the approach derived for encoder maps in Prop. 3.1. ", "page_idx": 14}, {"type": "text", "text": "Understanding Similarity of Language Encoders. Finally, several previous works characterize the landscape of language encoders and their sensitivity to slight changes to the pre-training or fine-tuning procedure [9, 13, 44]. This prompted multi-seed releases of encoders such as BERT [35, 44] that are frequently used for robustness or sensitivity analysis [12, 33], similar to the one presented in this work. ", "page_idx": 14}, {"type": "text", "text": "D Addenda on Affine Homotopy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide additional derivations and proofs complementing the discussion in $\\S3{-}\\S5$ ", "page_idx": 14}, {"type": "text", "text": "D.1 Preliminaries on Hemi-Metric Spaces ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition D.1. Let $(X,d)$ be a hemi-metric space. The open ball $B(x,\\epsilon)$ of center $x$ and radius $\\epsilon>0$ , is the set $\\{y\\in X\\mid d(x,y)<\\epsilon\\}$ . The open balls form a base for the open ball topology.26 ", "page_idx": 14}, {"type": "text", "text": "Lemma 4.1. Let $(X,d)$ be a hemi-metric space. The relation p $\\overline{{x}}\\succeq_{d}y$ iff $d(x,y)=0)$ q is a preorder27 and it will be called the specialization ordering of $d$ . ", "page_idx": 14}, {"type": "text", "text": "Example D.1. An example of a specialization ordering is the prefix ordering of strings $\\mathrm{\\leqslant}\\mathrm{prefix}^{28}$ . More precisely, for any $\\pmb{y},\\pmb{y}^{\\prime}\\in\\Sigma^{*}$ , we define $d_{\\Sigma^{*}}(\\boldsymbol{y},\\boldsymbol{y}^{\\prime})$ to be zero if $\\textit{\\textbf{y}}$ is a prefix of $\\pmb{y}^{\\prime}$ and $2^{-n}$ otherwise, where $n$ is the length of the longest prefix of $\\textit{\\textbf{y}}$ that is also a prefix of $\\pmb{y}^{\\prime}$ . Then $(\\Sigma^{*},d_{\\Sigma^{*}})$ is a hemi-metric space whose specialization ordering is $\\leqslant_{\\mathrm{prefix}}$ . // ", "page_idx": 14}, {"type": "text", "text": "Lemma D.1. Let $(X,d)$ be a hemi-metric space. ", "page_idx": 14}, {"type": "text", "text": "1. The set $\\{x\\in X\\mid d^{\\varkappa}(x,E)=0\\}$ is exactly the closure of $E$ in the open ball topology.   \n2. For any $x,x^{\\prime}\\in X$ , we have the inequality $d^{\\varkappa}(x,E)\\leqslant d(x,x^{\\prime})+d^{\\varkappa}(x^{\\prime},E)$ . If d is a metric, then $d^{\\mathcal{H}}(\\cdot,E)$ is 1-Lipschitz from $(X,d)$ to $\\overline{{\\mathbb{R}}}_{+}$ .   \n3. Let ${\\mathcal{Z}}\\subset{\\mathcal{P}}(X)$ be any space of non-empty subsets of $X$ . The Hausdorff\u2013Hoare map $d^{\\mathcal{H}}$ is hemi-metric on $\\mathcal{Z}$ . Its specialization ordering $\\succ_{d^{\\varkappa}}$ is given by $E\\gtrsim_{d}\\stackrel{\\sim}{c}E^{\\prime}i\\beta^{29}E\\subset\\stackrel{\\cdot}{c}c l(E^{\\prime}),$ , iff $c l(E)\\subset c l(E^{\\prime})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. See Goubault-Larrecq [17, Lemma 6.1.11, Proposition 6.2.16 & Lemma 7.5.1]. ", "page_idx": 14}, {"type": "text", "text": "D.2 Additional Derivations: Affine Alignment Measures ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Remark 3.2. $d_{\\mathrm{Aff}(V)}$ defined in Eq. (8a) is not a metric on ${\\mathcal{E}}_{V}$ .30 Further, when $S=\\operatorname{Aff}(V)$ , the map $\\operatorname{inf}_{\\psi,\\psi^{\\prime}\\in\\operatorname{Aff}(V)}$ $\\|\\dot{\\psi}\\circ h-\\psi^{\\prime}\\circ g\\|_{\\infty}$ is trivially zero by Cor. D.1. ", "page_idx": 15}, {"type": "text", "text": "Proof. To see that $d_{\\mathrm{Aff}(V)}$ is not a metric, consider the following two encoders: $g(\\pmb{y})=|\\pmb{y}|\\cdot e$ , where $e\\in V$ is any fixed vector, and $^h$ be any map from $\\Sigma^{*}$ to the ball $B(0_{V},1)_{.}$ of radius one. In such a case, we have $d_{\\mathrm{Aff}(V)}(h,g)=\\infty$ . Even on the space of bounded encoders 31 $d_{\\mathrm{Aff}(V)}$ is not a metric. We provide the following counter-example: Let $^h$ be any rank $R$ encoder, e.g., $^h$ can be any map that sends the first $R$ strings to the basis of $V$ . Let $A$ be a non-invertible linear map of $V$ and set $\\pmb{g}\\,=\\,A(\\pmb{h})$ . Then clearly $\\bar{d}_{\\mathrm{Aff}(V)}(h,g)=0$ , but $d_{\\mathrm{Aff}(V)}(\\boldsymbol{g},\\boldsymbol{h})$ can not be zero for dimensionality reasons (see Thm. 4.1). ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2 (Hausdorff Distance). Let $E,E^{\\prime}\\subset\\mathcal{E}_{V}$ . The map ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{\\infty}^{\\mathcal{H},\\;s y m}(E,E^{\\prime})\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}(d_{\\infty}^{\\mathcal{H}}(E,E^{\\prime}),d_{\\infty}^{\\mathcal{H}}(E^{\\prime},E))=\\operatorname*{sup}_{h\\in\\mathcal{E}_{V}}|d_{\\infty}^{\\mathcal{H}}(h,E)-d_{\\infty}^{\\mathcal{H}}(h,E^{\\prime})|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is an extended pseudo-metric on $\\mathcal{P}(\\mathcal{E}_{V})\\backslash\\{\\mathcal{D}\\}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. It follows readily from Lem. D.1. See also Burago et al. [4, $\\S7.3.1]$ . ", "page_idx": 15}, {"type": "text", "text": "For any affine subgroup $S\\subset\\operatorname{Aff}(V)$ , let $S(h)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{\\psi(h)\\ |\\ \\psi\\in S\\}$ . It then follows immediately from Lem. D.2 that the map $d_{S}^{\\mathcal{H},\\mathrm{\\sym}}(h,g)\\stackrel{\\mathrm{def}}{=}d_{\\infty}^{\\mathcal{H}}$ , $^{\\mathrm{sym}}(S({\\pmb h}),S({\\pmb g}))$ is an extended pseudo-metric on ${\\mathcal{E}}_{V}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma D.3. For any $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ , any $\\psi\\in\\mathrm{Iso}(V)$ and any non-empty $S\\subset{\\mathcal{E}}_{V}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{S}(\\psi\\circ h,g)=d_{\\psi^{-1}S}({\\pmb h},{\\pmb g}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, $d_{\\mathrm{Iso}(V)}(\\psi\\circ h,g)=d_{\\mathrm{Iso}(V)}(h,g)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Lem. D.3 follows by definition $d_{\\infty}(\\psi\\circ\\pmb{h},\\psi\\circ\\pmb{g})=d_{\\infty}(\\pmb{h},\\psi^{-1}\\circ\\psi\\circ\\pmb{g}).$ ", "page_idx": 15}, {"type": "text", "text": "Proposition 3.1. The pair $(\\mathcal{E}_{V},d_{\\mathrm{Iso}(V)})$ is an extended pseudo-metric space. ", "page_idx": 15}, {"type": "text", "text": "Proof. Using Lem. D.3, one can show that $d_{\\mathrm{Iso}(V)}(h,g)=d_{\\infty}^{\\gamma_{l}}$ , $^{\\mathrm{sym}}(\\mathrm{Iso}(h),\\mathrm{Iso}(g))$ , where $\\operatorname{Iso}(h)\\stackrel{\\mathrm{def}}{=}$ $\\{\\psi\\circ h\\colon\\psi\\in\\operatorname{Iso}(V)\\}$ . The proposition follows then from Lem. D.2. \u25a0 ", "page_idx": 15}, {"type": "text", "text": "For any $\\psi\\in\\operatorname{Aff}(V)$ and any $h\\in{\\mathcal{E}}_{V}$ , we then have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi\\circ h\\in\\mathcal{E}_{b}\\Leftrightarrow h\\in\\mathcal{E}_{b}\\Leftrightarrow\\|h\\|_{\\infty}<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma D.4. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. If $h\\in\\mathcal{E}_{b}$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|h\\|_{\\mathrm{Iso}(V)}=\\|h\\|_{\\mathcal{T}}=r_{h},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $r_{h}$ denotes the radius of $^h$ , which we define as the radius of the minimum enclosing ball of the set $h(\\Sigma^{*})$ , and the $\\|\\cdot\\|_{\\mathrm{Iso}(V)}$ norm is defined as in Eq. (8). ", "page_idx": 15}, {"type": "text", "text": "2. For any $\\psi\\in\\operatorname{Aff}(V)$ and a subset $S\\subset\\operatorname{Aff}(V)$ normalized $^{132}$ by $\\psi$ and containing $\\tau$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\psi\\circ\\pmb{h}\\|_{S}\\leqslant\\|\\psi_{l i n}\\|_{V}\\cdot\\|\\pmb{h}\\|_{S},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the $\\|\\cdot\\|_{S}$ norm is defined as in Eq. (8). ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "text", "text": ". Let $t\\in\\mathcal T$ be the translation moving the center of the ball enclosing $h(\\Sigma^{*})$ to the center $0_{V}$ . Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|h\\|_{\\mathrm{Iso}(V)}\\leqslant\\|h\\|_{\\mathcal{T}}\\leqslant\\|t\\circ h\\|_{\\infty}=r_{h}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now observe that for any other isometry $\\psi\\neq t$ , then $r_{\\psi\\circ h}=r_{h}$ . The ball $B(0_{V},\\|\\psi\\circ h\\|_{\\infty})$ clearly contains all points in $\\psi\\circ h(\\Sigma^{*})$ , hence by definition of the radius $r_{\\psi\\circ h}$ we must have $\\|\\psi\\circ\\pmb{h}\\|_{\\infty}\\leqslant r_{h}$ , which finishes the proof of $^{1}$ . ", "page_idx": 16}, {"type": "text", "text": "2. Write $\\psi=\\phi_{\\mathrm{lin}}\\circ t$ , with $t\\in\\mathcal T$ . We then have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\psi\\circ\\pmb{h}\\|_{S}=\\underset{\\phi\\in S}{\\operatorname*{inf}}\\;\\|\\phi(\\psi\\circ\\pmb{h})\\|_{\\infty}}&{}\\\\ {=\\underset{\\phi\\in S}{\\operatorname*{inf}}\\;\\|\\psi_{\\mathrm{lin}}(\\underbrace{\\psi_{\\mathrm{lin}}^{-1}\\circ\\phi\\circ\\psi_{\\mathrm{lin}}\\circ t\\circ\\pmb{h}}_{\\in S})\\|_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\phi\\mapsto\\psi_{\\operatorname*{lin}}^{-1}\\circ\\phi\\circ\\psi_{\\operatorname*{lin}}\\circ t$ is by definition a bijection of $S$ , hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\psi\\circ h\\|_{S}=\\underset{\\phi\\in S}{\\operatorname*{inf}}\\,\\,\\|\\psi_{\\mathrm{lin}}(\\phi\\circ h)\\|_{\\infty}}\\\\ &{\\qquad\\qquad=\\underset{\\phi\\in S}{\\operatorname*{inf}}\\,\\,\\underset{y\\in\\Sigma^{*}}{\\operatorname*{sup}}\\,\\,\\|\\psi_{\\mathrm{lin}}\\left((\\phi\\circ h)(y)\\right)\\|_{V}}\\\\ &{\\qquad\\qquad\\leqslant\\|\\psi_{\\mathrm{lin}}\\|_{V}\\underset{\\phi\\in S}{\\operatorname*{inf}}\\,\\,\\underset{y\\in\\Sigma^{*}}{\\operatorname*{sup}}\\,\\,\\big|(\\phi\\circ h)(y))\\big|_{V}}\\\\ &{\\qquad=\\|\\psi_{\\mathrm{lin}}\\|_{V}\\cdot\\|h\\|_{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary D.1. Let $S\\supset\\tau$ such that $\\operatorname{inf}_{\\psi\\in S}\\|\\psi_{\\operatorname{lin}}\\|_{V}$ .33 Then, $\\begin{array}{r}{d_{S}(\\pmb{h},\\pmb{g})\\stackrel{\\mathrm{def}}{=}\\operatorname*{inf}_{\\psi,\\psi^{\\prime}\\in S}\\|\\psi\\circ\\pmb{h}-\\psi^{\\prime}\\circ}\\end{array}$ $\\begin{array}{r}{\\pmb{g}\\|_{\\infty}=0}\\end{array}$ for all $\\boldsymbol{h},\\boldsymbol{g}\\in\\mathcal{E}_{b}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Note that $d_{S}(\\psi\\circ h,g)\\leqslant\\|\\psi_{\\mathrm{lin}}\\|_{V}\\cdot d_{S}(h,g)$ , which follows from Lem. D.4. Hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{S}(\\pmb{h},\\pmb{g})=\\underset{\\psi\\in S}{\\operatorname*{inf}}\\;d_{S}(\\psi\\circ\\pmb{h},\\pmb{g})}\\\\ &{\\qquad\\qquad\\leqslant\\underset{\\pmb{\\psi}\\in S}{\\operatorname*{inf}}\\;\\|\\psi_{\\operatorname*{lin}}\\|_{V}\\;d_{S}(\\pmb{h},\\pmb{g}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D.3 Proofs: Intrinsic Affine Homotopy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 4.2. The relation $\\succ_{\\mathrm{Aff}}$ is a preorder on ${\\mathcal{E}}_{V}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Since $d_{\\mathrm{Aff}(V)}(\\psi\\circ\\pmb{h},\\pmb{g})\\leqslant\\|\\psi_{\\mathrm{lin}}\\|_{V}\\cdot d_{\\mathrm{Aff}(V)}(\\pmb{h},\\pmb{g})$ (see Lem. D.4), ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{\\mathrm{Aff}(V)}({\\pmb h},{\\pmb g})=0\\Leftrightarrow d_{\\mathrm{Aff}(V)}^{\\varkappa}({\\pmb h},{\\pmb g})=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the relation $\\succ_{\\mathrm{Aff}}$ is the specialization ordering of the hemi-metric $d_{\\mathrm{Aff}(V)}^{\\mathcal{H}}$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 4.1. For $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nh\\gtrsim_{\\mathrm{Aff}}g\\Leftrightarrow h=\\psi(\\pi_{h}\\circ g)\\,f o r\\,s o m e\\ \\psi\\in\\mathrm{Aff}(V)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, $\\pi_{h}$ is the orthogonal projection of $V$ onto $V_{h}$ . In particular, if $d_{\\mathrm{Aff}(V)}(h,g)\\;=\\;0$ then $\\mathrm{rank}(h)\\leqslant\\mathrm{rank}(g)$ . If in addtion, we know ran $:(\\pmb{g})=\\mathrm{rank}(\\pmb{h}),$ , then $\\textbf{\\textit{g}}$ must by an affine transformation of $\\textbf{\\textit{g}}$ , i.e., $\\pmb{h}=\\psi\\circ\\pmb{g}$ for some $\\psi\\in\\operatorname{Aff}(V)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "text", "text": ". Recall from $\\S3.2$ that ${\\mathcal{E}}_{V}$ is complete with respect to the metric $d_{\\infty}$ . The condition $d(h,g)_{\\mathrm{Aff}(V)}=0$ simply means that there exists $\\phi_{n}\\in\\operatorname{Aff}(V)$ such that $\\mathrm{lim}_{n\\to\\infty}\\,\\phi_{n}\\circ\\pmb{g}=\\pmb{h}$ in ${\\mathcal{E}}_{V}$ , in other words $h\\in{\\overline{{\\mathrm{Aff}(g)}}}$ , i.e., $^h$ lies in the closure of $\\operatorname{Aff}(g)$ in ${\\mathcal{E}}_{V}$ . ", "page_idx": 16}, {"type": "text", "text": "Let $B_{h}\\subset\\Sigma^{*}$ such that $h(B_{h})$ is a basis for $V_{h}$ . Therefore, there exists $\\epsilon>0$ such that any family34 ", "page_idx": 17}, {"type": "equation", "text": "$$\n(v_{\\pmb y})_{\\pmb y}\\in\\prod_{{\\pmb y}\\in\\mathcal{B}_{h}}B({\\pmb h}({\\pmb y}),\\epsilon)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "has rank $\\dim_{\\mathbb{R}}(V_{h})$ . This shows that there exists $N\\geqslant1$ such that for any $n\\geqslant N$ one has $\\|h-\\phi_{n}\\circ\\pmb{g}\\|_{\\infty}<\\epsilon$ , and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{rank}(\\{\\phi_{n}\\circ\\pmb{g}(\\pmb{y})\\colon\\pmb{y}\\in\\mathcal{B}_{h}\\})=\\operatorname{rank}(\\pmb{h}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Which implies in particular ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dim_{\\mathbb{R}}(V_{h})\\leqslant\\dim_{\\mathbb{R}}(V_{g}){\\mathrm{~i.e.,~rank}}\\,h\\leqslant\\operatorname{rank}g.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2. If $\\operatorname{rank}(g)\\,=\\,\\operatorname{rank}(h)\\,=\\,D$ , then $\\textstyle\\operatorname*{lim}_{n\\to\\infty}\\phi_{n}\\,=\\,\\phi$ , where $\\phi$ is the affine map given by $g(y)\\mapsto h(y)$ for $y\\in\\mathcal{B}_{h}$ . Indeed, for any $\\begin{array}{r}{v=\\sum_{b\\in\\mathcal{B}_{h}}\\lambda_{b}b\\in V}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|(\\phi-\\phi_{n})(v)\\|\\leqslant\\|h-\\psi_{n}\\circ g\\|_{\\infty}\\sum_{b\\in h(\\mathcal{B}_{h})}|\\lambda_{b}|\\leqslant c\\|h-\\psi_{n}\\circ g\\|_{\\infty}\\,\\|v\\|_{V}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some constant $c>0$ , since all norms on $V$ are equivalent. Hence, $\\begin{array}{r l}{\\operatorname*{lim}_{n\\to\\infty}\\|\\phi-\\phi_{n}\\|_{V}=}&{{}}\\end{array}$ 0, which shows the claim. Accordingly, we must have $\\phi\\circ\\pmb{g}=\\pmb{h}$ . ", "page_idx": 17}, {"type": "text", "text": "Now we can prove Eq. (10): ", "page_idx": 17}, {"type": "text", "text": "$3\\Rightarrow$ . Given that $\\|h-\\pi_{h}\\circ\\phi_{n}\\circ g\\|_{\\infty}\\leqslant\\|h-\\phi_{n}\\circ g\\|_{\\infty}$ , we also have $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\pi_{h}\\circ\\phi_{n}\\circ g=h}\\end{array}$ . Write $\\pi^{\\perp}$ for the orthogonal projection on $V^{\\perp}$ and set $\\begin{array}{r}{\\pi_{h,n}=\\pi_{h}\\oplus\\frac{1}{n\\|\\phi_{n}\\|}\\pi_{h}^{\\perp}}\\end{array}$ . Note that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\pi_{h,n}=\\pi_{h}}\\end{array}$ . Accordingly, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\psi_{n}(\\pi\\circ{\\pmb g})=h,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\psi_{n}=\\pi_{h,n}\\phi_{n}\\pi_{h,n}^{-1}$ . From this, we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\mathrm{Aff}(V)}(h,\\pi_{h}\\circ g)=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now applying 2. yields $\\pmb{h}=\\phi(\\pi_{h}\\circ\\pmb{g})$ for some $\\phi_{h}\\in\\mathrm{Aff}(V_{h})$ , or $\\pmb{h}=\\phi(\\pi_{h}\\circ\\pmb{g})$ where $\\phi=\\phi_{h}\\oplus\\pi_{h}^{\\perp}\\in\\operatorname{Aff}(V)$ . ", "page_idx": 17}, {"type": "text", "text": "$3\\Leftarrow$ . Assume now that $\\pmb{h}=\\phi(\\pi_{h}\\circ\\pmb{g})$ for some $\\phi\\in\\operatorname{Aff}(V)$ . Then $\\begin{array}{r}{\\mathbf{\\lambda}_{h}=\\operatorname*{lim}_{n\\to\\infty}\\phi\\circ\\pi_{h,n}(g)}\\end{array}$ , where $\\pi_{h,n}=\\pi_{h}\\oplus\\textstyle{\\frac{1}{n}}\\pi_{h}^{\\perp}$ , which shows the desired implication. \u25a0 ", "page_idx": 17}, {"type": "text", "text": "D.4 Proofs: Extrinsic Homotopy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 5.2. Let $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ . We have ", "page_idx": 17}, {"type": "text", "text": "1. There exists a constant $c(\\lambda)>0$ such that for any $\\psi\\in\\operatorname{Aff}(V,W)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\infty,\\Delta^{N-1}}^{\\varkappa}(\\mathrm{softmax}_{\\lambda}(\\psi\\circ h),\\mathcal{V}_{N}(g))\\leqslant c(\\lambda)\\|\\psi_{l i n}\\|d_{\\mathrm{Aff}(V)}(\\pmb{h},\\pmb{g}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2. $d_{\\mathcal{V}(V,\\Delta)}^{\\varkappa}(h,g)\\leqslant c(\\lambda)d_{\\mathrm{Aff}(V,W)}^{\\varkappa}(h,g).$ ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "text", "text": "1. Clearly, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d_{\\infty,\\Delta^{N-1}}^{\\varkappa}(\\mathrm{softmax}_{\\lambda}(\\psi\\circ h),\\mathcal{V}_{N}(g))\\leqslant c(\\lambda)d_{\\mathcal{V}(V,W)}(\\psi\\circ h,\\mathrm{Aff}_{V,W}(g))}&{}\\\\ {\\leqslant c(\\lambda)\\underset{\\psi^{\\prime}\\in\\psi\\circ\\mathrm{Aff}(V)}{\\operatorname*{inf}}\\|\\psi\\circ h-\\psi^{\\prime}\\circ g\\|_{\\infty,W}}&{}\\\\ {=c(\\lambda)\\underset{\\psi^{\\prime}\\in\\mathrm{Aff}(V)}{\\operatorname*{inf}}\\|\\psi(h-\\psi^{\\prime}\\circ g)\\|_{\\infty,W}}&{}\\\\ {=c(\\lambda)\\|\\psi_{\\mathrm{lin}}\\|d_{\\mathrm{Aff}(V)}(h,g).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, the first inequality follows from the fact that softmax $\\cdot\\lambda$ is $c(\\lambda)$ -Lipschitz for some constant that depends on $\\lambda$ [16, Proposition 4]. ", "page_idx": 18}, {"type": "text", "text": "2. & 3. are are immediate consequences of 1. ", "page_idx": 18}, {"type": "text", "text": "Theorem 5.1 $\\epsilon$ -Intrinsic $\\Rightarrow\\mathcal{O}(\\epsilon)$ -Extrinsic). Let $\\boldsymbol h,\\boldsymbol g\\in\\mathcal E_{V}$ be two encoders. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)\\leqslant c(\\lambda)\\,d_{\\mathrm{Aff}(V)}^{\\mathcal{H}}(h,g).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\psi\\in\\operatorname{Aff}(V,W)$ . There exists a linear map $A\\colon V\\rightarrow W$ and a $\\phi_{V}\\in\\operatorname{GL}(V)$ , such that $\\psi=A\\circ\\phi$ and $\\|A\\|=1$ . Accordingly, Lem. 5.2 yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\infty,\\Delta^{N-1}}^{\\varkappa}(\\mathrm{softmax}_{\\lambda}(\\psi\\circ h),\\mathcal{V}_{N}(g))\\leqslant c(\\lambda)\\,d_{\\infty,W}(\\psi\\circ h,\\mathrm{Aff}_{V,W}(g))}\\\\ &{\\leqslant c(\\lambda)d_{\\mathrm{Aff}}(\\nu)(\\phi_{V}\\circ h,g)}\\\\ &{\\leqslant c(\\lambda)\\,\\operatorname*{sup}_{\\psi\\in\\mathrm{Aff}(V)}\\,(d_{\\mathrm{Aff}}(\\psi_{V}\\circ h,g))}\\\\ &{\\qquad\\qquad\\qquad\\quad=c(\\lambda)\\,d_{\\mathrm{Aff}(V)}^{\\varkappa}(h,g).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore $d_{\\mathcal{V}(V,\\Delta)}^{\\mathcal{H}}(h,g)\\leqslant c(\\lambda)\\,d_{\\mathrm{Aff}(V)}^{\\mathcal{H}}(h,g).$ . ", "page_idx": 18}, {"type": "text", "text": "E Addenda on Linear Alignment Methods for Finite Representation Sets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Linear Regression A common way to evaluate the similarity of two representation matrices $\\mathbf{H}\\ \\in\\ \\mathbb{R}^{N\\times\\breve{D}}$ and $\\textbf{G}\\in\\,\\mathbb{R}^{N\\times D}$ is through linear regression. Linear regression finds the matrix A\u02c6 P RD\u02c6D that minimizes the least squares error: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\bf A}=\\operatorname*{argmin}_{{\\bf A}\\in\\mathbb{R}^{D\\times D}}\\|{\\bf G}-{\\bf H}{\\bf A}\\|_{F}^{2}=({\\bf H}^{\\top}{\\bf H})^{-1}{\\bf H}^{\\top}{\\bf G}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ${\\bf H}\\,=\\,{\\bf Q_{H}}{\\bf R_{H}}$ and $\\mathbf{G}\\,=\\,\\mathbf{Q}_{\\mathbf{G}}\\mathbf{R}_{\\mathbf{G}}$ be the QR-decomposition of $\\mathbf{H}$ and $\\mathbf{G}$ , respectively. The goodness of fit is commonly evaluated through the $\\mathbf{R}$ -squared value $R_{L R}^{2}$ , i.e., as the proportion of variance in $\\mathbf{G}$ explained by the fit: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{L R}^{2}=1-{\\frac{\\|\\mathbf{G}-\\mathbf{H}\\mathbf{\\hat{A}}\\|_{F}^{2}}{\\|\\mathbf{G}\\|_{F}^{2}}}={\\frac{\\|\\mathbf{Q}_{\\mathbf{G}}^{\\top}\\mathbf{H}\\|_{F}^{2}}{\\|\\mathbf{G}\\|_{F}^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To derive Eq. (30), consider the fitted value $\\hat{\\bf G}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{G}}=\\mathbf{H}\\hat{\\mathbf{A}}=\\mathbf{H}(\\mathbf{H}^{\\top}\\mathbf{H})^{-1}\\mathbf{H}^{\\top}\\mathbf{G}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Q}_{\\mathbf{H}}\\mathbf{R}_{\\mathbf{H}}(\\mathbf{R}_{\\mathbf{H}}^{\\top}\\mathbf{Q}_{\\mathbf{H}}^{\\top}\\mathbf{Q}_{\\mathbf{H}}\\mathbf{R}_{\\mathbf{H}})^{-1}\\mathbf{R}_{\\mathbf{H}}^{\\top}\\mathbf{Q}_{\\mathbf{H}}^{\\top}\\mathbf{G}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Q}_{\\mathbf{H}}\\mathbf{Q}_{\\mathbf{H}}^{\\top}\\mathbf{G}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The residuals are therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{G}-\\hat{\\mathbf{G}}\\|_{F}^{2}=\\operatorname{tr}((\\mathbf{G}-\\hat{\\mathbf{G}})^{\\top}(\\mathbf{G}-\\hat{\\mathbf{G}}))}\\\\ &{\\qquad\\qquad=\\operatorname{tr}((\\mathbf{G}-\\hat{\\mathbf{G}})^{\\top}\\mathbf{G})}\\\\ &{\\qquad\\qquad=\\operatorname{tr}(\\mathbf{G}^{\\top}\\mathbf{G})-\\operatorname{tr}(\\mathbf{G}^{\\top}\\mathbf{Q}_{\\mathbf{H}}\\mathbf{Q}_{\\mathbf{H}}^{\\top}\\mathbf{G})}\\\\ &{\\qquad\\qquad=\\|\\mathbf{G}\\|_{F}^{2}-\\|\\mathbf{Q}_{\\mathbf{H}}^{\\top}\\mathbf{G}\\|_{F}^{2}.}\\end{array}(32\\mathbf{b},\\operatorname{residuals}\\ o r t h\\mathrm{ogo})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With this, we can compute the coefficient of determination as ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{L R}^{2}=1-\\frac{\\|\\mathbf G-\\hat{\\mathbf G}\\|_{F}^{2}}{\\|\\mathbf G\\|_{F}^{2}}=1-\\frac{\\|\\mathbf G\\|_{F}^{2}-\\|\\mathbf Q_{\\mathbf H}^{\\top}\\mathbf G\\|_{F}^{2}}{\\|\\mathbf G\\|_{F}^{2}}=\\frac{\\|\\mathbf Q_{\\mathbf H}^{\\top}\\mathbf G\\|_{F}^{2}}{\\|\\mathbf G\\|_{F}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Orthogonal Procrustes Problem. Let $\\mathbf{G}\\in\\mathbb{R}^{N\\times D}$ and $\\mathbf{H}\\in\\mathbb{R}^{N\\times D}$ representation matrices. In the orthogonal Procrustes problem, we seek to find the orthogonal matrix $\\mathbf{A}$ that best maps $\\mathbf{H}$ to $\\mathbf{G}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{argmin}_{\\mathbf{A}\\in\\mathrm{O}(V)}\\|\\mathbf{H}-\\mathbf{A}\\mathbf{G}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{G}-\\mathbf{H}\\mathbf{A}\\|_{F}^{2}=\\mathrm{tr}((\\mathbf{G}-\\mathbf{H}\\mathbf{A})^{\\top}(\\mathbf{G}-\\mathbf{H}\\mathbf{A}))}\\\\ &{\\qquad\\qquad=\\mathrm{tr}(\\mathbf{G}^{\\top}\\mathbf{G})-\\mathrm{tr}(\\mathbf{G}^{\\top}\\mathbf{H}\\mathbf{A})-\\mathrm{tr}(\\mathbf{A}^{\\top}\\mathbf{H}^{\\top}\\mathbf{G})+\\mathrm{tr}(\\mathbf{A}^{\\top}\\mathbf{H}^{\\top}\\mathbf{H}\\mathbf{A})}\\\\ &{\\qquad\\qquad=\\|\\mathbf{G}\\|_{F}^{2}+\\|\\mathbf{H}\\|_{F}^{2}-2\\mathrm{tr}(\\mathbf{A}^{\\top}\\mathbf{H}^{\\top}\\mathbf{G}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "an equivalent objective to Eq. (34) is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{A}}=\\underset{\\mathbf{A}\\in\\mathrm{O}(V)}{\\mathrm{argmax}}\\langle\\mathbf{A}\\mathbf{H},\\mathbf{G}\\rangle_{F}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\mathbf{U}\\mathbf{\\Sigma}\\Sigma\\mathbf{V}^{\\top}$ be the singular-value decomposition of $\\mathbf{H}^{\\top}\\mathbf{G}$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{A}}=\\underset{\\mathbf{A}\\in\\mathrm{O}(V)}{\\mathrm{argmax}}\\langle\\mathbf{A}\\mathbf{H},\\mathbf{G}\\rangle_{F}}\\\\ &{\\quad=\\underset{\\mathbf{A}\\in\\mathrm{O}(V)}{\\mathrm{argmax}}\\langle\\mathbf{A},\\mathbf{G}\\mathbf{H}^{\\top}\\rangle_{F}}\\\\ &{\\quad=\\underset{\\mathbf{A}\\in\\mathrm{O}(V)}{\\mathrm{argmax}}\\langle\\mathbf{A},\\mathbf{U}\\Sigma\\mathbf{V}^{\\top}\\rangle_{F}}\\\\ &{\\quad\\quad\\mathrm{A}\\mathrm{e}{\\mathrm{O}}(V)}\\\\ &{\\quad=\\underset{\\mathbf{A}\\in\\mathrm{O}(V)}{\\mathrm{argmax}}\\langle\\mathbf{U}^{\\top}\\mathbf{A}\\mathbf{V},\\Sigma\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{U}^{\\top}\\mathbf{A}\\mathbf{V}$ is a product of orthogonal matrices, and, therefore, orthogonal. Since $\\Sigma$ is diagonal, Eq. (35d) is maximized by $\\mathbf{U}^{\\top}\\hat{\\mathbf{A}}\\mathbf{V}=\\mathbf{I}$ , which means that $\\hat{\\mathbf{A}}=\\mathbf{U}\\mathbf{V}^{\\top}$ . ", "page_idx": 19}, {"type": "text", "text": "Canonical Correlation Analysis. We can rewrite the CCA objective from Eq. (21) as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{A},\\mathbf{B}}\\mathrm{tr}(\\mathbf{A}^{\\top}\\mathbf{H}\\mathbf{G}^{\\top}\\mathbf{B})\\qquad\\mathrm{s.t.}\\quad(\\mathbf{A}^{\\top}\\mathbf{H})(\\mathbf{A}^{\\top}\\mathbf{H})^{\\top}=(\\mathbf{B}^{\\top}\\mathbf{G})(\\mathbf{B}^{\\top}\\mathbf{G})^{\\top}=\\mathbf{I},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which, by definition of the Frobenius norm, is equivalent to Eq. (22). Let $\\mathbf{M}_{\\mathbf{H}\\mathbf{G}}=\\mathbf{H}\\mathbf{G}^{\\top}$ , $\\mathbf{M}_{\\mathbf{HH}}=$ $\\mathbf{H}\\mathbf{H}^{\\top}$ , $\\dot{\\mathbf{M}}_{\\mathbf{G}\\mathbf{G}}=\\mathbf{G}\\mathbf{G}^{\\top}$ , and let $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}=\\mathbf{M}_{\\mathbf{H}\\mathbf{G}}\\mathbf{\\Lambda}$ be the singular-value decomposition of $\\mathbf{M}_{\\mathbf{HG}}$ . One can show that the optimum of Eq. (22) is found at $(\\hat{\\mathbf{A}},\\hat{\\mathbf{B})}^{\\displaystyle\\circ}=(\\mathbf{M}_{\\mathbf{H}\\mathbf{H}}^{-\\frac{1}{2}}\\mathbf{U},\\mathbf{M}_{\\mathbf{G}\\mathbf{G}}^{-\\frac{1}{2}}\\mathbf{V})$ . Because $\\mathbf{A}^{\\top}\\mathbf{H}$ $\\mathbf{B}^{\\top}\\mathbf{G}$ , U, and $\\mathbf{V}$ are by definition orthogonal, we see that CCA first whitens the representations $(\\mathbf{H},\\mathbf{G})$ through $(\\mathbf{M}_{\\mathbf{H}\\mathbf{H}}^{-\\frac{1}{2}},\\mathbf{M}_{\\mathbf{G}\\mathbf{G}}^{-\\frac{1}{2}})$ and then orthogonally transforms them. This provides the intuition behind a close relationship between CCA and the Orthogonal Procrustes problem: For pre-whitened representation matrices, CCA (Eq. (22)) is equivalent to solving the Orthogonal Procrustes problem (Eq. (34)). To see this, let $\\mathbf{W}_{\\mathbf{H}}$ and $\\mathbf{W_{G}}$ be whitening transforms for $\\mathbf{H}$ and $\\mathbf{G}$ , respectively. Then, Eq. (22) is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{A},\\mathbf{B}\\in\\mathrm{O}(V)}\\|\\mathbf{A}^{\\top}\\mathbf{W}_{\\mathbf{H}}\\mathbf{H}-\\mathbf{B}^{\\top}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{A}\\mathbf{W}_{\\mathbf{H}}\\mathbf{H})(\\mathbf{A}\\mathbf{W}_{\\mathbf{H}}\\mathbf{H})^{\\top}=\\mathbf{A}\\mathbf{A}^{\\top}=\\mathbf{I},}\\\\ {(\\mathbf{B}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G})(\\mathbf{B}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G})^{\\top}=\\mathbf{B}\\mathbf{B}^{\\top}=\\mathbf{I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we can derive ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{\\mathbf{A},\\mathbf{B}\\in\\mathrm{O}(V)}{\\operatorname*{min}}\\,\\|\\mathbf{A}^{\\top}\\mathbf{W}_{\\mathbf{H}}\\mathbf{H}-\\mathbf{B}^{\\top}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G}\\|_{F}^{2}=\\underset{\\mathbf{A}\\mathbf{B}^{\\top}\\in\\mathrm{O}(V)}{\\operatorname*{min}}\\,\\|\\mathbf{A}^{\\top}\\|\\|\\mathbf{W}_{\\mathbf{H}}\\mathbf{H}-\\mathbf{A}\\mathbf{B}^{\\top}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G}\\|_{F}^{2}}&\\\\ &{}&{(39\\mathbf{a},\\mathbf{A}\\in\\mathrm{O}(V))}\\\\ &{}&{=\\underset{\\mathbf{C}\\in\\mathrm{O}(V)}{\\operatorname*{min}}\\,\\|\\mathbf{W}_{\\mathbf{H}}\\mathbf{H}-\\mathbf{C}^{\\top}\\mathbf{W}_{\\mathbf{G}}\\mathbf{G}\\|,}&\\\\ &{}&{\\quad(39\\mathbf{b}\\ \\mathrm{~C~\\stackrel{def}{=}~}\\mathbf{A}\\mathbf{R}^{\\top}\\mathbf{\\Xi}\\alpha\\mathrm{~O}(V))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is equivalent to solving the Orthogonal Procrustes problem (Eq. (34)) on the whitened matrices $\\mathbf{W}_{\\mathbf{H}}\\mathbf{H}$ and $\\mathbf{W}_{\\mathbf{G}}\\mathbf{G}$ . ", "page_idx": 19}, {"type": "text", "text": "F Experimental Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide additional details about the setup and compute resources of the experiments in $\\S7$ . To generate embeddings, we used the open-sourced code by Ren et al. [33]. Further, for Orthogonal Procrustes, CCA, PWCCA, and Linear CKA, we use the open source implementation by Ding et al. [12]. Our complete code is added as supplementary material. ", "page_idx": 20}, {"type": "text", "text": "Models and Datasets. We first extract the $\\textit{D}=\\ 768$ dimensional training set representations for SST-2, MRPC, RTE, CoLA, MNLI, and QQP across all 12 layers of ELECTRA [6], ROBERTA [28], and the 25 MULTIBERT [35] models from HuggingFace.35 The models and the MRPC dataset are licensed under Apache License 2.0. The SST-2 dataset is licensed under the Creative Commons CC0: Public Domain license. The RTE dataset is licensed under the CC BY 3.0 license. The CoLA dataset is licensed under the CC BY-SA 4.0 license. The MNLI dataset is licensed under the General Public License (GPL). THE QQP dataset is licensed under a custom non-commercial license.36 The dataset statistics are shown in Tab. 3. We note that for all experiments, MNLI and QQP were shortened to the first 10K training samples due to computational limitations. ", "page_idx": 20}, {"type": "table", "img_path": "FTpOwIaWUz/tmp/e5cb0378b29d54e1268bb050c11184bb7558a482fc8d1a53f2960cf23454f795.jpg", "table_caption": [], "table_footnote": ["Table 3: Statistics for the used GLUE benchmark [40] datasets. "], "page_idx": 20}, {"type": "text", "text": "Hyperparameters. Each experiment was run using Riemann $\\mathrm{1SGD}^{37}$ as an optimizer as it initially produced the best convergence when computing our affine similarity measures. Further, to account for convergence artifacts, we ran the intrinsic similarity computation optimizations in each experiment for learning rates r1E-4, 1E-3, 1E-2, 1E-1s and extrinsic computations for r1E-3, 1E-2, 2E-2s and report the best result. When training the task-specific linear probing classifier $\\psi^{\\prime}$ for $\\hat{d}_{\\psi^{\\prime}}$ , we use the crossentropy loss, RiemannSGD and optimize over the learning rates r1E-2, 1E-1, 2E-1, 4E-1s. For the computation of Hausdorff\u2013Hoare map $d^{\\mathcal{H}}$ , we fixed a lr of 1E-3 to save compute resources, as this lr generally leads to the best convergence in previous experiments. We used a batch size 64 and let optimization run for 20 epochs, keeping other parameters at default. For reproducibility, we set the initial seed to 42 during training. ", "page_idx": 20}, {"type": "text", "text": "Generating Random Affine Maps. For the last experiment, we generate random affine maps. To approximate $d^{\\mathcal{H}}$ we sample the matrix entries of the affine map from ${\\mathcal{N}}(0,1)$ . We then additionally normalize the transformed representation matrix as this leads to better convergence. To approximate $\\hat{d}_{\\nu(V,\\Delta)}^{\\mathcal{H}}$ , we fti a linear probe on $\\mathbf{H}$ to 100 sets of randomly generated class labels, for the embeddings of each task. The predictions of that probe then become what $\\mathbf{G}$ affinely maps to. In both cases, the seeds are set ascendingly from 0. ", "page_idx": 20}, {"type": "text", "text": "Compute Resources. We compute the embeddings on a single A100-40GB GPU, which took around two hours. All other experiments were run on $_{8-32}$ CPU cores, each with 8 GB of memory. Computing extrinsic distances between 600 model combinations across both datasets usually takes 2-3 hours on 8 cores, whereas intrinsic computation is more costly, and can run up to 4 hours. Note our approximation of Hausdorff\u2013Hoare maps (cf. Eq. (23)) across all models is significantly more costly due to our sampling approach and can take up to 72 hours to compute on 32 cores for large datasets such as SST-2, and up to 12 hours for MRPC. The resources needed for initially failed experiments do not significantly exceed the reported compute. ", "page_idx": 20}, {"type": "image", "img_path": "FTpOwIaWUz/tmp/18351f280c53ec82ba84696cb317ccdfef92663e37bd30d71f9bdbce8800de59.jpg", "img_caption": ["Figure 3: The effect of artificial rank deficiency averaged across MULTIBERTs. For each pair of embeddings $\\mathbf{H}^{(i)}$ and $\\mathbf{H}^{(j)}$ from MUTLIBERTs $\\mathcal{M}^{(i)}$ and $\\mathcal{M}^{(j)}$ we generate additional rankdeficient encoders HpXiq% and HpYj q% with X, Y P t20%, ..., 90%u of the full rank through SVD truncation. We compute dpHpYi q%, HpXjq%q, for each pair of possible rank-deficiency and finally report the median across all MULTIBERTs on row $X$ and column $Y$ on the grid. We additionally show row-, and column medians. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Additional Experimental Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Influence of Encoder Rank Deficiency. In Thm. 4.1 we discuss how the relative rank of encoders influences their affine alignment and derive the equivalence relation $\\simeq\\!\\mathrm{Aff}$ conditioned on equal rank between encoders. To test the effect of unequal rank on affine alignment in an isolated setup, we artificially construct reduced-rank encoders through singular value decomposition (SVD) truncation. In Figure 3 we expectedly find a trend in how the encoder rank influences affine mappability. We additionally highlight that the computed distances are rather symmetric, with no clear differences when mapping to $(^{\\rightarrow}\\mathcal{M})$ , rather than from $(\\mathcal{M}^{\\rightarrow})$ an encoder. Finally, we note the trend in the diagonal indicating that mapping between encoders of the same rank becomes easier between lower-rank encoders. ", "page_idx": 21}, {"type": "image", "img_path": "FTpOwIaWUz/tmp/20fdb974dc7fee6500435d9a1d30eb8e962cf447800307b6a3c21c3a8cb43490.jpg", "img_caption": ["Figure 4: Asymmetry between ELECTRA (E), RoBERTa (R), and MULTIBERT encoders (M1-M25) across layers. For each pair of the encoders $\\mathcal{M}^{(i)}$ and $\\mathcal{M}^{(j)}$ , we generate training set embeddings $\\mathbf{H}^{(i)},\\mathbf{H}^{(\\dot{j})}\\in\\mathbb{R}^{N\\times D}$ for the GLUE tasks SST-2, CoLA, MNLI, QQP, RTE, and MRPC. We then fit $\\mathbf{H}^{(i)}$ to $\\mathbf{H}^{(j)}$ with an affine map and report the goodness of fit through the max error L2 norm, i.e., an approximation of $d(\\mathbf{H}^{(j)},\\mathbf{H}^{(i)})$ on row $i$ and column $j$ of the grid. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims and contributions. The claims match the theoretical and experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We highlight limitations in the main text and add more general points in App. B. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: If not in the main text, all proofs along with their assumptions are in App. D and App. E. We additionally provide short proof sketches in the main text. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We describe all used models, procedures as well as parameters, seeds, and required compute resources either in the main text or in App. F. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Code pulls data from GLUE, is runnable, and is submitted as supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Again, if not listed in the main text, it is listed in App. F. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide significance indications in the form of p-values to all correlation computations and fitted regressions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Listed in App. F. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The authors do not foresee any ethical implications to this work. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Discussed in the final, \"Broader Impact\", section in the main body. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We cite the original paper that produced each re-used code snippet and dataset.   \nWe provide licenses for all models and datasets used in App. F. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not do crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We do not do crowdsourcing nor research with human subjects ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]