[{"figure_path": "QVG7j29Sta/tables/tables_2_1.jpg", "caption": "Table 1: Adding Gaussian noise to weights results in approximately equal correct \u2192 incorrect and incorrect \u2192 correct transitions, with the overall model accuracy mostly unchanged.", "description": "This table demonstrates that adding Gaussian noise to the model weights leads to a similar number of transitions from correct to incorrect answers and from incorrect to correct answers, while maintaining the overall model accuracy relatively unchanged. This finding highlights how adding noise to a model's weights can mimic the behavior of model compression techniques.", "section": "4 Results"}, {"figure_path": "QVG7j29Sta/tables/tables_7_1.jpg", "caption": "Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4", "description": "This table presents the MT-Bench average scores for various quantized models.  MT-Bench evaluates the free-form text generation capabilities of large language models. The scores are averages across two turns in the benchmark, with GPT-4 acting as an automated judge.  The table shows how different quantization techniques affect the overall performance of the models on the MT-Bench task, comparing them to the baseline 16-bit models.  Lower scores indicate poorer performance in free-form text generation.", "section": "6 MT-Bench evaluation"}, {"figure_path": "QVG7j29Sta/tables/tables_8_1.jpg", "caption": "Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4", "description": "This table presents the average scores obtained from evaluating the performance of various quantized language models on the MT-Bench task.  The evaluation was conducted using GPT-4 as an automated judge.  The table shows the average scores for two turns in the MT-Bench benchmark, allowing for a comparison of model performance across different quantization techniques (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4) and various model sizes (Llama-2 7b chat, Llama-2 13b chat, Llama-2 70b chat, Yi-6b chat, Yi-34b chat).  It demonstrates the impact of different compression techniques on the quality of free-form text generation in a multi-turn dialogue setting.", "section": "6 MT-Bench evaluation"}, {"figure_path": "QVG7j29Sta/tables/tables_16_1.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of evaluating the performance of different quantization schemes on the MMLU 5-shot task.  It compares the accuracy and the percentage of \"flips\" (instances where the model's answer changes from correct to incorrect or vice versa) for various models (Llama2-7b chat, Llama2-13b chat, Llama2-70b chat, Yi-6b chat, and Yi-34b chat) using six different quantization schemes (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, and BnB W4A4).  A key observation is that while the accuracy change is minimal for most schemes, the flip percentage is significantly higher for most quantization techniques than the baseline, indicating that there are considerable differences in the output of these models despite similar accuracy numbers. The exception is GPTQ W8A16, which exhibits negligible flips.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_16_2.jpg", "caption": "Table 5: PIQA (0-shot) change in % accuracy / %flips", "description": "This table presents the results of a zero-shot experiment on the PIQA dataset using various quantization schemes.  It shows the change in accuracy and the percentage of \"flips\" (instances where a model's answer changes from correct to incorrect or vice versa) compared to a 16-bit baseline model for Llama-2 7b, Llama-2 13b, Llama-2 70b, Yi-6b, and Yi-34b chat models.  Different quantization methods (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4) are evaluated.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_16_3.jpg", "caption": "Table 6: Hellaswag (0-shot) change in %accuracy / %flips", "description": "This table presents the results of evaluating six quantization schemes on the Hellaswag benchmark task. For each quantization scheme, the table shows the change in accuracy (percentage) and the percentage of flips compared to the baseline 16-bit model.  The results are broken down by model (Llama2-7b chat, Llama2-13b chat, Llama2-70b chat, Yi-6b chat, Yi-34b chat).  The table highlights the negligible difference in accuracy across different compression methods while simultaneously revealing the significant number of flips (indicating large underlying changes not captured by accuracy alone), except for the GPTQ W8A16 scheme.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_1.jpg", "caption": "Table 7: ARC Easy (0-shot) change in %accuracy / %flips", "description": "This table presents the results of experiments conducted on the ARC Easy dataset using a 0-shot setting.  It shows the change in accuracy and the percentage of flips observed for several different quantization schemes applied to various large language models.  The data illustrates the impact of these compression techniques on model behavior, highlighting the trade-off between maintaining accuracy and minimizing changes in the model's output.  A significant finding is the high number of flips, even when the overall accuracy is preserved, demonstrating that accuracy alone may not fully reflect the effect of these techniques on user experience.", "section": "A.1 Detailed results of Llama-2 chat and Yi-chat Families on various quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_2.jpg", "caption": "Table 8: ARC Challenge (0-shot) change in %accuracy / %flips", "description": "This table presents the results of evaluating the impact of six different quantization techniques on the ARC Challenge dataset.  The table shows the change in accuracy and the percentage of \"flips\" (instances where a correct answer becomes incorrect, or vice versa) compared to a 16-bit baseline model.  The results are broken down by model (Llama2 7b, 13b, 70b, and Yi 6b, 34b chat models) and quantization scheme (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4).  It highlights the observation that while accuracy changes are often negligible,  the percentage of flips can be substantial, indicating significant differences in the models' behavior even with similar accuracy.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_3.jpg", "caption": "Table 14: TriviaQA 5-shot Results", "description": "This table presents the results for the TriviaQA task, using a 5-shot setting.  It shows the baseline accuracy of the 16-bit model and the change in accuracy and percentage of flips for various lower-bit quantization schemes (BnB W8A8, BnB W4A4).  The table allows comparison of the performance of different quantization methods on this specific task, highlighting the trade-off between accuracy and the number of \"flips\" (where correct answers become incorrect and vice-versa).", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_4.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of experiments on the MMLU 5-shot task using different quantization schemes.  It shows that while the change in accuracy between the baseline 16-bit model and the quantized models is small (less than 2%), there's a significant difference in the number of \"flips\" (changes from correct to incorrect answers and vice-versa).  The table highlights the extent of this divergence for various models and quantization techniques, indicating that accuracy alone is an insufficient metric for evaluating model compression.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_5.jpg", "caption": "Table 12: MMLU 5-shot Results on Pretrained Models change in %Accuracy/%Flips", "description": "This table presents the results of experiments conducted on pretrained language models using the MMLU 5-shot benchmark.  It shows the change in accuracy and the percentage of \"flips\" (changes in answers from correct to incorrect, or vice versa) for various quantization schemes compared to a 16-bit baseline model. The models evaluated include Llama2-7b, Llama2-13b, Llama2-70b, Yi-6b, and Yi-34b.  Quantization techniques used are BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, and BnB W4A4. The table quantifies the impact of different quantization methods on model accuracy and answer consistency.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_6.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of applying six different quantization schemes to five different large language models (LLMs) on the MMLU 5-shot task. The goal was to evaluate the impact of quantization on model accuracy and a new metric called \"flips.\"  Flips measure the proportion of answers that change from correct to incorrect or vice-versa when comparing the quantized model to the baseline 16-bit model. The table shows that while the change in accuracy is minimal (between 0% and 2%), the number of flips is substantially larger for most quantization methods (except for GPTQ W8A16). This suggests that even when accuracy remains similar, the underlying model behavior can differ significantly due to quantization, highlighting the insufficiency of using accuracy alone for evaluation.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_17_7.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of an experiment evaluating the impact of various quantization schemes on the accuracy and the number of \"flips\" (changes in answers from correct to incorrect or vice-versa) in the MMLU (Massive Multitask Language Understanding) 5-shot benchmark.  It compares six different quantization techniques across four different language models (Llama2-7b chat, Llama2-13b chat, Llama2-70b chat, Yi-6b chat, and Yi-34b chat). The results show that while the overall accuracy remains largely unchanged, the number of flips varies significantly across different quantization methods, indicating a substantial divergence in model behavior despite similar accuracy scores.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_18_1.jpg", "caption": "Table 15: Turn 1 MT-Bench Scores", "description": "This table presents the MT-Bench scores for the first turn of the multi-turn dialogue task.  The scores are presented for various models and quantization methods.  The 16-bit model serves as a baseline, allowing for comparison across different compression techniques.  Higher scores indicate better performance.", "section": "A.2 MT-Bench Detailed results"}, {"figure_path": "QVG7j29Sta/tables/tables_18_2.jpg", "caption": "Table 16: Turn 2 MT-Bench Scores", "description": "This table presents the MT-Bench scores for the second turn of the multi-turn dialogue task.  The scores represent the average performance of various quantized models (using different quantization techniques and bit-depths) on the MT-Bench benchmark.  The baseline 16-bit model is included for comparison.  The table shows how the different compression methods impact the model's performance on this specific turn of the task.", "section": "A.2 MT-Bench Detailed results"}, {"figure_path": "QVG7j29Sta/tables/tables_18_3.jpg", "caption": "Table 17: MMLU (5-shot) change in % accuracy/ % flips", "description": "This table presents the results of a five-shot accuracy evaluation on the MMLU benchmark for various LLMs and different quantization schemes.  The baseline accuracy is shown alongside the change in accuracy and the percentage of flips observed when compared to the baseline 16-bit model. The table helps to quantify the divergence between the baseline and compressed models, even when accuracy differences are negligible.  The models compared are Qwen2-1.5B, Qwen2-7B, Qwen2-72B, Llama3-8B, and Llama3-70B.  The quantization methods are BnB W8A8, GPTQ W8A16, GPTQ W4A16, AWQ W4A16, and BnB W4A4.", "section": "A.3 Detailed results of Llama-3 and Qwen-2 Families on various quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_19_1.jpg", "caption": "Table 17: MMLU (5-shot) change in % accuracy/ % flips", "description": "This table presents the results of the MMLU 5-shot experiments, comparing the performance of various quantized models against a 16-bit baseline.  For several models (Qwen-2 and Llama-3 families), it shows the change in accuracy and the percentage of \"flips\" (answers changing from correct to incorrect or vice versa) for different quantization schemes (BnB W8A8, GPTQ W8A16, GPTQ W4A16, AWQ W4A16, BnB W4A4). The table highlights how even when accuracy remains relatively stable across various quantization techniques, the number of flips can vary significantly, indicating divergence in model behavior despite similar overall accuracy scores.", "section": "A.3 Detailed results of Llama-3 and Qwen-2 Families on various quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_19_2.jpg", "caption": "Table 19: ARC-Challenge (0-shot) change in % accuracy / % flips", "description": "This table presents the results of the ARC-Challenge (a question answering benchmark) zero-shot experiment.  It shows the change in accuracy and the percentage of \"flips\" (instances where a model changed an answer from correct to incorrect or vice versa) for various quantization schemes (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4) compared to a 16-bit baseline model.  The models used are Qwen2-1.5B, Qwen2-7B, Qwen2-72B, Llama3-8B, and Llama3-70B.  NA indicates that the data was not available for that specific model and quantization technique.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_19_3.jpg", "caption": "Table 20: MATH (4-shot) change in % accuracy / % flips", "description": "This table presents the results of evaluating the MATH dataset (a multiple-choice question-answering task) using various quantization methods. It shows the change in accuracy and the percentage of flips (answers changing from correct to incorrect or vice versa) compared to a 16-bit baseline model for different quantization techniques (BnB W8A8, GPTQ W8A16, GPTQ W4A16, AWQ W4A16, BnB W4A4).  The results are displayed for different model sizes (Qwen2-1.5B, Qwen2-7B, Qwen2-72B, Llama3-8B, Llama3-70B), demonstrating the impact of quantization on accuracy and answer consistency.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_19_4.jpg", "caption": "Table 21: GSM8k (8-shot) change in % accuracy/ % flips", "description": "This table presents the results of GSM8k (8-shot) experiments, evaluating the impact of various quantization schemes on model accuracy and flips.  The table compares the baseline 16-bit model to several quantized versions (BnB W8A8, GPTQ W8A16, GPTQ W4A16, AWQ W4A16, BnB W4A4), showing the percentage change in accuracy and the percentage of flips observed for each quantized model.  A flip is defined as an answer changing from correct to incorrect or vice versa.  The table is part of an analysis demonstrating that accuracy alone is insufficient for assessing the quality of compressed LLMs; the high number of flips indicates a substantial divergence in model behavior, despite only minor accuracy differences.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_19_5.jpg", "caption": "Table 17: MMLU (5-shot) change in % accuracy/ % flips", "description": "This table presents the results of experiments on the MMLU benchmark with 5-shot setting.  It shows the change in accuracy and the percentage of flips for various quantization schemes applied to Llama-3 and Qwen-2 families of models, in comparison to a 16-bit baseline model. The table helps demonstrate that even with negligible accuracy changes, a significant number of flips can occur due to quantization, highlighting the limitations of accuracy alone as a metric for assessing compressed models.", "section": "A.3 Detailed results of Llama-3 and Qwen-2 Families on various quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_20_1.jpg", "caption": "Table 23: BFCL-greedy change in % accuracy / % flips", "description": "This table presents the results of evaluating three different large language models (LLMs) using the BFCL-greedy benchmark. The BFCL-greedy benchmark is a tweaked version of the standard BFCL benchmark that uses greedy decoding instead of top-p sampling. This allows for a more direct measurement of the impact of quantization on model performance, without the added noise introduced by sampling. The table shows the change in accuracy and the percentage of \"flips\" (changes in answers from correct to incorrect, or vice versa) for each model when different quantization techniques are applied.  The models evaluated are Gemma-2B-it, Gemma-7B-it, and Llama-3-8B-Instruct.  The quantization techniques used are GPTQ W8A16, GPTQ W4A16, and AWQ W4A16.  The results demonstrate that while accuracy may not change significantly, the number of flips can increase substantially with quantization, indicating a divergence in model behavior.", "section": "4.2 Other model compression techniques"}, {"figure_path": "QVG7j29Sta/tables/tables_20_2.jpg", "caption": "Table 24: BFCL-standard change in % accuracy / % flips", "description": "This table presents the results of evaluating the BFCL task using the standard BFCL evaluation method (top_p sampling).  It shows the change in accuracy and the percentage of flips observed in Llama-3-8B-Instruct model with 16-bit baseline and different quantization schemes. The table highlights the significant percentage change in flips observed, even with small changes in accuracy, for two different runs of the 16-bit model and for GPTQ W8A16 and GPTQ W4A16.", "section": "4.3 Perplexity"}, {"figure_path": "QVG7j29Sta/tables/tables_22_1.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of experiments on the MMLU benchmark (5-shot setting) using various quantization schemes.  The accuracy change from the baseline 16-bit model is minimal for all schemes (within 2%). However, the percentage of \"flips\" (changes in answers from correct to incorrect or vice versa) is significant for all but one quantization method (GPTQ W8A16), suggesting that while accuracy remains largely unchanged, the compressed models behave differently from the baseline.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_22_2.jpg", "caption": "Table 26: Comparison of overlap coefficient amongst different quantization schemes: BnB W8A8, BnB W4A4, and GPTQ W4A16.", "description": "This table shows the overlap coefficient between pairs of quantization schemes. The overlap coefficient measures the proportion of samples that were impacted by both quantization schemes in a set of samples that were impacted by at least one of the quantization schemes. The higher the overlap coefficient, the higher the consistency between quantization schemes.  The results are calculated using Llama2-70b, MMLU, 5-shot and 15k questions.", "section": "A.4 Consistency of flips"}, {"figure_path": "QVG7j29Sta/tables/tables_22_3.jpg", "caption": "Table 27: Top margin when answer is correct/wrong. Top margin is higher for correct answers.", "description": "This table shows the average top margin for correct and incorrect answers across different datasets and LLMs.  The top margin is calculated as the difference between the probability assigned to the correct option and the probability assigned to the next most likely option.  A higher top margin indicates greater confidence in the model's prediction. The table demonstrates that when a model\u2019s prediction is correct, the top margin is generally much higher than when the prediction is incorrect.", "section": "A.5 Misc. Results"}, {"figure_path": "QVG7j29Sta/tables/tables_23_1.jpg", "caption": "Table 28: MMLU 5-shot. The first/second number indicates the % of correct/incorrect answers of the baseline model that changed. We see that more % of incorrect answers change.", "description": "This table presents the percentage of correct and incorrect answers that changed after applying different quantization methods to the Llama2 and Yi models on the MMLU 5-shot benchmark. The results show that a greater proportion of incorrect answers changed compared to correct answers after quantization.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_23_2.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of evaluating various quantization schemes on the MMLU 5-shot task.  It compares the accuracy and the percentage of \"flips\" (instances where a model changed its answer from correct to incorrect or vice versa) for different models (Llama2-7b, Llama2-13b, Llama2-70b, Yi-6b, Yi-34b) using six different quantization schemes (BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4).  The key observation is that while the change in accuracy across different quantization schemes is small (between 0 and 2%),  the number of \"flips\" significantly increases for most schemes, indicating a notable divergence in the model's behavior even when the overall accuracy remains similar.  This emphasizes the limitations of using accuracy alone to evaluate compressed models.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_28_1.jpg", "caption": "Table 28: MMLU 5-shot. The first/second number indicates the % of correct/incorrect answers of the baseline model that changed. We see that more % of incorrect answers change.", "description": "This table shows the percentage of correct and incorrect answers that changed from the baseline model to the quantized model for the MMLU 5-shot task.  The results are broken down by quantization method (BnB 8bit, SQ 8bit, GPTQ 4bit, AWQ 4bit, BnB 4bit) and model (Llama2-7b chat, Llama2-13b chat, Llama2-70b chat, Yi-6b chat, Yi-34b chat). A noteworthy observation is that a higher percentage of incorrect answers changed compared to correct answers.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_28_2.jpg", "caption": "Table 28: MMLU 5-shot. The first/second number indicates the % of correct/incorrect answers of the baseline model that changed. We see that more % of incorrect answers change.", "description": "This table shows the percentage of correct and incorrect answers that changed in the MMLU 5-shot experiment after applying different quantization methods.  The data is broken down for different models (Llama2-7b chat, Llama2-13b chat, Llama2-70b chat, Yi-6b chat, and Yi-34b chat). For each model and quantization technique, the table presents two numbers: the percentage of initially correct answers that became incorrect and the percentage of initially incorrect answers that became correct. This data demonstrates the phenomenon of \"flips\", where a significant proportion of correct answers become incorrect and vice-versa, even when overall accuracy remains relatively unchanged. The table highlights that a considerably higher percentage of incorrect answers change compared to correct answers.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_28_3.jpg", "caption": "Table 3: Qualitative evaluation of Llama2-70B-chat model text generations for MT-Bench prompts. Author's summary of model responses shown below; full model generated responses are in Appendix.", "description": "This table presents a qualitative evaluation of the Llama2-70B-chat model's performance on ten MT-Bench prompts.  The authors provide a summary of the model's responses, categorized by 16-bit, 8-bit, and 4-bit versions, and highlighting key differences and errors. Complete responses are available in the appendix.", "section": "6 MT-Bench evaluation"}, {"figure_path": "QVG7j29Sta/tables/tables_29_1.jpg", "caption": "Table 4: MMLU 5-shot accuracy and flips for several models using 16-bit baseline and various quantization schemes. Change in accuracy is negligible (0-2%) in all quantization schemes. However, except for GPTQ W8A16 (8-bit weights, 16-bit activation), all other schemes show large % flips, indicating significant deviation of quantized model from the baseline 16-bit model.", "description": "This table presents the results of experiments conducted on several large language models (LLMs) using various quantization schemes.  The goal was to evaluate the impact of quantization on the model's accuracy and the frequency of \"flips\" (changes from correct to incorrect answers, or vice versa). The table shows that while the change in accuracy is minimal for most quantization schemes, the percentage of flips is substantial in many cases, suggesting that model behavior can significantly differ even when aggregate accuracy remains similar.  This highlights the limitations of using accuracy alone as a metric for evaluating model compression.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/tables/tables_29_2.jpg", "caption": "Table 1: Adding Gaussian noise to weights results in approximately equal correct \u2192 incorrect and incorrect \u2192 correct transitions, with the overall model accuracy mostly unchanged.", "description": "This table demonstrates the effect of adding Gaussian noise to the model weights.  It shows that adding noise causes a roughly equal number of correct answers to become incorrect and incorrect answers to become correct, while leaving the overall accuracy relatively unchanged. This highlights the limitations of using accuracy alone as an evaluation metric for compressed models, as large underlying changes can occur without a significant change in overall accuracy.", "section": "Analyzing Flips"}, {"figure_path": "QVG7j29Sta/tables/tables_29_3.jpg", "caption": "Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4", "description": "This table presents the average scores for the two turns in the MT-Bench benchmark, as evaluated by GPT-4.  It compares the performance of different quantized models against the baseline (16-bit) model across various Llama and Yi model sizes. The scores reflect the quality of the model's free-form text generation capabilities in a multi-turn dialogue task.  Higher scores indicate better performance.  The table is useful in assessing how different quantization techniques affect the performance of the model on a more complex, open-ended task.", "section": "6 MT-Bench evaluation"}, {"figure_path": "QVG7j29Sta/tables/tables_30_1.jpg", "caption": "Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4", "description": "This table presents the average scores obtained from evaluating different quantized models on the MT-Bench task.  The evaluation was performed using GPT-4 as an automated judge, considering both turn 1 and turn 2 responses separately.  The table allows for a comparison of the performance of various compression techniques (indicated by the model names and quantization schemes) across multiple LLMs (Llama-2 and Yi families). Lower scores indicate poorer performance in the free-form text generation task.", "section": "6 MT-Bench evaluation"}, {"figure_path": "QVG7j29Sta/tables/tables_30_2.jpg", "caption": "Table 2: MT-Bench: Average of turn-1 and turn-2 scores, as evaluated by GPT4", "description": "This table presents the average scores obtained from evaluating various quantized models using the MT-Bench benchmark, with GPT-4 acting as the automated judge.  The scores reflect the models' performance on a multi-turn dialogue task and are broken down by model (Llama-2 7B chat, Llama-2 13B chat, Llama-2 70B chat, Yi-6B chat, Yi-34B chat) and quantization technique (16bit, BnB W8A8, GPTQ W8A16, SQ W8A8, GPTQ W4A16, AWQ W4A16, BnB W4A4).  Lower scores indicate poorer performance. The table highlights the impact of different quantization techniques on the models' ability to generate coherent and relevant responses in a conversational setting.", "section": "6 MT-Bench evaluation"}]