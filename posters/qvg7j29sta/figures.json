[{"figure_path": "QVG7j29Sta/figures/figures_1_1.jpg", "caption": "Figure 1: All six quantization schemes show negligible difference in accuracy compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), exhibit large number of flips, indicating severe divergence in model behavior.", "description": "This figure displays the results of six different quantization schemes applied to large language models (LLMs).  The x-axis represents the change in accuracy compared to a baseline 16-bit model across several benchmark tasks. The y-axis shows the percentage of \"flips,\" which are instances where the model changes its answer from correct to incorrect or vice-versa.  The key finding is that while the accuracy remains largely unchanged by the quantization, the number of flips increases significantly for all quantization schemes except GPTQ W8A16. This illustrates that seemingly small accuracy differences can mask substantial changes in model behavior as perceived by the user.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/figures/figures_4_1.jpg", "caption": "Figure 2: Flips and KL Divergence are well correlated. Each point corresponds to a model, quantization combination in Table 4", "description": "This figure displays the correlation between the percentage of flips and KL-Divergence across various model-quantization combinations.  It shows that as the percentage of flips increases, KL-Divergence also increases, indicating a strong positive correlation between the two metrics. This is true across three different tasks (ARC-Easy, ARC-Challenge, and MMLU 5-shot). The high correlation suggests that the flips metric can be used as a proxy for the KL-Divergence metric, which is more computationally expensive to calculate.  Each point on the graph represents a different model and quantization technique, as detailed in Table 4 of the paper.", "section": "4 Results"}, {"figure_path": "QVG7j29Sta/figures/figures_5_1.jpg", "caption": "Figure 3: MMLU 5-shot accuracy difference and flips for two compression techniques (Llama2-13b model). Even at early stages of pruning with no accuracy difference, flips indicate model divergence.", "description": "This figure shows the accuracy difference and percentage of flips for two different LLMs compression techniques, namely, layer dropping and WANDA pruning, on the MMLU 5-shot task.  The x-axis represents the fraction of layers dropped or sparsity, while the y-axis shows the percentage change in accuracy and the percentage of flips. The plot demonstrates that even when there is negligible change in accuracy (as measured by the difference from the baseline), there is a steady increase in the number of flips as the number of layers dropped or sparsity increases. This finding highlights that accuracy alone might not be sufficient to evaluate the quality of compressed models.", "section": "4.2 Other model compression techniques"}, {"figure_path": "QVG7j29Sta/figures/figures_6_1.jpg", "caption": "Figure 4: When the Top Margin is low, an-swer will more likely change (Llama2-70b, BnB W4A4, MMLU 5-shot)", "description": "This figure shows the relationship between the probability of an answer changing (i.e., flipping from correct to incorrect or vice versa) and the top margin of the model's prediction for a given question. The top margin is defined as the difference in probability between the most probable answer and the second most probable answer. The figure demonstrates that when the top margin is low (indicating low confidence in the model's prediction), there is a greater chance that the answer will change after quantization. The result is consistent across several LLMs and benchmarks.", "section": "5 Analyzing Flips"}, {"figure_path": "QVG7j29Sta/figures/figures_6_2.jpg", "caption": "Figure 7: Change in prediction probability vs top margin: For the MMLU 5-shot benchmark, we plot the baseline top-margin for the questions on the x-axis, and the change in prediction probability of the choices in the quantized vs the baseline model on the y-axis (computed as \u03a3\u2208A/B/C/D|Probabilitybasei\u2212Probabilityquantizedi|). We use the BnB W4A4 quantization scheme for these results. The observation holds across different quantization schemes.", "description": "This figure shows the relationship between the change in prediction probability and the baseline top margin for the MMLU 5-shot benchmark.  The x-axis represents the baseline top margin, while the y-axis shows the absolute difference in prediction probability between the quantized and baseline models for each answer choice. The BnB W4A4 quantization scheme was used. The results demonstrate that the change in prediction probability is more significant when the baseline top margin is low.  This observation remains consistent across different quantization schemes, indicating a correlation between prediction confidence and the likelihood of answer changes during quantization.", "section": "5 Analyzing Flips"}, {"figure_path": "QVG7j29Sta/figures/figures_8_1.jpg", "caption": "Figure 6: Flips is a better predictor of downstream task performance than Accuracy", "description": "This figure compares the correlation between the percentage of flips (a metric indicating changes in answers from correct to incorrect or vice versa) and the difference in MT-Bench scores (a multi-turn dialogue task evaluating free-form text generation capabilities). The left subplot shows a strong negative correlation between flips and MT-Bench scores for Llama2-70b and Yi-34b chat models. The right subplot indicates a weaker positive correlation between accuracy difference and MT-Bench scores. This suggests that flips are a more reliable indicator of downstream task performance compared to just accuracy differences.", "section": "4 Results"}, {"figure_path": "QVG7j29Sta/figures/figures_8_2.jpg", "caption": "Figure 1: All six quantization schemes show negligible difference in accuracy compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), exhibit large number of flips, indicating severe divergence in model behavior.", "description": "This figure shows the results of six different quantization schemes applied to four different large language models (LLMs) across seven benchmark tasks.  The key finding is that while the change in accuracy between the baseline (16-bit) models and the quantized models is minimal (almost negligible), there is a significant number of \"flips.\" Flips refer to instances where a correct answer from the baseline model becomes incorrect in the quantized model, and vice versa.  The only exception to this trend is the GPTQ W8A16 scheme. This highlights a critical limitation of using accuracy alone as a metric for evaluating LLM compression techniques, as it masks the significant divergence in model behavior revealed by the high number of flips. The figure visually represents this divergence, suggesting that using only accuracy as an evaluation metric can be misleading.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/figures/figures_24_1.jpg", "caption": "Figure 7: Change in prediction probability vs top margin: For the MMLU 5-shot benchmark, we plot the baseline top-margin for the questions on the x-axis, and the change in prediction probability of the choices in the quantized vs the baseline model on the y-axis (computed as  \u03a3<sub>A/B/C/D</sub>|Probability<sub>baseline</sub><sub>i</sub> - Probability<sub>quantized</sub><sub>i</sub>|). We use the BnB W4A4 quantization scheme for these results. The observation holds across different quantization schemes.", "description": "This figure shows the relationship between the change in prediction probability and the baseline top margin for the MMLU 5-shot benchmark.  The x-axis represents the baseline top margin, which is the difference between the highest and second-highest probabilities assigned to answer choices. The y-axis represents the absolute difference in prediction probabilities between the quantized model and the baseline model, summed across all answer choices.  The figure uses the BnB W4A4 quantization scheme, but the trend holds across other quantization schemes as well.  It demonstrates that when the baseline top margin is low (indicating less model certainty), the change in prediction probabilities is higher, suggesting a greater impact from quantization.", "section": "5 Analyzing Flips"}, {"figure_path": "QVG7j29Sta/figures/figures_24_2.jpg", "caption": "Figure 7: Change in prediction probability vs top margin: For the MMLU 5-shot benchmark, we plot the baseline top-margin for the questions on the x-axis, and the change in prediction probability of the choices in the quantized vs the baseline model on the y-axis (computed as \u03a3i\u2208A/B/C/D|Probabilitybaselinei\u2212Probabilityquantizedi|). We use the BnB W4A4 quantization scheme for these results. The observation holds across different quantization schemes.", "description": "This figure shows the relationship between the change in prediction probability of multiple-choice answers and the baseline top margin in the MMLU 5-shot benchmark after applying BnB W4A4 quantization.  The x-axis represents the baseline top margin, and the y-axis represents the absolute difference in prediction probabilities between the baseline and quantized models. The plot indicates a strong correlation between a low baseline top margin and a larger change in prediction probabilities after quantization. This correlation is consistent across different quantization schemes.", "section": "Analyzing Flips"}, {"figure_path": "QVG7j29Sta/figures/figures_24_3.jpg", "caption": "Figure 9: SliceGPT, Accuracy and Flips vs Sparsity", "description": "The figure shows the relationship between sparsity, accuracy difference, and flips for the SliceGPT model.  As sparsity increases, the accuracy difference remains relatively low, indicating that the model's overall accuracy is preserved. However, the number of flips increases significantly, implying a considerable divergence in model behavior despite similar accuracy scores.", "section": "4. Results"}, {"figure_path": "QVG7j29Sta/figures/figures_25_1.jpg", "caption": "Figure 3: MMLU 5-shot accuracy difference and flips for two compression techniques (Llama2-13b model). Even at early stages of pruning with no accuracy difference, flips indicate model divergence.", "description": "This figure shows the accuracy difference and percentage of flips for two model compression techniques: dropping the last n layers and WANDA pruning.  The Llama2-13b model was used with the MMLU 5-shot benchmark.  The key takeaway is that even when the accuracy remains similar to the baseline (16-bit model), the number of flips (changes from correct to incorrect answers and vice-versa) significantly increases as more layers are dropped or more sparsity is introduced using the pruning method. This highlights that accuracy alone might not be enough to evaluate compression techniques and other metrics, such as flips, are needed to capture the underlying differences in model behavior.", "section": "4.2 Other model compression techniques"}, {"figure_path": "QVG7j29Sta/figures/figures_25_2.jpg", "caption": "Figure 9: SliceGPT, Accuracy and Flips vs Sparsity", "description": "This figure shows the relationship between the sparsity of a model, resulting from the application of the SliceGPT compression technique, and two key metrics: accuracy difference and percentage of flips.  The x-axis represents the sparsity level, ranging from 0 to 0.5, indicating the fraction of model parameters removed. The y-axis displays the percentage change in accuracy and the percentage of flips.  The graph indicates that as sparsity increases, there is a gradual increase in the percentage of flips, even when the change in accuracy is relatively modest.  This highlights the potential divergence between the original and compressed model's behavior, even with similar accuracy scores.", "section": "4.3 Perplexity"}, {"figure_path": "QVG7j29Sta/figures/figures_25_3.jpg", "caption": "Figure 1: All six quantization schemes show negligible difference in accuracy compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), exhibit large number of flips, indicating severe divergence in model behavior.", "description": "This figure displays the results of six different quantization schemes applied to four large language models (LLMs) across seven distinct tasks.  The x-axis represents the percentage change in accuracy compared to a 16-bit baseline model, while the y-axis shows the baseline accuracy. Each point on the graph represents a specific model and quantization scheme combination applied to one of the tasks.  The key finding is that, despite minimal changes in accuracy (most are within \u00b12%), there is a substantial increase in the percentage of \"flips\" (instances where correct answers become incorrect, or vice-versa) for most of the quantization techniques, except GPTQ W8A16. This suggests that while the overall accuracy might be similar, the compressed models behave considerably differently from the original model at the granular answer level.", "section": "4.1 Quantization schemes"}, {"figure_path": "QVG7j29Sta/figures/figures_26_1.jpg", "caption": "Figure 2: Flips and KL Divergence are well correlated. Each point corresponds to a model, quantization combination in Table 4", "description": "This figure shows the correlation between two distance metrics: the percentage of flips and the KL divergence. Each point represents a specific model and its corresponding quantization technique used in the study, as detailed in Table 4. The strong correlation observed suggests that the percentage of flips can serve as a useful proxy for KL divergence, a more computationally expensive metric. This finding supports the paper's argument that the percentage of flips is a valuable metric for evaluating the quality of LLM compression.", "section": "4 Results"}, {"figure_path": "QVG7j29Sta/figures/figures_31_1.jpg", "caption": "Figure 1: All six quantization schemes show negligible difference in accuracy compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), exhibit large number of flips, indicating severe divergence in model behavior.", "description": "This figure shows the results of six different quantization schemes applied to four large language models (LLMs) across seven different benchmark tasks.  While the accuracy change compared to the baseline 16-bit models is negligible (within 2%), a significant number of \"flips\" (changes in answers from correct to incorrect or vice versa) are observed for most quantization methods. This indicates that although overall accuracy remains similar, the underlying behavior of the compressed models significantly deviates from the baseline, highlighting the inadequacy of using accuracy alone as an evaluation metric.", "section": "4.1 Quantization schemes"}]