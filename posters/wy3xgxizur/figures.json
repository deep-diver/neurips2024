[{"figure_path": "WY3xgXIZUR/figures/figures_1_1.jpg", "caption": "Figure 1: VisInContext significantly increases the in-context text length from 256 to 2048 during pre-training on NVIDIA H100 GPU. We implement PyTorch Flamingo [1] models with different in-context length during pre-training. The language model is a 56B MOE [2] model loaded with 4-bit quantization and the batch size on each GPU is 32 with FP16. We train the model with DeepSpeed [3] Zero-2.", "description": "The figure shows two graphs: GPU memory usage vs. in-context text length, and TFlops vs. in-context text length. It compares the performance of the original method with the VisInContext method. VisInContext significantly increases the in-context text length from 256 to 2048 tokens while maintaining a similar level of FLOPs, indicating improved efficiency. The experiments were performed on NVIDIA H100 GPUs using a 56B MOE language model with 4-bit quantization and a batch size of 32 with FP16.", "section": "1 Introduction"}, {"figure_path": "WY3xgXIZUR/figures/figures_2_1.jpg", "caption": "Figure 2: VisInContext Pipeline. The VisInContext pipeline builds upon the Flamingo model for in-context few-shot modeling (represented in gray). VisInContext processes interleaved image-text data by rendering portions of the in-context text into images. This approach maintains the Text Token Length of the model while allowing for a significantly extended In-context Text Length.", "description": "The figure illustrates the VisInContext pipeline, which extends the Flamingo model's ability to process long in-context text by converting portions of the text into images and feeding these images, along with the remaining text, into the model. This maintains the model's text token length but significantly increases the in-context text length, allowing for more context in the few-shot learning process.", "section": "2 Method"}, {"figure_path": "WY3xgXIZUR/figures/figures_5_1.jpg", "caption": "Figure 1: VisInContext significantly increases the in-context text length from 256 to 2048 during pre-training on NVIDIA H100 GPU. We implement PyTorch Flamingo [1] models with different in-context length during pre-training. The language model is a 56B MOE [2] model loaded with 4-bit quantization and the batch size on each GPU is 32 with FP16. We train the model with DeepSpeed [3] Zero-2.", "description": "This figure shows the impact of VisInContext on GPU memory usage and FLOPs during the pre-training of a 56B parameter MOE model.  The left graph illustrates how VisInContext significantly reduces GPU memory consumption while increasing in-context text length from 256 to 2048 tokens. The right graph demonstrates that the increase in in-context length using VisInContext does not result in a significant increase in FLOPs.", "section": "1 Introduction"}, {"figure_path": "WY3xgXIZUR/figures/figures_5_2.jpg", "caption": "Figure 1: VisInContext significantly increases the in-context text length from 256 to 2048 during pre-training on NVIDIA H100 GPU. We implement PyTorch Flamingo [1] models with different in-context length during pre-training. The language model is a 56B MOE [2] model loaded with 4-bit quantization and the batch size on each GPU is 32 with FP16. We train the model with DeepSpeed [3] Zero-2.", "description": "This figure shows the impact of using VisInContext on GPU memory usage and FLOPs during pre-training of a large language model.  It compares the original method's memory usage and FLOPs to the VisInContext method across different in-context text lengths. VisInContext significantly reduces the memory usage and FLOPs needed to process long in-context text, allowing for a much greater in-context length with similar computational cost. The experiment was performed on a 56B parameter MOE model using NVIDIA H100 GPUs.", "section": "1 Introduction"}, {"figure_path": "WY3xgXIZUR/figures/figures_6_1.jpg", "caption": "Figure 2: VisInContext Pipeline. The VisInContext pipeline builds upon the Flamingo model for in-context few-shot modeling (represented in gray). VisInContext processes interleaved image-text data by rendering portions of the in-context text into images. This approach maintains the Text Token Length of the model while allowing for a significantly extended In-context Text Length.", "description": "The figure illustrates the VisInContext pipeline, which enhances the Flamingo model's ability to handle long text contexts.  It shows how long text segments are converted into images using an image rendering module, processed by a vision encoder, and integrated with the main text processing stream. This approach effectively increases the \"in-context text length\" without substantially increasing the number of \"text tokens\", reducing computational costs. The process also involves token masking and text-centric contrastive learning to help the model learn effectively from the image-based textual representations.", "section": "2 Method"}, {"figure_path": "WY3xgXIZUR/figures/figures_13_1.jpg", "caption": "Figure 6: The main pipeline is based on Fuyu [8]. What's different is we introduce an additional text image. During pre-training, the rendered text image and original image is also alternatively. The DCSE is preserved. We show one image-text pair here for simplicity.", "description": "This figure illustrates the architecture of the VisInContext method adapted for the FuYu model, which uses linear embeddings instead of a visual encoder.  The key modification is the inclusion of an additional rendered text image, processed alongside the original image.  The overall process remains similar to FuYu's single-stream decoder approach, but now incorporates this additional visual input to improve context understanding.", "section": "A Extending VisInContext to MLLM Using Only Linear Embedding"}, {"figure_path": "WY3xgXIZUR/figures/figures_16_1.jpg", "caption": "Figure 1: VisInContext significantly increases the in-context text length from 256 to 2048 during pre-training on NVIDIA H100 GPU. We implement PyTorch Flamingo [1] models with different in-context length during pre-training. The language model is a 56B MOE [2] model loaded with 4-bit quantization and the batch size on each GPU is 32 with FP16. We train the model with DeepSpeed [3] Zero-2.", "description": "This figure shows the impact of VisInContext on GPU memory usage and FLOPs during the pre-training of a 56-billion parameter MOE model.  The left panel demonstrates that VisInContext significantly reduces GPU memory consumption while allowing for a substantial increase in the in-context length (from 256 to 2048 tokens).  The right panel shows that the FLOPs remain relatively constant despite the increased in-context length.  This highlights VisInContext's efficiency in handling longer texts.", "section": "1 Introduction"}]