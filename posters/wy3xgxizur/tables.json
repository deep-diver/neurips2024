[{"figure_path": "WY3xgXIZUR/tables/tables_4_1.jpg", "caption": "Table 1: Increasing in-context text length with VisInContext significantly improves performance on multi-modality downstream tasks. The model is pre-trained with a 56B MOE model. ICL stands for in-context text length. HM is short for hatefulmemes. With VisInContext, we increase the ICL from 256 to 2048, leading to clear improvements over the baseline. \u2020 indicates our implementation.", "description": "This table presents the results of experiments comparing the performance of a 56B parameter MOE model with and without the VisInContext method on several downstream tasks.  The key finding is that increasing the in-context length (ICL) from 256 to 2048 tokens using VisInContext leads to significant improvements in performance across various tasks (VQA, captioning, classification). The table shows improvements, even with a smaller number of shots (few-shot learning setting).", "section": "3.2 In-context Few-shot Evaluation"}, {"figure_path": "WY3xgXIZUR/tables/tables_4_2.jpg", "caption": "Table 2: VisInContext effectively incorporates in-context text with visual tokens, demonstrating significant performance improvements with consistent token usage. Here, T-shots refer to text-only in-context examples. Tokens indicate the length of the input to the LLM. Text source describes the preprocessing method for in-context examples. \u2020 denotes our implementation on 180M pretraining data.", "description": "This table compares the performance of VisInContext against a baseline model on various downstream tasks.  It shows the impact of using visual tokens to represent part of the in-context text.  The table highlights improved performance with VisInContext while maintaining a similar number of text tokens provided as input to the language model. This demonstrates the effectiveness of using visual tokens to extend the effective context length.", "section": "3.2 In-context Few-shot Evaluation"}, {"figure_path": "WY3xgXIZUR/tables/tables_5_1.jpg", "caption": "Table 3: VisInContext clearly boosting the baseline on document understanding tasks.", "description": "This table shows the performance of VisInContext and the baseline model (Open-Flamingo-9B) on document understanding tasks (DocVQA and OCR VQA).  It demonstrates the improvement achieved by using VisInContext, which converts text into images, leading to better understanding of document content. The results are presented for both validation and test sets, showing a consistent performance boost.", "section": "3.2 In-context Few-shot Evaluation"}, {"figure_path": "WY3xgXIZUR/tables/tables_6_1.jpg", "caption": "Table 4: The model pretrain with VisInContext significantly improves sequence understanding ability. We report the sequence retrieval result on OBELICS-Hybrid6.", "description": "This table presents the results of a sequential multi-modal retrieval task on the OBELICS-Hybrid6 dataset.  The task involves predicting the next image and text in a sequence of interleaved image-text data.  The table shows the performance of a model pre-trained using the VisInContext method, comparing different input types: raw images and text, or raw images with rendered text images replacing surrounding text.  The results demonstrate the improved sequence understanding ability achieved through incorporating VisInContext.", "section": "3.4 Sequential Multi-modal Retrieval"}, {"figure_path": "WY3xgXIZUR/tables/tables_7_1.jpg", "caption": "Table 5: Pretraining with VisInContext helps on long-context understanding task for FuYu model. \u2020 means our implementation on 180M data.", "description": "This table presents the results of using VisInContext with the FuYu9B model.  The baseline uses raw text for pretraining, while the VisInContext method uses rendered images in addition to raw text.  The results show an improvement in performance on the DocVQA-val task when using VisInContext.  The improvement is quantified as +2.2 percentage points.", "section": "3.5 Extension to MLLM with Linear Embedding"}, {"figure_path": "WY3xgXIZUR/tables/tables_7_2.jpg", "caption": "Table 6: Ablation study of the component in our pipeline for text-only 4-shot example.", "description": "This table presents the ablation study results for a text-only, 4-shot evaluation setup.  It shows the impact of different components of the VisInContext method on the performance of the model across various VQA datasets (Ok-VQA, TextVqa, VizWiz, VqaV2).  The components evaluated are the use of text images, token masking, and text-centric contrastive learning (TCCL). Each row represents a different combination of these components, with a checkmark indicating that the component was included. The numbers in the table are the average performance scores for each dataset and component combination.", "section": "3.6 Ablation Study"}, {"figure_path": "WY3xgXIZUR/tables/tables_7_3.jpg", "caption": "Table 7: Ablation study of the component in our pipeline for text-only 4-shot example.", "description": "This table presents the ablation study of different components of the VisInContext pipeline, focusing on the text-only setting with 4-shot examples. It shows how performance varies on TextVQA and DocVQA benchmarks when altering the font size (4, 6, 8, 10, 12) and the dataset size (2, 4, 8, 16, 32).  The goal is to determine the optimal configuration for text rendering within the VisInContext framework.", "section": "3.6 Ablation Study"}, {"figure_path": "WY3xgXIZUR/tables/tables_14_1.jpg", "caption": "Table 1: Increasing in-context text length with VisInContext significantly improves performance on multi-modality downstream tasks. The model is pre-trained with a 56B MOE model. ICL stands for in-context text length. HM is short for hatefulmemes. With VisInContext, we increase the ICL from 256 to 2048, leading to clear improvements over the baseline. \u2020 indicates our implementation.", "description": "This table presents the results of experiments comparing the performance of a model trained with and without VisInContext on several downstream tasks.  The model uses a 56-billion parameter Mixture of Experts (MOE) architecture.  The key finding is that increasing the in-context length (ICL) from 256 to 2048 tokens using VisInContext significantly boosts performance across all tasks (VQA, Caption, Classification). This demonstrates the effectiveness of the proposed VisInContext method for handling longer text contexts in multi-modal learning.", "section": "3.2 In-context Few-shot Evaluation"}, {"figure_path": "WY3xgXIZUR/tables/tables_14_2.jpg", "caption": "Table 10: The hyperparameters used in pre-training for three distinct VisInContext variations. The learning rate and batch size is smaller for sine the GPU memory limitation is 32GB.", "description": "This table details the hyperparameters employed during the pre-training phase of the VisInContext model.  It showcases three variations of the model, each with varying language model backbones (OPT-IML-1.8B, Mistral-7B, and MOE 56B).  The hyperparameters listed include cross-layer interval, text sequence length, in-context length (ICL), effective batch size, maximum training steps, weight decay, optimizer, gradient clipping, initial learning rate, learning rate decay schedule, and linear warmup steps. Note that the learning rate and batch size are adjusted to account for the 32GB GPU memory limitation.", "section": "3.1 Experimental Setup"}, {"figure_path": "WY3xgXIZUR/tables/tables_15_1.jpg", "caption": "Table 11: Parameter counts for each component in MLLM. \u2020 means our implementation.", "description": "This table shows the number of parameters for each component of three different multimodal large language models (MLLMs). The models are Flamingo-9B, Flamingo-9B Baseline (the authors' implementation), and MOE Baseline (the authors' implementation).  The components listed are the language model, vision model, gated cross-attention, and resampler.  The table highlights differences in parameter counts between the original Flamingo model and the authors' modified versions.", "section": "3. Experiment"}, {"figure_path": "WY3xgXIZUR/tables/tables_15_2.jpg", "caption": "Table 1: Increasing in-context text length with VisInContext significantly improves performance on multi-modality downstream tasks. The model is pre-trained with a 56B MOE model. ICL stands for in-context text length. HM is short for hatefulmemes. With VisInContext, we increase the ICL from 256 to 2048, leading to clear improvements over the baseline. \u2020 indicates our implementation.", "description": "This table presents the results of experiments evaluating the impact of VisInContext on several downstream multi-modal tasks.  A 56-billion parameter Mixture of Experts (MOE) model was used.  The table shows a significant performance improvement when using VisInContext to extend the in-context length from 256 to 2048 tokens, indicating the effectiveness of the proposed method in handling longer text sequences.  Results are shown for various metrics across different datasets (OK-VQA, TextVQA, VizWiz, VQAv2, COCO, Flickr, and HatefulMemes).", "section": "3.2 In-context Few-shot Evaluation"}, {"figure_path": "WY3xgXIZUR/tables/tables_17_1.jpg", "caption": "Table 13: The impact of opening visual encoder during pre-training.", "description": "This table presents the results of an experiment comparing two approaches to using the visual encoder in the model during pre-training: a frozen visual encoder and a learnable visual encoder. The performance of the model is evaluated on three downstream tasks: DocVQA (validation and test sets), OCR VQA, and Hatefulmems classification.  The numbers in parentheses indicate the improvement compared to the frozen visual encoder approach.  A positive number shows an increase in performance (green) and a negative number shows a decrease in performance (red). The table highlights a trade-off: while enabling the visual encoder during training improves some downstream tasks, particularly the document understanding tasks, it also introduces instability and slightly harms performance on the classification task.", "section": "3.3 Document understanding"}]