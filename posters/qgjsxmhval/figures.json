[{"figure_path": "QGJSXMhVaL/figures/figures_1_1.jpg", "caption": "Figure 1: Overall agent architecture. The agent also inputs a goal in natural language", "description": "The figure shows the overall architecture of the WorldCoder agent. It consists of four main components: the world, the planner, the world model (represented as a Python program), and the LLM. The agent interacts with the world, receives state, reward, and goal information, and uses the planner to select actions. The world model is a Python program that is updated based on the agent's experiences in the world. The LLM is used to generate and refine the world model. The replay buffer stores the agent's experiences, which are used to train the world model. The goal is given in natural language.", "section": "2 Methods"}, {"figure_path": "QGJSXMhVaL/figures/figures_5_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows a comparison of WorldCoder's performance against other methods on Sokoban tasks. (A) illustrates an example Sokoban gameplay. (B) presents learning curves showing the solve rate of WorldCoder with and without optimism, along with a ReAct baseline, highlighting WorldCoder's sample efficiency. (C) compares the LLM token costs of WorldCoder and ReAct, illustrating WorldCoder's asymptotic advantage. (D) compares WorldCoder to deep reinforcement learning methods (PPO, DreamerV3), revealing that WorldCoder needs significantly fewer interactions to solve basic Sokoban problems. (E) shows a variation of Sokoban with teleport gates, further demonstrating WorldCoder's adaptability and learning ability.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_6_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows the experimental results for Sokoban.  Panel A displays a sample Sokoban game environment. Panel B presents learning curves comparing the performance of WorldCoder (with and without the optimism objective), and ReAct.  WorldCoder demonstrates significantly faster learning.  Panel C shows the token cost of LLMs per task, highlighting the efficiency of WorldCoder compared to other LLM agents. Panel D compares WorldCoder with deep reinforcement learning (RL) approaches, showcasing WorldCoder's superior sample efficiency.  Finally, Panel E illustrates an extension of the Sokoban game with added teleport gates to further demonstrate WorldCoder's adaptability.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_7_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure compares the performance of WorldCoder to other methods on the Sokoban task. Panel A shows a sample Sokoban game. Panel B shows learning curves comparing WorldCoder with and without the optimism objective, and also comparing to ReAct, which uses an LLM for reasoning and action but does not update its model. Panel C compares the LLM calls/tokens used by different methods. Panel D shows the performance of deep RL on this same task. Panel E shows a nonstandard version of Sokoban with teleport gates.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_17_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comparison of the proposed WorldCoder model against other baselines on Sokoban, a classic puzzle-solving game involving pushing blocks to target locations.  Panel (A) shows a sample Sokoban game environment. (B) compares learning curves, demonstrating WorldCoder's significantly faster convergence to solve simple levels compared to ReAct (a language model agent), highlighting the effectiveness of the approach. Panel (C) illustrates the computational cost, showing that WorldCoder exhibits a lower asymptotic LLM call cost than ReAct, which requires LLM calls for each action. Panel (D) compares WorldCoder with deep reinforcement learning algorithms, revealing the far greater sample complexity of the latter (millions of interactions versus fewer than 100 for WorldCoder). Lastly, panel (E) showcases the robustness of WorldCoder in tackling a modified Sokoban environment with added teleportation features.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_19_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure compares the performance of WorldCoder against other methods (ReAct, Deep RL) on the Sokoban task.  Panel (A) shows a sample Sokoban game environment. Panel (B) presents learning curves, illustrating WorldCoder's significantly faster learning compared to ReAct. Panel (C) highlights the reduced LLM cost of WorldCoder, showing it's more efficient in terms of LLM calls than other LLM agents. Panel (D) contrasts WorldCoder's sample efficiency to that of deep RL methods, which need substantially more experience to achieve basic competence. Panel (E) shows a more complex variant of Sokoban used to further evaluate WorldCoder's ability to adapt to new rules and dynamics.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_19_2.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows experimental results on Sokoban. (A) shows the Sokoban environment. (B) compares the learning curves of WorldCoder with and without the optimism objective and ReAct. It demonstrates WorldCoder's sample efficiency. (C) compares the LLM token cost of WorldCoder with prior LLM agents, highlighting its compute efficiency. (D) compares the performance of WorldCoder with deep RL methods. (E) shows a nonstandard Sokoban domain with teleport gates to further illustrate WorldCoder's ability to handle subtle dynamics.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_20_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows the experimental results on Sokoban. (A) shows an example gameplay in Sokoban. (B) compares the learning curves of WorldCoder, ReAct, and deep RL on solving Sokoban levels with different numbers of boxes. (C) illustrates the LLM token costs for each method. (D) shows the comparison of sample efficiency of WorldCoder and deep RL in solving basic levels of Sokoban. (E) introduces a nonstandard Sokoban task with teleportation portals.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_21_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comprehensive comparison of the proposed WorldCoder agent against other approaches on Sokoban tasks. (A) illustrates the Sokoban environment, showing the agent's goal to push boxes onto target locations. (B) shows the learning curves of WorldCoder (with and without the optimism objective) compared to ReAct, illustrating improved sample efficiency.  (C) highlights WorldCoder's computational efficiency, showing significantly lower LLM costs. (D) compares WorldCoder's performance with deep RL methods (PPO and DreamerV3), demonstrating superior sample efficiency in learning to solve the task. Finally, (E) showcases WorldCoder's ability to adapt and learn in a modified Sokoban environment with teleport gates, highlighting its transfer learning capability.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_22_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comprehensive analysis of the proposed WorldCoder agent's performance on Sokoban tasks, comparing it against other methods like ReAct and deep RL.  Panel (A) shows a sample Sokoban environment, while (B) displays learning curves illustrating the superior sample efficiency of WorldCoder, especially with optimism enabled. Panel (C) highlights the computational efficiency of WorldCoder, showing it requires fewer LLM calls than ReAct. Panel (D) emphasizes the significant advantage over deep RL, requiring millions of steps for similar performance. Finally, (E) demonstrates WorldCoder's adaptability by showcasing its success in a modified Sokoban environment with teleportation portals.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_25_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comparison of the WorldCoder agent's performance against other methods on the Sokoban task.  Panel (A) shows a sample Sokoban game environment. Panel (B) compares the learning curves of WorldCoder (with and without the optimism objective) and ReAct, highlighting WorldCoder's superior sample efficiency. Panel (C) demonstrates WorldCoder's reduced computational cost compared to other LLM agents by showing the number of LLM calls per task. Panel (D) visually contrasts WorldCoder's learning speed against deep reinforcement learning methods, which require significantly more steps to reach the same level of performance. Finally, Panel (E) displays an example of a modified Sokoban environment featuring teleportation portals, showcasing WorldCoder's adaptability to new environment dynamics.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_27_1.jpg", "caption": "Figure 4: (A) Minigrid environments ordered into a curriculum that tests different kinds of transfer learning. (B) Transfer learning performance, compared with (C) performance when solving each environment independently. Appendix Fig. 6: deep RL comparison.", "description": "This figure shows the experimental results of the agent's performance on a sequence of Minigrid environments.  The environments are ordered into a curriculum designed to test different aspects of transfer learning. Part (A) shows the ordering of the environments and the types of transfer being tested (new transitions, new rewards). Part (B) shows the learning curves for the agent trained with the curriculum, demonstrating how knowledge transfer improves sample efficiency. Part (C) shows the learning curves for the agent trained on each environment independently, illustrating the lack of transfer and reduced sample efficiency compared to the curriculum approach. The Appendix Fig. 6, referenced in the caption, would contain a comparison of the agent's performance to that of deep RL algorithms.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_29_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comprehensive comparison of the proposed WorldCoder model against existing state-of-the-art models on the Sokoban task. Subfigure (A) illustrates the Sokoban environment. Subfigure (B) showcases the learning curves, highlighting the superior sample efficiency of WorldCoder compared to the ReAct model, even with the same pre-trained knowledge. Subfigure (C) emphasizes the significant reduction in LLM calls per task, showcasing the computational efficiency of the proposed approach. Subfigure (D) further illustrates WorldCoder's sample efficiency advantage over deep RL methods. Finally, subfigure (E) demonstrates the model's ability to adapt and generalize to novel situations, as evidenced by its performance on a modified Sokoban environment incorporating teleport gates.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_30_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comprehensive evaluation of the WorldCoder agent's performance on Sokoban tasks, comparing it with other approaches like ReAct and Deep RL.  Panel (A) shows a sample Sokoban game state.  Panel (B) displays learning curves, demonstrating WorldCoder's superior sample efficiency compared to ReAct, which, despite having the same pretrained knowledge, struggles to play effectively. Panel (C) highlights the computational efficiency of WorldCoder, showcasing its significantly lower LLM call cost per task compared to ReAct-style agents. Panel (D) emphasizes the substantial gap in sample complexity between WorldCoder and Deep RL approaches, with Deep RL requiring millions of steps to master simple tasks.  Finally, Panel (E) illustrates WorldCoder's ability to adapt to novel game dynamics by adding teleport gates.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_36_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comparison of WorldCoder's performance against other methods on the Sokoban task.  Panel (A) shows an example Sokoban game environment. Panel (B) displays learning curves, illustrating WorldCoder's superior sample efficiency compared to ReAct, even with equivalent pretrained knowledge. Panel (C) compares the asymptotic LLM token costs, highlighting WorldCoder's significant computational advantages over methods requiring LLM calls at each step.  Panel (D) demonstrates the substantial difference in sample efficiency compared to deep RL approaches. Finally, panel (E) showcases WorldCoder's ability to adapt to novel game dynamics through the introduction of teleport gates.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_40_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure displays several key results related to the Sokoban experiment, showcasing the learning capabilities of the proposed WorldCoder agent. (A) illustrates a sample Sokoban level. (B) presents learning curves that compare WorldCoder (with and without the optimism objective) against the ReAct baseline. (C) highlights the asymptotic differences in LLM cost between WorldCoder and prior LLM agents. (D) compares the sample efficiency of WorldCoder against Deep RL methods, demonstrating its superior performance. (E) illustrates the adaptability of WorldCoder to a modified Sokoban level with teleport gates, further emphasizing its transfer learning capabilities.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_44_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows a comparison of WorldCoder's performance with other methods on the Sokoban task.  Panel (A) shows a typical Sokoban game state. Panel (B) presents learning curves demonstrating WorldCoder's superior sample efficiency compared to ReAct and deep RL methods. Panel (C) illustrates that WorldCoder's cost in terms of LLM calls only increases by a constant factor, regardless of the problem's complexity, unlike other methods.  Panel (D) highlights the significant difference in the number of steps needed by deep RL compared to WorldCoder to solve the task. Finally, Panel (E) showcases an example of non-standard Sokoban, including teleportation portals, where WorldCoder successfully adapts to the new dynamics.", "section": "3 Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_48_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a comprehensive comparison of the proposed WorldCoder agent against several baselines across multiple aspects and domains.\n\n(A) Shows an example gameplay of the Sokoban domain, where an agent pushes boxes to target locations. The figure highlights the challenge of sparse rewards inherent to the domain.\n\n(B) Compares learning curves of WorldCoder with and without the proposed optimism learning objective, and ReAct, an LLM-based agent. The results demonstrate that WorldCoder achieves significantly higher sample efficiency and learns to solve the task quickly, unlike ReAct that only has the pretrained knowledge of Sokoban but fails to play effectively.\n\n(C) Illustrates the asymptotic LLM cost comparison. WorldCoder has lower asymptotic cost compared to the LLM-based agents that require frequent LLM calls for every action.\n\n(D) Shows a comparison to deep RL methods, highlighting that WorldCoder achieves sample efficiency compared to deep RL.\n\n(E) Presents an example of the non-standard Sokoban environment with teleport gates, showcasing the agent's ability to adapt and solve the problem even with subtle changes in world dynamics.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_51_1.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure shows experimental results for the Sokoban environment.  Panel (A) shows an example of the Sokoban game. Panel (B) presents learning curves comparing WorldCoder to ReAct, highlighting WorldCoder's superior sample efficiency. Panel (C) compares the LLM token cost of WorldCoder versus other LLM agents. Panel (D) compares WorldCoder to deep RL methods, showing significantly better sample efficiency. Lastly, Panel (E) shows the application of WorldCoder to a more complex variant of Sokoban involving teleportation gates.", "section": "Experimental Results"}, {"figure_path": "QGJSXMhVaL/figures/figures_51_2.jpg", "caption": "Figure 3: (A) Sokoban domain (per-step reward of -0.1 ellided from figure). (B) Learning curves. ReAct has the same pretrained knowledge of Sokoban but cannot effectively play the game. (C) Our method has different asymptotic LLM cost compared to prior LLM agents, which consume LLM calls/tokens at every action. (D) Deep RL takes >1 million steps to learn 2-box Sokoban. (E) Nonstandard Sokoban with teleport gates", "description": "This figure presents a qualitative comparison of the proposed WorldCoder model with existing deep reinforcement learning (deep RL) methods and large language model (LLM) agents on the Sokoban task.  Panel (A) shows an example Sokoban game environment. Panel (B) displays learning curves, highlighting the superior sample efficiency of WorldCoder compared to ReAct, an LLM-based agent.  Panel (C) demonstrates the computational efficiency of WorldCoder, requiring a significantly lower number of LLM calls than ReAct. Panel (D) shows the sample inefficiency of deep RL methods on Sokoban. Finally, panel (E) illustrates WorldCoder's robustness to modified environment dynamics, successfully solving a variant of Sokoban with added teleportation portals.", "section": "Experimental Results"}]