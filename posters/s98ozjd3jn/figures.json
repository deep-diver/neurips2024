[{"figure_path": "S98OzJD3jn/figures/figures_2_1.jpg", "caption": "Figure 1: Case study of directly replacing the denoiser with the original pre-trained model on lightly disturbed data (left). The changes in Fr\u00e9chet Inception Distance (FID) as the denoising steps are incrementally replaced by the original pre-trained model (right).", "description": "This figure presents a case study that investigates the impact of using a pre-trained diffusion model instead of a fine-tuned model at various denoising steps during the reverse diffusion process. The left panel shows example images generated by directly replacing the fine-tuned denoiser with the original pre-trained denoiser at different stages, illustrating how the fine-tuned model\u2019s performance relative to the pre-trained model changes as we incrementally replace more steps. The right panel shows a plot demonstrating how the Fr\u00e9chet Inception Distance (FID), a metric used for evaluating generative model performance, changes when replacing different percentages of the fine-tuned model with the pre-trained model. This plot illustrates that performance improves at the end of the diffusion process but decreases as we move towards the beginning, indicating the presence of a \u201cchain of forgetting\u201d phenomenon.", "section": "3 Method"}, {"figure_path": "S98OzJD3jn/figures/figures_4_1.jpg", "caption": "Figure 2: The conceptual illustration of the chain of forgetting (Left). The increasing forgetting tendency as t grows. (a) Build a knowledge bank for the pre-trained model before fine-tuning. (b) Diff-Tuning leverages knowledge retention and reconsolidation, via the chain of forgetting.", "description": "This figure illustrates the concept of \"chain of forgetting\" in diffusion models.  The left side shows a diffusion model's reverse process, where noise is progressively removed to generate an image. As the process moves from high-level noise to low-level noise (from left to right), the model's ability to generalize to new domains decreases (forgetting trend). The right side presents Diff-Tuning's strategy. First, knowledge from a pre-trained model is preserved in a \"knowledge bank\" (a). Then, Diff-Tuning integrates \"knowledge retention\" (preserving the general denoising capabilities of the pre-trained model) and \"knowledge reconsolidation\" (adapting the model to downstream task-specific domains) during fine-tuning (b), effectively balancing forgetting and retention by utilizing the chain of forgetting trend.", "section": "3 Method"}, {"figure_path": "S98OzJD3jn/figures/figures_6_1.jpg", "caption": "Figure 3: An example of evaluating dissimilarities between conditions (the Normal condition) to infer the occurrence of sudden convergence.", "description": "The figure shows a case study evaluating the dissimilarities between conditions to infer the occurrence of sudden convergence in ControlNet fine-tuning.  The x-axis represents the training steps, and the y-axis represents the Mean Squared Error (MSE). Two lines are plotted: one for the baseline ControlNet fine-tuning and one for the Diff-Tuning method. The Diff-Tuning line shows a steeper drop in MSE, indicating faster convergence.  A horizontal dashed line marks a picked threshold to define the sudden convergence. The points where the lines cross the threshold are marked with yellow circles. The red oval highlights the \"sudden convergence\" region where the MSE drops significantly.", "section": "4.2 Transfer to Controllable Generation"}, {"figure_path": "S98OzJD3jn/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative compare Diff-Tuning to the standard ControlNet. Red boxes refer to the occurence of \"sudden convergence\".", "description": "This figure shows a qualitative comparison of Diff-Tuning and standard ControlNet on five different control conditions: Edge, Sketch, Depth, Normal, and Segmentation.  For each condition, sample images generated at various training steps are displayed. Red boxes highlight the point at which \"sudden convergence\" occurs\u2014the point where the model rapidly acquires the ability to generate images that accurately reflect the control condition.  The figure demonstrates that Diff-Tuning consistently reaches sudden convergence significantly faster than standard ControlNet, suggesting a more efficient transfer learning process.", "section": "4.2 Transfer to Controllable Generation"}, {"figure_path": "S98OzJD3jn/figures/figures_8_1.jpg", "caption": "Figure 5: The compatibility of Diff-Tuning with PEFT (a), and catastrophic forgetting analysis (b-c).", "description": "This figure shows three subfigures that analyze the performance of Diff-Tuning in comparison to standard fine-tuning and DiffFit. Subfigure (a) shows the FID (Fr\u00e9chet Inception Distance) scores for several downstream datasets for each method, demonstrating the superior performance of Diff-Tuning. Subfigures (b) and (c) present the EWC (Elastic Weight Consolidation) values, a measure of catastrophic forgetting. The total EWC and average EWC across all tunable parameters are shown for the different methods. These subfigures show that Diff-Tuning achieves lower EWC values compared to the other methods, indicating less catastrophic forgetting.", "section": "4.4 Analysis and Ablation"}, {"figure_path": "S98OzJD3jn/figures/figures_8_2.jpg", "caption": "Figure 6: Transfer convergence analysis (a), ablation study (b), and sensitivity analysis (c-d).", "description": "This figure presents four subplots, each showing a different analysis related to the Diff-Tuning method. (a) Convergence analysis on SUN397 dataset shows the FID scores over training steps for various methods, highlighting Diff-Tuning's faster convergence. (b) Ablation study on DF20M dataset demonstrates the impact of individual components (retention and reconsolidation) of Diff-Tuning, showing their complementary roles. (c) Sensitivity analysis on the Stanford Cars dataset explores the influence of the reconsolidation coefficient function \u03c8(t), which relates to the chain of forgetting. (d) Analysis of the number of samples in memory shows the relationship between the number of samples in the augmented dataset and the performance, indicating a point of diminishing returns.", "section": "4.4 Analysis and Ablation"}, {"figure_path": "S98OzJD3jn/figures/figures_9_1.jpg", "caption": "Figure 7: The influence of the quality of the augmented dataset (a), sensitivity with respect to different sampling steps (b), and application of Diff-Tuning in a continual learning setup (c).", "description": "This figure shows three subfigures demonstrating different aspects of Diff-Tuning. Subfigure (a) illustrates the effect of the size of the augmented dataset on the model's performance for CUB-Bird and Stanford Cars datasets. It shows that increasing the size of the augmented dataset improves performance, but there are diminishing returns. Subfigure (b) demonstrates the influence of the number of sampling steps used in generating the augmented dataset. It shows that increasing the number of sampling steps also improves performance, but again there are diminishing returns.  Subfigure (c) shows the results of using Diff-Tuning in a continual learning setup using the Evolving Image Search dataset. It shows that the model's performance is consistently better when using Diff-Tuning than when using standard fine-tuning across multiple time periods.", "section": "4.4 Analysis and Ablation"}, {"figure_path": "S98OzJD3jn/figures/figures_15_1.jpg", "caption": "Figure 8: Samples show of different datasets.", "description": "This figure displays sample images from eight different datasets used in the paper for class-conditional image generation experiments.  Each dataset represents a unique set of visual categories: Food101 (food items), SUN397 (scenes), DF20M (fungi), Caltech101 (objects), CUB-200-2011 (birds), ArtBench10 (artwork styles), Oxford Flowers (flowers), and Stanford Cars (cars). The images illustrate the variety and complexity of visual data used to evaluate the proposed method.", "section": "B.1 Benchmark Descriptions"}, {"figure_path": "S98OzJD3jn/figures/figures_18_1.jpg", "caption": "Figure 1: Case study of directly replacing the denoiser with the original pre-trained model on lightly disturbed data (left). The changes in Fr\u00e9chet Inception Distance (FID) as the denoising steps are incrementally replaced by the original pre-trained model (right).", "description": "This figure shows two plots. The left plot is a case study of directly replacing the denoiser with the original pre-trained model on lightly disturbed data. It demonstrates the concept that a pre-trained model acts as a universal denoiser for lightly corrupted data, capable of recognizing and refining subtle distortions. The right plot shows the change in Fr\u00e9chet Inception Distance (FID) as the denoising steps are incrementally replaced by the original pre-trained model, demonstrating the concept of \"chain of forgetting\".", "section": "3 Method"}, {"figure_path": "S98OzJD3jn/figures/figures_20_1.jpg", "caption": "Figure 4: Qualitative compare Diff-Tuning to the standard ControlNet. Red boxes refer to the occurence of \"sudden convergence\".", "description": "This figure shows a qualitative comparison of the results obtained using Diff-Tuning and standard ControlNet on five different control conditions.  For each condition (Edge, Sketch, Depth, Normal, Segmentation), it displays image samples generated at various training steps. The red boxes highlight the training steps where \"sudden convergence\" is observed; that is, where the model abruptly starts generating high-quality images corresponding to the control condition. The comparison visually demonstrates that Diff-Tuning achieves \"sudden convergence\" much faster than the standard ControlNet, indicating that Diff-Tuning enhances the speed and quality of controllable generation in diffusion models.", "section": "4.2 Transfer to Controllable Generation"}, {"figure_path": "S98OzJD3jn/figures/figures_20_2.jpg", "caption": "Figure 11: Generated images using standard ControlNet (SSIM of 0.82) and Diff-Tuning ControlNet (SSIM of 0.76). Analyzing these cases, Diff-Tuning generates images with higher quality and more details hence results in a lower similarity, indicating the limitation of similarity metrics and the necessity of human assessment.", "description": "This figure compares image generation results using standard ControlNet and Diff-Tuning ControlNet with edge control. Diff-Tuning produces images with better quality and more detail, despite having a lower Structural Similarity Index (SSIM) score than the standard ControlNet. This demonstrates that the SSIM metric does not fully capture the nuanced aspects of image quality and the importance of incorporating human assessment in evaluating generation quality. ", "section": "F.2 More Qualitative Analysis for Human Assessment"}]