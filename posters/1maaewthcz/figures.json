[{"figure_path": "1mAaewThcz/figures/figures_1_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "This figure shows the relationship between node degree and test loss for three different Graph Neural Network (GNN) architectures (RW, SYM, ATT) on the CiteSeer dataset.  The x-axis represents the degree of a node (number of connections), and the y-axis represents the test loss.  The plot demonstrates that high-degree nodes consistently have a lower test loss than low-degree nodes, indicating the presence of degree bias in GNN performance. Error bars illustrate the standard deviation across multiple random trials.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_4_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "This figure shows the relationship between the degree of nodes and their test loss in a node classification task using three different Graph Neural Network (GNN) architectures: Random Walk (RW), Symmetric (SYM), and Attention (ATT). The x-axis represents the node degree, and the y-axis represents the test loss.  The plots demonstrate that, across all three GNN architectures, high-degree nodes tend to have lower test losses compared to low-degree nodes. Error bars, representing the standard deviation, indicate the consistency of this trend across multiple runs with different random initializations.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_6_1.jpg", "caption": "Figure 3: Inverse collision probability vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. Node degrees generally have a strong association with inverse collision probabilities.", "description": "This figure shows the relationship between node degree and inverse collision probability in the CiteSeer dataset for three different graph neural network models (RW, SYM, and ATT).  The x-axis represents node degree, and the y-axis represents the inverse collision probability.  The plot demonstrates a strong positive correlation: higher degree nodes tend to have a higher inverse collision probability. This suggests that high-degree nodes are less likely to be misclassified during testing.", "section": "4 Test-Time Degree Bias"}, {"figure_path": "1mAaewThcz/figures/figures_28_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "The figure shows the relationship between the test loss and node degree for three different Graph Neural Network (GNN) models (RW, SYM, ATT) on the CiteSeer dataset.  The x-axis represents the node degree, and the y-axis represents the test loss.  Each point represents the average test loss for nodes of a given degree, calculated across 10 different random runs. Error bars show the standard deviation.  The graph demonstrates that higher-degree nodes tend to have lower test loss, indicating a degree bias in GNN performance.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_29_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "This figure shows the relationship between the test loss and the node degree for three different graph neural network (GNN) architectures (RW, SYM, and ATT) on the CiteSeer dataset.  The x-axis represents the node degree, and the y-axis represents the test loss.  The plot demonstrates that, across all three GNN architectures, nodes with higher degrees tend to have lower test loss. Error bars show the standard deviation across 10 different random seeds, indicating the consistency of the observation.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_30_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "The figure shows the relationship between test loss and node degree for three different graph neural network models (RW, SYM, and ATT) on the CiteSeer dataset.  It demonstrates that high-degree nodes consistently exhibit lower test loss, indicating a degree bias in the performance of these models. The error bars represent the standard deviation calculated across ten independent trials, demonstrating the reliability of the trend.", "section": "Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_31_1.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node\u2019s class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure provides a visual summary of the performance and training dynamics of three different GNN models (RW, SYM, ATT) on the CiteSeer dataset.  It shows the distribution of node representations in a 2D PCA space, highlighting differences between low and high-degree nodes.  Additionally, it displays the variance of test representations and the training loss curves for both low and high-degree nodes across epochs, demonstrating how the models adjust their loss differently for these node types.", "section": "Visual Summary of the Geometry of Representations, Variance of Representations, and Training Dynamics"}, {"figure_path": "1mAaewThcz/figures/figures_31_2.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node's class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure summarizes the results of applying three different graph neural network (GNN) models (RW, SYM, ATT) to the CiteSeer dataset.  It shows the distribution of node representations in 2D PCA space, the variance of those representations across different node degrees, and the training loss curves over epochs for low and high degree nodes.  The key takeaway is that RW shows higher variance in low-degree node representations, SYM shows lower variance in low degree nodes, and SYM adjusts its training loss on low-degree nodes more slowly than on high-degree nodes.", "section": "Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer"}, {"figure_path": "1mAaewThcz/figures/figures_32_1.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node's class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure provides a visual summary of the performance of three different graph neural network (GNN) models (RW, SYM, and ATT) on the CiteSeer dataset. It shows the distribution of node representations in a 2D principal component space, the variance of representations for low-degree and high-degree nodes, and the training loss curves for both types of nodes over 500 epochs.  The key observation is that high-degree nodes tend to have lower test loss and less variance in their representation than low-degree nodes, particularly evident with the Random Walk model. The Symmetric model exhibits less rapid training loss adjustment for low-degree nodes compared to the other models.  This visualization helps illustrate the core findings of the paper regarding degree bias in GNNs.", "section": "Visual summary of the geometry of representations, variance of representations, and training dynamics"}, {"figure_path": "1mAaewThcz/figures/figures_32_2.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node's class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure provides a visual summary of the performance of three different Graph Neural Network (GNN) models (RW, SYM, and ATT) on the CiteSeer dataset.  It showcases the differences in representation geometry (scatter plots), variance in test representations (line plots), and training loss dynamics (line plots) between low-degree and high-degree nodes. The results highlight how RW models show greater variance in representations for low-degree nodes, whereas SYM models exhibit slower training loss adjustments for low-degree nodes.  This suggests that SYM might be less prone to degree bias.", "section": "Visual Summary of the Geometry of Representations, Variance of Representations, and Training Dynamics"}, {"figure_path": "1mAaewThcz/figures/figures_33_1.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node's class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure provides a visual summary of the performance of three different GNNs (RW, SYM, ATT) on the CiteSeer dataset.  It shows the distribution of node representations in a 2D PCA space, the variance of these representations, and the training loss curves for low-degree and high-degree nodes. The results highlight differences in the behavior of the three GNNs, particularly regarding variance and training speed for low-degree nodes.", "section": "Visual Summary of the Geometry of Representations, Variance of Representations, and Training Dynamics"}, {"figure_path": "1mAaewThcz/figures/figures_33_2.jpg", "caption": "Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the node's class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.", "description": "This figure provides a visual summary of the performance characteristics of three different Graph Neural Network (GNN) models (RW, SYM, ATT) on the CiteSeer dataset.  The left column shows the distribution of node representations in two principal components, colored by class. The middle column shows the trace of sample covariance of the test representations as a function of training epochs, for low and high-degree nodes. The right column shows the training loss curves for low and high-degree nodes across epochs.  The results illustrate differences in representation geometry, variance, and training dynamics between the models and node degree.", "section": "Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer"}, {"figure_path": "1mAaewThcz/figures/figures_34_1.jpg", "caption": "Figure 3: Inverse collision probability vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. Node degrees generally have a strong association with inverse collision probabilities.", "description": "This figure shows the relationship between the inverse collision probability and node degree in the CiteSeer dataset using three different graph neural network models (RW, SYM, and ATT).  The inverse collision probability, a measure of node representation diversity, is plotted against the node degree.  The plot demonstrates a strong positive correlation, indicating that higher-degree nodes tend to have higher inverse collision probabilities and therefore more diverse representations.", "section": "Additional Inverse Collision Probability Plots"}, {"figure_path": "1mAaewThcz/figures/figures_35_1.jpg", "caption": "Figure 3: Inverse collision probability vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. Node degrees generally have a strong association with inverse collision probabilities.", "description": "This figure shows the relationship between the inverse collision probability and the node degree for three different graph neural network models (RW, SYM, and ATT) on the CiteSeer dataset.  The inverse collision probability is a measure of the diversity of random walks starting from a node.  The plot demonstrates that higher-degree nodes tend to have a higher inverse collision probability, indicating a stronger association between node degree and the diversity of random walks.", "section": "4 Test-Time Degree Bias"}, {"figure_path": "1mAaewThcz/figures/figures_37_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "This figure shows the relationship between the test loss and node degree for three different graph neural networks (RW, SYM, and ATT) on the CiteSeer dataset.  The x-axis represents the node degree, and the y-axis represents the test loss.  For all three networks, high-degree nodes (nodes with many connections) tend to have lower test loss compared to low-degree nodes. The error bars indicate the standard deviation of the test loss across 10 different random runs of the experiments.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_38_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "The figure shows the relationship between the test loss and node degree for three different Graph Neural Network (GNN) architectures (RW, SYM, and ATT) on the CiteSeer dataset.  It demonstrates that high-degree nodes consistently exhibit lower test loss than low-degree nodes, indicating a degree bias in the performance of GNNs. Error bars represent the standard deviation across multiple random trials.", "section": "1 Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_39_1.jpg", "caption": "Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.", "description": "The figure shows the relationship between the test loss and the node degree for three different Graph Neural Network (GNN) models (RW, SYM, and ATT) on the CiteSeer dataset.  The x-axis represents the node degree, and the y-axis represents the test loss.  The plot demonstrates that high-degree nodes tend to have lower test losses compared to low-degree nodes. This indicates a degree bias in the GNNs' performance, where the models perform better on nodes with more connections.  Error bars showing the standard deviation are included to indicate the variability of the results across different random seeds.", "section": "Introduction"}, {"figure_path": "1mAaewThcz/figures/figures_40_1.jpg", "caption": "Figure 18: Mean absolute parameter gradient vs. training epoch for RW, SYM, and ATT GNNs on chameleon and squirrel. The training accuracy of SYM, RW, and ATT ultimately reach the accuracy of MAJWL.", "description": "This figure shows the mean absolute parameter gradient and training accuracy for three different GNNs (RW, SYM, ATT) on two datasets (chameleon and squirrel) across training epochs. The plots illustrate the change in model parameters during training and how the training accuracy approaches the accuracy of a majority voting classifier (MAJWL).", "section": "J Additional Training Loss Plots"}]