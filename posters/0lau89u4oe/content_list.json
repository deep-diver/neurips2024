[{"type": "text", "text": "Accelerating Non-Maximum Suppression: A Graph Theory Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "King-Siong Si1\\* Lu Sun1\\* Weizhan Zhang1\u2020 Tieliang Gong2 Jiahao Wang2 Jiang Liu3 Hao Sun3 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, MOEKLINNS Lab, Xi\u2019an Jiaotong University 2School of Computer Science and Technology, BDKE Lab, Xi\u2019an Jiaotong University 3Institute of Artificial Intelligence (TeleAI), China Telecom ", "page_idx": 0}, {"type": "text", "text": "{sjsinx, sunlu.cs}@stu.xjtu.edu.cn, {zhangwzh, gongtl}@xjtu.edu.cn uguisu@stu.xjtu.edu.cn, {black_liu_1025, sun.010}@163.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Non-maximum suppression (NMS) is an indispensable post-processing step in object detection. With the continuous optimization of network models, NMS has become the \u201clast mile\u201d to enhance the efficiency of object detection. This paper systematically analyzes NMS from a graph theory perspective for the first time, revealing its intrinsic structure. Consequently, we propose two optimization methods, namely QSI-NMS and BOE-NMS. The former is a fast recursive divideand-conquer algorithm with negligible mAP loss, and its extended version (eQSINMS) achieves optimal complexity of $\\mathcal{O}(n\\log n)$ . The latter, concentrating on the locality of NMS, achieves an optimization at a constant level without an mAP loss penalty. Moreover, to facilitate rapid evaluation of NMS methods for researchers, we introduce NMS-Bench, the first benchmark designed to comprehensively assess various NMS methods. Taking the YOLOv8-N model on MS COCO 2017 as the benchmark setup, our method QSI-NMS provides $6.2\\times$ speed of original NMS on the benchmark, with a $0.1\\%$ decrease in mAP. The optimal eQSI-NMS, with only a $0.3\\%$ mAP decrease, achieves $10.7\\times$ speed. Meanwhile, BOE-NMS exhibits $5.1\\times$ speed with no compromise in mAP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Object detection is a highly significant and popular topic in computer vision, widely applied in various domains, e.g., multiple object tracking [27, 41, 14], medical imaging analysis [42, 36], multimodal object detection [6, 44], and autonomous driving [30, 9, 11]. In recent years, there has been significant attention on the real-time performance of object detection, with notable successes achieved in several research endeavors [34, 31, 26]. Non-maximum suppression (NMS) [7] is a post-processing technique used to eliminate duplicate detection boxes and obtain final detections. Some research on NMS has indeed enhanced the mean average precision (mAP) of object detection, but they have also introduced additional computational overhead. ", "page_idx": 0}, {"type": "text", "text": "Currently, most CNN-based object detection models (such as the R-CNN family [13, 12, 34] and the YOLO series [31, 32, 33, 2]) consist of two parts in the testing phase: model inference and post-processing. In recent years, with the continuous emergence of model lightweighting techniques [17, 39, 8], the time cost of model inference has been significantly reduced. As a result, NMS gradually becomes a bottleneck in the pipeline of object detection [45]. To address this, some studies [4, 46, 40] have proposed parallelization methods to enhance NMS efficiency. However, these methods do not reduce computational overhead; they rely heavily on efficient parallel computing to reduce overall time costs. The degree of parallelism depends on the hardware environment (such as processor type, quantity, and number of cores) and architecture, leading to significant variations in efficiency when models are deployed across different platforms. Additionally, NMS research lacks a unified evaluation framework for two main reasons. First, existing NMS methods require a complete model inference for each test, consuming a significant amount of unnecessary computational resources. Second, different NMS methods are tested on different platforms using various models, making comparisons between different NMS algorithms challenging. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To reduce the computational overhead of NMS, we first map the set of bounding boxes obtained from model inference to a graph $\\mathcal{G}$ . We then conduct a comprehensive and systematic analysis of the intrinsic structure of NMS from a graph theory perspective. Each box is considered a node in the graph, and the suppression relationships are represented as arcs. We discovered that this forms a directed acyclic graph (DAG), allowing us to solve NMS using dynamic programming. This indicates that as long as the graph $\\mathcal{G}$ can be quickly constructed, NMS can be efficiently performed. Through the analysis of $\\mathcal{G}$ , we find that it contains many weakly connected components (WCCs), and most of them are small. Based on these two characteristics, we propose two optimization strategies. First, due to the nature of dynamic programming, different WCCs are independent. We can use a divideand-conquer algorithm to break down the problem into smaller subproblems corresponding to these WCCs and solve them recursively. Inspired by quicksort, we propose quicksort induced NMS (QSINMS), which provides $6.18\\times$ speed with a negligible $0.1\\%$ decrease in mAP compared to original NMS in YOLOv8-N [21] on MS COCO 2017 [24]. Furthermore, by analyzing the structure of QSI-NMS, we propose extended QSI-NMS (eQSI-NMS) with a complexity of $\\mathcal{O}(n\\log n)$ , achieving state-of-the-art performance. Second, leveraging the locality suppression characteristic of NMS, where most weakly connected components are small, we exclude boxes that cannot have suppression relationships through geometric analysis. This led to the development of boxes outside excluded NMS (BOE-NMS), which provides $5.12\\times$ speed with no compromise in mAP compared to original NMS in YOLOv8-N on MS COCO 2017. ", "page_idx": 1}, {"type": "text", "text": "To facilitate the evaluation and comparison of NMS algorithms, we introduce NMS-Bench, the first end-to-end benchmark for rapid NMS validation. By decoupling model inference and postprocessing, we save substantial computational resources, enabling NMS validation to be completed within minutes. Moreover, by implementing NMS algorithms fairly within this framework, different NMS algorithms can be compared on an equal footing. Thus, we integrate data, benchmarking methods, and evaluation metrics into a single framework, enabling end-to-end rapid validation and simplifying NMS research for researchers. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present the first comprehensive analysis of the NMS algorithm from a graph theory perspective, uncovering the intrinsic structure of NMS;   \n\u2022 We propose two efficient NMS algorithms based on the properties of the NMS-induced graph;   \n\u2022 We introduce NMS-Bench, the first end-to-end benchmark for rapid NMS validation. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Original NMS, employs the intersection over union (IOU) between bounding boxes as the criterion for mutual suppression. Specifically, Given a set of candidate bounding boxes $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , original NMS selects the box $b^{*}$ with the highest confidence score from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , removes it from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , and adds it to the final output set $\\mathcal{D}$ . Then, it computes IOUs between $b^{*}$ and all other boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . If the IOU with a certain box $b$ is greater than a given threshold $N_{t}$ , then $b$ is removed from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . This process is repeated until B is empty. ", "page_idx": 1}, {"type": "text", "text": "In general, NMS during post-processing is an algorithm, which takes a list of detection bounding boxes $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ with corresponding confidence scores $\\boldsymbol{S}$ as input, and outputs a subset $\\mathcal{D}$ of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . And for convenience, we denote the cardinality of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ by $n$ , i.e., $n=|\\boldsymbol{B}|$ . Formally, the NMS algorithm takes $(\\beta,{\\mathcal{S}})$ as input, and outputs a sequence $K=\\left(k_{1},k_{2},.\\,.\\,.\\,,k_{n}\\right)$ , where ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{k_{i}=1\\quad\\mathrm{if}\\ b_{i}\\in\\mathcal{D};\\right.}\\\\ {\\left.k_{i}=0\\quad\\mathrm{otherwise}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "And an evaluation function is a mapping $e:\\{0,1\\}^{n}\\mapsto\\mathbb{R}$ , where a larger value of $e$ indicates a better NMS. The goal of our research is to enhance algorithm efficiency under the condition that ", "page_idx": 2}, {"type": "equation", "text": "$$\ne(K_{o r i g i n})-e(K)<\\varepsilon,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon>0$ represents the tolerance factor and $K_{o r i g i n}$ is the output of original NMS algorithm. In the object detection tasks of this paper, we use mAP as the evaluation function $e$ , and we set $\\varepsilon$ to $1\\%$ . ", "page_idx": 2}, {"type": "image", "img_path": "0lau89u4oE/tmp/714c8e5bb9c81baa013f089c8469c5960623acf18cdde0dc53a8e630b540126d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Dynamic programming in topological sorting. The color of the node represents the $\\delta$ value, i.e., black represents 1, and white represents 0. Before suppression, each node is black. In topological sorting, traversed arcs are represented by dashed lines, showing they have been removed from the graph. After the topological sorting is completed, we can find that nodes 1, 6, and 8 are all black, that is, the last boxes retained are $b_{1}$ , $b_{6}$ , and $b_{8}$ . ", "page_idx": 2}, {"type": "text", "text": "3 A Graph Theory Perspective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The bottleneck of NMS algorithms lies in the extensive computation of IOUs. In practice, many IOUs are smaller than a given threshold $N_{t}$ and will not have any suppressive effect. We aim to consider only those IOUs that will affect the final result, thereby reducing the number of computations and improving efficiency. An IOU greater than $N_{t}$ indicates that two boxes have a suppressive effect on each other; otherwise, they are independent. We can treat this relationship as an edge in a graph, with each box as a node. This graph reflects the intrinsic structure of NMS, representing the connections between all boxes. By this transformation, we can directly analyze the NMS algorithm through the graph. Compared to a set of boxes in a two-dimensional plane, the structure of the graph is clearer and has more properties that can be utilized. ", "page_idx": 2}, {"type": "text", "text": "Specifically, we can regard the input $B,S,N_{t}$ of NMS algorithms as a directed graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ . That\u2019s because we can think of every box in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ as a node in a graph and draw an arc from $v$ to $u$ if box $v$ can suppress box $u$ . Here, we give a formal definition. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Given a 3-tuple $(\\mathcal{B},\\mathcal{S},N_{t})$ consisting of the bounding boxes, confidence scores and an IOU threshold, a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ induced by NMS described as follows, there is an injective mapping of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ into $\\mathcal{V}$ that maps each bounding box $b_{v}$ in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ to a node $v\\in\\mathcal V$ , and for any ordered pair $(v,u)$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\na r c\\left(v,u\\right)\\in\\mathcal{E}\\iff s_{v}>s_{u}\\land I O U(b_{v},b_{u})>N_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proposition 1. $\\mathcal{G}$ is a directed acyclic graph $(D A G)$ . ", "page_idx": 2}, {"type": "text", "text": "We prove Proposition 1 in the Appendix. Since $\\mathcal{G}$ is a DAG, we can use dynamic programming to get the answer to NMS, i.e., $K$ . Specifically, let $\\delta(\\boldsymbol{v})$ be the result of node $v$ , i.e., $v$ is retained if $\\bar{\\delta}(v)=1$ , otherwise it\u2019s not. In original NMS, if there is a node $v$ that can suppress the current node $u$ , then $u$ will not be retained. Therefore, we have the dynamic programming equation as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta(u)=\\left\\{\\!\\!\\begin{array}{l l}{\\neg\\left(\\bigvee_{v,(v,u)\\in\\mathcal{E}}\\delta(v)\\right)}&{\\mathrm{if}\\;d^{-}(u)>0;}\\\\ {1}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d^{-}(u)$ denotes the in-degree of $u$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. $\\forall k_{i}=K_{o r i g i n}[i]$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nk_{i}=\\delta(i).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1 shows that we can actually obtain the result through dynamic programming in topological sorting, shown in Figure 1. Because the result of DP depends only on valid topological sorts, which indicates that we do not need to sort confidence scores in descending order like original NMS to get the same answer, as long as the topological sort is valid. Additionally, we can observe that if there is no path from node $v$ to node $u$ , then $v$ does not influence $u$ . From this, we derive Corollary 1. ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. If $v$ and u are in two different weakly connected components (WCCs) of $\\mathcal{G}$ , then $\\delta(\\boldsymbol{v})$ and $\\delta(u)$ are independent. ", "page_idx": 3}, {"type": "image", "img_path": "0lau89u4oE/tmp/9d4e6cce0136e3cae10fa02b6dcbe256b0facf339d2996a62f64fb8eae4ac26c.jpg", "img_caption": ["Figure 2: Statistical characteristics of graph $\\mathcal{G}$ on MS COCO 2017 validation. 2(a) The scatter plot of $5000\\,\\,\\mathcal{G}\\mathrm{s}$ on MS COCO 2017. It indicates that the number of arcs $\\vert\\mathcal{E}\\vert$ and the number of WCCs $|\\mathcal{W}|$ exhibit an approximately linear relationship with the number of nodes $\\vert\\mathcal{V}\\vert$ , respectively. 2(b) The violin plot of the sizes of WCCs across different categories on MS COCO 2017. It reveals the distributional characteristics of the sizes of the WCCs. It shows that over $50\\%$ of the WCCs have a size less than 5, and more than $75\\%$ have a size less than 10. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "We find that completing dynamic programming in topological sorting requires $O(|\\mathcal{V}|+|\\mathcal{E}|)$ time. In real-world data, $|\\mathcal{E}|$ appears to have a linear relationship to $\\vert\\nu\\vert$ (see Figure 2(a)), so once $\\mathcal{G}$ is determined, NMS can be highly efficient via DP. However, quickly determining $\\mathcal{G}$ is not a simple task. This is because, given a bounding box $b$ , it is difficult to quickly determine which boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ have an $\\mathrm{IOU}>N_{t}$ with it. A related problem is improving the efficiency of the $\\boldsymbol{\\mathrm{k}}$ -nearest neighbors algorithm (kNN), where [22, 10, 19, 1] have made significant progress. However, IOU is more complex than the distance defined by norms, and we can only approximate $\\mathcal{G}$ through related algorithms. We tried the latest research [10], but it provided little help in acceleration due to its large constant. ", "page_idx": 3}, {"type": "text", "text": "Fortunately, The NMS task is quite special, as its input comes from well-trained models, meaning that bounding boxes will cluster around many possible object locations, and bounding boxes predicted as different objects are independent of each other. This implies that $\\mathcal{G}$ is a sparse graph with many WCCs, as shown in Figure 2(a). Additionally, we find that most of the WCCs are quite small, as shown in Figure 2(b). These two observations respectively suggest two optimization strategies (see Figure 3). Firstly, because WCCs are independent of each other, we can use a divide-and-conquer algorithm to break down many WCCs into fewer WCCs, continuously reducing the problem size to improve computational efficiency. Thus, we design QSI-NMS. Secondly, because most WCCs are quite small in size, we can reduce the cost of constructing arcs by geometric knowledge, leading to the design of BOE-NMS. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the graph-theoretic analysis of NMS in Section 3, we propose two optimization methods based on two distinct characteristics of graph $\\mathcal{G}$ . Our approach is to design algorithms through the analysis of these characteristics to quickly construct $\\mathcal{G}$ or an approximate graph $\\tilde{\\mathcal{G}}=(\\tilde{\\mathcal{V}},\\tilde{\\mathcal{E}})\\approx\\mathcal{G}$ , enabling the use of dynamic programming in topological sorting to obtain NMS results. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "0lau89u4oE/tmp/1efabf38db2339fd4009727c2e53b0dc307a6e9d1148dc86a3743f0cce4a7c4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: The key ideas behind QSI-NMS (left) and BOE-NMS (right). $\\mathcal{G}$ (middle) contains many small weakly connected components (WCCs). QSI-NMS considers the global structure of the graph $\\mathcal{G}$ , where there are many WCCs. It selects a pivot (the red node on the left) and computes IOUs (orange edges) with all current subproblem nodes using a divide-and-conquer algorithm. BOE-NMS focuses on the local structure (the red dashed box) of $\\mathcal{G}$ , where most WCCs are quite small in size. It selects a node (the red node on the right) and only computes IOUs (orange edges) with its nearby nodes (solid arrows), which is derived from 2D plane geometric analysis (dashed arrows). ", "page_idx": 4}, {"type": "text", "text": "4.1 QSI-NMS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We observe that graph $\\mathcal{G}$ contains many WCCs, and according to Corollary 1, these components do not affect each other. This implies that, unlike the original NMS, which processes bounding boxes sequentially after sorting by confidence scores and is therefore very slow, we can solve the problem more efficiently using a divide-and-conquer algorithm, breaking it down into independent subproblems that can be solved recursively. Inspired by quicksort, we design quicksort induced NMS (QSI-NMS). ", "page_idx": 4}, {"type": "text", "text": "In each subproblem on $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , we can similarly select a pivot and calculate IOUs between the pivot and all the other boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , thereby constructing some arcs in $\\mathcal{G}$ . Next, we devise a partitioning criterion to split $B\\setminus\\{\\mathrm{pivot}\\}$ into two disjoint sets, $B_{l}$ and $\\boldsymbol{{\\beta}}_{r}$ , which are then solved recursively. Since IOUs are not calculated between boxes in $B_{l}$ and $B_{r}$ , some arcs in the original $\\mathcal{G}$ might be missed. Therefore, we need to carefully choose the pivot and partitioning criterion to ensure that the constructed $\\tilde{\\mathcal{G}}$ is as similar to $\\mathcal{G}$ as possible. ", "page_idx": 4}, {"type": "text", "text": "For the pivot selection, we need to define a priority to choose the best pivot in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . We find that selecting nodes with an in-degree of 0 in $\\mathcal{G}$ is optimal for two main reasons. First, node $v_{0}$ with an in-degree of 0 belongs to some WCCs, and since most nodes in a WCC predict the same object, $v_{0}$ with the maximum confidence score will suppress most nodes, meaning it has many outgoing arcs. Choosing other nodes in the WCC might allocate $v_{0}$ \u2019s successors to different subsets, leading to significant discrepancies between $\\mathcal{G}$ and $\\mathcal{G}^{\\prime}$ . Second, according to De Morgan\u2019s laws, the value of $\\bar{\\delta(u)}$ is essentially the conjunction of the negations of the predecessors\u2019 $\\delta$ values, formally described as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta(u)=\\bigwedge_{v,(v,u)\\in\\mathcal{E}}\\neg\\delta(v)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This implies that missing an arc $(v_{0},v)$ could result in $\\delta(v)$ being incorrectly computed as 1, causing a chain reaction that significantly deviates $K$ from $K_{o r i g i n}$ . According to Definition 1, the node ", "page_idx": 4}, {"type": "text", "text": "$v^{*}\\in\\mathcal{V}$ corresponding to the box $b^{*}\\in\\mathfrak{B}$ with the highest confidence score has an in-degree of 0.   \nHence, we select the box $b^{*}$ with the highest confidence score as the pivot. ", "page_idx": 5}, {"type": "text", "text": "For the partitioning criterion, we need to consider the spatial characteristics of different WCCs. Different WCCs are relatively dispersed in 2D space, so we can define the partitioning criterion based on the positions of the boxes. We represent the position of a box by its centroid, as it is intuitive and representative. Since the centroid is an ordered pair $(x,y)$ , we can not compare it directly like real numbers. We need to define a preorder in $\\mathbb{R}^{2}$ . We find that different preorders have negligible effects on mAP. See Appendix C.4 for details. We finally adopt the Manhattan distance to the origin $O(0,0)$ , i.e., the $L^{1}$ norm, as the comparison standard. Formally, we define a homogeneous relation $\\preceq_{M}$ on $\\mathbb{R}^{2}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n(x_{1},y_{1})\\preceq_{M}(x_{2},y_{2})\\Leftrightarrow|x_{1}|+|y_{1}|\\leq|x_{2}|+|y_{2}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we partition the set $B\\setminus\\{b^{*}\\}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\mathcal{B}_{l}=\\{b_{c}|b_{c}\\preceq_{M}b_{c}^{*}\\wedge b\\in\\mathcal{B}\\setminus\\{b^{*}\\}\\right\\};}\\\\ {\\mathcal{B}_{r}=\\{b_{c}|b_{c}\\not\\in\\mathcal{A}\\;b_{c}^{*}\\wedge b\\in\\mathcal{B}\\setminus\\{b^{*}\\}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $b_{c}$ and $b_{c}^{*}$ denote the centroid of $b$ and $b^{*}$ , respectively. Since we always choose the box with the highest confidence score, this creates a valid topological sort of $\\tilde{\\mathcal{G}}$ . Thus, we can avoid explicitly constructing $\\tilde{\\mathcal{G}}$ , further reducing computational overhead. The pseudo-code for QSI-NMS can be found in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "eQSI-NMS Taking $\\mathcal{O}(n\\log n)$ Time Though QSI-NMS performs very well in the real world, it is not an $\\mathcal{O}(n\\log n)$ algorithm for the simple reason that the pivot is not chosen randomly. By analyzing the structure of QSI-NMS, we further optimize it and propose extended QSI-NMS (eQSI-NMS), which only takes $\\mathcal{O}(n\\log n)$ time. Since in carrying out QSI-NMS we always split the problem into two subproblems, we can thus construct a binary tree. ", "page_idx": 5}, {"type": "text", "text": "Definition 2. Given a $^3$ -tuple $(\\boldsymbol{\\mathcal{B}},\\boldsymbol{\\mathcal{S}},\\boldsymbol{N_{t}})$ , a QSI-tree for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ denoted by $Q T(B)$ is a binary tree defined recursively as follow: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Its root is a node corresponding to the box $b_{v}\\in\\boldsymbol{B}$ with maximum confidence $s_{v}$ .   \n\u2022 Its left subtree is $Q T(B_{l})$ , where $B_{l}$ is the left subset of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in QSI-NMS.   \n\u2022 Its right subtree is $Q T(B_{r})$ , where $B_{r}$ is the right subset of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in QSI-NMS. ", "page_idx": 5}, {"type": "text", "text": "The basic case is that if $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is empty, then $Q S I$ -tree is also empty, i.e., $Q T(\\emptyset)=\\emptyset$ . ", "page_idx": 5}, {"type": "text", "text": "An example of QSI-tree is shown in Figure 4. QSI-tree reveals the inherent structure of QSINMS, allowing us to consider QSI-NMS from a high-level perspective. More generally, in QSINMS, we tag each box with an ordered pair $(p,c)$ , where $p\\in\\mathcal{P}$ represents the priority and $c\\in{\\mathcal{C}}$ is the key used for partitioning. We define preorder relations $\\preceq\\!\\mathcal{P}$ on $\\mathcal{P}$ and $\\preceq c$ on $\\mathcal{C}$ . This indicates that the QSI-tree is essentially a binary search tree that satisfies the max-heap property: the priority of the parent node is not less than that of the child nodes, the keys in the left subtree are all less than or equal to the parent node, and the keys in the right subtree are all greater than the parent node. Furthermore, we have the following Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. We sort all the elements of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in ascending order of boxes\u2019 centroids according to the preorder $\\preceq\\!c$ into a sequence: ", "page_idx": 5}, {"type": "image", "img_path": "0lau89u4oE/tmp/cbcd8fc91af1b57370b18162d7d9c2a15a260471e86bdd4b84af2c0c63479ba6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: A Cartesian tree for $B$ . The $\\mathbf{X}$ -axis represents the centroid, where the node on the left $\\preceq\\!c$ the one on the right. The y-axis represents the confidence score, where the node below $\\preceq\\!\\mathcal{P}$ the one above. The values of the sequence below the $\\mathbf{X}$ -axis are the confidence scores of $B$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\nB=(b_{i_{1}},b_{i_{2}},\\ldots,b_{i_{n}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then QSI-tree is a Cartesian tree for $B$ in which each key is the confidence score of the corresponding box. ", "page_idx": 5}, {"type": "text", "text": "According to the dynamic programming, if a node $v$ can affect the result $\\delta(u)$ of node $u$ , there must exist a path between $v$ and $u$ . In QSI-NMS, this manifests as node $v$ only being able to influence nodes within its subtrees in QSI-tree. Theorem 2 states that QSI-tree is a Cartesian tree, indicating that the subtree of $v$ corresponds to a contiguous interval in $B$ , as shown in Figure 4. ", "page_idx": 6}, {"type": "text", "text": "Specifically, the subtree of $v$ corresponds to the contiguous interval $B[l_{v}+1:r_{v}-1]$ in $B$ , where $l_{v}$ is the last position before $v$ that is greater than $s_{v}$ , and $r_{v}$ is the first position after $v$ that is greater than $s_{v}$ . Finding $(l_{v},r_{v})$ for all $v$ is known as the all nearest greater values problem, which can be solved in ${\\mathcal{O}}(n)$ time by maintaining a stack. Similarly to QSI-NMS, we can complete the suppression during the algorithm. Therefore, we obtain the following time complexity: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{O}(n\\log n+n)=\\mathcal{O}(n\\log n).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "According to our best knowledge, this algorithm is the most optimal in terms of complexity. The pseudo-code can also be found in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.2 BOE-NMS ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We find that the vast majority of WCCs in $\\mathcal{G}$ are very small, as shown in Figure 2(b). This is because there are not many bounding boxes predicting the same object, and NMS is a form of local suppression. We hope to consider the locality of box distributions, so that the currently selected box only computes IOUs with boxes corresponding to nodes in the same WCC, rather than computing IOUs with all boxes as in original NMS. ", "page_idx": 6}, {"type": "text", "text": "We focus on the spatial locality of boxes. We found that a box is likely to have large IOUs only with neighbors that are relatively close to it in 2D space, which also indicates that $\\mathcal{G}$ is a sparse graph. Formally, we have the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Given a bounding box $b^{*}\\in\\boldsymbol{B}$ , $\\forall b\\in\\mathcal{B}$ , we have $\\begin{array}{r}{I O U(b^{*},b)\\le\\frac{1}{2}}\\end{array}$ if the centroid of $b$ does not lie within $b^{*}$ . Formally, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left({{x}_{c}^{(b)}}>{{x}_{r b}^{(b^{*})}}\\lor{{x}_{c}^{(b)}}<{{x}_{l t}^{(b^{*})}}\\right)\\lor\\left({{y}_{c}^{(b)}}>{{y}_{r b}^{(b^{*})}}\\lor{{y}_{c}^{(b)}}<{{y}_{l t}^{(b^{*})}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $(x_{c}^{(b)},y_{c}^{(b)})$ , $(x_{l t}^{(b^{*})},y_{l t}^{(b^{*})})$ and $(x_{r b}^{(b^{*})},y_{r b}^{(b^{*})})$ denote the coordinates of the centroid of $b,$ , the left-top and the right-bottom corners of $b^{*}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Since $N_{t}$ is usually greater than 0.5, e.g., 0.7 for YOLOv8 and Faster R-CNN. By Theorem 3 we can check IOUs only for those boxes whose centorids lie within the current box. Based on this, we propose boxes outside excluded NMS (BOE-NMS), a method devoid of mAP loss. ", "page_idx": 6}, {"type": "text", "text": "In BOE-NMS, We first sort the boxes by their centroids according to lexicographic order $\\preceq\\!L$ on $\\mathbb{R}^{2}$ which is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n(x_{1},y_{1})\\preceq_{L}(x_{2},y_{2})\\Leftrightarrow(x_{1}<x_{2})\\lor(x_{1}=x_{2}\\land y_{1}\\leq y_{2}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then for the current box $b^{*}$ , we can find all the boxes whose centroids may lie in $b^{*}$ in ${\\mathcal{O}}(\\log n)$ time, and we just need to check one by one whether the IOUs between $b$ and these boxes are greater than $N_{t}$ . The pseudo-code for BOE-NMS is described in Algorithm 3 which can be found in the Appendix. Let\u2019s set aside Theorem 3 for now. A more intuitive but weaker conclusion is that if two boxes do not intersect, their IOU must be 0. However, this is not conducive to efficient implementation because of the high cost of maintaining the corresponding data structure. We discuss this issue in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "$N_{t}$ is typically set to 0.7, and the method based on Theorem 3 does not introduce errors. We also provide a tighter bound to further optimize BOE-NMS. Based on Theorem 4 which is a generalization of Theorem 3, we can handle cases where $N_{t}$ is any real number $\\in(0,1)$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. We use s to denote a scaling factor, and then we can use $\\alpha(b,s)$ to represent the new box $b^{\\prime}$ obtained by scaling b. Formally, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{x_{l t}^{(b^{\\prime})}=x_{c}^{(b)}-s\\times|x_{l t}^{(b)}-x_{c}^{(b)}|,}\\\\ {x_{r b}^{(b^{\\prime})}=x_{c}^{(b)}+s\\times|x_{r b}^{(b)}-x_{c}^{(b)}|,}\\\\ {y_{l t}^{(b^{\\prime})}=y_{c}^{(b)}-s\\times|y_{l t}^{(b)}-y_{c}^{(b)}|,}\\\\ {y_{r b}^{(b^{\\prime})}=y_{c}^{(b)}+s\\times|y_{r b}^{(b)}-y_{c}^{(b)}|.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given any $N_{t}\\in(0,1)$ , if the centroid of $b$ does not lie within $\\alpha(b^{*},{^1\\mathord{\\left/{\\vphantom{^10}}\\right.\\kern-\\nulldelimiterspace}N_{t}}-1)$ , then $I O U(b,b^{*})\\le N_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Since BOE-NMS only excludes boxes with $\\mathrm{IOU}\\le N_{t}$ , the graph constructed by the BOE-NMS is the same as $\\mathcal{G}$ . In other words, the results of BOE-NMS are identical to original NMS. However, unlike original NMS, BOE-NMS does not need to compute IOUs with all remaining boxes but rather determines the boxes that could potentially be suppressed in ${\\mathcal{O}}(\\log n)$ time. Next, inspect each of these $t$ ( $t\\approx$ size of the corresponding WCC) boxes one by one in $\\mathcal{O}(t)$ time. As shown in Figure 2(b), the sizes of weakly connected components are almost all less than a constant, say 10. This means that the actual performance of BOE-NMS approaches linear time complexity, but strictly speaking, the complexity is still $O(n^{2})$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we first introduce NMS-Bench, the first end-to-end benchmark for rapid validation of NMS algorithms. Next, we validate our algorithms on NMS-Bench and compare them with classical algorithms: original NMS [7], Fast NMS [4], and Cluster-NMS [46]. We conduct tests on MS COCO 2017 [24] and Open Images V7 [23] using YOLOv5 [20], YOLOv8 [21], and Faster R-CNN [12] as validation models. More experimental details can be found in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "5.1 NMS-Bench ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "NMS-Bench is a robust framework that allows researchers to evaluate various NMS methods over different models and datasets in a few minutes. NMS-Bench primarily consists of three components: original bounding box data without NMS applied, implementations of various NMS algorithms as benchmarking methods, and evaluation metrics. The code for NMS-Bench is available on GitHub\u2020. ", "page_idx": 7}, {"type": "text", "text": "For the original boxes, we extracted non-NMS boxes using different models (YOLO series [31, 21] and Faster R-CNN [12]) on various datasets [24, 23] to create the NMS-Bench dataset, thereby decoupling the model inference and post-processing stages. This approach saves significant computational resources during inference. We provide a large amount of data for testing, including original boxes from a total of 273,100 images. More detailed information can be found in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "For benchmarking methods, NMS-Bench implements classical algorithms such as original NMS [7], Fast NMS [4], Cluster-NMS [46], and PSRR-MaxpoolNMS [43]. QSI-NMS (including eQSI-NMS) and BOE-NMS are also included in NMS-Bench. These methods enable researchers to reproduce and study NMS algorithms. All algorithms are implemented fairly. Researchers can also quickly implement and validate their own NMS algorithms, as NMS-Bench is a plug-and-play, end-to-end benchmark. ", "page_idx": 7}, {"type": "text", "text": "For evaluation metrics, we use COCO-style mAP as the accuracy metric and average processing latency per image as the efficiency metric. The latency calculation begins from the input of bounding boxes and ends when the retained bounding boxes are output. For a dataset containing $N$ images, latency is measured by using the bounding boxes generated per image as input, and the total latency for the $N$ images is averaged. To mitigate random errors, this measurement is repeated 5 times, and the average of these measurements is used as the final average latency. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Table 1, we compare our methods with some mainstream work on MS COCO 2017. We observe that our methods, particularly eQSI-NMS, demonstrate substantial performance enhancements in processing speed across different models on MS COCO 2017. eQSI-NMS stands out by offering up to $16.9\\times$ speed of original NMS, $4.3\\times$ speed of Fast NMS, and $8.9\\times$ speed of Cluster-NMS with a mAP decrease of about $0.5\\%$ . Similarly, QSI-NMS provides $8.8\\times$ speed of original NMS, $2.2\\times$ speed of Fast NMS, and $4.6\\times$ speed of Cluster-NMS with a marginal mAP decrease of about $0.2\\%$ . BOE-NMS also shows significant enhancements, being $9.1\\times$ as fast as original NMS, $2.3\\times$ as fast as Fast NMS, and $4.8\\times$ as fast as Cluster-NMS with no mAP decrease. ", "page_idx": 7}, {"type": "text", "text": "Table 2 shows that on Open Images V7, eQSI-NMS provides approximately $10.2\\times$ speed of original NMS, $3.7\\times$ speed of Fast NMS, and $7.0\\times$ speed of Cluster-NMS. QSI-NMS is about $5.6\\times$ as fast as original NMS, $2.0\\times$ as fast as Fast NMS, and $3.9\\times$ as fast as Cluster-NMS. Similarly, BOE-NMS achieves $5.4\\times$ speed of original NMS, $2.0\\times$ speed of Fast NMS, and $3.8\\times$ speed of Cluster-NMS. On Open Images V7, QSI-NMS and eQSI-NMS perferm well in mAP with about $0.2\\%$ mAP decreasing. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "0lau89u4oE/tmp/96e568416d66c7d2c81582e0811c4b16259f6affec3069069049b7cba51d8752.jpg", "table_caption": ["Table 1: NMS Methods Performance on MS COCO 2017 "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0lau89u4oE/tmp/203c6e201b45cb159020318f0103be5fe93e96a766f4efa1751c310c44467d9e.jpg", "table_caption": ["Table 2: NMS Methods Performance on Open Images V7 "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "0lau89u4oE/tmp/dd0f5da0f2d0e012a13aa2579ec5edd5611e70b43c44c639d408316232ffe3d2.jpg", "table_caption": ["Table 3: Comparisons of Our Methods and PSRR-MaxpoolNMS "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "MaxpoolNMS [5] and ASAP-NMS [38] are only applicable to the first stage of two-stage detectors, while the problem we are discussing is more general, so we do not include them in our comparison. We compare our methods with PSRR-MaxpoolNMS [43], which is applicable to anchor-based models. We conduct experiments on anchor-based models (Faster R-CNN and YOLOv5) using MS COCO 2017, and the results are shown in Table 3. As we can see, eQSI-NMS achieves the lowest latency while maintaining a favorable trade-off with mAP. However, PSRR-MaxpoolNMS experiences a $1\\sim2\\%$ mAP loss in the Faster R-CNN and YOLOv5 models. ", "page_idx": 8}, {"type": "text", "text": "In the case of Faster R-CNN, the latency performance of PSRR-MaxpoolNMS is not competitive. This is because PSRR-MaxpoolNMS requires 8 max-pooling operations, which, although not affecting the algorithm\u2019s complexity, introduces a large constant factor that hampers efficiency when the number of bounding boxes is small (e.g., the average number of bounding boxes in the three Faster R-CNN models is less than 300). However, it performs well when the number of bounding boxes is large (e.g., YOLOv5-S has an average of 2898 bounding boxes). This demonstrates that the speedup of PSRR-MaxpoolNMS is highly dependent on the degree of parallelism, whereas our methods directly reduce computational overhead (see Figure 5 in Appendix D.3), making it hardware-agnostic and suitable for resource-constrained edge devices. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "NMS algorithm is widely used in object detection tasks [35, 31, 34]. Original NMS [7] operates on a greedy principle, suppressing bounding boxes with an Intersection over Union (IOU) higher than a given threshold, starting from the ones with the highest confidence scores. On one hand, numerous improvements have been made to NMS to achieve higher mAP in certain scenarios [3, 29, 16, 25, 47, 18, 37]. On the other hand, some research focuses on enhancing speed. Fast NMS [4] improves NMS efficiency by avoiding the sequential processing of bounding boxes that need to be suppressed, making it more conducive to parallel computing and thus speeding up the process, though it may slightly reduce accuracy compared to original NMS. Cluster-NMS [46], employs matrix operations and iterative processing, running the Fast NMS algorithm in each iteration to accelerate the original NMS without compromising accuracy. MaxpoolNMS [5] and ASAP-NMS [38] take into account the setting of \"anchors\" in the region proposal network (RPN) of two-stage detectors. MaxpoolNMS maps anchors of different sizes onto several score maps and performs spatial max-pooling on these score maps to avoid calculating IOUs, thereby improving the speed of NMS. ASAP-NMS eliminates some boxes with relatively small IOUs by precomputing the IOUs between the current box and neighboring anchors. PSRR-MaxpoolNMS [43] improves upon MaxpoolNMS by introducing Relationship Recovery, which addresses the issue of score map mismatch that may arise in MaxpoolNMS, enabling PSRR-MaxpoolNMS to be used in the second stage of two-stage detectors. CUDA NMS by torchvision [28] is a CUDA implementation of the original NMS, leveraging a GPU to accelerate computation-intensive tasks, though it cannot be used in scenarios without a GPU. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we systematically analyze the NMS algorithm from a graph theory perspective and discover strong connections between NMS, directed graph topological sorting, dynamic programming, and weak connected components. Through these analyses, we first propose QSI-NMS, a fast divideand-conquer algorithm with negligible loss, and its extended version, eQSI-NMS, achieves the state-of-the-art complexity $\\mathcal{O}(n\\log n)$ . Additionally, starting from the sparsity of graphs, we design BOE-NMS, which considers the locality suppression feature of NMS, optimizes the NMS algorithm at a constant level, and maintains precision. Furthermore, we introduce NMS-Bench, the first end-toend benchmark integrating bounding box datasets, NMS benchmarking methods, and evaluations, facilitating NMS research for researchers. Finally, we conducted experiments on NMS-Bench, and the experimental results validated our theory, demonstrating the superiority of our algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements This work was supported in part by the National Natural Science Foundation of China under Grant 62192781, Grant 62172326 and Grant 62137002, and in part by the Project of China Knowledge Centre for Engineering Science and Technology. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In Proceedings of the 23rd International Conference on Machine Learning, pages 97\u2013104, 2006.   \n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.   \n[3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms \u2013 improving object detection with one line of code. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5561\u20135569, 2017.   \n[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9157\u20139166, 2019.   \n[5] Lile Cai, Bin Zhao, Zhe Wang, Jie Lin, Chuan Sheng Foo, Mohamed Sabry Aly, and Vijay Chandrasekhar. Maxpoolnms: Getting rid of nms bottlenecks in two-stage object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9348\u20139356, 2019.   \n[6] Yue Cao, Junchi Bin, Jozsef Hamari, Erik Blasch, and Zheng Liu. Multimodal object detection by channel switching and spatial attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 403\u2013411, 2023.   \n[7] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages 886\u2013893, 2005.   \n[8] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13733\u201313742, 2021. [9] Piotr Doll\u00e1r, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 304\u2013311, 2009.   \n[10] Yury Elkin and Vitaliy Kurlin. A new near-linear time algorithm for k-nearest neighbor search using a compressed cover tree. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 9267\u20139311, 2023.   \n[11] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.   \n[12] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440\u20131448, 2015.   \n[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 580\u2013587, 2014.   \n[14] Mei Han, Amit Sethi, Wei Hua, and Yihong Gong. A detection-based multiple object tracking method. In Proceedings of the International Conference on Image Processing, volume 5, pages 3065\u20133068, 2004.   \n[15] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 2961\u20132969, 2017.   \n[16] Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, and Xiangyu Zhang. Bounding box regression with uncertainty for accurate object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2883\u20132892, 2019.   \n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[18] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4507\u20134515, 2017.   \n[19] Mike Izbicki and Christian Shelton. Faster cover trees. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 1162\u20131170, 2015.   \n[20] Glenn Jocher. YOLOv5 by Ultralytics, May 2020.   \n[21] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics YOLO, January 2023.   \n[22] Thomas Kollar. Fast nearest neighbors. Technical report, Computer Science and Artificial Intelligence Lab, 2006.   \n[23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020.   \n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision, pages 740\u2013755, 2014.   \n[25] Songtao Liu, Di Huang, and Yunhong Wang. Adaptive nms: Refining pedestrian detection in a crowd. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6459\u20136468, 2019.   \n[26] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision, pages 21\u201337, 2016.   \n[27] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: A literature review. Artificial Intelligence, 293:103448, 2021.   \n[28] TorchVision maintainers and contributors. TorchVision: PyTorch\u2019s Computer Vision library, November 2016.   \n[29] Chengcheng Ning, Huajun Zhou, Yan Song, and Jinhui Tang. Inception single shot multibox detector for object detection. In Proceedings of the IEEE International Conference on Multimedia & Expo Workshops, pages 549\u2013554, 2017.   \n[30] Rui Qian, Xin Lai, and Xirong Li. 3d object detection for autonomous driving: A survey. Pattern Recognition, 130:108796, 2022.   \n[31] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 779\u2013788, 2016.   \n[32] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7263\u20137271, 2017.   \n[33] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.   \n[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, volume 28, 2015.   \n[35] A. Rosenfeld and M. Thurston. Edge and curve detection for visual scene analysis. IEEE Transactions on Computers, C-20(5):562\u2013569, 1971.   \n[36] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual Review of Biomedical Engineering, 19:221\u2013248, 2017.   \n[37] Charalampos Symeonidis, Ioannis Mademlis, Ioannis Pitas, and Nikos Nikolaidis. Efficient feature extraction for non-maximum suppression in visual person detection. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1\u20135, 2023.   \n[38] Rohun Tripathi, Vasu Singla, Mahyar Najibi, Bharat Singh, Abhishek Sharma, and Larry Davis. Asap-nms: Accelerating non-maximum suppression using spatially aware priors. arXiv preprint arXiv:2007.09785, 2020.   \n[39] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 390\u2013391, 2020.   \n[40] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. In Advances in Neural Information Processing Systems, volume 33, pages 17721\u201317732, 2020.   \n[41] Yingkun Xu, Xiaolong Zhou, Shengyong Chen, and Fenfen Li. Deep learning for multiple object tracking: a survey. IET Computer Vision, 13(4):355\u2013368, 2019.   \n[42] Ruixin Yang and Yingyan Yu. Artificial convolutional neural network in object detection and semantic segmentation for medical imaging analysis. Frontiers in Oncology, 11:638182, 2021.   \n[43] Tianyi Zhang, Jie Lin, Peng Hu, Bin Zhao, and Mohamed M. Sabry Aly. Psrr-maxpoolnms: Pyramid shifted maxpoolnms with relationship recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15840\u201315848, 2021.   \n[44] Zhihao Zhang, Shengcao Cao, and Yu-Xiong Wang. Tamm: Triadapter multi-modal learning for 3d shape understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21413\u201321423, 2024.   \n[45] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16965\u201316974, 2024.   \n[46] Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rongguang Ye, Qinghua Hu, and Wangmeng Zuo. Enhancing geometric factors in model learning and inference for object detection and instance segmentation. IEEE Transactions on Cybernetics, 52(8):8574\u20138586, 2021.   \n[47] Penghao Zhou, Chong Zhou, Pai Peng, Junlong Du, Xing Sun, Xiaowei Guo, and Feiyue Huang. Noh-nms: Improving pedestrian detection by nearby objects hallucination. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1967\u20131975, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Preliminaries 15   \nA.1 Graph Theory 15   \nA.2 Cartesian Tree 15   \nProofs 16   \nB.1 Proof of Proposition 1 . 16   \nB.2 Proof of Theorem 1 16   \nB.3 Proof of Theorem 2 16   \nB.4 Proof of Theorem 3 16   \nB.5 Proof of Theorem 4 17   \nB.6 Proof of Proposition 3 19   \nC Discussion 20   \nC.1 Discussion of Fast NMS and Cluster-NMS 20   \nC.2 Discussion of MaxpoolNMS, ASAP-NMS, and PSRR-MaxpoolNMS 21   \nC.3 Discussion on mAP loss of QSI-NMS 22   \nC.4 Further Discussion of QSI-NMS and eQSI-NMS 24   \nC.5 Further Discussion of BOE-NMS 24   \nExperimental Details 26   \nD.1 More Information about NMS-Bench 26   \nD.2 Experimental Environment and Settings 26   \nD.3 More Results 27   \nPseudo-Codes 29   \nE.1 Pseudo-Code for QSI-NMS 29   \nE.2 Pseudo-Code for eQSI-NMS 30   \nE.3 Pseudo-Code for BOE-NMS 31 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Graph Theory ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Notations and Terminology A directed graph $\\mathcal{G}$ , also called a digraph, is an ordered pair $(\\boldsymbol{\\nu},\\boldsymbol{\\mathcal{E}})$ consisting of a nonempty set $\\mathcal{V}$ of nodes, a set $\\mathcal{E}$ of arcs. An arc, also called an arrow, is an ordered pair $(v,u)$ of nodes, where $v,u\\in\\mathcal{V}$ . $v$ and $u$ are the head and tail of the arc $(v,u)$ , respectively. We say two nodes $v,u$ are adjacent if there exists an arc $e\\in\\mathcal{E}$ such that $\\boldsymbol{e}=(v,u)\\vee\\boldsymbol{e}=(u,v)$ . A direct predecessor of node $v$ is the head of an arc whose tail is $v$ , and a direct successor is the tail of an arc whose head is $v$ . The in-degree $d^{-}(v)$ of a node $v\\in\\mathcal{V}$ is the number of arcs with tail $v$ while the out-degree $d^{+}(v)$ is the number of arcs with head $v$ . A walk $(v_{0},v_{1},\\ldots,v_{k})$ in a directed graph is a sequence of nodes satisfying $(v_{i},v_{i+1})\\in\\mathcal{E}$ for all $i=0,1,\\ldots,k-1$ . A path in a directed graph is a walk in which all vertices are distinct. A cycle is a path $(v_{0},v_{1},\\ldots,v_{k})$ together with the arc $(v_{k},v_{0})$ . We say a graph $\\mathcal{G}$ is simple if there are no multi-edges or self-loops in $\\mathcal{G}$ . $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ contains a graph $\\mathcal{G}^{j}=\\mathbf{\\bar{\\alpha}}(\\bar{\\nu^{\\prime}},\\mathcal{E}^{\\prime})$ if $\\mathcal{V}^{\\,}\\bar{\\subset}\\,\\mathcal{V}\\wedge\\mathcal{E}^{\\prime}\\subset\\mathcal{E}$ , we then say $\\mathcal{G}^{\\prime}$ is a subgraph of $\\mathcal{G}$ and denote $\\mathcal{G}^{\\prime}\\subset\\mathcal{G}$ . ", "page_idx": 14}, {"type": "text", "text": "We next introduce several key concepts. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Directed acyclic graph). A directed acyclic graph $(D A G)$ is a directed graph without cycles. ", "page_idx": 14}, {"type": "text", "text": "Definition 4 (Strongly connected). A directed graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ is called strongly connected if it contains a path between $v,u$ and a path between $u,v,$ , for every pair $v,u\\in\\mathcal{V}$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 5 (Weakly connected). A directed graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ is weakly connected if the symmetric graph $\\mathcal{G}^{\\prime}=(\\mathcal{V},\\mathcal{E}^{\\prime})$ with $\\mathcal{E}^{\\prime}=\\mathcal{E}\\cup\\{(u,v)|(v,u)\\in\\mathcal{E}\\}$ is strongly connected. ", "page_idx": 14}, {"type": "text", "text": "Definition 6 (Strongly connected component). For a directed graph, an inclusion-maximal strongly connected subgraph is called strongly connected component $(S C C)$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 7 (Weakly connected component). For a directed graph, an inclusion-maximal weakly connected subgraph is called weakly connected component (WCC). ", "page_idx": 14}, {"type": "text", "text": "A.2 Cartesian Tree ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 8. A Cartesian tree for a sequence is a binary tree constructed as follows, \u2022 The root of the tree is the maximum element of the sequence. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "\u2022 Its left and right subtrees are formed by recursively constructing Cartesian tree for the subsequences to the left and right. ", "page_idx": 14}, {"type": "text", "text": "The basic case is that if the sequence is empty, then the Cartesian tree is also empty. ", "page_idx": 14}, {"type": "text", "text": "During the construction process of a Cartesian tree, the maximum value is selected from the current sequence as the root each time. Thus, the Cartesian tree obeys the max-heap property whose inorder traversal returns the original sequence. The complexity of constructing a Cartesian tree according to Definition 8 is $O(n^{2})$ in the worst-case which is too slow. Fortunately, we have the following proposition: ", "page_idx": 14}, {"type": "text", "text": "Proposition 2. Given a sequence $S$ of length $n$ , a Cartesian tree for $S$ can be constructed in ${\\mathcal{O}}(n)$ time. ", "page_idx": 14}, {"type": "text", "text": "Here, we provide an informal proof. We can establish Proposition 2 by designing an algorithm with $\\mathcal{O}(n)$ complexity. We consider adding elements from $S$ one by one to construct the Cartesian tree. Assume that we have already constructed the Cartesian tree for $S[1:k-1]$ . Now, when adding $S[k]$ , we observe that $S[k]$ will be the rightmost node of the tree. Furthermore, since Cartesian trees are max-heaps, this implies that the parent of $S[k]$ must be greater than or equal to $S[k]$ . Therefore, we need to find the rightmost node $v$ in the Cartesian tree that is greater than or equal to $\\bar{S}[k]$ , and assign its right child $u$ as the left child of $S[k]$ , updating the right child of $v$ to be $S[k]$ . We can implement this algorithm efficiently by maintaining a stack: pop the stack until the stack top is not less than $S[k]$ , and then update the tree structure accordingly. After this, push $S[k]$ onto the stack. When $k$ is iterated from 1 to $n$ , the construction of the Cartesian tree is also completed. Since each element is pushed onto and popped from the stack at most once, the overall complexity is $O(n)$ . ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Assume that there is cycle in the directed graph $\\mathcal{G}$ with length $k$ , i.e., there exists a sequence ", "page_idx": 15}, {"type": "equation", "text": "$$\n(v_{0},v_{1},v_{2},\\ldots,v_{k}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $v_{0}=v_{k}$ and $\\forall i=0,1,\\dots,k-1,(v_{i},v_{i+1})\\in\\mathcal{E}.$ . ", "page_idx": 15}, {"type": "text", "text": "By definition, we know that if $(v,u)\\in\\mathcal{E}$ , then $b_{v}>b_{u}$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\nb_{v_{0}}>b_{v_{1}}>b_{v_{2}}>\\ldots>b_{v_{k}}=b_{v_{0}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which demonstrates that ", "page_idx": 15}, {"type": "equation", "text": "$$\nb_{v_{0}}>b_{v_{0}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is contradictory to the fact that $b_{v_{0}}$ is a real number. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We first sort the scores in $\\boldsymbol{S}$ in descending order into a sequence $S:(s_{i_{1}},s_{i_{2}},\\ldots,s_{i_{n}})$ . To prove Theorem 1, we prove that statement $\\delta(i_{t})\\stackrel{}{=}k_{i_{t}}$ holds for all $t=1,2,\\ldots,n$ by mathematical induction. ", "page_idx": 15}, {"type": "text", "text": "For $t=1$ , $k_{i_{1}}=1$ , that\u2019s because no box can suppress $b_{i_{1}}$ with the maximum score $s_{i_{1}}$ . $\\delta(i_{1})=1=$   \n$k_{i_{1}}$ since the in-degree of $i_{1}$ is 0. Therefore, the statement is true for $t=1$ . ", "page_idx": 15}, {"type": "text", "text": "Assume it is true for integers $1,2,\\dots,t-1,t\\geq2$ , then we turn our attention to $t$ . If $k_{i_{t}}=1$ , it implies that there does not exist a $p<t$ such that retained $b_{i_{p}}$ can suppress $b_{i_{t}}$ , i.e., $\\mathrm{IOU}(b_{i_{t}},b_{i_{p}})\\leq$ $N_{t}\\vee k_{i_{p}}=0$ . Therefore, if there is an arc from $i_{p}$ to $i_{t}$ , then $\\delta(i_{p})\\,=\\,k_{i_{p}}\\,=\\,0$ , which indicates $\\delta(i_{t})=1$ . Similarly, we can prove that $\\delta(i_{t})=0$ if $k_{i_{t}}=0$ . ", "page_idx": 15}, {"type": "text", "text": "By mathematical induction, the statement above is true for $1,2,\\ldots,n$ . ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We denote $|\\beta|$ by $n$ , we prove Theorem 2 by mathematical induction. ", "page_idx": 15}, {"type": "text", "text": "For $n=1$ , the statement is true since there is only one node in both binary trees. ", "page_idx": 15}, {"type": "text", "text": "Suppose that the statement holds for all sets with size $1,2,\\ldots,t$ . Consider any set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ with $n=(t+1)$ elements. ", "page_idx": 15}, {"type": "text", "text": "The root of $Q T(B)$ corresponds $b_{v}$ with maximum $s_{v}$ in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ $\\mathcal{B}\\;(Q T(\\varnothing)\\,=\\,\\varnothing)$ ). And the root of the Cartesian tree corresponds the maximum element of the sequence $B$ , which implies it\u2019s also $b_{v}$ indexed by $m$ , i.e., $i_{m}=v$ . ", "page_idx": 15}, {"type": "text", "text": "According to Definition 2, the left child of $Q T(B)$ \u2019s root is $B_{l}$ \u2019s root which satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\nc_{u}\\preceq c\\ c_{v}\\ \\mathrm{for\\all}\\ b_{u}\\in B_{l},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where each $c_{u}$ is the centroid of the corresponding box $b_{u}$ , and $c_{v}$ is the centroid of $b_{v}$ . ", "page_idx": 15}, {"type": "text", "text": "Notice that $B$ is sorted in ascending order of centroids according to the order $\\preceq c$ , which demonstrates that the elements in the set $B_{l}$ are the same to that in the sequence $B[1:m\\!-\\!1]$ . Formally, $|B_{l}|=m\\!-\\!1$ and $\\forall b_{v}\\in B_{l}$ , there exists one and only one $x\\in\\{1,2,\\ldots,m-1\\}$ such that $v=i_{x}$ , i.e., $b_{v}=b_{i_{x}}$ . ", "page_idx": 15}, {"type": "text", "text": "According to the hypothesis, $Q T(B_{l})$ is a Cartesian tree for $B[1:m-1]$ . Similarly, we can prove $Q T(B_{r})$ is a Cartesian tree for $B[m+1:t+1]$ . ", "page_idx": 15}, {"type": "text", "text": "Hence, the statement is true for $n=(t+1)$ , which completes the proof. ", "page_idx": 15}, {"type": "text", "text": "B.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We can take $b$ as the frame of reference. As illustrated in Figure 5, the blue box is $b$ , and the green one is $b^{*}$ which is respectively positioned to the left (Figure 5(a)) or right (Figure $5({\\mathsf{b}})$ ) of the ", "page_idx": 15}, {"type": "image", "img_path": "0lau89u4oE/tmp/49401198bcd365258c519b190de037a8ad3e6eacc5e056acbf1ed06d1e7c4ddb.jpg", "img_caption": ["Figure 5: Four positions of $b^{*}$ relative to $b$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "vertical dashed line, or above (Figure 5(c)) or below (Figure 5(d)) the horizontal dashed line. The intersections are filled with red north east lines. Hence, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{IOU}(b^{*},b)=\\mathrm{IOU}(b,b^{*})}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\mathrm{Area}(\\mathrm{red})}{\\mathrm{Union}(b,b^{*})}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{1/2\\mathrm{Area}(b)}{\\mathrm{Area}(b)}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.5 Proof of Theorem 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We prove Theorem 4 by demonstrating the inequalities in the following two lemmas. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Given positive real numbers $\\theta$ and $\\beta_{z}$ , for any $x_{1},x_{2},y_{1},y_{2}\\in\\mathbb{R},$ , the following inequality holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta(x_{2}-x_{1})+\\beta(y_{2}-y_{1})\\geq(\\theta+\\beta)(\\operatorname*{min}\\{x_{2},y_{2}\\}-\\operatorname*{max}\\{x_{1},y_{1}\\}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 2. Given a positive real number $\\gamma,$ , for any $x_{1},x_{2},y_{1},y_{2}\\in\\mathbb{R}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\gamma+1)x_{2}+(1-\\gamma)x_{1}\\leq y_{1}+y_{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "the following inequality holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{2}+y_{2}-x_{1}-y_{1}\\geq(2+\\gamma)(\\operatorname*{min}\\{x_{2},y_{2}\\}-\\operatorname*{max}\\{x_{1},y_{1}\\}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Notice that $\\operatorname*{min}\\{x_{2},y_{2}\\}\\leq x_{2}$ and $\\operatorname*{min}\\{x_{2},y_{2}\\}\\leq y_{2}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{x_{2},y_{2}\\}\\leq\\frac{\\theta}{\\theta+\\beta}x_{2}+\\frac{\\beta}{\\theta+\\beta}y_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $\\operatorname*{max}\\{x_{1},y_{1}\\}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{x_{1},y_{1}\\}\\geq\\frac{\\theta}{\\theta+\\beta}x_{1}+\\frac{\\beta}{\\theta+\\beta}y_{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R H S=(\\theta+\\beta)(\\operatorname*{min}\\{x_{2},y_{2}\\}-\\operatorname*{max}\\{x_{1},y_{1}\\})}\\\\ &{\\quad\\quad\\leq(\\theta+\\beta)(\\frac{\\theta}{\\theta+\\beta}x_{2}+\\frac{\\beta}{\\theta+\\beta}y_{2}-\\frac{\\theta}{\\theta+\\beta}x_{1}-\\frac{\\beta}{\\theta+\\beta}y_{1})}\\\\ &{\\quad\\quad=(\\theta+\\beta)(\\frac{\\theta}{\\theta+\\beta}(x_{2}-x_{1})+\\frac{\\beta}{\\theta+\\beta}(y_{2}-y_{1}))}\\\\ &{\\quad\\quad=\\theta(x_{2}-x_{1})+\\beta(y_{2}-y_{1})}\\\\ &{\\quad\\quad=L H S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2. Similar to the approach used in the proof above, for $\\operatorname*{max}\\{x_{1},y_{1}\\}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{x_{1},y_{1}\\}\\geq\\frac{\\gamma}{2+\\gamma}x_{1}+\\frac{2}{2+\\gamma}y_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that $\\operatorname*{min}\\{x_{2},y_{2}\\}\\leq x_{2}$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L H S=x_{2}-x_{1}+(y_{1}+y_{2})-2y_{1}}\\\\ &{\\qquad\\geq x_{2}-x_{1}+(\\gamma+1)x_{2}+(1-\\gamma)x_{1}-2y_{1}}\\\\ &{\\qquad=(2+\\gamma)x_{2}-\\gamma x_{1}-2y_{1}}\\\\ &{\\qquad=(2+\\gamma)(x_{2}-\\frac{\\gamma}{2+\\gamma}x_{1}-\\frac{2}{2+\\gamma}y_{1})}\\\\ &{\\qquad\\geq(2+\\gamma)(\\operatorname*{min}\\{x_{2},y_{2}\\}-\\operatorname*{max}\\{x_{1},y_{1}\\})}\\\\ &{\\qquad=R H S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We next use these two inequalities to prove Theorem 4. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4. For convenience, we denote a bounding box $b$ by a 4-tuple $(x_{l t}^{(b)},x_{r b}^{(b)},y_{l t}^{(b)},y_{r b}^{(b)})$ , where $(x_{l t}^{(b)},y_{l t}^{(b)})$ and $(x_{r b}^{(b)},y_{r b}^{(b)})$ represent the left-top and right-bottom corners of $b$ , respectively. This says ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{x_{l t}^{(b)}\\leq x_{r b}^{(b)},\\right.}\\\\ {y_{l t}^{(b)}\\leq y_{r b}^{(b)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given bounding boxes $b^{*}$ and $b$ , their intersection can be calculated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Inter}(b^{*},b)=\\operatorname*{max}\\{0,\\operatorname*{min}\\{x_{r b}^{(b^{*})},x_{r b}^{(b)}\\}-\\operatorname*{max}\\{x_{l t}^{(b^{*})},x_{l t}^{(b)}\\}\\}}\\\\ {\\times\\operatorname*{max}\\{0,\\operatorname*{min}\\{y_{r b}^{(b^{*})},y_{r b}^{(b)}\\}-\\operatorname*{max}\\{y_{l t}^{(b^{*})},y_{l t}^{(b)}\\}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, b\u2217is represented by (xl(tb ), x(rbb ), yl(tb ), yr(bb )) and $b$ by $(x_{l t}^{(b)},x_{r b}^{(b)},y_{l t}^{(b)},y_{r b}^{(b)}).$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{I_{x}=\\operatorname*{min}\\{x_{r b}^{(b^{*})},x_{r b}^{(b)}\\}-\\operatorname*{max}\\{x_{l t}^{(b^{*})},x_{l t}^{(b)}\\},}\\\\ {I_{y}=\\operatorname*{min}\\{y_{r b}^{(b^{*})},y_{r b}^{(b)}\\}-\\operatorname*{max}\\{y_{l t}^{(b^{*})},y_{l t}^{(b)}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We observe that if $I_{x}\\leq0$ or $I_{y}\\leq0$ , then Inte $\\mathsf{r}(b^{*},b)=0$ , resulting in $\\mathrm{IOU}=0$ . This is a trivial case, and Theorem 4 holds. Therefore, we only need to consider the case where $I_{x}>0$ and $I_{y}>0$ . In this case, ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathrm{Inter}}(b^{*},b)=I_{x}I_{y}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then the union can be expressed as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Union}(b^{*},b)=\\mathrm{Area}(b^{*})+\\mathrm{Area}(b)-\\mathrm{Inter}(b^{*},b)}\\\\ &{\\qquad\\qquad\\qquad=L_{x}^{(b^{*})}L_{y}^{(b^{*})}+L_{x}^{(b)}L_{y}^{(b)}-I_{x}I_{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{L_{x}^{(b^{*})}=x_{r b}^{(b^{*})}-x_{l t}^{(b^{*})};}\\\\ {L_{y}^{(b^{*})}=y_{r b}^{(b^{*})}-y_{l t}^{(b^{*})};}\\\\ {L_{x}^{(b)}=x_{r b}^{(b)}-x_{l t}^{(b)};}\\\\ {L_{y}^{(b)}=y_{r b}^{(b)}-y_{l t}^{(b)},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "represent the width and height of $b^{*}$ and $b$ , respectively. We then have the following inequality holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{L_{x}^{(b^{*})}L_{y}^{(b^{*})}+L_{x}^{(b)}L_{y}^{(b)}}{I_{x}I_{y}}\\geq\\frac{1}{N_{t}}+1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\theta=L_{x}^{(b^{*})}>0,\\beta=L_{x}^{(b)}>0$ , according to Lemma 1 and $I_{y}>0$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{L_{x}^{(b^{*})}L_{y}^{(b^{*})}+L_{x}^{(b)}L_{y}^{(b)}}{I_{y}}\\geq L_{x}^{(b^{*})}+L_{x}^{(b)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, according to the proof of Theorem 3, $\\alpha(b^{*},{^1\\mathord{\\left/{\\vphantom{^10}}\\right.\\kern-\\nulldelimiterspace}N_{t}}-1)$ is respectively positioned to the left or right of the vertical dashed line, or above or below the horizontal dashed line (see Figure 5). Without loss of generality, we can assume that $\\alpha(b^{*},{^1\\mathord{\\left/{\\vphantom{^10}}\\right.\\kern-\\nulldelimiterspace}N_{t}}\\mathrm{~-~}1)$ is to the left of vertical dashed line passing through the centroid of $b$ . This means: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(x_{l t}^{(b)}+x_{r b}^{(b)})}{2}\\geq x_{c}^{(b^{*})}+s\\times|x_{r b}^{(b^{*})}-x_{c}^{(b^{*})}|}\\\\ &{\\phantom{\\frac{(x_{l t}^{(b)}+x_{r b}^{(b)})}{2}}=(x_{l t}^{(b^{*})}+x_{r b}^{(b^{*})})/2+(1/N_{t}-1)\\times(x_{r b}^{(b^{*})}-x_{l t}^{(b^{*})})/2}\\\\ &{\\phantom{\\frac{(x_{l t}^{(b)}+x_{r b}^{(b)})}{2}}=\\frac{\\left(1/N_{t}\\right)x_{r b}^{(b^{*})}+\\left(2-1/N_{t}\\right)x_{l t}^{(b^{*})}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "let $\\gamma$ be $1\\slash N_{t}-1>0$ , according to Lemma 2 and $I_{x}>0$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{L_{x}^{(b^{*})}+L_{x}^{(b)}}{I_{x}}\\ge2+(\\frac{1}{N_{t}}-1)=\\frac{1}{N_{t}}+1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining inequalities (2) and (3), inequality (1) is proven. ", "page_idx": 18}, {"type": "text", "text": "Finally, according to the calculation method of IOU, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{{IOU}}(b^{*},b)=\\frac{\\mathrm{{Inter}}(b^{*},b)}{{\\mathrm{Union}}(b^{*},b)}}&{}\\\\ &{=\\frac{I_{x}I_{y}}{L_{x}^{(b^{*})}L_{y}^{(b^{*})}+L_{x}^{(b)}L_{y}^{(b)}-I_{x}I_{y}}}\\\\ &{=\\frac{1}{\\frac{L_{x}^{(b^{*})}L_{y}^{(b^{*})}+L_{x}^{(b)}L_{y}^{(b)}}{L_{x}I_{y}}-1}}\\\\ &{\\le\\frac{1}{(\\frac{1}{N_{t}}+1)-1}}\\\\ &{=N_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.6 Proof of Proposition 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Since a point can be considered as an interval of length 0, we can construct the set $\\mathcal{M}^{\\prime}$ in $\\mathcal{O}(n)$ time: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{M}}^{\\prime}=\\{[m_{i},m_{i}]\\mid m_{i}\\in{\\mathcal{M}}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose there is an algorithm $A$ that can solve problem $X$ . This means: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{X}=A(\\mathcal{T},[l,r]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we can spend $O(1)$ time to call $A$ once, using $\\mathcal{M}^{\\prime}$ and $[l,r]$ as its inputs. Therefore: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{Y}=A(\\mathcal{M}^{\\prime},[l,r]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, problem $Y$ can be reduced to $X$ in polynomial time, i.e., $Y\\leq_{P}X$ . ", "page_idx": 18}, {"type": "text", "text": "C Discussion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From a graph theory perspective, we revisited the NMS algorithm and discovered statistical properties of the weakly connected components in graph $\\mathcal{G}$ : they are numerous overall but small in size locally. Leveraging these two characteristics, we proposed QSI-NMS and BOE-NMS. We then validated the efficiency and accuracy of QSI-NMS and BOE-NMS on NMS-Bench, showing significant improvements over the original NMS, parallelized Fast NMS, and Cluster-NMS. However, there are several areas for improvement in our work. ", "page_idx": 19}, {"type": "text", "text": "First, our NMS algorithms can be combined with other accuracy-enhancing NMS methods, such as Soft-NMS [3], to address the negligible accuracy loss introduced by QSI-NMS and eQSI-NMS. Since our algorithms serve as general frameworks, allowing other methods to be implemented by modifying the dynamic programming equation. Second, our algorithms can be further parallelized to improve efficiency. QSI-NMS uses a divide-and-conquer recursive strategy, enabling parallel processing of different subproblems, while BOE-NMS can adopt the approach of Fast NMS, where each box computes local IOUs in parallel. Third, the distribution characteristics of bounding boxes can be further studied to obtain more detailed information about graph $\\mathcal{G}$ , aiding in the analysis of accuracy loss in Fast NMS, QSI-NMS, and eQSI-NMS. These can be explored in future work. ", "page_idx": 19}, {"type": "text", "text": "In the latter part of this section, we delve deeper into our work. In Subsection C.1, we conduct a thorough graph theory analysis of Fast NMS and Cluster-NMS; in Subsection C.2, we compare our methods with MaxpoolNMS, ASAP-NMS, and PSRR-MaxpoolNMS; in Subsection C.3, we explain and analyze the reasons behind the slight mAP drop of QSI-NMS; in Subsection C.4, we explore some implementation details of QSI-NMS; and in Subsection C.5, we discuss the advantages of the BOE-NMS implementation. ", "page_idx": 19}, {"type": "text", "text": "C.1 Discussion of Fast NMS and Cluster-NMS ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Discussion of Fast NMS In Fast NMS [4], the computation of IOU is parallelized, and when the IOU exceeds a threshold $N_{t}$ , the bounding box with the higher confidence score always suppresses the one with the lower confidence score. This allows the results for all bounding boxes to be computed in parallel without depending on the results of previous boxes. Formally, in Fast NMS, the set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is first sorted into $B$ by confidence scores in descending order, and then an $n\\times n$ matrix $\\mathbf{Y}$ is defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\ny_{i,j}=\\left\\{{\\mathrm{IOU}}(b_{i},b_{j})\\quad{\\mathrm{if~}}i<j;\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For a bounding box $b_{j}$ , if there exists a bounding box $b_{i}$ with higher confidence $(i<j)$ such that $y_{i,j}>N_{t}$ , then $b_{j}$ is not retained; otherwise, it is retained. Since $N_{t}$ effectively classifies all IOUs into two categories, for ease of analysis, we define a matrix $\\mathbf{X}$ where: ", "page_idx": 19}, {"type": "equation", "text": "$$\nx_{i,j}=\\left\\{1\\ \\ \\ {\\mathrm{if}}\\ i<j\\ {\\mathrm{and}}\\ y_{i,j}>N_{t};\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nk_{j}=\\neg\\left(\\bigvee_{i<j}{x_{i,j}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can observe that $\\mathbf{X}$ is the adjacency matrix of $\\mathcal{G}$ . Fast NMS can also obtain the same result using dynamic programming, but the dynamic programming equation is as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta(u)=\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if~}d^{-}(u)>0;}\\\\ {1}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It is evident that Fast NMS suppresses more bounding boxes compared to original NMS, which explains the loss in mAP observed with Fast NMS. Additionally, this demonstrates that Fast NMS can achieve its results without the need for topological sorting, explaining why Fast NMS is parallelizable. However, the actual implementation of Fast NMS has a time complexity of $\\Theta(n^{2})$ , which is, in fact, greater than that of original NMS. In original NMS, suppressed boxes are not used in subsequent IOU calculations, whereas in Fast NMS, every box calculates IOUs with all preceding boxes. This highlights the heavy reliance of Fast NMS on efficient parallel computing. ", "page_idx": 19}, {"type": "text", "text": "Discussion of Cluster-NMS In Cluster-NMS [46], the issue of accuracy loss is addressed by iterating Fast NMS multiple times. We use a binary vector $\\pmb{r}$ to represent the result of applying Fast NMS to $\\mathbf{X}$ , denoted as $\\bar{\\mathbf{\\alpha}}=F(\\mathbf{X})$ , where: ", "page_idx": 20}, {"type": "equation", "text": "$$\nr_{j}=\\neg\\left(\\bigvee_{i<j}x_{i,j}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The iterative process of Cluster-NMS is as follows: ", "page_idx": 20}, {"type": "text", "text": "1. $\\mathbf{X}^{(0)}=\\mathbf{X}$ ;   \n2. $\\pmb{r}^{(t)}=F(\\mathbf{X}^{(t)})$ , for $t\\geq0$ ;   \n3. $\\mathbf{X}^{(t+1)}=\\mathrm{diag}(\\pmb{r}^{(t)})\\times\\mathbf{X}$ ;   \n4. Iterate until $\\mathbf{\\boldsymbol{r}}^{(t)}$ converges to $r^{*}$ , at which point $\\|\\boldsymbol{r}^{(t)}-\\boldsymbol{r}^{*}\\|=0$ . ", "page_idx": 20}, {"type": "text", "text": "In step 3, the iterative method for $\\mathbf{X}^{(t+1)}$ involves left-multiplying $\\mathbf{X}$ by a diagonal 01 matrix, meaning rows corresponding to 1 are retained and those corresponding to 0 are discarded. Formally expressed as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{i,j}^{(t+1)}={\\left\\{\\begin{array}{l l}{x_{i,j}}&{{\\mathrm{if~}}r_{i}^{(t)}=1;}\\\\ {0}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This shows that only boxes not suppressed in the current iteration can cause a suppression effect in the next iteration. Thus, bounding boxes incorrectly suppressed in this round will be re-evaluated with truly retained boxes in the next iteration to determine if they should indeed be suppressed. ", "page_idx": 20}, {"type": "text", "text": "[46] proves that its results are equivalent to original NMS and usually requires only a few iterations. To discuss the number of iterations, [46] defines a cluster: ", "page_idx": 20}, {"type": "text", "text": "Definition 9 (cluster in Cluster-NMS [46]). A subset $\\mathcal{U}=\\{b_{j_{1}},b_{j_{2}},\\dotsc,b_{j_{|\\mathcal{U}|}}\\}$ of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is a cluster if and only if for all $b_{j_{t}}\\in\\mathcal{U}$ , there exists $i\\in\\{j_{1},j_{2},\\ldots,j_{|\\mathcal{U}|}\\}\\setminus\\{j_{t}\\}$ such that $I O U(b_{j_{t}},b_{i})>N_{t}$ and for all $b\\in\\boldsymbol{B}\\setminus\\mathcal{U},I O U(b_{j_{t}},b)\\leq N_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "[46] proves that the number of iterations does not exceed the size of the largest cluster. ", "page_idx": 20}, {"type": "text", "text": "Analyzing Cluster-NMS from the perspective of graph $\\mathcal{G}$ , each iteration essentially determines whether all current nodes with an in-degree of 0 should be retained, i.e., the value of $\\delta(\\cdot)$ ; then it traverses all outgoing arcs of some node $v$ with in-degree 0 to decide if a successor node $u$ should be suppressed, updating $\\delta(u)$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta(u)\\gets\\delta(u)\\wedge\\neg\\delta(v).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, all outgoing arcs are deleted, and the next iteration begins. Hence, the essence of ClusterNMS is parallel topological sorting within each WCC. In fact, we can see that the boxes in a cluster correspond to a WCC in $\\mathcal{G}$ . This explains why the number of iterations until convergence does not exceed the size of the largest cluster: in each iteration, at least one node with an in-degree of 0 is expanded and added to the topological sort within each WCC, reducing its size by at least one per iteration. Although Cluster-NMS is correct, the matrix $\\mathbf{X}$ obtained by parallel IOU computation already encodes all the information of $\\mathcal{G}$ , which indicates a single dynamic programming can quickly produce results identical to original NMS. ", "page_idx": 20}, {"type": "text", "text": "C.2 Discussion of MaxpoolNMS, ASAP-NMS, and PSRR-MaxpoolNMS ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Discussion of MaxpoolNMS and ASAP-NMS MaxpoolNMS [5] and ASAP-NMS [38] take into account the information of pre-defined \u201canchors\u201d in the RPN and leverage the locality suppression characteristics of NMS to achieve impressive performance improvements. However, they have the following limitations: ", "page_idx": 20}, {"type": "text", "text": "\u2022 MaxpoolNMS and ASAP-NMS can only be used in the first stage of two-stage detectors. Our methods, however, can be used in any stage of any detector because we address the most general case (see Section 2). ", "page_idx": 20}, {"type": "text", "text": "\u2022 The complexity of MaxpoolNMS is $\\begin{array}{r}{\\mathcal{O}(n_{s}n_{r}\\lfloor\\frac{W}{\\beta}\\rceil\\lfloor\\frac{H}{\\beta}\\rceil\\!+\\!n\\log n)}\\end{array}$ , where $n_{s}$ and $n_{r}$ represent the number of anchor box scales and ratios, respectively. The complexity of ASAP-NMS is $O(n^{2})$ . Neither of these methods are more efficient than eQSI-NMS.   \n\u2022 These methods involve many manually defined hyperparameters and are complex to implement, which limits their generalization across different models and datasets. In contrast, our methods require no additional parameters beyond those in original NMS and are easy to implement.   \n\u2022 These methods are not rigorous and can lead to a certain degree of mAP degradation, whereas BOE-NMS is rigorously proven to cause no mAP loss. ", "page_idx": 21}, {"type": "text", "text": "Discussion of PSRR-MaxpoolNMS PSRR-MaxpoolNMS [43] introduces Relationship Recovery to address the issue of regression box and score map mismatch in MaxpoolNMS, allowing it to be used at any stage of all anchor-based detectors, and claims to achieve a time complexity of $O(n)$ . However, we do not believe that PSRR-MaxpoolNMS outperforms our methods for the following two reasons: ", "page_idx": 21}, {"type": "text", "text": "First, we implement PSRR-MaxpoolNMS and conduct efficiency comparisons. To better illustrate the impact of bounding box count on runtime, we perform experiments on YOLOv5-N, which has the highest number of bounding boxes, as shown in Figure 6(a). Original NMS has the highest time cost due to its quadratic growth. For a clearer comparison between our methods and PSRR-MaxpoolNMS, we exclude original NMS, as shown in Figure 6(b). As the number of boxes increases, PSRRMaxpoolNMS is faster than BOE-NMS and QSI-NMS but consistently slower than eQSI-NMS. ", "page_idx": 21}, {"type": "text", "text": "Second, strictly speaking, the complexity of PSRR-MaxpoolNMS is not ${\\mathcal{O}}(n)$ . PSRR-MaxpoolNMS requires prior knowledge of the input image size and generates confidence score maps related to the size of the image. This aspect is not considered in the complexity analysis (whereas our methods are designed and analyzed independently of the image size). During the Channel Recovery stage of PSRR-MaxpoolNMS, the complexity of computing the nearest distances for channel mapping is $O(n\\times n_{s}\\times n_{r})$ , where $n_{s}$ and $n_{r}$ represent the number of anchor box scales and ratios, respectively. These quantities vary with different datasets and detectors and increase as image size and object count increase. The remaining stages: Spatial Recovery, Pyramid MaxpoolNMS, and Shifted MaxpoolNMS can all be completed in $\\mathcal{O}(n)$ time. Thus, the overall complexity of PSRR-MaxpoolNMS is $\\begin{array}{r}{\\mathcal{O}(n_{s}n_{r}\\lfloor\\frac{W}{\\beta}\\rceil\\lfloor\\frac{H}{\\beta}\\rceil+n_{s}^{'}n_{r}n)}\\end{array}$ . ", "page_idx": 21}, {"type": "image", "img_path": "0lau89u4oE/tmp/e048b50bbf25b6057a3a1ae86af36ff64ed75bdb3970ac2afaa99eb6910fe87b.jpg", "img_caption": ["Figure 6: The line plot of the runtime of different methods as the number of bounding boxes varies in YOLOv5-N. 6(a) The histogram with a bin width of 1000 representing the number of bounding boxes in each interval. The input images are divided into 10 intervals based on the number of bounding boxes: $(0,1000]$ , (1000, 2000], . . . , (9000, 10000]. The line plot is drawn with the average number of boxes per interval as the $\\mathbf{X}_{\\mathrm{~}}$ -coordinate and the average time cost of the NMS algorithms as the y-coordinate. 6(b) The line plot of the runtime of different methods without original NMS. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 Discussion on mAP loss of QSI-NMS ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In QSI-NMS, we use a divide-and-conquer strategy, which means that bounding boxes in different subproblems do not affect each other. In some special cases, QSI-NMS may assign nodes from the same WCC to different subproblems, potentially causing some nodes that should have been suppressed to be retained. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "We provide a case study with results from the YOLOv8-M model on MS COCO 2017. In Figure 7(a), the blue boxes represent the outputs of original NMS/BOE-NMS, while Figure 7(b) shows the outputs of QSI-NMS, with red boxes indicating additional boxes retained by QSI-NMS. It can be seen that QSI-NMS retains four additional boxes. ", "page_idx": 22}, {"type": "text", "text": "For example, consider the box/node numbered 188. The WCC containing this node is shown in Figure 8(a). All other nodes in the WCC would suppress node 188, but when we use $\\preceq_{M}$ to define the partitioning criterion, node 188 ends up in a different subproblem than other nodes, as shown in Figure 8(b). The figure shows a partial structure of the QSI-tree: solid lines indicate parent-child relationships, and dashed lines indicate ancestor-descendant relationships. The red nodes are nodes from the WCC, while the black node 148 is the lowest common ancestor (LCA) of nodes 188 and 201; node 156 is the LCA of nodes 194 and 193. Since each node in this WCC can only be suppressed by its red ancestor nodes, node 188 is not suppressed. However, node 194 is still suppressed because node 201 is its ancestor. ", "page_idx": 22}, {"type": "text", "text": "This example highlights the core of QSI-NMS design: the pivot selection and the partitioning criterion. If we choose these two appropriately, the accuracy loss of QSI-NMS can be negligible. In our algorithm design: pivot selection chooses the most representative nodes (with the highest confidence scores), so node 194 is correctly suppressed by node 201 even after being placed in a different subproblem from node 193. The partitioning criterion aims to assign nodes from the same WCC to the same subproblem as much as possible, which helps reduce cases like node 188 being incorrectly retained. We also discuss other partitioning criteria in Appendix C.4. ", "page_idx": 22}, {"type": "image", "img_path": "0lau89u4oE/tmp/3205b327d88868558ff9265f7b13c0d8b535e1079a86d569fbb99bde6901570b.jpg", "img_caption": ["Figure 7: The output bounding boxes of original NMS (7(a)) and QSI-NMS (7(b)) in YOLOv8-M on the MS COCO 2017 image \u201c000000057027.jpg\u201d. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "0lau89u4oE/tmp/8dec6ad35aeb3e939b888b94faa5d2202ac8892ce7c15d969c690958c6d6154a.jpg", "img_caption": ["Figure 8: WCC in graph $\\mathcal{G}$ contains node 188 (8(a)), along with a partial structure of the QSI-tree (8(b)). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.4 Further Discussion of QSI-NMS and eQSI-NMS ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Algorithm 1 and Algorithm 2 describe QSI-NMS and eQSI-NMS respectively, which we use $C++$ to implement in NMS-Bench. ", "page_idx": 23}, {"type": "text", "text": "In the QSI-NMS and eQSI-NMS, we define the order $\\preceq\\!c$ , where $\\mathcal{C}$ represents the set of centroids of the original boxes. The preorder $\\preceq\\!c$ is defined on $\\mathbb{R}^{2}$ . Different preorders result in slight variations in the final graph $\\tilde{\\mathcal{G}}$ , which in turn cause minor differences in the NMS results. Therefore, we conduct comparative experiments on different orders. ", "page_idx": 23}, {"type": "text", "text": "We explore three classical orders $(\\preceq_{L},\\preceq_{M},\\preceq_{E})$ , defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(x_{1},y_{1}\\right)\\preceq_{L}\\left(x_{2},y_{2}\\right)\\Leftrightarrow\\left(x_{1}<x_{2}\\right)\\lor\\left(x_{1}=x_{2}\\land y_{1}\\leq y_{2}\\right)}\\\\ &{\\left(x_{1},y_{1}\\right)\\preceq_{M}\\left(x_{2},y_{2}\\right)\\Leftrightarrow\\left|x_{1}\\right|+\\left|y_{1}\\right|\\leq\\left|x_{2}\\right|+\\left|y_{2}\\right|}\\\\ &{\\left(x_{1},y_{1}\\right)\\preceq_{E}\\left(x_{2},y_{2}\\right)\\Leftrightarrow\\sqrt{x_{1}^{2}+y_{1}^{2}}\\leq\\sqrt{x_{2}^{2}+y_{2}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We conduct tests on the MS COCO 2017 using different weights of YOLOv8, with the results shown in Table 4. We find orders $\\preceq_{M}$ and $\\preceq_{E}$ outperform $\\preceq\\!L$ . ", "page_idx": 23}, {"type": "table", "img_path": "0lau89u4oE/tmp/40606499788d6690454a2d0a9e078d5bcc71af42b5391230271bbe117ae0748c.jpg", "table_caption": ["Table 4: AP 0.5:0.95 $(\\%)$ of QSI-NMS and eQSI-NMS under Different Orders on MS COCO 2017 "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "In the case where weakly connected components are independent of each other, $\\preceq_{M}$ and $\\preceq_{E}$ can better maintain the neighborhood consistency between the boxes. In contrast, $\\preceq\\!L$ may order the boxes of different weakly connected components closer together, thereby disrupting the hierarchical nature of QSI-NMS and resulting in more accuracy loss. Therefore, when selecting the order, it is necessary to consider whether the construction of the order can retain the positional information of the original boxes as much as possible. ", "page_idx": 23}, {"type": "text", "text": "C.5 Further Discussion of BOE-NMS ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "An intuitive conclusion is that if two bounding boxes do not overlap at all, their IOU is necessarily 0.   \nThis is a weaker conclusion than Theorems 3 and 4, but it is actually more challenging to implement. ", "page_idx": 23}, {"type": "text", "text": "Without loss of generality, we consider the one-dimensional case. Given a set of $n$ intervals $\\mathcal{Z}=$ $\\{[l_{1},r_{1}],[l_{2},r_{2}],\\bar{\\dots}.\\cdot.\\,,[l_{n},\\bar{r}_{n}]\\}$ , where $l_{i}\\leq r_{i}$ for $i=1,2,\\dots,n$ . ", "page_idx": 23}, {"type": "text", "text": "We define the following two problems: ", "page_idx": 23}, {"type": "text", "text": "Definition 10 (Problem $X$ ). Given $\\mathcal{T}$ and an arbitrary interval $[l,r]$ where $l\\leq r_{;}$ , find all intervals in $\\mathcal{T}$ that intersect with $[l,r]$ , i.e., determine the set $\\mathcal{Q}_{X}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{X}=\\{i\\mid1\\leq i\\leq n\\land[l,r]\\cap[l_{i},r_{i}]\\neq\\emptyset\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Definition 11 (Problem $Y$ ). Given $\\mathcal{T}$ and an arbitrary interval $[l,r]$ where $l\\leq r_{i}$ , find all intervals in $\\mathcal{T}$ whose midpoints lie within $[l,r]$ , i.e., determine the set $\\mathcal{Q}_{Y}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{Y}=\\{i\\mid1\\leq i\\leq n\\land l\\leq\\frac{l_{i}+r_{i}}{2}\\leq r\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In problem $X$ , we need to find all intervals in $\\mathcal{T}$ that intersect with the current interval, while in problem $Y$ , we need to find all intervals whose midpoints lie within the current interval. In Problem $Y$ , we only need to consider the set of midpoints, which can be represented as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{M}}=\\{m_{i}\\ |\\ m_{i}={\\frac{l_{i}+r_{i}}{2}},\\forall i=1,2,\\ldots,n\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For problem $X$ , if $i\\in\\mathcal{Q}_{X}$ , meaning that interval $[l_{i},r_{i}]$ intersects with $[l,r]$ , then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nl_{i}\\leq r\\wedge r_{i}\\geq l.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For problem $Y$ , if $i\\in\\mathcal{Q}_{Y}$ , then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nl\\leq m_{i}\\leq r.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we find that problem $X$ is more complex than problem $Y$ because, in problem $X$ , we need to maintain the partial order of both endpoints, whereas in Problem $Y$ , we only need to maintain the partial order of the midpoints. Formally, we have the following proposition: ", "page_idx": 24}, {"type": "text", "text": "Proposition 3. $Y\\leq_{P}X$ . ", "page_idx": 24}, {"type": "text", "text": "We prove Proposition 3 in Appendix B.6. Proposition 3 indicates that problem $Y$ can be reduced to problem $X$ . Therefore, $X$ is at least as hard as problem $Y$ . Through the above analysis, we can see that although this intuitive conclusion seems straightforward, it is a more difficult problem than determining whether a point lies within a bounding box. Actually, problem $X$ can essentially be equivalent to a 2D plane point filtering problem. Some algorithms can solve this problem by maintaining data structures with large constants, such as persistent segment trees. However, these methods are complex and offer limited optimization. For BOE-NMS, the comparison between points and segments is quite special. We can use sorting and preprocessing of the point set, and then use binary search based on monotonicity to determine the point set corresponding to the query interval. If the size of the point set corresponding to the query interval is $k$ , it requires ${\\mathcal{O}}(k+\\log(n))$ time complexity to obtain the result, avoiding redundant traversal of invalid points. In the other word, our proposed method BOE-NMS more profoundly exploits the properties of weakly connected components. ", "page_idx": 24}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 More Information about NMS-Bench ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 5 and Table 6 respectively present the number of original bounding boxes after inferences of different models on the MS COCO 2017 and Open Images V7 datasets. A larger number of bounding boxes indicates weaker filtering capabilities of the model, leading to longer post-processing times required for NMS. ", "page_idx": 25}, {"type": "table", "img_path": "0lau89u4oE/tmp/032e407702f6b2f392589117f013c407a3c2ac10a4f95b5667bc6227d047fc50.jpg", "table_caption": ["Table 5: Number of Bounding Boxes on MS COCO 2017 "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "0lau89u4oE/tmp/b230088c173cfb8c2900c518e300ead37fcd843306d67d19b95073123f3885d8.jpg", "table_caption": ["Table 6: Number of Bounding Boxes on Open Images V7 "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.2 Experimental Environment and Settings ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our experimental environment is shown as the Table 7. ", "page_idx": 25}, {"type": "table", "img_path": "0lau89u4oE/tmp/33070fd4e75d014efcd9fcbb9601fd5520d1d363a8172c390db601436829f281.jpg", "table_caption": ["Table 7: Experimental Environment "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "For the hyperparameter settings, we set the NMS threshold $N_{t}$ to 0.7 in our experiments. ", "page_idx": 25}, {"type": "text", "text": "Experiments in Torchvision Library To compare with the CUDA NMS from torchvision [28], we implement our methods as $C++$ operators under the torchvision library. We then fairly replace the different NMS operator modules for testing. We test on the MS COCO 2017 using different weights of YOLOv8. The experimental setup is the same as previously described, and we set bench size as 20. The experimental results are shown in the Table 8. This demonstrates that our methods provide performance improvements even when compared to highly optimized parallel implementations. ", "page_idx": 26}, {"type": "table", "img_path": "0lau89u4oE/tmp/4209ec4682e6c63925a1caf465bfac90f00901800f4677e1ab2616e418ccdf9b.jpg", "table_caption": ["Table 8: NMS Methods Performance under Torchvision Implementation "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Statistics of IOU Calculations During the NMS algorithm process, the computational cost of numerous IOU calculations is a performance bottleneck. We compare the number of IOU calculations between our methods and the original NMS. Figure 9 shows the relationship between the number of boxes and the number of IOU calculations for different methods. It can be observed that our methods significantly reduce the number of IOU calculations compared to original NMS, demonstrating the superiority. ", "page_idx": 26}, {"type": "image", "img_path": "0lau89u4oE/tmp/187877303f8cb17d7545f21da26dad4458780908576caaee705d73de3e57a845.jpg", "img_caption": ["Figure 9: Stackplots of IOU calculations for different methods. 9(a) shows the results of YOLOv8-N on MS COCO 2017, while 9(b) shows the results of Faster R-CNN X101-FPN on MS COCO 2017. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Results of Instance Segmentation Tasks We also evaluate our methods on instance segmentation tasks using Mask R-CNN [15] and YOLOv8, where they demonstrate significant superiority over other methods. Please refer to Table 9 for details. ", "page_idx": 27}, {"type": "table", "img_path": "0lau89u4oE/tmp/fa85cadbba8f3df283a54e6a296f755bdf4d7f81c361236e1b2fe1fa53f600c0.jpg", "table_caption": ["Table 9: NMS Methods on Instance Segmentation Tasks "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Pseudo-Codes ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "0lau89u4oE/tmp/86b2cf2fb4528b74bc2f1e39f364267bf6599138c6f5b83950bf9cd315cd154c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Algorithm 2: eQSI-NMS ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Input : $\\mathcal{B}=b_{1},\\cdot\\cdot\\cdot\\,,b_{n},\\mathcal{C}=c_{1},\\cdot\\cdot\\cdot\\,,c_{n},S=s_{1},\\cdot\\cdot\\cdot\\,,s_{n},i_{n},N_{t}$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the list of initial detection boxes, $\\mathcal{C}$ contains the centroids of the boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , $\\boldsymbol{S}$   \ncontains corresponding detection scores, $N_{t}$ is the NMS threshold.   \nOutput : $\\mathcal{D}$ : boxes to be retained.   \nProcedure Solve $(I)$ Stack $\\mathcal{L}^{b}\\gets[]$ ; $\\mathcal{L}^{s}\\leftarrow[]$ ; for $m=1,2,\\ldots,n$ do $b^{*}\\leftarrow b_{I_{m}}$ ; while $\\mathcal{L}^{s}$ is not empty $\\wedge\\,\\mathcal{L}^{b}$ is not empty do if $\\mathrm{TOP}({\\mathcal{L}}^{s})<s_{m}$ then if $\\operatorname{IOU}(b^{*},\\operatorname{TOP}({\\mathcal{L}}^{b}))>N_{t}$ then \u2014 $\\delta(\\mathrm{TOP}(\\mathcal{L}^{b}))\\gets\\mathrm{Fa}$ lse; end $\\operatorname{POP}(\\mathcal{L}^{b});\\operatorname{POP}(\\mathcal{L}^{s});$ ; end else Break; end end $\\mathrm{PUSH}(\\mathcal{L}^{b},b^{*});\\mathrm{PUSH}(\\mathcal{L}^{s},s_{m});$ end   \nbegin $D\\leftarrow\\{\\}\\,;\\delta\\leftarrow\\{{\\mathrm{True}}\\}^{n}$ ; $C\\gets$ the sorted $\\mathcal{C}$ in ascending order according to $\\preceq\\!c$ ; $I\\gets(i_{1},i_{2},\\ldots,i_{n})$ where $C=(c_{i_{1}},c_{i_{2}},\\ldots,c_{i_{n}})$ ; $\\mathtt{S o l v e}\\left(I\\right)$ ; $I\\leftarrow\\mathrm{reverse}(I)$ ; Solve $(I)$ ; for $b\\in\\mathfrak{B}$ do if $\\delta(b)$ then $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup b$ ; end end return D   \nend   \nAlgorithm 3: BOE-NMS   \nInput :B = b1, \u00b7 \u00b7 \u00b7 , bn, M = m $\\i_{1},\\.\\cdot\\cdot\\ ,m_{n},S=s_{1},\\cdot\\cdot\\cdot\\ ,s_{n},\\mathbb{Z}=i_{1},\\cdot\\cdot\\cdot\\ ,i_{n},N_{t}$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the list of initial detection boxes, $\\mathcal{M}$ contains the $\\mathbf{X}$ -coordinates of the centroids of   \nthe boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , $\\boldsymbol{S}$ contains corresponding detection scores, $\\mathcal{T}$ contains the ranks of all boxes in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ ,   \nwhich is sorted by $\\mathbf{X}$ -coordinate of the centroids of the boxes in ascending order, $N_{t}$ is the NMS   \nthreshold.   \nOutput : $\\mathcal{D}$ : boxes to be retained.   \nbegin $\\mathcal{D}\\leftarrow\\{\\}$ ; while ${\\dot{\\boldsymbol{B}}}\\neq\\varnothing$ do $m\\leftarrow\\arg\\operatorname*{max}S;$ $b^{*}\\leftarrow b_{m}$ ; $D\\gets\\mathcal{D}\\cup b^{*};\\mathcal{B}\\gets\\mathcal{B}-b^{*};S\\gets S-s_{m};$ ; $x_{l}\\gets$ left $\\mathbf{X}$ -coordinate $\\left(b^{*}\\right)$ ; $x_{r}\\gets$ right $\\mathbf{X}_{\\mathrm{}}$ -coordinate $\\left(b^{*}\\right)$ ; $l\\leftarrow$ lowerbound $(\\mathcal{T},x_{l})$ ; $\\triangleright$ Find the rank of the first item i in $\\mathcal{T}$ , s.t. $m_{i}\\geq x_{l}$ $r\\gets$ upperbound $(\\mathcal{T},x_{r})$ ; $\\triangleright$ Find the rank of the first item i in $\\mathcal{T}$ , s.t. $m_{i}>x_{r}$ I\u2032 \u2190Il\u223cr\u22121 ; for $i\\in\\mathcal{T}^{\\prime}$ do if $\\mathrm{IOU}(b^{*},b_{i})>N_{t}$ then $|\\quad\\mathcal{B}\\leftarrow\\mathcal{B}-b_{i};\\mathcal{S}\\leftarrow\\mathcal{S}-s_{i};$ end end end return D   \nend ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please refer to Section 1, Paragraph 3, 4, and 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Please refer to Appendix C, Paragraph 2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Please refer to Appendix A and B. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: For algorithm implementation details, please refer to Section 4 and Appendix E.   \nAnd for experimental details, please refer to Section 5 and Appendix D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have included the code, related documentation, and licenses in the supplementary material. We will open-source our code and data after acceptance. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Please refer to Section 5 and Appendix D. More details can be found in our source code. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Please refer to Table 1, Table 2, Table 8, and Figure 9. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Please refer to Table 7 for the CPU/GPU details. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in this article fully complies with the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our research on NMS is foundational work in the field of object detection, and does not have any societal impact. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not present any such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We cite the relevant models and datasets. Additionally, we provide the corresponding copyrights and licenses in the supplementary material. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide the corresponding instructional documentation and the license in the supplementary material. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]