{"references": [{"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-06-01", "reason": "This paper provides a convergence theory for deep learning using over-parameterization, which is a core concept in the target paper's theoretical analysis."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduces the Neural Tangent Kernel (NTK) theory, which is fundamental to understanding the generalization properties of neural networks and is directly relevant to the target paper's discussion of kernel methods."}, {"fullname_first_author": "Sanjeev Arora", "paper_title": "Implicit regularization in deep matrix factorization", "publication_date": "2019-12-01", "reason": "This paper discusses implicit regularization in deep learning, which is connected to the concept of over-parameterization and is relevant to the target paper's analysis of over-parameterized gradient descent."}, {"fullname_first_author": "Peter D. Hoff", "paper_title": "Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization", "publication_date": "2017-11-01", "reason": "This paper discusses Lasso, fractional norm, and structured sparse estimation using Hadamard product parametrization, which provides a related theoretical framework for the target paper's discussion of over-parameterization methods."}, {"fullname_first_author": "Tomas Va\u0161kevi\u010dius", "paper_title": "Implicit regularization for optimal sparse recovery", "publication_date": "2019-09-01", "reason": "This paper studies implicit regularization for sparse recovery, which is directly related to the target paper's discussion of the benefits of over-parameterization in improving generalization and adaptivity"}]}