[{"type": "text", "text": "A Sober Look at the Robustness of CLIPs to Spurious Features ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qizhou Wang1\u2217 Yong $\\mathbf{Lin}^{2*}$ Yongqiang Chen3\u2217 Ludwig Schmidt4 Bo Han1\u2020 Tong Zhang5 ", "page_idx": 0}, {"type": "text", "text": "1TMLR Group, Department of Computer Science, Hong Kong Baptist University 2The Hong Kong University of Science and Technology 3The Chinese University of Hong Kong 4University of Washington 5University of Illinois Urbana-Champaign ", "page_idx": 0}, {"type": "text", "text": "https://counteranimal.github.io/ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large vision language models, such as CLIP, demonstrate impressive robustness to spurious features than single-modal models trained on ImageNet. However, existing test datasets are typically curated based on ImageNet-trained models, which aim to capture the spurious features inherited in ImageNet. Benchmarking CLIP models based on the ImageNet-oriented spurious features may not be sufficient to reflect the extent to which CLIP models are robust to spurious correlations within CLIP training data, e.g., LAION. To this end, we craft a new challenging dataset named CounterAnimal designed to reveal the reliance of CLIP models on realistic spurious features. Specifically, we split animal photos into groups according to the backgrounds, and then identify a pair of groups for each class where a CLIP model shows high-performance drops across the two groups. Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models. We provide theoretical insights that the CLIP objective cannot offer additional robustness. Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data. We find that they still help mitigate the spurious features, providing a promising path for future developments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large vision language models (LVLMs) have demonstrated huge success across a wide range of vision and multi-modal tasks, surpassing conventional ImageNet (-trained) models by a remarkably large margin [1]. LVLMs are typically trained with or based on Contrastive Language Image Pre-training (CLIP) [2] on an unprecedented scale of real-world vision and language data such as LAION [3], which are significantly larger than ImageNet. The huge success of CLIP has presented a paradigm shift for modern vision and vision-language models to conduct the pre-training from ImageNet benchmarks to web-scale multi-modal datasets [4]. ", "page_idx": 0}, {"type": "text", "text": "A key signature of CLIP models is the impressive robustness against various ImageNet-oriented distribution shifts [2], which is shown to be prohibitive to ImageNet models [5]. The performance boosts over ImageNet models seem to suggest that CLIP resolves distribution shifts, thereby sparking a rich discussion about its rationale [6, 7, 8, 9, 10]. However, the elephant in the room is that adopted testsets (i.e., ImageNet variants) to evaluate the robustness of CLIPs are primarily designed for ImageNet-based models [5, 11]. These datasets may not correctly reflect the exact robustness of CLIP, given that CLIP models are trained on a large amount of data that may include, and possibly extend beyond those ImageNet variants during pre-training [10]. In this paper, we investigate the robustness of CLIP to distribution shifts caused by the presence of spurious features. These features are highly correlated with labels, but this correlation may break down under distributional shifts [12, 13, 14, 15, 16, 17, 18, 19, 20]. We raise a challenging research question in the following: ", "page_idx": 0}, {"type": "image", "img_path": "wWyumwEYV8/tmp/7a0e917d434a47e73ebe472c231a57e6a9ac8b39642e81011a26ccfe48c7a276.jpg", "img_caption": ["Figure 1: We showcase CounterAnimal examples from the class of ice bear, separated into easy and hard groups with different backgrounds (i.e., snow and grass). The zero-shot performance of CLIP-LAION400M-ViT-B/32 drops from $97.62\\%$ (easy) to $70.91\\%$ (hard). ", "Photos of ice bear in snow background (easy, accu 97.62) Photos of ice bear in grass background (hard, accu 70.91) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Is there a benchmark that reflects the exact reliance on spurious features of CLIP? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sadly, most of the existing benchmarks [21, 22, 23, 24] are tailored primarily for ImageNet models, which are unsuitable for CLIP. To fill this gap, we introduce a new testset, named CounterAnimal, specifically designed for assessing the robustness of CLIP models against real-world spurious features. Figure 1 presents several examples of CounterAnimal where data are divided into two groups, a) the easy group: animals in commonly appeared backgrounds that the CLIP models make correct predictions, and b) the hard group: animals in less commonly yet still plausible backgrounds, where the CLIP models are likely to misclassify them. Intuitively, the easy part captures some real-world biases that the web-scale data may naturally inherit. Hence, by comparing the performances of the two groups, one can quantify to what extent the model relies on spurious features. ", "page_idx": 1}, {"type": "text", "text": "More specifically, the CounterAnmial dataset is curated based on raw photos collected from iNaturalist3. The construction pipeline consists of 4 steps. a) Data collection: querying iNaturalist with each animal class, where we select some of the animal names from the ImageNet-1K dataset [25]. b) Data curation: manually cleansing low-quality photos that potentially contain ambiguity and corruption. c) Background labeling: manually annotating photos with their respective backgrounds, selected from the label space of the candidate backgrounds. d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot perfor", "page_idx": 1}, {"type": "table", "img_path": "wWyumwEYV8/tmp/de61be6cfb1ed885e9cf11ca4cb8eca8a9d9d6ccabb5782241d395cd255528c3.jpg", "table_caption": ["Table 1: 1 vs. 1000 results of exemplary animal classes within the CounterAnmial dataset for CLIP-LAION400M-ViT-B/32. \u201cbkg\u201d denotes the background label, \u201caccu\u201d $(\\%)$ denotes the zeroshot accuracy, and \u201cdrop\u201d $(\\%)$ denotes the drop in accuracy between easy and hard groups. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "mance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds. The resulting CounterAnimal dataset covers a total of 45 animal classes, and ends up with 7,174 easy photos and 5,926 hard photos, aligning with the standard size as an evaluation dataset, such as [26, 27]. Moreover, CLIP-LAION400M-ViT-B/32 is used as the proxy CLIP model in spurious discovering (cf., Appendix C.5 for the model naming rules). ", "page_idx": 1}, {"type": "text", "text": "We evaluate the CLIP models on our CounterAnmial with various backbones, e.g., ViT [28], along with different pre-train datasets, e.g., LAION [3]. We also consider more advanced LVLMs like MiniGPT4 [29] and LLaVA [30]. We employ two evaluation setups crafted for different families of models (cf., Appendix C): a) 1 vs. 1000 setup: using the full ImageNet-1K class names as the candidate label space and b) 1 vs. 20 setup: using the top-20 most confusing classes regarding CLIP-LAION400M-ViT-B/32 as the candidate label space. We provide some of results in Table 1 and Figure 2, highlighting the key observations in the following: ", "page_idx": 1}, {"type": "image", "img_path": "wWyumwEYV8/tmp/9bec37ad4e002ebbb78ced08efd023eaad6fe100f4775c55e3335600329e6785.jpg", "img_caption": ["(a) 1 vs. 1000 (label space of ImageNet-1K) "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "wWyumwEYV8/tmp/5a9e18138e4b86ce5e85e9b3413d01a899f9613083f0b35ad3924bcf78b796ee.jpg", "img_caption": ["(b) 1 vs. 20 (20 most confusing labels per class) "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The easy vs. hard performance $(\\%)$ for CLIP, ImageNet models, and more advanced LVLMs, i.e., MiniGPT4 and LLaVA. The marker size indicates the backbone scale and the color shade indicates pre-train data scale. We highlight the CLIP models pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and Data Filtering Networks (CLIP-DFN). We linearly fit the trends for CLIP (CLIP, CLIP-DC, and CLIP-DFN) and ImageNet models to show their effective robustness. We also depict the perfect trend, i.e., $y=x$ , where the models will not learn any bias. ", "page_idx": 2}, {"type": "text", "text": "CounterAnimal captures general spurious correlations within CLIP. As exemplified in Table 1, we observe a significant drop of CLIP-LAION400M-ViT-B/32 in zero-shot accuracy from the easy to hard groups for each class. Furthermore, the observed biases in CLIP-LAION400M-ViT-B/32 also generalize to other CLIP configurations, with non-trivial performance drop from the easy to hard groups across various backbones and pre-train datasets as in Figure 2. It implies that CounterAnimal characterizes some general spuriousness common in large-scale multi-modal datasets. ", "page_idx": 2}, {"type": "text", "text": "ImageNet models are more robust to spurious correlations captured by CounterAnimal. Figure 2 also illustrates the performance changes of ImageNet models (colored in red). Compared with CLIP models (colored in blue), we find that ImageNet models exhibit stronger robustness to spurious correlations captured by CounterAnimal. Our findings contrast with previous studies that assess the ImageNet variants [2], highlighting that CLIP models do not always generalize better than ImageNet models. It underscores the necessity of choosing appropriate benchmarks to comprehensively assess the robustness of different models and training schemes. ", "page_idx": 2}, {"type": "text", "text": "Larger CLIP models are more robust. Shown also in Figure 2, we use the sizes and the color shades of the markers to indicate the scales of backbones and the pre-train datasets, respectively. Overall, larger CLIP backbone models (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance model performance against spurious features. In contrast, increasing the scale of the pre-train dataset (i.e., darker markers) does not yield the same improvement, implying that collecting more data alone cannot rectify much bias, which provides some new understanding in addition to the data-centric perspective [6, 10]. ", "page_idx": 2}, {"type": "text", "text": "CLIP models trained on high-quality data are more robust. We categorize CLIP models into two distinct groups according to the pre-train data quality: a) CLIP-DC using DataComp [4] and CLIP-DFN employing Data Filtering Networks [31], as well as b) those pre-trained on datasets that lack stringent curation, labeled simply as CLIP. The results indicate that CLIP models pre-trained on high-quality datasets demonstrate enhanced robustness in general. It suggests that enhancing data quality remains a promising strategy for mitigating the spurious features. ", "page_idx": 2}, {"type": "text", "text": "The CLIP objective may not offer additional robustness. Complementary to our empirical observations, we also provide theoretical explanations for the reasons why CLIP learns spurious features. We further conduct confirmatory experiments that fine-tune CLIP models onto datasets with synthetic spurious features. The results align with our observations on CounterAnimal that the CLIP objective can not offer additional robustness over standard single-modal supervised training. ", "page_idx": 2}, {"type": "text", "text": "Comparison with previous results. Our work presents a new benchmark to effectively and systematically evaluate the robustness of CLIP models, which complements the literature in understanding the generalizability of CLIP models and LVLMs. More specifically, [32] reports that CLIP models may wrongly align co-occurred objects with their texts. [33] reports similar failure modes for more sophisticated LVLMs such as MiniGPT4 or LLaVA. [34] finds that CLIP misaligned samples will further cause the hallucination of LVLMs. Complementary to these works, our study explicitly characterizes the spurious features captured by CLIP and explains the existence of the reported failure cases. Our study provides interesting empirical and theoretical counterexamples to the previous beliefs for the substantial improvements in robustness for CLIP models, especially for those results observed on ImageNet variants [7, 8, 9, 35]. Based on the newly collected CounterAnimal dataset, we suggest that distribution shifts remain an open problem for CLIP models. Also, we need to be cautious about the test setups when evaluating new models pre-trained on datasets that differ significantly in scales and distributions from traditional ImageNet models. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Comparison with previous benchmarks. There are many other datasets to study distribution shifts, e.g., ImageNet variants [21, 26, 36, 37, 38, 39, 40], DomainBed [41], and Wilds [42]. However, these datasets have biases when assessing the OOD robustness of CLIP models, as they may fail to represent the true OOD scenarios during CLIP training. Moreover, numerous recently released datasets, such as [22, 23, 43, 44, 45], have also explored distribution shifts. However, these studies primarily focus on synthetic distribution shifts, which may not fully represent real-world cases. In fact, it has been shown that previous OOD benchmarks are contained in CLIP training [10], making it hard to ablate ID/OOD cases for data in these benchmarks. Consequently, CLIP models have shown to be more robust than ImageNet models on these contaminated datasets [46]. ", "page_idx": 3}, {"type": "text", "text": "2 Dataset and Evaluation Setups ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To begin with, we describe the basic experimental setups, including the pipelines in constructing CounterAnimal, its key characteristics, as well as the adopted evaluation settings. ", "page_idx": 3}, {"type": "text", "text": "2.1 Construction of CounterAnimal ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce the curation pipeline of our new dataset CounterAnimal, tailored for CLIP to investigate spurious correlations. The pipeline consists of 4 steps as follows: ", "page_idx": 3}, {"type": "text", "text": "Data Collection. We query animal names listed in the ImageNet-1K dataset and collect raw data via the search interface of iNaturalist, a global biodiversity data-sharing platform. We retrieve the latest 300-800 photos per animal class, organizing them based on the queried labels. ", "page_idx": 3}, {"type": "text", "text": "Data Curation. The collected raw samples are susceptible to noise and ambiguities. Therefore, we manually cleanse the low-quality data that fall into any one of the following 4 situations: label noise, feature noise, obscurity, and clarity. Label noise refers to cases where photos do not belong to the queried classes; feature noise refers to cases where some pixels are disrupted or missing; obscurity occurs when photos belong to more than one object class; clarity issues refer to cases where animal objects are largely occluded by the backgrounds or other irrelevant objects. It also includes the cases where animal objects do not occupy the majority of the space in photos. ", "page_idx": 3}, {"type": "text", "text": "Background labeling. We consider a typical form of spurious features where the backgrounds of photos can be biased [47]. To identify such data for CLIP models, we manually label the backgrounds for the curated data. The considered class space of backgrounds is defined as follows: ground, water, earth, sand, snow, grass, human, sky, road, rock, shrub, indoor, tree, and outdoor. Note that the class space of backgrounds as above is not entirely orthogonal due to the inherent ambiguity: Some backgrounds may be ambiguous and some photos may contain more than one background. Nevertheless, we try our best to determine the assigned background labels for each animal class and exclude those photos challenging to be labeled. ", "page_idx": 3}, {"type": "text", "text": "Spurious Discovery. For each class, we quantify the impacts of spurious correlations to CLIP models by comparing the performances on the associated samples across different backgrounds. We take those classes as containing spurious features on which we observe a relatively obvious decrease in accuracy when changing backgrounds. In realization, we adopt the checkpoint of CLIP-LAION400M-ViT-B/32 for evaluation, where the prompt for its text encoder is \u201cA photo of <object label>.\u201d, and the space of <object label> is the ImageNet-1K class names, i.e., we follow an 1 vs. 1000 setup. Then, we consider the classes where the zero-shot accuracy varies by more than $5\\%$ when changing backgrounds as the cases where CLIP model has learned the spurious features. The data with the preserved classes and backgrounds are used to create our final ", "page_idx": 3}, {"type": "image", "img_path": "wWyumwEYV8/tmp/ff276bad7653dfaf4e0cd28fb6b322b12aca75f6f1717d12ed7af8bb3219ceaa.jpg", "img_caption": ["Figure 3: The data layout across various animal classes. The horizontal axis denotes the class IDs and the vertical axis denotes the number of photos for the easy and hard groups, respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "wWyumwEYV8/tmp/d28c45c4cc3a5faeab295477672f79d477d10f32a07a3eacb7aceecb06e21471.jpg", "img_caption": ["Figure 4: The 1 vs. 1000 performance drop $(\\%)$ with CLIP-LAION400M-ViT-B/32. The horizontal axis denotes the class IDs and the vertical axis denotes the percentage points of decline. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "CounterAnimal dataset. Photos with the highest CLIP accuracy are assigned to the easy group, and those with the lowest CLIP accuracy are assigned to the hard group. We further refine the collected data to remove any mistake that the labelers may made during data curation and background labeling. ", "page_idx": 4}, {"type": "text", "text": "Our objective in developing CounterAnimal is to reflect the spurious correlations learned by CLIP. Therefore, we need to employ the CLIP models for dataset curation and thus ensure the construction is effectively biased towards CLIP configurations [26]. In Appendix E, we further show that our data curation pipeline is general and reliable to characterize the spurious features within the considered models. Moreover, our experimental results later in Section 3 will corroborate that the spurious features captured by our CounterAnimal dataset are general across different CLIP setups and may not be so influential for ImageNet benchmarks. These findings will justify that our crafted testset satisfies our primary objective in characterizing the spuriousness for CLIP specifically. ", "page_idx": 4}, {"type": "text", "text": "2.2 Characteristics of CounterAnimal ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We depict the data layout in Figure 3 and visualize the zero-shot gaps for each animal class in Figure 4, where we use CLIP-LAION400M-ViT-B/32 as our referred model. Please refer to the detailed object/background names concerning the easy and hard groups in Appendix B. Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the easy group yet is compromised when coming to the hard group. Accordingly, Figure 4 implies a reliance for the CLIP models on the backgrounds. ", "page_idx": 4}, {"type": "text", "text": "2.3 Evaluation Setups ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We evaluate a series of CLIP models on the CounterAnimal dataset for their zero-shot performance. For each class, we use the pre-defined prompt of \u201cA photo of <object label>.\u201d as in our data collection procedure and the similarity between image and text embeddings in classification. By default, we use the label space of the ImageNet-1K dataset and report the top-1 accuracy, i.e., the 1 vs. 1000 setup. Moreover, when involving more advanced LVLMs, we adopt the 1 vs. 20 setup where we employ the top-20 most confusing classes regarding CLIP-LAION400M-ViT-B/32 as the candidate label space. For re-productivity, we adopt the pre-trained CLIP checkpoints from OpenCLIP [48] and ImageNet model checkpoints from the PyTorch repository. The model naming rules are in Appendix C.5 and the evaluation details are discussed in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "3 Experimental Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our experiments center on the evaluation and the analysis of our CounterAnimal dataset. In Section 3.1, we examine the generality of the captured spurious correlations. In Section 3.2, we explore the potential facets that affect the robustness of CLIP models. In Section 3.3, we extend the evaluation to a broader family of models with different training paradigms. ", "page_idx": 4}, {"type": "text", "text": "3.1 Generality of the Spurious Correlations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 2.1, we discover spurious correlations using CLIP-LAION400M-ViT-B/32 and collect associated data to build the CounterAnimal dataset. A critical problem then arises: Is our dataset a general benchmark to examine spurious correlations of CLIP with other pretrain datasets and backbones? Hence, we need to examine whether the biases in the CounterAnimal dataset can hinder the robustness of other CLIP models, where we consider two situations: a) fixing pre-train datasets while varying backbones and b) varying pretrain datasets while fixing backbones. ", "page_idx": 5}, {"type": "text", "text": "Varying Backbones. We fix the pre-train dataset to be LAION400M and explore two other ", "page_idx": 5}, {"type": "image", "img_path": "wWyumwEYV8/tmp/032b44c7ad14c452a03a5f0beb3171627b4b4edc2876da2fbf6b11c2cc972660.jpg", "img_caption": ["Figure 5: The 1 vs. 1000 results for varying CLIP setups beyond CLIP-LAION400M-ViT-B/32: a) fixing the pre-train dataset to be LAION400M and b) fixing the backbone to be ViT-B/32. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "backbones within the ViT family [28], i.e., ViT-B/16 and ViT-L/14. Their zero-shot results are depicted in Figure 5(a). There remains a drop above 17 percentage points for both the cases of ViT-B/16 and ViT-L/14. It suggests that the CounterAnimal dataset captures some general spurious shifts that are at least present in the pre-train dataset of LAION400M. ", "page_idx": 5}, {"type": "text", "text": "Varying Pre-train Datasets. We fix the backbone to be ViT-B/32 and consider other pre-train datasets. Here, we consider LAION2B and the closed-source dataset used by OpenAI. Their easy and hard results are in Figure 5(b). Here, the spurious features affect the zero-shot robustness of CLIP models trained on both LAION2B and by OpenAI, indicating that our CounterAnimal dataset possesses some realistic shifts that are contained in various CLIP setups. Therefore, we conclude that CounterAnimal captures some general spurious features learned by CLIP models. ", "page_idx": 5}, {"type": "text", "text": "3.2 Scaling up May Relieve Spurious Correlations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We extend our evaluations to a wider range of CLIP models with different scales of parameters and pre-train data. The results are summarized in Table 2 and further depicted in Figure 2(a). Generally speaking, performance drops can be observed across all considered CLIP configurations, indicating that CLIP models in various scales still learn spurious features. More specifically, we investigate the influence of a) parameter scales and b) pre-train data scales in CLIP models on the sensitivity of spurious features. We exclude the backbone of ViT-B/32 and the dataset of LAION400M to avoid biases in data collection. ", "page_idx": 5}, {"type": "text", "text": "Scaling up Pre-train Data. To test the impacts of enlarging scales of pre-train datasets, we consider two CLIP backbones, namely, ViT-B/16 and $\\tt V i T-L/14$ , along with a series of pre-train datasets of increasing sizes. The results are summarized in Figure 6. We observe that scaling up the data scale does not necessarily reduce the performance drop, suggesting that directly enlarging the scale of ", "page_idx": 5}, {"type": "table", "img_path": "wWyumwEYV8/tmp/c83f34c0bf7878b9e41abf62864d227ca60bd47be66422f0a1043385554d63bf.jpg", "table_caption": ["Table 2: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal dataset. The pre-train datasets with high-quality data are marked by \u2217. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "pre-train data alone cannot enhance robustness. One possible explanation is that larger datasets do not imply fewer biases, whereas the CLIP models will still inherit the spurious correlations therein. ", "page_idx": 5}, {"type": "text", "text": "Scaling up CLIP Model Sizes. We also explore the connection between model scales and spurious correlations. In Figure 7, we consider two pre-train datasets, namely, LAION2B and the close-soured ", "page_idx": 5}, {"type": "image", "img_path": "wWyumwEYV8/tmp/210511c4738e20d54574a9de03fbb1dee3bd075a56f01f019e874c5373457e79.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: 1 vs. 1000 results for varying CLIP Figure 7: 1 vs. 1000 results for varying CLIP setups with different pre-train datasets. setups with different backbones. ", "page_idx": 6}, {"type": "image", "img_path": "wWyumwEYV8/tmp/00b1da19ddb3c4d3ad4fc783e1610b2966b3fff4a38e3afbf6db3219a6959bb3.jpg", "img_caption": ["Figure 8: 1 vs. 1000 drops for varying CLIP Figure 9: 1 vs. 1000 drops for varying training setups with filtered and unfiltered pre-train data. setups with CLIP and ImageNet supervision. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "data from OpenAI, along with backbones of increasing scales. We observe a clear trend indicating that larger models exhibit better performance against spurious correlations. It may tell us that larger models possess stronger robustness, making them less prone to the shortcuts of spurious features. ", "page_idx": 6}, {"type": "text", "text": "Data Quality Matters. Moreover, we observe that the results obtained with DataComp- and DFNtrained CLIPs exhibit better performance and smaller drops across backbones, Figure 8 offers their comparisons. We notice that these datasets have been stringently flitered and thus possess high-quality data. It indicates that enhancing data quality is still a promising way to improve OOD generalization. ", "page_idx": 6}, {"type": "text", "text": "Our analysis focuses on absolute performance drop. In Appendix F, we strengthen our conclusions by incorporating the analysis based on effective robustness [5], where our findings still hold. ", "page_idx": 6}, {"type": "text", "text": "3.3 Evaluations for other Learning Paradigms ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We extend our evaluations to broader families of models, including ImageNet-1K supervised models and more advanced LVLMs, such as MiniGPT4 and LLaVA. ", "page_idx": 6}, {"type": "table", "img_path": "wWyumwEYV8/tmp/7b0e16da0c9a7ea4b226ef66b6eb579813bb431034982c70d7d77155dd246d2e.jpg", "table_caption": ["Table 3: The 1 vs. 1000 performance for ImageNet models CounterAnimal. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "ImageNet Models. We first extend our evaluations to include ImageNet models. The main results are summarized in Table 3. Moreover, Figure 9 further illustrates the accuracy drops of various CLIP models, in comparison to ImageNet models. Surprisingly, we find that ImageNet models are more robust to spurious features in CounterAnimal. This finding indicates that our CounterAnimal specifically characterizes the spurious features that are unique to CLIP configurations. Additionally, it indicates that spurious correlations in large-scale multi-modal data are distinct from that of the ImageNet scenarios which are widely used in conventional single-modal supervised learning. It further highlights the importance of our proposed dataset, which is especially suitable to study the spurious correlations for vision-language pre-training. ", "page_idx": 6}, {"type": "text", "text": "Advanced LVLMs. We further evaluate for more advanced LVLMs, which align CLIP visual encoders with advanced large language models like Vi", "page_idx": 6}, {"type": "text", "text": "Table 4: The 1 vs. 20 results of CounterAnimal for advanced LVLMs and several CLIP models.   \nMore results of CLIP models and ImageNet models can be found in Appendix F. ", "page_idx": 7}, {"type": "table", "img_path": "wWyumwEYV8/tmp/b822cfcbae079cf44a495bbd8a89e805362de1623300a1b6649bc1211c9c1b7e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "cuna [49]. To reduce inference costs, our evaluation follows the 1 vs. 20 setup. We summarize their results in Table 4, along with the 1 vs. 20 results for several CLIP models (cf., Appendix F for more results). We further depict the full results in Figure 2(b). As we can see, these advanced LVLMs have lower performance yet smaller drops, but the spurious features in CounterAnimal still impact them. ", "page_idx": 7}, {"type": "text", "text": "4 Understanding Why CLIPs Rely on Spurious Features ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To better understand the observed phenomena in Section 3, we present a theoretical analysis of why the CLIP models rely on spurious features. We begin by establishing the setup for analyzing multi-modal contrastive learning following [9]. ", "page_idx": 7}, {"type": "text", "text": "Definition 1 (Multi-modal Dataset). Consider n image-text pairs $\\{(\\pmb{x}_{I}^{i},\\pmb{x}_{T}^{i})\\}_{i=1}^{n}$ , both image $\\pmb{x}_{I}^{i}$ and text ${\\pmb x}_{T}^{i}$ are generated from the latent factor $z_{i}$ , where $z=[z_{i n v},z_{s p u}]\\in\\mathbb{R}^{2}$ is composed of an invariant feature $z_{i n v}\\sim\\mathcal{N}(\\mu_{i n v}y,\\sigma_{i n v}^{2})$ and a spurious feature $z_{s p u}\\sim\\mathcal N(\\mu_{s p u}a,\\sigma_{s p u}^{2})$ with $\\operatorname*{Pr}(a=y)=p_{s p u}$ otherwise $a=-y$ . $y$ is the label uniformly drawn from $\\{-1,1\\}$ . The training data $\\mathcal{D}^{t r}$ is drawn with $\\begin{array}{r}{\\frac12\\leq p_{s p u}\\leq1}\\end{array}$ and OOD data $\\mathcal{D}^{*}$ is drawn with a $\\begin{array}{r}{p_{s p u}=\\frac{1}{2}\\;}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "We employ two linear encoders: $g_{I}:\\mathbb{R}^{d_{I}}\\rightarrow\\mathbb{R}^{h}$ for the image modality and $g_{T}:\\mathbb{R}^{d_{T}}\\rightarrow\\mathbb{R}^{h}$ for the text modality, implemented as $g_{I}({\\pmb x}_{I})=W_{I}{\\pmb x}_{I}$ and $g_{T}(\\pmb{x}_{T})=W_{T}\\pmb{x}_{T}$ with $W_{I}\\in\\mathbb{R}^{h\\times d_{I}}$ and $W_{T}\\in\\mathbb{R}^{h\\times d_{T}}$ . The encoders are trained through the linearized contrastive loss [9, 50] that mimics the CLIP dynamics: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CLIP}}=\\frac{1}{2n(n-1)}\\sum_{i}\\sum_{j\\neq i}(s_{i j}-s_{i i})+\\frac{1}{2n(n-1)}\\sum_{i}\\sum_{j\\neq i}(s_{j i}-s_{i i})+\\frac{\\rho}{2}||W_{I}^{T}W_{T}||_{F}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $s_{i j}=g_{I}({\\pmb x}_{I}^{i})^{T}g_{T}({\\pmb x}_{T}^{j})$ is the similarity with respect to the $i$ -th image and $j$ -th text representations. Once the CLIP $(g_{I},g_{T})$ has been trained, the performance will be measured in a zero-shot manner by matching the most similar caption with the corresponding object name fliled in, such as $\\mathbf{\\sigma}^{\\leftarrow}\\mathbf{a}$ photo of <object label>\u201d [2]. Intuitively, once the model focuses more on invariant features, it will have a better zero-shot classification accuracy across different distributions. Nevertheless, in the following theorem, we justify that CLIP remains to learn to use spurious features, aligning with our experimental observations on the CounterAnimal dataset. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1. Given a multi-modal dataset (Def. 1) with suitable variance in the features $\\sigma_{i n v}=$ $\\Theta(1)\\ >\\ \\sigma_{s p u}.$ , and spurious features with a large spurious correlation $p_{s p u}~=~1\\,-\\,o(1)$ , an overparameterized CLIP model where $n\\;=\\;\\omega(1),d_{M}\\;=\\;\\Omega(n)$ and $d_{T}\\;=\\;\\Omega(n),$ , if the spurious features (e.g., backgrounds of the image) takes up a relatively large amount of the image $\\begin{array}{r}{\\mu_{s p u}\\geq\\frac{\\sigma_{i n v}^{2}+2}{2}\\geq\\mu_{i n v}=1}\\end{array}$ , then with a high probability of at least $\\begin{array}{r}{1-O\\big(\\frac{1}{p o l y(n)}\\big)=1-o(1),}\\end{array}$ , the CLIP model achieves a large error in zero-shot accuracy in the OOD test data where $a\\neq y$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nE r r(g_{I},g_{T})\\geq1-\\Phi(\\kappa_{1})-o(1),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and a small error in the OOD test data where $a=y$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nA c c(g_{I},g_{T})\\geq1-\\Phi(\\kappa_{2})-o(1),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa_{1}=\\frac{\\sigma_{i n v}^{2}+2-2\\mu_{s p u}p_{s p u}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}},\\;\\kappa_{2}=\\frac{-2\\mu_{s p u}p_{s p u}-\\sigma_{i n v}^{2}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}}\\;a n d\\;\\Phi}\\end{array}$ denotes the CDF of a standard normal distribution. ", "page_idx": 7}, {"type": "image", "img_path": "wWyumwEYV8/tmp/ebe2dc745842da12e2d14328745e7b76b988ef0164f8e63da96c644840d3a2ec.jpg", "img_caption": ["Figure 10: Illustration of ColoredCOCO. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "wWyumwEYV8/tmp/9eeec35583703a6b22ecd87aef1daefa4a1caa506699df42e590ca150bbc0c18.jpg", "img_caption": ["Figure 11: CLIP performance on ColoredCOCO. \u201csupervised\u201d refers to supervised trained models, while \u201cobj\u201d and \u201cobjbkg\u201d refer to using different prompts to fine-tune CLIPs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We leave more theoretical details as well as the proof to Appendix D due to space limit. Intuitively, Theorem 1 implies that once there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features, with object captions. Although our theory discusses a simplistic case of one invariant and one spurious feature, there could exist more features describing the objects and even more features describing the backgrounds. CLIP will fail to robustly align the visual features of objects to its captions, once there exists a spurious correlation between any of the background features with the object caption. Our theory is the first to provably demonstrate the drawbacks of CLIPs in OOD generalization, providing the foundation for future developments tackling the issue. ", "page_idx": 8}, {"type": "text", "text": "To verify our theory, we construct multi-modal datasets named ColoredCOCO following [51]. It contains 9 classes and the spurious correlation in the training part is $80\\%$ , i.e., each class has a correlation of $80\\%$ to a specific biased color and $20\\%$ uniformly correlates to 10 different randomly chosen colors, cf., Figure 10. The OOD datasets are built with classes randomly correlating to other 8 biased colors. We consider two prompts with different descriptiveness: a) obj: \u201ca photo of <object label>\u201d and b) objbkg: \u201ca photo of <object label> in <color label $>$ background\u201d, with either objects or both objects and backgrounds. ", "page_idx": 8}, {"type": "text", "text": "We tune the pre-trained CLIP models using the CLIP objective, which has been shown to be most robust to distribution shifts [52]. In addition, we also incorporate the baseline of full fine-tuning with a new MLP onto the image encoder using the ERM objective. As shown in Figure 11, fine-tuning with CLIP objective based on neither of the prompts provides any non-trivial robustness against the vanilla full fine-tuning. The results further verify our theory. Nevertheless, the degraded robustness of CLIP could also be caused by the weak language understanding capability of the BERT encoder in the CLIP. To this end, we also conduct additional experiments with a perfect language encoder setting. The results are given in Appendix D.4. Nevertheless, we find that CLIP still performs similarly to ERM and is prone to distribution shifts even with perfect captions. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we highlight biases in previous evaluations for assessing the robustness of CLIP models, primarily relying on ImageNet variants. Such improper benchmarking would cause illusions that CLIP models seem to resolve spurious correlations, particularly in comparison with ImageNet models. It motivates us to craft the new testset, named CounterAnimal, which is specifically designed to probe the natural spurious correlations between animal and their backgrounds. The spuriousness captured by CounterAnimal is general across different CLIP setups and exerts relatively small impacts on the ImageNet benchmarks, thereby specifically capturing the spurious correlations within CLIP setups. Our experiments on CounterAnimal show that many conventional strategies, e.g., increasing backbone scales and improving pre-train data quality, remain effective in enhancing the robustness of CLIP models. Moreover, we present a theoretical analysis for the reasons of the CLIP objective to learn biases. Overall, we provide a platform for future developments of more advanced and robust CLIP and vision-language models, and we hope our presented experiments can offer a sober look at the robustness of CLIP models to spurious correlations. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to express their sincere gratitude to the anonymous reviewers and the area chairs for their thorough review and constructive feedback. Their insightful comments and valuable suggestions have significantly enhanced the quality and clarity of this manuscript. We deeply appreciate their time and effort in helping us improve our work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[3] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022.   \n[4] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.   \n[5] Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, and Yao Qin. Effective robustness against natural distribution shifts for models with different training data. In NeurIPS, 2023.   \n[6] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (CLIP). In ICML, 2022.   \n[7] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. Is a caption worth a thousand images? a study on representation learning. In ICLR, 2023.   \n[8] Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, and Julia E. Vogt. Identifiability results for multimodal contrastive learning. In ICLR, 2023.   \n[9] Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. Understanding the robustness of multi-modal contrastive learning to distribution shift. arXiv preprint arXiv:2310.04971, 2023.   \n[10] Prasanna Mayilvahanan, Thadd\u00e4us Wiedemer, Evgenia Rusak, Matthias Bethge, and Wieland Brendel. Does CLIP\u2019s generalization performance mainly stem from high train-test similarity? In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023.   \n[11] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In CVPR, 2022.   \n[12] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[13] Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang. Sparse invariant risk minimization. In International Conference on Machine Learning, pages 27222\u201327244. PMLR, 2022.   \n[14] Yong Lin, Hanze Dong, Hao Wang, and Tong Zhang. Bayesian invariant risk minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16021\u201316030, 2022.   \n[15] Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, and Tong Zhang. Model agnostic sample reweighting for out-of-distribution learning. In International Conference on Machine Learning, pages 27203\u201327221. PMLR, 2022.   \n[16] Yong Lin, Fan Zhou, Lu Tan, Lintao Ma, Jiameng Liu, Yansu He, Yuan Yuan, Yu Liu, James Zhang, Yujiu Yang, et al. Continuous invariance learning. arXiv preprint arXiv:2310.05348, 2023.   \n[17] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. arXiv preprint arXiv:2309.17230, 2023.   \n[18] Xiaoyu Tan, Lin Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu Yinghui, Peng Cui, and Yuan Qi. Provably invariant learning without domain information. In International Conference on Machine Learning, pages 33563\u201333580. PMLR, 2023.   \n[19] Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, MA KAILI, Han Yang, Peilin Zhao, Bo Han, and James Cheng. Pareto invariant risk minimization: Towards mitigating the optimization dilemma in out-of-distribution generalization. In The Eleventh International Conference on Learning Representations, 2023.   \n[20] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In Advances in Neural Information Processing Systems, 2023.   \n[21] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. NeurIPS, 2019.   \n[22] Joshua Vendrow, Saachi Jain, Logan Engstrom, and Aleksander Madry. Dataset interfaces: Diagnosing model failures using controllable counterfactual generation. arXiv preprint arXiv:2302.07865, 2023.   \n[23] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stresstesting visual models by generating language-guided counterfactual images. NeurIPS, 2023.   \n[24] Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue. Imagenet-e: Benchmarking neural network robustness via attribute editing. In CVPR, 2023.   \n[25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021.   \n[27] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019.   \n[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[29] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[31] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023.   \n[32] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman. Mitigating spurious correlations in multi-modal models during fine-tuning. arXiv preprint arXiv:2304.03916, 2023.   \n[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint, arXiv:2305.10355, 2023.   \n[34] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[35] Qi Zhang, Yifei Wang, and Yisen Wang. On the generalization of multi-modal contrastive learning. In ICML, 2023.   \n[36] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. NeurIPS, 2019.   \n[37] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. ArXiv preprint arXiv:2006.09994, 2020.   \n[38] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. NeurIPS, 2020.   \n[39] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? In ICCV, 2021.   \n[40] Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? NeurIPS, 2022.   \n[41] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.   \n[42] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \n[43] Gregory Plumb, Marco Tulio Ribeiro, and Ameet Talwalkar. Finding and Fixing Spurious Patterns with Explanations. arXiv preprint arXiv:2106.02112, 2021.   \n[44] X Li, Y Chen, Y Zhu, S Wang, R Zhang, and H ImageNet-E Xue. Benchmarking neural network robustness via attribute editing. In CVPR, 2023.   \n[45] Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, and Chengzhi Mao. Imagenet-d: Benchmarking neural network robustness on diffusion synthetic object. In CVPR, 2024.   \n[46] Yann Dubois, Yangjun Ruan, and Chris J Maddison. Optimal representations for covariate shifts. In NeurIPS 2021 workshop on distribution shifts: connecting methods and applications, 2021.   \n[47] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In ICLR, 2020.   \n[48] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023.   \n[49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.   \n[50] Ryumei Nakada, Halil Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and Linjun Zhang. Understanding multimodal contrastive learning and incorporating unpaired data. In AISTAT, 2023.   \n[51] Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron Courville. Systematic generalisation with group invariant predictions. In ICLR, 2021.   \n[52] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In CVPR, 2023.   \n[53] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.   \n[54] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In ICML, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Broader Impacts and Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The current community often overestimates the robustness of CLIP models, largely due to the potentially misleading reliance on ImageNet variants for testing. To address this issue, we propose a new testset, named CounterAnimal, specifically tailored for CLIP models. Our findings indicate that CLIP models may not be as robust to distribution shifts as previously believed. Our dataset serves as a real-world benchmark, poised to be meaningful for the subsequent works to understand and enhance CLIP concerning their OOD robustness. For real-world applications, the understanding of spurious correlations for CLIP is also critical. We raise practical concerns when deploying CLIP models, which pertain to fairness and potential biases that may arise from inherent spurious correlations. We also present general strategies and theoretical analysis to understand the spurious correlations within CLIP models, which may motivate subsequent works to further enhance CLIP in real-world applications. However, although our dataset reaches the bar as a standard evaluation dataset, its research potential can be further benefited from expanding the scale of our dataset, diversifying the raw data sources beyond iNaturalist, broadening the semantic scope beyond animal classes, and studying other testbeds beyond the ImageNet benchmarks. In the future, we will extend our focus beyond animal subjects and include a wider array of high-quality data that are suitable for evaluating the robustness of CLIP and more advanced LVLMs. ", "page_idx": 12}, {"type": "text", "text": "B Dataset Composition ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We release our dataset CounterAnimal structured as follows: CounterAnimal ", "page_idx": 12}, {"type": "image", "img_path": "wWyumwEYV8/tmp/ce61a0a4260a2c3a8e789391ec6f4528920de8944631c927363a1a0b91eddb3e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Overall, the CounterAnimal dataset is organized by the object names. The data therein are further separated into two parts, i.e., the easy and hard groups, where the background name is also provided for each sub-directory. By evaluating accuracy with respect to the easy and hard groups, one can quantify the impacts of the spurious correlations captured by CounterAnimal. We further summarize the ImageNet animal objects as well as the group names for the easy and hard groups in Table 5. ", "page_idx": 12}, {"type": "text", "text": "C Experimental Configurations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide more details about our experimental configurations. ", "page_idx": 12}, {"type": "text", "text": "C.1 Hardware Configurations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All experiments are realized by Pytorch 1.81 with CUDA 11.1, using machines equipped with GeForce RTX 3090 GPUs and AMD Threadripper 3960X Processors. ", "page_idx": 12}, {"type": "table", "img_path": "wWyumwEYV8/tmp/bca63845ec54cb3922221abc9d1e348404a1b8d3084cb164ca13afe37f204ae4.jpg", "table_caption": ["Table 5: The object names and the background names in the CounterAnimal dataset. The full names of labels are presented following the fashion of the ImageNet-1K dataset. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C.2 Candidate Label Space ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We consider two different label spaces of candidate labels: a) using the full ImageNet-1K class names and b) using the top-20 most confusing classes for more computing-intensive models like MiniGPT4. It leads to the following two evaluation setups, i.e., the 1 vs. 1000 setup and the 1 vs. 20 setup. ", "page_idx": 13}, {"type": "text", "text": "1 vs. 1000 Setup. As a default option, we use the full label space of the ImageNet-1K dataset, which is suitable given that the object labels for CounterAnimal all belong to that of the ImageNet-1K dataset. Furthermore, this choice also reflects a more realistic situation in the open world, where we have a vast number of candidate labels and the failure cases of LVLMs are common. ", "page_idx": 13}, {"type": "text", "text": "1 vs. 20 Setup. To suit more advanced LVLMs of which the inference costs are much higher than CLIP models, we constrain the sizes of candidate label space for each class. Specifically, based on CLIP-LAION400M-ViT-B/32, we select the top-20 most confusing labels, which is calculated by the average cosine similarity for both the easy and hard groups. ", "page_idx": 13}, {"type": "text", "text": "C.3 Evaluation Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Now, we discuss the evaluation metrics. Typically, they are applied to the easy and hard groups separately when we evaluate the robustness of various models. ", "page_idx": 13}, {"type": "text", "text": "Class-wise Accuracy. We are interested in the effects of spurious features for each class. Therefore, we calculate the prediction accuracy specifically for photos within each class. It can be referred to as the class-wise accuracy, which is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{A C C}(\\mathrm{1abe1})=\\frac{1}{|\\mathcal Z_{\\tt A b e1}|}\\sum_{i\\in\\mathcal Z_{\\tt A b e1}}{\\bf1}\\{\\hat{y}_{i}=\\tt1a b e1\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{T}_{\\mathtt{I a b e l}}$ is the indices of photos belonging to label and $\\hat{y}_{i}$ is the predicted label for the $i$ -th image. The class-wise accuracy reflects the class-level model reliability against spurious correlations. ", "page_idx": 13}, {"type": "text", "text": "Average Accuracy. Upon the class-wise accuracy, we can calculate the average performance of models, namely, ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathsf{A C C}}={\\frac{1}{|{\\mathcal{L}}|}}\\sum_{\\mathtt{1a b e1}\\in{\\mathcal{L}}}{\\mathsf{A C C}}(\\mathtt{1a b e1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Compared to the conventional average accuracy, i.e., $\\begin{array}{r}{\\frac{1}{|\\mathcal{Z}|}\\sum_{i\\in\\mathcal{Z}}\\mathbf{1}\\{\\hat{y}_{i}=\\mathrm{gt}\\}}\\end{array}$ with $\\mathcal{T}$ the image indices and $\\mathtt{g t}$ the true labels, our definition of the average accuracy further offsets the impact of class ", "page_idx": 13}, {"type": "text", "text": "Table 6: Adopted versions of CLIP checkpoints employed in our main experiments. ", "page_idx": 14}, {"type": "table", "img_path": "wWyumwEYV8/tmp/26c0780d2a5344dca26c7b6a3a9da4bf0f2c9869a3af22dd7f91385332a58e00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "imbalance. We default to using the average accuracy, and present the results without balancing in Tables 17-18 for CLIP and ImageNet models. ", "page_idx": 14}, {"type": "text", "text": "Accuracy Drop. To quantify the spurious correlations that make CLIP models fail, we measure the performance drop when moving from the easy and hard groups. At the class level, the accuracy drop is defined as the class-wise accuracy of easy minuses that of hard. At the dataset level, it is the average value for the class-level accuracy drop. ", "page_idx": 14}, {"type": "text", "text": "C.4 Evaluation Details of MiniGPT4 and LLaVA ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To evaluate LVLMs with a backend of language models, we follow the common practice that constructs questions to prompt LVLMs [29, 30]. Specifically, we construct the question as: ", "page_idx": 14}, {"type": "text", "text": "and then calculate the language modeling loss with respect to the answer: ", "page_idx": 14}, {"type": "text", "text": "for each ImageNet class name. Meanwhile, we also try another question prompt that is widely used in training MiniGPT4 and LLaVA [30, 53]: ", "page_idx": 14}, {"type": "text", "text": "while the performance will generically decrease. In addition, when we switch to the object-centric evaluation protocol as [33]: ", "page_idx": 14}, {"type": "text", "text": "or ", "page_idx": 14}, {"type": "text", "text": "and evaluate the answer with Yes for each class, we observe a severe performance decrease as LVLMs easily hallucinate the objects. If we strictly follow the evaluation metrics of [33] by simply fetching the answers instead of comparing the losses, there exist lots of hallucinated objects by LVLMs in our dataset. ", "page_idx": 14}, {"type": "text", "text": "C.5 CLIP Naming Rules ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the CLIP checkpoints, we adopt the naming rule of \u201cCLIP-<dataset>-<backbone>\u201d, where <dataset> is the name of pre-train datasets and <backbone> is the specific name of backbone models. For example, CLIP-LAION400M-ViT-B/32 indicates the ViT-B/32 model CLIP-trained on LAION400M. Different training setups are considered in OpenCLIP, and the versions of the adopted checkpoints are summarized in Table 6. Moreover, in Table 15, we consider the results of checkpoints beyond the adopted ones. ", "page_idx": 14}, {"type": "text", "text": "D Theoretical Understanding of CLIP\u2019s Robustness to Spurious Features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide a more detailed setup and analysis in complementary to Section 4. ", "page_idx": 15}, {"type": "text", "text": "D.1 Detailed Theoretical Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin by introducing more details about the data generation process following the literature [9, 50, 54]. ", "page_idx": 15}, {"type": "text", "text": "Definition 2 (Multi-modal Dataset). Consider n image-text pairs $\\{(\\pmb{x}_{I}^{i},\\pmb{x}_{T}^{i})\\}_{i=1}^{n}$ , both image $\\pmb{x}_{I}^{i}\\in$ $\\mathbb{R}^{d_{I}}$ and text $\\pmb{x}_{T}^{i}\\in\\mathbb{R}^{d_{T}}$ are generated from the underlying latent factor $z_{i}\\in\\mathbb{R}^{l}$ . The samples are generated as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $z=[z_{i n v},z_{s p u}]\\in\\mathbb{R}^{2}$ is composed of a invariant feature $z_{i n v}\\sim\\mathcal{N}(\\mu_{i n v}y,\\sigma_{i n v}^{2})$ and $a$ spurious feature $z_{s p u}\\sim\\mathcal N(\\mu_{s p u}a,\\sigma_{s p u}^{2})$ with $\\operatorname*{Pr}(a=y)=p_{s p u}$ otherwise $a=-y,$ , y is the label uniformly drawn from $\\{-1,1\\}$ , $\\mathcal{D}^{t r}$ is drawn with $1/2\\le p_{s p u}\\le1$ while the OOD test data $\\mathcal{D}^{*}$ is drawn uniformly with $p_{s p u}=1/2$ . \u2022 Given $_{\\textit{z}}$ , the x at modality $M$ is generated via ${\\pmb x}_{M}={\\pmb D}_{M}{\\pmb\\mu}_{M}({\\pmb z})\\,+\\,\\!\\xi_{M}$ , with $D_{M}\\in\\mathbb{R}^{d_{M}\\times l}$ and $\\xi_{M}\\sim\\mathcal{N}(0,\\sigma_{\\xi}^{2}/d_{m}\\pmb{I}_{d_{m}})$ . The matrix $D_{M}\\,\\in\\,\\mathbb{R}^{d_{m}\\,\\times\\,l}$ with $d_{m}\\,>\\,l$ is a matrix with orthonormal columns which can be considered as a dictionary matrix. ", "page_idx": 15}, {"type": "text", "text": "With the definition, we can write every $z^{i}=\\left[{{y^{i}}+\\eta_{1,i}}\\atop{\\mu_{s p u}p_{s p u}+\\eta_{2,i}}\\right]$ where $\\eta_{1,i},\\eta_{2,i}$ are two Gaussian variables in the definition. ", "page_idx": 15}, {"type": "text", "text": "CLIP Training. We employ two linear encoders $g_{I}:\\,\\mathbb{R}^{d_{I}}\\,\\rightarrow\\,\\mathbb{R}^{h}$ for the image modality and $g_{T}:\\mathbb{R}^{d_{T}}\\rightarrow\\mathbb{R}^{\\breve{h}}$ for the text modality, implemented as $g^{I}({\\pmb x}_{I})=W_{I}{\\pmb x}_{I}$ and $g_{T}\\bar{(}{\\bf x}_{T})={\\cal W}_{T}{\\pmb x}_{T}$ with $W_{I}\\in\\mathbb{R}^{h\\times d_{I}}$ and $W_{T}\\in\\mathbb{R}^{h\\times d_{T}}$ , respectively. The encoders are trained through the linearized contrastive loss [9, 50] that mimics CLIP training dynamics: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathrm{CLIP}}=\\displaystyle\\frac{1}{2n(n-1)}\\sum_{i}\\sum_{j\\neq i}(s_{i j}-s_{i i})}&{}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{2n(n-1)}\\sum_{i}\\sum_{j\\neq i}(s_{j i}-s_{i i})+\\displaystyle\\frac{\\rho}{2}\\|{\\boldsymbol W}_{I}^{T}{\\boldsymbol W}_{T}\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $s_{i j}=g_{I}({\\pmb x}_{I}^{i})^{T}g_{T}({\\pmb x}_{T}^{j})$ is the similarity with respect to the $i$ -th image and $j$ -th text representations, and $||W_{I}^{T}W_{T}||_{F}^{2}$ is the a regularization term with $\\rho>0$ . ", "page_idx": 15}, {"type": "text", "text": "Zero-shot Inference. Once the CLIP model $(g_{I},g_{T})$ is trained, the performance will be measured in a zero-shot manner by matching the most similar caption such as $\\bullet_{\\mathtt{a}}$ photo of {object name}\u2018 across different object name as class names. Meanwhile, one could also leverage several prompts and leverage the average text embeddings across the available prompts to facilitate the evaluation [2]. The prompt with respect to $y$ could be modeled as $p_{y}\\,=\\,D_{T}\\mathbb{E}[z^{t}|y]$ , where $D_{T}$ is the prompt transformation matrix. Then, the zero-shot accuracy of CLIP could be formalized as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Acc}(g_{I},g_{T})=\\mathbb{E}_{(\\mathbf{x},y)}[\\mathbf{1}(\\arg\\operatorname*{max}_{\\hat{y}}g_{I}(\\mathbf{x}_{I})^{T}g_{T}(\\mathbf{p}_{\\hat{y}}),y)],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "while the error is $\\mathrm{Err}(g_{I},g_{T})=1-\\mathrm{Acc}(g_{I},g_{T})$ . Intuitively, once the model extracts more of the invariant features, it will have a better zero-shot classification accuracy across different distributions. ", "page_idx": 15}, {"type": "text", "text": "D.2 Proof for Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 2 (Restatement of Theorem 1). Given a multi-modal dataset (Def. 2) with suitable variance in the features $\\sigma_{i n v}\\;=\\;\\Theta(1)\\;>\\;\\sigma_{s p u}$ , and spurious features with a large spurious correlation $p_{s p u}=1-o(1).$ , an overparameterized CLIP model where $n=\\omega(1),d_{M}=\\bar{\\Omega^{\\left(n\\right)}}$ and $d_{T}=\\Omega(n)$ , if the spurious features (e.g., backgrounds of the image) takes up a relatively large amount of the image $\\begin{array}{r}{\\bar{\\mu}_{s p u}\\geq\\frac{\\bar{\\sigma}_{i n v}^{2}+2}{2}\\geq\\bar{\\mu_{i n v}}=1}\\end{array}$ , then with a high probability of at least $\\begin{array}{r}{1-O\\big(\\frac{1}{p o l y(n)}\\big)=1-o(1),}\\end{array}$ the CLIP model achieves a large error in zero-shot accuracy in the OOD test data where $a\\neq y$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nE r r(g_{I},g_{T})\\geq1-\\Phi(\\kappa_{1})-o(1),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and a small error in the OOD test data where $a=y$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nA c c(g_{I},g_{T})\\geq1-\\Phi(\\kappa_{2})-o(1),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where \u03ba1 = $\\begin{array}{r}{\\kappa_{1}=\\frac{\\sigma_{i n v}^{2}+2-2\\mu_{s p u}p_{s p u}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}},\\;\\kappa_{2}=\\frac{-2\\mu_{s p u}p_{s p u}-\\sigma_{i n v}^{2}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}}\\;a n d\\;\\Phi}\\end{array}$ denotes the CDF of a standard normal distribution. ", "page_idx": 16}, {"type": "text", "text": "Proof. We will introduce some useful lemmas to help with our proof. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1 ([9]). The minimizer of linearized CLIP loss $W_{I}^{*T}W_{T}^{*}$ satisfies the following with $a$ probability of at least $\\begin{array}{r}{1-O(\\frac{1}{p o l y(n)})}\\end{array}$ such that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{W}_{I}^{*T}\\boldsymbol{W}_{T}^{*}-\\frac{1}{\\rho}\\boldsymbol{D}_{I}\\left[\\frac{1+\\sigma_{i n v}^{2}}{2\\mu_{s p u}p_{s p u}-1}\\quad2\\mu_{s p u}p_{s p u}-1\\right]\\boldsymbol{D}_{T}^{T}\\|_{2}\\leq\\frac{1}{\\rho}O(\\sqrt{\\epsilon_{0}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\epsilon_{0}=O(\\sqrt{\\frac{\\log n}{n}})$ . ", "page_idx": 16}, {"type": "text", "text": "Intuitively, the lemma indicates the importance of the training distribution, that the minimizer of CLIP will converge to the data characteristics of the latent features of the training distribution. ", "page_idx": 16}, {"type": "text", "text": "Then, consider the case where the model is inferred onto a test sample with $y=1,a=-1$ . Then, with the aforementioned lemma, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{I}^{T}W_{I}^{*}W_{T}^{*}x_{T}^{\\hat{y}}-\\frac{1}{\\rho}x_{I}^{T}D_{I}\\left[\\underset{2\\mu_{s p u}p_{s p u}-1}{\\overset{1+\\sigma_{i n v}^{2}}{\\sum}}\\right.}&{\\left.2\\mu_{s p u}p_{s p u}-1\\right]D_{T}^{T}x_{T}^{\\hat{y}}||_{2}\\leq||x_{I}||||x_{T}^{\\hat{y}}||\\frac{1}{\\rho}O(\\sqrt{\\epsilon_{0}})}\\\\ &{\\leq\\cfrac{1}{\\rho}O(\\sqrt{\\epsilon_{0}}\\log n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{3}x_{I}^{T}D_{I}\\left[\\underset{2\\mu_{s p u}p_{s p u}-1}{1+\\sigma_{i n v}^{2}}\\right.}&{\\left.2\\mu_{s p u}p_{s p u}-1\\right]D_{T}^{T}x_{T}^{\\hat{y}}=\\hat{y}((1+\\eta_{1})(1+\\sigma_{i n v}^{2})+(-1+\\eta_{2})(2\\mu_{s p u}p_{s p u}-1))+}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When CLIP makes an incorrect prediction, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{I}^{T}W_{I}^{*}W_{T}^{*}x_{T}^{\\hat{y}=1}<x_{I}^{T}W_{I}^{*}W_{T}^{*}x_{T}^{\\hat{y}=-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{\\rho}{x_{I}^{T}}D_{I}\\left[\\begin{array}{c c}{1+\\sigma_{i n v}^{2}}&{2\\mu_{s p u}p_{s p u}-1}\\\\ {2\\mu_{s p u}p_{s p u}-1}&{1+\\sigma_{s p u}^{2}}\\end{array}\\right]D_{T}^{T}{x_{T}^{\\hat{y}=1}}-\\displaystyle\\frac{1}{\\rho}O(\\sqrt{\\epsilon_{0}}\\log n)<}&{{}}\\\\ {\\displaystyle\\frac{1}{\\rho}{x_{I}^{T}}D_{I}\\left[\\begin{array}{c c}{1+\\sigma_{i n v}^{2}}&{2\\mu_{s p u}p_{s p u}-1}\\\\ {2\\mu_{s p u}p_{s p u}-1}&{1+\\sigma_{s p u}^{2}}\\end{array}\\right]D_{T}^{T}{x_{T}^{\\hat{y}=-1}}-\\displaystyle\\frac{1}{\\rho}O(\\sqrt{\\epsilon_{0}}\\log n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with Eq. 5 plugged in, denote $\\epsilon_{1}=O(\\sqrt{\\epsilon_{0}}\\log n)$ , we further have ", "page_idx": 16}, {"type": "equation", "text": "$$\n-2\\left[(1+\\eta_{1})(1+\\sigma_{i n v}^{2})+(-1+\\eta_{2})(2\\mu_{s p u}p_{s p u}-1)-\\epsilon_{1}\\right]>0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\eta_{1}(1+\\sigma_{i n v}^{2})+\\eta_{2}(2\\mu_{s p u}p_{s p u}-1)$ is a Gaussian variable follows the distribution of ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{1}(1+\\sigma_{i n v}^{2})+\\eta_{2}(2\\mu_{s p u}p_{s p u}-1)\\sim\\mathcal{N}(0,(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(-2\\left[(1+\\eta_{1})(1+\\sigma_{i n v}^{2})+(-1+\\eta_{2})(2\\mu_{s p u}p_{s p u}-1)-\\epsilon_{1}\\right]>0)}\\\\ &{\\ =\\operatorname*{Pr}_{v\\sim\\mathcal{N}(0,1)}(v>\\frac{\\sigma_{i n v}^{2}+2-2\\mu_{s p u}p_{s p u}+\\epsilon_{1}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}})}\\\\ &{\\ =1-\\Phi(\\frac{\\sigma_{i n v}^{2}+2-2\\mu_{s p u}p_{s p u}+\\epsilon_{1}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "wWyumwEYV8/tmp/c38a19b2858dc2f7aeff5c8a8ab7ad3af7a4a9246fa9877b28af3f28cb6bf5ae.jpg", "table_caption": ["Table 7: Comparison between CLIPs and standard supervised learning on ColoredCOO "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "where $\\Phi$ is the CDF of the standard Gaussian distribution. Then, it suffices to know that the $\\mathrm{Err}_{y=1,a=-1}$ is lower bounded by $\\begin{array}{r}{\\Phi\\big(\\frac{\\sigma_{i n v}^{2}+2-2\\mu_{s p u}p_{s p u}+\\epsilon_{1}}{\\sqrt{(1+\\sigma_{i n v}^{2})^{2}\\sigma_{i n v}^{2}+(2\\mu_{s p u}p_{s p u}-1)^{2}\\sigma_{s p u}^{2}}}\\big)}\\end{array}$ , which also applies to the case $y=-1,a=1$ . ", "page_idx": 17}, {"type": "text", "text": "Similarly, given the case $y=a$ , as the model fits the spurious feature, we could derive the lower bound for its Acc by leveraging the spurious features as \u03a6(\u221a(1+\u03c3i2nv)\u221222\u03c3\u00b5i2nspvu+p(s2p\u00b5us\u2212pu\u03c3ipnspvu\u22121)2\u03c3s2pu ). .\u53e3 ", "page_idx": 17}, {"type": "text", "text": "D.3 More Details on ColoredCOO Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To further validate our theoretical results, we construct the ColoredCOO dataset following [51]. More specifically, ColoredCOO is constructed as follows: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The dataset contains 9 classes of COCO objects. The spurious correlation in the trainset is $80\\%$ such that each class has a correlation of $80\\%$ to a specific biased color and $20\\%$ uniformly correlates to 10 sufficiently different randomly chosen colors. \u2022 The OOD testsets are constructed with classes randomly correlating to 8 biased colors different from the one correlated in the training set. ", "page_idx": 17}, {"type": "text", "text": "Then, we further generate two prompts for each sample: ", "page_idx": 17}, {"type": "text", "text": "1. obj: a photo of <object label>;   \n2. objbkg: a photo of <object label> in <color label> background ", "page_idx": 17}, {"type": "text", "text": "We tune the pre-trained CLIP models using the CLIP objective based on the OpenCLIP library. We consider the learning rate of $\\left\\{1e-3,1e-4,1e-5\\right\\}$ , with a weight decay of $\\left\\{1e-1,1e-3,1e-5\\right\\}$ , and a warmup of $\\{0,1000\\}$ steps. We select the model according to the best in-distribution test performance. The detailed results are given in Table 7. As we can see, the CLIPs finetuned using either the CLIP objective or the standard supervised training both exhibit high sensitivity to the spurious features. ", "page_idx": 17}, {"type": "text", "text": "D.4 More Details on MultiColoredMNIST Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "One possible explanation for the failure of CLIP objective in ColoredCOCO is that, the language encoder of the CLIP models may not understand the captions well. Therefore, we further construct a new setup called MultiColoredMNIST, where each image contains only the digit information from MNIST dataset and the color information. Therefore, we can directly derive the one hot encoding for all of the useful factors in the dataset. ", "page_idx": 17}, {"type": "text", "text": "Data. We consider a multi-class classification setting with a number of classes no less than 2. The objects are the ", "page_idx": 17}, {"type": "text", "text": "\u2022 Training data: Fix two class $\\left(0/1\\right)$ and color $\\left(\\mathrm{r/g}\\right)$ , they are spurious correlated by a correlation $p_{s p u}$ . The invariant feature\u2019s correlation with labels is $p_{i n v}$ .   \n\u2022 Test data (Rand): All classes and the colors are randomly correlated, given $k$ class, $p_{s p u}=$ $1/k$ .   \n\u2022 Test data (Rev): All classes and the colors are reversely correlated, $p_{s p u}$ is $10\\%\\,0/1$ classes and $1/k$ for others. ", "page_idx": 17}, {"type": "image", "img_path": "wWyumwEYV8/tmp/d62fb5dad8bcfd3bcec9932f81f871f957d23490182b81052d06330f87fd555a.jpg", "img_caption": ["Figure 12: Examples of MultiColoredMNIST dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In Figure 12, we give some examples for the MultiColoredMnist dataset. ", "page_idx": 18}, {"type": "text", "text": "Experimental setting. We compare the standard supervised training and CLIP. To avoid noises or information loss in encoding language modality, we consider the perfect language supervision for a single model. Given a batch of image and caption representations $\\{(\\boldsymbol{h}^{x_{i}},\\boldsymbol{h}^{c_{i}})\\}_{i}^{B}$ , for a image-caption pair, the CLIP objective aims to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}(M_{x}h^{x_{i}}\\cdot M_{c}h^{c_{i}})-(M_{x}h^{x_{j}}\\cdot M_{c}h^{c_{j}}),\\forall i\\neq j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $M_{x}\\in\\mathbb{R}^{d\\times h_{x}}$ and $M_{c}^{d\\times h_{c}}$ are the learnable projection layers for image and caption representations. Assuming the perfect language encoding as the one-hot encoding for all possible object and background appearance $h^{c_{i}}\\in[0,1]^{|\\mathcal{O}|+|B|}$ , and $M_{c}$ can simply be an identity matrix, then Eq. 9 can be considered as a classification task for objects and backgrounds respectively: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\mathrm{{CE}}(M_{c}^{T}M_{x}{h}^{x_{i}},{h}^{c_{i}}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the labels are simply the one-hot encodings of the objects and the backgrounds, and the classifier is $M_{c}^{T}M_{x}$ . For the MultiColoredMNIST task where there is only one object and background (i.e., color), to implement Eq. 10, we only need to construct an additional classification head for the background. Given the aforementioned setup, we conduct experiments comparing CLIP-based contrastive learning to the standard supervised learning. The results are given in Table 8. As we can see, both contrastive learning and supervised learning perform similarly across different numbers of classes and bias degrees. ", "page_idx": 18}, {"type": "table", "img_path": "wWyumwEYV8/tmp/119d3cb76cb959a11060806d6b1bf85d30b5e8dd9c97d943faf94c878aab2456.jpg", "table_caption": ["Table 8: Comparison of standard supervised learning and contrastive learning on MultiColoredMNIST dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Ablation Studies ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present ablation studies to further validate the feasibility of our data curation process. ", "page_idx": 20}, {"type": "text", "text": "Biasing to ImageNet Setups. We follow the same curation procedure while using ImageNet models (i.e., ResNet50-ImageNet) to construct easy and hard splits, where we name the corresponding dataset as CounterAnimal-I. We present the results of CLIP and ImageNet models on CounterAnimal-I in Tables 9-10, respectively. The effective robustness is further shown in Figure 13. Contrary to the observations within original CounterAnimal, CLIP models can demonstrate better robustness against spurious features within CounterAnimal-I. It aligns with our expectation since different training data (e.g., LAION for CLIPs, and ImageNet for ImageNet models) follow different distributions and naturally contain different spurious features. It also demonstrates the generality of our data curation method to reveal the spurious features for different kinds of the models. We also list the background names for easy and hard splits with respect to some of the selected classes in Table 11. As observed, using different models to split data will capture very different spurious features. It highlights the necessity to curate an OOD testset for CLIP models, as CLIP models learn different spurious features than ImageNet models. ", "page_idx": 20}, {"type": "text", "text": "Correctness vs. Frequency. We further explain why easy and hard examples can characterize spurious features within CLIP setups. In general, spurious features can be caused by biases inherent in the data distribution concerning backgrounds. For example, for the animal class of ice bear, the background of ice is more common than other backgrounds, such as grass, thus causing spurious correlations learned by CLIP models. Therefore, we investigate in terms of the background frequency, employing the searching tool of Have I Been Trained 4 that can retrieve images from LAION5B closely matching a given class name. We examine 10 animal classes as our case studies. For each class, we collect the top 100 most relevant images and tally the occurrences of the backgrounds of our consideration. It is important to note that our counting process excludes cartoon images, irrelevant photos, corrupted photos, and those featuring multiple distinct animal subjects or ambiguous backgrounds. The results are summarized in Table 12. As we can see, in general, the spurious features captured align with our conjecture that hard examples contain uncommon backgrounds in the CLIP training data, e.g., LAION5B, further justifying the feasibility of our CounterAnimal in assessing the robustness of CLIP models in real-world situations. ", "page_idx": 20}, {"type": "text", "text": "Table 9: The 1 vs. 1000 results for CLIP checkpoints on the CounterAnimal-I dataset. ", "page_idx": 21}, {"type": "table", "img_path": "wWyumwEYV8/tmp/2825236a9cef73804c2a67567d77e6f7100cadfd02cf1ccff2dbfddde9350d1a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 10: The 1 vs. 1000 performance for ImageNet models on the CounterAnimal-I dataset. ", "page_idx": 21}, {"type": "image", "img_path": "wWyumwEYV8/tmp/8e4670ae1400a56bcc70328d196ab90f827e3073edcb5b4f091f81afc51f6297.jpg", "img_caption": ["Figure 13: The easy verus hard performance $(\\%)$ for CLIP and ImageNet models on CounterAnimal-I. The 1 vs. 1000 setup is considered. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 11: Selected animal object and background names in CounterAnimal and CounterAnimal-I.   \nWe bold the background names differently between CounterAnimal and CounterAnimal-I. ", "page_idx": 22}, {"type": "table", "img_path": "wWyumwEYV8/tmp/3b499ffd13bbc4718e4b1fb0827a07192e8c19be6ae1fd17f56bea12b1423c82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "wWyumwEYV8/tmp/248612419b3596b4b7d20a8588d784010efe84e68068eb7cb38dbc2936416e2c.jpg", "table_caption": ["Table 12: The number of photos counted with respect to easy and hard backgrounds, based on the searching tool of Have I Been Trained. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "wWyumwEYV8/tmp/64890dcf9b5cc9dd3cb78541225fd29c938e7a551d21b37d97f8cca1355f2042.jpg", "img_caption": ["Figure 14: Comparison for the effective robustness with respect to a) different backbones, b) different pre-train datasets, as well as c) high-quality (HQ) and low-quality (LQ) pre-train datasets. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F More Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present more experimental results to support our claims. ", "page_idx": 23}, {"type": "text", "text": "Effective Robustness. In Section 3, we mainly examine the absolute robustness to assess and compare the OOD performance across various CLIP setups, which are well known to be sensitive to the original value scales. Therefore, in Figure 14, we apply the measures of effective robustness [5] to further substantiate our conclusions. Overall, our previous conclusions are upheld, demonstrating that the benefits derived from increasing model scales and enhancing data quality notably outweigh those obtained by merely expanding dataset sizes. ", "page_idx": 23}, {"type": "text", "text": "Top-5 Results for CLIP models. We present 1 vs. 1000 results for more CLIP checkpoints on the CounterAnimal dataset in Table 13, which is an extension of Table 2. Moreover, we present more results for the evaluations on CounterAnimal, supplementing our analysis of CLIP models under spurious correlations. To begin with, we report the top-5 scores under the 1 vs. 1000 setup, where we check if the target label is one of the top-5 model predictions. The results are summarized in Table 14. Comparing with the top-1 results in Table 13, we find that there is still a large performance gap between the easy and hard groups, indicating that the label confusion is quite diverse and not limited to the top two classes. ", "page_idx": 23}, {"type": "text", "text": "Other Versions of Pre-train Datasets. OpenCLIP provides other CLIP checkpoints beyond our adopted ones. Table 15 summarizes the results of CLIP models similar to Table 2 while using different versions of checkpoints. As we can see, the performance for both easy and hard is very stable across varying versions, except for DataComp1B. The reason is that their various checkpoints use subsets of DataComp1B, where XL indicates the fully DataComp1B, L indicates a 140M subset, M indicates a 14M subset, and S indicates a 1.4M subset. ", "page_idx": 23}, {"type": "text", "text": "Results of OpenAI Prompts. We further consider the prompt setups following OpenAI CLIP [2], using average text embeddings over 80 predefined prompts as the final text embeddings. The results are summarized in Table 16. As we can see, the average performance for both the easy and hard groups generally improves 1 to 3 percentage points over the results of our simpler prompt. However, our main conclusion remains unchanged: the ImageNet models generally exhibit better performance and smaller drops. Another interesting finding is that when evaluating with CLIP-LAION400M-ViT-B/32 (the CLIP checkpoint employed in our data collection), the performance drop with OpenAI prompts is not as high as that of our simple prompt used in Table 2. It indicates that our curation procedure mainly overfit the adopted prompt instead of the particular CLIP checkpoint. ", "page_idx": 23}, {"type": "text", "text": "Average Performance without Balancing. We by default adopt the balanced average accuracy to offset the impacts of class imbalance. In Tables 17-18, we further summarize the results without class balance, following $\\begin{array}{r}{\\frac{1}{|\\mathcal{Z}|}\\sum_{i\\in\\mathcal{Z}}\\mathbf{1}\\{\\hat{y}_{i}=\\mathrm{gt}\\}}\\end{array}$ . As we can see, the performance drop remains obvious, and similar conclusions can be drawn as the balanced results: a) Backbone scales are more important for spurious robustness than pre-train dataset scales, and b) ImageNet models are more reliable when facing spurious features in CounterAnimal. ", "page_idx": 23}, {"type": "text", "text": "1 vs. 20 Results for CLIP and ImageNet Models. We adopt the 1 vs. 20 setup for the evaluations of more advanced LVLMs in Table 4. For a fair comparison, we further summarize the 1 vs. 20 ", "page_idx": 23}, {"type": "text", "text": "Table 13: The 1 vs. 1000 results for CLIP checkpoints on CounterAnimal. The pre-train datasets with high-quality data are marked by \u2217. ", "page_idx": 24}, {"type": "table", "img_path": "wWyumwEYV8/tmp/ddd82a9f9915373eee92f23f9d3f73dc7dea873ed00a162a8639014126b7b87b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "results for CLIP models in Table 19 and for ImageNet models in Table 20. As we can see, there does not exist a significant change in performance drop compared to 1 vs. 1000 results, indicating that mistakes made by CLIP models are relatively concentrated. As in Figure 2, we also depict the easy versus hard performance for various learning setups with their names, following the 1 vs. 1000 setup in Figure 15 and 1 vs. 20 setups in Figure 16. ", "page_idx": 24}, {"type": "text", "text": "Class-wise Results. In Tables 21-22, we summarize the detailed results of the class-wise accuracy for the main results in Figure 5. We further depict the drop in accuracy in Figure 17. Generally speaking, the spurious features found in CLIP-LAION400M-ViT-B/32 can also fail other CLIP setups, and the general trends of decline are preserved class-wise. However, there are some cases where the drop in accuracy between easy and hard is negative, e.g., for data in class ID 33 and 42. It means that for these cases, our collection pipeline may have a large overfit to the adopted CLIP setup, i.e., CLIP-LAION400M-ViT-B/32. ", "page_idx": 24}, {"type": "table", "img_path": "wWyumwEYV8/tmp/668166389aa926f41c17635ad9a5d21a5de9acfeb948e163b18e27c907271186.jpg", "table_caption": ["Table 14: The 1 vs. 1000 results with top-5 performance scores for CLIP checkpoints on CounterAnimal. The pre-train datasets with high-quality data are marked by \u2217. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "wWyumwEYV8/tmp/4862a9226d2fe83c75719239847d0dfafedc6fae0105d565822177584d1e1ad3.jpg", "table_caption": ["Table 15: The 1 vs. 1000 performance with other versions of CLIP checkpoints in OpenCLIP. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "wWyumwEYV8/tmp/a6a9f8b5a9cfc8d90c2467f5d8ccb02dcd38b80d7df42570894ac625eac31b8a.jpg", "table_caption": ["Table 16: The 1 vs. 1000 performance using prompts of OpenAI CLIP. The pre-train datasets with high-quality data are marked by \u2217. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "wWyumwEYV8/tmp/baaa2439f59bff343cbf6becf3fed9f4f3dfc70d06e3ab820dd538a4f6229b59.jpg", "table_caption": ["Table 17: The 1 vs. 1000 performance on CounterAnimal for CLIP models, evaluating based on the accuracy without balancing. The pre-train datasets with high-quality data are marked by \u2217. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 18: The 1 vs. 1000 performance on CounterAnimal for ImageNet models, evaluating based on the accuracy without balancing. ", "page_idx": 27}, {"type": "table", "img_path": "wWyumwEYV8/tmp/6bbf77160e5e0e4c723f855e83b90f898b94178f57e0b4866c47f046da1bd13b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 19: The 1 versus 20 performance on CounterAnimal for CLIP models. The pre-train datasets with high-quality data are marked by \u2217. ", "page_idx": 27}, {"type": "table", "img_path": "wWyumwEYV8/tmp/e411ddecc5b13eadf762eadfb3bc9f3b2a6530cb9bf01b7b544d72d85f816246.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 20: The 1 versus 20 performance on CounterAnimal for ImageNet models. ", "page_idx": 27}, {"type": "table", "img_path": "wWyumwEYV8/tmp/ffea541222eecc80985cdbb161af9dbb4a8f0745a154469acc18ebec175963c0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "wWyumwEYV8/tmp/baa60f758638ee888c802689f64bf939ef316beb4c8f7bd42bc11f865313f19a.jpg", "img_caption": ["Figure 15: The easy versus hard performance $(\\%)$ for CLIP and ImageNet models, following the 1 vs. 1000 setup. We also present the model setups for each easy-hard result pair. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "wWyumwEYV8/tmp/c2da18537125a5b64c1b321c08752b2d8a2e8d3886b360e9113034eca4482c7a.jpg", "img_caption": ["Figure 16: The easy versus hard performance $(\\%)$ for CLIP, ImageNet models, and more advanced LVLMs, following the 1 vs. 20 setup. We also present the model setups for each easy-hard result pair. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "wWyumwEYV8/tmp/d31ee2b391fc12e793a09e70e523e4cf4e13794d0a521bbe385951dddac72d5a.jpg", "table_caption": ["Table 21: Class-wise 1 vs. 1000 performance on CounterAnimal for different backbones CLIPtrained on LAION400M. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "wWyumwEYV8/tmp/1abe5163747f2e229e1652d69d472f4462fc6c2f1188e065cd001f64e6b6b76f.jpg", "table_caption": ["Table 22: Class-wise 1 vs. 1000 performance on CounterAnimal for ViT-B/32 CLIP-trained on different datasets. "], "table_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "wWyumwEYV8/tmp/5fd41f650799d7b980c0675e392f8e6dcd5f4bb5100cbfd45c6f1bbbe6daac58.jpg", "img_caption": ["(a) CLIP-LAION400M-ViT-B/32 "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "wWyumwEYV8/tmp/6891d78a4b24548633a446c1970c161acf2f0b7f9c0314ff6a2b2847d69861e0.jpg", "img_caption": ["(b) CLIP-LAION400M-ViT-B/16 "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class ID ", "page_idx": 32}, {"type": "image", "img_path": "wWyumwEYV8/tmp/98da019ab4ad5624d5acbed8b7c2be813f8aea366f500bfa36ba1b020d22f295.jpg", "img_caption": ["(c) CLIP-LAION400M-ViT-L/14 "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class ID ", "page_idx": 32}, {"type": "image", "img_path": "wWyumwEYV8/tmp/1725314f185054417df5e10ea6067b0a20eac7a1434c6586998798d4ae0770d8.jpg", "img_caption": ["(d) CLIP-LAION2B-ViT-B/32 "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "wWyumwEYV8/tmp/ff6941017e09ba16519d0b0ebfbf0ce96b5225780f3234a50b01aec577af6b22.jpg", "img_caption": ["(e) CLIP-OpenAI-ViT-B/32 "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class ID ", "page_idx": 32}, {"type": "text", "text": "Figure 17: The performance drop $(\\%)$ between easy to hard on varying CLIP setups. The horizontal axis denotes the class ids and the vertical axis denotes the class-wise accuracy drop. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our main claim is that CLIP may not rely less on spurious features compared to ImageNet models. To support our claim, we curate a new evaluation dataset named CounterAnimal, specifically tailored for a comparative evaluation for the robustness of CLIP. Furthermore, we also conduct comprehensive experiments in Section 3 and theoretical justification in Section 4, both of which fully support our main position. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitation of our work in Appendix A, especially focusing on the further improvements for our dataset. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: In Definition 1, we clearly outline the assumptions made for the subsequent theoretical analysis presented in Theorem 1. In Appendix D, we present further details regarding our assumptions, proofs, and the associated experiments to validate their feasibility. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: In Section 2.1, we elaborate on the exact process used to construct the CounterAnimal dataset, including details about the raw data sources, the construction pipeline, and the dataset configurations. Moreover, for reproducibility, most of our experiments utilize open-sourced model checkpoints from the OpenCLIP and PyTorch repositories. We also comprehensively document the evaluation setups in Appendix C. We will release our dataset and the evaluation codes in the future. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 34}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We establish an anonymous repository for the access to our dataset, which can be found at the link of https://figshare.com/s/f9b0f34312168f4a8ddb. We will provide clearer and easier access instructions once the paper is accepted. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We utilize open-sourced model checkpoints from OpenCLIP and PyTorch. We collect the CounterAnimal dataset following the precise pipelines in Section 2.1. Moreover, we detail the evaluation setups of our experiments in Appendix C. We utilize open-sourced model checkpoints from OpenCLIP and PyTorch; we have collected the CounterAnimal dataset according to the procedures outlined in Section 2.1; and we provide detailed descriptions of our evaluation setups in Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: We utilize open-sourced model checkpoints without additional training or finetuning, and employ the complete dataset of CounterAnimal to conduct our experiments. There is no stochastic factor that may appear in our experiments, and thus we do not need to report the error bars for our results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide details our hardware configurations in Appendix C.1. However, we do not list memory and time requirements since our experiments are not computationally demanding. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss the broader impacts of our work in Appendix A, focusing on our impacts for both the research community and the real-world applications. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: To avoid any safety risk, we control the label space for our CounterAnimal dataset and manually cleanse any unsafe image during our data collection procedure. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: For models, only open-sourced checkpoints are adopted, following the licenses from https://github.com/mlfoundations/open_clip/blob/main/LICENSE and https://github.com/pytorch/pytorch/blob/main/LICENSE. The versions of the adopted OpenCLIP checkpoints can be found in Table 6. For scraped data, we only use those photos from iNaturalist following the CC BY-NC license, which are allowed for scientific usage. Please refer to the following link for the details about CC BY-NC: https://creativecommons.org/licenses/by-nc/4.0/. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Our CounterAnimal dataset follows the MIT license. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation flies, to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The CounterAnimal dataset is constructed with human labeling, yet without crowdsourcing. The labeling and flitering procedure adhere strictly to the pipeline outlined in Section 2.1. Our project does not involve any form of compensation or profit. All individuals involved in the manual data curation are from our team, and they all have been notified and agreed to grant consent to the use of the data in this work. this work. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]