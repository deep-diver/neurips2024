{"references": [{"fullname_first_author": "A. v. d. Oord", "paper_title": "Representation learning with contrastive predictive coding", "publication_date": "2018-07-18", "reason": "This paper introduces contrastive predictive coding, a foundational method for self-supervised learning that is heavily referenced in the target paper."}, {"fullname_first_author": "K. He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "publication_date": "2020-06-01", "reason": "MoCo-v3, a key algorithm used in the experiments of the target paper, is directly based on the momentum contrast approach proposed here."}, {"fullname_first_author": "T. Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "publication_date": "2020-06-01", "reason": "SimCLR, another key algorithm in the target paper's experiments, is based on the contrastive learning framework introduced in this paper."}, {"fullname_first_author": "M. Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-01", "reason": "DINO, a third important algorithm used in the target paper's experiments, is based on the self-supervised training of Vision Transformers, as described in this paper."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-20", "reason": "Vision Transformers (ViTs), the neural network architecture at the core of the target paper, are introduced and explored in this highly influential work."}]}