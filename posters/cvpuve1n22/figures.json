[{"figure_path": "CVpuVe1N22/figures/figures_0_1.jpg", "caption": "Figure 1: The importance of information seeking in medical diagnosis. The patient initially only complains of a headache, but by asking the right questions, the doctor uncovers the critical information needed for a correct diagnosis.", "description": "This figure shows a doctor-patient conversation illustrating the importance of information seeking in medical diagnosis.  The patient initially reports only a headache. Through a series of well-placed questions by the doctor, additional crucial details (light sensitivity and a recent head injury) are elicited, leading to a correct diagnosis of post-concussion syndrome. The figure highlights how incomplete initial information necessitates active information gathering to achieve an accurate diagnosis.", "section": "1 Introduction"}, {"figure_path": "CVpuVe1N22/figures/figures_2_1.jpg", "caption": "Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards Ra over past questions, and expected rewards Re capturing expected future gains. The process ends by choosing questions with the highest expected reward.", "description": "This figure illustrates the Uncertainty of Thoughts (UoT) algorithm's three main components.  (a) shows how UoT generates candidate questions and simulates possible future scenarios using an LLM. (b) explains how UoT calculates uncertainty-based rewards for each question, reflecting how much information gain is expected from the answer. (c) describes UoT's reward propagation scheme, which combines accumulated rewards from previous questions with expected future gains to determine the optimal question to ask.  The algorithm iteratively refines its understanding and selects questions that maximize the expected information gain.", "section": "2.2 Uncertainty of Thoughts: Overview"}, {"figure_path": "CVpuVe1N22/figures/figures_8_1.jpg", "caption": "Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards Ra over past questions, and expected rewards Re capturing expected future gains. The process ends by choosing questions with the highest expected reward.", "description": "This figure provides a high-level overview of the Uncertainty of Thoughts (UoT) algorithm.  It illustrates the three main components: 1) Question Generation and Simulation: The model generates candidate questions and simulates the possible outcomes (future scenarios) of asking each question.  2) Uncertainty-based Rewards: A reward is assigned to each question based on how much it reduces the model's uncertainty.  This reward is calculated using information gain from information theory.  3) Reward Propagation: The rewards from the simulation are propagated back through the tree of possibilities to determine the expected reward of asking each question. The question with the highest expected reward is selected.", "section": "2.2 Uncertainty of Thoughts: Overview"}, {"figure_path": "CVpuVe1N22/figures/figures_9_1.jpg", "caption": "Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards Ra over past questions, and expected rewards Re capturing expected future gains. The process ends by choosing questions with the highest expected reward.", "description": "This figure illustrates the Uncertainty of Thoughts (UoT) framework's three main components.  (a) shows the process of generating candidate questions and simulating potential future scenarios using an LLM. (b) details how uncertainty-based rewards are calculated to assess the value of information gain from each question. Finally, (c) demonstrates the reward propagation scheme, which uses accumulated and expected rewards to select the question with the highest potential information gain.", "section": "2.2 Uncertainty of Thoughts: Overview"}, {"figure_path": "CVpuVe1N22/figures/figures_16_1.jpg", "caption": "Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards Ra over past questions, and expected rewards Re capturing expected future gains. The process ends by choosing questions with the highest expected reward.", "description": "This figure provides a high-level overview of the Uncertainty of Thoughts (UoT) algorithm. It consists of three main components:\n\n(a) **Question Generation and Simulation:** The LLM generates candidate questions and simulates possible future scenarios (the outcomes of asking a question), creating a tree of possibilities.\n\n(b) **Uncertainty-based Rewards:**  The algorithm assigns a reward to each question based on how much uncertainty reduction it is expected to bring. The reward reflects information gain.\n\n(c) **Reward Propagation Scheme:** The algorithm propagates these rewards through the tree, calculating the accumulated reward (total reward over all possible paths) and the expected reward (the average reward over all possible paths) for each question. The question with the highest expected reward is selected.", "section": "2.2 Uncertainty of Thoughts: Overview"}, {"figure_path": "CVpuVe1N22/figures/figures_17_1.jpg", "caption": "Figure 5: Curve of uncertainty based reward on Eq 10, where pA can be replaced by (1 \u2013 pN). The horizontal axis pA is conditional probabilities of affirmative at node v, which are introduced in Section \u00a72.4.", "description": "This figure shows the curve of the uncertainty-based reward function (Ru(v)) used in the UoT algorithm.  The reward is a function of pA, the conditional probability of an affirmative answer at a given node in the simulation tree. The curve peaks at pA = 0.5, indicating that questions which lead to a roughly even split between affirmative and negative answers receive the highest reward.  This reflects the algorithm's goal of maximizing information gain by reducing uncertainty.", "section": "2.4 Uncertainty-Based Reward Calculation"}]