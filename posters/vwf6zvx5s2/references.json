{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to the Vision Transformer used in this work."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-04-01", "reason": "This paper introduced LoRA, a parameter-efficient technique crucial to the proposed EMTAL method's efficiency."}, {"fullname_first_author": "Robert A Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper introduced Mixture of Experts (MoE), a fundamental concept for the MoEfied LoRA component in EMTAL."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduced the Vision Transformer, providing the base model for the multi-task learning approach."}, {"fullname_first_author": "Jiaqi Ma", "paper_title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts", "publication_date": "2018-08-01", "reason": "This paper introduced the Multi-gate Mixture-of-Experts (MMoE) architecture, which is relevant to the MoE approach in EMTAL."}]}