[{"figure_path": "VWf6ZVx5S2/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of the top-1 accuracy (%) on the Multi-task FGVC benchmark, by using ViT-B/16 supervised pre-trained on ImageNet-21K. \u2018FT\u2019 denotes \u2018Full Fine-tuning\u2019. The best results are highlighted in bold and the second best ones are underlined.", "description": "This table compares the performance of various multi-task learning (MTL) methods on the Multi-task FGVC benchmark using a Vision Transformer (ViT-B/16) model pre-trained on ImageNet-21K.  It contrasts different baselines (separate and unified full fine-tuning) with several state-of-the-art gradient-based and loss-based MTL approaches, as well as other efficient multi-task learners. The key metric is top-1 accuracy across four fine-grained visual classification tasks (CUB-200-2011, Stanford Cars, FGVC-Aircraft, Oxford Flowers).  The table also lists the number of tunable parameters and inference time for each method, showcasing the trade-off between accuracy and efficiency.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_7_1.jpg", "caption": "Table 2: Comparison results (%) with the state-of-the-art PEFT and MTL methods on Specialized VTAB-1k by using ViT-B/16 models supervised pre-trained on ImageNet-21K. \u2018FT\u2019 denotes \u2018Full Fine-tuning\u2019. The best results are highlighted in bold and the second best is underlined.", "description": "This table compares the performance of the proposed EMTAL method with several state-of-the-art parameter-efficient fine-tuning (PEFT) and multi-task learning (MTL) methods on the Specialized VTAB-1k benchmark.  The results show the top-1 accuracy (%) for each method on four different tasks (Patch Camelyon, EuroSAT, Resisc45, Retinopathy), as well as the average performance across all tasks.  The table also indicates whether each method uses a unified model and the number of tunable parameters (in millions).  The best and second-best performing methods for each task are highlighted in bold and underlined respectively, indicating the superior performance of EMTAL across multiple tasks.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_7_2.jpg", "caption": "Table 3: More evaluation results on NYUv2 with ViT-B/16. The best results are highlighted in bold.", "description": "This table presents the performance comparison of the proposed EMTAL method against the baseline method (TaskPrompter-Base) on the NYUv2 dataset for four different tasks: semantic segmentation (Semseg mIoU), depth estimation (Depth RMSE), normal estimation (Normal mErr), and boundary detection (Boundary odsF).  The results show that adding EMTAL to the baseline model improves performance across all four tasks, leading to a 1.57% average improvement.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_8_1.jpg", "caption": "Table 4: Ablation results (%) of the main components on the Multi-task FGVC by using ViT-B/16 backbone. The best results are highlighted in bold.", "description": "This table presents the ablation study results on the Multi-task FGVC benchmark using a ViT-B/16 backbone. It shows the impact of the two main components of the proposed EMTAL method: MoEfied LoRA and Quality Retaining.  Each row represents a different combination of including or excluding these components. The table reports the top-1 accuracy for four datasets (CUB-200-2011, Stanford Cars, FGVC-Aircraft, Oxford Flowers) as well as the average performance across them. The number of tunable parameters is also shown, demonstrating the efficiency of the model.  The results highlight that both components contribute to improved performance.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_8_2.jpg", "caption": "Table 1: Comparison of the top-1 accuracy (%) on the Multi-task FGVC benchmark, by using ViT-B/16 supervised pre-trained on ImageNet-21K. \u2018FT\u2019 denotes \u2018Full Fine-tuning\u2019. The best results are highlighted in bold and the second best ones are underlined.", "description": "This table compares the performance of different multi-task learning methods on the Multi-task FGVC benchmark using a Vision Transformer (ViT-B/16) model pre-trained on ImageNet-21K.  It contrasts several baselines (separate and union full fine-tuning, gradient-based and loss-based multi-task optimization methods), as well as other efficient multi-task learners.  The table shows top-1 accuracy for each of four datasets (CUB-200-2011, Stanford Cars, FGVC-Aircraft, Oxford Flowers), the mean accuracy across all four datasets, the number of tunable parameters (in millions), and the inference time (in milliseconds).  The best and second-best performing methods are highlighted.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_8_3.jpg", "caption": "Table 6: Ablation results (%) of the proposed MoEfied LoRA with different numbers of clusters (i.e., k) and distinct ways to construct experts.", "description": "This table presents the ablation study on the impact of different numbers of clusters (k) and expert construction methods on the performance of the proposed MoEfied LoRA.  It shows that the optimal number of clusters is 16, resulting in the highest mean accuracy (90.27).  The table also compares three different expert construction methods: co-activation, gradient-cluster, and the authors' proposed method, with the latter achieving the best performance.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_18_1.jpg", "caption": "Table 2: Comparison results (%) with the state-of-the-art PEFT and MTL methods on Specialized VTAB-1k by using ViT-B/16 models supervised pre-trained on ImageNet-21K. \u2018FT\u2019 denotes \u2018Full Fine-tuning\u2019. The best results are highlighted in bold and the second best is underlined.", "description": "This table compares the performance of the proposed EMTAL method with other state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) and Multi-Task Learning (MTL) methods on the Specialized VTAB-1k dataset.  It uses a ViT-B/16 model pre-trained on ImageNet-21K. The table shows the top-1 accuracy for each method on four sub-tasks within VTAB-1k (Patch Camelyon, EuroSAT, Resisc45, and Retinopathy), along with the mean accuracy across all tasks and the number of tunable parameters (in millions).  The best and second-best results for each task are highlighted in bold and underlined, respectively.", "section": "3 Experimental Results and Analysis"}, {"figure_path": "VWf6ZVx5S2/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of the top-1 accuracy (%) on the Multi-task FGVC benchmark, by using ViT-B/16 supervised pre-trained on ImageNet-21K. \u2018FT\u2019 denotes \u2018Full Fine-tuning\u2019. The best results are highlighted in bold and the second best ones are underlined.", "description": "This table compares the top-1 accuracy of various multi-task learning methods on the Multi-task FGVC benchmark.  It uses the ViT-B/16 model pre-trained on ImageNet-21K. The table categorizes methods into baseline, gradient-based MTO, loss-based MTO, and efficient multi-task learners. For each method, it presents the accuracy for each of the four datasets in the benchmark (CUB-200-2011, Stanford Cars, FGVC-Aircraft, Oxford Flowers), the mean accuracy across datasets, the number of tunable parameters (in millions), and the inference time (in milliseconds). The best and second-best results are highlighted.", "section": "3 Experimental Results and Analysis"}]