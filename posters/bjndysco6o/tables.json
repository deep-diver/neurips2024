[{"figure_path": "BJndYScO6o/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD)", "description": "This table summarizes the key differences between Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD).  MBD leverages model information to estimate the score function and uses Monte Carlo score ascent to refine samples, while MFD learns the score function from data and uses reverse stochastic differential equations (SDEs). The table highlights differences in the target distribution, objective, score approximation method, and backward process.", "section": "4 Model-Based Diffusion"}, {"figure_path": "BJndYScO6o/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD)", "description": "This table compares Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD) methods across several key aspects. It highlights the differences in how they approach the target distribution, the objective of the sampling process, the score approximation technique used, and the nature of the backward diffusion process. MBD leverages model information for score approximation and aims to sample from high-likelihood regions of the target distribution, while MFD learns the score function from data and relies on reverse stochastic differential equations to move samples towards the data distribution. The table effectively summarizes the key distinctions between the model-based and model-free approaches to diffusion-based trajectory optimization.", "section": "4 Model-Based Diffusion"}, {"figure_path": "BJndYScO6o/tables/tables_8_1.jpg", "caption": "Table 2: Reward of different methods on non-continuous tasks. *RL requires offline training and generate a closed-loop policy so it is not an apple-to-apple baseline.", "description": "This table compares the performance of different trajectory optimization methods on several non-continuous control tasks.  The methods compared include CMA-ES, CEM, MPPI, a reinforcement learning (RL) approach, and the proposed Model-Based Diffusion (MBD) method.  The table shows the average reward achieved by each method on each task. The RL method is marked with an asterisk (*) to indicate that it uses offline training and a closed-loop policy, making it not directly comparable to the other methods, which are model-free.", "section": "5 Experimental Results"}, {"figure_path": "BJndYScO6o/tables/tables_8_2.jpg", "caption": "Table 3: Computational time of different methods on non-continuous tasks.", "description": "This table presents the computational time required by different trajectory optimization methods (CMA-ES, CEM, MPPI, RL, and MBD) to solve several non-continuous control tasks. The results highlight the efficiency of model-based diffusion (MBD) compared to other approaches, particularly reinforcement learning (RL).", "section": "5 Experimental Results"}, {"figure_path": "BJndYScO6o/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD)", "description": "This table highlights the key differences between Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD).  MBD leverages model information to estimate the score function, while MFD learns it from data. MBD uses Monte Carlo score ascent to quickly move samples to high-density regions, whereas MFD uses reverse stochastic differential equations to maintain sample diversity.  MBD assumes a known target distribution, which is different from MFD.", "section": "4 Model-Based Diffusion"}, {"figure_path": "BJndYScO6o/tables/tables_20_1.jpg", "caption": "Table 4: MBD hyperparameters for various tasks", "description": "This table shows the hyperparameters used for the Model-Based Diffusion (MBD) algorithm across different tasks.  It specifies the horizon length (number of time steps considered in each optimization iteration), the number of samples used to estimate the score function at each step, and the temperature parameter (\u03bb), which controls the exploration-exploitation tradeoff in the diffusion process.", "section": "4.2 Model-based Diffusion for Trajectory Optimization"}, {"figure_path": "BJndYScO6o/tables/tables_21_1.jpg", "caption": "Table 5: General RL configuration for various environments", "description": "This table shows the hyperparameters used for training reinforcement learning (RL) agents in different robotic control tasks.  The table lists the environment, the RL algorithm used (PPO or SAC), the number of timesteps for training, the reward scaling factor applied, and the length of each episode.", "section": "5 Experimental Results"}, {"figure_path": "BJndYScO6o/tables/tables_21_2.jpg", "caption": "Table 6: RL specifics for various environments", "description": "This table lists the hyperparameters used for training reinforcement learning (RL) agents in various simulated robotic environments.  The hyperparameters include the minibatch size, the number of updates per batch, the discount factor, and the learning rate. These settings are crucial for achieving optimal performance in each environment and are specific to the RL algorithms used in the paper.", "section": "5 Experimental Results"}, {"figure_path": "BJndYScO6o/tables/tables_21_3.jpg", "caption": "Table 7: Online Running Frequency of receding horizon MBD", "description": "This table shows the online running frequency of the receding horizon version of Model-Based Diffusion (MBD) for various locomotion tasks.  The frequency is measured in Hertz (Hz) and represents how many times per second the MBD algorithm can replan a trajectory using a receding horizon approach.  The values indicate the computational efficiency of MBD for online control.", "section": "5 Experimental Results"}]