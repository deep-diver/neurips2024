[{"type": "text", "text": "Model-Based Diffusion for Trajectory Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chaoyi $\\mathbf{Pan}^{*}$ , Zeji $\\mathbf{Y}\\mathbf{i}^{*}$ , Guanya $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{\\dagger}$ , Guannan $\\mathbf{Q}\\mathbf{u}^{\\dagger}$ Carnegie Mellon University {chaoyip,zejiy,guanyas,gqu}@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as modelbased diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD\u2019s ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes: https://lecar-lab.github.io/mbd/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trajectory optimization (TO) aims to optimize the state and control sequence to minimize a cost function while subject to specified dynamics and constraints. Given nonlinear, non-smooth dynamics and non-convex objectives and constraints, traditional optimization methods like gradient-based methods and interior point methods are less effective in solving TO problems. In response, diffusion models have emerged as a powerful tool for trajectory generation in complex dynamical systems due to their expressiveness and scalability [12, 54, 34, 33, 40, 5]. ", "page_idx": 0}, {"type": "image", "img_path": "BJndYScO6o/tmp/3574a484b85d989711a40dfa8827c6400107ac5ec563529a606924b9a085834d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Model-based diffusion in humanoid running tasks Figure 1: MBD refines the trajectory by leveraging the dynamics model directly without relying on demonstration data. ", "page_idx": 0}, {"type": "text", "text": "Although diffusion models excel when learning from large-scale, high-dimensional, and high-quality demonstrations, their dependency on such data limits their practicality. For example, after training a manipulation task with a specific robotic arm, the model may struggle to generalize to new tasks with a different arm as the underlying dynamics change. This limitation arises from the model-free nature of existing diffusion-based methods, which do not leverage readily available model information to enhance adaptability. Moreover, existing diffusion-based approaches often require high-quality (in terms of optimality and feasibility) demonstration data, which limits their applications in various scenarios with imperfect data, such as dynamically infeasible trajectories (e.g., generated by high-level planners using simplified models) and partial demonstrations (e.g., lower-body-only demonstrations for a high-dimensional humanoid). ", "page_idx": 1}, {"type": "text", "text": "Fortunately, unlike diffusion model\u2019s applications in vision or language where data is from unknown distributions (e.g., internet-scale image data), in trajectory optimization, we often know the distribution of desired trajectories, which is described by the optimization objectives, constraints, and the underlying dynamics model, although such a distribution is intractable to directly sample from. Diffusion models offer a tantalizing new perspective, by iteratively refining samples from isotropic Gaussians to meaningful desired distributions in manageable steps, rather than directly learning the complex desired distribution. Inspired by this, we propose Model-Based Diffusion (MBD) that utilizes model information to approximate the gradient of the log probability density function (a.k.a. score function) and uses it to iteratively refine sampled trajectories to solve TO problems, as depicted in Fig. 1. This model-centric strategy allows for the generation of dynamically feasible trajectories in a data-free manner, and gradually moves them towards more optimal solutions. Furthermore, by using demonstrations as observations of the target distribution, MBD can be smoothly combined with data of different qualities to steer the diffusion process and enhance its effectiveness. Particularly, we merge the demonstration data into the sampling process by evaluating their likelihoods with the model and use them to improve the estimation of the score function. Our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the Model-Based Diffusion (MBD) framework for trajectory optimization, utilizing the dynamics model to estimate the score function. This enables an effective trajectory planner given non-smooth dynamics and non-convex objectives, such as contact-rich manipulation tasks or high-dimensional humanoids. \u2022 Our analysis and empirical evaluations demonstrate that MBD matches, and often exceeds, the performance of existing reinforcement learning and sampling-based TO methods. In particular, MBD outperforms PPO by $59\\%$ in various tasks within tens of seconds of diffusing. \u2022 We demonstrate MBD\u2019s flexibility in utilizing diverse imperfect data to steer the diffusion process and further enhance the performance. Specifically, the resulting whole-body humanoid trajectory from MBD is more natural by utilizing the lower-body-state-only human motion data. Similarly, MBD can effectively address long-horizon sparse-reward Umaze navigation tasks by leveraging infeasible demonstrations generated by an RRT planner with simplified dynamics. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Models. Diffusion models have been widely adopted as generative models for highdimensional data, such as image [51], audio [13], and text [8] through iterative refinement processes [50, 28]. The backward process can be viewed as gradient prediction [52] or score matching [53], which learns the score function to move samples towards the data distribution. We deliver new methods to perform the backward diffusion process using the available model information. ", "page_idx": 1}, {"type": "text", "text": "Sampling-based Optimization. Optimization involving black-box functions is widely applied across various fields, including hyperparameter tuning and experimental design [49, 27]. Evolutionary algorithms like CMA-ES are often used to tackle black-box optimization problems, dynamically modifying the covariance matrix to produce new samples [24]. Such problems can also be efficiently addressed within the Bayesian optimization framework [50, 19], which offers greater efficiency. Nonetheless, traditional BO algorithms are generally restricted to low-dimensional problems. ", "page_idx": 1}, {"type": "text", "text": "Trajectory Optimization. Traditionally, trajectory optimization (TO) is solved using gradientbased optimization, which faces challenges such as non-convex problem structures, nonlinear or discontinuous dynamics, and high-dimensional state and control action spaces. As two equivalent formulations, direct methods [25] and shooting-based methods [29] are commonly used to solve TO problems, where gradient-based optimizers such as Augmented Lagrangian [32], Interior Point [36], and Sequential Quadratic Programming [3, 48] are employed. To leverage the parallelism of modern hardware and improve global convergence properties, sampling-based methods like Cross-Entropy Motion Planning (CEM) [37] and Model Predictive Path Integral (MPPI) [58, 62] have been proposed to solve TO by sampling from target distributions. To solve stochastic optimal control problems, trajectory optimization has also been framed as an inference problem in a probabilistic graphical model, where system dynamics defines the graph structure [35, 39]. This perspective extends methods such as iLQG by integrating approximate inference techniques to improve trajectory optimization [55]. The connection between diffusion and optimal control has been explored in [10], which motivates us to use diffusion models as solvers for trajectory optimization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Diffusion for Planning. Diffusion-based planners have been used to perform human motion generation [12, 54] and multi-agent motion prediction [34]. Diffusion models are capable of generating complete trajectories by folding both dynamics and optimization processes into a single framework, thus mitigating compounding errors and allowing flexible conditioning [33, 40, 5]. In addition, they have been adeptly applied to policy generation, enhancing the capability to capture multimodal demonstration data in high-dimensional spaces for long-horizon tasks [46, 15]. These works assume no access to the underlying dynamics, limiting the generalization to new environments. To enforce dynamics constraints, SafeDiffuser [60] integrates control barrier functions into the learned diffusion process, while Diffusion-CCSP [61] composes the learned geometric and physical conditions to guarantee constraint compliance. Our approach uses diffusion models directly as solvers, rather than simply distilling solutions from demonstrations. ", "page_idx": 2}, {"type": "text", "text": "Langevin-based Markov Chain Monte Carlo for Global Optimization. Gradient-based sampling algorithms have been widely used in global optimization, where the energy function $J$ is optimized by sampling from the Boltzmann distribution $p\\propto\\exp(-\\frac{J}{\\lambda})$ [57, 43]. By annealing the temperature $\\lambda$ to zero, the sampling process converges to t h\u221de glob(a\u2212l m)inimum of the energy function $J$ [31]. The convergence of Langevin-based MCMC methods has been well studied in both continuous and discretized settings [16, 21], showing that the distribution will converge in probability to the target distribution with certain decreasing schedule of the step size and temperature $\\lambda$ . In practice, the most common Langevin-based MCMC methods are unadjusted Langevin Monte Carlo (ULMC) [17] and Underdamped Langevin Monte Carlo (UdLMC) [14], with convergence rates of $\\begin{array}{r}{O\\big(\\frac{1}{\\epsilon^{2}}\\log\\bigl(\\frac{1}{\\epsilon}\\bigr)\\big)}\\end{array}$ and $O(\\frac{1}{\\epsilon})$ given strongly convex and smooth energy functions, respectively. Recently, Langevin -based M(C M)C methods have been integrated into diffusion processes to improve global convergence and sampling efficiency [30, 6], where the score function is estimated by Monte Carlo to accelerate and stabilize the diffusion process. Our work differs from these methods in that we aim to sample from the high-probability region of the target distribution without assuming access to the gradient of the energy function, and without assuming the energy function is smooth or convex. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Statement and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations: We use lower (upper) scripts to specify the time (diffusion) step, e.g., $x_{t},u_{t},y_{t}$ represent the state, control and state-control pair at time $t$ , and $Y^{(i)}$ represents the diffusion state at step $i$ . ", "page_idx": 2}, {"type": "text", "text": "This paper focuses on a class of trajectory optimization (TO) problems whose objective is to find the sequences $\\left\\{x_{t}\\right\\}$ and $\\left\\{u_{t}\\right\\}$ that minimize the cost function $J(x_{1:T};u_{1:T})$ subject to the dynamics and constraints. The optimization problem 1 can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{x_{1:T},u_{1:T}}J\\big(x_{1:T};u_{1:T}\\big)=l_{T}\\big(x_{T}\\big)+\\displaystyle\\sum_{t=0}^{T-1}l_{t}\\big(x_{t},u_{t}\\big)}&{}\\\\ {\\mathrm{s.t.}\\quad x_{0}=x_{\\mathrm{init}}}&{}\\\\ {\\displaystyle x_{t+1}=f_{t}\\big(x_{t},u_{t}\\big),\\quad\\forall t=0,1,\\ldots,T-1,}\\\\ {\\displaystyle g_{t}\\big(x_{t},u_{t}\\big)\\leq0,\\quad\\forall t=0,1,\\ldots,T-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{t}\\in\\mathbb{R}^{n_{x}}$ and $u_{t}\\in\\mathbb{R}^{n_{u}}$ are the state and control at time $t$ , $f_{t}:\\mathbb{R}^{n_{x}}\\times\\mathbb{R}^{n_{u}}\\rightarrow\\mathbb{R}^{n_{x}}$ represents the dynamic s\u2208, $g_{t}:\\mathbb{R}^{n_{x}}\\times\\mathbb{R}^{n_{u}}\\rightarrow\\mathbb{R}^{n_{g}}$ are the constraint functions and \u2236 $l_{t}:\\mathbb{R}^{n_{x}}\\times\\mathbb{R}^{n_{u}}\\rightarrow\\mathbb{R}$ are the stage costs. We use $Y=\\left[x_{1:T};u_{1:T}\\right]$ to denote all decision variables. Traditionally, TO is solved using nonlinear progra m=m [ing, which  ]faces challenges such as non-convex problem structures, nonlinear or discontinuous dynamics, and high-dimensional state and control action spaces. Recently, there has been a growing interest in bypassing these challenges by directly generating samples from the optimal trajectory distribution using diffusion models trained on optimal demonstration data [12, 40, 46, 61]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To use diffusion for TO, (1) is first transformed into a sampling problem. The target distribution $p_{0}(Y^{(0)})$ is proportional to dynamical feasibility $\\begin{array}{r}{p_{d}(Y)\\propto\\prod_{t=1}^{T}\\mathbf{1}\\big(x_{t}=f_{t-1}\\big(x_{t-1},u_{t-1}\\big)\\big)}\\end{array}$ , optimality $p_{J}(Y)\\propto\\exp{\\left(-\\frac{J(Y)}{\\lambda}\\right)}$ and the constraints $\\begin{array}{r}{p_{g}(Y)\\propto\\prod_{t=1}^{T}\\mathbf{1}(g_{t}(x_{t},u_{t})\\leq0)}\\end{array}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{0}(Y)\\propto p_{d}(Y)p_{J}(Y)p_{g}(Y)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Obtaining the solution $Y^{*}$ from the TO problem in Eq. (1) is equivalent to sampling from Eq. (2) given a low temperature $\\lambda\\to0$ . In fact, in Appendix A.2, we prove that the distribution of $J(Y)$ with $Y\\sim p_{0}(\\cdot)$ converges i n \u2192probability to the optimal value $J^{*}$ as $\\lambda\\to0$ , under mild assumpti(on s). Howev e\u223cr, it (i\u22c5s) generally difficult to directly sample from the high-d i\u2192mensional and sparse target distribution $p_{0}(\\bar{\\cdot})$ . To address this issue, the diffusion process iteratively refines the samples following a backward process, which reverses a predefined forward process as shown in Fig. 1. The forward process corrupts the original distribution $p_{0}(\\cdot)$ to an isotropic Gaussian $p_{N}(\\cdot)$ by incrementally adding small noise to it and scaling it down(\u22c5 )by $\\sqrt{\\alpha_{i}}$ to maintain an invaria(\u22c5n)t noise covariance (see Fig. 2(b) for an example). Mathematically, this means we iteratively obtain $Y^{(i)}\\sim p_{i}(\\cdot)$ with $p_{i|i-1}(\\cdot|Y^{(i-1)})\\sim\\mathcal{N}(\\sqrt{\\alpha_{i}}Y^{(i-1)},(1-\\alpha_{i})I)$ . Because the noise at each time step is inde\u223cpend(e\u22c5)nt, the conditional distribution of $Y^{(i)}|Y^{(i-1)}$ also leads to that of $Y^{(i)}|Y^{(0)}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{i|0}(\\cdot|Y^{(0)})\\sim\\mathcal{N}(\\sqrt{\\bar{\\alpha}_{i}}Y^{(0)},(1-\\bar{\\alpha}_{i})I),\\quad\\bar{\\alpha}_{i}=\\prod_{k=1}^{i}\\alpha_{k}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The backward process $p_{i-1|i}(Y^{(i-1)}|Y^{(i)})$ is the reverse of the forward process $p_{i|i-1}(Y^{(i)}|Y^{(i-1)})$ , which removes the noise from the corrupted distribution $p_{N}(\\cdot)$ to obtain the target distribution $p_{0}(\\cdot)$ . The target distribution $p_{0}(\\cdot)$ in the diffusion process is given( \u22c5b)y: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p_{i-1}(Y^{(i-1)})=\\int p_{i-1|i}(Y^{(i-1)}|Y^{(i)})p_{i}(Y^{(i)})\\,d Y^{(i)},}}\\\\ &{}&{p_{0}(Y^{(0)})=\\int p_{N}(Y^{(N)})\\prod_{i=N}^{1}p_{i-1|i}(Y^{(i-1)}|Y^{(i)})d Y^{(1:N)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Standard diffusion models [33, 40, 61], which we refer to as Model-Free Diffusion (MFD), solve the backward process by learning score function merely from data. In contrast, we propose leveraging the dynamics model to estimate the score to improve the generalizability of the model and allow a natural integration with diverse quality data. ", "page_idx": 3}, {"type": "text", "text": "4 Model-Based Diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we formally introduce our MBD algorithm that leverages model information to perform backward process. To streamline the discussion, in Section 4.1, we first present MBD with Monte Carlo score ascent to solve simplified and generic unconstrained optimization problems. In Section 4.2, we extend MBD to the constrained optimization setting to solve the TO problem given complex dynamics and constraints. Lastly, in Section 4.3, we augment the MBD algorithm with demonstrations to improve sample quality and steer the diffusion process. ", "page_idx": 3}, {"type": "text", "text": "4.1 Model-based Diffusion as Multi-stage Optimization ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "BJndYScO6o/tmp/eda36ba1ceb2660dd584482d8d7b30ea915f95f8d19cb14348438b42547a4c7d.jpg", "img_caption": ["Figure 2: Reverse SDE vs. Monte Carlo score ascent (MCSA) on a synthetic highly non-convex objective function. (a) Synthesized objective function with multiple local minima. (b) The intermediate stage density $p_{i}(\\cdot)$ , where peaked $\\bar{p_{0}(\\cdot)}$ is iteratively corrupted to a Gaussian $p_{N}(\\cdot)$ . (c) Reverse SDE vs. MCSA: Bac(k\u22c5)ground colors repr(e\u22c5s)ent the density of $p_{i}(\\cdot)$ at different stage(s\u22c5. )MCSA converges faster due to larger step size and lower sampling noise whil(e\u22c5 )still capturing the multimodality. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We consider the reverse process for an unconstrained optimization problem minY $J(Y)$ , where the target distribution is $\\begin{array}{r}{p_{0}(Y^{(0)})\\propto\\exp(-\\frac{J(Y^{(0)})}{\\lambda})}\\end{array}$ . In our MBD framework, \u201cmode(l\u201d  i)mplies that we can evaluate $J(Y^{(0)})$ for arbitrary $Y^{(0)}$ , enabling us to compute the target distribution up to the normalizing consta(nt. ", "page_idx": 4}, {"type": "text", "text": "MBD uses Monte Carlo score ascent instead of the commonly adopted reverse SDE approach in MFD. Specifically, when denoising from $i$ to $i-1$ , MBD performs one step of gradient ascent on log pi Y (i)  and then scales the sample by the f a\u2212ctor\u221a1\u03b1i as defined in the forward process: ", "page_idx": 4}, {"type": "equation", "text": "$$\nY^{(i-1)}=\\frac{1}{\\sqrt{\\alpha_{i}}}\\left(Y^{(i)}+\\big(1-\\bar{\\alpha}_{i}\\big)\\nabla_{Y^{(i)}}\\log p_{i}\\big(Y^{(i)}\\big)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Critically, with the model-informed $p_{0}(Y^{(0)})$ , we can estimate the score function $\\nabla_{Y^{(i)}}\\log p_{i}(Y^{(i)})$ by connecting $p_{i}(Y^{(i)})$ to $p_{0}(Y^{(0)})$ via Bayes\u2019 rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{Y^{(i)}}\\log p_{i}(Y^{(i)})=\\frac{\\nabla_{Y^{(i)}}\\int p_{i0}(Y^{(i)}\\mid Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}{\\int p_{i0}(Y^{(i)}\\mid Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}=\\frac{\\int\\nabla_{Y^{(i)}}p_{i0}(Y^{(i)}\\mid Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}{\\int p_{i0}(Y^{(i)}\\mid Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\frac{\\int-\\frac{Y^{(i)}-\\sqrt{\\alpha_{i}}Y^{(0)}}{1-\\bar{\\alpha}_{i}}p_{i|0}\\bigl(Y^{(i)}\\mid Y^{(0)}\\bigr)p_{0}\\bigl(Y^{(0)}\\bigr)\\,d Y^{(0)}}{\\int p_{i|0}\\bigl(Y^{(i)}\\mid Y^{(0)}\\bigr)p_{0}\\bigl(Y^{(0)}\\bigr)\\,d Y^{(0)}}}}\\\\ {{\\displaystyle=-\\,\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\frac{\\int Y^{(0)}p_{i|0}\\bigl(Y^{(i)}\\,\\big|\\,Y^{(0)}\\bigr)p_{0}\\bigl(Y^{(0)}\\bigr)\\,d Y^{(0)}}{\\int p_{i|0}\\bigl(Y^{(i)}\\,\\big|\\,Y^{(0)}\\bigr)p_{0}\\bigl(Y^{(0)}\\bigr)\\,d Y^{(0)}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Between Eq. (7a) and Eq. (7b), we use the forward Gaussian density in Eq. (3): $p_{i|0}(Y^{(i)}\\mid Y^{(0)})\\propto$ exp 1(Y (i)\u2212\u221a\u03b1\u00afiY (0))\u22ba(Y (i)\u2212\u221a\u03b1\u00afiY (0)) . $\\begin{array}{r}{-\\frac{1}{1-\\bar{\\alpha}_{i}}(Y^{(i)}-\\sqrt{\\bar{\\alpha}_{i}}Y^{(0)})p_{i|0}(Y^{(i)}\\mid Y^{(0)})}\\end{array}$ t.s  loGgiv-leink $Y^{(0)}$ o da s gtrhaed iienntte igsr $\\nabla_{Y^{(i)}}p_{i|0}(Y^{(i)}\\ |\\ Y^{(0)})\\ =$ $p_{i|0}(Y^{(i)}\\mid Y^{(0)})$ is evaluated as a function of parameterized by $Y^{(i)}$ . Based on that, we define the function $\\phi_{i}(Y^{(0)})$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{i}(Y^{(0)})\\propto p_{i[0}(Y^{(i)}\\mid Y^{(0)})\\propto\\exp(-\\frac{1}{2}\\frac{\\left(Y^{(0)}-\\frac{Y^{(i)}}{\\sqrt{\\bar{\\alpha}_{i}}}\\right)^{\\top}\\left(Y^{(0)}-\\frac{Y^{(i)}}{\\sqrt{\\bar{\\alpha}_{i}}}\\right)}{\\frac{1-\\bar{\\alpha}_{i}}{\\bar{\\alpha}_{i}}})\\propto\\mathcal{N}(\\frac{Y^{(i)}}{\\sqrt{\\bar{\\alpha}_{i}}},\\frac{I}{\\bar{\\alpha}_{i}}-I)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This finding enables the Monte-Carlo estimation for computing the score function. We collect a batch of samples from $\\phi_{i}(\\cdot)$ which we denote as ${\\boldsymbol{\\mathcal{V}}}^{(i)}$ and approximate the score as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{Y^{(i)}}\\log p_{i}(Y^{(i)})=-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\frac{\\displaystyle\\int Y^{(0)}\\phi_{i}(Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}{\\displaystyle\\int\\phi_{i}(Y^{(0)})p_{0}(Y^{(0)})\\,d Y^{(0)}}}}\\\\ {{\\displaystyle\\approx-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\,\\frac{\\sum_{Y^{(0)}\\in\\mathcal{Y}^{(i)}}Y^{(0)}p_{0}(Y^{(0)})}{\\displaystyle\\sum_{Y^{(0)}\\in\\mathcal{Y}^{(i)}}p_{0}(Y^{(0)})}:=-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\bar{Y}^{(0)}(\\mathcal{Y}^{(i)})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "BJndYScO6o/tmp/9834981f1f2422d1e3124d10ce04acbc5762fac915dea95f9a38e69376567bd5.jpg", "table_caption": ["M        o      n     t     e           C       a     r    l    o    A p   p     r      o      x     i      m        a    t    i     o      n "], "table_footnote": ["Table 1: Comparison of Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD) "], "page_idx": 5}, {"type": "text", "text": "Comparison between MFD and MBD. Table 1 highlights the key differences between MBD and MFD, which originate from two assumptions made in MBD: (a) a known target distribution $p_{0}(Y^{(0)})$ given the model; (b) the objective of sampling $Y^{(0)}$ from the high-likelihood region of $p_{0}(Y^{(0)})$ to minimize the cost function. For (a), MBD leverages $p_{0}$ to estimate the score following Eq. (9a), whereas MFD learns that from the data. For (b), MBD runs Monte Carlo score ascent in Eq. (6) to quickly move the samples to the high-density region as depicted in Fig. 2(c), while MFD runs reverse SDE $\\begin{array}{r}{Y^{(i-1)}\\,=\\,\\frac{\\cdot}{\\sqrt{\\alpha_{i}}}\\left(Y^{(i)}+\\frac{1-\\alpha_{i}}{2}\\nabla_{Y^{(i)}}\\bar{\\log{p_{i}}}(Y^{(i)})\\right)\\bar{+}\\,\\sqrt{1-\\alpha_{i}}\\mathbf{z}_{i}^{-}}\\end{array}$ , where $\\mathbf{z}_{i}$ is Gaussian noise, to maintain the sampl e diversity. Given low temperature $\\lambda$ , it can be shown that $\\begin{array}{r}{\\nabla\\log p_{i}(Y^{(i)})\\approx-\\frac{1}{(1-\\bar{\\alpha}_{i})}\\big(Y^{(i)}-\\arg\\operatorname*{max}p_{i}(\\cdot)\\big)^{\\frac{1}{2}}}\\end{array}$ , i.e., the function $\\log p_{i}(Y^{(i)})$ is (1\u22121\u03b1\u00afi)-smooth. Therefore, choosing the step size $\\left(1-\\bar{\\alpha}_{i}\\right)$ in Eq. (6) is considered optimal, as for $L$ -smooth functions, $\\begin{array}{r}{O(\\frac{1}{L})}\\end{array}$ is the step size that achieves the fastest convergence [64]. ", "page_idx": 5}, {"type": "text", "text": "How diffusion helps? The diffusion process plays an important role in helping Monte Carlo score ascent overcome the local minimum issue in highly non-convex optimization problems, as shown in Fig. 2(a). Compared with optimizing a highly non-convex objective, Monte Carlo score ascent is applied to the intermediate distribution $\\begin{array}{r}{p_{i}(\\bar{\\cdot})\\overset{\\cdot}{=}\\int p_{0}(Y^{(0)})p_{i|0}(\\cdot)d Y^{(0)}}\\end{array}$ , which is made concave by convoluting $p_{0}(\\cdot)$ with a Gaussian distribution $p_{i|0}(\\cdot)$ , as shown in Fig. 2(b). Starting from the strongly concave Gaussian distribution $p_{N}\\sim\\mathcal{N}(\\mathbf{0},I)$ with scale $\\bar{\\alpha}_{N}\\,\\rightarrow\\,0$ , the density is easy to sample. The covariance of the sampling density $\\begin{array}{r}{\\sum_{\\phi_{i}}\\,=\\,\\big(\\frac{1}{\\bar{\\alpha}_{i}}\\,-1\\big)I}\\end{array}$ is large when $i=N$ , implying that we are searching a wide space for global minim a. In t he less-noised stage, the intermediate distribution $p_{i}(\\cdot)$ is more peaked and closer to the target distribution $p_{0}(\\cdot)$ , and $\\bar{\\alpha}_{i}\\to1$ produces a smaller sampli(n\u22c5g) covariance $\\Sigma_{\\phi_{i}}$ to perform a local search. By iteratively (r\u22c5u)nning gr a\u2192dient ascent on the smoothed distribution, MBD can effectively optimize a highly non-convex objective function as presented in Fig. 2. The MBD algorithm is formally depicted in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Connection with Sampling-based Optimization. When diffusion step is set to $N\\,=\\,1$ , MBD effectively reduces to the Cross-Entropy Method (CEM) [11] for optimization. To s e=e this, we can plug the estimated score Eq. (9b) into the Monte Carlo score ascent Eq. (6) and set $N\\;=$ 1: $\\begin{array}{r}{\\dot{Y^{(0)}\\,=\\,\\frac{\\alpha_{1}}{\\alpha_{1}}\\bar{Y}^{(0)}(\\mathcal{Y}^{(1)})\\,=\\,\\bar{Y}^{(0)}(\\mathcal{Y}^{(1)})\\,=\\,\\frac{\\Sigma_{Y^{(0)}\\in\\mathcal{Y}^{(1)}}Y^{(0)}w(Y^{(0)})}{\\sum_{Y^{(0)}\\in\\mathcal{Y}^{(1)}}w(Y^{(0)})}}\\end{array}$ where $w(Y^{(0)})\\;=\\;p_{0}(Y^{(0)})\\;\\propto\\;$ $\\exp\\bigl(-\\frac{J(Y^{(0)})}{\\lambda}\\bigr)$ and $\\begin{array}{r}{\\mathcal{V}^{(1)}\\sim\\mathcal{N}\\big(\\frac{{Y}^{(1)}}{\\alpha_{0}},\\big(\\frac{1}{\\alpha_{0}}-1\\big)I\\big)}\\end{array}$ .  Th\u2208iYs precisely mirrors the update mechanism in CEM, which aims to optimize the objective function $f_{\\mathrm{CEM}}(Y^{(0)})\\;=\\;J(Y^{(0)})$ and determine the sampling covariance $\\begin{array}{r}{\\Sigma_{\\mathrm{CEM}}\\,=\\,\\big(\\frac{1}{\\alpha_{0}}\\,-1\\big)I}\\end{array}$ , thus linking the sampling strategy of CEM with the $\\alpha$ schedule in MBD. The advances th at distinguish MBD from CEM-like methods are (1) the careful scheduling of $\\alpha$ and (2) the intermediate refinements on $p_{i}$ , both following the forward diffusion process. This allows MBD to optimize for smoothed functions in the early stage and gradually refine the solution for the original objective. On the contrary, CEM\u2019s solution could either be biased given a large $\\boldsymbol{\\Sigma}_{\\mathrm{CEM}}$ which overly smoothes the distribution as in $p_{20},p_{100}$ of Fig. 2(b), or stuck in local minima with a small $\\Sigma_{\\mathrm{CEM}}$ as in $p_{1}$ of Fig. 2(b) where the distribution is highly non-concave. ", "page_idx": 5}, {"type": "table", "img_path": "BJndYScO6o/tmp/9bac1d0407dc6ecee57465cd82f29dfc720827b5f5f6aaef3fa5a65b7a6fc05d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Model-based Diffusion for Trajectory Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For TO, we have to accommodate the constraints in Eq. (1) which change the target distribution to $p_{0}(Y^{(0)})\\propto p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})$ . Given that $p_{d}(Y^{(0)})$ is a Dirac delta function that assigns non-zero probability only to dynamically feasible trajectories, sampling from $\\phi_{i}(Y^{(0)})$ could result in low densities. To enhance sampling efficiency, we collect a batch of dynamical(ly fea)sible samples $\\mathcal{V}_{d}^{(i)}$ from the distribution $\\phi_{i}(Y^{(0)})p_{d}(Y^{(0)})$ with model information. Proceeding from Eq. (9a), and incorporating $p_{0}(Y^{(0)})\\propto p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})$ , we show the score function is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla_{Y}(\\imath)\\log p_{i}(Y^{(0)})=\\displaystyle{-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\displaystyle{\\frac{\\int Y^{(0)}\\phi_{i}(Y^{(0)})p_{d}(Y^{(0)})p_{g}(Y^{(0)})p_{f}(Y^{(0)})d Y^{(0)}}{\\int\\phi_{i}(Y^{(0)})p_{d}(Y^{(0)})p_{g}(Y^{(0)})p_{f}(Y^{(0)})}}}\\\\ {{\\displaystyle}}\\\\ {{\\approx-\\displaystyle{\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\displaystyle{\\frac{\\sum_{Y^{(0)}\\in{\\cal y}_{i}^{(i)}}Y^{(0)}p_{f}(Y^{(0)})p_{g}(Y^{(0)})}{\\sum_{Y^{(0)}\\in{\\cal y}_{i}^{(i)}}p_{f}(Y^{(0)})p_{g}(Y^{(0)})}}}\\\\ {{\\displaystyle}}\\\\ {{=-\\displaystyle{\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\displaystyle{\\bar{Y}^{(0)}},}}\\\\ {{\\mathrm{where}~~~\\bar{Y}^{(0)}=\\displaystyle{\\frac{\\sum_{Y^{(0)}\\in{\\cal y}_{i}^{(i)}}Y^{(0)}w(Y^{(0)})}{\\sum_{Y^{(0)}\\in{\\cal y}_{i}^{(i)}}w(Y^{(0)})}},~~w(Y^{(0)})=p_{f}(Y^{(0)})p_{g}(Y^{(0)})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The model plays a crucial role in score estimation by transforming infeasible samples ${\\bf\\nabla}\\b{\\mathscr{V}}^{(i)}$ from Line 3 in Algorithm 2 into feasible ones $\\mathcal{V}_{d}^{(i)}$ . The conversion is achieved by putting the cont rYol part $U=u_{1:T}$ of $Y^{(0)}=\\left[x_{1:T};u_{1:T}\\right]$ into the d yYnamics Eq. (1c) recursively to get the dynamically feasible s a=mples $Y_{d}^{(0)}$ (Li=n e[ 4), which  ]shares the same idea with the shooting method [29] in TO. MBD then evaluates the weight of each sample with $p_{g}(Y^{(0)})p_{J}(Y^{(0)})$ in Line 5. One common limitation of shooting methods is that they could be ineffic(ient f)or lo(ng-ho)rizon tasks due to the combinatorial explosion of the constrained space $\\begin{array}{r}{p_{g}(Y)\\propto\\prod_{t=1}^{T}\\mathbf{1}\\big(g_{t}(x_{t},u_{t})\\le0\\big)}\\end{array}$ , leading to low constraint satisfaction rates. To address this issue, we will intr\u220foduce demonstration-augmented MBD in Section 4.3 to guide the sampling process from the state space to improve sample quality. ", "page_idx": 6}, {"type": "text", "text": "4.3 Model-based Diffusion with Demonstration ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With the ability to leverage model information, MBD can also be seamlessly integrated with various types of data, including imperfect or partial-state demonstrations by modeling them as noisy observations of the desired trajectory $\\bar{p(Y_{\\mathrm{demo}}\\mid Y^{(0)})}\\;\\sim\\;{\\mathcal N}(Y^{(0)},\\sigma^{\\bar{2}}I)$ . Given suboptimal demonstrations, sampling from the posterior $p(Y^{(0)}\\;\\mid\\;Y_{\\mathrm{demo}})\\;\\propto\\;p_{0}(Y^{(0)})p(Y_{\\mathrm{demo}}\\;\\mid\\;Y^{(0)})$ could lead to poor solutions as the demonstration likelihood $p(Y_{\\mathrm{demo}}\\mid Y^{(0)})$ could domi nate the model-based distribution $p_{0}(Y^{(0)})\\propto p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})$ an d\u2223 misle)ad the sampling pro", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Model-based Diffusion for Trajectory Optimization ", "page_idx": 7}, {"type": "text", "text": "1: Input: $Y^{(N)}\\sim\\mathcal{N}(\\mathbf{0},I)$   \n32:: for $i=N$ l te   \n$\\begin{array}{r}{\\bar{\\mathcal{V}}^{(i)}\\sim\\mathcal{N}(\\frac{Y^{(i)}}{\\sqrt{\\bar{\\alpha}_{i-1}}},\\big(\\frac{1}{\\bar{\\alpha}_{i-1}}-1\\big)I)}\\end{array}$   \n4: Get dynamically feasibl\u2212e samp\u2212les: $\\mathcal{V}_{d}^{(i)}\\leftarrow\\mathrm{rollout}(\\mathcal{Y}^{(i)})$   \n5: Calculate ${\\bar{Y}}^{(0)}$ with Eq. (10d) (model only) or Eq. (13) (model $^+$ demonstration)   \n6: Estimate the score Eq. (10c): $\\begin{array}{r}{\\nabla_{Y^{(i)}}\\log p_{i}\\big(Y^{(i)}\\big)\\stackrel{}{\\approx}-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\bar{Y}^{(0)}}\\end{array}$   \n7: Monte Carlo score ascent Eq. (6): $\\begin{array}{r}{Y^{(i-1)}\\gets\\frac{1}{\\sqrt{\\alpha_{i}}}\\left(Y^{(i)}+\\big(1-\\bar{\\alpha}_{i}\\big)\\nabla_{Y^{(i)}}\\log p_{i}\\big(Y^{(i)}\\big)\\right)}\\end{array}$   \n8: end for ", "page_idx": 7}, {"type": "text", "text": "cess. Rather, we assess $Y^{(0)}$ using $p(Y_{\\mathrm{demo}}\\mid Y^{(0)})$ , employing a similar technique to interchange the distribution\u2019s parameter with the( rand o\u2223m var)iable, as demonstrated in Eq. (8), to establish $p_{\\sf d e m o}(Y^{(0)})\\propto p(\\bar{Y}_{\\sf d e m o}\\mid Y^{(0)})\\propto\\mathcal{N}(Y^{(0)}\\mid Y_{\\sf d e m o},\\sigma^{2}I)$ . ", "page_idx": 7}, {"type": "text", "text": "To accommodate demonstrations of varying qualities, instead of fixing target to $p_{0}(Y^{(0)})p(Y_{\\mathrm{demo}}\\mid$ $Y^{(0)})$ , we propose seperating the $p_{0}(Y^{(0)})$ from $p_{\\mathrm{demo}}(Y^{(0)})$ to form a new target d(istrib)uti(on3: ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{0}^{\\prime}(Y^{(0)})\\propto(1-\\eta)p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})+\\eta p_{\\mathrm{demo}}(Y^{(0)})p_{J}(Y_{\\mathrm{demo}})p_{g}(Y_{\\mathrm{demo}})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\eta$ is a constant to balance the model and the demonstration. Here, we have introduced two extra constant terms $p_{J}(Y_{\\mathrm{demo}})$ and $p_{g}(Y_{\\mathrm{demo}})$ to ensure that the demonstration likelihood is properly scaled to match the model likelihood $p_{0}(Y^{(0)})$ . With these preparations, we propose to adaptively determine the significance of the demons(tratio)n by choosing $\\eta$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta=\\Bigg\\{1\\begin{array}{r l}&{p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})<p_{\\mathrm{demo}}(Y^{(0)})p_{J}(Y_{\\mathrm{demo}})p_{g}(Y_{\\mathrm{demo}})}\\\\ &{p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})\\geq p_{\\mathrm{demo}}(Y^{(0)})p_{J}(Y_{\\mathrm{demo}})p_{g}(Y_{\\mathrm{demo}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When samples have a high model-likelihood $p_{0}$ , we ignore the demonstration and sample from the model. Otherwise, we trust the demonstration. With the demonstration-augmented target distribution, we modify the way to calculate $\\bar{Y}^{(0)}$ in Eq. (10d) as follows to obtain the score estimate: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{Y}^{(0)}=\\frac{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}Y^{(0)}w(Y^{(0)})}{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}w(Y^{(0)})},\\quad w(Y^{(0)})=\\operatorname*{max}\\left\\{\\begin{array}{c}{p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)}),}\\\\ {p_{\\mathrm{demo}}(Y^{(0)})p_{J}(Y_{\\mathrm{demo}})p_{g}(Y_{\\mathrm{demo}})}\\end{array}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The experimental section will focus on demonstrating the capabilities of MBD in: (1) its effectiveness as a zeroth-order solver for high-dimensional, non-convex, and non-smooth trajectory optimization problems, and (2) its flexibility in utilizing dynamically infeasible data to enhance performance and regularize solutions. Our benchmark shows that MBD outperforms PPO by $59\\%$ in various control tasks with $10\\%$ computational time. ", "page_idx": 7}, {"type": "text", "text": "Beyond control problems, in Appendix A.3, we also show that MBD significantly improves sampling efficiency by an average of $2\\bar{3}\\bar{\\%}$ over leading baselines in high-dimensional (up to 800d) blackbox optimization testbeds [23, 18, 56, 42, 41, 44]. We also apply MBD to optimize an MLP network with 28K parameters in a gradient-free manner, achieving $86\\%$ accuracy within 2s for the MNIST classification task [2], which is comparable to the gradient-based optimizer (SGD with momentum, $93\\%$ accuracy). To further extend MBD to closed-loop control, we employ receding horizon strategy to MBD in Appendix A.6 to update control sequence at each timestep, further improving the performance of MBD by $9.6\\%$ in terms of reward. ", "page_idx": 7}, {"type": "text", "text": "5.1 MBD for Planning in Contact-rich Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To test the effectiveness of MBD as a trajectory optimizer for systems involving non-smooth dynamics, we run MBD on both locomotion and manipulation tasks detailed in Appendix A.5.1. The locomotion tasks includes hopper, half-cheetah, ant, walker2d, humanoid-standup, and humanoid-running. The selected manipulation task, pushT [15], presents its own challenges due to the complexity introduced by contact dynamics and the long-horizon nature of the task. These tasks are widely considered difficult due to their hybrid nature and high dimensionality. ", "page_idx": 7}, {"type": "table", "img_path": "BJndYScO6o/tmp/3c682a90b644a4c130a84afccdcc623db8e5d0a8f9c9775aff7ce1905c0866c2.jpg", "table_caption": [], "table_footnote": ["Table 2: Reward of different methods on non-continuous tasks. \u2217RL requires offline training and generate a closed-loop policy so it is not an apple-to-apple baseline. "], "page_idx": 8}, {"type": "table", "img_path": "BJndYScO6o/tmp/6958f2f69bece1a1481c3e174fb0ae6885f4409dcaf42b5c0ec59d2a733fce7f.jpg", "table_caption": [], "table_footnote": ["Table 3: Computational time of different methods on non-continuous tasks. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "MBD is compared with the state-of-the-art zeroth-order optimization methods, including CMAES [7], CEM [11], and MPPI [59], as well as reinforcement learning (RL) algorithms (e.g., PPO [47] and SAC [22]) on these tasks. Please note RL is only used for performance reference not as there is no existing TO method that can solve such high-dimensional discontinuous tasks as we have shown in the experiments. Model-free RL, especially PPO/SAC, is widely used in such tasks and is considered the SOTA method. The difference between RL and MBD is further discussed in Appendix A.8 and A.6. The RL implementation follows the high-performance parallelized framework from Google Brax [20] elaborated in Appendix A.5.3. For the zeroth-order optimizer, we match the iteration and sample number with the MBD. All the experiments were conducted on a single NVIDIA RTX 4070 Ti GPU. Quantitative metrics including the average step reward and the computational time tested over 50 steps repeated for 8 seeds are reported in Tables 2 and 3. MBD substantially outperforms zeroth-order optimization methods and even outperforms RL in most tasks. Specifically, for the pushT task, MBD achieves a significantly higher reward than the RL algorithm thanks to its iterative refinement process, which effectively explores the full control space while keeping fine-grained control to precisely push the object. Compared with the computationally heavy RL algorithms, MBD only requires one-tenth of time, which is similar to other zeroth-order optimization methods. The optimization process of MBD is visualized in Fig. 3, where the iterative refinement process with the model plays a crucial role in optimizing high-dimensional tasks. ", "page_idx": 8}, {"type": "image", "img_path": "BJndYScO6o/tmp/06cb62dc101f64991681cb82d041b8c9eb1fda75d1a8b85bea14322a2d1cbc94.jpg", "img_caption": ["Figure 4: MBD optimized trajectory with data augmentation on the (a) Humanoid Jogging and (b) Car UMaze Navigation tasks. With data augmentation, the trajectory is regularized and refined to achieve the desired objective. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "BJndYScO6o/tmp/24be9dfa856c8c9800b556bbfb4a6799bc5ab9120133787437003d569b9bb2a5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Optimization process of MBD on the (a) Humanoid Standup, (b) Push T, and (c) Humanoid Running tasks. The trajectory is iteratively refined to achieve the desired objective in the highdimensional space with model information. ", "page_idx": 9}, {"type": "text", "text": "5.2 Data-augmented MBD for Trajectory Optimization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We also evaluate the performance of MBD with data augmentation on the Car UMaze Navigation and Humanoid Jogging tasks to see how partial and dynamically infeasible data can help the exploration of MBD and regularize the solution by steering the diffusion process. ", "page_idx": 9}, {"type": "text", "text": "For Car UMaze Navigation, the map blocked by U-shaped obstacles is challenging to explore given a nonlinear dynamics model. Therefore, random shooting has a low chance of reaching the goal region. To sample with loosened dynamical constraints, we augment MBD with data from the RRT [38] algorithm through goal-directed exploration with simplified dynamics. Fig. 4(b) shows the difference between data-augmented MBD and data-free one: the former can refine the infeasible trajectory and further improve it to reach the goal in less time, while the latter struggles to find a feasible solution. The reason is that the infeasible trajectory from RRT serves as a good initialization for MBD, which can be further refined to minimize the cost function with MBD. ", "page_idx": 9}, {"type": "text", "text": "For Humanoid Jogging, we aim to regularize the solution for the task with multiple solutions to the desired one with partial state data. Due to the infinite possibilities for humanoid jogging motion, the human motion data provide a good reference to regularize MBD to converge to a more human-like and robust solution instead of an aggressive or unstable one [26, 45]. We use data from the CMU Mocap dataset [1], from which we extract torso, thigh, and shin positions and use them as a partial state reference. Fig. 4(a) demonstrates a more stable motion generated by data-augmented MBD. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces Model-Based Diffusion (MBD), a novel diffusion-based trajectory optimization framework that employs a dynamics model to approximate the score function. MBD not only outperforms existing methods in terms of sample efficiency and generalization, but also provides a new perspective on trajectory optimization by leveraging diffusion models as powerful samplers. Future directions involve theoretically understanding its convergence, optimizing the standard Gaussian forward process using the model information, adapting it to online tasks with receding horizon strategies, and exploring advanced sampling and scheduling techniques to further improve performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSF Grant 2154171, NSF CAREER 2339112 and CMU CyLab Seed Funding. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Carnegie Mellon University - CMU Graphics Lab - motion capture library. http://mocap.cs.cmu.edu/. ", "page_idx": 9}, {"type": "text", "text": "[2] The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web] | IEEE Journals & Magazine | IEEE Xplore. https://ieeexplore.ieee.org/document/6296535.   \n[3] Sequential Quadratic Programming. In Jorge Nocedal and Stephen J. Wright, editors, Numerical Optimization, pages 529\u2013562. Springer, New York, NY, 2006.   \n[4] David H. Ackley. A Connectionist Machine for Genetic Hillclimbing, volume 28 of The Kluwer International Series in Engineering and Computer Science. Springer US, Boston, MA, 1987.   \n[5] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is Conditional Generative Modeling all you need for Decision-Making?, July 2023.   \n[6] Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Alexander Tong. Iterated Denoising Energy Matching for Sampling from Boltzmann Densities, February 2024.   \n[7] Youhei Akimoto, Yuichi Nagata, Isao Ono, and Shigenobu Kobayashi. Theoretical Foundation for CMA-ES from Information Geometry Perspective. Algorithmica, 64(4):698\u2013716, December 2012.   \n[8] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured Denoising Diffusion Models in Discrete State-Spaces, February 2023.   \n[9] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization, December 2020.   \n[10] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusionbased generative modeling, July 2023.   \n[11] Zdravko I. Botev, Dirk P. Kroese, Reuven Y. Rubinstein, and Pierre L\u2019Ecuyer. Chapter 3 - The Cross-Entropy Method for Optimization. In C. R. Rao and Venu Govindaraju, editors, Handbook of Statistics, volume 31 of Handbook of Statistics, pages 35\u201359. Elsevier, January 2013.   \n[12] Jo\u00e3o Carvalho, An T. Le, Mark Baierl, Dorothea Koert, and Jan Peters. Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1916\u20131923, Detroit, MI, USA, October 2023. IEEE.   \n[13] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. WaveGrad: Estimating Gradients for Waveform Generation, October 2020.   \n[14] Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped Langevin MCMC: A non-asymptotic analysis, January 2018.   \n[15] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion, June 2023.   \n[16] Tzuu-Shuh Chiang, Chii-Ruey Hwang, and Shuenn Jyi Sheu. Diffusion for Global Optimization in $\\mathbb{S}^{\\flat}$ \\mathbb $\\{\\mathbf{R}\\}\\land\\mathbf{n}\\,\\mathfrak{S}$ . SIAM Journal on Control and Optimization, 25(3):737\u2013753, May 1987.   \n[17] Murat A. Erdogdu and Rasa Hosseinzadeh. On the Convergence of Langevin Monte Carlo: The Interplay between Tail Growth and Smoothness. In Proceedings of Thirty Fourth Conference on Learning Theory, pages 1776\u20131822. PMLR, July 2021.   \n[18] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable Global Optimization via Local Bayesian Optimization. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[19] Peter I. Frazier. A Tutorial on Bayesian Optimization, July 2018.   \n[20] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax \u2013 A Differentiable Physics Engine for Large Scale Rigid Body Simulation, June 2021.   \n[21] Saul B. Gelfand and Sanjoy K. Mitter. Recursive Stochastic Algorithms for Global Optimization in $\\mathbb{S}^{\\prime}$ \\mathbb $\\{\\boldsymbol{\\mathrm{R}}\\}\\backslash\\mathsf{d}\\,\\mathbb{S}$ . SIAM Journal on Control and Optimization, 29(5):999\u20131018, September 1991.   \n[22] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.   \n[23] Nikolaus Hansen. The CMA Evolution Strategy: A Tutorial, March 2023.   \n[24] Nikolaus Hansen, Sibylle D. M\u00fcller, and Petros Koumoutsakos. Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES). Evolutionary Computation, 11(1):1\u201318, March 2003.   \n[25] C. R. Hargraves and S. W. Paris. Direct trajectory optimization using nonlinear programming and collocation. Journal of Guidance, Control, and Dynamics, May 2012.   \n[26] Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation, March 2024.   \n[27] Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, James Requeima, Edward O. Pyzer-Knapp, and Al\u00e1n AspuruGuzik. Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space. In Proceedings of the 34th International Conference on Machine Learning, pages 1470\u20131479. PMLR, July 2017.   \n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020.   \n[29] Taylor A. Howell, Brian E. Jackson, and Zachary Manchester. ALTRO: A Fast Solver for Constrained Trajectory Optimization. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7674\u20137679, Macau, China, November 2019. IEEE.   \n[30] Xunpeng Huang, Hanze Dong, Yifan Hao, Yi-An Ma, and Tong Zhang. Reverse Diffusion Monte Carlo, March 2024.   \n[31] Chii-Ruey Hwang. Laplace\u2019s Method Revisited: Weak Convergence of Probability Measures. The Annals of Probability, 8(6):1177\u20131182, December 1980.   \n[32] Wilson Jallet, Antoine Bambade, Nicolas Mansard, and Justin Carpentier. Constrained Differential Dynamic Programming: A primal-dual augmented Lagrangian approach, October 2022.   \n[33] Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey Levine. Planning with Diffusion for Flexible Behavior Synthesis, December 2022.   \n[34] Chiyu Max Jiang, Andre Cornman, Cheolho Park, Ben Sapp, Yin Zhou, and Dragomir Anguelov. MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion, June 2023.   \n[35] B. Kappen, V. Gomez, and M. Opper. Optimal control as a graphical model inference problem. Machine Learning, 87(2):159\u2013182, May 2012.   \n[36] Seung-Jean Kim, K. Koh, M. Lustig, Stephen Boyd, and Dimitry Gorinevsky. An Interior-Point Method for Large-Scale -Regularized Least Squares. IEEE Journal of Selected Topics in Signal Processing, 1(4):606\u2013617, December 2007.   \n[37] Marin Kobilarov. Cross-entropy motion planning. The International Journal of Robotics Research, 31(7):855\u2013871, June 2012.   \n[38] Steven LaValle. Rapidly-exploring random trees: A new tool for path planning. Research Report 9811, 1998.   \n[39] Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review, May 2018.   \n[40] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners, May 2023.   \n[41] Jialin Liu, Antoine Moreau, Mike Preuss, Baptiste Roziere, Jeremy Rapin, Fabien Teytaud, and Olivier Teytaud. Versatile Black-Box Optimization, April 2020.   \n[42] Amin Nayebi, Alexander Munteanu, and Matthias Poloczek. A Framework for Bayesian Optimization in Embedded Subspaces. In Proceedings of the 36th International Conference on Machine Learning, pages 4752\u20134761. PMLR, May 2019.   \n[43] Radford M. Neal. Annealed Importance Sampling, September 1998.   \n[44] Leonard Papenmeier, Luigi Nardi, and Matthias Poloczek. Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces, April 2023.   \n[45] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control. ACM Transactions on Graphics, 40(4):1\u201320, August 2021.   \n[46] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-Conditioned Imitation Learning using Score-based Diffusion Policies, June 2023.   \n[47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017.   \n[48] Guanya Shi, Wolfgang Honig, Xichen Shi, Yisong Yue, and Soon-Jo Chung. Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms Using Learned Interactions. IEEE Transactions on Robotics, 38(2):1063\u20131079, April 2022.   \n[49] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[50] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics, November 2015.   \n[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In International Conference on Learning Representations, October 2020.   \n[52] Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution, October 2020.   \n[53] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations, February 2021.   \n[54] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano. Human Motion Diffusion Model, October 2022.   \n[55] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1049\u20131056, Montreal Quebec Canada, June 2009. ACM.   \n[56] Linnan Wang, Rodrigo Fonseca, and Yuandong Tian. Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search. In Advances in Neural Information Processing Systems, volume 33, pages 19511\u201319522. Curran Associates, Inc., 2020.   \n[57] Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics.   \n[58] Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model Predictive Path Integral Control: From Theory to Parallel Computation. Journal of Guidance, Control, and Dynamics, 40(2):344\u2013357, February 2017.   \n[59] Grady Williams, Paul Drews, Brian Goldfain, James M. Rehg, and Evangelos A. Theodorou. Information-Theoretic Model Predictive Control: Theory and Applications to Autonomous Driving. IEEE Transactions on Robotics, 34(6):1603\u20131622, December 2018.   \n[60] Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, and Daniela Rus. SafeDiffuser: Safe Planning with Diffusion Probabilistic Models, May 2023.   \n[61] Zhutian Yang, Jiayuan Mao, Yilun Du, Jiajun Wu, Joshua B. Tenenbaum, Tom\u00e1s Lozano-P\u00e9rez, and Leslie Pack Kaelbling. Compositional Diffusion-Based Continuous Constraint Solvers, September 2023.   \n[62] Zeji Yi, Chaoyi Pan, Guanqi He, Guannan Qu, and Guanya Shi. CoVO-MPC: Theoretical Analysis of Sampling-based MPC and Optimal Covariance Design, January 2024.   \n[63] Zeji Yi, Yunyue Wei, Chu Xin Cheng, Kaibo He, and Yanan Sui. Improving sample efficiency of high dimensional Bayesian optimization with MCMC, January 2024.   \n[64] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method, December 2012. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / Supplemental Material ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "BJndYScO6o/tmp/72384964d5ac9dd394cd83b43aca6f4a72bf4372a7167de381d87ae29e8673c6.jpg", "table_caption": ["A.1 Notation Table "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Convergence of Distribution with Small $\\lambda$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first give the definition of the volume of the sub-level set for cost $J$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 1. Let $F:\\mathcal{R}^{d}\\rightarrow\\mathcal{R}$ be a measurable function. Define the volume of the sub-level set for a given level $t$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{F}(t)=\\int_{\\mathcal{R}^{d}}\\chi_{\\{Y\\in\\mathcal{R}^{d}:F(Y)\\leq t\\}}(Y)\\,d Y,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\chi$ denotes the indicator function. ", "page_idx": 14}, {"type": "text", "text": "The volume function $V_{J}(t)$ plays a crucial role in linking geometric properties with probabilistic outcomes in optimization and learning algorithms. This function provides a quantitative measure that helps us to understand how changes in parameters like $\\lambda$ influence the distribution and concentration of probability mass. ", "page_idx": 14}, {"type": "text", "text": "The interplay between geometry and probability, represented by $V_{J}(t)$ , is crucial for evaluating the convergence and stability of algorithms. It provides a significant met(ho)d for utilizing the PDF of the random variable $\\mathbf{Y}$ to constrain the CDF, thereby facilitating convergence in distribution. ", "page_idx": 14}, {"type": "text", "text": "Proposition 2. Given the target distribution $Y\\sim p(\\cdot)$ with $\\begin{array}{r}{P(Y)\\propto\\exp\\left({-\\frac{J(Y)}{\\lambda}}\\right),Y\\in\\mathcal{R}^{d},}\\end{array}$ , where $J$ is a cost function with minY $J(Y)=0$ and $Y^{*}=\\arg\\operatorname*{min}J(Y)$ , and assuming that the volume function $V_{J}(t)$ is bounded by poly(no )m i=al inequaliti=es: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P o l y_{l}(t)\\leq V_{J}(t)\\leq P o l y_{u}(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{P o l y_{l}(t)=\\sum_{k=0}^{M}c_{k}^{l}t^{\\alpha_{k}}}\\end{array}$ and $\\begin{array}{r}{P o l y_{u}(t)=\\sum_{k=0}^{M}c_{k}^{u}t^{\\alpha_{k}}}\\end{array}$ are polynomials with coefficients satisfying $c_{k}^{l}=0$ if and  \u2211only if $c_{k}^{u}=0$ . The exponen t \u2211term satisfies that $\\alpha_{k}\\in\\mathcal{R}$ , and $0<\\alpha_{0}<\\alpha_{1}<\\cdots<$ $\\alpha_{M}<\\infty$ , $I t$ follows that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\rightarrow0}J(Y)\\stackrel{p}{\\rightarrow}J(Y^{*})=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The cost value $J(Y)$ converges in probability to $J(Y^{\\ast})$ as $\\lambda\\to0$ . ", "page_idx": 14}, {"type": "text", "text": "The condition on the polynomial bounds of $V_{J}(t)$ is generally not restrictive. For instance, consider $J=\\eta_{c}\\left\\|Y-Y_{*}\\right\\|^{m}$ , where $Y^{\\ast}$ is the optimal po(in)t and $\\eta_{c}>0$ is any constant. In this case, $V_{J}(t)=$ $C t^{\\frac{d}{m}}$ , where $C$ is a constant, meets the constraint in a straightforward way. This condition can be extended beyond this simple scenario, as even if $J$ has multiple modes, it can still adhere to this polynomial constraint. ", "page_idx": 14}, {"type": "text", "text": "Proof. The convergence in distribution of $\\mathbf{Y}$ towards $Y^{\\ast}$ as $\\lambda\\to0$ is established by analyzing the behavior of the probability density function, defined up to a multiplicative constant. Consider the density $Y_{0}\\sim p_{\\lambda}(Y)$ approximating $Y^{\\ast}$ when $\\lambda$ approaches zero. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(J(\\mathbf{Y})\\leq t)=\\displaystyle\\int_{\\{J(Y)\\leq t\\}}p(Y)d Y,}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{t}\\int_{\\{J(Y)=x\\}}p(Y)d Y d x,}\\\\ &{\\qquad\\qquad\\quad\\propto\\displaystyle\\int_{0}^{t}e x p(-\\frac{x}{\\lambda})\\int_{\\{J(Y)=x\\}}d Y d x,}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int_{0}^{t}e x p(-\\frac{x}{\\lambda})\\frac{d V_{J}(x)}{d x}d x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where Eq. (14c) is valid since $\\begin{array}{r}{P(Y)\\propto\\exp\\left(-\\frac{J(Y)}{\\lambda}\\right)}\\end{array}$ and $J(Y)$ represents the sufficient statistics of the distribution. We can obtain Eq. (14d) by computing the derivative of $V_{J}(x)$ based on the volume definition as shown in Definition 1. ", "page_idx": 15}, {"type": "text", "text": "We denote $J_{\\mathrm{min}}=\\mathrm{min}_{Y}\\,J(Y)=0$ and $J_{\\operatorname*{max}}=\\operatorname*{max}_{Y}J(Y)$ with $J_{\\mathrm{max}}$ satisfying $0\\leq J_{\\operatorname*{max}}\\leq+\\infty$ . We proceed to anal y=ze Eq. (14(d)  )b y= performing  =integration( b y) parts as shown in Eq. ( 1\u22645a). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int_{J_{\\mathrm{min}}}^{J_{\\mathrm{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)=\\left.\\displaystyle\\int_{J_{\\mathrm{min}}}^{J_{\\mathrm{max}}}d\\left[\\exp\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)\\right]+\\frac{1}{\\lambda}\\exp\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t\\right.}\\\\ &{}&{\\left.=\\exp\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)\\right|_{J_{\\mathrm{min}}}^{J_{\\mathrm{max}}}+\\frac{1}{\\lambda}\\left.\\displaystyle\\int_{J_{\\mathrm{min}}}^{J_{\\mathrm{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To establish convergence in probability, we need to demonstrate that for any small $\\epsilon>0$ and $\\delta>0$ , there exists sufficiently small $\\lambda>0$ , such that ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(J(Y)<\\epsilon)=\\frac{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{0}^{J_{\\operatorname*{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality is due to Eq. (14d). Setting $\\begin{array}{r}{\\delta^{\\prime}=\\frac{1-\\delta}{\\delta}}\\end{array}$ , it suffices to show that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{\\epsilon}^{J_{\\operatorname*{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}\\geq\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assuming without loss of generality that $J_{\\operatorname*{max}}=\\infty$ , becuase $\\begin{array}{r}{d V_{J}(t)\\geq0,e x p(-\\frac{t}{\\lambda})>0}\\end{array}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{\\epsilon}^{J_{\\operatorname*{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}\\ge\\frac{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{\\epsilon}^{\\infty}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This ratio as in Eq. (17) can be expanded using the integral bounds and the polynomial approximations for $V_{J}(t)$ ,then it suffices to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{\\epsilon}^{\\infty}\\exp\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}\\geq\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By inserting Eq. (15b) into both the numerator and denominator on the LHS of Eq. (19), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\int_{0}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}{\\int_{\\epsilon}^{\\infty}e x p\\left(-\\frac{t}{\\lambda}\\right)d V_{J}(t)}=\\frac{e x p\\left(-\\frac{\\epsilon}{\\lambda}\\right)V\\left(\\epsilon\\right)+\\frac{1}{\\lambda}\\int_{0}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}{-e x p\\left(-\\frac{\\epsilon}{\\lambda}\\right)V\\left(\\epsilon\\right)+\\frac{1}{\\lambda}\\int_{\\epsilon}^{\\infty}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}}\\\\ &{\\phantom{\\frac{\\int_{0}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}{\\int_{\\epsilon}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}}\\geq\\frac{\\int_{0}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}{\\int_{\\epsilon}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}}\\\\ &{\\phantom{\\frac{\\int_{0}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)V_{J}(t)d t}{\\int_{\\epsilon}^{\\epsilon}e x p\\left(-\\frac{t}{\\lambda}\\right)\\sum_{k=0}^{M}c_{k}^{l}t^{\\alpha_{k}}d t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To bound the expression in Eq. (20c), we first derive the following integrals by utilizing a change of variables $\\begin{array}{r}{x=\\frac{t}{\\lambda}}\\end{array}$ , which simplifies the expressions: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)t^{\\alpha_{k}}\\,d t=\\lambda^{k+1}\\int_{0}^{\\frac{\\epsilon}{\\lambda}}\\exp(-x)x^{\\alpha_{k}}\\,d x,}\\\\ {\\displaystyle\\int_{\\frac{\\epsilon}{\\lambda}}^{\\infty}\\exp\\left(-\\frac{t}{\\lambda}\\right)t^{\\alpha_{k}}\\,d t=\\lambda^{k+1}\\int_{\\frac{\\epsilon}{\\lambda}}^{\\infty}\\exp(-x)x^{\\alpha_{k}}\\,d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For these transformed integrals, we can observe that $\\begin{array}{r}{\\int_{0}^{\\infty}\\exp(-x)x^{\\alpha_{k}}\\,d x=\\Gamma(\\alpha_{k}+1)}\\end{array}$ , the gamma function, which is well-defined for all non-negative $\\alpha_{k}$ \u222b. Given (th\u2212at) $\\delta^{\\prime}$ is a f u=ncti(on  o+f $\\delta$ ), by applying the intermediate value theorem and definition of the limit of the integral, we can choose $\\epsilon_{k}$ in such a way that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{0}^{\\epsilon_{k}}\\exp(-x)x^{\\alpha_{k}}\\,d x\\geq\\frac{c_{k}\\delta^{\\prime}}{1+c_{k}\\delta^{\\prime}}\\Gamma(\\alpha_{k}+1),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $c_{k}\\,=\\,\\frac{c_{k}^{l}}{c_{k}^{u}}$ denotes the ratio of coefficients in polynomial lower and upper bounds for $V_{J}(t)$ . By selecting $\\epsilon_{\\operatorname*{max}}=\\operatorname*{max}\\epsilon_{0},\\epsilon_{1},\\cdots,\\epsilon_{M}$ to be the maximum of all such $\\epsilon_{k}$ , ensuring coverage for all polynomial terms  u=p to $M$ , we e\u22efstablish that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{\\epsilon_{\\operatorname*{max}}}\\exp\\left(-\\frac{t}{\\lambda}\\right)c_{k}^{l}t^{\\alpha_{k}}\\,d t}\\\\ &{\\overline{{\\int_{\\epsilon_{\\operatorname*{max}}}^{\\infty}\\exp\\left(-\\frac{t}{\\lambda}\\right)c_{k}^{u}t^{\\alpha_{k}}\\,d t}}\\geq\\delta^{\\prime},\\quad\\mathrm{for~all~}k=0,1,\\ldots,M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By ensuring that $\\lambda\\le\\frac{\\epsilon}{\\epsilon_{\\mathrm{max}}}$ , we can conclude: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{\\epsilon}\\exp\\left(-\\frac{t}{\\lambda}\\right)\\sum_{k=0}^{M}c_{k}^{l}t^{\\alpha_{k}}\\,d t}\\\\ &{\\int_{\\epsilon}^{\\infty}\\exp\\left(-\\frac{t}{\\lambda}\\right)\\sum_{k=0}^{M}c_{k}^{u}t^{\\alpha_{k}}\\,d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, the condition specified in Eq. (16) is satisfied, validating that the distribution of $\\mathbf{\\deltaY}$ converges in distribution to $Y^{*}$ as $\\lambda$ approaches zero. ", "page_idx": 16}, {"type": "text", "text": "By adding another mild assumption regarding the landscape of $J$ near the global optimum, we can demonstrate the convergence of the random variable $\\mathbf{Y}$ itself, rather than the convergence of $J(\\mathbf{Y})$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 3. We denote the minimum of the complementary set of neighborhood as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{B}^{*}(\\delta)=\\operatorname*{min}_{\\|Y-Y^{*}\\|>\\delta}J(Y)-J(Y^{*}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 4. Given the context and conditions specified in Definitions 1 and 3 and Proposition 2, and given that $J$ has only one golbal minimizer $Y^{*}$ , i.e. there exist small $\\delta^{*}$ , that for $\\bar{\\delta}\\in(0,\\delta^{*}],$ , $J_{B}^{*}(\\bar{\\delta})$ is strictly increasing, and $J_{B}^{*}(\\delta^{*})<\\infty$ . It follows that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\to0}Y\\stackrel{p}{\\to}Y^{*}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The random variable $\\mathbf{Y}$ converges in probability to $Y^{*}\\;a s\\;\\lambda\\to0$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. In order to prove that $\\mathrm{lim}_{\\lambda\\to0}\\,Y\\stackrel{p}{\\to}Y^{*}$ . We need to prove that for any sufficient small $\\gamma>0$ and $\\delta>0$ , there exists small $\\lambda>0$ , such t \u2192hat ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\|Y-Y^{*}\\|\\leq\\delta)\\geq1-\\gamma\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Definition 3 and due to the strict increase of $J_{B}^{*}(\\delta)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|Y-Y^{*}\\|\\leq\\delta,\\ \\ \\ \\forall Y\\in\\left\\{Y\\in{\\mathcal{R}}^{d}\\,|\\,J(Y)-J(Y^{*})<J_{B}^{*}(\\delta)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $0<\\delta\\leq\\delta^{*}$ . Because i $\\begin{array}{r}{\\mathrm{f}\\;\\|Y-Y^{*}\\|>\\delta,J(Y)-J(Y^{*})<J_{B}^{*}(\\delta)=\\operatorname*{min}_{\\|Y-Y^{*}\\|>\\delta}J(Y)-J(Y^{*})}\\end{array}$ contradicts Definition 3. ", "page_idx": 16}, {"type": "text", "text": "Given that $\\begin{array}{r}{\\operatorname*{lim}_{\\lambda\\to0}J(Y)\\stackrel{p}{\\to}J(Y^{*})}\\end{array}$ . and any sufficient small $\\epsilon,\\gamma>0$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(J(Y)-J(Y^{*})\\leq\\epsilon)\\geq1-\\gamma\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\exists\\lambda>0$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\big(J(Y)-J(Y^{*})\\leq J_{\\mathcal{B}}^{*}(\\delta)\\big)\\geq1-\\gamma.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And From Eq. (25), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\big(\\|Y-Y^{*}\\|\\leq\\delta\\big)\\geq P\\big(J(Y)-J\\big(Y^{*}\\big)\\leq J_{\\mathcal{B}}^{*}(\\delta\\big)\\big)\\geq1-\\gamma\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have that $\\mathbf{Y}$ converges in probability to $Y^{*}$ ,i.e, $\\mathrm{lim}_{\\lambda\\to0}\\,Y\\stackrel{p}{\\to}Y^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 5. Given the context and conditions specified in Propositions 2 and $^{4}$ and the way we define the forward process as in Eq. (3). The diffused $Y_{i}$ converge in density to a Gaussian distribution. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\rightarrow0}Y^{(i)}\\stackrel{d}{\\rightarrow}\\mathcal{N}(\\sqrt{\\bar{\\alpha}_{i}}Y^{*},\\sqrt{1-\\bar{\\alpha}_{i}}I),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $Y^{(i)}\\sim p_{i}(\\cdot)$ as in Eq. (3). ", "page_idx": 17}, {"type": "text", "text": "Proposition 5 is derived by using Slutsky\u2019s theorem on Proposition 4 and offers insight into choosing the stepsize as discussed in Section 4.1. ", "page_idx": 17}, {"type": "text", "text": "A.3 Black-box Optimization with MBD ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As a zeroth order optimizer, MBD is capable of addressing both trajectory optimization and broader, high-dimensional unconstrained optimization challenges. Such black-box optimization tasks are universally acknowledged as difficult [7, 63]. We first show superior performance of MBD within this black-box optimization context. In such settings, the Bayesian Optimization technique struggles due to the computational intensity required to develop surrogate models and identify new potential solutions [18]. Alternative black-box optimization strategies [23] are not limited by computational issues but tend to be less efficient because they do not estimate the black-box function as accurately. MBD\u2019s effectiveness is evaluated using two well-known highly non-convex black-box optimization benchmarks: Ackley [4] and Rastrigin [9], each tested across three different dimensionalities. Comparisons were made with CMA-ES [23], TuRBO [18], LA-MCTS [56], HesBO [42], Shiwa [41], and BAxUS [44]. ", "page_idx": 17}, {"type": "image", "img_path": "BJndYScO6o/tmp/c93ea905c1c5f115211fe55110f0830db8ef2e639c4aa00a05eb6e57f78e5d08.jpg", "img_caption": ["Figure 5: Performance of MBD on high-dimensional black-box optimization benchmarks. MBD outperforms other Gaussian Process-based Bayesian Optimization methods by a clear margin. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Fig. 5 shows the performance of MBD on the Ackley and Rastrigin benchmarks. MBD demonstrates superior performance over other algorithms for several reasons. Firstly, the implementation of a scheduled forward process that determines the total number of samples consequently boosts sample efficiency. Secondly, the application of Monte Carlo score ascent on various $\\log p_{i}(Y^{(i)})$ facilitates its escape from local optima of varying scales. It is important to acknowledge( that )comparing computational efficiency may not be entirely fair, given that black-box optimization problems typically involve functions that are costly to evaluate. However, MBD markedly outperforms other Gaussian Process-based Bayesian Optimization approaches, achieving computational time savings of more than twentyfold, similar to the improvements observed with different evolutionary optimization strategies. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Here are the implementation detail for the benchmarks. For the BO benchmarks, the experiments were conducted on an A100 GPU because of the high computational demands of the Gaussian Process Regression Model it incorporates. ", "page_idx": 18}, {"type": "text", "text": "TuRBO: TuRBO is implemented based on tutorials from Botorch [9]. ", "page_idx": 18}, {"type": "text", "text": "LA-MCTS: LA-MCTS, we refer to authors\u2019 reference implementations, and use TuRBO as its local BO solver [56]. ", "page_idx": 18}, {"type": "text", "text": "HesBO: For HesBO, we refer to authors\u2019 reference implementations [42]. We transformed default GP component into Gpytorch version for faster inference speed on GPU. We set the embedding dimension to 20 for all tasks ", "page_idx": 18}, {"type": "text", "text": "CMA-ES: We use pycma4 to implement CMA-ES, and use default setting except setting population size eqauls to batch size. ", "page_idx": 18}, {"type": "text", "text": "Shiwa: We use Nevergrad5 to implement Shiwa, and use default setting to run experiments. ", "page_idx": 18}, {"type": "text", "text": "BAxUS: We refer to the authors\u2019 reference implementations [44]. ", "page_idx": 18}, {"type": "text", "text": "A.3.1 MBD for DNN Training without Gradient Information ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further demonstrate the effectiveness of MBD in high-dimensional systems, we apply MBD to optimize an MLP network for MNIST classification [2] without access to the gradient information. MBD achieve $85.5\\%$ accuracy with 256 samples within 2s, which is comparable to the performance of the SGD optimizer with momentum $(92.7\\%$ accuracy). We use MLP with 2 hidden layers, each with 32 neurons, and ReLU activation function. The input is flattened to 784 dimensions, and the output is a 10-dimensional vector. We use cross-entropy loss as the objective function. The network has 27,562 parameters in total, which makes sampling-based optimization challenging. MBD can effectively optimize the network with a small number of samples, demonstrating its effectiveness in high-dimensional black-box optimization tasks. ", "page_idx": 18}, {"type": "text", "text": "A.4 MBD with Demonstration Explaination ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Data-augmented MBD calculate the score function with demostration as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Y^{(i-1)}=\\displaystyle\\frac{1}{\\sqrt{\\alpha_{i}}}\\left(Y^{(i)}+(1-\\bar{\\alpha}_{i})\\nabla_{Y^{(i)}}\\log p_{i}(Y^{(i)})\\right)}}\\\\ {{\\displaystyle\\nabla_{Y^{(i)}}\\log p_{i}(Y^{(i)})=-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}\\bar{Y}^{(0)}}}\\\\ {{\\displaystyle\\bar{Y}^{(0)}=\\frac{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}Y^{(0)}w(Y^{(0)})}{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}w(Y^{(0)})}}}\\\\ {{\\displaystyle w(Y^{(0)})=\\operatorname*{max}\\left\\{\\begin{array}{l}{{w_{\\mathrm{model}}(Y^{(0)})=p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)}),}}\\\\ {{w_{\\mathrm{demo}}((Y^{(0)}))=p_{\\mathrm{demo}}(Y^{(0)}))p_{J}(Y_{\\mathrm{demo}}),}}\\end{array}\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "BJndYScO6o/tmp/bf4099b8a53a356bc164dbc3cf9810427808f400a8d212d1cb225c43da48a058.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: MBD with data vs. without data on a nonconvex function with constraints $\\left\\vert\\left\\vert Y\\right\\vert-0.4\\right\\vert>0.3$ . We want MBD converge to the optimal point $\\star$ with the help of demonstration data \u25cf. Although the demostration point is not optimal, MBD can still converge to the optimal point with the guidance of the demonstration data. Here data serves as a regularization term to guide the diffusion process to the negative optimal point while allowing to use model further to refine the solution. ", "page_idx": 19}, {"type": "text", "text": "where demonstrate likelihood term $w_{\\mathrm{demo}}(Y^{(0)})$ will draw samples towards data without considering the model. Given $w=w_{\\mathrm{demo}}$ , $\\begin{array}{r}{\\bar{Y}^{(0)}=\\frac{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}Y^{(0)}p_{\\mathrm{demo}}(Y^{(0)})}{\\sum_{Y^{(0)}\\in\\mathcal{Y}_{d}^{(i)}}p_{\\mathrm{demo}}(Y^{(0)})}=Y_{\\mathrm{demo}}}\\end{array}$ . The score function would be $\\begin{array}{r}{\\nabla_{Y^{(i)}}\\log p_{i}\\bigl(Y^{(i)}\\bigr)=-\\frac{Y^{(i)}}{1-\\bar{\\alpha}_{i}}+\\frac{\\sqrt{\\bar{\\alpha}_{i}}}{1-\\bar{\\alpha}_{i}}Y_{\\mathrm{demo}}}\\end{array}$ , which means the score function is a linear combination of the current sample and the  demonstration data. ", "page_idx": 19}, {"type": "text", "text": "If we don\u2019t use Eq. (11) and employ the posterior distribution $p(Y^{(0)}|Y_{\\mathrm{demo}})\\propto p_{0}(Y^{(0)})p_{\\mathrm{demo}}(Y^{(0)})$ , it will yields update weights $w=w_{\\mathrm{demo}}w_{\\mathrm{model}}$ , which will draw( samp\u2223les to )b o\u221dth m(odel a)nd dem(onstra)- tion data. If the demonstration  =data is not optimal, the final solution will be a compromise between the model and demonstration data. In Fig. 6, the resulted solution will lie between optimal point $\\star$ with the help of demonstration data $\\bigcirc$ . ", "page_idx": 19}, {"type": "text", "text": "Using the max function in $w$ can aviod this issue. In the early stage while $p_{J}(Y^{(0)})$ is low due to poor sample quality, $w_{\\mathrm{demo}}$ will dominate thanks to the high $p_{J}(Y_{\\mathrm{demo}})$ . This wi(ll dra)w samples towards the demonstration data as shown in the earlier stage of Fig(. 6. As) the sample quality improves and $p_{J}(Y^{(0)})>p_{J}(Y_{\\mathrm{demo}}),$ $w_{\\mathrm{model}}$ will dominate and the sample will converge to the optimal point. ", "page_idx": 19}, {"type": "text", "text": "A.5 Experiment Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.5.1 Simulator and Environment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We leverage the GPU-accelerated simulator Google Brax [20] to design the locomotion and manipulation tasks. All task is set to use positional backend in Brax except for the pushT task, which uses the generalizable backend for better contact dynamics simulation. Here we provide a brief description of each task implementations: ", "page_idx": 19}, {"type": "text", "text": "1. Ant: The Ant task is a 3D locomotion task where the agent is required to move forward as fast as possible. The reward is composed of the forward velocity of the agent and control cost, same as the original Brax implementation. The control dimension is 8.   \n2. Hopper: The Hopper task is a 2D locomotion task where the agent is required to jumping forward as fast as possible. We use the same reward function as the original Brax implementation. We modify the simulation substeps from 10 to 20 for longer planning horizon given the same control node. The control dimension is 3.   \n3. Walker2d: The Walker2d task is a 2D locomotion task where the agent is required to walk forward. The reward is composed of keep the agent upright and moving forward. The control dimension is 6.   \n4. Halfcheetah: The Halfcheetah task is a 2D locomotion task where the agent is required to run forward. The reward is composed of the forward velocity of the agent and control ", "page_idx": 19}, {"type": "text", "text": "cost. We follow the same reward function as the original Brax implementation. The control dimension is 6. ", "page_idx": 20}, {"type": "text", "text": "5. Humanoidrun: The Humanoidrun task is a 3D locomotion task where the agent is required to run forward. The reward is composed of the forward velocity of the agent and standing upright. Here we also modify the simulation substeps from 10 to 20 for longer planning horizon. The control dimension is 17.   \n6. Humanoidstandup: The Humanoidstandup task is a 3D locomotion task where the agent is required to stand up. The reward is the upright torso position of the agent. The control dimension is 17.   \n7. PushT: The PushT task is a 2D manipulation task where you can apply force to a sphere to push the T-shaped object to the target location. The reward is composed of the distance between the target and the object and orientation difference between the target and the object. To make the task more challenging, we randomize the target location $20\\mathrm{cm}$ away from the initial position and make sure the rotational angle is greater than 135 degrees, which makes it hard to solve the task with single continous contact policy. The control dimension is 2.   \n8. Car2D: We implement a 2D car task with standard bicycle dynamics model, where state is $\\boldsymbol{x}\\,=\\,\\left[{x,y,\\theta,v,\\delta}\\right]$ , and action is $u\\,=\\,[a,\\delta]$ . The dynamics is defined as ${\\dot{x}}\\,=\\,f(x,u)\\,=$ $[v\\cos(\\theta),v\\sin(\\theta)$ ,] $\\textstyle{\\frac{v}{L}}\\tan(\\delta),a,\\delta\\rceil$ .  T=h e[ con]straints are defined as the U-shap e= are(a in )t h=e [middle( o)f the m(a)p, where( t)he car] cannot enter. The reward is composed of the distance between the target and the car and the control cost. The control dimension is 2. ", "page_idx": 20}, {"type": "text", "text": "A.5.2 MBD Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In general, MBD is very little hyperparameters to tune compared with RL. We use the same hyperparameters for all the tasks, with small tweaks for harder tasks. ", "page_idx": 20}, {"type": "table", "img_path": "BJndYScO6o/tmp/8b8d6720f1aa16e2c803eb27caf5d2d2ed2bb297da619d4d46ff2c638a913a23.jpg", "table_caption": [], "table_footnote": ["Table 4: MBD hyperparameters for various tasks "], "page_idx": 20}, {"type": "text", "text": "For diffusion noise schedulling, we use simple linear scheduling $\\beta_{0}=1\\times10^{-4}$ and $\\beta_{N}=1\\times10^{-2}$ , and the diffusion step number is 100 across all tasks. Each step\u2019s $\\alpha_{i}$ i s calculated as $\\alpha_{i}=1-\\beta_{i}$ . ", "page_idx": 20}, {"type": "text", "text": "A.5.3 Baseline Algorithms Implementation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For reinforcement learning implementation, we strictly follow the hyperparameters and implementation details provided by the original Brax repository, which optimize for the best performance. For our self-implemented PushT task, the hyperparameters is ported from Pusher task in Brax for fair comparison. The hyperparameters for the RL tasks are shown in Table 5 and Table 6. ", "page_idx": 20}, {"type": "text", "text": "For the zeroth order optimization tasks, we the same hyperparameters as the MBD algorithm. ", "page_idx": 20}, {"type": "text", "text": "A.5.4 Demonstration Collections ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For RRT algorithm in Car2D task, we set the max step size to 0.2, and the max iterations to 1000 given the maximum episode length is 50. ", "page_idx": 20}, {"type": "text", "text": "For the demonstration collection in Humanoid Jogging task, we first download the mocap data which contains each joints\u2019 position in the world frame. Then we use the joint data to calculate the position of torso, thigh and shin position as partial state reference for our task. ", "page_idx": 20}, {"type": "table", "img_path": "BJndYScO6o/tmp/2e634b5973c7c7c0b41e2903b3901dd5c095d3f38b5573ab71a0c407cc8b40da.jpg", "table_caption": [], "table_footnote": ["Table 5: General RL configuration for various environments "], "page_idx": 21}, {"type": "table", "img_path": "BJndYScO6o/tmp/eda5ff6835b955a37c00a87dabad29b5ad6e23e683cca79e3d5da56aec73736a.jpg", "table_caption": [], "table_footnote": ["Table 6: RL specifics for various environments "], "page_idx": 21}, {"type": "text", "text": "A.6 MBD for Online Control ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Even though MBD is designed as a trajectory optimization algorithm, it can be naturally extended to receding horizon control as shown in Algorithm 3. By conditioning the planning on each step\u2019s observation, MBD\u2019s MPC extension can further improve the performance of MBD by $9.6\\%$ in terms of reward, especially given control noise as shown in Fig. 7. ", "page_idx": 21}, {"type": "text", "text": "The online running frequence of MBD MPC is shown in Table 7 on RTX 4070Ti GPU. Please note that the frequency is calculated under the assumption of solving the whole 50 steps TO problem without reduced model at each iteration. Besides, the Brax code environment we used is not optimized for GPU, so the actual frequency could be higher with optimized environment. In our work we just use Brax as a simple and easy-to-use option. As the major computation time of MBD is spent on the forward dynamics simulation, it can be further improved by using more efficient physics engine. ", "page_idx": 21}, {"type": "table", "img_path": "BJndYScO6o/tmp/7331b3f9dcf5ee33aa8e980354b9ac2ea1561577587c0be385c6ef064c59d4dd.jpg", "table_caption": [], "table_footnote": ["Table 7: Online Running Frequency of receding horizon MBD "], "page_idx": 21}, {"type": "text", "text": "A.7 Sample Number Abalation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Given that sampling-based optimization is the core of MBD, we ablate the sample number in Fig. 8. We can see that MBD converges to the optimal point with as few as 128 samples. The harder the task is, the larger performance gap between MBD and other TO solvers. ", "page_idx": 21}, {"type": "text", "text": "Algorithm 3 Model-based Diffusion with Receding Horizon ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1: Initialize: Optimize trajectory $x_{0:T}$ , $u_{0:T-1}$ with MBD   \n2: for $t=0$ to $t_{\\mathrm{final}}$ do   \n3: Observe the state $x_{t}$   \n4: Optimize trajectory $x_{t:t+T}$ , $u_{t:t+T-1}$ with single-step MBD   \n5: Apply the first control input $u_{t}$ to the system   \n6: Shift trajectory for next initialization: ${\\scriptstyle x_{t+1:t+T}}$ , $u_{t+1:t+T-1}$   \n7: end for ", "page_idx": 22}, {"type": "image", "img_path": "BJndYScO6o/tmp/ab2e7403895b8cf88adc3fd0bc471c8801cd90c9487a4f6131def9950fefa853.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 7: Comparison of the performance between RL, MBD and MBD receding horizon version with both perfect model and noisy model (with $5\\%$ control noise). Given perfect model, we find MBD\u2019s performance can be further augmented by $9.6\\%$ with naive receding horizon planning and leads RL by larger $74.2\\%$ . Given noisy model, MBD receding horizon version still outperforms RL by $65.3\\%$ . ", "page_idx": 22}, {"type": "text", "text": "A.8 Objective Function Abalation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Given that RL and TO have different objective settings, especially the horizon difference, we conducted an ablation study by swapping the optimization objectives of MBD and RL. RL optimizes for a longer horizon discounted reward $\\begin{array}{r}{\\bar{J}=\\sum_{t=0}^{\\bar{H}_{\\mathrm{RL}}}\\gamma^{t}r_{t},H=\\bar{1}000,\\gamma<1}\\end{array}$ while MBD optimizes for a spherofroterrm haonrciez oonf  uMnBdiDs caonudn tRedL  cuundmeurl aetaicvhe   r=oe th\u2211wearr\u2019sd so $\\begin{array}{r}{J=\\sum_{t=0}^{H_{\\mathrm{MBD}}}{r_{t},H}=50}\\end{array}$ ,. $\\gamma=1$ . Figure 9 shows the ", "page_idx": 22}, {"type": "text", "text": "MBD outperforms RL by $44.5\\%$ under the RL objective and $805.5\\%$ under the TO objective. The results demonstrate that MBD\u2019s superior performance is attributed to its better diffusion-style iterative optimization process compared with RL\u2019s random exploration. ", "page_idx": 22}, {"type": "image", "img_path": "BJndYScO6o/tmp/27b5a7bbc6645d927819054e9ef16c91a1b7fc0e98a2af7eaa0c427b7903ac4c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 8: Ablation study on the number of samples\u2019s effect on the performance of MBD and other TO solver. For lower-dimensional task like Ant, Hopper, MBD\u2019s performance is less sensitive to the number of samples. For higher-dimensional task like Humanoid, MBD\u2019s performance is more sensitive to the number of samples but still outperforms all the baselines. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Section 1 list the main contributions of the paper, which are accurately reflected in the abstract and introduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 6 discusses the limitations of the proposed approach. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors ", "page_idx": 23}, {"type": "image", "img_path": "BJndYScO6o/tmp/72ad98dba5be0f61c69432358524a3ae9151f07d0941ba8a3ef908a44717d025.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 9: Ablation study on RL and TO objective function. $\\gamma\\;=\\;1,H\\;=\\;50$ is TO objective, $\\gamma\\,<\\,1,H\\,=\\,1000$ is RL objective, where $\\gamma$ is the discount factor a n=d $H$ is  t=he horizon. For both o bj<ective  f=unction, MBD outperforms RL by $44.5\\%$ and $805.5\\%$ on average. We also find RL\u2019s overall performance is better given the discounted objective while MBD\u2019s performance is better given the total objective, which is consistent with our main results. ", "page_idx": 24}, {"type": "text", "text": "should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper includes theoretical results, and all the assumptions and proofs are provided in the main paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 24}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Appendix A.5 provides all the necessary information to reproduce the main experimental results. The code and data are also provided in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, with dedicated README flies and instructions to reproduce the main experimental results comes with the supplemental material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Appendix A.5 provides all the necessary information to understand the experimental results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Main quantitative results in Table 2 report the mean and standard deviation over 8 runs given different random seeds. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the hardware requirements in Appendix A.5.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We mentioned the package used in the paper on Appendix A.5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not introduce new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]