[{"figure_path": "7eFS8aZHAM/figures/figures_2_1.jpg", "caption": "Figure 1: Causal graphs of the SCMs considered in our work.", "description": "This figure shows two causal graphs representing the structural causal models (SCMs) used in the paper to characterize two types of distribution shifts: (a) concept shift and (b) covariate shift. In both cases, C represents the unobservable causal latent variable, S represents the unobservable spurious latent variable, Y represents the node label, G represents the observed graph, and E represents the environment variable. The dashed arrows indicate that the environment variable is not directly observed but affects the data generation process. The concept shift model shows that the causal latent variable directly influences the node labels, while the spurious latent variable influences the graph structure.  The covariate shift model shows that the spurious variable influences the node features but not the node labels.  These models help to explain how different types of distributional shifts can affect node-level out-of-distribution (OOD) generalization on graphs.", "section": "2 A Causal Data Model on Graphs"}, {"figure_path": "7eFS8aZHAM/figures/figures_5_1.jpg", "caption": "Figure 2. The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder \u03c6\u03b8 aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "The figure illustrates the framework of the proposed CIA-LRA method, highlighting the invariant subgraph extractor, GNN encoder, localized alignment, reweighting alignment, and the total loss calculation. It emphasizes the method's ability to identify invariant features and eliminate spurious ones, even without environment labels, by focusing on local node pairs and weighting them based on their neighborhood label distribution discrepancies.", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_7_1.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder de aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "This figure illustrates the framework of the proposed CIA-LRA method, which is designed for node-level out-of-distribution generalization on graphs without environment labels.  It details the steps of invariant subgraph extraction, GNN encoding, localized alignment (restricting alignment to local neighborhoods to avoid feature collapse), and reweighting alignment (prioritizing alignment of node pairs exhibiting significant discrepancies in heterophilic NLD and minor discrepancies in homophilic NLD to remove spurious features while preserving invariant ones).", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_9_1.jpg", "caption": "Figure 3: Left: OOD test accuracy. Mid: the variance of the invariant representation. Right: the norm of the spurious representation. CIA and CIA-LRA use \u03bb = 0.5 in this figure.", "description": "This figure shows the results of an ablation study comparing CIA and CIA-LRA on a synthetic dataset.  The left column displays the out-of-distribution (OOD) accuracy over training epochs for both methods. The middle column shows the variance of the invariant representation, and the right column shows the norm of the spurious representation.  The plots reveal that CIA-LRA is more effective at eliminating spurious features and preserving invariant features, leading to better OOD generalization performance compared to CIA, especially with a larger regularization parameter \u03bb = 0.5.", "section": "5.4 Empirical Understanding of the Role of CIA-LRA"}, {"figure_path": "7eFS8aZHAM/figures/figures_9_2.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder de aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "The figure illustrates the framework of CIA-LRA, highlighting its key components: invariant subgraph extraction, GNN encoding, localized alignment, and reweighting alignment. Localized alignment focuses on aligning nearby nodes to prevent feature collapse, while reweighting alignment prioritizes aligning nodes with significant differences in heterophilic NLD and minor differences in homophilic NLD to eliminate spurious features.  The method leverages localized label distribution to effectively distinguish and preserve invariant features.", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_19_1.jpg", "caption": "Figure 3: Left: OOD test accuracy. Mid: the variance of the invariant representation. Right: the norm of the spurious representation. CIA and CIA-LRA use \u03bb = 0.5 in this figure.", "description": "This figure shows the results of an ablation study comparing the performance of CIA and CIA-LRA on a synthetic dataset.  The left panel displays OOD accuracy, demonstrating that CIA-LRA consistently maintains higher accuracy than CIA across training epochs. The middle panel shows the variance of the invariant representation, illustrating that CIA-LRA preserves higher variance, suggesting that it better maintains the invariant features. The right panel shows the norm (magnitude) of the spurious representation. CIA-LRA effectively reduces the norm of spurious features, confirming that spurious features are eliminated by the proposed method. Overall, the figure highlights that CIA-LRA's ability to avoid collapsing the invariant features during training, which improves generalization performance.", "section": "5.4 Empirical Understanding of the Role of CIA-LRA"}, {"figure_path": "7eFS8aZHAM/figures/figures_19_2.jpg", "caption": "Figure 3: Left: OOD test accuracy. Mid: the variance of the invariant representation. Right: the norm of the spurious representation. CIA and CIA-LRA use \u03bb = 0.5 in this figure.", "description": "This figure visualizes the impact of CIA and CIA-LRA on a synthetic dataset with concept and covariate shifts.  The left panel displays OOD accuracy, demonstrating the superior performance of CIA-LRA. The middle panel shows the variance of invariant representations, highlighting how CIA-LRA maintains higher variance, preventing representation collapse. The right panel shows the norm of spurious representations, illustrating that both CIA and CIA-LRA effectively suppress them, but CIA-LRA is more effective.  Overall, the figure shows that CIA-LRA offers better OOD generalization by effectively balancing the preservation of invariant features and the removal of spurious features.", "section": "5.2 Main Results of OOD Generalization"}, {"figure_path": "7eFS8aZHAM/figures/figures_20_1.jpg", "caption": "Figure 3: Left: OOD test accuracy. Mid: the variance of the invariant representation. Right: the norm of the spurious representation. CIA and CIA-LRA use \u03bb = 0.5 in this figure.", "description": "This figure visualizes the effects of CIA and CIA-LRA on OOD generalization performance using a synthetic dataset.  The left panel shows the OOD accuracy over training epochs for both methods. The middle panel displays the variance of the invariant representation, indicating the stability of the learned features.  The right panel shows the norm of the spurious representation, reflecting the influence of irrelevant information. The results demonstrate that CIA-LRA is superior in maintaining the stability of invariant features while effectively reducing spurious ones, leading to better OOD generalization.", "section": "5.4 Empirical Understanding of the Role of CIA-LRA"}, {"figure_path": "7eFS8aZHAM/figures/figures_21_1.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder de aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "This figure illustrates the overall framework of the proposed CIA-LRA method for node-level out-of-distribution generalization on graphs.  CIA-LRA leverages invariant subgraph extraction, localized alignment (to avoid feature collapse), and reweighted alignment (based on heterophilic and homophilic neighborhood label distributions) to learn invariant representations without relying on environment labels.", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_22_1.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder de aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "This figure shows the overall framework of the proposed CIA-LRA method, which consists of an invariant subgraph extractor, a GNN encoder, localized alignment, and reweighting alignment.  The invariant subgraph extractor identifies invariant subgraphs. The GNN encoder aggregates information from these subgraphs. Localized alignment aligns representations of nearby nodes to prevent over-alignment. Reweighting alignment assigns weights based on differences in heterophilic and homophilic neighborhood label distributions to eliminate spurious features while preserving invariant ones.", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_23_1.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03a6\u03b8 identifies the invariant subgraph for each node. Then the GNN encoder de aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "This figure illustrates the framework of the proposed CIA-LRA method.  It shows how invariant subgraphs are extracted for each node using a GNN encoder, and then how node representations are generated. It details the two main strategies of the method: localized alignment (restricting alignment to nearby nodes) and reweighting alignment (assigning higher weights to node pairs with significant differences in heterophilic NLD and minor differences in homophilic NLD to better eliminate spurious features).", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_24_1.jpg", "caption": "Figure 2: The overall framework of our proposed CIA-LRA. The invariant subgraph extractor \u03d5\u03b8m identifies the invariant subgraph for each node. Then the GNN encoder \u03d5\u0398 aggregates information from the estimated invariant subgraphs to output node representations. CIA-LRA mainly contains two strategies: localized alignment and reweighting alignment. Localized alignment: we restrict the alignment to a local range to avoid overalignment that may cause the collapse of invariant features (shown in Appendix D.1). Reweighting alignment: to better eliminate spurious features and preserve invariant features without using environment labels, we assign large weights to node pairs with significant discrepancies in heterophilic Neighborhood Label Distribution (NLD) and minor discrepancies in homophilic NLD. See Section 3.2 for a detailed analysis of CIA-LRA.", "description": "This figure illustrates the overall framework of the proposed CIA-LRA method.  It shows how the method identifies invariant subgraphs, uses a GNN encoder to produce node representations, and employs localized and reweighting alignment strategies to eliminate spurious features and preserve invariant features, all while not requiring environment labels.  The localized alignment restricts alignment to a local graph area, while the reweighting alignment prioritizes aligning node pairs that show significant differences in heterophilic neighborhood label distribution (HeteNLD) and minor differences in homophilic NLD.", "section": "3 The Proposed Methods"}, {"figure_path": "7eFS8aZHAM/figures/figures_26_1.jpg", "caption": "Figure 12: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora word domain, covariate shift. Each sub-figure is a class, and each dot in the figure represents a node pair in the graph. The red line is obtained by linear regression. The positive correlation is clear.", "description": "This figure shows the relationship between the distance of aggregated neighborhood representations and the distance of Heterophilic Neighborhood Label Distribution (HeteNLD) on the Cora dataset under covariate shift. Each subplot represents a different class, and each dot represents a node pair.  A linear regression line is fitted to each plot, highlighting a clear positive correlation between the two variables, suggesting that HeteNLD can reflect spurious features on graphs.", "section": "D.5 Heterophilic Neighborhood Labels Distribution Reflect Spurious Feature Distribution"}, {"figure_path": "7eFS8aZHAM/figures/figures_26_2.jpg", "caption": "Figure 3: Left: OOD test accuracy. Mid: the variance of the invariant representation. Right: the norm of the spurious representation. CIA and CIA-LRA use \u03bb = 0.5 in this figure.", "description": "This figure visualizes the results of the experiments conducted on a synthetic dataset to understand the effects of CIA and CIA-LRA on OOD generalization.  The left panel shows the OOD test accuracy for both methods over training epochs. The middle panel displays the variance of the invariant representation, indicating the stability of learned features, and the right panel shows the norm of the spurious representation, illustrating the extent to which the model relies on irrelevant features.  The results show that CIA-LRA maintains better performance and prevents the collapse of invariant representations which is observed with CIA as \u03bb increases.", "section": "5.4 Empirical Understanding of the Role of CIA-LRA"}, {"figure_path": "7eFS8aZHAM/figures/figures_27_1.jpg", "caption": "Figure 12: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora word domain, covariate shift. Each sub-figure is a class, and each dot in the figure represents a node pair in the graph. The red line is obtained by linear regression. The positive correlation is clear.", "description": "This figure visualizes the correlation between the distance of aggregated neighborhood representations and the Heterophilic Neighborhood Label Distribution (HeteNLD) discrepancy for each class in the Cora dataset under covariate shift.  Each subplot represents a class, with each dot showing a node pair. A linear regression line is fitted to highlight the positive correlation. This suggests HeteNLD discrepancy effectively captures the difference in aggregated neighborhood representations.", "section": "D.5 Heterophilic Neighborhood Labels Distribution Reflect Spurious Feature Distribution"}, {"figure_path": "7eFS8aZHAM/figures/figures_27_2.jpg", "caption": "Figure 12: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora word domain, covariate shift. Each sub-figure is a class, and each dot in the figure represents a node pair in the graph. The red line is obtained by linear regression. The positive correlation is clear.", "description": "This figure shows the relationship between the distance of the aggregated neighborhood representation and the distance of HeteNLD on the Cora word domain under covariate shift. Each subplot represents a class, and each dot in a subplot represents a node pair in the graph.  Linear regression lines are included.  The positive correlation shown supports the claim that HeteNLD discrepancy reflects the distance of the aggregated neighborhood representation.", "section": "D.5 Heterophilic Neighborhood Labels Distribution Reflect Spurious Feature Distribution"}, {"figure_path": "7eFS8aZHAM/figures/figures_28_1.jpg", "caption": "Figure 12: The relationship between the distance of the aggregated neighborhood representation and distance of HeteNLD on Cora word domain, covariate shift. Each sub-figure is a class, and each dot in the figure represents a node pair in the graph. The red line is obtained by linear regression. The positive correlation is clear.", "description": "This figure shows the correlation between the distance of aggregated neighborhood representation and the HeteNLD (heterophilic neighborhood label distribution) distance for each class in the Cora dataset under covariate shift. Each point represents a pair of nodes with the same number of homophilic neighbors.  The positive correlation suggests that HeteNLD distance effectively reflects the difference in aggregated neighborhood representations, supporting the use of HeteNLD in CIA-LRA for identifying node pairs with significant spurious feature differences for alignment.", "section": "D.5 Heterophilic Neighborhood Labels Distribution Reflect Spurious Feature Distribution"}]