[{"type": "text", "text": "Knowledge Composition using Task Vectors with Learned Anisotropic Scaling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Frederic Z. Zhang\u2217 Paul Albert\u2217 Cristian Rodriguez-Opazo Anton van den Hengel Ehsan Abbasnejad ", "page_idx": 0}, {"type": "text", "text": "Australian Institute for Machine Learning The University of Adelaide {firstname.lastname}@adelaide.edu.au https://github.com/fredzzhang/atlas ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning that enables the model to capture these specialised representations. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks enables modular learning that effectively leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One practical advantage of neural networks is the fact that knowledge learned from a previous problem, in the form of network weights, can be transferred to solve other related problems. Commonly referred to as transfer learning [6, 73], this technique is often applied when a model trained on a general-purpose dataset\u2014ImageNet [52] for many years\u2014is fine-tuned on other datasets to improve performance on downstream problems. In the past, classification models [18, 53] have been used as the medium for such knowledge transfer, which played a crucial part in the success of detection and segmentation [7, 19, 51, 66\u201368]. In recent years, foundation models [4] trained on broad data, CLIP [47] particularly, have demonstrated strong performance on a multitude of tasks, even when applied in a zero-shot manner. Besides the conventional way of exploiting the knowledge in these models via fine-tuning, recent works [28, 44, 62] have presented more direct measures to manipulate the network weights. In particular, Ilharco et al. [28] showed that, a task vector, defined as the weight difference between a pre-trained and a fine-tuned model, can be used as a carrier of the task-specific knowledge learned via fine-tuning. As such, multiple task vectors, when combined with simple arithmetic, can form a multi-task model that largely retains its performance across all fine-tuning tasks. Linearisation techniques [44], in addition, have been shown to further enhance this compositionality. ", "page_idx": 0}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/911c686d2505db387b096f506557d2c2ff6e60b353319f41a9c15d30df9088ad.jpg", "img_caption": ["Figure 1: Illustration of (a) learning task vector compositions ( $n=2$ , $\\pmb{\\theta}_{0}$ denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector $\\tau=\\left(\\tau^{(1)},\\tau^{(2)}\\right)$ has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Intrigued by this phenomenon, we investigate the potential of task vectors being knowledge carriers in this paper, by learning linear combinations of them (Figure 1a) for various problems. In particular, parameter blocks, e.g., weights and biases, tend to encode different learned representations in different layers. We thus learn an independent scaling coefficient per block for more precise adjustments tailored to the unique roles of each parameter block. This results in anisotropic scaling of task vectors (Figure 1b), and allows us to exploit their modularity in knowledge composition, granting higher controllability when steering the behaviours of a model for task arithmetic [28]. ", "page_idx": 1}, {"type": "text", "text": "The potential applications of task vector composition extend beyond model editing. With the coefficients being the only learnable parameters, our method exploits the rich knowledge encapsulated in the task vectors by searching in a low-dimensional coefficient space. As a result, it is a competitive parameter-efficient fine-tuning (PEFT) method, and is particularly effective in cases where labelled data is scarce. This offers new opportunities for few-shot learning [34, 69] and test-time adaptation [35, 57]. Furthermore, for multi-purpose models such as CLIP [47], variants of the model trained with different data sources or fine-tuned on different downstream tasks are often available [26]. These resources constitute a significant knowledge bank, with task vectors being the knowledge carrier. Many learning problems may be simplified to learning a combination of task vectors. ", "page_idx": 1}, {"type": "text", "text": "Our primary contribution is a learning algorithm named aTLAS, wherein otherwise complex learning problems can be framed as learning linear combinations of task vectors. The algorithm is broadly applicable to optimising supervised and unsupervised objectives. Its effectiveness is demonstrated in task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, where we show that (1) learning linear combinations of task vectors directly exploits the low intrinsic dimensionality of pre-trained models [1, 33], resulting in a small number of learnable parameters; (2) standard task vectors, otherwise inferior to linearised variants [44] in task arithmetic, can produce stronger multi-task models with learned anisotropic scaling; (3) aTLAS is effective in low-data regimes, and improves the accuracy of CLIP by 6.5 absolute points averaged over 22 datasets with unlabelled data; (4) aTLAS is complementary to previous few-shot adaptation methods, in that one third of the examples it improves upon are unique; (5) aTLAS as a few-shot learning method is less prone to domain shift, and achieves better generalisation on out-of-domain datasets; (6) the most informative parameter blocks from different task vectors can be mixed prior to training, allowing for flexible and efficient knowledge transfer under memory constraints; (7) aTLAS is a strong PEFT method when data is limited, and existing PEFT methods such as low-rank adaptations (LoRA) [23] can be seamlessly integrated into aTLAS to improve memory efficiency. ", "page_idx": 1}, {"type": "text", "text": "2 Models and task vectors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As Ilharco et al. [28] demonstrated, task vectors exhibit many intriguing properties across a wide range of models, such as CLIP [47], GPT-2 [46] and T5-based models [48]. To facilitate more in-depth experimentation and analysis, we focus on the CLIP model in this paper, due to its wide availability and manageable size. In particular, we follow previous practice [28, 44] and acquire task vectors by fine-tuning the image encoder, with the text representations frozen. This ensures that image encoders fine-tuned on different datasets produce features residing in the same representation space, through a common text encoder. The task vectors obtained from these fine-tuned encoders can thus be combined more effectively to form a unified multi-task model. ", "page_idx": 2}, {"type": "text", "text": "Formally, denote the CLIP image encoder by $f:{\\mathcal{X}}\\times\\Theta\\to{\\mathcal{Z}}$ , such that for input image $\\mathbf{x}\\in\\mathcal{X}$ and parameters $\\pmb\\theta\\in\\Theta$ , $\\mathbf{z}=f(\\mathbf{x};\\theta)$ is the learned latent representation for the input image. Denote the weights of a pre-trained model by $\\pmb\\theta_{0}$ , and the weights of its fine-tuned variant by $\\theta_{i},i\\in\\mathbb{N}^{+}$ , where $i$ indexes a dataset $\\mathcal{D}_{i}$ . We follow Ilharco et al. [28] and define a task vector as $\\pmb{\\tau}_{i}=\\pmb{\\theta}_{i}-\\pmb{\\theta}_{0}$ . In addition, we investigate task vectors produced by linearised variants of the image encoder using the first-order Taylor expansion, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\mathbf{x};\\pmb{\\theta}):=f(\\mathbf{x};\\pmb{\\theta}_{0})+(\\pmb{\\theta}-\\pmb{\\theta}_{0})^{\\top}\\nabla_{\\pmb{\\theta}}f(\\mathbf{x};\\pmb{\\theta}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Ortiz-Jim\u00e9nez et al. [44] showed that, task vectors obtained from fine-tuning the linearised variants have low disentanglement errors, and exhibit strong compositional properties. ", "page_idx": 2}, {"type": "text", "text": "3 Learning task vector compositions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Parameters in a neural network, depending on the depth of the layer, often have different significance. For instance, early layers in convolutional neural networks [18, 53] are known for extracting generic, low-level features, such as edges, corners, etc., while deeper layers produce features more specific to the task. We recognise the non-uniform impacts parameters at different layers can have, and do not perform isotropic scaling on task vectors. Instead, weights, biases and any other forms of parameterisation, which we collectively refer to as parameter blocks, will be scaled independently. ", "page_idx": 2}, {"type": "text", "text": "3.1 Proposed method: aTLAS ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, denote a task vector with $m$ parameter blocks by $\\pmb{\\tau}\\,=\\,\\left(\\pmb{\\tau}^{(1)},\\dots,\\pmb{\\tau}^{(m)}\\right)$ , where each parameter block $\\tau^{(j)}$ is vectorised, and round brackets denote column vector concatenation. We learn a block diagonal matrix $\\Lambda$ , parameterised as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda=\\left[\\begin{array}{c c c c}{{\\lambda^{(1)}I^{(1)}}}&{{\\ldots}}&{{{\\bf0}}}\\\\ {{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{{\\bf0}}}&{{\\ldots}}&{{\\lambda^{(m)}I^{(m)}}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda^{(j)}\\in\\mathbb{R}$ is a learnable coefficient; $I^{(j)}$ denotes an identity matrix with its number of columns matching the dimension of $\\tau^{(j)}$ ; and the superscript $j\\in\\mathbb{N}^{+}$ indexes a parameter block. This results in anisotropic scaling of a task vector, that is, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda_{i}\\pmb{\\tau}_{i}=\\Big(\\lambda_{i}^{(1)}\\pmb{\\tau}_{i}^{(1)},\\cdot\\cdot\\cdot,\\lambda_{i}^{(m)}\\pmb{\\tau}_{i}^{(m)}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the subscript $i\\in\\mathbb{N}^{+}$ indexes a task vector. As such, assuming a supervised objective, finding the optimal composition of task vectors can be defined as the following optimisation problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Lambda_{1},\\ldots,\\Lambda_{n}}{\\arg\\operatorname*{min}}\\ \\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}_{t}}\\big[\\mathcal{L}\\big(f(\\mathbf{x};\\pmb{\\theta}_{0}+\\sum_{i=1}^{n}\\Lambda_{i}\\pmb{\\tau}_{i}),\\mathbf{y}\\big)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}$ is the loss function for a target task; $n$ is the number of task vectors; $\\mathbf{y}$ is the labels corresponding to inputs $\\mathbf{x}$ ; $\\mathcal{D}_{t}$ denotes a target dataset. The number of learnable parameters, as a result, is precisely $m n$ , Let us denote the solution to the aforementioned optimisation problem by $\\{\\Lambda_{i}^{\\star}\\}_{i=1}^{n}$ . In inference, model $\\begin{array}{r l}{f(\\mathbf{x},\\pmb{\\theta}_{0}+\\sum_{i=1}^{n}\\Lambda_{i}^{\\star}\\pmb{\\tau}_{i})}\\end{array}$ will be deployed, which incurs no additional computational cost compared to models trained in the conventional way. ", "page_idx": 2}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/90311083e9d9f6620cabe1769ceb4b88490ff90b405c802e20adb5be68c5ac29.jpg", "img_caption": ["Figure 2: Recognition accuracy versus the number of bases when optimising in a low-dimensional subspace. The accuracy is normalised by that of the fully fine-tuned model. Using task vectors to construct the projection matrix performs consistently better than using random bases on (a) MNIST [32], (b) CIFAR100 [31]. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In addition, we investigate the task vectors obtained from fine-tuning linearised variants of the model, i.e., $g(x)$ in Eq. 1. Denote such task vectors by $\\widetilde{\\tau}$ . The learning objective with linearised task vectors can be derived as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Lambda_{1},\\ldots,\\Lambda_{n}}{\\arg\\operatorname*{min}}\\ \\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}_{t}}\\bigg[\\mathcal{L}\\Big(f\\big(\\mathbf{x};\\pmb{\\theta}_{0}\\big)+\\big(\\sum_{i=1}^{n}\\Lambda_{i}\\widetilde{\\pmb{\\tau}}_{i}\\big)^{\\mathsf{T}}\\nabla_{\\theta}f\\big(\\mathbf{x};\\pmb{\\theta}_{0}\\big),\\mathbf{y}\\Big)\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Relation to intrinsic dimensionality ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A notable characteristic of aTLAS is its parameter efficiency. To offer more intuitions, we refer to previous findings [1, 33] that deep neural networks often produce solutions residing in a subspace with much lower intrinsic dimensionality. This is measured by finding a minimum number of $d$ parameters, such that learning these parameters $(\\pmb{\\hat{\\theta}}\\in\\mathbb{R}^{d})$ leads to approximately the same performance as optimising in the full parameter space $\\left(\\pmb{\\theta}\\in\\mathbb{R}^{D}\\right)$ ). This can be expressed as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb\\theta}={\\pmb\\theta}_{0}+P\\hat{\\pmb\\theta},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{D}$ denotes the pre-trained weights and $P\\in\\mathbb{R}^{D\\times d}$ is a random projection matrix. We demonstrate that learning task vector compositions leads to the same formulation. For brevity of exposition, let us consider compositions at the block level. For the $j$ -th parameter block, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\theta}^{(j)}=\\pmb{\\theta}_{0}^{(j)}+\\sum_{i=1}^{n}\\lambda_{i}^{(j)}\\pmb{\\tau}_{i}^{(j)}}\\\\ &{\\qquad=\\pmb{\\theta}_{0}^{(j)}+\\underbrace{\\left[\\pmb{\\tau}_{1}^{(j)},\\dots,\\pmb{\\tau}_{n}^{(j)}\\right]}_{\\mathrm{projection\\,matrix}}\\underbrace{\\left[\\lambda_{1}^{(j)},\\dots,\\lambda_{n}^{(j)}\\right]^{\\mathsf{T}}}_{\\mathrm{learnable\\,parameters}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We draw a parallel between Eqs. 6 and 8 and note that aTLAS explicitly exploits the low intrinsic dimensionality by learning a small set of coefficients. The number of task vectors, i.e., $n$ , is much smaller than the dimension of weight vector $\\pmb{\\theta}_{i}^{(j)}$ , and is analogous to the intrinsic dimensionality $d$ . However, as opposed to using a random projection matrix $P$ , aTLAS constructs the projection matrix from task vectors, making use of the learned representations. To demonstrate its advantage, we use the same number of bases for task vectors2 and random bases3, and show that task vectors consistently achieve higher performance in Figure 2. These results solidify our understanding of task vectors being knowledge carriers. We thus set out to apply aTLAS to various applications. ", "page_idx": 3}, {"type": "text", "text": "4 Task arithmetic", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Task arithmetic [28] is comprised of a few tasks aimed at editing pre-trained models using task vectors. Following previous practice [28, 44], we conduct experiments under the settings of task negation and task addition on eight image classification datasets (details included in Appendix A). ", "page_idx": 3}, {"type": "table", "img_path": "", "table_caption": ["Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least $95\\%$ of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7. "], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/7545d2b456059aa5c77590f4bc3176a4e767fa8a8de3ed796db1711dfdd799e0.jpg", "img_caption": ["Figure 3: Box-and-whisker plots for the learned coefficients. As each transformer layer consists of a fixed set of parameter blocks, we visualise the distribution of coefficients for these parameter blocks across all layers, for (a) task negation and (b) task addition, as well as (c) distribution of coefficients by layer. We denote the learnable LayerNorm parameters by $\\gamma$ and $\\beta$ . Weights and biases are denoted by $W$ and $\\mathbf{b}$ , respective, with attention layer parameters indexed by superscripts and the MLP parameters indexed by subscripts. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Previous works acquire the optimal isotropic scaling factor on task vectors via a hyper-parameter search on validation sets. As such, we learn anisotropic scaling matrices on the same validation sets, and visualise the learned coefficients to shed light on this mechanism. ", "page_idx": 4}, {"type": "text", "text": "4.1 Task negation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Task negation aims to reduce undesired biases, characterised by the performance, on a target task, while maintaining performance on a control dataset, ImageNet [52] in this case. Denote the validation sets for the target and control tasks by $\\mathcal{D}_{t}$ and $\\mathcal{D}_{c}$ , respectively. We perform a simultaneous gradient ascent on the target task and gradient descent on the control task, described as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\Lambda_{t}}{\\arg\\operatorname*{min}}\\,\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}_{t}}[-\\mathcal{L}(f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{t}\\pmb{\\tau}_{t}),\\mathbf{y})]+\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}_{c}}[\\mathcal{L}(f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{t}\\pmb{\\tau}_{t}),\\mathbf{y})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau_{t}$ is the task vector for the target dataset, and cross-entropy loss is used. The learning objectives with linearised task vectors can be derived easily based on Eq. 5, and so are omitted. ", "page_idx": 4}, {"type": "text", "text": "We summarise the task negation results in Table 1, and show that our method significantly improves upon standard task vectors, while the improvement upon linear task vectors is less prominent. In particular, we observe that weights matrices tend to have much larger negative coefficients, as shown in Figure 3a. To investigate this, we instead only learn coefficients for the weight matrices, with zero coefficients on other parameter blocks, effectively reducing the number of learnable parameters by two thirds. With ViT-B/32 as the backbone, we observe an average accuracy of 20.14 (vs. 18.76) on target tasks and 61.23 (vs. 61.21) on the control task, which shows that weight matrices carry majority of the knowledge required for task negation. ", "page_idx": 4}, {"type": "text", "text": "4.2 Task addition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Task addition aims at producing a multi-task model using task vectors acquired from a range of datasets. We utilise task vectors from the eight image classification datasets, and learn the anisotropic scaling matrices with the objectives described in Eqs. 4, 5 using the cross-entropy loss. The training data is comprised of the validation sets for all eight dataset, i.e., $\\textstyle D_{t}=\\bigcup_{i=1}^{8}D_{i}$ . ", "page_idx": 4}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/03a1de8989fb9d2f46cea44dcea5a83347e69b73a6407d4cb59c77008007af18.jpg", "table_caption": ["Table 2: Performance of task addition averaged across eight datasets. We report the absolute accuracy (Abs.) and the relative accuracy (Rel.) with respect to the fine-tuned model. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 8. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Performance comparison against previous methods is shown in Table 2, where our method yields substantial improvements. Interestingly, we note that with previous methods [28, 44], linear task vectors outperform the standard ones in terms of absolute accuracy, while the converse is true with our method. To investigate this, we compute the pairwise disentanglement error $\\xi\\left[44\\right]$ , which measures the percentage of data with inconsistent predictions when two task vectors are combined (more details in Appendix C.2). Results in Figure 4 show that standard task vectors with learned anisotropic scaling achieve the lowest average error, indicating less interference in task vector composition. Along with higher fine-tuning accuracy, previously referred to as the non-linear advantage [44], standard task vectors demonstrate stronger performance in task addition. ", "page_idx": 5}, {"type": "text", "text": "Furthermore, we again observe that weight matrices have consistently larger coefficients in Figure 3b, and learning coefficients on weight matrices alone results in an accuracy of 84.17 (vs. 84.98) using ViT-B/32. This suggests that weight matrices in transformers are the primary knowledge carrier, which enabled knowledge composition and negation. Note that for better clarity in visualisation, we add $L_{1}$ regularisation on the learned coefficients during learning, which causes marginal performance drop (84.23 vs. 84.98) but significantly improves interpretability. In addition, we observe substantially higher coefficients on deeper layers (Figure 3c). This aligns with our understanding that early layers extract generic features that do not vary significantly across datasets [29], while the deeper layers produce task-specific features and require more careful adaptations. ", "page_idx": 5}, {"type": "text", "text": "5 Knowledge transfer in low-data regimes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Beyond model editing for task arithmetic, we explore the idea of transferring existing knowledge in task vectors to previously unseen tasks. To this end, we use the CLIP [47] model and a total of 22 image classification datasets, each of which produces a task vector. We defer the details of datasets and the process to acquire task vectors to Appendix A. Denote the set of available task vectors by $T=\\{\\tau_{i}\\}_{i=1}^{n}$ , and the dataset corresponding to task vector $\\tau_{i}$ by $\\mathcal{D}_{i}$ . For each target dataset $\\mathcal{D}_{t}$ , we learn task vector compositions using the subset $T\\setminus\\{\\tau_{t}\\}$ , excluding the task vector for the target dataset to avoid information leakage. We test our method in few-shot and test-time adaptation, to demonstrate its effectiveness in low-data regimes. Notably, we observe that task vectors complement existing few-shot methods. Combining aTLAS with them thus leads to significant improvements. ", "page_idx": 5}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/182fb51a104dac5e47d3e66c88ed45bb34fadd918d4b09c7c5d46141d31402a1.jpg", "img_caption": ["Figure 5: Few-shot experiment results averaged across 22 datasets and three seeds, showing (a) comparison against state-of-the-art few-shot methods with ViT-B/32 backbone and (b) percentage of images in the validation sets that become correctly classified after applying few-shot methods. We also show (c) performance difference compared to pre-trained CLIP model on OOD datasets. More detailed results are included in Appendix D. ", "(a) Few-shot recognition performance (b) Images $(\\%)$ that become correctly classified (c) Improvements on OOD datasets "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.1 Few-shot adaptation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Few-shot recognition requires learning new objects or concepts using a limited amount labelled data\u2014 $\\cdot k$ per class for $k$ -shot. Following previous practice [69], we approach this problem by adapting a pre-trained CLIP model [47] to each target dataset $\\mathcal{D}_{t}$ . We use the subset of task vectors $T\\setminus\\{\\tau_{t}\\}$ and $k\\in\\{1,2,4,8,16\\}$ images from dataset $\\mathcal{D}_{t}$ . During training, we adopt the cross-entropy loss and minimise objectives described in Eqs. 4 and 5 for standard and linear task vectors, respectively. ", "page_idx": 6}, {"type": "text", "text": "We compare against Tip-Adapter [69] and $\\mathrm{LP++}$ [25] using CLIP with ViT-B/32 backbone, across 22 datasets over three random seeds, and summarise the results in Figure 5a. We show that with $k=1$ , our approach, aTLAS, significantly outperforms previous methods, demonstrating the effectiveness of knowledge transfer with scarce labelled data. More importantly, we note that the idea of task vector composition is highly complementary to those presented in previous methods. As such, combining aTLAS with them results in significant improvements. This is also illustrated in Figure 5b as a Venn diagram, where we show the percentage of examples in the validation set that are incorrectly classified by the pre-trained model but correctly classified with few-shot methods. Out of the examples aTLAS improves upon, around half are unique compared against either Tip-Adapter or $\\mathrm{LP++}$ , demonstrating its complementarity. We also found that standard task vectors generally perform better than their linearised counterparts, and so defer the results of linear task vectors to Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "In addition, due to the low number of learnable parameters, aTLAS exhibits strong generalisability. To demonstrate this, we learn task vector composition on ImageNet [52], and test it on out-of-domain (OOD) datasets: ImageNet-A [22], ImageNet-R [21], ImageNet-sketch [60] and ImageNetV2 [50]. We summarise the results in Figure 5c, which shows the performance difference against the pre-trained model. Notably, aTLAS is the only method that consistently improves upon the pre-trained model on OOD datasets, and combining aTLAS with other methods can improve their generalisability. ", "page_idx": 6}, {"type": "text", "text": "We also test our method and variants integrated with Tip-Adapter and ${\\mathrm{LP}}++$ using other backbones, including ViT- $\\{{\\bf B}/16,\\mathrm{L}/14\\}$ and ResNet- $\\{50,101\\}$ , and find that the results are consistent with those for ViT-B/32. More details can be found in Appendix D.3. ", "page_idx": 6}, {"type": "text", "text": "5.2 Task vector budget and selection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In practical applications, there may only be a limited number of task vectors available, or the number of task vectors used in training may be restricted due to memory constraints. To this end, we study the influence of task vector budget $b$ on few-shot recognition performance. We experiment with four selection strategies: (1) random selection; (2) feature-based selection; (3) gradient-based selection; and (4) blockwise gradient-based selection. To elaborate, feature-based selection computes the mean image feature representation of each dataset, and selects $b$ task vectors from datasets most similar to the target dataset. Gradient-based selection computes the gradient with respect to each of the learnable coefficients, and either select entire task vectors with the highest $L_{1}$ gradient norm, or select task vectors with the highest blockwise gradient for the corresponding parameter block, and repeat the process for all parameter blocks. The blockwise selection therefore allows parameter blocks across different task vectors to be mixed prior to training. More details can be found in Appendix D.6. ", "page_idx": 6}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/ec468fc9267c548613dc1265510b14189173a1720c81471e802749ee141968fa.jpg", "table_caption": ["Table 3: Test-time adaptation accuracy averaged over 22 dataset, with $\\times1$ standard error over 3 random seeds. LN refers to tuning the LayerNorm layers. CLIP with the ViT-B/32 backbone is used. Highest performance is highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For a task vector budget $b\\in\\{1,2,5,10,15,21\\}$ , we summarise the few-shot recognition performance in Figure 6. First, we note that the accuracy of aTLAS does not plateau with the maximum number of task vectors available (21), indicating that more task vectors could be beneficial. Second, we find that selecting task vectors based on feature similarity is a simple yet effective approach with sufficient budgets $[b>5]$ ). Selecting whole task vectors with gradient is less effective, generally on par with random selection. Nevertheless, the blockwise variant achieves the best accuracy, particularly for very low budgets $(b\\in\\{1,2\\})$ ), as it is able to exploit knowledge from more task vectors than the budget dictates. We thus deduce that parameter blocks can function as knowledge carriers in isolation, independent of the task vectors to which they belong. In fact, a parameter block $\\tau^{(1)}$ as part of the task vector $\\pmb{\\tau}=\\left(\\bar{\\pmb{\\tau}}^{(1)},\\dots,\\pmb{\\tau}^{(m)}\\right)$ can be considered as a task vector by itself, i.e., $(\\tau^{(1)},\\mathbf{0},\\dots,\\mathbf{0})$ . This modular nature underscores the potential of task vectors for flexible and efficient knowledge transfer. ", "page_idx": 7}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/3de48a811e9d85ab86a5af6357d520d61263cb9d53eb7e5c221044f2090683c1.jpg", "img_caption": ["Figure 6: Few-shot performance of aTLAS with various task vector budgets. The accuracy is averaged across 22 datasets and over three random seeds. Standard deviation $\\times1$ is overlaid as the error margin. Performance under the 16-shot setting is visualised, while additional detailed results are included in Table 14. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Test-time adaptation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Test-time adaptation (TTA) [35, 57, 59] assumes no labelled data is available for the target task, requiring the model to adapt in an unsupervised fashion. We conduct experiments under the offline adaptation setting, which allows access to the target dataset. We consider three categories of selfsupervised techniques for TTA: constrastive objectives, entropy objectives and pseudo labelling. Contrastive objectives align representations of the same image under different data augmentations. For this category, we adopt SimCLR [9], a simple yet effective method. Entropy objectives encourage the pre-trained model to produce confident predictions on unseen datasets by minimising the entropy over the predictions. This technique was previously explored by Yang et al. [65] in model merging. While effective in simpler cases, it can lead to catastrophic collapse in TTA. Therefore, we utilise a state-of-the-art sharpness-aware entropy minimisation algorithm named SAR [43]. Last, we experiment with an unsupervised pseudo-labelling algorithm inspired by FixMatch [54], which we refer as unsupervised FixMatch (UFM). UFM selects an equal number of highly confident examples per class as the labelled set, and then employs FixMatch to produce pseudo-labels from rest of the unlabelled examples. Details are available in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "We summarise the results in Table 3 and compare our method, i.e., learning task vector compositions, against the conventional approach of tuning the layer normalisation parameters [43, 57, 59]. We show that under all self-supervised objectives, aTLAS achieves higher accuracy than tuning the LayerNorm. In particular, LayerNorm has 30k learnable parameters with ViT-B/32 while our method only has $3.5\\mathrm{k}$ learnable parameters. We note that with the UFM objective, aTLAS performs the best and improves the accuracy by an average of 6.5 absolute points over the zero-shot baseline. ", "page_idx": 7}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/af77f7aa19ec1ac97968cce2beda57ddaff5384083beefbfd6b235fceccc46e7.jpg", "table_caption": ["Table 4: Few-shot recognition performance using standard task vectors or LoRAs as sparse task vectors. Results are averaged across 22 datasets over three seeds, with $\\times1$ standard deviation. The memory consumption for ViT-B/32 backbone is annotated under each variant. For standard task vectors, we learn compositions on all parameter blocks or weight matrices only. For LoRAs as task vectors, we report results with rank 4, 16 and 64. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Relation to parameter-efficient fine-tuning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One of the key advantages of aTLAS is its ability to adapt pre-trained models with few learnable parameters, making it suitable for parameter-efficient fine-tuning (PEFT). Similar to popular PEFT methods such as low-rank adaptation (LoRA) [23], our approach does not introduce additional modules, thereby avoiding an increase in inference complexity. In addition, since only the encoded weight matrices in LoRAs have non-zero weight difference, LoRAs are in fact sparse task vectors. They can thus be seamlessly integrated into our method, significantly reducing the memory cost. ", "page_idx": 8}, {"type": "text", "text": "6.1 LoRAs as task vectors ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Due to the sparsity and rank deficiency, LoRAs as task vectors may have limited representation capacity and carry less knowledge. Therefore, they may be inferior to standard task vectors for knowledge transfer. We investigate this by learning linear combinations of $\\mathrm{LoRAs^{4}}$ using our method, under the settings of few-shot recognition. Results are summarised in Table 4. We first shed light on the impact of sparsity, and compare two variants of our method that either learns linear combinations of all parameter blocks or just the weight matrices. Results show that sparsity results in an accuracy decrease of around $0.5\\%$ on average, except for the one-shot setting. The rank deficiency, on the other hand, causes more substantial accuracy drop. Nevertheless, this can be largely mitigated by increasing the rank. Using a rank of 64 leads to similar performance compared to learning compositions of only weight matrices in standard task vectors. In conclusion, while the sparsity and rank deficiency introduce some performance drops, especially in low-shot settings, LoRAs are competitive alternatives to standard task vectors due to their low memory cost. ", "page_idx": 8}, {"type": "text", "text": "6.2 Scalability of aTLAS ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Despite the parameter efficiency of aTLAS, its performance is not as competitive when sufficient training data is available. To address this, we devise a strategy to flexibly scale up the number of learnable parameters as needed. Specifically, we randomly divide each parameter block into $K$ partitions, and assign a learnable coefficient to each partition, naturally increasing the number of learnable parameters by $K$ -fold. We denote these variants by aTLAS $\\times K$ . We conduct experiments with these variants using $\\{1,5,10,\\bar{25},35,50,100\\}\\%$ of the total available training data across the 22 datasets used in Section 5. The results are summarised in Figure 7, showing that our method consistently improves as $K$ increases. Compared to LoRAs, particularly with limited training data, ", "page_idx": 8}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/52289e8d82fb94cb9eb6d9d22ff504458e027256d3db9291f9064bf08f79cb8e.jpg", "img_caption": ["Figure 7: Scalability of aTLAS. We compare the accuracy of our method against LoRAs, and vary the amount of training data. Results are averaged over 22 datasets. Detailed results are included in Table 17. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "our method achieves higher performance with fewer learnable parameters. With sufficient training data, the variant aTLAS $\\times1200$ leads to higher performance with a similar number of learnable parameters, as it is able to exploit the knowledge contained in the task vectors that may otherwise be unobtainable from the target dataset. ", "page_idx": 9}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Task vectors and model compositions. Recent studies have demonstrated the possibility of manipulating the behaviours of neural networks directly in the weight space [27, 62, 64]. In particular, task vectors [28], as a carrier of the domain-specific knowledge learned through fine-tuning, exhibit strong compositional properties. Such compositionality can be enhanced via linearisation using first-order Taylor expansion [44], and improves model editing with simple arithmetic, e.g., addition, negation, etc. Yang et al. [65] also investigated the idea of learning layer-wise coefficients to improve task arithmetic. In addition, low-rank adaptations [23], as special forms of task vectors, were shown to also support such arithmetic operations. A recent study [3] also investigated the idea of learning combinations of LoRAs for few-shot recognition. ", "page_idx": 9}, {"type": "text", "text": "Model-based transfer learning. One interpretation of transfer learning [73] is to exploit the knowledge encapsulated in a pre-trained model for a target domain. Amongst various sub-modules of a pre-trained model, transferring the feature extractor is the most extensively studied. This ranges from early convolutional neural networks [18, 53] to modern transformers [58], from vision backbones [14, 37] to language models [13, 46]. For vision applications, classification models trained on ImageNet [52] have been used as the medium for knowledge transfer. In recent years, contrastively pre-trained multi-modal models such as CLIP [47] have emerged as a prevelant choice. Such models are trained on large volumes of data by aligning image and language representations, leading to strong baselines well suited for transfer learning. CLIP representations have since been use for medical imaging [70], semantic segmentation [72], satellite imaging [40], etc. ", "page_idx": 9}, {"type": "text", "text": "Model adaptation in low-data regimes. The performance of pre-trained models is often constrained when applied to specific tasks with limited labelled data. To address this limitation, extensive research has been conducted on few-shot adaptation of CLIP [47]. These studies focus on various techniques, including prompt engineering [71], feature adaptation [16], and more recently classifier adaptation [25, 69]. In addition to few-shot adaptation, test-time adaptation represents an even more challenging scenario where no annotated data is available. This typically requires leveraging self-supervised objectives to adapt the model, employing methods such as entropy minimisation [35, 43, 59], contrastive learning [8], pseudo labelling [35] and image rotation prediction [57]. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced aTLAS, a learning algorithm that leverages the rich knowledge encapsulated in task vectors through learned linear combinations with anisotropic scaling. Unlike conventional methods that learn network parameters, our approach focuses on learning coefficients on task vectors, significantly reducing the number of learnable parameters. We conducted experiments across task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, demonstrating the effectiveness of our method with supervised and unsupervised objectives. In particular, we highlighted several properties of aTLAS, including low disentanglement error, robustness against domain shift, effectiveness in low-data regimes, complementarity with existing few-shot methods, etc. These properties paved the way for efficient knowledge composition and transfer. ", "page_idx": 9}, {"type": "text", "text": "Limitations. As a task vector is defined with respect to a specific pre-trained model, knowledge composition and transfer are not yet feasible across different architectures. This may become possible with suitable projections and remains part of the future work. In addition, combining large numbers of task vectors can consume a substantial amount of GPU memory when training larger models. This can be mitigated by selecting a subset of task vectors, using LoRAs as task vectors or by offloading the computation of task vector composition to CPU, at the cost of training speed decrease. It is also possible to perform task vector composition at bit-width lower than floating point precision, e.g., 4-bit. Similar features are being tested with popular deep learning frameworks such as PyTorch, and we expect the memory requirement of larger models to be less of a constraint in the future. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. This research is funded in part by the Australian Government through the Australian Research Council (Project DP240103278), and the Centre of Augmented Reasoning at the Australian Institute for Machine Learning, established by a grant from the Department of Education. We would like to thank Stephen Gould for his valuable feedback on the paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7319\u20137328. Association for Computational Linguistics, Aug 2021. URL https://aclanthology.org/2021.acl-long.568. 2, 4 [2] Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning. In International Joint Conference on Neural Networks (IJCNN), 2020. URL https://arxiv.org/pdf/1908.02983. 29 [3] Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, and Xi Chen. Does combining parameter-efficient modules improve few-shot transfer accuracy? arXiv preprint arXiv:2402.15414, 2024. URL https://arxiv.org/pdf/2402.15414. 10 [4] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. arXiv, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258. 2 [5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Proceedings of European Conference on Computer Vision, pages 446\u2013461. Springer,   \n2014. URL https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29. 16 [6] Stevo Bozinovski and Ante Fulgosi. The influence of pattern similarity and transfer learning upon the training of a base perceptron b2. Proceedings of Symposium Informatica, 3(125), 1976. 1 [7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of European Conference on Computer Vision (ECCV), pages 213\u2013229, Cham, 2020. Springer International Publishing. URL https://arxiv.org/pdf/2005.12872. 2 [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n295\u2013305, Jun 2022. URL https://arxiv.org/pdf/2204.10377. 10 [9] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In International Conference on Machine Learning (ICML), 2020. URL https://arxiv.org/pdf/2002.05709. 8 [10] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of IEEE, 105(10):1865\u20131883, Oct 2017. URL https://arxiv.org/abs/   \n1703.00121. 16 [11] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3606\u20133613, Columbus, OH, USA, 24\u201327 Jun 2014. URL https://arxiv.org/abs/   \n1311.3618. 16 [12] Adam Coates, Andrew $\\mathrm{Ng}.$ , and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of International Conference on Artificial Intelligence and Statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011. URL https://proceedings.mlr. press/v15/coates11a/coates11a.pdf. 16   \n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\", pages 4171\u20134186, Minneapolis, Minnesota, Jun 2019. URL https://aclanthology.org/N19-1423. pdf. 10   \n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of International Conference on Learning Representations (ICLR), 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. 10   \n[15] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The pascal visual objective classes challenges: A retrospective. Interational Journal of Computer Vision (IJCV), 2015. URL https://link.springer.com/article/10.1007/s11263-014-0733-5. 16   \n[16] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP-Adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. URL https://arxiv.org/pdf/2404.02285. 10   \n[17] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. URL https://authors.library.caltech.edu/records/5sv1j-ytw97. 16   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, Las Vegas, NV, USA, 26 Jun \u2013 1 Jul 2016. URL https://arxiv.org/pdf/1512.03385. 2, 3, 10   \n[19] Kaiming He, Georgia Gkioxari, Pitor Doll\u00e1r, and Ross Girshick. Mask R-CNN. In Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV), pages 2980\u20132988, Venice, Italy, 22\u201329 Oct 2017. URL https://arxiv.org/pdf/1703.06870. 2   \n[20] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing euroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. In Proceedings of IEEE International Geoscience and Remote Sensing Symposium, pages 204\u2013207, Valencia, Spain, 22\u201327 Jul 2018. URL https://arxiv.org/abs/1709.00029. 16   \n[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 8340\u20138349, 2021. URL https://arxiv.org/pdf/2006.16241. 7   \n[22] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15262\u2013 15271, 2021. URL https://arxiv.org/pdf/1907.07174. 7   \n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proceedings of International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/pdf/2106.09685. 2, 9, 10, 27, 29   \n[24] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoRAhub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023. URL https://arxiv.org/pdf/2307.13269. 28   \n[25] Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, and Ismail Ben Ayed. $\\mathrm{LP++}$ : A surprisingly strong linear probe for few-shot clip. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. URL https://arxiv.org/pdf/2404.02285. 7, 10, 23   \n[26] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, Jul 2021. URL https://doi.org/10.5281/zenodo.5143773. 2, 29   \n[27] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 29262\u201329277. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/bc6cddcd5d325e1c0f826066c1ad0215-Paper-Conference.pdf. 10   \n[28] Gabriel Ilharco, Marco T\u00falio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In Proceedings of International Conference on Learning Representations (ICLR), Kigali, Rwanda, 1\u20135 May 2023. OpenReview.net. URL https://openreview. net/pdf?id=6t0Kwf8-jrj. 2, 3, 4, 6, 10, 18, 21, 23   \n[29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In Proceedings of International Conference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pages 3519\u20133529. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/kornblith19a.html. 6   \n[30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In IEEE/CVF International Conference on Computer Vision (ICCV) Workshop on 3D Representation and Recognition, pages 554\u2013561, Sydney, Australia, 1\u20138 Dec 2013. URL http://vision. stanford.edu/pdf/3drr13.pdf. 16   \n[31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 4, 16   \n[32] Yann LeCun, Corinna Cortez, and Christopher C. J. Burges. The mnist handwritten digit database, 1998. URL http://yann.lecun.com/exdb/mnist/. 4, 16   \n[33] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the instrinsic dimension of objective landscapes. In Proceedings of International Conference on Learning Representations (ICLR), Vancouver, Canada, 30 Apr\u20133 May 2018. URL https://openreview.net/pdf?id=ryup8-WCW. 2, 4   \n[34] Fei-Fei Li, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 28(4):594\u2013611, 2006. URL http://vision. stanford.edu/documents/Fei-FeiFergusPerona2006.pdf. 2, 16   \n[35] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In Proceedings of International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 6028\u20136039. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/liang20a.html. 2, 8, 10   \n[36] Baijiong Lin. LoRA-Torch: PyTorch reimplementation of LoRA, 2023. URL https://github.com/ Baijiong-Lin/LoRA-Torch. 27   \n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV), 11\u201317 Oct 2021. 10   \n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of International Conference on Learning Representations (ICLR), New Orleans, LA, USA, 6\u20139 may 2019. OpenReview.net. URL https://openreview.net/forum?id $\\equiv$ Bkg6RiCqY7. 16, 23   \n[39] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 16   \n[40] Sangwoo Mo, Minkyu Kim, Kyungmin Lee, and Jinwoo Shin. S-clip: Semi-supervised vision-language learning using few specialist captions. Advances in Neural Information Processing Systems, 36, 2024. 10   \n[41] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In Neural Information Processing Systems (NeurIPS) Workshop on Deep Learning and Unsupervised Feature Learning, Granada, Spain, 12\u201317 Dec 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf. 16   \n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008. 16   \n[43] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In International Conference on Learning Representations (ICLR), 2023. 8, 10   \n[44] Guillermo Ortiz-Jim\u00e9nez, Alessandro Favero, and Pscal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 66727\u201366754, New Orleans, LA, USA, 10\u201316 Dec 2023. Curran Associates, Inc. 2, 3, 4, 5, 6, 10, 18, 21, 23   \n[45] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), pages 3498\u20133505. IEEE, 2012. 16   \n[46] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. 3, 10   \n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of International Conference on Machine Learning (ICML), volume 139, pages 8748\u20138763. Proceedings of Machine Learning Research (PMLR), 18\u201324 Jul 2021. 2, 3, 6, 7, 10, 16, 17   \n[48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. 3   \n[49] J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.com/ FacebookResearch/Nevergrad, 2018. 28   \n[50] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning (ICML), pages 5389\u20135400. PMLR, 2019. 7   \n[51] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS), volume 28, pages 91\u201399, Montr\u00e9al, Canada, 7\u201312 Dec 2015. Curran Associates, Inc. 2   \n[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. 1, 5, 7, 10, 16, 26   \n[53] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of International Conference on Learning Representations (ICLR), San Diego, CA, USA, 7\u20139 May 2015. OpenReview.net. 2, 3, 10   \n[54] K. Sohn, D. Berthelot, C.-L. L, Z. Zhang, N. Carlini, E. Cubuk, A Kurakin, H. Zhang, and C. Raffel. FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. arXiv: 2001.07685, 2020. 8, 29   \n[55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 16   \n[56] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: A multi-class classification competition. In Proceedings of International Joint Conference on Neural Networks (IJCNN), pages 1453\u20131460, San Jose, CA, USA, 31 Jul\u20135 Aug 2011. URL https: //ieeexplore.ieee.org/document/6033395. 16   \n[57] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 9229\u20139248. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/sun20b.html. 2, 8, 10   \n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, pages 6000\u20136010. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 10   \n[59] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully testtime adaptation by entropy minimization. In Proceedings of International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id $=$ uXl3bZLkr3c. 8, 10   \n[60] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), pages 10506\u201310518, 2019. 7 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[61] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010. 16 ", "page_idx": 14}, {"type": "text", "text": "[62] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of International Conference on Machine Learning (ICML), volume 162, pages 23965\u201323998, 23\u201329 Jul 2022. 2, 10   \n[63] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. Interational Journal of Computer Vision (IJCV), 119(1): 3\u201322, 2016. ISSN 1573-1405. URL https://doi.org/10.1007/s11263-014-0748-y. 16   \n[64] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. TIESMerging: Resolving interference when merging models. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 7093\u20137115. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf. 10   \n[65] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. In Proceedings of International Conference on Learning Representations (ICLR), 2024. 8, 10   \n[66] Frederic Z. Zhang, Dylan Campbell, and Stephen Gould. Spatially conditioned graphs for detecting human\u2013object interactions. In Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV), pages 13319\u201313327, Oct 2021. URL https://arxiv.org/pdf/2012.06060. 2   \n[67] Frederic Z. Zhang, Dylan Campbell, and Stephen Gould. Efficient two-stage detection of human\u2013object interactions with a novel unary\u2013pairwise transformer. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20104\u201320112, New Orleans, LA, USA, Jun 2022. URL https://arxiv.org/pdf/2112.01838.   \n[68] Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, and Stephen Gould. Exploring predicate visual context in detecting of human\u2013object interactions. In Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV), pages 10411\u201310421, Oct 2023. 2   \n[69] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-Adapter: Training-free adaption of CLIP for few-shot classification. In Proceedings of European Conference on Computer Vision (ECCV), pages 493\u2013510, Tel Aviv, Israel, 23\u201327 Oct 2022. Springer Nature Switzerland. ISBN 978-3-031-19833-5. 2, 7, 10, 23, 29   \n[70] Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. arXiv preprint arXiv:2312.07353, 2023. 10   \n[71] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. Interational Journal of Computer Vision (IJCV), 130(9):2337\u20132348, Sep 2022. ISSN 0920-5691. URL https://doi.org/10.1007/s11263-022-01653-1. 10   \n[72] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11175\u201311185, 2023. 10   \n[73] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2021. ISSN 0018-9219. 1, 10 ", "page_idx": 14}, {"type": "text", "text": "We acquire task vectors by fine-tuning CLIP [47] on a variety of 22 image recognition datasets: (1) Stanford Cars [30], (2) DTD [11], (3) EuroSAT [20], (4) GTSRB [56], (5) MNIST [32], (6) RESISC45 [10], (7) SUN397 [63], (8) SVHN [41], (9) CIFAR10 [31], (10) CIFAR100 [31], (11) ImageNet [52], (12) STL10 [12], (13) Food101 [5], (14) Caltech101 [34], (15) Caltech256 [17], (16) FGVCAircraft [39], (17) Flowers102 [42], (18) Oxford Pets [45], (19) CUB200 [61], (20) PascalVOC [15], (21) Country211 [47], and (22) UCF101 [55]. Fine-tuning was conducted using AdamW optimiser [38], with a learning rate of $10^{-5}$ , batch size of 128 and weight decay of 0.1. Details of the datasets, additional dataset-specific hyper-parameters, and the accuracy after fine-tuning for an assortment of backbones are shown in Table 5. We use the same hyper-parameters for the linearised variants of the model. ", "page_idx": 15}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/5406ccf47893f47791434c87257f80261a92aa537ac5ca6afd2558d105e96505.jpg", "table_caption": ["Table 5: Details of the 22 image classification datasets used in experiments, the number of epochs for fine-tuning and the final accuracy for different backbones of the CLIP model. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "To shed light on the semantic relationships amongst datasets, we extract the features of all images for each dataset, and visualise the distributions as ellipses (Figure 8). Specifically, for each dataset, the mean $\\pmb{\\mu}_{t}\\in\\mathbb{R}^{d}$ and covariance $\\Sigma_{t}\\in\\mathbb{R}^{d\\times d}$ of image features are computed. Principal component analysis (PCA) is used produce a projection matrix $P\\in\\mathbb{R}^{d\\times2}$ from the mean features $\\pmb{\\mu}_{t}$ . Subsequently, the mean and covariance with reduced dimensionality can be expressed as $P^{\\top}\\pmb{\\mu}_{t}$ and $P^{\\dagger}\\Sigma_{t}P$ , respectively. ", "page_idx": 15}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/4315e4ecb8b757cd314a3616f8183c8ff58a9fa02798d19d51d42ce72cfc7b4b.jpg", "img_caption": ["(b) Distributions of dataset features as ellipses with ${}_{3\\times}$ standard deviation "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: visualisation of dataset image feature distributions as ellipses. The mean image features for all datasets are visualised as the ellipse center, with the dimensionality reduced to 2 using Principal Component Analysis (PCA). The dimensionality of covariance matrices are also reduced using the same principal components. We show visualisations with (a) $\\times1$ and (b) $\\times3$ standard deviations. Pre-trained CLIP [47] with ViT-B/32 is used to extract image features. ", "page_idx": 16}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/ea232c6504601c0d62b60e4643df8d905ad06ebb3be8e40ea3abf9238545c940.jpg", "table_caption": ["Table 6: Learning rates and training epochs for task negation. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 7: Accuracy on target and control tasks of task negation for each of the eight datasets. Highest performance in each section is highlighted in bold. The method search corresponds to model $f(x;\\pmb\\theta_{0}+\\alpha\\pmb\\tau_{t})$ , where $\\alpha$ is determined via a hyper-parameter search. Our method aniso. corresponds to model $f(x;\\pmb\\theta_{0}+\\Lambda_{t}\\pmb\\tau_{t})$ , where $\\Lambda_{t}$ is a learnable scaling matrix. ", "page_idx": 17}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/53601e5458882985f8e5a95f9447de1ee263aa446d56dee1115b83cbf36e5184.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Task negation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The evaluation of task negation is conducted on eight classification datasets (1\u20138 in Table 5), following previous practice [28, 44]. In particular, we learn anisotropic scaling using the validation set of each dataset. We also adjust the learning rates and training epochs on the same validation set. The details are shown in Table 6. We report detailed task negation results for each dataset in Table 7. In addition, for more evidence that weight matrices learn large negative coefficients, we show a detailed visualisation of the learned coefficients in Figure 9 and distribution of the coefficients in Figure 10. ", "page_idx": 17}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/d3101987055fe67d35b0041f83cdcec96a9fbe1d5209a73d546b048de6baeb55.jpg", "img_caption": ["(a) Learned coefficients on standard task vectors "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/24cce1b12d10609dfedfccc89dc2344484741745177619cd1e6325c298103e27.jpg", "img_caption": ["(b) Learned coefficients on linear task vectors "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: visualisation of the learned coefficients for (a) standard and (b) linear task vectors in task negation. Note that coefficients for different datasets are learned independently, despite being visualised jointly. Large negative coefficients can be observed on weight matrices. CLIP with ViT-B/32 backbone is used. ", "page_idx": 18}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/9c55ceb6b871053787e49c7844c48f3f572e85a026b8816f12ef6eeb010bf702.jpg", "img_caption": ["Figure 10: Additional box-and-whisker plots for the learned coefficients in task negation, beside previous visualisation on Cars (Figure 3a), including results on (a, b) DTD, (c, d) EuroSAT, (e, f) GTSRB, (g, h) MNIST, $(\\mathrm{i},\\mathrm{j})$ RESISC45, (k, l) SUN397 and (m, n) SVHN. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/bc8012c3687ddea078362c360ecaf9a595584df21bd0aa7423df30823db34ac8.jpg", "table_caption": ["Table 8: Detailed performance for task addition across all eight datasets. We show additional performance with learned isotropic scaling, which have comparable accuracy to the simple hyper-parameter search used in previous methods [28, 44]. Highest performance in each section is highlighted in bold. The method search corresponds to model $f(x;\\pmb\\theta_{0}+\\alpha\\sum\\tau_{i})$ , where $\\alpha$ is determined via a hyper-parameter search. Methods iso. and aniso. use learned coefficients and correspond to models $f(x;\\pmb\\theta_{0}+\\dot{\\sum}\\alpha_{i}\\dot{\\pmb\\tau}_{i})$ and $f(x;\\pmb\\theta_{0}+\\textstyle\\sum\\Lambda_{i}\\pmb\\tau_{i})$ , respectively. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Task addition ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Task addition is also evaluated on datasets 1\u20138 shown in Table 5. The hyper-parameters are identical to fine-tuning, except the learning rate is modified to $10^{-3}$ . We show detailed performance on each dataset in Table 8, where we compare our method against hyper-parameter search used in previous works [28, 44], and another variant with learned isotropic scaling. We also visualise the learned coefficients with $L_{1}$ regularisation in Figure 12. It can be easily observed that weight matrices, particularly those in the deeper layers, have significantly higher learned coefficients, which conforms to our observations in Figures 3b and 3c. ", "page_idx": 20}, {"type": "text", "text": "C.1 Comparison against full-parameter optimisation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since our method involves learning the coefficients, unlike previous methods [28, 44] that only require a hyper-parameter search, we also compare against the direct fine-tuning approach. We fine-tune the pre-trained model on the union of eight datasets, assuming only the validation sets are available. The results are shown in Figure 11. Unsurprisingly, task vector compositions, whether the coefficients are searched or learned, are less susceptible to the lack of data, as the accuracy only starts to drop with less than $35\\%$ of the data. The performance of fullparameter fine-tuning, however, drops substantially as the amount of data available decreases. ", "page_idx": 20}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/2bbcc66bc8d6fa977834f0373e51457e4e831ffc9daad894cef988452248f6c7.jpg", "img_caption": ["Figure 11: Task addition accuracy averaged across eight datasets (1\u20138) versus different percentage of validation data used. Standard task vectors are used. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Disentanglement error ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition, we provide more technical details and intuitions on the pairwise disentanglement error [44], which was visualised in Figure 4. Specifically, we make a few changes to the formulation proposed by Ortiz-Jim\u00e9nez et al. [44], and evaluate the disentanglement error only with the optimal ", "page_idx": 20}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/83ce2ecca18d4e9e21a2a1e430adb64a6d32940d2d2e5ba43f3278df2637131c.jpg", "img_caption": ["(a) Learned coefficients on standard task vectors ", "(b) Learned coefficients on linear task vectors "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 12: visualisation of the learned coefficients for (a) standard and (b) linear task vectors in task addition. Note that $L_{1}$ regularisation has been applied to the coefficients during training for better clarity. CLIP with ViT-B/32 backbone is used to produce the results. ", "page_idx": 21}, {"type": "text", "text": "coefficients. Given two datasets $\\boldsymbol{D}_{1},\\boldsymbol{D}_{2}$ and the respective task vectors $\\tau_{1},\\tau_{2}$ , we overload the definition of function $f$ to denote the mapping from data space $\\mathcal{X}$ to the label space $\\boldsymbol{\\wp}$ , and define the disentanglement error as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi(\\tau_{1},\\tau_{2})=\\mathbf{E}_{\\mathbf{x}\\in\\mathcal{D}_{1}}\\big[\\delta\\big(f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{1}^{\\star}\\tau_{1}),f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{1}^{\\star}\\tau_{1}+\\Lambda_{2}^{\\star}\\tau_{2})\\big)\\big],}\\\\ &{\\xi(\\tau_{2},\\tau_{1})=\\mathbf{E}_{\\mathbf{x}\\in\\mathcal{D}_{2}}\\big[\\delta\\big(f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{2}^{\\star}\\tau_{2}),f(\\mathbf{x};\\pmb{\\theta}_{0}+\\Lambda_{1}^{\\star}\\tau_{1}+\\Lambda_{2}^{\\star}\\tau_{2})\\big)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Lambda_{1}^{\\star},\\Lambda_{2}^{\\star}$ are the learned coefficients in task addition, and $\\delta$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta(x_{1},x_{2})={\\left\\{\\begin{array}{l l}{0}&{x_{1}=x_{2},}\\\\ {1}&{x_{1}\\neq x_{2}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The error metric $\\xi(\\tau_{1},\\tau_{2})$ measures the percentage of data in dataset $\\mathcal{D}_{1}$ , such that when a second task vector $\\tau_{2}$ is added to the model, the predicted labels differ from when only using task vector $\\tau_{1}$ . As task vector $\\tau_{1}$ is acquired from dataset $\\mathcal{D}_{1}$ , a low disentanglement error indicates that most predictions made by $\\tau_{1}$ \u2014highly likely to be correct\u2014will be retained, thus resulting in higher performance in task addition. ", "page_idx": 21}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/b5943db628e8af39738a2a45cd00518ef0b76ba6d3625b62e2251784d8fc6ac3.jpg", "table_caption": ["Table 9: Average accuracy for few-shot recognition over 22 datasets. We report accuracy averaged over 3 random $\\mathbf{n}\\cdot$ -shot sample selections, with $1\\times$ standard error. Results are produced using CLIP with ViT-B/32 backbone. For our method, we show results with both standard [28] and linearised [44] task vectors. The best method for each choice of $k\\in\\{1,2,4,8,16\\}$ is highlighted in bold. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Few-shot learning ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Baselines: Tip-Adapter and $\\mathbf{LP++}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Two variants of Tip-Adapter [69] were proposed for few-shot recognition where the weights of the adaptor are either fixed based on features of the few-shot examples or further fine-tuned. We only study the fine-tuned variant due to its higher performance. Tip-Adapter has two hyper-parameters, which in the original paper are optimised through hyper-parameter search on a separate validation set. This practice may not align with the principles of few-shot learning, where access to extensive validation data is typically limited. In addition, Huang et al. [25] note that the performance of TipAdapter is very sensitive to these hyper-parameters. We thus opt to learn these two hyper-parameters together with the feature adaptor through gradient descent. The learning rates for the feature adaptor and the hyper-parameters are set to $10^{\\overline{{-}}3}$ and $10^{-1}$ , respectively. ", "page_idx": 22}, {"type": "text", "text": "For both Tip-Adapter and $\\mathrm{LP++}$ [25], we conduct experiments using the publicly available codebase 5. We train both ${\\mathrm{LP}}++$ and Tip-Adapter for 300 epochs on frozen zero-shot features. We apply a cosine annealing decay for Tip-Adapter and maintain fixed learning rates for ${\\mathrm{LP}}++$ as per the official implementation. ", "page_idx": 22}, {"type": "text", "text": "D.2 linearised task vectors ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We report the average few-shot accuracy over the 22 datasets in Table 9, which corresponds to results in Figure 5a. In particular, we show results with linearised task vectors, as proposed by Ortiz-Jim\u00e9nez et al. [44]. As highlighted in Section 4, learned anisotropic scaling allows standard task vectors to achieve stronger performance than the linear variants in task addition. For few-shot recognition, we again observe that standard task vectors result in superior performance in most cases. We, however, note the exception that linear task vectors when combined with $\\mathrm{LP++}$ achieve higher performance in the 1-shot setting. Nevertheless, the margin over standard task vectors is not very significant, and aTLAS using standard task vectors when integrated with Tip-Adapter is generally a stronger few-shot model. ", "page_idx": 22}, {"type": "text", "text": "D.3 Integrating state-of-the-art methods into aTLAS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use the AdamW [38] optimiser with a learning rate of $10^{-1}$ and a weight decay of $10^{-1}$ . Our method by itself is trained for 10 epochs with ViT backbones and 30 epochs with ResNet backbones. ", "page_idx": 22}, {"type": "text", "text": "We show that state-of-the-art few-shot methods can be seamlessly integrated into our method, since both Tip-Adapter and $\\mathrm{LP++}$ focus on the classifier, while aTLAS improves the feature representations. We experiment with two strategies to combine aTLAS with previous methods, where we either (1) train our method first and use the frozen representations to train a previous method, or (2) train parameters in both methods jointly. Results in Table 10 shows that the joint training strategy results in higher performance, particularly in low-shot settings. We therefore adopt the joint training strategy when combing our method with Tip-Adapter. During training, we adopt different learning rates for different parameter groups, that is, $\\bar{10}^{-1}$ for learnable coefficients in aTLAS and the hyper-parameters in Tip-Adapter, and $10^{-3}$ for the adaptor. The joint training takes 20 epochs for ViT backbones and 60 epochs on ResNet backbones, twice the number of epochs when training aTLAS alone. ", "page_idx": 22}, {"type": "text", "text": "Table 10: Comparison of few-shot recognition accuracy between training our method and Tip-Adapter sequentially and jointly over different shots $(k)$ . ViT-B/32 is used as the backbone. Results are averaged across three random seeds. Highest performance in each section is highlighted in bold. ", "page_idx": 23}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/77d56962e32687c86ceaf5e34e273d54a196a92064021ae08e68f4d7eae2d920.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/d9b243d3595f9ded8477e35aea892556684b2751edc99eb8163f529c9dde0d21.jpg", "table_caption": ["Table 11: Detailed few-shot accuracy for each dataset over different shots $(k)$ using ViT-B/32 backbone. We report results averaged over 3 random seeds. In the case where the results are worse than the zero-shot accuracy, we report zero-shot accuracy. Highest performance and those within a range of 0.1 in each section are highlighted in bold. Tip-Adapter is abbreviated as Tip. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "On the other hand, The joint training strategy with ${\\mathrm{LP}}++$ is non-trivial, due to ${\\mathrm{LP}}++$ \u2019s superconvergence strategy being designed around frozen feature representations, which would have been updated every iteration by aTLAS. We thus use the sequential strategy to combine aTLAS and ${\\mathrm{LP}}++$ . We include detailed results for each dataset with ViT-B/32 in Table 11 and additional results with different backbones in Table 12, where we show our method scales well across different datasets and backbones. ", "page_idx": 23}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/71083507f0b8dfb90efb12d3f0753914acd3b20278ad917f30f6982f7f14166c.jpg", "table_caption": ["Table 12: Detailed few-shot accuracy for each dataset across RN50, RN101, ViT-B/16, and ViT-L/14 backbones. We report results for the same random seed. In the case where the results are worse than the zero-shot accuracy, we report zero-shot accuracy. Highest performance and those within a range of 0.1 in each section are highlighted in bold. Tip-Adapter is abbreviated as Tip. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/d7d461da06ee38ff05f976d5fef8c22ea1c6f5760bef1cc4cb59483595a46ebc.jpg", "table_caption": ["Table 13: Accuracy of few-shot methods trained on ImageNet [52] and tested on out-of-domain datasets, for $k\\in\\{4,16\\}$ . Results are produced by CLIP with ViT-B/32 backbone and averaged across three random seeds. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "G9OJUgKo4B/tmp/05ce5d5c04f7ed1fba3fce9e2fba0b5b8de95b23e98be8ebfe7158cc6b78d678.jpg", "img_caption": ["Figure 13: Accuracy improvement of aTLAS (16-shot) using one task vector normalised by that of fine-tuning in the full parameter space (all training data). Each column corresponds to a unique task vector, and reflects the relative improvement it leads to on different target datasets. Each row reflects the relative improvement on a dataset, using different task vectors. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.4 Out-of-domain generalisation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We show detailed results for out-of-domain generalisation over $k\\in\\{4,16\\}$ shots in Table 13. These results correspond to those presented in Figure 5c. aTLAS is the only method that consistently improves test accuracy over the zero-shot model on out-of-domain images. When combined with ${\\mathrm{LP}}++$ or Tip-Adapter, aTLAS can be observed to improve the out-of-domain generalisation of these methods. ", "page_idx": 25}, {"type": "text", "text": "D.5 Relative significance of individual task vectors ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we examine the informativeness of a task vector across different target datasets. To this end, we apply aTLAS to each of the 22 datasets using only one task vector. For each dataset, we compute the relative accuracy improvement, that is, the accuracy improvement of aTLAS normalised by that of fine-tuning in the full parameter space. Note that aTLAS is applied under the 16-shot setting, while standard fine-tuning uses all training data available. Results are shown in Figure 13. We first note that certain datasets are more prone to accuracy improvement, such as EuroSAT, MNIST, etc., as indicated by the high percentage across entire rows. This is most likely due to the low intrinsic dimensionality of the task. In addition, we highlight the average improvement in the last row. Notably, certain task vectors, e.g., ImageNet task vector, are particularly informative while others, such as those from Flowers102 and OxfordPets are much less so. These results illustrate the varying contributions different task vectors can have depending on the target dataset, which also motivated subsequent efforts on careful task vector selection. ", "page_idx": 25}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/ac26b176016ad8c4cdcf1455ff89e7e72484c327eee60b1469a820d87c6790fe.jpg", "table_caption": ["Table 14: Few-shot accuracy when only using a budget of $^b$ task vectors with different selection strategies. We report results for 4 and 16 shots. The results are averaged over 22 datasets and three random seeds. CLIP with ViT-B/32 backbone is used. Highest performance in each section is highlighted in bold. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.6 Task vector budget and selection ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide details for selecting a budget of $b$ task vectors with feature-based and gradient-based strategies, as introduced in Section 5.2. ", "page_idx": 26}, {"type": "text", "text": "Feature based selection. For each dataset $\\mathcal{D}_{i}$ , we compute the average image representation $\\bar{\\bf z}_{i}$ of the dataset using the zero-shot model as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{z}}_{i}=\\mathbf{E}_{\\mathbf{x}\\in\\mathcal{D}_{i}}[f(\\mathbf{x};\\pmb\\theta_{0})].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Given a target dataset $\\mathcal{D}_{t}$ , we simply compute the cosine similarity between its feature representation $\\bar{\\mathbf{z}}_{t}$ and that of each other dataset $\\bar{\\bf z}_{i}$ , $i\\neq t$ . Subsequently, $b$ task vectors corresponding to the datasets with highest similarity will be selected. ", "page_idx": 26}, {"type": "text", "text": "Gradient-based selection. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Given a target dataset $\\mathcal{D}_{t}$ , we may directly compute the gradient with respect to the $m$ learnable coefficients for each of the $n$ task vectors. However, as one important motivation behind task vector selection is to reduce memory consumption, using all $n$ task vectors to compute the gradient defeats the purpose. Therefore, we instead only load a group of $b$ task vectors $(b\\,<\\,n)$ , compute the gradient with respect to their learnable coefficients, and repeat for other groups. With this sequential computation, the gradient across different groups is not calibrated. Nevertheless, we empirically found this strategy to work well. Denote the partial derivative of the loss on dataset $\\mathcal{D}_{t}$ with respective to a learnable coefficient $\\lambda_{i}^{(j)}$ by $\\dot{\\lambda}_{i}^{(j)}$ , such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\dot{\\lambda}_{i}^{(j)}=\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}_{t}}\\left[\\frac{\\partial\\mathcal{L}\\bigg(f\\bigg(\\mathbf{x};\\pmb{\\theta}_{0}+\\sum_{i=1}^{b}\\Lambda_{i}\\pmb{\\tau}_{i}\\bigg),\\mathbf{y}\\bigg)}{\\partial\\lambda_{i}^{(j)}}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the $i$ -th task vector, we may compute its $L_{1}$ gradient norm, i.e., $\\left\\|\\dot{\\lambda}_{i}^{(1)},\\dots,\\dot{\\lambda}_{i}^{(m)}\\right\\|_{1}$ , and select task vectors with larger gradient. Alternatively, we may select task vectors block by block. Specifically, for the $j$ -th parameter block, we inspect the absolute values of the partial derivatives for the corresponding coefficients, i.e., $\\left\\vert\\dot{\\lambda}_{i}^{(j)}\\right\\vert$ , and select task vectors with higher absolute values. This process is repeated for each parameter block, thus allowing different parameter blocks to have different selections. Crucially, for low budgets, particularly $b=1$ , this enables our method to effectively exploit more task vectors than the budget specifies. The impact of this can be observed in Table 14 (corresponding to Figure 6), that blockwise selection significantly outperforms other methods when the budget is low. ", "page_idx": 26}, {"type": "text", "text": "D.7 LoRAs as task vectors ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We fine-tune LoRAs for ViT-B/32 using the LoRA-Torch [36] library with ranks 4, 16 and 64. We stop at rank 64 as we do not observe improvements beyond it. We train LoRAs on attention and MLP layers and use the same settings as for full finetuning but with a learning rate of $10^{-3}$ . ", "page_idx": 26}, {"type": "text", "text": "Table 15 shows additional results using LoRAs as task vectors. We study learning the effect of fine-tuning the LoRAs task vectors on attention layers only (as done in the original LoRA paper [23]) or on the MLPs. Although the original LoRA paper recommendeds training on the attention layers only [23], we observe that training on MLP layers is important to produce strong LoRA task vectors. ", "page_idx": 26}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/c217d2a573eb810e38ba912970fe9be876f9a73b8ed424a4881da5bdda7dd2dd.jpg", "table_caption": ["Table 15: Additional few-shot recognition results using LoRAs trained on attention layers, MLP layers or both. Results are averaged across 22 datasets over three seeds, with $\\times1$ standard deviation. Rank 16 is used for LoRAs. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/018bf71a8c1adba6e72c7cbde96b91bd61ab2a2556c9610054a01689c9a073fa.jpg", "table_caption": ["Table 16: Few-shot recognition performance with gradient-free optimisation. Results are averaged accuracy over 22 datasets, with $1\\times$ standard error over 3 random seeds. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.8 Gradient-free optimisation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "An alternative to save memory during training is to utilise gradient-free methods to learn the coefficients. We follow previous work on the combination of LoRAs [24] and use the nevergrad [49] library. We observe a memory usage reduction of $60\\%$ from 10GB to 4GB calculated using a dedicated pytorch function6. Results for few-shot recognition are summarised in Table 16. We show that although gradient-free optimisation improves upon the zero-shot model, the performance quickly plateaus as the amount of data increases. In addition, learning anisotropic scaling results in worse performance, most likely due to the relatively high number of parameters. ", "page_idx": 27}, {"type": "table", "img_path": "G9OJUgKo4B/tmp/f8ba6c4239a423995503fda90c0a4b846bcdfc9d89eb82ab8036563afac58fe3.jpg", "table_caption": ["Table 17: Accuracy after fine-tuning on different percentage of training data for variants of aTLAS $\\times K$ and LoRAs [23]. Results are averaged across 22 datasets. Highest accuracy in each section is highlighted in bold. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "E Unsupervised FixMatch ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We provide more details on the Unsupervised FixMatch (UFM) approach in this section. FixMatch [54] utilises a labelled set to guide training, which is given as part of the semi-supervised learning protocol, while we produce a class-balanced \u201clabelled\u201d set from unlabelled images. Given a target dataset $\\mathcal{D}_{t}$ consisting of $N$ unlabelled images, we first rank the examples by the prediction scores from the zero-shot model across $C$ classes. We then select the top $\\operatorname*{min}(N/C,100)$ examples, that is, at most 100 examples per class, as a trusted set in absence of a labelled set. The standard cross-entropy loss is applied to the trusted set. For the rest of the unlabelled images, we use a weakly augmented (Open-CLIP [26] validation augmentations) view of an image to produce pseudo-labels, and incur a loss on the strongly augmented view (Tip-Adapter [69] augmentations). Denote an image with weak augmentation by $\\mathbf{x}$ , its strongly augmented view by $\\mathbf{x}^{\\prime}$ , and the predictions made by network by $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{y}}^{\\prime}$ , respectively, the unsupervised loss can be expressed as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{u}(\\hat{\\mathbf{y}},\\hat{\\mathbf{y}}^{\\prime})=-\\mathbb{1}(\\operatorname*{max}(\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}}))>\\boldsymbol{\\omega})\\;\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})^{\\top}\\log(\\hat{\\mathbf{y}}^{\\prime}),}\\\\ &{\\qquad\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})=\\frac{\\hat{\\mathbf{y}}^{0.5}}{1^{\\top}\\hat{\\mathbf{y}}^{0.5}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbb{1}(\\cdot)$ denotes the indicator function, $\\sigma(\\cdot)$ performs re-normalisation with adjusted temperature scaling, and $\\omega$ is a confidence threshold that is linearly adjusted from 0.9 to 1 during training. The trusted set is re-estimated at the beginning of each epoch to account for the improving accuracy of the model. In training, images in the trusted set are over-sampled to constitute one fourth of each batch, as this practice prevents the model from diverging due to confirmation bias [2, 54]. ", "page_idx": 28}, {"type": "text", "text": "F Details of aTLAS $\\times K$ variants ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Dividing a parameter block into $K$ random partitions allows us to introduce more learnable coefficients to each block, thus scaling up our method flexibly. One draw back of this approach, however, is that masks for the partitions have to be stored in memory, resulting in a linear memory increase with respect to the size of the parameter block and the value $K$ . To reduce the memory consumption the of $\\mathrm{aTLAS}\\times K$ variants, we only apply it to LoRAs task vectors. Nevertheless, these memory requirements could most likely be reduced by exploiting sparse matrices or memory efficient matrix indexing techniques, which we plan to investigate in the future. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All claims made in the introducing summarise findings in Sections 4, 5 and 6. We clearly enunciate our claims and hypothesis by numbering them by order of appearance in the main body of the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalise to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss limitations of our work in a paragraph found in the conclusion, Section 8. The dominant limitation we observe to our work is possible memory limitations when applied to larger models with LoRA task vectors. We otherwise tested our approach on a larger array of varied image classification datasets, ranging from simple to larger datasets. Some of the datasets we tested on are specialized while others are more generic. This should ensure that our results are generalisable to a large array of tasks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognise that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not contribute any theoretical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data is provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The algorithms we propose is fully explained in Section 3 with all the information needed to reproduce our results being available the appendix Sections A, D.1, D.3, E and F. Furthermore, the pre-trained models and every dataset we use are publicly available. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data is provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognise that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 30}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have released the entirety of our code base and task vector checkpoint under the link provided in the paper. All the data used in this paper is not owned by us and is publicly available. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All hyper-paramters, number of training samples, optimiser used, model architectures and everything else needed to reproduce our results are available the appendix Sections A, D.1, D.3, E and F. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We perform 3 independent runs in Figures 5a, ?? and ?? with standard errors being reported. The standard error is computed as the standard deviation over 3 runs divided by the number of runs (3). This is calculated with the numpy library. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Although we do not report compute requirements in the paper, on a single A100 or 4090GPU except for ViT-L/14 experiments that were performed on 2 A100. The typical run time for an experiment is 2 hours with ViT-L/14 experiments going up to 8h. We estimate the total compute needed to 1000 GPU-hours for repeated results over 3 seeds. The research project includes failed experiements and iterations on the method that are not reported. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We find no direct societal impact for this research paper as research is conducted on controlled open-sourced datasets. This paper did not study applicability to specialized datasets that can be used to impact society. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimising neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognise that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] Justification: All datasets we use are credited in Section A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No new assets. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: No human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No participants. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognise that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]