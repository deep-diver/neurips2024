[{"figure_path": "G9OJUgKo4B/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, \u03b80 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T = [T(1), T(2)] has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates the core idea of the aTLAS algorithm.  Panel (a) shows how multiple task vectors (T1 and T2) are linearly combined with learned coefficients (\u03bb1 and \u03bb2) to form a composite task vector. This composite vector, when added to the pre-trained model weights (\u03b80), produces a new model adapted for a specific task. Panel (b) compares isotropic and anisotropic scaling. Isotropic scaling applies the same scaling factor to all parameter blocks within a task vector, while anisotropic scaling allows for different scaling factors for each block, offering greater flexibility in composing task vectors.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_3_1.jpg", "caption": "Figure 2: Recognition accuracy versus the number of bases when optimising in a low-dimensional subspace. The accuracy is normalised by that of the fully fine-tuned model. Using task vectors to construct the projection matrix performs consistently better than using random bases on (a) MNIST [32], (b) CIFAR100 [31].", "description": "This figure compares the performance of using task vectors versus random bases for dimensionality reduction in few-shot image classification.  The results show that using task vectors to construct the projection matrix consistently leads to higher accuracy compared to random bases, particularly on MNIST and CIFAR100 datasets.  The accuracy is normalized relative to a fully fine-tuned model, highlighting the efficiency of the task vector approach.", "section": "Relation to intrinsic dimensionality"}, {"figure_path": "G9OJUgKo4B/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, 00 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T =  T(1), T(2)  has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates two key concepts of the aTLAS algorithm. (a) shows how task vector compositions are learned by linearly combining multiple task vectors (T1, T2, etc.) with learned coefficients (\u03bb1, \u03bb2, etc.).  The pre-trained model's weights are represented by \u03b80. (b) highlights the advantage of anisotropic scaling over isotropic scaling.  Anisotropic scaling allows for independent scaling of different parameter blocks within a task vector, providing greater flexibility in combining task vectors. This is visualized with an example of a loss contour plot, demonstrating how anisotropic scaling allows for more efficient optimization.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_6_1.jpg", "caption": "Figure 5: Few-shot experiment results averaged across 22 datasets and three seeds, showing (a) comparison against state-of-the-art few-shot methods with ViT-B/32 backbone and (b) percentage of images in the validation sets that become correctly classified after applying few-shot methods. We also show (c) performance difference compared to pre-trained CLIP model on OOD datasets. More detailed results are included in Appendix D.", "description": "This figure presents the results of few-shot learning experiments conducted on 22 datasets using three different methods: aTLAS, Tip-Adapter, and LP++.  Subfigure (a) compares the accuracy of these methods across different numbers of training examples (shots), demonstrating aTLAS's superior performance. Subfigure (b) illustrates, through a Venn diagram, the number of images correctly classified by each method that were misclassified by the pretrained CLIP model, highlighting aTLAS's unique contributions. Finally, subfigure (c) shows the improvement in accuracy achieved by each method on out-of-distribution (OOD) datasets, revealing aTLAS's robustness and generalizability.", "section": "Few-shot adaptation"}, {"figure_path": "G9OJUgKo4B/figures/figures_7_1.jpg", "caption": "Figure 2: Recognition accuracy versus the number of bases when optimising in a low-dimensional subspace. The accuracy is normalised by that of the fully fine-tuned model. Using task vectors to construct the projection matrix performs consistently better than using random bases on (a) MNIST [32], (b) CIFAR100 [31].", "description": "This figure shows the results of an experiment comparing the performance of using task vectors versus random bases for constructing a projection matrix in a low-dimensional subspace. The accuracy is normalized to that of a fully fine-tuned model.  The results, shown for MNIST and CIFAR100 datasets, demonstrate that using task vectors consistently outperforms using random bases.", "section": "Relation to intrinsic dimensionality"}, {"figure_path": "G9OJUgKo4B/figures/figures_8_1.jpg", "caption": "Figure 7: Scalability of aTLAS. We compare the accuracy of our method against LoRAs, and vary the amount of training data. Results are averaged over 22 datasets. Detailed results are included in Table 17.", "description": "This figure shows how the performance of aTLAS scales with the amount of training data.  It compares aTLAS with different numbers of learnable parameters (2k, 10k, 40k, 160k, and 2.4M) against LoRA (2.4M).  The x-axis represents the percentage of training data used, and the y-axis shows the average accuracy across 22 datasets. The results demonstrate that aTLAS's performance improves as the number of learnable parameters and the amount of training data increase, and that it becomes competitive with LoRA when sufficient data is available.", "section": "6.2 Scalability of aTLAS"}, {"figure_path": "G9OJUgKo4B/figures/figures_16_1.jpg", "caption": "Figure 8: visualisation of dataset image feature distributions as ellipses. The mean image features for all datasets are visualised as the ellipse center, with the dimensionality reduced to 2 using Principal Component Analysis (PCA). The dimensionality of covariance matrices are also reduced using the same principal components. We show visualisations with (a) \u00d71 and (b) \u00d73 standard deviations. Pre-trained CLIP [47] with ViT-B/32 is used to extract image features.", "description": "This figure visualizes the distributions of image features extracted from 22 different datasets using Principal Component Analysis (PCA) to reduce dimensionality to 2. The mean features of each dataset are represented by the center of an ellipse, and the covariance matrix is used to determine the shape and size of the ellipse.  The visualizations are shown with both 1 and 3 standard deviations, providing a visual representation of the spread of the image features for each dataset. This helps to illustrate the relationships and similarities between different image datasets.", "section": "A Datasets and task vectors"}, {"figure_path": "G9OJUgKo4B/figures/figures_18_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, 00 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T =  T(1) (2) has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates two key concepts of the aTLAS algorithm. (a) shows how task vectors, which represent the difference in weights between a pre-trained model and a model fine-tuned for a specific task, can be linearly combined to create new representations.  The coefficients of the linear combination are learned parameters. (b) highlights that anisotropic scaling, where different parameter blocks within a task vector are scaled differently, provides greater flexibility in composing task vectors compared to isotropic scaling (where all blocks are scaled equally).  Anisotropic scaling allows for a more nuanced and efficient combination of task-specific knowledge.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_18_2.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, \u03b80 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector \u03c4 = (\u03c4(1), \u03c4(2)) has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure shows two illustrations. The first one (a) illustrates how to learn task vector compositions using a pre-trained model and two task vectors. The second one (b) illustrates the flexibility of anisotropic scaling, assuming a task vector with two parameter blocks. In summary, this figure explains the main idea of the proposed method, aTLAS.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_19_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, \u03b80 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T =  T(1),T(2) has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates the core idea of the aTLAS algorithm proposed in the paper.  Panel (a) shows how task vector compositions are learned by linearly combining multiple task vectors (T1 and T2 in this case) with learned coefficients (\u03b11 and \u03b12). The pre-trained model weights (\u03b80) are also included in the composition. Panel (b) demonstrates the difference between isotropic and anisotropic scaling. Anisotropic scaling allows for independent scaling of different components (blocks) of the task vector, providing more flexibility in combining task vectors and thus enhancing the knowledge composition process.  The example loss contour plot shows how anisotropic scaling allows finding a more flexible and accurate optimal point compared to isotropic scaling, illustrating the advantage of the method.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_20_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, 00 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T =  has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates two key concepts of the paper: task vector composition and anisotropic scaling.  Panel (a) shows how multiple task vectors (representing knowledge learned for different tasks) can be combined linearly to create a new representation. The weights of a pre-trained model (\u03b8\u2080) serve as a baseline, and learned scaling coefficients (\u03bb\u2081, \u03bb\u2082) adjust the contribution of each task vector. Panel (b) highlights the advantage of anisotropic scaling.  Isotropic scaling would uniformly scale all parameter blocks within a task vector, limiting flexibility. Anisotropic scaling, however, scales each parameter block (e.g., weights, biases) independently with a unique coefficient, leading to greater flexibility in composing task vectors.", "section": "1 Introduction"}, {"figure_path": "G9OJUgKo4B/figures/figures_21_1.jpg", "caption": "Figure 9: visualisation of the learned coefficients for (a) standard and (b) linear task vectors in task negation. Note that coefficients for different datasets are learned independently, despite being visualised jointly. Large negative coefficients can be observed on weight matrices. CLIP with ViT-B/32 backbone is used.", "description": "This figure visualizes the learned coefficients for both standard and linearized task vectors during task negation.  Each row represents a different parameter block within the model, and each column corresponds to one of the eight datasets used in the experiment. The heatmap shows the learned coefficients, with warmer colors indicating larger (more positive) values and cooler colors indicating smaller (more negative) values.  The visualization highlights that weight matrices tend to learn large negative coefficients in the task negation process,  and the coefficients for different datasets are learned independently.", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/figures/figures_25_1.jpg", "caption": "Figure 1: Illustration of (a) learning task vector compositions (n = 2, 00 denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector T =  has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.", "description": "This figure illustrates the core idea of the paper, which is to improve task vector composition by learning anisotropic scaling.  Panel (a) shows how task vectors (T1, T2) from different domains are linearly combined with learned coefficients (A1, A2). Panel (b) visually demonstrates the concept of anisotropic scaling where the different components or blocks of the task vectors (represented as a vector) are weighted differently compared to isotropic scaling where each component would be equally weighted. This anisotropic scaling enables more flexible composition as different components can be weighted differently according to their relevance to the target task.", "section": "1 Introduction"}]