{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational model used extensively in the research for its strong performance on various tasks, even with zero-shot application."}, {"fullname_first_author": "Gabriel Ilharco", "paper_title": "Editing models with task arithmetic", "publication_date": "2023-05-01", "reason": "This paper introduces the concept of task vectors and demonstrates their compositional properties, forming the basis for the current research."}, {"fullname_first_author": "Guillermo Ortiz-Jim\u00e9nez", "paper_title": "Task arithmetic in the tangent space: Improved editing of pre-trained models", "publication_date": "2023-12-10", "reason": "This paper enhances task vector compositionality through linearization, improving the disentanglement of task vectors and enhancing the compositionality of the task arithmetic."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method that is highly relevant to the research due to its efficiency and compatibility with the proposed method."}, {"fullname_first_author": "Armen Aghajanyan", "paper_title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning", "publication_date": "2021-08-01", "reason": "This paper supports the method by providing theoretical justification for the parameter efficiency of the proposed approach."}]}