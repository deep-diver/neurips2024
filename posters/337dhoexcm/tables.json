[{"figure_path": "337dHOexCM/tables/tables_6_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the ROC Curve (AUC) scores achieved by various algorithms on three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  For each algorithm and dataset size, it shows both the Interquartile Mean (IQM) AUC and the Mean AUC along with their corresponding 95% confidence intervals.  The results highlight the performance of different algorithms across varying dataset complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_14_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by various algorithms on different datasets.  It breaks down the results by dataset size (all datasets, small datasets, and medium/large datasets) and includes confidence intervals to indicate the reliability of the results. The algorithms compared include KNN, TabPFN, TabPFN-3k, LightGBM, RandomForest, CatBoost, XGBoost, TabPFN-kNN, and LoCalPFN.  This comparison allows for an assessment of the relative performance of each algorithm across diverse datasets.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_14_2.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by various algorithms on three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC is a common metric for evaluating the performance of classification models. The table shows the Interquartile Mean (IQM) AUC and the mean AUC, along with 95% confidence intervals, for each algorithm and dataset size. This allows for a comprehensive comparison of the algorithms' performance across different dataset scales and complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_15_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC is a common metric for evaluating the performance of classification models, providing a measure of how well the model can distinguish between different classes.  The confidence intervals give a range within which the true AUC is likely to fall, reflecting the uncertainty in the estimated score due to the limited sample size of the datasets. The table allows for a comparison of LoCalPFN and other machine learning algorithms across various dataset sizes and difficulties.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_15_2.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by various algorithms on three dataset groups: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC is a metric for evaluating the performance of binary classification models.  The table includes the Interquartile Mean (IQM) AUC and the range of the 95% confidence intervals for each algorithm, offering a comprehensive view of the model performance across different dataset sizes and complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_16_1.jpg", "caption": "Table 4: 71 Datasets Selected for Benchmarking Deep Learning Models", "description": "This table lists 71 datasets used for benchmarking deep learning models in the paper.  For each dataset, it provides the dataset ID from OpenML, the number of instances, the number of features, the number of classes, the number of categorical features, and the imbalance ratio.  This subset of datasets was chosen to ensure that all deep learning models included in the comparison could run on them, unlike the full set of 95 datasets which contained larger datasets that were computationally prohibitive for some methods.", "section": "4.1 Experimental Setup"}, {"figure_path": "337dHOexCM/tables/tables_17_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for different machine learning algorithms.  The algorithms are evaluated on three dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets. The results provide a quantitative comparison of the performance of various algorithms across different dataset sizes, highlighting their strengths and weaknesses in handling tabular data of varying complexity and scale.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_17_2.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for the LoCalPFN model and several baseline models. The results are broken down for all 95 datasets, as well as separately for 47 smaller datasets and 48 medium/large datasets, to illustrate performance variation based on dataset size.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_17_3.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by various algorithms on three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC is a common metric for evaluating the performance of classification models. For each algorithm and dataset size, the table shows the Interquartile Mean (IQM) AUC and the 95% confidence interval.  The IQM AUC provides a robust estimate of the central tendency of the AUC scores, while the confidence interval indicates the uncertainty associated with the estimate.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_17_4.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for different machine learning algorithms.  The algorithms are evaluated on three groups of datasets: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The results showcase the performance comparison of various algorithms, highlighting the differences in their accuracy across different dataset sizes and complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_20_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by different algorithms on three dataset groups: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC metric is a common measure of classifier performance, indicating the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance. For each algorithm and dataset group, the table reports both the interquartile mean (IQM) of the AUC scores and the corresponding 95% confidence interval. The IQM is used because it is more robust to outliers compared to the traditional mean. The confidence intervals provide an indication of the uncertainty in the AUC estimates.  The table allows for the evaluation of the relative performance of different algorithms on datasets of varying sizes and complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_20_2.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for the LoCalPFN model and several baseline models across three dataset groups: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The AUC score is a measure of the model's ability to distinguish between classes.  The confidence intervals provide a range within which the true AUC score is likely to fall. This allows for comparison of the performance of different models on datasets of various sizes and complexities.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_21_1.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for various algorithms evaluated on three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets. The algorithms include kNN, TabPFN, TabPFN-3k, LightGBM, RandomForest, CatBoost, XGBoost, TabPFN-kNN, and LoCalPFN.  The results show a comparison of performance across different algorithms and dataset sizes, highlighting the relative strengths and weaknesses of each approach under varying conditions.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_21_2.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents the Area Under the Curve (AUC) scores and their corresponding 95% confidence intervals for various algorithms across three different dataset sizes: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  The algorithms compared include several tree-based methods (Random Forest, LightGBM, CatBoost, XGBoost) and several deep learning methods (KNN, TabPFN, TabPFN-kNN, LoCalPFN). The table allows for a comparison of the performance of different algorithms across various dataset sizes and helps to determine the best performing algorithm for each dataset size category.", "section": "4.2 Main Experiments"}, {"figure_path": "337dHOexCM/tables/tables_21_3.jpg", "caption": "Table 1: AUC scores and confidence intervals for all 95 datasets, 47 small datasets, and 48 medium/large datasets, respectively.", "description": "This table presents a comparison of the Area Under the Curve (AUC) scores achieved by different algorithms on three dataset groups: all 95 datasets, 47 small datasets, and 48 medium/large datasets.  For each algorithm and dataset group, the table shows the Interquartile Mean (IQM) AUC and the 95% confidence interval. The algorithms compared include KNN, TabPFN, TabPFN-3k, LightGBM, RandomForest, CatBoost, XGBoost, TabPFN-kNN, and LoCalPFN. The results demonstrate the superior performance of LoCalPFN across all dataset groups compared to other methods.", "section": "4.2 Main Experiments"}]