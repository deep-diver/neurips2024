[{"figure_path": "nkzSE5KkCA/figures/figures_2_1.jpg", "caption": "Figure 1: Our Pilot Study. We generated a set of prompts (262144 in total) following a fixed template, grouping them according to the different parts of speech (POS). These grouped texts are then passed into the CLIP text encoder, and we calculate the sensitivity as the average sentence distance within each group. As shown on the left-hand side, compared to POS representing content, CLIP is less sensitive to POS representing motion. (Results are consistent across different templates and different sets of words within each POS. Further details can be found in the appendix.)", "description": "This figure presents a pilot study to evaluate the sensitivity of the CLIP text encoder to different parts of speech (POS) related to content and motion. By generating a large set of prompts with fixed templates and varying POS, the study measures how well CLIP distinguishes between motion and content words. The results show that CLIP is less sensitive to POS related to motion, indicating a need for improved motion encoding in text-to-video models.", "section": "3 Method"}, {"figure_path": "nkzSE5KkCA/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of DEMO Training. As shown in the left-hand side, DEMO incorporates dual text encoding and text conditioning (for simplicity, other layers in the UNet are omitted). As shown in the right-hand side, during training, the Ltext-motion is used to enhance motion encoding, the Lreg is used to avoid catastrophic forgetting, the Lvideo-motion is to enhance motion integration. The snowflakes and flames denote frozen and trainable parameters, respectively.", "description": "This figure illustrates the DEMO (Decomposed Motion) framework for text-to-video generation.  The left side shows the architecture's dual text encoding (content and motion) and conditioning, highlighting the separation of content and motion information processing. The right side details the training process, emphasizing three key loss functions: L_text-motion (aligning cross-attention maps with temporal changes), L_reg (preventing catastrophic forgetting in the text encoder), and L_video-motion (constraining predicted video latent to real video motion).  Frozen and trainable parameters are also identified.", "section": "3 Method"}, {"figure_path": "nkzSE5KkCA/figures/figures_5_1.jpg", "caption": "Figure 3: Qualitative Comparison. Each video is generated with 16 frames. We display frames 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in the supplementary materials.", "description": "This figure shows a qualitative comparison of video generation results from four different models: LaVie, VideoCrafter2, ModelScopeT2V, and DEMO.  Three example video generation prompts are used: slow-motion falling flower petals, an old man speaking, and a horse race.  The figure displays selected frames from each generated video to highlight the visual quality and motion dynamics produced by each model. The full videos are available in supplementary materials.", "section": "4.2 Qualitative Evaluations"}, {"figure_path": "nkzSE5KkCA/figures/figures_9_1.jpg", "caption": "Figure 4: Limitations. DEMO does not support creating videos containing sequential motions specified by text. As shown in the example, two motions, \u201ca man standing in a kitchen and talking", "description": "This figure shows the limitations of the DEMO model in generating videos with sequential motions.  The caption points out that DEMO struggles to create videos where multiple actions occur one after another, instead generating a video where all actions happen simultaneously.  The example image shows a man both talking and the appearance of a mixer and milk carton at the same time;  these actions should occur sequentially.", "section": "5 Limitations and Future Work"}, {"figure_path": "nkzSE5KkCA/figures/figures_18_1.jpg", "caption": "Figure 3: Qualitative Comparison. Each video is generated with 16 frames. We display frames 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in the supplementary materials.", "description": "This figure shows a qualitative comparison of video generation results from four different models (LaVie, VideoCrafter2, ModelScopeT2V, and DEMO) on three different prompts.  Each model generated a 16-frame video, and the figure displays selected frames (1, 2, 4, 6, 8, 10, 12, 14, 15, and 16) for visual comparison. The prompts depict various scenes, with different levels of dynamic movement. The comparison aims to visually demonstrate the quality and realism of motion in the generated videos.", "section": "4.2 Qualitative Evaluations"}, {"figure_path": "nkzSE5KkCA/figures/figures_19_1.jpg", "caption": "Figure 6: Extended qualitative comparison. Each video is generated with 16 frames. We display frames 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in the supplementary materials.", "description": "This figure shows a qualitative comparison of video generation results from four different models: LaVie, VideoCrafter2, ModelScopeT2V, and DEMO.  Each model generated a short video (16 frames) based on three textual prompts: \"A woman wearing Hanfu opens a paper fan in her hand.\", \"A roast turkey, counterclockwise\", and \"Apples and oranges, clockwise.\" The figure displays selected frames (1, 2, 4, 6, 8, 10, 12, 14, 15, 16) of each video to illustrate the motion quality and visual fidelity of each model's output. Full videos are available in the supplementary materials.", "section": "9.5 Extended Qualitative Evaluations"}, {"figure_path": "nkzSE5KkCA/figures/figures_20_1.jpg", "caption": "Figure 7: Extended qualitative comparison. Each video is generated with 16 frames. We display frames 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in the supplementary materials.", "description": "This figure compares the video generation results of four different models: LaVie, VideoCrafter2, ModelScopeT2V, and DEMO.  Three different scenarios are shown, each with a different level of complexity: giraffes in a savanna, Superman and Spiderman shaking hands in a watercolor style, and a lion catching its prey. Each scenario is represented by a sequence of 16 frames, with a subset of the frames displayed for comparison. The comparison highlights the differences in motion realism and quality achieved by the four methods. The full videos for each comparison can be found in the supplementary materials.", "section": "9.5 Extended Qualitative Evaluations"}, {"figure_path": "nkzSE5KkCA/figures/figures_21_1.jpg", "caption": "Figure 8: Extended qualitative comparison. Each video is generated with 16 frames. We display frames 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in the supplementary materials.", "description": "This figure shows a qualitative comparison of video generation results for three different prompts, comparing the outputs of four different models: LaVie, VideoCrafter2, ModelScopeT2V, and DEMO.  Each row represents a single prompt, with the generated videos displayed for each model.  The models are compared based on their ability to generate realistic and visually appealing videos that accurately reflect the text prompt. The frames shown are a subset of the full 16-frame videos available in the supplementary materials.", "section": "9.5 Extended Qualitative Evaluations"}]