{"references": [{"fullname_first_author": "Toby Shevlane", "paper_title": "Model evaluation for extreme risks", "publication_date": "2023-05-15", "reason": "This paper is foundational for the concept of trustworthy evaluations of AI systems, which is central to the main argument."}, {"fullname_first_author": "Evan Hubinger", "paper_title": "Risks from learned optimization in advanced machine learning systems", "publication_date": "2021-12-15", "reason": "This paper introduces the concept of deceptive alignment, a crucial idea related to the incentives behind AI sandbagging."}, {"fullname_first_author": "Ryan Greenblatt", "paper_title": "AI control: Improving safety despite intentional subversion", "publication_date": "2023-12-12", "reason": "This paper directly addresses the challenges of ensuring AI safety when developers might intentionally subvert safety evaluations, a key aspect of sandbagging."}, {"fullname_first_author": "Rohin Shah", "paper_title": "Goal misgeneralization: why correct specifications aren't enough for correct goals", "publication_date": "2022-10-01", "reason": "This paper highlights the complexities in specifying AI goals, which are relevant to understanding the strategic motivations behind sandbagging behavior."}, {"fullname_first_author": "Joseph Halpern", "paper_title": "Towards formal definitions of blameworthiness, intention, and moral responsibility", "publication_date": "2018-01-01", "reason": "This paper provides a formal framework for understanding intentionality and responsibility which is critical for distinguishing between strategic underperformance and accidental failures."}]}