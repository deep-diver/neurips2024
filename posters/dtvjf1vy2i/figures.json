[{"figure_path": "dtvJF1Vy2i/figures/figures_0_1.jpg", "caption": "Figure 1: Idefics2-chatty analyzes the table to compute and answer the query.", "description": "The figure shows a prompt in natural language, a table of financial data, and the Idefics2 model's response. The prompt asks to calculate the combined total of interest and long-term debt for 2024, given that interest expense is double the 2014 value and long-term debt is 10% higher than in 2015. Idefics2 correctly answers the question by performing the calculations step by step, demonstrating its ability to understand and process numerical data presented in table format.", "section": "Introduction"}, {"figure_path": "dtvJF1Vy2i/figures/figures_2_1.jpg", "caption": "Figure 2: Idefics2 fully-autoregressive architecture: Input images are processed by the Vision encoder. The resulting visual features are mapped (and optionally pooled) to the LLM input space to get the visual tokens (64 in our standard configuration). They are concatenated (and potentially interleaved) with the input sequence of text embeddings (green and red column). The concatenated sequence is fed to the language model (LLM), which predicts the text tokens output.", "description": "This figure illustrates the architecture of Idefics2, a fully autoregressive vision-language model.  The model takes image and text inputs. The image is first processed by a vision encoder, producing a sequence of hidden states. These states are then mapped and optionally pooled to the language model's input space, creating visual tokens. These visual tokens are then concatenated with text embeddings, creating a combined sequence that is fed into the language model (LLM). The LLM predicts the output text tokens. The figure clearly shows the process of image encoding, modality projection (mapping visual features to the LLM's space), pooling (optional reduction of visual tokens), and concatenation of visual and text tokens before processing with the language model.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/figures/figures_7_1.jpg", "caption": "Figure 3: An example of text transcription with Idefics2-base.", "description": "This figure shows an example of Idefics2-base performing text transcription from a handwritten letter image. The prompt is to transcribe the content of the letter, and the model successfully transcribes it in a clean, readable format. This showcases the model's ability to extract text information from images, even if it is handwritten and not perfectly clear.", "section": "4.2 Instruction fine-tuning"}, {"figure_path": "dtvJF1Vy2i/figures/figures_19_1.jpg", "caption": "Figure 4: Comparison of the cross-attention and fully autoregressive architectures through the number of steps, the number of images and the number of text tokens.", "description": "This figure compares the performance of cross-attention and fully autoregressive architectures across three axes: the number of optimization steps, the number of images, and the number of text tokens.  It shows that the fully autoregressive architecture with LoRA generally outperforms the cross-attention architecture across all three axes. The differences are visually apparent, showing a clear advantage for the fully autoregressive approach in terms of average score.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/figures/figures_23_1.jpg", "caption": "Figure 5: Idefics2-chatty finds the requested information in the resume, and organizes it in JSON format.", "description": "The figure shows an example of Idefics2-chatty's ability to extract specific information from a resume and format the information into a JSON object.  The prompt requests the name, email, current job, and education. The model correctly extracts this information from the provided resume image and structures the data into a well-formatted JSON output, demonstrating its capability for information extraction and structured data generation.", "section": "4.2 Instruction fine-tuning"}, {"figure_path": "dtvJF1Vy2i/figures/figures_24_1.jpg", "caption": "Figure 6: Idefics2-chatty describes an AI-generated image.", "description": "The figure shows an example of Idefics2-chatty's ability to describe an image.  The prompt is simply \"Describe the image.\" The AI-generated image depicts a whimsical scene with three robot soldiers carrying large cannons shaped like bread rolls, positioned in front of the Eiffel Tower in Paris. The description provided by the model is detailed and accurate, demonstrating its capacity for image understanding and creative textual generation.", "section": "A.4 Limitations"}, {"figure_path": "dtvJF1Vy2i/figures/figures_25_1.jpg", "caption": "Figure 7: Idefics2-chatty answers a question on a scientific diagram.", "description": "The figure shows a prompt asking what happens to fish if the number of pelicans increases.  A diagram depicting a terrestrial and aquatic food chain is included in the prompt.  The Idefics2 model's response correctly explains that an increase in the pelican population would likely lead to a decrease in the fish population due to increased predation. The model also correctly points out the potential effects this would have on the ecosystem and other species that rely on fish for food.", "section": "A.5 Red-teaming"}]