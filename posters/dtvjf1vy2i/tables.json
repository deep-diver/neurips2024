[{"figure_path": "dtvJF1Vy2i/tables/tables_2_1.jpg", "caption": "Table 1: Ablation on the language model backbone.", "description": "This table presents the results of an ablation study on the language model backbone used in a vision-language model. Two different language model backbones, Llama-1-7B and Mistral-7B, were compared. The average score on vision-language benchmarks is reported for each backbone, showing that Mistral-7B achieves a higher average score than Llama-1-7B, highlighting the impact of the choice of language model backbone on the model's performance.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_3_1.jpg", "caption": "Table 2: Ablation on the vision encoder backbone.", "description": "This table presents the results of an ablation study comparing the performance of different vision encoders when used in a vision-language model.  The study keeps other aspects of the model (such as the language model and training procedures) constant.  The goal is to isolate the impact of vision encoder choice on overall model performance, allowing for a direct comparison of the effects of different vision encoders.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_3_2.jpg", "caption": "Table 1: Ablation on the language model backbone.", "description": "The table presents the results of an ablation study comparing the performance of different language model backbones when used in a vision-language model.  The study keeps other variables, like the vision backbone and training data, constant to isolate the impact of the language model choice on the average performance score across several vision-language benchmarks.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_3_3.jpg", "caption": "Table 3: Ablation for the architecture and method of training.", "description": "This table compares the performance of fully autoregressive and cross-attention architectures with different training methods (Frozen and LoRA).  It shows that the fully autoregressive architecture, particularly when using LoRA, outperforms the cross-attention architecture in terms of average score, despite having fewer trainable parameters in some cases. This highlights the impact of architectural choices and training methods on vision-language model performance.", "section": "3.2 How does the fully autoregressive architecture compare to the cross-attention architecture?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_4_1.jpg", "caption": "Table 4: Ablation on the pooling strategy.", "description": "This table presents the results of an ablation study on different pooling strategies used in the vision encoder.  The study compares the performance of using a Perceiver with 128 visual tokens versus a Perceiver with 64 visual tokens. The average score across multiple downstream benchmarks is reported for each configuration, demonstrating the impact of the number of visual tokens on model performance.  The results show that reducing the number of visual tokens (from 128 to 64) using a Perceiver leads to a slight improvement in average performance.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/tables/tables_4_2.jpg", "caption": "Table 5: Ablation on the aspect-ratio preserving strategy.", "description": "This table presents the ablation study results on the impact of preserving the original aspect ratio and image resolution on the model's performance.  It compares the average scores achieved when using square images (resized to a fixed resolution) versus when preserving the original aspect ratio and allowing variable resolutions (between 378 and 768 pixels). The results show a minor difference in average score, suggesting that maintaining the aspect ratio doesn't significantly harm performance, which is beneficial for efficiency and handling diverse image formats.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/tables/tables_6_1.jpg", "caption": "Table 6: Ablation on synthetic captions against alt-text for image-text pairs.", "description": "This table presents the results of an ablation study comparing the use of synthetic captions versus alt-texts for training a vision-language model.  The \"Avg. score\" column shows the average performance of the model trained with each type of caption, indicating that synthetic captions lead to slightly better performance than alt-texts. This suggests that synthetic captions might be a more effective approach for training vision-language models, potentially due to factors such as greater quantity or consistency of data.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_6_2.jpg", "caption": "Table 7: Ablation on the synergy between OCR data and image resolution. We pre-trained the models for 5,500 steps, followed by 500 steps of fine-tuning on DocVQA.", "description": "This table shows the results of an ablation study on the impact of using OCR data and different image resolutions on the performance of vision-language models for the DocVQA task.  The model was pre-trained for 5,500 steps, and then fine-tuned for an additional 500 steps on the DocVQA dataset.  The table compares three scenarios: (1) no OCR data and a resolution of 384 pixels, (2) no OCR data and a resolution of 768 pixels, and (3) OCR data included and a resolution of 768 pixels.  The results show a clear improvement in DocVQA performance with higher resolution and the inclusion of OCR data.", "section": "3.3 Where are the efficiency gains?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_7_1.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of Idefics2-base, a foundational vision-language model with 8 billion parameters, against other state-of-the-art base VLMs (Vision-Language Models).  The comparison is based on four downstream benchmarks: VQAv2 (visual question answering), TextVQA (text in visual question answering), OKVQA (external knowledge visual question answering), and COCO (image captioning).  The evaluation uses 8 random in-context examples and an open-ended setting for VQA tasks.  The table highlights the model's size (in billions of parameters), architecture (fully autoregressive or cross-attention), average number of visual tokens per image, and average scores on each benchmark.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_8_1.jpg", "caption": "Table 9: Performance of Idefics2 against state-of-the-art VLMs up to a size of 14B parameters. The evaluations are done in zero shot. Idefics2 with 64 or 320 tokens per image is the same model (same weights), only the inference differs. The full table is present in Appendix A.3.2.\n(Benchmark, Split, Metric): (MMMU, val/test, MMMU score), (MathVista, testmini, MMMU score), (TextVQA, val, VQA acc.), (MMBench, test, accuracy).", "description": "This table compares the performance of the Idefics2 model against other state-of-the-art Vision-Language Models (VLMs) on four different benchmarks: MMMU, MathVista, TextVQA, and MMBench.  The comparison is done using zero-shot evaluation, meaning the models are not fine-tuned for any specific task before evaluation.  Importantly, the table shows that Idefics2, even with a much smaller number of tokens per image (64 vs. hundreds or thousands for others), achieves state-of-the-art performance comparable to much larger models (up to 14B parameters). The results suggest that Idefics2's efficient architecture contributes significantly to its performance.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_19_1.jpg", "caption": "Table 2: Ablation on the vision encoder backbone.", "description": "This table presents the results of an ablation study on the vision encoder backbone used in the vision-language model.  Three different vision encoders were compared: CLIP-ViT-H, EVA-CLIP-5B, and SigLIP-SO400M. The table shows the average score across four downstream benchmarks (VQAv2, OKVQA, TextVQA, and COCO) and the resolution of the image processed by each encoder.  The results suggest that using a better pre-trained vision encoder improves the performance of the vision-language model.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_20_1.jpg", "caption": "Table 11: Ablation on the modality projection", "description": "This table shows the average scores achieved by three different vision-language connector methods: Linear Projection, Mapping Network, and Perceiver.  The Perceiver method significantly outperforms the other two, indicating its effectiveness in fusing visual and textual information.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/tables/tables_20_2.jpg", "caption": "Table 13: Ablation on the addition of a modality projection before the perceiver resampler", "description": "This table presents the ablation study on adding a Multilayer Perceptron (MLP) modality projection layer before the perceiver resampler in the model architecture.  The experiment compares the model's average performance with and without the MLP layer. The results demonstrate the impact of the MLP on the model's overall performance, showing improvement when it is included. This highlights the effectiveness of the MLP in enhancing the fusion of visual and textual information.", "section": "3 Exploring the design space of vision-language models"}, {"figure_path": "dtvJF1Vy2i/tables/tables_20_3.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of Idefics2-base, a foundational vision-language model with 8 billion parameters, against other state-of-the-art base vision-language models.  The comparison is made across four different benchmarks evaluating various capabilities: VQAv2 (general visual question answering), TextVQA (OCR abilities), OKVQA (external knowledge usage), and COCO (image captioning).  The evaluation uses 8 random in-context examples and an open-ended setting for VQA tasks.  The table also indicates whether the models used a fully autoregressive (FA) or cross-attention (CA) architecture.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_21_1.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of Idefics2-base, a foundational vision-language model, against other state-of-the-art base VLMs on four downstream benchmarks: VQAv2 (visual question answering), TextVQA (text VQA), OKVQA (external knowledge VQA), and COCO (captioning).  The evaluation used 8 random in-context examples and an open-ended setting for VQA tasks.  The table also indicates the model architecture (fully autoregressive or cross-attention) for each model.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_22_1.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of the Idefics2-base model to other state-of-the-art Vision-Language Models (VLMs) on four different benchmarks.  The benchmarks assess different capabilities, including visual question answering (VQAv2, TextVQA, OKVQA), and image captioning (COCO). The table shows the average score for each model across the four benchmarks, considering the model size and the architecture (fully autoregressive or cross-attention).  The number of visual tokens per image is also provided to help understand the compute cost.", "section": "4 Idefics2 - an open state-of-the-art vision-language foundation model"}, {"figure_path": "dtvJF1Vy2i/tables/tables_22_2.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of the Idefics2-base model against other state-of-the-art baseline Vision-Language Models (VLMs) across four different benchmark datasets.  The metrics used vary depending on the dataset (VQAv2 uses VQA accuracy, TextVQA uses VQA accuracy, OKVQA uses VQA accuracy, and COCO uses CIDEr). The evaluations were conducted using 8 random in-context examples, and an open-ended setting was used for the VQA tasks.  The table also indicates whether each VLM used a fully autoregressive (FA) or cross-attention (CA) architecture.", "section": "4 Idefics2 - an open state-of-the-art vision-language foundation model"}, {"figure_path": "dtvJF1Vy2i/tables/tables_23_1.jpg", "caption": "Table 9: Performance of Idefics2 against state-of-the-art VLMs up to a size of 14B parameters. The evaluations are done in zero shot. Idefics2 with 64 or 320 tokens per image is the same model (same weights), only the inference differs. The full table is present in Appendix A.3.2.\n(Benchmark, Split, Metric): (MMMU, val/test, MMMU score), (MathVista, testmini, MMMU score), (TextVQA, val, VQA acc.), (MMBench, test, accuracy).", "description": "This table compares the performance of the Idefics2 model against other state-of-the-art Vision-Language Models (VLMs) on several benchmark datasets.  It highlights Idefics2's performance in various tasks related to vision and language understanding, demonstrating its capabilities despite having a smaller parameter count than many of its competitors. The table also shows that varying the number of tokens per image (64 vs. 320) doesn't significantly alter the model's performance, demonstrating its efficiency.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}, {"figure_path": "dtvJF1Vy2i/tables/tables_24_1.jpg", "caption": "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done with 8 random in-context examples, and in an open-ended setting for VQA tasks. FA: fully autoregressive architecture. CA: cross-attention architecture. (Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc., val), (COCO, CIDEr, test)", "description": "This table compares the performance of the Idefics2-base model against other state-of-the-art Vision-Language Models (VLMs) on four benchmark tasks: VQAv2, TextVQA, OKVQA, and COCO.  The benchmarks evaluate different capabilities, such as visual question answering, OCR abilities, external knowledge access, and image captioning.  The table shows the average score for each model across the four benchmarks, considering the model size (in billions of parameters) and the architecture type (fully autoregressive or cross-attention).  It also notes the number of visual tokens used per image, indicating a model's efficiency.", "section": "3.1 Are all pre-trained backbones equivalent for VLMs?"}]