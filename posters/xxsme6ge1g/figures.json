[{"figure_path": "XxSME6GE1G/figures/figures_1_1.jpg", "caption": "Figure 1: Performance comparison of various fine-tuning methods under three OOD data mixing scenarios. The target domain is medical knowledge, using Chinese subset of MMedBench [52] as the in-domain training dataset. (a) The dataset is mixed with medical OOD data from CMExam [32], maintaining a total dataset size of 20k; (b) The dataset is mixed with general OOD data from CoT-Collection [24], also keeping the total dataset size at 20k; (c) The dataset includes general OOD data from CoT-Collection, while the size of the in-domain training dataset remains at 20k. As the proportion of OOD data increases, the performance of the vanilla fine-tuning declines significantly, whereas TAIA manages to sustain robust performance in the target domain (details in Appendix E.5).", "description": "This figure compares the performance of different fine-tuning methods (vanilla and TAIA) when dealing with varying amounts of out-of-distribution (OOD) data in a medical knowledge domain.  Three scenarios are shown: (a) mixing with medical OOD data, (b) mixing with general OOD data, and (c) increasing the amount of general OOD data while keeping the in-domain data constant.  The results demonstrate that vanilla fine-tuning's performance degrades significantly as the proportion of OOD data increases, while TAIA maintains robust performance.", "section": "1 Introduction"}, {"figure_path": "XxSME6GE1G/figures/figures_4_1.jpg", "caption": "Figure 3: Performance of TOA and TAIA with the layer-wise FFN LoRA. All models are equipped with attention LoRA at each layer and fine-tuned on a corpus mixture with 50% OOD data.", "description": "This figure shows the performance comparison between TOA (Train Only Attention Parameters) and TAIA (Training All parameters but Inferring with only Attention) methods.  Both methods use LoRA (Low-Rank Adaptation) and fine-tune only part of the model parameters.  The x-axis represents the number of FFN (Feed-Forward Network) layers that are fine-tuned. The y-axis represents the accuracy achieved on a downstream task.  The figure demonstrates that TAIA consistently outperforms TOA, highlighting the benefit of incorporating some FFN parameter updates during training, even when only the attention parameters are used during inference.", "section": "3.3 Towards Optimal Parameter Selection Strategy"}, {"figure_path": "XxSME6GE1G/figures/figures_9_1.jpg", "caption": "Figure 4: (a)Average performance with different sizes of fine-tuning datasets; (b) The few-shot performance on MATH; (c) The layer-wise residual rank of the hidden states on MATH.", "description": "This figure presents a three-part analysis of the impact of fine-tuning dataset size on model performance using the TAIA method.  Part (a) shows the average accuracy across different sizes of fine-tuning datasets, highlighting the performance gains of TAIA over the vanilla fine-tuning and base model. Part (b) focuses on few-shot learning performance using the MATH dataset, revealing TAIA's ability to retain few-shot capabilities. Part (c) displays the layer-wise residual rank of the hidden states within the MATH dataset, demonstrating TAIA's effectiveness in leveraging the representation capabilities of self-attention modules.", "section": "Analysis"}, {"figure_path": "XxSME6GE1G/figures/figures_16_1.jpg", "caption": "Figure 5: (a) The performance of LLMs fine-tuned with three specific downstream datasets on C-Eval and (b) the cosine similarity distribution of the hidden layer. The cosine similarity is calculated as the average distance between the output hidden state of three fine-tuned models. TAIA achieves the best performance on C-Eval and has the most consistent hidden state among the three cases.", "description": "This figure shows the performance comparison of different fine-tuning methods (Vanilla, TAIF, and TAIA) on three downstream datasets (COT Collection, Medical Collection, and OpenMath) using C-Eval benchmark.  The left panel (a) presents a bar chart showing the accuracy of each method on each dataset. The right panel (b) displays kernel density estimates illustrating the distribution of cosine similarity for the hidden states of the three models.  TAIA consistently outperforms other methods and exhibits higher similarity among hidden states, indicating better generalization.", "section": "Analysis"}]