[{"type": "text", "text": "Optimal, Efifcient and Practical Algorithms for Assortment Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We address the problem of active online assortment optimization problem   \n2 with preference feedback, which is a framework for modeling user choices   \n3 and subsetwise utility maximization. The framework is useful in various   \n4 real-world applications including ad placement, online retail, recommender   \n5 systems, and fine-tuning language models, amongst many others. The prob  \n6 lem, although has been studied in the past, lacks an intuitive and practical   \n7 solution approach with simultaneously efficient algorithm and optimal re  \n8 gret guarantee. E.g., popularly used assortment selection algorithms often   \n9 require the presence of a \u2018strong reference\u2019 which is always included in the   \n0 choice sets, further they are also designed to offer the same assortments   \n1 repeatedly until the reference item gets selected\u2014all such requirements   \n2 are quite unrealistic for practical applications. In this paper, we designed   \n3 efficient algorithms for the problem of regret minimization in assortment   \n4 selection with Plackett Luce (PL) based user choices. We designed a novel   \n5 concentration guarantee for estimating the score parameters of the PL model   \n6 using \u2018Pairwise Rank-Breaking\u2019, which builds the foundation of our proposed   \n17 algorithms. Moreover, our methods are practical, provably optimal, and   \n8 devoid of the aforementioned limitations of the existing methods. Empirical   \n9 evaluations corroborate our findings and outperform the existing baselines. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Studies have shown that it is often easier, faster and less expensive to collect feedback on a   \n22 relative scale rather than asking ratings on an absolute scale. E.g., to understand the liking   \n23 for a given pair of items, say (A,B), it is easier for the users to answer preference-based   \n24 queries like: \u201cDo you prefer Item A over B?\", rather than their absolute counterparts: \u201cHow   \n25 much do you score items A and B in a scale of [0-10]?\". Due to the widespread applicability   \n26 and ease of data collection with relative feedback, learning from preferences has gained much   \n27 popularity in the machine-learning community, especially the active learning literature which   \n28 has applications in Medical surveys, AI tutoring systems, Multi-player sports/games, or any   \n29 real-world systems that have ways to collect feedback in terms of preferences. The problem   \n30 is famously studied as the Dueling-Bandit (DB) problem in the active learning community   \n31 [41, 3, 45, 46, 44], which is an online learning framework for identifying a set of \u2018good\u2019 items   \n32 from a fixed decision-space (set of items) by querying preference feedback of actively chosen   \n33 item-pairs. Consequently, the generalization of Dueling-Bandits, with subset-wise preferences   \n34 has also been developed into an active field of research. For instance, applications like   \n35 Web search (e.g. Google, Bing, or even in some versions of ChatGPT), online shopping   \n36 (Amazon, App stores, Google Flights), recommender systems (e.g. Youtube, Netflix, Google   \n37 News/Maps, Spotify) typically involve users expressing preferences by choosing one result (or   \n38 a handful of results) from a subset of offered items and often the objective of the system is to   \n39 identify the \u2018most-profitable\u2019 subset to offer to their users. The problem, popularly termed   \n40 as \u2018Assortment Optimization\u2019 is studied in many interdisciplinary literature, e.g. Online   \n41 learning and bandits [10], Operations research [40, 2], Game theory [15], RLHF [20, 30], to   \n42 name a few.   \n43 Problem (Informal): Active Optimal Assortment (AOA) Active Assortment Opti  \n44 mization (a.k.a. Utility Maximization with Subset Choices) [13, 2, 23, 22] is an active   \n45 learning framework for finding the \u2018optimal\u2019 profit-maximizing subset. Formally, assume   \n46 we have a decision set of $[K]:=\\{1,2,\\dots K\\}$ of $K$ items, with each item being associated   \n47 with the score (or utility) parameters $\\pmb{\\theta}:=(\\theta_{1},\\theta_{2},\\ldots,\\theta_{K})$ (without loss of generality assume   \n48 $\\theta_{1}\\geq\\theta_{2}\\geq...\\geq\\theta_{K}\\geq0$ ). At each round $t=1,2,\\dots$ , the learner or the algorithm gets to   \n49 query an assortment (typically subsets containing up to ${\\boldsymbol{r}}n$ -items) $S_{t}\\,\\subseteq\\,[K]$ , upon which   \n50 it gets to see some (noisy) relative preferences across the items in $S_{t}$ , typically generated   \n51 according to an underlying Plackett-Luce (PL) choice model with parameters $\\pmb{\\theta}$ (1). Further,   \n52 to allow the event where no items are selected, we also model a No-Choice (NC) item, indexed   \n53 by item-0, with PL parameter $\\theta_{0}\\in\\mathbb{R}_{+}$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "54 (Objective 1.) Top- ${\\boldsymbol{r}}n$ : identify the top- ${\\boldsymbol{r}}n$ item-set: $\\{\\theta_{1},\\ldots,\\theta_{m}\\}$ , for some $m\\in[1,K]$ . ", "page_idx": 1}, {"type": "text", "text": "55 (Objective 2.) Wtd-Top- ${\\boldsymbol{r}}n$ : A more general objective could also consider a weight (or   \n56 price) $r_{i}\\,\\in\\,\\mathbb{R}_{+}$ associated with the item $i\\,\\in\\,[K]$ , and the goal could be to identify the   \n57 assortment (subset) with maximum weighted utility 1, as detailed in Sec. 2.   \n58 Related Works and Limitations: As stated above, the problem of AOA is fundamental   \n59 in many practical scenarios, and thus widely studied in multiple research areas, including   \n60 Online ML/learning theory and operations research.   \n61 \u2022 In the Online ML literature, the problem is well-studied as Multi-Dueling Bandits [39, 14],   \n62 or Battling Bandits [35, 34, 11], which is an extension of the famous Dueling Bandit problem   \n63 [46, 45]. The main limitation of this line of work is the lack of practical objectives, which either   \n64 aim to identify the \u2018best-item\u2019 $1(=\\arg\\operatorname*{max}_{i\\in[K]}\\theta_{i})$ within a PAC (probably approximately   \ncorrect) framework [36, 16, 17, 31] or quantifying regret against the best items [35, 12]. Note   \n66 the latter actually leads to the optimal subset choice of repeatedly selecting the optimal item,   \n67 arg maxi $\\theta_{i}$ , $m$ times, i.e. $(1,1,\\ldots1)$ , which is unrealistic from the viewpoint of real-world   \n68 system design. Selecting an assortment of distinct top- ${\\boldsymbol{r}}n$ items (Top- $m$ -AOA) or maximum   \n69 expected utility (Wtd-Top- ${\\boldsymbol{r}}n$ -AOA) makes more sense.   \n\u2022 On the other hand, a similar line of the problem has been studied in operations research   \n71 and dynamic assortment selection literature, where the goal is to offer a subset of items to   \n72 the customers in order to maximize expected revenue. The problem has been studied under   \n73 different user choice models, e.g. PL or Multinomial-Logit models [2], Mallows and mixture of   \n74 Mallows [22], Markov chain-based choice models [23], single transition model [27] etc. While   \n75 these works indeed consider a more practical objective of finding the best assortment (subset)   \n76 with the highest expected utility for a regret minimization objective, (1) a major drawback   \n77 in their approach lies in the algorithm design which requires to keep on querying the same set   \n78 multiple times, e.g. [2, 29, 18, 1]. Such design techniques could be impractical to be deployed   \n79 in real systems where users could easily get annoyed if the same items are shown again and   \n80 again. For example, in ad-placement, music/movies/news/tweets/reels recommendations,   \n81 offering the same assortment could increase user dissatisfaction and disengagement.   \n82 (2) The second major drawback of this line of work lies in the structural assumption of   \n83 their underlying choice models which requires the existence of a reference/default item, that   \n84 needs to be part of every assortment $S_{t}$ . This leads to assuming a No-Choice item, typically   \ndenoted as item-0, which is a default choice of any assortment $S_{t}$ . Further a stronger and   \n86 more unrealistic assumption lies in the fact that they require to assume that the above pivot   \n87 is stronger than the rest of the $K$ items, i.e. $\\theta_{0}\\ \\geq\\operatorname*{max}_{\\ldots}i\\in[K]\\ \\theta_{i}$ , i.e. the No-Choice (NC)   \n88 action is the most likely outcome of any assortment $S_{t}$ . This is often unrealistic, e.g., during   \n89 user interactions with language models, or online shopping, or Route recommendation in   \n90 GPS navigation, a NC action is highly improbable. Consequently, such assumption limits the   \n91 use in real-systems. In the existing literature [2, 28, 1, 24], such assumptions are primarily   \n92 adapted solely for theoretical needs, precisely for maintaining concentration bounds of the   \n93 PL parameters $\\pmb{\\theta}$ , and hence not well justified from a practical viewpoint. Some recent   \n94 developments also generalized the AOA problem to linear MNL scores to incorporate large   \n95 actions embedded in $d$ -dimension [43, 42, 28], however, their approaches are either limited   \n96 to the above restrictions or suffer sub-o\u221aptimal regret guarantees without those assumptions   \n97 (e.g. the regret bound of [28] is $O(d^{3/2}{\\sqrt{T}})$ which is suboptimal by a $d$ -factor). Considering   \n98 the above limitations of the AOA literature, we set to answer two questions:   \n99 (1) Can we consider a general AOA model where the default item, like the NC item defined   \n100 above, is not necessarily the strongest one, i.e. $\\theta_{0}\\geq\\operatorname*{max}_{i\\in[K]}\\theta_{i}{}$ ?   \n101 (2) Can we design a practical and regret optimal algorithm for the AOA framework, without   \n102 needing to play the same repetitive actions and yet converge to the optimal assortment?   \n103 Contributions We answer these questions in the affirmative and present best of all   \n104 scenarios. We design practical algorithms on practical AOA framework with practical   \n105 objectives\u2013Unlike the existing approaches of the AOA, literature [2, 18], we do not have to   \n106 keep playing the same assortment multiple times, neither require a strongest default item   \n107 (like NC satisfying $\\theta_{0}\\geq\\operatorname*{max}_{i\\in[K]}\\theta_{i})$ . Moreover, our objectives do not require us to converge   \n108 to a multiset of replicated arms like $(1,1,\\ldots1)$ , but converge to the utility-maximizing set of   \n109 distinct items. We list our contributions below:   \n110 1. A General AOA Setup: We work with a general problem of AOA for PL model,   \n111 which requires no additional structural assumption of the $\\pmb{\\theta}$ parameters such as $\\theta_{0}\\ge\\operatorname*{max}_{i}\\theta_{i}$ ,   \n112 unlike the existing works. We designed algorithms for two separate objectives Top- $m$ and   \n113 Wtd-Top- $m$ as discussed above (Sec. 2).   \n114 2. Practical, Efifcient and Optimal Algorithm: In Sec. 3, we give a practical,   \n115 efficient and optimal algorithm for MNL Assortment (up to log factors and t\u221ahe magnitude of   \n116 $\\theta_{\\mathrm{max}}$ ). The regret bound of our algorithm AOA-RBPL (Alg. 1) yields $\\tilde{O}(\\sqrt{K T})$ regret for   \n117 both Top- ${\\boldsymbol{r}}n$ and Wtd-Top- ${\\boldsymbol{r}}n$ objective. Our algorithms use a novel parameter estimation   \n118 technique for discrete choice models based on the concept of Rank-Breaking (RB) which is   \n119 one of our key contributions towards designing the efficient and optimal algorithm. This   \n120 enables our algorithm to perform optimally without requiring the No-Choice item to be   \n121 the strongest. Appendix A details the key concept of our parameter estimation technique   \n122 exploiting the concept of RB. Our resulting algorithm plays optimistically based on the UCB   \n123 estimates of PL parameters and does not require repeating the same subset multiple times,   \n124 justifying our title.   \n125 3. Improvement with Adaptive Pivots: In Sec. 4, we refine the performance of   \n126 our algorithm by employing the novel idea of \u2018adaptive pivots\u2019 (a reference item) and   \n127 proposed AOA-RB $\\mathrm{PL}$ -Adaptive. Performance-wise this removes the asymptotic dependence   \n128 on $\\theta_{\\mathrm{max}}=\\operatorname*{max}_{i}\\theta_{i}/\\theta_{0}$ in the regret analysis. This enables the algorithm to work effectively   \n129 in scenarios where the No-Choice item is less likely to be selected, i.e., $\\theta_{\\mathrm{max}}\\gg1$ . This   \n130 leads to a huge improvement in our experiments, especially in the range of low $\\theta_{0}$ , where   \n131 AOA-RBPL-Adaptive drastically outperforms over the existing baseline. Comparison of our   \n132 regret bound with existing work is detailed in Table 1.   \n133 4. Emperical Analysis. Finally, we corroborate our theoretical results with empirical   \n134 evaluations (Sec. 5), which certify our superior performance in the general AOA setups.   \n135 It is also worth mentioning that our proposed algorithm and their respective regret analysis   \n136 could be extended to any general random utility (RUM) based preference models [38, 37],   \n137 as explained in Rem. 1. However, to keep the focus on the AOA problem and ease the   \n138 presentation, we stick to the special case of MNL choice model based preferences. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "7VNvM9SnRE/tmp/7b64231d42c9cc08ce964565278e65f7e69eb0312d263dd0d19cd6d8933bc29c.jpg", "table_caption": ["Table 1: Our Contribution vs the Existing Results in the $K$ -armed MNL-Assortment literature "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "139 2 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 We write $[n]=\\{1,2,...,n\\}$ and $\\mathbb{1}\\{\\cdot\\}$ denotes the indicator function. The symbol $\\lesssim$ , employed   \n141 in the proof sketches, represents a coarse inequality.   \n142 We consider the sequential decision-making problem of Active Optimal Assortment (AOA),   \n143 with preference/choice feedback. Formally, the learner is given $[K]$ , a finite set of $K$ items   \n144 ( $K>2$ ). At each decision round $t=1,2,\\ldots$ , the learner selects a subset $S_{t}\\subseteq[K]$ of up to   \n145 ${\\boldsymbol{r}}n$ items, and receives some (stochastic) feedback about the item preferences of $S_{t}$ , drawn   \n146 according to some unknown underlying Plackett-Luce (PL) choice model (1) with parameters   \n147 $\\pmb{\\theta}=(\\theta_{1},\\theta_{2},\\ldots,\\theta_{K})\\in\\mathbb{R}_{+}^{K}$ . We assume $\\theta_{1}\\geq\\theta_{2}\\geq...\\geq\\theta_{K}$ without loss of generality. An   \n148 interested reader may check App. A.1 for a detailed discussion on PL models. Given any   \n149 assortment $S_{t}$ we also consider the possibility of \u2018no-selection\u2019 of any items given an $S_{t}$ .   \n150 Following the literature of [2], we model this mathematically as a No-Choice (NC) item,   \n151 indexed by item-0, and its corresponding PL utility parameter $\\theta_{0}$ . Unlike most existing   \n152 literature on assortment selection, we are not assuming $\\theta_{0}\\not\\geq\\operatorname*{max}_{i\\in[K]}\\theta_{i}$ . Further, since the   \n153 PL model is scale independent, we set $\\theta_{0}=1$ and scale the rest of the PL parameters.   \n154 Feedback model The feedback model formulates the information received (from the   \n155 \u2018environment\u2019) once the learner plays a subset $S_{t}\\subseteq[K]$ of at most $m$ items. Given $S_{t}$ we   \n156 consider the algorithm receives a winner feedback (or index of an item) $i_{t}\\in S_{t}\\cup\\{0\\}$ , drawn   \n157 according to the underlying PL choice model as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(i_{t}=i|S_{t})=\\theta_{i}/\\big(\\theta_{0}+\\sum_{j\\in S_{t}}\\theta_{j}\\big),\\ \\ \\forall i\\in S_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 We consider the following two objectives for the learner: ", "page_idx": 3}, {"type": "text", "text": "159 1. Top- ${\\boldsymbol{r}}n$ -Ojective. One simple objective could be to identify the top- ${\\boldsymbol{r}}n$ item-set:   \n160 $\\{\\theta_{1},\\ldots,\\theta_{m}\\}$ , for some $m\\,\\in\\,[1,K]$ . The performance of the learner can be captured by   \n161 minimizing the following regret: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR e g_{T}^{\\mathrm{top}}:=\\sum_{t=1}^{T}\\frac{\\Theta_{S^{*}}-\\Theta_{S_{t}}}{m},\\quad\\mathrm{where}\\quad S^{*}:=\\operatorname*{argmax}_{S\\subseteq[K]:|S|=m}\\left\\{\\Theta_{S}:=\\sum_{i\\in S}\\theta_{i}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "162 2. Wtd-Top- $m$ -Objective. Here, each item- $i$ is associated with a weight (for example   \n163 price) $r_{i}\\in\\mathbb{R}_{+}$ , and the goal is to identify the set of size at most $m$ with maximum weighted   \n164 utility. One could measure the regret of the learner as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR e g_{T}^{\\tt s t d}:=\\sum_{t=1}^{T}(\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta)),\\mathrm{~where~}\\,\\mathcal{R}(S,\\theta):=\\sum_{i\\in S}\\frac{r_{i}\\theta_{i}}{\\theta_{0}+\\sum_{j\\in S}\\theta_{j}},\\,\\,\\forall S\\subseteq[K],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "165 denotes $S^{*}:=\\operatorname{argmax}_{S\\subseteq[K]||S|\\leq m}{\\mathcal{R}}(S,\\pmb{\\theta})$ is the optimal utility-maximizing subset. This   \n166 objective corresponds to the standard objective in the MNL litterature [2]. ", "page_idx": 3}, {"type": "text", "text": "167 3 A Practical and Efifcient Algorithm for AOA with PL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "168 In this section, we introduce our first algorithm, which works for both objectives. ", "page_idx": 3}, {"type": "text", "text": "169 3.1 Algorithm Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "170 At each time $t$ , our algorithm (Alg. 1) maintains a pairwise preference matrix $\\widehat{\\mathbf{P}}_{t}\\in[0,1]^{n\\times n}$ ,   \n171 whose $(i,j)$ -th entry $\\widehat{p}_{i j,t}$ records the empirical probability of $i$ having beaten $j$ in a pairwise   \n172 duel, and a corresponding upper confidence bound $p_{i j,t}^{\\mathrm{ucb}}$ . Let $[\\bar{K}]:=[K]\\cup\\{0\\}$ . We define for   \n173 each pair $(i,j)\\in[\\tilde{K}]\\times[\\tilde{K}]$ , ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{i j,t}^{\\mathrm{ucb}}:=\\widehat{p}_{i j,t}+\\sqrt{\\frac{2\\widehat{p}_{i j,t}\\left(1-\\widehat{p}_{i j,t}\\right)x}{n_{i j,t}}}+\\frac{3x}{n_{i j,t}},\\qquad\\mathrm{where}\\quad\\widehat{p}_{i j,t}:=\\frac{w_{i j,t}}{n_{i j,t}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 where $\\begin{array}{r}{w_{i j,t}=\\sum_{s=1}^{t-1}\\mathbb{1}\\{i_{s}=i,j\\in S_{s}\\}}\\end{array}$ denotes the number of pairwise wins of item- $i$ over $j$   \n175 and $n_{i j,t}=w_{i j,t}+w_{j i,t}$ being the number of times $(i,j)$ has been compared. The above UCB   \n176 estimates $p_{i j,t}^{\\mathrm{ucb}}$ are further used to design UCB estimates of the PL parameters $\\theta_{i}$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{i,t}^{\\mathrm{ucb}}=p_{i0,t}^{\\mathrm{ucb}}/(1-p_{i0,t}^{\\mathrm{ucb}})_{+}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 The estimates $\\theta_{i,t}^{\\mathrm{ucb}}\\mathrm{s}$ are then used to select the set $S_{t}$ , that maximizes the underlying objective.   \n178 This optimization problem transforms into a static assortment optimization problem with   \n179 upper confidence bounds $\\theta_{i,t}^{\\tt u c b}$ as the parameters, and efficient solution methods for this case   \n180 are available (see e.g., [7, 21, 32]). ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 AOA for PL model with RB $\\mathbf{(AOA-RB_{PL})}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: input: $x>0$   \n2: init: $\\dot{K}\\gets K+1$ , $[\\tilde{K}]=[K]\\cup\\{0\\}$ , $\\mathbf{W}_{1}\\gets[0]_{\\tilde{K}\\times\\tilde{K}}$   \n3: for $t=1,2,3,\\ldots,T$ do   \n4: Set $\\mathbf{N}_{t}=\\mathbf{W}_{t}+\\mathbf{W}_{t}^{\\top}$ , and $\\begin{array}{r}{\\widehat{\\mathbf{P}}_{t}=\\frac{\\mathbf{W}_{t}}{\\mathbf{N}_{t}}}\\end{array}$ . Denote $\\mathbf{N}_{t}=[n_{i j,t}]_{\\tilde{K}\\times\\tilde{K}}$ and $\\widehat{\\mathbf{P}}_{t}=[\\widehat{p}_{i j,t}]_{\\tilde{K}\\times\\tilde{K}}$ .   \n5: Define for all $i$ , $\\textstyle p_{i i,t}^{\\mathrm{ucb}}=\\frac{1}{2}$ and for all $i,j\\in[\\Tilde{K}],i\\neq j$   \n6: $\\begin{array}{r l}&{\\theta_{i j,t}^{\\mathrm{ucb}}=\\widehat{p_{i j,t}}=\\widehat{p_{i j,t}}+\\bigg(\\frac{2\\widehat{p_{i j,t}}(1-\\widehat{p_{i j,t}})x}{n_{i j,t}}\\bigg)^{1/2}+\\frac{3x}{n_{i j,t}}}\\\\ &{\\theta_{i,t}^{\\mathrm{ucb}}:=p_{i0,t}^{\\mathrm{ucb}}/(1-p_{i0,t}^{\\mathrm{ucb}})_{+}}\\\\ &{S_{t}\\gets\\left\\{\\begin{array}{l l}{\\mathrm{Top-}m\\mathrm{~items~from~argsort(\\{\\theta_{1,t}^{\\mathrm{ucb}},~...~,\\theta_{R,t}^{\\mathrm{ucb}}\\})},}\\\\ {\\mathrm{for~Top-}m\\mathrm{~objective}}\\\\ {\\mathrm{argmax}_{S\\subseteq[K]||S|\\leq m}\\,\\mathcal{R}(S,\\theta_{t}^{\\mathrm{ucb}}),}\\\\ {\\mathrm{for~Wtd-Top-}m\\mathrm{~objective}}\\end{array}\\right.}\\end{array}$   \n7:   \n8: Play $S_{t}$   \n9: Receive the winner $i_{t}\\in[\\tilde{K}]$ (drawn as per (1))   \n10: Update: $\\mathbf{W}_{t+1}=[w_{i j,t+1}]_{\\tilde{K}\\times\\tilde{K}}$ s.t. $w_{i_{t}j,t+1}\\leftarrow w_{i_{t}j,t}+1\\ \\ \\forall j\\in S_{t}\\cup\\{0\\}$   \n11: end for ", "page_idx": 4}, {"type": "text", "text": "181 3.2 Analysis: Concentration Lemmas ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 We start the analysis by providing two technical lemmas, whose proofs are deferred to the   \n183 appendix and that provide confidence bounds for the $\\theta_{i}$ .   \n184 Lemma 1. Let $T\\geq1$ and $x>0$ . Then, with probability at least $1-3K T e^{-x}$ , for all $t\\in[T]$   \n185 and $i\\in[K]$ : $\\theta_{i}\\leq\\theta_{i,t}^{u c b}$ atleast one of the following two inequalities is satisfied ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nn_{i0,t}<69x(\\theta_{0}+\\theta_{i})\\;\\;\\;\\;\\;o r\\;\\;\\;\\;\\theta_{i,t}^{u c b}\\leq\\theta_{i}+4(\\theta_{0}+\\theta_{i})\\sqrt{\\frac{2\\theta_{0}\\theta_{i}x}{n_{i0,t}}}+\\frac{22x(\\theta_{0}+\\theta_{i})^{2}}{n_{i0,t}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 The above lemma depends on $n_{i0,t}$ the number of times items $i$ have been compared with   \n187 item 0 up to round $t$ . The latter is controlled using the following lemma:   \n188 Lemma 2. Let $T\\geq1$ and $x>0$ . Then, with probability at least $1-K T e^{-x}$ : simultaneously   \n189 for all $t\\in[T]$ and $i\\in[K]$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{i,t}<2x(\\theta_{0}+\\Theta_{S^{\\ast}})^{2}\\;\\;\\;o r\\;\\;n_{i0,t}\\geq\\frac{(\\theta_{0}+\\theta_{i})\\tau_{i,t}}{2(\\theta_{0}+\\Theta_{S^{\\ast}})}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "190 where $\\begin{array}{r}{\\tau_{i,t}=\\sum_{s=1}^{t-1}\\mathbb{1}\\{i\\in S_{s}\\}}\\end{array}$ denotes the number of rounds item i got selected before round $t$ . ", "page_idx": 4}, {"type": "text", "text": "192 We are now ready to provide the regret upper bound for Algorithm 1 with Top- ${\\boldsymbol{r}}n$ objective.   \n193 Theorem 3 (Top- $m$ Objective). Let $\\theta_{\\mathrm{max}}\\geq1$ . Consider any instance of $P L$ model on $K$   \n194 items with parameters $\\theta\\in[0,\\theta_{\\mathrm{max}}]^{K}$ , $\\theta_{0}=1$ . The regret of Alg. 1 with parameter $x=2\\log T$   \n195 is bounded as ", "page_idx": 5}, {"type": "equation", "text": "$$\nR e g_{T}^{\\mathit{t o p}}=O\\big(\\theta_{\\mathrm{max}}^{3/2}\\sqrt{K T\\log T}\\big)\\quad w h e n\\;T\\to\\infty\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "196 The above rate of $\\tilde{O}(K T)$ is optimal (up to log-factors), as a lower bound can be derived from   \n197 standard multi-armed bandits [5, 6]. We only state here a sketch of the proof of Theorem 3.   \n198 The detailed proof is deferred to the App. B. ", "page_idx": 5}, {"type": "text", "text": "199 Proof Sketch of Theorem 3. Let us define for any $S\\subseteq[K]$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Theta_{S}=\\sum_{i\\in S}\\theta_{i},\\quad\\mathrm{and}\\quad\\Theta_{S}^{\\mathrm{ucb}}:=\\sum_{i\\in S}\\theta_{i}^{\\mathrm{ucb}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "200 Let $\\boldsymbol{\\varepsilon}$ be the high-probability event such that both Lemma 1 and 2 holds true. Then, $\\mathbb{P}(\\mathcal{E})\\ge$   \n201 $1-4T K e^{-x}$ . Let us first assume that $\\boldsymbol{\\xi}$ holds true. Then, by Lemma $1$ , $\\Theta_{S^{*}}\\le\\Theta_{S^{*}}^{\\mathtt{u c b}}\\le\\Theta_{S_{t}}^{\\mathtt{u c b}}$ ,   \n202 which yields ", "page_idx": 5}, {"type": "equation", "text": "$$\nR e g_{T}^{\\mathrm{top}}=\\frac{1}{m}\\sum_{t=1}^{T}\\Theta_{S^{*}}-\\Theta_{S_{t}}\\le\\frac{1}{m}\\sum_{t=1}^{T}\\Theta_{S_{t}}^{\\mathrm{ucb}}-\\Theta_{S_{t}}\\lesssim\\tau_{0}+\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\bigl(\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i}\\bigr)\\mathbb{1}\\left\\{\\tau_{i,t}\\ge\\tau_{0}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "203 where $\\tau_{0}=138x(m+1)^{2}\\theta_{\\mathrm{max}}^{2}$ corresponds to an exploration phase needed for the confidence   \n204 upper bounds of Lem $1$ and 2 to be satisfied. Then, noting that if $\\boldsymbol{\\xi}$ holds true, we can show   \n205 by Lemma 2, that $\\mathbb{1}\\{\\tau_{i,t}\\ge\\tau_{0}\\}\\le\\mathbb{1}\\{n_{i0,t}\\ge69x(\\theta_{0}+\\theta_{i})\\}$ . Therefore, we can apply Lemma 1   \n206 that entails, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}(\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i})\\mathbb{I}\\big\\{\\tau_{i,t}\\geq\\bar{n}_{i0}\\big\\}\\lesssim\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\bigg((\\theta_{0}+\\theta_{i})\\sqrt{\\frac{\\theta_{0}\\theta_{i}x}{n_{i0,t}}}\\mathbb{I}\\big\\{\\tau_{i,t}\\geq\\tau_{0}\\big\\}\\bigg)}}\\\\ &{}&{\\stackrel{\\mathrm{Lem.~2}}{\\lesssim}\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\theta_{\\mathrm{max}}^{3/2}\\sqrt{\\frac{m x}{\\tau_{i,t}}}\\lesssim\\frac{1}{m}\\sum_{i=1}^{K}\\theta_{\\mathrm{max}}^{3/2}\\sqrt{m x\\tau_{i,t}}\\lesssim\\theta_{\\mathrm{max}}^{3/2}\\sqrt{x K T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 where we used $\\textstyle\\sum_{i=1}^{n}1/{\\sqrt{i}}\\leq2{\\sqrt{n}}$ and $\\textstyle\\sum_{i}\\tau_{i,t}=m T$ together with Jens\u221aen\u2019s inequality in the   \n208 last inequality.  We thus have under th e event $\\boldsymbol{\\xi}$ that $R{\\stackrel{\\smile}{e}}{}g_{T}^{\\mathrm{top}}\\leq O(\\theta_{\\mathrm{max}}^{3/2}\\sqrt{x K T})$ and the proof   \n209 is concluded by taking the expectation with $x=2\\log T$ to control $\\mathbb{P}(\\mathcal{E}^{c})$ . $\\sqsubset$ ", "page_idx": 5}, {"type": "text", "text": "210 3.4 Analysis: Wtd-Top- $m$ Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 We turn now to the analysis of the Wtd-Top- $m$ objective (2). We start by stating a lemma   \n212 from [2] that shows that the expected utility $\\mathcal{R}(S^{\\ast},\\theta)$ that corresponds to the optimal   \n213 assortment $\\begin{array}{r}{S^{*}=\\operatorname*{argmax}_{S\\subset[K],|S|\\leq m}\\mathcal{R}(S,\\theta)}\\end{array}$ is non-decreasing in the parameters $\\theta$ . ", "page_idx": 5}, {"type": "text", "text": "214 Lemma 4 (Lemma A.3 of [2]). Assume $\\theta_{i}^{u c b}\\geq\\theta_{i}$ for all $i\\in[K]$ , then $\\mathcal{R}(S^{\\ast},\\theta)\\leq\\mathcal{R}(S^{\\ast},\\theta^{u c b})$ . ", "page_idx": 5}, {"type": "text", "text": "215 Theorem 5 (Wtd-Top- ${\\boldsymbol{r}}n$ Objective). Let $\\theta_{\\mathrm{max}}\\geq1$ . Then, for any $\\theta\\:\\in\\:[0,\\theta_{\\mathrm{max}}]^{K}$ and   \n216 weights $\\mathbf{r}\\in[0,1]^{K}$ , the weighted regret of AOA- $R B_{P L}$ (Alg. 1) with $x=2\\log T$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nR e g_{T}^{w t d}=O(\\sqrt{\\theta_{\\mathrm{max}}K T}\\log T)\\qquad w h e n\\quad T\\to\\infty\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "217 The complete proof is postponed to App. B. The rate $\\Omega({\\sqrt{K T}})$ is optimal as proved by the   \n218 l\u221aower bound in [19] for MNL bandit problems for $\\theta_{\\mathrm{max}}=1$ . Our result recovers (up to a factor   \n219 $\\sqrt{\\log T})$ the one of [2] when $\\theta_{\\mathrm{max}}=1$ . However, their algorithm relies on more sophisticated   \n220 estimators that necessitate epochs repeating the same assortment until the No-Choice item   \n221 is selected. Note for our problem setting, where it is possible to have $\\theta_{\\mathrm{max}}\\gg\\theta_{0}=1$ , the   \n222 length of these epochs could be of $O(K\\theta_{\\mathrm{max}})$ , which could be potentially very large when   \n223 $\\theta_{\\mathrm{max}}\\gg1$ . This reduces the number of effective epochs, leading to poor estimation of the PL   \n224 parameters. We see this tradeoff in our experiments (Sec. 5) where the MNL-UCB algorithm   \n225 of [2] yields linear ${\\cal O}(T)$ regret for such choice of the problem parameters.   \n226 Remark 1 (Beyond MNL Models). Although, in this paper, we primarily focused on MNL   \n227 based choice models, it is worth mentioning that our proposed algorithms can be generalized   \n228 to more general random utility based models (RUMs) [9, 33] pursuing the ideas from [36]   \n229 that extends the RB based parameter estimation technique to any RUM(\u03b8) choice models.   \n230 Our algorithms and analyses thus apply to any general RUM(\u03b8) based choice models; we stick   \n231 to the special case of MNL models in this paper for brevity and keep the main focus on the   \n232 AOA problem and the related algorithmic novelties.   \n233 Proof sketch of Thm. 5. Let $\\boldsymbol{\\xi}$ be the high-probability event such that both Lemma 1 and 2   \n234 are satisfied. Then, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\mathrm{sta}}=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta)\\big]\\lesssim\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[(\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}\\}\\big]+T\\mathbb{P}(\\mathcal{E}^{c})}\\\\ &{\\lesssim\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[(\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{ucb}})-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}\\}\\big]+T\\mathbb{P}(\\mathcal{E}^{c})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "235 because $\\begin{array}{r}{\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{{ucb}}})\\,\\ge\\,\\mathcal{R}(S^{*},\\theta_{t}^{\\mathrm{{ucb}}})\\,\\ge\\,\\mathcal{R}(S^{*},\\theta)}\\end{array}$ under the event $\\boldsymbol{\\varepsilon}$ by Lemma 4. We now   \n236 upper-bound the first term of the right-hand-side ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\big(\\mathcal{R}\\big(S_{t},\\theta_{t}^{\\mathrm{ncb}}\\big)-\\mathcal{R}\\big(S_{t},\\theta\\big)\\big)\\Big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Bigg[\\bigg(\\sum_{i\\in S_{t}}\\frac{r_{i}\\theta_{i,t}^{\\mathrm{ncb}}}{\\theta_{0}+\\Theta_{S_{t},t}^{\\mathrm{ncb}}}-\\frac{r_{i}\\theta_{i}}{\\theta_{0}+\\Theta_{S_{t}}}\\bigg)\\mathbb{1}\\{\\mathcal{E}\\}\\Bigg]}\\\\ &{\\displaystyle\\qquad\\le\\sum_{t=1}^{T}\\mathbb{E}\\Bigg[\\bigg(\\sum_{i\\in S_{t}}\\frac{r_{i}\\big(\\theta_{i,t}^{\\mathrm{ncb}}-\\theta_{i}\\big)}{\\theta_{0}+\\Theta_{S_{t}}}\\bigg)\\mathbb{1}\\{\\mathcal{E}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "237 Because $\\Theta_{S_{t},t}^{\\tt u c b}\\ge\\Theta_{S_{t}}$ under the event $\\boldsymbol{\\xi}$ by Lemma 1. Then, using $r_{i}\\leq1$ , we further upper  \n238 bound using an exploration parameter $\\tau_{0}=O(\\log(T))$ so that the upper-confidence-bounds   \n239 in Lemmas $^{1}$ and 2 are satisfied ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\Big(\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{ucb}})-\\mathcal{R}(S_{t},\\theta)\\Big)\\Big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]\\le\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{\\vert\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i}\\vert}{\\theta_{0}+\\Theta_{S_{t}}}\\right)\\mathbb{1}\\{i\\in S_{t},\\mathcal{E}\\}\\right]}\\\\ &{\\lesssim O(\\tau_{0})+\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\vert\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i}\\vert}{\\theta_{0}+\\Theta_{S_{t}}}\\mathbb{1}\\{i\\in S_{t},\\tau_{i,t}\\ge\\tau_{0},\\mathcal{E}\\}\\right]}\\\\ &{\\lesssim O(\\tau_{0})+\\displaystyle\\sum_{i=1}^{K}\\sqrt{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\theta_{i}\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\right]}\\times\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left(\\frac{\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i}}{\\theta_{0}+\\Theta_{S_{t}}}\\right)^{2}\\frac{\\theta_{0}+\\Theta_{S_{t}}}{\\theta_{i}}\\mathbb{1}\\{i\\in S_{t},\\tau_{i,t}\\ge\\tau_{0},\\mathcal{E}\\}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "240 where the last inequality is by Cauchy-Schwarz inequality. Now, the term $A_{T}(i)$ above may   \n241 be upper-bounded using Lemmas 1 and 2, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{T}(i)=\\mathbb{E}\\Bigg[\\frac{(\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i})^{2}}{\\theta_{i}(\\theta_{0}+\\Theta_{S_{t}})}\\mathbb{1}\\{i\\in S_{t},\\tau_{i,t}\\geq\\tau_{0},\\mathcal{E}\\}\\Bigg]\\lesssim\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}\\Bigg[\\frac{(\\theta_{0}+\\theta_{i})^{2}x}{n_{i0,t}(\\theta_{0}+\\Theta_{S_{t}})}\\mathbb{1}\\{i\\in S_{t}\\}\\Bigg]}\\\\ &{\\lesssim\\theta_{\\operatorname*{max}}x\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}\\Bigg[\\frac{(\\theta_{0}+\\theta_{i})\\mathbb{1}\\{i\\in S_{t}\\}}{(\\theta_{0}+\\Theta_{S_{t}})n_{i0,t}}\\Bigg]=\\theta_{\\operatorname*{max}}x\\mathbb{E}\\Bigg[\\underset{t=1}{\\overset{T}{\\sum}}\\frac{\\mathbb{1}\\{i_{t}\\in\\{i,0\\},i\\in S_{t}\\}}{n_{i0,t}}\\Bigg]\\lesssim\\theta_{\\operatorname*{max}}x\\log T}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "242 where in the last inequality we used that $\\begin{array}{r}{\\sum_{n=1}^{T}n^{-1}\\,\\le\\,1+\\log T}\\end{array}$ . Substituting into (6),   \n243 Jensen\u2019s inequality entails, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\big(\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{{ncb}}})-\\mathcal{R}(S_{t},\\theta)\\big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]\\lesssim O(\\tau_{0})+\\mathbb{E}\\Bigg[\\sqrt{\\theta_{\\operatorname*{max}}x\\log T}\\sum_{i=1}^{K}\\sqrt{\\sum_{t=1}^{T}\\frac{\\theta_{i}\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}}\\Bigg]\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "244 The proof is finally concluded by applying Cauchy-Schwarz inequality which yields: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\sqrt{\\sum_{t=1}^{T}\\frac{\\theta_{i}\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}}\\le\\sqrt{K\\sum_{t=1}^{T}\\frac{\\sum_{i=1}^{K}\\theta_{i}\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}}\\le\\sqrt{K T}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "245 Finally, combining the above result with (5) and (7) concludes the proof ", "page_idx": 7}, {"type": "equation", "text": "$$\nR e g_{T}^{\\mathsf{s t d}}\\lesssim T P(\\mathcal{E}^{c})+O(\\tau_{0})+\\sqrt{\\theta_{\\mathrm{max}}x K T\\log T}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "246 Choosing $x=2\\log T$ ensures $T P({\\mathcal{E}}^{c})\\leq O(1)$ and $\\tau_{0}\\le{\\cal O}(\\log T)$ . ", "page_idx": 7}, {"type": "text", "text": "247 4 Improved dependance on $\\theta_{\\mathrm{max}}$ with Adaptive Pivot Selection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "248 A problem with Algorithm 1 stems from estimating all $\\theta_{i}$ based on pairwise comparisons with   \n249 item 0. When $\\theta_{\\mathrm{max}}\\gg\\theta_{0}=1$ , item 0 may not be sampled enough as the winner, leading to   \n250 poor estimators. This deficiency contributes to the suboptimal dependence on $\\theta_{\\mathrm{max}}$ observed   \n225512 itnh eT phievoorte. mFso 3r  aalnl $i,j\\in[K]\\cup\\{0\\}$ wwoer kd,e sfiunceh $\\begin{array}{r}{\\gamma_{i j}=\\frac{\\theta_{i}}{\\theta_{j}}}\\end{array}$ ,W ae npdr otphoes ee sttihem afotlolrosw:ing fix to optimize ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\gamma_{i j,t}^{\\mathrm{ucb}}=p_{i j,t}^{\\mathrm{ucb}}/(1-p_{i j,t}^{\\mathrm{ucb}})_{+}\\quad\\quad\\mathrm{and}\\quad\\quad\\gamma_{i i,t}^{\\mathrm{ucb}}=1\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "253 where $p_{i j,t}^{\\tt u c b}$ are defined in (3). For all rounds $t$ , the algorithm AOA-RB -Adaptive selects $\\mathrm{PL}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nS_{t}=\\underset{|S|\\leq m}{\\operatorname{argmax}}\\,\\mathcal{R}\\big(S,\\widehat{\\theta}_{t}^{\\mathrm{ucb}}\\big)\\qquad\\mathrm{where}\\qquad\\widehat{\\theta}_{i,t}^{\\mathrm{ucb}}:=\\underset{j\\in[K]\\cup\\{0\\}}{\\operatorname*{min}}\\,\\gamma_{i j,t}^{\\mathrm{ucb}}\\gamma_{j0,t}^{\\mathrm{ucb}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "254 We offer below a regret bound that underscores the value of optimizing the pivot when   \n255 $\\theta_{\\mathrm{max}}\\gg K$ . Note that while the algorithm and analysis are presented for the weighted   \n256 objective with winner feedback only, it can be adapted to other objectives by replacing   \n257 $\\mathcal{R}(S,\\theta)$ with the new objective in the analysis, as long as Lemma 4 remains valid.   \n258 Theorem 6. Let $\\theta_{\\mathrm{max}}\\geq1$ . For any $\\theta\\in[0,\\theta_{\\mathrm{max}}]^{K}$ and weights $\\mathbf{r}\\in[0,1]^{K}$ , the weighted   \n259 regret of $A O A{-}R B_{P L}$ -Adaptive is upper-bounded as ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nR e g_{T}^{w t d}=O\\big(\\sqrt{\\operatorname*{min}\\{\\theta_{\\operatorname*{max}},K\\}K T}\\log T\\big)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "260 as $T\\rightarrow\\infty$ for the choice $x=2\\log T$ (when definining $p_{i j,t}^{u c b}$ ). ", "page_idx": 7}, {"type": "text", "text": "261 Asymptotically, when $\\theta_{\\mathrm{max}}$ is constant, the regret is $O(K{\\sqrt{T}}\\log T)$ , eliminating any depen  \n262 dence on $\\theta_{\\mathrm{max}}$ . This allows for handling scenarios where the No-Choice item is highly unlikely,   \n263 which is not achievable in previous works such as [2, 1]. [2] did attempt in their Thm\u221a. 4 to   \n264 relax the assumption of $\\theta_{\\mathrm{{max}}}=\\theta_{0}$ and shows a bound of order $O\\big(\\operatorname*{max}\\{\\theta_{\\operatorname*{max}}/\\theta_{0},1\\}^{1/2}\\sqrt{K T}\\big)$ ,   \n265 which unfortunately blows to $\\infty$ as $\\theta_{0}\\to0$ or equivalently $\\theta_{\\mathrm{max}}\\rightarrow\\infty$ , leading to a vac  \n266 uous bound. Here, lies the stark improvement and one of the key contributions, as also   \n267 corroborated in our experimental evaluation Sec. 5 (Fig. 2). ", "page_idx": 7}, {"type": "text", "text": "222667890 $\\boldsymbol{j}_{t}=\\underset{\\boldsymbol{\\mathbf{\\rho}}}{\\mathrm{argmax}}_{\\boldsymbol{j}\\in S_{t}\\cup\\{0\\}}\\,\\boldsymbol{\\theta}_{j}$ .vedT hdee puesned eonfc $|\\hat{\\theta}_{i,t}^{\\mathrm{{acb}}}-\\theta_{i}|\\leq|\\hat{\\gamma}_{i j_{t},t}^{\\mathrm{{ucb}}}-\\theta_{i}|$ t hper ofvaicdte st hcaotn .u pDpeure$\\theta_{\\mathrm{max}}$ $\\theta_{j_{t}}~\\geq\\theta_{i}$ 271 to the varying pivot over time, a telescoping argument introduces an additive factor $\\sqrt{K}$ . ", "page_idx": 7}, {"type": "text", "text": "272 5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "273 We provide here a synthetic experiments. All results are averaged across 100 runs. We   \n274 evaluate the performance of our main algorithm AOA-RB $\\mathrm{PL}$ -Adaptive (Sec. 4), referred   \n275 as \u201cOur Alg-1 (Adaptive Pivot)\", with the following two algorithms: AOA-RBPL (Sec. 3)   \n276 referred as \u201cOur Alg-2 (No-Choice Pivot)\", and MNL-UCB, the state-of-the-art algorithm   \n277 for AOA ([2], Alg. 1).   \n278 Different PL (\u03b8) Environments. We report our experiment results on two datasets with   \n279 $K=50$ items: (1) Arith50 with PL parameters $\\theta_{i}=1-(i-1)0.2$ , \u2200i \u2208[50]. (2) Bad50   \n280 with PL parameters $\\theta_{i}=0.6$ , $\\forall i\\in[50]\\setminus\\{25\\}$ and $\\theta_{25}=0.8$ . For simplicity of computing   \n281 the assortment choices $S_{t}$ , we assume $r_{i}=1,\\;\\forall i\\in[K]$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "282 (1). Averaged Regret with weak NC ( $\\theta_{\\mathrm{max}}/\\theta_{0}\\gg1\\$ ) (Fig. 1): In our first experiment, 283 we set set $m\\,=\\,5$ and $\\theta_{0}/\\theta_{\\mathrm{max}}\\,=\\,0.01$ and report the average regret of the above three algorithms for our two objectives. ", "page_idx": 8}, {"type": "image", "img_path": "7VNvM9SnRE/tmp/6ce67eaabd51446874b04d4f2ea32b5e7618bfea008c435f506fc20e71d75f4a.jpg", "img_caption": ["Figure 1: Averaged Regret for $m=5$ , $\\theta_{0}=0.01$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "285 Fig. 1 shows that our algorithm AOA- $\\cdot\\mathrm{RB}_{\\mathrm{PL}}$ -Adaptive (with adaptive pivot) significantly   \n286 outperforms the other two algorithms, while our algorithm AOA- $\\cdot\\mathrm{RB}_{\\mathrm{PL}}$ with no-choice (NC)   \n287 pivot still outperforms MNL-UCB.   \n288 (2). Averaged Regret vs No-Choice PL Parameter $\\left(\\theta_{\\operatorname*{max}}/\\theta_{0}\\right)$ (Fig. 2): In this   \n289 experiment, we evaluate the regret performance of our algorithm AOA-RBPL-Adaptive. We   \n290 report the experiment on Artith50 PL dataset and set the subsetsize $m\\,=\\,5$ , $\\theta_{\\mathrm{max}}/\\theta_{0}=$   \n291 $\\{1,0.5,0.1,0.05,0.01,0.005,0.001\\}$ . Fig. 2 shows the increase in the performance gap between   \n292 our algorithm AOA-RBPL-Adaptive (with adaptive pivot) with decreasing $\\theta_{0}/\\theta_{\\mathrm{max}}$ .   \n294 (3). Averaged Regret vs Length of the rank-ordered feedback $(k)$ (Fig. 3): We   \n295 also run a thought experiment to understand the tradeoff between learning rate with $k$ -length   \n296 rank-ordered feedback, where given any assortment $S_{t}\\subseteq[K]$ of size ${\\boldsymbol{r}}n$ , the learner gets to   \n297 see the top- $k$ draws $(k\\leq m)$ from the PL model without replacement. This is a stronger   \n298 feedback than the winner (i.e. top-1 for $k=1$ ) feedback and, as expected, we see in Fig. 3   \n299 an improved regret (for both notions) when increasing $k$ . The experiment are run on the   \n300 Artith50 dataset with $m=30$ and $k\\in\\{1,2,4,8\\}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "7VNvM9SnRE/tmp/5d87e8eba9310886bbf5a9b771f2d8d1098f4ed8a9791f9503cafe96920209c3.jpg", "img_caption": ["Figure 2: Comparative performance for varying $\\theta_{0}/\\theta_{\\mathrm{max}}$ , $m=5$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7VNvM9SnRE/tmp/0ef8f9c3a241915c017d383f0a29323113e2250f2bfc17fabcf762e10c701fb1.jpg", "img_caption": ["Figure 3: Tradofff: Averaged Regret vs length of the $k$ rank-ordered feedback "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "301 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "302 We address the Active Optimal Assortment Selection problem with PL choice models, in  \n303 troducing a versatile framework $(A O A)$ that eliminates the need for a strong default item,   \n304 typically assumed as the No-Choice (NC) item in the existing literature. Our proposed   \n305 algorithms employ a novel \u2019Rank-Breaking\u2019 technique to establish tight concentration guar  \n306 antees for estimating the score parameters of the PL model. Our approach stands out for   \n307 its practicality and avoids the suboptimal practice of repeatedly selecting the same set of   \n308 items until the default item prevails. This is beneficial when the default item\u2019s quality   \n309 $\\left(\\theta_{0}\\right)$ is significantly lower than the quality of the best item ( $\\theta_{\\mathrm{max}}$ ). Our algorithms are   \n310 computationally efficient, optimal (up to log factors), and free from restrictive assumptions   \n311 on the default item.   \n312 Future Works. Among many interesting questions to address in the future, it will be   \n313 interesting to understand the role of the No-Choice (NC) item in the algorithm design,   \n314 precisely, can we design efficient algorithms without the existence of NC items with a regret   \n315 rate still linear in $\\theta_{\\mathrm{max}}$ ? Further, it will be interesting to extend our results to more general   \n316 choice models beyond the PL model [18, 22, 23]. What is the tradeoff between the subsetsize   \n317 $m$ and the regret for such general choice models? Extending our results to large (potentially   \n318 infinite) decision spaces and contextual settings would also be a very useful and practical   \n319 contribution to the literature of assortment optimization. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "320 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "321 [1] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for   \n322 the mnl-bandit. In Conference on learning theory, pages 76\u201378. PMLR, 2017.   \n323 [2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic   \n324 learning approach to assortment selection. Operations Research, 67(5):1453\u20131485, 2019.   \n325 [3] Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits.   \n326 In International Conference on Machine Learning, pages 856\u2013864. PMLR, 2014.   \n327 [4] Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Exploration\u2013exploitation tradeoff   \n328 using variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876\u2013   \n329 1902, 2009.   \n330 [5] Peter Auer. Using upper confidence bounds for online learning. In Foundations of Computer   \n331 Science, 2000. Proceedings. 41st Annual Symposium on, pages 270\u2013279. IEEE, 2000.   \n332 [6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed   \n333 bandit problem. Machine learning, 47(2-3):235\u2013256, 2002.   \n334 [7] Vashist Avadhanula, Jalaj Bhandari, Vineet Goyal, and Assaf Zeevi. On the tightness of   \n335 an lp relaxation for rational optimization and its applications. Operations Research Letters,   \n336 44(5):612\u2013617, 2016.   \n337 [8] Hossein Azari, David Parkes, and Lirong Xia. Random utility theory for social choice. In   \n338 Advances in Neural Information Processing Systems, pages 126\u2013134, 2012.   \n339 [9] Hossein Azari, David Parks, and Lirong Xia. Random utility theory for social choice. Advances   \n340 in Neural Information Processing Systems, 25, 2012.   \n341 [10] Viktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preference  \n342 based online learning with dueling bandits: A survey. Journal of Machine Learning Research,   \n343 2021.   \n344 [11] Viktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preference  \n345 based online learning with dueling bandits: A survey. J. Mach. Learn. Res., 22:7\u20131, 2021.   \n346 [12] Viktor Bengs, Aadirupa Saha, and Eyke H\u00fcllermeier. Stochastic contextual dueling bandits   \n347 under linear stochastic transitivity models. In International Conference on Machine Learning,   \n348 pages 1764\u20131786. PMLR, 2022.   \n349 [13] Gerardo Berbeglia and Gwena\u00ebl Joret. Assortment optimisation under a general discrete choice   \n350 model: A tight analysis of revenue-ordered assortments. arXiv preprint arXiv:1606.01371, 2016.   \n351 [14] Brian Brost, Yevgeny Seldin, Ingemar J. Cox, and Christina Lioma. Multi-dueling bandits and   \n352 their application to online ranker evaluation. CoRR, abs/1608.06253, 2016.   \n353 [15] Niladri S Chatterji, Aldo Pacchiano, Peter L Bartlett, and Michael I Jordan. On the theory of   \n354 reinforcement learning with once-per-episode feedback. arXiv preprint arXiv:2105.14363, 2021.   \n355 [16] Xi Chen, Sivakanth Gopi, Jieming Mao, and Jon Schneider. Competitive analysis of the top-k   \n356 ranking problem. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on   \n357 Discrete Algorithms, pages 1245\u20131264. SIAM, 2017.   \n358 [17] Xi Chen, Yuanzhi Li, and Jieming Mao. A nearly instance optimal algorithm for top-k ranking   \n359 under the multinomial logit model. In Proceedings of the Twenty-Ninth Annual ACM-SIAM   \n360 Symposium on Discrete Algorithms, pages 2504\u20132522. SIAM, 2018.   \n361 [18] Xi Chen, Chao Shi, Yining Wang, and Yuan Zhou. Dynamic assortment planning under nested   \n362 logit models. Production and Operations Management, 30(1):85\u2013102, 2021.   \n363 [19] Xi Chen and Yining Wang. A note on a tight lower bound for mnl-bandit assortment selection   \n364 models. arXiv preprint arXiv:1709.06109, 2017.   \n365 [20] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep   \n366 reinforcement learning from human preferences. Advances in neural information processing   \n367 systems, 30, 2017.   \n368 [21] James Davis, Guillermo Gallego, and Huseyin Topaloglu. Assortment planning under the   \n369 multinomial logit model with totally unimodular constraint structures. Work in Progress, 2013.   \n370 [22] Antoine D\u00e9sir, Vineet Goyal, Srikanth Jagabathula, and Danny Segev. Assortment optimization   \n371 under the mallows model. In Advances in Neural Information Processing Systems, pages   \n372 4700\u20134708, 2016.   \n373 [23] Antoine D\u00e9sir, Vineet Goyal, Danny Segev, and Chun Ye. Capacity constrained assortment   \n374 optimization under the markov chain based choice model. Operations Research, 2016.   \n375 [24] James A Grant and David S Leslie. Learning to rank under multinomial logit choice. Journal   \n376 of Machine Learning Research, 24(260):1\u201349, 2023.   \n377 [25] Minje Jang, Sunghyun Kim, Changho Suh, and Sewoong Oh. Optimal sample complexity of   \n378 m-wise data for top-k ranking. In Advances in Neural Information Processing Systems, pages   \n379 1685\u20131695, 2017.   \n380 [26] Ashish Khetan and Sewoong Oh. Data-driven rank breaking for efifcient rank aggregation.   \n381 Journal of Machine Learning Research, 17(193):1\u201354, 2016.   \n382 [27] Kameng Nip, Zhenbo Wang, and Zizhuo Wang. Assortment optimization under a single   \n383 transition model. 2017.   \n384 [28] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits.   \n385 Advances in Neural Information Processing Systems, 32, 2019.   \n386 [29] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. Multinomial logit bandit with linear   \n387 utility functions. arXiv preprint arXiv:1805.02971, 2018.   \n388 [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,   \n389 Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to   \n390 follow instructions with human feedback. Advances in Neural Information Processing Systems,   \n391 35:27730\u201327744, 2022.   \n392 [31] Wenbo Ren, Jia Liu, and Ness B Shrof.f PAC ranking from pairwise and listwise queries: Lower   \n393 bounds and upper bounds. arXiv preprint arXiv:1806.02970, 2018.   \n394 [32] Paat Rusmevichientong, Zuo-Jun Max Shen, and David B Shmoys. Dynamic assortment   \n395 optimization with a multinomial logit choice model and capacity constraint. Operations   \n396 research, 58(6):1666\u20131680, 2010.   \n397 [33] Aadirupa Saha and Suprovat Ghoshal. Exploiting correlation to achieve faster learning rates in   \n398 low-rank preference bandits. In International Conference on Artificial Intelligence and Statistics,   \n399 pages 456\u2013482. PMLR, 2022.   \n400 [34] Aadirupa Saha and Aditya Gopalan. Active ranking with subset-wise preferences. International   \n401 Conference on Artificial Intelligence and Statistics (AISTATS), 2018.   \n402 [35] Aadirupa Saha and Aditya Gopalan. Combinatorial bandits with relative feedback. In Advances   \n403 in Neural Information Processing Systems, 2019.   \n404 [36] Aadirupa Saha and Aditya Gopalan. PAC Battling Bandits in the Plackett-Luce Model. In   \n405 Algorithmic Learning Theory, pages 700\u2013737, 2019.   \n406 [37] Aadirupa Saha and Aditya Gopalan. Best-item learning in random utility models with subset   \n407 choices. In International Conference on Artificial Intelligence and Statistics, pages 4281\u20134291.   \n408 PMLR, 2020.   \n409 [38] Hossein Azari Soufiani, David C Parkes, and Lirong Xia. Computing parametric ranking models   \n410 via rank-breaking. In ICML, pages 360\u2013368, 2014.   \n411 [39] Yanan Sui, Vincent Zhuang, Joel Burdick, and Yisong Yue. Multi-dueling bandits with   \n412 dependent arms. In Conference on Uncertainty in Artificial Intelligence, UAI\u201917, 2017.   \n413 [40] Kalyan Talluri and Garrett Van Ryzin. Revenue management under a general discrete choice   \n414 model of consumer behavior. Management Science, 50(1):15\u201333, 2004.   \n415 [41] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The $k$ -armed dueling   \n416 bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012.   \n417 [42] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret   \n418 and constant computation cost. Advances in Neural Information Processing Systems, 36, 2024.   \n419 [43] Zihan Zhang and Xiangyang Ji. Regret minimization for reinforcement learning by evaluating   \n420 the optimal bias function. In Advances in Neural Information Processing Systems, pages   \n421 2827\u20132836, 2019.   \n422 [44] Masrour Zoghi, Zohar S Karnin, Shimon Whiteson, and Maarten De Rijke. Copeland dueling   \n423 bandits. In Advances in Neural Information Processing Systems, pages 307\u2013315, 2015.   \n424 [45] Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke, et al. Relative upper   \n425 confidence bound for the $k$ -armed dueling bandit problem. In JMLR Workshop and Conference   \n426 Proceedings, number 32, pages 10\u201318. JMLR, 2014.   \n427 [46] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative confidence   \n428 sampling for efifcient on-line ranker evaluation. In Proceedings of the 7th ACM international   \n429 conference on Web search and data mining, pages 73\u201382. ACM, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "430 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "32 Question: Do the main claims made in the abstract and introduction accurately   \n33 reflect the paper\u2019s contributions and scope?   \n35 Justification: In the abstract, we list the main claims of this paper in a general   \n36 fashion. Then, in the introduction we state them in more detail. They accurately   \n37 reflect the paper\u2019s contribution and scope. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "448 2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: We discuss the limitations and assumptions of our work throughout the paper. Additional limitations are highlighted in the discussion. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "Justification: The assumptions can be found in the Problem Setup section and in the paragraphs before the theorems and remarks. We provide proof sketches in the main text and complete proofs in the appendix. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "502 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: experimental details including algorithms and setups are clearly provided. In addition, the main contribution of the paper is theoretical and synthetic experiments are mostly provided as an illustration. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. ", "page_idx": 13}, {"type": "text", "text": "535 (c) If the contribution is a new model (e.g., a large language model), then there   \n536 should either be a way to access this model for reproducing the results or a   \n537 way to reproduce the model (e.g., with an open-source dataset or instructions   \n538 for how to construct the dataset).   \n539 (d) We recognize that reproducibility may be tricky in some cases, in which   \n540 case authors are welcome to describe the particular way they provide for   \n541 reproducibility. In the case of closed-source models, it may be that access to   \n542 the model is limited in some way (e.g., to registered users), but it should be   \n543 possible for other researchers to have some path to reproducing or verifying   \n544 the results. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "545 5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "546 Question: Does the paper provide open access to the data and code, with sufficient   \n547 instructions to faithfully reproduce the main experimental results, as described in   \n548 supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 14}, {"type": "text", "text": "Justification: experimental setups are synthetic and can easily be reproduced. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "573 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "574 Question: Does the paper specify all the training and test details (e.g., data splits,   \n575 hyperparameters, how they were chosen, type of optimizer, etc.) necessary to   \n576 understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "578 Justification: All details are provided to reproduce the experiments.   \n579 Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "585 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "588 Answer: [Yes]   \n589 Justification: [Yes]   \n590 Guidelines:   \n91 \u2022 The answer NA means that the paper does not include experiments.   \n92 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars,   \n93 confidence intervals, or statistical significance tests, at least for the experiments   \n94 that support the main claims of the paper.   \n95 \u2022 The factors of variability that the error bars are capturing should be clearly   \n96 stated (for example, train/test split, initialization, random drawing of some   \n597 parameter, or overall run with given experimental conditions).   \n98 The method for calculating the error bars should be explained (closed form   \n99 formula, call to a library function, bootstrap, etc.)   \n00 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n01 \u2022 It should be clear whether the error bar is the standard deviation or the standard   \n02 error of the mean.   \n03 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors   \n04 should preferably report a 2-sigma error bar than state that they have a $96\\%$   \n05 CI, if the hypothesis of Normality of errors is not verified.   \n606 \u2022 For asymmetric distributions, the authors should be careful not to show in   \n07 tables or figures symmetric error bars that would yield results that are out of   \n608 range (e.g. negative error rates).   \n09 \u2022 If error bars are reported in tables or plots, The authors should explain in the   \n610 text how they were calculated and reference the corresponding figures or tables   \n11 in the text.   \n12 8. Experiments Compute Resources   \n13 Question: For each experiment, does the paper provide sufficient information on the   \n614 computer resources (type of compute workers, memory, time of execution) needed   \nto reproduce the experiments?   \n16 Answer: [NA]   \n617 Justification: [NA]   \n618 Guidelines:   \n619 \u2022 The answer NA means that the paper does not include experiments.   \n20 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal   \n21 cluster, or cloud provider, including relevant memory and storage.   \n622 \u2022 The paper should provide the amount of compute required for each of the   \n623 individual experimental runs as well as estimate the total compute.   \n24 \u2022 The paper should disclose whether the full research project required more   \n25 compute than the experiments reported in the paper (e.g., preliminary or failed   \n626 experiments that didn\u2019t make it into the paper).   \n627 9. Code Of Ethics   \n628 Question: Does the research conducted in the paper conform, in every respect, with   \n629 the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n630 Answer: [Yes]   \n631 Justification: We do not see any potential negative social impact of this work and it   \n632 follows the NeurIPS code of ethics.   \n633 Guidelines:   \n34 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code   \n635 of Ethics.   \n36 \u2022 If the authors answer No, they should explain the special circumstances that   \n37 require a deviation from the Code of Ethics. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Justification: This work addresses the problem of designing efficient and optimal algorithms for different assortment selection problems with MNL models. Our work is purely theoretical and studies a fundamental mathematical optimization framework that is unrelated to societal considerations ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678   \n679   \n680   \n681   \n682   \n683   \n684   \n685   \n686   \n687   \n688   \n689   \n690   \n691   \n692   \n693 ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "694 Answer: [NA]   \n695 Justification: [NA]   \n696 Guidelines:   \n697 \u2022 The answer NA means that the paper does not use existing assets.   \n698 \u2022 The authors should cite the original paper that produced the code package or   \n699 dataset.   \n700 \u2022 The authors should state which version of the asset is used and, if possible,   \n701 include a URL.   \n702 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n703 \u2022 For scraped data from a particular source (e.g., website), the copyright and   \n704 terms of service of that source should be provided.   \n705 \u2022 If assets are released, the license, copyright information, and terms of use in   \n706 the package should be provided. For popular datasets, paperswithcode.com/   \n707 datasets has curated licenses for some datasets. Their licensing guide can help   \n708 determine the license of a dataset.   \n709 \u2022 For existing datasets that are re-packaged, both the original license and the   \n710 license of the derived asset (if it has changed) should be provided.   \n711 \u2022 If this information is not available online, the authors are encouraged to reach   \n712 out to the asset\u2019s creators.   \n713 13. New Assets   \n714 Question: Are new assets introduced in the paper well documented and is the   \n715 documentation provided alongside the assets?   \n716 Answer: [NA]   \n717 Justification: [NA]   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper does not release new assets.   \n720 \u2022 Researchers should communicate the details of the dataset/code/model as part   \n721 of their submissions via structured templates. This includes details about   \n722 training, license, limitations, etc.   \n723 \u2022 The paper should discuss whether and how consent was obtained from people   \n724 whose asset is used.   \n725 \u2022 At submission time, remember to anonymize your assets (if applicable). You   \n726 can either create an anonymized URL or include an anonymized zip file.   \n727 14. Crowdsourcing and Research with Human Subjects   \n728 Question: For crowdsourcing experiments and research with human subjects, does   \n729 the paper include the full text of instructions given to participants and screenshots,   \n730 if applicable, as well as details about compensation (if any)?   \n731 Answer: [NA]   \n732 Justification: [NA]   \n733 Guidelines:   \n734 \u2022 The answer NA means that the paper does not involve crowdsourcing nor   \n735 research with human subjects.   \n736 \u2022 Including this information in the supplemental material is fine, but if the main   \n737 contribution of the paper involves human subjects, then as much detail as   \n738 possible should be included in the main paper.   \n739 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection,   \n740 curation, or other labor should be paid at least the minimum wage in the   \n741 country of the data collector.   \n742 15. Institutional Review Board (IRB) Approvals or Equivalent for Research   \n743 with Human Subjects   \n744 Question: Does the paper describe potential risks incurred by study participants,   \n745 whether such risks were disclosed to the subjects, and whether Institutional Review   \n746 Board (IRB) approvals (or an equivalent approval/review based on the requirements   \n747 of your country or institution) were obtained?   \n748 Answer: [NA]   \n749 Justification: [NA]   \n750 Guidelines:   \n751 \u2022 The answer NA means that the paper does not involve crowdsourcing nor   \n752 research with human subjects.   \n753 \u2022 Depending on the country in which research is conducted, IRB approval (or   \n754 equivalent) may be required for any human subjects research. If you obtained   \n755 IRB approval, you should clearly state this in the paper.   \n756 \u2022 We recognize that the procedures for this may vary significantly between insti  \n757 tutions and locations, and we expect authors to adhere to the NeurIPS Code of   \n758 Ethics and the guidelines for their institution.   \n759 \u2022 For initial submissions, do not include any information that would break   \n760 anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Supplementary: Optimal, Efifcient and Practical Algorithms for Assortment Optimization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "763 A Preliminaries: Some Useful Concepts for PL choice models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "764 A.1 Plackett-Luce (PL): A Discrete Choice Model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "765 A discrete choice model specifies the relative preferences of two or more discrete alternatives   \n766 in a given set. A widely studied class of discrete choice models is the class of Random   \n767 Utility Models (RUMs), which assume a ground-truth utility score $\\theta_{i}\\in\\mathbb{R}$ for each alternative   \n768 $i\\in[n]$ , and assign a conditional distribution $\\mathcal{D}_{i}(\\cdot|\\theta_{i})$ for scoring item $i$ . To model a winning   \n769 alternative given any set $S\\subseteq[n]$ , one first draws a random utility score $X_{i}\\sim\\mathcal{D}_{i}(\\cdot|\\theta_{i})$ for   \n770 each alternative in $S$ , and selects an item with the highest random score. ", "page_idx": 19}, {"type": "text", "text": "One widely used RUM is the Multinomial-Logit (MNL) or Plackett-Luce model $(P L)$ , where the $D_{i}$ s are taken to be independent Gumbel distributions with parameters $\\theta_{i}^{\\prime}$ [8], i.e., with probability densities ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{D}_{i}(x_{i}|\\theta_{i}^{\\prime})=e^{-(x_{j}-\\theta_{j}^{\\prime})}e^{-e^{-(x_{j}-\\theta_{j}^{\\prime})}},\\qquad\\theta_{i}^{\\prime}\\in R,~\\forall i\\in\\left[n\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "771 Moreover assuming $\\theta_{i}^{\\prime}=\\ln\\theta_{i}$ , $\\theta_{i}>0\\;\\forall i\\in[n]$ , it can be shown in this case the probability   \n772 that an alternative $i$ emerges as the winner in the set $S\\ni i$ becomes: $\\mathbb{P}(i|S)=\\frac{\\theta_{i}}{\\sum_{j\\in S}\\theta_{j}}$   \n773 Other families of discrete choice models can be obtained by imposing different probability   \n774 distributions over the utility scores $X_{i}$ , e.g. if $(X_{1},\\hdots X_{n})\\sim\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Lambda})$ are jointly normal   \n775 with mean $\\pmb{\\theta}=(\\theta_{1},.~.~.~\\theta_{n})$ and covariance $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ , then the corresponding RUM-based   \n776 choice model reduces to the Multinomial Probit (MNP). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "777 A.2 Rank Breaking ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "778 Rank breaking (RB) is a well-understood idea involving the extraction of pairwise comparisons   \n779 from (partial) ranking data, and then building pairwise estimators on the obtained pairs by   \n780 treating each comparison independently [26, 25], e.g., a winner $a$ sampled from among $a,b,c$ is   \n781 rank-broken into the pairwise preferences $a\\succ b$ , $a\\succ c$ . We use this idea to devise estimators   \n782 for the pairwise win probabilities $p_{i j}=\\mathbb{P}(i|\\{i,j\\})=\\theta_{i}/(\\theta_{i}+\\theta_{j})$ for our problem setting.   \n783 We used the idea of RB in both our algorithms (AOA-RBPL and AOA-RBPL-Adaptive) to   \n784 update the pairwise win-count estimates $w_{i,j,t}$ for all the item pairs $(i,j)\\in[K]\\times[K]$ , which   \n785 is further used for deriving the empirical pairwise preference estimates $\\widehat{p}_{i j,t}$ , at any time $t$ . ", "page_idx": 19}, {"type": "text", "text": "786 A.3 Parameter Estimation with PL based preference data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "787 Lemma 7 (Pairwise win-probability estimates for the PL model [34]). Consider a Plackett  \n788 Luce choice model with parameters $\\pmb{\\theta}=(\\theta_{1},\\theta_{2},\\ldots,\\theta_{n})$ , and fix two items $i,j\\in[n]$ . Let   \n789 $S_{1},...,S_{T}$ be a sequence of (possibly random) subsets of $[n]$ of size at least 2, where $T$ is   \n790 a positive integer, and $i_{1},\\dots,i_{T}$ a sequence of random items with each $i_{t}\\in S_{t}$ , $1\\leq t\\leq T$ ,   \n791 such that for each $1\\leq t\\leq T$ , (a) $S_{t}$ depends only on $S_{1},\\ldots,S_{t-1}$ , and $(b)\\ i_{t}$ is distributed   \n792 as the Plackett-Luce winner of the subset $S_{t}$ , given $S_{1},i_{1},\\ldots,S_{t-1},i_{t-1}$ and $S_{t}$ , and (c)   \n793 $\\forall t:\\{i,j\\}\\subseteq S_{t}$ with probability 1. Let $\\begin{array}{r}{n_{i}(T)=\\sum_{t=1}^{T}\\mathbb{P}(i_{t}=i)}\\end{array}$ and $\\begin{array}{r}{n_{i j}(T)=\\sum_{t=1}^{T}\\mathbb{P}(\\{i_{t}\\in\\;\\}}\\end{array}$   \n794 $\\{i,j\\}\\}$ . Then, for any positive integer $\\boldsymbol{v}$ , and $\\eta\\in(0,1)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\frac{n_{i}(T)}{n_{i j}(T)}-\\frac{\\theta_{i}}{\\theta_{i}+\\theta_{j}}\\geq\\eta,\\ n_{i j}(T)\\geq v\\right)\\leq e^{-2v\\eta^{2}},}\\\\ &{\\mathbb{P}\\left(\\frac{n_{i}(T)}{n_{i j}(T)}-\\frac{\\theta_{i}}{\\theta_{i}+\\theta_{j}}\\leq-\\eta,\\ n_{i j}(T)\\geq v\\right)\\leq e^{-2v\\eta^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "795 B Omitted Proofs from Sec. 3 and Sec. 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "796 B.1 A concentration bounds for the $p_{i j,t}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "797 We first prove below a concentration inequality based on Bernstein\u2019s inequality for the   \n798 estimators pij,t.   \n799 Lemma 8. Let $(i,j)\\in[K]\\times[K]$ . Let $T\\geq1$ and $x>0$ . Then, with probability at least   \n800 $1-3T e^{-x}$ , ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\np_{i j}\\leq p_{i j,t}^{u c b}\\leq p_{i j}+2\\sqrt{\\frac{2p_{i j}(1-p_{i j})x}{n_{i j,t}}}+\\frac{11x}{n_{i j,t}}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "801 simultaneously for all $t\\in[T]$ . ", "page_idx": 20}, {"type": "text", "text": "802 Proof of Lemma $\\boldsymbol{\\&}$ . Let $T\\geq1$ , $x>0$ and $i,j\\in[K]$ . Applying Thm. 1 of [4], with probability   \n803 at least $1-\\beta(x,T)$ , we get simultaneously for all $t\\in[T]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\widehat{p}_{i j,t}-p_{i j}\\right|\\leq\\sqrt{\\frac{2\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})x}{n_{i j,t}}}+\\frac{3x}{n_{i j,t}}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "804 where $\\begin{array}{r}{\\beta(x,T)=3\\operatorname*{inf}_{1<\\alpha\\leq3}\\operatorname*{min}\\big\\{\\frac{\\log T}{\\log\\alpha},T\\big\\}e^{-x/\\alpha}\\leq3T e^{-x}}\\end{array}$ . Note that the inequality holds   \n805 true although $n_{i j,t}$ is a random variable. This, shows the first inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{i j}\\leq p_{i j,t}^{\\tt u c b}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "806 For the second inequality, (9) implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{i j,t}^{\\mathrm{ucb}}=\\widehat{p}_{i j,t}+\\sqrt{\\frac{2\\widehat{p}_{i j,t}\\left(1-\\widehat{p}_{i j,t}\\right)x}{n_{i j,t}}}+\\frac{3x}{n_{i j,t}}}\\\\ {\\leq p_{i j}+2\\sqrt{\\frac{2\\widehat{p}_{i j,t}\\left(1-\\widehat{p}_{i j,t}\\right)x}{n_{i j,t}}}+\\frac{6x}{n_{i j,t}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "807 Furthermore, because $x\\mapsto x(1-x)$ is 1-Lipschitz on [0, 1], we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})-p_{i j}(1-p_{i j})\\right|\\leq\\left|\\widehat{p}_{i j,t}-p_{i j}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\overset{(9)}{\\leq}\\sqrt{\\displaystyle\\frac{2\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})x}{n_{i j,t}}}+\\displaystyle\\frac{3x}{n_{i j,t}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "808 Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})\\leq p_{i j}(1-p_{i j})+\\sqrt{\\frac{2\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})x}{n_{i j,t}}}+\\frac{3x}{n_{i j,t}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\sqrt{p_{i j}(1-p_{i j})}+\\sqrt{\\frac{3x}{n_{i j,t}}}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "809 which yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{\\widehat{p}_{i j,t}(1-\\widehat{p}_{i j,t})}\\le\\sqrt{p_{i j}(1-p_{i j})}+\\sqrt{\\frac{3x}{n_{i j,t}}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "810 Plugging back into (10), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{i j,t}^{\\mathrm{ucb}}\\leq2\\sqrt{\\frac{2p_{i j}(1-p_{i j})x}{n_{i j,t}}}+\\frac{11x}{n_{i j,t}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "811 ", "page_idx": 20}, {"type": "text", "text": "812 B.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "813 Proof. Let $i\\in[K]$ and $x>0$ . Then, by a union bound on Lemma 8 and 2, with probability   \n814 at least $1-4T e^{-x}$ , (8) and (4) hold true for all $t\\in[T]$ . We consider this high-probability   \n815 event in the rest of the proof. Define the function $f:x\\mapsto x/(1-x)_{+}$ on $[0,1]$ (with the   \n816 convention $f(1)=+\\infty$ ), so that $\\theta_{i,t}^{\\tt u c b}=f(p_{i0,t}^{\\tt u c b})$ and $\\theta_{i}=f(p_{i0})$ . Because $f$ is non-decreasing,   \n817 and $p_{i0,t}^{\\tt u c b}\\geq p_{i0}$ by (8), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{i,t}^{\\tt u c b}\\geq\\theta_{i}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "818 Furthermore, denote ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{i,t}:=2\\sqrt{\\frac{2p_{i j}(1-p_{i j})x}{n_{i0,t}}}+\\frac{11x}{n_{i0,t}}=2\\sqrt{\\frac{2\\theta_{0}\\theta_{i}x}{(\\theta_{0}+\\theta_{i})^{2}n_{i0,t}}}+\\frac{11x}{n_{i0,t}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "819 In the rest of the proof we assume, $n_{i0,t}\\ge69x(\\theta_{0}+\\theta_{i})$ . Then, using that $\\theta_{0}\\theta_{i}\\,\\leq\\,\\theta_{0}+\\theta_{i}$   \n820 since $\\theta_{0}=1$ , it implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\theta_{0}+\\theta_{i})\\Delta_{i,t}\\leq2\\sqrt{\\frac{2\\theta_{0}\\theta_{i}x}{n_{i0,t}}}+\\frac{11x(\\theta_{0}+\\theta_{i})}{n_{i0,t}}\\leq\\frac{1}{2}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "821 and ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{i0}+\\Delta_{i,t}=\\frac{\\theta_{i}}{\\theta_{0}+\\theta_{i}}+\\Delta_{i,t}\\leq\\frac{\\theta_{i}+1/2}{\\theta_{i}+1}<1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "822 Thus, because $f$ is non-decreasing ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{\\theta}_{i,t}^{\\mathrm{as}}-\\theta_{i}+f(p_{i0}^{\\mathrm{sc}})-f(p_{i0})}&{}\\\\ {\\stackrel{\\mathrm{c}}{\\le}f(p_{i0}+\\Delta_{i},t)-f(p_{i0})}&{}\\\\ {}&{=\\frac{p_{i0}+\\Delta_{i}}{1-p_{i0}-\\Delta_{i},t}-\\frac{p_{i0}}{1-p_{i0}}}\\\\ {}&{=\\frac{\\Delta_{i}}{(1-p_{i0})(1-p_{i0}-\\Delta_{i},t)}}\\\\ {}&{=\\frac{(\\theta_{0}+\\theta_{i})^{2}\\Delta_{i},t}{1-(\\theta_{0}+\\theta_{i})\\Delta_{i},t}}\\\\ {}&{\\le2(\\theta_{0}+\\theta_{i})^{2}\\Delta_{i},t}\\\\ {}&{\\stackrel{\\mathrm{(1)}}{\\le}4(\\theta_{0}+\\theta_{i})\\sqrt{\\frac{2\\theta_{0}\\theta_{i}x}{n_{0,0}}}+\\frac{22x(\\theta_{0}+\\theta_{i})^{2}}{n_{0,0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "823 which concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "824 B.3 Proof of Lemma 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "825 Proof. Let $T\\geq1$ and $i\\in[K]$ . Recall that $\\begin{array}{r}{\\tau_{i,t}=\\sum_{s=1}^{t-1}\\mathbb{1}\\{i\\in S_{s}\\}}\\end{array}$ is the number of times $i$   \n826 was played at the start of round t and ni0,t =   ts\u2212=11 $\\begin{array}{r}{n_{i0,t}=\\sum_{s=1}^{t-1}\\mathbb{1}\\{i_{t}\\in\\{i,0\\},i\\in S_{t}\\}}\\end{array}$ is the number of   \n827 times or won up to round when played together. When $i$ is played the probability of $0$   \n828 or $i$ to win is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}(i_{t}\\in\\{i,0\\}|S_{t})=\\frac{\\theta_{0}+\\theta_{i}}{\\theta_{0}+\\Theta_{S_{t}}}\\geq\\frac{\\theta_{0}+\\theta_{i}}{\\theta_{0}+\\Theta_{S^{\\ast}}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "829 Therefore, applying Chernoff-Hoeffding inequality together with a union bound (to deal with   \n830 the fact that $\\tau_{i,t}$ is random), we have with probability at least $1-T e^{-x}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nn_{i0,t}\\geq\\frac{\\theta_{0}+\\theta_{i}}{\\theta_{0}+\\Theta_{S^{*}}}\\tau_{i,t}-\\sqrt{\\frac{\\tau_{i,t}x}{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "831 simultaneously for all $t\\in[T]$ . Noting that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\theta_{0}+\\theta_{i}}{\\theta_{0}+\\Theta_{S^{*}}}\\tau_{i,t}-\\sqrt{\\frac{\\tau_{i,t}x}{2}}\\geq\\frac{\\theta_{0}+\\theta_{i}}{2(\\theta_{0}+\\Theta_{S^{*}})}\\tau_{i,t}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "832 if $\\begin{array}{r}{\\tau_{i,t}\\geq2x(\\theta_{0}+\\Theta_{S^{\\ast}})^{2}\\geq\\frac{2x(\\theta_{0}+\\Theta_{S^{\\ast}})^{2}}{(\\theta_{0}+\\theta_{i})^{2}}}\\end{array}$ concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "833 B.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "834 Proof. Let us define for any $S\\subseteq[K]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Theta_{S}=\\sum_{i\\in S}\\theta_{i},\\quad\\mathrm{and}\\quad\\Theta_{S}^{\\mathrm{ucb}}:=\\sum_{i\\in S}\\theta_{i}^{\\mathrm{ucb}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "835 Let $\\mathcal{E}$ be the high-probabality event such that both Lemma 1 and 2 holds true. Then,   \n836 $\\mathbb{P}(\\mathcal{E})\\ge1-4T K e^{-x}$ . Let us first assume that $\\boldsymbol{\\xi}$ holds true. Then, by Lemma 1, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle R e_{\\boldsymbol{Q}_{T}^{\\mathrm{top}}}^{\\mathrm{top}}=\\frac{1}{m}\\sum_{t=1}^{T}\\boldsymbol{\\Theta}_{S^{\\ast}}-\\boldsymbol{\\Theta}_{S_{t}}}\\\\ {\\displaystyle}&{\\leq\\frac{1}{m}\\sum_{t=1}^{T}\\operatorname*{min}\\left\\lbrace\\boldsymbol{\\Theta}_{S^{\\ast}},\\boldsymbol{\\Theta}_{S_{t}}^{\\mathrm{neb}}-\\boldsymbol{\\Theta}_{S_{t}}\\right\\rbrace\\ \\ \\gets\\ \\mathrm{because}\\ \\boldsymbol{\\Theta}_{S^{\\ast}}\\leq\\boldsymbol{\\Theta}_{S^{\\ast}}^{\\mathrm{neb}}\\leq\\boldsymbol{\\Theta}_{S_{t}}^{\\mathrm{neb}}\\mathrm{~under~the~event}}\\\\ {\\displaystyle}&{=\\frac{1}{m}\\sum_{t=1}^{T}\\operatorname*{min}\\left\\lbrace\\boldsymbol{\\Theta}_{S^{\\ast}},\\sum_{i\\in S_{t}}\\boldsymbol{\\theta}_{i,t}^{\\mathrm{neb}}-\\boldsymbol{\\theta}_{i}\\right\\rbrace}\\\\ {\\displaystyle}&{\\leq\\frac{1}{m}\\boldsymbol{\\Theta}_{S^{\\ast}}\\sum_{i=1}^{K}\\bar{\\boldsymbol{\\tau}}_{i0}+\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}(\\theta_{i,t}^{\\mathrm{neb}}-\\theta_{i})\\mathbb{I}\\left\\lbrace\\boldsymbol{\\tau}_{i,t}\\geq\\bar{\\boldsymbol{\\tau}}_{i0}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "837 where $\\bar{\\tau}_{i0}=2x(\\theta_{0}+\\Theta_{S^{\\ast}})\\operatorname*{max}\\{\\theta_{0}+\\Theta_{S^{\\ast}},69\\}\\leq138x(m+1)^{2}\\theta_{\\operatorname*{max}}^{2}$ , where $\\theta_{\\mathrm{max}}:=\\operatorname*{max}_{i}\\theta_{i}$ .   \n838 Then, noting that if $\\boldsymbol{\\xi}$ holds true, by Lemma 2, we also have $\\begin{array}{r}{n_{i0,t}\\,\\geq\\,\\frac{1}{2(\\theta_{0}+\\Theta_{S^{*}})}(\\theta_{0}+\\theta_{i})\\tau_{i,t}}\\end{array}$   \n839 which yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{1}\\{\\tau_{i,t}\\ge\\bar{\\tau}_{i0}\\}\\le\\mathbb{1}\\{n_{i0,t}\\ge69x(\\theta_{0}+\\theta_{i})\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "840 Therefore, we can apply Lemma 1 that entails, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}(\\theta_{i,t}^{\\mathrm{acb}}-\\theta_{i})\\mathbb{I}\\left\\{\\tau_{i,t}\\geq\\bar{\\tau}_{i0}\\right\\}}}\\\\ &{\\qquad\\stackrel{\\mathrm{Lem.~1}}{\\leq}\\frac{1}{m}\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(4(\\theta_{0}+\\theta_{i})\\sqrt{\\frac{2\\theta_{0}\\theta_{i}x}{n_{0,t}}}+\\frac{22x(\\theta_{0}+\\theta_{i})^{2}}{n_{0,t}}\\right)\\mathbb{I}\\left\\{n_{0,t}\\geq69x(\\theta_{0}+\\theta_{i})\\right\\}}\\\\ &{\\qquad\\stackrel{\\mathrm{Lem.~2}}{\\leq}\\displaystyle\\frac{1}{m}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(8\\sqrt{\\frac{(\\theta_{0}+\\Theta_{S^{\\star}})(\\theta_{0}+\\theta_{i})\\theta_{0}\\theta_{i}x}{\\tau_{i,t}}}+\\frac{44x(\\theta_{0}+\\Theta_{S^{\\star}})(\\theta_{0}+\\theta_{i})}{\\tau_{i,t}}\\right)}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{m}\\sum_{i=1}^{K}16\\sqrt{(\\theta_{0}+\\Theta_{S^{\\star}})(\\theta_{0}+\\theta_{i})\\theta_{0}\\theta_{i}x\\tau_{i,T}}+44x(\\theta_{0}+\\Theta_{S^{\\star}})\\sum_{i=1}^{K}(\\theta_{0}+\\theta_{i})(1+\\log(\\tau_{i,T}}\\end{array})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "841 where we used $\\textstyle\\sum_{i=1}^{n}1/{\\sqrt{i}}\\leq2{\\sqrt{n}}$ and $\\textstyle\\sum_{i=1}^{n}i^{-1}\\leq1+\\log n$ . We thus have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\mathrm{top}}\\leq138x(m+1)^{2}K\\theta_{\\operatorname*{max}}^{3}+\\displaystyle\\frac{1}{m}\\sum_{i=1}^{K}16\\theta_{\\operatorname*{max}}^{3/2}\\sqrt{(m+1)x\\tau_{i,T}}}\\\\ &{\\phantom{R e g_{T}^{\\mathrm{top}}\\leq138x(m+1)^{2}K\\theta_{\\operatorname*{max}}^{3}+16\\theta_{\\operatorname*{max}}^{3/2}\\sqrt{2x K T}+88x(m+1)K\\theta_{\\operatorname*{max}}^{2}\\Bigl(1+\\log\\bigl(\\frac{m T}{K}\\bigr)\\Bigr)}.}\\\\ &{\\phantom{R e g_{T}^{\\mathrm{top}}\\leq138x(m+1)^{2}K\\theta_{\\operatorname*{max}}^{3}+16\\theta_{\\operatorname*{max}}^{3/2}\\sqrt{2x K T}+88x(m+1)K\\theta_{\\operatorname*{max}}^{2}\\Bigl(1+\\log\\Bigl(\\frac{m T}{K}\\Bigr)\\Bigr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "842 Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[R e g_{T}^{\\mathrm{top}}]\\leq12\\sqrt{2}x m K\\theta_{\\operatorname*{max}}^{3}+16\\theta_{\\operatorname*{max}}^{3/2}\\sqrt{2x K T}+88x m K\\theta_{\\operatorname*{max}}^{2}\\Big(1+\\log\\Big(\\frac{m T}{K}\\Big)\\Big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "843 Choosing $x=2\\log T$ concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "845 Proof. Let $\\boldsymbol{\\xi}$ be the high-probabality event such that Lemma $^1$ and 2 are satisfied, so that   \n846 $\\mathbb{P}(\\mathcal{E})\\ge1-4K T e^{-x}$ . Then, denoting $x\\wedge y:=\\operatorname*{min}\\{x,y\\}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\mathrm{std}}=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathcal{R}(S^{\\ast},\\theta)-\\mathcal{R}(S_{t},\\theta)\\big]}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[(\\mathcal{R}(S^{\\ast},\\theta)-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}\\}+(\\mathcal{R}(S^{\\ast},\\theta)-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\big]}\\\\ &{\\qquad\\le\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\big((\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{ncb}})-\\mathcal{R}(S_{t},\\theta))\\wedge\\mathcal{R}(S^{\\ast},\\theta)\\big)\\mathbb{1}\\{\\mathcal{E}\\}+\\mathcal{R}(S^{\\ast},\\theta)\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "847 because $\\begin{array}{r}{\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{{ucb}}})\\ge\\mathcal{R}(S^{*},\\theta_{t}^{\\mathrm{{ucb}}})\\ge\\mathcal{R}(S^{*},\\theta)}\\end{array}$ under the event $\\boldsymbol{\\varepsilon}$ by Lemma 4. Then, using   \n848 $\\mathcal{R}(S^{*},\\theta)\\leq\\operatorname*{max}_{i}r_{i}\\leq1$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\mathrm{std}}\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\big((\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{ncb}})-\\mathcal{R}(S_{t},\\theta))\\wedge1\\big)\\mathbb{1}\\{\\mathcal{E}\\}+\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4T^{2}K e^{-x}+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\big(\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{ncb}})-\\mathcal{R}(S_{t},\\theta)\\big)\\wedge1\\Big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "849 Let us upper-bound the second term of the right-hand-side ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg((\\eta(S_{t},\\rho_{t})-\\mathcal{R}(S_{t},\\theta))+\\mathcal{A}_{t}\\bigg)\\bigg\\lambda|\\varepsilon\\bigg]}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg(\\bigg(\\sum_{t_{k}=0}^{T/\\tau_{k}}\\frac{\\rho(S_{t})}{\\Phi_{t}+\\Phi_{y_{k}}}-\\frac{\\rho(R_{k})}{\\Phi_{t}+\\Theta_{y_{k}}}\\bigg)\\lambda+\\bigg)\\bigg.}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg(\\bigg(\\sum_{t_{k}=0}^{T/\\tau_{k}}\\frac{\\rho(S_{t})}{\\Phi_{t}+\\Phi_{y_{k}}}\\bigg)\\wedge1\\bigg)\\bigg.\\bigg.}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg(\\bigg(\\sum_{t_{k}=0}^{T/\\tau_{k}}\\frac{\\rho(S_{t})}{\\Phi_{t}+\\Phi_{y_{k}}}\\bigg)\\wedge1\\bigg)\\bigg.\\bigg.}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg(\\sum_{t_{k}=0}^{T/\\tau_{k}}\\frac{\\rho(S_{t})}{\\Phi_{t}+\\Phi_{y_{k}}}\\bigg)\\wedge1\\bigg.\\bigg.}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg.\\bigg(\\frac{\\rho(S_{t})}{\\Phi_{t}+\\Phi_{y_{k}}}\\bigg)\\bigg.1\\bigg.\\bigg.}\\\\ &{\\leq\\frac{\\sum_{t}}{L_{\\tau}}\\mathbb{E}\\bigg[\\bigg.\\bigg.\\frac{\\sum_{t_{k}=0}^{T}}{\\Phi_{t}+\\Phi_{y_{k}}}\\bigg.\\bigg.\\bigg.}\\\\ &{\\leq13\\lambda\\times m^{2}K\\theta_{m}^{2}+\\frac{\\sum_{t_{k}=0}^{T}}{\\sum_{t_{k}=0}^{T}}\\bigg[\\frac{\\sum_{t_{k}=0}^{T/\\tau_{k}}-\\theta_{t_{k}}}{\\Phi_{t}+\\Theta_{y_{k}}}\\bigg.\\bigg.\\bigg.}\\\\ &{\\leq\\mathrm{Bix}m^{2}K\\theta_{m}^{2}-\\frac{\\sum_{t \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "850 where the last inequality is by Cauchy-Schwarz inequality. Now, the term $A_{T}(i)$ above may   \n851 be upper-bounded as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\nA_{T}(i):=\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left(\\frac{|\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i}|}{\\theta_{0}+\\Theta_{S_{t}}}\\right)^{2}\\frac{\\theta_{0}+\\Theta_{S_{t}}}{\\frac{\\theta_{0}}{m}+\\theta_{i}}\\mathbb{1}\\{i\\in S_{t},\\tau_{i,t}\\ge138x(m+1)^{2}\\theta_{\\operatorname*{max}}^{2}\\}\\mathbb{1}\\{\\mathcal{E}\\}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\mathbb{E}\\left[\\frac{(\\theta_{i,t}^{\\mathrm{ucb}}-\\theta_{i})^{2}}{\\big(\\frac{\\theta_{0}}{m}+\\theta_{i}\\big)\\theta_{0}+\\Theta_{S_{t}}}\\mathbb{1}\\{i\\in S_{t},\\tau_{i,t}\\ge138x(m+1)^{2}\\theta_{\\mathrm{max}}^{2}\\}\\mathbb{1}\\{\\mathcal{E}\\}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "852 Now, since under the event $\\boldsymbol{\\xi}$ by Lemma 2, $\\tau_{i,t}\\geq138x(m+1)^{2}\\theta_{\\mathrm{max}}^{2}$ implies ", "page_idx": 24}, {"type": "equation", "text": "$$\nn_{i0,t}\\geq69x(\\theta_{0}+\\theta_{i})(m+1)\\theta_{\\mathrm{max}}\\geq69x(\\theta_{0}+\\theta_{i})\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "853 Therefore, we can apply Lemma $^{1}$ , which further upper-bounds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{A_{T}(i)\\leq\\sum_{t=1}^{T}\\mathbb{E}\\bigg[\\bigg(\\frac{2^{6}\\left(\\theta_{0}+\\theta_{i}\\right)^{2}x}{n_{i0,t}}+\\frac{2(22x)^{2}(\\theta_{0}+\\theta_{i})^{4}}{n_{i0,t}^{2}(\\frac{\\theta_{0}}{m}+\\theta_{i})}\\bigg)}}\\\\ &{}&{\\quad\\times\\frac{\\mathbb{I}\\{i\\in S_{t},\\tau_{i,t}\\geq138x(m+1)^{2}\\theta_{\\operatorname*{max}}^{2}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\mathbb{I}\\{\\mathcal{E}\\}\\bigg]}\\\\ &{}&{\\quad\\leq\\sum_{t=1}^{T}\\mathbb{E}\\bigg[\\bigg(\\frac{2^{6}\\left(\\theta_{0}+\\theta_{i}\\right)^{2}x}{n_{i0,t}}+\\frac{15x(\\theta_{0}+\\theta_{i})^{3}}{n_{i0,t}\\theta_{\\operatorname*{max}}(\\theta_{0}+m\\theta_{i})}\\bigg)\\times\\frac{\\mathbb{I}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\mathbb{I}\\{\\mathcal{E}\\}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "854 where we used $n_{i0,t}\\ge69x(\\theta_{0}+\\theta_{i})m\\theta_{\\mathrm{{max}}}$ in the last inequality. Then, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{T}(i)\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\left(\\frac{(\\theta_{0}+\\theta_{i})^{2}x}{n_{i0,t}}+\\frac{30x(\\theta_{0}+\\theta_{i})}{n_{i0,t}}\\right)\\times\\frac{\\mathbf{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{s_{t}}}\\mathbf{1}\\{\\varepsilon\\}\\right]}\\\\ &{\\leq\\left(94+64\\theta_{i}\\right)x\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{(\\theta_{0}+\\theta_{i})\\mathbf{1}\\{i\\in S_{t}\\}}{(\\theta_{0}+\\Theta_{s_{t}})n_{i0,t}}\\right]}\\\\ &{=\\left(94+64\\theta_{i}\\right)x\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\mathbf{1}\\{i_{t}\\in\\{i,0\\},i\\in S_{t}\\}}{n_{i0,t}}\\right]}\\\\ &{=\\left(94+64\\theta_{i}\\right)x\\mathbb{E}\\left[1+\\log\\left(n_{i0}(T)\\right)\\right]}\\\\ &{\\leq158\\theta_{\\operatorname*{max}}x(1+\\log T)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "855 Substituting into (16), we then obtain using Cauchy-Schwarz inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\big(\\mathcal{R}(S_{t},\\theta_{t}^{\\mathrm{neb}})-\\mathcal{R}(S_{t},\\theta)\\big)\\wedge1\\Big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]}\\\\ &{\\displaystyle\\qquad\\leq138x m^{2}K\\theta_{\\operatorname*{max}}^{2}+13\\sqrt{\\theta_{\\operatorname*{max}}x(1+\\log T)}\\displaystyle\\sum_{i=1}^{K}\\sqrt{\\sum_{t=1}^{T}\\mathbb{E}\\left[\\frac{\\big(\\frac{\\theta_{0}}{m}+\\theta_{i}\\big)\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\right]}}\\\\ &{\\displaystyle\\qquad\\leq138x m^{2}K\\theta_{\\operatorname*{max}}^{2}+13\\sqrt{\\theta_{\\operatorname*{max}}x(1+\\log T)}\\sqrt{\\mathbb{E}\\left[K\\displaystyle\\sum_{t=1}^{T}\\frac{\\sum_{i=1}^{K}\\big(\\frac{\\theta_{0}}{m}+\\theta_{i}\\big)\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\right]}}\\\\ &{\\displaystyle\\qquad=138x m^{2}K\\theta_{\\operatorname*{max}}^{2}+13\\sqrt{\\theta_{\\operatorname*{max}}x(1+\\log T)}K T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "856 Finally, replacing into Inequality (15) yields ", "page_idx": 24}, {"type": "equation", "text": "$$\nR e g_{T}^{\\tt{u t d}}\\leq4T^{2}K e^{-x}+138x m^{2}K\\theta_{\\mathrm{max}}^{2}+13\\sqrt{\\theta_{\\mathrm{max}}x(1+\\log T)K T}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "857 Choosing $x=2\\log T$ concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "858 B.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "859 The proof follows the one of Theorem 5, except that the concentration lemmas should be   \n860 generalized to any pairs $(i,j)$ instead of only with respect to item 0, whose proofs are left   \n861 to the reader and closely follows the one of Lemma 1 and 2. For simplicity, this proof is   \n862 performed up to universal multiplicative constants, using the rough inequality $\\lesssim$ .   \n863 Lemma 9. Let $T\\geq1$ and $x\\,>\\,0$ . Then, with probability at least $1-3K(K+1)T e^{-x}$ ,   \n864 simultaneously for all $t\\in[T]$ and $i\\neq j$ in $[\\tilde{K}]$ : $\\begin{array}{r}{\\gamma_{i j}:=\\frac{\\theta_{i}}{\\theta_{j}}\\leq\\gamma_{i j,t}^{u c b}}\\end{array}$ and one of the following   \n865 two inequalities is satisfied ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\nn_{i j,t}<69x(1+\\gamma_{i j})\\qquad o r\\qquad\\gamma_{i j,t}^{u c b}\\leq\\gamma_{i j}+4(\\gamma_{i j}+1)\\sqrt{\\frac{2\\gamma_{i j}x}{n_{i j,t}}}+\\frac{22x(\\gamma_{i j}+1)^{2}}{n_{i j,t}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "866 Lemma 10. Let $T\\geq1$ and $x>0$ . Then, with probability at least $1-3K(K+1)T e^{-x}$ ,   \n867 simultaneously for all $t\\in[T]$ and $i\\in[K]$ : $\\begin{array}{r}{\\widehat{\\theta}_{i,t}^{u c b}:=\\operatorname*{min}_{j}\\gamma_{i j,t}^{u c b}\\gamma_{j0,t}^{u c b}\\geq\\theta_{i}}\\end{array}$ and for all $j$ one of   \n868 the following two inequalities is satisfied ", "page_idx": 25}, {"type": "equation", "text": "$$\nn_{i j,t}\\lesssim x(1+\\gamma_{i j})\\qquad o r\\qquad n_{j0,t}\\lesssim x(1+\\theta_{j})^{2}\\theta_{j}^{-1}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "869 or ", "page_idx": 25}, {"type": "equation", "text": "$$\n^{u c b}_{i j,t}\\gamma_{j0,t}^{u c b}-\\theta_{i}\\lesssim\\sqrt{(\\gamma_{i j}+1)\\theta_{i}x}\\bigg(\\sqrt{\\frac{(\\theta_{i}+\\theta_{j})}{n_{i j,t}}}+\\sqrt{\\frac{(1+\\theta_{j})}{n_{j0,t}}}\\bigg)+(\\gamma_{i j}+1)\\frac{(\\theta_{i}+\\theta_{j})x}{n_{i j,t}}+\\frac{\\gamma_{i j}(1+\\theta_{j})^{2}x}{n_{j0,t}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "870 Proof of Lemma $1\\theta$ . The proof follows from Lemma 9. If $n_{i j,t}>C x(1+\\gamma_{i j})$ and $n_{j0,t}>$   \n871 $C x(1+\\theta_{j})$ for some large enough constant C, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{i j,t}^{\\mathrm{ucb}}\\leq\\gamma_{i j}+4(\\gamma_{i j}+1)\\sqrt{\\frac{2\\gamma_{i j}x}{n_{i j,t}}}+\\frac{22x(\\gamma_{i j}+1)^{2}}{n_{i j,t}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "872 and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{j0,t}^{\\mathrm{ucb}}\\le\\gamma_{j0}+4\\bigl(\\gamma_{j0}+1\\bigr)\\sqrt{\\frac{2\\gamma_{j0}x}{n_{j0,t}}}+\\frac{22x(\\gamma_{j0}+1)^{2}}{n_{j0,t}}\\le2\\gamma_{j0}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "873 This implies,", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{i j,t}^{\\mathrm{ucb}}\\gamma_{j0,t}^{\\mathrm{ucb}}-\\theta_{i}=\\gamma_{i j,t}^{\\mathrm{ucb}}\\gamma_{j0,t}^{\\mathrm{ucb}}-\\gamma_{i j}\\gamma_{j0}=\\big(\\gamma_{i j,t}^{\\mathrm{ucb}}-\\gamma_{i j}\\big)\\gamma_{j0,t}^{\\mathrm{acb}}+\\gamma_{i j}\\big(\\gamma_{j0,t}^{\\mathrm{ucb}}-\\gamma_{j0}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\big(\\gamma_{i j,t}^{\\mathrm{ucb}}-\\gamma_{i j}\\big)\\gamma_{j0}+\\gamma_{i j}\\big(\\gamma_{j0,t}^{\\mathrm{acb}}-\\gamma_{j0}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq8\\gamma_{j0}\\big(\\gamma_{i j}+1\\big)\\sqrt{\\frac{2\\gamma_{i j}x}{n_{i j,t}}}+\\frac{44x\\gamma_{j0}\\big(\\gamma_{i j}+1\\big)^{2}}{n_{i j,t}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+4\\gamma_{i j}\\big(\\gamma_{j0}+1\\big)\\sqrt{\\frac{2\\gamma_{j0}x}{n_{j0,t}}}+\\frac{22x\\gamma_{i j}\\big(\\gamma_{j0}+1\\big)^{2}}{n_{j0,t}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "874 Replacing $\\gamma_{i j}=\\theta_{i}/\\theta_{j}$ and $\\gamma_{j0}=\\theta_{j}$ concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "875 Lemma 11. Let $T\\geq1$ and $x>0$ . Then, with probability at least $1-K(K+1)T e^{-x}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau_{i j,t}<2x\\frac{(\\theta_{0}+\\Theta_{S^{*}})^{2}}{\\theta_{i}+\\theta_{j}}\\;\\;\\;o r\\;\\;n_{i j,t}\\geq\\frac{(\\theta_{i}+\\theta_{j})\\tau_{i j,t}}{2(\\theta_{0}+\\Theta_{S^{*}})}\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "876 where $\\begin{array}{r}{\\tau_{i j,t}:=\\sum_{s=1}^{t-1}\\mathbb{1}\\{\\{i,j\\}\\subseteq S_{s}\\}}\\end{array}$ simultaneously for all $t\\in[T]$ and $i\\neq j\\in[K]$ . ", "page_idx": 25}, {"type": "text", "text": "877 Proof of Theorem $\\it6$ . Let $\\boldsymbol{\\xi}$ be the high-probabality event of Lemmas 10 and 11 are satisfied,   \n878 so that $\\mathbb{P}(\\mathcal{E})\\ge1-4K^{2}T e^{-x}$ . First, note that since we have under the event $\\boldsymbol{\\xi}$ , $\\widehat{\\theta}_{t}^{\\mathrm{ucb}}\\leq\\theta_{t}^{\\mathrm{ucb}}$ ,   \n879 our procedure also satisfies the regret upper-bound ", "page_idx": 25}, {"type": "equation", "text": "$$\nR e g_{T}^{\\mathrm{wtd}}\\leq O(\\sqrt{\\theta_{\\mathrm{max}}K T}\\log T)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "880 of Theorem 5. Indeed, all upper-bounds of the proof of Theorem 5 remain valid upper-bounds   \n881 except the probability of the event $\\mathcal{E}^{c}$ which is $O(T^{-1})$ for $x=2\\log T$ .   \n882 Let us now prove that we also have $R_{T}\\leq O(K{\\sqrt{T}}\\log T)$ with no asymptotic dependence on   \n883 $\\theta_{\\mathrm{max}}$ when $T\\rightarrow\\infty$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "884 Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\mathrm{std}}=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta)\\big]}\\\\ &{\\qquad=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[(\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}\\}+(\\mathcal{R}(S^{*},\\theta)-\\mathcal{R}(S_{t},\\theta))\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\big]}\\\\ &{\\qquad\\le\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\big((\\mathcal{R}(S_{t},\\widehat{\\theta}_{t}^{\\mathrm{ncb}})-\\mathcal{R}(S_{t},\\theta))\\wedge\\mathcal{R}(S^{*},\\theta)\\big)\\mathbb{1}\\{\\mathcal{E}\\}+\\mathcal{R}(S^{*},\\theta)\\mathbb{1}\\{\\mathcal{E}^{c}\\}\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "885 Then, using $\\mathcal{R}(S^{*},\\theta)\\leq\\operatorname*{max}_{i}r_{i}\\leq1$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R e g_{T}^{\\tt u t d}\\leq\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\big((\\mathcal{R}(S_{t},\\widehat{\\theta}_{t}^{\\tt u c b})-\\mathcal{R}(S_{t},\\theta))\\wedge1\\big){\\mathbb1}\\{\\mathcal{E}\\}+{\\mathbb1}\\{\\mathcal{E}^{c}\\}\\Big]}\\\\ &{\\qquad\\qquad\\leq4T^{2}K(K+1)^{2}e^{-x}+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\big(\\mathcal{R}(S_{t},\\widehat{\\theta}_{t}^{\\tt u c b})-\\mathcal{R}(S_{t},\\theta)\\big)\\wedge1\\Big){\\mathbb1}\\{\\mathcal{E}\\}\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "886 Follow the proof of Theorem 5, we upper-bound the second term of the right-hand-side   \n887 of (19): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Gamma}{\\sum_{t=1}^{T}}\\mathbb{E}\\Big[\\Big(\\big(\\Phi(S_{t},\\hat{\\theta}_{t}^{u*})-\\mathcal{R}(S_{t},\\theta)\\big)\\wedge\\big)\\mathbf{1}\\{\\varepsilon\\big\\}\\Big]}\\\\ &{\\quad\\quad-\\frac{\\Gamma}{l+1}\\mathbb{E}\\Bigg[\\bigg(\\bigg(\\frac{\\operatorname*{min}}{\\sqrt{l+1}}\\sum_{i\\in S_{t}}\\frac{r\\hat{\\theta}_{t}^{u*}}{\\sqrt{l+1}}-\\frac{r\\hat{\\theta}_{t}^{u}}{\\sqrt{l+1}}\\bigg.\\bigg.\\bigg.\\bigg.\\frac{r\\hat{\\theta}_{t}^{u}}{1+\\sum_{j\\in S_{t}}\\theta_{j}}\\bigg)\\wedge\\mathbf{1}\\Big)\\mathbf{1}\\{\\varepsilon\\}\\Bigg]}\\\\ &{\\quad\\quad\\leq\\frac{\\Gamma}{l+1}\\mathbb{E}\\Bigg[\\bigg(\\bigg(\\sum_{i\\in S_{t}}\\frac{r\\hat{\\theta}_{t}^{u*}}{\\theta_{t}+\\theta_{t}}\\bigg)\\wedge\\mathbf{1}\\}\\bigg)\\mathbf{1}\\{\\varepsilon\\}\\Bigg]}\\\\ &{\\quad\\quad\\leq\\frac{\\Gamma}{l+1}\\mathbb{E}\\Bigg[\\bigg(\\bigg(\\sum_{i\\in S_{t}}\\frac{\\hat{\\theta}_{t}^{u*}}{\\theta_{t}+\\theta_{t}}\\bigg.\\bigg.\\bigg.\\wedge\\bigg.\\bigg.\\bigg)\\mathbf{1}\\{\\varepsilon\\}\\Bigg]\\Bigg]}\\\\ &{\\quad\\quad\\leq\\frac{\\Gamma}{l+1}\\mathbb{E}\\Bigg[\\bigg(\\bigg(\\sum_{i\\in S_{t}}\\frac{\\hat{\\theta}_{t}^{u}}{\\theta_{t}+\\theta_{t}}\\bigg.\\bigg.\\bigg.\\bigg.\\bigg\\}\\bigg.\\bigg.\\bigg\\}\\end{array}\\mathrm{berase}\\!\\bigg.\\Bigg.\\frac{\\sqrt{l+1}}{\\theta_{t}+\\sqrt{l+1}}\\bigg)\\Bigg.}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\mathrm{K}}{l+1}\\Bigg[\\sum_{i\\in\\partial_{t}}\\bigg(\\frac{\\hat{\\theta}_{t}^{u*}}{\\theta_{t}+\\theta_{t}}\\bigg.\\bigg.\\wedge\\bigg.\\bigg.\\Big\\}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "888 where $j_{t}\\;=\\;\\mathrm{argmax}_{j\\in S_{t}\\cup\\{0\\}}\\,\\theta_{j}$ , where the last inequality is by definition of $\\widehat{\\theta}_{i,t}^{\\mathrm{ucb}}$ . Now,   \n889 from Lemma 10, paying an additive exploration cost to ensure that $n_{i j,t}\\gtrsim x(1+\\gamma_{i j})$ and   \n890 $n_{j0,t}\\gtrsim x(1+\\theta_{j})^{2}\\theta_{j}$ for all $j\\in S_{t}$ such that $\\theta_{j}\\geq\\theta_{0}$ . From Lemma 11, this is satisfied if for   \n891 some constant $C>0$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tau_{i j,t}>C m^{2}\\theta_{\\mathrm{max}}^{2}x\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "892 Such a condidtion can be wrong for a couple $(i,j)\\in S_{t}^{2}$ at most during $C K^{2}m^{2}\\theta_{\\mathrm{max}}^{2}x=$ 893 ${\\cal O}(\\log T)$ rounds (since increases then). Thus, for $C$ large enough, $\\tau_{i j,t}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\Big(\\big(\\mathcal{R}\\big(S_{t},\\widehat{\\theta}_{t}^{\\mathrm{acb}}\\big)-\\mathcal{R}\\big(S_{t},\\theta\\big)\\big)\\wedge1\\Big)\\mathbb{1}\\{\\mathcal{E}\\}\\Big]}\\\\ &{\\displaystyle\\qquad\\leq{\\cal O}(\\log T)+\\sum_{i=1}^{K}\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\frac{\\big|\\gamma_{i j_{t},t}^{\\mathrm{acb}}\\big>\\gamma_{j_{t}0,t}^{\\mathrm{acb}}-\\theta_{i}\\big|}{\\theta_{0}+\\Theta_{S_{t}}}\\mathbb{1}\\{i\\in S_{t},\\tau_{i j_{t},t}\\wedge\\tau_{j_{t},t}\\geq C x m^{2}\\theta_{\\operatorname*{max}}^{2}\\}\\mathbb{1}\\{\\mathcal{E}\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim O(\\log T)+\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\Bigg[\\displaystyle\\sum_{t=1}^{T}\\bigg(\\sqrt{(\\gamma_{i j_{t}}+1)\\theta_{i}x}\\bigg(\\sqrt{\\frac{(\\theta_{i}+\\theta_{j_{t}})}{n_{i j_{t},t}}}+\\sqrt{\\frac{(1+\\theta_{j})}{n_{j_{t}0,t}}}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(\\gamma_{i j_{t}}+1\\right)\\!\\frac{(\\theta_{i}+\\theta_{j_{t}})x}{n_{i j_{t},t}}+\\frac{\\gamma_{i j_{t}}(1+\\theta_{j_{t}})^{2}x}{n_{j_{t}0,t}}\\bigg)\\frac{\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\Bigg]}\\\\ &{\\leq O(\\log T)+\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\Bigg[\\displaystyle\\sum_{t=1}^{T}\\sqrt{(\\gamma_{i j_{t}}+1)\\theta_{i}x}\\bigg(\\sqrt{\\frac{(\\theta_{i}+\\theta_{j_{t}})}{n_{i j_{t},t}}}+\\sqrt{\\frac{(1+\\theta_{j_{t}})}{n_{j_{t}0,t}}}\\bigg)\\frac{\\mathbb{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "894 where the last inequality is because using that $\\{i,j_{t},0\\}\\subseteq S_{t}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\frac{1+\\theta_{j_{t}}}{(1+\\Theta_{S_{t}})n_{j_{t}0,t}}\\bigg]=\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\sum_{j=1}^{K}\\frac{\\mathbb{1}\\{i_{t}\\in\\{j,0\\}\\}}{n_{j0,t}}\\mathbb{1}\\{j=j_{t}\\}\\bigg]\\leq K(1+\\log T).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "895 and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\frac{\\theta_{i}+\\theta_{j_{t}}}{(1+\\Theta_{S_{t}})n_{i j_{t},t}}\\bigg]=\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\sum_{j=1}^{K}\\frac{\\mathbb{1}\\{i_{t}\\in\\{j,i\\}\\}}{n_{j0,t}}\\mathbb{1}\\{j=j_{t}\\}\\bigg]\\leq K(1+\\log T).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "896 Then, by Cauchy-Schwarz inequality we further get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\bigg[\\Big(\\big(\\mathcal{R}(S_{t}\\hat{\\eta}_{t}^{\\star})-\\mathcal{R}(S_{t},\\theta)\\big)\\wedge1\\big)\\mathbf{1}\\{\\xi\\}\\bigg]}\\\\ &{\\lesssim{\\mathcal O}(\\log T)+\\displaystyle\\sum_{t=1}^{K}\\sqrt{\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\frac{\\big(\\gamma_{t}\\hat{\\eta}_{t}+1\\big)\\theta_{1}\\mathbf{1}\\{i\\in S_{t}\\}_{\\infty}\\big]}{\\theta_{0}+\\Theta_{S_{t}}}}}\\\\ &{\\lesssim{\\mathcal O}(\\log T)+\\displaystyle\\sum_{t=1}^{K}\\sqrt{\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\bigg(\\frac{\\big(\\hat{\\eta}_{t}+\\hat{\\eta}_{t}\\big)}{n_{t}\\hat{\\eta}_{t}}+\\frac{\\big(1+\\phi_{t}\\big)}{n_{t}\\hat{\\eta}_{t}\\alpha}\\bigg)\\frac{\\mathbf{1}\\{i\\in S_{t}\\}}{\\theta_{0}+\\Theta_{S_{t}}}\\bigg]}}\\\\ &{\\lesssim{\\mathcal O}(\\log T)+\\displaystyle\\sum_{t=1}^{K}\\sqrt{\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\frac{\\big(\\gamma_{t}+1\\big)\\theta_{1}\\mathbf{1}\\{i\\in S_{t}\\}_{\\infty}\\big]}{\\theta_{0}+\\Theta_{S_{t}}}}\\sqrt{K\\log T}}\\\\ &{\\lesssim{\\mathcal O}(\\log T)+\\displaystyle\\sum_{t=1}^{K}\\sqrt{\\mathbb{E}\\bigg[\\sum_{t=1}^{T}\\frac{\\theta_{1}\\{i\\in\\xi_{t}\\}_{t}\\big(\\xi+S_{t}\\big)T}{\\theta_{0}+\\Theta_{S_{t}}}\\bigg]}\\sqrt{K\\log T}\\ (\\exp\\exp\\zeta_{t}\\leq1\\log\\phi)}\\\\ &{\\leq{\\mathcal O}(K\\sqrt{T}\\pi\\log T)={\\mathcal O}(K\\sqrt{T}\\log T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "897 where the last inequality is by Jensen\u2019s inequality and the equality by setting $x=2\\log T$ to   \n898 control the probability that $\\varepsilon^{c}$ occurs. This concludes the proof. $\\sqsupset$ ", "page_idx": 27}]