[{"type": "text", "text": "Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexis Bellot\u02da Silvia Chiappa Google DeepMind London, UK ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As many practical fields transition to provide personalized decisions, data is increasingly relevant to support the evaluation of candidate plans and policies (e.g., guidelines for the treatment of disease, government directives, etc.). In the machine learning literature, significant efforts have been put into developing machinery to predict the effectiveness of policies efficiently. The challenge is that, in practice, the effectiveness of a candidate policy is not always identifiable, i.e., not uniquely estimable from the combination of the available data and assumptions about the domain at hand (e.g., encoded in a causal graph). In this paper, we develop graphical characterizations and estimation tools to bound the effect of policies given a causal graph and observational data collected in non-identifiable settings. Specifically, our contributions are two-fold: (1) we derive analytical bounds for general probabilistic and conditional policies that are tighter than existing results, (2) we develop an estimation framework to estimate bounds from finite samples, applicable in higher-dimensional spaces and continuously-valued data. We further show that the resulting estimators have favourable statistical properties such as fast convergence and robustness to model misspecification. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding how to act upon the world around us requires humans and artificial systems to thoughtfully evaluate the effect of different plans, policies, and interventions one might consider. Data and methods that facilitate this process are increasingly relevant for high-stakes decision-making, notably in medical care with the rise of precision medicine [41], but also in education [25], law enforcement [13], public policy [44], and economics [2]. The interventions that a practitioner might consider, could consist of complex policies where a variable is set to follow a conditional or stochastic relationship depending on other variables in the system. For example, policy-makers might contemplate higher taxes on processed foods and stronger campaigns on its health risks targeted to overweight individuals. A sensible question in this context could be \u201cwhat is the effect of a policy that reduces the consumption of processed foods by $50\\%$ in people with a body mass index above $30?^{\\circ}$ . Contrary to atomic (also called hard) interventions that force a particular value, this policy suggests a softer intervention. ", "page_idx": 0}, {"type": "text", "text": "The identifiability and estimation of policies from data is a widely studied problem in the reinforcement learning [37, 39] and causal inference [2, 12, 15, 31, 32, 40] literatures. Despite the generality entailed by many common approaches in these fields, they often rely on an impractical condition: the assumption that the effectiveness of the policy is uniquely computable from the observed data and assumptions about the data generating process. This can be violated in real-world settings subject to unobserved confounding, and lead to the non-identifiability of the effectiveness of a given policy. This holds irrespective of the number of samples collected so that in reality multiple answers for the effect of a policy may be equally plausible. In their seminal work in the early 1990\u2019s, Manski and Robins [27, 33] showed that, nevertheless, the effect of an atomic intervention may always be bounded in a non-trivial interval (i.e., probabilities strictly contained in $[0,1])$ , irrespective of unobserved confounders or the causal structure underlying the variables involved. Causal effects are therefore in general said to be partially identifiable, i.e., one can derive bounds that shrink the range of a priori possible values for a given effect and potentially serve as a useful support for decision-making. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Starting from this insight, the problem of partial identification has been gaining growing attention in the literature. To improve upon these bounds, one common restriction on the data generating mechanism is to assume knowledge of a causal graph describing the phenomenon of interest. Several results have derived progressively tighter bounds by exploiting the independencies in observational and interventional distributions implied by the causal graph [4, 5, 14, 46, 47, 48]. For instance, Balke and Pearl [4] (and subsequent refinements, e.g., [14, 48]) defined polynomial optimization programs to derive bounds that are provably optimal. Tighter analytical bounds have also been derived in selected settings, such as for the \"instrumental variable\" graph [47] and discrete systems with general graphs or equivalence classes [5, 46]. These results, however, are almost exclusively concerned with atomic interventions and small systems of discretely-valued variables. Despite their generality, there is still a gap towards making the partial-identification and estimation of the effectiveness of policies practical. In particular, making scalable inferences from finite samples with continuous outcomes and higher-dimensional covariates is not possible with existing tools. ", "page_idx": 1}, {"type": "text", "text": "This paper aims to provide a novel estimation framework to support decision-making in nonidentifiable settings, overcoming some of these challenges. We introduce several new results for the partial identification and estimation of the effect of stochastic and conditional policies from a combination of observational data and assumptions on the domain, encoded in a causal graph. Our contributions may be summarized as follows. We introduce several graphical criteria to derive new analytical bounds on the effect of policies with continuous outcomes and covariates, that improve upon the non-parametric bounds of [27, 33] and [47]. Given these analytical bounds, we then construct estimators leveraging the double machine learning [10] toolkit for scalable inference, and demonstrate that estimators exhibit favourable statistical properties such as robustness to noise and fast convergence. These results are applicable to arbitrary stochastic or conditional policies that an investigator might design, and high-dimensional covariate spaces. Recent work has made progress is developing powerful estimators for identifiable causal effects [6, 8, 19, 20, 36, 49]. However, these estimators are not applicable in non-identifiable settings. To our knowledge, the proposed estimation machinery is the first result for bounding the effect of policies given a causal diagram. ", "page_idx": 1}, {"type": "text", "text": "Preliminaries. We use capital letters to denote variables $(X)$ , small letters for their values $(x)$ , bold letters for sets of variables $(X)$ and their values ${(x)}$ , and use supp to denote their domains of definition $(x\\in\\operatorname{supp}_{X})$ ). $\\mathbb{I}_{x}(X)$ is the indicator function that equals 1 if the statement $\\{X=x\\}$ is true, and equal to 0 otherwise. ", "page_idx": 1}, {"type": "text", "text": "The framework we use to underpin the estimation of the effect of policies rests on structural causal models (SCMs) [31, Def. 7.1.1]. An SCM $\\mathcal{M}$ is a tuple $\\langle V,U,{\\mathcal{F}},P(U)\\rangle$ , where $V$ is a set of endogenous (observed) variables, $U$ is a set of exogenous latent variables, and $\\mathcal{F}=\\{f_{V}\\}_{V\\in V}$ is a set of functions such that $f_{V}$ determines values of $V$ taking as argument variables $P a_{V}\\subseteq V$ and $U_{V}\\subseteq U$ , i.e. $V\\,\\leftarrow\\,f_{V}\\big({\\cal P}a_{V},U_{V}\\big)$ . Values of $U$ are drawn from an exogenous distribution $P(u)$ . We assume the model to be recursive, i.e. that there are no cyclic dependencies among the variables. Graphically, each $\\mathsf{S C M}\\,\\mathcal{M}$ is associated with a causal diagram $\\mathcal{G}$ over $V$ , where $V\\rightarrow W$ if $V$ appears as an argument of $f_{W}$ in $\\mathcal{M}$ , and $V\\leftarrow\\---\\rightarrow W$ if $U_{V}\\cap U_{W}\\neq\\emptyset$ , i.e. $V$ and $W$ share an unobserved confounder.We will use graph-theoretic family abbreviations to represent graphical relations, e.g. an, de, etc. Moreover, for a causal diagram $\\mathcal{G}$ over $V$ , the $\\mathbf{\\deltaX}$ -lower-manipulation of $\\mathcal{G}$ deletes all those edges that are out of variables in $\\mathbf{\\deltaX}$ , and otherwise keeps $\\mathcal{G}$ as it is. The resulting graph is denoted as $\\mathcal{G}_{\\underline{{X}}}$ . The $\\mathbf{\\deltaX}$ -upper-manipulation of $\\mathcal{G}$ deletes all those edges that are into variables in $\\mathbf{\\deltaX}$ , and otherwise keeps $\\mathcal{G}$ as it is. The resulting graph is denoted as $\\mathcal{G}_{\\overline{{X}}}$ . We use $^{\\cdot d}$ to denote $d$ -separation in causal diagrams [31, Def. 1.2.3]. ", "page_idx": 1}, {"type": "text", "text": "For a sample set $\\mathbf{\\mathcal{D}}\\,:=\\,\\{\\pmb{v}^{(i)}\\}_{i=1,\\dots,n}\\,\\sim\\,P$ , we use $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}}[f(\\pmb{V})]\\,:=\\,(1/n)\\sum_{i=1}^{n}f(\\pmb{v}^{(i)})}\\end{array}$ . We use $\\|f\\|_{P}:=\\sqrt{\\mathbb{E}_{P}[\\{f(V)\\}^{2}]}$ . ${\\hat{f}}-f=o_{P}(r_{n})$ denotes that a function $\\hat{f}$ is a co nsistent estimator of $f$ at a rate $r_{n}$ , ${\\hat{f}}-f=O_{P}(r_{n})$ denotes that it is bounded in probabili tpy at rate $r_{n}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Partial Identification of the Effect of Policies ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A policy $\\pi$ over a subset $X\\,\\subset\\,V$ is a sequence of decision rules or plans $\\pi\\;:=\\;\\{\\pi_{X}\\}_{X\\in{\\pmb X}}$ for determining the assignment of variables X. In its most general form, every \u03c0X : suppX \u02c6suppCX \u00d1 $[0,1]$ is a probability mapping from domain of $C_{X}\\subset C\\subset V$ to the domain of $X$ . ", "page_idx": 2}, {"type": "text", "text": "Qualitatively different types of interventions may be modelled with $\\pi_{X}$ . Specifically, a deterministic intervention setting $X\\,:=\\,g(c_{X})$ based on the values of $C_{X}$ can be encoded as $\\pi_{X}(x\\mid c_{X}):=\\mathbb{1}_{g(c_{X})}(x)$ while a probabilistic intervention may be written $\\pi_{X}^{}(x^{'}|\\ c_{X}):=\\ P_{\\mathrm{induced}\\,\\mathrm{by}\\:\\pi_{X}}(X\\,=\\,x\\mid\\,{\\pmb c}_{x}).$ The intervened model $\\mathcal{M}_{\\pi}$ represents a different regime in which the assignment $\\{f_{X}\\}_{X\\in X}$ is replaced with the assignment induced by $\\pi$ . For an outcome $Y\\in V$ , the interventional ", "page_idx": 2}, {"type": "equation", "text": "$$\nX\\xrightarrow{\\varkappa\\leftarrow-\\ i\\forall i}Y\\qquad X\\xrightarrow{C\\rightarrow\\quad}Y\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: Graphs for Example 1. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "distribution $P_{\\mathcal{M}_{\\pi}}(Y)$ , equivalently $P_{\\pi}(Y)$ , is defined as the distribution over $Y$ in $\\mathcal{M}_{\\pi}$ . It will be useful to adopt the notation $\\textstyle{\\bar{\\pi}}:=\\prod_{X\\in{\\pmb X}}\\pi_{X}(x\\mid{\\pmb c}_{X})$ to denote the product of policy assignments on individual variables subject to int e\u015brvention in $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "Example 1 (Illustration of policies). In the context of a public health program, let $X$ be a measure of an individual\u2019s weekly physical exercise routine, $C$ an individual\u2019s family history of metabolic diseases, and $Y$ hospital admissions related to heart disease. Consider the causal diagram Fig. 1a; currently, an individuals exercise voluntarily, which depends on unobserved factors that could be associated with risk of heart disease. The government is considering an incentive plan aimed at increasing the frequency of exercise depending individual\u2019s family history of metabolic diseases (with some probability). The proposed intervention can be encoded as $\\pi=\\{\\pi_{X}\\}$ and sets the new assignment $\\tilde{f}_{X}$ such that the variable $X$ follows the pre-specified distribution. Graphically, this policy is represented by Fig. 1b (with the new edge corresponding to the implementation of the policy highlighted in blue) and may be evaluated with the following quantity, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\sum_{x,c}\\mathbb{E}_{P_{\\pi}}[Y\\mid x,c]\\pi_{X}(x\\mid c)P_{\\pi}(c).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The inferential challenge is that this quantity is not uniquely computable from $P(x,y,c)$ and $\\mathcal{G}$ , requiring more complex and nuanced notions of policy evaluation. ", "page_idx": 2}, {"type": "text", "text": "Formally, we are interested in the evaluation of the effectiveness of a plan or policy $\\pi$ , acting on a (potentially multivariate) discrete action $\\mathbf{\\deltaX}$ based on values of observed covariates $C\\in\\mathbb{R}^{d}$ , on an outcome of interest $Y\\in[0,1]^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Average treatment effect). The effectiveness of a policy $\\pi$ on $Y$ is $\\mathbb{E}_{P_{\\pi}}[Y]$ . ", "page_idx": 2}, {"type": "text", "text": "From the investigator\u2019s perspective, only the causal diagram $\\mathcal{G}$ of the environment $\\mathcal{M}$ is available. No assumptions about the form or shape of $P(U)$ and $\\mathcal{F}$ are made, but for the structural knowledge encoded in $\\mathcal{G}$ . In general, there might exist multiple SCMs $\\mathcal{M}$ that induce the same causal diagram $\\mathcal{G}$ and entail $P(V)$ but result in different values of $\\mathbb{E}_{P_{\\pi}}[Y]$ . The identification of a set of possible solutions that contain the true value of $\\mathbb{E}_{P_{\\pi}}[Y]$ leads to the notion of partial identification. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Partial Identification). The effectiveness of a policy $\\pi$ is said to be partially identifiable from $\\mathcal{G}$ and $P(v)$ if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi^{\\ell}(P)\\leqslant\\mathbb{E}_{P_{M_{\\pi}}}[Y]\\leqslant\\psi^{u}(P),\\quad{f o r\\,a n y\\,\\mathcal{M}}\\,s u c h\\,t h a t\\,{\\mathcal{G}}_{\\mathcal{M}}={\\mathcal{G}},P_{M}(v)=P(v),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(\\psi^{\\ell},\\psi^{u})$ are functionals of $P$ that are bounded away from 0 and 1, respectively. ", "page_idx": 2}, {"type": "text", "text": "The earliest bound on the effect of policies, the so-called natural bounds, were developed considering discrete atomic interventions, i.e. of the form $\\pi_{X}=\\mathbb{1}_{x}(X),X\\in\\left\\{1,\\ldots,d_{X}\\right\\}$ . ", "page_idx": 2}, {"type": "image", "img_path": "u5enPCwaLt/tmp/3e24ecaae08468459ccc067d5ff5be2e4abc4f51fb5b9e259e715a0d0740c2dc.jpg", "img_caption": ["Figure 2: Graphs used in Sec. 2 and 3. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Natural Bounds). For any x, $\\mathbb{E}_{P}[Y\\mathbb{1}_{x}(X)]\\leqslant\\mathbb{E}_{P_{x}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\mathbb{1}_{x}(X)]+1.$ ", "page_idx": 3}, {"type": "text", "text": "Remarkably, these inequalities hold irrespective of the causal graph of the system $\\mathcal{G}$ ; a result that dates back to [27, 33]. Similarly, we could adapt the underlying proof strategy to derive functionals of $P$ that bound the effect of more general probabilistic or conditional policies $\\pi$ , given as follows. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Natural Policy Bounds (NPB)). For any $\\pi$ , $\\mathbb{E}_{P}[Y\\bar{\\pmb{\\pi}}]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\bar{\\pmb{\\pi}}]\\!+\\!1.$ ", "page_idx": 3}, {"type": "text", "text": "These inequalities also hold irrespective of the causal graph of the system. The natural bounds from [27, 33] are a special case of the natural policy bounds (NPBs) by setting $\\pi:=\\{\\pi_{X}\\}_{X\\in{\\pmb X}}$ where $\\pi_{X}:=\\mathbb{1}_{x}(X)$ , and therefore $\\bar{\\pi}=\\mathbb{1}_{\\pmb{x}}(\\bar{X)}$ . We could show that the NPBs are tight in some cases. For example, in Example 1, provably, no better bounds for $\\mathbb{E}_{P_{\\pi}}[Y]$ than those defined by Prop. 2 could be derived (a result given in Prop. 10 in Appendix B). For other systems that involve variables that are \u201cseparated\u201d in $\\mathcal{G}$ , however, better bounds may be derived by exploiting the implications of the causal graph and definition of the policy on the induced observational and interventional distributions. Consider the following example as a first illustration of this idea. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Tighter bounds with causal diagram). We are interested in evaluating the effect of a policy $\\pi\\;:=\\;{\\bar{\\{\\pi}_{X_{1}}(x_{1}\\mid c),\\mathbb{1}_{x_{2}}(X_{2})\\}}$ from $P(x_{1},c,x_{2},y)$ and $\\mathcal{G}$ in Fig. 2a. In particular, this problem involves a stochastic conditional intervention on $X_{1}$ given $C$ and a deterministic intervention on $X_{2}$ ; both variables having differing dependencies onto the rest of the system. We could show that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\mathbb{E}_{P_{\\pi_{X_{1}}}}[Y]\\geqslant\\mathbb{E}_{P}[Y\\pi_{X_{1}}]=\\psi^{\\ell}(P).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The inequality gives an expression for the bound in terms of $P$ that exploits the fact that intervening on $X_{2}$ does not influence $Y$ , and is tighter that the natural policy lower bound as $\\psi^{\\ell}(P)\\;\\geqslant\\;\\mathbb{E}_{P}[Y\\pi_{X_{1}}]P(X_{2}\\;=\\;x_{2})\\;=\\;\\mathbb{E}_{P}[Y\\pi_{X_{1}}\\mathbb{1}_{x_{2}}(X_{2})]$ $\\left(\\mathrm{=\\NPB}\\right)$ . For the upper bound we could similarly establish that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}\\left[(Y-1)\\pi_{X_{1}}\\right]+1=\\psi^{u}(P).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is smaller than the natural policy bound as $\\psi^{u}(P)\\leqslant\\mathbb{E}_{P}[(Y-1)\\pi_{X_{1}}\\mathbb{1}_{x_{2}}(X_{2})]+1\\;(=\\mathrm{NPB}).$ ", "page_idx": 3}, {"type": "text", "text": "This example, although relatively straightforward, serves to illustrate the potential of causal diagrams (and the constraints they imply on the effect of policies) for defining tighter bounds. ", "page_idx": 3}, {"type": "text", "text": "3 Graphical Criteria for Partial Identification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section aims to consider more general separation statements between variables encoded in a causal diagram and the decomposition they imply to provide a systematic algorithm to bound the effectiveness of policies. We start by introducing the notion of partial adjustment sets (Def. 3) that is applicable with multiple intervention variables. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Partial adjustment set). Let $\\pi\\ :=\\ \\left\\{\\pi_{\\pmb{X}_{1}},\\pi_{\\pmb{X}_{2}}\\right\\}$ be a policy on $\\{X_{1},X_{2}\\}$ with $a$ conditioning set $_{C}$ . A set $W\\subseteq V\\backslash(X_{1}\\cup X_{2}\\cup C\\cup Y)$ is said to be a partial adjustment set for $\\pi_{\\boldsymbol{X}_{2}}$ in G if pY dX2 | W , C, X1qG\u03c0X X2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3. Let $\\pi:=\\left\\{\\pi_{X_{1}},\\pi_{X_{2}}\\right\\}$ be a policy mapping a set of covariates $_{C}$ to a set of treatment variables $\\{X_{1},X_{2}\\}$ . Let $W$ be a partial adjustment set for $\\pi_{X_{2}}$ in $\\mathcal{G}$ . Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}_{P}[Y\\bar{\\pi}\\gamma]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\gamma]+1,}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ {\\mathit{\\check{\\gamma}}(X,C,W)=1/P(X_{2}\\mid C,W).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "text", "text": "In words, partial adjustment sets are designed to exploit the unconfounded status of some intervention variables with respect to the outcome, and could be leveraged to derive tighter bounds when available. ", "page_idx": 3}, {"type": "text", "text": "Example 3 (Tighter bounds with partial adjustment sets). Consider the problem of evaluating the effect of a policy $\\pi\\,:=\\,\\{\\pi_{X_{1}}({\\bar{x}}_{1}\\mid c),\\pi_{X_{2}}(x_{2}\\mid c)\\}$ from $P(x_{1},c,x_{2},w,y)$ and $\\mathcal{G}$ in Fig. 2b. Following Def. 3, we could establish that $W=\\emptyset$ is a valid partial adjustment set for $\\pi_{X_{2}}$ since we can verify that $(Y\\bot_{d}X_{2}\\mid C,X_{2}){_{\\mathcal{G}_{\\pi_{X_{1}}\\underline{{{X}}}_{2}}}}$ . Prop. 3 then gives us a valid expression for bounding the effect of the policy, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{P_{\\pi}}[Y]\\geqslant\\mathbb{E}_{P}\\left[Y\\bar{\\pi}/P(X_{2}\\mid C)\\right]\\qquad}&{\\left(\\mathcal{P}\\mathbb{E}_{P}[Y\\bar{\\pi}],\\mathrm{~the~NPB}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the upper bound we similarly find that, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}\\left[(Y-1){\\bar{\\pi}}/P(X_{2}\\mid C)\\right]+1\\qquad\\quad(\\leqslant\\mathbb{E}_{P}[(Y-1){\\bar{\\pi}}]+1,{\\mathrm{~the~NPB}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Next, we define a second useful notion, so-called partial conditional instrumental variables sets, that can be exploited to evaluate bounds in $_{\\textit{z}}$ -specific distributions $P(\\mathbb{1}_{z}(Z)/P(Z))$ instead of in $P$ as described in Prop. 4. ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Partial conditional instrumental set). A set $Z\\subseteq V$ is said to be a partial instrumental set conditional on $\\boldsymbol{R}$ for a policy $\\pi$ in $\\mathcal{G}$ $i f\\,(Y\\bot_{d}Z\\mid R)_{\\mathcal{G}_{\\pi}}$ . ", "page_idx": 4}, {"type": "text", "text": "For illustration, we give a simple criterion below to show how this subgroup structure could be exploited to derive tighter bounds. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4. Let $_{z}$ be an unconditional partial instrumental set with respect to a policy $\\pi$ , i.e. $(Y\\bar{\\perp}_{-d}Z)_{\\mathcal{G}_{\\pi}}$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z}\\mathbb{E}_{P}[Y\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\operatorname*{min}_{z}\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)]+1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The following example shows that partial adjustment sets and partial conditional instrumental sets may be usefully combined to derive tighter bounds than would be available had each proposition (Props. 3 and 4) been applied in isolation. ", "page_idx": 4}, {"type": "text", "text": "Example 4 (Tighter bounds with partial adjustment and instrumental sets). For this example, consider the evaluation of an atomic intervention $\\pi\\,:=\\,\\{\\mathbb{1}_{x_{1}}(X_{1}),\\mathbb{1}_{x_{2}}(X_{2})\\}$ in the causal diagram $\\mathcal{G}$ given in Fig. 2c. Note that ${\\bar{\\pi}}=\\mathbb{1}_{\\pmb{x}}(X)$ . Following Def. 3, $\\{\\varnothing\\}$ is a valid partial adjustment set for $\\pi_{\\boldsymbol{X}_{1}}$ since we can verify that $(Y\\bot\\!{\\mid}_{d}X_{2}\\mid X_{1}){\\mathcal G}_{\\pi_{X_{1}}\\underline{{{X}}}_{2}}$ . Further, we could verify that $\\{Z\\}$ is a partial instrumental set conditional on $\\left\\{X_{2}\\right\\}$ with respect to $\\pi_{\\boldsymbol{X}_{1}}$ since $(Y\\bot_{d}Z\\mid X_{2})\\dot{{\\mathcal G}}_{\\pi_{X_{1}}}$ . These two separation statements in (manipulated versions of) $\\mathcal{G}$ could be leveraged to derive a tighter bound than previously considered: ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Bounds for the effect of policies Input: Graph $\\mathcal{G}$ , policy $\\pi$ , outcome $Y$ .   \nOutput: Bounds for $\\mathbb{E}_{P_{\\pi}}[Y]$ .   \n1: Let $C=\\cup_{X\\in X}C_{X},\\pmb{K}=V\\backslash(\\pmb{X}\\cup\\pmb{C}\\cup\\cal{Y})$ . /\\* 1. Omit redundant intervention variables. $^{*}/$ 2: Let $R=X\\cap A n(Y)$ in $\\mathcal{G}_{\\pi}$ .   \n3: Let $\\pi_{R}:=\\{\\pi_{X}(c_{X})\\}_{X\\in R}$ .   \n$/{*}\\,2$ . Find partial adjustment sets $W$ . \\*/   \n4: Initialize $W=\\mathcal{D},\\pmb{S}=\\pmb{R}$ .   \n5: for $R\\in R$ do   \n6: Let $\\mathbf{\\boldsymbol{S}}=\\mathbf{\\boldsymbol{S}}\\backslash R$ .   \n7: Let WR \u201c AnpC Y q K in GS,R.   \n8: if $(Y\\bot\\!\\!\\bot\\!_{d}R\\;|\\;C,S,W,W_{R})$ in $\\mathcal{G}_{\\overline{{S}},\\underline{{R}}}$ then 9: $W=W\\cup W_{R}$ .   \n10: end if ", "page_idx": 4}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z}{\\operatorname*{max}}\\,\\mathbb{E}_{P}[Y\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(X_{2},Z)]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]}\\\\ &{\\leqslant\\underset{z}{\\operatorname*{min}}\\,\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(X_{2},Z)]+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To derive bounds in a more systematic fashion, combining the notions developed so far, we present Alg. 1 that recursively seeks to find partial adjustment and partial instrumental sets in an efficient and automatic manner. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, Alg. 1 seeks to recursively simplify the query. First by omitting the intervened variables that have no effect on the outcome; second by finding the set of intervened variables for which a partial adjustment set could be used to ", "page_idx": 4}, {"type": "text", "text": "$/*3$ . Find partial instrumental sets $Z$ . \\*/   \n12: Let ${\\pmb T}=\\{{\\pmb R}:{\\pmb W}_{{\\pmb R}}\\in{\\pmb W}\\},{\\pmb U}={\\pmb R}\\backslash{\\pmb T}$ .   \n13: Initialize $Z=\\mathcal{Q}$ .   \n14: for $Z\\in K$ do   \n15: if $(Y\\bot_{d}Z\\mid W\\backslash Z,Z,T,C)$ in $\\mathcal{G}_{\\overline{{U}}}$ then   \n16: $Z=Z\\cup Z$ .   \n17: end if ", "page_idx": 4}, {"type": "text", "text": "18: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "/\\* 4. Return bounds. \\*/   \n19: Let $\\gamma:={\\bar{\\pi}}_{R}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C).$ .   \n20: Let $\\psi_{z}^{\\ell}:=\\mathbb{E}_{P}[Y\\gamma]$ .   \n21: Let $\\psi_{z}^{\\bar{u}}:=\\mathbb{E}_{P}\\bar{[}(Y\\bar{-}1)\\gamma]+1$ .   \n22: Return bounds: $(\\operatorname*{max}_{z}\\psi_{z}^{\\ell},\\operatorname*{min}_{z}\\psi_{z}^{u})$ . ", "page_idx": 4}, {"type": "text", "text": "tighten the bound, and third by finding the set of variables that act as valid partial instrumental sets to evaluate bounds on the most favorable conditional distributions rather than on the joint distribution. ", "page_idx": 4}, {"type": "text", "text": "In words, Prop. 5 says that for a given causal diagram, policy $\\pi$ , and joint distribution $P(v)$ , the average treatment effect $\\mathbb{E}_{P_{\\pi}}[Y]$ is contained in the bounds produced by Alg. 1. ", "page_idx": 5}, {"type": "text", "text": "Proposition 6. Let $k$ be the number of variables and m be the number of edges in $\\mathcal{G}$ . The run time of Alg. 1 is ${\\mathcal{O}}(k(k^{2}+m))$ . ", "page_idx": 5}, {"type": "text", "text": "Example 5 (Steps of Alg. 1). For this example we consider evaluating a policy $\\pi\\;:=\\;\\{\\pi_{X_{1}}(\\cdot\\;\\mid\\;$ $C_{\\prime}$ , $\\pi_{\\bar{X_{2}}}^{-}(\\cdot\\mid C),\\bar{\\pi}_{X_{3}}(\\cdot\\mid\\bar{C})\\}$ from observational data $P(x_{1},c,x_{2},x_{3},a,w,y)$ compatible with $\\mathcal{G}$ in Fig. 2d by explicitly following Alg. 1. On line 1, we define $C=\\{C\\}$ , ${\\cal K}\\,=\\,\\{{\\cal A},{\\cal W}\\}$ . We start by omitting potentially irrelevant intervention variables by evaluating ${\\pmb R}=\\{X_{1},X_{2}\\}$ and noticing that $\\mathbb{E}_{P_{\\pi}}[Y]\\,=\\,\\mathbb{E}_{P_{\\pi_{R}}}[Y]$ . To find partial adjustment sets in line 4, we evaluate the for loop iterating over $\\boldsymbol{R}$ . For $R\\,=\\,X_{1}$ , we find that ${\\cal S}\\,=\\,{\\o{X_{2}}{,}{W_{X_{1}}}}\\,=\\,\\{A,W\\}$ but the if condition fails as $Y$ is not $d$ -separated from $X_{1}$ given $\\{X_{2},C,A,W\\}$ . We turn onto $R\\,=\\,X_{2}$ , where $S\\,=\\,X_{1},W_{X_{2}}\\,=$ $\\{A,W\\}$ . The if condition is triggered as $(Y\\bot_{d}X_{2}\\mid X_{1},C,A,W)$ in $\\mathcal{G}_{\\pi_{X_{1}},\\underline{{X}}_{2}}$ and therefore we update $W\\,=\\,W_{X_{2}}\\,=\\,\\{A,W\\}$ (giving a partial adjustment set). We continue with line 12, set $\\pmb{T}=\\{X_{2}\\},\\pmb{U}=\\{\\bar{X}_{1}\\}$ , and iterate over $\\bar{\\cal K}\\,=\\,\\{A,\\bar{{\\cal W}}\\}$ in the for loop. For $Z\\,=\\,A$ , we have that $(Y\\bot_{d}A\\mid W,X_{2})$ in $\\mathcal{G}_{\\pi_{X_{1}}}$ , the if condition is triggered and therefore we update ${\\cal Z}\\,=\\,\\{{\\cal A}\\}$ . For $Z\\,=\\,W$ , we find that $(Y\\ \\overset{.}{\\downarrow}\\!\\!\\downarrow\\ W\\ |\\ A,X_{2})$ in $\\mathcal{G}_{\\pi_{X_{1}}}$ and therefore terminate the for loop. Finally putting together the pieces, we evaluate the bounds to be, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z}\\mathbb{E}_{P}[Y\\gamma]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\operatorname*{min}_{z}\\mathbb{E}_{P}[(Y-1)\\gamma]+1,\\quad\\gamma:={\\bar{\\pi}}_{R}\\mathbb{1}_{a}(A)/P(X_{2},A\\mid W,C)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark (Multiple bound expressions). Alg. 1 is designed to make use of partial adjustment and conditional instrumental variable sets but it will return a single bounding expression. In general multiple different expressions could be derived for a given causal diagram and data distribution. For example, given $\\mathcal{G}$ in Fig. 3 and a policy $\\pi$ on $\\{X_{1},X_{2}\\}$ $,X_{2}\\mathbf{\\boldsymbol{\\updownarrow}},\\mathbb{E}_{P}[\\bar{\\boldsymbol{\\pi}}Y/P(X_{2}\\mathbf{\\boldsymbol{\\mid}}\\;W_{1})]$ and $\\bar{\\mathbb{E}}_{P}[\\bar{\\pi}Y/P(\\bar{X_{2}}\\mid W_{2})]$ give valid, but numerically different lower bounds for $\\mathbb{E}_{P_{\\pi}}[Y]$ . The superiority of one bound over another might depend on the ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{X_{2}\\underbrace{{\\bf{\\sigma}}^{\\leftarrow}{W_{1}}\\bf{\\to}}_{\\bf{\\sigma}}}}\\\\ {{\\qquad\\underbrace{{\\bf{\\sigma}}^{\\downarrow}}_{{\\bf{F}}_{\\mathrm{~\\uparrow~}}}\\underbrace{{\\bf{\\sigma}}^{\\downarrow}}_{{\\bf{F}}_{\\mathrm{~\\uparrow~}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: $\\mathcal{G}$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "associations in the underlying distribution $P(v)$ . In this case, we could construct two different SCMs compatible with $\\mathcal{G}$ that entail different orders in the numerical values for the two bounds, such that neither is always optimal. In general, we conjecture that finding an optimal analytical bound (using the proposed procedure) is undecidable from the graph alone. ", "page_idx": 5}, {"type": "text", "text": "The observation in this remark motivates to develop a procedure to automatically enumerate all partial adjustment sets to facilitate the search for tighter bounds. The following proposition adapts the ListSep algorithm by [43] to enumerate all partial adjustment sets. ", "page_idx": 5}, {"type": "text", "text": "Proposition 7 (Enumerating partial adjustment sets). Let $\\pi:=\\left\\{\\pi_{X_{1}},\\pi_{X_{2}}\\right\\}$ be a policy on $\\{X_{1},X_{2}\\}$ with a conditioning set $_{C}$ . All partial adjustment sets may be enumerated in time $O(k(k+m))$ where k are the number of variables and m be the number of edges in $\\mathcal{G}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark (Related methods for policy evaluation). In parallel to the graphical approach for encoding structural assumptions about the domain at hand, a number of works have adopted sensitivity assumptions that quantify the degree of unobserved confounding through various data statistics, such as odds ratios, propensity scores, etc. A rich literature on sensitivity assumptions exists, including Tan\u2019s sensitivity model [38] and Rosenbaum\u2019s sensitivity model [34]. Under these models, [17, 21, 28, 45], among others, present methods that achieve validity and rate properties for the resulting estimators. These approaches start with estimators that optimize the average or conditional treatment effect bounds subject to constraints implied by the sensitivity model. Several of these works leverage Neyman orthogonality techniques to obtain rate guarantees and doubly-robustness properties. For instance, [17, 21, 28] study the estimation of bounds under Tan\u2019s sensitivity model which quantifies the degree of unobserved confounding through odds ratios, and propose estimators with various validity, sharpness, and favourable convergence rate guarantees. In contrast, [45] study the estimation of bounds under Rosenbaum\u2019s sensitivity model also deriving estimators with sharpness and fast convergence guarantees. Some works under these assumptions have also considered policy evaluation (as opposed to the evaluation of atomic interventions) under various sensitivity models in the context of Reinforcement Learning, e.g., [7, 22, 23]. We interpret this line of work as complementary (applicable under different assumptions, i.e., sensitivity models rather than causal diagrams) to the techniques proposed in this paper. ", "page_idx": 5}, {"type": "text", "text": "4 Estimation of the Effect of Policies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section aims to develop an estimation framework for the effect of a policy $\\mathbb{E}_{P_{\\pi}}[Y]$ given finite samples from $P$ that partially identifies its value. The key observation of this section is that multiple characterizations for estimation could be derived for a given bound returned by lines 20-21 in Alg. 1. ", "page_idx": 6}, {"type": "text", "text": "In a first instance, parameterized by the probability ratio given in Alg. 1 defined by $\\gamma=(\\gamma_{1},\\gamma_{2})$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{T^{\\mathrm{PW},\\ell}:=\\mathbb{E}_{P}[\\gamma_{2}Y]}&{(=\\psi_{z}^{\\ell}),}&&{\\gamma_{2}:=\\bar{\\pi}_{U}\\gamma_{1},}&{\\gamma_{1}:=\\bar{\\pi}_{T}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In a second instance, parameterized by a collection of regression parameters $\\pmb{\\mu}=(\\mu_{0},\\mu_{1},\\tilde{\\mu}_{1},\\mu_{2})$ where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{2}:=\\mu_{2}(R,C,W,Z)=\\mathbb{E}_{P}[Y\\mid R,C,W,Z],}\\\\ &{\\tilde{\\mu}_{1}:=\\tilde{\\mu}_{1}(R,C,W,Z)=\\bar{\\pi}_{U}(U\\mid C)\\mu_{2}(R,C,W,Z),}\\\\ &{\\mu_{1}:=\\mu_{1}(T,C,W,Z)=\\mathbb{E}_{P}[\\tilde{\\mu}_{1}(R,C,W,Z)\\mid T,C,W,Z],}\\\\ &{\\mu_{0}:=\\mu_{0}(C,W,Z)=\\sum_{t}\\mu_{1}(t,C,W,Z)\\bar{\\pi}_{T}(t\\mid C).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$T^{\\mathrm{REG},\\ell}:=\\mathbb{E}_{P}[\\mu_{0}(C,W,z)]$ could be shown to equal $\\psi_{z}^{\\ell}$ ", "page_idx": 6}, {"type": "text", "text": "Both formulations define equivalent but different estimation targets for the lower bound3 and may be combined leveraging the double machine learning (DML) toolkit for more efficient and robust inferences [10]. ", "page_idx": 6}, {"type": "text", "text": "The following procedure is the main contribution of this section. It defines a DML estimator for bounding the effectiveness $\\mathbb{E}_{P_{\\pi}}[Y]$ of a policy $\\pi$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 5 (DML Estimator). Given $\\pi$ and $\\mathcal{G}$ , let $\\{R,T,W,C,Z,Y\\}$ be defined as in Alg. 1. Consider a finite sample of data $\\mathcal{D}\\sim P$ , randomly split into $K$ folds. The $k$ \u2019th partition of the sample is denoted $\\overset{\\cdot}{\\mathcal{D}^{(k)}}$ and $\\mathcal{D}^{(-k)}\\,:=\\,\\mathcal{D}\\backslash\\mathcal{D}^{(k)}$ . For each $k$ , learn approximate nuisances $(\\hat{\\gamma}_{k},\\hat{\\pmb{\\mu}}_{k})$ with $D^{(-k)}$ . Then, define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left(\\underset{z}{\\operatorname*{max}}\\ \\hat{T}^{D M L,\\ell}\\ ,\\ \\underset{z}{\\operatorname*{min}}\\ \\hat{T}^{D M L,u}\\ \\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "to be an estimate for the bounds on $\\mathbb{E}_{P_{\\pi}}[Y]$ where, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{T}^{D M L,\\ell}:=\\cfrac{1}{K}\\,\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}_{D^{(k)}}[\\hat{\\gamma}_{2,k}\\{Y-\\hat{\\mu}_{2,k}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{1,k}\\{\\hat{\\tilde{\\mu}}_{1,k}-\\hat{\\mu}_{1,k}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\mu}_{0,k}]}\\\\ &{\\hat{T}^{D M L,u}:=1+\\cfrac{1}{K}\\,\\displaystyle\\sum_{k=1}^{K}\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{2,k}\\{(Y-1)-\\hat{\\mu}_{2,k}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{1,k}\\{\\hat{\\tilde{\\mu}}_{1,k}-\\hat{\\mu}_{1,k}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\mu}_{0,k}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To analyse the error of the DML estimator, we consider the case that nuisances can be estimated consistently. Thus requirement is relatively mild in practice as accurate probability estimation employing off-the-shelf classification and regression methods is feasible in general. Its error with respect to the true bounds is given by the following proposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 8 (Error rates). Suppose the nuisance estimates $(\\hat{\\pmb{\\mu}},\\hat{\\pmb{\\gamma}})$ are $L_{2}$ -consistent and bounded. Then, the error of the DML estimator $\\hat{T}^{D M L}\\in\\{\\hat{T}^{D M L,\\ell},\\hat{T}^{D M L,\\dot{u}}\\}$ in Def. 5 is given as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{T}^{D M L}-T^{D M L}=\\frac{1}{K}\\sum_{k=1}^{K}R_{k}+O_{P}\\Big(\\|\\hat{\\gamma}_{2,k}-\\gamma_{2}\\|\\|\\hat{\\mu}_{2,k}-\\mu_{2}\\|\\Big)+O_{P}\\Big(\\|\\hat{\\gamma}_{1,k}-\\gamma_{1}\\|\\|\\hat{\\mu}_{1,k}-\\hat{\\tilde{\\mu}}_{1,k}\\|\\Big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $R_{k}$ is a random variable that converges to zero at a rate $\\mathcal{O}_{P}(1/\\sqrt{n})$ . ", "page_idx": 6}, {"type": "text", "text": "In words, the DML estimator exhibits a robustness property since the error of $\\hat{T}^{\\mathrm{DML}}$ is bounded in probability at $n^{-1/2}$ rate whenever the nuisances converge at a rate $n^{-1/4}$ . Note that the term ", "page_idx": 6}, {"type": "image", "img_path": "u5enPCwaLt/tmp/b7aa3df286ca934f905e4da6ce62db8c7916aacd31ada84d3fe0976c8343114d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Experimental results on bounding the effectiveness of policies with the proposed estimators. Different rows highlight evaluations on different data generating mechanisms: the first row tests estimation with a partial instrumental set, the second row tests estimation with a partial adjustment set, the third row tests estimation with a high-dimensional partial adjustment set $\\mathbf{\\Delta}[W\\in\\mathbb{R}^{\\breve{1}00}]$ ), and the fourth row tests estimation with a combination of partial adjustment and instrumental sets. ", "page_idx": 7}, {"type": "text", "text": "$\\|\\hat{\\tilde{\\mu}}_{1,k}-\\hat{\\mu}_{1,k}\\|$ quantifies the error in approximating the conditional expectation in Eq. (13) for a given $\\textstyle{\\hat{\\tilde{\\mu}}}_{1}$ estimated at that stage, and that the estimation error of $\\hat{\\gamma}_{2}$ and $\\hat{\\gamma}_{2}$ are equivalent since they are deterministic transformations of each other. The following proposition is a corolloray that shows that the DML estimator is unbiased under misspecification. ", "page_idx": 7}, {"type": "text", "text": "Proposition 9 (Bias under misspecification). Suppose either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\mu}_{2}=\\mu_{2}$ and that either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\tilde{\\mu}}_{1}=\\hat{\\mu}_{1}$ . Then, $\\hat{T}^{D M\\bar{L}}\\in\\{\\hat{T}^{D M L,\\ell},\\hat{T}^{D M\\bar{L},u}\\}$ is an unbiased estimator of the corresponding bound defined in Alg. 1. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section evaluates the quality of policy effect estimation from finite samples. Our goal is to illustrate the computation of bounds and provide empirical evidence of the fast convergence and robustness to misspecification of estimators. ", "page_idx": 7}, {"type": "text", "text": "Finite sample bounds are estimated with gradient boosting classification and regression models (for conditional expectations) or by taking sample averages (for unconditional expectations). We truncated estimates of probability mass functions in the interval r0.01, 0.99s to ensure positivity. We assess the quality of an estimator $\\hat{T}$ by computing the absolute average error (AAE) with respect to (a proxy for) the true bounds (estimated in practice with larger sample sizes), i.e., AAE $\\begin{array}{r}{=|\\hat{T}^{u}-\\operatorname*{min}_{z}\\psi_{z}^{u}|+|\\hat{T}^{\\ell}-\\operatorname*{max}_{z}\\psi_{z}^{\\ell}|}\\end{array}$ . Throughout, we report various statistics: 25th, 50th, 75th percentile, etc., across evaluation runs with ten different random seeds. Further details of the simulations are provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5.1 Synthetic Simulations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The synthetic simulations consider 4 data generating mechanisms constructed according to the graphs in Fig. 4. They highlight the use of partial instrumental sets, partial adjustment sets, high-dimensional partial adjustment sets, and combinations of partial instrumental and adjustment sets of varying dimensionality. The task is to estimate bounds on the effectiveness of a policy $\\pi:=\\left\\{\\pi_{X_{1}},\\dot{\\pi}_{X_{2}}\\right\\}$ defined as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi_{X_{1}}:=\\pi_{X_{1}}(X_{1}=1)=0.5,\\quad\\pi_{X_{2}}:=\\pi_{X_{2}}(X_{2}=1\\mid c)=1/(1-\\exp\\{-c\\}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "That is the policy assigns $X_{1}\\in\\{0,1\\}$ randomly with probability 0.5, and assigns $X_{2}\\in\\{0,1\\}$ as a function of $C$ with the probability of $X_{2}=1$ increasing with the value of $C$ . ", "page_idx": 8}, {"type": "text", "text": "To estimate bounds on $\\mathbb{E}_{P_{\\pi}}[Y]$ , we consider the proposed estimators, labelled: $T^{\\mathrm{PW}}$ , estimated with the nuisances in Eq. (10), $T^{\\mathrm{REG}}$ , estimated with the nuisances in Eq. (11), and $T^{\\mathrm{DML}}$ estimated with the procedure in Def. 5. Our evaluations test performance across 4 different settings designed to highlight various properties. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setting 1: All nuisances estimated correctly. This setting aims to show that all estimators converge to the bound of interest. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setting 2: Nuisances $\\hat{\\gamma}$ are sampled from a uniform distribution to induce misspecification in the estimation of $\\gamma$ . ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setting 3: Nuisances $\\hat{\\pmb{\\mu}}$ are sampled from a uniform distribution to induce misspecification in the estimation of $\\pmb{\\mu}$ . Settings 2 and 3 aim to demonstrate the doubly-robustness property of the DML estimator. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setting 4: Noise $\\epsilon$ is introduced in the estimation of all nuisances $(\\gamma,\\mu)$ to emphasize error due to finite sample variation. Specifically, noise $\\epsilon\\sim\\mathrm{Normal}(n^{-\\alpha},n^{-\\alpha}),\\stackrel{\\cdot}{\\alpha}=$ $\\alpha=1/4$ , that induces a slower rate of convergence as a function of sample size, inspired by [24, 20]. This setting aims to show that the fast convergence behavior of the DML estimator compared to competing estimators. ", "page_idx": 8}, {"type": "text", "text": "The results are given in Fig. $4^{4}$ . We observe that across all data generating mechanisms, estimators improve with the size of the dataset and converge under no misspecification (Setting 1) to the underlying bounds. It is interesting to note also the differing accuracy of estimators in the small sample regime. $T^{\\mathrm{PW}}$ , based the estimation of a ratio of probabilities, can be unstable with low sample sizes if the ratio denominator is estimated to be close to zero while $T^{\\mathrm{REG}}$ , based on a sequence of regression tasks, tends to be better behaved. $T^{\\mathrm{DML}}$ in contrast is constructed as a combination of elements of $T^{\\mathrm{PW}}$ and $T^{\\mathrm{REG}}$ . In particular, note in Def. 5 the use of nuisances $\\pmb{\\mu}$ and $\\gamma$ . As a result, $T^{\\mathrm{DML}}$ has quite a different performance profile. Settings 2 and 3 in Fig. 4 show that the DML estimator $T^{\\mathrm{{\\hat{DML}}}}$ is robust to misspecification in either the nuisances $\\pmb{\\mu}$ or $\\gamma$ that highlights the robustness property. Further, when decaying noise is introduced in the estimation of nuisances (Setting 4), the DML estimator outperforms in general with a faster convergence rate. We also observe that performance remains close to optimal with high-dimensional variables $W$ , demonstrating that the DML estimator provides a practical toolkit for bounding in practice. ", "page_idx": 8}, {"type": "text", "text": "5.1.1 Width of Bounds According to Different Graphical Criteria ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section evaluates the width of the bounds returned by exploiting the different graphical criteria provided in Sec. 3. The simulations are based on the data generating mechanism described by causal diagram illustrated in the fourth row of Fig. 4. We consider evaluating the policy in Eq. (16) and compute the bounds obtained by applying Prop. 2 (most conservative), Prop. 3 (using the partial adjustment set $W$ only), Prop. 4 (using the partial instrumental set $Z$ only), and finally Alg. 1 (that combines all propositions and is the proposed approach). Fig. 5 gives the results over 10 seeds of the data and across multiple data sizes, highlighting the gain achieved by exploiting the causal structure using the proposed approaches. ", "page_idx": 8}, {"type": "image", "img_path": "u5enPCwaLt/tmp/eb9901ed07364916d678d84403e25b53f69b1e27c72b1e7bd49470b91f13d241.jpg", "img_caption": ["Figure 5: Width of bounds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Remark (Actual width of bounds in practice). The majority of our empirical evaluations are spent on evaluating the accuracy of different methods at estimating bounds, without addressing whether the returned bounds are actually informative. In practice, the graph structure can play an important role in tightening bounds but the actual width of the bounds in a particular problem are primarily driven by the distribution of data. Here is an example to make this more concrete. ", "page_idx": 8}, {"type": "text", "text": "For a given policy $\\pi$ , Prop. 2 defines tight bounds (under some circumstances) on $\\mathbb{E}_{P_{\\pi}}[Y]$ . The width of this bound is given by $1-\\mathbb{E}_{P}[\\bar{\\pi}]$ that is ultimately driven by $P(x)$ . This term may therefore evaluate to anything between 0 and 1 depending on $P(x)$ and $\\pi$ . ", "page_idx": 8}, {"type": "image", "img_path": "u5enPCwaLt/tmp/5b29ce0ce52cdfd6584eaee27f1f152a5fb02683d9e6ed268e99be92988d1b48.jpg", "img_caption": ["Figure 6: Health campaign evaluations. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For more complex causal structures, the bounds proposed in Props. 3 and 4 (and Alg. 1) reduce the width of the interval above. Loosely written, from $1-\\mathbb{E}_{P}[\\bar{\\pi}]$ to $1-\\mathbb{E}_{P}[\\bar{\\pi}]\\times\\alpha$ for some $\\alpha$ that is a function of the joint distribution $P$ and the structure of the graph. But again, the actual width of the interval is ultimately determined by the values of $P$ and $\\pi$ that may be large or small depending on the value probabilities involved. ", "page_idx": 9}, {"type": "text", "text": "5.2 Evaluating Health Campaigns ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section illustrates the evaluation of lifestyle recommendations for the mitigation of obesity in individuals from Colombia, Peru and Mexico [30]. The data was collected from anonymous users using a web platform, and includes reported obesity levels measured according to BMI $(Y)$ , age $(A)$ , smoking status $(S)$ , frequency of consumption of high caloric food $(H)$ , whether individuals monitored their calorie intake $(M)$ , family history being overweight $(F)$ , exercise frequency $(E)$ , and time using technology devices $(T)$ . Obesity is a multi-factored medical condition for which several causes have been acknowledged in the literature; causal diagrams relating the variables above have been curated in several related studies [1, 9]. We considered these findings to construct the causal diagram in Fig. 6a that we assumed for this example. ", "page_idx": 9}, {"type": "text", "text": "We aim to study the effect of a health campaign designed to lower the intake of high caloric food $(H)$ and increase the frequency of exercise $(E)$ on obesity levels $(Y)$ . For instance, we could hypothesize that the campaign leads to an increase in the observed proportion of individuals rarely consuming of high caloric food $(H)$ from 0.12 to 0.5 and that of individuals doing exercise $(E)$ regularly from 0.05 to 0.5. These statements could be formulated as a stochastic policy $\\pi^{\\alpha}:=\\{\\pi_{H}^{\\alpha},\\pi_{E}^{\\alpha}\\}$ acting on $H$ and $E$ , with new assignments, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\pi_{H}^{\\alpha}:=\\pi_{H}^{\\alpha}(H=\\mathtt{r a r e1y})=\\alpha,\\quad\\pi_{E}^{\\alpha}:=\\pi_{E}^{\\alpha}(E=\\mathtt{r e g u l a r1y})=\\alpha.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We consider the evaluation of expected BMI levels $\\mathbb{E}_{P_{\\pi^{\\alpha}}}[Y]$ that range from 12 to 50 in the population, with a mean of 29.3. First note that this or other policies acting on $(H,E)$ are not identifiable due to the bi-directed edge $\\{H\\ \\gets-\\ \\to\\ Y\\}$ , but may nevertheless be bounded using Alg. 1. We find that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{t}\\mathbb{E}_{P}[Y\\gamma]\\leqslant\\mathbb{E}_{P_{\\pi^{\\alpha}}}[Y]\\leqslant\\operatorname*{min}_{t}\\mathbb{E}_{P}[(Y-1)\\gamma]+1,\\quad\\gamma:={\\bar{\\pi}}_{\\pi_{\\alpha}}\\mathbb{1}_{t}(T)/P(E,T\\mid A,S,F).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "To illustrate the inference of. bounds with the DML estimator, we consider evaluating policies with $\\alpha\\,=\\,0.2,0.4,0.6,0.8$ . Fig. 6b gives the results. The end-points of the intervals denote estimated lower and upper bounds. We see that policies that promote a healthier lifestyle (larger values of $\\alpha$ ) are expected to reduce obesity levels on average but substantial uncertainty is still expected. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The evaluation of policies is arguably one of the critical ingredients enabling more personalized decision-making systems. When the effect of policies is not identifiable, bounds can provide an effective support for making informed decisions. In this paper we developed partial identification and estimation tools for bounding the effect of a (stochastic or conditional) policy given data and assumptions encoded in a causal graph. We introduced several graphical characterizations that induce tighter bounds, and developed an estimation framework that exhibit robustness to noise and fast convergence. The results of this paper were illustrated through synthetic simulations and a real-world health campaign example for the reduction of obesity levels. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers for helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Steven Allender, Brynle Owen, Jill Kuhlberg, Janette Lowe, Phoebe Nagorcka-Smith, Jill Whelan, and Colin Bell. A community based systems diagram of obesity causes. PloS one, 10(7):e0129683, 2015.   \n[2] Susan Athey and Guido W Imbens. The state of applied econometrics: Causality and policy evaluation. Journal of Economic perspectives, 31(2):3\u201332, 2017.   \n[3] Vahid Balazadeh Meresht, Vasilis Syrgkanis, and Rahul G Krishnan. Partial identification of treatment effects with implicit generative models. Advances in Neural Information Processing Systems, 35:22816\u201322829, 2022.   \n[4] Alexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect compliance. Journal of the American Statistical Association, 92(439):1171\u20131176, 1997.   \n[5] Alexis Bellot. Towards bounding causal effects under Markov equivalence. In The 40th Conference on Uncertainty in Artificial Intelligence. PMLR, 2024.   \n[6] Alexis Bellot, Anish Dhir, and Giulia Prando. Generalization bounds and algorithms for estimating conditional average treatment effect of dosage. arXiv preprint arXiv:2205.14692, 2022.   \n[7] Andrew Bennett and Nathan Kallus. Policy evaluation with latent confounders via optimal balance. Advances in neural information processing systems, 32, 2019.   \n[8] Rohit Bhattacharya, Razieh Nabi, and Ilya Shpitser. Semiparametric inference for causal effects in graphical models with hidden variables. The Journal of Machine Learning Research, 23(1):13325\u201313400, 2022.   \n[9] Bryony Butland, Susan Jebb, Peter Kopelman, Klim McPherson, Sandy Thomas, Jane Mardell, Vivienne Parry, et al. Tackling obesities: future choices-project report, volume 10. Citeseer, 2007.   \n[10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018.   \n[11] David Maxwell Chickering and Judea Pearl. A clinician\u2019s tool for analyzing non-compliance. In Proceedings of the National Conference on Artificial Intelligence, pages 1269\u20131276, 1996.   \n[12] Juan Correa and Elias Bareinboim. A calculus for stochastic interventions: Causal effect identification and surrogate experiments. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 10093\u201310100, 2020.   \n[13] Elizabeth K Drake, Steve Aos, and Marna G Miller. Evidence-based public policy options to reduce crime and criminal justice costs: Implications in washington state. Victims and offenders, 4(2):170\u2013196, 2009.   \n[14] Noam Finkelstein and Ilya Shpitser. Deriving bounds and inequality constraints using logical relations among counterfactuals. In Conference on Uncertainty in Artificial Intelligence, pages 1348\u20131357. PMLR, 2020.   \n[15] Limor Gultchin, Virginia Aglietti, Alexis Bellot, and Silvia Chiappa. Functional causal Bayesian optimization. In Uncertainty in Artificial Intelligence, pages 756\u2013765. PMLR, 2023.   \n[16] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework for bounding confounded causal effects. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12104\u201312112, 2021.   \n[17] Andrew Jesson, S\u00f6ren Mindermann, Yarin Gal, and Uri Shalit. Quantifying ignorance in individual-level causal-effect estimates under hidden confounding. In International Conference on Machine Learning, pages 4829\u20134838. PMLR, 2021.   \n[18] Shalmali Joshi, Junzhe Zhang, and Elias Bareinboim. Towards safe policy learning under partial identifiability: A causal approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13004\u201313012, 2024.   \n[19] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating identifiable causal effects on Markov equivalence class through double machine learning. In International Conference on Machine Learning, pages 5168\u20135179. PMLR, 2021.   \n[20] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating joint treatment effects by combining multiple experiments. In International Conference on Machine Learning, pages 15451\u201315527. PMLR, 2023.   \n[21] Nathan Kallus, Xiaojie Mao, and Angela Zhou. Interval estimation of individual-level causal effects under unobserved confounding. In The 22nd international conference on artificial intelligence and statistics, pages 2281\u20132290. PMLR, 2019.   \n[22] Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. Advances in neural information processing systems, 31, 2018.   \n[23] Nathan Kallus and Angela Zhou. Confounding-robust policy evaluation in infinite-horizon reinforcement learning. Advances in neural information processing systems, 33:22293\u201322304, 2020.   \n[24] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020.   \n[25] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offilne policy evaluation across representations with applications to educational games. In AAMAS, volume 1077, 2014.   \n[26] Henry B Mann and Abraham Wald. On stochastic limit and order relationships. The Annals of Mathematical Statistics, 14(3):217\u2013226, 1943.   \n[27] Charles F Manski. Nonparametric bounds on treatment effects. The American Economic Review, 80(2):319\u2013323, 1990.   \n[28] Miruna Oprescu, Jacob Dorn, Marah Ghoummaid, Andrew Jesson, Nathan Kallus, and Uri Shalit. B-learner: Quasi-oracle bounds on heterogeneous causal effects under hidden confounding. arXiv preprint arXiv:2304.10577, 2023.   \n[29] Kirtan Padh, Jakob Zeitler, David Watson, Matt Kusner, Ricardo Silva, and Niki Kilbertus. Stochastic causal programming for bounding treatment effects. arXiv preprint arXiv:2202.10806, 2022.   \n[30] Fabio Mendoza Palechor and Alexis de la Hoz Manotas. Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from colombia, peru and mexico. Data in brief, 25:104344, 2019.   \n[31] Judea Pearl. Causality. Cambridge university press, 2009.   \n[32] Judea Pearl and James M Robins. Probabilistic evaluation of sequential plans from causal models with hidden variables. In UAI, volume 95, pages 444\u2013453. Citeseer, 1995.   \n[33] James M Robins. The analysis of randomized and non-randomized aids treatment trials using a new approach to causal inference in longitudinal studies. Health service research methodology: a focus on AIDS, pages 113\u2013159, 1989.   \n[34] Paul R Rosenbaum, P Briskman Rosenbaum, and Briskman. Design of observational studies, volume 10. Springer, 2010.   \n[35] Andrea Rotnitzky, James Robins, and Lucia Babino. On the multiply robust estimation of the mean of the $\\mathrm{g}$ -functional. arXiv preprint arXiv:1705.08582, 2017.   \n[36] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In International conference on machine learning, pages 3076\u20133085. PMLR, 2017.   \n[37] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[38] Zhiqiang Tan. A distributional approach for causal inference using propensity scores. Journal of the American Statistical Association, 101(476):1619\u20131637, 2006.   \n[39] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139\u20132148. PMLR, 2016.   \n[40] Jin Tian. Identifying dynamic sequential plans. arXiv preprint arXiv:1206.3292, 2012.   \n[41] Anastasios A Tsiatis, Marie Davidian, Shannon T Holloway, and Eric B Laber. Dynamic treatment regimes: Statistical methods for precision medicine. CRC press, 2019.   \n[42] Mark J van der Laan and Susan Gruber. Targeted minimum loss based estimation of causal effects of multiple time point interventions. The international journal of biostatistics, 8(1), 2012.   \n[43] Benito van der Zander. Algorithmics of Identifying Causal Effects in Graphical Models. PhD thesis, Universit\u00e4t zu L\u00fcbeck, 2020.   \n[44] Evert Vedung. Public policy and program evaluation. Routledge, 2017.   \n[45] Steve Yadlowsky, Hongseok Namkoong, Sanjay Basu, John Duchi, and Lu Tian. Bounds on the conditional and average treatment effect with unobserved confounding factors. arXiv preprint arXiv:1808.09521, 2018.   \n[46] Junzhe Zhang. Designing optimal dynamic treatment regimes: A causal reinforcement learning approach. In International Conference on Machine Learning, pages 11012\u201311022. PMLR, 2020.   \n[47] Junzhe Zhang and Elias Bareinboim. Bounding causal effects on continuous outcome. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12207\u201312215, 2021.   \n[48] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational and experimental data. arXiv preprint arXiv:2110.05690, 2021.   \n[49] Yao Zhang, Alexis Bellot, and Mihaela Schaar. Learning overlapping representations for the estimation of individualized treatment effects. In International Conference on Artificial Intelligence and Statistics, pages 1005\u20131014. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Preliminaries . 15   \nA.2 Related Work 15   \nA.3 Broader Impact Statement 16   \nB Proofs 18   \nB.1 Proofs of statements in Sec. 2 18   \nB.2 Proofs of statements in Sec. 4 25   \nC Details on experiments 30   \nC.1 Simulations . 30   \nC.2 Health Campaign Evaluation 31 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "D NeurIPS Paper Checklist 33 ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries, Related Work, and Impact Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the derivation of results, we use the notion of counterfactuals defined as follows. ", "page_idx": 14}, {"type": "text", "text": "For an SCM $\\mathcal{M}$ , arbitrary subsets of endogenous variables $X,Y$ , the potential outcome of $\\mathbf{Y}$ to intervention $d o(x)$ , denoted by $Y_{x}(u)$ , is the solution for $\\mathbf{\\deltaY}$ with $\\boldsymbol{U}=\\boldsymbol{u}$ in the sub-model $\\mathcal{M}_{x}$ . It can be read as the counterfactual sentence \u201cthe value that $\\mathbf{Y}$ would have obtained in situation $\\boldsymbol{U}=\\boldsymbol{u}$ , had $\\mathbf{\\deltaX}$ been $\\textbf{\\em x}$ .\u201d. Statistically, averaging $\\textbf{\\em u}$ over the distribution $P(u)$ leads to the counterfactual variables ${\\cal Y}_{x}$ . The distribution of the variable $\\mathbf{Y}_{x}$ is denoted $P({\\cal Y}_{x})$ . With this formalism, we can express all the quantities in the main body of this paper in the language of counterfactuals. For instance, the distribution of $\\mathbf{\\deltaY}$ in sub-model $\\mathcal{M}_{x}$ can alternatively be written $P(Y_{x})\\equiv P_{x}(Y)$ . Moreover, $\\mathbb{E}_{P}[f(Y_{x})\\mid z]$ , also written $\\mathbb{E}_{P(\\cdot|z)}[f(Y_{x})]$ , denotes the conditional expectation of $f(\\ensuremath{\\boldsymbol{Y}})$ over $P(y_{x}\\mid z)$ . Similarly, the distribution over $Y$ in the sub-model $\\mathcal{M}_{\\pi}$ can be written ${\\cal P}_{\\mathcal{M}}(Y_{\\pi})$ , or $P(Y_{\\pi})\\equiv P_{\\pi}(Y)$ for short. See [31, Chapter 7] for further context on counterfactuals. ", "page_idx": 14}, {"type": "text", "text": "Definition 6 (The Axioms of Counterfactuals, Chapter 7.3.1 [31]). For any three sets of endogenous variables $X,Y,W$ in a causal model and $x,w$ in the domains of $\\mathbf{\\deltaX}$ and $W$ , the following holds: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Composition: $W_{x}=w$ implies that $Y_{x,w}=Y_{x}$ .   \n\u2022 Effectiveness: $X_{w,x}=x$ .   \n\u2022 Reversibility: $\\boldsymbol{Y_{x,w}}=\\boldsymbol{y}$ and $W_{x,y}=w$ imply that $\\mathbf{Y}_{x}=\\mathbf{y}$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 1 (Soundness and Completeness of the Axioms Theorems 7.3.3, 7.3.6 [31]). The Axioms of counterfactuals are sound and complete for all causal models. ", "page_idx": 14}, {"type": "text", "text": "The following rules to manipulate experimental distributions produced by policies extend the docalculus and will be used for the proof of several theoretical statements [12]. ", "page_idx": 14}, {"type": "text", "text": "Theorem 2 (Inference Rules $\\sigma$ -calculus [12]). Let $\\mathcal{G}$ be a causal diagram compatible with an SCM $\\mathcal{M}$ , with endogenous variables $V$ . For any disjoint subsets $X,Y,Z\\subseteq V$ , two disjoint subsets $T,W\\subseteq V\\backslash(Z\\cup Y)$ (i.e., possibly including $\\mathbf{\\deltaX}$ ), the following rules are valid for any intervention strategies $\\pi_{\\mathbf{X}}$ , $\\pi_{Z}$ , and $\\pi_{Z}^{\\prime}$ such that $\\mathcal{G}_{\\pi_{\\mathbf{x}}\\pi_{Z}},\\mathcal{G}_{\\pi_{\\mathbf{x}}\\pi_{Z}^{\\prime}}$ have no cycles: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Rule $^{\\,l}$ (Insertion/Deletion of observations): $P_{\\pi_{X}}\\!\\left(\\pmb{y}\\mid\\pmb{w},t\\right)=P_{\\pi_{X}}\\!\\left(\\pmb{y}\\mid\\pmb{w}\\right)\\quad i f\\left(\\pmb{T}\\!\\bot_{d}\\pmb{Y}\\mid\\pmb{W}\\right)\\,i n\\,\\mathcal{G}_{\\pi_{X}}.$   \n\u2022 Rule 2 (Change of regimes under observation): $P_{\\pi_{X},\\pi_{Z}}(y\\mid z,w)=P_{\\pi_{X},\\pi_{Z}^{\\prime}}(y\\mid z,w)\\;\\;\\;\\;\\;i f\\left(Y\\bot_{d}Z\\mid W\\right)i n\\,\\mathcal{G}_{\\pi_{X},\\pi_{Z},\\underline{{{Z}}}}\\;a n d\\,\\mathcal{G}_{\\pi_{X},\\pi_{Z}^{\\prime},\\underline{{{Z}}}}$   \n\u2022 Rule $^3$ (Change of regimes without observation): $P_{\\pi_{X},\\pi_{Z}}(y\\mid w)=P_{\\pi_{X},\\pi_{Z}^{\\prime}}(y\\mid w)\\;\\;\\;\\;i f\\left(Y\\bot_{d}Z\\mid W\\right)i n\\;\\mathcal{G}_{\\pi_{X},\\pi_{Z}}\\overline{{{z(W)}}}\\;a n d\\;\\mathcal{G}_{\\pi_{X},\\pi_{Z}^{\\prime}}\\overline{{{z(W)}}}$ ", "page_idx": 14}, {"type": "text", "text": "where $Z(W)$ is the set of elements in $_{z}$ that are not ancestors of $W$ in $\\mathcal{G}_{\\pi_{x}}$ ", "page_idx": 14}, {"type": "text", "text": "A.2 Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Active experimentation by physically manipulating reality is generally not feasible for many consequential applications. This motivates the study of causal effect identification and estimation. The former studies the problem of inferring whether a unique expression or bound of the desired causal effect could be derived from the available data and assumptions. The latter studies the problem of providing efficient estimators from finite samples to compute causal effects or bounds on causal effects in practice. In the following, we review these two lines of work to better contextualize our contribution. ", "page_idx": 14}, {"type": "text", "text": "Partial identification. The natural bounds were derived to demonstrate that useful inference about causal effects could be drawn without making identifying assumptions beyond the observed data [27]. Relatedly, an analysis of bounds of causal effects was also provided for studies with imperfect compliance under a set of instrumental variable assumptions that are not sufficient however to identify the causal effect of interest [33]. These bounds may be written in closed form and have recently been extended as a more general strategy for bounding causal effects given assumptions encoded in causal diagrams in discrete systems [5, 46]. Recent work has also considered bounds in closed form with access to both observational and interventional distributions [18]. These techniques were developed alongside a second line of research that employs a polynomial optimization program to compute causal bounds given a causal diagram [4]. They proposed a family of canonical models parameterized according to the causal diagram, reducing the bounding problem to a series of equivalent linear programs. [11] further used Bayesian techniques to investigate the sharpness of these bounds with regard to the observational sample size. Recently, [14, 48] describe a polynomial programming approach to solve the partial identification for general causal graphs. More recent proposals consider parameterizations in the space of linear combinations of a set of fixed basis functions [29] and neural networks [3, 16]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "In parallel, a number of works have adopted sensitivity assumptions (as an alternative or in combination with a causal diagram) that quantify the degree of unobserved confounding through various data statistics, such as odds ratios, propensity scores, etc. A rich literature on sensitivity assumptions exists, including Tan\u2019s sensitivity model [38] and Rosenbaum\u2019s sensitivity model [34]. Under these models, [17, 21, 28, 45], among others, present methods that achieve validity and rate properties for the resulting estimators. These approaches start with estimators that optimize the average or conditional treatment effect bounds subject to constraints implied by the sensitivity model. Several of these works leverage Neyman orthogonality techniques to obtain rate guarantees and doublyrobustness properties. For instance, [17, 21, 28] study the estimation of bounds under Tan\u2019s sensitivity model which quantifies the degree of unobserved confounding through odds ratios, and propose estimators with various validity, sharpness, and favourable convergence rate guarantees. In contrast, [45] study the estimation of bounds under Rosenbaum\u2019s sensitivity model also deriving estimators with sharpness and fast convergence guarantees. ", "page_idx": 15}, {"type": "text", "text": "While some works have considered policy evaluation under various sensitivity models in the context of Reinforcement Learning, e.g., [7, 22, 23], most works target estimation of bounds on the effect of atomic interventions. We interpret this line of work as complementary (applicable under different assumptions, i.e., sensitivity models rather than causal diagrams) to the techniques proposed in this paper. ", "page_idx": 15}, {"type": "text", "text": "Causal effect estimation. With a causal diagram as input, causal effect estimation has traditionally focused on a subset of identifiable scenarios, relying on assumptions such as backdoor criterion or the availability of adjustment sets. Identification expressions in those cases are given by (sequential) covariate adjustments that have lead to statistically appealing estimators from observational data [32]. Notable examples are doubly robust estimators [10, 35, 42]. Recently these techniques have been extended to settings identifiable from multiple experimental distributions [8, 19, 20]. To our knowledge, no estimation framework specific to the estimation of analytical expressions derived for bounds on the effect of policies or atomic interventions has been developed. ", "page_idx": 15}, {"type": "text", "text": "A.3 Broader Impact Statement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our work investigates the conditions under which policies may be bounded from observational data. Issues of policy evaluation are central to fields of application involved in decision-making as well as AI and ML. We believe that a tool to bound the effect of stochastic and conditional policies in systems with unobserved confounding is an important addition to the scientific toolbox. Reasoning instead without acknowledging for the potential of unobserved confounding may lead researchers to operate on a more heuristical basis. For instance, the hypothetical effect of public policies might be misrepresented if informed by the evaluation of an idealized atomic intervention that is difficult to implement in practice. And, once implemented, might have unintended consequences. With this background, we believe that research on the partial identification and estimation of policies based on stochastic or conditional interventions can help scientists and individuals make more informed decisions. ", "page_idx": 15}, {"type": "text", "text": "In this work, we start from the assumption that a causal diagram that is consistent with the underlying data generating system of interest is available. In general, this requires domain knowledge. While some of the bounds provided do not require full knowledge of the causal graph, whenever a $d\\!.$ - separation is assumed its truth value should be justified by prior knowledge or experiment. It is important also to make the distinction between the task of partial identification, that is inferring an expression for bounds on causal effects, and that of estimation, that is providing efficient estimators from finite samples to compute bounds in practice. This set of results concerns both of these tasks. The first objective of our procedure is to provide an expression for lower and upper bounds, irrespective of the accuracy with which one is able to approximate $P(V)$ from finite samples. The second objective is introduce efficient estimators for bounds on the effect of policies using finite samples from $P(V)$ . In higher-dimensional systems, the computational complexity of estimating the conditional distributions that define lower and upper bounds on causal effects is a substantial challenge. Consequently, practitioners must exercise caution when deploying the proposed method in small sample scenarios where estimators may be inaccurate. Moreover, we have stated our convergence guarantees in the infinite sample limit, without quantifying the finite-sample estimation uncertainty. Finite-sample properties could be explored similarly to [24] given a particular choice of function class to extend our results with high-probability bounds. Finally, we emphasize that simulations on real and synthetic data are provided for illustration purposes only. These results do not recommend or advocate for the implementation of a particular policy, and should be considered in practice in combination with other aspects of the decision-making process. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section presents proofs for theoretical statements in the main body of this paper. ", "page_idx": 17}, {"type": "text", "text": "B.1 Proofs of statements in Sec. 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Prop. 1 restated. For any $\\textbf{\\em x}$ , $\\mathbb{E}_{P}[Y\\mathbb{1}_{x}(X)]\\leqslant\\mathbb{E}_{P_{x}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\mathbb{1}_{x}(X)]+1.$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Consider the derivation of the lower bound. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\mathbf{x}}]=\\underset{x^{\\prime}}{\\sum}\\mathbb{E}_{P}[Y_{x}~|~x^{\\prime}]P(x^{\\prime})}\\\\ &{\\qquad\\qquad\\geqslant\\mathbb{E}_{P}[Y_{x}~|~x]P(x)}\\\\ &{\\qquad\\stackrel{(1)}{=}\\mathbb{E}_{P}[Y~|~x]P(x)}\\\\ &{\\qquad=\\underset{y}{\\sum}y P(y~|~x)P(x)}\\\\ &{\\qquad=\\underset{y,x}{\\sum}y\\mathbb{I}_{x}(x)P(y,x)}\\\\ &{\\qquad=\\mathbb{E}_{P}[\\mathbb{I}_{x}(X)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(1) follows by the composition axiom of counterfactuals. ", "page_idx": 17}, {"type": "text", "text": "Consider the derivation of the upper bound. It holds that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{x}]=\\displaystyle\\sum_{x^{\\prime}}\\mathbb{E}_{P}[Y_{x}\\ |\\ x^{\\prime}]P(x^{\\prime})}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P}[Y_{x}\\ |\\ x]P(x)+\\displaystyle\\sum_{x^{\\prime}\\neq x}\\mathbb{E}_{P}[Y_{x}\\ |\\ x^{\\prime}]P(x^{\\prime})}\\\\ &{\\qquad\\qquad\\overset{(1)}{\\leqslant}\\mathbb{E}_{P}[Y\\ |\\ x]P(x)+\\displaystyle\\sum_{x^{\\prime}\\neq x}P(x^{\\prime})}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P}[Y\\ |\\ x]P(x)+1-P(x)}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P}[Y_{x}(X)]+1-\\mathbb{E}_{P}[\\mathbb{I}_{x}(X)]}\\\\ &{\\qquad=\\mathbb{E}_{P}[(Y-1)\\mathbb{I}_{x}(X)]+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(1) follows by the boundedness of $Y$ , here assumed bounded by 1 for simplifying the derivation. ", "page_idx": 17}, {"type": "text", "text": "Proposition 10. The natural bounds are tight in general. ", "page_idx": 17}, {"type": "text", "text": "Proof. We show this statement by providing a pair of SCMs that agree on the input observational distribution but evaluate to the lower and upperbounds, respectively, specified by the natural bounds. Let $\\mathbb{M}(\\mathcal{G})$ denote the space of SCMs that induce the causal diagram $\\mathcal{G}$ in Fig. 1a. We introduce a pair of SCMs compatible with the causal diagram $\\mathcal{G}$ that evaluate to lower and upper bounds respectively. Let $X$ be binary, $Y\\in[0,1]$ , and $C\\in\\mathbb{R}$ , and consider $\\mathcal{M}_{1},\\mathcal{M}_{2}\\in\\mathbb{M}(\\mathcal{G})$ defined as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{1}:=\\left\\{\\begin{array}{l l}{x:=f_{X}(u_{1})}\\\\ {c:=f_{C}(u_{2})}\\\\ {y:=\\left\\{f_{Y}(x,c,u_{1},u_{2})\\quad\\mathrm{if}\\;x=f_{X}(u_{1}),\\right.}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{2}:=\\left\\{\\begin{array}{l l}{x:=f_{X}(u_{1})}\\\\ {c:=f_{C}(u_{2})}\\\\ {y:=\\left\\{f_{Y}(x,c,u_{1},u_{2})\\quad\\mathrm{if}\\ x=f_{X}(u_{1}),\\right.}\\\\ {\\left.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a $P_{\\mathcal{M}_{1}}(\\pmb{u})~=~P_{\\mathcal{M}_{2}}(\\pmb{u})$ , both SCMs agree on observational distributions $P_{\\mathcal{M}_{1}}(x,y,c)\\;=$ \u201c $P_{\\mathcal{M}_{2}}(x,y,c)$ . However the following derivations show that the interventional expectation $\\mathbb{E}_{P_{\\mathcal{M}_{1}}}[Y\\mid$ $d o(x=1)]$ differs across models: for $\\mathcal{M}_{1}$ equal to the analytical lower bound, and for $\\mathcal{M}_{2}$ equal to the analytical upper bound demonstrating that (in this case) the bound is tight. In particular, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P\\times[\\!\\textbf{\\i}]}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Prop. 2 restated. For any $\\begin{array}{r}{\\pi,\\mathbb{E}_{P}[Y\\bar{\\pi}]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}]+1.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Consider the derivation of the lower bound. Let $\\pi$ denote a policy from an arbitrary set of covariates $_{C}$ to $\\mathbf{\\deltaX}$ . By marginalizing over $X,C$ we find that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\mathbb{E}_{P}[Y_{\\pi}]=\\int\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c]\\prod_{X\\in X}\\pi_{X}(x\\mid c_{X})P(c)\\;d c d x.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$C_{X}$ denotes the subset of $_{C}$ that is used to inform the intervention on $X$ . Moreover, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c]\\stackrel{(1)}{=}\\mathbb{E}_{P}[Y_{x}\\mid c]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x^{\\prime}}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c]P(x^{\\prime}\\mid c)}\\\\ &{\\quad\\quad\\quad\\quad\\geqslant\\mathbb{E}_{P}[Y_{x}\\mid x,c]P(x\\mid c)}\\\\ &{\\quad\\quad\\quad\\stackrel{(2)}{=}\\mathbb{E}_{P}[Y\\mid x,c]P(x\\mid c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(1) holds by Rule 2 in Thm. 2 by swapping the policy $\\pi$ by the do intervention $d o(X=x)$ : given that the policy acts on $\\mathbf{\\deltaX}$ taking as inputs $_{C}$ it is always true that $Y\\bot\\!\\bot\\!X\\mid C$ in $\\mathcal{G}_{\\pi\\underline{{X}}}$ and $\\mathcal{G}_{\\underline{{X}}\\overline{{X}}}$ in any graph $\\mathcal{G}$ . The following equality follows by marginalizing over the domain of $\\mathbf{\\deltaX}$ , which is assumed discrete. (2) follows by the composition axiom of counterfactuals. By combining these two expressions we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}]\\geqslant\\displaystyle\\int\\prod_{X\\in\\pmb X}\\pi(x\\mid c_{X})P(c)\\mathbb{E}_{P}[Y\\mid x,c]P(x\\mid c)\\;d c d x}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\int\\bar{\\pi}y P(y\\mid x,c)P(x\\mid c)P(c)\\;d y d c d x}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{P}[Y\\Bar{\\pi}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consider the derivation of the upper bound. It holds that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c]=\\mathbb{E}_{P}[Y_{x}\\mid c]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x^{\\prime}}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c]P(x^{\\prime}\\mid c)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{P}[Y_{x}\\mid x,c]P(x\\mid c)+\\displaystyle\\sum_{x^{\\prime}\\neq x}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c]P(x^{\\prime}\\mid c)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\overset{(1)}{\\leqslant}\\mathbb{E}_{P}[Y\\mid x,c]P(x\\mid c)+\\displaystyle\\sum_{x^{\\prime}\\neq x}P(x^{\\prime}\\mid c)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{P}[Y\\mid x,c]P(x\\mid c)+1-P(x\\mid c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(1) follows by the boundedness of $Y$ , here assumed bounded by 1 for simplifying the derivation. As above by combining this inequality with the decomposition of the policy effect we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\int\\prod_{X\\in X}\\pi(x\\mid c_{X})P(c)\\left\\{\\mathbb{E}_{P}[Y\\mid x,c]P(x\\mid c)+1-P(x\\mid c)\\right\\}\\;d c d x\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Prop. 3 restated. Let $\\pi\\,:=\\,\\left\\{\\pi_{X_{1}},\\pi_{X_{2}}\\right\\}$ be a policy mapping a set of covariates $_{C}$ to a set of treatment variables $\\{X_{1},X_{2}\\}$ . Let $W$ be a partial adjustment set for $\\pi_{X_{2}}$ in $\\mathcal{G}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathbb{E}_{P}[Y\\bar{\\pi}\\gamma]\\leqslant\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\gamma]+1,}\\\\ {\\gamma:=\\gamma(X,C,W)=1/P(X_{2}\\mid C,W).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Consider the derivation of the lower bound. By marginalizing over $X,C$ we find that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\mathbb{E}_{P}[Y_{\\pi}]=\\int\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c]\\prod_{X\\in X}\\pi_{X}(x\\mid c_{X})P(c)\\;d c d x.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$C_{X}$ denotes the subset of $_{C}$ that is used to inform the intervention on $X$ . Denote $\\textstyle{\\bar{\\pi}}:=\\prod_{X\\in X}\\pi_{X}(x\\mid$ $c_{X})$ . Let $X_{1},X_{2}$ be a partition of $\\mathbf{\\deltaX}$ such that $W\\subseteq V\\backslash(X\\cup C\\cup Y)$ is partial adj u\u015bstment set for $\\pi_{\\boldsymbol{X}_{2}}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{P}[Y_{\\pi}~|~x,c]=\\mathbb{E}_{P}[Y_{\\pi_{1}},\\pi_{S,z}~|~x,c]}\\\\ {\\quad}&{=\\int\\mathbb{E}_{P}[Y_{\\pi_{1},\\pi_{2}}~|~c,w]P(w~|~c)~d w}\\\\ {\\overset{(1)}{=}\\int\\mathbb{E}_{P}[Y_{\\pi_{1}}~|~x_{2},w,c]P(w~|~c)~d w}\\\\ {\\quad}&{=\\int\\sum_{\\tau_{1}^{\\prime}}\\left\\{\\mathbb{E}_{P}[Y_{\\pi_{1}}~|~x_{1}^{\\prime},x_{2},w,c]P(x_{1}^{\\prime}~|~x_{2},w,c)\\right\\}P(w~|~c)~d w}\\\\ {\\quad}&{\\geqslant\\int\\mathbb{E}_{P}[Y_{\\pi_{1}}~|~x_{1},x_{2},w,c]P(x_{1}~|~x_{2},w,c)P(w~|~c)~d w}\\\\ {\\quad}&{=\\int\\mathbb{E}_{P}[Y~|~x_{1},x_{2},w,c]P(x_{1}~|~x_{2},w,c)P(w~|~c)~d w.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(1) follows by the definition of $X_{2}$ as a partial adjustment set: it holds that $(Y\\perp_{d}{\\pmb X}_{2}\\mid{\\pmb W},C,{\\pmb X}_{1})$ in $\\mathcal{G}_{\\overline{{\\mathbf{X}_{1}}},\\underline{{X}}_{2}}$ which induces the equality $\\vec{\\mathbb{E}_{P}}[Y_{x_{1},x_{2}}\\mid c,w]=\\mathbb{E}_{P}[Y_{x_{1}}\\mid x_{2},w,c]$ . The last equality follows by the composition axiom of counterfactuals. Combining this expression with the decomposition of the policy effect we get, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}]\\geqslant\\displaystyle\\int\\prod_{X\\in X}\\pi(x\\mid c_{X})P(c)\\mathbb{E}_{P}[Y\\mid x_{1},x_{2},w,c]P(x_{1}\\mid x_{2},w,c)P(w\\mid c)\\;d w d c}\\\\ &{\\qquad\\quad=\\displaystyle\\int\\bar{\\pi}y P(y\\mid x_{1},x_{2},w,c)P(x_{1}\\mid x_{2},w,c)P(w\\mid c)P(c)\\;d y d c d x}\\\\ &{\\qquad\\quad=\\mathbb{E}_{P}[Y\\bar{\\pi}/P(X_{2}\\mid W,C)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the upper bound, consider the following derivation, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi},\\tau]}\\\\ &{=\\displaystyle\\int\\mathbb{E}_{P}[Y_{\\pi},\\lfloor x_{2},w_{\\tau}\\rfloor P(w\\mid c)\\,d w}\\\\ &{=\\displaystyle\\int\\sum_{\\tau_{i}^{\\prime}}[\\mathbb{E}_{P}[Y_{\\pi_{1}}\\mid x_{1}^{\\prime},x_{2},w_{\\tau}\\]P(w_{1}^{\\prime}\\mid x_{2},w,c)]P(w\\mid c)\\,d w}\\\\ &{=\\displaystyle\\int\\left\\{\\mathbb{E}_{P}[Y_{\\pi_{1}}\\mid x_{1},x_{2},w,c]P(x_{1}\\mid x_{2},w,c)P(w\\mid c)\\right.}\\\\ &{\\left.\\ \\ \\ \\ \\sum_{\\tau_{i}^{\\prime}}\\sum_{\\ell=1}^{\\infty}\\mathbb{E}_{P}[Y_{\\pi_{1}}\\mid x_{1}^{\\prime},x_{2},w,c]P(x_{1}^{\\prime}\\mid x_{2},w,c)P(w\\mid c)\\right\\}\\,d w}\\\\ &{\\stackrel{}{\\le}\\displaystyle\\int\\left\\{\\mathbb{E}_{P}[Y_{\\pi_{1}}\\mid x_{1}^{\\prime},x_{2},w,c]P(x_{1}\\mid x_{2},w,c)P(w\\mid c)+\\displaystyle\\sum_{\\tau_{i}^{\\prime}\\in\\mathcal{T}_{\\pi_{i}}}P(x_{1}^{\\prime}\\mid x_{2},w,c)P(w\\mid c)\\right\\}\\,d w}\\\\ &{=\\displaystyle\\int\\left\\{\\mathbb{E}_{P}[Y\\mid x_{1},x_{2},w,c]P(x_{1}\\mid x_{2},w,c)P(w\\mid c)+(1-P(x_{1}\\mid x_{2},w,c))P(w\\mid c)\\right\\}\\,d w}\\\\ &{=\\displaystyle\\int\\left\\{\\mathbb{E}_{P}[Y\\mid x_{1},x_{2},w,c]P(x_{1}\\mid x_{2},w,c)P(w\\mid c)\\,d w+1-\\int P(x_{1}\\mid x_{2},w,c)P(w\\mid c)\\right\\}\\,d w}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The inequality follows from the boundedness of $Y$ and the rest of the arguments are analogous to the lower bound derivation. Combining this expression with the decomposition of the policy effect implies that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}/P(X_{2}\\mid W,C)]+1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Prop. 4 restated. Let $Z$ be an unconditional partial instrumental set with respect to a policy $\\pi$ , i.e. $(Y\\bar{\\perp}_{-d}Z)_{\\mathcal{G}_{\\pi}}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{\\pi}}[Y]\\geqslant\\underset{z}{\\operatorname*{max}}\\,\\mathbb{E}_{P}[Y\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)]}\\\\ &{\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\underset{z}{\\operatorname*{min}}\\,\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)]+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Consider the derivation of the lower bound. Let $\\pi$ denote a policy from an arbitrary set of covariates $_{C}$ to $\\mathbf{\\deltaX}$ . Given that $(Y\\bot\\|_{-d}Z)_{\\mathcal{G}_{\\pi}}$ , by marginalizing over $X,C$ we find that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\mathbb{E}_{P}[Y_{\\pi}]=\\mathbb{E}_{P}[Y_{\\pi}\\mid z]=\\int\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c,z]\\prod_{X\\in X}\\pi_{X}(x\\mid c_{X})P(c\\mid z)\\;d c d x.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$C_{X}$ denotes the subset of $_{C}$ that is used to inform the intervention on $X$ . Moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c,z]\\stackrel{(1)}{=}\\mathbb{E}_{P}[Y_{x}\\mid c,z]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x^{\\prime}}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c,z]P(x^{\\prime}\\mid c,z)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geqslant\\mathbb{E}_{P}[Y_{x}\\mid x,c,z]P(x\\mid c,z)}\\\\ &{\\quad\\quad\\quad\\quad\\stackrel{(2)}{=}\\mathbb{E}_{P}[Y\\mid x,c,z]P(x\\mid c,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(1) holds by Rule 2 in Thm. 2 by swapping the policy $\\pi$ by the do intervention $d o(X=x)$ : given that the policy acts on $\\mathbf{\\deltaX}$ taking as inputs $_{C}$ it is always true that $Y\\bot\\!\\bot\\!X\\mid C$ in $\\mathcal{G}_{\\pi\\underline{{X}}}$ and $\\mathcal{G}_{\\underline{{X}}\\overline{{X}}}$ in any graph $\\mathcal{G}$ . The following equality follows by marginalizing over the domain of $\\mathbf{\\deltaX}$ , which is assumed discrete. (2) follows by the composition axiom of counterfactuals. By combining these two ", "page_idx": 20}, {"type": "text", "text": "expressions we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{P}[Y_{\\pi}]\\geqslant\\int\\prod_{X\\in X}\\pi(x\\mid c_{X})P(c\\mid z)\\mathbb{E}_{P}[Y\\mid x,c,z]P(x\\mid c,z)\\;d c d x}\\\\ {\\displaystyle\\qquad=\\int\\bar{\\pi}y P(y\\mid x,c,z)P(x\\mid c,z)P(c\\mid z)\\;d y d c d x}\\\\ {\\displaystyle\\qquad=\\mathbb{E}_{P}[Y\\bar{\\pi}\\mid z]}\\\\ {\\displaystyle\\qquad=\\mathbb{E}_{P}[Y\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And since the l.h.s. does not depend on $Z$ we can further tighten the bound by writing, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[Y_{\\pi}]\\geqslant\\operatorname*{max}_{z}\\mathbb{E}_{P}[Y\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider the derivation of the upper bound. It holds that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid x,c,z]=\\mathbb{E}_{P}[Y_{x}\\mid c,z]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x^{\\prime}}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c,z]P(x^{\\prime}\\mid c,z)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{P}[Y_{x}\\mid x,c,z]P(x\\mid c,z)+\\displaystyle\\sum_{x^{\\prime}\\neq x}\\mathbb{E}_{P}[Y_{x}\\mid x^{\\prime},c,z]P(x^{\\prime}\\mid c,z)}\\\\ &{\\quad\\quad\\quad\\quad\\overset{(1)}{\\leqslant}\\mathbb{E}_{P}[Y\\mid x,c,z]P(x\\mid c,z)+\\displaystyle\\sum_{x^{\\prime}\\neq x}P(x^{\\prime}\\mid c,z)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{P}[Y\\mid x,c,z]P(x\\mid c,z)+1-P(x\\mid c,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(1) follows by the boundedness of $Y$ , here assumed bounded by 1 for simplifying the derivation. As above by combining this inequality with the decomposition of the policy effect we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{P_{\\pi}}[Y]\\leqslant\\int\\prod_{X\\in X}\\pi(x\\mid c_{X})P(c\\mid z)\\left\\{\\mathbb{E}_{P}[Y\\mid x,c,z]P(x\\mid c,z)+1-P(x\\mid c,z)\\right\\}\\;d c d x}\\\\ {\\quad\\quad=\\mathbb{E}_{P}[Y\\Bar{\\pi}\\mid z]+1-\\mathbb{E}_{P}[\\Bar{\\pi}\\mid z]}\\\\ {\\quad\\quad=\\mathbb{E}_{P}[(Y-1)\\Bar{\\pi}\\mid z]+1}\\\\ {\\quad\\quad=\\mathbb{E}_{P}[(Y-1)\\Bar{\\pi}\\Bar{1}_{z}(Z)/P(Z)]+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And since the l.h.s. does not depend on $Z$ we can further tighten the bound by writing, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[Y_{\\pi}]\\geqslant\\operatorname*{min}_{z}\\mathbb{E}_{P}[(Y-1)\\bar{\\pi}\\mathbb{1}_{z}(Z)/P(Z)]+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Prop. 5 restated. Alg. 1 is sound. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. For the soundness of Alg. 1, we will consider each operation in turn and show that recovered adjustment sets and conditional instrumental sets lead to a valid bound. ", "page_idx": 21}, {"type": "text", "text": "1. Omit redundant intervention variables. For a policy $\\pi~:=~\\{\\pi_{X}(\\pmb{c}_{X})\\}_{X\\in\\pmb{X}}$ denote $\\pi_{R}\\::=\\:$ $\\{\\pi_{X}(\\mathbf{c}_{X})\\}_{X\\in R},R\\,\\subseteq\\,X$ . Let $R\\,=\\,X\\cap A n(Y)$ in $\\mathcal{G}_{\\pi}$ . By Rule 3 of the $\\sigma$ -calculus we have that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}}[Y]=\\mathbb{E}_{P_{\\pi_{R}}}[Y],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $Y\\bot\\!\\bot\\!X\\backslash R$ in $\\mathcal{G}_{\\pi,\\overline{{X\\backslash R}}}$ and $\\mathcal{G}_{\\pi_{R},\\overline{{X\\backslash R}}}$ as there are no directed paths from $X\\backslash R$ to $Y$ by definition of $\\boldsymbol{R}$ . The first two lines therefore reduce the number of intervention variables. We proceed to bound the equivalent $\\mathbb{E}_{P_{\\pi_{R}}}[Y]$ . ", "page_idx": 21}, {"type": "text", "text": "2. Find partial adjustment sets $W$ . Line 4-11 consider finding partial adjustment sets by traversing the set of treatment variables $R\\in R$ . Recall that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi_{R}}}[Y]=\\mathbb{E}_{P}[Y_{\\pi_{R}}]=\\int\\mathbb{E}_{P}[Y_{\\pi_{R}}\\mid r,c]\\prod_{X\\in R}\\pi_{X}(x\\mid c_{X})P(c)\\;d c d r.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote $\\textstyle{\\bar{\\pi}}_{R}\\;:=\\;\\prod_{X\\in{\\pmb R}}\\pi_{X}(x\\;\\mid\\;{\\pmb c}_{X})$ . In words lines 4-11 iteratively considers the existence of separators sets  be\u015btween $R\\in R$ and $Y$ to reduce the scope of the policy on $Y$ , i.e. $Y_{\\pi_{R}}$ . By [43, Lemma 3.18.], if there exists a separator $W$ between two sets of variables $\\mathbf{\\deltaX}$ and $\\mathbf{Y}$ such that $S\\subseteq W\\subseteq K$ in a graph $\\mathcal{G}$ , i.e. $X\\bot\\!\\!i_{d}Y\\mid W$ in $\\mathcal{G}$ , then $W=A n(X\\cup Y\\cup S)\\cap K$ is also a valid separator. The definition of $W_{R}$ in line 7 aims to leverage this result to check for the existence of separators in our case. ", "page_idx": 22}, {"type": "text", "text": "Consider a $R\\in R$ , let $=R\\backslash R,K=V\\backslash(X\\cup C\\cup Y)$ . In particular, if there exist a set $\\{C,R\\}\\subset$ $W\\subset K$ such that $(Y\\bot_{d}R\\mid W)$ in $\\mathcal{G}_{\\pi_{S},\\underline{{R}}}$ then $W_{R}=A\\bar{n}(R\\cup Y\\cup C)\\cap K=A n(Y\\cup C)\\cap K$ in $\\mathcal{G}_{\\pi_{S}}$ satisfies $(Y\\bot_{d}R\\mid W_{R})$ in $\\mathcal{G}_{\\pi_{S},\\underline{{R}}}$ , i.e. $W_{R}$ is a separator if one exists. Note that $W_{R}$ does not include any member of $\\{C,R,Y\\}$ . Assume this separation holds and that the if statement in line 8 is triggered. Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{P}[Y_{\\pi_{R}}\\mid{\\boldsymbol r},c]=\\mathbb{E}_{P}[Y_{r}\\mid c]}\\\\ {\\displaystyle=\\mathbb{E}_{P}[Y_{s,r}\\mid c]}\\\\ {\\displaystyle=\\int\\mathbb{E}_{P}[Y_{s,r}\\mid c,w_{R}]P(w_{R}\\mid c)\\;d w_{R}}\\\\ {\\displaystyle=\\int\\mathbb{E}_{P}[Y_{s}\\mid r,c,w_{R}]P(w_{R}\\mid c)\\;d w_{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last equality holds by assumption $(Y\\bot_{d}R\\mid W_{R})$ in $\\mathcal{G}_{\\overline{{S}},\\underline{{R}}}$ . ", "page_idx": 22}, {"type": "text", "text": "Now considering the term $\\mathbb{E}_{P}[Y_{s}\\mid r,c,\\pmb{w}_{R}]$ , for the second pass through the for loop, we can see that if the $d$ -separation statement in the if condition is fulfliled we can further reduce the scope of the intervention on $\\boldsymbol{S}$ . In particular, for $R^{\\prime}\\in R\\backslash R$ , assuming that the if statement is triggered for $W_{R^{\\prime}}$ we have that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{P}[Y_{s}\\ |\\ r,c,w_{R}]=\\int\\mathbb{E}_{P}[Y_{s}\\ |\\ r,c,w_{R},w_{R^{\\prime}}]P(w_{R^{\\prime}}\\ |\\ w_{R},c)\\ d w_{R^{\\prime}}}}\\\\ &{}&{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\int\\mathbb{E}_{P}[Y_{s\\backslash r^{\\prime}}\\ |\\ r^{\\prime},r,c,w_{R},w_{R^{\\prime}}]P(w_{R^{\\prime}}\\ |\\ w_{R},c)\\ d w_{R^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Upon reaching the end of the for loop we have for $W$ defined in line 9, ${\\cal T}=\\{R:{\\cal W}_{R}\\in{\\cal W}\\},{\\cal U}=$ $_{R\\setminus T}$ that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[Y_{\\pi}\\mid r,c]=\\int\\mathbb{E}_{P}[Y_{u}\\mid t,c,w]P(w\\mid c)\\ d w.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. Find partial instrumental sets $Z$ . Starting line 12, we look for partial conditional instrumental sets. Consider first the case that a potential instruments $Z\\in W$ is evaluated in the for loop. On line 15, assume that the if statement is triggered, i.e. $(Y\\perp_{d}Z\\mid W\\backslash Z,T)$ in $\\mathcal{G}_{\\overline{{U}}}$ It then holds that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid r,c]:=\\displaystyle\\int\\mathbb{E}_{P}[Y_{u}\\mid w,t,c]P(w\\mid c)~d w}\\\\ &{\\qquad\\qquad\\quad\\qquad=\\displaystyle\\int\\mathbb{E}_{P}[Y_{u}\\mid w\\backslash z,z,t,c]\\displaystyle\\int_{z}P(w\\mid c)~d w}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int\\mathbb{E}_{P}[Y_{u}\\mid w\\backslash z,z,t,c]P(w\\backslash z\\mid c)~d w\\backslash z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In turn, for the case that some $Z^{\\prime}\\notin W$ is evaluated in the for loop, if $(Y\\bot\\!\\!\\!\\mid_{\\!\\!d}Z^{\\prime}\\mid W,T)$ in $\\mathcal{G}_{\\overline{{U}}}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[Y_{\\pi}\\mid r,c]:=\\displaystyle\\int\\mathbb{E}_{P}[Y_{u}\\mid w,t,c]P(w\\mid c)~d w}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\int\\mathbb{E}_{P}[Y_{u}\\mid w,t,z^{\\prime},c]P(w\\mid c)~d w}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the $_{z}$ obtained in line 15, we derive that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[Y_{\\pi}\\mid r,c]=\\int\\mathbb{E}_{P}[Y_{u}\\mid w\\backslash z,z,t,c]P(w\\backslash z\\mid c)~d w\\backslash z.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "4. Return bounds. We now proceed to use the expression above to lower and upper bound the effect of interest. In particular, for the lower bound we have that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb E_{P}[Y_{\\pi}\\mid x,c]=\\int\\mathbb E_{P}[Y_{u}\\mid w\\langle z,z,t,c]P(w\\backslash z\\mid d w\\backslash z}}\\\\ &{}&{\\qquad=\\displaystyle\\int\\sum_{u^{\\prime}}\\mathbb E_{P}[Y_{u}\\mid u^{\\prime},w\\backslash z,z,t,c]P(u^{\\prime}\\mid w\\backslash z,z,t,c)P(w\\backslash z\\mid c)\\;d w\\backslash z}\\\\ &{}&{\\qquad\\gg\\displaystyle\\int\\mathbb E_{P}[Y_{u}\\mid u,w\\backslash z,z,t,c]P(u\\mid w\\backslash z,z,t,c)P(w\\backslash z\\mid c)\\;d w\\backslash z}\\\\ &{}&{\\qquad=\\displaystyle\\int\\mathbb E_{P}[Y\\mid u,w\\backslash z,z,t,c]P(u\\mid w\\backslash z,z,t,c)P(w\\backslash z\\mid c)\\;d w\\backslash z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By combining the inequality above with the decomposition of the policy effect we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\lefteqn{\\sum_{P\\pi}\\bigl[Y\\bigr]}}\\\\ {\\geqslant\\displaystyle\\int\\prod_{X\\in R}\\pi(x\\mid c_{X})P(c)\\mathbb{E}_{P}[Y\\mid u,w\\rangle z,z,t,c]P(u\\mid w\\mid z,z,t,c)P(w\\mid z\\mid c)\\,d c d r d w\\colon z}\\\\ {=\\displaystyle\\int\\bar{\\pi}_{R}y P(y\\mid r,w\\rangle z,z,c)P(u\\mid w\\cup z,z,t,c)P(w\\backslash z\\mid c)P(c)\\,d c d r d w\\colon z d y}\\\\ {=\\displaystyle\\int\\bar{\\pi}_{R}y P(y\\mid r,w\\vee z,z,c)P(u\\mid w\\backslash z,z,t,c)\\frac{P(t,z\\mid w\\backslash z,c)}{P(t,z\\mid w\\backslash z,c)}P(w\\backslash z\\mid c)P(c)\\mathbb{I}_{z}(z)\\,d c d r d w\\colon z}\\\\ {=\\displaystyle\\int\\bar{\\pi}_{R}y P(y,r,w\\backslash z,z,c)\\frac{1_{z}(z)}{P(t,z\\mid w\\backslash z,c)}\\,d c d r d w\\backslash z d y d z}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\gamma:={\\bar{\\pi}}_{R}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C)$ . Note that we have used the definition ${\\cal R}={\\cal T}\\cup{\\cal U}$ . In turn, for the upper bound we have that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{T}\\langle Y_{w}\\mid\\mathcal{X}_{\\tau}\\rangle=\\Bigg\\lceil\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\mathrm{P}_{\\varepsilon}\\big\\rceil\\vee\\big\\vert w\\big\\vert\\le\\varepsilon\\Big\\rceil\\sqrt{w}\\big\\vert\\lesssim\\varepsilon\\Big\\rceil\\Big\\langle w\\Big\\vert\\lesssim\\underline{{\\theta}}}\\\\ &{\\quad-\\int\\Bigg\\lfloor\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\mathrm{P}_{\\varepsilon}\\big\\rceil\\Big\\langle w\\big\\vert w\\big\\vert,w,w,\\varepsilon,t\\big\\rangle\\Big\\vert\\Big\\langle w\\big\\vert w^{\\top}\\big\\vert w^{\\top}\\big\\vert w^{\\top}\\big\\vert w^{\\top}\\big\\vert\\ e\\Big\\rangle\\,d w\\lesssim0\\,,}\\\\ &{\\quad-\\left\\lceil\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert Y_{w}\\big\\vert,\\ln w,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert e\\right\\rceil\\sqrt{\\big(\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert w^{\\top}\\big\\vert,w,t\\big)}r\\{w\\in\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert P(w)\\in\\mathrm{\\ensuremath{Z}}_{T}\\big\\vert\\ e\\}\\,d w\\right\\rceil z}\\\\ &{\\quad+\\Bigg\\lceil\\sum_{w\\in\\mathbb{Z}_{0}}\\ensuremath{\\mathbb{E}}_{T}\\big\\vert w_{\\tau}\\big\\vert w_{\\tau}^{\\top}\\big\\vert w^{\\top}\\big\\vert w^{\\top}\\big\\vert\\mathrm{\\ensuremath{\\mathbb{Z}}}_{\\tau}\\big\\vert e\\Big\\rceil\\,,}\\\\ &{\\quad\\leqslant\\int\\mathbb{E}_{T}\\big\\vert Y_{w}\\big\\vert w,w,\\varepsilon,t\\big\\vert e\\vert P(w)\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{\\tau}\\big\\vert e\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert P(w)\\,z\\big\\vert\\in\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert w\\big\\vert\\,z\\big\\vert}\\\\ &{\\quad+\\int\\sum_{w\\in\\mathbb{Z}}P(w^{\\top}\\big\\vert w^{\\top}\\big\\vert w^{\\top},z,t,e)P(w)\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{\\tau}\\big\\vert e\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert}\\\\ &{\\quad-\\int\\sum_{w\\in\\mathbb{Z}}P(w^{\\top}\\big\\vert w^{\\top}\\big\\vert w^{\\top},z,t,e)P(w)\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{\\tau}\\big\\vert e\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{T}\\big\\vert e\\,\\mathrm{\\ensuremath{\\mathbb{Z}}}_{\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By combining the inequality above with the decomposition of the policy effect we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{\\pi}}[Y]}\\\\ &{\\leqslant1+\\displaystyle\\int\\prod_{X\\in R}\\pi(x\\mid c_{X})P(c)\\mathbb{E}_{P}[Y-1\\mid u,w\\backslash z,z,t,c]P(u\\mid w\\backslash z,z,t,c)P(w\\backslash z\\mid c)\\;d c d r d u}\\\\ &{\\,=1+\\displaystyle\\int\\bar{\\pi}_{R}(y-1)P(y\\mid r,w\\backslash z,z,c)P(u\\mid w\\backslash z,z,t,c)P(w\\backslash z\\mid c)P(c)\\;d c d r d w\\backslash z d y}\\\\ &{\\,=1+\\displaystyle\\int\\bar{\\pi}_{R}(y-1)P(y,r,w\\backslash z,z,c)\\frac{\\mathbb{1}_{z}(z)}{P(t,z\\mid w\\backslash z,c)}\\;d c d r d w\\backslash z d y d z}\\\\ &{\\,=1+\\mathbb{R}_{P}[(Y-1)\\gamma]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\gamma:={\\bar{\\pi}}_{R}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C)$ . Note that we have used the definition ${\\cal R}={\\cal T}\\cup{\\cal U}$ . ", "page_idx": 24}, {"type": "text", "text": "Prop. 6 restated. Let k be the number of variables and m be the number of edges in $\\mathcal{G}$ . The run time of Alg. 1 is ${\\mathcal{O}}(k(k^{2}+m))$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $k$ be the number of variables and $m$ be the number of edges in $\\mathcal{G}$ . Operations in Alg. 1, such as computing ancestors (e.g. lines 2,7), could be done in ${\\mathcal{O}}(k^{2})$ time, e.g. with a Breadth-First Search algorithm. Checking for $d\\!.$ -separation is commonly done with the Bayes-Ball algorithm that can be implemented with a reachability search method, e.g. [43, Sec. 3.2.1], and could be done in $O(k+m)$ time by [43, Prop. 3.17]. The sets $\\boldsymbol{R}$ and $_{z}$ have size at most $k$ , so the for loops in lines 5 and 14 are executed at most $k$ times each. Combining these we get that Alg. 1 requires $\\mathcal{O}(\\dot{k}(k^{2}+m))$ time to return the bounds. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Prop. 7 restated. Let $\\pi:=\\left\\{\\pi_{X_{1}},\\pi_{X_{2}}\\right\\}$ be a policy on $\\{X_{1},X_{2}\\}$ with a conditioning set $_{C}$ . All partial adjustment sets may be enumerated in time $O(k(k+m))$ where $k$ are the number of variables and m be the number of edges in $\\mathcal{G}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. This proposition is adapts Proposition 3.20 in [43] to find partial adjustment sets using the ListSep algorithm. ListSep performs backtracking to enumerate all separator sets $_{z}$ between $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ such that $I\\subseteq Z\\subseteq R$ , aborting branches that will not find a valid separator. In particular, it calls the TestSep and FindSep algorithms recursively. ", "page_idx": 24}, {"type": "text", "text": "The TestSep algorithm takes as input a graph, two sets of nodes to be separated, and a candidate separator set. By setting the graph to be $\\mathcal{G}_{\\pi x_{1}\\underline{{X}}_{2}}$ , the two sets to be separated to be $Y$ and $X_{2}$ , and the separator set $Z$ , TestSep will provably return whether $W=Z\\backslash(C,X_{1})$ is a partial adjustment set. ", "page_idx": 24}, {"type": "text", "text": "The FindSep algorithm uses the observation that if there exists a separator $_{z}$ between $\\mathbf{\\deltaX}$ and $\\mathbf{\\deltaY}$ such that $I\\subseteq Z\\subseteq R$ then $Z:=\\,A n(X\\cup Y\\cup I)\\cap R$ is a separator. For each possible partition $(X_{1},X_{2})$ of $\\mathbf{\\deltaX}$ , we can therefore find a separator by testing (using $\\mathtt{T e s t.S e p)}$ whether $A n(X\\cup Y\\cup C)\\cap(V\\backslash(X\\cup Y))$ in $\\mathcal{G}_{\\pi x_{1}\\underline{{X}}_{2}}$ is a separator. If it is then we have found a partial adjustment set. ", "page_idx": 24}, {"type": "text", "text": "To find all partial adjustment sets, we then proceed as follows. For each possible partition $(X_{1},X_{2})$ of $\\mathbf{\\deltaX}$ , apply the ListSep algorithm with graph $\\mathcal{G}_{\\pi_{X_{1}}\\underline{{X}}_{2}}$ , sets to be separated $Y$ and $X_{2}$ , and possible separator sets $_{z}$ constrained as $C,X_{1}\\subseteq{Z\\subseteq V\\backslash(X\\cup Y)}$ . Then, for every separator set $Z$ found, output partial adjustment sets $W=Z\\backslash(C,X_{1})$ . ", "page_idx": 24}, {"type": "text", "text": "This procedure finds all possible partial adjustment sets in time $O(k(k+m))$ where $k$ are the number of variables and $m$ be the number of edges in $\\mathcal{G}$ by Proposition 3.20 [43]. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "B.2 Proofs of statements in Sec. 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We make use of two auxiliary lemmas from the literature. ", "page_idx": 24}, {"type": "text", "text": "Lemma 1 (Continuous Mapping Theorem, [26]). Let $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ , $X$ be random elements defined on a metric space $S$ . Consider a continuous function $g:S\\rightarrow S^{\\prime}$ (where $S^{\\prime}$ is another metric space). Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\nX_{n}\\to_{p}X\\Rightarrow g(X_{n})\\to_{p}g(X).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 2 (Lemma 2, [24]). Let $f_{\\eta}:=\\,f(V;\\eta)$ denote a finite and continuous functional and $\\eta$ denote its nuisances. For $n$ independent samples of $P$ , $D:=\\{\\pmb{v}^{(i)}\\}_{i=1,\\ldots,n}\\sim P_{!}$ , let $\\hat{T}=\\mathbb{E}_{D}[f_{\\hat{\\eta}}]$ and $T:=\\mathbb{E}_{P}[f_{\\eta}]$ for some $\\eta$ . Let $\\mathbb{E}_{D-P}[f_{\\eta}]:=\\mathbb{E}_{D}[f_{\\eta}]-\\mathbb{E}_{P}[\\dot{f}_{\\eta}]$ . Then, the following decomposition holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{D}[f_{\\hat{\\eta}}]-\\mathbb{E}_{P}[f_{\\eta}]=\\underbrace{\\mathbb{E}_{D-P}[f_{\\eta}]}_{=A}+\\underbrace{\\mathbb{E}_{D-P}[f_{\\hat{\\eta}}-f_{\\eta}]}_{=B}+\\mathbb{E}_{P}[f_{\\hat{\\eta}}-f_{\\eta}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Suppose further that samples used for estimating $\\eta$ are independent and separate; and the nuisa?nces are consistent. Then, $R=A+B$ is a random variable that converges to zero at a rate $\\mathcal{O}_{P}(1/\\sqrt{n})$ . ", "page_idx": 25}, {"type": "text", "text": "Prop. 8 restated. Suppose the nuisance estimates $(\\hat{\\pmb{\\mu}},\\hat{\\pmb{\\gamma}})$ are $L_{2}$ -consistent and bounded. Then, the error of the DML estimator $\\hat{T}^{D M L}\\in\\{\\hat{T}^{D M L,\\ell},\\hat{T}^{D M L,u}\\}$ in Def. 5 is given as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{T}^{D M L}-T^{D M L}=\\frac{1}{K}\\sum_{k=1}^{K}R_{k}+O_{P}\\Big(\\|\\hat{\\gamma}_{2,k}-\\gamma_{2}\\|\\|\\hat{\\mu}_{2,k}-\\mu_{2}\\|\\Big)+O_{P}\\Big(\\|\\hat{\\gamma}_{1,k}-\\gamma_{1}\\|\\|\\hat{\\mu}_{1,k}-\\hat{\\tilde{\\mu}}_{1,k}\\|\\Big)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $R_{k}$ is a random variable that converges to zero at a rate $\\mathcal{O}_{P}(1/\\sqrt{n})$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. In this proof we consider the estimation of the lower bund without loss of generality. Recall the definition of nuisance functions and estimators: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma_{2}:=\\bar{\\pi}_{U}\\gamma_{1},\\quad\\gamma_{1}:=\\bar{\\pi}_{T}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{2}:=\\mu_{2}(R,C,W,Z)=\\mathbb{E}_{P}[Y\\mid R,C,W,Z],}\\\\ &{\\tilde{\\mu}_{1}:=\\tilde{\\mu}_{2}(R,C,W,Z)=\\bar{\\pi}_{U}(U\\mid C)\\mu_{2}(R,C,W,Z),}\\\\ &{\\mu_{1}:=\\mu_{1}(T,C,W,Z)=\\mathbb{E}_{P}[\\tilde{\\mu}_{2}(R,C,W,Z)\\mid T,C,W,Z],}\\\\ &{\\mu_{0}:=\\mu_{0}(C,W,Z)=\\sum_{t}\\mu_{1}(t,C,W,Z)\\bar{\\pi}_{T}(t\\mid C).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We first verify that the population level value of the regression estimator coincides with the lower bound $\\psi_{z}^{\\ell}$ . This can be seen with the following derivation, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T^{R R,G,\\gamma}:=\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\mu_{\\theta}(\\mathbf{W};\\mathbf{Z},{\\mathbf{C}},z)]}\\\\ &{\\qquad=\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\mu_{\\theta}(\\mathbf{w},\\mathbf{z},z)P(\\mathbf{w};\\mathbf{z},c)]}\\\\ &{\\qquad=\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}\\left(\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\mu_{\\mathbf{R}}(\\mathbf{t},\\mathbf{w},\\mathbf{z},\\mathbf{z})]\\tau_{\\theta}^{\\gamma}\\left\\vert P(\\mathbf{w});\\mathbf{z},\\mathbf{z}\\right\\rangle\\right.}\\\\ &{\\qquad=\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}\\left(\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\mu_{\\mathbf{R}}(\\mathbf{t},\\mathbf{w},\\mathbf{z})]\\tau_{\\theta}^{\\gamma}\\left\\vert\\mathbf{z}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z},\\mathbf{w}\\right\\vert\\ z,\\mathbf{z}\\right\\rangle\\right)\\frac{\\tilde{\\pi}}{P(t,z)\\,\\left\\vert\\mathbf{w}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z},\\mathbf{w}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z},\\mathbf{z}\\right\\rangle}P(\\mathbf{t},\\mathbf{z},\\mathbf{w})\\tau_{\\theta}^{\\gamma}\\left\\vert\\mathbf{z},\\mathbf{z}\\right\\rangle}\\\\ &{\\qquad=\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}\\left(\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\mu_{\\mathbf{R}}(\\mathbf{t},\\mathbf{z},\\mathbf{w});\\mathbf{z},\\mathbf{w}]\\right)\\frac{\\tilde{\\pi}_{R}}{P(t,z)\\,\\left\\vert\\mathbf{w}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z},\\mathbf{w}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z},\\mathbf{w}\\right\\rangle}P(\\mathbf{t},\\mathbf{z},\\mathbf{w})}\\\\ &{\\qquad\\qquad\\qquad\\left.\\underset{\\theta\\in\\Theta}{\\sum_{\\mathbf{R}^{\\otimes}}}[\\frac{\\tilde{\\pi}_{R}\\mathbf{L}_{\\mathbf{z}}(z^{\\prime})}{P(t,z^{\\prime})\\,\\left\\vert\\mathbf{w}\\right\\vert\\left\\vert\\mathbf{z},\\mathbf{z}^{\\prime}\\right\\vert\\left\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The DML estimator is similarly unbiased as, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T^{\\mathrm{DML},\\ell}:=\\mathbb{E}_{P}[\\gamma_{2}\\{Y-\\mu_{2}\\}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P}[\\gamma_{2}\\{\\mathbb{E}_{P}[Y\\mid R,W\\backslash Z,C,Z]-\\mu_{2}\\}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]}\\\\ &{\\qquad\\quad=\\mathbb{E}_{P}[\\gamma_{2}\\{\\mu_{2}-\\mu_{2}\\}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\mathbb{E}_{P}[\\tilde{\\mu}_{1}\\mid T,C,W\\backslash Z,Z]-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]}\\\\ &{\\qquad=\\mathbb{E}_{P}[\\gamma_{1}\\{\\mu_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]}\\\\ &{\\qquad=\\mathbb{E}_{P}[\\mu_{0}]}\\\\ &{\\qquad=\\psi_{z}^{\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the estimated value of $T^{\\mathrm{DML,\\it{\\ell}}}$ following the procedure in Def. 5, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{T}^{\\mathrm{DML},\\ell}:=\\frac{1}{K}\\sum_{k=1}^{K}\\hat{T}_{k}^{\\mathrm{DML},\\ell},\\quad\\hat{T}_{k}^{\\mathrm{DML},\\ell}:=\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\mu}_{0}],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We could then write, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{T}_{k}^{\\mathrm{DML,\\it\\ell}}-T^{\\mathrm{DML,\\it\\ell}}=A+B+C\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\mathbb{E}_{\\mathcal{D}^{(k)}-P}\\Big[\\gamma_{2}\\{Y-\\mu_{2}\\}+\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}+\\mu_{0}\\Big]}\\\\ &{B=\\mathbb{E}_{\\mathcal{D}^{(k)}-P}\\Big[\\Big(\\gamma_{2}\\{Y-\\mu_{2}\\}\\big]+\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}+\\mu_{0}\\Big)-\\Big(\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}+\\hat{\\mu}_{0}\\Big)\\Big]}\\\\ &{C=\\mathbb{E}_{P}\\Big[\\Big(\\gamma\\{Y-\\mu_{2}\\}\\big]+\\gamma\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}+\\mu_{0}\\Big)-\\Big(\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}+\\hat{\\mu}_{0}\\Big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By the construction in Def. 5, the samples used for estimating $\\left(\\gamma_{1},\\gamma_{2},\\mu_{0},\\mu_{1},\\tilde{\\mu}_{1},\\mu_{2}\\right)$ and evaluating the outer expectation are independent and separate. Under the assumption that nuisance parameters $(\\hat{\\gamma}_{1},\\hat{\\gamma}_{2},\\hat{\\mu}_{0},\\hat{\\mu}_{1},\\hat{\\tilde{\\mu}}_{1},\\hat{\\mu}_{2})$ are consistent, $R=A+B$ converges to zero at a rate $\\mathcal{O}_{P}(1/\\sqrt{|\\mathcal{D}^{(k)}|})$ by Lem. 2. ", "page_idx": 26}, {"type": "text", "text": "Before manipulating $C$ and deriving its large sample behaviour, consider the following intermediate results: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\mu_{0}]=\\displaystyle\\sum_{c,w}\\mu_{0}(c,w,z)P(c,w)}\\\\ &{\\qquad=\\displaystyle\\sum_{c,w}\\frac{\\tau}{t}\\pi_{T}\\mu_{1}(t,c,w,z)P(c,w)}\\\\ &{\\qquad=\\displaystyle\\sum_{c,w,z^{\\prime},t}\\bar{\\pi}_{T}\\mathbb{1}_{z}\\bigl(z^{\\prime}\\bigr)\\mu_{1}(t,c,w,z^{\\prime})P(c,w)}\\\\ &{\\qquad=\\displaystyle\\sum_{c,w,z^{\\prime},t}\\frac{\\bar{\\pi}_{T}\\mathbb{1}_{z}\\bigl(z^{\\prime}\\bigr)}{P(t,z^{\\prime}\\mid c,w)}\\mu_{1}(t,c,w,z^{\\prime})P(t,c,w,z^{\\prime})}\\\\ &{\\qquad=\\displaystyle\\mathbb{E}_{P}[\\gamma_{1}\\mu_{1}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and similarly $\\mathbb{E}_{P}[\\hat{\\mu}_{0}]=\\mathbb{E}_{P}[\\gamma_{1}\\hat{\\mu}_{1}]$ . Note further that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}\\big[\\gamma_{2}\\mu_{2}\\big]=\\displaystyle\\sum_{c,w,t,u,z^{\\prime}}\\frac{\\bar{\\pi}_{T}\\bar{\\pi}_{U}\\mathbb{1}_{z}\\big(z^{\\prime}\\big)}{P(t,z^{\\prime}\\mid c,w)}{\\mu_{2}(c,w,t,u,z^{\\prime})P(c,w,t,u,z^{\\prime})}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{c,w,t,u,z^{\\prime}}\\gamma_{1}\\bar{\\pi}_{U}\\mu_{2}(c,w,t,u,z^{\\prime})P(c,w,t,u,z^{\\prime})}\\\\ &{\\qquad=\\displaystyle\\sum_{c,w,t,u,z^{\\prime}}\\gamma_{1}\\tilde{\\mu}_{1}(c,w,t,u,z^{\\prime})P(c,w,t,z^{\\prime})}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P}\\big[\\gamma_{1}\\tilde{\\mu}_{1}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and similarly $\\mathbb{E}_{P}[\\gamma_{2}\\hat{\\mu}_{2}]=\\mathbb{E}_{P}[\\gamma_{1}\\hat{\\tilde{\\mu}}_{1}]$ . ", "page_idx": 26}, {"type": "text", "text": "Now consider $C=C_{1}+C_{2}+C_{3}$ where, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}:=\\mathbb{E}_{P}\\big[\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}-\\gamma_{2}\\{Y-\\mu_{2}\\}\\big]=\\mathbb{E}_{P}\\big[\\hat{\\gamma}_{2}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}\\big]}\\\\ &{C_{2}:=\\mathbb{E}_{P}\\big[\\hat{\\gamma}_{1}\\{\\hat{\\tilde{\\mu}}_{1}-\\hat{\\mu}_{1}\\}-\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}\\big]}\\\\ &{C_{3}:=\\mathbb{E}_{P}\\big[\\hat{\\mu}_{0}-\\mu_{0}\\big]=\\mathbb{E}_{P}\\big[\\gamma_{1}\\hat{\\mu}_{1}-\\gamma_{1}\\mu_{1}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last equality follows from the result above. Therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C=\\mathbb{E}_{P}\\Big[\\hat{\\gamma}_{2}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}-\\gamma_{1}\\{\\hat{\\mu}_{1}-\\mu_{1}\\}+\\gamma_{1}(\\hat{\\mu}_{1}-\\mu_{1})\\Big]}\\\\ &{\\quad=\\mathbb{E}_{P}\\Big[\\{\\hat{\\gamma}_{2}-\\gamma_{2}\\}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}+\\gamma_{2}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}-\\gamma_{1}\\{\\hat{\\mu}_{1}-\\mu_{1}\\}+\\gamma_{1}(\\hat{\\mu}_{1}-\\mu_{1})\\Big]}\\\\ &{\\quad=\\mathbb{E}_{P}\\Big[\\{\\hat{\\gamma}_{2}-\\gamma_{2}\\}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}\\Big]+\\mathbb{E}_{P}\\Big[\\gamma_{2}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}-\\gamma_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}\\Big]}\\\\ &{\\stackrel{(1)}{=}\\mathbb{E}_{P}\\Big[\\{\\hat{\\gamma}_{2}-\\gamma_{2}\\}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}\\Big]+\\mathbb{E}_{P}\\Big[\\gamma_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}-\\gamma_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}\\Big]}\\\\ &{\\quad=\\mathbb{E}_{P}\\Big[\\{\\hat{\\gamma}_{2}-\\gamma_{2}\\}\\{\\mu_{2}-\\hat{\\mu}_{2}\\}\\Big]+\\mathbb{E}_{P}\\Big[-\\gamma_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}+\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}\\Big]}\\\\ &{\\quad=O_{P}\\Big(\\|\\hat{\\gamma}_{2}-\\gamma \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, this implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{T}^{\\mathrm{DML},\\ell}-T^{\\mathrm{DML},\\ell}=R+O_{P}\\Big(\\|\\hat{\\gamma}_{2}-\\gamma_{2}\\|\\|\\mu_{2}-\\hat{\\mu}_{2}\\|\\Big)+O_{P}\\Big(\\|\\hat{\\gamma}_{1}-\\gamma_{1}\\|\\|\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\|\\Big)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $R$ is a random variable that converges to zero at a rate $\\mathcal{O}_{P}(1/\\sqrt{n})$ . ", "page_idx": 27}, {"type": "text", "text": "For the upper bound, consider the following definition of nuisance functions and estimators: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\gamma_{2}:=\\bar{\\pi}_{U}\\gamma_{1},\\quad\\gamma_{1}:=\\bar{\\pi}_{T}\\mathbb{1}_{z}(Z)/P(T,Z\\mid W\\backslash Z,C),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $T^{\\mathrm{PW},\\ell}:=\\mathbb{E}_{P}[\\gamma_{2}(Y-1)]+1=\\psi_{z}^{\\ell}$ . And $\\pmb{\\mu}=\\left(\\mu_{0},\\mu_{1},\\tilde{\\mu}_{2},\\mu_{2}\\right)$ defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{2}:=\\mu_{2}(\\overset{\\cdot\\cdot}{R_{\\oslash}},W,\\overset{\\cdot\\sim}{Z})=\\mathbb{E}_{P}[Y-1\\mid R,C,W,Z],}\\\\ &{\\tilde{\\mu}_{1}:=\\tilde{\\mu}_{2}(R,C,W,Z)=\\bar{\\pi}_{U}(U\\mid C)\\mu_{2}(R,C,W,Z),}\\\\ &{\\mu_{1}:=\\mu_{1}(T,C,W,Z)=\\mathbb{E}_{P}[\\tilde{\\mu}_{2}(R,C,W,Z)\\mid T,C,W,Z],}\\\\ &{\\mu_{0}:=\\mu_{0}(C,W,Z)=\\sum_{t}\\mu_{1}(t,C,W,Z)\\bar{\\pi}_{T}(t\\mid C).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We first verify that the population level value of the regression estimator coincides with the lower bound $\\psi_{z}^{\\ell}$ . This can be seen with the following derivation, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T^{\\theta\\boxtimes\\alpha}:=\\mathbb{E}_{T}[\\mu_{0}(W\\!\\!\\mid\\!Z,C,z)]+1}\\\\ &{=1+\\sum_{\\mu=0}^{\\infty}\\mu_{0}(w,c,z)P(w\\!\\mid\\!z,c)}\\\\ &{\\qquad\\underset{w\\in\\mathcal{S}_{r}\\times\\mathcal{C}}{\\longrightarrow}\\left(\\!\\!\\left[\\mu_{1}(t,w,c,z)\\!\\!\\right]\\!\\!\\!\\pi\\left(w\\!\\mid\\!z,c\\right)\\!\\!\\right)P(w\\!\\mid\\!z,c)}\\\\ &{=1+\\sum_{\\mu=\\lfloor\\frac{\\pi}{2}\\rfloor\\in\\mathcal{L}}\\left(\\!\\!\\left[\\mu_{1}(t,w,c,z)\\!\\!\\right]\\!\\!\\pi_{T}\\!\\left(\\!\\left[\\mu_{1},z,w\\right\\rangle\\!\\!\\right]\\!\\!\\frac{\\tilde{w}}{f(t,z)}\\!\\!\\right)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The DML estimator is similarly unbiased as, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T^{\\mathrm{DML},u}:=\\mathbb{E}_{P}[\\gamma_{2}\\{(Y-1)-\\mu_{2}\\}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]+1}\\\\ &{\\hphantom{T^{\\mathrm{DML},u}:=}=\\mathbb{E}_{P}[\\gamma_{2}\\{\\mathbb{E}_{P}[Y-1\\;|\\;R,W\\}Z,C,Z]-\\mu_{2}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\tilde{\\mu}_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]+1}\\\\ &{\\hphantom{T^{\\mathrm{DML},u}:=}=\\mathbb{E}_{P}[\\gamma_{2}\\{\\mu_{2}-\\mu_{2}\\}]+\\mathbb{E}_{P}[\\gamma_{1}\\{\\mathbb{E}_{P}[\\tilde{\\mu}_{1}\\;|\\;T,C,W\\backslash Z,Z]-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]+1}\\\\ &{\\hphantom{T^{\\mathrm{DML},u}:=\\mathbb{E}_{P}[\\gamma_{1}\\{\\mu_{1}-\\mu_{1}\\}]+\\mathbb{E}_{P}[\\mu_{0}]+1}}\\\\ &{\\hphantom{T^{\\mathrm{DML},u}:=\\mathbb{E}_{P}[\\mu_{0}]+1}=\\mathbb{E}_{P}[\\mu_{0}]+1}\\\\ &{\\hphantom{T^{\\mathrm{DM},u}:=\\mathbb{E}_{Z}\\{\\mu_{0}\\}}=\\psi_{z}^{u}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The arguments of the proof are now analogous to that of the lower bound. We can conclude therefore that, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{T}^{\\mathrm{DML},u}-T^{\\mathrm{DML},u}=R+O_{P}\\Big(\\|\\hat{\\gamma}_{2}-\\gamma_{2}\\|\\|\\mu_{2}-\\hat{\\mu}_{2}\\|\\Big)+O_{P}\\Big(\\|\\hat{\\gamma}_{1}-\\gamma_{1}\\|\\|\\hat{\\tilde{\\mu}}_{1}-\\hat{\\mu}_{1}\\|\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Prop. 9 restated. Suppose either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\mu}_{2}=\\mu_{2}$ and that either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\tilde{\\mu}}_{1}=\\hat{\\mu}_{1}$ . Then, $\\hat{T}^{D M L}\\in\\{\\hat{T}^{D M L,\\ell},\\hat{T}^{D\\bar{M}\\bar{L},u}\\}$ is an unbiased estimator of the corresponding bound defined in Alg. 1. ", "page_idx": 28}, {"type": "text", "text": "Proof. Consider the estimated value of $T^{\\mathrm{DML,\\ell}}$ following the procedure in Def. 5, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{T}^{\\mathrm{DML},\\ell}:=\\frac{1}{K}\\sum_{k=1}^{K}\\hat{T}_{k}^{\\mathrm{DML},\\ell},\\quad\\hat{T}_{k}^{\\mathrm{DML},\\ell}:=\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\gamma}_{1}\\{\\hat{\\tilde{\\mu}}_{1}-\\hat{\\mu}_{1}\\}]+\\mathbb{E}_{\\mathcal{D}^{(k)}}[\\hat{\\mu}_{0}],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Under the assumption that, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\hat{T}^{\\mathrm{{DML},\\ell}}]=\\mathbb{E}_{P}[\\hat{\\gamma}_{2}\\{Y-\\hat{\\mu}_{2}\\}]+\\mathbb{E}_{P}[\\hat{\\gamma}_{1}\\{\\hat{\\mu}_{1}-\\hat{\\mu}_{1}\\}]+\\mathbb{E}_{P}[\\hat{\\mu}_{0}]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The bias of the estimator is given by, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\widehat{T}^{\\mathrm{{DML}},\\ell}]-T^{\\mathrm{{DML}},\\ell}}\\\\ &{\\quad=\\mathbb{E}_{P}\\Big[\\Big(\\gamma\\{Y-\\mu_{2}\\}\\big]+\\gamma\\{\\widetilde{\\mu}_{1}-\\mu_{1}\\}+\\mu_{0}\\Big)-\\Big(\\widehat{\\gamma}_{2}\\{Y-\\widehat{\\mu}_{2}\\}+\\widehat{\\gamma}_{1}\\{\\widehat{\\mu}_{1}-\\widehat{\\mu}_{1}\\}+\\widehat{\\mu}_{0}\\Big)\\Big]}\\\\ &{\\quad=\\mathbb{E}_{P}\\Big[\\{\\widehat{\\gamma}_{2}-\\gamma_{2}\\}\\{\\mu_{2}-\\widehat{\\mu}_{2}\\}\\Big]+\\mathbb{E}_{P}\\Big[\\{\\widehat{\\gamma}_{1}-\\gamma_{1}\\}\\{\\widehat{\\mu}_{1}-\\widehat{\\mu}_{1}\\}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The last equality follows from the derivation of (C) in the proof of Prop. 8. ", "page_idx": 28}, {"type": "text", "text": "If either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\mu}_{2}=\\mu_{2}$ and that either $\\hat{\\gamma}_{1}=\\gamma_{1}$ or $\\hat{\\tilde{\\mu}}_{1}=\\tilde{\\mu}_{1}$ then this expression equals 0 and we have that, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\hat{T}^{\\mathrm{{DML}},\\ell}]-T^{\\mathrm{{DML}},\\ell}=0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "An analogous result holds for the upper bound. ", "page_idx": 28}, {"type": "text", "text": "C Details on experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "This section provides details of the data generating mechanisms used for synthetic experiments, and implementations. ", "page_idx": 29}, {"type": "text", "text": "As described in the experimental section, for estimating nuisances $(\\gamma,\\mu)$ we used Gradient Boosting models for classification and regression where appropriate. We implemented the models using Python using the commands GradientBoostingClassifier() and GradientBoostingRegressor() using default hyperparameters. We systematically bound probability values in the interval $[0.01,0.99]$ to avoid propagating potentially large approximating errors. ", "page_idx": 29}, {"type": "text", "text": "C.1 Simulations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We make use of the following 4 data generating systems, summarized by the graphs in Fig. 4. In this section, we use the notation $\\mathbb{1}\\{X\\}$ that equals 1 if the statement $\\{X\\}$ is true, and equal to 0 otherwise. ", "page_idx": 29}, {"type": "text", "text": "Example 1. The data generating mechanism for the first scenario is given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{Z},U_{X Y},U_{C}\\sim\\mathcal{N}(0,1),}\\\\ &{\\quad\\quad\\quad\\quad Z:=f_{Z}(U_{Z}),}\\\\ &{\\quad\\quad\\quad\\quad C:=f_{C}(U_{C}),}\\\\ &{\\quad\\quad\\quad\\quad X:=f_{X}(Z,C,U_{X Y}),}\\\\ &{\\quad\\quad\\quad\\quad Y:=f_{Y}(X,C,U_{X Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{Z}(U_{Z}):=-1\\cdot\\mathbb{1}\\{U_{Z}\\leqslant-0.5\\}+0\\cdot\\mathbb{1}\\{-0.5<U_{Z}<0.5\\}+1\\cdot\\mathbb{1}\\{U_{Z}\\geqslant0.5\\},}\\\\ &{\\quad\\quad\\quad f_{C}(U_{C}):=U_{C},}\\\\ &{\\quad\\quad\\ f_{X}(C,U_{X Y}):=\\mathbb{1}\\{U_{X Y}>1+\\exp\\{0.75C+0.5Z+0.5\\}\\},}\\\\ &{\\quad\\quad\\ f_{Y}(X,C,U_{X Y}):=(2X-1)-2\\cdot(2U_{X Y}-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The outcome $Y$ is then re-scaled to lie in the r0, 1s interval. The target for estimation can be derived with Alg. 1 and equals, for the lower and upper bound respectively, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z\\in\\{-1,0,1\\}}\\mathbb{E}_{P}[\\bar{\\pi}\\mathbb{1}_{z}(Z)Y/P(Z\\mid C)],\\qquad\\operatorname*{min}_{z\\in\\{-1,0,1\\}}\\mathbb{E}_{P}[\\bar{\\pi}\\mathbb{1}_{z}(Z)(Y-1)/P(Z\\mid C)]+1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Example 2. The data generating mechanism for the second scenario is given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X_{1}Y},U_{X_{2}W},U_{X_{2}},U_{X_{1}},U_{C},U_{W}\\sim\\mathcal{N}(0,1),}\\\\ &{\\phantom{U_{X_{1}Y},}{C}:=f_{C}(U_{C}),}\\\\ &{W:=f_{W}\\big(U_{X_{2}W}\\big),}\\\\ &{X_{1}:=f_{X_{1}}(C,U_{X_{1}Y},U_{X_{1}}),}\\\\ &{X_{2}:=f_{X_{2}}\\big(U_{X_{2}W},U_{X_{2}}\\big),}\\\\ &{Y:=f_{Y}\\big(X_{1},X_{2},C,W,U_{X_{1}Y}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{f_{C}(U_{C}):=U_{C},}}\\\\ &{}&{f_{W}(U_{W}):=U_{X_{2}W},}\\\\ &{}&{f_{X_{1}}(C,U_{X_{1}},U_{X_{1}Y}):=\\mathbb{1}\\{U_{X_{1}Y}\\cdot(1+\\exp\\{0.75C+0.5\\})>U_{X_{1}}\\},}\\\\ &{}&{f_{X_{2}}(U_{X_{2}W},U_{X_{2}}):=\\mathbb{1}\\{U_{X_{2}W}+U_{X_{2}}>0\\},}\\\\ &{}&{f_{Y}(X_{1},X_{2},C,W,U_{X_{1}Y}):=(2X_{1}-1)\\cdot(C+1)+2\\sin(2C\\cdot(2X_{1}-1))}\\\\ &{}&{-\\ 2(2U_{X_{1}Y}-1)(1+0.5C)+X_{2}+W.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The outcome $Y$ is then re-scaled to lie in the r0, 1s interval. The target for estimation can be derived with Alg. 1 and equals, for the lower and upper bound respectively, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[{\\bar{\\pi}}Y/P(X_{2}\\mid W,C)],\\qquad\\mathbb{E}_{P}[{\\bar{\\pi}}(Y-1)/P(X_{2}\\mid W,C)]+1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Example 3. The data generating mechanism for the third scenario is given by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X_{1}Y},U_{X_{2}W},U_{X_{2}},U_{X_{1}},U_{C}\\sim\\mathcal{N}(0,1),}\\\\ &{\\begin{array}{r l}{W:=\\{W_{1},\\ldots,W_{100}\\}\\;\\mathrm{s.t.}\\;W_{i}\\sim\\mathcal{N}(0,1),i=1,\\ldots,100,}\\\\ {C:=f_{C}(U_{C}),}\\\\ {X_{1}:=f_{X_{1}}(C,U_{X_{1}Y},U_{X_{1}}),}\\\\ {X_{2}:=f_{X_{2}}(W,U_{X_{2}}),}\\\\ {Y:=f_{Y}(X_{1},X_{2},C,W,U_{X_{1}Y}),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f_{C}(U_{C}):=U_{C},}}\\\\ {{f_{X1}(C,U_{X1}Y,U_{X1}):=\\mathbb{1}\\{U_{X1Y}\\cdot(1+\\exp\\{0.75C+0.5\\})>U_{X1}\\},}}\\\\ {{f_{X2}(W,U_{X2}):=\\mathbb{1}\\{W_{1}+W_{2}+U_{X2}>0\\},}}\\\\ {{f_{Y}(X_{1},X_{2},C,{\\pmb W},U_{X1Y}):=(2X_{1}-1)\\cdot(C+1)+2\\sin(2C\\cdot(2X_{1}-1))}}\\\\ {{-\\,2(2U_{X1Y}-1)(1+0.5C)+X_{2}+\\displaystyle\\sum_{i=1,\\dots,100}W_{i}/100.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The outcome $Y$ is then re-scaled to lie in the r0, 1s interval. The target for estimation can be derived with Alg. 1 and equals, for the lower and upper bound respectively, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\bar{\\pi}Y/P(X_{2}\\mid W,C)],\\quad\\quad\\mathbb{E}_{P}[\\bar{\\pi}(Y-1)/P(X_{2}\\mid W,C)]+1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Example 4. The data generating mechanism for the fourth scenario is given by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{Z},U_{X_{1}Y},U_{X_{2}},U_{X_{1}},U_{C},U_{Z}\\sim\\mathcal{N}(0,1),}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\Big[Z_{Z}\\Big)}}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\Big[Z_{Z}\\Big]},}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\Big[Z_{Z}\\Big]},}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\Big[Z_{Z}\\Big[Z_{Z}\\Big]_{Z},}}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\Big[W_{Z},U_{X_{2}}\\Big],}}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}_{Z}\\sum_{Z}_{Z}}}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}_{Z}}}\\\\ &{\\phantom{\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}\\sum_{Z}Z\\sum_{Z}\\sum_{Z}\\sum_{Z}Z}}\\end{Z_{Z}\\sum_{Z}\\sum_{Z}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{C}(U_{C}):=U_{C},}\\\\ &{f_{Z}(U_{Z}):=1\\{U_{Z}>0\\},}\\\\ &{f_{X_{1}}(U_{X_{1}Y},U_{X_{1}},Z,U_{C}):=1\\{U_{X_{1}Y}\\cdot(1+\\exp\\{0.75U_{C}+0.5Z+0.5\\})>U_{X_{1}}\\},}\\\\ &{\\begin{array}{r l}{f_{X_{2}}(W,U_{X_{2}}):=1\\{W_{1}+W_{2}+U_{X_{2}}>0\\},}\\\\ {f_{Y}(X_{1},X_{2},C,W,U_{X_{1}Y}):=(2X_{1}-1)\\cdot(C+1)+2\\sin(2C\\cdot(2X_{1}-1))}\\\\ {-2(2U_{X_{1}Y}-1)(1+0.5C)+X_{2}+W_{1}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The outcome $Y$ is then re-scaled to lie in the r0, 1s interval. The target for estimation can be derived with Alg. 1 and equals, for the lower and upper bound respectively, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z\\in\\{0,1\\}}\\mathbb{E}_{P}[\\bar{\\pi}\\mathbb{1}_{z}(Z)Y/P(Z,X_{2}\\mid W,C)],\\quad\\operatorname*{min}_{z\\in\\{0,1\\}}\\mathbb{E}_{P}[\\bar{\\pi}\\mathbb{1}_{z}(Z)(Y-1)/P(Z,X_{2}\\mid W,C)]+1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.2 Health Campaign Evaluation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The data was curated from anonymous from Colombia, Peru and Mexico, using a web platform [30]. The exact questions considered in the survey can be found in [30]. The authors made the data available under a Creative Commons license5 and is currently hosted by Kaggle as a c.s.v flie, accessible through the following link: kaggle.com/code/mpwolke/obesity-levels-life-style/. ", "page_idx": 30}, {"type": "image", "img_path": "u5enPCwaLt/tmp/994075d9b699d9089eedeca5ff869cfd52a4ed6c7d0d2492a7a57ed53c940aae.jpg", "img_caption": ["Figure 7: Obesity diagrams. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "We considered a number of pre-processing steps. In particular, we created a BMI variable from weight and height data that were a posteriori removed from the dataset, imputed (with median or mode among observed instances) missing values, scaled continuously-valued covariates by subtracting the mean and dividing by the standard deviation, and scaled the outcome BMI to lie in the interval $[0,1]$ . Several covariates were ignored, including data on the consumption of water, consumption of vegetables, consumption of food between meals, consumption of alcohol, and number of main meals. These may be interpreted as unobserved variables, potentially confounding other relationships in the data as specified by the assumed causal diagram. The data from a total of 2111 individuals was recorded in this study. ", "page_idx": 31}, {"type": "text", "text": "The bounds on the effect of a policy $\\pi^{\\alpha}:=\\{\\pi_{H}^{\\alpha},\\pi_{E}^{\\alpha}\\}$ acting on $H$ and $E$ , with new assignments defined as $\\{P_{\\mathrm{new}}(H\\,=\\,\\mathtt{r a r e1y})\\,=\\,\\alpha\\},P_{\\mathrm{new}}(\\bar{E}\\,=\\,\\overset{\\cdot}{\\mathtt{r e g u l}}\\,\\mathtt{a r}\\,\\mathtt{l y})\\,=\\,\\alpha\\}$ are approximated from the expression in Prop. 4. Following this proposition we find that Age $(A)$ , Smoking $(S)$ , calories consumption Monitoring $(M)$ , and Family history with overweight $(F)$ , that is $W=\\{A,S,M,F\\}$ form a partial adjustment set for $\\pi_{E}^{\\alpha}$ given that $(Y\\perp_{d}E\\mid W,H)_{\\mathcal{G}_{\\pi_{H}E}}$ . We can verify also that $Z=\\{T\\}$ is a partial instrumental set for $\\pi$ since $(Y\\bot_{d}T)_{\\mathcal{G}_{\\pi}}$ . For illustration, the corresponding mutilated diagrams are shown in Fig. 7. Finally, we partition the data equally to obtain two sets of samples for training first and second stage classifiers and regressors, respectively, obtaining a first estimate of bounds, before then switching the role of the two data samples and averaging over the resulting estimates. ", "page_idx": 31}, {"type": "text", "text": "D NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The claims made match the theoretical and experimental results presented in the paper. The impact statement in the Appendix reflects how much the results can be expected to generalize to other settings. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We include several remarks in the paper to qualify our contribution, e.g. the remark in Sec. 3, and include an Impact Statement in the Appendix to more thoroughly describe the limitations of our analysis, assumptions, and applicability in real-world settings. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All theoretical statement are quoted in full in the paper. We have attempted to provide an example to illustrate the significance of each theoretical statement and highlight its implications. The formal proof of all statements is given in the Appendix. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the data generating mechanisms, details of the target of estimation and information as to what python functions and libraries can be used to fit the proposed estimators. We do not, however, disclose an open source implementation of the proposed methods at this moment. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 33}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] and [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: The data is publicly available and we have provided full details as to where to access the data and how to run the synthetic data generation pipeline. The code will not be open sourced at this moment but we believe to have provided sufficient details to reproduce our results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We provided details where applicable. In our case, data splits, hyperparameters, optimizers, etc., are not significant for the implementation the method. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We report our results as box plots that include several statistics, e.g. median, 25th, 75th, percentiles, etc., to summarize our results for different seeds of the data and algorithm. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Experiments were run on a single CPU in under a few minutes of wall time. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have read the guidelines and we do not think that our work presents any notable concerns. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We include a broader impact statement in the Appendix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No risk of misuse. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not use existing asset ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]