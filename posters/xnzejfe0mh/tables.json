[{"figure_path": "xNZEjFe0mh/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication and sample complexities of several algorithms for solving distributionally robust optimization problems in federated learning.  The algorithms are categorized by the type of constraint used (CVaR or KL) and whether they are proposed in prior work or in this paper. The complexities are expressed in terms of the desired precision level (e). The table highlights the improvements in communication and sample complexity achieved by the proposed algorithms in this work.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for achieving an e-stationary point in federated learning.  It contrasts the performance of existing methods (DRFA, DR-DSGD, NDP-SONT) with the proposed algorithms (FGDRO with CVaR constraint and FGDRO with KL regularization). The table shows that the proposed algorithms achieve significantly lower communication and sample complexity compared to the existing ones.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_8_2.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms in achieving an e-stationary point (a point where the magnitude of the gradient is less than epsilon). It shows that the proposed algorithms (FGDRO-CVaR and FGDRO-KL) significantly reduce both communication and sample complexity compared to existing methods like DRFA and DR-DSGD. The table also includes the naive deployment of the SONT algorithm as a benchmark for comparison.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_29_1.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms in achieving an e-stationary point in federated learning. It contrasts the complexity of existing methods (DRFA, DR-DSGD, NDP-SONT) with the proposed algorithms (FGDRO with CVaR constraint and FGDRO with KL regularization).  The table highlights the significant reduction in communication and sample complexity achieved by the proposed algorithms.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_29_2.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for achieving an e-stationary point in federated learning.  It shows that the proposed algorithms (This Work) significantly reduce both communication and sample complexity compared to existing methods (DRFA [11], DR-DSGD [30], NDP-SONT [22]).  The table highlights the differences based on whether a CVaR constraint or a KL regularization is used. The sample complexity measures the amount of data needed by each machine, while communication complexity refers to the number of communications needed to converge to a solution.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_30_1.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for solving the Federated Group Distributionally Robust Optimization (FGDRO) problem. It shows the complexity of achieving an e-stationary point or near e-stationary point for various algorithms, including the proposed algorithms (FGDRO-CVaR and FGDRO-KL) and existing methods such as DRFA, DR-DSGD, and NDP-SONT.  The complexity is measured in terms of the order of communication rounds and number of samples required on each machine to reach the desired precision level (e).", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_30_2.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for achieving an e-stationary point in federated learning.  It contrasts the performance of existing methods (DRFA, DR-DSGD, NDP-SONT) with the proposed algorithms (FGDRO with CVaR and KL regularizers). The table highlights the significant reduction in communication and sample complexity achieved by the proposed algorithms, showcasing their efficiency in federated group distributionally robust optimization.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_30_3.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for achieving an e-stationary point in federated learning. It contrasts the proposed methods (FGDRO-CVaR and FGDRO-KL) with existing state-of-the-art techniques, highlighting the significant reduction in communication and sample complexity achieved by the proposed algorithms. The table also shows the sample complexity on each machine, which is the number of samples required to achieve the desired precision level.", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_30_4.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of various algorithms for solving the Federated Group Distributionally Robust Optimization (FGDRO) problem.  It shows the complexity for achieving an e-stationary point (or near e-stationary point) under different regularization methods (CVaR and KL) and existing methods (DRFA, DR-DSGD, NDP-SONT).  The table highlights the communication and sample complexity improvements of the proposed algorithms (FGDRO-CVaR and FGDRO-KL).", "section": "1 Introduction"}, {"figure_path": "xNZEjFe0mh/tables/tables_31_1.jpg", "caption": "Table 1: Comparison of communication cost and sample complexity on each machine to achieve e-stationary point or near to e-stationary point, where e-stationary point has a (sub-)gradient ||JF(w)||2 < e\u00b2. NDP-SONT denotes the naive deployment of the SONT algorithm [22] in a federated environment, communicating in all iterations.", "description": "This table compares the communication cost and sample complexity of different algorithms for achieving an e-stationary point or near e-stationary point in federated learning.  It shows the complexity for each machine to achieve a (sub)gradient norm less than epsilon squared.  The algorithms compared include DRFA [11], DR-DSGD [30], NDP-SONT [22], and the proposed algorithms FGDRO-CVaR and FGDRO-KL.  The table highlights the significant reduction in communication and sample complexity achieved by the proposed algorithms.", "section": "1 Introduction"}]