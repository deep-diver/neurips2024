[{"figure_path": "77kCJzvpOa/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of LM-GC. Our method initially converts every 4 bits into hexadecimal numbers and groups them with separators in between, e.g., commas in the figure. The grouped text is then input to a pre-trained, frozen tokenizer and LLM to produce the probability of each token. These probabilities are used for arithmetic encoding, where a line segment between 0 and 1 is repeatedly split according to the token probability until reaching a predefined maximum length. Any number from that region (e.g., the midpoint) can accurately represent the original data. We provide an example of how arithmetic coding works in Sec. 3.", "description": "This figure illustrates the LM-GC (Language Model Gradient Compression) method.  It shows how raw gradient data (represented as bits) is converted into a text-like format using hexadecimal numbers and separators. This textual representation is then fed into a pre-trained Language Model (LLM) to predict the probability of each token. Finally, arithmetic encoding uses these probabilities to compress the gradient data. The diagram also visually explains the basic principle of arithmetic encoding.", "section": "4 LM-GC"}, {"figure_path": "77kCJzvpOa/figures/figures_6_1.jpg", "caption": "Figure 2: Compression rates of LLAMA 2-7B using context window sizes of 256, 512, 1024, 2048, and 4096. The compression rates improve as the context window increases.", "description": "This figure shows an ablation study on the effect of different context window sizes on the compression rate achieved by the LM-GC method using the LLAMA 2-7B language model.  The x-axis represents the context window size (in tokens), while the y-axis shows the resulting compression rate (percentage).  As the context window increases, the model has more information to work with, leading to improved compression rates. The graph illustrates the trade-off between context size and computational cost.", "section": "5.3 Ablation study"}, {"figure_path": "77kCJzvpOa/figures/figures_7_1.jpg", "caption": "Figure 3: Ablation study on numbers of grouped bytes. We report the compression rates and the number of tokens yielded by different serializations. The settings that closely obey the data format perform better. However, smaller numbers yield higher computation overhead.", "description": "This figure shows the results of an ablation study on the number of bytes grouped together during the serialization process of LM-GC.  It compares different serialization methods, varying the number of bytes grouped (1, 2, 3, 4, 8 bytes, and no grouping).  The results demonstrate that grouping bytes according to the underlying structure of the floating-point numbers (4 bytes for each float) leads to better compression rates and fewer tokens used. While smaller group sizes increase computational overhead due to more tokens, following the data structure improves efficiency.", "section": "5.3 Ablation study"}, {"figure_path": "77kCJzvpOa/figures/figures_7_2.jpg", "caption": "Figure 4: Compatibility analysis with sparsification (left) and quantization (right).", "description": "This figure demonstrates the compatibility of the proposed LM-GC method with existing lossy compression techniques: sparsification and quantization.  The left panel shows how combining LM-GC with sparsification (reducing the number of gradient elements transmitted) leads to better compression ratios than using sparsification alone or with LZMA compression. The right panel illustrates similar results for quantization (reducing the precision of each gradient element).  In both cases, LM-GC improves compression ratios regardless of the sparsification or quantization level.", "section": "5.4 Compatibility"}]