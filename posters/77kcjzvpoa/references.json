{"references": [{"fullname_first_author": "D. Ulyanov", "paper_title": "Deep image prior", "publication_date": "2018-06-01", "reason": "This paper introduces a novel approach to image denoising and super-resolution using deep generative models, which is highly relevant to the concept of using deep generative priors for gradients in the target paper."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This foundational paper introduces the Transformer architecture, which underpins the large language models (LLMs) used as gradient compressors in the target paper."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrates the remarkable few-shot learning capabilities of large language models, which are leveraged in the zero-shot setting of the target paper for gradient compression."}, {"fullname_first_author": "J. Bernstein", "paper_title": "signsgd: Compressed optimization for non-convex problems", "publication_date": "2018-06-01", "reason": "This paper introduces a lossy gradient compression technique, which is compared to the lossless compression method of the target paper."}, {"fullname_first_author": "Y. Gandelsman", "paper_title": "\u201cdouble-dip\u201d: unsupervised image decomposition via coupled deep-image-priors", "publication_date": "2019-06-01", "reason": "This paper explores another application of deep generative priors, showing the effectiveness of such models in image processing tasks, and is used as a comparison method in the current paper."}]}