[{"figure_path": "77kCJzvpOa/tables/tables_5_1.jpg", "caption": "Table 1: Gradient compression rate using PNG, FLAC, GZIP, LZMA, FPZIP, and our method with various language models. Our method considers different serializations including iso-8859-1 (ISO), hexadecimal numbers without separators (Hn) and with spaces (H5), commas (Hc), commas+spaces (Hc+s), and semicolons (Hsemi) to group every four bytes from the same floating point.", "description": "This table compares the compression rates achieved by various lossless compression methods, including traditional codecs (PNG, FLAC, GZIP, LZMA, FPZIP) and the proposed LM-GC method using different LLMs (Tinyllama 1.1B, Openllama 3B, LLAMA 2 7B).  The LM-GC method incorporates different serialization techniques to convert gradients into text-like formats, which are then processed by the LLMs for compression.  The table highlights the impact of serialization on compression efficiency and demonstrates the superior performance of LM-GC compared to traditional codecs.", "section": "5.2 Compression effectiveness"}, {"figure_path": "77kCJzvpOa/tables/tables_6_1.jpg", "caption": "Table 1: Gradient compression rate using PNG, FLAC, GZIP, LZMA, FPZIP, and our method with various language models. Our method considers different serializations including iso-8859-1 (ISO), hexadecimal numbers without separators (Hn) and with spaces (Hs), commas (Hc), commas+spaces (Hc+s), and semicolons (Hsemi) to group every four bytes from the same floating point.", "description": "This table compares the gradient compression rates achieved by several traditional lossless compression methods (PNG, FLAC, GZIP, LZMA, FPZIP) against the proposed LM-GC method.  LM-GC is tested with different LLMs and several serialization techniques (ISO-8859-1 encoding, hexadecimal representation with and without separators). The table highlights the superior compression rates achieved by LM-GC, especially when using appropriate serialization methods that improve LLM understanding of the gradient data structure.", "section": "5.2 Compression effectiveness"}, {"figure_path": "77kCJzvpOa/tables/tables_6_2.jpg", "caption": "Table 3: Compression effectiveness on MNIST, CIFAR-10, and TinyImageNet datasets. We use a Tinyllama as the compressor to compress the gradients of ConvNets. The raw data are converted to hexadecimal numbers with spaces as the separator. The improvement (Impr.) over the best baseline highlights the capability of LM-GC in modeling complex gradients.", "description": "This table compares the compression rates achieved by LM-GC against several baseline codecs (PNG, FLAC, GZIP, LZMA, FPZIP) on three image datasets of varying complexity: MNIST, CIFAR-10, and TinyImageNet.  The LM-GC method uses a Tinyllama language model and a specific serialization method (hexadecimal with spaces). The \"Impr.\" column shows the percentage improvement of LM-GC over the best-performing baseline for each dataset.", "section": "5. Experiments"}, {"figure_path": "77kCJzvpOa/tables/tables_12_1.jpg", "caption": "Table 4: Run length encoding results of gradients collected from ConvNets trained on TinyImageNet.", "description": "This table compares the compression rates achieved by Run Length Encoding (RLE) with different encoding schemes (binary, hexadecimal with and without separators) against the LM-GC method proposed in the paper.  It demonstrates the inefficiency of RLE for compressing gradients, particularly compared to LM-GC, even with various adaptations to the RLE approach. The results highlight the superiority of LM-GC in effectively compressing gradient data.", "section": "5.2 Compression effectiveness"}]