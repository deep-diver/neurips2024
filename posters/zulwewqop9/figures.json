[{"figure_path": "ZulWEWQOp9/figures/figures_0_1.jpg", "caption": "Figure 1: Guidance-free structure and appearance control of Stable Diffusion XL (SDXL) [27]. Ctrl-X enables training-free and guidance-free zero-shot control of pretrained text-to-image diffusion models given any structure conditions and appearance images.", "description": "This figure showcases the capabilities of Ctrl-X, a novel method for controlling both structure and appearance in text-to-image generation.  It presents several examples where a user provides a structure image (e.g., a sketch or segmentation map) and an appearance image (a photo with desired visual style). Ctrl-X then generates an image that incorporates the structure from the first image and the visual style of the second, all without requiring any further training or guidance (zero-shot). The results demonstrate that Ctrl-X can effectively separate structure and appearance control, enabling flexible and creative image generation.", "section": "Abstract"}, {"figure_path": "ZulWEWQOp9/figures/figures_2_1.jpg", "caption": "Figure 2: Visualizing early diffusion features. Using 20 real, generated, and condition images of animals, we extract Stable Diffusion XL [27] features right after decoder layer 0 convolution. We visualize the top three principal components computed for each time step across all images. t = 961 to 881 correspond to inference steps 1 to 5 of the DDIM scheduler with 50 time steps. We obtain xt by directly adding Gaussian noise to each clean image xo via the diffusion forward process.", "description": "This figure visualizes early diffusion features from Stable Diffusion XL.  It shows the top three principal components of features extracted after the decoder layer 0 convolution, using 20 real images, 20 generated images, and 20 condition images of animals. The visualization is done for different time steps (t) during the DDIM diffusion process, with t=961 to 881 representing inference steps 1 to 5.  The purpose is to demonstrate how the features capture rich spatial structure and high-level appearance from early diffusion steps which are sufficient for structure and appearance control.", "section": "3 Preliminaries"}, {"figure_path": "ZulWEWQOp9/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of Ctrl-X. (a) At each sampling step t, we obtain x and x via the forward diffusion process, then feed them into the T2I diffusion model to obtain their convolution and self-attention features. Then, we inject convolution and self-attention features from x and leverage self-attention correspondence to transfer spatially-aware appearance statistics from x to x. (b) Details of our spatially-aware appearance transfer, where we exploit self-attention correspondence between x and x to compute weighted feature statistics M and S applied to x.", "description": "This figure provides a visual explanation of the Ctrl-X model's pipeline.  Panel (a) shows the overall process: noisy structure and appearance images are fed into a pretrained T2I diffusion model.  Convolution and self-attention features are extracted and selectively injected into later stages of the process.  Panel (b) zooms in on the spatially-aware appearance transfer module, detailing how the model uses self-attention maps to transfer appearance statistics from the appearance image to the output image.  The arrows in (a) show feature injection, self-attention injection, and spatially aware appearance transfer.", "section": "4 Guidance-free structure and appearance control"}, {"figure_path": "ZulWEWQOp9/figures/figures_5_1.jpg", "caption": "Figure 4: Qualitative results for T2I diffusion structure and appearance control and conditional generation. Ctrl-X supports a diverse variety of structure images for both (a) structure and appearance controllable generation and (b) prompt-driven conditional generation.", "description": "This figure showcases the qualitative results of the Ctrl-X model on text-to-image generation tasks.  It demonstrates the model's ability to control both the structure and appearance of generated images based on user-provided structure and appearance images.  The figure presents examples of diverse structures (photographs, paintings, sketches, etc.) and demonstrates how these structures are preserved and combined with different appearance styles in the generated outputs.  The figure also shows that the model can handle prompt-driven conditional generation, further highlighting its flexibility and controllability.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparison of structure and appearance control. Ctrl-X displays comparable structure control and superior appearance transfer compared to training-based methods. It is also more robust than guidance-based and guidance-free methods across diverse structure types.", "description": "This figure compares the performance of Ctrl-X against several baseline methods for structure and appearance control in text-to-image generation.  It showcases the results using diverse structure conditions, including natural images, ControlNet-supported conditions (e.g., canny edge maps, depth maps, segmentation masks), and in-the-wild conditions (e.g., 3D meshes, point clouds). The comparison highlights Ctrl-X's ability to maintain comparable structure control while achieving superior appearance transfer compared to training-based methods.  The figure also demonstrates that Ctrl-X shows more robustness than guidance-based and guidance-free methods across different structure types.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparison of structure and appearance control. Ctrl-X displays comparable structure control and superior appearance transfer compared to training-based methods. It is also more robust than guidance-based and guidance-free methods across diverse structure types.", "description": "This figure shows a qualitative comparison of the proposed Ctrl-X method against several baselines for structure and appearance control in text-to-image generation.  Different rows represent different types of structure conditions (e.g., natural images, ControlNet-supported conditions, and in-the-wild conditions). The columns compare the results of Ctrl-X against methods like ControlNet + IP-Adapter, T2I-Adapter + IP-Adapter, Uni-ControlNet + IP-Adapter, FreeControl, and Cross-Image Attention. The comparison highlights Ctrl-X's ability to maintain comparable structure while achieving superior appearance transfer compared to other methods, demonstrating its robustness across diverse structure types.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_8_1.jpg", "caption": "Figure 7: Qualitative comparison of conditional generation. Ctrl-X displays comparable structure control and superior prompt alignment to training-based methods, and it also has better image quality and is more robust than guidance-based and guidance-free methods across different conditions.", "description": "This figure compares the results of conditional image generation using Ctrl-X and other methods.  It shows that Ctrl-X achieves comparable results to training-based approaches while exhibiting improved image quality and robustness across diverse conditions compared to guidance-based and guidance-free alternatives.  The comparison highlights Ctrl-X's ability to maintain structural consistency while accurately reflecting the given text prompts.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_9_1.jpg", "caption": "Figure 8: Ablations. We study ablations on control, appearance transfer method, and inversion.", "description": "This figure shows the ablation studies conducted by the authors to evaluate the impact of different components of their method. The three ablation studies are: (a) Ablation on control: comparing the results of no control, structure-only control, appearance-only control, and the full Ctrl-X method. (b) Ablation on appearance transfer method: comparing the results of using the proposed spatially-aware appearance transfer method with a method without attention weighting. (c) Ablation on inversion vs. our method: comparing the results of using inversion versus the proposed forward diffusion method.  Each ablation study is shown for three different image generation examples.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_9_2.jpg", "caption": "Figure 8: Ablations. We study ablations on control, appearance transfer method, and inversion.", "description": "This figure presents ablation studies to analyze the impact of different components of the Ctrl-X model.  It shows comparisons between the full Ctrl-X model and versions with either the structure control, appearance transfer, or inversion components removed. This allows for a quantitative assessment of each component's contribution to the overall performance of the model in terms of structure preservation and appearance transfer.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_9_3.jpg", "caption": "Figure 1: Guidance-free structure and appearance control of Stable Diffusion XL (SDXL) [27]. Ctrl-X enables training-free and guidance-free zero-shot control of pretrained text-to-image diffusion models given any structure conditions and appearance images.", "description": "This figure showcases the capabilities of Ctrl-X, a novel method for controlling the structure and appearance of images generated by text-to-image diffusion models.  The figure displays several examples of image generation, each demonstrating how Ctrl-X allows for independent control of both the image's structure (using a provided structure image) and its appearance (using a provided appearance image).  The results show that Ctrl-X can achieve this control without requiring any additional training of the model or the use of guidance during inference.", "section": "Introduction"}, {"figure_path": "ZulWEWQOp9/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative results for T2I diffusion structure and appearance control and conditional generation. Ctrl-X supports a diverse variety of structure images for both (a) structure and appearance controllable generation and (b) prompt-driven conditional generation.", "description": "This figure demonstrates the qualitative results of the Ctrl-X model on text-to-image diffusion tasks.  It showcases the model's ability to control both the structure and appearance of generated images using various input types.  Part (a) shows examples of structure and appearance control, where the model generates images that match a given structure image (e.g., a photo of a bear) while incorporating the appearance of a separate appearance image (e.g., an avocado). Part (b) demonstrates conditional generation based on a text prompt, where the model generates images aligning with a specified structure image. The figure highlights the versatility of Ctrl-X across different structure and appearance conditions, showing its effective combination of structure preservation and appearance transfer.", "section": "5 Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_15_1.jpg", "caption": "Figure 11: Ablation on control schedules. By varying Ctrl-X's structure and appearance control schedules (\u03c4s and \u03c4a), we change the influence of the structure and appearance images on the output.", "description": "This figure shows the effect of changing the structure and appearance control schedules (\u03c4s and \u03c4a) on the generated images.  Different combinations of \u03c4s and \u03c4a values resulted in different levels of structure preservation and appearance transfer. The default values (\u03c4s = 0.6 and \u03c4a = 0.6) produced good results for many image pairs but not all. The figure demonstrates that adjusting these schedules allows for a tradeoff between structure alignment and appearance transfer, enabling better results for challenging structure-appearance pairs and the use of higher-level conditions without clearly defined subject outlines.", "section": "5.2 Ablations"}, {"figure_path": "ZulWEWQOp9/figures/figures_16_1.jpg", "caption": "Figure 1: Guidance-free structure and appearance control of Stable Diffusion XL (SDXL) [27]. Ctrl-X enables training-free and guidance-free zero-shot control of pretrained text-to-image diffusion models given any structure conditions and appearance images.", "description": "This figure shows examples of  guidance-free structure and appearance control using the Ctrl-X method on Stable Diffusion XL.  The left side demonstrates structure control, where the generated images match the provided structure (layout) while maintaining the text prompt's content. The right side shows appearance control, where the generated image adopts the style or appearance of a given reference image while adhering to the textual description.  The results showcase the ability of Ctrl-X to control both structure and appearance of generated images without any additional training or guidance.", "section": "Introduction"}, {"figure_path": "ZulWEWQOp9/figures/figures_16_2.jpg", "caption": "Figure 7: Qualitative comparison of conditional generation. Ctrl-X displays comparable structure control and superior prompt alignment to training-based methods, and it also has better image quality and is more robust than guidance-based and guidance-free methods across different conditions.", "description": "This figure shows a qualitative comparison of the conditional generation results from Ctrl-X and various baseline methods.  It demonstrates that Ctrl-X achieves comparable or better performance across several metrics, including structure control and prompt alignment, compared to the training-based methods (ControlNet, T2I-Adapter) and guidance-based/guidance-free methods (FreeControl, SDEdit, Prompt-to-Prompt, Plug-and-Play, InfEdit). The improved image quality and robustness of Ctrl-X are also highlighted.  Various conditions are tested, illustrating the versatility of the Ctrl-X approach.", "section": "5.1 T2I diffusion with structure and appearance control"}, {"figure_path": "ZulWEWQOp9/figures/figures_18_1.jpg", "caption": "Figure 14: Additional results of structure and appearance control. We present additional Ctrl-X results of structure and appearance control.", "description": "This figure shows more examples of the results generated by the Ctrl-X model.  The top row shows examples where the input structure is a normal map and the input appearance is a photo, painting, and artistic rendering of horses.  The bottom row shows similar experiments but with the input structure being a hand-drawn sketch, a 3D model, and color blocks of a room, with the corresponding output images again showing photos, paintings, and artistic renderings of rooms.", "section": "Additional results"}, {"figure_path": "ZulWEWQOp9/figures/figures_19_1.jpg", "caption": "Figure 15: Appearance-only control. Ctrl-X can do appearance-only control by dropping the structure control branch. Compared to IP-Adapter [43], our method shows better appearance alignment for both subjects and backgrounds.", "description": "This figure compares the appearance-only control results of Ctrl-X and IP-Adapter.  It demonstrates that Ctrl-X, even without structure guidance, achieves better alignment of appearance from an input image to generated images than IP-Adapter. This is shown across various subject matters including giraffes, tigers, houses, and living rooms.  The improved alignment suggests Ctrl-X's strength in transferring appearance details accurately.", "section": "5.1 T2I diffusion with structure and appearance control"}, {"figure_path": "ZulWEWQOp9/figures/figures_19_2.jpg", "caption": "Figure 6: Multi-subject generation. Ctrl-X is capable of multi-subject generation with semantic correspondence between appearance and structure images across both subjects and backgrounds. In comparison, ControlNet + IP-Adapter [44, 43] often fails at transferring all subject appearances.", "description": "This figure demonstrates the capability of Ctrl-X in generating images with multiple subjects while maintaining semantic correspondence between the appearance and structure images.  It showcases Ctrl-X's ability to handle complex scenes with multiple subjects and backgrounds, successfully transferring appearance features across all elements.  In contrast, a comparison with ControlNet + IP-Adapter highlights Ctrl-X's superior performance, as the latter often struggles to transfer appearances consistently to all subjects within a scene.", "section": "Experiments"}, {"figure_path": "ZulWEWQOp9/figures/figures_20_1.jpg", "caption": "Figure 17: Extension to text-to-video (T2V) models. Ctrl-X can be directly applied to T2V models for controllable video structure and appearance control, with AnimateDiff [9] with Realistic Vision v5.1 [32] and LaVie [39] here as examples. A playable video version of the AnimateDiff results can be found in the attached supplementary zip file as ctrl_x_animatediff.mp4.", "description": "This figure shows several examples of applying Ctrl-X to text-to-video (T2V) models for controllable video structure and appearance.  The examples utilize AnimateDiff with Realistic Vision and LaVie models.  It demonstrates Ctrl-X's ability to transfer both structure and appearance to video generation.  A video of the AnimateDiff results is available in a supplementary file.", "section": "Extension to video diffusion models"}]