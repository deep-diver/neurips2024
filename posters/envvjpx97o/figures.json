[{"figure_path": "eNvVjpx97O/figures/figures_0_1.jpg", "caption": "Figure 1: Attention map visualization. (a) Llama-2-7B/Chat with \u201c</s>\u201d and \u201c\\n\u201d as EoU (\u201c</s>\u201d counts as one token, \u201c\\n\u201d as two). (b) StreamingLLM versus StreamingDialogue attention on Llama-2-7B with \u201c</s>\u201d as EoU.", "description": "This figure visualizes the attention maps of different LLMs.  (a) shows the attention patterns of Llama-2-7B and Llama-2-7B-Chat models, highlighting how the end-of-utterance tokens (</s> and \\n) aggregate attention. (b) compares the attention mechanisms of StreamingLLM and the proposed StreamingDialogue, demonstrating that StreamingDialogue focuses attention more effectively on the end-of-utterance tokens, which are crucial for aggregating utterance information.  This difference in attention distribution is key to StreamingDialogue's ability to handle longer contexts.", "section": "1 Introduction"}, {"figure_path": "eNvVjpx97O/figures/figures_3_1.jpg", "caption": "Figure 2: StreamingDialogue framework. SMR & LMR strategies co-train the model by adjusting attention mechanisms. In supervised learning, the SMR & LMR-trained model is fine-tuned with dialogue datasets. During inference, only specific tokens are cached, with critical historical dialogue information in bold italics for clarity.", "description": "This figure illustrates the StreamingDialogue framework, highlighting the two main learning strategies: short-memory reconstruction (SMR) and long-memory reactivation (LMR).  It shows how these strategies work together to co-train the model by adjusting attention mechanisms.  The figure also depicts the supervised learning phase and the inference phase, where specific tokens (EoU tokens and recent tokens) are cached to maintain efficiency.  The bold italics in the inference section emphasize how the critical historical information is maintained for coherent responses.", "section": "3.2 Framework overview"}, {"figure_path": "eNvVjpx97O/figures/figures_6_1.jpg", "caption": "Figure 1: Attention map visualization. (a) Llama-2-7B/Chat with \u201c</s>\u201d and \u201c\\n\u201d as EoU (\u201c</s>\u201d counts as one token, \u201c\\n\u201d as two). (b) StreamingLLM versus StreamingDialogue attention on Llama-2-7B with \"</s>\" as EoU.", "description": "The figure visualizes attention maps from different LLMs.  Panel (a) shows attention maps from Llama-2-7B and Llama-2-7B-Chat models, highlighting how end-of-utterance tokens (EoU) aggregate attention. Panel (b) compares the attention patterns of StreamingLLM and the proposed StreamingDialogue approach on Llama-2-7B, demonstrating how StreamingDialogue focuses attention on conversational attention sinks (EoUs) for efficient long-context processing.", "section": "1 Introduction"}, {"figure_path": "eNvVjpx97O/figures/figures_7_1.jpg", "caption": "Figure 4: Average perplexity and BLEU for StreamingLLM and StreamingDialogue on the MSC test set across varying utterance counts.", "description": "This figure compares the performance of StreamingLLM and StreamingDialogue on the Multi-Session Chat (MSC) test dataset across different lengths of conversation (measured by the number of utterances).  Specifically, it shows how average perplexity (a measure of how well the model predicts the next token) and BLEU score (a measure of the similarity between generated text and reference text) change as the number of utterances in the conversation increases.  The goal is to illustrate how well each method handles increasing context length in dialogue generation.", "section": "4.5 Performance on different context length"}, {"figure_path": "eNvVjpx97O/figures/figures_7_2.jpg", "caption": "Figure 10: Attention maps\u2019 visualization of StreamingDialogue and various other methods. In a dialogue with T utterances, each averaging L tokens, dense attention caches TL tokens, local attention caches R tokens (where R is the window size), Big Bird caches global size + random size + R tokens, StreamingLLM caches R + 1 tokens, and StreamingDialogue requires caching up to 1+T+2L tokens.", "description": "This figure compares the memory usage of different attention mechanisms (dense, local, Big Bird, StreamingLLM) with the proposed StreamingDialogue approach.  It visually represents how many tokens each method needs to cache in memory during dialogue processing.  Dense attention has the highest memory requirement, scaling quadratically with the number of tokens (TL).  Methods like local and Big Bird aim to reduce this by using windows, but still have significant memory demands.  StreamingLLM uses attention sinks, caching a limited window plus the sinks.  StreamingDialogue optimizes this further by only caching the first token, the attention sinks (representing utterances), and the last two utterances, resulting in the lowest memory footprint (1+T+2L).", "section": "A Dataset details"}, {"figure_path": "eNvVjpx97O/figures/figures_7_3.jpg", "caption": "Figure 6: The perplexity for StreamingDialogue under the concatenated MSC test set, evaluating approximately 25K tokens.", "description": "This figure shows the perplexity (a measure of how well the model predicts the next token) for the StreamingDialogue model when tested on the concatenated MSC test set (a collection of multiple dialogues).  The x-axis represents the number of utterances processed.  The y-axis represents the perplexity. The graph demonstrates the stability of StreamingDialogue's perplexity even with a very long dialogue context (25,000 tokens), highlighting its ability to handle prolonged conversations.", "section": "4.5 Performance on different context length"}, {"figure_path": "eNvVjpx97O/figures/figures_8_1.jpg", "caption": "Figure 7: Comparison of attention maps before and after learning. \"Base\" denotes Llama-2-7B, while \"SMR & LMR\" represents the model obtained post co-training with SMR and LMR on Llama-2-7B. The \"\\<\\/s\\>\" positions in the encoded sentences are: 3, 6, 13, and 21.", "description": "This figure compares attention maps of the base Llama-2-7B model and the model after training with short-memory reconstruction (SMR) and long-memory reactivation (LMR). The visualization shows that after SMR & LMR training, the model's attention focuses more sharply on the end-of-utterance tokens, demonstrating the effectiveness of the proposed learning strategies in enhancing information aggregation.", "section": "4.9 Impacts of SMR & LMR learning"}, {"figure_path": "eNvVjpx97O/figures/figures_8_2.jpg", "caption": "Figure 10: Attention maps\u2019 visualization of StreamingDialogue and various other methods. In a dialogue with T utterances, each averaging L tokens, dense attention caches TL tokens, local attention caches R tokens (where R is the window size), Big Bird caches global size + random size + R tokens, StreamingLLM caches R + 1 tokens, and StreamingDialogue requires caching up to 1+T+2L tokens.", "description": "This figure visualizes the attention mechanisms used in various methods for handling long dialogues.  It compares dense attention, local attention, Big Bird, StreamingLLM, and StreamingDialogue (the proposed method).  It shows how many tokens each method needs to cache to maintain context during dialogue generation.  Dense attention caches the largest number, while StreamingDialogue efficiently caches a significantly smaller number, demonstrating its memory efficiency advantage.", "section": "3.1 Empirical observation"}, {"figure_path": "eNvVjpx97O/figures/figures_9_1.jpg", "caption": "Figure 9: Normalized performance scores (PPL, B-avg, R-L, and D-3) on MSC for various l with s fixed at 28 and various s with l fixed at 24.", "description": "This figure shows the impact of hyperparameters *s* (number of utterances in SMR samples) and *l* (number of query-response pairs in LMR samples) on the model's performance.  The x-axis represents the values of *s* and *l*, ranging from 8 to 32. The y-axis shows the normalized scores for perplexity (PPL), BLEU (B-avg), ROUGE-L (R-L), and Distinct-3 (D-3). The results indicate that higher values of both *s* and *l* generally lead to better performance, with optimal values around *s* = 28 and *l* = 24.", "section": "4.13 Hyper-parameter sensitives"}, {"figure_path": "eNvVjpx97O/figures/figures_15_1.jpg", "caption": "Figure 10: Attention maps\u2019 visualization of StreamingDialogue and various other methods. In a dialogue with T utterances, each averaging L tokens, dense attention caches TL tokens, local attention caches R tokens (where R is the window size), Big Bird caches global size + random size + R tokens, StreamingLLM caches R + 1 tokens, and StreamingDialogue requires caching up to 1+T+2L tokens.", "description": "This figure compares the memory usage of different attention mechanisms for handling long dialogues.  Dense attention has quadratic complexity, needing to cache all token pairs.  Local attention and Big Bird are more efficient but still require a linearly increasing amount of memory with dialogue length. StreamingLLM introduces attention sinks to reduce memory, but StreamingDialogue further improves upon this by identifying \u2018conversational attention sinks\u2019 (EoUs) \u2013 tokens such as end-of-utterance markers \u2013 that effectively summarize utterances, allowing for quadratic memory savings.", "section": "3 StreamingDialogue"}, {"figure_path": "eNvVjpx97O/figures/figures_18_1.jpg", "caption": "Figure 11: Attention maps under different settings.", "description": "This figure compares attention maps from different models (BERT, GLM, Llama2) under different settings (two-party and multi-party dialogues).  It visually demonstrates that regardless of the model or dialogue structure, End-of-Utterance (EoU) tokens consistently attract significantly more attention than other tokens. This visual evidence supports the paper's core argument that EoU tokens act as \"conversational attention sinks\", effectively aggregating information within the dialogue context.", "section": "4.11 Analysis of EoU tokens' information aggregation capability"}]