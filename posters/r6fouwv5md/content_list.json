[{"type": "text", "text": "Understanding Model Selection for Learning in Strategic Environments ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tinashe Handina ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eric Mazumdar ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computing $^+$ Mathematical Sciences   \nCalifornia Institute of Technology Pasadena, CA 91125 thandina@caltech.edu   \nComputing $^+$ Mathematical Sciences   \nCalifornia Institute of Technology Pasadena, CA 91125 mazumdar@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model class one optimizes over\u2014and the more data one has access to\u2014the more one can improve performance. As models get deployed in a variety of real-world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects the relationship between performance at equilibrium and the expressivity of model classes. We find that strategic interactions can break the conventional view\u2014meaning that performance does not necessarily monotonically improve as model classes get larger or more expressive (even with infinite data). We show the implications of this result in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning. In particular, we show that each of these settings admits a Braess\u2019 paradox-like phenomenon in which optimizing over less expressive model classes allows one to achieve strictly better equilibrium outcomes. Motivated by these examples, we then propose a new paradigm for model selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning\u2014and deep learning in particular\u2014 has already demonstrated enormous potential to enable new services across a wide spectrum of everyday life. Examples range from chatbots [1], to hiring [2], and content moderation [3]. Driving this proliferation is the fact that the increasing availability of compute resources coupled with the abundance of data provided by internet-scale datasets allows one to train larger and larger models while monotonically improving performance [4, 5, 6]. Despite signs of diminishing returns, this consensus has broadly held true. Machine learning algorithms tend to follow a monotonic scaling law: with more compute and data, one can train more expressive models and eke out performance gains. ", "page_idx": 0}, {"type": "text", "text": "As algorithms are deployed into real-world scenarios, however, they will inevitably come into contact with some form of strategic decision-making\u2014whether that be in the form of adversarial agents attempting to manipulate the output of the algorithm [7], gig-workers taking actions to enforce better working conditions from learning-powered platforms [8], or more broadly individuals whose goals are misaligned with those of the algorithm [9]. ", "page_idx": 0}, {"type": "text", "text": "Reflecting this reality, recent years have seen a surge in research seeking to understand the effects of strategic decision-making on learning algorithms. Two sub-fields in particular include strategic classification [10]\u2014in which one seeks to learn a classifier or predictor in the presence of agents who strategically manipulate data\u2014 and multi-agent reinforcement learning [11]\u2014 in which agents attempt to learn optimal decision-making policies in the presence of other learning agents. Both of these domains draw on ideas from game theory and economics to understand how to design algorithms for strategic settings. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In these different regimes, a common refrain is that the presence of strategic interactions invalidates many of the foundational assumptions underlying many machine learning algorithms. For example, strategic interactions result in highly non-stationary environments for multi-agent reinforcement learning (MARL) [11] and seemingly innocuous design decisions like stepsize choices and gradient estimators have been shown to give rise to qualitatively different outcomes in strategic classification [12]. Despite these works, our understanding of model class selection in strategic settings remains underdeveloped. To that end, in this paper, we consider the following question: ", "page_idx": 1}, {"type": "text", "text": "How do strategic interactions affect the relationship between model class expressivity and equilibrium performance? ", "page_idx": 1}, {"type": "text", "text": "Contributions: We show through simple theoretical models, illustrative examples, and experiments that strategic interactions can yield a non-trivial relationship between model class expressivity and equilibrium performance. In particular, we show how\u2014even in highly structured regimes in which one has full access to the underlying data distribution\u2014strategic interactions can result in a Braess\u2019 paradox-like phenomenon: the larger and more expressive the model class a learner optimizes over, the lower their performance at equilibrium. ", "page_idx": 1}, {"type": "text", "text": "To understand why this is possible, we make links with the literature in economics on comparative statics, which seeks to understand how the equilibria of games vary with exogenous factors. We show that even in convex games with a unique equilibrium, if the equilibrium is not Pareto optimal (i.e., there exists coordinated deviations that improve the utilities of both players), then there always exists a unilateral restriction of one\u2019s action set over which one could have played and had a better equilibrium outcome. Conversely, we show that if an equilibrium is Pareto optimal (which encompasses not only traditional optimization but also adversarial games), performance at equilibrium will tend to scale monotonically with respect to model class expressivity. To make this result concrete, we give examples of strategic regression, strategic classification, and MARL in which reverse scaling occurs. ", "page_idx": 1}, {"type": "text", "text": "Our result suggests that\u2014if the model will be deployed into a strategic environment\u2014 the choice of model class should be treated as a strategic action. Following up on this observation, we formulate a problem of model-selection in games. Whereas learning in games traditionally takes the action set for a player as given, we propose a new formulation in which a player has a number of action sets to choose from and must find the one that yields the best payoff. As a proof-of-concept, we provide an algorithm to identify the best set in a class of structured games. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Before describing our model and results, we comment on related work in both machine learning and economics. ", "page_idx": 1}, {"type": "text", "text": "Scaling laws in Machine Learning: Within statistical machine learning, the study of scaling laws is motivated by the task of choosing a sufficiently expressive class of models to optimize over a given dataset [13]. Non-monotonicity of scaling laws emerged classically due to overftiting [14]\u2014in which the model is too expressive relative to the size of the data set, which can degrade performance at deployment. Such problems are fundamentally linked to understanding the behavior of the empirical risk as one optimized over larger and larger model classes [15]. ", "page_idx": 1}, {"type": "text", "text": "Deep learning upended this way of thinking with the development of a theory for generalization beyond what is called the threshold of interpolation[16]. More recently, work has investigated how the performance of large language models scales with expressivity (measured in the number of parameters) [4, 5, 6] and the size of the dataset it is trained on [17]. In each of these works, the scaling laws increase monotonically in both the amount of data and expressivity. ", "page_idx": 1}, {"type": "text", "text": "In our paper, we sidestep issues of sample complexity (i.e., dataset size) to isolate the interplay between expressivity and strategic interactions. While the minimum of the population risk for supervised learning monotonically decreases because optimizing over a larger space can only improve performance, we show that the population risk is non-monotonic in strategic environments. Thus, the phenomenon that we highlight holds even without consideration of sample sizes or generalization errors and adds to a growing body of literature on the difficulties of learning in game theoretic environments. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Learning in strategic environments: Recent years have seen a surge of interest in understanding the effects of strategic interactions on learning algorithms. Some of the most relevant areas of interest are strategic classification [10] and performative prediction [18], adversarial machine learning [7], and multi-agent reinforcement learning [11]. A unifying theme across these areas is the integration of ideas from game theory into problems of machine learning, wherein one seeks to learn an optimal model given the presence of strategic agents who may themselves be learning. For example, papers on strategic classification [19], strategic regression [20], and participation dynamics [21, 22] all analyze games in which a learner deploys learning algorithms in game-theoretic environments. Similarly, work on MARL naturally builds upon the foundation of Markov games [23, 24]. ", "page_idx": 2}, {"type": "text", "text": "One can view all of these problems as an instance of learning in games [25]\u2014which has seen a resurgence in the machine learning literature in recent years due to these connections [26, 27]. In this paper, we adopt, in particular, the framework of continuous games [28] in which players\u2019 action sets can be compact convex subsets of $\\mathbb{R}^{n}$ . ", "page_idx": 2}, {"type": "text", "text": "In this class of games, recent work has made clear that learning can be much more complex than in stationary environments\u2014 with non-trivial consequences including instability and convergence to cycles and chaos when using gradient-based learning [29], small design choices like stepsizes and gradient estimators leading to different equilibria [12], and strategic manipulations allowing for better causal discovery [30]. ", "page_idx": 2}, {"type": "text", "text": "The question of whether our current understanding of scaling laws holds in these environments is still relatively understudied. Recent empirical work has shown that scaling laws in zero-sum MARL mirror those for deep reinforcement learning and deep learning more generally [31]. Most relevant to our work is a recent paper that studied the non-monotonicity of users\u2019 social welfare as firms deploy larger and larger models [32]. The paper considers an environment in which multiple firms compete over a set of users and analyzes the welfare of the users as all firms choose more complex models. They show through a simple model and extensive experiments that if all firms use larger models, the users\u2019 welfare can decrease. In this paper, we formulate a more general model that encompasses their interaction and more general problems of strategic classification and MARL. We take an orthogonal track, which is to ask whether it is rational for self-interested learners to unilaterally restrict the expressivity of their models in strategic settings. We show that this is indeed the case under certain conditions. ", "page_idx": 2}, {"type": "text", "text": "Changing equilibrium outcomes in game theory: Finally, we would be remiss if we did not discuss the large body of work in economics that studies changes in equilibrium outcomes in games. A similar phenomenon to the one we highlight is the well-known Braess\u2019 paradox in strategic routing [33] in which one can add a road to a network and increase congestion. Even more related is the informational Braess\u2019 paradox [34] in which more information over the network can yield worse equilibrium outcomes for agents in routing games. Many classic works in dynamic game theory have also highlighted the unintuitive ways information and statistical estimation affect equilibrium outcomes in games [35, 36]. ", "page_idx": 2}, {"type": "text", "text": "More generally, a large body of work in economics studies comparative statics\u2014i.e., how equilibrium payoffs change as exogenous variables are changed [37]. The literature has mostly been concerned with deriving conditions under which payoffs change monotonically in the exogenous variables, a field known as a monotone comparative statics [38]. Our work can be seen as an attempt to understand these ideas in the context of strategic machine learning. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To understand the dependencies between strategic decision-making and model complexity, we examine different strategic environments. In our model, the learner has access to an ordered set of model classes A, which are all subsets of one large class $\\Omega$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. A set of model classes $\\mathbb{A}=\\{\\Theta_{k}\\}_{k=1}^{N}$ is ordered if for all $\\Theta_{i},\\Theta_{j}\\,\\in\\mathbb{A}$ , if $i<j$ it implies that $\\Theta_{i}\\subseteq\\Theta_{j}$ ", "page_idx": 2}, {"type": "text", "text": "The set A may be a set of nested policy classes in MARL or a set of neural network architectures of increasing size for strategic classification. Importantly, the model classes have monotonically increasing expressivity when measured in classic notions of expressivity like V-C dimension [15]. ", "page_idx": 3}, {"type": "text", "text": "Before engaging with the strategic environment, the learner chooses a model class $\\Theta_{i}\\,\\in\\,\\mathbb{A}$ over which to optimize. In some instances, we refer to model classes as action spaces, and we use these two terms interchangeably. The selection of a model class fixes the optimization problem the learner will then attempt to solve through interactions with the environment. We model interactions with the environment as a two-player game and assume that players find equilibrium outcomes. We assume the learner has a loss function $f_{l}:\\Omega\\times\\mathcal{E}\\to\\mathbb{R}$ which they seek to minimize that also depends on the action of the environment. Similarly, the environment will have a loss function $f_{e}:\\Omega\\times\\mathcal{E}\\to\\mathbb{R}$ . Here $\\Omega$ is the learner\u2019s action space whilst $\\mathcal{E}$ is the environment\u2019s action space. ", "page_idx": 3}, {"type": "text", "text": "To model different strategic interactions, we make different assumptions on the nature of the game played and the equilibrium outcomes. We focus on four types of strategic environments: Stationary Environments where the environment actor only has a single action, Stackelberg Environments where the Learner leads, Stackelberg Environments where the learner follows, and General Nash Environments. We provide a concrete description of each of these environments in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "In the next section, we analyze General Nash environments and show that payoffs do not necessarily monotonically increase in expressivity. In Appendix A.2, we provide a set of theoretical results that show how equilibrium payoffs are monotonically increasing in expressivity in Stationary and Stackelberg environments where the learner leads. We then show in Appendix A.3 how payoffs also do not necessarily monotonically increase in expressivity in Stackelberg environments where the learner follows. ", "page_idx": 3}, {"type": "text", "text": "3 Non-Monotonic Scaling of Performance in Nash Settings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our investigation of the relationship between model class expressivity and equilibrium performance in Nash setting. We show how even under strong assumptions on the regularity of the game, there always exists a way for a player to restrict their model class, resulting in a game with a Nash equilibrium that has a lower loss if the original Nash equilibrium is not Pareto optimal (i.e., the game is not zero-sum or strategically zero-sum). We note that this is a negative result which is an existence proof. To concretely establish this phenomenon, we illustrate through examples in multi-agent reinforcement learning and strategic classification how in settings where these assumptions are relaxed, this phenomenon still exhibits itself. ", "page_idx": 3}, {"type": "text", "text": "To prove our main result, we assume the two-player game is strongly monotone on the space $\\Omega\\times\\mathcal{E}\\subset\\mathbb{R}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. A two-player game is $\\mu$ -strongly monotone if the generalized gradient operator $F:\\Omega\\times\\mathcal{E}\\to\\mathbb{R}^{n}$ given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(x)=\\left[\\nabla_{\\theta}f_{l}(\\theta,e)\\right]{\\mathrm{~}}\\;{\\mathrm{where:~}}x=(\\theta,e),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle F(x)-F(x^{\\prime}),x-x^{\\prime}\\rangle\\geq\\mu\\|x-x^{\\prime}\\|^{2}\\ \\forall\\,x,x^{\\prime}\\ \\in\\ \\Omega\\times\\mathcal{E}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A strongly monotone game is a convex game [28]. Implicitly, it assumes that the two players\u2019 losses are $\\mu$ strongly convex in their own action and makes a further assumption on the interaction between players\u2019 actions [27]. The assumption of strong monotonicity ensures that there is always a unique Nash equilibrium and that issues of multiple equilibria do not arise. ", "page_idx": 3}, {"type": "text", "text": "This assumption is once again made to isolate the phenomenon of interest. In the case with multiple Nash equilibria we believe that it is possible to have different equilibrium outcomes exhibiting different scaling behavior\u2014though we leave such analyses for future work. We remark that we make these assumptions for illustrative purposes and that many of our numerical experiments show the same result under milder game structures. ", "page_idx": 3}, {"type": "text", "text": "On top of the assumption of strong monotonicity we require several smoothness conditions on the players\u2019 objectives as well as an assumption that their interaction is not trivially zero at Nash. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. Assume the game defined on $f_{e}$ and $f_{l}$ is strongly monotone on $\\Omega\\times\\mathcal{E}$ . Further assume that ", "page_idx": 3}, {"type": "image", "img_path": "R6FOuWv5MD/tmp/75f643334b52fa4e9b906d14f902d54ebc1d71c6fac9adcc2f7c5d49b3aad946.jpg", "img_caption": ["Figure 1: (a.) A visual description of a 2-player Markov game in which the learner can unilaterally increase their payoff by restricting the expressivity of their policy class. (b.) the payoff of the learner at Nash in a 50-state version of this Markov game as their policy class is restricted to take the form $\\pi_{l}(s)=[p,1-p]$ in all states $s$ for $p\\in[1-\\bar{p},\\bar{p}]$ for different discount factors (assumed to be the same for both players). In all cases, we see the learners\u2019 payoff broadly increase at Nash as they optimize over smaller policy classes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "1. $f_{l}$ and $f_{e}$ are jointly convex in $\\theta$ and $e$ . ", "page_idx": 4}, {"type": "text", "text": "2. The gradient mappings, $\\nabla f_{l}$ and $\\nabla f_{e}$ exist and are well defined for all $(\\theta,e)$ . Furthermore, the gradient mappings are $L$ -Lipschitz continuous in the joint action space. ", "page_idx": 4}, {"type": "text", "text": "3. The Nash equilibrium $\\theta^{*}\\in\\Theta$ is on the interior of $\\Theta$ with $\\nabla_{\\theta}B R_{e}(\\theta^{*})\\neq0$ . ", "page_idx": 4}, {"type": "text", "text": "To show how the restriction of a model class yields a decrease in loss in a large class of games, we leverage the idea that in many games, a Nash equilibrium is not necessarily a Pareto optimal point [39]. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3. A point $(\\theta,e)\\in\\Omega\\times\\mathcal{E}$ is Pareto-optimal, if there does not exist $({\\hat{\\theta}},{\\hat{e}})$ such that $f_{l}(\\hat{\\theta},\\hat{e})<f_{l}(\\theta,e)$ and $f_{e}(\\hat{\\theta},\\hat{e})\\leq f_{e}(\\theta,e)$ 2 ", "page_idx": 4}, {"type": "text", "text": "Given these assumptions, we prove the following theorem. For ease of exposition, we defer the proof to Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. For a two-player monotone game $G$ on $\\Theta\\times{\\mathcal{E}}$ which satisfies Assumption 3.2, if the unique Nash equilibrium in $\\Theta\\times{\\mathcal{E}}$ , $(\\theta^{*},e^{*})$ , is not Pareto optimal then there exists a restriction of the learner\u2019s model class (i.e., a set $\\Theta^{\\prime}\\subset\\Theta.$ ) such that the restricted game $G^{\\prime}$ on $\\Theta^{\\prime}\\times\\mathcal{E}$ admits $a$ Nash equilibrium $(\\theta^{\\prime},e^{\\prime})$ with: $f_{l}(\\theta^{\\prime},e^{\\prime})<f_{l}(\\theta^{*},e^{*})$ . ", "page_idx": 4}, {"type": "text", "text": "This theorem highlights the fact that the non-monotonicity of scaling laws is, in fact, something we should expect in large classes of games. Indeed, even under mild conditions one can show that there always exists a unilateral restriction that improves payoffs. ", "page_idx": 4}, {"type": "text", "text": "While the theorem guarantees the existence of a unilateral restriction, which improves equilibrium performance, we remark that it does not say anything about the ease with which one can find this restricted space. While the proof is constructive, it makes use of information of the environment\u2019s loss to construct the set\u2014information that may not always be available to the learner. Furthermore, as we show in the following examples, the non-monotonicity can play out in complex ways. ", "page_idx": 4}, {"type": "text", "text": "Example 1: Multi-Agent Reinforcement Learning We first demonstrate an extreme form of the reverse scaling predicted by Theorem 3.4 in the context of multi-agent reinforcement learning. To do so, we construct a Markov game in which the more the learner restricts their policy class, the more their expected payoff increases. Note that in keeping with the language of MARL, we consider the case when both players would like to maximize their long-run discounted rewards. ", "page_idx": 4}, {"type": "text", "text": "The Markov game in question is a two-player game with $n$ states. In each state $s_{i}$ , with $i\\ \\in$ $\\{1,2,\\ldots,n\\}$ , both players have two actions available to them $\\{0,1\\}$ with 0 corresponding to the top row or left column. In each state, the environment is allowed to choose a policy that is unrestricted, meaning that $\\pi_{e}(s)=[p,1-p]$ for any $p\\in[0,1]$ . The learner can choose from policies such that: $\\pi_{l}(s)=[p,1-p]$ for all $p\\in[1-\\bar{p},\\bar{p}]$ for some $\\bar{p}\\in[0.5,1]$ . Varying $\\bar{p}$ generates model classes of varying expressivity. For example, when $\\bar{p}=1$ , then they are allowed to choose any policy, and for $\\bar{p}=0.5$ , they are constrained to only playing uniform policies. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Given actions $a,b$ from the learner and environment, respectively, we define the transition probabilities as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(s_{i+1}|s_{i},a=0,b=1)=p(s_{i+1}|s_{i},a=1,b=1)=1}\\\\ &{\\quad p(s_{i}|s_{i},a=0,b=0)=p(s_{i}|s_{i},a=1,b=0)=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, the transitions are deterministic, given the action of the environment. This results in the following utilities for the two players, which are simply their sum of discounted rewards3: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{i}(\\pi_{l},\\pi_{e})=\\mathbb{E}_{\\pi_{l},\\pi_{e}}\\left[\\sum_{t=0}^{\\infty}\\gamma_{i}^{t}R_{i}(s_{t},a_{t},b_{t})\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $i\\in\\{e,l\\}$ and $\\gamma_{e},\\gamma_{l}$ are the player\u2019s discount factors. We construct the payoffs for the learner such that they have a dominant strategy of $\\pi_{l}(s)=[\\bar{p},1\\!-\\!\\bar{p}]$ in all states, and their expected cumulative payoff increases as the players end up further along the chain of states (as seen in Figure 1). ", "page_idx": 5}, {"type": "text", "text": "We construct the payoffs of the environment player such that they trigger a switch to the next state if and only if the probability that the learner puts on action 0 is below some threshold $p^{*}(s)$ . We do so for a sequence of thresholds $p_{i}^{*}$ for $i=1,...,n$ such that $p_{i}^{*}>p_{i+1}^{*}>0.5$ . With this construction, for $p_{i+1}^{*}<\\bar{p}<p_{i}^{*}$ the game will result in the players staying in state $s_{i}$ for all time. More details on the construction of the payoff matrices are left to Appendix $\\boldsymbol{\\mathrm E}$ and a plot of the equilibrium rewards for the learner as a function of $\\bar{p}$ is shown in Figure 1 for different discount factors for games having $n=50$ states. ", "page_idx": 5}, {"type": "text", "text": "We empirically observe that as $\\bar{p}$ decreases (i.e., the policy class of the learner is restricted), the performance of the learner behaves in non-monotonic ways and can, in fact, be made to increase as the policy class gets closer to the uniform policy. The highly non-convex nature of the case where $\\gamma=0.95$ also highlights the difficulty in choosing a model class in general since it can be posed as a non-convex optimization problem. ", "page_idx": 5}, {"type": "text", "text": "A key takeaway of this example is that in general-sum MARL, restrictive policy parametrizations like e.g., softmax policies or function approximation may not lead to worse performance at equilibrium like in competitive and single-agent RL [40]. Indeed our example suggests that the payoff in quantal response equilibria [41] of Markov games (i.e., equilibria in which agents constrain their strategies to a class of quantal responses\u2013see e.g., [41]) can sometimes have a higher payoff than the unrestricted Nash equilibrium. ", "page_idx": 5}, {"type": "text", "text": "Example 2: Participation dynamics Our second example is similar to problems considered in the literature on performative prediction [18] though the setup we consider also fits the literature on understanding participation dynamics [32, 21] and algorithmic collective action [9]. ", "page_idx": 5}, {"type": "text", "text": "In this model, there is a base distribution ${\\mathcal P}_{0}$ over the input-output space $\\mathcal X\\times\\mathcal X$ where $\\mathcal{X}$ is feature space and $\\boldsymbol{\\wp}$ is the output space. The learner is trying to perform supervised learning to learn a mapping $g:\\mathcal{X}\\mapsto\\mathcal{Y}$ . The environment, on the other hand, takes the form of a population of agents that selects a distribution on $\\mathcal{P}$ on the input-output space (i.e., $\\mathcal{P}\\in\\Delta(\\mathcal{X},\\mathcal{Y}))$ to maximize their own utility which depends on the choice of the learner. ", "page_idx": 5}, {"type": "text", "text": "The least restrictive class of models the learner has access to $\\Omega$ is the set of all functions $g:\\mathcal{X}\\to\\mathcal{Y}$ . We also consider a restricted function class $\\Theta$ which is the class of all functions $g_{r}:\\mathcal{X}^{\\prime}\\to\\mathcal{Y}$ where $\\mathcal{X}^{\\prime}\\subset\\mathcal{X}$ is the result of some feature mapping $\\phi:{\\mathcal{X}}\\rightarrow{\\mathcal{X}}^{\\prime}$ . Thus, $\\Theta$ is the space of all functions of the form $g_{r}(\\phi(x))$ . Clearly $\\Theta\\subset\\Omega$ . ", "page_idx": 5}, {"type": "text", "text": "We assume that the strategic manipulations of the environment take the form of manipulations to the data distribution which take place by mixing the base distribution $\\mathcal{P}_{0}$ with a manipulated data distribution $\\mathcal{P}_{e}$ such that the distribution seen by the learner is given by $\\mathcal P=\\alpha\\mathcal P_{e}+(1-\\alpha)\\mathcal P_{0}$ for some $\\alpha\\in[0,1]$ . The parameter $\\alpha$ relates to the strength of the response distribution within the mixture that the learner observes. It might represent the fraction of the population that engages in strategic manipulations of their data. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Finally, we assume that the learner is optimizing the zero-one loss, such that for any distribution $\\mathcal{P}$ , the best response $g$ or $g_{r}$ is the Bayes-Optimal classifier on $\\mathcal{X}$ and $\\mathcal{X^{\\prime}}$ respectively are: ", "page_idx": 6}, {"type": "equation", "text": "$$\ng^{\\ast}(x)=\\arg\\operatorname*{max}_{y}\\mathcal{P}(y|x)\\;\\&\\;g_{r}^{\\ast}(x)=\\arg\\operatorname*{max}_{y}\\mathcal{P}(y|\\phi(x)).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, the learner\u2019s loss at equilibrium is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{l}(g^{*},\\mathcal{P})=P r(g^{*}(x)\\neq y)}\\\\ &{f_{l}(g_{r}^{*},\\mathcal{P})=P r(g_{r}^{*}(x)\\neq y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "in each case, the probability is taken with respect to $\\mathcal{P}$ . ", "page_idx": 6}, {"type": "text", "text": "For the population of strategic agents, we assume that they would like the learner to avoid making use of certain \u2018protected\u2019 features and focus on a set of restricted features $\\phi^{*}$ . To do so, the strategic agents\u2019 response to the learner\u2019s model depends on the set of features it makes use of. Concretely, if the learner makes use of a set of features $\\phi^{\\prime}:\\mathcal{X}\\rightarrow\\mathcal{X}^{\\prime}$ that are more informative than some $\\phi^{*}:\\mathcal{X}\\rightarrow\\mathcal{X}^{*}$ \u2014i.e., ${\\mathcal{X}}^{*}\\subset{\\mathcal{X}}^{\\prime}$ , then the strategic agents add uniform noise to the base distribution, and if not they report their true data. This can be represented by the following utility function: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{e}(g,\\mathcal{P})=\\left\\{T V(\\mathcal{P}_{e},U):P r(g(x)\\neq g(\\phi^{*}(x)))>0\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $U$ is the uniform distribution on $\\mathcal X\\times\\mathcal X$ and $T V$ represents the $T V$ distance between distributions. ", "page_idx": 6}, {"type": "text", "text": "As we will show, for sufficiently large $\\alpha$ , the learner is always better off optimizing over the less expressive model class at equilibrium. To do so, we assume that $\\phi^{*}$ preserves enough information for the Bayes optimal classifier on the space $\\textstyle{\\mathcal{X}}^{*}$ to be strictly better than random choice. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.5. Let $|\\mathcal{V}|=n$ . Assume that the Bayes optimal classifier on $\\mathcal{X}^{\\ast}$ for ${\\mathcal P}_{0}$ denoted $\\begin{array}{r}{g_{r}^{*}(x)=\\arg\\operatorname*{max}_{y\\in\\mathcal{y}}\\mathcal{\\dot{P}}(\\dot{y}|\\phi^{*}(x))}\\end{array}$ satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{l}(g_{r}^{*},\\mathcal{P}_{0})=P r(g_{r}^{*}(x)=y)<\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This leads to the following result for this game. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.6. Under Assumption 3.5, consider two functions classes over which the learner can optimize, $\\Omega$ , and $\\Theta$ which is the set of all functions from $g_{r}:\\mathcal{X}^{*}\\to\\mathcal{Y}$ , where $\\mathcal{X}^{*}=\\phi^{*}(\\mathcal{X})$ such that $\\phi^{*}(x)=x$ for $x\\in\\mathcal{X}^{*}$ . Consider the corresponding games denoted $G$ and $G^{*}$ , respectively. Then the Nash equilibrium in $G$ is $(g^{*},\\mathcal{P}^{*})$ where $\\mathcal{P}^{*}=\\bar{(1-\\alpha)}\\mathcal{P}_{0}+\\alpha U$ and the Nash equilibrium in $G^{*}$ is given by $(g_{r}^{*},\\mathcal{P}_{0})$ . Furthermore, there exists a range of $\\alpha\\in(0,1)$ such that: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{l}(g^{*},\\mathcal{P}^{*})>f_{l}(g_{r}^{*},\\mathcal{P}_{0})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of this proposition can be found in Appendix B.2. This proposition highlights the fact that interactions with strategic agents can make less expressive function classes yield better performance in strategic settings. ", "page_idx": 6}, {"type": "text", "text": "4 Online Learning for Model Selection in Games ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The previous results emphasize the importance of careful model selection in strategic environments. In this section, we consider the problem of learning the best model class to optimize in strategic environments. ", "page_idx": 6}, {"type": "text", "text": "Due to the unknown and non-stationary nature of the environment, in game theoretic settings, the learner will have to interact repeatedly with the environment to learn which model class and, consequently, which strategy to play. Thus, we formulate a problem of learning in games in which the learner seeks to find the best model class across a set of candidate model classes as well as the best strategy. We frame this as a problem of model selection for games. ", "page_idx": 6}, {"type": "text", "text": "We remark that model-selection is an area of recent interest in online learning [42, 43], though\u2014to the best of our knowledge\u2014the paradigm has not been applied to games as yet. Most similar to ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Stochastic gradient descent to find Nash equilibrium in a strongly monotone game this problem is a line of work on meta-learning in games, which seeks to find good strategies that generalize across environments [44]. ", "page_idx": 7}, {"type": "table", "img_path": "R6FOuWv5MD/tmp/db54c797e7ceb437900dc37f6d4a0a469e8148e27e1a15b0a4df169fc5791af0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We describe an algorithm for how the learner can select model classes to identify which model class to use. As a proof-of-concept, we assume that all players use stochastic gradient descent and adopt the structure of a problem we analyzed in the Nash environment regime. In particular, we assume the learner has access to sets of subsets of $\\Omega=\\mathbb{R}^{d}$ and that their loss and the environment\u2019s loss satisfy the following generalization of Assumption 3.2. For simplicity, we let the tuple of a particular model and the environment action be denoted by $x$ (i.e., $(\\theta,e)=x!$ ). We note here that $F$ is the generalized gradient mapping as described in Definition 3.1. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4.1. Assume the game defined on $f_{e}$ and $f_{l}$ is strongly monotone and that they are $L$ -Lipschitz continuous on $\\Omega\\times\\mathcal{E}$ . Further, assume that the players have access to stochastic gradient estimators such that the estimated monotone mapping $\\hat{F}$ satisfies, $\\forall x\\in\\Omega\\times\\mathcal{E}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\hat{F}}(x)]=F(x)~{\\mathrm{and}}~~\\mathbb{E}[\\|{\\hat{F}}(x)-F(x)\\|^{2}]\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Given this assumption and under the simplifying assumption that all players use decreasing stepsizes, we assume that for a given model class $\\Theta_{i}$ the players engage in projected stochastic gradient descent of the form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{x}_{t+1}=\\Pi_{\\Theta_{i}\\times\\mathcal{E}}\\left(\\boldsymbol{x}_{t}-\\eta_{t}\\hat{\\boldsymbol{F}}(\\boldsymbol{x}_{t})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Pi$ denotes the Euclidean projection onto $\\Theta_{i}\\times{\\mathcal{E}}$ . The pseudocode for this is described in Algorithm 1. We show that the running average of the iterates resulting from running this algorithm in an environment satisfying Assumption 4.1 concentrates quickly around the payoff at the Nash equilibrium. The proof of this proposition can be found in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2. Let $\\Theta$ correspond to a particular model class which results in an instance of continuous action \u00b5\u2212strongly monotone game with a unique Nash Equilibrium $x^{*}$ . Under Assumption $4.1$ and the assumption that all players use stepsize schedule $\\begin{array}{r}{\\eta_{t}=\\frac{\\dot{2}}{\\mu(t+1)}}\\end{array}$ , for any $\\delta\\in(0,1)$ Algorithm $^{\\,l}$ yields an estimate $\\hat{x}_{T}$ such that: ", "page_idx": 7}, {"type": "equation", "text": "$$\n|f_{l}(\\hat{x}_{T})-f_{l}(x^{*})|\\leq\\mathcal{O}\\left(\\frac{L^{2}\\log(\\frac{1}{\\delta})+L^{3}}{\\mu^{2}T}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 7}, {"type": "text", "text": "To derive this bound, we generalize an existing result from convex optimization [45]. Given these confidence bounds, we now propose a successive elimination algorithm for identifying the best model class in a game. The underlying assumption remains that the environment player is simply doing stochastic gradient descent. This should also extend to the case when the environment performs stochastic mirror descent [46]. The specific form of successive elimination is described in Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "As we show, this algorithm has strong properties in terms of identification of the best model class due to the fast concentration of our estimator from Proposition 4.2. We show the results with respect to identification and defer the proof to Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.3. Under the assumptions of Proposition 4.2, let $\\mathcal{A}=\\{\\Theta_{i}\\}_{i=1}^{n}$ . With probability at least $1-\\delta_{i}$ , Algorithm 2 identifies the model class whose Nash equilibrium yields the highest payoff after: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{n(L^{2}\\log(\\frac{n}{\\delta})+L^{3})}{\\mu^{2}\\Delta^{*}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "1: procedure SUCCESSIVEELIMINATION({\u0398i}in=1, \u03b4)   \n2: S \u2190{\u0398i}in=1   \n3: \u03c4 = 1   \n4: while $|S|>1$ do   \n5: T = \u03b12\u03c4   \n6: \u03b4\u2032 \u21902n\u03b4T 2   \n7: for all $\\vec{\\Theta_{i}}\\in S$ do   \n8: $x_{T}^{i}\\leftarrow\\mathrm{Algorithm}\\:1$ with $(\\Theta_{i},x_{0},T)$   \n9: end for   \n10: $S\\leftarrow S\\setminus\\{\\Theta_{i}\\in S:\\exists\\Theta_{j}$ such that:   \n11: $\\begin{array}{r l}&{f(x_{T}^{i})+\\frac{L^{2}\\log(\\frac{1}{\\delta^{\\prime}})+L^{3}}{\\mu^{2}T}<f(x_{T}^{j})-\\frac{L^{2}\\log(\\frac{1}{\\delta^{\\prime}})+L^{3}}{\\mu^{2}T}\\}}\\\\ &{\\qquad\\qquad\\tau=\\tau+1}\\end{array}$   \n12: end while   \n13: return $S$   \n14: end procedure ", "page_idx": 8}, {"type": "text", "text": "interactions with the environment, where $\\Delta^{*}$ is the minimum suboptimality gap of the Nash equilibrium of a function class compared to that of the best function class. ", "page_idx": 8}, {"type": "text", "text": "This result indicates that finding the best model class out of a set of candidate model classes may be computationally tractable in certain regimes. An interesting question that we leave for future work is whether it is possible to be no regret, not just within a model class, but across a set of model classes as well. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work seeks to provide a framework for understanding the complexities that arise when models are released into strategic environments. We show that the prevailing understanding of scaling laws in machine learning fails to hold in large classes of strategic environments and show its implications for MARL and strategic classification, among other areas. Lastly we highlight a possible algorithmic solution to overcoming the problem of model selection in games in which we were able to design an algorithm to efficiently learn the best model class to optimize over without sacrificing performance in terms of regret. ", "page_idx": 8}, {"type": "text", "text": "Altogether, our results are a first step towards understanding scaling laws and hence, model selection in strategic environments. Our results suggest that we need to rethink our understanding of scaling laws before blindly deploying ever more complex models into real-world environments in which they will be faced with strategic behaviors. We leave many avenues of future work open, including questions about generalization and finite sample considerations, as well as the potential for more sophisticated algorithmic approaches to model selection. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Eleni Adamopoulou and Lefteris Moussiades. Chatbots: History, technology, and applications. Machine Learning with Applications, 2020. [2] Dana Pessach, Gonen Singer, Dan Avrahami, Hila Chalutz Ben-Gal, Erez Shmueli, and Irad Ben-Gal. Employees recruitment: A prescriptive analytics approach via machine learning and mathematical programming. Decision Support Systems, 134:113290, 2020.   \n[3] Robert Gorwa, Reuben Binns, and Christian Katzenbach. Algorithmic content moderation: Technical and political challenges in the automation of platform governance. Big Data & Society, 7(1), 2020.   \n[4] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.   \n[5] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws, 2021.   \n[6] Utkarsh Sharma and Jared Kaplan. Scaling Laws from the Data Manifold Dimension. Journal of Machine Learning Research, 23(9):1\u201334, 2022.   \n[7] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR, 2018.   \n[8] Jamie Woodcock and Mark Graham. The gig economy: A critical introduction. 2020.   \n[9] Moritz Hardt, Eric Mazumdar, Celestine Mendler-D\u00fcnner, and Tijana Zrnic. Algorithmic collective action in machine learning. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[10] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Innovations in Theoretical Computer Science, page 111\u2013122, 2016.   \n[11] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba\u00b8sar. Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms, pages 321\u2013384. Springer International Publishing, Cham, 2021.   \n[12] Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan. Who leads and who follows in strategic classification? In Advances in Neural Information Processing Systems, 2021.   \n[13] Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., 1995.   \n[14] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. In Computational Learning Theory, pages 224\u2013240. Springer Berlin Heidelberg, 2001.   \n[15] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264\u2013280, 1971.   \n[16] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: Where Bigger Models and More Data Hurt, December 2019. arXiv:1912.02292 [cs, stat].   \n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training Compute-Optimal Large Language Models, March 2022.   \n[18] Juan Perdomo, Tijana Zrnic, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning, pages 7599\u20137609, 2020.   \n[19] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, 2018.   \n[20] Omer Ben-Porat and Moshe Tennenholtz. Best Response Regression. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[21] Sarah Dean, Mihaela Curmei, Lillian J. Ratliff, Jamie Morgenstern, and Maryam Fazel. Emergent segmentation from participation dynamics and multi-learner retraining, 2023.   \n[22] Lauren E Conger, Franca Hoffman, Eric Mazumdar, and Lillian J Ratliff. Strategic distribution shift of interacting agents via coupled gradient flows. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[23] Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095\u20131100, 1953.   \n[24] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157\u2013163. Elsevier, 1994.   \n[25] D. Fudenberg and D. K. Levine. The theory of learning in games. MIT Press, Cambridge, MA., 1998.   \n[26] Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1):465\u2013507, 2019.   \n[27] Eric Mazumdar, Lillian J. Ratliff, and S. Shankar Sastry. On gradient-based learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103\u2013131, 2020.   \n[28] J. B. Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Econometrica, 1965.   \n[29] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in Adversarial Regularized Learning, pages 2703\u20132717. 2018.   \n[30] Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 1234\u20131242, 2021.   \n[31] Oren Neumann and Claudius Gros. Scaling laws for a multi-agent reinforcement learning model. In The Eleventh International Conference on Learning Representations, 2023.   \n[32] Meena Jagadeesan, Michael Jordan, Jacob Steinhardt, and Nika Haghtalab. Improved bayes risk can yield reduced social welfare under competition. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[33] D. Braess. \u00dcber ein Paradoxon aus der Verkehrsplanung. Unternehmensforschung Operations Research - Recherche Op\u00e9rationnelle, 12(1):258\u2013268, December 1968.   \n[34] Daron Acemoglu, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. Informational Braess\u2019 Paradox: The Effect of Information on Traffic Congestion. Operations Research, 66(4):893\u2013917, August 2018.   \n[35] Tamer Basar and Yu-Chi Ho. Informational properties of the nash solutions of two stochastic nonzero-sum games. Journal of Economic Theory, 7(4):370\u2013387, 1974.   \n[36] T. Basar and G. J. Olsder. Dynamic noncooperative game theory: second edition. SIAM, 1999.   \n[37] Donald M. Topkis. Supermodularity and Complementarity. Princeton University Press, 1998.   \n[38] Susan Athey. Monotone Comparative Statics under Uncertainty. The Quarterly Journal of Economics, 117(1):187\u2013223, 2002. Publisher: Oxford University Press.   \n[39] Pradeep Dubey. Inefficiency of nash equilibria. Mathematics of Operations Research, 11(1):1\u20138, 1986.   \n[40] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, pages 1587\u20131596, 2018.   \n[41] Jacob K. Goeree, Charles. Holt, and Thomas Palfrey. Quantal Response Equilibrium: A Stochastic Theory of Games. Princeton University Press, 2016.   \n[42] Aldo Pacchiano, My Phan, Yasin Abbasi-Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesv\u00e1ri. Model selection in contextual stochastic bandit problems. In Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.   \n[43] Dylan J. Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. 2019.   \n[44] Keegan Harris, Ioannis Anagnostides, Gabriele Farina, Mikhail Khodak, Zhiwei Steven Wu, and Tuomas Sandholm. Meta-learning in games. 2023.   \n[45] Nicholas J. A. Harvey, Christopher Liaw, and Sikander Randhawa. Simple and optimal highprobability bounds for strongly-convex stochastic gradient descent, September 2019.   \n[46] Tor Lattimore and Csaba Szepesvari. Bandit algorithms. 2017.   \n[47] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 2018.   \n[48] Drew Fudenberg and Jean Tirole. Game Theory. MIT Press, 1991.   \n[49] Kevin Jamieson, William Agnew, and Tomer Kaftan. Lecture 4: Stochastic Multi-Armed Bandits, Pure Exploration.   \n[50] Junling Hu and Michael P. Wellman. Nash q-learning for general-sum stochastic games. J. Mach. Learn. Res., 4(null):1039\u20131069, dec 2003. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Learner-Environment Interactions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Description of learner environment settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We begin by providing a concrete description of each of the learner -environment settings we explore: ", "page_idx": 12}, {"type": "text", "text": ". Stationary Environments: The environment has only one action (i.e., $\\mathcal{E}=\\{e\\}$ ), and the problem reduces to that of classical ML. The resulting equilibrium is simply the minimum of the learner\u2019s loss given $e$ and the model class $\\Theta_{i}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta_{i}}f_{l}(\\theta,e).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2. Stackelberg Environments - Learner Leads: The equilibrium outcome is the Stackelberg equilibrium of the two-player game under the assumption that the learner leads. This is, for example, the setup adopted in strategic classification [10]. The equilibrium is a joint strategy $(\\theta^{*},e^{*})$ such that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta^{*}=\\underset{\\theta\\in\\Theta_{i}}{\\arg\\operatorname*{min}}\\,f_{l}(\\theta,B R_{e}(\\theta)),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and $e^{*}=B R_{e}(\\theta^{*})=\\arg\\operatorname*{min}_{e\\in\\mathcal{E}}\\,f_{e}(\\theta^{*},e).$ ", "page_idx": 12}, {"type": "text", "text": "3. Stackelberg Environments - Learner Follows: The equilibrium outcome is the Stackelberg equilibrium of the two-player game under the assumption that the learner follows. This is, for example, the case that arises when agents attempt to perform data poisoning attacks [47] or engage in collective action [9] against the learner. Here the equilibrium is a joint strategy $(\\theta^{*},e^{*})$ such that: ", "page_idx": 12}, {"type": "equation", "text": "$$\ne^{*}=\\underset{e\\in\\mathcal{E}}{\\arg\\operatorname*{min}}\\;f_{e}(B R_{l}(e),e),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and $\\begin{array}{r}{\\theta^{*}=B R_{l}(e^{*})=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}f_{l}(\\theta,e^{*}).}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "4. General Nash Environments: Which allows us to model the general case when the interaction results in a Nash equilibrium. This is, for example, the desired solution in MARL [11] and participation and regression games [21, 32]. In this setting, the equilibrium outcome is a joint strategy $(\\theta,e)$ such that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{e}(\\theta,e^{\\prime})\\geq f_{e}(\\theta,e)~\\forall\\,e^{\\prime}\\in\\mathcal{E},}\\\\ &{f_{l}(\\theta^{\\prime},e)\\geq f_{l}(\\theta,e)~\\forall\\,\\theta^{\\prime}\\in\\Theta_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We remark that in this last case, the assumption of a two-player game is made for simplicity, and our results would go through in $n$ -player games. ", "page_idx": 12}, {"type": "text", "text": "A.2 Stationary environments and Stackelberg games with the learner leading ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We investigate the two cases in which performance monotonically increases as a function of complexity: stationary environments and Stackelberg interactions where the learner has commitment power (i.e., the learner \u201cleads\"). This fact follows from the elementary observation that in both of these regimes, the learner simply solves the same optimization problem over a larger space. ", "page_idx": 12}, {"type": "text", "text": "Proposition A.1. Let A be an ordered set. Consider two model classes, $\\Theta_{i},\\Theta_{j}\\in\\mathbb{A}$ with $i<j$ . If $(\\theta_{i},\\bar{e}_{i})$ and $(\\theta_{j},e_{j})$ are equilibrium outcomes in stationary environments or Stackelberg environments in which the learner leads with the instantiated model classes being $\\Theta_{i}$ and $\\Theta_{j}$ respectively, then $f_{l}(\\theta_{i},e_{i})\\ge f_{l}(\\theta_{j},e_{j})$ ", "page_idx": 12}, {"type": "text", "text": "Proof. For a stationary game, the proposition above follows naturally. We know that $e_{i}\\;=\\;e_{j}$ which we denote $e^{*}$ . Since $\\theta_{i}\\in\\Theta_{i}\\subseteq\\Theta_{j}$ , the equilibrium $(\\theta_{j},e^{*})$ necessarily must be such that $f_{l}(\\theta_{j},e^{*})\\leq f_{l}(\\theta_{i},e^{*})$ , otherwise $(\\theta_{j},e^{*})$ is not an equilibrium point. ", "page_idx": 12}, {"type": "text", "text": "For a Stackelberg game where the learner leads, the proposition follows the same argument. According to Stackelberg dynamics, we know that $B R(\\theta_{i})=e_{i}$ and $B R(\\theta_{j})=e_{j}$ . We can see that it must be the case that $f_{l}(\\theta_{j},e_{j})\\leq f_{l}(\\theta_{i},e_{i})$ otherwise simply selecting $\\theta_{i}$ when optimizing in $\\Theta_{j}$ would be a profitable deviation. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "While this proposition is trivial to prove, it has implications for adversarial machine learning and strategic classification. Adversarial learning can be modeled as zero-sum or min-max games [7]. In these games, if the Nash equilibrium exists, it coincides with the Stackelberg equilibria via simple min-max theorems [48]. This implies that the basic intuition of scaling laws holds true for adversarial learning. Similar takeaways hold true for strategic classification because it is commonly modeled as a Stackelberg game in which the learner leads. ", "page_idx": 13}, {"type": "text", "text": "A.3 Stackelberg games where the learner follows ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We also consider the case in which the environment results in a Stackelberg equilibrium where the learner follows. In lieu of a general result for this case, we construct a simple example in strategic linear regression that highlights the fact that when the learner follows in a Stackelberg game\u2014as happens in settings such as collective action and non-adversarial backdoor attacks in machine learning\u2014 the use of more features can actually hurt. ", "page_idx": 13}, {"type": "image", "img_path": "R6FOuWv5MD/tmp/570779fe2c1b31c0f468ce294d71b8cc998824353a60dd24c5da71bb12db10fd.jpg", "img_caption": ["Figure 2: The loss for the learner at their best response in a regression game as the magnitude of the environment\u2019s perturbation vector varies with the payoffs achieved at equilibrium as derived in Proposition A.2. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Example 3: Strategic Linear Regression In this setup, we have a decision maker who would like to solve a regression problem and is choosing between different feature mappings they can use to fti a model. More precisely, we consider a learner deciding between whether to pick $\\phi_{\\theta}^{\\bar{1}}(x)=\\theta^{T}x$ or $\\phi_{\\theta}^{2}(x)=\\theta_{1}^{T}x+\\theta_{2}\\exp(-\\|x\\|^{2})$ as the function to use for the regression task. The classifier\u2019s goal is to learn $\\theta$ so as to minimize the expected squared error. In this framework, the environment can add a deviation $e$ to the dataset. This would mean that the input to the regressor model would be $x+e$ . ", "page_idx": 13}, {"type": "text", "text": "Consider the model classes $\\Theta_{\\phi_{\\theta}^{1}(x)},\\Theta_{\\phi_{\\theta}^{2}(x)}$ derived from $\\phi_{\\theta}^{1}(x),\\phi_{\\theta}^{2}(x)$ respectively. We show that despite $\\Theta_{\\phi_{\\theta}^{1}(x)}\\subset\\Theta_{\\phi_{\\theta}^{2}(x)}$ , at a Stackelberg equilibrium, the learner has a higher payoff when they learn from $\\Theta_{\\phi_{\\theta}^{1}(x)}$ as opposed to $\\Theta_{\\phi_{\\theta}^{2}(x)}$ . This is in stark contrast to what happens in stationary environments in which adding features never hurts performance since they can just be given a weight of 0. More concretely: ", "page_idx": 13}, {"type": "text", "text": "Proposition A.2. Consider a dataset where each data point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is drawn from a distribution $\\mathcal{D}$ . A learner has the option to select one of two model classes: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\phi_{\\theta}^{1}(x)}=\\{\\phi_{\\theta}^{1}(x):\\theta\\in\\mathbb{R}^{d}\\:w h e r e\\colon\\phi_{\\theta}^{1}(x)=\\theta^{T}x\\}}\\\\ &{\\Theta_{\\phi_{\\theta}^{2}(x)}=\\{\\phi_{\\theta}^{2}(x):\\theta_{1},\\theta_{2}\\in\\mathbb{R}^{d}\\times\\mathbb{R}}\\\\ &{\\quad\\quad\\quad\\quad w h e r e\\colon\\phi_{\\theta}^{2}(x)=\\theta_{1}^{T}x+\\theta_{2}\\exp(-\\|x\\|^{2})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assume that each data point can be perturbed by an error vector $e\\in C\\subseteq\\mathbb{R}^{d}$ , resulting in the learner observing $x+e$ instead of $x$ . For some compact convex $C$ , distribution $\\mathcal{D}$ and dimension $d$ , the Stackelberg equilibrium attained by optimizing over $\\Theta_{\\phi_{\\theta}^{1}(x)}$ results in a strictly lower loss than that attained by optimizing over \u0398\u03d52(x). ", "page_idx": 13}, {"type": "text", "text": "The calculations for this proposition are in the Appendix D. Figure 2 highlights the non-monotonicity of performance between different equilibria in the two spaces. We see that at the Stackelberg equilibrium, the larger model class incurs a greater loss than the smaller model class despite the fact that the loss incurred by the larger model class is lower than that incurred by, the lower model class in a point-wise sense. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Additional proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We begin by revisiting the main assumptions of this proof: ", "page_idx": 14}, {"type": "text", "text": "Assumption B.1 (Restatement of Assumption 3.2). Assume the game defined on $f_{e}$ and $f_{l}$ is strongly monotone on $\\Omega\\times\\mathcal{E}$ . Further assume that ", "page_idx": 14}, {"type": "text", "text": "1. $f_{l}$ and $f_{e}$ are jointly convex in $\\theta$ and $e$ . 2. The gradient mappings, $\\nabla f_{l}$ and $\\nabla f_{e}$ exist and are well defined for all $(\\theta,e)$ . Furthermore, the gradient mappings are $L$ -Lipschitz continuous in the joint action space. 3. The Nash equilibrium $\\theta^{*}\\in\\Theta$ is on the interior of $\\Theta$ with $\\nabla_{\\theta}B R_{e}(\\theta^{*})\\neq0$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem B.2 (Restatement of Theorem 3.4). For a two-player monotone game $G$ on $\\Theta\\times{\\mathcal{E}}$ which satisfies Assumption 3.2, if the unique Nash equilibrium in $\\Theta\\times{\\mathcal{E}}$ is not Pareto optimal then there exists a restriction of the learner\u2019s model class (i.e., a set $\\Theta^{\\prime}\\subset\\Theta_{,}$ ) such that the restricted game $G^{\\prime}$ on $\\Theta^{\\prime}\\times\\mathcal{E}$ admits a Nash equilibrium $(\\theta^{\\prime},e^{\\prime})$ with: $f_{l}(\\theta^{\\prime},e^{\\prime})<f_{l}(\\theta^{*},e^{*})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 3.4. Note the following: because we can achieve strictly lower loss for the jointly convex loss function $f_{l}$ , we know that $\\nabla\\bar{f}_{l}(\\theta^{*},e^{*})\\neq0$ ", "page_idx": 14}, {"type": "text", "text": "Consider the following the function $\\bar{f}_{l}(\\theta):=f_{l}(\\theta,B R_{e}(\\theta))$ . Here $B R_{e}(\\theta)$ is the best response of the other player to the action $\\theta$ , i.e., $B R_{e}(\\theta):=\\mathrm{arg}\\,\\mathrm{min}_{e\\in\\mathcal{E}}\\,f_{e}(\\theta,e)$ . ", "page_idx": 14}, {"type": "text", "text": "Relying on this function definition, we describe a particular restriction on the model class $\\Theta$ and then find a Nash equilibrium for this model class, which is different from $(\\theta^{*},e^{*})$ and whose loss for the $\\Theta$ \u2212player is lower in this new equilibrium. ", "page_idx": 14}, {"type": "text", "text": "Realize that $\\nabla\\bar{f}_{l}(\\theta^{*})\\,=\\,\\nabla_{\\theta}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))\\,+\\,\\nabla_{\\theta}B R_{e}(\\theta^{*})\\nabla_{e}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))\\,\\neq\\,0$ . In particular, we note that $\\nabla_{\\theta}B R_{e}(\\theta^{*})\\cdot\\nabla_{e}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))\\neq0$ since if it were, it would mean that either $\\nabla_{\\theta}B R_{e}(\\theta^{*})$ or $\\nabla_{e}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))$ were equal to zero which is not the case. $\\nabla_{e}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))=$ 0, would mean that both $\\nabla_{\\theta}f_{l}\\big(\\theta^{*},B R_{e}(\\theta^{*})\\big)$ and $\\nabla_{e}f_{l}(\\theta^{*},B R_{e}(\\theta^{*}))=0$ implying a global minimum which would contradict the existence of a Pareto improving point. $\\nabla_{\\theta}B R_{e}(\\theta^{*})\\neq0$ follows from the Assumption 3.2. ", "page_idx": 14}, {"type": "text", "text": "Noting the fact that $\\nabla\\bar{f}_{l}(\\theta^{*})\\neq0$ allows us to pick a direction with respect to the inner product with $\\nabla\\bar{f}_{l}(\\theta^{*})$ , let $v$ be any vector $\\in\\mathbb{R}^{d_{\\theta}}$ such that $\\langle v,\\nabla\\bar{f}_{l}(\\theta^{*})\\rangle>0$ . Consider $\\theta^{\\prime}=\\theta^{*}-\\delta v$ for some $\\delta>0$ . Let $e^{\\prime}=B R_{e}\\overset{\\cdot}{(}\\theta^{\\prime})$ . We will now define $\\Theta^{\\prime}\\subset\\Theta$ such that the game on the model class $\\Theta^{\\prime}\\times\\mathcal{E}$ has $(\\theta^{\\prime},e^{\\prime})$ as a Nash equilibrium ", "page_idx": 14}, {"type": "text", "text": "Let $\\tilde{\\Theta}=\\{\\theta\\in\\Theta:\\langle\\theta\\!-\\!\\theta^{\\prime},v\\rangle\\leq0\\}$ . We then go on to define $\\Theta^{\\prime}=\\{\\theta\\in\\tilde{\\Theta}:\\langle\\nabla_{\\theta}f_{l}(\\theta^{\\prime},B R_{e}(\\theta^{\\prime})),\\theta^{\\prime}-$ $\\theta\\rangle\\leq0\\}$ . Notice how the first step removes $(\\theta^{*},e^{*})$ from the construction. The second step makes it such that $B R_{\\theta}(e^{\\prime})=\\theta^{\\prime}$ for all $\\theta\\in\\Theta^{\\prime}$ . Since $e^{\\prime}$ is the best response to $\\theta^{\\prime}$ we get that the point $(\\theta^{\\prime},e^{\\prime})$ is a Nash equilibrium. ", "page_idx": 14}, {"type": "text", "text": "What is left to show is that we can create such a restriction with the characteristic that the $\\Theta$ -player\u2019s loss function is lowered at the new equilibrium. To do this, we rely on the choice of the $\\delta$ parameter. ", "page_idx": 14}, {"type": "text", "text": "Claim B.3. There exists a value of $\\delta>0$ such that $f_{l}(\\theta^{\\prime},e^{\\prime})<f_{l}(\\theta^{*},e^{*})$ ", "page_idx": 14}, {"type": "text", "text": "To see this, consider the Taylor expansion of $\\bar{f}_{l}(\\theta^{\\prime})$ at $\\theta^{*}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{f}_{l}(\\theta^{\\prime})=\\bar{f}_{l}(\\theta^{*})+\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),\\theta^{\\prime}-\\theta^{*}\\rangle+\\displaystyle\\frac{1}{2}(\\theta^{\\prime}-\\theta^{*})^{T}H_{\\theta}(\\bar{\\theta})(\\theta^{\\prime}-\\theta^{*})}\\\\ {\\displaystyle\\leq\\bar{f}_{l}(\\theta^{*})+\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),\\theta^{\\prime}-\\theta^{*}\\rangle+\\displaystyle\\frac{L}{2}\\|\\theta^{\\prime}-\\theta^{*}\\|^{2}}\\\\ {\\displaystyle=\\bar{f}_{l}(\\theta^{*})-\\delta\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),v\\rangle+\\delta^{2}\\displaystyle\\frac{L}{2}\\|v\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the first line ${\\bar{\\theta}}\\in\\{\\theta\\in\\Theta:\\theta=\\lambda\\theta^{*}+(1-\\lambda)\\theta^{\\prime},\\lambda\\in[0,1]\\}$ . We note that by our construction $\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),v\\rangle>0$ . Furthermore, we realize that the term $\\delta\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),v\\rangle\\in\\mathcal{O}(\\delta)$ and that $\\delta^{2}\\frac{L}{2}\\lVert v\\rVert^{2}\\in$ ${\\mathcal{O}}(\\delta^{2})$ . This means we can select a value of $\\delta$ such that the term $\\begin{array}{r}{\\delta\\langle\\nabla\\bar{f}_{l}(\\theta^{*}),v\\rangle-\\delta^{2}\\frac{L}{2}\\|v\\|^{2}}\\end{array}$ is positive. With this, we can then deduce that $\\bar{f}_{l}(\\theta^{*})>\\bar{f}_{l}(\\theta^{\\prime})$ which then completes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2 Other proofs in Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin with proving the proposition on Strategic classification. Here we restate the assumptions again: ", "page_idx": 15}, {"type": "text", "text": "Assumption B.4 (Restatement of Assumption 3.5). Let $|\\mathcal{V}|=n$ . Assume that the Bayes optimal classifier on $\\scriptstyle{\\mathcal{X}}^{*}$ for ${\\mathcal P}_{0}$ denoted $g_{r}^{*}(x)=\\arg\\operatorname*{max}_{y\\in\\mathcal{y}}\\mathcal{P}(y|\\phi^{*}(x))$ satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{l}(g_{r}^{*},\\mathcal{P}_{0})=P r(g_{r}^{*}(x)=y)<\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition B.5 (Restatement of Proposition 3.6). Under Assumption 3.5, consider two functions classes over which the learner can optimize, $\\Omega$ , and $\\Theta$ which is the set of all functions from $g_{r}:\\mathcal{X}^{*}\\rightarrow$ $\\boldsymbol{\\wp}$ , where $\\mathcal{X}^{*}=\\phi^{*}(\\mathcal{X})$ such that $\\phi^{*}(x)=x$ for $x\\in\\mathcal{X}^{*}$ . Consider the corresponding games denoted $G$ and $G^{*}$ respectively. Then the Nash equilibrium in $G$ is $(g^{*},\\mathcal{P}^{*})$ where $\\mathcal{P}^{*}=(1-\\alpha)\\mathcal{P}_{0}+\\alpha U$ and the Nash equilibrium in $G^{*}$ is given by $(g_{r}^{*},\\mathcal{P}_{0})$ . Furthermore, there exists a range of $\\alpha\\in(0,1)$ such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{l}(g^{*},\\mathcal{P}^{*})>f_{l}(g_{r}^{*},\\mathcal{P}_{0})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 3.6. To prove the first part of this proposition, we note that $(g^{*},\\mathcal{P}^{*})$ is a Nash equilibrium in that no player has any incentive to unilaterally deviate given their action sets. Indeed, if $g^{*}$ is the Bayes-optimal classifier on $P^{*}$ in $\\Omega$ then $P r(g(x)\\neq g(\\phi^{*}(x))>0$ and consequently the environment\u2019s best-response perturbation is $\\mathcal{P}_{e}=U$ . Similarly, $g_{r}^{*},\\mathcal{P}_{0}$ is the Bayes-optimal classifier on $\\mathcal{X}^{\\ast}$ and satisfies $P r\\bar{(}g(x)=g(\\phi^{*}(x))=0$ by definition. As such, the environment\u2019s best response is given by $\\mathcal{P}_{e}=\\mathcal{P}_{0}$ . ", "page_idx": 15}, {"type": "text", "text": "To prove the second part of the proof we note that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{l}(g^{*},\\mathcal{P}^{*})\\geq(1-\\alpha)\\operatorname*{min}_{g\\in\\Omega}f_{l}(g,\\mathcal{P}_{0})+\\frac{\\alpha}{n}>\\frac{\\alpha}{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, note that for $1\\geq\\alpha\\geq n f_{l}(g_{r}^{*},\\mathcal{P}_{0})$ , we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{l}(g^{*},\\mathcal{P}^{*})>f_{l}(g_{r}^{*},\\mathcal{P}_{0})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Further results on Online Learning for Model Selection in Games ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We now present a proof for convergence for the stochastic gradient descent algorithm. We restate the main assumptions here as well: ", "page_idx": 15}, {"type": "text", "text": "Assumption C.1 (Restatement of Assumption 4.1). Assume the game defined on $f_{e}$ and $f_{l}$ is strongly monotone on $\\Omega\\times\\mathcal{E}$ . Further, assume that ", "page_idx": 15}, {"type": "text", "text": "1. The functions $f_{l}$ and $f_{e}$ are $L$ -Lipschitz continuous on $\\Omega\\times\\mathcal{E}$ . ", "page_idx": 15}, {"type": "text", "text": "2. The players have access to stochastic gradient estimators such that the estimated monotone mapping $\\hat{F}$ satisfies, $\\forall x\\in\\Omega\\times\\mathcal{E}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\hat{F}}(x)]=F(x)~{\\mathrm{and}}~~\\mathbb{E}[\\|{\\hat{F}}(x)-F(x)\\|^{2}]\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition C.2 (Restatement of Proposition 4.2). Let $\\Theta$ correspond to a particular model class which results in an instance of continuous action $\\mu$ \u2212strongly monotone game with a unique Nash Equilibrium $(x^{*})$ . Under Assumption 4.1 and the assumption that all players use stepsize schedule =\u00b5(t2+1), for any \u03b4 \u2208(0, 1) Algorithm 2 yields an estimate x\u02c6T such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n|f_{l}(\\bar{x})-f_{l}(x^{*})|\\leq\\mathcal{O}\\left(\\frac{L^{2}\\log(\\frac{1}{\\delta})+L^{3}}{\\mu^{2}T}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 4.2. Let $x_{t}\\,=\\,(x_{1},...\\,,x_{n})$ for a fixed number of players. We then define $F(x_{t})=\\left[\\begin{array}{c}{\\dot{\\nabla}_{1}u_{1}(x_{t})}\\\\ {\\nabla_{2}u_{2}(x_{t})}\\\\ {\\vdots}\\\\ {\\nabla_{n}u_{n}(x_{t})}\\end{array}\\right].$ Let $\\hat{F}(x_{t})=F(x_{t})-\\hat{E}(x_{t})$ , where $\\mathbb{E}[\\hat{E}(x_{t})]=\\mathbf{0}$ and $\\|\\hat{E}(x_{t})\\|_{2}\\leq1$ a.s.. ", "page_idx": 16}, {"type": "text", "text": "From the description of Algorithm 2, we can see that $\\boldsymbol{x}_{t+1}=\\Pi_{\\Theta\\times\\mathcal{E}}\\big(\\boldsymbol{x}_{t}-\\eta_{t}\\hat{F}(\\boldsymbol{x}_{t})\\big)$ The proof of this proposition closely mirrors [45]. Let $\\textstyle{\\bar{x}}$ be the output of Algorithm 1. We can see that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}-x^{*}\\|^{2}=\\|\\Pi_{\\Theta\\times\\mathcal{E}}(x_{t}-\\eta_{t}\\hat{F}(x_{t}))-x^{*}\\|^{2}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}\\leq\\|x_{t}-\\eta_{t}\\hat{F}(x_{t})-x^{*}\\|^{2}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=\\|x_{t}-x^{*}\\|^{2}-2\\eta_{t}\\langle\\hat{F}(x_{t}),x_{t}-x^{*}\\rangle-2\\eta_{t}\\langle F(x_{t}),x_{t}-x^{*}\\rangle+2\\eta_{t}\\langle F(x_{t}),x_{t}-x^{*}\\rangle+}\\\\ &{\\phantom{x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging terms we get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(x_{t}),x_{t}-x^{*}\\Big\\leq\\|x_{t}-x^{*}\\|^{2}-\\|x_{t+1}-x^{*}\\|^{2}+2\\eta_{t}\\langle F(x_{t})-\\hat{F}(x_{t}),x_{t}-x^{*}\\rangle+\\eta_{t}^{2}\\|\\hat{F}(x_{t})\\|^{2}}\\\\ &{F(x_{t}),x_{t}-x^{*}\\leq\\frac{\\|x_{t}-x^{*}\\|^{2}-\\|x_{t+1}-x^{*}\\|^{2}}{2\\eta_{t}}+(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{\\eta_{t}}{2}\\|\\hat{F}(x_{t})\\|^{2}}\\\\ &{F(x_{t}),x_{t}-x^{*}\\leq\\frac{\\|x_{t}-x^{*}\\|^{2}-\\|x_{t+1}-x^{*}\\|^{2}}{2\\eta_{t}}-\\frac{1}{2}(F(x_{t}),x_{t}-x^{*})+(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{\\eta_{t}}{2}\\|\\hat{F}(x_{t})\\|^{2}}\\\\ &{F(x_{t}),x_{t}-x^{*}\\leq2\\cdot\\left(\\frac{\\|x_{t}-x^{*}\\|^{2}-\\|x_{t+1}-x^{*}\\|^{2}}{2\\eta_{t}}-\\frac{\\mu}{2}\\|x_{t}-x^{*}\\|^{2}+(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{\\eta_{t}}{2}\\|\\hat{F}(x_{t})\\|^{2}\\right)}\\\\ &{F(x_{t}),x_{t}-x^{*}\\leq2t\\cdot\\left(\\frac{\\|x_{t}-x^{*}\\|^{2}-\\|x_{t+1}-x^{*}\\|^{2}}{2\\eta_{t}}-\\frac{\\mu}{2}\\|x_{t}-x^{*}\\|^{2}+(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{\\eta_{t}}{2}\\|\\hat{F}(x_{t})\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{=2\\cdot\\left(t\\left(\\frac{\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We then sum over all $t$ and noting that the right-hand side has a portion with a telescoping sum, we see that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}t\\langle F(x_{t}),x_{t}-x^{*}\\rangle\\leq2\\cdot\\left(\\displaystyle\\sum_{t=1}^{T}t\\langle\\hat{E}(x_{t}),x_{t}-x^{*}\\rangle+\\displaystyle\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\displaystyle\\frac{1}{T(T+1)/2}\\sum_{t=1}^{T}t\\langle F(x_{t}),x_{t}-x^{*}\\rangle\\leq\\displaystyle\\frac{4}{T(T+1)}\\cdot\\left(\\displaystyle\\sum_{t=1}^{T}t\\langle\\hat{E}(x_{t}),x_{t}-x^{*}\\rangle+\\displaystyle\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By monotonicity we get note that: $\\mu\\|x_{t}-x^{*}\\|^{2}\\leq\\langle F(x_{t}),x_{t}-x^{*}\\rangle$ and thus: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mu}{I(T+1)/2}\\underset{t=1}{\\overset{r}{\\sum}}t|x_{t}-x^{*}|^{2}\\leq\\frac{4}{I(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\frac{1}{I(T+1)/2}\\underset{t=1}{\\overset{r}{\\sum}}t|x_{t}-x^{*}|^{2}\\leq\\frac{4}{\\mu T(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\underset{t=1}{\\overset{r}{\\sum}}\\frac{t}{T(T+1)/2}z_{t}-x^{*}|^{2}\\leq\\frac{4}{\\mu T(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\underset{t=1}{\\overset{r}{\\sum}}\\frac{\\left\\Vert\\hat{E}^{\\lambda}-x^{*}\\right\\Vert^{2}}{\\mu T(T+1)/2}z_{t}\\frac{4}{\\mu T(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{x})-f_{i}(x^{*})|\\leq\\frac{4L}{\\mu T(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\\\ &{\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{x})-f_{i}(x^{*})|\\leq\\frac{4L}{\\mu T(T+1)}\\cdot\\left(\\underset{t=1}{\\overset{r}{\\sum}}t(\\hat{E}(x_{t}),x_{t}-x^{*})+\\frac{T\\cdot(L+1)^{2}}{\\mu}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To complete the proof, we rely on a high probability bound on $E_{T}$ which makes use of a specialized form of the Generalized Freedman\u2019s Inequality. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.3 ([45] Lemma 4.1). Let $E_{T}=\\sum_{t=1}^{T}t\\langle\\hat{E}(x_{t}),x_{t}-x^{*}\\rangle$ . Then for any $\\delta\\in(0,1)$ we have that $\\begin{array}{r}{E_{T}\\leq\\mathcal{O}\\left(\\frac{L}{\\mu}\\cdot T\\log(\\frac{1}{\\delta})\\right)}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "plugging in the bound on $E_{T}$ completes the proof. ", "page_idx": 17}, {"type": "text", "text": "We then proceed to provide a proof of the successive elimination protocol. This proof which closely follows [49] ", "page_idx": 17}, {"type": "text", "text": "Proposition C.4 (Restatement of Proposition 4.3). Under the assumptions of Proposition 4.2, let $\\mathcal{A}=\\{\\Theta_{i}\\}_{i=1}^{n}$ . With probability at least $1-\\delta_{i}$ , Algorithm 2 identifies the model class whose Nash equilibrium yields the highest payoff after: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{n(L^{2}\\log(\\frac{n}{\\delta})+L^{3})}{\\mu^{2}\\Delta^{*}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "interactions with the environment, where $\\Delta^{*}$ is the minimum suboptimality gap of the Nash equilibrium of a function class compared to that of the best function class. ", "page_idx": 17}, {"type": "text", "text": "Proof of 4.3. We begin by showing an \u201canytime\" confidence interval bound. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.5. Let $\\begin{array}{r l r}{X_{i,T}}&{{}=}&{f(x_{T}^{i})}\\end{array}$ where $\\ensuremath{\\boldsymbol{x}}_{T}^{i}\\gets$ Algorithm $1(\\Theta_{i},x_{0},T)$ and $\\begin{array}{r l r}{X_{i}^{*}}&{{}=}&{f(x_{i}^{*})}\\end{array}$ where $\\boldsymbol{x}_{i}^{*}$ is the Nash equilibrium point for $\\Theta_{i}$ . We then have that: $\\begin{array}{r}{\\mathbb{P}\\Bigg(\\bigcup_{i=1}^{n}\\left\\{\\bigcup_{T=1}^{\\infty}\\left\\{|X_{i,T}-X_{i}^{*}|\\ge\\frac{L^{2}\\log(\\frac{2T^{2}n}{\\delta})+L^{3}}{\\mu^{2}T}\\right\\}\\right\\}\\Bigg)\\leq\\delta}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Here we rely on the union bound to note that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle>\\left(\\bigcup_{i=1}^{n}\\left\\{\\bigcup_{T=1}^{\\infty}\\left\\{|X_{i,T}-X_{i}^{*}|\\ge\\frac{L^{2}\\log(2\\frac{T^{2}n}{\\delta})+L^{3}}{\\mu^{2}T}\\right\\}\\right\\}\\right)\\le\\sum_{i=1}^{n}\\sum_{T=1}^{\\infty}\\mathbb{P}\\left(|X_{i,T}-X_{i}^{*}|\\ge\\frac{L^{2}\\log(\\frac{2T^{2}n}{\\delta})+L^{3}}{\\mu^{2}T}\\right)}&{{}}&{}\\\\ {\\displaystyle\\le\\sum_{i=1}^{n}\\frac{\\delta}{2n}\\sum_{T=1}^{\\infty}\\frac{1}{T^{2}}}&{{}}&{}\\\\ {\\displaystyle\\le\\delta}&{{}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.6. With probability greater than or equal to $1-\\delta_{i}$ , the best model class $\\Theta_{k}$ , is retained in the active set $S$ until the end of Algorithm 2. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\mathcal{D}$ be the event $\\begin{array}{r}{\\bigcup_{i=1}^{n}\\left\\{\\bigcup_{T=1}^{\\infty}\\left\\{|X_{i,T}-X_{i}^{*}|\\ge\\frac{L^{2}\\log(2\\frac{T^{2}n}{\\delta})+L^{3}}{\\mu^{2}T}\\right\\}\\right\\}}\\end{array}$ . We know that $\\mathcal{D}^{C}$ occurs with probability at least 1\u2212\u03b4. Let U(T, \u03b4) = L2 log(2\u00b5 2T T\u03b42n)+L3, $\\Theta_{k}$ is dropped if there exists $j,T$ such that $X_{j,T}-U(T,\\delta)>X_{k,T}+U(T,\\delta)$ . We consider the scenario where the event $\\mathcal{D}^{C}$ occurs. In this scenario we have that $X_{j\\,+\\,}^{*}+U(T,\\delta)\\geq X_{j,T}$ and that $X_{k,T}\\geq X_{k}^{*}-U(T,\\delta)$ for all $T$ . Plugging these two inequalities into the first expression gives us that $X_{j}^{*}\\geq X_{k}^{*}$ which is a contradiction. Therefore, with probability greater than or equal to $1-\\delta$ we have the best model class $\\Theta_{i}$ remaining in $S$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma C.7. Given that the best model class $\\Theta_{k}$ is identified by Algorithm 2, it will terminate after $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{n(L^{2}\\log(\\frac{n}{\\delta})+L^{3})}{\\mu^{2}\\Delta^{*}}\\right)}\\end{array}$ samples ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\Delta_{i}=X_{k}^{*}-X_{i}^{*}$ . Let $\\Delta^{*}=\\operatorname*{min}_{i}\\Delta_{i}$ . Let $\\Theta_{k}$ be the action that corresponds to the highest payoff at the Nash equilibrium point. ", "page_idx": 18}, {"type": "text", "text": "We note that one of the conditions which leads to action $\\Theta_{i}$ being removed from the consideration set $S$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{k,T}-U(T,\\delta)\\geq X_{i,T}+U(T,\\delta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assuming that the event $\\mathcal{D}^{C}$ holds, for each model class $\\Theta_{i}$ we have that, $X_{k,T}\\geq X_{k}^{*}-U(t,\\delta)$ and that $X_{i,t}^{}\\dot{\\leq}\\;X_{i}^{*}+U(t,\\delta)$ . Substituting these expressions into what we have by 1, we get that : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{X_{k}^{\\ast}-X_{i}^{\\ast}\\geq2U(T,\\delta)+2U(T,\\delta)}}\\\\ {{\\Delta_{i}\\geq4U(T,\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we consider the case where $\\Delta_{i}=\\Delta^{*}$ to find a bound for $T$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta^{*}\\geq4U(T,\\delta)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\Delta^{*}\\geq\\frac{4\\left(L^{2}\\log(\\frac{2T^{2}n}{\\delta})+L^{3}\\right)}{\\mu^{2}T}}\\\\ {\\mu^{2}T-\\frac{8L^{2}\\log(T)}{\\Delta^{*}}\\geq\\frac{4\\left(L^{2}\\log(\\frac{2n}{\\delta})+L^{3}\\right)}{\\Delta^{*}}\\qquad}\\\\ {\\mu^{2}T\\geq\\frac{4(L^{2}\\log(\\frac{2n}{\\delta})+L^{3})}{\\Delta^{*}}\\qquad}\\\\ {T\\geq\\mathcal{O}\\left(\\frac{L^{2}\\log(\\frac{n}{\\delta})+L^{3}}{\\mu^{2}\\Delta^{*}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From here we proceed to find the $\\tau$ which corresponds to $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{L^{2}\\log(\\frac{n}{\\delta})+L^{3}}{\\mu^{2}\\Delta^{*}}\\right)}\\end{array}$ steps which is simply $\\begin{array}{r}{\\mathcal{O}\\left(\\log(\\frac{L^{2}\\log(\\frac{n}{\\delta})+L^{3}}{\\mu^{2}\\Delta^{*}})\\right)}\\end{array}$   \nfound by taking the log. We then note that $\\begin{array}{r}{2^{\\tau}=\\mathcal{O}\\left(\\frac{L^{2}\\log(\\frac{n}{\\delta})+L^{3}}{\\mu^{2}\\Delta^{*}}\\right)}\\end{array}$ . Summing \u03c4=1   \nover n \u22121 decision actions we get O n(L2 lo\u00b5g2(\u2206 \u03b4n\u2217 )+L3) samples which completes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D Additional calculations for Linear Regression Example ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Consider the following setup. The decision-maker would like to solve a regression problem and has the choice of two different regression models. For a distribution of input data $\\mathcal{D}$ , and a datapoint $x$ , they can either compute: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{\\theta}^{1}(x)=\\theta^{T}(x+e)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{\\theta}^{2}(x)=\\theta_{1}^{T}(x+e)+\\theta_{2}\\exp(-\\|x+e\\|^{2})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the rest of the calculation we take the distribution of the input data to be ${\\mathcal{N}}(0,I)$ . Suppose the true relationship between features $x$ and outcomes $y$ is linear and is by $y\\,=\\,\\beta^{T}x$ . However, the training data is reported by a population of strategic agents who commit to all manipulating their features in the same way such that the reported features are given by $x^{\\prime}=x+e$ . Equivalently, this can be seen as the input data being misreported and generated from a distribution $\\bar{\\mathcal{N}}(e,I)$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose the population of strategic agents knows that the learner will solve a regression problem. Then, they would like to choose $e$ to maximize their expected prediction given. Concretely: ", "page_idx": 19}, {"type": "equation", "text": "$$\ne^{*}=\\arg\\operatorname*{max}_{e}\\mathbb{E}[\\phi_{\\theta^{*}}^{i}(x+e)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given the fact that $\\begin{array}{r}{\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}[(y-\\phi_{\\theta}(x+e))^{2}]}\\end{array}$ . For this example, the set $C=\\{e\\in\\mathbb{R}^{d}:e=$ k\u2225\u03b2\u03b2\u2225for k \u2208[\u221210, 10]}. We select this direction because, at a high level, the best deviation for the environment can be shown for many model classes to be in the direction of $\\beta$ . As for the magnitude boundaries, the equilibrium points we found lie in the interior of the set, and hence, there was nothing special about the boundaries selected for this example. ", "page_idx": 19}, {"type": "text", "text": "Case 1: Small model For this model, we do not rely explicitly on the definition of $C$ . We find that the Stackelberg equilibrium action for the environment over $\\dot{\\mathbb{R}}^{d}$ already lies in $C$ . As such, this calculation does not make use of the structure of $C$ . ", "page_idx": 19}, {"type": "text", "text": "To begin, we compute the the optimal $\\theta$ for a given $e$ when $\\phi_{\\theta}(x)=\\theta^{T}(x+e)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{\\theta}f_{l}(e,\\theta)=\\nabla_{\\theta}\\mathbb{E}[(y-\\phi_{\\theta}(x))^{2}]}&{}\\\\ {=\\nabla_{\\theta}\\mathbb{E}[(\\beta^{T}x-\\theta^{T}(x+e))^{2}]}&{}\\\\ {=2\\beta-2\\theta-2e e^{T}\\theta}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Setting this equal to 0, we find that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\theta^{*}(e)=\\left(I-\\frac{e e^{T}}{1+\\|e\\|^{2}}\\right)\\beta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "plugging this into the problem for the strategic agents, we find that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{e^{*}=\\arg\\operatorname*{max}_{e}\\mathbb{E}[\\phi_{\\theta^{*}}(x+e)]}}\\\\ {{\\displaystyle=\\arg\\operatorname*{max}_{e}\\theta^{*}(e)^{T}e}}\\\\ {{\\displaystyle=\\arg\\operatorname*{max}_{e}\\frac{e^{T}\\beta}{1+\\|e\\|^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This results in the optimal choice of $e$ for the population of strategic agents being $\\begin{array}{r}{e^{*}=\\frac{\\beta}{\\|\\beta\\|}}\\end{array}$ \u2225\u03b2\u03b2\u2225, which in turn results in the regression accuracy of the decision-maker being: ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{l}(e^{*},\\theta^{*}(e^{*}))=\\frac{1}{2}\\|\\beta\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case 2: Larger model In this case, while we make use of the structure of $C$ , numerical experiments suggest that this point may be an equilibrium point over a far larger set than $C$ . As this was an illustrative example, we did not venture to formally prove that the point we found was a Stackelberg equilibrium point across $\\mathbb{R}^{d}$ . For the second case, let us first expand the loss for the decision-maker as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\tau}_{l}(e,\\theta)=\\mathbb{E}[(\\beta^{T}x-\\theta_{1}^{T}(x+e))^{2}]-2\\theta_{2}\\mathbb{E}[\\exp(-\\|x+e\\|^{2})(\\beta^{T}x-\\theta_{1}^{T}(x+e))]+\\theta_{2}^{2}\\mathbb{E}[\\exp(-2\\|x+e\\|^{2})]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{:||^{2})(\\beta^{T}x\\,-\\,\\theta_{1}^{T}(x\\,+\\,e))\\rangle}&{=}&{-2\\theta_{2}\\beta^{T}\\mathbb{E}\\bigl[\\exp(-\\|x\\,+\\,e\\|^{2})x\\bigr]^{\\prime}\\,+\\,2\\theta_{2}\\theta_{1}^{T}\\mathbb{E}[\\exp(-\\|x\\,+\\,^{\\circ}\\|^{2}){\\^x}]\\,+\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2\u03b82\u03b81T E[exp(\u2212\u2225x + e\u22252)e] ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbb{E}[\\exp(-\\|x+e\\|^{2})x]=({\\frac{1}{2\\pi}})^{4}\\int_{\\mathbb{R}^{d}}x\\exp(-\\|x+e\\|^{2})\\exp(-{\\frac{1}{2}}\\|x\\|^{2})\\,d x}\\\\ &{=({\\frac{1}{2\\pi}})^{4}\\exp(-\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}x\\exp(-{\\frac{3}{2}}\\|x\\|^{2}-2x^{T}e)\\,d x}\\\\ &{=({\\frac{1}{2\\pi}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}x\\exp(-{\\frac{3}{2}}\\|x+{\\frac{2}{3}}e\\|^{2})\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}x\\cdot N(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}x\\cdot N(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}x\\cdot N(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\cdot{-\\frac{2}{3}}e}\\\\ &{=-2({\\frac{1}{3}})^{4}+\\exp(-{\\frac{1}{3}}\\|e\\|^{2})e}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Additionally we consider $\\mathbb{E}[\\exp(-\\|x+e\\|^{2})e]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbb{E}[\\exp(-\\|x+e\\|^{2})e]=({\\frac{1}{2\\pi}})^{4}\\int_{\\mathbb{R}^{d}}e\\exp(-\\|x+e\\|^{2})\\exp(-{\\frac{1}{2}}\\|x\\|^{2})\\,d x}\\\\ &{=({\\frac{1}{2\\pi}})^{4}\\exp(-\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}e\\exp(-{\\frac{3}{2}}\\|x\\|^{2}-2x^{T}e)\\,d x}\\\\ &{=({\\frac{1}{2\\pi}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}e\\exp(-{\\frac{3}{2}}\\|x+{\\frac{2}{3}}e\\|^{2})\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}e x(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}e\\cdot N(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\int_{\\mathbb{R}^{d}}e\\cdot N(-{\\frac{2}{3}}e,{\\frac{1}{3}}I)\\,d x}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})\\cdot e}\\\\ &{=({\\frac{1}{3}})^{4}\\exp(-{\\frac{1}{3}}\\|e\\|^{2})e}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting everything together, we get the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\theta_{2}\\mathbb{E}[\\exp(-\\|x+e\\|^{2})(\\beta^{T}x-\\theta_{1}^{T}(x+e))]}\\\\ &{=-2\\theta_{2}\\beta^{T}\\mathbb{E}[\\exp(-\\|x+e\\|^{2})x]+2\\theta_{2}\\theta_{1}^{T}\\mathbb{E}[\\exp(-\\|x+e\\|^{2})x]+2\\theta_{2}\\theta_{1}^{T}\\mathbb{E}[\\exp(-\\|x+e\\|^{2})e]}\\\\ &{=-2\\theta_{2}\\beta^{T}(-2(\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)+2\\theta_{2}\\theta_{1}^{T}(-2(\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)+2\\theta_{2}\\theta_{1}^{T}((\\frac{1}{3})^{\\frac{d}{2}}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)}\\\\ &{=4\\theta_{2}\\beta^{T}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)-4\\theta_{2}\\theta_{1}^{T}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)+2\\theta_{2}\\theta_{1}^{T}((\\frac{1}{3})^{\\frac{d}{2}}\\exp(-\\frac{1}{3}\\|e\\|^{2}))}\\\\ &{=2\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})(\\theta_{1}^{T}e+2\\beta^{T}e))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then go on to evaluate $\\theta_{2}^{2}\\mathbb{E}[\\exp(-2\\|x+e\\|^{2})]$ using the same calculation method as above, and   \nfind that $\\theta_{2}^{2}\\mathbb{E}[\\exp(-2\\|x+e\\|^{2})]=\\theta_{2}^{2}((\\frac{1}{5})^{\\frac{d}{2}}\\exp(-\\frac{2}{5}\\|e\\|^{2}))$   \nPutting everything together we find that the loss of the model is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\iota}(e,\\theta)=\\beta^{T}\\beta-2\\beta^{T}\\theta_{1}+\\theta_{1}^{T}\\theta_{1}+\\theta_{1}^{T}e e^{T}\\theta_{1}+2\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})(\\theta_{1}^{T}e+2\\beta^{T}e))+\\theta_{2}^{2}((\\frac{1}{5})^{\\frac{d}{2}}\\exp(-\\frac{1}{3}\\|e\\|^{2})(\\theta_{1}^{T}e+2\\beta^{T}e))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We take the derivative with respect to $\\theta_{1}$ and we find that the loss\u2019 derivative is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n-2\\beta+2\\theta_{1}+2e e^{T}\\theta_{1}+2\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Solving for $\\theta_{1}$ after equating the derivative to zero, we find that $\\theta_{1}$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\theta_{1}=(I-\\frac{e e^{T}}{1+\\|e\\|^{2}})(\\beta-\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, we take the derivative with respect to $\\theta_{2}$ and we find it to be: ", "page_idx": 21}, {"type": "equation", "text": "$$\n2((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})(\\theta_{1}^{T}e+2\\beta^{T}e))+2\\theta_{2}((\\frac{1}{5})^{\\frac{d}{2}}\\exp(-\\frac{2}{5}\\|e\\|^{2}))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "setting the derivative to zero and solving for $\\theta_{2}$ we find that $\\theta_{2}$ is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{2}=\\frac{-\\left(\\frac{1}{3}\\right)^{\\frac{d}{2}+1}\\exp\\left(-\\frac{1}{3}\\|e\\|^{2}\\right)\\left(\\theta_{1}^{T}e+2\\beta^{T}e\\right)}{\\left(\\frac{1}{5}\\right)^{\\frac{d}{2}}\\exp\\left(-\\frac{2}{5}\\|e\\|^{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From this point on we make the assumption that $e$ is in the direction of $\\beta$ (i.e., $\\begin{array}{r}{e=k\\frac{\\beta}{\\parallel\\beta\\parallel})}\\end{array}$ for some $k\\,\\in\\,\\mathbb{R}$ . As such, note that $\\|e\\|\\,=\\,k$ . For simplification and ease of computation, we make the following notational substitutions: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(\\displaystyle\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\displaystyle\\frac{1}{3}k^{2})=m}}\\\\ {{(\\displaystyle\\frac{1}{5})^{\\frac{d}{2}}\\exp(-\\displaystyle\\frac{2}{5}k^{2})=y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Re-writing the expressions of $\\theta_{1}$ and $\\theta_{2}$ we get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\theta_{1}=(I-\\frac{e e^{T}}{1+\\|e\\|^{2}})(\\beta-\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})e)}\\\\ {\\displaystyle\\quad=(I-\\frac{e e^{T}}{1+\\|e\\|^{2}})(\\beta-\\theta_{2}m e)}\\\\ {\\displaystyle\\theta_{2}=\\frac{-m(\\theta_{1}^{T}e+2\\beta^{T}e)}{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We go on to simplify the expressions for $\\theta_{1}$ and $\\theta_{2}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{1}=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})(\\beta-\\theta_{2}m e)}\\\\ &{\\quad=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})(\\beta+\\frac{m(\\theta_{1}^{T}e+2\\beta^{T}e)}{y}m e)}\\\\ &{\\quad=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})(\\beta+\\frac{m^{2}(\\theta_{1}^{T}e+2\\beta^{T}e)}{y}e)}\\\\ &{\\quad\\quad=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})(\\beta+\\frac{m^{2}(\\theta_{1}^{T}e+2\\beta^{T}e)}{y})}\\\\ &{\\theta_{1}-(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})\\frac{m^{2}(\\theta_{1}^{T}e+2\\beta^{T}e)}{y}e=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})\\beta}\\\\ &{\\quad\\quad\\quad\\quad\\theta_{1}-(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})\\frac{m^{2}\\theta_{1}^{T}e}{y}e=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})\\beta+(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})^{2}\\frac{2m^{2}\\beta^{T}e}{y}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\theta_{1}-(\\frac{1}{1+\\Vert e\\Vert^{2}})\\frac{e m^{2}\\theta_{1}^{T}e}{y}=(I-\\frac{e^{T}}{1+\\Vert e\\Vert^{2}})\\beta+(\\frac{1}{1+\\Vert e\\Vert^{2}})^{2}\\frac{2\\epsilon m^{2}\\beta^{T}e}{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We let $\\begin{array}{r}{z=-\\big(\\frac{1}{1+k^{2}}\\big)\\frac{m^{2}}{y}}\\end{array}$ and realize that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{(I+z e e^{T})\\theta_{1}=(I-\\frac{e e^{T}}{1+\\|e\\|^{2}})\\beta+(\\frac{1}{1+\\|e\\|^{2}})\\frac{2e m^{2}\\beta^{T}e}{y}}\\\\ {(I+z e e^{T})\\theta_{1}=(I-\\frac{e e^{T}}{1+\\|e\\|^{2}})\\beta+2\\frac{m^{2}}{y}\\frac{e e^{T}}{1+\\|e\\|^{2}}\\beta}\\\\ {(I+z e e^{T})\\theta_{1}=\\beta-\\frac{e e^{T}}{1+\\|e\\|^{2}}\\beta+2\\frac{m^{2}}{y}\\frac{e e^{T}}{1+\\|e\\|^{2}}\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We make use of the substitution e = k\u2225\u03b2\u03b2\u2225 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(I+z e e^{T})\\theta_{1}=\\beta-\\displaystyle\\frac{k^{2}}{1+k^{2}}\\beta+2\\displaystyle\\frac{m^{2}}{y}\\displaystyle\\frac{k^{2}}{1+k^{2}}\\beta}}\\\\ {{(I+z e e^{T})\\theta_{1}=\\beta(\\displaystyle\\frac{1}{1+k^{2}})(1+2\\displaystyle\\frac{m^{2}}{y}k^{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We invert the left side using the Sherman Morrison formula: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle{\\theta_{1}=(I-\\frac{z e e^{T}}{1+z\\|e\\|^{2}})\\beta(\\frac{1}{1+k^{2}})(1+2\\frac{m^{2}}{y}k^{2})}}\\\\ {\\displaystyle{\\theta_{1}=(I-\\frac{z e e^{T}}{1+z\\|e\\|^{2}})\\beta(\\frac{1}{1+k^{2}})(1+2\\frac{m^{2}}{y}k^{2})}}\\\\ {\\displaystyle{\\theta_{1}=\\beta(\\frac{1}{1+z k^{2}})(\\frac{1}{1+k^{2}})(1+2\\frac{m^{2}}{y}k^{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We simplify this by letting $\\begin{array}{r}{\\big(\\frac{1}{1+z k^{2}}\\big)\\big(\\frac{1}{1+k^{2}}\\big)\\big(1+2\\frac{m^{2}}{y}k^{2}\\big)=c}\\end{array}$ and thus $\\theta_{1}=\\beta c$ . We now substitute this expression back to find the expression of $\\theta_{2}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\theta_{2}=\\displaystyle\\frac{-m(\\theta_{1}^{T}e+2\\beta^{T}e)}{y}}}\\\\ {{\\theta_{2}=\\displaystyle\\frac{-m(c\\beta^{T}e+2\\beta^{T}e)}{y}}}\\\\ {{\\theta_{2}=\\displaystyle\\frac{-m}{y}(c\\beta^{T}e+2\\beta^{T}e)}}\\\\ {{\\theta_{2}=\\displaystyle\\frac{-m}{y}(\\beta^{T}e)(2+c)}}\\\\ {{\\theta_{2}=\\displaystyle\\frac{-m}{y}k\\|\\beta\\|(2+c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We simplify the expression for $\\theta_{2}$ as well by noting that $\\theta_{2}=p\\|\\beta\\|$ where $\\begin{array}{r}{p=\\frac{-m}{y}k(2+c)}\\end{array}$ We now calculate the loss of the strategic agent: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{e}(\\theta,e)=\\mathbb{E}[\\theta_{1}^{T}(x+e)+\\theta_{2}\\exp(-\\|x+e\\|^{2})]}\\\\ &{\\quad\\quad\\quad=\\theta_{1}^{T}e+\\theta_{2}(\\frac{1}{3})^{\\frac{d}{2}}\\exp(-\\frac{1}{3}\\|e\\|^{2})}\\\\ &{\\quad\\quad\\quad=\\theta_{1}^{T}e+\\theta_{2}3(\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})}\\\\ &{\\quad\\quad\\quad=\\theta_{1}^{T}e+\\theta_{2}3m}\\\\ &{\\quad\\quad\\quad=c\\beta^{T}e+3m p\\|\\beta\\|}\\\\ &{\\quad\\quad\\quad=c\\mathbb{k}\\|\\beta\\|+3m p\\|\\beta\\|}\\\\ &{\\quad\\quad\\quad=\\|\\beta\\|(c k+3m p)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then now re-evaluate the loss of the model player in terms of the simplified expressions we have found. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{l}(e,\\theta)=\\beta^{T}\\beta-2\\beta^{T}\\theta_{1}+\\theta_{1}^{T}\\theta_{1}+\\theta_{1}^{T}e e^{T}\\theta_{1}+}\\\\ &{\\quad\\quad\\quad2\\theta_{2}((\\frac{1}{3})^{\\frac{d}{2}+1}\\exp(-\\frac{1}{3}\\|e\\|^{2})(\\theta_{1}^{T}e+2\\beta^{T}e))+\\theta_{2}^{2}((\\frac{1}{5})^{\\frac{d}{2}}\\exp(-\\frac{2}{5}\\|e\\|^{2}))}\\\\ &{\\quad\\quad=\\beta^{T}\\beta-2\\beta^{T}\\theta_{1}+\\theta_{1}^{T}\\theta_{1}+\\theta_{1}^{T}e e^{T}\\theta_{1}+2\\theta_{2}(m(\\theta_{1}^{T}e+2k\\|\\beta\\|))+\\theta_{2}^{2}y}\\\\ &{\\quad\\quad=\\|\\beta\\|^{2}-2c\\beta^{T}\\beta+c^{2}\\beta^{T}\\beta+c^{2}\\beta^{T}e e^{T}\\beta+2p\\|\\beta\\|(m(c\\beta^{T}e+2k\\|\\beta\\|))+p^{2}\\|\\beta\\|^{2}y}\\\\ &{\\quad\\quad=\\|\\beta\\|^{2}-2c\\|\\beta\\|^{2}+c^{2}\\|\\beta\\|^{2}+c^{2}k^{2}\\|\\beta\\|^{2}+2p m c k\\|\\beta\\|^{2}+4p m k\\|\\beta\\|^{2}+p^{2}y\\|\\beta\\|^{2}}\\\\ &{\\quad\\quad=(1-2c+c^{2}+c^{2}k^{2}+2p m c k+4p m k+p^{2}y)\\|\\beta\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We assume that $d\\,=\\,2$ and we consider the loss for the model player with varying values of $k$ . Optimizing over this, we see that in the small model setting, the agent is incentivized to use the value of $k=1$ , which corresponds to $\\frac{\\beta}{\\|\\beta\\|}$ . This then gives the model a loss of $\\textstyle{\\frac{1}{2}}\\|\\beta\\|^{2}$ . However, in the larger model case, the agent is incentivized to give a value of $k$ of $\\approx3.4$ . This results in a higher model loss of $\\approx0.78\\|\\beta\\|^{2}$ . Figure 2 shows the learner plots. ", "page_idx": 23}, {"type": "text", "text": "E Further details on the Multi-Agent RL Example ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our procedure for constructing the Markov follows from a couple of foundational principles. Given that we are in a two-player game with players $A$ and $B$ , we make payoff matrices that make one action for player $A$ the dominant strategy across all states (e.g., our example in 3. We choose the action 0). It is important to note that though a particular strategy is dominant across states, it does not mean that player $A$ will have the same payoff across all these states. We then make all the transitions entirely independent of this player $A$ \u2019s actions. With this, we then design the payoff matrices for player $B$ to be such that depending on how much weight the player $A$ puts on action $0\\;(p)$ , they are incentivized to move to another state. ", "page_idx": 23}, {"type": "text", "text": "To do this concretely, we first instantiate a number of states and corresponding thresholds for which the player $B$ would be incentivized to transition from one state to the next. We then use Nash $Q$ learning [50] to find what values of player $B$ \u2019s payoff matrix would result in behavior that is such that the Nash policy for player $B$ below some threshold has them preferring, for example, moving to the next state but above this threshold they would prefer staying in the current state. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction do reflect the contributions and scope of the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We do provide a description of the limitations of the work and avenues for further exploration. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a set of assumptions and complete proofs for each theoretical result ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the information needed to reproduce the simulations detailed in this work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our contributions are primarily theoretical in nature. The experimental evaluations provided in this paper do not rely on private datasets and can be easily reproduced with the provided settings and parameters. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide all the necessary details to understand the results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We include the necessary information to understand the significance of the experimental procedures. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We focus on theoretical contributions. All of the compute is not sophisticated and is not intense. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The research does conform in every respect with NeurIPS\u2019 Code of Ethics Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: This paper does discuss the societal impacts of the work being performed. Our work is theoretical in nature but it contributes to the growing body of research which seeks to enhance our understanding of how machine learning operates in real world environments. Having a better understanding of the interplay between machine learning systems and strategic environments allows for a more principled understanding of the impact and consequences that machine learning algorithms have on society. We outline scenarios of societal engagement in the introduction. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Paper poses no such risks Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not use existing assets Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This project does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not leverage crowdsourcing nor research with human subjects ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]