[{"figure_path": "vpEq2bzsS0/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of existing VLM knowledge transfer methods. (a) Trade-off plots between zero-shot (Harmonic mean of UCF, HMDB, and K600) and close-set (K400) performance of recent CLIP-based methods (ViT-B/16). (b) As the number of temporal layers increases, the generalization of the standard Transformer layer severely degrades while our proposed MoTE consistently improves the zero-shot and close-set performance. (c) Our proposed MoTE seeks to construct a reconciled feature space between the optimal generalized and specialized manifolds.", "description": "This figure provides a comparison of existing visual-language model (VLM) transfer methods for video recognition.  Panel (a) shows a trade-off curve illustrating the relationship between zero-shot (generalization) and close-set (specialization) performance. Panel (b) demonstrates how the number of temporal layers impacts both zero-shot and close-set performance, with MoTE showing consistent improvement regardless of layer count.  Panel (c) visually represents how MoTE aims to bridge the gap between generalized and specialized solutions, creating a reconciled feature space.", "section": "1 Introduction"}, {"figure_path": "vpEq2bzsS0/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the MoTE framework. (Left): We independently extract the feature of each frame with the CLIP visual encoder. Then, the frame token sequences from a given batch are routed to an activated expert for temporal pattern encoding. To regularize the merging process, we sample the temperature \u03c4 from a discrete set and use it to collapse multi-experts into one merged FFN. (Right): Temporal feature modulation. We modulate the contribution of the temporal feature with the semantic association, which is measured by the similarity between the proxy text features retrieved from the fine-tuning and the test categories. The modulated embedding is used for inference.", "description": "This figure illustrates the MoTE (Mixture-of-Temporal-Experts) framework for video recognition.  The left side shows the process during training: frame-level features are extracted using a CLIP visual encoder; these are then fed to a router that assigns them to one of multiple temporal experts (FFNs) based on a multinomial distribution;  the outputs of the experts are merged using a softmax function with temperature sampling; finally, the merged features are used for training.  The right side demonstrates inference:  a semantic association is calculated between features from fine-tuning and test data to modulate the temporal features before they're used to produce the final video embedding. The overall goal is to reconcile generalization and specialization in VLM transfer for video recognition by creating a mixture of expert modules that can generalize well to new data while also specializing in the task at hand.", "section": "Methodology"}, {"figure_path": "vpEq2bzsS0/figures/figures_9_1.jpg", "caption": "Figure 3: Expert-wise performance of MoTE. CLIP-Mean denotes a fine-tuned CLIP model with mean pooling for temporal modeling.", "description": "This figure shows the performance of each individual expert and the final merged model in terms of Top-1 accuracy on Kinetics-400 close-set, UCF101 zero-shot, and HMDB zero-shot video recognition tasks.  It demonstrates that each expert learns distinct knowledge, leading to varying performance across different tasks. The merged model combines the strengths of all experts, resulting in improved performance compared to individual experts or a baseline CLIP model using mean pooling.", "section": "4.2 Ablation Studies"}, {"figure_path": "vpEq2bzsS0/figures/figures_15_1.jpg", "caption": "Figure 5: Illustration of optional architecture designs for the temporal expert. We omit the activation function between the projection matrices for brevity.", "description": "This figure presents four different designs for the temporal expert module used in the MoTE architecture.  The standard Transformer Layer is shown on the left. The four variations (a) - (d) modify the feed-forward network (FFN) within the Transformer Layer. (a) keeps the FFN intact. (b) replaces the FFN with separate projection layers for both upward and downward paths, each with 4 experts and a router. (c) only replaces the upward projection path with experts. (d) only replaces the downward projection path with experts.  The router is used to select an expert for each input.", "section": "3.2 Mixture-of-Temporal-Experts"}, {"figure_path": "vpEq2bzsS0/figures/figures_16_1.jpg", "caption": "Figure 6: Visualization of attention maps. We show the RGB image and the attention maps of the merged expert, expert_0, and expert_4.", "description": "This figure visualizes attention maps generated by the merged expert and two individual experts (Expert 0 and Expert 4) in the MoTE model.  The top row shows the original RGB images, and the rows below show the corresponding attention maps, highlighting the regions of the image that each model focuses on. The purpose is to demonstrate that different experts focus on different aspects of the video frames, and that the merged expert integrates information from all experts. This visualization supports the claim that MoTE effectively combines the knowledge learned by individual experts to achieve better performance.", "section": "Qualitative Analysis"}, {"figure_path": "vpEq2bzsS0/figures/figures_18_1.jpg", "caption": "Figure 7: Visualization of representation similarities across each expert and the final merged model.", "description": "This figure displays a heatmap visualizing the cosine similarity between the feature representations of individual temporal experts (Expert_0, Expert_1, Expert_2, Expert_3) and the merged expert (the combined output of all experts) in the MoTE model.  Each cell in the heatmap represents the average cosine similarity calculated across 100 randomly sampled videos from unseen categories of the Kinetics-600 dataset. Warmer colors (yellow) indicate higher similarity, signifying that the feature spaces are more alike. Cooler colors (purple) indicate lower similarity, suggesting greater differences in the learned representations between experts. The figure demonstrates that the merged expert's feature representation incorporates features from all individual experts while maintaining distinct characteristics.", "section": "E Qualitative Analysis"}]