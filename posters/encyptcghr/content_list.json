[{"type": "text", "text": "First-Order Methods for Linearly Constrained Bilevel Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guy Kornowski\u2020 Swati Padmanabhan\u2021 Kai Wang\u00a7 Jimmy Zhang\u00b6 Suvrit Sra\u2225 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithms for bilevel optimization often encounter Hessian computations, which are prohibitive in high dimensions. While recent works offer first-order methods for unconstrained bilevel problems, the constrained setting remains relatively underexplored. We present first-order linearly constrained optimization methods with finite-time hypergradient stationarity guarantees. For linear equality constraints, we attain $\\epsilon$ -stationarity in $\\widetilde{\\cal O}(\\epsilon^{-2})$ gradient oracle calls, which is nearly-optimal. For linear inequality constraints, we attain $(\\delta,\\epsilon)$ -Goldstein stationarity in $\\widetilde{\\cal O}(d\\delta^{-1}\\epsilon^{-3})$ gradient oracle calls, where $d$ is the upper-level dimension. Finally, we obtain for the linear inequality setting dimension-free rates of $\\widetilde{O}(\\delta^{-1}\\epsilon^{-4})$ oracle complexity under the additional assumption of oracle access to the optimal dual variable. Along the way, we develop new nonsmooth nonconvex optimization methods with inexact oracles. Our numerical experiments verify these guarantees. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization [1\u20134], an important problem in optimization, is defined as follows: ", "page_idx": 0}, {"type": "text", "text": "Here, the value of the upper-level problem at any point $x$ depends on the solution of the lowerlevel problem. This framework has recently found numerous applications in meta-learning [5\u20138], hyperparameter optimization [9\u201311], and reinforcement learning [12\u201315]. Its growing importance has spurred increasing efforts towards designing computationally efficient algorithms for it. ", "page_idx": 0}, {"type": "text", "text": "As demonstrated by [16], a key computational step in algorithms for bilevel optimization is estimating $d y^{*}(x)/d x$ , the gradient of the lower-level solution. This gradient estimation problem has been extensively studied in differentiable optimization [17, 18] by applying the implicit function theorem to the KKT system of the given problem [19\u201324]. However, this technique typically entails computing (or estimating) second-order derivatives, which can be prohibitive in high dimensions [25\u201327]. ", "page_idx": 0}, {"type": "text", "text": "Recently, [28] made a big leap forward towards addressing this computational bottleneck. Restricting themselves to the class of unconstrained bilevel optimization, they proposed a fully first-order method with finite-time stationarity guarantees. While a remarkable breakthrough, [28] does not directly extend to the important setting of constrained bilevel optimization. This motivates the question: ", "page_idx": 0}, {"type": "text", "text": "Can we develop a first-order algorithm for constrained bilevel optimization? ", "page_idx": 0}, {"type": "text", "text": "Besides being natural from the viewpoint of complexity theory, this question is well-grounded in applications such as mechanism design [29, 30], resource allocation [31\u201334], and decision-making under uncertainty [20, 35, 36]. Our primary contribution is an affirmative answer to the highlighted question for bilevel programs with linear constraints, an important problem class often arising in adversarial training, decentralized meta learning, and sensor networks (see [37]). While there have been some other recent works [37\u201339] on this problem, our work is first-order (as opposed to [37]) and offers, in our view, a stronger guarantee on stationarity (compared to [38, 39])\u2014 cf. Section 1.2. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We provide first-order algorithms (with associated finite-time convergence guarantees) for linearly constrained bilevel programs (Problem 1.1). By \u201cfirst-order\u201d, we mean that we use only zeroth and first-order oracle access to $f$ and $g$ . Our assumptions for each of our contributions are in Section 2.1. ", "page_idx": 1}, {"type": "text", "text": "(1) Linear equality constraints. As our first contribution, we design first-order algorithms for solving Problem 1.1 where the lower-level constraint set $S(x):=\\{y:A x-B y-b=0\\}$ comprises linear equality constraints, and $\\mathcal{X}$ a convex compact set. With appropriate regularity assumptions on $f$ and $g$ , we show in this case smoothness in $x$ of the hyperobjective $F$ . Inspired by ideas from Kwon et al. [40], we use implicit differentiation of the KKT matrix of a slightly perturbed version of the lower-level problem to design a first-order approximation to $\\nabla F$ . Constructing our first-order approximation entails solving a strongly convex optimization problem on affine constraints, which can be done efficiently. With this inexact gradient oracle in hand, we then run projected gradient descent, which converges in $\\widetilde{\\cal O}(\\epsilon^{-2})$ iterations for smooth functions. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Informal; cf. Theorem 3.1). Given Problem 1.1 with linear equality constraints $S(x):=\\{y:A x-B y-b=0\\}$ and $\\mathcal{X}$ a convex compact set, under regularity assumptions on $f$ and $g$ (Assumptions 2.2 and 2.3), there exists an algorithm, which in $\\widetilde{\\cal O}(\\epsilon^{-2})$ oracle calls to $f$ and $g_{\\mathrm{:}}$ , converges to an \u03f5-stationary point of $F$ . ", "page_idx": 1}, {"type": "text", "text": "For linear equality constrained bilevel optimization, this is the first first-order result attaining \u03f5- stationarity of $F$ with assumptions solely on the constituent functions $f$ and $g$ and none on $F$ \u2014 cf. Section 1.2 for a discussion of the results of Khanduri et al. [37] for this setting. ", "page_idx": 1}, {"type": "text", "text": "(2) Linear inequality constraints. Next, we provide first-order algorithms for solving Problem 1.1 where the lower-level constraint set $S(x):={\\overline{{\\{y:A x-B y-b\\leq0\\}}}}$ comprises linear inequality constraints, and the upper-level variable is unconstrained. ", "page_idx": 1}, {"type": "text", "text": "Our measure of convergence of algorithms in this case is that of $(\\delta,\\epsilon)$ -stationarity [41]: for a Lipschitz function, we say that a point $x$ is $(\\delta,\\epsilon)$ -stationary if within a $\\delta$ -ball around $x$ there exists a convex combination of subgradients of the function with norm at most $\\epsilon$ (cf. Definition 2.1). ", "page_idx": 1}, {"type": "text", "text": "To motivate this notion of convergence, we note that the hyperobjective $F$ (in Problem 1.1) as a function of $x$ could be nonsmooth and nonconvex (and Lipschitz, as we later prove). Minimizing such a function in general is known to be intractable [42], necessitating local notions of stationarity. Indeed, not only is it impossible to attain $\\epsilon$ -stationarity in finite time [43], even getting near an approximate stationary point of an arbitrary Lipschitz function is impossible unless the number of queries is exponential in the dimension [44]. Consequently, for this function class, $(\\delta,\\epsilon)$ -stationarity has recently emerged [43] to be a natural and algorithmically tractable notion of stationarity. We give the following guarantee under regularity assumptions on $f$ and $g$ . ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Informal; Theorem 4.1). Consider Problem 1.1 with linear inequality constraints $S(x):=\\{y:A x-B y-b\\leq0\\}$ . Under mild assumptions on $f$ and $g$ (Assumption 2.2) and the lower-level primal solution $y^{\\ast}$ (Assumption 2.4), there exists an algorithm, which converges to $a$ $(\\delta,\\epsilon)$ -stationary point of $F$ in $\\widetilde{\\cal O}(d\\delta^{-1}\\epsilon^{-3})$ oracle calls to $f$ and $g$ , where $d$ is the upper-level variable dimension. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, this is the first result to offer a first-order finite-time stationarity guarantee on the hyperobjective for linear inequality constrained bilevel optimization (cf. Section 1.2 for a discussion of related work [37\u201339]). We obtain our guarantee in Theorem 1.2 by first invoking a result by Zhang and Lan [45] to obtain inexact hyperobjective values of $F$ using only $\\widetilde O(1)$ oracle calls to $f$ and $g$ . We also show (Lemma 4.3) that this hyperobjective $F$ is Lipschitz. We  then employ our inexact zeroth-order oracle for $F$ in Algorithm 2 designed to minimize Lipschitz nonsmooth nonconvex functions (in particular, $F$ ), with the following convergence guarantee. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.3 (see Theorem C.1). Given $L$ -Lipschitz $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and $|\\widetilde{F}(\\cdot)-F(\\cdot)|\\leq\\epsilon,$ , there exists an algorithm, which, in $\\widetilde{\\cal O}(d\\delta^{-1}\\epsilon^{-3})$ calls to $\\widetilde{F}(\\cdot)$ , outputs $x^{\\mathrm{out}}$ with $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{out}}))]\\leq2\\epsilon$ . ", "page_idx": 2}, {"type": "text", "text": "While such algorithms using exact zeroth-order access already exist [46], extending them to the inexact gradient setting is non-trivial; we leverage recent ideas connecting online learning to nonsmooth nonconvex optimization by Cutkosky, Mehta, and Orabona [47] (cf. Section 4). ", "page_idx": 2}, {"type": "text", "text": "(3) Linear inequality under assumptions on dual variable access. For the inequality setting (i.e., Problem 1.1 with the lower-level constraint set $S(x):=\\{y:A x-B y-b\\leq{\\bar{0}}\\}$ ), we obtain dimension-free rates under an additional assumption (Assumption 2.5) on oracle access to the optimal dual variable $\\lambda^{*}$ of the lower-level problem. We are not aware of a method to obtain this dual variable in a first-order fashion (though in practice, highly accurate approximations to $\\lambda^{*}$ are readily available), hence the need for imposing this assumption. We believe that removing this assumption and obtaining dimension-free first-order rates in this setting would be an important direction for future work. Our guarantee for this setting is summarized below. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Informal; Theorem 4.4 combined with Theorem 5.3). Consider Problem 1.1 with linear inequality constraints $S(x)\\ :=\\ \\{y:A x-B y-b\\leq0\\}$ and unconstrained upper-level variable. Under mild regularity assumptions on $f$ and $g$ (Assumption 2.2), on $y^{\\ast}$ (Assumption 2.4), and assuming oracle access to the optimal dual variable $\\lambda^{*}$ (Assumption 2.5), there exists an algorithm, which in ${\\widetilde O}(\\delta^{-1}\\epsilon^{-4})$ oracle calls to $f$ and $g$ converges to a $(\\delta,\\epsilon)$ -stationary point for $F$ . ", "page_idx": 2}, {"type": "text", "text": "We obtain this result by first reformulating Problem 4.1 via the penalty method and constructing an inexact gradient oracle for the hyperobjective $F$ (cf. Section 5). We then employ this inexact gradient oracle within an algorithm (Algorithm 3) designed to minimize Lipschitz nonsmooth nonconvex functions (in particular, $F$ ), with the following convergence guarantee. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.5 (Informal; Theorem 4.4). Given Lipschitz $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and $\\lVert\\widetilde{\\nabla}F(\\cdot)-\\nabla F(\\cdot)\\rVert\\leq\\epsilon,$ , there exists an algorithm that, in $T=O(\\delta^{-1}\\epsilon^{-3})$ calls to $\\widetilde\\nabla F$ , outputs a $(\\delta,2\\epsilon)$ -stationary point of $F$ . ", "page_idx": 2}, {"type": "text", "text": "Our Algorithm 3 is essentially a \u201cfirst-order\u201d version of Algorithm 2. Similar to Algorithm 2, despite the existence of algorithms with these guarantees with access to exact gradients [48], their extensions to the inexact gradient setting are not trivial and also make use of the new framework of Cutkosky, Mehta, and Orabona [47]. We believe our analysis for this general task can be of independent interest to the broader optimization community. Lastly, we also use a more implementation-friendly variant of Algorithm 3 (with slightly worse theoretical guarantees) in numerical experiments. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The vast body of work on asymptotic results for bilevel programming, starting with classical works such as Anandalingam and White [49], Ishizuka and Aiyoshi [50], White and Anandalingam [51], Vicente, Savard, and J\u00fadice [52], Zhu [53], and Ye and Zhu [54], typically fall into two categories: those based on approximate implicit differentiation: Amos and Kolter [17], Agrawal et al. [18], Domke [55], Pedregosa [56], Gould et al. [57], Liao et al. [58], Grazzi et al. [59], and Lorraine, Vicol, and Duvenaud [60] and those via iterative differentiation: Franceschi et al. [9], Shaban et al. [10], Domke [55], Grazzi et al. [59], Maclaurin, Duvenaud, and Adams [61], and Franceschi et al. [62]. Another recent line of work in this category includes Khanduri et al. [37], Liu et al. [63], Ye et al. [64], and Gao et al. [65], which use various smoothing techniques. ", "page_idx": 2}, {"type": "text", "text": "The first non-asymptotic result for bilevel programming was provided by Ghadimi and Wang [16], which was followed by a flurry of work: for example, algorithms that are single-loop stochastic: Chen, Sun, and Yin [66], Chen et al. [67], and Hong et al. [68], projection-free: Akhtar et al. [69], Jiang et al. [70], Abolfazli et al. [71], and Cao et al. [72], use variance-reduction and momentum: Khanduri et al. [73], Guo et al. [74], Yang, Ji, and Liang [75], and Dagr\u00e9ou et al. [76], those for single-variable bilevel programs: Jiang et al. [70], Sabach and Shtern [77], Amini and Yousefian [78, 79], and Merchav and Sabach [80], and for bilevel programs with special constraints: Khanduri et al. [37], Abolfazli et al. [71], Tsaknakis, Khanduri, and Hong [81], and Xu and Zhu [82]. ", "page_idx": 2}, {"type": "text", "text": "The most direct predecessors of our work are those by Khanduri et al. [37], Yao et al. [38], Lu and Mei [39], Kwon et al. [40], and Liu et al. [63]. As alluded to earlier, Liu et al. [28] recently made a significant contribution by providing for bilevel programming a fully first-order algorithm with finite-time stationarity guarantees. This was extended to the stochastic setting by Kwon et al. [40] (which we build upon), simplified and improved by Chen, Ma, and Zhang [83], and extended to the constrained setting by Khanduri et al. [37], Yao et al. [38], and Lu and Mei [39]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The works of Yao et al. [38] and Lu and Mei [39] study the more general problem of bilevel programming with general convex constraints. However, they use KKT stationarity as a proxy to the hypergradient stationarity. Our Theorem 1.4 is restricted to linear inequality constraints, we provide stationarity guarantees directly in terms of the objective of interest. Moreover, Yao et al. [38] assumes joint convexity of the lower-level constraints in upper and lower variables to allow for efficient projections, while we require convexity only in the lower-level variable. ", "page_idx": 3}, {"type": "text", "text": "The current best result for the linearly constrained setting is that of Khanduri et al. [37]. However, this work requires Hessian computations (and is therefore not fully first-order). Moreover, Khanduri et al. [37] imposes strong regularity assumptions on the hyperobjective $F$ , which are, in general, impossible to verify. In contrast, Theorem 1.1 imposes assumptions solely on the constituent functions $f$ and $g$ , none directly on $F$ , thus making substantial progress on these two fronts. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We follow standard notation (see Appendix A), with only the following crucial definition stated here. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1. Consider a locally Lipschitz function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},$ a point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , and a parameter $\\delta>0$ . The Goldstein subdifferential [41] of $f$ at x is the set $\\partial_{\\delta}f(x):=\\mathrm{conv}\\bigl(\\cup_{y\\in\\mathbb{B}_{\\delta}(x)}\\partial f(y)\\bigr)$ , where $\\begin{array}{r}{\\partial f(x)=\\mathrm{conv}\\left\\{\\operatorname*{lim}_{n\\to\\infty}\\nabla f(x_{n}):x_{n}\\to x,\\;x_{n}\\right.}\\end{array}$ $x_{n}\\in\\mathrm{dom}(\\nabla f)\\}$ is the Clarke subdifferential [84] of $f$ and $\\mathbb{B}_{\\delta}(x)$ denotes the Euclidean ball of radius $\\delta$ around $x$ . A point $x$ is called $(\\delta,\\epsilon)$ -stationary $i f$ $\\mathrm{dist}(0,\\partial_{\\delta}f(x))\\leq\\epsilon$ , where $\\mathrm{dist}(x,S):=\\operatorname*{inf}_{y\\in S}\\|x-y\\|$ . ", "page_idx": 3}, {"type": "text", "text": "2.1 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider Problem 1.1 with linear equality constraints (Section 3) under Assumptions 2.2 and 2.3 and linear inequality constraints (Sections 4 and 5) under Assumptions 2.2, 2.4 and 2.5. We assume the upper-level (UL) variable $x\\in\\mathbb{R}^{d_{x}}$ , lower-level (LL) variable $\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}$ , and $A\\in\\mathbb{R}^{d_{h}\\times d_{x}}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. For Problem 1.1, we assume the following for both settings we study: ", "page_idx": 3}, {"type": "text", "text": "(i) Upper-level: The objective $f$ is $C_{f}$ -smooth and $L_{f}$ -Lipschitz continuous in $(x,y)$ . (ii) Lower-level: The objective $g$ is $C_{g}$ -smooth. Fixing any $x\\in\\mathscr{X}$ , $g(x,\\cdot)$ is $\\mu_{g}$ -strongly convex. (iii) We assume that the linear independence constraint qualification $(L I C Q)$ condition holds for the ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3. For Problem 3.1 (with linear equality constraints), we additionally assume that the set $\\mathcal{X}$ is convex and compact, and that the objective $g$ is $S_{g}$ -Hessian smooth, that is, $\\begin{array}{r}{\\left\\|\\nabla^{2}g(x,y)-\\nabla^{2}g(\\bar{x},\\bar{y})\\right\\|\\leq S_{g}\\left\\|(x,y)-(\\bar{x},\\bar{y})\\right\\|\\forall x,\\bar{x}\\in\\mathcal{X}}\\end{array}$ , and $\\boldsymbol{y},\\bar{\\boldsymbol{y}}\\in\\mathbb{R}^{d_{\\boldsymbol{y}}}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.4. For Problem 4.1 (with linear inequality constraints), we additionally assume that $y^{\\ast}$ is $L_{y}$ -Lipschitz in $x$ , where $y^{\\ast}$ is the LL primal solution $y^{*}(x),\\lambda^{*}(x)=\\arg\\operatorname*{max}_{y}\\operatorname*{min}_{\\beta\\geq0}g(x,y)\\,+$ $\\beta^{\\top}h(x,y)$ , where $h(x,y):=A x-B y-b$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.5. We provide additional results for Problem 4.1 under additional stronger assumptions stated here: Denote the $L L$ primal and dual solution $\\begin{array}{r l}{y^{*}(x),\\lambda^{*}(x)}&{{}=}\\end{array}$ arg $\\begin{array}{r}{\\operatorname*{max}_{y}\\operatorname*{min}_{\\beta\\geq0}g(x,y)+\\beta^{\\top}h(x,y)}\\end{array}$ , where $h(x,y)\\,:=\\,A x\\,-\\,B y\\,-\\,b_{!}$ ; then, we assume exact access to $\\lambda^{*}$ and that $\\|\\lambda^{*}(x)\\|\\leq R$ . ", "page_idx": 3}, {"type": "text", "text": "Assumptions 2.2(i) and 2.2(ii) are standard in bilevel optimization. Assumption 2.2(iii) is the same as the complete recourse assumption in stochastic programming [85], that is, the LL problem is feasible $y$ for every $x\\in\\mathbb{R}^{d_{x}}$ . Assumption 2.3 is used only in the equality case and guarantees smoothness of $F$ . Assumption 2.4 is used in the inequality case and implies Lipschitzness of $F$ . We need the stronger assumption in Assumption 2.5 for our dimension-free result for the linear inequality case. ", "page_idx": 3}, {"type": "text", "text": "3 Lower-level problem with linear equality constraint ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first obtain improved results for the setting of bilevel programs with linear equality constraints in the lower-level problem. Our formal problem statement is: ", "page_idx": 3}, {"type": "text", "text": "minimize $x{\\in}X$ $F(x):=f(x,y^{*}(x))$ subject to $y^{*}(x)\\in\\arg\\operatorname*{min}_{y:h(x,y)=0}g(x,y),$ where $f,g,h(x,y):=A x-B y-b$ , and $\\mathcal{X}$ satisfy Assumptions 2.2 and 2.3. The previous best result on Problem 3.1 providing finite-time $\\epsilon_{}$ -stationarity guarantees, by Khanduri et al. [37], required certain regularity assumptions on $F$ as well as Hessian computations. In contrast, our finite-time guarantees require assumptions only on $f$ and $g_{\\mathrm{:}}$ , not on $F$ ; indeed, in our work, these desirable properties of $F$ are naturally implied by our analysis. Specifically, our key insight is that the hypergradient $\\begin{array}{r}{\\nabla F(x):=\\nabla_{x}f(x,y^{*})+\\overline{{\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*})}}}\\end{array}$ for Problem 3.1 is Lipschitz-continuous and admits an easily computable \u2014 yet highly accurate \u2014 finite-difference approximation. Therefore, $O(\\epsilon^{-2})$ iterations of gradient descent on $F$ with this finite-difference gradient proxy yield an $\\epsilon_{}$ -stationary point. Specifically, for any fixed $x\\in\\mathscr{X}$ , our proposed finite-difference gradient proxy approximating the non-trivial-to-compute component $\\bar{\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}}\\nabla_{y}f(x,y^{*})$ of the hypergradient is given by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{x}:=\\frac{\\nabla_{x}[g(x,y_{\\delta}^{*})+\\langle\\lambda_{\\delta}^{*},h(x,y^{*})\\rangle]-\\nabla_{x}[g(x,y^{*})+\\langle\\lambda^{*},h(x,y^{*})\\rangle]}{\\delta},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(y_{\\delta}^{*},\\lambda_{\\delta}^{*})$ are the primal and dual solutions to the perturbed lower-level problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\delta}^{*}:=\\arg\\operatorname*{min}_{y:h(x,y)=0}\\ g(x,y)+\\delta f(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We show in Lemma 3.2 that $v$ in (3.2) approximates dyd\u2217x(x)  \u2207yf(x, y\u2217) up to an O(\u03b4)-additive error, implying the gradient oracle construction outlined in the pseudocode presented in Algorithm 1. Our full implementable algorithm for solving Problem 3.1 is displayed in Algorithm 5. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Inexact Gradient Oracle for Bilevel Program with Linear Equality Constraint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: Current $x$ , accuracy $\\epsilon$ , perturbation $\\delta=\\epsilon^{2}$ .   \n2: Compute $y^{\\ast}$ (as in Problem 3.1) and corresponding optimal dual $\\lambda^{*}$ (as in (B.1))   \n3: Compute $y_{\\delta}^{*}$ (as in (3.3)) and and corresponding optimal dual $\\lambda_{\\delta}^{*}$ (as in (B.7))   \n4: Compute $v_{x}$ as in (3.2) $\\triangleright$ Approximates $\\left(d y^{*}(x)/d x\\right)^{\\top}\\nabla_{y}f(x,y^{*})$   \n5: Output: $\\widetilde{\\nabla}F=v_{x}+\\nabla_{x}f(x,y^{*})$ ", "page_idx": 4}, {"type": "text", "text": "Notice that the finite-difference term in (3.2) avoids differentiating through the implicit function $y^{\\ast}$ . Instead, all we need to evaluate it are the values of $(y^{*},\\lambda^{*},y_{\\delta}^{*},\\lambda_{\\delta}^{*})$ (and gradients of $g$ and $h$ ). Since $(y^{*},\\lambda^{*})$ are solutions to a smooth strongly convex linearly constrained problem, they can be approximated at a linear rate. Similarly, since the approximation error in (3.2) is proportional to $\\delta$ (cf. Lemma 3.2), a small enough $\\delta$ in the perturbed objective $g+\\delta f$ in (3.3) ensures that it is dominated by the strongly convex and smooth $g$ , whereby accurate approximates to $(y_{\\delta}^{*},\\lambda_{\\delta}^{*})$ can also be readily obtained. Putting it all together, the proposed finite-difference hypergradient proxy in (3.2) is efficiently computable, yielding the following guarantee. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Consider Problem 3.1 under Assumption 2.2, and let $\\kappa=C_{g}/\\mu_{g}$ be the condition number of $g_{\\cdot}$ . Then Algorithm 5 finds an $\\epsilon$ -stationary point (in terms of gradient mapping, see $(B.I4),$ ) after $T\\,=\\,\\widetilde{\\cal O}(C_{F}(F(x_{0})\\,-\\,\\operatorname{inf}\\,F)\\sqrt{\\kappa}\\epsilon^{-2})$ oracle calls to $f$ and $g$ , where $C_{F}\\,:=\\,2(L_{f}+C_{f}\\,+$ $C_{g})C_{H}^{3}S_{g}(L_{g}+\\|A\\|)^{2}$ is the smoothness constant of the hyperobjective $F$ . ", "page_idx": 4}, {"type": "text", "text": "We now sketch the proof of Theorem 3.1. The complete proofs may be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "3.1 Main technical ideas ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We briefly outline the two key technical building blocks alluded to above, that together give us Theorem 3.1: the approximation guarantee of our finite-difference gradient proxy ((3.2)) and the smoothness of hyperobjective $F$ (for Problem 3.1). The starting point for both these results is the following simple observation obtained by implicitly differentiating, with respect to $x$ , the KKT system associated with $y^{*}=\\arg\\operatorname*{min}_{h(x,y)=0}g(x,y)$ and optimal dual variable $\\lambda^{*}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\frac{d y^{*}(x)}{d x}\\right]=\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[-\\nabla_{y x}^{2}g(x,y^{*})\\right]}\\\\ &{\\left[\\frac{d\\lambda^{*}(x)}{d x}\\right]=\\left[\\nabla_{y}h(x,y^{*})\\quad\\qquad0\\qquad\\right]^{-1}\\left[-\\nabla_{x}^{2}h(x,y^{*})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The invertibility of the matrix in the preceding equation is proved in Corollary B.3: essentially, this invertibility is implied by strong convexity of $g$ and $\\nabla_{y}h(\\bar{x},y^{*})=B$ having full row rank. This in conjunction with the compactness of $\\mathcal{X}$ implies that the inverse of the matrix is bounded by some constant $C_{H}$ (cf. Corollary B.3 for details). Our hypergradient approximation guarantee follows: ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. For Problem 3.1 under Assumption 2.2, with $v_{x}$ as in (3.2), the following holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|v_{x}-\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*})\\right\\|\\leq O(C_{F}\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch; see Appendix $B$ . The main idea is that the two terms being compared are essentially the same by the implicit function theorem. First, we use the expression for dy (x)from (3.4): $\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*})=\\left[-\\nabla_{y x}^{2}g(x,y^{*})\\right]^{\\top}\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[\\nabla_{y}f(x,y^{*})\\right].$ ", "page_idx": 5}, {"type": "text", "text": "We now examine $v_{x}$ . For simplicity of exposition, we instead consider lim\u03b4\u2192 0\u2207x[g(x,y\u03b4\u2217 )+\u27e8\u03bb\u03b4\u2217,h(x,y\u2217)\u27e9]\u03b4\u2212\u2207x[g(x,y\u2217)+\u27e8\u03bb\u2217,h(x,y\u2217)\u27e9], which, by the fundamental theorem of calculus and Assumption 2.3 , equals $v_{x}$ up to an $O(\\delta)$ -additive error. Note that this expression is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{x y}^{2}g(x,y^{*})\\frac{d y_{\\delta}^{*}}{d\\delta}+\\nabla_{x}h(x,y^{*})^{\\top}\\frac{d\\lambda_{\\delta}^{*}}{d\\delta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $y_{\\delta}^{*}$ is minimizes a strongly convex function over a linear equality constraint (3.3), the reasoning that yields (3.4) also gives the following, which, when combined with (3.5), finishes the proof: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\frac{d y_{\\delta}^{*}(x)}{d\\delta}\\right]\\Bigg\\rvert_{\\delta=0}=\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[-\\nabla_{y}f(x,y^{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Having shown the construction of the hypergradient approximation, we now state smoothness of the hyperobjective $F$ (proof in Appendix B), crucial to getting our claimed rate. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3. The solution $y^{\\ast}$ (as defined in Problem 3.1) is $O(C_{H}\\cdot(C_{g}+\\|A\\|))$ -Lipschitz continuous and $O(C_{H}^{3}\\cdot S_{g}\\cdot(C_{g}+\\|A\\|)^{2})$ -smooth as a function of $x$ . Thus the hyper-objective $F$ is gradientLipschitz with a smoothness constant of $C_{F}:=O\\{(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+||A||)^{2}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Nonsmooth nonconvex optimization with inexact oracle ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now shift gears from the case of linear equality constraints to that of linear inequality constraints. Specifically, defining $h(x,y)=A x-B y-b$ , the problem we now consider is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{minimize}_{x}\\ F(x):=f(x,y^{*}(x))\\quad\\mathrm{~subject~to~}y^{*}(x)\\in\\arg\\operatorname*{min}_{y:h(x,y)\\leq0}g(x,y).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As noted earlier, for this larger problem class, the hyperobjective $F$ can be nonsmooth nonconvex, necessitating our measure of convergence to be the now popular notion of Goldstein stationarity [43]. ", "page_idx": 5}, {"type": "text", "text": "Our first algorithm for solving Problem 4.1 is presented in Algorithm 2, with its convergence guarantee in Theorem 4.1. At a high level, this algorithm first obtains access to an inexact zeroth-order oracle to $F$ (we shortly explain how this is done) and uses this oracle to construct a (biased) gradient estimate of $F$ . It then uses this gradient estimate to update the iterates with a rule motivated by recent works reducing nonconvex optimization to online optimization [47]. We explain this in Section 4.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Consider Problem 4.1 under Assumptions 2.2 and 2.4. Let $\\kappa\\,=\\,C_{g}/\\mu_{g}$ be the condition number of $g$ . Then combining the procedure for Lemma 4.2 with Algorithm 2 run with n \u03b42, F (xL0)f \u2212Liynf F , \u03bd = \u03b4\u2212\u03c1, D = \u0398 $\\begin{array}{r}{D=\\Theta\\left(\\frac{\\nu\\epsilon^{2}\\rho^{2}}{d_{x}\\rho^{2}L_{f}^{2}L_{y}^{2}+\\alpha^{2}d_{x}^{2}}\\right)}\\end{array}$ dx\u03c12L\u03bd2f \u03f5L2y\u03c1+\u03b12d2x , and \u03b7 = \u0398 $\\begin{array}{r}{\\eta=\\Theta\\left(\\frac{\\nu\\epsilon^{3}\\rho^{4}}{(d_{x}\\rho^{2}L_{f}^{2}L_{y}^{2}+\\alpha^{2}d_{x}^{2})^{2}}\\right)}\\end{array}$ outputs $x^{\\mathrm{out}}$ such that $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{out}}))]\\le\\epsilon+\\alpha$ with $T$ oracle calls to $f$ and $g$ , where: ", "page_idx": 5}, {"type": "equation", "text": "$$\nT=O\\left(\\frac{\\sqrt{\\kappa}d_{x}(F(x_{0})-\\operatorname*{inf}F)}{\\delta\\epsilon^{3}}\\cdot\\left(L_{f}^{2}L_{y}^{2}+\\alpha^{2}\\left(\\frac{d_{x}}{\\delta^{2}}+\\frac{d_{x}L_{f}^{2}L_{y}^{2}}{(F(x_{0})-\\operatorname*{inf}F)^{2}}\\right)\\right)\\cdot\\log(L_{f}/\\alpha)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1: Input: Initialization $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{d}$ , clipping parameter $D>0$ , step size $\\eta>0$ , smoothing parameter   \n$\\rho>0$ , accuracy parameter $\\nu>0$ , iteration budget $T\\in\\mathbb N$ , inexact zero-order oracle $\\bar{\\tilde{F}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ .   \n2: Initialize: $\\Delta_{1}=\\mathbf{0}$   \n3: for $t=1,\\dots,T$ do   \n4: Sample $s_{t}\\sim\\mathrm{Unif}[0,1]$ , $w_{t}\\sim\\mathrm{Unif}(\\mathbb{S}^{d-1})$   \n5: $\\begin{array}{r l}&{\\mathfrak{a u m p r}\\;s_{t}\\sim\\mathfrak{o u m}_{\\lfloor0,\\rfloor},\\,\\mathfrak{a}_{t}\\sim\\mathfrak{o u m}_{\\lfloor0\\rfloor},}\\\\ &{x_{t}=x_{t-1}+\\Delta_{t},\\ \\ z_{t}=x_{t-1}+s_{t}\\Delta_{t}}\\\\ &{\\widetilde{g}_{t}=\\frac{d}{2\\rho}\\big(\\widetilde{F}(z_{t}+\\rho w_{t})-\\widetilde{F}(z_{t}-\\rho w_{t})\\big)w_{t}}\\\\ &{\\Delta_{t+1}=\\mathrm{clip}_{D}\\,(\\Delta_{t}-\\eta\\widetilde{g}_{t})}\\end{array}$   \n6:   \n7: $\\begin{array}{r}{\\triangleright\\mathrm{clip}_{D}(z):=\\operatorname*{min}\\{1,\\frac{D}{\\|z\\|}\\}\\cdot z}\\end{array}$   \n8: $\\begin{array}{r}{M=\\left\\lfloor\\frac{\\nu}{D}\\right\\rfloor}\\end{array}$ , $\\textstyle K=\\left\\lfloor{\\frac{T}{M}}\\right\\rfloor$   \n9: for $k=\\overline{{1}},\\ldots,K$ do   \n10: xk =M m=1 z(k\u22121)M+m   \n11: Sample $x^{\\mathrm{out}}\\sim\\operatorname{Unif}\\{\\overline{{x}}_{1},\\ldots,\\overline{{x}}_{K}\\}$   \n12: Output: xout. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 is a variant of gradient descent with momentum and clipping, with $\\widetilde{g}_{t}$ the inexact gradient, $\\Delta_{t}$ a clipped accumulated gradient (hence accounts for past gradients, which  s erve as a momentum), and the clipping ensuring that consecutive iterates of the algorithm reside within a $\\delta$ -ball of each other. While similar algorithms have appeared in prior work on nonsmooth nonconvex optimization (e.g. [47]), none of them account for inexactness in the gradient, crucial in our setting. ", "page_idx": 6}, {"type": "text", "text": "4.1 Nonsmooth nonconvex optimization with inexact zeroth-order oracle ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We can obtain inexact zeroth-order oracle access to $F$ because (as formalized in Lemma 4.2) despite potential nonsmoothness and nonconvexity of $F$ in Problem 4.1, estimating its value $F(x)$ at any point $x$ amounts to solving a single smooth and strongly convex optimization problem, which can be done can be done in $\\widetilde O(1)$ oracle calls to $f$ and $g$ by appealing to a result by Zhang and Lan [45]. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.2 (Proof in Appendix C.1). Given any $x$ , we can return $\\widetilde{F}(\\boldsymbol{x})$ such that $|F(x)-\\widetilde{F}(x)|\\le\\alpha$ using $O(\\sqrt{C_{g}/\\mu_{g}}\\log(L_{f}/\\alpha))$ first-order oracle calls to $f$ and $g$ . ", "page_idx": 6}, {"type": "text", "text": "Having computed an inexact value of the hyperobjective $F$ , we now show how to use it to develop an algorithm for solving Problem 4.1. To this end, we first note that $F$ , despite being possibly nonsmooth and nonconvex, is Lipschitz and hence amenable to the use of recent algorithmic developments in nonsmooth nonconvex optimization pertaining to Goldstein stationarity. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3. Under Assumption 2.2 and 2.5, $F$ in Problem 4.1 is $O(L_{f}L_{y})$ -Lipschitz in $x$ . ", "page_idx": 6}, {"type": "text", "text": "With this guarantee on the Lipschitzness of $F$ , we prove Theorem C.1 for attaining Goldstein stationarity using the inexact zeroth-order oracle of a Lipschitz function. Our proof of Theorem C.1 crucially uses the recent online-to-nonconvex framework of Cutkosky, Mehta, and Orabona [47]. Combining Lemma 4.2 and Theorem C.1 then immediately implies Theorem 4.1. ", "page_idx": 6}, {"type": "text", "text": "4.2 Nonsmooth nonconvex optimization with inexact gradient oracle ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 5, we provide a way to generate approximate gradients of $F$ . Here, we present an algorithm that attains Goldstein stationarity of Problem 4.1 using this inexact gradient oracle. While there has been a long line of recent work on algorithms for nonsmooth nonconvex optimization with convergence to Goldstein stationarity [43, 48, 86\u201388], these results necessarily require exact gradients. This brittleness to any error in gradients renders them ineffective in our setting, where our computed (hyper)gradient necessarily suffers from an additive error. While inexact oracles are known to be effective for smooth or convex objectives [89], utilizing inexact gradients in the nonsmooth nonconvex regime presents a nontrivial challenge. Indeed, without any local bound on gradient variation due to smoothness, or convexity that ensures that gradients are everywhere correlated with the direction pointing at the minimum, it is not clear a priori how to control the accumulating price of inexactness throughout the run of an algorithm. To derive such results, we use the recently proposed connection between online learning and nonsmooth nonconvex optimization by Cutkosky, Mehta, and Orabona [47]. By controlling the accumulated error suffered by online gradient descent for linear losses (cf. Lemma C.3), we derive guarantees for our setting of interest, providing Lipschitz optimization algorithms that converge to Goldstein stationary points even with inexact gradients. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "This algorithm matches the best known complexity in first-order nonsmooth nonconvex optimization [43, 47, 48], merely replacing the convergence to a $(\\delta,\\epsilon)$ -stationary point by $(\\delta,\\epsilon+\\alpha)$ -stationarity, where $\\alpha$ is the inexactness of the gradient oracle. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 3 Nonsmooth Nonconvex Algorithm with Inexact Gradient Oracle ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: Initialization $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{d}$ , clipping parameter $D>0$ , step size $\\eta>0$ , accuracy parameter $\\delta>0$ , iteration budget $T\\in\\mathbb N$ , inexact gradient oracle $\\widetilde{\\nabla}F:\\mathbb R^{d}\\rightarrow\\mathbb R^{d}$ . 2: Initialize: $\\Delta_{1}=\\mathbf{0}$ 3: for $t=1,\\dots,T$ do 4: Sample $s_{t}\\sim\\mathrm{Unif}[0,1]$ 5: 6: $\\begin{array}{r l}&{x_{t}=x_{t-1}+\\Delta_{t},\\;\\;\\dot{z}_{t}=x_{t-1}+s_{t}\\Delta_{t}}\\\\ &{\\widetilde{g}_{t}=\\widetilde{\\nabla}F\\big(z_{t}\\big)}\\\\ &{\\Delta_{t+1}=\\mathrm{clip}_{D}\\left(\\Delta_{t}-\\eta\\widetilde{g}_{t}\\right)}\\end{array}$ 7: $\\begin{array}{r}{\\triangleright\\mathrm{clip}_{D}(z):=\\operatorname*{min}\\{1,\\frac{D}{\\|z\\|}\\}\\cdot z}\\end{array}$ 8: $\\begin{array}{r}{M=\\left\\lfloor\\frac{\\delta}{D}\\right\\rfloor}\\end{array}$ , $\\textstyle K=\\left\\lfloor{\\frac{T}{M}}\\right\\rfloor$ 190:: for $k=1,\\ldots,K$ $\\begin{array}{r}{\\overline{{x}}_{k}=\\frac{1}{M}\\sum_{m=1}^{M}z_{(k-1)M+m}}\\end{array}$ 11: Sample $x^{\\mathrm{out}}\\sim\\operatorname{Unif}\\{\\overline{{x}}_{1},\\ldots,\\overline{{x}}_{K}\\}$ 12: Output: xout. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. Suppose $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $L$ -Lipschitz and that $\\|\\widetilde{\\nabla}F(\\cdot)-\\nabla F(\\cdot)\\|\\le\\alpha$ . Then running Algorithm 3 with $\\begin{array}{r}{D=\\Theta(\\frac{\\delta\\epsilon^{2}}{L^{2}}),\\eta=\\Theta(\\frac{\\delta\\epsilon^{3}}{L^{4}})}\\end{array}$ , outputs a point $x^{\\mathrm{out}}$ such that $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{out}}))]\\le$ \u03f5 + \u03b1, with T = O (F (x0)\u2212i3nf F )L2 calls to $\\widetilde{\\nabla}F(\\cdot)$ . ", "page_idx": 7}, {"type": "text", "text": "We defer the proof of Theorem 4.4 to Appendix C.1. Plugging the complexity of computing inexact gradients, as given by Theorem 5.3, into the result above, we immediately obtain convergence to a $(\\delta,\\epsilon)$ -stationary point of Problem 1.1 with $\\widetilde{O}(\\delta^{-1}\\epsilon^{-4})$ gradient calls overall. ", "page_idx": 7}, {"type": "text", "text": "Implementation-friendly algorithm. While Algorithm 3 matches the best-known results in nonsmooth nonconvex optimization, it could be impractical due to several hyperparameters which need tuning. Arguably, a more natural application of the hypergradient estimates would be simply plugging them into gradient descent, which requires tuning only the stepsize. Since $F$ is neither smooth nor convex, perturbations are required to guarantee differentiability along the trajectory. We therefore complement Theorem 4.4 by analyzing perturbed (inexact) gradient descent in the nonsmooth nonconvex setting (Algorithm 7) and state its theoretical guarantee in Theorem C.4. Despite its suboptimal worst-case theoretical guarantees, we find this algorithm easier to implement in practice. ", "page_idx": 7}, {"type": "text", "text": "5 Inequality constraints: constructing the inexact gradient oracle ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Computing a stationary point of $F$ of Problem 4.1 via any first-order method would require: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla F(x)=\\nabla_{x}f(x,y^{*})+\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for which the key challenge lies in computing $d y^{*}(x)/d x$ . This requires differentiating through an argmin operator, which typically requires second-order derivatives. Instead, here we differentiate (using the implicit function theorem) through the KKT conditions describing $y^{*}(x)$ and get: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h\\right.}&{\\nabla_{y}h_{T}^{\\top}\\right]\\left[\\frac{d y^{*}(x)}{d x}\\right]=-\\left[\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h\\right]}\\\\ {\\left.\\operatorname{diag}(\\lambda_{T}^{*})\\nabla_{y}h_{T}\\right.}&{\\quad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where given $x$ , we assume efficient access to the optimal dual solution $\\lambda^{*}(x)$ of the LL problem in Problem 4.1. In (5.2), we use $\\mathcal{Z}:=\\{i\\in[d_{h}]:h_{i}^{\\bar{(}}(x,y)=0,\\lambda_{i}^{*}>0\\}$ to denote the set of active ", "page_idx": 7}, {"type": "text", "text": "constraints with non-zero dual solution, with $h_{\\mathcal{T}}:=[h_{i}]_{i\\in\\mathbb{Z}}$ and $\\lambda_{\\mathcal{T}}^{*}:=[\\lambda_{i}^{*}]_{i\\in\\mathbb{Z}}$ being the constraints and dual variables, respectively, corresponding to $\\mathcal{T}$ . ", "page_idx": 8}, {"type": "text", "text": "Observe that as currently stated, (5.2) leads to a second-order computation of $d y^{*}(x)/d x$ . In the rest of the section, we provide a fully first-order approximate hypergradient oracle by constructing an equivalent reformulation of Problem 4.1 using a penalty function. ", "page_idx": 8}, {"type": "text", "text": "5.1 Reformulation via the penalty method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We begin by reformulating Problem 4.1 into a single level constrained optimization problem: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{minimize}_{x,y}\\;f(x,y)\\;\\mathrm{subject}\\;\\mathrm{to}\\;\\left\\{\\!g(x,y)+(\\lambda^{*}(x))^{\\top}h(x,y)\\leq g^{*}(x)\\!\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{g^{*}(x):=\\operatorname*{min}_{y:h(x,y)\\leq0}g(x,y)=g(x,y^{*}(x))}\\end{array}$ and $\\lambda^{*}(x)$ is the optimal dual solution. The equivalence of this reformulation to Problem 4.1 is spelled out in Appendix D. From (5.3), we define the following penalty function, crucial to our analysis: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)=f(x,y)+\\alpha_{1}\\left(g(x,y)+(\\lambda^{*})^{\\top}h(x,y)-g^{*}(x)\\right)+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(x,y)\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\pmb{\\alpha}=[\\alpha_{1},\\alpha_{2}]\\geq0$ are the penalty parameters. Notably, we can compute its derivative with respect to $x$ of (5.4) in a fully first-order fashion by the following expression: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{I}_{x}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)=\\nabla_{x}f(x,y)+\\alpha_{1}(\\nabla_{x}g(x,y)+\\nabla_{x}h(x,y)^{\\top}\\lambda^{*}-\\nabla_{x}g^{*}(x))+\\alpha_{2}\\nabla_{x}h_{\\mathbb{Z}}(x,y)^{\\top}h_{\\mathbb{Z}}(x,y)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To give some intuition for our choice of penalties in (5.4), we note that the two constraints in (5.3) behave quite differently. The first constraint $g(x,y)+\\lambda^{*}(x)^{\\top}h(x,y)\\leq g^{*}(x)$ is one-sided, i.e., can only be violated or met, and hence just needs a penalty parameter $\\alpha_{1}$ to weight the \u201cviolation\u201d. As to the second constraint $h(x,y)\\leq0$ , it can be arbitrary. To allow for such a \u201ctwo-sided\u201d constraint, we penalize only the active constraints $\\mathcal{T}$ , i.e., we use $\\left\\|h_{\\mathcal{T}}(x,y)\\right\\|^{2}$ to penalize deviation. ", "page_idx": 8}, {"type": "text", "text": "Next, we define the optimal solutions to the penalty function optimization by: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\lambda^{*},\\alpha}^{*}(x):=\\arg\\operatorname*{min}_{y}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We now show that this minimizer is close to the optimal solution of the LL problem, while suffering only a small constraint violation. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1. Given any $x$ , the corresponding dual solution $\\lambda^{*}(x)$ , primal solution $y^{*}(x)$ of the lower optimization problem in Problem 4.1, and $y_{\\lambda^{*},\\alpha}^{*}(x)$ as in (5.5), satisfy: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|y_{\\lambda^{*},\\alpha}^{*}(x)-y^{*}(x)\\right\\|\\leq O(\\alpha_{1}^{-1})\\;\\,a n d\\;\\;\\big\\|h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*}(x))\\big\\|\\leq O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof of Lemma 5.1 is based on the Lipschitzness of $f$ and strong convexity of $g$ for sufficiently large $\\alpha_{1}$ . The aforementioned constraint violation bound on $h_{\\mathcal{T}}(x,y)$ is later used in Lemma 5.2 to bound the inexactness of our proposed gradient oracle. ", "page_idx": 8}, {"type": "text", "text": "5.2 Main result: approximating the hypergradient ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The main export of this section is the following bound on the approximation of the hypergradient.   \nThis, together with the bounds in Lemma 5.1, validate our use of the penalty function in (5.4). ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.2. Consider $F$ as in Problem 4.1, $\\mathcal{L}$ as in (5.4), a fixed $x$ , and $y_{\\lambda^{*},\\alpha}^{*}$ as in (5.5). Then under Assumptions 2.2 and 2.5, we have: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\nabla F(x)-\\nabla_{x}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y_{\\lambda^{*},\\alpha}^{*})\\|\\leq O(\\alpha_{1}^{-1})+O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2})+O(\\alpha_{1}^{1/2}\\alpha_{2}^{-1/2})+O(\\alpha_{1}^{-3/2}\\alpha_{2}^{1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof can be found in Appendix G. With this hypergradient approximation guarantee, we design Algorithm 4 to compute an inexact gradient oracle for the hyperobjective $F$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.3. Given any accuracy parameter $\\alpha\\ >\\ 0$ , Algorithm $^{4}$ outputs $\\widetilde{\\nabla}_{x}F(x)$ such that $\\|\\widetilde{\\nabla}F(x)-\\nabla F(x)\\|\\le\\alpha$ within $\\widetilde{O}(\\alpha^{-1})$ gradient oracle evaluations. ", "page_idx": 8}, {"type": "text", "text": "The full proof of this result may be found in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 4 Inexact Gradient Oracle for General Inequality Constraints ", "page_idx": 9}, {"type": "text", "text": "1: Input: Upper level variable $x$ , accuracy $\\alpha$ , penalty parameters $\\alpha_{1}=\\alpha^{-2},\\alpha_{2}=\\alpha^{-4}$ .   \n2: Compute $y^{\\ast}$ , $\\lambda^{*}$ , and active constraints $\\mathcal{T}$ of the constrained LL problem $\\begin{array}{r}{\\operatorname*{min}_{y:h(x,y)\\leq0}g(x,y)}\\end{array}$ .   \n3: Define penalty function $\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)$ by (5.4)   \n4: Compute the minimizer $y_{\\lambda^{*},\\alpha}^{*}=\\arg\\operatorname*{min}_{y}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)$ (as in (5.5)).   \n5: Output: $\\tilde{\\nabla}F:=\\nabla_{x}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y_{\\lambda^{*},\\alpha}^{*})$ . ", "page_idx": 9}, {"type": "image", "img_path": "eNCYpTCGhr/tmp/5d79fe8eca0fbb81ad7eab151f25f546c0ed859ae320b4b91668ce1ba24b58d9.jpg", "img_caption": ["(a) Convergence and gradient er- (b) Convergence analysis with vary- (c) Computation cost per gradient ror of Fully First-order Constrained ing gradient inexactness $\\alpha$ to mea- step of varying problem size $d_{y}$ . We Bilevel Algorithm (F2CBA) com- sure the tradeoff of accuracy and vary $d_{y}$ from 100 to 1000 to meapared to cvxpylayer [18]. convergence. sure the computation cost. ", "Figure 1: We run Algorithm 3 using Algorithm 4 on the bilevel optimization in the toy example in Problem L.1 with $d_{x}=100$ , $d_{y}=200$ , $n_{\\mathrm{const}}=d_{y}/5$ , and accuracy $\\alpha=0.1$ . Figure 2a, Figure 2b, Figure 2c vary $\\#$ of iterations, gradient exactness $\\alpha$ , and $d_{y}$ , respectively, to compare the performance under different settings. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We generate instances of the following constrained bilevel optimization problem: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\boldsymbol{x}}c^{\\top}\\boldsymbol{y}^{*}+0.01\\left\\|\\boldsymbol{x}\\right\\|^{2}+0.01\\left\\|\\boldsymbol{y}^{*}\\right\\|^{2}\\;\\mathrm{~s.t.~}\\,\\boldsymbol{y}^{*}=\\arg\\operatorname*{min}_{\\boldsymbol{y}:h(\\boldsymbol{x},\\boldsymbol{y})\\leq0}\\frac{1}{2}\\boldsymbol{y}^{\\top}Q\\boldsymbol{y}+\\boldsymbol{x}^{\\top}P\\boldsymbol{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $h(x,y)=A y-b$ is a $d_{h}$ -dim linear constraint. The PSD matrix $Q\\in\\mathbb{R}^{d_{y}\\times d_{y}},c\\in\\mathbb{R}^{d_{y}},P\\in$ $\\mathbb{R}^{d_{x}\\times d_{y}}$ , and constraints $A\\in\\mathbb{R}^{d_{h}\\times d_{y}}$ , $b\\in\\mathbb{R}^{d_{h}}$ are randomly generated from normal distributions (cf. Appendix K). We compare Algorithm 3 with a non-fully first-order method using cvxpyLayer [18]. Both algorithms use Adam [90] to control the learning rate, and are averaged over 10 random seeds. ", "page_idx": 9}, {"type": "text", "text": "Figure 2a shows that both the algorithms converge to the same optimal solution at the same rate. Simultaneously, the colorful bars represent the gradient differences between two methods, showing the inexactness of our gradients. Figure 2b additionally varies this inexactness to demonstrate its impact on convergence with standard deviation plotted. Figure 2c compares the computation costs for different lower-level problem sizes. Our fully first-order method significantly outperforms, in computation cost, the non-fully first-order method implemented using differentiable optimization method. The implementation can be found in https://github.com/guaguakai/constrained-bilevel-optimization. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations and future directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One limitation to our approach is that the inexact gradient oracle we constructed in Section 5 requires access to the exact dual multiplier $\\lambda^{*}$ . For a first-order algorithm, the closest proxy one could get to this would be a highly accurate approximation (which could be computed up to $\\epsilon$ error in ${\\cal O}(\\log(1/\\epsilon))$ gradient oracle evaluations). Removing this \u201cexact dual access\u201d assumption (Assumption 2.5) would be an important result. ", "page_idx": 9}, {"type": "text", "text": "Another important direction for future work would be to extend the hypergradient stationarity guarantee of Theorem 1.4 to bilevel programs with general convex constraints. To this end, we conjecture that the use of a primal-only gradient approximation oracle could be potentially effective. ", "page_idx": 9}, {"type": "text", "text": "Finally, our current rate of $\\widetilde{O}(\\delta^{-1}\\epsilon^{-4})$ oracle calls for reaching $(\\delta,\\epsilon)$ -Goldstein stationarity is not necessarily inherent to the  problem; indeed, it might be the case that an alternate approach could improve it to the best known rate of $O(\\delta^{-1}\\epsilon^{-3})$ for nonsmooth nonconvex optimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "GK is supported by an Azrieli Foundation graduate fellowship. KW is supported by Schmidt Sciences AI2050 Fellowship and NSF IIS-2403240. SS acknowledges generous support from the Alexander von Humboldt Foundation. SP is supported by NSF CCF-2112665 (TILOS AI Research Institute) and gratefully acknowledges Ali Jadbabaie for many useful discussions. ZZ is supported by NSF FODSI Fellowship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jerome Bracken and James T McGill. \u201cMathematical programs with optimization problems in the constraints\u201d. In: Operations research 21.1 (1973), pp. 37\u201344 (cit. on p. 1). [2] Beno\u00eet Colson, Patrice Marcotte, and Gilles Savard. \u201cAn overview of bilevel optimization\u201d. In: Annals of operations research 153 (2007), pp. 235\u2013256 (cit. on p. 1).   \n[3] Jonathan F Bard. Practical bilevel optimization: algorithms and applications. Vol. 30. Springer Science & Business Media, 2013 (cit. on p. 1).   \n[4] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. \u201cA review on bilevel optimization: From classical to evolutionary approaches and applications\u201d. In: IEEE transactions on evolutionary computation 22.2 (2017), pp. 276\u2013295 (cit. on p. 1). [5] Jake Snell, Kevin Swersky, and Richard Zemel. \u201cPrototypical networks for few-shot learning\u201d. In: Advances in neural information processing systems 30 (2017) (cit. on p. 1). [6] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. \u201cMeta-learning with differentiable closed-form solvers\u201d. In: arXiv preprint arXiv:1805.08136 (2018) (cit. on p. 1).   \n[7] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. \u201cMeta-learning with implicit gradients\u201d. In: Advances in neural information processing systems 32 (2019) (cit. on p. 1).   \n[8] Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. \u201cConvergence of meta-learning with task-specific adaptation over partial parameters\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 11490\u201311500 (cit. on p. 1). [9] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. \u201cBilevel programming for hyperparameter optimization and meta-learning\u201d. In: International conference on machine learning. PMLR. 2018, pp. 1568\u20131577 (cit. on pp. 1, 3).   \n[10] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. \u201cTruncated backpropagation for bilevel optimization\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 1723\u20131732 (cit. on pp. 1, 3).   \n[11] Matthias Feurer and Frank Hutter. \u201cHyperparameter optimization\u201d. In: Automated machine learning: Methods, systems, challenges (2019), pp. 3\u201333 (cit. on p. 1).   \n[12] Vijay Konda and John Tsitsiklis. \u201cActor-critic algorithms\u201d. In: Advances in neural information processing systems 12 (1999) (cit. on p. 1).   \n[13] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018 (cit. on p. 1).   \n[14] M Hong, H Wai, Z Wang, and Z Yang. \u201cA two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. arXiv e-prints, art\u201d. In: arXiv preprint arXiv:2007.05170 (2020) (cit. on p. 1).   \n[15] Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. \u201cBi-level actor-critic for multi-agent coordination\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 05. 2020, pp. 7325\u20137332 (cit. on p. 1).   \n[16] Saeed Ghadimi and Mengdi Wang. \u201cApproximation methods for bilevel programming\u201d. In: arXiv preprint arXiv:1802.02246 (2018) (cit. on pp. 1, 3, 24).   \n[17] Brandon Amos and J Zico Kolter. \u201cOptnet: Differentiable optimization as a layer in neural networks\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 136\u2013145 (cit. on pp. 1, 3).   \n[18] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. \u201cDifferentiable convex optimization layers\u201d. In: Advances in neural information processing systems 32 (2019) (cit. on pp. 1, 3, 10, 37, 38).   \n[19] Priya Donti, Brandon Amos, and J Zico Kolter. \u201cTask-based end-to-end model learning in stochastic optimization\u201d. In: Advances in neural information processing systems 30 (2017) (cit. on p. 1).   \n[20] Bryan Wilder, Bistra Dilkina, and Milind Tambe. \u201cMelding the data-decisions pipeline: Decision-focused learning for combinatorial optimization\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 01. 2019, pp. 1658\u20131665 (cit. on pp. 1, 2).   \n[21] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. \u201cEnd-to-end constrained optimization learning: A survey\u201d. In: arXiv preprint arXiv:2103.16378 (2021) (cit. on p. 1).   \n[22] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. \u201cMeta-learning with differentiable convex optimization\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 10657\u201310665 (cit. on p. 1).   \n[23] Bo Tang and Elias B Khalil. \u201cPyepo: A pytorch-based end-to-end predict-then-optimize library for linear and integer programming\u201d. In: arXiv preprint arXiv:2206.14234 (2022) (cit. on p. 1).   \n[24] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. \u201cDeep equilibrium models\u201d. In: Advances in neural information processing systems 32 (2019) (cit. on p. 1).   \n[25] Akshay Mehra and Jihun Hamm. \u201cPenalty method for inversion-free deep bilevel optimization\u201d. In: Asian conference on machine learning. PMLR. 2021, pp. 347\u2013362 (cit. on p. 1).   \n[26] Kaiyi Ji, Junjie Yang, and Yingbin Liang. \u201cBilevel optimization: Convergence analysis and enhanced design\u201d. In: International conference on machine learning. PMLR. 2021, pp. 4882\u2013 4892 (cit. on p. 1).   \n[27] Kai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, and Milind Tambe. \u201cLearning mdps from features: Predict-then-optimize for sequential decision making by reinforcement learning\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8795\u20138806 (cit. on p. 1).   \n[28] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. \u201cBome! bilevel optimization made easy: A simple first-order approach\u201d. In: Advances in neural information processing systems 35 (2022), pp. 17248\u201317262 (cit. on pp. 1, 3).   \n[29] Kai Wang, Lily Xu, Andrew Perrault, Michael K Reiter, and Milind Tambe. \u201cCoordinating followers to reach better equilibria: End-to-end gradient descent for stackelberg games\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 5. 2022, pp. 5219\u20135227 (cit. on p. 2).   \n[30] Paul D\u00fctting, Zhe Feng, Harikrishna Narasimhan, David C Parkes, and Sai S Ravindranath. \u201cOptimal auctions through deep learning\u201d. In: Communications of the ACM 64.8 (2021), pp. 109\u2013116 (cit. on p. 2).   \n[31] Jiuping Xu, Yan Tu, and Ziqiang Zeng. \u201cBilevel optimization of regional water resources allocation problem under fuzzy random environment\u201d. In: Journal of Water Resources Planning and Management 139.3 (2013), pp. 246\u2013264 (cit. on p. 2).   \n[32] Walter J Gutjahr and Nada Dzubur. \u201cBi-objective bilevel optimization of distribution center locations considering user equilibria\u201d. In: Transportation Research Part E: Logistics and Transportation Review 85 (2016), pp. 1\u201322 (cit. on p. 2).   \n[33] Yue Zhang, Oded Berman, Patrice Marcotte, and Vedat Verter. \u201cA bilevel model for preventive healthcare facility network design with congestion\u201d. In: IIE Transactions 42.12 (2010), pp. 865\u2013 880 (cit. on p. 2).   \n[34] Amir M Fathollahi-Fard, Mostafa Hajiaghaei-Keshteli, Reza Tavakkoli-Moghaddam, and Neale R Smith. \u201cBi-level programming for home health care supply chain considering outsourcing\u201d. In: Journal of Industrial Information Integration 25 (2022), p. 100246 (cit. on p. 2).   \n[35] Adam N Elmachtoub and Paul Grigas. \u201cSmart \u201cpredict, then optimize\u201d\u201d. In: Management Science 68.1 (2022), pp. 9\u201326 (cit. on p. 2).   \n[36] Miguel Angel Mu\u00f1oz, Salvador Pineda, and Juan Miguel Morales. \u201cA bilevel framework for decision-making under uncertainty with contextual information\u201d. In: Omega 108 (2022), p. 102575 (cit. on p. 2).   \n[37] Prashant Khanduri, Ioannis Tsaknakis, Yihua Zhang, Jia Liu, Sijia Liu, Jiawei Zhang, and Mingyi Hong. \u201cLinearly constrained bilevel optimization: A smoothed implicit gradient approach\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 16291\u2013 16325 (cit. on pp. 2\u20135).   \n[38] Wei Yao, Chengming Yu, Shangzhi Zeng, and Jin Zhang. Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm. 2024. arXiv: 2401.16164 [cs.LG] (cit. on pp. 2\u20134).   \n[39] Zhaosong Lu and Sanyou Mei. First-order penalty methods for bilevel optimization. 2024. arXiv: 2301.01716 [math.OC] (cit. on pp. 2\u20134).   \n[40] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. \u201cA fully first-order method for stochastic bilevel optimization\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 18083\u201318113 (cit. on pp. 2, 3).   \n[41] AA Goldstein. \u201cOptimization of Lipschitz continuous functions\u201d. In: Mathematical Programming 13 (1977), pp. 14\u201322 (cit. on pp. 2, 4).   \n[42] Arkadij Semenovi\u02c7c Nemirovskij and David Borisovich Yudin. \u201cProblem complexity and method efficiency in optimization\u201d. In: (1983) (cit. on p. 2).   \n[43] Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, and Ali Jadbabaie. \u201cComplexity of finding stationary points of nonconvex nonsmooth functions\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 11173\u201311182 (cit. on pp. 2, 6\u20138).   \n[44] Guy Kornowski and Ohad Shamir. \u201cOracle complexity in nonsmooth nonconvex optimization\u201d. In: Journal of Machine Learning Research 23.314 (2022), pp. 1\u201344 (cit. on p. 2).   \n[45] Zhe Zhang and Guanghui Lan. \u201cSolving Convex Smooth Function Constrained Optimization Is Almost As Easy As Unconstrained Optimization\u201d. In: arXiv preprint arXiv:2210.05807 (2022) (cit. on pp. 2, 7, 24, 34).   \n[46] Guy Kornowski and Ohad Shamir. \u201cAn algorithm with optimal dimension-dependence for zero-order nonsmooth nonconvex stochastic optimization\u201d. In: Journal of Machine Learning Research 25.122 (2024), pp. 1\u201314 (cit. on pp. 3, 25).   \n[47] Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. In: International Conference on Machine Learning. PMLR. 2023, pp. 6643\u20136670 (cit. on pp. 3, 6\u20138, 26).   \n[48] Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, and Guanghao Ye. \u201cA gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions\u201d. In: Advances in neural information processing systems 35 (2022), pp. 6692\u20136703 (cit. on pp. 3, 7, 8).   \n[49] G Anandalingam and DJ White. \u201cA solution method for the linear static Stackelberg problem using penalty functions\u201d. In: IEEE Transactions on automatic control 35.10 (1990), pp. 1170\u2013 1173 (cit. on p. 3).   \n[50] Yo Ishizuka and Eitaro Aiyoshi. \u201cDouble penalty method for bilevel optimization problems\u201d. In: Annals of Operations Research 34.1 (1992), pp. 73\u201388 (cit. on p. 3).   \n[51] Douglas J White and G Anandalingam. \u201cA penalty function approach for solving bi-level linear programs\u201d. In: Journal of Global Optimization 3 (1993), pp. 397\u2013419 (cit. on p. 3).   \n[52] Luis Vicente, Gilles Savard, and Joaquim J\u00fadice. \u201cDescent approaches for quadratic bilevel programming\u201d. In: Journal of Optimization theory and applications 81.2 (1994), pp. 379\u2013399 (cit. on p. 3).   \n[53] DL Zhu. \u201cOptimality conditions for bilevel programming problems\u201d. In: Optimization 33.1 (1995), pp. 9\u201327 (cit. on p. 3).   \n[54] JJ Ye and DL Zhu. \u201cExact penalization and necessary optimality conditions for generalized bilevel programming problems\u201d. In: SIAM Journal on optimization 7.2 (1997), pp. 481\u2013507 (cit. on p. 3).   \n[55] Justin Domke. \u201cGeneric methods for optimization-based modeling\u201d. In: Artificial Intelligence and Statistics. PMLR. 2012, pp. 318\u2013326 (cit. on p. 3).   \n[56] Fabian Pedregosa. \u201cHyperparameter optimization with approximate gradient\u201d. In: International conference on machine learning. PMLR. 2016, pp. 737\u2013746 (cit. on p. 3).   \n[57] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. \u201cOn differentiating parameterized argmin and argmax problems with application to bi-level optimization\u201d. In: arXiv preprint arXiv:1607.05447 (2016) (cit. on p. 3).   \n[58] Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun, and Richard Zemel. \u201cReviving and improving recurrent back-propagation\u201d. In: International Conference on Machine Learning. PMLR. 2018, pp. 3082\u20133091 (cit. on p. 3).   \n[59] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. \u201cOn the iteration complexity of hypergradient computation\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 3748\u20133758 (cit. on p. 3).   \n[60] Jonathan Lorraine, Paul Vicol, and David Duvenaud. \u201cOptimizing millions of hyperparameters by implicit differentiation\u201d. In: International conference on artificial intelligence and statistics. PMLR. 2020, pp. 1540\u20131552 (cit. on p. 3).   \n[61] Dougal Maclaurin, David Duvenaud, and Ryan Adams. \u201cGradient-based hyperparameter optimization through reversible learning\u201d. In: International conference on machine learning. PMLR. 2015, pp. 2113\u20132122 (cit. on p. 3).   \n[62] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. \u201cForward and reverse gradient-based hyperparameter optimization\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 1165\u20131173 (cit. on p. 3).   \n[63] Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. \u201cA value-functionbased interior-point method for non-convex bi-level optimization\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 6882\u20136892 (cit. on p. 3).   \n[64] Jane J Ye, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. \u201cDifference of convex algorithms for bilevel programs with applications in hyperparameter selection\u201d. In: Mathematical Programming 198.2 (2023), pp. 1583\u20131616 (cit. on p. 3).   \n[65] Lucy L. Gao, Jane J. Ye, Haian Yin, Shangzhi Zeng, and Jin Zhang. Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. 2024. arXiv: 2306.16761 [math.OC] (cit. on p. 3).   \n[66] Tianyi Chen, Yuejiao Sun, and Wotao Yin. \u201cClosing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 25294\u201325307 (cit. on p. 3).   \n[67] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. \u201cA single-timescale method for stochastic bilevel optimization\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022, pp. 2466\u20132488 (cit. on p. 3).   \n[68] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. \u201cA two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic\u201d. In: SIAM Journal on Optimization 33.1 (2023), pp. 147\u2013180 (cit. on p. 3).   \n[69] Zeeshan Akhtar, Amrit Singh Bedi, Srujan Teja Thomdapu, and Ketan Rajawat. \u201cProjectionfree stochastic bi-level optimization\u201d. In: IEEE Transactions on Signal Processing 70 (2022), pp. 6332\u20136347 (cit. on p. 3).   \n[70] Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. \u201cA conditional gradient-based method for simple bilevel optimization with convex lower-level problem\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2023, pp. 10305\u201310323 (cit. on p. 3).   \n[71] Nazanin Abolfazli, Ruichen Jiang, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. \u201cAn Inexact Conditional Gradient Method for Constrained Bilevel Optimization\u201d. In: arXiv preprint arXiv:2306.02429 (2023) (cit. on p. 3).   \n[72] Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, and Aryan Mokhtari. \u201cProjection-free methods for stochastic simple bilevel optimization with convex lower-level problem\u201d. In: Advances in Neural Information Processing Systems 36 (2024) (cit. on p. 3).   \n[73] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. \u201cA near-optimal algorithm for stochastic bilevel optimization via double-momentum\u201d. In: Advances in neural information processing systems 34 (2021), pp. 30271\u201330283 (cit. on p. 3).   \n[74] Zhishuai Guo, Quanqi Hu, Lijun Zhang, and Tianbao Yang. \u201cRandomized stochastic variance-reduced methods for multi-task stochastic bilevel optimization\u201d. In: arXiv preprint arXiv:2105.02266 (2021) (cit. on p. 3).   \n[75] Junjie Yang, Kaiyi Ji, and Yingbin Liang. \u201cProvably faster algorithms for bilevel optimization\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 13670\u201313682 (cit. on p. 3).   \n[76] Mathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. \u201cA framework for bilevel optimization that enables stochastic and global variance reduction algorithms\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 26698\u201326710 (cit. on p. 3).   \n[77] Shoham Sabach and Shimrit Shtern. \u201cA first order method for solving convex bilevel optimization problems\u201d. In: SIAM Journal on Optimization 27.2 (2017), pp. 640\u2013660 (cit. on p. 3).   \n[78] Mostafa Amini and Farzad Yousefian. \u201cAn iterative regularized incremental projected subgradient method for a class of bilevel optimization problems\u201d. In: 2019 American Control Conference (ACC). IEEE. 2019, pp. 4069\u20134074 (cit. on p. 3).   \n[79] Mostafa Amini and Farzad Yousefian. \u201cAn iterative regularized mirror descent method for ill-posed nondifferentiable stochastic optimization\u201d. In: arXiv preprint arXiv:1901.09506 (2019) (cit. on p. 3).   \n[80] Roey Merchav and Shoham Sabach. \u201cConvex Bi-level Optimization Problems with Nonsmooth Outer Objective Function\u201d. In: SIAM Journal on Optimization 33.4 (2023), pp. 3114\u20133142 (cit. on p. 3).   \n[81] Ioannis Tsaknakis, Prashant Khanduri, and Mingyi Hong. \u201cAn implicit gradient-type method for linearly constrained bilevel problems\u201d. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2022, pp. 5438\u20135442 (cit. on p. 3).   \n[82] Siyuan Xu and Minghui Zhu. \u201cEfficient gradient approximation method for constrained bilevel optimization\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 10. 2023, pp. 12509\u201312517 (cit. on p. 3).   \n[83] Lesi Chen, Yaohua Ma, and Jingzhao Zhang. \u201cNear-Optimal Fully First-Order Algorithms for Finding Stationary Points in Bilevel Optimization\u201d. In: arXiv preprint arXiv:2306.14853 (2023) (cit. on p. 4).   \n[84] Frank H Clarke. \u201cGeneralized gradients of Lipschitz functionals\u201d. In: Advances in Mathematics 40.1 (1981), pp. 52\u201367 (cit. on p. 4).   \n[85] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic programming: modeling and theory. SIAM, 2021 (cit. on p. 4).   \n[86] Michael Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir, and Manolis Zampetakis. \u201cDeterministic nonsmooth nonconvex optimization\u201d. In: The Thirty Sixth Annual Conference on Learning Theory. PMLR. 2023, pp. 4570\u20134597 (cit. on p. 7).   \n[87] Siyu Kong and AS Lewis. \u201cThe cost of nonconvexity in deterministic nonsmooth optimization\u201d. In: Mathematics of Operations Research (2023) (cit. on p. 7).   \n[88] Benjamin Grimmer and Zhichao Jia. \u201cGoldstein Stationarity in Lipschitz Constrained Optimization\u201d. In: arXiv preprint arXiv:2310.03690 (2023) (cit. on p. 7).   \n[89] Olivier Devolder, Fran\u00e7ois Glineur, and Yurii Nesterov. \u201cFirst-order methods of smooth convex optimization with inexact oracle\u201d. In: Mathematical Programming 146 (2014), pp. 37\u201375 (cit. on p. 7).   \n[90] Diederik P Kingma and Jimmy Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv preprint arXiv:1412.6980 (2014) (cit. on pp. 10, 38).   \n[91] Guanghui Lan. First-order and stochastic optimization methods for machine learning. Vol. 1. Springer, 2020 (cit. on p. 23).   \n[92] Ohad Shamir. \u201cAn optimal algorithm for bandit and zero-order convex optimization with twopoint feedback\u201d. In: The Journal of Machine Learning Research 18.1 (2017), pp. 1703\u20131713 (cit. on p. 24).   \n[93] Simon S Du and Wei Hu. \u201cLinear convergence of the primal-dual gradient method for convexconcave saddle point problems without strong convexity\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 196\u2013205 (cit. on p. 36).   \n[94] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. \u201cPytorch: An imperative style, high-performance deep learning library\u201d. In: Advances in neural information processing systems 32 (2019) (cit. on p. 37).   \n[95] Steven Diamond and Stephen Boyd. \u201cCVXPY: A Python-embedded modeling language for convex optimization\u201d. In: Journal of Machine Learning Research 17.83 (2016), pp. 1\u20135 (cit. on p. 37). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use $\\langle\\cdot,\\cdot\\rangle$ to denote inner products and $\\|\\cdot\\|$ for the Euclidean norm. Unless transposed, all vectors are column vectors. For $f:\\mathbb{R}^{d_{2}}\\rightarrow\\mathbb{R}^{d_{1}}$ its Jacobian with respect to $\\boldsymbol{x}\\in\\mathbb{R}^{d_{2}}$ is $\\nabla f\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ . For $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , we overload $\\nabla f$ to refer to its gradient (the transposed Jacobian), a column vector. We use $\\nabla_{x}$ to denote partial derivatives with respect to $x$ . ", "page_idx": 15}, {"type": "text", "text": "A function $f:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{m}$ is $L$ -Lipschitz if for any $x,y$ , we have $\\|f(x)-f(y)\\|\\,\\leq\\,L\\|x-y\\|$ . A differentiable function $f\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ is convex if for any $\\boldsymbol{x},\\boldsymbol{y}\\,\\in\\,\\mathbb{R}^{n}$ we have $f(y)\\;\\geq\\;f(x)\\;+$ $\\nabla f(x)^{\\top}(y-x)$ ; it is $\\mu$ -strongly convex if $f-{\\frac{\\mu}{2}}\\|\\cdot\\|^{2}$ is convex; it is $\\beta$ -smooth if $\\nabla f$ is $\\beta$ -Lipschitz. For a Lipschitz function $f$ , a point $x$ is $(\\delta,\\epsilon)$ -stationary if within a $\\delta$ -ball around $x$ , there exists a convex combination of subgradients of $f$ with norm at most $\\epsilon$ . For a differentiable function $f$ , we say that $x$ is $\\epsilon$ -stationary if $\\|\\nabla{\\bar{f}}(x)\\|\\leq\\epsilon$ . ", "page_idx": 15}, {"type": "text", "text": "B Proofs from Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the full proofs of claims for bilevel programs with linear equality constraints, as stated in Section 3. We first state a few technical results using the implicit function theorem that we repeatedly invoke in our results for this setting. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Fix a point $x$ . Given $y^{*}=\\arg\\operatorname*{min}_{y:h(x,y)=0}g(x,y)$ where $g$ is strongly convex in $y$ and $\\lambda^{*}$ is the dual optimal variable for this problem, define $\\mathcal{L}_{\\mathrm{eq}}(x,y,\\lambda)=g(x,y)+\\langle\\lambda,h(x,y)\\rangle$ . Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\nabla_{y y}^{2}\\mathcal{L}_{\\mathrm{eq}}(x,y^{*},\\lambda^{*})\\right.}_{\\nabla_{y}h(x,y^{*})}\\left.\\begin{array}{c}{\\nabla_{y}h(x,y^{*})^{\\top}}\\\\ {0}\\end{array}\\right]\\left[\\frac{d y^{*}}{d x}\\right]=\\left[-\\nabla_{y x}^{2}g(x,y^{*})-\\nabla_{y x}^{2}\\langle\\lambda^{*},h(x,y^{*})\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since $g$ is strongly convex, by linear constraint qualification, the KKT condition is both sufficient and necessary condition for optimality. Hence, consider the following KKT system obtained via first order optimality of $y^{\\ast}$ , with dual optimal variable $\\lambda^{*}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{y}g(x,y^{*})+\\nabla_{y}\\langle\\lambda^{*},h(x,y^{*})\\rangle=0,\\;\\mathrm{and}\\;h(x,y^{*})=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Differentiating the system of equations in (B.1) with respect to $x$ and rearranging terms in a matrixvector format yields: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left.\\nabla_{y y}^{2}g(x,y^{*})+\\nabla_{y y}^{2}\\langle\\lambda^{*},h(x,y^{*})\\rangle\\right.}&{\\nabla_{y}h(x,y^{*})^{\\top}\\right]\\left[\\frac{d y^{*}}{d x}\\right]=\\left[-\\nabla_{y x}^{2}g(x,y^{*})-\\nabla_{y x}^{2}\\langle\\lambda^{*},h(x,y^{*})\\rangle\\right]}\\\\ {\\left.\\nabla_{y}h(x,y^{*})\\right.}&{\\qquad\\qquad\\qquad\\qquad\\left.0\\right]\\left[\\frac{d x^{*}}{d x}\\right]=-\\nabla_{x}h(x,y^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Noting that $\\nabla_{y y}^{2}\\mathcal{L}_{\\mathrm{eq}}(x,y,\\lambda)=\\nabla_{y y}^{2}g(x,y)+\\nabla_{y y}^{2}\\langle\\lambda,h(x,y)\\rangle$ , we can write (B.2) in the form shown in the lemma. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. Consider the setup in Lemma B.1. The matrix $H$ defined in (3.4) is invertible if the Hessian $\\nabla_{y y}^{2}\\mathcal{L}_{\\mathrm{eq}}(x,y^{*},\\lambda^{*}):=\\dot{\\nabla}_{y y}^{2}g(x,y^{*})+\\nabla_{y y}^{2}\\langle\\lambda^{*},h(x,y^{*})\\rangle$ satisfies \u22072yyLeq(x, y\u2217, \u03bb\u2217) \u227b0 over the tangent plane $T:=\\{y:\\nabla_{y}h(x,y^{*})y=0\\}$ and $\\nabla_{y}h$ has full rank. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $u=[y,\\lambda]$ . We show that $H u=0$ implies $u=0$ , which in turn implies invertibility of $H$ . If $\\nabla_{y}h(x,y^{*})y\\neq0$ , then by construction of $u$ and $H$ , we must also have $H u\\neq0$ . Otherwise if $\\nabla_{y}h(x,y^{*})y=0$ and $y\\ne0$ , the quadratic form $u^{\\top}H u$ is positive, as seen by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{u}^{\\top}\\boldsymbol{H}\\boldsymbol{u}=\\boldsymbol{y}^{\\top}\\nabla_{y y}^{2}\\mathcal{L}_{\\mathrm{eq}}(\\boldsymbol{x},\\boldsymbol{y}^{*},\\lambda^{*})\\boldsymbol{y}>0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the final step is by the assumption of $\\mathcal{L}_{\\mathrm{eq}}$ being positive definite over the defined tangent plane $T=\\{y:\\nabla_{y}h(x,y^{*})\\dot{y^{}}=0\\}$ . If $y=0$ while $H u=0$ , then $\\nabla_{y}h$ having full rank implies $\\lambda=0$ Combined with $y=0$ , this means $u=0$ , as required when $H u=0$ . This concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "Corollary B.3. For Problem 3.1 under Assumption 2.2 and Assumption 2.3, the matrix $H$ (as defined in (3.4)) is non-singular. Further, there exists $a$ finite $C_{H}$ such that $\\lVert H^{-1}\\rVert\\leq C_{H}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Since we are assuming strong convexity of $g$ , Lemma B.2 applies, yielding the claimed invertibility of $H$ . Combined with the boundedness of variables $x$ (per Assumption 2.3) and continuity of the inverse implies a bound on $\\|H^{-1}\\|$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.1 Construction of the inexact gradient oracle ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now show how to construct the inexact gradient oracle for the objective $F$ in Problem 3.1. As sketched in Section 3, we then use this oracle in a projected gradient descent algorithm to get the claimed guarantee. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. Consider Problem 3.1 under Assumption 2.2 and Assumption 2.3. Let $y_{\\delta}^{*}$ be as defined in (3.3). Then, for any $\\delta\\in[0,\\Delta]$ with $\\Delta\\leq\\mu_{g}/2C_{f}$ , the following relation is valid: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|y_{\\delta}^{*}-y^{*}\\|\\leq M(x)\\delta,\\ w i t h\\ M(x):=\\frac{2}{\\mu_{g}}\\|\\nabla_{y}f(x,y^{*})\\|\\leq\\frac{2L_{f}}{\\mu_{g}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The first-order optimality condition applied to $g(x,y)+\\delta f(x,y)$ at $y^{\\ast}$ and $y_{\\delta}^{*}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{y}g(x,y_{\\delta}^{*})+\\delta\\nabla_{y}f(x,y_{\\delta}^{*}),y^{*}-y_{\\delta}^{*}\\rangle\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which upon adding and subtracting $\\nabla_{y}f(x,y^{*})$ transforms into ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{y}g(x,y_{\\delta}^{*})+\\delta[\\nabla_{y}f(x,y_{\\delta}^{*})-\\nabla_{y}f(x,y^{*})]+\\delta\\nabla_{y}f(x,y^{*}),y^{*}-y_{\\delta}^{*}\\rangle\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, the first-order optimality condition applied to $g$ at $y^{\\ast}$ and $y_{\\delta}^{*}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\nabla_{y}g(x,y^{*}),y_{\\delta}^{*}-y^{*}\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Adding Inequality (B.3) and Inequality (B.4) and rearranging yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\nabla_{y}g(x,y_{\\delta}^{*})-\\nabla_{y}g(x,y^{*})+\\delta[\\nabla_{y}f(x,y_{\\delta}^{*})-\\nabla_{y}f(x,y^{*})],y_{\\delta}^{*}-y^{*}\\rangle\\leq\\langle\\delta\\nabla_{y}f(x,y^{*}),y^{*}-y_{\\delta}^{*}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying to the left side above a lower bound via strong convexity of $g+\\delta f$ and to the right hand side an upper bound via Cauchy-Schwarz inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\ns\\|y_{\\delta}^{*}-y^{*}\\|\\leq\\delta\\|\\nabla_{y}f(x,y^{*})\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $s$ is the strong convexity of $g+\\delta f$ . Since $f$ is $C_{f}$ -smooth, the worst case value of this is $\\begin{array}{r}{s=\\mu_{g}-\\delta C_{f}=\\bar{\\mu_{g}}-\\frac{\\mu_{g}}{2C_{f}}\\dot{C_{f}}=\\bar{\\mu_{g}}/2}\\end{array}$ , which when plugged in Inequality (B.5) then gives the claimed bound. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma B.5. Consider Problem 3.1 under Assumption 2.2 and Assumption 2.3. Then the following relation is valid. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{im}_{\\to0}\\frac{\\nabla_{x}[g(x,y_{\\delta}^{*}(x))+\\lambda_{\\delta}^{*}h(x,y^{*})]-\\nabla_{x}[g(x,y^{*}(x))+\\lambda^{*}h(x,y^{*})]}{\\delta}=\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*}(x)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that by definition, $g$ is strongly convex and $\\begin{array}{r}{y^{\\ast}=\\arg\\operatorname*{min}_{y:h(x,y)=0}g(x,y)}\\end{array}$ . Hence, we can apply Lemma B.1. Combining this with Lemma B.2 and further applying that linearity of $h$ implies $\\nabla_{y y}^{2}h=0$ and $\\nabla_{x y}^{2}h=0$ , we obtain the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left[\\frac{d y^{*}}{d x}\\right]=\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[-\\nabla_{y x}^{2}g(x,y^{*})\\right].}\\\\ &{}&{\\left[\\frac{d\\lambda^{*}}{d x}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So we can express the right-hand side of the claimed equation in the lemma statement by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*}(x))=\\left[\\left(\\frac{d y^{*}}{d x}\\right)^{\\top}}&{\\left(\\frac{d\\lambda^{*}}{d x}\\right)^{\\top}\\right]\\left[\\nabla_{y}f(x,y^{*}(x))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be further simplified to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[-\\nabla_{y x}^{2}g(x,y^{*})^{\\top}\\right.}&{\\left.-\\nabla_{x}h(x,y^{*})^{\\top}\\right]\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[\\nabla_{y}f(x,y^{*}(x))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now apply Lemma B.1 to the perturbed problem defined in (3.3). We know from Lemma B.4 that $\\mathrm{lim}_{\\delta\\to0}\\,y_{\\delta}^{*}=y^{*}$ . The associated KKT system is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta\\nabla_{y}f(x,y_{\\delta}^{*})+\\nabla_{y}g(x,y_{\\delta}^{*})+\\nabla_{y}\\langle\\lambda_{\\delta}^{*},h(x,y_{\\delta}^{*})\\rangle=0\\mathrm{~and~}h(x,y_{\\delta}^{*})=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking the derivative with respect of (B.7) gives the following implicit system, where we used the fact that $h$ is linear and hence $\\bar{\\nabla}_{y y}^{2}h=0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\delta\\nabla_{y y}^{2}f(x,y_{\\delta}^{*})+\\nabla_{y y}^{2}g(x,y_{\\delta}^{*})\\quad\\nabla_{y}h(x,y_{\\delta}^{*})^{\\top}\\right]}_{\\nabla_{y}h(x,y_{\\delta}^{*})}\\left[\\frac{d y_{\\delta}^{*}}{d\\delta}\\right]=\\left[-\\nabla_{y}f(x,y_{\\delta}^{*})^{\\top}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a sufficiently small $\\delta$ , we have $\\begin{array}{r}{\\nabla_{y y}^{2}g(x,y_{\\delta}^{*})+\\delta\\nabla_{y y}^{2}f(x,y_{\\delta}^{*})\\succeq.\\frac{\\mu_{g}}{2}I.}\\end{array}$ , which implies invertibility of $H_{\\delta}$ by an application of Lemma B.2. Since Lemma B.4 implies $\\mathrm{lim}_{\\delta\\to0}\\,y_{\\delta}^{*}=y^{*}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\frac{d y_{\\delta}^{*}}{d\\delta}\\right]|_{\\delta=0}=\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[-\\nabla_{y}f(x,y^{*})\\right].}\\\\ {\\left[\\frac{d\\lambda_{\\delta}^{*}}{d\\delta}\\right]|_{\\delta=0}=\\left[\\nabla_{y}h(x,y^{*})\\quad\\qquad0\\qquad\\right]\\qquad\\qquad0\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So we can express the left-hand side of the expression in the lemma statement by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\delta\\rightarrow0}{\\operatorname*{lim}}\\frac{\\nabla_{x}[g(x,y_{\\delta}^{*}(x))+\\langle\\lambda_{\\delta}^{*},h(x,y^{*})\\rangle]-\\nabla_{x}[g(x,y^{*}(x))+\\langle\\lambda^{*},h(x,y^{*})\\rangle]}{\\delta}}\\\\ &{=\\nabla_{x y}^{2}g(x,y^{*})\\frac{d y_{\\delta}^{*}}{d\\delta}+\\nabla_{x}h(x,y^{*})^{\\top}\\frac{d\\lambda_{\\delta}^{*}}{d\\delta}}\\\\ &{=\\left[\\nabla_{x y}^{2}g(x,y^{*})\\quad\\nabla_{x}h(x,y^{*})^{\\top}\\right]\\left[\\nabla_{y}^{2}y(x,y^{*})\\quad\\nabla_{y}h(x,y^{*})^{\\top}\\right]^{-1}\\left[-\\nabla_{y}f(x,y^{*})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which matches (B.6) (since $\\left(\\nabla_{y x}^{2}g\\right)^{\\top}=\\nabla_{x y}^{2}g)$ , thus concluding the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3.3. The solution $y^{\\ast}$ (as defined in Problem 3.1) is $O(C_{H}\\cdot(C_{g}+\\|A\\|))$ -Lipschitz continuous and $O(C_{H}^{3}\\cdot S_{g}\\cdot(C_{g}+\\|A\\|)^{2})$ -smooth as a function of $x$ . Thus the hyper-objective $F$ is gradientLipschitz with a smoothness constant of $C_{F}:=O\\{(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+||A||)^{2}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Rearranging (3.4) and applying Corollary B.3, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\frac{d y^{*}}{d x}\\right]=\\left[\\nabla_{y y}^{2}g(x,y^{*})\\quad B^{\\top}\\right]^{-1}\\left[-\\nabla_{y x}^{2}g(x,y^{*})\\right].}\\\\ {\\left[\\frac{d\\lambda^{*}}{d x}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies a Lipschitz bound of $C_{H}\\cdot(C_{g}+\\|A\\|)$ . Next, note that in the case with linear equality constraints, the terms in (B.2) involving second-order derivatives of $h$ are all zero; differentiating (B.2) with respect to $x$ , we notice that the linear system we get again has the same matrix $H$ from before. We can therefore again perform the same inversion and apply the bound on $\\|H^{-1}\\|$ and on the third-order derivatives of $g$ (Assumption 2.3) to observe that $\\begin{array}{r}{\\|\\frac{d^{2}y^{*}}{d x^{2}}\\|\\leq O(C_{H}\\cdot S_{g}\\|\\frac{d y^{*}}{d x}\\|^{2})=}\\end{array}$ $O(C_{H}^{3}\\cdot S_{g}\\cdot(C_{g}+\\|A\\|)^{2})$ , where we are hiding numerical constants in the Big-Oh notation. ", "page_idx": 17}, {"type": "text", "text": "As a result, we can calculate the Lipschitz smoothness constant associated with the hyper-objective $F$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla F(x)-\\nabla F(\\bar{x})\\|}\\\\ &{\\le\\|\\frac{d y^{*}(x)}{d x}\\nabla_{y}f(x,y^{*}(x))-\\frac{d y^{*}(\\bar{x})}{d x}\\nabla_{y}f(\\bar{x},y^{*}(\\bar{x}))\\|+\\|\\nabla_{x}f(x,y^{*}(x))-\\nabla_{x}f(\\bar{x},y^{*}(\\bar{x}))\\|}\\\\ &{\\le[C_{f}C_{H}(L_{g}+\\|A\\|)+C_{f}C_{H}^{2}(L_{g}+\\|A\\|)^{2}+L_{f}C_{H}^{3}S_{g}(L_{g}+\\|A\\|)^{2}]\\|x-\\bar{x}\\|}\\\\ &{\\ +[C_{f}+C_{f}C_{H}(L_{g}+\\|A\\|)]\\|x-\\bar{x}\\|}\\\\ &{\\le\\underbrace{2(L_{f}+C_{f}+C_{g})C_{H}^{3}S_{g}(L_{g}+\\|A\\|)^{2}}_{C_{F}}\\|x-\\bar{x}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 3.2. For Problem 3.1 under Assumption 2.2, with $v_{x}$ as in (3.2), the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|v_{x}-\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\nabla_{y}f(x,y^{*})\\right\\|\\leq O(C_{F}\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For simplicity, we adopt the following notation throughout this proof: $g_{x y}(x,y)=\\nabla_{x y}^{2}g$ , and $g_{x y y}$ denotes the tensor such that its $i j k$ entry is given by $\\frac{\\partial^{3}g}{\\partial x_{i}\\partial y_{j}\\partial y_{k}}$ . We first consider the terms involving . By the fundamental theorem of calculus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{x}g(x,y_{\\delta}^{*}(x))-\\nabla_{x}g(x,y^{*}(x))=\\int_{t=0}^{\\delta}g_{x y}(x,y_{t}^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}d t.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\nabla_{x}g(x,y_{\\delta}^{*}(x))-\\nabla_{x}g(x,y^{*}(x))}{\\delta}-g_{x y}(x,y^{*}(x))\\frac{d y_{\\delta}^{*}(x)}{d t}|_{t=0}}\\\\ &{=\\displaystyle\\frac{1}{\\delta}\\int_{t=0}^{\\delta}\\left(g_{x y}(x,y_{t}^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}-g_{x y}(x,y^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}|_{t=0}\\right)d t}\\\\ &{=\\displaystyle\\frac{1}{\\delta}\\int_{t=0}^{\\delta}\\left(g_{x y}(x,y_{t}^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}-g_{x y}(x,y^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}|_{t=0}\\right)d t}\\\\ &{=\\displaystyle\\frac{1}{\\delta}\\int_{t=0}^{\\delta}\\left(g_{x y}(x,y_{t}^{*}(x))-g_{x y}(x,y^{*}(x))\\frac{d y_{t}^{*}(x)}{d t}d t+\\frac{1}{\\delta}\\int_{t=0}^{\\delta}g_{x y}(x,y^{*}(x))\\cdot\\left(\\frac{d y_{t}^{*}(x)}{d t}-\\frac{d y_{t}^{*}(x)}{d t}|_{t=0}\\right)d t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now bound each of the terms on the right-hand side of (B.9). For the first term, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\frac{1}{\\delta}\\displaystyle\\int_{t=0}^{\\delta}\\big(g_{x y}(x,y_{t}^{*}(x))-g_{x y}(x,y^{*}(x))d t\\big)\\,\\frac{d y_{t}^{*}(x)}{d t}\\|}\\\\ &{\\leq\\frac{1}{\\delta}\\displaystyle\\int_{t=0}^{\\delta}\\|\\frac{d y_{t}^{*}(x)}{d t}\\|\\cdot\\int_{s=0}^{t}\\|g_{x y y}(x,y_{s}^{*}(x))\\|\\|\\frac{d y_{s}^{*}(x)}{d s}\\|d s\\cdot d t}\\\\ &{\\leq\\frac{1}{\\delta}\\displaystyle\\int_{t=0}^{\\delta}\\|\\frac{d y_{t}^{*}(x)}{d t}\\|\\cdot\\operatorname*{max}_{s\\in\\{0,\\delta\\}}\\|g_{x y y}(x,y_{s}^{*}(x))\\|\\cdot\\|\\frac{d y_{s}^{*}(x)}{d s}\\|t d t}\\\\ &{\\leq\\frac{1}{\\delta}\\cdot\\operatorname*{max}_{s\\in\\{0,\\delta\\}}\\|g_{x y y}(x,y_{u}^{*}(x))\\|\\cdot\\delta^{2}\\cdot\\operatorname*{max}_{t\\in\\{0,\\delta\\}}\\|\\frac{d y_{t}^{*}(x)}{d t}\\|^{2}}\\\\ &{\\leq\\delta\\cdot\\operatorname*{max}_{u\\in\\{0,\\delta\\}}\\|g_{x y y}(x,y_{u}^{*}(x))\\|\\cdot\\operatorname*{max}_{t\\in\\{0,\\delta\\}}\\|\\frac{d y_{t}^{*}(x)}{d t}\\|^{2}}\\\\ &{=\\delta\\cdot x_{g}\\cdot M_{g}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $M_{y}$ is the Lipschitz bound on $y^{\\ast}$ as shown in Lemma 3.3, and $S_{g}$ is the smoothness of $g$ from Assumption 2.3. For the second term on the right-hand side of (B.9), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{\\delta}\\int_{t=0}^{\\delta}g_{x y}(x,y^{*}(x))\\cdot\\left(\\frac{d y_{t}^{*}(x)}{d t}-\\frac{d y^{*}(x)}{d t}\\right)\\|\\leq\\frac{1}{\\delta}\\cdot\\|g_{x y}(x,y^{*}(x))\\|\\cdot\\int_{t=0}^{\\delta}\\left(\\int_{s=0}^{t}\\|\\frac{d^{2}}{d s^{2}}y_{s}^{*}(x)\\|d s\\right)}}\\\\ &{}&{\\leq\\frac{1}{\\delta}\\cdot\\|g_{x y}(x,y^{*}(x))\\|\\cdot\\operatorname*{max}_{s\\in[0,\\delta]}\\|\\frac{d^{2}}{d s^{2}}y_{s}^{*}(x)\\|\\cdot\\delta^{2}}\\\\ &{}&{\\leq\\delta\\cdot\\|g_{x y}(x,y^{*}(x))\\|\\cdot\\underset{s\\in[0,\\delta]}{\\operatorname*{max}}\\|\\frac{d^{2}}{d s^{2}}y_{s}^{*}(x)\\|}\\\\ &{}&{=\\delta\\cdot C_{g}\\cdot C_{y},\\quad\\ \\ \\ \\ \\ }\\end{array}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C_{g}$ is the bound on smoothness of $g$ as in Assumption 2.3, and $C_{y}$ is the bound on $\\left\\|{\\frac{d^{2}y^{*}}{d x^{2}}}\\right\\|$ from Lemma 3.3. For the terms involving the function , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\frac{\\lambda_{\\delta}^{*}-\\lambda^{*}}{\\delta}-\\frac{d\\lambda_{\\delta}^{*}}{d\\delta}|_{\\delta=0}\\|=\\frac{1}{\\delta}\\int_{t=0}^{\\delta}\\|\\frac{d\\lambda_{t}^{*}}{d t}-\\frac{d\\lambda_{\\delta}^{*}}{d\\delta}|_{\\delta=0}\\|d t}}\\\\ &{\\leq\\frac{1}{\\delta}\\int_{t=0}^{\\delta}\\int_{s=0}^{t}\\|\\frac{d^{2}}{d s^{2}}\\lambda_{s}^{*}\\|d s\\cdot d t}\\\\ &{\\leq\\frac{1}{\\delta}\\operatorname*{max}_{s\\in[0,\\delta]}\\|\\frac{d^{2}}{d s^{2}}\\lambda_{s}^{*}\\|\\cdot\\delta^{2}\\leq\\delta\\cdot\\underset{s\\in[0,\\delta]}{\\operatorname*{max}}\\|\\frac{d^{2}}{d s^{2}}\\lambda_{s}^{*}\\|}\\\\ &{=\\delta\\cdot C_{\\ell},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{\\ell}$ is the bound on $\\left\\|{\\frac{d^{2}\\lambda^{*}}{d s^{2}}}\\right\\|$ from Lemma 3.3. Combining (B.9), Inequality (B.10), Inequality (B.11), and Inequality (B.12), along with Lemma B.5, Corollary B.3, and Lemma 3.3, we have that overall bound is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta\\cdot(S_{g}M_{y}^{2}+C_{g}C_{y}+C_{\\ell})\\leq O(\\delta\\cdot(S_{g}\\cdot C_{H}^{3}\\cdot(C_{g}+\\|A\\|)^{2}\\cdot(C_{g}+C_{f}+L_{f}))).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Cost of linear equality constrained bilevel program ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y:A x_{t}+B y=b}g(x_{t},y)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "4: Run Algorithm 6 to generate $\\tilde{\\delta}$ -accurate primal and dual solutions $(\\hat{y}_{\\delta}^{*},\\hat{\\lambda}_{\\delta}^{*})$ for ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y:A x_{t}+B y=b}g(x_{t},y)+\\delta f(x_{t},y)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "5: Compute $\\begin{array}{r}{\\hat{v}_{t}:=\\frac{\\nabla_{x}[g(x_{t},\\hat{y}_{\\delta}^{*})+\\hat{\\lambda}_{\\delta}^{*}h(x,\\hat{y}^{*})]-\\nabla_{x}[g(x_{t},\\hat{y}^{*})+\\hat{\\lambda}^{*}h(x,\\hat{y}^{*})]}{\\delta},}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\nabla}F(x_{t}):=\\hat{v}^{t}+\\nabla_{x}f(x,\\hat{y}^{*}(x)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "6: Set $\\begin{array}{r}{x_{t+1}\\gets\\arg\\operatorname*{min}_{z\\in\\mathcal{X}}\\|z-(x_{t}-\\frac{1}{C_{F}}\\widetilde{\\nabla}F(x_{t}))\\|^{2}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 3.1. Consider Problem 3.1 under Assumption 2.2, and let $\\kappa=C_{g}/\\mu_{g}$ be the condition number of $g_{\\cdot}$ . Then Algorithm 5 finds an $\\epsilon$ -stationary point (in terms of gradient mapping, see $(B.I4),$ ) after $T\\,=\\,\\widetilde{\\cal O}(C_{F}(F(x_{0})\\,-\\,\\operatorname{inf}\\,F)\\sqrt{\\kappa}\\epsilon^{-2})$ oracle calls to $f$ and $g$ , where $C_{F}\\,:=\\,2(L_{f}+C_{f}\\,+$ $C_{g})C_{H}^{3}S_{g}(L_{g}+\\|A\\|)^{2}$ is the smoothness constant of the hyperobjective $F$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We first show the inexact gradient $\\widetilde{\\nabla}\\boldsymbol{F}(\\boldsymbol{x}_{t})$ generated in Algorithm 5 is an $\\delta$ -accurate approximation to the hyper-gradient $\\nabla F(x_{t})$ . Co nsider the inexact gradient defined in (3.2) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|v_{t}-\\hat{v}_{t}\\|\\leq\\displaystyle\\frac{1}{\\delta}\\{\\|[\\nabla_{x}g(x_{t},\\hat{y}_{\\delta}^{*})-\\nabla_{x}[g(x_{t},\\hat{y}^{*})]-[\\nabla_{x}g(x_{t},y_{\\delta}^{*})-\\nabla_{x}[g(x_{t},y^{*})]}\\\\ &{\\phantom{\\|}+\\|\\hat{\\lambda}_{\\delta}^{*}-\\hat{\\lambda}^{*}-[\\lambda_{\\delta}^{*}-\\lambda^{*}\\|\\|A\\|\\}}\\\\ &{\\phantom{\\|}\\leq\\displaystyle\\frac{2}{\\delta}[C_{g}+\\|A\\|]\\tilde{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\nabla}F(x_{t})-\\nabla F(x_{t})\\|\\leq\\|\\nabla_{x}f(x_{t},y^{*})-\\nabla_{x}f(x_{t},\\hat{y}^{*})\\|+\\left\\|\\hat{v}^{t}-v^{t}\\right\\|+\\|v^{t}-\\frac{d y^{*}(x^{t})}{d x}\\nabla_{y}f(x_{t},y^{*}(x_{t},y^{t}))\\right\\|}\\\\ &{\\leq C_{f}\\tilde{\\delta}+\\frac{2}{\\delta}[C_{g}+\\|A\\|]\\tilde{\\delta}+C_{F}\\delta}\\\\ &{\\leq\\frac{2\\tilde{\\delta}}{\\delta}[C_{f}+C_{g}+\\|A\\|]+C_{F}\\delta}\\\\ &{\\leq\\frac{\\epsilon^{2}}{4C_{F}R_{X}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applied to the $C_{F}$ -smooth hyper-objective $F$ , such an inexact gradient oracle satisfies the requirement for Proposition B.6. Thus an $\\epsilon$ -stationary point with $\\|\\mathcal{G}_{F}(x^{t})\\|\\leq\\epsilon$ (see Eq. (B.14) for the definition of gradient mapping) must be found in $\\begin{array}{r}{N=O(\\frac{C_{F}(F(x^{0})-F^{*})}{\\epsilon^{2}})}\\end{array}$ iterations. Noting the evaluation of inexact solutions $(\\hat{y}^{*},\\hat{\\lambda}^{*},\\hat{y}_{\\delta}^{*},\\hat{\\lambda}_{\\delta}^{*})$ requires $\\tilde{O}(\\sqrt{C_{g}/\\mu_{g}})$ first order oracle evaluations, we arrive at the total oracle complexity of $\\begin{array}{r}{\\tilde{O}(\\sqrt{C_{g}/\\mu_{g}}\\frac{C_{F}(F(x^{0})-F^{*})}{\\epsilon^{2}})}\\end{array}$ \u00b5gCF (F (\u03f5x20)\u2212F \u2217)) for finding an \u03f5-stationary point. ", "page_idx": 20}, {"type": "text", "text": "B.3 The cost of inexact projected gradient descent method ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this subsection, we state the number of iterations required by projected gradient descent method to find an $\\epsilon$ -stationary point using inexact gradient oracles. Specifically, we consider the following non-convex smooth problem where the objective $F$ is assumed to be $C_{F}$ -Lipschitz smooth: ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathrm{minimize}}_{x\\in{\\mathcal{X}}}F(x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the feasible region $\\mathcal{X}$ is compact, we use the norm of the following gradient mapping ${\\mathcal{G}}_{F}(x)$ as the stationarity criterion ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal G_{F}(x):=C_{F}(x-x^{+})~\\mathrm{where}~x^{+}=\\arg\\operatorname*{min}_{z\\in\\mathcal{X}}\\left\\|z-\\left(x-\\frac{1}{C_{F}}\\nabla F(x)\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Initialized to some $x_{0}$ and the inexact gradient oracle $\\widetilde{\\nabla}F$ , the updates of the inexact projected gradient descent method is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{Set}x_{t}\\gets\\arg\\operatorname*{min}_{z\\in\\mathcal{X}}\\left\\|z-\\left(x_{t-1}-\\frac{1}{C_{F}}\\widetilde{\\nabla}F(x_{t-1})\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The next proposition calculates the complexity result. ", "page_idx": 20}, {"type": "text", "text": "Proposition B.6. Consider the constrained optimization problem in (B.13) with $F$ being $C_{F}$ -Lipschitz smooth and $\\mathcal{X}$ having a radius of $R$ . When supplied with a $\\delta=\\epsilon^{2}/4C_{F}R$ -inexact gradient oracle $\\widetilde\\nabla F$ , that is, $\\|\\nabla F(x)-\\widetilde\\nabla F(x)\\|\\,\\le\\,\\delta$ , the solution generated by the projected gradient descent method (B.15) satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[N]}\\|\\mathcal{G}_{F}(x_{t})\\|^{2}\\leq\\frac{C_{F}(F(x_{0})-F^{*})}{N}+\\delta C_{F}R,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that is, it takes at most $\\scriptstyle O\\left({\\frac{C_{F}(F(x^{0})-F^{*})}{\\epsilon^{2}}}\\right)$ iterations to generate some $\\textstyle{\\bar{x}}$ with $\\|\\mathcal{G}_{F}(x)\\|\\leq\\epsilon$ ", "page_idx": 20}, {"type": "text", "text": "Proof. By $C_{F}$ -smoothness of $F$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{^{\\mathrm{\\Deltat}}(x_{t+1})=f(x_{t}-\\displaystyle\\frac{1}{C_{F}}\\widetilde{\\mathcal{G}_{F}}(x_{t}))\\leq f(x_{t})-\\displaystyle\\frac{1}{C_{F}}\\widetilde{\\mathcal{G}_{F}}(x_{t})^{\\top}\\nabla f(x_{t})+\\displaystyle\\frac{1}{2C_{F}}\\|\\widetilde{\\mathcal{G}_{F}}(x_{t})\\|^{2}}\\\\ {\\quad\\qquad\\qquad\\qquad\\qquad=f(x_{t})-\\displaystyle\\frac{1}{2C_{F}}\\|\\widetilde{\\mathcal{G}_{F}}(x_{t})(x_{t})\\|^{2}+\\displaystyle\\frac{1}{C_{F}}\\widetilde{\\mathcal{G}_{F}}(x_{t})^{\\top}(\\widetilde{\\mathcal{G}_{F}}(x_{t})-\\nabla f(x_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now show that $\\begin{array}{r}{\\frac{1}{\\beta}\\widetilde{\\mathcal{G}_{F}}(x_{t})^{\\top}(\\widetilde{\\mathcal{G}_{F}}(x_{t})-\\nabla f(x_{t}))\\,\\leq\\,0.}\\end{array}$ . Let $\\begin{array}{r}{\\widetilde{y}_{t}\\,=\\,x_{t}\\,-\\,\\frac{1}{C_{F}}\\widetilde{\\nabla}F(x_{t})}\\end{array}$ , and let $y_{t}=$ xt \u2212C1 \u2207f(xt). The n have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{C_{F}}\\widetilde{\\mathcal{G}_{F}}(x_{t})^{\\top}(\\frac{1}{C_{F}}\\widetilde{\\mathcal{G}_{F}}(x_{t})-\\nabla f(x_{t}))=C_{F}(x_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))^{\\top}(y_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=C_{F}(x_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))^{\\top}(\\widetilde{y}_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\;C_{F}(x_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))^{\\top}(y_{t}-\\widetilde{y}_{t})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq C_{F}(x_{t}-\\mathrm{proj}_{\\chi}(\\widetilde{y}_{t}))^{\\top}(y_{t}-\\widetilde{y}_{t})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\delta C_{F}R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the penultimate inequality uses the fact that $\\mathcal{X}$ is a convex set, and $R$ is the diameter of the set $X$ . Combining this with Inequality (B.16), we have that the function decrease per iteration is ", "page_idx": 21}, {"type": "equation", "text": "$$\nF(x_{t+1})\\leq F(x_{t})-\\frac{1}{2C_{F}}\\|\\widetilde{\\mathcal{G}_{F}}(x_{t})\\|^{2}+\\delta C_{F}R.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing over $N$ iterations telescopes the terms, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[N]}\\|\\widetilde{\\mathcal{G}_{F}}(x_{t})\\|^{2}\\leq\\frac{1}{N}C_{F}(F(x^{0})-F^{*})+\\delta C_{F}R.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substituting in $\\begin{array}{r}{N=\\frac{4}{\\epsilon^{2}}C_{F}(F(x^{0})-F^{*})}\\end{array}$ and the choice of $\\delta=\\epsilon^{2}/4C_{F}R$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[N]}\\|\\widetilde{\\mathcal{G}_{F}}(x_{t})\\|^{2}\\leq\\frac{\\epsilon^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking into account the fact that $||\\widetilde{\\mathcal{G}_{F}}(x_{t})-\\mathcal{G}_{F}(x_{t})||\\leq||\\nabla F(x^{t})-\\widetilde{\\nabla}F(x^{t})||\\leq\\delta$ , we obtain the desired result. ", "page_idx": 21}, {"type": "text", "text": "B.4 The cost of generating approximate solutions to the linearly constrained LL problem ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we address the issue of generating approximations to the primal and dual solutions $(y^{*},\\lambda^{*})$ associated with the lower-level problem in Problem 3.1. These approximations are required for computing the approximate hypergradient in Algorithm 1. For notational simplicity, we are going to consider the following constrained strongly convex problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{minimize}_{y\\in\\mathbb{R}^{d}}\\quad g(y)}\\\\ &{\\mathrm{subject~to~}}&{B y=b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We propose the following simple scheme to generate approximate solutions to Problem B.17. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\Big|\\mathrm{Compute~a~feasible~}\\hat{y}\\mathrm{~such~that~}\\lVert\\hat{y}-y^{*}\\rVert\\leq\\delta.\\mathrm{~Then~solve~}}&\\\\ &{\\Big|\\hat{\\lambda}=\\arg\\displaystyle\\operatorname*{min}_{\\lambda\\in\\mathbb{R}^{m}}\\,\\lVert\\nabla_{y}g(\\hat{y})-B^{\\top}\\lambda\\rVert^{2}.}&{(\\mathrm{B}.18)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The following lemma tells us that $\\hat{\\lambda}$ is close to $\\lambda^{*}$ if $B$ has full row rank. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.7. Suppose $g$ in Problem $B.I7$ is a $C_{g}$ -Lipschitz smooth, and the matrix $B$ has full row rank such that the following matrix $M_{B}$ is invertible ", "page_idx": 21}, {"type": "equation", "text": "$$\nM_{B}=\\left[{\\cal{I}}_{\\!\\!B}\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!-B\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!-B\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\slash\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the approximate solution $(\\hat{\\lambda},\\hat{y})$ from (B.18) satisfies $\\lVert\\hat{\\lambda}-\\lambda^{*}\\rVert\\leq\\lVert M_{B}^{-1}\\rVert(1+C_{g})\\delta.$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since $(\\lambda^{*},y^{*})$ satisfy the KKT conditions, they are the solution to the following linear system ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underbrace{\\left[I^{\\left.\\begin{array}{l l}{B^{\\top}}\\end{array}\\right]}_{=M_{B}}\\left[\\!\\!\\begin{array}{l}{y^{*}}\\\\ {\\lambda^{*}\\right]}\\end{array}\\right]}_{=M_{B}}=\\left[\\!\\!\\begin{array}{c}{-\\nabla_{y}g(y^{*})+I y^{*}}\\\\ {b}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{{y^{\\ast}}}\\\\ {{\\lambda^{\\ast}}}\\end{array}\\!\\!\\right]=M_{B}^{-1}\\left[\\!\\!\\begin{array}{c}{{-\\nabla_{y}g(y^{\\ast})+I y^{\\ast}}}\\\\ {{b}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, the approximate solutions $(\\hat{y},\\hat{\\lambda})$ in (B.18) satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\!\\!\\begin{array}{c c}{I}&{B^{\\top}}\\\\ {B}&{0}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c}{\\hat{y}}\\\\ {\\hat{\\lambda}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c}{B^{\\top}\\hat{\\lambda}+I\\hat{y}}\\\\ {b}\\end{array}\\!\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We show the right hand side (r.h.s) of the above equation to be close to the r.h.s of (B.19). Let $S:=\\{B^{\\top}\\lambda:\\lambda\\in\\mathbb{R}^{m}\\}$ denote the subspace spanned by the rows of $B$ . We can rewrite $B^{\\top}\\hat{\\lambda}$ as the projection of $\\nabla g(\\hat{y})$ onto $S$ , that is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{B^{\\top}\\hat{\\lambda}=\\arg\\underset{s\\in S}{\\operatorname*{min}}\\left\\|\\nabla_{y}g(\\hat{y})-s\\right\\|^{2}}\\\\ {-\\nabla_{y}g(y^{*})=B^{\\top}\\lambda^{*}=\\arg\\underset{s\\in S}{\\operatorname*{min}}\\left\\|\\nabla_{y}g(y^{*})-s\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second relation follows from the KKT conditon associated with $(\\lambda^{*},y^{*})$ . Since the projection is an non-expansive operation, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|B^{\\top}\\hat{\\boldsymbol{\\lambda}}-(-\\nabla_{y}g(y^{*}))\\|=\\|B^{\\top}\\hat{\\boldsymbol{\\lambda}}-B^{\\top}\\boldsymbol{\\lambda}^{*}\\|\\leq\\|\\nabla_{y}g(\\hat{y})-\\nabla g(y^{*})\\|\\leq C_{g}\\|\\hat{y}-y^{*}\\|\\leq C_{g}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can rewrite $(\\hat{y},\\hat{\\lambda})$ as solutions to the following linear system with some $\\|\\tau\\|\\leq(1+C_{g})\\delta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{{\\hat{y}}}\\\\ {{\\hat{\\lambda}}}\\end{array}\\!\\!\\right]=M_{B}^{-1}\\left[\\!\\!\\begin{array}{c}{{-\\nabla_{y}g(y^{*})+I y^{*}+\\tau}}\\\\ {{b}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\left[\\displaylimits_{\\hat{\\lambda}}^{\\hat{y}}\\right]-\\left[\\displaylimits_{\\lambda^{*}}^{y^{*}}\\right]\\|=\\|M_{B}^{-1}\\|\\|\\left[\\displaylimits_{0}^{\\tau}\\right]\\leq\\|M_{B}^{-1}\\|(1+C_{g})\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we can just use the AGD method to generate a close enough approximate solution $\\hat{y}$ and call up the Subroutine in (B.18) to generate the approximate dual solution \u03bb\u02c6. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 6 The Projected Gradient Method to Generate Primal and Dual Solutions for a Linearly Constrained Problem ", "page_idx": 22}, {"type": "text", "text": "1: Input: accuracy requirement $\\epsilon>0$ and linearly constrained problem $\\scriptstyle\\operatorname*{min}_{y:B y=b}g(y)$ .   \n2: Starting from $y^{0}=0$ and using $Y:=\\left\\{y\\in\\mathbb{R}^{d}:B y=b\\right\\}$ as the simple feasible region.   \n3: Run the Accelerated Gradient Descent (AGD) Method (Section 3.3 in [91]) for $\\textit{N}=$   \n$\\begin{array}{r}{\\lceil4\\sqrt{C_{g}/\\mu_{g}}\\log\\frac{\\|y^{*}\\|\\|M_{B}^{-1}\\|(C_{g}+1)}{\\mu_{g}\\epsilon}\\rceil}\\end{array}$ iterations.   \n4: Use the $y^{N}$ as the approximate solution $\\hat{y}$ to generate $\\hat{\\lambda}$ according to (B.18).   \n5: return $(\\hat{y},\\hat{\\lambda})$ ", "page_idx": 22}, {"type": "text", "text": "Proposition B.8. Suppose the objective function $g$ is both $L_{g}$ -smooth and $\\mu_{g}$ -strongly convex, and that the constraint satisfies the assumption in Lemma B.7. Fix an $\\epsilon>0$ , the solution $(\\hat{y},\\hat{\\lambda})$ returned by the above procedure satisfies $\\|y^{*}-\\hat{y}\\|\\leq\\epsilon$ and $\\lVert\\hat{\\lambda}-\\lambda^{*}\\rVert\\leq\\epsilon$ . In another words, the cost of generating $\\epsilon$ -close primal and dual solutions are bounded by $\\begin{array}{r}{O(\\sqrt{\\frac{C_{g}}{\\mu_{g}}}\\log\\frac{1}{\\epsilon})}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. With $\\begin{array}{r}{N:=\\lceil4\\sqrt{C_{g}/\\mu_{g}}\\log\\frac{\\|y^{\\ast}\\|\\|M_{B}^{-1}\\|(L_{g}+1)}{\\mu_{g}\\epsilon}\\rceil}\\end{array}$ , Theorem 3.7 in [91] shows that $\\lVert y^{N}-\\hat{y}\\rVert\\leq$ $\\epsilon/\\|M_{B}^{-1}\\|(1+L_{g})$ . Then we can apply Lemma B.7 to obtain the desired bound. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C Proofs for Section 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our algorithms are based on the Lipschitzness of $F$ , which we prove below. ", "page_idx": 23}, {"type": "text", "text": "Lemma 4.3. Under Assumption 2.2 and 2.5, $F$ in Problem 4.1 is $O(L_{f}L_{y})$ -Lipschitz in $x$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemma 2.1 of [16], the hypergradient of $F$ computed with respect to the variable $x$ may be expressed as $\\begin{array}{r}{\\nabla_{x}F(x)=\\nabla_{x}f(x,y^{*}(x))+\\left(\\frac{d y^{*}(x)}{d x}\\right)^{\\top}\\cdot\\nabla_{y}f(x,y^{*}(x))}\\end{array}$ . Since we impose Lipschitzness on $f$ and $y^{\\ast}$ , we can bound each of the terms of $\\nabla_{x}F(x)$ by the claimed bound. ", "page_idx": 23}, {"type": "text", "text": "C.1 Faster algorithm for low upper-level dimensions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we analyze Algorithm 2, which as stated in Section 4, requires evaluating only the hyperobjective $F$ (as opposed to estimating the hypergradient in Algorithm 3). ", "page_idx": 23}, {"type": "text", "text": "The motivation for designing such an algorithm, is that while evaluating $\\nabla F$ up to $\\alpha$ accuracy requires $O(\\alpha^{-1})$ gradient evaluations, the hyperobjective value can be estimated at a linear rate: ", "page_idx": 23}, {"type": "text", "text": "Lemma 4.2 (Proof in Appendix C.1). Given any $x$ , we can return $\\widetilde{F}(\\boldsymbol{x})$ such that $|F(x)-\\widetilde{F}(x)|\\le\\alpha$ using $O(\\sqrt{C_{g}/\\mu_{g}}\\log(L_{f}/\\alpha))$ first-order oracle calls to $f$ and $g$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 4.2. We note that it suffices to find $\\tilde{y}^{\\ast}$ such that $\\|\\tilde{y}^{*}-y^{*}(x)\\|\\,\\leq\\,\\alpha/L_{f}$ , since setting $\\widetilde{F}(x):=f(x,\\widetilde{y}^{*})$ will then satisfy $\\begin{array}{r}{|\\widetilde{F}(x)-F(x)|=|f(x,\\widetilde{y}^{*})-f(x,y^{*}(x))|\\le L_{f}\\cdot\\frac{\\alpha}{L_{f}}=\\alpha}\\end{array}$ by Lispchitzness of $f$ , as required. Noting that $y^{*}(x)\\,=\\,\\arg\\operatorname*{min}_{h(x,y)\\le0}{g(x,y)}$ is the solution to a constrained smooth, strongly-convex problem with condition number $C_{g}/\\mu_{g}$ , it is possible to approximate it up to $\\alpha/L_{f}$ with $O(\\sqrt{C_{g}/\\mu_{g}}\\log(L_{f}/\\alpha))$ first-order oracle calls using the result of Zhang and Lan [45]. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Accordingly, we consider Algorithm 2, which is a zero-order variant of Algorithm 3, whose guarantee is summarized is the theorem below. ", "page_idx": 23}, {"type": "text", "text": "Theorem C.1. Suppose $F:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is $L$ -Lipschitz, and that $|\\widetilde{F}(\\cdot)\\,-\\,F(\\cdot)|\\,\\le\\,\\alpha.$ . Then running Algorithm 3 with $\\begin{array}{r}{\\rho\\;=\\;\\operatorname*{min}\\left\\{\\frac{\\delta}{2},\\frac{F(x_{0})-\\operatorname*{inf}F}{L}\\right\\},\\nu\\;=\\;\\delta-\\rho,\\ D\\;=\\;\\Theta\\left(\\frac{\\nu\\epsilon^{2}\\rho^{2}}{d\\rho^{2}L^{2}+\\alpha^{2}d^{2}}\\right),\\eta\\;=\\;}\\end{array}$ $\\begin{array}{r}{\\Theta\\left({\\frac{\\nu\\epsilon^{3}\\rho^{4}}{(d\\rho^{2}L^{2}+\\alpha^{2}d^{2})^{2}}}\\right)}\\end{array}$ , outputs a point $x^{\\mathrm{out}}$ such that $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{out}}))]\\le\\epsilon+\\alpha$ with ", "page_idx": 23}, {"type": "equation", "text": "$$\nT=O\\left(\\frac{d(F(x_{0})-\\operatorname*{inf}{F})}{\\delta\\epsilon^{3}}\\cdot\\left(L^{2}+\\alpha^{2}(\\frac{d}{\\delta^{2}}+\\frac{d L^{2}}{(F(x_{0})-\\operatorname*{inf}{F})^{2}})\\right)\\right)\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the result of Theorem C.1 with the complexity of hyperobjective estimation, as given by Lemma 4.2, we obtain convergence to a $(\\delta,\\epsilon)$ -stationary point of Problem 4.1 with $\\widetilde{\\cal O}(d_{x}\\delta^{-1}\\epsilon^{-3})$ gradient calls overall. ", "page_idx": 23}, {"type": "text", "text": "C.1.1 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Denoting the uniform randomized smoothing $F_{\\rho}(x):=\\mathbb{E}_{\\|z\\|\\leq1}[F(x+\\rho\\cdot z)]$ where the expectation, here and in what follows, is taken with respect to the uniform measure, it is well known [92, Lemma 10] that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\|w\\|=1}\\left[\\frac{d}{2\\rho}(F(x+\\rho w)-F(x-\\rho w))w\\right]=\\nabla F_{\\rho}(x)\\,,\\quad}\\\\ {\\mathbb{E}_{\\|w\\|=1}\\left\\|\\nabla F_{\\rho}(x)-\\frac{d}{2\\rho}(F(x+\\rho w)-F(x-\\rho w))w\\right\\|^{2}\\lesssim d L^{2}\\,.\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We first show that replacing the gradient estimator with the inexact evaluations $\\widetilde{F}(\\cdot)$ leads to a biased gradient estimator of $F$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma C.2. Suppose $|F(\\cdot)-\\widetilde{F}(\\cdot)|\\le\\alpha$ . Denoting ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{x}=\\frac{d}{2\\rho}\\big(F(x+\\rho w)-F(x-\\rho w)\\big)w\\;,}\\\\ {\\widetilde{g}_{x}=\\frac{d}{2\\rho}(\\widetilde{F}(x+\\rho w)-\\widetilde{F}(x-\\rho w))w\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\|w\\|=1}\\left\\|g_{x}-\\widetilde{g}_{x}\\right\\|\\le\\frac{\\alpha d}{\\rho}\\,,\\quad a n d\\quad\\mathbb{E}_{\\|w\\|=1}\\left\\|\\widetilde{g}_{x}\\right\\|^{2}\\lesssim\\frac{\\alpha^{2}d^{2}}{\\rho^{2}}+d L^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For the first bound, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\|w\\|=1}\\left\\|g_{x}-\\widetilde{g}_{x}\\right\\|\\le\\frac{d}{2\\rho}(2\\alpha)\\mathbb{E}_{\\|w\\|=1}\\left\\|w\\right\\|=\\frac{\\alpha d}{\\rho}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "while for the second bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{L}_{\\|w\\|=1}\\left\\|\\widetilde{g}_{x}\\right\\|^{2}=\\mathbb{E}_{\\|w\\|=1}\\left\\|\\widetilde{g}_{x}-g_{x}+g_{x}\\right\\|^{2}\\leq2\\mathbb{E}_{\\|w\\|=1}\\left\\|\\widetilde{g}_{x}-g_{x}\\right\\|^{2}\\!+\\!2\\mathbb{E}_{\\|w\\|=1}\\left\\|g_{x}\\right\\|^{2}\\lesssim\\frac{d^{2}}{\\rho^{2}}\\!\\cdot\\!\\alpha^{2}\\!+\\!d L^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last step invoked (C.1). ", "page_idx": 24}, {"type": "text", "text": "We are now ready to analyze Algorithm 2. We denote $\\alpha^{\\prime}\\,=\\,{\\frac{\\alpha d}{\\rho}}$ , $\\begin{array}{r}{\\widetilde{G}\\,=\\,\\sqrt{\\frac{\\alpha^{2}d^{2}}{\\rho^{2}}+d L^{2}}}\\end{array}$ . Since $x_{t}=x_{t-1}+\\Delta_{t}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{F_{\\rho}(x_{t})-F_{\\rho}(x_{t-1})=\\int_{0}^{1}\\left\\langle\\nabla F_{\\rho}(x_{t-1}+s\\Delta_{t}),\\Delta_{t}\\right\\rangle d s}\\quad}&{}\\\\ &{=\\mathbb{E}_{s_{t}\\sim\\mathrm{Unif}[0,1]}\\left[\\nabla F_{\\rho}(x_{t-1}+s_{t}\\Delta_{t}),\\Delta_{t}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\langle\\nabla F_{\\rho}(z_{t}),\\Delta_{t}\\right\\rangle\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By summing over $t\\in[T]=[K\\times M]$ , we get for any fixed sequence $u_{1},\\ldots,u_{K}\\in\\mathbb{R}^{d}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{inf}\\displaystyle F_{\\rho}\\leq F_{\\rho}(x_{T})\\leq F_{\\rho}(x_{0})+\\displaystyle\\sum_{k=1}^{T}\\mathbb{E}\\left[\\langle\\nabla F_{\\rho}(z_{t}),\\Delta_{t}\\rangle\\right]}&{}\\\\ {=F_{\\rho}(x_{0})+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F_{\\rho}(z_{(k-1)M+m}),\\Delta_{(k-1)M+m}-u_{k}\\rangle\\right]}&{}\\\\ {+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F_{\\rho}(z_{(k-1)M+m}),u_{k}\\rangle\\right]}&{}\\\\ {\\leq F_{\\rho}(x_{0})+\\displaystyle\\sum_{k=1}^{K}\\mathbb{K}\\mathrm{eg}_{M}(u_{k})+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F_{\\rho}(z_{(k-1)M+m}),u_{k}\\rangle\\right]}&{}\\\\ {\\leq F_{\\rho}(x_{0})+K D\\tilde{G}\\sqrt{M}+K\\alpha^{\\prime}D M+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F_{\\rho}(z_{(k-1)M+m}),u_{k}\\rangle\\right]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows by combining Lemma C.2 and Lemma C.3. By setting $u_{k}:=$ \u2225  mmM==11  \u2207\u2207FF\u03c1\u03c1((zz((kk\u2212\u221211))MM++mm))\u2225, rearranging and dividing by DT = DKM we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\nabla F_{\\rho}(z_{(k-1)M+m})\\right\\|\\leq\\frac{F_{\\rho}(x_{0})-\\operatorname*{inf}F_{\\rho}}{D T}+\\frac{\\displaystyle\\frac{K}{\\sqrt{M}}+\\alpha^{\\prime}}{\\sqrt{M}}}&{}\\\\ {\\displaystyle=\\frac{F_{\\rho}(x_{0})-\\operatorname*{inf}F_{\\rho}}{K\\nu}+\\frac{\\displaystyle\\sqrt{\\frac{\\alpha^{2}d^{2}}{\\rho^{2}}+L^{2}d}}{\\sqrt{M}}+\\frac{\\alpha d}{\\rho}}&{}\\\\ {\\displaystyle\\leq\\frac{F_{\\rho}(x_{0})-\\operatorname*{inf}F_{\\rho}}{K\\nu}+\\frac{\\frac{\\displaystyle\\frac{\\alpha d}{\\rho}}{\\sqrt{M}}}{\\sqrt{M}}+\\frac{L\\sqrt{d}}{\\sqrt{M}}+\\frac{\\alpha d}{\\rho}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, note that for all $m\\in[M]:\\left\\|z_{(k-1)M+m}-\\overline{{x}}_{k}\\right\\|\\leq M D\\leq\\nu.$ , therefore $\\nabla F_{\\rho}\\big(z_{(k-1)M+m}\\big)\\in$ $\\partial_{\\nu}F_{\\rho}({\\overline{{x}}}_{k})\\subset\\partial_{\\delta}F({\\overline{{x}}}_{k})$ , where the last containment is due to [46, Lemma 4] by using our assignment $\\rho+\\nu=\\delta$ . Invoking the convexity of the Goldstein subdifferential, this implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\nabla F_{\\rho}(z_{(k-1)M+m})\\in\\partial_{\\delta}F(\\overline{{x}}_{k})\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus it suffices to bound the first three summands in (C.2) by $\\epsilon$ in order to finish the proof. This happens as long as $\\begin{array}{r}{\\frac{F_{\\rho}(x_{0})-\\operatorname{inf}F_{\\rho}}{K\\nu}\\,\\leq\\,\\frac{\\epsilon}{3}}\\end{array}$ , $\\begin{array}{r}{\\frac{\\frac{\\alpha d}{\\rho}}{\\sqrt{M}}\\,\\leq\\,\\frac{\\epsilon}{3}}\\end{array}$ , and $\\begin{array}{r}{\\frac{L\\sqrt{d}}{\\sqrt{M}}\\leq\\frac{\\epsilon}{3}}\\end{array}$ d \u2264 \u03f5, which imply K \u2273 F\u03c1(x0)\u2212inf F\u03c1, $\\begin{array}{r}{M\\gtrsim\\frac{\\alpha^{2}d^{2}}{\\rho^{2}\\epsilon^{2}}}\\end{array}$ , and $\\begin{array}{r}{M\\gtrsim\\frac{L^{2}d}{\\epsilon^{2}}}\\end{array}$ . By our assignments of $\\rho$ and $\\nu$ , these result in ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=K M=O\\left(\\frac{F_{\\rho}(x_{0})-\\operatorname*{inf}F_{\\rho}}{\\nu\\epsilon}\\cdot\\left(\\frac{\\alpha^{2}d^{2}}{\\rho^{2}\\epsilon^{2}}+\\frac{L^{2}d}{\\epsilon^{2}}\\right)\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\frac{(F(x_{0})-\\operatorname*{inf}F)d}{\\delta\\epsilon^{3}}\\cdot\\left(\\frac{\\alpha^{2}d}{\\rho^{2}}+L^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\frac{(F(x_{0})-\\operatorname*{inf}F)d}{\\delta\\epsilon^{3}}\\cdot\\left(\\alpha^{2}d\\cdot\\operatorname*{max}\\left\\{\\frac{1}{\\delta^{2}},\\frac{L^{2}}{(F(x_{0})-\\operatorname*{inf}F)^{2}}\\right\\}+L^{2}\\right)\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "completing the proof. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We recall Theorem 4.4 below to keep this section self-contained. ", "page_idx": 25}, {"type": "text", "text": "Theorem 4.4. Suppose $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $L$ -Lipschitz and that $\\|\\widetilde{\\nabla}F(\\cdot)-\\nabla F(\\cdot)\\|\\le\\alpha$ . Then running Algorithm 3 with $\\begin{array}{r}{D=\\Theta(\\frac{\\delta\\epsilon^{2}}{L^{2}}),\\eta=\\Theta(\\frac{\\delta\\epsilon^{3}}{L^{4}})}\\end{array}$ , outputs a point $x^{\\mathrm{out}}$ such that $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{out}}))]\\le$ $\\epsilon+\\alpha$ , with $\\begin{array}{r}{T=O\\left(\\frac{(F(x_{0})-\\operatorname{inf}F)L^{2}}{\\delta\\epsilon^{3}}\\right)}\\end{array}$ calls to $\\widetilde{\\nabla}F(\\cdot)$ . ", "page_idx": 25}, {"type": "text", "text": "Our analysis is inspired by the reduction from online learning to nonconvex optimization given by [47]. To that end, we start by proving a seemingly unrelated result, asserting that online gradient descent minimizes the regret with respect to inexact evaluations. Recalling standard definitions from online learning, given a sequence of linear losses $\\ell_{m}(\\cdot)\\,=\\,\\langle g_{m},\\cdot\\rangle$ , if an algorithm chooses $\\Delta_{1},\\ldots,\\Delta_{M}$ we denote the regret with respect to $u$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathop{\\mathrm{Reg}}_{M}(u):=\\sum_{m=1}^{M}\\left\\langle g_{m},\\Delta_{m}-u\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider an update rule according to online projected inexact gradient descent: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta_{m+1}:=\\mathrm{clip}_{D}\\big(\\Delta_{m}-\\eta_{m}\\widetilde{g}_{m}\\big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma C.3 (Inexact Online Gradient Descent). In the setting above, suppose that $(\\widetilde{g}_{m})_{m=1}^{M}$ are possibly randomized vectors, such that $\\mathbb{E}\\left\\|\\widetilde{g}_{m}-g_{m}\\right\\|\\leq\\alpha$ and $\\mathbb{E}\\left\\|\\widetilde{g}_{m}\\right\\|^{2}\\leq\\widetilde{G}^{2}$ for all $m\\in[M]$ . Then for any $\\lVert u\\rVert\\leq D$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Reg}}_{M}(u)\\right]\\leq\\frac{D^{2}}{\\eta_{M}}+\\widetilde G^{2}\\sum_{m=1}^{M}\\eta_{m}+\\alpha D M\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the expectation is with respect to the (possible) randomness of $(\\widetilde{g}_{m})_{m=1}^{M}$ . In particular, setting $\\begin{array}{r}{\\eta_{m}\\equiv\\frac{D}{\\widetilde{G}\\sqrt{M}}}\\end{array}$ G\u221aM yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Reg}_{M}(u)\\right]\\lesssim D\\widetilde{G}\\sqrt{M}+\\alpha D M\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. For any $m\\in[M]$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Delta_{m+1}-u\\right\\|^{2}=\\left\\|\\mathrm{clip}_{D}\\big(\\Delta_{m}-\\eta_{m}\\widetilde{g}_{m}\\big)-u\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\Delta_{m}-\\eta_{m}\\widetilde{g}_{m}-u\\right\\|^{2}=\\left\\|\\Delta_{m}-u\\right\\|^{2}+\\eta_{m}^{2}\\left\\|\\widetilde{g}_{m}\\right\\|^{2}-2\\eta_{m}\\left\\langle\\Delta_{m}-u,\\widetilde{g}_{m}\\right\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\langle\\widetilde{g}_{m},\\Delta_{m}-u\\right\\rangle\\leq\\frac{\\left\\Vert\\Delta_{m}-u\\right\\Vert^{2}-\\left\\Vert\\Delta_{m+1}-u\\right\\Vert^{2}}{2\\eta_{m}}+\\frac{\\eta_{m}}{2}\\left\\Vert\\widetilde{g}_{m}\\right\\Vert^{2}\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "from which we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\langle g_{m},\\Delta_{m}-u\\right\\rangle=\\mathbb{E}\\left\\langle\\tilde{g}_{m},\\Delta_{m}-u\\right\\rangle+\\mathbb{E}\\left\\langle g_{m}-\\tilde{g}_{m},\\Delta_{m}-u\\right\\rangle}\\\\ &{\\phantom{\\leq\\leq}\\leq\\frac{\\left\\|\\Delta_{m}-u\\right\\|^{2}-\\left\\|\\Delta_{m+1}-u\\right\\|^{2}}{2\\eta_{m}}+\\frac{\\eta_{m}}{2}\\mathbb{E}\\left\\|\\tilde{g}_{m}\\right\\|^{2}+\\mathbb{E}\\left\\|g_{m}-\\tilde{g}_{m}\\right\\|\\cdot\\left\\|\\Delta_{m}-u\\right\\|}\\\\ &{\\phantom{\\leq\\frac{}{2}\\frac{\\left\\|\\Delta_{m}-u\\right\\|^{2}-\\left\\|\\Delta_{m+1}-u\\right\\|^{2}}{2\\eta_{m}}+\\frac{\\eta_{m}}{2}\\tilde{G}^{2}+\\alpha D\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Summing over $m\\in[M]$ , we see that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[{\\mathrm{Reg}}_{M}(u)\\right]\\leq\\displaystyle\\sum_{m=1}^{M}\\|\\Delta_{m}-u\\|^{2}\\left(\\frac{1}{\\eta_{m}}-\\frac{1}{\\eta_{m-1}}\\right)+\\frac{\\widetilde G^{2}}{2}\\displaystyle\\sum_{m=1}^{M}\\eta_{m}+M\\alpha D}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{D^{2}}{\\eta_{M}}+\\widetilde G^{2}\\displaystyle\\sum_{m=1}^{M}\\eta_{m}+\\alpha D M\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The simplification for $\\begin{array}{r}{\\eta_{m}\\equiv\\frac{D}{\\widetilde{G}\\sqrt{M}}}\\end{array}$ readily follows. ", "page_idx": 26}, {"type": "text", "text": "We are now ready to analyze Algorithm 3 in the inexact gradient setting. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 4.4. Since Algorithm 3 has $x_{t}=x_{t-1}+\\Delta_{t}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{F(x_{t})-F(x_{t-1})=\\int_{0}^{1}\\left\\langle\\nabla F(x_{t-1}+s\\Delta_{t}),\\Delta_{t}\\right\\rangle d s}}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{s_{t}\\sim\\mathrm{Unif}[0,1]}\\left[\\left\\langle\\nabla F(x_{t-1}+s_{t}\\Delta_{t}),\\Delta_{t}\\right\\rangle\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\left\\langle\\nabla F(z_{t}),\\Delta_{t}\\right\\rangle\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By summing over $t\\in[T]=[K\\times M]$ , we get for any fixed sequence $u_{1},\\ldots,u_{K}\\in\\mathbb{R}^{d}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}F\\leq F(x_{T})\\leq F(x_{0})+\\displaystyle\\sum_{i=1}^{T}\\mathbb{E}\\left[\\langle\\nabla F(z_{i}),\\Delta_{t}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\qquad=F(x_{0})+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F(z_{(k-1)M+m}),\\Delta_{(k-1)M+m}-u_{k}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F(z_{(k-1)M+m}),u_{k}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\leq F(x_{0})+\\displaystyle\\sum_{k=1}^{K}\\mathrm{Reg}_{M}(u_{k})+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F(z_{(k-1)M+m}),u_{k}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\leq F(x_{0})+K D\\widetilde{G}\\sqrt{M}+K\\alpha D M+\\displaystyle\\sum_{k=1}^{K}\\sum_{m=1}^{M}\\mathbb{E}\\left[\\langle\\nabla F(z_{(k-1)M+m}),u_{k}\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows from Lemma C.3 for $\\widetilde{G}\\ =\\ \\sqrt{L^{2}+\\alpha^{2}}$ , $\\eta~=~{\\frac{D}{{\\widetilde{G}}{\\sqrt{M}}}}$ , since (deterministically) for all $t\\ \\ \\in\\ \\ [T]$ by assumption. Letting $u_{k}:=$ $\\begin{array}{r l}{\\lefteqn{-D\\frac{\\sum_{m=1}^{M}\\nabla F\\left(z_{\\left(k-1\\right)M+m}\\right)}{\\left\\|\\sum_{m=1}^{M}\\nabla F\\left(z_{\\left(k-1\\right)M+m}\\right)\\right\\|}}}\\end{array}$ , rearranging and dividing by $D T=D K M$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{E}\\left\\|\\frac{1}{M}\\sum_{m=1}^{M}\\nabla F(z_{(k-1)M+m})\\right\\|\\leq\\frac{F(x_{0})-\\operatorname*{inf}F}{D T}+\\frac{\\widetilde G}{\\sqrt{M}}+\\alpha}}\\\\ &{}&{=\\frac{F(x_{0})-\\operatorname*{inf}F}{K\\delta}+\\frac{\\widetilde G}{\\sqrt{M}}+\\alpha\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, note that for all $k\\ \\in\\ [K],m\\ \\in\\ [M]\\ :\\ \\left\\|z_{(k-1)M+m}-{\\overline{{x}}}_{k}\\right\\|\\ \\leq\\ M D\\ \\leq\\ \\delta,$ , therefore $\\nabla F(z_{(k-1)M+m})\\;\\in\\;\\partial_{\\delta}F(\\overline{{x}}_{k})$ . Invoking the convexity of the Goldstein subdifferential, we see that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}\\nabla F(z_{(k-1)M+m})\\in\\partial_{\\delta}F(\\overline{{x}}_{k})\\:,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "thus it suffices to bound the first two summands on the right-hand side in (C.3) by $\\epsilon$ in order to finish the proof. This happens as long as $\\begin{array}{r}{\\frac{F(x_{0})-\\operatorname{inf}F}{K\\delta}\\,\\le\\,\\frac{\\epsilon}{2}}\\end{array}$ and \u221aG M \u2264 \u03f52. These are equivalent to $\\begin{array}{r}{K\\ge\\frac{2(F(x_{0})-\\operatorname*{inf}F)}{\\delta\\epsilon}}\\end{array}$ and $\\begin{array}{r}{M\\ge\\frac{4\\widetilde{G}^{2}}{\\epsilon^{2}}}\\end{array}$ , which results in ", "page_idx": 26}, {"type": "equation", "text": "$$\nT=K M=O\\left(\\frac{F(x_{0})-\\operatorname*{inf}F}{\\delta\\epsilon}\\cdot\\frac{L^{2}+\\alpha^{2}}{\\epsilon^{2}}\\right)=O\\left(\\frac{(F(x_{0})-\\operatorname*{inf}F)L^{2}}{\\delta\\epsilon^{3}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C.3 An implementation-friendly algorithm and its analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Algorithm 7 Perturbed Inexact GD ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "eNCYpTCGhr/tmp/fe380f6cfca6f44e8af314d6b9196201cfedb22c16286539b56b2a783f50abec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Theorem C.4. Suppose $F:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is $L$ -Lipschitz, and that $\\|\\widetilde\\nabla F(\\cdot)-\\nabla F(\\cdot)\\|\\,\\le\\,\\alpha$ . Then running Algorithm 7 with \u03b7 = \u0398 ((F (Tx 10/)2\u2212Li1n/f 2Fd )1/+4\u03b4(L\u03b1)+1/L2)\u03b41/2 outputs a point $x^{\\mathrm{out}}$ such that $\\mathbb{E}[\\mathrm{dist}(0,\\partial_{\\delta}F(x^{\\mathrm{{out}}}))]\\le\\epsilon+\\sqrt{\\alpha L}$ , with ", "page_idx": 27}, {"type": "equation", "text": "$$\nT={\\cal O}\\left(\\frac{(F(x_{0})-\\operatorname*{inf}{F}+\\delta L)L^{3}\\sqrt{d}}{\\delta{\\epsilon}^{4}}\\right)\\,\\,c a l l s\\,t o\\,\\widetilde\\nabla F(\\cdot).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Throughout the proof we denote $z_{t}\\;=\\;x_{t}\\,+\\,\\delta\\,\\cdot\\,w_{t}$ . Since $F$ is $L$ -Lipschitz, $F_{\\delta}(x)\\;:=\\;$ $\\mathbb{E}_{w\\sim\\mathrm{Unif}(\\mathbb{S}^{d-1})}[F(x+\\delta\\cdot w)]$ is $L$ -Lipschitz and $O(L{\\sqrt{d}}/\\delta)$ -smooth. By smoothness we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\tau}}_{\\delta}(x_{t+1})-F_{\\delta}(x_{t})\\leq\\langle\\nabla F_{\\delta}(x_{t}),x_{t+1}-x_{t}\\rangle+O\\left(\\frac{L\\sqrt{d}}{\\delta}\\right)\\cdot\\|x_{t+1}-x_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\eta\\,\\langle\\nabla F_{\\delta}(x_{t}),\\widetilde{g}_{t}\\rangle+O\\left(\\frac{\\eta^{2}L\\sqrt{d}}{\\delta}\\right)\\cdot\\|\\widetilde{g}_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\eta\\,\\langle\\nabla F_{\\delta}(x_{t}),\\nabla F(z_{t})\\rangle-\\eta\\,\\langle\\nabla F_{\\delta}(x_{t}),\\widetilde{g}_{t}-\\nabla F(z_{t})\\rangle+O\\left(\\frac{\\eta^{2}L\\sqrt{d}}{\\delta}\\right)\\cdot\\|\\widetilde{g}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Noting that $\\mathbb{E}[\\nabla F(z_{t})]=\\nabla F_{\\delta}(x_{t})$ and that $\\begin{array}{r}{\\|\\widetilde{g}_{t}\\|\\leq\\|\\widetilde{g}_{t}-\\nabla F(z_{t})\\|+\\|\\nabla F(z_{t})\\|\\leq\\alpha+L}\\end{array}$ , we see that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{\\delta}(x_{t+1})-F_{\\delta}(x_{t})]\\le-\\eta\\mathbb{E}\\left\\|\\nabla F_{\\delta}(x_{t})\\right\\|^{2}+\\eta L\\alpha+O\\left(\\frac{\\eta^{2}L\\sqrt{d}}{\\delta}(\\alpha+L)^{2}\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\nabla F_{\\delta}(x_{t})\\right\\|^{2}\\leq\\frac{\\mathbb{E}[F_{\\delta}(x_{t})]-\\mathbb{E}[F_{\\delta}(x_{t+1})]}{\\eta}+L\\alpha+O\\left(\\frac{\\eta L\\sqrt{d}(\\alpha+L)^{2}}{\\delta}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Averaging over $t=0,\\dots,T-1$ and noting that $F_{\\delta}(x_{0})-\\operatorname*{inf}F_{\\delta}\\leq(F(x_{0})-\\operatorname*{inf}F)+\\delta L$ results in ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{\\updownarrow}\\left\\|\\nabla F_{\\delta}(x^{\\mathrm{out}})\\right\\|^{2}=\\frac{1}{T}\\sum_{t=0}^{T-1}E\\left\\|\\nabla F_{\\delta}(x_{t})\\right\\|^{2}\\leq\\frac{\\left(F(x_{0})-\\operatorname*{inf}F\\right)+\\delta L}{\\eta T}+L\\alpha+O\\left(\\frac{\\eta L\\sqrt{d}(\\alpha+L)^{2}}{\\delta}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Jensen\u2019s inequality and the sub-additivity of the square root, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\nabla F_{\\delta}(x^{\\mathrm{out}})\\right\\|\\leq\\sqrt{\\frac{(F(x_{0})-\\operatorname*{inf}F)+\\delta L}{\\eta T}}+\\sqrt{L\\alpha}+O\\left(\\sqrt{\\frac{\\eta L\\sqrt{d}(\\alpha+L)^{2}}{\\delta}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Setting \u03b7 = ((F\u221a (x0)\u221a\u2212inf F )+\u03b4L)\u03b4 yields the final bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|\\nabla F_{\\delta}(x^{\\mathrm{out}})\\right\\|\\lesssim\\frac{((F(x_{0})-\\operatorname*{inf}F)+\\delta L)^{1/4}L^{1/4}d^{1/8}(\\alpha+L)^{1/2}}{\\delta^{1/4}T^{1/4}}+\\sqrt{L\\alpha}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and the first summand is bounded by \u03f5 for T = O ((F (x0)\u2212inf F )\u03b4+\u03f54\u03b4L)L d(L+\u03b1)2 ", "page_idx": 28}, {"type": "text", "text": "D Reformulation equivalence ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem D.1 (Reformulation equivalence). When $\\lambda^{*}$ matches to an optimal dual solution to the lower level problem $y^{*}=\\arg\\operatorname*{min}_{y}g(x,y)$ s.t. $h(x,y)\\leq0$ , we show that for each $x$ , the reformulation has the same feasible region of $y$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We first show that lower-level feasibility implies feasibility of the reformulated problem. Let $y^{*},\\bar{\\lambda}^{*}=\\operatorname*{min}_{y}\\operatorname*{max}_{\\beta\\geq0}g(x,y)+\\beta^{\\top}h(x,y)$ be the primal and the dual solution to the lower level problem with parameter $x$ . We can verify that $y^{\\ast}$ satisfies all the constraints in the reformulation problem. The feasibility condition $h(x,y^{*})$ is automatically satisfied. We just need to check: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{g}^{*}(\\boldsymbol{x}):=\\underset{\\boldsymbol{\\theta}}{\\min}\\,\\boldsymbol{g}(\\boldsymbol{x},\\boldsymbol{\\theta})+(\\lambda^{*})^{\\top}\\boldsymbol{h}(\\boldsymbol{x},\\boldsymbol{\\theta})}\\\\ &{\\quad\\quad\\quad=\\boldsymbol{g}(\\boldsymbol{x},\\boldsymbol{y}^{*})+(\\lambda^{*})^{\\top}\\boldsymbol{h}(\\boldsymbol{x},\\boldsymbol{y}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, $x,y^{*}$ is a feasible point to the reformulation problem. ", "page_idx": 28}, {"type": "text", "text": "We now show the other direction, i.e., that feasibility of the reformulaed problem implies that of the lower-level problem. Given $\\lambda^{*}$ , let us assume $y$ satisfies $g(x,y)\\leq g_{\\lambda^{*}}^{*}(x)$ and $h(x,y)\\leq0$ . On the other hand, assume $y^{*},\\lambda^{*}=\\operatorname*{min}_{y}\\operatorname*{max}_{\\beta\\geq0}g(x,y)+\\beta^{\\top}h(x,y)$ be the primal and the dual solution. We can show that: ", "page_idx": 28}, {"type": "equation", "text": "$$\ng(x,y)+(\\lambda^{*})^{\\top}h(x,y)\\leq g^{*}(x):=\\operatorname*{min}_{\\theta}g(x,\\theta)+(\\lambda^{*})^{\\top}h(x,\\theta).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the strong convexity of $g+(\\lambda^{*})^{\\top}h$ , we know that $y$ matches to the unique minimum $y^{\\ast}$ , which implies that $y=y^{*}$ is also a feasible point to the original bilevel problem. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E Active constraints in differentiable optimization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "By computing the derivative of the KKT conditions in Section 2.1, we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h)+(\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h)\\frac{d y^{*}}{d x}+(\\nabla_{y}h)^{\\top}\\frac{d\\lambda^{*}}{d x}=0}\\\\ {\\mathrm{diag}(\\lambda^{*})\\nabla_{x}h+\\mathrm{diag}(\\lambda^{*})\\nabla_{y}h\\frac{d y^{*}}{d x}+\\mathrm{diag}(h)\\frac{d\\lambda^{*}}{d x}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $\\mathcal{T}=\\{i\\in[d_{h}]|h(x,y^{*})_{i}=0,\\lambda_{i}^{*}>0\\}$ be the set of active constraints with positive dual solution, and $\\mathcal{T}_{1}=\\{i|h(x,y^{*})_{i}\\neq0\\}$ be the set of inactive constraints and $\\mathcal{T}_{2}=\\{i|h(\\bar{x},y^{*})_{i}=0,\\lambda_{i}^{*}=0\\}$ . We know that $\\bar{Z}=\\Zint_{1}\\cup\\Zint_{2}$ . For each $i\\in\\mathcal{T}_{1}$ , due to complementary slackness, we know that $\\lambda_{i}^{*}=0$ . For $i\\in\\mathcal{T}_{1}$ in (E.1), we have $\\begin{array}{r}{\\lambda_{i}^{*}\\nabla_{x}h(x,y^{*})_{i}\\!+\\!\\lambda_{i}^{*}\\nabla_{y}h(x,y^{*})_{i}\\frac{d y^{*}}{d x}\\!+\\!h(x,y^{*})_{i}\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ , which implies $\\begin{array}{r}{h(x,y^{*})_{i}\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ because $\\lambda_{i}^{*}=0$ . This in turn impl\u2217ies $\\begin{array}{r}{\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ because $h(x,y^{*})_{i}<0$ . That means the dual variable \u03bbi\u2217 = 0 and has zero gradient dd\u03bbxi = 0 for any index i \u2208I1. Therefore, we can remove row $i\\in\\mathcal{Z}_{1}$ in (E.2) and obtain $\\lambda_{i}^{*}=0$ and $\\begin{array}{r}{\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "For i \u2208I2, the KKT condition in (E.2) is degenerate. Therefore, dd\u03bbxi can be arbitrary, i.e., nondifferentiable. As a subgradient choice, we can set $\\begin{array}{r}{\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ for such $i$ . This choice will also eliminate its impact on the KKT condition in (E.1) because $\\frac{d\\lambda_{i}^{*}}{d x}$ is set to be 0. By this choice of subgradient, we can also remove row $i\\in\\mathcal{Z}_{2}$ (E.2). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Thus (E.2) can be written as the following set of equations, for $h_{\\mathbb{Z}}=[h_{i}]_{i\\in\\mathbb{Z}}$ and $\\lambda_{\\mathcal{T}}^{*}=[\\lambda_{i}^{*}]_{i\\in\\mathbb{Z}}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\mathrm{diag}(\\lambda^{*})\\nabla_{x}h_{\\mathbb{Z}}+\\mathrm{diag}(\\lambda_{\\mathbb{Z}}^{*})\\nabla_{y}h_{\\mathbb{Z}}\\frac{d y^{*}}{d x}+\\mathrm{diag}(h_{\\mathbb{Z}})\\frac{d\\lambda_{\\mathbb{Z}}^{*}}{d x}=0}}\\\\ {{\\displaystyle\\implies\\mathrm{diag}(\\lambda^{*})\\nabla_{x}h_{\\mathbb{Z}}+\\mathrm{diag}(\\lambda_{\\mathbb{Z}}^{*})\\nabla_{y}h_{\\mathbb{Z}}\\frac{d y^{*}}{d x}=0\\quad(\\mathrm{due~to~}h_{\\mathbb{Z}}(x,y^{*})=0).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In (E.1), due to $\\begin{array}{r}{\\frac{d\\lambda_{i}^{*}}{d x}=0}\\end{array}$ for all $i\\in\\bar{\\mathcal{Z}}$ , we can remove d\u03bbi \u2200i \u2208I\u00af in (E.1) by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=(\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h)+(\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h)\\frac{d y^{*}}{d x}+(\\nabla_{y}h)^{\\top}\\frac{d\\lambda^{*}}{d x}}\\\\ &{\\quad=(\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h)+(\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h)\\frac{d y^{*}}{d x}+(\\nabla_{y}h_{Z})^{\\top}\\frac{d\\lambda_{T}^{*}}{d x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining (E.4) and (E.3), we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h)+(\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h)\\frac{d y^{*}}{d x}+(\\nabla_{y}h_{\\mathcal{Z}})^{\\top}\\frac{d\\lambda_{\\mathcal{Z}}^{*}}{d x}=0}\\\\ &{}&{\\mathrm{diag}(\\lambda^{*})\\nabla_{x}h_{\\mathcal{Z}}+\\mathrm{diag}(\\lambda_{\\mathcal{Z}}^{*})\\nabla_{y}h_{\\mathcal{Z}}\\frac{d y^{*}}{d x}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which can be written in its matrix form: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h}&{\\nabla_{y}h_{T}^{\\top}\\right]\\left[\\frac{d y^{*}}{d x}\\right]=-\\left[\\nabla_{y x}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h\\right]}\\\\ {\\mathrm{diag}(\\lambda_{T}^{*})\\nabla_{y}h_{T}}&{\\quad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This concludes the derivation of the derivative of constrained optimization in (5.2). ", "page_idx": 29}, {"type": "text", "text": "F Inequality case: bounds on primal solution error and constraint violation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma 5.1. Given any $x$ , the corresponding dual solution $\\lambda^{*}(x)$ , primal solution $y^{*}(x)$ of the lower optimization problem in Problem 4.1, and $y_{\\lambda^{*},\\alpha}^{*}(x)$ as in (5.5), satisfy: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|y_{\\lambda^{*},\\alpha}^{*}(x)-y^{*}(x)\\right\\|\\leq O(\\alpha_{1}^{-1})\\;\\,a n d\\;\\;\\big\\|h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*}(x))\\big\\|\\leq O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We first provide the claimed bound on $\\|y_{\\alpha_{1},\\alpha_{2}}^{*}-y^{*}(x)\\|$ . ", "page_idx": 29}, {"type": "text", "text": "Part 1: Bound on the convergence of $y$ . ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Since $y_{\\lambda^{*},\\alpha}^{*}$ minimizes $\\mathcal{L}_{\\alpha,\\lambda^{*}}(x,y)$ , the first-order condition gives us: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0=\\nabla_{y}\\mathcal{L}_{\\alpha,\\lambda^{*}}(x,y_{\\lambda^{*},\\alpha}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, we can compute the gradient of $\\mathcal{L}_{\\alpha,\\lambda^{*}}(x,y)$ at $y^{\\ast}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{y}\\mathcal{L}_{\\alpha}(x,y^{*})=\\nabla_{y}f(x,y^{*})+\\alpha_{1}(\\nabla_{y}g(x,y^{*})+(\\lambda^{*})^{\\top}\\nabla_{y}h(x,y^{*}))+\\alpha_{2}\\nabla_{y}h_{Z}(x,y^{*})^{\\top}h_{Z}(x,y^{*})}\\\\ &{\\qquad\\qquad\\quad=\\nabla_{y}f(x,y^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second step is due to the property of the primal and dual solution: $\\nabla_{y}g(x,y^{*})\\,+$ $(\\lambda^{*})^{\\top}\\nabla_{y}h(x,y^{*})\\,=\\,0$ by the stationarity condition in the KKT conditions, and by definition of the active constraints $h z$ where the optimal $y^{\\ast}$ must have $h_{\\mathcal{T}}(x,y^{*})=0$ . ", "page_idx": 29}, {"type": "text", "text": "Since, for a sufficiently large $\\alpha_{1}$ , the penalty function is $\\begin{array}{r}{\\alpha_{1}\\mu_{g}-L_{f}\\geq\\frac{\\alpha_{1}\\mu_{g}}{2}}\\end{array}$ \u03b112\u00b5g strongly convex in y, we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{1}\\mu_{g}}{2}\\left\\|y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\right\\|\\leq\\left\\|\\nabla_{y}\\mathcal{L}_{\\alpha,\\lambda^{*}}(x,y^{*})-\\nabla_{y}\\mathcal{L}_{\\alpha,\\lambda^{*}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right\\|=\\|\\nabla_{y}f(x,y^{*})\\|\\leq L_{f}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, upon rearranging the terms, we obtain the claimed bound: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|y^{*}-y_{\\alpha,\\lambda^{*}}^{*}\\right\\|\\leq\\frac{2L_{f}}{\\alpha_{1}\\mu_{g}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Part 2: bound on the constraint violation. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "When we plug $y^{\\ast}$ into (5.4), we get: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\dot{\\gamma}_{\\alpha,\\lambda^{*}}(\\boldsymbol{x},\\boldsymbol{y}^{*})=f(\\boldsymbol{x},\\boldsymbol{y}^{*})+\\alpha_{1}(g(\\boldsymbol{x},\\boldsymbol{y}^{*})+(\\lambda^{*})^{\\top}h(\\boldsymbol{x},\\boldsymbol{y}^{*})-g_{\\lambda^{*}}^{*}(\\boldsymbol{x}))+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathcal{T}}(\\boldsymbol{x},\\boldsymbol{y}^{*})\\right\\|^{2}=f(\\boldsymbol{x},\\boldsymbol{y}^{*}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Plugging in $y_{\\alpha,\\lambda^{*}}^{*}$ , we may obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\overline{{\\mathbf{\\Xi}}}}_{\\alpha,\\lambda^{*}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})=f(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+\\alpha_{1}(g(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+(\\lambda^{*})^{\\top}h(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})-g^{*}(\\boldsymbol{x}))+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})\\right\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad=f(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+\\alpha_{1}(g(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+(\\lambda^{*})^{\\top}h(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})-g(\\boldsymbol{x},\\boldsymbol{y}^{*})-(\\lambda^{*})^{\\top}h(\\boldsymbol{x},\\boldsymbol{y}^{*}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geq f(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+\\alpha_{1}\\frac{\\mu_{g}}{2}\\left\\|\\boldsymbol{y}^{*}-\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*}\\right\\|^{2}+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we used the strong convexity (with respect to $y$ ) of $g(x,y)+(\\lambda^{*})^{\\top}h(x,y)$ and the optimality of $y^{\\ast}$ for $g(x,y)+(\\lambda^{*})^{\\mp}h(x,y)$ . By the optimality of $y_{\\lambda^{*},\\alpha}^{*}$ for $\\mathcal{L}_{\\alpha,\\lambda^{*}}$ , we know that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\boldmath~\\lambda~}}\\\\ {\\mathrm{\\boldmath~\\lambda~}}\\end{array}=\\mathcal{L}_{\\alpha,\\lambda^{*}}(\\boldsymbol{x},\\boldsymbol{y}^{*})\\geq\\mathcal{L}_{\\alpha,\\lambda^{*}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})\\geq f(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})+\\alpha_{1}\\frac{\\mu_{g}}{2}\\left\\|\\boldsymbol{y}^{*}-\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*}\\right\\|^{2}+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathcal{Z}}(\\boldsymbol{x},\\boldsymbol{y}_{\\lambda^{*},\\alpha}^{*})\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, by the Lipschitzness of the function f in terms of y, and the bound \u2225y\u2217\u2212y\u03bb\u2217\u2217,\u03b1\u2225\u2264\u03b121L\u00b5fg , we know that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(x,y_{\\lambda^{\\star},\\alpha}^{*})\\right\\|^{2}\\leq f(x,y^{*})-f(x,y_{\\lambda^{*},\\alpha}^{*})-\\alpha_{1}\\frac{\\mu_{g}}{2}\\left\\|y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\right\\|^{2}}}\\\\ &{\\leq L_{f}\\left\\|y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\right\\|-\\alpha_{1}\\frac{\\mu_{g}}{2}\\left\\|y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\right\\|^{2}}\\\\ &{\\leq L_{f}\\left\\|y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\right\\|}\\\\ &{=O(\\alpha_{1}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Rearranging terms then gives the claimed bound. ", "page_idx": 30}, {"type": "text", "text": "The bound on the constraint violation in Lemma 5.1 is an important step in the following theorem. ", "page_idx": 30}, {"type": "text", "text": "G Proof of Lemma 5.2: gradient approximation for inequality constraints ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma 5.2. Consider $F$ as in Problem 4.1, $\\mathcal{L}$ as in (5.4), a fixed $x$ , and $y_{\\lambda^{*},\\alpha}^{*}$ as in (5.5). Then under Assumptions 2.2 and 2.5, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\nabla F(x)-\\nabla_{x}\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y_{\\lambda^{*},\\alpha}^{*})\\|\\leq O(\\alpha_{1}^{-1})+O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2})+O(\\alpha_{1}^{1/2}\\alpha_{2}^{-1/2})+O(\\alpha_{1}^{-3/2}\\alpha_{2}^{1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. First, we recall (5.4) here: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)=f(x,y)+\\alpha_{1}\\left(g(x,y)+(\\lambda^{*})^{\\top}h(x,y)-g^{*}(x)\\right)+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(x,y)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Next, recall from Equation D.1, we can express $g^{*}(x)=g(x,y^{*})+(\\lambda^{*})^{\\top}h(x,y^{*})$ , which we use in the first step below: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{x}F(x)-\\frac{d}{d x}C_{\\lambda^{*},\\alpha}(x,y_{\\lambda^{*},\\alpha}^{*})}\\ ~}\\\\ {{\\displaystyle=\\left(\\nabla_{x}f(x,y^{*})+\\frac{d y^{*}}{d x}^{\\top}\\nabla_{y}f(x,y^{*})\\right)-\\left(\\nabla_{x}f(x,y_{\\lambda^{*},\\alpha}^{*})+\\alpha_{1}(\\nabla_{x}g(x,y_{\\lambda^{*},\\alpha}^{*})+\\nabla_{x}h(x,y_{\\lambda^{*},\\alpha}^{*})\\right.}}\\\\ {{\\displaystyle~~}}\\\\ {{\\displaystyle~~~-\\alpha_{1}(\\nabla_{x}g(x,y^{*})+\\nabla_{x}h(x,y^{*})^{\\top}\\lambda^{*})+\\alpha_{2}\\nabla_{x}h_{Z}(x,y_{\\lambda^{*},\\alpha}^{*})^{\\top}h_{Z}(x,y_{\\lambda^{*},\\alpha}^{*})\\Bigg)}\\ ~}\\\\ {{\\displaystyle=\\nabla_{x}f(x,y^{*})-\\nabla_{x}f(x,y_{\\lambda^{*},\\alpha}^{*})}\\ ~}\\\\ {{\\displaystyle~~~~+\\frac{d y^{*}}{d x}^{\\top}\\nabla_{y}f(x,y^{*})-\\frac{d y^{*}}{d x}^{\\top}\\nabla_{y}f(x,y_{\\lambda^{*},\\alpha}^{*})}\\ ~}\\\\ {{\\displaystyle~~~~+\\frac{d y^{*}}{d x}^{\\top}\\nabla_{y}f(x,y_{\\lambda^{*},\\alpha}^{*})-\\alpha_{1}\\left[\\nabla_{y\\lambda^{*}}^{2}g(x,y_{\\lambda^{*},\\alpha}^{*})\\nabla_{y\\lambda}^{2}h\\right]^{\\top}\\left[\\mathbb{B}_{\\lambda^{*},\\alpha}^{*}-y^{*}\\right]}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to (5.2) and (E.5), we let ", "page_idx": 31}, {"type": "equation", "text": "$$\nH=\\left[\\begin{array}{c c}{\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y y}^{2}h}&{\\nabla_{y}h_{\\mathcal{Z}}^{\\top}}\\\\ {\\operatorname{diag}((\\lambda^{*})_{\\mathcal{Z}}^{*})\\nabla_{y}h_{\\mathcal{Z}}}&{0}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which is invertible by Assumption 2.2(iii) and by the fact that we remove all the inactive constraints.   \nWe now bound the terms in (G.1), (G.2), (G.3), (G.4), and (G.5). ", "page_idx": 31}, {"type": "text", "text": "Bounding (G.1) and (G.2): (G.1) can be easily bounded by the smoothness of $f$ in terms of $x$ and $y$ , and the bound on $\\|y^{*}-y_{(\\lambda^{*})^{*},\\alpha}^{*}\\|\\leq O(\\alpha_{1}^{-1})$ from Lemma 5.1. Therefore, we know: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{x}f(x,y^{*})-\\nabla_{x}f(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\right\\|\\leq C_{f}\\left\\|y^{*}-y_{(\\lambda^{*})^{*},\\alpha}^{*}\\right\\|\\leq C_{f}\\cdot O(\\alpha_{1}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, given Assumption 2.5 by which $y^{*}(x)$ is $L_{y}$ -Lipschitz in $x$ , we have the bound $\\begin{array}{r}{\\left\\|\\frac{d y^{*}}{d x}\\right\\|\\leq L_{y}}\\end{array}$ . Therefore, (G.2) can be bounded by: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\frac{d{y^{*}}}{d x}^{\\top}\\nabla_{y}f(x,y^{*})-\\frac{d{y^{*}}^{\\top}}{d x}\\nabla_{y}f(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\right\\|\\leq C_{f}\\left\\|\\frac{d{y^{*}}}{d x}\\right\\|\\left\\|y^{*}-y_{(\\lambda^{*})^{*},\\alpha}^{*}\\right\\|\\leq C_{f}L_{y}\\cdot O(\\alpha_{1}^{-1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Bounding (G.3): ", "page_idx": 31}, {"type": "text", "text": "Using (5.2) to solve $\\begin{array}{r}{\\left[\\frac{\\frac{d y^{*}}{d x}}{d x}\\right]=-H^{-1}\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h\\right]}\\\\ {\\frac{d(\\lambda^{*})^{*}}{d x}\\right]=-H^{-1}\\left[\\nabla_{y x}^{2}g((\\lambda^{*})_{\\mathcal{Z}}^{*})\\nabla_{x}h_{\\mathcal{Z}}\\right]}\\end{array}$ , we can write: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d{y^{*}}^{\\top}}{d x}\\nabla_{y}f(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})=\\left[\\!\\!\\begin{array}{c}{\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h}\\\\ {\\mathrm{diag}((\\lambda^{*})_{\\bar{\\pi}}^{*})\\nabla_{x}h_{Z}}\\end{array}\\!\\!\\right]^{\\top}(H^{-1})^{\\top}\\left[\\!\\!\\begin{array}{c}{-\\nabla_{y}f(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\!\\!\\right]}\\\\ {0}\\end{array}\\!\\!\\right]}\\\\ &{\\,=-\\frac{d{y^{*}}^{\\top}}{d x}^{\\top}\\Bigg(\\!\\alpha_{1}\\left[\\!\\!\\begin{array}{c}{\\nabla_{y}g(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})+\\nabla_{y}h(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})^{\\top}(\\lambda^{*})^{*}\\!\\!\\right]}\\\\ {0}\\end{array}\\!\\!\\right)}\\\\ &{\\,\\quad\\,+\\,\\alpha_{2}\\left[\\!\\!\\nabla_{y}h_{Z}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})^{\\top}h_{Z}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\!\\!\\right]\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we use the optimality of $y_{(\\lambda^{*})^{*},\\alpha}^{*}$ from (5.5): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{y}f(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})+\\alpha_{1}\\left(\\nabla_{y}g(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})+\\nabla_{y}h(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})^{\\top}(\\lambda^{*})^{*}\\right)}\\\\ &{\\quad+\\,\\alpha_{2}\\nabla_{y}h_{Z}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})^{\\top}h_{Z}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Further, recall that $H$ is non-degenerate by Assumption 2.2, as a result of which, the added term 1 in (G.3) can be modified as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h\\right]^{\\top}\\left[\\alpha_{1}(y_{(\\lambda^{*})^{*},\\alpha}^{*}-y^{*})\\right]}\\\\ &{\\qquad\\mathrm{diag}((\\lambda^{*})_{\\bar{\\mathcal{Z}}}^{*})\\nabla_{x}h_{\\mathcal{Z}}}\\\\ &{=\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h\\right]^{\\top}(H^{-1})^{\\top}H^{\\top}\\left[\\alpha_{1}(y_{(\\lambda^{*})^{*},\\alpha}^{*}-y^{*})\\right]}\\\\ &{\\qquad\\mathrm{diag}((\\lambda^{*})_{\\bar{\\mathcal{Z}}}^{*})\\nabla_{x}h_{\\mathcal{Z}}}\\\\ &{=\\alpha_{1}\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h\\right]^{\\top}(H^{-1})^{\\top}\\left[(\\nabla_{y y}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y y}^{2}h)^{\\top}(y_{(\\lambda^{*})^{*},\\alpha}^{*}-y^{*})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The added term 2 in (G.3) can be expanded to: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{2}\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h(\\lambda^{*})^{*}\\right]^{\\top}\\left[\\operatorname{diag}(1/(\\lambda^{*})_{\\bar{x}}^{*})h_{\\bar{Z}}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\right]}\\\\ &{\\qquad\\qquad\\mathrm{diag}((\\lambda^{*})_{\\bar{Z}}^{*})\\nabla_{x}h_{\\bar{Z}}}\\\\ &{=\\!\\alpha_{2}\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h(\\lambda^{*})^{*}\\right]^{\\top}(H^{-1})^{\\top}H^{\\top}\\left[\\operatorname{diag}(1/(\\lambda^{*})_{\\bar{Z}}^{*})h_{\\bar{Z}}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\right]}\\\\ &{\\qquad\\qquad\\mathrm{diag}((\\lambda^{*})_{\\bar{Z}}^{*})\\nabla_{x}h_{\\bar{Z}}}\\\\ &{=\\!\\alpha_{2}\\left[\\nabla_{y x}^{2}g+((\\lambda^{*})^{*})^{\\top}\\nabla_{y x}^{2}h(\\lambda^{*})^{*}\\right]^{\\top}(H^{-1})^{\\top}\\left[\\nabla_{y}h_{\\bar{Z}}(x,y^{*})^{\\top}h_{\\bar{Z}}(x,y_{(\\lambda^{*})^{*},\\alpha}^{*})\\right]}\\\\ &{\\qquad\\qquad\\mathrm{diag}((\\lambda^{*})_{\\bar{Z}}^{*})\\nabla_{x}h_{\\bar{Z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we can compute the difference between (G.6), (G.8), and (G.9) to bound (G.3), and use the fact that $\\nabla_{y}g(x,y^{*})+(\\lambda^{*})^{\\top}\\nabla_{y}h(x,y^{*})=0$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\left|\\frac{d y^{*}}{d x}^{\\top}\\nabla_{y}f(x,y_{\\lambda^{*},\\alpha}^{*})-\\mathrm{added~term~1~-~added~term~2~}\\right.}\\\\ &{}&{\\qquad\\left.=\\left[\\nabla_{y y}^{2}g+(\\lambda^{*})^{\\top}\\nabla_{y x}^{2}h\\right]^{\\top}(H^{-1})^{\\top}\\left(\\alpha_{1}\\left[\\nabla_{y}g(x,y_{\\lambda^{*},\\alpha}^{*})-\\nabla_{y}g(x,y^{*})-\\nabla_{y y}^{2}g(x,y^{*})(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right]\\right.\\right.}\\\\ &{}&{\\qquad\\left.\\left.(\\mathrm{G.1})\\nabla_{y}h(x,y_{\\lambda^{*},\\alpha}^{*})^{\\top}\\lambda^{*}-\\nabla_{y}h(x,y^{*})^{\\top}\\lambda^{*}-\\nabla_{y y}^{2}h(x,y^{*})^{\\top}\\lambda^{*}(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right]\\right.\\qquad\\qquad\\left.(\\mathrm{G.1})\\right.}\\\\ &{}&{\\qquad\\left.\\qquad-\\alpha_{1}\\left[\\nabla_{y}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*})(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right]\\qquad\\qquad\\qquad\\qquad\\qquad\\right.}\\\\ &{}&{\\qquad\\qquad\\left.\\left.+\\alpha_{2}\\left[\\nabla_{y}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*})^{\\top}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right]-\\left[\\nabla_{y}h_{\\mathcal{X}}(x,y^{*})^{\\top}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right]\\right)\\qquad\\qquad\\qquad\\right.(\\mathrm{G.1})}\\\\ &{}&{\\qquad\\qquad\\left.(\\mathrm{G.1})\\partial_{y}h(x,y_{\\lambda^{*},\\alpha}^{*})\\right]^{\\top}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right]-\\left[\\nabla_{y}h_{\\mathcal{X}}(x,y^{*})^{\\top}h_{\\mathcal{X}}(x,y_{\\lambda^{*},\\alpha}^{*\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The terms in (G.10) and (G.11) can both be bounded by $\\alpha_{1}C_{g}L_{y}\\|y_{\\lambda^{*},\\alpha}^{*}\\ -\\ y^{*}\\|^{2}$ and $\\alpha_{1}R C_{h}L_{y}\\|y_{\\lambda^{*},\\alpha}^{*}-y^{*}\\|^{2}$ by the smoothness of $g$ and $h^{\\top}\\lambda^{*}$ . Further, plugging in $\\lVert y^{*}-y_{\\lambda^{*},\\alpha}^{*}\\rVert\\leq$ $O(\\alpha_{1}^{-1})$ from Lemma 5.1 bounds both these terms by $O(\\alpha_{1}^{-1})$ . ", "page_idx": 32}, {"type": "text", "text": "To bound the term in (G.12), we use: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})-h_{\\mathbb{Z}}(x,y^{*})-\\nabla_{y}h_{\\mathbb{Z}}(x,y^{*})(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right\\|\\leq C_{h}\\left\\|y_{\\lambda^{*},\\alpha}^{*}-y^{*}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{y}h_{\\mathbb{Z}}(x,y^{*})(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right\\|\\leq\\left\\|h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right\\|+\\left\\|h_{\\mathbb{Z}}(x,y^{*})\\right\\|+C_{h}O(\\left\\|y_{\\lambda^{*},\\alpha}^{*}-y^{*}\\right\\|^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2})+0+O(\\alpha_{1}^{-2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2}+\\alpha_{1}^{-2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which upon scaling by $\\alpha_{1}$ gives us the following bound on the term in (G.12): ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{1}\\left\\|\\nabla_{y}h_{\\mathbb{Z}}(x,y^{*})(y_{\\lambda^{*},\\alpha}^{*}-y^{*})\\right\\|\\leq O(\\alpha_{1}^{1/2}\\alpha_{2}^{-1/2}+\\alpha_{1}^{-1})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The term in (G.13) can be bounded by: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{2}\\left\\lVert\\nabla_{x}h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})^{\\top}h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})-\\nabla_{x}h_{\\mathbb{Z}}(x,y^{*})^{\\top}h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right\\rVert}\\\\ &{=\\!\\alpha_{2}\\left\\lVert\\nabla_{x}h_{\\mathbb{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})-\\nabla_{x}h_{\\mathbb{Z}}(x,y^{*})\\right\\rVert O(\\left\\lVert h_{\\mathbb{Z}}(x,y_{\\alpha,\\lambda^{*}}^{*})\\right\\rVert)}\\\\ &{=\\!\\alpha_{2}\\cdot O(\\alpha_{1}^{-1})O(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2})}\\\\ &{=\\!O(\\alpha_{1}^{-3/2}\\alpha_{2}^{1/2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Bounding (G.4): This can be easily bounded by the smoothness of $g$ and $h$ , and the bound on the dual solution $\\|\\lambda^{*}\\|\\leq R$ . Thus (G.4) can be bounded by $R\\cdot O(\\alpha_{1}^{-1})=O(\\alpha_{1}^{-1})$ . ", "page_idx": 33}, {"type": "text", "text": "Bounding (G.5): By the same argument in (G.14), we get: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{2}\\left\\lVert\\nabla_{y}h_{\\mathcal{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})^{\\top}h_{\\mathcal{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})-\\nabla_{y}h_{\\mathcal{Z}}(x,y^{*})^{\\top}h_{\\mathcal{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right\\rVert}\\\\ &{\\leq\\!\\alpha_{2}\\left\\lVert\\nabla_{y}h_{\\mathcal{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})-\\nabla_{y}h_{\\mathcal{Z}}(x,y^{*})\\right\\rVert\\left\\lVert h_{\\mathcal{Z}}(x,y_{\\lambda^{*},\\alpha}^{*})\\right\\rVert}\\\\ &{=\\!\\alpha_{2}\\cdot{\\cal O}(\\alpha_{1}^{-1}){\\cal O}(\\alpha_{1}^{-1/2}\\alpha_{2}^{-1/2})}\\\\ &{=\\!{\\cal O}(\\alpha_{1}^{-3/2}\\alpha_{2}^{1/2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining all upper bounds gives the claimed bound. ", "page_idx": 33}, {"type": "text", "text": "H Proof of the main result (Theorem 5.3): convergence and computation cost ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Theorem 5.3. Given any accuracy parameter $\\alpha\\,>\\,0,$ , Algorithm $^{4}$ outputs $\\widetilde{\\nabla}_{x}F(x)$ such that $\\|\\widetilde{\\nabla}F(x)-\\nabla F(x)\\|\\le\\alpha$ within $\\widetilde{O}(\\alpha^{-1})$ gradient oracle evaluations. ", "page_idx": 33}, {"type": "text", "text": "Proof. First, given the bound in Lemma 5.2, we choose $\\alpha_{1}=\\alpha^{-2}$ and $\\alpha_{2}=\\alpha^{-4}$ to ensure the inexactness of the gradient oracle is bounded by $\\alpha$ . In the later analysis, we will still use $\\alpha_{1}$ and $\\alpha_{2}$ in the penalty function for clarity. ", "page_idx": 33}, {"type": "text", "text": "Now we estimate the computation cost of the inexact gradient oracle: ", "page_idx": 33}, {"type": "text", "text": "Lower-level problem. Given the oracle access to the optimal dual solution $\\lambda^{*}(x)$ , we can recover the primal solution $y^{*}(x)$ efficiently (e.g, by [45]). Therefore, we can use the primal and dual solutions to construct the penalty function $\\mathcal{L}_{\\lambda^{*},\\alpha}(x,y)$ in (5.4). ", "page_idx": 33}, {"type": "text", "text": "Penalty function minimization problem. The second main optimization problem is the penalty minimization problem in Line 4 of Algorithm 4. Recall from (5.4) that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\lambda,\\alpha}(x,y)=f(x,y)+\\alpha_{1}\\left(g(x,y)+\\lambda^{\\top}h(x,y)-g^{*}(x)\\right)+\\frac{\\alpha_{2}}{2}\\left\\|h_{\\mathbb{Z}}(x,y)\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use the approximate dual solution $\\lambda$ as opposed to the optimal dual solution $\\lambda^{*}$ . ", "page_idx": 33}, {"type": "text", "text": "Given (H.1), we solve the penalty minimization problem: ", "page_idx": 33}, {"type": "equation", "text": "$$\ny_{\\lambda,\\alpha}^{\\prime}(x):=\\arg\\operatorname*{min}_{y}\\mathcal{L}_{\\lambda,\\alpha}(x,y).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The penalty minimization is a unconstrained strongly convex optimization problem, which is known to have linear convergence rate. We further analyze its convexity and smoothness below to precisely estimate the computation cost: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The smoothness of $\\mathcal{L}_{\\lambda,\\alpha}(x,y)$ is dominated by the smoothness of $\\alpha_{2}\\left\\|h_{\\mathbb{Z}}(x,y)\\right\\|^{2}$ since $\\alpha_{2}\\gg\\alpha_{1}$ . By Lemma 5.2, we know that the optimal solution must lie in an open ball $B(y^{\\ast},{O}(1/\\alpha_{1}))$ with center $y^{\\ast}$ (inner optimization primal solution) and a radius of the order of $\\textstyle O\\big(\\frac{1}{\\alpha_{1}}\\big)$ . This implies that we just need to search over a bounded feasible set of $y$ , which we can bound $\\|\\nabla_{y}h(x,y)\\|\\leq L_{h}$ and $h(x,y)\\leq H$ within the bounded region $y\\;\\in\\;B(y^{*},O(1/\\alpha_{1}))$ . We can show that $h^{2}$ is smooth (gradient Lipschitz) within the bounded region by the following: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{y y}^{2}h^{2}\\right\\|=\\left\\|h\\nabla_{y y}^{2}h+\\nabla_{y}h^{\\top}\\nabla_{y}h\\right\\|\\leq\\left\\|h\\nabla_{y y}^{2}h\\right\\|+\\left\\|\\nabla_{y}h^{\\top}\\nabla_{y}h\\right\\|\\leq H C_{h}+L_{h}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which also implies $h_{\\mathcal{T}}^{2}$ is also smooth (gradient Lipschitz). Therefore, $\\alpha h_{\\mathscr Z}^{2}$ is $(H C_{h}\\mathrm{~+~}$ $L_{h}^{2})\\alpha_{2}=O(\\stackrel{\\star}{\\alpha_{2}})$ smooth. ", "page_idx": 34}, {"type": "text", "text": "Choosing $\\textstyle\\alpha_{1}={\\frac{1}{\\alpha^{2}}}$ and $\\textstyle\\alpha_{2}={\\frac{1}{\\alpha^{4}}}$ , the condition number of $\\mathcal{L}_{\\pmb{\\alpha},\\lambda}(x,y)$ becomes $\\kappa=O(\\alpha_{2}/\\alpha_{1})=$ $\\textstyle O({\\frac{1}{\\alpha^{2}}})$ . Therefore, by the linear convergence of gradient descent in strongly convex smooth optimization, the number of iterations needed to get to $\\alpha$ accuracy is $\\begin{array}{r}{O(\\sqrt{\\alpha^{-2}}\\times\\log(\\frac{1}{\\alpha}))=O(\\frac{1}{\\alpha}\\log(\\frac{1}{\\alpha}))}\\end{array}$ . Therefore, we can get a near optimal solution $y_{\\lambda,\\alpha}^{\\prime}$ with inexactness $\\alpha$ in $\\textstyle O({\\frac{1}{\\alpha}})$ oracle calls. ", "page_idx": 34}, {"type": "text", "text": "Computation cost and results. Overall, for the inner optimization, we can invoke the efficient optimal dual solution oracle to get the optimal dual solution $\\lambda^{*}(x)$ and recover the optimal primal solution $y^{*}(x)$ from there. For the penalty minimization, we need $\\textstyle O({\\frac{1}{\\alpha}})$ oracle calls to solve an unconstrained strongly convex smooth optimization problem to get to $\\alpha$ accuracy. In conclusion, combining everything in Appendix H, we run $\\textstyle O({\\frac{1}{\\alpha}})$ oracle calls to obtain an $\\alpha$ accurate gradient oracle to approximate the hyperobjective gradient $\\nabla_{x}\\overleftrightarrow{F}(x)$ . This concludes the proof of Theorem 5.3. ", "page_idx": 34}, {"type": "text", "text": "Remark H.1. The following analysis quantifies how the error in the optimal dual solution propagates to the inexact gradient estimate. This is not needed if such a dual solution oracle exists. But in practice, the oracle may come with some error, for which we bound the error. ", "page_idx": 34}, {"type": "text", "text": "Bounding the error propagation in error in dual solution and the penalty minimization. First, if we do not get an exact optimal dual solution, the error in the dual solution $\\lambda$ with $\\|\\lambda-\\lambda^{*}\\|\\leq\\alpha$ will slightly impact the analysis in Lemma 5.2. Specifically, in Appendix G, the approximate $\\lambda$ will impact the inexact gradient $\\nabla_{x}\\mathcal{L}_{\\lambda,\\alpha}(x,y_{\\lambda,\\alpha}^{\\prime})$ computation and the analysis in (G.4) and (G.7). In (G.4), to change $\\lambda$ to $\\lambda^{*}$ , we get an additional error: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha_{1}\\bigg(\\nabla_{x}h(x,y^{\\prime})^{\\top}(\\lambda-\\lambda^{*})-\\nabla_{x}h(x,y_{\\lambda,\\alpha}^{\\prime})^{\\top}(\\lambda-\\lambda^{*})\\bigg)}\\\\ &{=\\!\\alpha_{1}\\big(\\nabla_{x}h(x,y^{\\prime})-\\nabla_{x}h(x,y_{\\lambda,\\alpha}^{\\prime})\\big)^{\\top}(\\lambda-\\lambda^{*})}\\\\ &{\\le\\!\\alpha_{1}C_{h}\\left\\|y^{\\prime}-y_{\\lambda,\\alpha}^{\\prime}\\right\\|(\\lambda-\\lambda^{*})}\\\\ &{\\le\\!O(\\alpha_{1}\\alpha_{1}^{-1}\\alpha)=O(\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality is due to $\\left\\|y^{\\prime}-y_{\\lambda,\\alpha}^{\\prime}\\right\\|\\leq O(\\alpha_{1}^{-1})$ that is based on a similar analysis in Lemma 5.1 with a near-optimal $y_{\\lambda,\\alpha}^{\\prime}$ under $\\alpha^{2}=\\alpha_{1}$ accuracy. ", "page_idx": 34}, {"type": "text", "text": "Therefore, the error incurred by inexact $\\lambda$ in (G.4) is at most $O(\\alpha)$ , which is of the same rate as the current gradient inexactness $\\bar{O(\\alpha)}$ . ", "page_idx": 34}, {"type": "text", "text": "In (G.7), the optimality holds approximately for the approximate $\\lambda$ . Therefore, by the near optimality of $y_{\\lambda,\\alpha}^{\\prime}$ (strongly convex optimization), we know that the following gradient is also $\\alpha$ -close to 0, i.e., ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{y}f(x,y_{\\lambda,\\alpha}^{\\prime})+\\alpha_{1}\\left(\\nabla_{y}g(x,y_{\\lambda,\\alpha}^{\\prime})+\\nabla_{y}h(x,y_{\\lambda,\\alpha}^{\\prime})^{\\top}\\lambda\\right)}\\\\ &{\\quad+\\,\\alpha_{2}\\nabla_{y}h_{\\mathbb{Z}}(x,y_{\\lambda,\\alpha}^{\\prime})^{\\top}h_{\\mathbb{Z}}(x,y_{\\lambda,\\alpha}^{\\prime})\\|\\leq\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "whose inexactness matches the inexactness of the gradient oracle $\\alpha$ , and thus we do not incur additional order of inexactness here. ", "page_idx": 34}, {"type": "text", "text": "Moreover, there is an additional error because we need $\\lambda^{*}$ as opposed to a near-optimal $\\lambda$ to make the analysis in Appendix $\\mathrm{G}$ work. The error between using $\\lambda$ and $\\lambda^{*}$ in (H.3) can be bounded by: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{y}h(x,y_{\\lambda,\\alpha}^{\\prime})^{\\top}(\\lambda-\\lambda^{*})\\right\\|\\leq L_{h}\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we use the local Lipschitzness of the function $h$ in an open ball near $y^{\\ast}$ . Therefore, the additional error is also $O(\\bar{\\alpha})$ , which matches the inexactness of the inexact gradient oracle. ", "page_idx": 35}, {"type": "text", "text": "Therefore, we conclude that in order to bound the inexactness of the gradient oracle, we just need an efficient inexact dual solution with $\\alpha$ accuracy. ", "page_idx": 35}, {"type": "text", "text": "I Practical oracle to optimal (approximate) dual solution ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Here we discuss how practical the assumption on the oracle access to the optimal dual solution is. ", "page_idx": 35}, {"type": "text", "text": "For linear inequality constraint $h(x,y)=A x-B y-b$ , the LL problem is a constrained strongly convex smooth optimization problem. To show that we can compute an approximate solution to the optimal dual solution for linear inequality constraints, we apply the result from [93]: ", "page_idx": 35}, {"type": "text", "text": "Corollary I.1 (Application of Corollary 3.1 in [93]). When $h(x,y)=A x-B y+b$ is linear in $y$ , the primal and dual solutions can be written as: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{y^{*},\\lambda^{*}=\\arg\\underset{y}{\\operatorname*{min}}\\operatorname*{max}_{\\lambda}g(x,y)+(\\lambda^{*})^{\\top}h(x,y)=g(x,y)-(\\lambda^{*})^{\\top}B y+R(x)}\\\\ {\\Longleftrightarrow y^{*},\\lambda^{*}=\\arg\\underset{y}{\\operatorname*{min}}\\operatorname*{max}_{\\lambda}g(x,y)-(\\lambda^{*})^{\\top}B y}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $g$ is strongly convex in y and $B$ is of full rank by Assumption 2.2. According to Corollary 3.1 from [93], the primal-dual gradient method guarantees a linear convergence. More precisely, in $\\textstyle t=O(\\log{\\frac{1}{\\alpha}})$ iterations, we get: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|y^{t}-y^{*}\\right\\|\\leq\\alpha\\,a n d\\,\\left\\|\\lambda^{t}-\\lambda^{*}\\right\\|\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Given Corollary I.1, we can efficiently approximate the primal and dual solutions up to high accuracy with $O(\\log{\\frac{1}{\\alpha}})$ oracle calls when the inequality constraints are linear. This gives us an efficient approximate oracle access to the dual solution. ", "page_idx": 35}, {"type": "text", "text": "Remark I.2. Under the assumption of an optimal dual solution oracle, all the analyses mentioned in Section 5 hold for the general convex inequality constraints. However, the main technical challenge is that the dual solution oracle for general convex inequality cannot be guaranteed in practice. In fact, to the best of our knowledge, there is no iterate convergence in the dual solution \u03bb for general convex inequality constraints. Most of the literature in strongly-convex-concave saddle point convergence only guarantees dual solution convergence in terms of its duality gap or some other merit functions. We are not aware of any successful bound on the dual solution iterate convergence, which is an important research question to answer by itself. This is the main technical bottleneck for general convex inequality constraints as well. ", "page_idx": 35}, {"type": "text", "text": "Remark I.3. On the other hand, we need the dual solution iterate convergence with rate ${\\cal O}(1/\\alpha)$ to ensure the error to be bounded. But this is not a necessary condition. To ensure a bound on the error propagation, we just need to bound some forms of merit functions ((H.2) and $(H.4))$ of the dual solutions, which we believe that this is much more tractable than the actual iterate dual solution convergence. We leave this as a future direction and this will generalize the analysis from linear inequality constraints to general convex inequality constraints. ", "page_idx": 35}, {"type": "text", "text": "J The role of $\\lambda^{*}(x)$ in the derivative of Equation (5.4) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Notice that Equation (5.4), we treat the dual solution $\\lambda^{*}(x)$ as a constant to define the penalty function derivative. Yet, the dual solution $\\lambda^{*}(x)$ is in fact also a function of $x$ . Therefore, in theory, we should also compute its derivative with respect to $x$ . ", "page_idx": 35}, {"type": "text", "text": "However, notice that the following: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla_{x}(\\lambda^{*}(x))^{\\top}h(x,y)=\\nabla_{x}h(x,y)^{\\top}\\lambda^{*}+\\frac{d\\lambda(x)}{d x}^{\\top}h(x,y)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The later term in Equation (J.1) can be divided into two cases: ", "page_idx": 35}, {"type": "text", "text": "\u2022 For active constraint $\\textit{i}\\in\\textit{\\mathcal{T}}$ with $h(x,y^{*})\\;=\\;0$ , we know that $y_{\\lambda,\\alpha}^{*}$ is close to $y^{\\ast}$ by Lemma 5.1. Therefore, the derivativ e  d\u03bbd(xx)\u22a4h(x, y\u03bb\u2217,\u03b1)  \u2264LhL\u03bb\u03b11 = O(\u03b11) = O(\u03b12) by the local smoothness of $h$ near $y^{\\ast}$ and the Lipschitzness assumption of $\\lambda^{*}$ in Assumption 2.5.   \n\u2022 For inactive constraint $i\\:\\in\\:\\bar{\\mathcal{Z}}$ and $\\lambda_{i}^{*}~>~0$ , we can solve the KKT conditions and get d\u03bbd(xx)= 0. Therefore, the second term becomes 0.   \n\u2022 For inactive constraint $i\\in\\bar{\\mathcal{T}}$ and ${{\\lambda}_{i}^{*}}=0$ , the KKT system degenerates and we need to use subgradient. By solving the KKT system, we find that $\\begin{array}{r}{\\frac{d\\lambda(x)}{d x}=0}\\end{array}$ is a valid subgradient. Therefore, by choosing this subgradient, the second term also vanishes. ", "page_idx": 36}, {"type": "text", "text": "Therefore, we do not need to compute the derivative of $\\lambda^{*}$ as the terms involved its derivative is negligible compared to other major terms. ", "page_idx": 36}, {"type": "text", "text": "K Experimental setup ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "All experiments were run on a computing cluster with Dual Intel Xeon Gold 6226 CPUs $@$ 2.7 GHz and DDR4-2933 MHz DRAM. No GPU was used, and we used 1 core with 8GB RAM per instance of the experiment. The cutoff time for running the algorithms is set to be 6 hours. All experiments were run and averaged over 10 different random seeds. All parameters in the constrained bilevel optimization in Section 6, including the objective parameters and the constrain parameters, are randomly generated from a normal distribution with 0-mean and standard deviation 1. ", "page_idx": 36}, {"type": "text", "text": "For our fully first-order algorithm, we implement Algorithm 3, where the inexact gradient oracle subroutine is provided by implementing Algorithm 4. All algorithms are implemented in PyTorch [94] to compute gradients, and using Cvxpy [95] to solve the LL problem and the penalty minimization problem. We implement our fully first-order method based on the solutions returned by Cvxpy with certain accuracy requirement, and use PyTorch to compute the inexact gradient discussed in Section 5. We implement the non-fully first-order method using the CvxpyLayer [18], which is a Cvxpy compatible library that can differentiate through the LL convex optimization problem. ", "page_idx": 36}, {"type": "text", "text": "L Additional Experimental Results ", "text_level": 1, "page_idx": 36}, {"type": "image", "img_path": "eNCYpTCGhr/tmp/f1073ea1c1450546c3e367a9f652e5131497aaeec79eee8c6dd270f2c4e484cb.jpg", "img_caption": ["(a) Convergence and gradient er- (b) Convergence analysis with vary- (c) Computation cost per gradient ror of Fully First-order Constrained ing gradient inexactness $\\alpha$ . We set step of varying problem size $d_{y}$ . We Bilevel Algorithms (F2CBA). We $d_{x}\\,=\\,100$ to measure the tradeoff set $d_{x}=100$ to measure large-scale set $d_{x}=20$ of accuracy and convergence. computation cost. ", "Figure 2: We run Algorithm 3 using Algorithm 4 on the bilevel optimization in the toy example in Problem L.1 with varying upper-level variable dimensions $d_{x}$ , a fixed lower-level variable dimension $d_{y}=200$ , and the number of constraints $n_{\\mathrm{const}}=d_{y}/5=40$ , and accuracy $\\alpha=0.1$ . Figure 2a, Figure 2b, Figure 2c vary # of iterations, gradient exactness $\\alpha$ , and $d_{y}$ , respectively, to compare the performance under different settings. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "We generate instances of the following constrained bilevel optimization problem: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{minimize}_{x}\\;c^{\\top}y^{*}+0.01\\;\\big\\|x\\big\\|^{2}+0.01\\;\\big\\|y^{*}\\big\\|^{2}\\;\\mathrm{~subject~to~}\\;y^{*}=\\operatorname*{arg\\,min}_{y:h(x,y)\\leq0}\\frac{1}{2}y^{\\top}Q y+x^{\\top}P y,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $h_{i}(x,y)\\,=\\,x^{\\top}A_{i}y\\,-\\,b_{i}^{\\top}x\\ \\forall i\\,\\in\\,[d_{h}]$ is a $d_{h}$ -dim bilinear constraint, where the constraint bilinear matrix $A_{i}\\;\\in\\;\\mathbb{R}^{d_{x}\\times d_{y}}$ , $b_{i}\\ \\in\\ \\mathbb{R}^{d_{x}}$ for all $i\\ \\in\\ [d_{h}]$ are randomly generated from normal distributions. The bilinear (nonlinear) constraint of the lower-level problem is the major difference compared to the experiment in Section 6. We are interested in whether our algorithms work beyond the linear constraints where our theory guarantees. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "The rest of the parameters are the same as in Section 6. The PSD matrix $Q\\in\\mathbb{R}^{d_{y}\\times d_{y}}$ , $c\\in\\mathbb{R}^{d_{y}}$ , $P\\in\\mathbb{R}^{d_{x}\\times d_{y}}$ . We compare our Algorithm 3 with a non-fully first-order method implemented using cvxpyLayer [18]. Both algorithms use Adam [90] to control the learning rate in gradient descent. All the experiments are averaged over ten random seeds. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide algorithms and corresponding theoretical guarantees for all our claims in the abstract. We provide experiments (and relevant code) as claimed. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: This is discussed in Section 7 ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 37}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The assumptions, theorem statements, and proof sketches are included in the main paper. The full proofs are included in the appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We provide our full code in the supplemental material, and it can be used to reproduce the experimental results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The code is included in the supplemental material and can be used to reproduce the experiments. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide this information in Appendix K. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide this in Section 6 ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide this information in Appendix K. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: There is no societal impact of this work. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects is involved. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]