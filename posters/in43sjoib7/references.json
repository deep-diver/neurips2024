{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to many of the models discussed and compared in this paper."}, {"fullname_first_author": "Haixu Wu", "paper_title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting", "publication_date": "2021-12-01", "reason": "This paper proposed Autoformer, one of the key models benchmarked against in this paper's experimental evaluation."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-05-01", "reason": "This paper introduced PatchTST, a highly influential Transformer-based time series forecasting model, that is centrally compared against the proposed model in this paper."}, {"fullname_first_author": "Ailing Zeng", "paper_title": "Are transformers effective for time series forecasting?", "publication_date": "2023-05-01", "reason": "This paper is critical in challenging the effectiveness of Transformer-based architectures for long-term time series forecasting, thereby motivating the research presented in this paper."}, {"fullname_first_author": "Shiyu Wang", "paper_title": "TimeMixer: Decomposable multiscale mixing for time series forecasting", "publication_date": "2024-05-01", "reason": "This paper introduced TimeMixer, a state-of-the-art long-term time series forecasting model that serves as a key benchmark for this paper's proposed method."}]}