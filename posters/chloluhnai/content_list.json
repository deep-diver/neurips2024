[{"type": "text", "text": "Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuhang Cai1 Jingfeng Wu1 Song Mei1 Michael Lindsey1,2 Peter L. Bartlett1,3 1UC Berkeley 2Lawrence Berkeley National Laboratory 3Google DeepMind {willcai,uuujf,songmei,lindsey,peter}@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks are mostly optimized by gradient descent (GD) or its variants. Understanding the behavior of GD is one of the key challenges in deep learning theory. However, there is a nonnegligible discrepancy between the GD setups in theory and in practice. In theory, GD is mostly analyzed with relatively small stepsizes such that its dynamics are close to the continuous gradient flow dynamics, although a few exceptions will be discussed later. However, in practice, GD is often used with a relatively large stepsize, with behaviors significantly deviating from that of small stepsize GD or gradient flow. Specifically, notice that small stepsize GD (hence also gradient flow) induces monotonically decreasing empirical risk, but in practice, good optimization and generalization performance is usually achieved when the stepsize is large and the empirical risk oscillates [see $\\mathrm{Wu}$ and Ma, 2018, Cohen et al., 2020, for example]. Therefore, it is unclear which of the theoretical insights drawn from analyzing small stepsize GD apply to large stepsize GD used practically. ", "page_idx": 0}, {"type": "text", "text": "The behavior of small stepsize GD is relatively well understood. For instance, classical optimization theory suggests that GD minimizes convex and $L$ -smooth functions if the stepsize $\\tilde{\\eta}$ is well below $2/L$ , with a convergence rate of $\\mathcal{O}(1/(\\tilde{\\eta}t))$ , where $t$ is the number of steps [Nesterov, 2018]. More recently, Soudry et al. [2018], Ji and Telgarsky [2018] show an implicit bias of small stepsize GD in logistic regression with separable data, where the direction of the GD iterates converges to the max-margin direction. Subsequent works extend their implicit bias theory from linear model to homogenous networks [Lyu and Li, 2020, Chizat and Bach, 2020, Ji and Telgarsky, 2020]. These theoretical results all assume the stepsize of GD is small (and even infinitesimal) such that the empirical risk decreases monotonically and, therefore cannot be directly applied to large stepsize GD used in practice. ", "page_idx": 0}, {"type": "image", "img_path": "chLoLUHnai/tmp/303e5d8038fd75e4bfa352a9f2b245ee5475b9d0479ad0412f59a0dd7988a5ee.jpg", "img_caption": ["Figure 1: The behavior of (GD) for optimizing a non-homogenous four-layer MLP with GELU activation function on a subset of CIFAR-10 dataset. We randomly sample $6,000$ data with labels \u201cairplane\u201d and \u201cautomobile\u201d from CIFAR-10 dataset. The normalized margin is defined as $\\begin{array}{r}{\\operatorname*{min}_{i\\in[n]}\\mathbf{\\bar{\\}{\\}}y_{i}f(\\mathbf{w}_{t};\\mathbf{x}_{i})/\\|\\mathbf{w}_{t}\\|^{4}}\\end{array}$ , which is close to (3). The blue curves correspond to GD with a large stepsize $\\tilde{\\eta}=0.2$ , where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. The orange curves correspond to GD with a small stepsize $\\tilde{\\eta}=0.005$ , where the empirical risk decreases monotonically. Furthermore, Figure 1(b) suggests the normalized margins of both two curves increase and converge in the stable phases. Finally, Figure 1(c) suggests that large stepsize achieves a better test accuracy, consistent with larger-scale learning experiment [Hoffer et al., 2017, Goyal et al., 2017]. More details can be found in Section 5. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "More recently, large stepsize GD that induces oscillatory risk has been analyzed in simplified setups [see Ahn et al., 2023, Zhu et al., 2022, Kreisler et al., 2023, Chen and Bruna, 2023, Wang et al., 2022a, Wu et al., 2023, 2024, for an incomplete list of references]. In particular, in logistic regression with linearly separable data, Wu et al. [2023] showed that the implicit bias of GD (that maximizes the margin) holds not only for small stepsizes [Soudry et al., 2018, Ji and Telgarsky, 2018] but also for an arbitrarily large stepsize. In the same problem, Wu et al. [2024] further showed that large stepsize GD that undergoes risk oscillation can achieve an $\\tilde{\\mathcal{O}}(1/t^{2})$ empirical risk, whereas small stepsize GD that monotonically decreases the empirical risk must suffer from a $\\Omega(1/t)$ empirical risk. Nonetheless, these theories of large stepsize GD are limited to relatively simple setups such as linear models. The theory of large stepsize GD for nonlinear networks is underdeveloped. ", "page_idx": 1}, {"type": "text", "text": "This work fills the gap by providing an analysis of large stepsize GD for nonlinear networks. In the following, we set up our problem formally and summarize our contributions. ", "page_idx": 1}, {"type": "text", "text": "Setup. Consider a binary classification dataset $(\\mathbf{x}_{i},y_{i})_{i=1}^{n}$ , where $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ is a feature vector and $y_{i}\\in\\{\\pm1\\}$ is a binary label. For simplicity, we assume $\\|\\mathbf{x}_{i}\\|\\leq1$ for all $i$ throughout the paper. For a predictor $f$ , the empirical risk under logistic loss is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(\\mathbf{w}):=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})),\\quad\\ell(t):=\\log(1+e^{-t}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, the predictor $f(\\mathbf{w};\\cdot):\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ is parameterized by trainable parameters w and is assumed to be continuously differentiable with respect to w. The predictor is initialized from $\\mathbf{w}_{0}$ and then trained by gradient descent (GD) with a constant stepsize $\\tilde{\\eta}>0$ , that is, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}:=\\mathbf{w}_{t}-\\tilde{\\eta}\\nabla L(\\mathbf{w}_{t}),\\quad t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We are interested in a nonlinear predictor $f$ and a large stepsize $\\tilde{\\eta}$ . A notable example in our theory is two-layer networks with Lipschitz, smooth, and nearly homogenous activations (see (2)). Note that minimizing $L(\\mathbf{w})$ is a non-convex problem in general. ", "page_idx": 1}, {"type": "text", "text": "Observation. Empirically, large stepsize GD often undergoes a phase transition, where the empirical risk defined in (1) oscillates in the first phase but decreases monotonically in the second phase (see empirical evidence in Appendix A in [Cohen et al., 2020] and a formal proof in [Wu et al., 2024] for linear predictors). This is illustrated in Figure 1. We follow Wu et al. [2024] and call the two phases the edge of stability (EoS) phase [name coined by Cohen et al., 2020] and the stable phase, respectively. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We prove the following results for large stepsize GD for training nonlinear predictors under logistic loss. ", "page_idx": 2}, {"type": "text", "text": "1. For Lipschitz and smooth predictor $f$ trained by GD with stepsize $\\tilde{\\eta}$ , we show that as long as the empirical risk is below a threshold depending on $\\tilde{\\eta}$ , GD monotonically decreases the empirical risk (see Theorem 2.2). This result extends the stable phase result in Wu et al. [2024] from linear predictors to nonlinear predictors, demonstrating the generality of the existence of a stable phase. ", "page_idx": 2}, {"type": "text", "text": "2. Assuming that GD enters the stable phase, if in addition the preditor has a bounded homogenous error (see Assumption 1C), we show that the normalized margin induced by GD, $\\begin{array}{r}{\\operatorname*{min}_{i}y_{i}f(\\mathbf{w}_{t};\\mathbf{x}_{i})/\\|\\mathbf{w}_{t}\\|}\\end{array}$ , nearly monotonically increases (see Theorem 2.2). To the best of our knowledge, this is the first characterization of implicit bias of GD for non-homogenous predictors. In particular, our theory covers two-layer networks with commonly used activations functions (which are often non-homogenous) that cannot be covered by existing results [Lyu and Li, 2020, Ji and Telgarsky, 2020, Chizat and Bach, 2020]. ", "page_idx": 2}, {"type": "text", "text": "3. Under additional technical assumptions (the dataset is linearly separable and the derivative of the activation function is bounded away from zero), we show that the initial EoS phase must stop in $\\mathcal{O}(\\tilde{\\eta})$ steps and GD transits to the stable phase afterwards. Furthermore, by choosing a suitably large stepsize, GD achieves a $\\tilde{\\mathcal{O}}(1/t^{2})$ empirical risk after $t$ steps. In comparison, GD that converges monotonically incurs an $\\Omega(1/t)$ risk. This result indicates an optimization benefti of using large stepsize and generalizes the results in [Wu et al., 2024] from linear predictors to neural networks. ", "page_idx": 2}, {"type": "text", "text": "2 Stable Phase and Margin Improvement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our results for the stable phase of large stepsize GD in training nonlinear predictors. Specifically, our results apply to nonlinear predictors that are Lipschitz, smooth, and nearly homogeneous, as described by the following assumption. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Model conditions). Consider a predictor $f(\\mathbf{w};\\mathbf{x}_{i})$ , where $\\mathbf{x}_{i}$ is one of the feature vectors in the training set. ", "page_idx": 2}, {"type": "text", "text": "A. Lipschitzness. Assume there exists $\\rho>0$ such that for every w, $\\begin{array}{r}{\\operatorname*{sup}_{i}\\|\\nabla_{\\mathbf{w}}f(\\mathbf{w};\\mathbf{x}_{i})\\|\\leq\\rho.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "B. Smoothness. Assume there exists $\\beta>0$ such that for all w, $\\mathbf{v}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{w};\\mathbf{x}_{i})-\\nabla f(\\mathbf{v};\\mathbf{x}_{i})\\|\\leq\\beta\\|\\mathbf{w}-\\mathbf{v}\\|,\\quad i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "C. Near homogeneity. Assume there exists $\\kappa>0$ such that for every w, ", "page_idx": 2}, {"type": "equation", "text": "$$\n|f(\\mathbf{w};\\mathbf{x}_{i})-\\langle\\nabla_{\\mathbf{w}}f(\\mathbf{w};\\mathbf{x}_{i}),\\mathbf{w}\\rangle|\\leq\\kappa,\\quad i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Assumptions 1A and 1B are commonly used conditions in the optimization literature. Note that Assumption 1B implies continuous differentiability, thus ruling out networks with ReLU activation function. The continuous differentiability is only used in our current stable phase analysis. We conjecture it can be relaxed using subdifferentiability [Lyu and Li, 2020] for allowing ReLU networks. ", "page_idx": 2}, {"type": "text", "text": "If $\\kappa=0$ , then Assumption 1C requires the predictor to be exactly 1-homogenous. Our Assumption 1C allows the predictor to have a bounded homogenous error. It is clear that linear predictors $f(\\bar{\\mathbf{w}};\\mathbf{x}):=$ $\\mathbf{w}^{\\top}\\mathbf{x}$ satisfy Assumption 1 with $\\rho=\\mathrm{sup}_{i}\\left\\|\\mathbf{x}_{i}\\right\\|\\leq1$ , $\\beta=0$ , and $\\kappa=0$ . Another notable example is two-layer networks given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\mathbf{w};\\mathbf{x}):=\\frac{1}{m}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)}),\\quad\\mathbf{w}^{(j)}\\in\\mathbb{R}^{d},\\quad j=1,\\dots,m,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we assume $a_{j}\\,\\in\\,\\{\\pm1\\}$ are fixed and $\\textbf{w}\\in\\mathbb{R}^{m d}$ , the stack of $(\\mathbf{w}_{--}^{(j)})_{j=1}^{m}$ , are the trainable parameters. We define two-layer networks with the mean-field scaling [Song et al., 2018, Chizat and Bach, 2020, Chen et al., 2022, Suzuki et al., 2023]. However, our results hold for any width. The effect of rescaling the model will be discussed in Section 4. The following example shows that Assumption 1 covers two-layer networks with many commonly used activations $\\phi(\\cdot)$ . The proof is provided in Appendix F.1. ", "page_idx": 2}, {"type": "text", "text": "Example 2.1 (Two-layer networks). Two-layer networks defined in (2) with the following activation functions satisfy Assumption 1 with the described constants: ", "page_idx": 3}, {"type": "text", "text": "\u2022 GELU. $\\phi(x)\\;:=\\;{\\textstyle\\frac{x}{2}}\\mathrm{erf}\\left(1\\,+\\,(x/\\sqrt{2})\\right)$ with $\\kappa\\;=\\;e^{-1/2}/\\sqrt{2\\pi}$ , $\\beta\\;=\\;2/m$ , and $\\rho~=~({\\sqrt{2\\pi}}~+$ $e^{-1/2})/\\sqrt{2\\pi m}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Softplus. $\\phi(x):=\\log(1+e^{x})$ with $\\kappa=\\log2$ , $\\beta=1/m,$ , and $\\rho=1/\\sqrt{m}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 SiLU. $\\phi(x):=x/(1+e^{-x})$ with $\\kappa=1$ , $\\beta=4/m$ , and $\\rho=2/\\sqrt{m}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Huberized ReLU [Chatterji et al., 2021]. For a fixed $h>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(x):=\\left\\{\\begin{array}{l l}{0}&{x<0,}\\\\ {\\frac{x^{2}}{2h}}&{0\\leq x\\leq h,}\\\\ {x-\\frac{h}{2}}&{x>h,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\kappa=h/2,$ , $\\beta=1/(h m)$ , and $\\rho=1/\\sqrt{m}$ . ", "page_idx": 3}, {"type": "text", "text": "Margin for nearly homogenous predictors. For a nearly homogenous predictor $f(\\mathbf{w};\\cdot)$ (see Assumption 1C), we define its normalized margin (or margin for simplicity) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\gamma}(\\mathbf{w}):=\\frac{\\operatorname*{min}_{i\\in[n]}y_{i}f(\\mathbf{w};\\mathbf{x}_{i})}{\\|\\mathbf{w}\\|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A large normalized margin $\\bar{\\gamma}(\\mathbf{w})$ guarantees the prediction of each sample is away from the decision boundary. The normalized margin (3) is introduced by Lyu and Li [2020] for homogenous predictors. However, we show that the same notion is also well-defined for non-homogenous predictors that satisfy Assumption 1C. The next theorem gives sufficient conditions for large stepsize GD to enter the stable phase in training non-homogenous predictors and characterizes the increase of the normalized margin. The proof of Theorem 2.2 is deferred to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2 (Stable phase and margin improvement). Consider (GD) with stepsize $\\tilde{\\eta}$ on a predictor $f(\\mathbf{w};\\mathbf{x})$ that satisfies Assumptions $1A$ and $I B$ . If there exists $r\\geq0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{r})\\leq\\frac{1}{\\tilde{\\eta}(2\\rho^{2}+\\beta)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then GD is in the stable phase for $t\\geq r$ , that is, $(L(\\mathbf{w}_{t}))_{t\\geq r}$ decreases monotonically. If additionally the predictor satisfies Assumption $I C$ and there exists $s\\geq0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\Big\\{\\frac{1}{e^{\\kappa+2}2n},\\;\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}\\Big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "we have the following for $t\\geq s$ : ", "page_idx": 3}, {"type": "text", "text": "\u2022 Risk convergence. $L(\\mathbf{w}_{t})=\\Theta(1/t)$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Parameter increase. $\\|\\mathbf{w}_{t+1}\\|\\geq\\|\\mathbf{w}_{t}\\|$ and $\\|\\mathbf{w}_{t}\\|=\\Theta(\\log(t))$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Margin improvement. There exists a modified margin function $\\gamma^{c}(\\mathbf{w})$ such that ", "page_idx": 3}, {"type": "text", "text": "\u2013 $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is increasing and bounded. \u2013 $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is a multiplicative approximiator of $\\bar{\\gamma}({\\bf w}_{t})$ , that is, there exists $c>0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf{w}_{t})\\leq\\bar{\\gamma}(\\mathbf{w}_{t})\\leq\\bigg(1+\\frac{c}{\\log(1/L(\\mathbf{w}_{t}))}\\bigg)\\gamma^{c}(\\mathbf{w}_{t}),\\quad t\\geq s.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a direct consequence, $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\bar{\\gamma}(\\mathbf{w}_{t})=\\operatorname*{lim}_{t\\rightarrow\\infty}\\gamma^{c}(\\mathbf{w}_{t}).}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.2 shows that for an arbitrarily large stepsize $\\tilde{\\eta}$ , GD must enter the stable phase if the empirical risk falls below a threshold depending on $\\tilde{\\eta}$ given by (4). Furthermore, for nearly homogenous predictors, Theorem 2.2 shows that under a stronger risk threshold condition (5), the risk must converge at a $\\Theta(1/t)$ rate and that the normalized margin nearly monotonically increases. This demonstrates an implicit bias of GD, even when used with a large stepsize and the trained predictor is non-homogenous. ", "page_idx": 3}, {"type": "text", "text": "Limitations. The stable phase conditions in Theorem 2.2 require GD to enter a sublevel set of the empirical risk. However, such a sublevel set might be empty. For instance, let $f(\\mathbf{w};\\mathbf{x})$ be a two-layer network (2) with sigmoid activation. Notice that the predictor is uniformly bounded, $|f(\\mathbf{w};\\mathbf{x})|\\leq1$ , so we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^{n}\\log(1+e^{-y_{i}f(\\mathbf{w};\\mathbf{x}_{i})})\\geq\\log(1+e^{-1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "On the other hand, we can also verify that Assumption 1C is satisfied by $f(\\mathbf{w};\\mathbf{x})$ with $\\kappa=1$ but no smaller $\\kappa$ . Therefore (5) cannot be satisfied. In general, the sublevel set given by the right-hand side of Assumption 1C is non-empty if ", "page_idx": 4}, {"type": "text", "text": "The above condition requires the data can be separated arbitrarily well by some predictor within the hypothesis class. This condition is general and covers (sufficiently large) two-layer networks (2) with many commonly used activations such as GeLU and SiLU. Moreover, although two-layer networks with sigmoid activation violate this condition, they can be modified by adding a leakage to the sigmoid to satisfy the condition. Furthermore, this condition can be satisfied for some nonlinear problems like XOR (or $k$ -parity problems) since they can be realized by two-layer networks. ", "page_idx": 4}, {"type": "text", "text": "In the next section, we will provide sufficient conditions such that large stepsize GD will enter the stable phases characterized by (4) or (5). ", "page_idx": 4}, {"type": "text", "text": "Comparisons to existing works. Our Theorem 2.2 makes several important extensions compared to existing results [Wu et al., 2024, Lyu and Li, 2020, Chizat and Bach, 2020, Ji and Telgarsky, 2020]. First, Theorem 2.2 suggests that the stable phase happens for general nonlinear predictors such as two-layer networks, while the work by Wu et al. [2024] only studied the stable phase for linear predictors. Second, the margin improvement is only known for small (and even infinitesimal) stepsize GD and homogenous predictors [Lyu and Li, 2020, Chizat and Bach, 2020, Ji and Telgarsky, 2020], while we extend this to non-homogenous networks. To the best of our knowledge, Theorem 2.2 is the first implicit bias result covering large stepsize GD and non-homogenous predictors. ", "page_idx": 4}, {"type": "text", "text": "From a technical perspective, our proof uses techniques introduced by Lyu and Li [2020] for analyzing homogenous predictors. Our main innovation is the construction of new auxiliary margin functions that can deal with errors caused by large stepsize and non-homogeneity. More details are discussed in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "3 Edge of Stability Phase ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our stable phase results in Theorem 2.2 require the risk to be below a certain threshold (see (4) and (5)). In this section, we show that the risk can indeed be below the required threshold, even when GD is used with large stepsize. Recall that minimizing the empirical risk with a nonlinear predictor is non-convex, therefore solving it by GD is hard in general. We make additional technical assumptions to conquer the challenges caused by non-convexity. We conjecture that these technical assumptions are not necessary and can be relaxed. ", "page_idx": 4}, {"type": "text", "text": "We focus on two-layer networks (2). We make the following assumptions on the activation function. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Activation function conditions). In the two-layer network (2), let the activation function $\\phi:\\mathbb{R}\\rightarrow\\mathbb{R}$ be continuously differentiable. Moreover, ", "page_idx": 4}, {"type": "text", "text": "A. Derivative condition. Assume there exists $0<\\alpha<1$ such that $\\alpha\\leq|\\phi^{\\prime}(z)|\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "B. Smoothness. Assume there exists $\\tilde{\\beta}>0$ such that for all $x,y\\in\\mathbb{R},\\,|\\phi^{\\prime}(x)-\\phi^{\\prime}(y)|\\leq\\tilde{\\beta}|x-y|.$ . ", "page_idx": 4}, {"type": "text", "text": "C. Near homogeneity. Assume there exists $\\kappa>0$ such that for every $z\\in\\mathbb{R},$ , $|\\phi(z)-\\phi^{\\prime}(z)z|\\leq\\kappa$ . ", "page_idx": 4}, {"type": "text", "text": "Recall that $\\operatorname{sup}_{i}\\|\\mathbf{x}_{i}\\|\\leq1$ . One can then check by direct computation that, under Assumption 2, two-layer networks (2) satisfy Assumption 1 with $\\rho=1/\\sqrt{m},\\beta=\\tilde{\\beta}/m$ , and $\\kappa=\\kappa$ . ", "page_idx": 4}, {"type": "text", "text": "Assumptions 2B and 2C cover many commonly used activation functions. In Assumption 2A, we assume $|\\phi^{\\prime}(z)|\\,\\leq\\,1$ . This is just for the simplicity of presentation and our results can be easily generalized to allow $|\\phi^{\\prime}(z)|\\leq C$ for a constant $C>0$ . The other condition in Assumption 2A, $\\left|\\bar{\\phi}^{\\prime}(z)\\right|\\geq\\alpha$ , however, is non-trivial. This condition is widely used in literature [see Brutzkus et al., 2018, Frei et al., 2021, and references thereafter] to facilitate GD analysis. Technically, this condition guarantees that each neuron in the two-layer network (2) will always receive a non-trivial gradient in the GD update; otherwise, neurons may be frozen during the GD update. Furthermore, commonly used activation functions can be combined with an identity map to satisfy Assumption 2A. This is formalized in the following example. The proof is provided in Appendix F.2. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Example 3.1 (Leaky activation functions). F $i x\\;0.5\\leq c<1$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 Let \u03d5 be GELU, Softplus, or SilU in Example 2.1, then its modification $\\tilde{\\phi}(x):=c x\\!+\\!(1\\!-\\!c)/4\\!\\cdot\\!\\phi(x)$ satisfies Assumption 2 with $\\kappa=1$ , $\\alpha=0.25$ , and $\\tilde{\\beta}=1$ . In particular, the modification of softplus can be viewed as a smoothed leaky ReLU.   \n\u2022 Let $\\phi$ be the Huberized ReLU in Example 2.1, then its modification $\\tilde{\\phi}(x):=c x+(1-c)/4\\cdot\\phi(x)$ satisfies Assumption 2 with $\\kappa=h/2$ , $\\alpha=0.5$ , and $\\tilde{\\beta}=1/4h$ .   \n\u2022 The \u201cleaky\u201d tanh, $\\tilde{\\phi}(x)\\;:=\\;c x\\,+\\,(1\\,-\\,c)\\operatorname{tanh}(x),$ , and the \u201cleaky\u201d sigmoid, $\\tilde{\\phi}(x)\\;:=\\;c x\\;+$ $c/(1+e^{-x}),$ , both satisfy Assumption 2 with $\\kappa=1,\\alpha=0.5$ and $\\tilde{\\beta}=1$ . ", "page_idx": 5}, {"type": "text", "text": "For the technical difficulty of non-convex optimization, we also need to assume a linearly separable dataset to conduct our EoS phase analysis. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Linear separability). Assume there is a margin $\\gamma>0$ and a unit vector $\\mathbf{w}_{*}$ such that $y_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{*}^{\\top}\\geq\\gamma$ for every $i=1,\\hdots,n$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 serves as a sufficient condition for two-layer neural networks, regardless of width, to reach the initial bound of the stable phase under large stepsizes. We remark that our stable phase results do not need this assumption. ", "page_idx": 5}, {"type": "text", "text": "The following theorem shows that when GD is used with large stepsizes, the average risk must decrease even though the risk may oscillate locally. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (The EoS phase for two-layer networks). Under Assumption 3, consider (GD) on two-layer networks (2) that satisfy Assumptions 2A and 2C. Denote the stepsize by $\\tilde{\\eta}:=m\\eta$ , where $m$ is the network width and $\\eta$ can be arbitrarily large. Then for every $t>0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\sum_{k=0}^{t-1}L(\\mathbf{w}_{k})\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta t)/\\alpha^{2}+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}}{\\gamma^{2}\\eta t}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta t}=\\mathcal{O}\\bigg(\\frac{\\log^{2}(\\eta t)+\\eta^{2}}{\\eta t}\\bigg).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 suggests that the average risk of training two-layer networks decreases even when GD is used with large stepsize. Consequently, the risk thresholds (4) and (5) for GD to enter the stable phase must be satisfied after a finite number of steps. This will be discussed in depth in the next section. ", "page_idx": 5}, {"type": "text", "text": "Compared to Theorem 1 in [Wu et al., 2024], Theorem 3.2 extends their EoS phase bound from linear predictors to two-layer networks. ", "page_idx": 5}, {"type": "text", "text": "4 Phase Transition and Fast Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For two-layer networks trained by large stepsize GD, Theorem 3.2 shows that the average risk must decrease over time. Combining this with Theorem 2.2, GD must enter the stable phase in finite steps, and the loss must converge while the normalized margin must improve. ", "page_idx": 5}, {"type": "text", "text": "However, a direct application of Theorem 3.2 only leads to a suboptimal bound on the phase transition time. Motivated by Wu et al. [2024], we establish the following sharp bound on the phase transition time by tracking the gradient potential (see Lemma C.3). The proof is deferred to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Phase transition and stable phase for two-layer networks). Under Assumption 3, consider (GD) on two-layer networks (2) that satisfy Assumption 2. Clearly, the two-layer networks also satisfy Assumption 1 with $\\rho=1/\\sqrt{m},\\,\\beta=\\tilde{\\beta}/m,$ , and $\\kappa=\\kappa$ . Denote the stepsize by $\\tilde{\\eta}:=m\\eta,$ , where m is the network width and $\\eta>0$ can be arbitrarily large. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Phase transition time. There exists $s\\leq\\tau$ such that (5) in Theorem 2.2 holds, where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau:=\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\operatorname*{max}\\bigg\\{c_{1}\\eta,c_{2}n,e,\\frac{c_{2}\\eta+c_{1}n}{\\eta}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta},\\frac{(c_{2}\\eta+c_{1}n)}{\\eta}\\cdot\\frac{\\|\\mathbf{w}_{0}\\|}{\\sqrt{m}}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{1}:=4e^{\\kappa+2}$ and $c_{2}:=(8+4\\tilde{\\beta})$ . Therefore (GD) is in the stable phase from s onwards. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Explicit risk bound in the stable phase. We have $(L(\\mathbf{w}_{t}))_{t\\geq s}$ monotonically decreases and ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{t})\\leq\\frac{2}{\\alpha^{2}\\gamma^{2}\\eta(t-s)},\\quad t\\geq s.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorems 2.2, 3.2 and 4.1 together characterize the behaviors of large stepsize GD in training twolayer networks. Specifically, large stepsize GD may induce an oscillatory risk in the beginning; but the averaged empirical risk must decrease (Theorem 3.2). After the empirical risk falls below a certain stepsize-dependent threshold, GD enters the stable phase, where the risk decreases monotonically (Theorem 4.1). Finally, the normalized margin (3) induced by GD increases nearly monotonically as GD stays in the stable phase (Theorem 2.2). ", "page_idx": 6}, {"type": "text", "text": "Our intuition behind the phase transition phenomenon is as follows. The initial EoS phase occurs when $\\mathrm{gGD}$ oscillates within a steep valley, transitioning to a stable phase once it navigates into a flatter valley. We believe this insight generalizes to broader nonlinear models. Moreover, our theory of large step sizes aligns with the celebrated flat minima intuition [Keskar et al., 2016]. ", "page_idx": 6}, {"type": "text", "text": "Fast optimization. Our bounds for two-layer networks are comparable to those for linear predictors shown by Wu et al. [2024]. Specifically, when used with a larger stepsize, GD achieves a faster optimization in the stable phase but stays longer in the EoS phase. Choosing a suitably large stepsize that balances the steps in EoS and stable phases, we obtain an accelerated empirical risk in the following corollary. The proof is included in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2 (Acceleration of large stepsize). Under the same setup as in Theorem 4.1, consider (GD) with a given budget of $T$ steps such that ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq\\frac{256(1+4\\kappa)}{\\alpha^{2}\\gamma^{2}}\\operatorname*{max}\\bigg\\{c_{1}n,~4c_{2}^{2},~\\frac{2c_{2}\\|\\mathbf{w}_{0}\\|}{\\sqrt{m}}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{1}:=4e^{\\kappa+2}$ and $c_{2}:=(8+4\\tilde{\\beta})$ are as in Theorem 4.1. Then for stepsize $\\tilde{\\eta}:=\\eta m$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta:=\\frac{\\alpha^{2}\\gamma^{2}}{256(1+4\\kappa)c_{2}}T,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we have $\\tau\\leq T/2$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\nL({\\bf w}_{T})\\le\\frac{2048(1+4\\kappa)c_{2}}{\\alpha^{4}\\gamma^{4}}\\cdot\\frac{1}{T^{2}}=\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 and Corollary 4.2 extend Theorem 1 and Corollary 2 in Wu et al. [2024] from linear predictors to two-layer networks. Another notable difference is that we obtain a sharper stable phase bound (and thus a better acceleration bound) compared to theirs, where we remove a logarithmic factor through a more careful analysis. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2 suggests an accelerated risk bound of $\\mathcal{O}(1/T^{2})$ by choosing a large stepsize that balances EosS and stable phases. We also show the following lower bound, showing that such acceleration is impossible if (GD) does not enter the EoS phase. The proof is included in Appendix C.3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (Lower bound in the classical regime). Consider (GD) with initialization $\\mathbf{w}_{0}=0$ and stepsize $\\tilde{\\eta}>0$ for a two-layer network (2) satisfying Assumption 2. Suppose the training set is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{x}_{1}=(\\gamma,\\sqrt{1-\\gamma^{2}}),\\quad\\mathbf{x}_{2}=(\\gamma,-\\sqrt{1-\\gamma^{2}}/2),\\quad y_{1}=y_{2}=1,\\quad0<\\gamma<0.1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is clear that $(\\mathbf{x}_{i},y_{i})_{i=1,2}$ satisfy Assumption 3. If $\\mathbf{\\nabla}\\left(L\\left(\\mathbf{w}_{t}\\right)\\right)_{t\\geq0}$ is non-increasing, then ", "page_idx": 6}, {"type": "equation", "text": "$$\nL\\left(\\mathbf{w}_{t}\\right)\\geq c_{0}/t,\\quad t\\geq1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{0}>0$ is a function of $(\\alpha,\\phi,{\\bf x}_{1},{\\bf x}_{2},\\gamma,\\kappa,\\beta)$ but is independent of $t$ and $\\tilde{\\eta}$ . ", "page_idx": 6}, {"type": "text", "text": "Effect of model rescaling. We conclude this section by discussing the impact of rescaling the model. Specifically, we replace the two-layer network in the mean-field scaling (2) by the following ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\mathbf{w};\\mathbf{x}):=b\\cdot\\frac{1}{m}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and evaluate the impact of the scaling factor $b$ on our results. By choosing the optimal stepsize that balances the EoS and stable phases as in Corollary 4.2, we optimize the risk bound obtained by GD with a fixed budget of $T$ steps and get the following bound. Detailed derivations are deferred to Appendix D. ", "page_idx": 7}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{T})=\\left\\{\\mathcal{O}(1/T^{2})\\begin{array}{l l}{\\mathrm{~if~}b\\geq1,}\\\\ {\\mathrm{~if~}b<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This suggests that as long as $b\\geq1$ , we get the same acceleration effect. In particular, the mean-field scalin\u221ag $b=1$ [Song et al., 2018, Chizat and Bach, 2020] and the neural tangent kernel (NTK) scaling $b={\\sqrt{m}}$ [Du et al., 2018, Jacot et al., 2018] give the same acceleration effect. An NTK analysis of large stepsize is included in [Wu et al., 2024] and their conclusion is consistent with ours. Finally, we remark that our analysis holds for any width $m$ and uses techniques different from the mean-field or NTK methods. However, our acceleration analysis only allows linearly separable datasets. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct three sets of experiments to validate our theoretical insights. In the first set, we use a subset of the CIFAR-10 dataset [Krizhevsky et al., 2009], which includes 6,000 randomly selected samples from the \u201cairplane\u201d and \u201cautomobile\u201d classes. Our model is a multilayer perceptron (MLP) with four trainable layers and GELU activation functions, with a hidden dimension of 200 for each hidden layer. The MLP is trained using gradient descent with random initialization, as described in (GD). The results are shown in Figures 1(a) to 1(c). ", "page_idx": 7}, {"type": "text", "text": "In the second set of experiments, we consider an XOR dataset consisting of four samples: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{x}_{1}=(-1,-1),y_{1}=1;\\;\\mathbf{x}_{2}=(1,1),y_{2}=1;\\;\\mathbf{x}_{3}=(1,-1),y_{3}=-1;\\;\\mathbf{x}_{4}=(-1,1),y_{4}=-1.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above XOR dataset is not linearly separable. We test (GD) with different stepsizes on a two-layer network (2) with the leaky softplus activation (see Example 3.1 with $c=0.5)$ ). The network width is $m=20$ . The initialization is random. The results are presented in Figures 2(a) to 2(c). ", "page_idx": 7}, {"type": "text", "text": "In the third set of experiments, we consider the same task as in the first set of experiments, but we test (GD) with different stepsizes on a two-layer network (2) with the softplus activation. The network width is $m=40$ . The initialization is random. The results are presented in Figures 2(d) to 2(f). ", "page_idx": 7}, {"type": "text", "text": "Margin improvement. Figures 1(b), 2(c) and 2(f) show that the normalized margin nearly monotonically increases once gradient descent (GD) enters the stable phase, regardless of step size. This observation aligns with our theoretical findings in Theorem 2.2. ", "page_idx": 7}, {"type": "text", "text": "Fast optimization. From Figures 1(a), 2(a) and 2(d), we observe that after GD enters the stable phase, a larger stepsize consistently leads to a smaller empirical risk compared to the smaller stepsizes, which is consistent with our Theorem 4.1 and Corollary 4.2. Besides, Figures 2(b) and 2(e) suggest that, asymptotically, GD converges at a rate of $\\mathcal{O}(1/(\\dot{\\tilde{\\eta}}t))=\\mathcal{O}(1/(\\eta t))$ (The width of networks is fixed), which verifies the sharpness of our stable phase bound in Theorem 4.1. ", "page_idx": 7}, {"type": "text", "text": "Margin of individual neurons. It is important to note that while the normalized margin behaves as expected, the margin for individual neurons may not increase and can remain negative, even when the dataset is linearly separable. A detailed example illustrating this is provided in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss related papers. ", "page_idx": 7}, {"type": "image", "img_path": "chLoLUHnai/tmp/e753cb49aafff66d27339bff222f15b77b9d617c617e2b7dea37a6fba8d5561a.jpg", "img_caption": ["Figure 2: Behavior of (GD) for two-layer networks (2) with leaky softplus activation function (see Example 3.1 with $c=0.5)$ ). We consider an XOR dataset and a subset of CIFAR-10 dataset. In both cases, we observe that (1) GD with a large stepsize achieves a faster optimization compared to GD with a small stepsize, (2) the asymptotic convergence rate of the empirical risk is $\\mathcal{O}(1/(\\bar{\\tilde{\\eta}}t))$ , and (3) in the stable phase, the normalized margin (nearly) monotonically increases. These observations are consistent with our theoretical understanding of large stepsize GD. More details about the experiments are explained in Section 5. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Small stepsize and implicit bias. For logistic regression on linearly separable data, Soudry et al. [2018], Ji and Telgarsky [2018] showed that the direction of small stepsize GD converges to the max-margin solution. Their results were later extended by Gunasekar et al. [2017, 2018], Nacson et al. [2019c,a,b], Ji et al. [2021], Lyu and Li [2020], Ji and Telgarsky [2020], Chizat and Bach [2020], Chatterji et al. [2021], Kunin et al. [2022] to other algorithms and non-linear models. However, in all of their analysis, the stepsize of GD needs to be small such that the empirical risk decreases monotonically. In contrast, our focus is GD with a large stepsize that induces non-monotonic risk. ", "page_idx": 8}, {"type": "text", "text": "Two papers [Nacson et al., 2019a, Kunin et al., 2022] studied margin maximization theory for a special form of non-homogenous models. Specifically, when viewed in terms of different subsets of the trainable parameters, the model is homogeneous, although the order of homogeneity may vary. Compared to their setting, our non-homogenous models only require a bounded homogenous error (see Assumption 1C). Therefore, our theory can cover two-layer networks (2) with non-homogeneous activations such as GELU and SiLU that cannot be covered by [Nacson et al., 2019a, Kunin et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "Large stepsize and EoS. In practice, large stepsizes are often preferred when using GD to train neural networks to achieve effective optimization and generalization performance [see Wu and Ma, 2018, Cohen et al., 2020, Barrett and Dherin, 2020, and references therein]. In such scenarios, the empirical risk often oscillates in the beginning. This phenomenon is named edge of stability (EoS) by Cohen et al. [2020]. The theory of EoS is mainly studied in relatively simplified cases such as oneor two-dimensional functions [Zhu et al., 2022, Chen and Bruna, 2023, Ahn et al., 2022, Kreisler et al., 2023, Wang et al., 2023], linear model [Wu et al., 2023, 2024], matrix factorization [Wang et al., 2022a, Chen and Bruna, 2023], scale-invariant networks [Lyu et al., 2022], linear networks under MSE loss [Ren et al., 2024, Andriushchenko et al., 2023], for an incomplete list of references. Compared to them, we focus on a more practical setup of training two-layer non-linear networks with large stepsize GD. There are some general theories of EoS subject to subtle assumptions [for ", "page_idx": 8}, {"type": "text", "text": "In what follows, we make a detailed discussion about papers that directly motivate our work [Lyu and Li, 2020, Ji and Telgarsky, 2020, Chatterji et al., 2021, Wu et al., 2024]. ", "page_idx": 9}, {"type": "text", "text": "Comparison with Lyu and Li [2020], Ji and Telgarsky [2020]. Both results in [Lyu and Li, 2020, Ji and Telgarsky, 2020] focused on $L$ -homogenous networks. Specifically, Lyu and Li [2020] showed that a modified version of normalized margin (see (3)) induced by GD with small stepsize (such that the risk decreases monotonically) increases, with limiting points of $\\{\\mathbf{w}_{t}/\\|\\mathbf{w}_{t}\\|\\}_{t=1}^{\\infty}$ converging to KKT points of a margin-maximization problem. Under additional o-minimal conditions, Ji and Telgarsky [2020] showed that gradient flow converges in direction. Our work is different from theirs in two aspects. First, we allow GD with a large stepsize that may cause risk oscillation. Second, our theory covers non-homogenous predictors, which include two-layer networks with many commonly used activation functions beyond the scope of [Lyu and Li, 2020, Ji and Telgarsky, 2020]. Compared to Lyu and Li [2020], Ji and Telgarsky [2020], we only show the improvement of the margin, and our theory is limited to nearly 1-homogenous predictors (Assumption 2C). It remains open to show directional convergence and to extend our near 1-homogenity condition to a \u201cnear $L$ -homogeneity\u201d condition for a general $L$ . ", "page_idx": 9}, {"type": "text", "text": "Comparison with Chatterji et al. [2021]. The work by Chatterji et al. [2021] studies the convergence of GD in training deep networks under logistic loss. Their results are related to ours as we both consider networks with nearly homogeneous activations and we both have a stable phase analysis (although this is not explicitly mentioned in their paper). However, our results are significantly different from theirs. Specifically, in our notation, they require the homogenous error $\\kappa$ (see Assumption 2C) to be smaller than $\\mathcal{O}(\\mathrm{log}({1/L(\\mathbf{w}_{s})})/{\\|\\mathbf{w}_{s}\\|})^{\\prime}\\approx\\bar{\\mathcal{O}}(\\bar{\\gamma}(\\mathbf{w}_{s}))$ , where $s$ is the time for GD to enter the stable phase. Note that the margin when GD enters the stable phase could be arbitrarily small. In comparison, we only require the homogenous error to be bounded by a constant. As a consequence, we can handle many commonly used activation functions (see Example 2.1) while they can only handle the Huberized ReLU with a small $h$ in Example 2.1. Moreover, they require the stepsize $\\tilde{\\eta}$ to be smaller than $O(\\kappa/\\|\\mathbf{w}_{s}\\|^{8})$ , thus they only allow very small stepsize. In contrast, we allow $\\tilde{\\eta}$ to be arbitrarily large. ", "page_idx": 9}, {"type": "text", "text": "Comparison with Wu et al. [2023, 2024]. The works by Wu et al. [2023, 2024] directly motivate our paper. In particular, for logistic regression on linearly separable data, Wu et al. [2023] showed margin maximization of GD with large stepsize and Wu et al. [2024] showed fast optimization of GD with large stepsize. Our work can be viewed as an extension of [Wu et al., 2023, 2024] from linear predictors to non-linear predictors such as two-layer networks. Besides, our results for margin improvement and convergence within the stable phase (Theorem 2.2) hold for the general dataset, while their results strongly rely on the linear separability of the dataset. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provide a theory of large stepsize gradient descent (GD) for training non-homogeneous predictors such as two-layer networks using the logistic loss function. Our analysis explains the empirical observations: large stepsize GD often reveals two distinct phases in the training process, where the empirical risk oscillates in the beginning but decreases monotonically subsequently. We show that the phase transition happens because the average empirical risk decreases despite the risk oscillation. In addition, we show that large stepsize GD improves the normalized margin in the long run, which extends the existing implicit bias theory for homogenous predictors to non-homogenous predictors. Finally, we show that large stepsize GD, by entering the initial oscillatory phase, achieves acceleration when minimizing the empirical risk. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Fabian Pedregosa for his suggestions on an early draft and Jason D. Lee and Kaifeng Lyu for their comments clarifying the applicability of our result to sigmoid networks. We gratefully acknowledge the support of the NSF for FODSI through grant DMS-2023505, of the NSF and the ", "page_idx": 9}, {"type": "text", "text": "Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639, and of the ONR through MURI award N000142112431. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra. Understanding the unstable convergence of gradient descent. In International Conference on Machine Learning, pages 247\u2013257. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via edge of stability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nMaksym Andriushchenko, Aditya Vardhan Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step sizes learns sparse features. In International Conference on Machine Learning, pages 903\u2013925. PMLR, 2023.   \nDavid GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint arXiv:2009.11162, 2020.   \nAlon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. In International Conference on Learning Representations, 2018.   \nNiladri S Chatterji, Philip M Long, and Peter Bartlett. When does gradient descent with logistic loss interpolate using deep networks with smoothed relu activations? In Conference on Learning Theory, pages 927\u20131027. PMLR, 2021.   \nFan Chen, Zhenjie Ren, and Songbo Wang. Uniform-in-time propagation of chaos for mean field langevin dynamics. arXiv preprint arXiv:2212.03050, 2022.   \nLei Chen and Joan Bruna. Beyond the edge of stability via two-step gradient updates. In International Conference on Machine Learning, pages 4330\u20134391. PMLR, 2023.   \nLenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on learning theory, pages 1305\u20131338. PMLR, 2020.   \nJeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2020.   \nAlex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In The Eleventh International Conference on Learning Representations, 2022.   \nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018.   \nSpencer Frei, Yuan Cao, and Quanquan Gu. Provable generalization of sgd-trained neural networks of any width in the presence of adversarial label noise. In International Conference on Machine Learning, pages 3427\u20133438. PMLR, 2021.   \nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.   \nSuriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017.   \nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in Neural Information Processing Systems, 31, 2018.   \nElad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: Closing the generalization gap in large batch training of neural networks. Advances in Neural Information Processing Systems, 30, 2017.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \nZiwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.   \nZiwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176\u201317186, 2020.   \nZiwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860\u20134869. PMLR, 2021.   \nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations, 2016.   \nLingkai Kong and Molei Tao. Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function. Advances in Neural Information Processing Systems, 33:2625\u20132638, 2020.   \nItai Kreisler, Mor Shpigel Nacson, Daniel Soudry, and Yair Carmon. Gradient descent monotonically decreases the sharpness of gradient flow solutions in scalar networks and beyond. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17684\u201317744. PMLR, 23\u201329 Jul 2023.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.   \nDaniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli. The asymmetric maximum margin bias of quasi-homogeneous neural networks. In The Eleventh International Conference on Learning Representations, 2022.   \nMiao Lu, Beining Wu, Xiaodong Yang, and Difan Zou. Benign oscillation of stochastic gradient descent with large learning rate. In The Twelfth International Conference on Learning Representations, 2023.   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020.   \nKaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. Advances in Neural Information Processing Systems, 35: 34689\u201334708, 2022.   \nChao Ma, Daniel Kunin, Lei Wu, and Lexing Ying. Beyond the quadratic approximation: The multiscale structure of neural network loss landscapes. arXiv preprint arXiv:2204.11326, 2022.   \nMor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In International Conference on Machine Learning, pages 4683\u20134692. PMLR, 2019a.   \nMor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420\u20133428. PMLR, 2019b.   \nMor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3051\u20133059. PMLR, 2019c.   \nYurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd edition, 2018. ISBN 3319915770.   \nAlbert BJ Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata. New York, NY, 1962.   \nYinuo Ren, Chao Ma, and Lexing Ying. Understanding the generalization benefits of late learning rate decay. In International Conference on Artificial Intelligence and Statistics, pages 4465\u20134473. PMLR, 2024.   \nMei Song, Andrea Montanari, and P Nguyen. A mean field view of the landscape of two-layers neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.   \nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1): 2822\u20132878, 2018.   \nTaiji Suzuki, Denny Wu, and Atsushi Nitanda. Convergence of mean-field langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction. arXiv preprint arXiv:2306.07221, 2023.   \nYuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homogeneity: Convergence and balancing effect. In International Conference on Learning Representations, 2022a.   \nYuqing Wang, Zhenghao Xu, Tuo Zhao, and Molei Tao. Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. arXiv preprint arXiv:2310.17087, 2023.   \nZixuan Wang, Zhouzi Li, and Jian Li. Analyzing sharpness along GD trajectory: Progressive sharpening and edge of stability. Advances in Neural Information Processing Systems, 35:9983\u2013 9994, 2022b.   \nJingfeng Wu, Vladimir Braverman, and Jason D. Lee. Implicit bias of gradient descent for logistic regression at the edge of stability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nJingfeng Wu, Peter L Bartlett, Matus Telgarsky, and Bin Yu. Large stepsize gradient descent for logistic loss: Non-monotonicity of the loss improves optimization efficiency. Conference on Learning Theory, 2024.   \nLei Wu and Chao Ma. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. Advances in Neural Information Processing Systems, 31, 2018.   \nXingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, and Rong Ge. Understanding edge-of-stability training dynamics with a minimalist example. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Stable Phase Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will prove results for a general smooth predictor $f(\\mathbf{w};\\mathbf{x})$ under the logistic loss in the stable phase. Before the proof, we introduce some notations here. ", "page_idx": 13}, {"type": "text", "text": "Notation. We use the following notation to simplify the presentation. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ q_{i}(t):=y_{i}f(\\mathbf w_{t};\\mathbf x_{i}),\\ q_{\\operatorname*{min}}(t):=\\operatorname*{min}_{i\\in[n]}q_{i}(t).}\\\\ &{\\bullet\\ L_{t}:=L(\\mathbf w_{t}),\\ \\rho_{t}:=||\\mathbf w_{t}||_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we have the following expression: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{t})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(q_{i}(t)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, we give a summary of this section. The proofs are organized into 5 parts. ", "page_idx": 13}, {"type": "text", "text": "\u2022 In Appendix A.1, we characterize the decrease of loss $L_{t}$ .   \n\u2022 In Appendix A.2, we characterize the change of the parameter norm $\\rho_{t}$ .   \n\u2022 In Appendix A.3, we show the convergence of the normalized margin $\\bar{\\gamma}({\\bf w}_{t})$ .   \n\u2022 In Appendix A.4, we characterize the sharp rates of loss $L_{t}$ and parameter norm $\\rho_{t}$ .   \n\u2022 In Appendix A.5, we give the proof of Theorem 2.2. ", "page_idx": 13}, {"type": "text", "text": "A.1 Decrease of the Loss ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will show that the loss $L_{t}$ decreases monotonically in the stable phase. To begin with, we introduce the following definition which is another characterization of $\\beta$ -smoothness. ", "page_idx": 13}, {"type": "text", "text": "Definition 1 (Linearization error). Given a continuously differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and two points w, $\\mathbf{v}\\in\\mathbb{R}^{d}$ , the linearization error of $f(\\mathbf{v})$ with respect to w is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\xi[f]({\\mathbf{w}},{\\mathbf{v}}):=f({\\mathbf{v}})-f({\\mathbf{w}})-\\nabla f({\\mathbf{w}})^{\\top}({\\mathbf{v}}-{\\mathbf{w}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a $\\beta$ -smooth function, standard convex optimization theory gives the following linearization error bound. ", "page_idx": 13}, {"type": "text", "text": "Fact A.1 (Linearization error of $\\beta$ -smooth function). For a $\\beta$ -smooth function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}.$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\xi[f]({\\mathbf w},{\\mathbf v}):=f({\\mathbf v})-f({\\mathbf w})-\\nabla f({\\mathbf w})^{\\top}({\\mathbf v}-{\\mathbf w})\\leq\\frac{\\beta}{2}\\|{\\mathbf v}-{\\mathbf w}\\|_{2}^{2},\\ \\ \\,f o r\\,e\\nu e r y\\,{\\mathbf w}\\,a n d\\,{\\mathbf v}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first show a stable phase bound for general smooth and Lipschitz predictors. The following is an extension of Lemma 10 in [Wu et al., 2024]. Since we do not require $f$ to be twice differentiable, extra efforts are needed. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2 (Self-boundedness of logistic loss). For the logistic loss $\\ell(z):=\\log(1+\\exp(-z))$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\ell(z)-\\ell(x)-\\ell^{\\prime}(x)(z-x)\\leq2\\ell(x)(z-x)^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for $\\cdot\\left|z-x\\right|<1$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.2. See the proof of Proposition 5 in [Wu et al., 2024]. The lower bound is by the convexity of $\\ell(\\cdot)$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "The next lemma controls the decrease of the risk $L_{t}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.3 (Decrease of $L_{t}$ ). Suppose Assumptions $1A$ and $I B$ hold. If $\\begin{array}{r}{L(\\mathbf{w}_{t})\\leq\\frac{1}{\\tilde{\\eta}\\rho^{2}}}\\end{array}$ , then we have $\\begin{array}{r}{-\\tilde{\\eta}(1+\\beta\\tilde{\\eta}L(\\mathbf w_{t}))\\|\\nabla L(\\mathbf w_{t})\\|^{2}\\leq L(\\mathbf w_{t+1})-L(\\mathbf w_{t})\\leq-\\tilde{\\eta}(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L(\\mathbf w_{t}))\\|\\nabla L(\\mathbf w_{t})\\|^{2}.}\\end{array}$ Particularly, this indicates that if $\\begin{array}{r}{L(\\mathbf{w}_{t})\\leq\\frac{1}{\\tilde{\\eta}(2\\rho^{2}+\\beta)}}\\end{array}$ , then $L(\\mathbf{w}_{t+1})\\leq L(\\mathbf{w}_{t})$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.3. By Assumptions 1A and 1B, we have $\\|\\nabla f\\|_{2}\\leq\\rho$ and $f(\\mathbf{w};\\mathbf{x})$ is $\\beta$ -smooth as a function of w. Therefore, for every $i\\in[n]$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{q_{i}(t+1)-q_{i}(t)|=|y_{i}(f\\left(\\mathbf w_{t+1};\\mathbf x_{i}\\right)-f\\left(\\mathbf w_{t};\\mathbf x_{i}\\right))|}&{}\\\\ {=\\left|\\nabla f\\left(\\mathbf w_{t}+\\theta\\left(\\mathbf w_{t+1}-\\mathbf w_{t}\\right);\\mathbf x_{i}\\right)^{\\top}\\left(\\mathbf w_{t+1}-\\mathbf w_{t}\\right)\\right|}&{\\mathrm{by~intermediate~value~theoren}}\\\\ &{\\le\\rho\\left\\|\\mathbf w_{t+1}-\\mathbf w_{t}\\right\\|}&\\\\ &{\\le\\rho\\tilde{\\eta}\\|\\nabla L_{t}\\|}&{\\mathrm{since~}\\mathbf w_{t+1}=\\mathbf w_{t}-\\tilde{\\eta}\\nabla L_{t}}\\\\ &{\\le\\rho^{2}\\tilde{\\eta}L_{t}\\le1.}&{\\mathrm{since~}\\|\\nabla L_{t}\\|\\le L_{t}\\rho}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then by Lemma A.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(q_{i}(t+1))\\leq\\ell(q_{i}(t))+\\ell^{\\prime}(q_{i}(t))(q_{i}(t+1)-q_{i}(t))+2\\ell(q_{i}(t))(q_{i}(t+1)-q_{i}(t))^{2}}\\\\ &{\\qquad\\qquad\\leq\\ell(q_{i}(t))+\\ell^{\\prime}(q_{i}(t))\\langle y_{i}\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t+1}-{\\mathbf w}_{t}\\rangle+|\\ell^{\\prime}(q_{i}(t))|\\cdot|\\xi[f]({\\mathbf w}_{t},{\\mathbf w}_{t+1})|}\\\\ &{\\qquad\\qquad+\\,2\\ell(q_{i}(t))(q_{i}(t+1)-q_{i}(t))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{since~}q_{i}(t+1)-q_{i}(t)=\\langle y_{i}\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t+1}-{\\mathbf w}_{t}\\rangle+y_{i}\\xi[f]({\\mathbf w}_{t},{\\mathbf w}_{t+1})}\\\\ &{\\qquad\\qquad\\leq\\ell(q_{i}(t))+\\ell^{\\prime}(q_{i}(t))\\langle y_{i}\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t+1}-{\\mathbf w}_{t}\\rangle+\\ell(q_{i}(t))(\\beta+2\\rho^{2})\\|{\\mathbf w}_{t+1}-{\\mathbf w}_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking an average over all data points, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{t+1}\\leq L_{t}-\\tilde{\\eta}\\|\\nabla L_{t}\\|^{2}+(2\\rho^{2}+\\beta)\\tilde{\\eta}^{2}L_{t}\\|\\nabla L_{t}\\|^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}\\leq-\\tilde{\\eta}(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t})\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We complete the proof of the right hand side inequality. The left hand side inequality can be proved similarly. In detail, we can show that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(q_{i}(t+1))\\geq\\ell(q_{i}(t))+\\ell^{\\prime}(q_{i}(t))(q_{i}(t+1)-q_{i}(t)}\\\\ &{\\qquad\\qquad\\geq\\ell(q_{i}(t))+\\ell^{\\prime}(q_{i}(t))\\langle y_{i}\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t+1}-{\\mathbf w}_{t}\\rangle-|\\ell^{\\prime}(q_{i}(t))|\\cdot|\\xi[f]({\\mathbf w}_{t},{\\mathbf w}_{t+1})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking the average over all data points, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{t+1}\\geq L_{t}-\\tilde{\\eta}(1+\\beta\\tilde{\\eta}L_{t})\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we have completed the proof. ", "page_idx": 14}, {"type": "text", "text": "A.2 Increase of the Parameter Norm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we demonstrate that the parameter norm, $\\rho_{t}$ , increases monotonically during the stable phase. We introduce a crucial quantity, $v_{t}$ , defined as the inner product of the gradient and the negative weight vector: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{t}:=\\langle\\nabla L(\\mathbf{w}_{t}),-\\mathbf{w}_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This quantity, $v_{t}$ , plays a key role in controlling the increase of the parameter norm. Notably, $v_{t}$ appears as the cross term in the expression $\\|\\mathbf{w}_{t+1}\\|^{2}=\\|\\mathbf{w}_{t}-\\tilde{\\eta}\\nabla L(\\mathbf{w}_{t}^{\\bullet})\\|^{2}$ . By managing $v_{t}$ , we can effectively characterize the increase in the parameter norm. ", "page_idx": 14}, {"type": "text", "text": "Recall that our loss function is $\\ell(x):=\\log(1+e^{-x})$ . Inspired by Lyu and Li [2020], we define the following two auxiliary functions for the logistic loss: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(x):=-\\log(\\ell(x))=-\\log\\log(1+e^{-x}),\\quad x\\in\\mathbb{R},}\\\\ &{\\iota(x):=\\psi^{-1}(x)=-\\log(e^{e^{-x}}-1),\\quad x\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "One important remark is that if we change the loss to the exponential loss, both $\\psi$ and $\\iota$ will be the identity function. Since the logistic loss and the exponential loss have similar tails, our $\\psi(x)$ and $\\iota(x)$ are close to the identity function, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi(x)\\approx\\iota(x)\\approx x,\\quad\\mathrm{for}\\;x\\mathrm{\\large\\;enough}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we have an exponential-loss-like decomposition of $L_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{t}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(q_{i}(t))=\\frac{1}{n}\\sum_{i=1}^{n}e^{-\\psi(q_{i}(t))}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "These two functions $\\psi,\\iota$ will help us to analyze the lower bound of $v_{t}$ . First, we list some properties of $\\psi$ and $\\iota$ here. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.4 (Auxiliary functions of $\\ell$ ). The following claims hold for \u2113, $\\psi$ , and $\\iota$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\ell$ is monotonically decreasing, while $\\psi$ and \u03b9 are monotonically increasing. ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\psi^{\\prime}(\\iota(x))=\\frac{1}{\\iota^{\\prime}(x)}}\\end{array}$ ;   \n\u2022 $\\psi^{\\prime}(x)x$ is increasing for $x\\in(0,+\\infty)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.4. The first two properties are straightforward. For the third property, we apply chain rule on $\\psi(\\iota(x))=x$ to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi^{\\prime}(\\iota(x))\\iota^{\\prime}(x)=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the fourth property, notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi^{\\prime}(x)x=\\frac{x}{(1+e^{x})\\log(1+e^{-x})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The denominator is positive and decreasing since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d x}\\big[(1+e^{x})\\log(1+e^{-x})\\big]=e^{x}\\log(1+e^{-x})-1\\leq e^{x}e^{-x}-1=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining this with the fact that $x$ is positive and increasing, we have the desired result. ", "page_idx": 15}, {"type": "text", "text": "Besides, we have the following property of $\\iota$ . This is the key lemma to handle the homogeneous error.   \nActually, this lemma is another way to show $\\iota(x)$ is close to the identity function. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.5 (Property of $\\iota$ ). For every $x\\in\\mathbb R$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\iota(x)}{\\iota^{\\prime}(x)}}\\geq x+\\log\\log2.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.5. Recall that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\iota(x)=-\\log(e^{e^{-x}}-1),\\quad\\iota^{\\prime}(x)=\\frac{e^{e^{-x}}e^{-x}}{e^{e^{-x}}-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $y=e^{-x}$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\iota(x)}{\\iota^{\\prime}(x)}}={\\frac{-\\log(e^{e^{-x}}-1)(e^{e^{-x}}-1)}{e^{e^{-x}}e^{-x}}}={\\frac{-\\log(e^{y}-1)(e^{y}-1)}{e^{y}y}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $\\begin{array}{r}{s(y):=\\frac{\\iota(x)}{\\iota^{\\prime}(x)}-x-\\log\\log2}\\end{array}$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s(y)=\\frac{-\\log\\left(e^{y}-1\\right)\\left(e^{y}-1\\right)}{e^{y}y}+\\log(y)-\\log\\log2,}\\\\ &{s^{\\prime}(y)=\\log(e^{y}-1)\\cdot\\underbrace{\\frac{e^{y}-y-1}{e^{y}y^{2}}}_{>0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the sign of $s^{\\prime}$ is determined by $\\log(e^{y}\\mathrm{~-~}1)$ . For $0\\,<\\,e^{y}\\,\\leq\\,2$ , $s^{\\prime}(y)\\leq\\,0$ and $s(y)$ is decreasing; for $e^{y}\\geq2$ , $s(y)$ is increasing. Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y\\in(0,\\infty)}s(y)=s(\\log2)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $x=-\\log y$ , we have the desired result. ", "page_idx": 15}, {"type": "text", "text": "Another important property of $\\iota$ is that it can provide a lower bound for $q_{\\mathrm{min}}(t)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.6 ( $\\iota$ bound $q_{\\mathrm{min}}$ ). For every $t\\geq0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nq_{\\operatorname*{min}}(t)\\geq\\iota\\big(-\\log(L_{t})-\\log n\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.6. We use (7) to get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac1n\\ell(q_{\\mathrm{min}}(t))\\leq L_{t}\\Rightarrow\\displaystyle\\frac1n e^{-\\psi(q_{\\mathrm{min}}(t))}\\leq L_{t}}&{\\qquad\\qquad}\\\\ {\\Rightarrow\\psi(q_{\\mathrm{min}}(t))\\geq-\\log n-\\log L_{t}}&{}\\\\ {\\Rightarrow q_{\\mathrm{min}}(t)\\geq\\iota\\big(-\\log(L_{t})-\\log n\\big).}&{\\qquad\\qquad\\mathrm{by~L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we complete the proof. ", "page_idx": 16}, {"type": "text", "text": "Now, we are ready to give a lower bound of $v_{t}$ . The following lemma is an extension of Corollary E.6 in Lyu and Li [2020], where they dealt with a homogeneous model and the exponential loss; we extend this to a non-homogeneous model. The key ingredient is Lemma A.5. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.7 (A lower bound of $v_{t}$ ). Suppose Assumption $I C$ holds. Consider $v_{t}:=\\langle\\nabla L(\\mathbf{w}_{t}),-\\mathbf{w}_{t}\\rangle$ . If $\\begin{array}{r}{L_{t}\\leq\\frac{1}{2n e^{\\kappa}}}\\end{array}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\nv_{t}\\geq-L_{t}\\log(2n e^{\\kappa}L_{t})\\geq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma A.7. By definition, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{v_{t}}:=\\langle\\nabla L({\\mathbf w}_{t}),-{\\mathbf w}_{t}\\rangle}\\\\ {\\displaystyle}&{\\displaystyle=-\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i}))y_{i}\\langle\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t}\\rangle}\\\\ {\\displaystyle}&{\\displaystyle=-\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i}))y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i})-\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i}))\\Big(y_{i}\\langle\\nabla f({\\mathbf w}_{t};{\\mathbf x}_{i}),{\\mathbf w}_{t}\\rangle-y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i})\\Big)}\\\\ {\\displaystyle}&{\\displaystyle\\ge-\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i}))y_{i}f({\\mathbf w}_{t};{\\mathbf x}_{i})-\\kappa L_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n=\\frac{1}{n}\\sum_{i=1}^{n}e^{-\\psi(q_{i}(t))}\\psi^{\\prime}(q_{i}(t))q_{i}(t)-\\kappa L_{t}.\\qquad\\mathrm{since}\\;\\ell(\\cdot)=\\mathrm{exp}(-\\psi(\\cdot))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Lemma A.6 and Lemma A.4, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{i}(t)\\geq q_{\\operatorname*{min}}(t)\\geq\\iota\\big(-\\log(n L_{t})\\big):=-\\log(e^{n L_{t}}-1)\\geq-\\log(e^{\\frac{1}{2}}-1)\\geq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can apply Lemma A.4 to get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi^{\\prime}(q_{i}(t))q_{i}(t)\\geq\\psi^{\\prime}\\Big(\\iota\\big(-\\log(n L_{t})\\big)\\Big)\\iota\\big(-\\log(n L_{t})\\big)=\\frac{\\iota\\big(-\\log(n L_{t})\\big)}{\\iota^{\\prime}\\big(-\\log(n L_{t})\\big)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Invoking Lemma A.5, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\iota\\big(-\\log(n L_{t})\\big)}{\\iota^{\\prime}\\big(-\\log(n L_{t})\\big)}\\geq-\\log(n L_{t})+\\log\\log2\\geq-\\log(n L_{t})+\\log\\log e^{\\frac{1}{2}}=-\\log(2n L_{t}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting the above two inequalities together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi^{\\prime}(q_{i}(t))q_{i}(t)\\geq-\\log(2n L_{t}),\\quad{\\mathrm{for~every~}}i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging this back to the bound of $v_{t}$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle v_{t}\\geq-\\frac{1}{n}\\sum_{i=1}^{n}e^{-\\psi(q_{i}(t))}\\log(2n L_{t})-\\kappa L_{t}}}\\\\ {{\\displaystyle\\quad=-L_{t}\\log(2n L_{t})-\\kappa L_{t}}}\\\\ {{\\displaystyle\\quad=-L_{t}\\log(2n e^{\\kappa}L_{t})\\geq0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "Right now, we get a lower bound for $v_{t}$ , which is the cross term in the expression of $\\|\\mathbf{w}_{t+1}\\|^{2}$ . The next lemma controls the increase of the parameter norm $\\rho_{t}$ using $v_{t}$ and $L_{t}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma A.8 (The increase of $\\rho_{t}$ ). Suppose Assumptions $1A$ and $I C$ hold. If $L_{t}$ \u2264 $\\begin{array}{r}{\\operatorname*{min}\\left\\{\\frac{1}{2n e^{\\kappa}},\\frac{1}{\\tilde{\\eta}\\left(4\\rho^{2}+2\\beta\\right)}\\right\\}}\\end{array}$ 2ne\u03ba ,\u03b7\u02dc(4\u03c12+2\u03b2) , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n0\\leq2\\tilde{\\eta}v_{t}\\leq\\rho_{t+1}^{2}-\\rho_{t}^{2}\\leq2\\tilde{\\eta}v_{t}\\cdot\\bigg(1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}\\bigg).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.8. By definition, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{t+1}^{2}-\\rho_{t}^{2}=2\\tilde{\\eta}\\langle\\nabla L_{t},-\\mathbf{w}_{t}\\rangle+\\tilde{\\eta}^{2}\\|\\nabla L_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=2\\tilde{\\eta}v_{t}+\\tilde{\\eta}^{2}\\|\\nabla L_{t}\\|^{2}\\geq2\\tilde{\\eta}v_{t}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality is by Lemma A.7. Besides, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\rho_{t+1}^{2}-\\rho_{t}^{2}=2\\tilde{\\eta}v_{t}\\left(1+\\frac{\\tilde{\\eta}\\|\\nabla L_{t}\\|^{2}}{2v_{t}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\le2\\tilde{\\eta}v_{t}\\left(1+\\frac{\\tilde{\\eta}L_{t}^{2}\\rho^{2}}{2v_{t}}\\right)}&&{\\mathrm{by~}\\ell^{\\prime}\\le\\ell,\\mathrm{Assumption~1A,\\and~Lemma~}}\\\\ &{\\quad\\quad\\quad\\le2\\tilde{\\eta}v_{t}\\left(1+\\frac{L_{t}}{2v_{t}}\\right)}&&{\\mathrm{by~}L_{t}\\le\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}}\\\\ &{\\quad\\quad\\quad\\le2\\tilde{\\eta}v_{t}\\left(1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}\\right).}&&{\\mathrm{by~Lemma~A.}7}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 Convergence of the Margin ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we show that the normalized margin of a general predictor converges in the stable phase. Recall that we define the (normalized) margin as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\gamma}(\\mathbf{w}):=\\frac{\\operatorname*{min}_{i\\in[n]}y_{i}f(\\mathbf{w};\\mathbf{x}_{i})}{\\|\\mathbf{w}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "However, this normalized margin is not a smooth function of w. Instead, we consider a smoothed margin $\\gamma^{a}$ as an easy-to-analyze approximator of the normalized margin [Lyu and Li, 2020] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma^{a}(\\mathbf{w}):=\\frac{-\\log L(\\mathbf{w}_{t})}{\\|\\mathbf{w}\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We see that $\\gamma^{a}$ is a good approximator of $\\bar{\\gamma}$ . We can then use $\\gamma^{a}$ to analyze the convergence of the normalized margin since they share the same limit (if it exists). While $\\gamma^{a}$ is relatively easy to analyze for gradient flow [Lyu and Li, 2020], analyzing that for GD with a large (but fixed) stepsize is hard. To mitigate this issue, we construct another two margins that work well with large stepsize GD following the ideas of Lyu and Li [2020]. ", "page_idx": 17}, {"type": "text", "text": "Under Assumption 1, we define an auxiliary margin as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma^{b}(\\mathbf{w}):=\\frac{-\\log(2n e^{\\kappa}L(\\mathbf{w}))}{||\\mathbf{w}||},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and a modified margin as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf{w}):=\\frac{e^{\\Phi(L(\\mathbf{w}))}}{\\|\\mathbf{w}\\|},\\quad\\mathrm{where~}\\Phi(x):=\\log(-\\log(2n e^{\\kappa}x))+\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{\\log(2n e^{\\kappa}x)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "These two margins provide a second-order correction when viewing large stepsize GD as a first-order approximation of gradient flow. In the following discussion, we will show that $\\bar{\\gamma}(\\mathbf{w})\\approx\\gamma^{a}(\\mathbf{w})\\approx$ $\\gamma^{b}(\\mathbf{w})\\approx\\gamma^{c}(\\mathbf{w})$ . At last, we will use the convergence of $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ to prove $\\bar{\\gamma}({\\bf w}_{t})$ converges. ", "page_idx": 17}, {"type": "text", "text": "The following lemma shows that $\\bar{\\gamma}(\\mathbf{w})\\approx\\gamma^{a}(\\mathbf{w})$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.9 (Smoothed margin). For the smooth margin $\\gamma^{a}\\big(\\mathbf{w}_{t}\\big)$ defined in (8) and the normalized margin $\\bar{\\gamma}({\\bf w}_{t})$ defined in (3), we have ", "page_idx": 17}, {"type": "text", "text": "\u2022 When $L_{t}\\leq{\\frac{1}{2n}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{\\mathrm{min}}(t)\\leq-\\log L_{t}\\leq\\log(2n)+q_{\\mathrm{min}}(t),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\gamma}(\\mathbf{w}_{t})\\leq\\gamma^{a}(\\mathbf{w}_{t})\\leq\\bar{\\gamma}(\\mathbf{w}_{t})+\\frac{\\log(2n)}{\\rho_{t}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022 Assume Assumption $I C$ holds. If $L_{t}\\to0$ , then $\\left|\\gamma^{a}(\\mathbf{w}_{t})-\\bar{\\gamma}\\!\\left(\\mathbf{w}_{t}\\right)\\right|\\rightarrow0.$ ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.9. To prove the first claim, notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{t}\\leq\\frac{1}{2n}\\implies\\ell(q_{\\mathrm{min}}(t))=\\log(1+\\exp(-q_{\\mathrm{min}}(t)))\\leq n L_{t}\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore we have ", "page_idx": 18}, {"type": "equation", "text": "$$\ne^{-q_{\\mathrm{min}}(t)}\\leq e^{\\frac{1}{2}}-1\\leq1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using $\\textstyle{\\frac{x}{2}}\\leq\\log(1+x)\\leq x$ for $0\\leq x\\leq1$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{2}e^{-q_{\\mathrm{min}}(t)}\\leq\\ell\\big(q_{\\mathrm{min}}(t)\\big)=\\log\\!\\big(1+e^{-q_{\\mathrm{min}}(t)}\\big)\\leq e^{-q_{\\mathrm{min}}(t)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we can bound $L_{t}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{2n}e^{-q_{\\mathrm{min}}(t)}\\leq\\frac{1}{n}\\ell(q_{\\mathrm{min}}(t))\\leq L_{t}\\leq\\ell(q_{\\mathrm{min}}(t))\\leq e^{-q_{\\mathrm{min}}(t)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{\\mathrm{min}}(t)\\leq-\\log L_{t}\\leq\\log(2n)+q_{\\mathrm{min}}(t).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Dividing both sides by $\\rho_{t}$ proves the second claim: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{\\gamma}(\\mathbf{w}_{t}):=\\frac{q_{\\operatorname*{min}}(t)}{\\rho_{t}}\\leq\\gamma^{a}(\\mathbf{w}_{t}):=\\frac{-\\log L_{t}}{\\rho_{t}}\\leq\\bar{\\gamma}(\\mathbf{w}_{t})+\\frac{\\log(2n)}{\\rho_{t}}=\\frac{\\log(2n)+q_{\\operatorname*{min}}(t)}{\\rho_{t}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the last claim, we only need to show that $\\rho_{t}\\,\\rightarrow\\,\\infty$ . This is because if $L_{t}\\,\\to\\,0$ , we have for any $i\\in[n],\\,y_{i}f(\\mathbf{w}_{t};x_{i})\\to\\infty$ . Using $y_{i}f(\\mathbf{w}_{t};x_{i})\\leq C_{r,\\kappa}\\|\\mathbf{w}_{t}\\|+C_{r}$ from Lemma G.2, we have $\\rho_{t}=\\|\\mathbf{w}_{t}\\|_{2}\\to\\infty$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "The following lemma shows that $\\gamma^{c}(\\mathbf{w})\\approx\\bar{\\gamma}(\\mathbf{w})$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma A.10 (Modified and auxiliary margins). Suppose that Assumption $I$ holds. For the modified margin $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ defined in (10) and the auxiliary margin $\\gamma^{b}({\\bf w}_{t})$ defined in (9), we have ", "page_idx": 18}, {"type": "text", "text": "\u2022 If $\\begin{array}{r}{L_{t}\\leq\\frac{1}{2n e^{\\kappa+2}}}\\end{array}$ , there exists a constant c such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf{w}_{t})\\leq\\gamma^{b}(\\mathbf{w}_{t})\\leq\\bar{\\gamma}(\\mathbf{w}_{t})\\leq\\bigg(1+\\frac{c}{\\log(1/L(\\mathbf{w}_{t}))}\\bigg)\\gamma^{c}(\\mathbf{w}_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $A.I O$ . To prove the first two inequalities, notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(L_{t})=-\\log(2n e^{\\kappa}L_{t})\\cdot\\exp\\left(\\displaystyle\\frac{1+\\left(4\\rho^{2}+2\\beta\\right)\\tilde{\\eta}}{\\log(2n e^{\\kappa}L_{t})}\\right)\\qquad\\mathrm{using~(10)}}\\\\ &{\\qquad\\le-\\log(2n e^{\\kappa}L_{t}).\\quad\\mathrm{since~}L_{t}\\le\\displaystyle\\frac{1}{2n e^{\\kappa}},\\exp\\left(\\displaystyle\\frac{1+\\left(4\\rho^{2}+2\\beta\\right)\\tilde{\\eta}}{\\log(2n e^{\\kappa}L_{t})}\\right)\\le1,\\mathrm{~and~log}(2n e^{\\kappa}L_{t})>0}\\\\ &{\\qquad\\le-\\log(L_{t})-\\log(2n)}\\\\ &{\\qquad\\le q_{\\mathrm{min}}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the above, (8) to (10), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf w_{t}):=\\frac{e^{\\Phi(L(\\mathbf w_{t}))}}{\\|\\mathbf w_{t}\\|}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\|\\mathbf w_{t}\\|}=:\\gamma^{b}(\\mathbf w_{t})\\leq\\frac{q_{\\operatorname*{min}}(t)}{\\|\\mathbf w_{t}\\|}=:\\bar{\\gamma}(\\mathbf w_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we will prove the remaining inequality. First, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\bar{\\gamma}(\\mathbf{w}_{t})}{\\gamma^{c}(\\mathbf{w}_{t})}=\\frac{\\bar{\\gamma}(\\mathbf{w}_{t})}{\\gamma^{b}(\\mathbf{w}_{t})}\\cdot\\frac{\\gamma^{b}(\\mathbf{w}_{t})}{\\gamma^{c}(\\mathbf{w}_{t})}}\\\\ &{\\qquad\\qquad=\\frac{q_{\\mathrm{min}}(t)}{-\\log(2n e^{\\kappa}L_{t})}\\cdot\\exp\\left(\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{-\\log(2n e^{\\kappa}L_{t})}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{-\\log(L_{t})}{-\\log(2n e^{\\kappa}L_{t})}\\cdot\\exp\\left(\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{-\\log(2n e^{\\kappa}L_{t})}\\right)}\\\\ &{\\qquad\\qquad=\\bigg(1+\\frac{\\log(2n e^{\\kappa})}{-\\log(2n e^{\\kappa}L_{t})}\\bigg)\\cdot\\exp\\left(\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{-\\log(2n e^{\\kappa}L_{t})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To simplify the notation, we let c1 := 1 + (4\u03c12 + 2\u03b2)\u03b7\u02dc and c2 = log(2ne\u03ba). Since Lt \u22642ne1\u03ba+2 \u21d2 $-\\log(2n e^{\\kappa}L_{t})\\geq2>1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{-\\log(2n e^{\\kappa}L_{t})}=\\frac{c_{1}}{-\\log(2n e^{\\kappa}L_{t})}\\leq c_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Besides, given $x<c$ , we have $e^{x}\\leq1+e^{c}x$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\exp\\left(\\frac{1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}}{-\\log(2n e^{\\kappa}L_{t})}\\right)=\\exp\\left(\\frac{c_{1}}{-\\log(2n e^{\\kappa}L_{t})}\\right)\\leq1+\\frac{c_{1}\\exp(c_{1})}{-\\log(2n e^{\\kappa}L_{t})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging this into the bound for $\\bar{\\gamma}(\\mathbf{w}_{t})/\\gamma^{c}(\\mathbf{w}_{t})$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r l}&{{\\frac{{\\bar{\\gamma}}(\\mathbf{w}_{t})}{\\gamma^{c}(\\mathbf{w}_{t})}}=\\left(1+{\\frac{c_{2}}{-\\log(2n e^{\\kappa}L_{t})}}\\right)\\cdot\\exp\\left({\\frac{c_{1}}{-\\log(2n e^{\\kappa}L_{t})}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\left(1+{\\frac{c_{2}}{-\\log(2n e^{\\kappa}L_{t})}}\\right)\\cdot\\left(1+{\\frac{\\exp(c_{1})c_{1}}{-\\log(2n e^{\\kappa}L_{t})}}\\right)}\\\\ &{\\qquad\\qquad\\leq1+{\\frac{c_{2}+\\exp(c_{1})c_{1}+c_{2}c_{1}\\exp(c_{1})}{-\\log(2n e^{\\kappa}L_{t})}}}&&{{\\mathrm{Since-log}(2n e^{\\kappa}L_{t})\\geq1}}\\\\ &{\\qquad\\qquad=1+{\\frac{c_{2}+\\exp(c_{1})c_{1}+c_{2}c_{1}\\exp(c_{1})}{-\\log L_{t}-c_{2}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $-\\log L_{t}-c_{2}\\geq2>1$ . Because $\\frac{x}{x-c_{2}}$ is decreasing when $x\\geq c_{2}+1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{-\\log L_{t}}{-\\log L_{t}-c_{2}}\\leq c_{2}+1\\implies\\frac{1}{-\\log L_{t}-c_{2}}\\leq\\frac{c_{2}+1}{-\\log L_{t}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plug this inequality into the previous bound for $\\bar{\\gamma}({\\bf w}_{t})/\\gamma^{c}({\\bf w}_{t})$ and we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\bar{\\gamma}(\\mathbf{w}_{t})}{\\gamma^{c}(\\mathbf{w}_{t})}\\leq1+\\frac{(c_{2}+\\exp(c_{1})c_{1}+c_{2}c_{1}\\exp(c_{1}))(c_{2}+1)}{-\\log L_{t}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $c:=(c_{2}+\\exp(c_{1})c_{1}+c_{2}c_{1}\\exp(c_{1}))(c_{2}+1)$ . We complete the proof. ", "page_idx": 19}, {"type": "text", "text": "The next lemma shows the convexity of $\\Phi$ defined in (10). The convexity will help us analyze the change of $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ in the gradient descent dynamics. Specifically, we are going to use the property that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Phi(x)-\\Phi(y)\\geq\\Phi^{\\prime}(y)(x-y),\\quad{\\mathrm{for~all~}}x,y.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.11 (Convexity of $\\Phi$ ). The function $\\Phi(x)$ defined in (10) is convex for $\\begin{array}{r}{0<x<\\frac{1}{2n e^{2+\\kappa}}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma A.11. Check that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Phi^{\\prime}(x)=\\frac{1-\\frac{1}{\\log(2n e^{\\kappa}x)}(1+(4\\rho^{2}+2\\beta)\\tilde{\\eta})}{x\\log(2n e^{\\kappa}x)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Phi^{\\prime\\prime}(x)={\\frac{\\left(1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}\\right)\\cdot(2+\\log(2n e^{\\kappa}x))-\\log^{2}(2n e^{\\kappa}x)-\\log(2n e^{\\kappa}x)}{x^{2}\\log^{3}(2n e^{\\kappa}x)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that when $\\begin{array}{r}{x\\leq\\frac{1}{2n e^{2+\\kappa}}}\\end{array}$ , we have $\\log(2n e^{\\kappa}x)\\leq-2$ , which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n2+\\log(2n e^{\\kappa}x)\\leq0,\\ \\log(2n e^{\\kappa}x)<0,\\ \\mathrm{and}\\ -\\log^{2}(2n e^{\\kappa}x)-\\log(2n e^{\\kappa}x)<0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging these into the previous equality, we have $\\Phi^{\\prime\\prime}(x)\\geq0$ when $\\begin{array}{r}{0<x<\\frac{1}{2n e^{2+\\kappa}}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Before we dive into the proof of the monotonic increasing $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ , we show that $\\gamma^{c}$ is bounded first. The convergence of $\\gamma^{c}$ is a direct consequence of the monotonic increasing and the boundedness of $\\gamma^{c}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma A.12 (An upper bound on $\\gamma^{c},\\gamma^{b}$ , $\\gamma^{a}$ and $\\gamma^{c}$ ). When $\\begin{array}{r}{L_{t}\\,\\leq\\,\\left\\{\\frac{1}{2n e^{\\kappa}},\\frac{1}{\\tilde{\\eta}\\left(4\\rho^{2}+2\\beta\\right)}\\right\\}f o r\\;t\\geq\\varepsilon}\\end{array}$ there exists $B_{0}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf{w}_{t})\\leq\\gamma^{b}(\\mathbf{w}_{t})\\leq\\gamma^{a}(\\mathbf{w}_{t})\\leq\\bar{\\gamma}(\\mathbf{w}_{t})+\\frac{\\log2n}{\\rho_{s}}\\leq B_{0}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Apply lemma A.8, we have $\\|\\mathbf{w}_{t}\\|\\geq\\rho_{t}\\geq\\rho_{s}$ . Then we can apply lemma G.2 and there exists a constant $C_{\\rho_{s},\\kappa}$ such that for all $i$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|y_{i}f(\\mathbf{w}_{t};\\mathbf{x}_{i})|\\leq C_{\\rho_{s},\\kappa}\\|\\mathbf{w}_{t}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\gamma}(\\mathbf{w}_{t})=\\frac{\\arg\\operatorname*{min}_{i\\in[n]}y_{i}f(\\mathbf{w}_{t};\\mathbf{x}_{i})}{\\|\\mathbf{w}_{t}\\|}\\leq C_{\\rho_{s},\\kappa}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Besides, by Lemma A.9, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma^{a}(\\mathbf{w}_{t})\\leq\\bar{\\gamma}(\\mathbf{w}_{t})+\\frac{\\log2n}{\\rho_{t}}\\leq C_{\\rho_{s},\\kappa}+\\frac{\\log2n}{\\rho_{s}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma A.10, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma^{c}(\\mathbf{w}_{t})\\leq\\gamma^{b}(\\mathbf{w}_{t})\\leq\\gamma^{a}(\\mathbf{w}_{t})\\leq C_{\\rho_{s},\\kappa}+\\frac{\\log2n}{\\rho_{s}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{B_{0}=C_{\\rho_{s},\\kappa}+\\frac{\\log2n}{\\rho_{s}}}\\end{array}$ . Then, we complete the proof. ", "page_idx": 20}, {"type": "text", "text": "The following lemma is a variant of Proposition 5, item 1, in [Wu et al., 2024]. Before the lemma, we need some auxiliary definitions. let us define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\theta}_{t}:=\\frac{\\mathbf{w}_{t}}{\\|\\mathbf{w}_{t}\\|},\\quad\\pmb{\\nu}_{t}:=\\pmb{\\theta}_{t}\\pmb{\\theta}_{t}^{\\top}(-\\nabla L_{t}),\\quad\\pmb{\\mu}_{t}:=\\big(\\mathbf{I}-\\pmb{\\theta}_{t}\\pmb{\\theta}_{t}^{\\top}\\big)(-\\nabla L_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla L_{t}\\|^{2}=\\|\\pmb{\\nu}_{t}\\|^{2}+\\|\\pmb{\\mu}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The key point of this decomposition is that we consider the gradient of the loss function as a sum of two orthogonal components. The first component $\\pmb{\\nu}_{t}$ is the component in the direction of ${\\bf w}_{t}$ , and the second component $\\pmb{\\mu}_{t}$ is the component orthogonal to ${\\bf w}_{t}$ . We will show that the modified margin $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is monotonically increasing. And the increase of $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is lower bounded by a term that depends on $\\|\\pmb{\\mu}_{t}\\|^{2}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma A.13 (Modified margin is monotonically increasing). Suppose Assumption $^{l}$ holds. If there exists $s$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nL_{s}\\le\\operatorname*{min}\\bigg\\{\\frac{1}{e^{\\kappa+2}2n},\\:\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then for $t\\geq s$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\gamma^{c}(\\mathbf{w}_{t+1})-\\log\\gamma^{c}(\\mathbf{w}_{t})\\geq\\frac{\\rho_{t}^{2}}{v_{t}^{2}}\\|\\pmb{\\mu}_{t}\\|^{2}\\log\\frac{\\rho_{t+1}}{\\rho_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a consequence, $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ admits $a$ finite limit. ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma A.13. The first claim is by Lemma A.3 and induction. The second and the third claims are consequences of Lemmas A.7 and A.8, respectively. We now prove the last claim. By Lemma A.3, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}:=L(\\mathbf{w}_{t+1})-L(\\mathbf{w}_{t})\\leq-\\tilde{\\eta}(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t})\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Multiplying both sides by 2vt1\u22122( 2lo\u03c1g2(2+n\u03b2e)\u03ba\u03b7\u02dcLLtt) $\\begin{array}{r}{2v_{t}\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t}}>0}\\end{array}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t}}2v_{t}\\big(L_{t+1}-L_{t}\\big)\\leq-2\\tilde{\\eta}v_{t}\\bigg(1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}\\bigg)\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Lemma A.8 we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\leq\\rho_{t+1}^{2}-\\rho_{t}^{2}\\leq2\\tilde{\\eta}v_{t}\\Big(1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the above we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t}}2v_{t}\\big(L_{t+1}-L_{t}\\big)\\leq-\\big(\\rho_{t+1}^{2}-\\rho_{t}^{2}\\big)\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla L_{t}\\|^{2}=\\|\\pmb{\\nu}_{t}\\|^{2}+\\|\\pmb{\\mu}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\pmb{\\nu}_{t}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\pmb{\\nu}_{t}\\|=\\frac{1}{\\rho_{t}}\\langle\\mathbf{w}_{t},-\\nabla L_{t}\\rangle=\\frac{v_{t}}{\\rho_{t}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we can decompose $\\|\\nabla L_{t}\\|^{2}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla L_{t}\\|^{2}=\\|\\nu_{t}\\|^{2}+\\|\\mu_{t}\\|^{2}=\\frac{v_{t}^{2}}{\\rho_{t}^{2}}+\\|\\mu_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Plugging this into (11) and dividing both two sides by $2v_{t}^{2}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t})v_{t}}(L_{t+1}-L_{t})\\leq-\\frac{1}{\\rho_{t}^{2}}(\\rho_{t+1}^{2}-\\rho_{t}^{2})\\bigg(\\frac{1}{2}+\\frac{\\rho_{t}^{2}}{2v_{t}^{2}}\\|\\mu_{t}\\|^{2}\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma A.7, we have $v_{t}\\geq-L_{t}\\log(2n e^{\\kappa}L_{t})$ . Define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Psi(x):=-\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}x)}}{(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}x)x\\log(2n e^{\\kappa}x)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi(L_{t})(L_{t+1}-L_{t}):=-\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t})L_{t}\\log(2n e^{\\kappa}L_{t})}(L_{t+1}-L_{t})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}L_{t})}}{(1-(2\\rho^{2}+\\beta)\\tilde{\\eta}L_{t})v_{t}}(L_{t+1}-L_{t})}\\\\ &{\\qquad\\qquad\\leq-\\frac{1}{\\rho_{t}^{2}}(\\rho_{t}^{2}-\\rho_{t+1}^{2})\\biggl(\\frac{1}{2}+\\frac{\\rho_{t}^{2}}{2v_{t}^{2}}\\|\\mu_{t}\\|^{2}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We are going to show that $\\Psi(x)\\leq-\\Phi^{\\prime}(x)$ . Note that when $\\begin{array}{r}{0<x\\leq\\operatorname*{min}\\left\\lbrace\\frac{1}{e^{\\kappa+2}2n},\\frac{1}{\\widetilde{\\eta}(4\\rho^{2}+2\\beta)}\\right\\rbrace}\\end{array}$ have $\\log(2n e^{\\kappa}x)<0$ and $\\begin{array}{r}{1-(2{\\rho}^{2}+\\beta)\\tilde{\\eta}x)\\geq\\frac{1}{2}>0}\\end{array}$ . Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Psi(x)=\\underbrace{\\frac{1-\\frac{1}{2\\log(2n e^{\\kappa}x)}}{1-(2\\rho^{2}+\\beta)\\tilde{\\eta}x}}_{=:J>0}\\cdot\\underbrace{\\frac{-1}{x\\log(2n e^{\\kappa}x)}}_{>0}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To get an upper bound of $\\Psi(x)$ , we just need an upper bound of $J$ . Let $\\begin{array}{r}{a:=\\frac{-1}{2\\log(2n e^{\\kappa}x)}>0}\\end{array}$ and $b:=(2\\rho^{2}+\\beta)\\tilde{\\eta}x\\in(0,1/2]$ . Then we invoke Lemma G.4 to get ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ:=\\frac{1+a}{1-b}\\leq1+2a+2b=1-\\frac{1}{\\log(2n e^{\\kappa}x)}+(4\\rho^{2}+2\\beta)\\tilde{\\eta}x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that x \u2264 e\u03ba+122n $\\begin{array}{r}{x\\,\\leq\\,\\frac{1}{e^{\\kappa+2}2n}\\,\\leq\\,\\frac{1}{2n e^{\\kappa}}}\\end{array}$ and $2n e^{\\kappa}\\geq1$ . Then we apply Lemma G.3 to get $\\begin{array}{r}{x\\leq\\frac{-1}{\\log(2n e^{\\kappa}x)}}\\end{array}$ . Plugging this into the bound of $J$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ\\leq1-\\frac{1}{\\log(2n e^{\\kappa}x)}+(4\\rho^{2}+2\\beta)\\tilde{\\eta}x\\leq1-\\frac{1}{\\log(2n e^{\\kappa}x)}(1+(4\\rho^{2}+2\\beta)\\tilde{\\eta}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging (15) into (14), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Psi(x)=J\\cdot\\frac{-1}{x\\log(2n e^{\\kappa}x)}\\leq-\\frac{1-\\frac{1}{\\log(2n e^{\\kappa}x)}(1+(4\\rho^{2}+2\\beta)\\tilde{\\eta})}{x\\log(2n e^{\\kappa}x)}=-\\Phi^{\\prime}(x),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which verifies that $\\Psi(x)\\leq-\\Phi^{\\prime}(x)$ . By this and (13), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi^{\\prime}(L_{t})(L_{t+1}-L_{t})+\\varphi^{\\prime}(\\rho_{t}^{2})(\\rho_{t+1}^{2}-\\rho_{t}^{2})\\bigg(\\frac{1}{2}+\\frac{\\rho_{t}^{2}}{2v_{t}^{2}}\\|\\mu_{t}\\|^{2}\\bigg)\\geq0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\varphi(x)=-\\log x=\\log(1/x)$ . Recall that for $\\begin{array}{r}{0<x\\leq\\frac{1}{2n e^{\\kappa+2}},\\Phi}\\end{array}$ $\\Phi(x)$ is convex by Lemma A.11. By convexity of $\\varphi$ and $\\Phi$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi(L_{t+1})-\\Phi(L_{t})+\\left(\\log\\frac{1}{\\rho_{t+1}^{2}}-\\log\\frac{1}{\\rho_{t}^{2}}\\right)\\left(\\frac{1}{2}+\\frac{\\rho_{t}^{2}}{2v_{t}^{2}}\\|\\mu_{t}\\|^{2}\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the definition of $\\gamma^{c}$ in (10), this can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\gamma^{c}(\\mathbf w_{t+1})-\\log\\gamma^{c}(\\mathbf w_{t})=(\\Phi(L_{t+1})-\\Phi(L_{t}))+\\bigg(\\log\\frac{1}{\\rho_{t+1}}-\\log\\frac{1}{\\rho_{t}}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ge-\\bigg(\\log\\frac{1}{\\rho_{t+1}^{2}}-\\log\\frac{1}{\\rho_{t}^{2}}\\bigg)\\frac{\\rho_{t}^{2}}{2v_{t}^{2}}\\|\\mu_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\rho_{t}^{2}}{v_{t}^{2}}\\|\\mu_{t}\\|^{2}\\log\\frac{\\rho_{t+1}}{\\rho_{t}}}\\\\ &{\\qquad\\qquad\\qquad\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality is because of Lemma A.8. We have shown that $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is monotonically increasing. By Lemma A.12, $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is bounded. Therefore $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ admits a finite limit. This completes the proof. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.4 Sharp rates of Loss and Parameter Norm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Right now, we have already proved that $\\gamma^{c}(\\mathbf{w}_{t})$ is monotonically increasing and bounded, which indicates $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ converges. However, if we want to show that $\\bar{\\gamma}\\big(\\mathbf{w}_{t}\\big)$ converges, we still need to verify that $L_{t}\\to0$ , which is the crucial condition for $\\gamma^{c}(\\mathbf{w}_{t}),\\gamma^{b}(\\mathbf{w}_{t}),\\gamma^{a}(\\mathbf{w}_{t})$ , and $\\bar{\\gamma}({\\bf w}_{t})$ to share the same limit, by Lemma A.9 and Lemma A.10. ", "page_idx": 22}, {"type": "text", "text": "Fortunately, with the monotonicity of $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ , we can prove that $L_{t}$ converges to zero and even characterize the rate of $L_{t}$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma A.14 (Rate of $L_{t}$ in general model). Suppose Assumption $I$ holds. If there is an s such that ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\big\\{\\frac{1}{e^{\\kappa+2}2n},\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}\\big\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then for every $t\\geq s$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{\\frac{1}{L(\\mathbf{w}_{s})}+3\\Tilde{\\eta}\\rho^{2}(t-s)}\\leq L(\\mathbf{w}_{t})\\leq\\frac{2}{(t-s)\\Tilde{\\eta}\\gamma^{c}(\\mathbf{w}_{s})^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "That is, $L(\\mathbf{w}_{t})=\\Theta(1/t)\\to0$ as $t\\to\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma A.14. By Lemma A.3 and (12) in the proof of Lemma A.13, we know $L_{t}$ is decreasing and ", "page_idx": 22}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}\\leq-\\frac{\\tilde{\\eta}}{2}\\|\\nabla L_{t}\\|^{2}\\leq-\\frac{\\tilde{\\eta}}{2}\\|\\pmb{\\nu}_{t}\\|_{2}^{2}\\leq-\\frac{\\tilde{\\eta}}{2}\\frac{v_{t}^{2}}{\\rho_{t}^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will establish an upper bound for $\\rho_{t}$ first. Note that $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is increasing for $t\\geq s$ by Lemma A.13 and $\\gamma^{b}({\\bf w}_{t})\\geq\\gamma^{c}\\big({\\bf w}_{t}\\big)$ by Lemma A.10. By (9), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho_{t}=\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{b}(\\mathbf w_{t})}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{c}(\\mathbf w_{t})}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{c}(\\mathbf w_{s})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining this with Lemma A.7, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{v_{t}}{\\rho_{t}}\\geq\\frac{-L_{t}\\log(2n e^{\\kappa}L_{t})}{\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{c}(\\mathbf{w}_{s})}}=L_{t}\\gamma^{c}(\\mathbf{w}_{s}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging this into (16), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}\\leq-\\frac{\\tilde{\\eta}}{2}L_{t}^{2}\\gamma^{c}(\\mathbf{w}_{s})^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\frac{\\Tilde{\\eta}\\gamma(\\mathbf{w}_{s})^{2}}{2}\\leq\\frac{L_{t}-L_{t+1}}{L_{t}^{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{L_{t}-L_{t+1}}{L_{t}L_{t+1}}}&&{\\qquad\\qquad\\mathrm{Since}\\ L_{t+1}\\leq L_{t}}\\\\ &{\\qquad\\qquad=\\frac{1}{L_{t+1}}-\\frac{1}{L_{t}},}&{t\\geq s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Telescoping the sum from $s$ to $t$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n(t-s)\\frac{\\tilde{\\eta}\\gamma^{c}(\\mathbf w_{s})^{2}}{2}\\leq\\frac{1}{L_{t}}-\\frac{1}{L_{s}}\\leq\\frac{1}{L_{t}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t}\\leq\\frac{2}{(t-s)\\tilde{\\eta}\\gamma^{c}(\\mathbf{w}_{s})^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we show the lower bound on the risk. By Lemma A.3 we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}\\geq-\\tilde{\\eta}(1+\\beta\\tilde{\\eta}L_{t})\\|\\nabla L_{t}\\|^{2}\\geq-\\frac{3}{2}\\eta\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Observe that under Assumption 1A, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\nabla L_{t}\\right\\|=\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(q_{i}(t))y_{i}\\nabla f(\\mathbf{w}_{t};\\mathbf{x}_{i})\\right\\|\\leq\\rho L_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t+1}-L_{t}\\ge-\\tilde{\\eta}\\frac{3}{2}\\rho^{2}L_{t}^{2},\\quad t\\ge s.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tilde{L}_{t}\\,:=\\,\\frac{3\\tilde{\\eta}\\rho^{2}}{2}L_{t}}\\end{array}$ , we have $\\begin{array}{r}{\\tilde{L}_{s}\\;\\le\\;\\frac{3\\tilde{\\eta}\\rho^{2}}{2}\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}\\;\\le\\;\\frac{3}{8}\\;\\le\\;\\frac{1}{2}}\\end{array}$ . Furthermore, since $L_{t}$ decreases monotonically, $\\tilde{L}_{t}\\leq\\tilde{L}_{s}\\leq\\frac{1}{2}$ . The inequality becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{L}_{t+1}-\\tilde{L}_{t}\\geq-\\tilde{L}_{t}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, let $\\begin{array}{r}{c=\\frac{1}{\\tilde{L}_{s}}}\\end{array}$ and apply Lemma G.1, we have for any $t\\geq s$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{L}_{t}\\geq\\frac{1}{c+2(t-s)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{t}\\ge\\frac{1}{\\frac{1}{L_{s}}+3\\tilde{\\eta}\\rho^{2}(t-s)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We have completed the proof. ", "page_idx": 23}, {"type": "text", "text": "Furthermore, we can characterize the order of $\\rho_{t}$ in the stable phase. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.15 (Order of $\\rho_{t}$ in general model). Suppose Assumption $I$ holds. If there is s such that ", "page_idx": 24}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\bigg\\{\\frac{1}{e^{\\kappa+2}2n},\\;\\frac{1}{\\tilde{\\eta}(4\\rho^{2}+2\\beta)}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then for $t\\geq s$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{t}=\\Theta(\\log(t)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma A.15. Note that $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ is increasing for $t\\geq s$ by Lemma A.13 and $\\gamma^{b}(\\mathbf{w}_{t})\\geq$ $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ by Lemma A.10. Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{t}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{b}(\\mathbf w_{t})}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{c}(\\mathbf w_{t})}\\leq\\frac{-\\log(2n e^{\\kappa}L_{t})}{\\gamma^{c}(\\mathbf w_{s})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining this with Lemma A.14, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{t}\\leq\\frac{\\log\\frac{1/L(\\mathbf{w}_{s})+3\\Tilde{\\eta}\\rho^{2}(t-s)}{2n e^{\\kappa}}}{\\gamma^{c}(\\mathbf{w}_{s})}=\\mathcal{O}(\\log(\\Tilde{\\eta}t)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Besides, we have $q_{\\mathrm{min}}\\ \\geq\\ \\iota(\\log\\frac{1}{L}\\,-\\,\\log n)$ by Lemma A.9 and $q_{\\mathrm{min}}~\\le~B_{0}\\rho_{t}$ by Lemma A.12. Therefore we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{t}\\geq\\frac{\\iota(\\log\\frac{1}{n L_{t}})}{B_{0}}\\geq\\frac{\\log\\frac{1}{n L_{t}}}{2B_{0}}\\geq\\frac{\\log\\frac{(t-s)\\tilde{\\eta}\\gamma^{c}(\\mathbf{w}_{s})^{2}}{2n}}{2B_{0}}=\\Omega(\\log(t)),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality is because for $\\iota(x)$ defined in (6), $\\textstyle\\iota(x)\\geq{\\frac{x}{2}}$ for $x\\ge0.6$ , and the third inequality is by Lemma A.14. Combining them, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\rho_{t}=\\Theta(\\log(t)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "A.5 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 2.2. We prove the items one by one. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The monotonicity of $L_{t}$ comes from the result of Lemma A.3 directly.   \n\u2022 Item 1 is due to Lemma A.14 .   \n\u2022 For item 2, the monotonicity of $\\rho_{t}$ comes from the result of Lemma A.8 and the order is due to Lemma A.15.   \n\u2022 For item 3, we first know that $L_{t}~\\to~0$ by Lemma A.14. Then, by Lemma A.13 and Lemma A.12, we know that $\\gamma^{c}\\big(\\mathbf{w}_{t}\\big)$ converges. Combining these with Lemma A.9 and Lemma A.10, we know that $\\gamma^{c}(\\mathbf{w}_{t})$ is an $\\begin{array}{r}{\\left(\\bar{1}+O\\right(1/(\\log\\frac{\\bar{1}}{L(\\mathbf{w}_{t})})\\right)}\\end{array}$ -multiplicative approximation $\\mathrm{of}\\bar{\\gamma}(\\mathbf{w}_{t})$ , and $\\bar{\\gamma}({\\bf w}_{t})$ shares the same limit as $\\gamma^{c}(\\mathbf{w}_{t})$ . ", "page_idx": 24}, {"type": "text", "text": "B EoS Phase Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we focus on the linearly separable case, that is, we work under Assumption 3. We mainly follow the idea of [Wu et al., 2024] for the proof. In detail, we consider a comparator ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{u}:=\\mathbf{u}_{1}+\\mathbf{u}_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{u}_{1}:=\\left(\\begin{array}{l}{\\mathbf{u}_{1}^{(1)}}\\\\ {\\vdots}\\\\ {\\mathbf{u}_{1}^{(m)}}\\end{array}\\right),\\ \\mathrm{with\\}\\mathbf{u}_{1}^{(j)}:=a_{j}\\frac{\\log(\\gamma^{2}\\eta T)+\\kappa}{\\alpha\\gamma}\\cdot\\mathbf{w}_{*},\\quad j=1,\\ldots,m,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{u}_{2}:=\\left(\\begin{array}{l}{\\mathbf{u}_{2}^{(1)}}\\\\ {\\vdots}\\\\ {\\mathbf{u}_{2}^{(m)}}\\end{array}\\right),\\;\\;\\mathrm{with}\\;\\;\\mathbf{u}_{2}^{(j)}:=a_{j}\\frac{\\eta}{2\\gamma}\\cdot\\mathbf{w}_{\\ast},\\quad j=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the following decomposition, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf w_{t+1}-\\mathbf u\\|^{2}=\\|\\mathbf w_{t}-\\mathbf u\\|^{2}+2m\\eta\\langle\\nabla L(\\mathbf w_{t}),\\mathbf u-\\mathbf w_{t}\\rangle+m^{2}\\eta^{2}\\|\\nabla L(\\mathbf w_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|\\mathbf w_{t}-\\mathbf u\\|^{2}+2m\\eta\\underbrace{\\langle\\nabla L(\\mathbf w_{t}),\\mathbf u_{1}-\\mathbf w_{t}\\rangle}_{=:I_{1}(\\mathbf w_{t})}+m\\eta\\bigl(\\underbrace{2\\langle\\nabla L(\\mathbf w_{t}),\\mathbf u_{2}\\rangle}_{=:I_{2}(\\mathbf w_{t})}+m\\eta\\|\\nabla L(\\mathbf w_{t})\\|^{2}\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We aim to prove $\\begin{array}{r}{I_{1}(\\mathbf{w}_{t})\\leq\\frac{1}{T}-L(\\mathbf{w}_{t})}\\end{array}$ and $I_{2}(\\mathbf{w}_{t})\\leq0$ . Then we can get a bound for the average loss by telescope summing the decomposition. Here we also introduced the following vector $\\bar{\\bf w}_{*}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{w}}_{*}:=\\left(\\begin{array}{l}{a_{1}\\mathbf{w}_{*}}\\\\ {\\quad\\vdots}\\\\ {a_{n}\\mathbf{w}_{*}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can observe that $\\begin{array}{r}{\\mathbf{u}_{1}=\\frac{\\log(\\gamma^{2}\\eta T)+\\kappa}{\\alpha\\gamma}\\bar{\\mathbf{w}}_{*}}\\end{array}$ log(\u03b3\u03b7T )+\u03baw\u00af\u2217and u2 = $\\begin{array}{r}{\\mathbf{u}_{2}=\\frac{\\eta}{2\\gamma}\\bar{\\mathbf{w}}_{\\ast}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma B.1 (A bound on $I_{1}(\\mathbf{w})$ in the EoS phase). For $\\mathbf{u}_{1}$ defined in (17), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nI_{1}(\\mathbf{w}):=\\langle\\nabla L(\\mathbf{w}),\\mathbf{u}_{1}-\\mathbf{w}\\rangle\\leq\\frac{1}{\\gamma^{2}\\eta T}-L(\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since $L$ is averaged over the individual losses incurred at the data $(\\mathbf{x}_{i},y_{i})_{i=1}^{n}$ and gradient is a linear operator, it suffices to prove the claim assuming there is only a single data point $\\left(\\mathbf{x},y\\right)$ . Then by Assumption 3, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle y\\mathbf{x},\\mathbf{w}_{\\ast}\\rangle\\geq\\gamma>0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the loss becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\nL(\\mathbf{w})=\\ell(y f(\\mathbf{w};\\mathbf{x}))=\\ell\\bigg(y\\frac{1}{m}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)})\\bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we expand $I_{1}(\\mathbf{w})$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}(\\mathbf{w}):=\\langle\\nabla L(\\mathbf{w}),\\mathbf{u}_{1}-\\mathbf{w}\\rangle}\\\\ &{\\qquad=\\varepsilon^{\\prime}(y f(\\mathbf{w};\\mathbf{x}))\\langle y\\nabla f(\\mathbf{w};\\mathbf{x}),\\mathbf{u}_{1}-\\mathbf{w}\\rangle}\\\\ &{\\qquad=\\varepsilon^{\\prime}(y f(\\mathbf{w};\\mathbf{x}))\\displaystyle\\frac{1}{m}\\sum_{k=1}^{m}a_{k}y\\phi^{\\prime}(\\boldsymbol{x}^{\\top}\\mathbf{w}^{(k)})\\mathbf{x}^{\\top}(\\mathbf{u}_{1}^{(k)}-\\mathbf{w}^{(k)})}\\\\ &{\\qquad=\\varepsilon^{\\prime}(y f(\\mathbf{w};\\mathbf{x}))\\left[\\displaystyle\\underbrace{\\frac{1}{m}\\sum_{k=1}^{m}a_{k}y\\left(\\phi^{\\prime}(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})\\mathbf{x}^{\\top}\\mathbf{u}_{1}^{(k)}+\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})-\\phi^{\\prime}(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})\\mathbf{x}^{\\top}\\mathbf{w}^{(k)}\\right)}_{=:J_{1}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\underbrace{1}_{\\underbrace{m}_{k=1}^{m}}\\displaystyle\\sum_{k=1}^{m}a_{k}y\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})\\right]\\cdot\\;\\;(19)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By definition we have $J_{2}=y f(\\mathbf{w};\\mathbf{x})$ . As for $J_{1}$ , using $\\phi^{\\prime}\\geq\\alpha$ and $a_{k}y x^{\\top}\\mathbf{u}_{1}^{(k)}\\geq0$ by Assumption 3, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle J_{1}:=\\frac{1}{m}\\sum_{k=1}^{m}a_{k}y\\Big(\\phi^{\\prime}({\\bf x}^{\\top}{\\bf w}^{(k)}){\\bf x}^{\\top}{\\bf u}_{1}^{(k)}+\\phi({\\bf x}^{\\top}{\\bf w}^{(k)})-\\phi^{\\prime}({\\bf x}^{\\top}{\\bf w}^{(k)}){\\bf x}^{\\top}{\\bf w}^{(k)}\\Big)}}\\\\ {{\\displaystyle\\quad\\geq\\frac{1}{m}\\sum_{k=1}^{m}a_{k}\\alpha y{\\bf x}^{\\top}{\\bf u}_{1}^{(k)}+\\frac{1}{m}\\sum_{k=1}^{m}a_{k}y\\Big(\\phi({\\bf x}^{\\top}{\\bf w}^{(k)})-\\phi^{\\prime}({\\bf x}^{\\top}{\\bf w}^{(k)}){\\bf x}^{\\top}{\\bf w}^{(k)}\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\geq\\frac{1}{m}\\displaystyle\\sum_{k=1}^{m}a_{k}^{2}\\alpha\\frac{\\log(\\gamma^{2}\\eta T)+\\kappa}{\\alpha\\gamma}y\\mathbf{x}^{\\top}\\mathbf{w}_{*}-\\frac{1}{m}\\displaystyle\\sum_{k=1}^{m}|a_{k}|}\\\\ &{\\quad\\displaystyle\\ s\\mathrm{ince}\\,|\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})-\\phi^{\\prime}(\\mathbf{x}^{\\top}\\mathbf{w}^{(k)})\\mathbf{x}^{\\top}\\mathbf{w}^{(k)}|\\leq\\kappa}\\\\ &{\\displaystyle\\geq\\log(\\gamma^{2}\\eta T)+\\kappa-\\kappa}\\\\ &{\\quad\\mathrm{~since~}y\\mathbf{x}^{\\top}\\mathbf{w}_{*}\\geq\\gamma\\mathrm{~and~}\\displaystyle\\sum_{k=1}^{m}a_{k}^{2}=m}\\\\ &{\\displaystyle=\\log(\\gamma^{2}\\eta T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging in $J_{2}=y f(\\mathbf{w};\\mathbf{x})$ and (20) into (19), we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}(\\mathbf{w})=\\langle\\nabla L(\\mathbf{w}),\\mathbf{u}_{1}-\\mathbf{w}\\rangle=\\ell^{\\prime}\\big(y f(\\mathbf{w};\\mathbf{x})\\big)(J_{1}-J_{2})}\\\\ &{\\qquad\\quad\\leq\\ell^{\\prime}\\big(y f(\\mathbf{w};\\mathbf{x})\\big)\\Big[\\log(\\gamma^{2}\\eta T)-y f(\\mathbf{w};\\mathbf{x})\\Big]}\\\\ &{\\qquad\\quad\\leq\\ell(\\log(\\gamma^{2}\\eta T))-\\ell(y f(\\mathbf{w};\\mathbf{x}))}\\\\ &{\\qquad\\quad\\leq\\frac{1}{\\gamma^{2}\\eta T}-L(\\mathbf{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last inequality, we use $\\ell(x)\\leq\\exp(-x)$ and we only consider a single data point. This completes the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Lemma B.2 (A bound on $I_{2}(\\mathbf{w})$ in EoS). For $\\mathbf{u}_{2}$ defined in (18), for every w, ", "page_idx": 26}, {"type": "equation", "text": "$$\nI_{2}(\\mathbf{w}):=2\\langle\\nabla L(\\mathbf{w}),\\mathbf{u}_{2}\\rangle+m\\eta\\|\\nabla L(\\mathbf{w})\\|^{2}\\leq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. For simplicity, we define ", "page_idx": 26}, {"type": "equation", "text": "$$\ng_{i}(\\mathbf{w}^{(j)}):=\\ell^{\\prime}(y_{i}f(\\mathbf{w};\\mathbf{x}_{i}))\\phi^{\\prime}(\\mathbf{x}_{i}^{\\top}\\mathbf{w}^{(j)}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $-1\\leq\\ell^{\\prime}(\\cdot)\\leq0$ and $0<\\alpha\\leq\\phi^{\\prime}(\\cdot)\\leq1$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n-1\\leq g_{i}(\\mathbf{w}^{(j)})\\leq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Under this notation, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}({\\bf w})}{\\partial{\\bf w}_{i}}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}\\big(y_{i}f({\\bf w};{\\bf x}_{i})\\big)y_{i}a_{j}m^{-1}\\phi^{\\prime}\\big({\\bf x}_{i}^{\\top}{\\bf w}^{(j)}\\big){\\bf x}_{i}}\\\\ {\\displaystyle\\qquad=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\\big({\\bf w}^{(j)}\\big)a_{j}m^{-1}y_{i}{\\bf x}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal I}_{2}({\\bf w}):=2\\langle\\nabla L({\\bf w}),{\\bf u}_{2}\\rangle+m\\eta\\|\\nabla L({\\bf w})\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~=\\frac{1}{m}\\sum_{j=1}^{m}\\biggl[\\frac{2}{n}\\sum_{i=1}^{n}g_{i}({\\bf w}^{(j)})a_{j}y_{i}\\cdot{\\bf x}_{i}^{\\top}{\\bf u}_{2}^{(j)}+\\eta\\biggl\\|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}({\\bf w}^{(j)})a_{j}y_{i}{\\bf x}_{i}\\biggr\\|^{2}\\biggr]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the term inside the bracket, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}y_{i}\\cdot\\mathbf{x}_{i}^{\\top}\\mathbf{u}_{2}^{(j)}+\\eta\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}y_{i}\\mathbf{x}_{i}\\right\\|^{2}}&\\\\ {\\displaystyle=\\!\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}y_{i}\\cdot\\mathbf{x}_{i}^{\\top}\\frac{\\eta}{2\\gamma}a_{j}\\mathbf{w}_{*}+\\eta\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}y_{i}\\mathbf{x}_{i}\\right\\|^{2}}&{\\mathrm{since~}\\mathbf{u}_{2}^{(j)}:=\\frac{\\eta a_{j}}{2\\gamma}\\mathbf{w}_{*}\\mathrm{~by~(18)}}\\\\ {\\displaystyle\\leq\\!\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}\\frac{\\eta}{2\\gamma}\\gamma+\\eta\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})a_{j}y_{i}\\mathbf{x}_{i}\\right\\|^{2}}&{\\mathrm{since~}g_{i}(\\cdot)\\leq0\\mathrm{~and~}y_{i}x_{i}^{\\top}\\mathbf{w}_{*}\\geq}\\\\ {\\displaystyle=\\!\\eta\\!\\left(\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})+\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})y_{i}\\mathbf{x}_{i}\\right\\|^{2}\\right)}&{\\mathrm{since~}a_{j}^{2}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\leq\\!\\eta\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}(\\mathbf{w}^{(j)})+\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{2}(\\mathbf{w}^{(j)})\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{since}-1\\leq g_{i}(\\cdot)\\leq0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, we prove that $I_{2}(\\mathbf{w})\\leq0$ . ", "page_idx": 27}, {"type": "text", "text": "Theorem B.3 (A split optimization bound). For every $\\eta>0$ and $\\mathbf{u}=\\mathbf{u}_{1}+\\mathbf{u}_{2}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{u}_{1}:=\\left(\\begin{array}{l}{\\mathbf{u}_{1}^{(1)}}\\\\ {\\vdots}\\\\ {\\mathbf{u}_{1}^{(m)}}\\end{array}\\right),\\ \\ w i t h\\ \\mathbf{u}_{1}^{(j)}:=a_{j}\\frac{\\log(\\gamma^{2}\\eta t)+\\kappa}{\\alpha\\gamma}\\cdot\\mathbf{w}_{*},\\quad j=1,\\ldots,m,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{u}_{2}:=\\left(\\begin{array}{l}{\\mathbf{u}_{2}^{(1)}}\\\\ {\\vdots}\\\\ {\\mathbf{u}_{2}^{(m)}}\\end{array}\\right),\\ \\ w i t h\\ \\mathbf{u}_{2}^{(j)}:=a_{j}\\frac{\\eta}{2\\gamma}\\cdot\\mathbf{w}_{\\ast},\\ \\ \\ j=1,\\dots,m.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{w}_{T}-\\mathbf{u}\\|^{2}}{2m\\eta T}+\\frac{1}{T}\\sum_{k=0}^{T-1}L\\bigl(\\mathbf{w}^{(k)}\\bigr)\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta T)/\\alpha^{2}+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}}{\\gamma^{2}\\eta T}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta T},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $T$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. By Lemma B.1 and Lemma B.2, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\mathbf{w}_{t+1}-\\mathbf{u}\\|^{2}=\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}+2m\\eta I_{1}(\\mathbf{w}_{t})+\\eta m I_{2}(\\mathbf{w}_{t})}&{}\\\\ {\\leq\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}+2m\\eta I_{1}(\\mathbf{w}_{t})}&{}\\\\ {\\leq\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}+2m\\eta\\Big(\\frac{1}{\\gamma^{2}\\eta T}-L(\\mathbf{w}_{t})\\Big).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Telescoping the sum, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{w}_{T}-\\mathbf{u}\\|^{2}}{2m\\eta}+\\sum_{t=0}^{T-1}L(\\mathbf{w}_{t})\\leq1+\\frac{\\|\\mathbf{w}_{0}-\\mathbf{u}\\|^{2}}{2m\\eta}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By (17)and (18), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\mathbf{w}_{0}-\\mathbf{u}\\right\\|^{2}\\leq2\\|\\mathbf{w}_{0}\\|^{2}+2\\|\\mathbf{u}\\|^{2}}\\\\ {\\leq2\\|\\mathbf{w}_{0}\\|_{2}^{2}+4\\|\\mathbf{u}_{1}\\|^{2}+4\\|\\mathbf{u}_{2}\\|^{2}}\\\\ {=2\\|\\mathbf{w}_{0}\\|_{2}^{2}+\\frac{8m\\log(\\gamma^{2}\\eta T)^{2}+8m\\kappa^{2}}{\\alpha^{2}\\gamma^{2}}+\\frac{m\\eta^{2}}{\\gamma^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{w}_{T}-\\mathbf{u}\\|^{2}}{2m\\eta t}+\\frac{1}{T}\\sum_{k=0}^{T-1}L\\big(\\mathbf{w}^{(k)}\\big)\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta T)/\\alpha^{2}+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}}{\\gamma^{2}\\eta T}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta T}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We complete the proof. ", "page_idx": 27}, {"type": "text", "text": "B.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 3.2. By Theorem B.3, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{k=0}^{T-1}L\\big(\\mathbf{w}^{(k)}\\big)\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta T)/\\alpha^{2}+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}}{\\gamma^{2}\\eta T}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta T}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This completes the proof. ", "page_idx": 27}, {"type": "text", "text": "C Phase Transition Analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we will analyze the phase transition. In detail, we follow the idea of [Wu et al., 2024] and apply the perceptron argument [Novikoff, 1962] to locate the phase transition time. Compare to the previous EoS phase analysis, we need an extra assumption on the smoothness of the activation function, which is the Assumption 2B. ", "page_idx": 28}, {"type": "text", "text": "To proceed, let us define the following quantities for the GD process: ", "page_idx": 28}, {"type": "equation", "text": "$$\nG(\\mathbf{w}):=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{1+\\exp\\big(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\big)},\\quad F(\\mathbf{w}):=\\frac{1}{n}\\sum_{i=1}^{n}\\exp\\big(-y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Due to the self-boundedness of the logistic function, we can show that $G(\\mathbf{w}),L(\\mathbf{w}),F(\\mathbf{w})$ are equivalent in the following sense. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.1 (Equivalence of $G,L,F)$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. The first claim is by the property of the logistic loss. For the second one, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla L(\\mathbf{w})\\|^{2}=\\displaystyle\\sum_{j=1}^{m}\\!\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f(\\mathbf{w};\\mathbf{x}_{i}))\\cdot y_{i}\\cdot a_{j}m^{-1}\\phi(\\mathbf{x}_{i}^{\\top}\\mathbf{w}^{(j)})\\mathbf{x}_{i}\\right\\|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{m}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}f(\\mathbf{w};\\mathbf{x}_{i}))\\cdot m^{-1}\\right)^{2}}&{\\mathrm{since~}\\|y_{i}a_{j}\\phi(\\mathbf{x}_{i}^{\\top}\\mathbf{w}^{(j)})\\mathbf{x}_{i}\\|}\\\\ &{=\\displaystyle\\frac{1}{m}G^{2}(\\mathbf{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Besides, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{\\left/m\\right\\|\\nabla L(\\mathbf{w})\\left\\|\\ge\\left\\langle-\\nabla L(\\mathbf{w}),\\overline{{\\mathbf{w}}}_{*}\\right\\rangle}}}&{\\mathrm{since~\\|\\bar{\\mathbf{w}}_*\\|\\le\\sqrt{m}~}}\\\\ &{}&{=-\\frac{1}{n m}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\ell^{\\prime}(y_{i}f(\\mathbf{w};\\mathbf{x}_{i}))y_{i}\\phi^{\\prime}(\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{*})\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{*}}\\\\ &{}&{\\ge\\alpha\\gamma\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\frac{1}{1+\\exp\\left(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\right)}}&{\\mathrm{since~}\\phi^{\\prime}\\ge\\alpha\\mathrm{~and~}y_{i}\\mathbf{x}_{i}^{\\top}\\mathbf{w}^{*}\\ge\\gamma}\\\\ &{}&{=\\alpha\\gamma G(\\mathbf{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the third claim, by the assumption, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\cdot\\frac{1}{1+\\exp\\left(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\right)}\\leq G(\\mathbf{w})\\leq\\frac{1}{2n},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\ny_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\geq0,\\quad\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\nG(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{1+\\exp\\bigl(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\bigr)}\\geq\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2\\exp\\bigl(y_{i}f(\\mathbf{w};\\mathbf{x}_{i})\\bigr)}=\\frac{1}{2}F(\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We complete the proof. ", "page_idx": 28}, {"type": "text", "text": "The key ingredient of the phase transition analysis is the following lemma. The main idea is to consider the gradient potential $G(\\mathbf{w})$ instead of the loss function $L(\\mathbf{w})$ in EoS phase. And this will decrease the order of the bound of phase transition time from $\\tilde{O}(\\eta^{2})$ to $\\tilde{\\cal{O}}(\\eta)$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma C.2 (A bound of $\\left\\Vert\\mathbf{w}_{t}\\right\\Vert$ ). For every $\\eta_{:}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}_{t}\\|\\leq\\sqrt{m}\\cdot\\frac{2+8\\log(\\gamma^{2}\\eta t)/\\alpha+8\\kappa/\\alpha+4\\eta}{\\gamma}+2\\|\\mathbf{w}_{0}\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma C.2. By Theorem B.3, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}}{2m\\eta t}\\leq\\frac{\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}}{2m\\eta t}+\\frac{1}{t}\\sum_{k=0}^{t-1}L\\big(\\mathbf{w}^{(k)}\\big)\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta t)/\\alpha^{2}+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}}{\\gamma^{2}\\eta t}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta t}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Besides, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{u}\\|^{2}\\leq2\\|\\mathbf{u}_{1}\\|^{2}+2\\|\\mathbf{u}_{2}\\|^{2}={\\frac{4m\\log(\\gamma^{2}\\eta t)^{2}+4m\\kappa^{2}}{\\alpha^{2}\\gamma^{2}}}+{\\frac{m\\eta^{2}}{2\\gamma^{2}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining them, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}_{t}\\|^{2}\\leq2\\|\\mathbf{w}_{t}-\\mathbf{u}\\|^{2}+2\\|\\mathbf{u}\\|^{2}\\leq m\\cdot\\frac{2+24\\log^{2}(\\gamma^{2}\\eta t)/\\alpha^{2}+24\\kappa^{2}/\\alpha^{2}+3\\eta^{2}}{\\gamma^{2}}+2\\|\\mathbf{w}_{0}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, we can get a bound for $\\left\\Vert\\mathbf{w}_{t}\\right\\Vert$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}_{t}\\|\\leq\\sqrt{m}\\cdot\\frac{2+8\\log(\\gamma^{2}\\eta t)/\\alpha+8\\kappa/\\alpha+4\\eta}{\\gamma}+2\\|\\mathbf{w}_{0}\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma C.3 (Gradient potential bound in the EoS phase). For every $\\eta_{:}$ , we have ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\sum_{k=0}^{t-1}G(\\mathbf{w}^{(k)})\\leq\\frac{\\langle\\mathbf{w}_{t},\\bar{\\mathbf{w}}_{*}\\rangle-\\langle\\mathbf{w}_{0},\\bar{\\mathbf{w}}_{*}\\rangle}{m\\alpha\\gamma\\eta t}\\leq\\frac{\\sqrt{m}\\|\\mathbf{w}_{t}\\|-\\langle\\mathbf{w}_{0},\\bar{\\mathbf{w}}_{*}\\rangle}{m\\alpha\\gamma\\eta t},\\quad t\\geq1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Additionally, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\sum_{k=0}^{t-1}G(\\mathbf{w}^{(k)})\\leq\\frac{2+8\\log\\left(\\gamma^{2}\\eta t\\right)/\\alpha+8\\kappa/\\alpha+4\\eta}{\\alpha\\gamma^{2}\\eta t}+\\frac{3\\|\\mathbf{w}_{0}\\|}{\\alpha\\gamma\\eta t},\\quad t\\geq1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This ", "page_idx": 29}, {"type": "text", "text": "Proof. This is from the perceptron argument [Novikoff, 1962]. Specifically, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf w_{t+1},\\bar{\\mathbf w}_{*}\\rangle=\\langle\\mathbf w_{t},\\bar{\\mathbf w}_{*}\\rangle-m\\eta\\langle\\nabla L(\\mathbf w_{t}),\\bar{\\mathbf w}_{*}\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\mathbf w_{t},\\bar{\\mathbf w}_{*}\\rangle-\\eta\\displaystyle\\sum_{i=1}^{n}\\sum_{k=1}^{m}a_{k}^{2}\\ell^{\\prime}(y_{i}f(\\mathbf w_{t};\\mathbf x_{i}))y_{i}\\phi(\\mathbf x_{i}^{\\top}\\mathbf w_{t}^{(k)})\\langle\\mathbf x_{i},\\mathbf w_{*}\\rangle}\\\\ &{\\qquad\\qquad\\geq\\langle\\mathbf w_{t},\\bar{\\mathbf w}_{*}\\rangle-\\eta\\displaystyle\\sum_{i=1}^{n}\\sum_{k=1}^{m}a_{k}^{2}\\ell^{\\prime}(y_{i}f(\\mathbf w_{t};\\mathbf x_{i}))\\alpha\\gamma}\\\\ &{\\qquad\\qquad\\geq\\langle\\mathbf w_{t},\\bar{\\mathbf w}_{*}\\rangle+m\\alpha\\gamma\\eta G(\\mathbf w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Telescoping the sum, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{t}\\sum_{k=0}^{t-1}G(\\mathbf{w}^{(k)})\\leq\\frac{\\left\\langle\\mathbf{w}_{t},\\bar{\\mathbf{w}}_{*}\\right\\rangle-\\left\\langle\\mathbf{w}_{0},\\bar{\\mathbf{w}}_{*}\\right\\rangle}{m\\alpha\\gamma\\eta t}}}\\\\ &{}&{\\leq\\frac{\\sqrt{m}\\left\\|\\mathbf{w}_{t}\\right\\|-\\left\\langle\\mathbf{w}_{0},\\bar{\\mathbf{w}}_{*}\\right\\rangle}{m\\alpha\\gamma\\eta t}}\\\\ &{}&{\\leq\\frac{2+8\\log\\left(\\gamma^{2}\\eta t\\right)/\\alpha+8\\kappa/\\alpha+4\\eta}{\\alpha\\gamma^{2}\\eta t}+\\frac{3\\left\\|\\mathbf{w}_{0}\\right\\|}{\\sqrt{m}\\alpha\\gamma\\eta t}.\\qquad\\mathrm{~by~Lemma~C.2~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We have completed the proof. ", "page_idx": 29}, {"type": "text", "text": "Besides, we can make use of the equivalence between $G$ and $L$ to get a bound for the loss function which is independent of the initial margin at $s$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma C.4 (A risk bound in the stable phase for Two-layer NN). Suppose that there exists a time s such that ", "page_idx": 30}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\bigg\\{\\frac{1}{\\eta(4+2\\tilde{\\beta})},\\frac{1}{2e^{\\kappa+2}n}\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then for every $t\\ge s+1$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{t})\\leq\\frac{2}{(t-s)\\alpha^{2}\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Lemma A.3 and $f(x)$ is $\\textstyle{\\frac{1}{\\sqrt{m}}}$ Lipschitz and $\\frac{\\tilde{\\beta}}{m}$ smooth, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{k+1}\\leq L_{k}-m\\eta(1-(2+\\tilde{\\beta})\\eta L(\\mathbf{w}_{k}))\\|\\nabla L_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma C.1 and $\\begin{array}{r}{L_{t}\\leq\\frac{1}{\\eta(4+2\\tilde{\\beta})}}\\end{array}$ \u03b7(4+12 \u03b2\u02dc), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{k+1}\\leq L_{k}-\\frac{\\alpha^{2}\\gamma^{2}}{2}L_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Multiplying $\\frac{1}{L_{k}^{2}}$ in both sides, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\alpha^{2}\\gamma^{2}}{2}\\leq\\frac{L_{t}-L_{k+1}}{L_{k}^{2}}\\leq\\frac{1}{L_{k+1}}-\\frac{1}{L_{k}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking summation for $k=s,\\ldots,t-1$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{L_{t}}>\\frac{1}{L_{t}}-\\frac{1}{L_{s}}\\geq\\frac{(t-s)\\alpha^{2}\\gamma^{2}}{2}\\implies L_{t}\\leq\\frac{2}{(t-s)\\alpha^{2}\\gamma^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "At last, we will use the bound for the gradient potential to get an upper bound for the phase transition time. ", "page_idx": 30}, {"type": "text", "text": "C.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Theorem 4.1. Applying Lemma C.3, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{\\tau}\\sum_{k=0}^{\\tau-1}G(\\mathbf w^{(k)})\\leq\\frac{2+8\\log\\big(\\gamma^{2}\\eta\\tau\\big)/\\alpha+8\\kappa/\\alpha+4\\eta}{\\alpha\\gamma^{2}\\eta\\tau}+\\frac{3\\|\\mathbf w_{0}\\|}{\\sqrt{m}\\alpha\\gamma\\eta\\tau}}}\\\\ &{\\leq\\frac{2+8\\kappa/\\alpha+8\\log(\\gamma^{2}\\tau)/\\alpha+\\big(4+8/\\alpha\\big)\\eta}{\\alpha\\gamma^{2}\\eta\\tau}+\\frac{3\\|\\mathbf w_{0}\\|}{\\sqrt{m}\\alpha\\gamma\\eta\\tau}\\quad\\mathrm{since~log}(\\eta)\\leq\\eta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $c_{1}=4e^{\\kappa+2},c_{2}=(8+4\\tilde{\\beta})$ . Note that we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{2+8\\kappa/\\alpha}{\\alpha\\gamma^{2}\\eta\\tau}\\leq\\frac{1}{4(c_{1}n+c_{2}\\eta)}\\ensuremath{\\mathrm{~if~}}\\gamma^{2}\\tau\\geq4(2+8\\kappa)\\frac{c_{2}\\eta+c_{1}n}{\\eta\\alpha^{2}}}\\\\ &{\\frac{8\\log(\\gamma^{2}\\tau)/\\alpha}{\\alpha\\gamma^{2}\\eta\\tau}\\leq\\frac{1}{4(c_{1}n+c_{2}\\eta)}\\ensuremath{\\mathrm{~if~}}\\gamma^{2}\\tau\\geq128\\frac{c_{2}\\eta+c_{1}n}{\\eta\\alpha^{2}}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta},\\mathrm{since~Lemma~G.S}}\\\\ &{\\quad\\frac{(4+8/\\alpha)\\eta}{\\alpha\\gamma^{2}\\eta\\tau}\\leq\\frac{1}{4(c_{1}n+c_{2}\\eta)}\\ensuremath{\\mathrm{~if~}}\\gamma^{2}\\tau\\geq\\frac{48}{\\alpha^{2}}(c_{2}\\eta+c_{1}n),}\\\\ &{\\quad\\frac{3\\ensuremath{\\|\\mathbf{w}_{0}\\parallel}}{m\\alpha\\gamma\\eta\\tau}\\leq\\frac{1}{4(c_{1}n+c_{2}\\eta)}\\ensuremath{\\mathrm{~if~}}\\gamma\\tau\\geq\\frac{12}{\\alpha}\\frac{(c_{2}\\eta+c_{1}n)}{\\eta}\\cdot\\frac{\\ensuremath{\\|\\mathbf{w}_{0}\\parallel}}{\\sqrt{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and that the two conditions are satisfied because ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma^{2}\\tau:=\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\operatorname*{max}\\left\\lbrace c_{2}\\eta,c_{1}n,e,\\frac{c_{2}\\eta+c_{1}n}{\\eta}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta},\\frac{(c_{2}\\eta+c_{1}n)\\|\\mathbf{w}_{0}\\|}{\\eta\\sqrt{m}}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\geq\\operatorname*{max}\\left\\{4(2+8\\kappa)\\frac{c_{2}\\eta+c_{1}n}{\\eta\\alpha^{2}},128\\frac{c_{2}\\eta+c_{1}n}{\\eta\\alpha^{2}}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta},\\frac{48}{\\alpha^{2}}(c_{2}\\eta+c_{1}n),\\frac{12}{\\alpha}\\frac{(c_{2}\\eta+c_{1}n)}{\\eta}\\cdot\\frac{\\|\\mathbf{w}\\|^{2}}{\\sqrt{\\eta}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "So there exits $s\\leq\\tau$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nG(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\left\\{\\frac{1}{e^{\\kappa+2}4n},\\frac{1}{\\eta(8+4\\tilde{\\beta})}\\right\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then we have $\\begin{array}{r}{L(\\mathbf{w}_{s})\\leq F(\\mathbf{w}_{s})\\leq2G(\\mathbf{w}_{s})\\leq\\left\\{\\frac{1}{e^{\\kappa+2}2n},\\frac{1}{\\eta(4+2\\tilde{\\beta})}\\right\\}}\\end{array}$ . We complete the proof. ", "page_idx": 31}, {"type": "text", "text": "C.2 Proof of Corollary 4.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Corollary 4.2. The main idea is to show that $\\begin{array}{r}{\\tau\\leq\\frac{T}{2}}\\end{array}$ . Note that by Theorem 4.1, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau=\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\operatorname*{max}\\bigg\\{c_{2}\\eta,c_{1}n,e,\\frac{c_{2}\\eta+c_{1}n}{\\eta}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta},\\frac{(c_{2}\\eta+c_{1}n)}{\\eta}\\cdot\\frac{\\|\\mathbf{w}_{0}\\|}{\\sqrt{m}}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "in which expression $c_{1}=4e^{\\kappa+2}$ and $c_{2}=(8+4\\tilde{\\beta})$ . We can verify that, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{128(1+4\\kappa)}{\\alpha^{2}}c_{2}\\eta=\\frac{128(1+4\\kappa)}{\\alpha^{2}}c_{2}\\cdot\\frac{\\alpha^{2}\\gamma^{2}}{256(1+4\\kappa)c_{2}}T=\\frac{T}{2},}\\\\ &{\\displaystyle\\frac{128(1+4\\kappa)c_{1}n}{\\alpha^{2}}\\leq\\frac{T}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Furthermore, we have $\\begin{array}{r}{n\\leq\\frac{\\alpha^{2}\\gamma^{2}T}{256(1+4\\kappa)c_{1}}}\\end{array}$ . Hence, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{c_{2}\\eta+c_{1}n}{\\eta}=\\frac{\\frac{\\alpha^{2}\\gamma^{2}T}{256(1+4\\kappa)}+c_{1}n}{\\frac{\\alpha^{2}\\gamma^{2}T}{256(1+4\\kappa)c_{2}}}\\leq\\frac{2\\cdot\\frac{\\alpha^{2}\\gamma^{2}T}{256(1+4\\kappa)}}{\\frac{\\alpha^{2}\\gamma^{2}T}{256(1+4\\kappa)c_{2}}}\\leq2c_{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We get that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\cdot\\frac{c_{2}\\eta+c_{1}n}{\\eta}\\log\\frac{c_{2}\\eta+c_{1}n}{\\eta}\\leq2\\frac{128(1+4\\kappa)}{\\alpha^{2}}c_{2}\\ln(2c_{2})\\leq\\frac{128(1+4\\kappa)}{\\alpha^{2}}4c_{2}^{2}\\leq\\frac{T}{2},}\\\\ &{\\qquad\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\cdot\\frac{(c_{2}\\eta+c_{1}n)}{\\eta}\\cdot\\frac{\\|{\\bf w}_{0}\\|}{\\sqrt{m}}\\leq\\frac{128(1+4\\kappa)}{\\alpha^{2}}\\cdot2c_{2}\\frac{\\|{\\bf w}_{0}\\|}{\\sqrt{m}}\\leq\\frac{T}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, we have $\\begin{array}{r}{\\tau\\leq\\frac{T}{2}}\\end{array}$ . Applying Theorem 4.1, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{T})\\leq\\frac{2}{\\alpha^{2}\\gamma^{2}\\eta(T-\\tau)}\\leq\\frac{4}{\\alpha^{2}\\gamma^{2}\\eta T}\\leq\\frac{2048(1+4\\kappa)c_{2}}{\\alpha^{4}\\gamma^{4}T^{2}}=\\mathcal{O}(1/T^{2}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have completed the proof. ", "page_idx": 31}, {"type": "text", "text": "C.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 4.3. The main idea is to construct an upper bound of $\\eta$ and apply the analysis in Theorem 2.2. Note that give $\\mathbf{w}_{0}=0$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(\\mathbf{w}_{0};\\mathbf{x}_{i})=\\frac{1}{m}\\sum_{k=1}^{m}a_{k}\\phi(\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{0}^{(k)})=s_{a}\\phi(0),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\textstyle s_{a}=\\sum_{k=1}^{m}a_{k}/m$ . Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\nabla L(\\mathbf{w}_{0})]^{(k)}=\\displaystyle\\frac{1}{2}\\ell^{\\prime}(s_{a}\\phi(0))\\cdot\\frac{a_{k}}{m}\\phi^{\\prime}(0)\\mathbf{x}_{1}+\\frac{1}{2}\\ell^{\\prime}(s_{a}\\phi(0))\\cdot\\frac{a_{k}}{m}\\phi^{\\prime}(0)\\mathbf{x}_{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{a_{k}}{m}\\ell^{\\prime}(s_{a}\\phi(0))\\phi^{\\prime}(0)\\frac{\\mathbf{x}_{1}+\\mathbf{x}_{2}}{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{a_{k}}{m}\\ell^{\\prime}(s_{a}\\phi(0))\\phi^{\\prime}(0)(\\gamma,\\frac{\\sqrt{1-\\gamma^{2}}}{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{\\mathbf{x}}:=\\frac{1}{m}\\ell^{\\prime}(s_{a}\\phi(0))\\phi^{\\prime}(0)(\\gamma,\\frac{\\sqrt{1-\\gamma^{2}}}{4})}\\end{array}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{w}_{1}^{(k)}=0-\\eta\\nabla[L(\\mathbf{w}_{0})]^{(k)}=-\\eta a_{k}\\bar{\\mathbf{x}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\nf(\\mathbf{w}_{1};\\mathbf{x}_{i})=\\frac{1}{m}\\sum_{k=1}^{m}a_{k}\\phi(-\\mathbf{x}_{i}^{\\top}(\\eta a_{k}\\bar{\\mathbf{x}})).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can notice that $-\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}<0$ and $-\\mathbf{x}_{2}^{\\top}\\bar{\\mathbf{x}}>0$ , when $\\gamma\\le0.1$ . Furthermore, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(\\mathbf{w}_{1};\\mathbf{x}_{1})=\\frac{1}{m}\\sum_{a_{k}=1}\\phi(-\\mathbf{x}_{1}^{\\top}(\\boldsymbol{\\eta}\\bar{\\mathbf{x}}))+\\frac{1}{m}\\sum_{a_{k}=-1}-\\phi(\\mathbf{x}_{1}^{\\top}(\\boldsymbol{\\eta}\\bar{\\mathbf{x}}))}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{1}{m}\\sum_{a_{k}=1}[\\phi(0)-\\mathbf{x}_{1}^{\\top}(\\boldsymbol{\\eta}\\bar{\\mathbf{x}})\\phi^{\\prime}(\\epsilon_{1})]+\\frac{1}{m}\\sum_{a_{k}=-1}[-\\phi(0)-\\mathbf{x}_{1}^{\\top}(\\boldsymbol{\\eta}\\bar{\\mathbf{x}})\\phi^{\\prime}(\\epsilon_{2})]}\\\\ {\\displaystyle\\qquad=s_{a}\\phi(0)-\\eta\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}\\frac{1}{m}[\\displaystyle\\sum_{a_{k}=1}\\phi^{\\prime}(\\epsilon_{1})+\\displaystyle\\sum_{a_{k}=-1}\\phi^{\\prime}(\\epsilon_{2})]}\\\\ {\\displaystyle\\qquad\\leq s_{a}\\phi(0)-\\eta\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(\\epsilon_{i})\\geq\\alpha.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac12\\ell(s_{a}\\phi(0)-\\eta\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}\\alpha)\\le\\frac12\\ell(f(\\mathbf{w}_{1};\\mathbf{x}_{1}))\\le L(\\mathbf{w}_{1})\\le L(\\mathbf{w}_{0})=\\ell(s_{a}\\phi(0)).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We apply Lemma G.7 to get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{|s_{a}\\phi(0)|+\\ln3}{\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}\\alpha}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We use $\\begin{array}{r}{c_{3}:=\\frac{|s_{a}\\phi(0)|+\\ln3}{\\mathbf{x}_{1}^{\\top}\\bar{\\mathbf{x}}\\alpha}}\\end{array}$ . Now we know $\\eta\\leq c_{3}$ . Furthermore, notice that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\nabla L(\\mathbf{w}_{t})\\|\\leq L_{t}\\leq L_{0}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We get that $\\|\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\|\\leq\\eta L_{0}\\leq c_{3}L_{0}$ . Hence, ", "page_idx": 32}, {"type": "equation", "text": "$$\n|f(\\mathbf{w}_{t+1};\\mathbf{x}_{i})-f(\\mathbf{w}_{t};\\mathbf{x}_{i})|\\leq c_{3}L_{0}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Assume that $\\begin{array}{r}{l_{b}=\\operatorname*{min}\\left\\{\\frac{1}{e^{\\kappa+2}4n},\\frac{1}{\\eta\\left(8+4\\tilde{\\beta}\\right)}\\right\\}}\\end{array}$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{s-1}\\geq l_{b},\\quad L_{s}\\leq l_{b}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We know that ", "page_idx": 32}, {"type": "equation", "text": "$$\nl_{b}\\ge\\operatorname*{min}\\left\\{\\frac{1}{e^{\\kappa+2}4n},\\frac{1}{c_{3}(8+4\\tilde{\\beta})}\\right\\}=:l_{c}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We want to show that there is an lower bound for $L_{s}$ . Now that ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{s}=\\frac{1}{2}\\Big[\\ell(f(\\mathbf{w}_{s};\\mathbf{x}_{1}))+\\ell(f(\\mathbf{w}_{s};\\mathbf{x}_{2}))\\Big].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying Lemma G.6, we can get that ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{s}\\geq\\exp(-c_{3}L_{0})L_{s-1}\\geq\\exp(-c_{3}L_{0})l_{b}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that by Lemma A.14, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{t}\\ge\\frac{1}{\\frac{1}{L_{s}}+3\\tilde{\\eta}\\rho^{2}(t-s)},\\quad t\\ge s.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combine this with $\\begin{array}{r}{\\rho=\\frac{1}{\\sqrt{m}},\\tilde{\\eta}=\\eta m}\\end{array}$ and $L_{s}\\ge\\exp(-c_{3}L_{0})l_{b}$ and we get ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{t}\\ge\\frac{1}{\\frac{\\exp(c_{3}L_{0})}{l_{b}}+3\\eta(t-s)},\\quad t\\ge s.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that when $t\\leq s,L_{t}\\geq l_{b}$ . We can get a lower bound for $L_{t}$ by ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{t}\\ge\\frac{1}{\\frac{\\exp(c_{3}L_{0})}{l_{b}}t+3\\eta t}\\ge\\frac{1}{\\frac{\\exp(c_{3}L_{0})}{l_{b}}t+3c_{3}t}\\ge\\frac{1}{\\frac{\\exp(c_{3}L_{0})}{l_{c}}t+3c_{3}t}=\\frac{c_{4}}{t},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where c4 = $\\begin{array}{r}{c_{4}=\\frac{1}{\\frac{\\exp(c_{3}L_{0})}{l_{c}}+3c_{3}}}\\end{array}$ depends only on $\\{a_{j}\\}_{j=1}^{m},\\phi(0),\\kappa,\\tilde{\\beta}$ and $n$ . ", "page_idx": 32}, {"type": "text", "text": "D Scaling and Homogenous Error ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we consider different scaling of two-layer networks. We add a scaling factor $b$ into the model, i.e., ", "page_idx": 33}, {"type": "equation", "text": "$$\nf(\\mathbf{w};\\mathbf{x})=\\frac{b}{m}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We will show that given a limited computation budget $T$ (total iterations), larger $b$ and a corresponding best ${\\tilde{\\eta}}=\\eta\\cdot m$ will achieve the same best rate as $b=1$ , i.e., ${\\cal O}(1/T^{2})$ . While for smaller $b$ , the rate is $O(b^{-3}/T^{2})$ . Before we present the analysis, here are the bounds with $b$ and $\\tilde{\\eta}=m\\cdot\\eta$ following the process of Lemma C.3: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{t}\\displaystyle\\sum_{k=0}^{t-1}L(\\mathbf{w}_{k})\\leq\\frac{1+8\\log^{2}(\\gamma^{2}\\eta t)/(\\alpha^{2}b^{2})+8\\kappa^{2}/\\alpha^{2}+\\eta^{2}b^{2}}{\\gamma^{2}\\eta t}+\\frac{\\|\\mathbf{w}_{0}\\|^{2}}{m\\eta t},}\\\\ {\\frac{1}{t}\\displaystyle\\sum_{k=0}^{t-1}G(\\mathbf{w}_{k})\\leq\\frac{2+8\\log(\\gamma^{2}\\eta t)/(\\alpha b)+8\\kappa/\\alpha+2\\eta b}{\\alpha\\gamma^{2}b\\eta t}+\\frac{3\\|\\mathbf{w}_{0}\\|}{\\sqrt{m}\\eta b t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Case when $b\\geq1$ . Given the previous bounds, we have the following results following the idea in Appendix C: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Gradient potential bound: $\\begin{array}{r}{G(\\mathbf{w}_{t})\\leq\\frac{C}{t}}\\end{array}$ for all $t\\geq0$ ,   \n\u2022 Phase transition threshold: $G(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\Big\\{1/4e^{\\kappa+2}n,1/\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b)\\Big\\},$   \n\u2022 Stable phase bound: L(wt) \u2264Cb2\u03b72(t\u2212s), ", "page_idx": 33}, {"type": "text", "text": "where $C$ depends on $\\alpha,\\gamma$ . Combine the first two arguments and assume $\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b)\\geq4e^{\\kappa+2}n$ We get $s\\leq C\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b)$ . Plug this into the third bound. We have ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{T})\\leq\\frac{2}{C b^{2}\\eta(T-C\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b))}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It\u2019s obvious that the best \u03b7 = 16\u03c12b2CT+8 \u03b2\u02dcbC . Hence, ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{T})\\leq\\frac{8(8\\rho^{2}b^{2}C+4\\tilde{\\beta}b C)}{C b^{2}T^{2}}=\\mathcal{O}\\bigg(\\frac{1}{T^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, the rate is still ${\\cal O}(1/T^{2})$ . ", "page_idx": 33}, {"type": "text", "text": "Case when $b<1$ . Similarly, we can get the following bounds: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Gradient potential bound: $\\begin{array}{r}{G(\\mathbf{w}_{t})\\leq\\frac{C b^{-2}}{t}}\\end{array}$ for all $t\\geq0$ , \u2022 Phase transition threshold: $G(\\mathbf{w}_{s})\\leq\\operatorname*{min}\\Big\\{1/4e^{\\kappa+2}n,1/\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b)\\Big\\},$ \u2022 Stable phase bound: L(wt) \u2264Cb2\u03b72(t\u2212s), ", "page_idx": 33}, {"type": "text", "text": "where $C$ depends on $\\alpha,\\gamma$ . Without loss of generality, we can assume $\\eta(8\\rho^{2}b^{2}+4\\tilde{\\beta}b)\\geq4e^{\\kappa+2}n$ , since $\\eta$ can be small enough. Then, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\ns\\leq C\\eta(8\\rho^{2}+4\\tilde{\\beta}b^{-1}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, we can get ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\cal L}({\\bf w}_{T})\\leq{\\frac{2}{C b^{2}\\eta(T-C\\eta(8\\rho^{2}+4\\tilde{\\beta}b^{-1}))}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It\u2019s obvious that the best \u03b7 = 16\u03c12C+8 \u03b2\u02dcb\u22121C . Hence, ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mathbf{w}_{T})\\leq\\frac{8(8\\rho^{2}C b^{-2}+4\\Tilde{\\beta}b^{-3}C)}{C T^{2}}=\\mathcal{O}\\Big(\\frac{b^{-3}}{T^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining the analysis for two cases, we observe that when $b\\geq1$ , the fast loss rate is $\\mathcal{O}(1/T^{2})$ given finite budget $T$ . While $b<1$ , the rate is $\\mathcal{O}(b^{-3}/T^{2})$ . In our main results, we set $b=1$ for the mean-field scaling. Under the mean-field regime, all bounds are independent of the number of neuro\u221ans since we consider the dynamics of the distributions of neurons. Alternatively, if we set $b={\\sqrt{m}}$ , then the model becomes: ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(\\mathbf{w};\\mathbf{x})=\\frac{1}{\\sqrt{m}}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The model falls into the NTK regime. The loss threshold will be related to $m$ , but the loss rate is the same as that of the mean-field scaling. ", "page_idx": 34}, {"type": "text", "text": "E Extra Experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Here we provide additional experiments to support our theoretical results. In Figure 3, we show the test accuracy of two-layer networks for CIFAR-10 under the same setting of Figure 2. We can observe that large stepsizes lead to stronger implicit biases with \u201cnicer\u201d features. ", "page_idx": 34}, {"type": "text", "text": "In Figure 4, we show the training loss and margins of a two-layer network with leaky softplus activations on a synthetic linear separable dataset. We can observe that both neurons have negative margins during the training, while the network\u2019s margin increases and becomes positive. This indicates that even the two-layer networks can have complicated dynamics. It remains an open question to understand each neuron\u2019s dynamics in deep networks. ", "page_idx": 34}, {"type": "image", "img_path": "chLoLUHnai/tmp/48de692056fa7e8e3dbc4b6dd0912aa67e8505e2df8cb9eb0a4ecfbe5bb001a6.jpg", "img_caption": ["Figure 3: Test accuracy of two-layer networks for CIFAR-10 under the same setting of Figure 2(d)-(f).The results support our intuition that large stepsizes lead to stronger implicit biases with \u201cnicer\u201c features. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "F Additional Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "F.1 Proof of Example 2.1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Example 2.1. Recall that the two-layer neural network is defined as: ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(\\mathbf{w};\\mathbf{x})=\\frac{1}{m}\\sum_{j=1}^{m}a_{j}\\phi(\\mathbf{x}^{T}\\mathbf{w}^{(j)}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can \u221averify that if $\\phi(x)$ is $\\beta$ -smooth and $\\rho$ -Lipschitz with respect to $x$ , then $f(\\mathbf{w};\\mathbf{x})$ is $\\beta/m$ -smooth and $\\rho/\\sqrt{m}$ -Lipschitz with respect to w. This is because: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla L(\\mathbf{w})=\\hat{\\mathbb{E}}\\ell^{\\prime}(y f(\\mathbf{w};\\mathbf{x}))y\\nabla f(\\mathbf{w};\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "image", "img_path": "chLoLUHnai/tmp/9ae90c0a0f469532791afebbe62c42316dc150107107c11de7468f5d795b03c9.jpg", "img_caption": ["(a) Training loss, synthetic dataset. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "chLoLUHnai/tmp/48d3f528c13e21cb7270a88cdaf7fe51deb29ceab02794e6db54811ce79a415e.jpg", "img_caption": ["", "(b) Normalized Margins, synthetic dataset. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 4: Training loss and margins of a two-layer network with leaky softplus activations on a synthetic linear separable dataset. There are five samples in the dataset, which are $(0.05,1,\\bar{2}),1),((0.05,-2,\\bar{1}),1),((-1,0,2),-1),((0.05,-2,-2\\bar{2},1),((0.05,1,-2),1),-1)$ . The max margin direction is $(1,0,0)$ with a normalized margin of 0.05. The network only has two neurons with fixed weights $1/2$ and $-1/2$ . The leaky softplus activation is $\\tilde{\\phi}(x)=(x+\\phi(x))/2$ , where $\\phi$ is the softplus activation. The stepsize is 3. We can observe that both neurons have negative margins during the training, while the network\u2019s margin increases and becomes positive. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla^{2}L(\\mathbf{w})=\\hat{\\mathbb{E}}\\ell^{\\prime\\prime}(y f(\\mathbf{w};\\mathbf{x}))\\nabla f(\\mathbf{w};\\mathbf{x})^{\\otimes2}+\\ell^{\\prime}(y f(\\mathbf{w};\\mathbf{x}))y\\nabla^{2}f(\\mathbf{w};\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{w};\\mathbf{x})=\\left(\\frac{\\vdots}{m}/\\sum_{\\mathbf{a}_{j}\\phi^{\\prime}}(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)})\\mathbf{x}\\right),\\quad\\nabla^{2}f(\\mathbf{w};\\mathbf{x})=\\left(\\begin{array}{l l l}{\\ddots}&{0}&{0}\\\\ {0}&{\\frac{1}{m}a_{j}\\phi^{\\prime\\prime}(\\mathbf{x}^{\\top}\\mathbf{w}^{(j)})\\mathbf{x}\\mathbf{x}^{\\top}}&{0}\\\\ {0}&{0}&{\\ddots}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, we will focus on the parameters of each activation function. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\phi(x)=x\\cdot\\mathrm{erf}(1+(x/\\sqrt{2}))/2=x\\cdot F(x).}}\\\\ {{\\phi^{\\prime}(x)=F(x)+x\\cdot f(x),}}\\\\ {{\\phi^{\\prime\\prime}(x)=2f(x)+x\\cdot f^{\\prime}(x),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $F(x),f(x)$ are the CDF and PDF of standard normal distribution. Note that $x f(x)=$ ${\\frac{x}{\\sqrt{2\\pi}}}e^{-x^{2}/2}$ and $\\begin{array}{r}{(x f(x))^{\\prime}=\\frac{1}{\\sqrt{2\\pi}}(1-x^{2})e^{-x^{2}/2}}\\end{array}$ . We can find the maximum of $x f(x)$ is $\\scriptstyle{\\frac{1}{\\sqrt{2\\pi}}}e^{-1/2}$ . Besides, we know that $F(x),f(x)\\leq1$ and $x\\cdot f^{\\prime}(x)\\leq0$ . Combining them, we have $\\rho=1+e^{-1/2}/\\sqrt{2\\pi}$ and $\\beta=2$ . For $\\kappa$ , $\\phi-\\phi^{\\prime}(x)x=-x\\cdot f(x)$ . So the bound of $\\kappa$ is $e^{-1/2}/{\\sqrt{2\\pi}}$ . ", "page_idx": 35}, {"type": "text", "text": "\u2022 Softplus. $\\phi(x)=\\log(1+e^{x})$ . Therefore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi^{\\prime}(x)=\\displaystyle\\frac{e^{x}}{1+e^{x}}\\leq1,}\\\\ {\\phi^{\\prime\\prime}(x)=\\displaystyle\\frac{e^{x}}{(1+e^{x})^{2}}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Besides, ", "page_idx": 35}, {"type": "equation", "text": "$$\n(\\phi(x)-\\phi^{\\prime}(x)x)^{\\prime}=\\left(\\log(1+e^{x})-{\\frac{e^{x}x}{1+e^{x}}}\\right)^{\\prime}=-{\\frac{e^{x}x}{(1+e^{x})^{2}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "So the maximum is $\\phi(0)-\\phi^{\\prime}(0)0=\\log2$ . Besides, when $x>1$ , $\\begin{array}{r}{\\phi(x)\\ge x\\ge\\frac{e^{x}x}{1+e^{x}}}\\end{array}$ . When $x\\rightarrow-\\infty$ , $\\phi(x)-\\phi^{\\prime}(x)x\\rightarrow0$ . Therefore, $\\kappa=\\log2$ . ", "page_idx": 35}, {"type": "text", "text": "\u2022 Sigmoid. $\\phi(x)=1/(1+e^{-x})$ . Hence, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi^{\\prime}(x)=\\displaystyle\\frac{e^{-x}}{(1+e^{-x})^{2}}\\leq1,}\\\\ {\\phi^{\\prime\\prime}(x)=\\displaystyle\\frac{e^{-2x}-e^{-x}}{(1+e^{-x})^{3}}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As for $\\kappa$ , we know that ", "page_idx": 36}, {"type": "equation", "text": "$$\n|\\phi(x)-\\phi^{\\prime}(x)x|=\\left|\\frac{1+e^{-x}-x e^{-x}}{(1+e^{-x})^{2}}\\right|\\leq\\frac{1+e^{-x}+|x|e^{-x}}{(1+e^{-x})^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that $|x|e^{-x}\\leq e^{-2x}+1$ . We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n|\\phi(x)\\leq\\frac{1+e^{-x}+e^{-2x}+1}{(1+e^{-x})^{2}}\\leq2.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 Tanh. $\\begin{array}{r}{\\phi(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\\leq1}\\end{array}$ . Note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi^{\\prime}(x)=1-\\phi(x)^{2}\\leq1}\\\\ {\\phi^{\\prime\\prime}(x)=2\\phi(x)^{3}-\\phi(x)\\leq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Besides, we know that ", "page_idx": 36}, {"type": "equation", "text": "$$\n|x\\phi^{\\prime}(x)|=\\frac{4|x|}{(e^{x}+e^{-x})^{2}}\\leq4.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence ", "page_idx": 36}, {"type": "equation", "text": "$$\n|\\phi(x)-\\phi^{\\prime}(x)|\\leq|\\phi(x)|+|x\\phi^{\\prime}(x)|\\leq5.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 SiLU. Note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi^{\\prime}(x)=\\displaystyle\\frac{1+e^{-x}+x e^{-x}}{(1+e^{-x})^{2}},}\\\\ {\\phi^{\\prime\\prime}(x)=\\displaystyle\\frac{(2-x)e^{-x}}{(1+e^{-x})^{2}}+\\frac{x e^{-2x}}{(1+e^{-x})^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Because $|x|e^{-x}\\leq e^{-2x}+1$ and $|x|e^{-2x}\\leq e^{-3x}+1$ . We get $|\\phi^{\\prime}(x)|\\leq2$ and $\\phi^{\\prime\\prime}(x)|\\leq4$ . At last, ", "page_idx": 36}, {"type": "equation", "text": "$$\n|\\phi(x)-x\\phi^{\\prime}(x)|={\\frac{|x|e^{-x}}{(1+e^{-x})^{2}}}\\leq1.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 Huberized ReLU. It\u2019s obvious that $\\phi^{\\prime}(x)\\leq1$ and $\\beta=1/h$ . Note that $\\phi$ is not second-order differentiable. At last, ", "page_idx": 36}, {"type": "equation", "text": "$$\n|\\phi(x)-x\\phi^{\\prime}(x)|=\\left\\{\\!\\!\\begin{array}{l l}{0}&{x<0,}\\\\ {x^{2}/2h}&{0\\leq x\\leq h,}\\\\ {h/2}&{x>h.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, it\u2019s upper bounded by $h/2$ . ", "page_idx": 36}, {"type": "text", "text": "F.2 Proof of Example 3.1 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof of Example 3.1. Because for activation functions in Example 2.1, $\\beta\\leq4$ and $\\rho\\leq2$ . Hence, for $\\tilde{\\phi}(x)\\,=\\,c x+(1-c)\\phi(x)/4,$ ${\\tilde{\\beta}}\\,=\\,1$ and $\\rho=1$ . Besides, since $0.5\\,<\\,c\\,<\\,1$ , we must have $(\\tilde{\\phi}(x))^{\\prime}\\geq0.25$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "G Additional Lemmas ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma G.1. If $\\quad{\\mathrm{~\\cdot~}}_{\\frac{1}{2}}\\geq L_{1}\\geq{\\frac{1}{c}}$ and $L_{2}\\geq L_{1}-L_{1}^{2}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{2}\\geq\\frac{1}{c+2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. For function $g(x)=x-x^{2}$ , $g^{\\prime}(x)=1-2x$ . If $x\\leq\\frac{1}{2}$ , then $g(x)$ is increasing. Then ", "page_idx": 37}, {"type": "equation", "text": "$$\ng(L_{1})\\geq g(\\frac{1}{c})=\\frac{c-1}{c^{2}}=\\frac{c^{2}+c-2}{c^{2}(c+2)}\\geq\\frac{1}{c+2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma G.2. Given a continuous function $f(x)$ s.t. $|f(x)-\\langle\\nabla f(x),x\\rangle|\\,\\leq\\,\\kappa,$ , then for a fixed constant $r>0$ there exists $C_{r,\\kappa}$ and $C_{r}$ s.t. for any $\\|x\\|\\geq r$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n|f(x)|\\leq C_{r,\\kappa}\\|x\\|,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and for any $x$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|f(x)\\right|\\leq C_{r,\\kappa}\\|x\\|+C_{r}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Since $f$ is continuous, let ", "page_idx": 37}, {"type": "equation", "text": "$$\nC_{r}=\\operatorname*{max}_{\\|\\boldsymbol{x}\\|=r}|f(\\boldsymbol{x})|/r.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now for any \u2225x\u2225> r, let y =\u2225rxx\u2225 and consider $\\begin{array}{r}{g(s)=\\frac{f(s y)}{s}}\\end{array}$ . Then we have ", "page_idx": 37}, {"type": "equation", "text": "$$\ng^{\\prime}(s)={\\frac{\\langle\\nabla f(s y),s y\\rangle-f(s y)}{s^{2}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{-\\frac{\\kappa}{s^{2}}\\leq g^{\\prime}(s)\\leq\\frac{\\kappa}{s^{2}}}\\end{array}$ . Let $s=\\|\\boldsymbol{x}\\|/r$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{f({\\boldsymbol x})r}{\\|{\\boldsymbol x}\\|}=g(s)=g(1)+\\displaystyle\\int_{1}^{s}{g^{\\prime}(t)}d t}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq g(1)+\\displaystyle\\int_{1}^{s}\\frac{\\kappa}{t^{2}}d t\\leq g(1)+\\kappa}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq r C_{r}+\\kappa.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{f(x)\\le(C_{r}+\\frac{\\kappa}{r})\\cdot\\|x\\|}\\end{array}$ . Similarly, we can show that $\\begin{array}{r}{-f(x)\\le(C_{r}+\\frac{\\kappa}{r})\\cdot\\|x\\|}\\end{array}$ . Therefore, for any $\\|x\\|\\geq r$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n|f(x)|\\leq(C_{r}+\\frac{\\kappa}{r})\\cdot\\|x\\|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $D=\\operatorname*{max}_{\\|x\\|\\leq r}|f(x)|$ , we have for any $x$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n|f(x)|\\leq(C_{r}+\\frac{\\kappa}{r})\\cdot\\|x\\|+D.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We have completed the proof. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.3. Fixing $c>1$ , then for every $0<x\\leq{\\frac{1}{c}}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nx\\leq{\\frac{-1}{\\log(c x)}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma G.3. This is equivalent to show that ", "page_idx": 37}, {"type": "equation", "text": "$$\nx\\log(c x)\\geq-1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $s(x)=x\\log(c x)$ , then $s^{\\prime}(x)=1+\\log(c x)$ . Hence $s(x)$ is decreasing when $\\begin{array}{r}{0<x<\\frac{1}{c e}}\\end{array}$ and is increasing when $x\\geq{\\frac{1}{c e}}$ . The minimum of $s(x)$ is achieved at $\\textstyle x={\\frac{1}{c e}}$ , which is ", "page_idx": 37}, {"type": "equation", "text": "$$\ns(1/(c e))=-\\frac{1}{c e}\\geq-1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This completes the proof. ", "page_idx": 37}, {"type": "text", "text": "Lemma G.4. Given $0<b\\leq\\textstyle{\\frac{1}{2}}$ and $a>0$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\frac{1+a}{1-b}}\\leq(1+2a+2b).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. This is equivalent to show that ", "page_idx": 38}, {"type": "equation", "text": "$$\n(1+a)\\leq(1+2a+2b)(1-b)=1+2a+b-2a b-2b^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This is equivalent to ", "page_idx": 38}, {"type": "equation", "text": "$$\n2b(a+b)\\leq(a+b).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since $a+b>0$ and $b\\leq\\frac{1}{2}$ , this is true. ", "page_idx": 38}, {"type": "text", "text": "Lemma G.5. Given $c>e$ , we have for any $x>2c\\log c,$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\frac{\\log x}{x}}\\leq{\\frac{1}{c}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. It\u2019s equivalent to show that $x-c\\log x\\geq0$ . Let $g(x)=x-c\\log x$ . $g^{\\prime}(x)=1-c/x$ . When $x>2c\\log c>2c$ , $g^{\\prime}(x)<0$ . Hence, the minimal is $_{g}(2c\\log c)$ . Note that ", "page_idx": 38}, {"type": "text", "text": "g $\\iota(2c\\log c)=2c\\log c-c\\log c-c\\log2-c\\log\\log c=c\\log c-c\\log2-c\\log\\log c=c\\log{\\frac{c}{2\\log c}}.$ Now we want to show that $c>2\\log c.$ . Let $h(y)=y-2\\log y.$ $h^{\\prime}(y)=1-2/y>0$ when $y>e$ . $h(e)=e-2>0$ . Hence $h(c)>h(e)>0$ and $g(2c\\log c)>0$ . This leads to $g(x)>0$ . Then, we complete the proof. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Lemma G.6. Given $\\ell(x)=\\log(1+e^{-x})$ and $c>0$ , we have for any $x$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\ell(x+c)\\geq\\exp(-c)\\ell(x).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Let $g(x)=\\ell(x+c)-\\exp(-c)\\ell(x)$ . Then, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\ng^{\\prime}(x)=\\frac{-1}{1+\\exp(x+c)}+\\frac{1}{\\exp(c)+\\exp(x+c)}<0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, $g(x)$ is monotonically decreasing. When $x\\to\\infty$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{x\\to\\infty}g(x)=\\operatorname*{lim}_{x\\to\\infty}\\left[\\ell(x+c)-\\exp(-c)\\ell(x)\\right]=\\exp(-x-c)-\\exp(-c)\\exp(-x)=0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, $g(x)\\geq0$ for any $x$ . Now, we complete the proof. ", "page_idx": 38}, {"type": "text", "text": "Lemma G.7. Assume $\\ell(x)=\\log(1+e^{-x})$ . If $\\ell(x+c)\\leq2\\ell(x),$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nc\\leq\\ln3+|x|.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\ell(x+c)-2\\ell(x)=\\log{\\frac{1+e^{x+c}}{1+2e^{x}+e^{2x}}}\\leq0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1+e^{x+c}}{1+2e^{x}+e^{2x}}\\leq1\\implies e^{c}\\leq2+e^{x}\\leq2+e^{|x|}\\leq3e^{|x|}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, $c\\leq\\ln3+|x|$ . ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The claims are accurate and reflect the contributions and scope of the paper. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: In Sections 6 and 7, we compare our results with existing works and discuss the limitations of our approach and future directions. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All the theoretical results in the paper are accompanied by a full set of assumptions and proofs. We put all the proofs in the Appendix due to space constraints. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All the experimental results are reproducible, and we provide all the necessary information to reproduce the results in section 5. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our experiments are easy to reproduce, and we provide all the necessary information in the paper. We focus on the theoretical analysis in this paper, and the code is not provided. The dataset CIFAR-10 can be found online. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All the training and test details are provided in section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our experiments are completely deterministic and it\u2019s not necessary to plot error bars. We provide the convergence plots of different models and show that the convergence rates are consistent with our theoretical results. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [No] ", "page_idx": 42}, {"type": "text", "text": "Justification: We focus on the theoretical analysis in this paper, and we only provide some illustrative experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and ensured that our research conforms to it. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This is a theoretical paper, and we do not discuss societal impacts. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 42}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: We do not release any data or models that have a high risk for misuse. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We only use the CIFAR-10 dataset. It does not have any license restrictions. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: We don\u2019t introduce any new assets in this paper. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}]