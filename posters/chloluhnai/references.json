{"references": [{"fullname_first_author": "Simon S Du", "paper_title": "Gradient descent provably optimizes over-parameterized neural networks", "publication_date": "2018-00-00", "reason": "This paper provides theoretical guarantees for the convergence of gradient descent in over-parameterized neural networks, which is a fundamental concept relevant to the current paper's analysis of large stepsize gradient descent."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-00-00", "reason": "This paper introduces the Neural Tangent Kernel (NTK), a powerful tool for analyzing the training dynamics of wide neural networks, which is directly relevant to the current paper's focus on large stepsize optimization."}, {"fullname_first_author": "Ziwei Ji", "paper_title": "Risk and parameter convergence of logistic regression", "publication_date": "2018-03-07", "reason": "This paper analyzes the convergence behavior of logistic regression, a simplified model for neural network training, and establishes important theoretical foundations that inform the current work's investigation into non-convex settings."}, {"fullname_first_author": "Daniel Soudry", "paper_title": "The implicit bias of gradient descent on separable data", "publication_date": "2018-00-00", "reason": "This paper studies the implicit bias of gradient descent in linearly separable settings, showing that it tends to find maximum margin solutions, a concept directly related to the current paper's study of implicit bias in non-homogeneous neural networks."}, {"fullname_first_author": "Jingfeng Wu", "paper_title": "Implicit bias of gradient descent for logistic regression at the edge of stability", "publication_date": "2023-00-00", "reason": "This paper analyzes large stepsize gradient descent in logistic regression, a closely related problem to the current work, providing key insights into the behavior of gradient descent when the empirical risk oscillates, a phenomenon also explored in the current paper."}]}