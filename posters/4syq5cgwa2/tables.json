[{"figure_path": "4syq5cgwA2/tables/tables_8_1.jpg", "caption": "Table 1: Deep Convolution EBM Log likelihood scores on test data as estimated by AIS. GWG results are taken from [Grathwohl et al., 2021]. ACS is able to achieve better results than the baselines.", "description": "This table presents the test log-likelihood scores for deep convolutional EBMs, estimated using Annealed Importance Sampling (AIS).  The results compare the performance of the proposed Automatic Cyclical Sampler (ACS) against three other gradient-based discrete sampling methods: Gibbs-with-Gradient (GWG), Discrete Metropolis Adjusted Langevin Algorithm (DMALA), and Any-scale Balanced Sampler (AB).  Lower log-likelihood values indicate better model performance.  The table shows that ACS consistently achieves better or comparable results compared to the other methods across different datasets (Static MNIST, Dynamic MNIST, Omniglot, Caltech).  GWG results are sourced from a previous study by Grathwohl et al. (2021).", "section": "6.2 Learning RBMs and EBMS"}, {"figure_path": "4syq5cgwA2/tables/tables_9_1.jpg", "caption": "Table 2: Empirical evaluation of the generated sentences. ACS outperforms DMALA for all metrics related to diversity.", "description": "This table presents the results of an empirical evaluation of the generated sentences using two different methods: DMALA and ACS.  The table shows that ACS outperforms DMALA across various metrics related to the diversity of the generated sentences.  The metrics include perplexity (lower is better), COLA (higher is better), Self-Bleu (lower is better), and the percentage of unique 2-grams and 3-grams (higher is better).  The results indicate that ACS generates more diverse and unique sentences compared to DMALA.", "section": "6.3 Text Infilling"}, {"figure_path": "4syq5cgwA2/tables/tables_22_1.jpg", "caption": "Table 3: Quantitative comparison of sampler performance on uneven multi-modal distribution. ACS retains the ability to accurately capture all the modes within the distribution despite the uneven weighting.", "description": "This table compares the performance of DMALA and ACS samplers on an uneven multi-modal distribution.  It shows the KL divergence (a measure of difference between two probability distributions) and average energy of samples generated by each method. Lower KL divergence indicates better accuracy in capturing the target distribution.  The results demonstrate ACS's superior performance in accurately capturing all the modes, even with uneven weighting of the modes in the target distribution.", "section": "D.1 Multi-modal Experiment Design"}, {"figure_path": "4syq5cgwA2/tables/tables_25_1.jpg", "caption": "Table 1: Deep Convolution EBM Log likelihood scores on test data as estimated by AIS. GWG results are taken from [Grathwohl et al., 2021]. ACS is able to achieve better results than the baselines.", "description": "This table shows the test set log-likelihoods of deep convolutional EBMs trained using different gradient-based discrete sampling methods.  The log-likelihoods were estimated using Annealed Importance Sampling (AIS).  The table compares the performance of the proposed Automatic Cyclical Sampler (ACS) against three baselines: Gibbs-with-Gradient (GWG), Discrete Metropolis Adjusted Langevin Algorithm (DMALA), and Any-Scale Balanced Sampler (AB).  The results show that ACS achieves better log-likelihood scores than the baselines across all four datasets (Static MNIST, Dynamic MNIST, Omniglot, Caltech).", "section": "6.2 Learning RBMs and EBMS"}]