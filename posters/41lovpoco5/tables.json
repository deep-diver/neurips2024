[{"figure_path": "41lovPOCo5/tables/tables_4_1.jpg", "caption": "Table 1: Token complexities of primary table prompting approaches without truncation. Note that Read Schema does not aware of any cell content. N: number of rows, M: number of columns, K: number of top retrieval results, D: number of distinct values in the table. It is generally observed that K < M <D\u226aNM.", "description": "This table compares the token complexity of different table prompting approaches used in large language models for table understanding tasks.  It breaks down the complexity into encoding and reasoning components, showing how the number of rows (N), columns (M), top retrieval results (K), and distinct values (D) impact the overall complexity.  The table highlights the significant difference in complexity between methods that read the entire table and methods that utilize retrieval techniques to reduce the amount of data processed by the LM.", "section": "3.4 Token Complexity Analysis"}, {"figure_path": "41lovPOCo5/tables/tables_6_1.jpg", "caption": "Table 2: Performance comparison of table prompting approaches on ArcadeQA and BirdQA across LMs.", "description": "This table presents the performance comparison of different table prompting approaches (ReadTable, ReadSchema, RandRowSampling, RowColRetrieval, and TableRAG) on two benchmark datasets (ArcadeQA and BirdQA) using three different Large Language Models (LLMs): GPT 3.5 Turbo, Gemini 1.0 Pro, and Mistral Nemo.  The results are expressed as accuracy scores, illustrating the effectiveness of each method in handling large-scale table understanding tasks across various LLM models. TableRAG consistently demonstrates superior performance compared to other methods.", "section": "4.3 Experimental Setup"}, {"figure_path": "41lovPOCo5/tables/tables_6_2.jpg", "caption": "Table 3: Evaluation of retrieval performance. TableRAG shows best retrieval quality on all tasks. R: recall, P: precision.", "description": "This table presents the results of the retrieval performance evaluation comparing different methods: ReadSchema, RandRowSampling, RowColRetrieval, and TableRAG.  The evaluation is done for both column and cell retrieval on two datasets, ArcadeQA and BirdQA.  The metrics used are Recall (R), Precision (P), and F1 score (F1).  TableRAG demonstrates the best retrieval quality across all tasks and metrics.", "section": "4.3 Retrieval Performance Analysis"}, {"figure_path": "41lovPOCo5/tables/tables_7_1.jpg", "caption": "Table 4: Comparison of TableRAG with state-of-the-art methods on WikiTableQA.", "description": "This table presents a comparison of the accuracy achieved by TableRAG against other state-of-the-art methods on the WikiTableQA benchmark dataset.  It highlights TableRAG's superior performance in comparison to existing approaches, demonstrating its effectiveness in the context of large-scale table understanding tasks.", "section": "4.6 Scalability Test on TabFact"}, {"figure_path": "41lovPOCo5/tables/tables_7_2.jpg", "caption": "Table 2: Performance comparison of table prompting approaches on ArcadeQA and BirdQA across LMs.", "description": "This table presents a comparison of the performance of different table prompting approaches (ReadTable, ReadSchema, RandRowSampling, RowColRetrieval, and TableRAG) on two datasets (ArcadeQA and BirdQA) using three different large language models (LLMs): GPT 3.5 Turbo, Gemini 1.0 Pro, and Mistral Nemo.  The performance metric used is not explicitly stated but is likely accuracy or F1-score.  The table highlights the superior performance of TableRAG across all LMs and datasets.", "section": "4.3 Experimental Setup"}, {"figure_path": "41lovPOCo5/tables/tables_8_1.jpg", "caption": "Table 6: Ablation study of schema retrieval (Rows 1 vs 3 and 2 vs 4) and cell retrieval (Rows 1 vs 2 and 3 vs 4). The first column indicates whether the LM processed all schemas or only retrieved schemas. The second column indicates whether the LM ignored cell values or processed retrieved column-cell pairs. Schema retrieval leads to an improvement in accuracy of up to 9.4%, while cell retrieval results in an increase of up to 11.5%.", "description": "This table presents the results of an ablation study on TableRAG, evaluating the impact of schema and cell retrieval on the model's performance.  It shows that both schema and cell retrieval methods significantly improve accuracy, indicating the importance of incorporating both types of information for effective table understanding.", "section": "4.3 Experimental Setup"}, {"figure_path": "41lovPOCo5/tables/tables_14_1.jpg", "caption": "Table 7: Dataset statistics. Values are presented in averages, except for the total counts of tables and questions.", "description": "This table presents a statistical summary of six datasets used in the paper's experiments.  The datasets include two real-world datasets (ArcadeQA and BirdQA), and four synthetic datasets derived from TabFact, each with varying sizes.  The table shows the number of tables, questions, rows, columns, total number of cells, number of distinct values, and number of categorical columns in each dataset.  This information provides a comprehensive overview of the scale and characteristics of the data used to evaluate the proposed TableRAG method.", "section": "4.1 Dataset"}]