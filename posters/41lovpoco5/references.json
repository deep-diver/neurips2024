{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a large language model used in TableRAG's experiments, providing key background information on the capabilities and limitations of the model."}, {"fullname_first_author": "Wenhu Chen", "paper_title": "TabFact: A large-scale dataset for table-based fact verification", "publication_date": "2020-00-00", "reason": "This paper introduces TabFact, a dataset used to evaluate TableRAG's scalability and performance on large-scale table understanding tasks."}, {"fullname_first_author": "Zhoujun Cheng", "paper_title": "Binding language models in symbolic languages", "publication_date": "2023-00-00", "reason": "This paper is highly relevant because it presents a methodology for integrating Language Models (LMs) with symbolic reasoning, a key concept utilized in TableRAG's architecture."}, {"fullname_first_author": "Julian Eisenschlos", "paper_title": "Understanding tables with intermediate pre-training", "publication_date": "2020-00-00", "reason": "This paper discusses intermediate pre-training techniques for table understanding, which is a relevant area for improving efficiency and performance of LMs in TableQA."}, {"fullname_first_author": "Xi Fang", "paper_title": "Large language models on tabular data-a survey", "publication_date": "2024-02-17", "reason": "This paper provides a comprehensive overview of the state-of-the-art in using large language models for tabular data, which is the core focus of TableRAG, making it a valuable contextual reference for the work."}]}