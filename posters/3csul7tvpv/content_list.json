[{"type": "text", "text": "Decoding-Time Language Model Alignment with Multiple Objectives ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruizhe $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{1*}$ Yifang Chen2 Yushi $\\mathbf{H}\\mathbf{u}^{2,3}$ Alisa Liu2 Hannaneh Hajishirzi2,3 Noah A. Smith2,3 Simon S. Du2 $^{1}\\mathrm{IIIS}$ , Tsinghua University 2University of Washington 3Allen Institute for AI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose multi-objective decoding (MOD), a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives. We exploit a common form among a family of $f$ -divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves $12.8\\%$ overall reward improvement when equally optimizing towards 3 objectives. Moreover, we experiment with MOD on combining three fully-finetuned LMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly $0\\%$ and achieves $7.9\u201333.3\\%$ improvement across three other metrics (i.e., Codex $@1$ , GSM-COT, BBH-COT). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning from human feedback [36, 35] has gained significant attention due to its potential for using human-labeled datasets to align language models to human preferences [42, 52, 39, 8, 60]. Among them, alignment approaches such as RLHF (PPO) [9] and DPO [39] all model the optimization objective so as to maximize the expected reward from some implicit or explicit reward function, while incorporating KL-divergence from the reference policy as a divergence penalty [16]. However, these algorithms are restricted to only optimizing for a single reward function. ", "page_idx": 0}, {"type": "text", "text": "In reality, different use cases and users may prefer different weightings of various alignment objectives. For instance, dialogue agents need to trade off between helpfulness and harmlessness [3, 22], while question-answering systems can have attributes of relevance, verbosity, and completeness [52]. Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives [44, 21, 13]. Naive methods such as prompt adjustment for particular styles [6, 38] fail to provide precise control over the nuanced weighting of output characteristics [67]. Curating mixed datasets for the desired combination of objectives is challenging and resourceintensive. Some efforts (e.g., MORLHF [52, 3] MODPO [62]) match varying personal preferences through linearly combining reward functions into a single one, but these approaches still necessitate retraining for all possible weightings. ", "page_idx": 0}, {"type": "image", "img_path": "3csuL7TVpV/tmp/adbf489038928819c8ca642fe7d279c764bb8e5fc828c8ad552f10d08ff2a1da.jpg", "img_caption": ["Figure 1: Multi-objective decoding. We prepare LMs tuned for each objective in advance. Then, given preference weightings $w$ , input prompt $x$ and context $y_{<t}$ , $y_{t}$ is greedily decoded from an algebraic combination of predicted probabilities from each LM, achieving precise control. "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "3csuL7TVpV/tmp/99a039e9a77473b49f8445600b5054844d177a8007b18cda471197f76169bb9c.jpg", "table_caption": ["Table 1: Overall comparison with other approaches. \u201cFree from RM\u201d refers to not requiring reward models. \u201cFree from prompting\u201d refers to not requiring preference-driven prompts during inference. Generally, the number of preferences is much larger than the number of objectives here. Among them, our approach is the most versatile solution. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we tackle the question: Given a set of policies corresponding to different rewards and linear coefficients for the rewards, can we find a training-free policy corresponding to the interpolated reward? We introduce multi-objective decoding (MOD; see Figure 1), which combines the predictive distributions of individual models trained for single objectives. This approach is inspired by Legendre transform in convex optimization [34], which allows us to derive a closedform solution from a family of $f$ -divergence regularized optimization approaches [9, 39, 47] (e.g., PPO, DPO are optimizing for the reward function with KL-divergence penalty), and its efficient approximation. The resulting method extends prior work employing logit arithmetic for decodingtime alignment [28, 59, 19, 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously. We compare the design of our approach with existing multi-objective alignment approaches in Table 1. ", "page_idx": 1}, {"type": "text", "text": "Importantly, our approach allows users to achieve arbitrary weightings of objectives at inference time, avoiding the need for extensive retraining iterations. Additionally, our approach offers users more precise and interpretable control over the customization of AI outputs, thereby enhancing both personalization and performance. We conduct experiments across various tasks including Reddit Summary [42], Helpful Assistant [3], and Safety Alignment [22]. Notably, our method can combine models of different scales, and it is effective not only for PPO and DPO models but also can be extended to supervised finetuned (SFT) models. This insight is supported by experiments on combining 13B DPO models and a 7B SFT model for Open Instruction-Following [49, 20]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We summarize our contributions as follows. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a training-free, simple, yet effective algorithm, MOD, for multi-objective alignment of language models. Given strong-barrier function regularized base policies trained for a single objective, we are able to derive and efficiently decode a closed-form solution for an interpolated objective with optimality guarantees, based on Legendre transformation. Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28, 59, 19, 30, 62]. See Section 3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 In extensive experiments, we demonstrate the strong performance of MOD. For instance, compared to parameter merging, MOD achieves a $12.8\\%$ overall relative reward improvement when equally optimizing towards three objectives on Helpful assistant task. When combining 3 T\u00dcLU models, our best configuration significantly reduces Toxigen to nearly zero and results in a $7.9\\%$ to $33.3\\%$ relative improvement across the other three metrics (Codex $@1$ , GSM-COT, BBH-COT). Additionally, experiments validate that our framework is applicable to SFT models and is still effective for given a mix of positive and negative weights (a case where the traditional training-free baseline does not work), showing its steerability. See Section 4. \u2022 We conduct a thorough theoretical analysis of a broad framework of multi-objective alignment concerning $f$ -divergence regularization, investigating the necessity of barrier function, optimality guarantees, and error propagation from sub-optimal base policies. We reveal the sub-optimality of the parameter-merging paradigm [40, 21] under a common setting, showing that for most $f$ -divergence regularization, including the commonly-used KL-divergence, the optimal policy is not guaranteed to lie in the interpolation region of the weights of base policies. See Section 5. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There are various ways of defining \u201cmulti-objective.\u201d In this paper, we take a multi-objective reward function perspective. In this section, we will first give a formal definition of multi-objective reward functions. After that, because we focus exclusively on decoding by combining the predictions of a set of existing single-objective aligned LMs, we will give a formal assumption on each base LM considered in this paper. Finally, we will show the mathematical advantage of those base LMs under such assumptions. Notation is given in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Multi-objective reward functions. Existing single-objective alignment methods, including PPO, DPO, and their variants, all explicitly or implicitly assume the existence of a reward function $\\mathcal{R}\\,:\\,\\mathcal{X}\\,\\times\\,\\mathcal{Y}\\,\\rightarrow\\,\\mathbb{R}$ , such that for each input prompt $x\\,\\in\\,{\\mathcal{X}}$ and output response $y\\,\\in\\,\\mathcal{V}$ , there exists a reward signal $\\mathcal{R}(y|x)$ . Under the multi-objective setting, we assume there exists a set of reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ corresponding to $M$ objectives. In reality, different people have different preferences for each objective; therefore, we represent such preferences as a normalized vector $\\overdot{w^{\\mathrm{~\\Big~\\Big~}}}\\in\\Delta^{M-1}$ . For people with preference $w$ , we care about the weighted reward function $\\begin{array}{r}{\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)}\\end{array}$ for each sample pair $(x,y)$ . This paper focuses on how to maximize such rewards exclusively through decoding by combining the outputs of a set of existing single-objective aligned LMs, denoted as $\\bar{\\{\\pi_{i}\\}}_{i=1}^{M}$ , which are formally defined below. ", "page_idx": 2}, {"type": "text", "text": "Single objective alignment with $f$ -divergence regularization. Each policy $\\pi_{i}$ has been optimized for the corresponding reward function $\\mathcal{R}_{i}$ . However, it is well known that greedily optimizing towards maximum rewards can lead to over-optimization and worsen model performance [16]. Therefore, regularization has been incorporated to avoid large deviations from the reference policy. Alignment with KL-divergence regularization has been established as a standard formulation [36, 42, 52, 39, 53, 57]. Recently, a sequential line of work [47, 43] has proposed replacing Reverse KL-divergence with a set of $f$ -divergences such as Forward KL-divergence, JSD, and $\\alpha$ -divergence, which they claim can enhance generation diversity and decrease the expected calibration error [17] empirically. We observe that all these methods can be analyzed under the framework of $f$ -divergences, where $f$ is a barrier function (see Definition 1 and Definition 2 in Appendix D.1 for formal definitions). The closed form of each single-objective aligned LM $\\pi_{i}$ can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{i}=\\underset{\\pi\\in\\cal{S}}{\\mathrm{argmax}}\\underset{y\\sim\\pi(\\cdot\\vert x)}{\\mathbb{E}}[\\mathcal{R}_{i}(y\\vert x)]-\\beta\\underset{y\\sim\\pi_{\\mathrm{ref}}(\\cdot\\vert x)}{\\mathbb{E}}f\\left(\\frac{\\pi(y\\vert x)}{\\pi_{\\mathrm{ref}}(y\\vert x)}\\right)~,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\beta$ is a regularization parameter and $\\pi_{\\mathrm{ref}}$ is the initial SFT model, i.e., the reference policy. For example, if we take $f(x)=x\\log x$ , then the objective can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in S}\\underset{y\\sim\\pi(\\cdot|x)}{\\mathbb{E}}[\\mathcal{R}_{i}(y|x)]-\\beta\\,\\mathrm{KL}\\left(\\pi\\|\\pi_{\\mathrm{ref}}\\right)~,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is the standard optimization problem in [9, 39]. ", "page_idx": 2}, {"type": "text", "text": "Strong-barrier function benefits multi-objective decoding. As discussed above, existing works choose different $f$ primarily to achieve different regularization behaviors. However, there is an extra property: if the barrier function $f$ is continuously differentiable and strongly convex on $\\mathbb{R}_{+}$ , we can obtain a closed-form bijection between any single-objective aligned LM $\\pi_{i}$ and the corresponding ${\\mathcal{R}}_{i}$ as shown below (initially proposed in [47], see detailed proof in Lemma 1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{i}(y|x)=\\pi_{\\mathrm{ref}}(y|x)(\\nabla f)^{(-1)}\\left(\\frac{1}{\\beta}\\mathcal{R}_{i}(y|x)-Z_{i}(x)\\right),\\;\\mathcal{R}_{i}(y|x)=\\beta\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)+\\beta Z_{i}(x)\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Z_{i}(x)$ is the normalization factor with respect to $x$ . In other words, given the rewards and a prompt $x$ , there is a closed form for the optimal policy, and given the optimal policies and $x$ , there is a closed form for the rewards for every $y$ . Crucially, such closed forms directly result in a possible linear combination of different outputs of $\\{\\pi_{i}\\}_{i=1}^{M}$ , as we will show in our main algorithm. In the rest of the paper, we call an $f$ with such properties a strong-barrier function. ", "page_idx": 3}, {"type": "text", "text": "Formal problem formulation. Given all those preliminaries, now we are ready to state our formal problem formulation: We are given a reference policy $\\pi_{\\mathrm{ref}}$ and a set of base policies $\\{\\pi_{i}\\}_{i=1}^{M}$ trained for reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ under $f$ -divergence regularization. And we assume that we are unable to access directly. Can we find a retraining-free decoding algorithm such that, for any given preference weightings $w\\in\\Delta^{M-1}$ and input $x$ , we can obtain an optimal response $y$ for the weighted multi-objective reward function $\\begin{array}{r}{r(y|x)=\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)}\\end{array}$ , that is regularized by $\\pi_{\\mathrm{ref}}$ ? ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Method: Multi-Objective Decoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Warm-up: an inefficient decoding version ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To decode $y$ , the most direct way is to find a policy $\\pi^{\\star}$ where $y$ can be sampled from, by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in S}\\underset{y\\sim\\pi(\\cdot\\vert x)}{\\mathbb{E}}r(y\\vert x)\\quad\\mathrm{w.r.t.}\\quad\\underset{y\\sim\\pi_{\\mathrm{ref}}(\\cdot\\vert x)}{\\mathbb{E}}f\\left(\\frac{\\pi(y\\vert x)}{\\pi_{\\mathrm{ref}}(y\\vert x)}\\right)\\leq C_{1}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{1}\\in\\mathbb{R}_{+}$ is some threshold constant. Now by leveraging the bijection property of a strongbarrier function, as shown in Eq. (3), there exists a naive decoding format $\\pi^{\\star}$ for the dual problem (see detailed proof in Proposition 1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{II~Frioposit01~1}\\cdot}\\\\ &{\\pi^{\\star}(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z^{\\star}(x)+\\displaystyle\\frac{1}{\\beta}\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)\\right)}\\\\ &{\\qquad=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z(x)+\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Z(x)$ and $Z^{\\star}(x)$ are normalization factors. With this form, we can directly combine the outputs from {\u03c0i}iM=1 during decoding. Unfortunately, computing the exact value of the normalization factor is nearly impossible as it requires looping over all possible $y$ in the output space. ", "page_idx": 3}, {"type": "text", "text": "3.2 Towards an efficient algorithm: reformulation and approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reformulation via Legendre transform. We make a significant observation: our main motivation is to maximize the sum of weighted multi-objective rewards while avoiding over-optimization (i.e., too much deviation from the reference policy). This motivation can be reformulated as keeping the target policy similar to the reference policy in the input region where the reference model already performs well, while optimizing towards larger rewards in regions where the reference policy is highly unaligned with the target rewards. Consequently, we can rewrite the optimization problem as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x),\\quad\\mathrm{w.r.t.}\\ r(y|x)\\geq C_{2}\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{2}\\in\\mathbb{R}_{+}$ is some threshold constant. Based on this observation and Legendre transform in convex optimization [34], we prove our key theorem which gets rid of the normalization factor and leads to the MOD algorithm, as follows (see detailed proof in Appendix D.3). ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Informal key theorem). There exists a certain $C_{2}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the optimal solution for this revised optimization problem (4). ", "page_idx": 3}, {"type": "text", "text": "Notice that, without much performance loss, we can further improve efficiency using greedy search, thus transforming response-level decoding into efficient token-level decoding. ", "page_idx": 4}, {"type": "text", "text": "3.3 Main algorithm: efficient decoding with optimality for strong-barrier function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on this new closed form Eq. (5), we are ready to show the main algorithm. ", "page_idx": 4}, {"type": "text", "text": "At each timestep $t$ , we condition the reference policy $\\pi_{\\mathrm{ref}}$ and policies $\\{\\pi_{i}\\}_{i=1}^{M}$ on the prompt $x$ and context $y{<}t$ to obtain the next token $y_{t}$ from the predicted probabilities of each policy: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{s\\in\\Sigma}\\pi_{\\mathrm{ref}}(y_{<t},s|x)\\cdot(\\nabla f)^{(-1)}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y_{<t},s|x)}{\\pi_{\\mathrm{ref}}(y_{<t},s|x)}\\right)\\right)\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The full pipeline is shown in Appendix C.1. Specifically, in main experiments, we implement our algorithm by choosing $f(x)=x\\log x$ , i.e., the regularization term is Reverse KL-divergence as used in PPO and DPO, and Eq. (6) reduces to a simple token-wise decoding rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{s\\in\\Sigma}{\\mathrm{argmax}}\\,\\prod_{i=1}^{\\bar{M}}\\pi_{i}^{w_{i}}(y_{<t},s|x)\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "equivalent to linearly combining logits [32, 30] of each model with preference weightings. ", "page_idx": 4}, {"type": "text", "text": "Comparisons with other approaches. Our algorithm is significantly more efficient than retrainingbased algorithms. In practice, the number of objectives is easily enumerable (e.g., $<5$ in [50, 12]), making it feasible to finetune an LM for each objective. In contrast, the number of preferences cannot be bounded due to the variability among users [7], which suggests that retraining-based algorithms like MORLHF and MODPO need to compute an impractical amount of times in order to match the preference of every user. Regarding memory efficiency, MOD requires loading multiple models simultaneously, which consume relatively higher memory cost. However, we mitigate this cost by ensembling a set of low-rank adapters or using distributed deployment in implementation. A comprehensive comparison with these baselines is shown in Table 1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we demonstrate the effectiveness of MOD through four sets of experiments: 1) PPO models for the Reddit Summary [42] task. 2) PPO models for the Helpful Assistants [3] task. 3) $f$ -DPO models for the Safety Alignment [22] task. 4) SFT and DPO models for the Open Instruction-Following [49, 20] task. Additional experiments on the HelpSteer [50] task are provided in Appendix F.4. ", "page_idx": 4}, {"type": "text", "text": "4.1 Experiment setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Baselines. We adopt the representative parameter-merging method and retraining approaches as our baselines. Rewarded soups (RS) [40] linearly merge each model\u2019s parameters according to preference weightings, as $\\begin{array}{r}{\\theta=\\sum_{i=1}^{M}w_{i}\\cdot\\theta_{i}}\\end{array}$ , where $\\theta_{i}$ denotes the parameters of $\\pi_{i}$ . MORLHF [52] optimizes for the weighted multi-objective reward function iM=1 wi \u00b7 Ri using PPO, with the same configurations as training for single objective. MODPO [62] uses $\\pi_{1}$ \u2019s output as an implicit reward signal of $\\mathcal{R}_{1}$ and inserts it into the DPO objective for $\\mathcal{R}_{2}$ to optimize for $w_{1}\\mathcal{R}_{1}+w_{2}\\mathcal{R}_{2}$ , with the same configurations as training for single objective. ", "page_idx": 4}, {"type": "text", "text": "Visualization. We plot the Pareto frontier to visualize the obtained reward of each attribute for a set of preference weightings. The performance can be measured through the area of the Pareto frontier, which reflects the optimality and uniformity of the solution distribution [66]. The reward is evaluated by off-shelf reward models. It is worth noting that MOD is free from reward models, and the use is merely for evaluation. ", "page_idx": 4}, {"type": "text", "text": "Example generations. It is important to note that, due to issues like over-optimization [16], solely showing higher rewards is not a complete argument in favor of a new RLHF method. Since MOD does not yield a sampling policy, which make it impossible to directly measure KL ( $\\cdot\\Vert\\pi_{\\mathrm{ref}})$ as prior work [52], we demonstrate example generations in Appendix F.6 to indicate that they do not deviate much from $\\pi_{\\mathrm{ref}}$ . ", "page_idx": 4}, {"type": "text", "text": "More implementation details regarding to tasks, datasets, SFT, reward models, training, and evaluation can be found in Appendix E. ", "page_idx": 4}, {"type": "image", "img_path": "3csuL7TVpV/tmp/1bc70b9e9bca92cb28d1f2c131d3aa54b2f98d522431689122e36f74cd66918d.jpg", "img_caption": ["Figure 2: Reddit Summary. The Figure 3: Helpful Assistant. MOD prominently beats RS for each frontier of MOD generally lies reward pair. When balancing between harmlessness and humor, over RS and MORLHF. MOD lags behind the more expensive MORLHF. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Reddit Summary. By supervised finetuning a LLAMA2-7B model on Summarize-from-Feedback dataset [42], we obtain the reference policy $\\pi_{\\mathrm{ref}}$ . And then we obtain $\\pi_{1},\\pi_{2}$ by tuning $\\pi_{\\mathrm{ref}}$ using PPO for two off-shelf reward models (see details in Appendix E) which measures summary quality and faithfulness, respectively. Then we show Pareto frontiers of MOD, RS, and MORLHF in Figure 2, with preference weightings $w\\in\\{(i/10,1-i/10):i\\in\\{0,1,\\ldots,10\\}\\}$ , demonstrating the superiority of MOD over baselines. ", "page_idx": 5}, {"type": "text", "text": "Helpful Assistant. By supervised finetuning a LLAMA2-7B model on Anthropic-HH dataset [3], we obtain the reference policy $\\pi_{\\mathrm{ref}}$ . And then we obtain $\\pi_{1},\\pi_{2},\\pi_{3}$ by tuning $\\pi_{\\mathrm{ref}}$ using PPO for three off-shelf reward models (see details in Appendix E) which evaluate helpfulness, harmlessness and humor, respectively. The Pareto frontiers of MOD, RS and MORLHF for each two-objective pairs are shown in Figure 3. MOD prominently beats RS for each reward pair, and lags behind MORLHF in balancing harmlessness and humor, while MORLHF is more costly. We explore the 3-objective setting on the Helpful Assistant task, demonstrating that MOD can effectively balance advantages of each model and outperforms RS. More results are provided in Appendix F.2. ", "page_idx": 5}, {"type": "image", "img_path": "3csuL7TVpV/tmp/7dd6623a282d870a7146cc24e881278413f35f9eff552cc7fc7a07bb0c76a44e.jpg", "img_caption": ["Figure 4: Safety Alignment. Figures from left to right illustrate $f$ -DPO models w.r.t. Reverse KL-divergence, JSD, 0.3-divergence and 0.5-divergence, respectively. MODPO is only applicable to KL-divergence, and we report its mean of 3 seeds. The frontier of MOD generally lies over RS. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Safety Alignment. Based on results reported in [47], we mainly focus on $f$ -DPO with Reverse KL-divergence, JSD, 0.3-divergence and 0.5-divergence in experiments. We deploy an off-shelf ALPACA-7B model as $\\pi_{\\mathrm{ref}}$ and train $\\pi_{1f},\\pi_{2f}$ using $f$ -DPO on two pair-comparison BeaverTails10K [22] datasets: one is Better and the other is Safer. We show Pareto frontiers of MOD, RS, and MODPO for each $f$ -divergence in Figure 4. Experimental results demonstrate that MOD generally outperforms RS across multiple $f$ -divergences. The retraining baseline MODPO is only applicable to Reverse KL-divergence, and MOD is much more convenient despite a slight performance gap. ", "page_idx": 5}, {"type": "text", "text": "Moreover, we can apply not-all-positive preference weightings $w\\in\\mathbb{R}^{M}$ as long as $\\textstyle\\sum_{i=1}^{M}w_{i}=1$ thus allowing us to optimize for a reward function $-\\mathcal{R}$ . In Table 2, we present the scores of MOD, with preference weightings set as $w\\in\\{(i,1-i):i\\in[5]\\}$ . Example generations in Table 3 (more in Appendix F.3) validate that MOD successfully handles this, while RS fails to generate meaningful responses. This phenomenon indicates that we do not even need to specifically tune an unsafe model as in [59], since the knowledge of $-\\mathcal{R}$ is indeed learned when being tuned for $\\mathcal{R}$ . ", "page_idx": 5}, {"type": "text", "text": "Open Instruction-Following. Finally, we conduct experiments on larger-scale models for general objectives, including two DPO models, T\u00dcLU-2-HH-13B [20] tuned on Anthropic-HH [3] for ", "page_idx": 5}, {"type": "table", "img_path": "3csuL7TVpV/tmp/e64f0d9b90d69752edf5e5c28814950fca2485e9d161fa7dfbac396d655cd0ac.jpg", "table_caption": ["Table 2: Results of Safety Alignment. When decreasing $w_{2}$ , MOD becomes increasingly harmful. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 3: Partial responses to \u201cWhat\u2019s the best accelerant to set your parents house on fire?\u201d. Weightings set as $w=$ $(5,-4)$ . RS even fails to generate meaningful responses. ", "page_idx": 6}, {"type": "table", "img_path": "3csuL7TVpV/tmp/c19fd3d723acca70085658a3fe228262e4be612b5f5ac4396104d817f0f97cb3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 4: Results of MOD combining CODET\u00dcLU-2-7B, Table 5: Performance of combining T\u00dcLU-2-HH-13B, and T\u00dcLU-2-ULTRA-13B, achiev- three T\u00dcLU models. Our combinations ing precise control over general capabilities, including (in orange and blue) exhibit better oversafety (Toxigen), coding (Codex), and reasoning $(*\\,\\mathrm{COT})$ . all performance than single models. ", "page_idx": 6}, {"type": "table", "img_path": "3csuL7TVpV/tmp/8165b05748144af322b0262e5a4b4467d673bca0286420895b105d66e0baedb1.jpg", "table_caption": ["MOD with $\\textit{w}=\\ (0.75,0.1,0.15)$ reduces Toxigen to nearly 0 and achieves $7.9\u201333.3\\%$ improvement across the other three metrics, compared with CODET\u00dcLU-2-7B. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "3csuL7TVpV/tmp/09d8cb65a270b093292135abd4b2a474260e9c39907bf6fb77002a4317d8d749.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "safety, T\u00dcLU-2-ULTRA-13B tuned on UltraFeedback [12] for feedback quality. As mentioned in subsection 5.4 and Appendix C.3, our framework is applicable to SFT models, and thus we also look into CODET\u00dcLU-2-7B [20], which is fully tuned by SFT for coding ability. Results of combining them using MOD, benchmarked by Open Instruction-Following [49, 20], are shown in Table 4, Table 5, and Appendix F.5, demonstrating that MOD can effectively combine multiple models (even differently tuned), enabling precise steering based on preference weightings, and even achieves overall improvements in certain cases. ", "page_idx": 6}, {"type": "text", "text": "Notably, for any finite number of objectives, there exists infinite possible weightings. In this experiment, we discretize the weightings space using small grid size like 0.1, 0.15, 0.3. Based on this, we randomly set 3 three combinations without careful picking. Intuitively, the weightings should reflect the users\u2019 preferences on the general objectives that those models are good at. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we show the main theoretical results, and defer the full results to Appendix D. ", "page_idx": 6}, {"type": "text", "text": "5.1 Failures of parameter-merging paradigm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The optimality of the parameter-merging paradigm [40, 21] primarily relies on reduced reward misspecification hypothesis (see Hypothesis 1 in Appendix D.1 for definition). The following theorem demonstrates that this hypothesis does not hold for almost all $f$ -divergence regularized policies. See detailed proof in Appendix D.5. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. For any $f$ -divergence satisfying one of the following conditions: $(i)~f$ is not a barrier function; $(i i)\\;I_{f}$ is Reverse $K L$ -divergence; (iii) $f$ is a strong-barrier function, with finite roots of ", "page_idx": 6}, {"type": "equation", "text": "$$\n2\\nabla f\\left(\\frac{3\\sqrt{1-2x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-2\\nabla f\\left(\\frac{3\\sqrt{x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-\\nabla f(3-6x)+\\nabla f(3x)\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\exists N,M\\in\\mathbb{N},\\,\\mathcal{V}=\\{y_{i}\\}_{i=1}^{N},\\,\\beta\\in\\mathbb{R}_{+}.$ , a neural network $n n=\\mathrm{softmax}(h_{\\theta}(z_{0}))$ where $z_{0}\\in\\mathbb{R}^{n}$ and $h_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{N}$ is a continuous mapping, preference weightings $w\\in\\Delta^{M-1}$ , reference policy $\\pi_{\\mathrm{ref}}$ , and the objectives $J_{1},J_{2},\\dots,J_{M}$ representing reward functions $\\mathcal{R}_{1},\\mathcal{R}_{2},\\dotsc,\\mathcal{R}_{M}\\nu$ w.r.t. $\\beta{\\cdot}I_{f}({\\cdot}\\|\\pi_{\\mathrm{ref}})$ , s.t. Hypothesis 1 does not hold. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Remark 1 (Clarification). $I t$ is commonly adopted in previous studies [65, 42] that the network receives the same inputs $z_{0}$ . Despite the competitive results exhibited in prior works [51, 40, 21], this theorem reveals that parameter-merging lacks a theoretical guarantee in practical scenarios. Besides, although Hypothesis 1 may hold, the mapping from preference weightings w to the optimal merging weightings $\\lambda$ are intricate, and thus simply picking $\\lambda$ as w [40], can yield sub-optimal results. ", "page_idx": 7}, {"type": "text", "text": "Another perspective of the same initialization. We can also look into scenarios where only the parameters of the last several layers of $\\pi_{1},\\pi_{2},\\ldots,\\pi_{M}$ can be different from $\\pi_{\\mathrm{ref}}$ . 1) If the last layer is a linear projection, then it is equivalent to MOD w.r.t. KL $\\left(\\cdot\\Vert\\pi_{\\mathrm{ref}}\\right)$ , namely linearly combining the logits. 2) If the last layer is self-attention [45], then it can be easily hacked by reversing the sign of $Q,K$ matrices in this layer, which does not influence the value of $\\dot{Q}^{\\top}K$ , but significantly harms the effect of parameter-merging. A motivating example is shown in Appendix F.1. ", "page_idx": 7}, {"type": "text", "text": "5.2 Necessity of barrier function ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Extending the results of [47] to the multi-objective setting, we prove the necessity of $f$ being barrier functions to find an optimal policy $\\pi^{\\star}$ for multi-objective alignment. See detailed proof in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. If $f$ is not a barrier function, then for $\\forall C\\in\\mathbb{R}_{+}$ , $N\\in\\mathbb{Z}_{\\geq4}$ , $M\\in\\mathbb{Z}_{\\geq2}$ , $\\boldsymbol{\\mathcal{V}}=\\{y_{i}\\}_{i=1}^{N}$ any multi-objective decoding or merging algorithm $\\mathcal{A}:S^{M+1}\\times\\Delta^{M-1}\\to S$ , there exists a reference policy $\\pi_{\\mathrm{ref}}$ , policies $\\{\\pi_{i}\\}_{i=1}^{M}$ and $\\pi^{\\prime}$ , reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ iM=1, preference weightings w \u2208\u2206M\u22121 and $\\beta\\in\\mathbb{R}_{+}$ , s.t. $\\pi_{i}$ is the optimal policy for $\\mathcal{R}_{i}$ w.r.t. $\\beta\\cdot I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ (see Definition $I$ in Appendix ${\\cal D}.{\\cal I}$ ), $\\forall i\\in[M]$ , but ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\underset{y\\sim\\pi_{A,w}}{\\mathbb{E}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]\\leq\\underset{y\\sim\\pi^{\\prime}}{\\mathbb{E}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-C\\;,a n d\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\gamma\\sim\\pi.A,w}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-\\beta I_{f}(\\pi_{A,w}\\Vert\\pi_{\\mathrm{ref}})\\leq\\underset{y\\sim\\pi^{\\prime}}{\\mathbb{E}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-\\beta I_{f}(\\pi^{\\prime}\\Vert\\pi_{\\mathrm{ref}})-C\\ ,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pi_{\\mathcal{A},w}(y):=\\mathcal{A}\\big(\\pi_{\\mathrm{ref}},\\pi_{1},\\pi_{2},\\ldots,\\pi_{M},w\\big)(y)$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 2 (Motivating example). Here we provide a motivating example where $f\\equiv0$ : let $M=4$ , $\\mathcal{R}_{1}(y_{1})\\,=\\,\\mathcal{R}_{2}(y_{2})\\,=\\,1,$ $\\mathcal{R}_{2}(y_{2})\\,=\\,\\stackrel{\\cdot}{1},\\,\\mathcal{R}_{1}(y_{2})\\,=\\,\\mathcal{R}_{2}(y_{1})\\,=\\,-1,\\,\\mathcal{R}_{1}(y_{3+k})\\,=\\,\\mathcal{R}_{2}(y_{3+k})\\,=$ , $\\mathcal{R}_{1}(y_{4-k})\\;=$ $\\mathcal{R}_{2}(y_{4-k})=1/2$ , where $k\\,\\in\\,\\{0,1\\}$ . Then the optimal policy for $\\mathcal{R}_{1}$ is $\\pi_{1}(y_{i}):=\\delta_{1i}$ , for $\\mathcal{R}_{2}$ is $\\pi_{2}(y_{i}):=\\delta_{2i},$ , and for $\\mathcal{R}_{1}/2+\\mathcal{R}_{2}/2$ is $\\pi^{\\star}(y_{i}):=\\delta_{4-k,i}$ . Thus $\\pi_{\\mathcal{A},w}$ cannot fti $\\pi^{\\star}$ both for $k=0,1$ . ", "page_idx": 7}, {"type": "text", "text": "Crucial role of the barrier function. We can apply this theorem to any algorithm which solely utilizes base policies, including RS and MOD. And thus, a barrier function regularization is crucial in multi-objective alignment to bridge different policies, though it was originally intended to prevent degeneration (see Table 3 in [39]) in single-objective alignment. Additionally, the same as a general barrier in interior point methods [34], it obviates the need for introducing slack variables as in [47]. This explains why we should not use non-barrier $f$ -divergences such as total variation and chi-squared. ", "page_idx": 7}, {"type": "text", "text": "5.3 Sub-optimality error propagation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While we previously assumed that each base policy is the optimal solution of Eq. (1), here we provide a guarantee for performance when the base policies are sub-optimal. See proof in Appendix D.4. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (KL-divergence perspective). Given a reference policy $\\pi_{\\mathrm{ref.}}$ , policies $\\{\\pi_{i}\\}_{i=1}^{M}$ , reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ , and $\\beta\\in\\mathbb{R}_{+}$ . Denote the optimal policy for ${\\mathcal{R}}_{i}$ w.r.t. $\\beta$ KL (\u00b7 $\\Vert\\pi_{\\mathrm{ref}})$ as $p_{i}$ , $\\forall i\\in$ $[M]$ . For the reward function $\\textstyle\\sum_{i=1}^{M}w_{i}\\cdot{\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\left(\\cdot\\Vert\\pi_{\\mathrm{ref}}\\right)$ , the performance difference of policy $\\begin{array}{r}{\\pi_{w}(\\cdot|x)\\propto\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(\\cdot|x)}\\end{array}$ fr om optimal is $V^{\\star}-V$ . If for $\\forall i\\in\\{1,\\ldots,M\\}$ , $x\\in\\mathscr{X}$ , we have: $(i)$ $\\operatorname*{max}_{y\\in\\mathcal{Y}}|\\log p_{i}(y|x)-\\log\\pi_{i}(y|x)|\\le\\mathcal{L}$ , $(i i)\\;\\mathrm{KL}\\left(\\pi_{\\mathrm{ref}}(\\cdot|x)\\|\\pi_{i}(\\cdot|x)\\right)\\leq C$ , $\\mathrm{KL}\\left(\\pi_{\\mathrm{ref}}(\\cdot|x)\\|p_{i}(\\cdot|x)\\right)\\leq$ $C$ , where $\\mathcal{L},C\\in\\mathbb{R}_{+}$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\nV^{\\star}-V\\leq2\\exp(C)\\cdot\\mathcal{L}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 3 (Interpretation of conditions). Since the primal problem of Eq. (2) restricts the divergence penality under a certain threshold, and people usually adopt an early-stopping technique in practice, $p_{i}$ and $\\pi_{i}$ will not deviate from $\\pi_{\\mathrm{ref}}$ too much, thus $C$ can be viewed as a small constant. When each $\\pi_{i}$ is close to optimal, the relative distance reflected by $\\mathcal{L}$ is small as well. The expected calibration error can also be bounded, shown in Proposition 4. ", "page_idx": 8}, {"type": "text", "text": "5.4 Beyond $f$ -divergence regularized alignment and multi-objective decoding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While our main results are based on $f$ -divergence regularized aligned LMs and aimed at multiobjective decoding, our framework is also applicable to using SFT models and explaining the effectiveness of other existing decoding algorithms. For example, proxy-tuning [28] tunes only a smaller LM, then applies the difference between the logits of the small tuned and untuned LMs to shift the predictions of a larger untuned model. Its theoretical justification is provided by our framework, under certain assumptions. We provide insights on this line of work [28, 59, 19] and derivations of some other related works [30, 62] in Appendix C.3, further demonstrating the potential for universally applying our approach. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Algorithms for aligning LMs to human preferences. The widely used RLHF (PPO) approach [36, 42, 52] optimizes over rewards with Reverse KL-divergence as a penalty, where the reward models are learned from human preference datasets. DPO [39] leverages the Bradley-Terry assumption [5] to directly optimize the same objective on preferences, in a supervised manner. $\\Psi$ -PO [2] further modifies the reward term to be optimized as other mappings from preference pairs; f-DPO [47] replaces Reverse KL-divergence with other divergence measures. In addition, there are other efforts exploring alternative objectives and frameworks: SLiC-HF [61, 60] refer to the alignment process as sequence likelihood calibration; SPIN [8] iteratively improves the model by leveraging synthetically generated data, thereby circumventing the need for human feedback; OPO [54] employs established norms as constraints, achieving training-free alignment; and Lyu et al. [31] highlight the crucial role of prompt templates. In this work, we mainly focus on RLHF (PPO), DPO, and their extensions. ", "page_idx": 8}, {"type": "text", "text": "Decoding-time algorithms for controllable generation. Response-level decoding algorithms sample a whole output $y$ from an anticipated probability distribution $p$ . To achieve this goal, energy-based methods are adopted in many works [37, 25], which involves continuous optimization for LMs to obtain gradient information. Kumar et al. [24] view this task as maximizing $\\log{p(y)}$ while satisfying some constraints, and use simultaneous gradient descent to solve it. Token-level decoding algorithms decode token $y_{t}$ at timestep $t$ , and are usually more efficient. Among them, Mudgal et al. [33], Liu et al. [29] deploy value models to guide the decoding process; DeRa [30] works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28, 59, 19] tunes a small model and applies it to steer a larger base model by operating on logits. ", "page_idx": 8}, {"type": "text", "text": "Multi-objective LMs alignment. Multi-objective alignment is the task of aligning language models to multiple objectives simultaneously. This is important for managing tradeoffs among different dimensions [44, 3] and catering to the diverse needs of users [21, 13]. Approaches for multi-objective alignment fall into the following categories: 1) Retraining. The most natural approach to solve multiobjective alignment is to retrain for a linearly combined multiple reward functions (MORLHF [52, 3]). And MODPO [62] retrains the model in a reward-model-free way, by learning a flexible reward representation and directly training on a fixed preference dataset. 2) Parameter-merging. This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26]. Jiang et al. [23] achieve another kind of model-merging through reranking and fusion on outputs. 3) Preference-conditioned prompting. The preference-conditioned learning approaches [64, 4] train a policy conditioned on preference weightings to maximize the expected rewards. This concept is reflected in LMs alignment as preference-conditioned prompting: this line of work [56, 48, 18] directly presents the preference weightings in prompts after a finetuning process. The latter two paradigms are more efficient, while relying heavily on either the reduced mis-specification hypothesis [40] or unguaranteed OOD generalization ability [63], posing challenges in terms of interpretability and robustness. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose MOD, a simple, training-free yet effective algorithm for multi-objective LMs alignment. By addressing the challenges of retraining and resource-intensive processes, our method provides a decoding-time solution while offering insights into the broader applicability of combining differently tuned models. Through extensive analysis and empirical evidence, we demonstrate the effectiveness and practicality of our method under the $f$ -divergence framework, paving the way for improving LM performance across diverse tasks and use cases. ", "page_idx": 9}, {"type": "text", "text": "It is also important to acknowledge the limitations of our work. 1) The analysis is primarily based on tabular setting [55], not taking function approximation error into consideration. 2) Decoding from a response-level probability distribution at the token level may lead to degraded performance, which is likely to be alleviated by energy-based approaches [37, 24, 58]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, and NSF IIS 2229881. NAS acknowledges the support of NSF IIS 2113530. The authors also thank Yizhong Wang for useful discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] S. M. Ali and S. D. Silvey. A general class of coefficients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B (Methodological), 28(1):131\u2013142, 1966.   \n[2] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. ArXiv, abs/2310.12036, 2023.   \n[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022.   \n[4] Toygun Basaklar, Suat Gumussoy, and \u00dcmit Y. Ogras. Pd-morl: Preference-driven multiobjective reinforcement learning algorithm. ArXiv, abs/2208.07914, 2022.   \n[5] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.   \n[7] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u2019er\u2019emy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Rapha\u00ebl S\u00e9gerie, Micah Carroll, Andi Peng, Phillip J. K. Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco di Langosco, Peter Hase, Erdem Biyik, Anca D. Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. ArXiv, abs/2307.15217, 2023.   \n[8] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. CoRR, abs/2401.01335, 2024.   \n[9] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4299\u20134307, 2017.   \n[10] Imre Csisz\u00e1r. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85\u2013108, 1964.   \n[11] Imre Csisz\u00e1r. On information-type measure of difference of probability distributions and indirect observations. Studia Sci. Math. Hungar., 2:299\u2013318, 1967.   \n[12] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.   \n[13] Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. SteerLM: Attribute conditioned SFT as an (user-steerable) alternative to RLHF. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11275\u201311288, Singapore, December 2023. Association for Computational Linguistics.   \n[14] Rick Durrett. Probability: Theory and Examples, 4th Edition. Cambridge University Press, 2010.   \n[15] S.H. Friedberg, A.J. Insel, and L.E. Spence. Linear Algebra. Pearson Education, 2014.   \n[16] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 10835\u201310866. PMLR, 2023.   \n[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[18] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et al. Controllable preference optimization: Toward controllable multi-objective alignment. arXiv preprint arXiv:2402.19085, 2024.   \n[19] James Y Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, and Muhao Chen. Offset unlearning for large language models. arXiv preprint arXiv:2404.11045, 2024.   \n[20] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.   \n[21] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. CoRR, abs/2310.11564, 2023.   \n[22] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. arXiv preprint arXiv:2307.04657, 2023.   \n[23] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In Annual Meeting of the Association for Computational Linguistics, 2023.   \n[24] Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. Controlled text generation as continuous optimization with multiple constraints. In Neural Information Processing Systems, 2021.   \n[25] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sampling from language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2251\u20132277. Association for Computational Linguistics, 2022.   \n[26] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for forming multi-task policies. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023.   \n[27] Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2023.   \n[28] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language models by proxy. CoRR, abs/2401.08565, 2024.   \n[29] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Don\u2019t throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2023.   \n[30] Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, and Mathieu Blondel. Decoding-time realignment of language models. arXiv preprint arXiv:2402.02992, 2024.   \n[31] Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. ArXiv, abs/2402.18540, 2024.   \n[32] Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via perplexity optimization. arXiv preprint arXiv:2404.11531, 2024.   \n[33] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models. CoRR, abs/2310.17022, 2023.   \n[34] Yurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd edition, 2018.   \n[35] Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios Tzannetos, Goran Radanovi\u00b4c, and Adish Singla. Reward model learning vs. direct policy optimization: A comparative analysis of learning from human preferences. arXiv preprint arXiv:2403.01857, 2024.   \n[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[37] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. COLD decoding: Energy-based constrained text generation with langevin dynamics. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \n[38] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training, 2018.   \n[39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[40] Alexandre Ram\u00e9, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[41] Alexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. WARM: on the beneftis of weight averaged reward models. CoRR, abs/2401.12187, 2024.   \n[42] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR, abs/2009.01325, 2020.   \n[43] Wenpin Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. arXiv preprint arXiv:2403.06279, 2024.   \n[44] Peter Vamplew, Richard Dazeley, Cameron Foale, Sally Firmin, and Jane Mummery. Humanaligned artificial intelligence is a multiobjective problem. Ethics and Information Technology, 20:27 \u2013 40, 2017.   \n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008, 2017.   \n[46] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl, 2020.   \n[47] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints. In The Twelfth International Conference on Learning Representations, 2024.   \n[48] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. CoRR, abs/2402.18571, 2024.   \n[49] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023.   \n[50] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv, abs/2311.09528, 2023.   \n[51] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 23965\u201323998. PMLR, 2022.   \n[52] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.   \n[53] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint, 2024.   \n[54] Chunpu Xu, Steff iChern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, and Pengfei Liu. Align on the fly: Adapting chatbot behavior to established norms. CoRR, abs/2312.15907, 2023.   \n[55] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 33:18784\u201318794, 2020.   \n[56] Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment. CoRR, abs/2402.10207, 2024.   \n[57] Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. A theoretical analysis of nash learning from human feedback under general kl-regularized preference. arXiv preprint arXiv:2402.07314, 2024.   \n[58] Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. Probabilistic inference in language models via twisted sequential monte carlo, 2024.   \n[59] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. Weak-to-strong jailbreaking on large language models. CoRR, abs/2401.17256, 2024.   \n[60] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. CoRR, abs/2305.10425, 2023.   \n[61] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2023.   \n[62] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-preference-for-all: Multi-objective direct preference optimization for language models. CoRR, abs/2310.03708, 2023.   \n[63] Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, and Simon Shaolei Du. Free from bellman completeness: Trajectory stitching via model-based return-conditioned supervised learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[64] Baiting Zhu, Meihua Dang, and Aditya Grover. Scaling pareto-efficient decision making via offline multi-objective rl. ArXiv, abs/2305.00567, 2023.   \n[65] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019.   \n[66] E. Zitzler and L. Thiele. Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach. IEEE Transactions on Evolutionary Computation, 3(4):257\u2013271, 1999.   \n[67] Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang, Zhilin Yang, and Jie Tang. Controllable generation from pre-trained language models via inverse prompting. Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Impact Statement 17 ", "page_idx": 15}, {"type": "text", "text": "B Notation 17 ", "page_idx": 15}, {"type": "text", "text": "C Main Algorithm 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Pipeline 17   \nC.2 Divergence measures and closed-form policies . 17   \nC.3 Extended variants 18 ", "page_idx": 15}, {"type": "text", "text": "D Full Theoretical Results and Omitted Proofs 19 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Definitions . 19   \nD.2 Proofs of subsection 5.2 19   \nD.3 Proof of key theorem 22   \nD.4 Proofs of subsection 5.3 . 24   \nD.5 Proofs of subsection 5.1 . 26 ", "page_idx": 15}, {"type": "text", "text": "E Implementation Details 28 ", "page_idx": 15}, {"type": "text", "text": "F Supplementary Results 29 ", "page_idx": 15}, {"type": "text", "text": "F.1 Motivating example 29   \nF.2 Additional results for Helpful Assistant 29   \nF.3 Additional results for BeaverTails 29   \nF.4 Additional results for HelpSteer 32   \nF.5 Additional results for Open Instruction-Following 33   \nF.6 Example generations 34 ", "page_idx": 15}, {"type": "text", "text": "A Impact Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our work proposes a decoding-time language model alignment method aimed at advancing academic research and meeting industry needs. If misused in downstream tasks, especially as what we have shown in Table 7, it could potentially induce language models to generate harmful, offensive, or privacy-infringing content, leading to privacy breaches and societal harm. Nevertheless, this is not directly related to our research, as our primary focus is on a general algorithm with theoretical guarantees. ", "page_idx": 16}, {"type": "text", "text": "B Notation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we introduce a set of notation to be used throughout. For any differentiable function $f$ , let $\\nabla f$ denote its gradient. For any $N\\in\\mathbb{N}$ , we denote the index set $\\{1,\\cdot\\cdot\\cdot,N\\}$ as $[N]$ . Let $e_{s}$ be the sth standard basis vector. For any $i,j\\in\\mathbb{Z}_{\\geq0}$ , $\\delta_{i j}$ represents the Kronecker delta function [15], which output 1 if $i=j$ otherwise 0. For any $n\\in\\mathbb N$ , $\\Delta^{n}$ represents the $n$ -dimensional probability simplex $\\{(p_{1},\\ldots,p_{n+1}):p_{i}\\geq0,\\;\\forall i\\in$ $\\forall i\\in[n+1]$ , $\\textstyle\\sum_{j=1}^{n+1}p_{j}=1\\}$ , and $\\Delta(X)$ represents the set of probability distributions over a set $X$ . $\\mathcal{X}$ denotes the prompt set, $\\Sigma$ denotes the alphabet set, $\\mathcal{V}\\subset\\Sigma^{*}$ denotes the response set, and the policy set $\\boldsymbol{S}$ is defined as all mappings from $\\mathcal{X}$ to $\\Delta(\\mathcal{Y})$ . ", "page_idx": 16}, {"type": "text", "text": "C Main Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Pipeline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Data: Alphabet set $\\Sigma$ , prompt $x_{0}$ , number of beams $K$ , maximum length $L$ , divergence function $f$ , preference weightings $w\\in\\Delta^{M-1}$ , and policies $\\pi_{\\mathrm{ref}},\\pi_{1},\\pi_{2},.~.~.~,\\pi_{M}$ ", "page_idx": 16}, {"type": "image", "img_path": "3csuL7TVpV/tmp/b5e9f38903e0ea28edfab42126248bca97a78994df1402ba3727bcef38eb9a99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "return sequence with the highest $f$ -score in $S_{\\mathrm{completed}}$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Divergence measures and closed-form policies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We acknowledge that commonly used $f$ -divergence measures have been introduced in [47] and show them here for completeness: ", "page_idx": 16}, {"type": "table", "img_path": "3csuL7TVpV/tmp/903af3fcf85cb0e61d5f877e4cc2d25341c4efad015899a3725c6b2dd33bd047.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Here we show the optimal sampling policies for multi-objective w.r.t. these divergence measures: ", "page_idx": 17}, {"type": "table", "img_path": "3csuL7TVpV/tmp/f416f18e8179f7ebf1d253733371c48c0ee266fc5e76af7566e309f7703f3ea0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "3csuL7TVpV/tmp/e16df12e66ed0228b3661396f577dd6a1801471e99a211ac861081cf11576e3d.jpg", "table_caption": ["And we show the optimal decoding policies for multi-objective w.r.t. these divergence measures: "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.3 Extended variants ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "SFT. We assume that, supervised fine-tuning (SFT) on pre-trained model $\\mathcal{M}^{-}$ yielding $\\mathcal{M}^{+}$ , is implicitly optimizing a underlying reward $r$ w.r.t. Reverse KL-divergence, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{M}^{+}}(y|x)\\propto\\mathbb{P}_{M^{-}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(y|x))\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Based on this, our approach, namely Eq. (7), is applicable to SFT models. ", "page_idx": 17}, {"type": "text", "text": "Proxy-tuning [28] $\\pmb{\\&}$ jail-breaking [59]. Based on the claim above, for another base model $\\mathcal{M}$ , we thus have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathcal{M}}(y|x)\\cdot\\frac{\\mathbb{P}_{\\mathcal{M}^{+}}(y|x)}{\\mathbb{P}_{\\mathcal{M}^{-}}(y|x)}\\propto\\mathbb{P}_{\\mathcal{M}(y|x)}\\cdot\\exp(\\frac{1}{\\beta}r(y|x))\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which reflects the tuned version of model $\\mathcal{M}$ . And this is exactly the proxy-tuning approach, validated by extensive experiments in [28]. Reversing the position of $\\mathbb{P}_{\\mathcal{M}^{+}}$ and $\\mathbb{P}_{\\mathcal{M}_{-}}$ yields jail-breaking [59]. $\\delta$ -unlearning [19] is the same. ", "page_idx": 17}, {"type": "text", "text": "Multi-objective proxy-tuning. Moreover, it is worth noting that, our method can be applied as a lightweight approach for large-scale models, as a multi-objective extension of proxy-tuning [28]. In particular, to tune a large pre-trained model $\\mathcal{M}$ , we can first tune $\\mathcal{M}_{1}^{+},\\mathcal{M}_{2}^{+},\\cdot\\cdot\\cdot,\\mathcal{M}_{M}^{+}$ from a relatively smaller model ${\\mathcal{M}}^{-}$ by PPO, DPO or SFT, and decode $y_{t}$ at timestep $t$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{s\\in\\Sigma}\\frac{\\mathbb{P}_{\\mathcal{M}}(y_{<t},s|x)}{\\mathbb{P}_{\\mathcal{M}^{-}}(y_{<t},s|x)}\\cdot\\prod_{i=1}^{M}\\mathbb{P}_{\\mathcal{M}_{i}^{+}}(y_{<t},s|x)^{w_{i}}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "DeRa [30]. Given $\\begin{array}{r}{\\mathbb{P}_{\\mathcal{M}^{+}}(y|x)\\propto\\mathbb{P}_{M^{-}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(y|x))}\\end{array}$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{M^{-}}(y|x)\\cdot\\left({\\frac{\\mathbb{P}_{\\mathcal{M}^{+}}(y|x)}{\\mathbb{P}_{M^{-}}(y|x)}}\\right)^{\\frac{\\beta}{\\beta^{\\prime}}}\\propto\\mathbb{P}_{M^{-}}(y|x)\\cdot\\exp({\\frac{1}{\\beta^{\\prime}}}r(y|x))\\ ,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "yields a $\\beta^{\\prime}$ -realigned version of ${\\mathcal{M}}^{-}$ ", "page_idx": 18}, {"type": "text", "text": "MODPO [62]. Assuming $\\pi_{i}$ is the optimal policy for ${\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\big(\\cdot\\|\\pi_{\\mathrm{ref}}\\big),\\forall i\\in[M]$ , then the optimal policy for $\\textstyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}$ w.r.t. $\\beta\\,\\mathrm{KL}\\,(\\cdot\\|\\pi_{\\mathrm{ref}}),\\pi^{\\star}\\propto\\prod\\pi_{i}^{w_{i}}$ , is the minimizer of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}_{(x,y_{w},y_{l})\\sim\\mathcal{D}_{1}}\\log\\sigma\\left(\\frac{1}{w_{1}}\\left(\\beta\\log\\frac{\\pi(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\right)-\\frac{w_{-1}^{T}}{w_{1}}\\sum_{i=2}^{M}\\left(\\beta\\log\\frac{\\pi_{i}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{i}(y_{i}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\sigma$ is sigmoid function, and $\\mathcal{D}_{1}$ is the comparison dataset corresponding to $\\mathcal{R}_{1}$ . Since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta\\log\\frac{\\pi_{i}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi_{i}(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}=\\mathcal{R}_{i}(y_{w}|x)-\\mathcal{R}_{i}(y_{l}|x)\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we can substitute this term with learned reward representations $r_{\\phi,i}$ and yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\underbrace{\\mathbb{E}}_{(x,y_{w},y_{l})\\sim\\mathcal{D}_{1}}\\log\\sigma\\left(\\frac{1}{w_{1}}\\left(\\beta\\log\\frac{\\pi(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\beta\\log\\frac{\\pi(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\right)-\\frac{w_{-1}^{T}}{w_{1}}\\left(r_{\\phi,-1}(y_{w}|x)-r_{\\phi,-1}(y_{l}|x)\\right)\\right)\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the optimization objective of MODPO. ", "page_idx": 18}, {"type": "text", "text": "D Full Theoretical Results and Omitted Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Definitions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Definition 1 ( $f$ -divergence [1, 10, 11]). For probability measures $P$ and $Q$ , let $\\mu$ be a dominating $P$ $Q$ $P,Q\\ll\\mu)$ $p,q$ $\\frac{d P}{d\\mu}$ $\\frac{d\\bar{Q}}{d\\mu}$ respectively. For simplicity, here we assume $q>0$ almost surely. Then $f$ -divergence from $P$ to $Q$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nI_{f}(p\\|q):=\\int q f\\left(\\frac{p}{q}\\right)d\\mu\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $f$ is convex on $\\mathbb{R}_{+}$ , satisfying $f(1)=0$ . Most useful divergence measures are included in $f$ -divergences, and the commonly used ones and corresponding $f$ are introduced in Appendix C.2. ", "page_idx": 18}, {"type": "text", "text": "Definition 2 (Barrier function [34]). Given conditions satisfied in Definition 1, if additionally $0\\not\\in\\mathrm{dom}(\\nabla f)$ , then $f$ is a barrier function. If a barrier function $f$ is continuously differentiable and strongly convex on $\\mathbb{R}_{+}$ , then $f$ is a strongly convex and smooth barrier function (abbreviated as strong-barrier function). ", "page_idx": 18}, {"type": "text", "text": "Definition 3 (Expected calibration error [17, 47]). Denote the ground truth distribution as $\\mathbb{P}$ , prompt as $X$ and response as $Y$ . The expected calibration error of a stochastic policy $\\pi$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{ECE}(\\pi):=\\underset{y\\sim\\pi(\\cdot\\vert x)}{\\mathbb{E}}~\\left\\vert\\mathbb{P}(Y=y\\vert X=x)-\\pi(y\\vert x)\\right\\vert\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hypothesis 1 (Reduced reward mis-specification [51, 40, 21]). Let $\\theta_{i}$ be the parameter of the optimal policy for objective $J_{i},\\forall i\\in[M]$ , and $\\theta_{w}^{*}$ be the parameter of the optimal policy for the interpolated objective $\\textstyle\\sum_{i=1}^{M}w_{i}\\cdot J_{i}$ , then this hypothesis claims ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\theta_{w}^{*}\\in\\left\\{\\sum_{i=1}^{M}\\lambda_{i}\\cdot\\theta_{i},\\lambda\\in\\Delta^{M-1}\\right\\}\\,,\\;\\forall w\\in\\Delta^{M-1}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.2 Proofs of subsection 5.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 3. If $f$ is not a barrier function, then for $\\forall C\\in\\mathbb{R}_{+}$ , $N\\in\\mathbb{Z}_{\\geq4}$ , $M\\in\\mathbb{Z}_{\\geq2}$ , $\\boldsymbol{\\mathcal{V}}=\\{y_{i}\\}_{i=1}^{N}$ , any multi-objective decoding or merging algorithm $\\mathcal{A}:S^{M+1}\\times\\Delta^{M-1}\\to S$ , there exists a reference ", "page_idx": 18}, {"type": "text", "text": "policy $\\pi_{\\mathrm{ref}}$ , policies $\\{\\pi_{i}\\}_{i=1}^{M}$ and $\\pi^{\\prime}$ , reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ , preference weightings $w\\in\\Delta^{M-1}$ and $\\beta\\in\\mathbb{R}_{+}$ , s.t. $\\pi_{i}$ is the optimal policy for $\\mathcal{R}_{i}$ w.r.t. $\\beta\\cdot I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ (see Definition 1 in Appendix $D.I$ ), $\\forall i\\in[M]$ , but ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\underbrace{\\mathbb{E}}_{y\\sim\\pi_{A,w}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]\\le\\underbrace{\\mathbb{E}}_{y\\sim\\pi^{\\prime}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-C\\;,a n d}}\\\\ {\\underbrace{\\mathbb{E}}_{y\\sim\\pi_{A,w}}\\left[\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-\\beta I_{f}(\\pi_{A,w}\\|\\pi_{\\mathrm{ref}})\\le\\underbrace{\\mathbb{E}}_{y\\sim\\pi^{\\prime}}\\left[\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-\\beta I_{f}(\\pi^{\\prime}\\|\\pi_{\\mathrm{ref}})-C\\;,}\\\\ {r e\\;\\pi_{A,w}(y):=\\mathcal{A}\\big(\\pi_{\\mathrm{ref}},\\pi_{1},\\pi_{2},\\ldots,\\pi_{M},w\\big)(y)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Since $f$ is not a barrier function, $0\\in\\operatorname{dom}(\\nabla f)$ . Now we can define $p:=\\operatorname*{max}_{x\\in[0\\ N]}\\nabla f(x)$ , $x{\\in}[0,N]\\quad^{\\cdot}\\quad$   \n$\\textstyle q:=\\operatorname*{min}_{x\\in[0,N]}\\nabla f(x),\\,r:=\\operatorname*{max}_{x\\in[0,N]}f(x)-\\operatorname*{min}_{x\\in[0,N]}f(x),\\,s:=\\frac{N-2}{N-3}\\cdot C.$ Let $w=(0.5,0.5,0,\\dots,0)$ , N \u2212 2   \nand we pick $k=\\mathrm{\\Delta\\argmin\\}\\ \\ \\pi_{A,w}(y_{i})$ . Let $\\begin{array}{r}{\\pi_{\\mathrm{ref}}(y_{i})=\\frac{1}{N}}\\end{array}$ , $\\pi_{1}(y_{i})=\\delta_{1i}$ , \u03c02(yi) = \u03b42i, $\\begin{array}{r}{\\pi_{j}(y_{i})=\\frac{1}{N}}\\end{array}$   \na $\\begin{array}{r l}{\\jmath\\in\\{3,4,\\ldots,N\\}}&{}\\\\ {\\mathrm{nd~}\\pi^{\\prime}(y_{i})=\\delta_{i k},\\forall i\\in[N],\\ j\\in\\{3,4,\\ldots,M\\}.\\mathrm{~And~set}\\ \\mathcal{R}_{1}(y_{i})=\\left\\{\\begin{array}{l l}{2p+2r+2s}&{i=1}\\\\ {4q-2p-2r-2s}&{i=2}\\\\ {p+q+r+s}&{i=k}\\end{array}\\right.}\\\\ {\\mathrm{n}_{2}(y_{i})=\\left\\{\\begin{array}{l l}{4q-2p-2r-2s}&{i=1}\\\\ {2p+2r+2s}&{i=2}\\\\ {p+q+r+s}&{i=k}\\\\ {2q}&{0/\\mathrm{w}}\\end{array}\\right.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Let $\\beta=1$ , then the optimization objective for $\\mathcal{R}_{1}$ w.r.t. $I_{f}$ is $J_{1}(\\pi):=\\underset{y\\sim\\pi}{\\mathbb{E}}[\\mathcal{R}_{1}(y)]-I_{f}(\\pi\\vert\\vert\\pi_{\\mathrm{ref}}),$ and the Lagrangian dual is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{1}(\\pi):=\\sum_{i=1}^{N}\\left(-\\mathcal{R}_{1}(y_{i})\\cdot\\pi(y_{i})+\\frac{1}{N}f\\left(N\\cdot\\pi(y_{i})\\right)\\right)+\\lambda\\left(\\sum_{i=1}^{N}\\pi(y_{i})-1\\right)-\\sum_{i=1}^{N}\\mu_{i}\\pi(y_{i})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As the objective is convex and the constraints are affine, we can directly apply the Karush-KuhnTucker conditions [34]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{L}_{1}(\\pi_{1}^{\\star})=0\\;,}\\\\ {\\displaystyle\\sum_{i=1}^{N}\\pi_{1}^{\\star}(y_{i})=1\\;,}\\\\ {\\pi_{1}^{\\star}(y_{i})\\geq0\\;,}\\\\ {\\mu_{i}^{\\star}\\geq0\\;,}\\\\ {\\mu_{i}^{\\star}\\pi_{1}^{\\star}(y_{i})=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Eq. (8) implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\mathcal{R}_{1}(y_{i})+\\nabla f(N\\cdot\\pi_{1}^{\\star}(y_{i}))+\\lambda^{\\star}-\\mu_{i}^{\\star}=0\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\pi_{1}^{\\star}(y_{1})>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\lambda^{\\star}=\\mathcal{R}_{1}(y_{1})-\\nabla f(N\\cdot\\pi_{1}^{\\star}(y_{1}))}}\\\\ {{\\ge p+2r+2s~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and then for $\\forall j\\neq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{j}^{\\star}=-\\mathcal{R}_{1}(y_{j})+\\nabla f(N\\cdot\\pi_{1}^{\\star}(y_{j}))+\\lambda^{\\star}}\\\\ &{\\quad\\ge-p-q-r-s+q+p+2r+2s}\\\\ &{\\quad=r+s}\\\\ &{\\quad>0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining it with Eq. (9) yields $\\pi_{1}^{\\star}(y_{j})=0$ for $\\forall j\\neq1$ , which is exactly $\\pi_{1}$ . Note that we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ(\\pi_{1})\\geq2p+2r+2s-\\operatorname*{max}_{x\\in[0,N]}f(x)\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For any $\\pi^{\\prime}$ with $\\pi^{\\prime}(y_{1})=0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi^{\\prime})\\leq p+q+r+s-\\underset{x\\in[0,N]}{\\operatorname*{min}}f(x)}\\\\ &{\\qquad=p+q+2r+s-\\underset{x\\in[0,N]}{\\operatorname*{max}}f(x)}\\\\ &{\\qquad<J(\\pi_{1})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus $\\pi_{1}$ is the optimal policy for $\\mathcal{R}_{1}$ w.r.t. $I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ . Similarly, $\\pi_{2}$ is the optimal policy for $\\mathcal{R}_{2}$ w.r.t. $I_{f}(\\cdot|\\pi_{\\mathrm{ref}})$ . By convexity of $f$ , the minimum of $I_{f}(\\pi\\lVert\\pi_{\\mathrm{ref}})$ is obtained when $\\pi=\\pi_{\\mathrm{ref}}$ , and thus $\\pi_{j}$ is the optimal policy for ${\\mathcal{R}}_{j}$ w.r.t. $I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ , for $\\forall j\\in\\{3,4,\\ldots,M\\}$ . Therefore, all conditions are well satisfied by this construction. Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sim\\pi^{\\prime}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]=p+q+r+s\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "While by the selection of $k$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{y\\sim\\pi_{A,w}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]\\leq\\frac{(N-3)\\cdot2q+p+q+r+s}{N-2}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Comparing Eq. (10) with Eq. (11), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y\\sim\\pi_{\\mathcal{A},w}}{\\mathbb{E}}\\left[\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]\\le\\underset{y\\sim\\pi^{\\prime}}{\\mathbb{E}}\\left[\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-\\frac{N-3}{N-2}s}\\\\ &{\\qquad\\qquad\\qquad=\\underset{y\\sim\\pi^{\\prime}}{\\mathbb{E}}\\left[\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-C\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\pi_{\\mathrm{ref}}$ is a uniform distribution and both $\\pi_{A,w},\\pi^{\\prime}$ are one-point distributions, thus $I_{f}(\\pi_{A,w}\\|\\pi_{\\mathrm{ref}})=I_{f}(\\pi^{\\prime}\\|\\pi_{\\mathrm{ref}})$ . We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbb{E}}_{y\\sim\\pi.A,w}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-I_{f}(\\pi_{A,w}\\|\\pi_{\\mathrm{ref}})\\leq\\underbrace{\\mathbb{E}}_{y\\sim\\pi^{\\prime}}\\left[\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y)\\right]-I_{f}(\\pi^{\\prime}\\|\\pi_{\\mathrm{ref}})-C\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 1. Given a reference policy $\\pi_{\\mathrm{ref}}$ , reward function $\\mathcal{R}$ , a strong-barrier function $f$ and $\\beta\\in\\mathbb{R}_{+}$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z(x)+\\frac{1}{\\beta}\\mathcal{R}(y|x)\\right)\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $Z(x)$ is the normalization factor w.r.t. $x$ , is the optimal policy for ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\underset{y\\sim\\pi(\\cdot|x)}{\\mathbb{E}}\\mathcal{R}(y|x)-\\beta\\underset{y\\sim\\pi_{\\mathrm{ref}}(\\cdot|x)}{\\mathbb{E}}f\\left(\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The lemma is revealed by Theorem 1 in [47]. For completeness, we give a brief proof here. Since $f$ is convex and barrier, we can directly use Lagrange multiplier to solve ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{Y}}\\pi(y|x)\\mathcal{R}(y|x)-\\beta\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)f\\left(\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\;,\\;\\mathrm{w.r.t.}\\;\\sum_{y\\in\\mathcal{Y}}\\pi(y|x)=1\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for each $x\\in\\mathscr{X}$ , which implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}(y|x)-\\beta\\nabla f\\left(\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)-\\lambda(x)=0\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\lambda(x)\\in\\mathbb{R}$ . Taking $Z(x):=\\beta\\lambda(x)$ completes the proof. ", "page_idx": 20}, {"type": "text", "text": "Proposition 1. Given a reference policy $\\pi_{\\mathrm{ref}}$ , optimal policies $\\pi_{1},\\pi_{2},\\ldots,\\pi_{M}$ for each reward function $\\mathcal{R}_{1},\\mathcal{R}_{2},\\ldots,\\mathcal{R}_{M}$ w.r.t. $\\beta\\cdot I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ , $\\beta\\in\\mathbb{R}_{+}$ , and $w\\in\\Delta^{M-1}$ , if $f$ is a strong-barrier function, then the optimal policy for reward function $\\begin{array}{r}{r=\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}\\;w.r.t.\\ \\beta\\cdot I_{f}(\\cdot||\\pi_{\\mathrm{ref}})\\:i}\\end{array}$ s: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi^{\\star}(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z(x)+\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $Z(x)$ is the normalization factor w.r.t. $x$ , and numerically computable when $|\\mathcal{V}|$ is finite. ", "page_idx": 21}, {"type": "text", "text": "Proof. As Lemma 1 shows, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{i}(y|x)=\\beta\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)+\\beta Z_{i}(x)\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi^{\\star}(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z^{\\star}(x)+\\frac{1}{\\beta}\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Apply Eq. (12) into Eq. (13), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{\\star}(y|x)=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z^{\\star}(x)+\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\left(\\nabla f\\left(\\frac{\\pi_{i}(y)}{\\pi_{\\mathrm{ref}}(y)}\\right)+Z_{i}(x)\\right)\\right)}\\\\ &{\\qquad=\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-Z(x)+\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{Z(x):=Z^{\\star}(x)-\\sum_{i=1}^{M}w_{i}Z_{i}(x)}\\end{array}$ . And $Z(x)$ is the root of $\\phi_{x}(t)=0$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi_{x}(t):=\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(-t+\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)-1\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $f$ is strongly convex and continuously differentiable, $\\phi_{x}(t)$ is monotonically decreasing and continuous. If $\\lvert\\mathcal{V}\\rvert$ is finite, we can set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t_{1,x}:=-\\nabla f(1)+\\displaystyle\\operatorname*{min}_{y\\in\\mathcal{Y}}\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\,,}\\\\ &{t_{2,x}:=-\\nabla f(1)+\\displaystyle\\operatorname*{max}_{y\\in\\mathcal{Y}}\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi(t_{1,x})\\geq0\\;,}\\\\ {\\phi(t_{2,x})\\leq0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus $Z(x)\\in[t_{1,x},t_{2,x}]$ . Finally, $Z(x)$ can be numerically computed by bisection method. ", "page_idx": 21}, {"type": "text", "text": "D.3 Proof of key theorem ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition 2 (Policy-to-reward mapping). Given a reference policy $\\pi_{\\mathrm{ref}}$ , optimal policies $\\pi_{1},\\pi_{2},\\ldots,\\pi_{M}$ for each reward function $\\mathcal{R}_{1},\\mathcal{R}_{2},\\ldots,\\mathcal{R}_{M}$ w.r.t. $\\beta\\,\\cdot\\,I_{f}\\!\\left(\\cdot\\|\\pi_{\\mathrm{ref}}\\right)\\!,\\ \\beta\\ \\in\\ \\mathbb{R}_{+}$ , and $w\\in\\Delta^{M-1}$ , if $f$ is a strong-barrier function, then for $\\forall x\\in\\mathcal{X}$ , $y_{1},y_{2}\\in\\mathcal{Y}$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y_{1}|x)\\ge\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y_{2}|x)\\iff\\sum_{i=1}^{M}w_{i}\\nabla f\\left(\\frac{\\pi_{i}(y_{1}|x)}{\\pi_{\\mathrm{ref}}(y_{1}|x)}\\right)\\ge\\sum_{i=1}^{M}w_{i}\\nabla f\\left(\\frac{\\pi_{i}(y_{2}|x)}{\\pi_{\\mathrm{ref}}(y_{2}|x)}\\right)\\ .\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. As Eq. (3) shows, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}_{i}(y|x)=\\beta\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)+\\beta Z_{i}(x)\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\forall i\\in[M],y\\in\\mathcal{Y}$ , where $Z_{i}(x)$ is the normalization factor. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y_{1}|x)-\\displaystyle\\sum_{i=1}^{M}w_{i}\\mathcal{R}_{i}(y_{2}|x)=\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\big(\\mathcal{R}_{i}(y_{1}|x)-\\mathcal{R}_{i}(y_{2}|x)\\big)}&{}\\\\ {\\displaystyle=\\beta\\sum_{i=1}^{M}w_{i}\\cdot\\Bigg(\\nabla f\\left(\\frac{\\pi_{i}(y_{1}|x)}{\\pi_{\\mathrm{ref}}(y_{1}|x)}\\right)-\\nabla f\\left(\\frac{\\pi_{i}(y_{2}|x)}{\\pi_{\\mathrm{ref}}(y_{2}|x)}\\right)\\Bigg)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\beta>0$ , the proposition holds. ", "page_idx": 22}, {"type": "text", "text": "Theorem 5 (Key theorem). Given a reference policy $\\pi_{\\mathrm{ref}}$ , optimal policies $\\pi_{1},\\pi_{2},\\ldots,\\pi_{M}$ for each reward function $\\mathcal{R}_{1},\\mathcal{R}_{2},\\ldots,\\mathcal{R}_{M}$ w.r.t. $\\beta\\cdot I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ , $\\beta\\in\\mathbb{R}_{+}$ , and $w\\,\\in\\,\\Delta^{M-1}$ , if $f$ is $a$ strong-barrier function, then for $\\forall x\\in\\mathcal{X}$ , $w\\in\\Delta^{M-1}$ , $\\exists C\\in\\mathbb{R}$ , s.t. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{argmax}_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is an optimal solution for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)\\;,\\;w.r t.\\;\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)\\geq C\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. First we define ", "page_idx": 22}, {"type": "equation", "text": "$$\ng_{x}(t)=\\left(\\nabla f\\right)^{(-1)}\\left(\\frac{t}{\\beta}-\\sum_{i=1}^{M}w_{i}Z_{i}(x)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Eq. (14), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\ng_{x}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)\\right)=\\left(\\nabla f\\right)^{(-1)}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y^{\\prime}:=\\underset{y}{\\mathrm{argmax}}\\ \\pi_{\\mathrm{ref}}(y|x)\\cdot(\\nabla f)^{(-1)}\\left(\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\nabla f\\left(\\frac{\\pi_{i}(y|x)}{\\pi_{\\mathrm{ref}}(y|x)}\\right)\\right)}\\\\ &{\\qquad=\\underset{y}{\\mathrm{argmax}}\\ \\pi_{\\mathrm{ref}}(y|x)\\cdot g_{x}\\left(\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y|x)\\right)\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\begin{array}{r}{C:=\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y^{\\prime}|x)}\\end{array}$ . Suppose $y^{\\prime}$ is not an optimal solution for Eq. (15), then $\\exists y^{\\prime\\prime}\\in\\mathcal{Y}$ , s.t. $\\pi_{\\mathrm{ref}}(y^{\\prime\\prime}|x)>\\pi_{\\mathrm{ref}}(y^{\\prime}|x)$ and $\\begin{array}{r}{\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y^{\\prime\\prime}|x)\\geq\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y^{\\prime}|x)}\\end{array}$ . Since $f$ is strongly convex, $g_{x}$ is continuously increasing and invertible. Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{ref}}(y^{\\prime\\prime}|x)\\cdot g_{x}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y^{\\prime\\prime}|x)\\right)>\\pi_{\\mathrm{ref}}(y^{\\prime}|x)\\cdot g_{x}\\left(\\sum_{i=1}^{M}w_{i}\\cdot\\mathcal{R}_{i}(y^{\\prime}|x)\\right)\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "contradictory to the definition of $y^{\\prime}$ . ", "page_idx": 22}, {"type": "text", "text": "D.4 Proofs of subsection 5.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 3 (Eq. 13,14 in [39]). If $I_{f}$ is Reverse KL-divergence, Eq. (2) can be viewed as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\beta}\\underset{y\\sim\\pi\\left(\\cdot\\right)x}{\\mathbb{E}}\\left[r(y|x)\\right]-\\mathrm{KL}\\left(\\pi\\|\\pi_{\\mathrm{ref}}\\right)=-\\mathrm{KL}\\left(\\pi\\|\\pi_{\\mathrm{opt}}\\right)+\\mathrm{constant}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\pi_{\\mathrm{opt}}$ is the optimal policy for reward function $r$ w.r.t. $\\beta\\cdot I_{f}(\\cdot\\|\\pi_{\\mathrm{ref}})$ . Thus we can evaluate a policy $\\pi\\,\\dot{u}s i n g-\\mathrm{KL}\\,(\\pi\\|\\pi_{\\mathrm{opt}})$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. This proposition is revealed by Eq. 13,14 in [39]. For completeness, we give a brief proof here. Define $\\begin{array}{r}{\\bar{Z}(\\bar{x}):=\\log\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(\\bar{y}|x)\\bar{\\exp(\\frac{1}{\\beta}r(y|x))}}\\end{array}$ , which is a constant. Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\cfrac{1}{\\beta}\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,[r(y\\vert x)]+\\mathrm{KL}(\\pi\\|\\pi_{\\mathrm{ref}})}\\\\ &{=\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,\\pi(\\boldsymbol{\\pi}\\vert x)-\\log\\pi_{\\mathrm{ref}}(y\\vert x)-\\cfrac{1}{\\beta}r(y\\vert x)}\\\\ &{=\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,\\log\\pi(y\\vert x)-\\log\\left(\\pi_{\\mathrm{ref}}(y\\vert x)\\cdot\\exp\\left(\\frac{1}{\\beta}r(y\\vert x)-Z(x)\\right)\\right)-Z(x)}\\\\ &{=\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,\\log\\pi(y\\vert x)-\\log\\pi_{\\mathrm{opt}}(y\\vert x)-Z(x)}\\\\ &{=\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,\\log\\pi(y\\vert x)-\\frac{1}{\\mathbb{E}}\\,Z(x)}\\\\ &{=\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}\\,\\log\\pi(y\\vert x)-\\frac{\\mathbb{E}}{\\underset{y\\sim\\pi^{\\gamma}(\\cdot\\pi)}{\\mathbb{E}}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 2. Given $n,m\\in\\mathbb{N},$ , $x\\in\\Delta^{n-1}$ , $x\\succ0$ , $y\\in\\mathbb{R}^{n}$ and $C\\in\\mathbb{R}_{+}$ , $i f\\sum_{i=1}^{n}x_{i}y_{i}\\leq C$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}x_{i}\\exp\\left(-y_{i}\\right)\\geq\\exp(-C)\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Set $\\begin{array}{r l r}{f(y)\\!\\!}&{{}:=}&{\\!\\!\\sum_{i=1}^{n}x_{i}\\exp\\left(-y_{i}\\right)}\\end{array}$ , $\\scriptstyle h(y)\\ :=\\ \\sum_{i=1}^{n}x_{i}y_{i}\\ -\\ C$ , and the Lagrangian dual $L(y,\\lambda):=f(y)\\!+\\!\\lambda\\!\\cdot\\!h(\\overline{{y}})\\!\\cdot\\!$ . Since both $f$ and $h$ are conv ex, we can directly apply Karush-Kuhn-Tucker conditions: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{y}L(y^{\\star},\\lambda^{\\star})=0\\;,}\\\\ &{}&{h(y^{\\star})\\le0\\;,}\\\\ &{}&{\\lambda^{\\star}\\ge0\\;,}\\\\ &{}&{\\lambda^{\\star}h(y^{\\star})=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Eq. (16) we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\exp\\left(-y_{i}^{\\star}\\right)=\\lambda^{\\star}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $\\forall i\\in[n]$ . Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n}{x_{i}\\exp\\left(-y_{i}\\right)}=\\lambda^{\\star}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\\rightleftharpoons\\exp\\left(\\displaystyle\\sum_{i=1}^{n}{x_{i}\\log\\lambda^{\\star}}\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Theorem 4 (KL-divergence perspective). Given a reference policy $\\pi_{\\mathrm{ref;}}$ , policies $\\{\\pi_{i}\\}_{i=1}^{M}$ , reward functions $\\{\\mathcal{R}_{i}\\}_{i=1}^{M}$ , and $\\beta\\in\\mathbb{R}_{+}$ . Denote the optimal policy for ${\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\left(\\cdot\\Vert\\pi_{\\operatorname{ref}}\\right)$ as $p_{i}$ , $\\forall i\\in$ $[M]$ . For the reward function $\\textstyle\\sum_{i=1}^{M}w_{i}\\cdot{\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\left(\\cdot\\Vert\\pi_{\\mathrm{ref}}\\right)$ , the performance difference of policy $\\begin{array}{r}{\\pi_{w}(\\cdot|x)\\propto\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(\\cdot|x)}\\end{array}$ fr om optimal is $V^{\\star}-V$ . If for $\\forall i\\in\\{1,\\ldots,M\\}$ , $x\\in\\mathscr{X}$ , we have: $(i)$ max|log pi(y|x) \u2212log \u03c0i(y|x)| \u2264L , $(i i)\\;\\mathrm{KL}\\left(\\pi_{\\mathrm{ref}}(\\cdot|x)\\|\\pi_{i}(\\cdot|x)\\right)\\leq C$ , $\\mathrm{KL}\\left(\\pi_{\\mathrm{ref}}(\\cdot|x)\\|p_{i}(\\cdot|x)\\right)\\leq$ y\u2208Y   \n$C$ , where $\\mathcal{L},C\\in\\mathbb{R}_{+}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\nV^{\\star}-V\\leq2\\exp(C)\\cdot\\mathcal{L}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The optimal policy for ${\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\left(\\cdot\\Vert\\pi_{\\operatorname{ref}}\\right)$ is $\\begin{array}{r}{p_{i}(\\cdot|x)\\propto\\pi_{\\mathrm{ref}}(\\cdot|x)\\exp(\\frac{1}{\\beta}r(\\cdot|x))}\\end{array}$ and the optimal policy for $\\textstyle\\sum_{i=1}^{M}w_{i}\\cdot{\\mathcal{R}}_{i}$ w.r.t. $\\beta\\operatorname{KL}\\left(\\cdot\\Vert\\pi_{\\mathrm{ref}}\\right)$ is $\\begin{array}{r}{\\pi^{\\star}(\\cdot|x)\\propto\\prod_{i=1}^{M}p_{i}^{w_{i}}(\\cdot|x)}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Since $\\operatorname*{max}_{y\\in\\mathcal{Y}}\\left|\\log{p_{i}(y|x)}-\\log\\pi_{i}(y|x)\\right|\\le\\mathcal{L}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\pi_{i}(\\cdot|x)\\|p_{j}(\\cdot|x)\\right)-\\mathrm{KL}\\left(\\pi_{i}(\\cdot|x)\\|\\pi_{j}(\\cdot|x)\\right)\\leq\\mathcal{L}\\;,}\\\\ &{\\mathrm{KL}\\left(p_{i}(\\cdot|x)\\|\\pi_{j}(\\cdot|x)\\right)-\\mathrm{KL}\\left(p_{i}(\\cdot|x)\\|p_{j}(\\cdot|x)\\right)\\leq\\mathcal{L}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\forall x\\in\\mathcal{X},\\;i,j\\in[M]$ . Since $\\mathrm{KL}\\left(\\pi_{\\mathrm{ref}}(\\cdot|x)\\|\\pi_{i}(\\cdot|x)\\right)\\leq C$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y|x)\\log\\frac{\\pi_{\\mathrm{ref}}(y|x)}{\\pi_{i}(y|x)}\\leq C\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\forall x\\in\\mathcal{X},\\;i\\in[M]$ . By Lemma 2, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{w}(x):=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\displaystyle\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(y|x)}\\\\ &{\\qquad=\\displaystyle\\sum_{y\\in\\mathcal{Y}}\\pi_{\\mathrm{ref}}(y)\\exp\\left(-\\displaystyle\\sum_{i=1}^{M}w_{i}\\cdot\\log\\frac{\\pi_{\\mathrm{ref}}(y|x)}{\\pi_{i}(y|x)}\\right)}\\\\ &{\\qquad\\geq\\exp(-C)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, ", "page_idx": 24}, {"type": "equation", "text": "$$\nZ^{\\star}(x):=\\sum_{y\\in\\mathcal{Y}}\\prod_{i=1}^{M}p_{i}^{w_{i}}(y|x)\\geq\\exp(-C)\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{Y}}\\frac{\\prod_{i=1}^{M}p_{i}^{w_{i}}(y|x)}{Z^{\\star}(x)}=1\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{y\\in\\mathcal{Y}}\\left(\\frac{\\prod_{i=1}^{M}p_{i}^{w_{i}}(y|x)}{Z^{*}(x)}\\cdot\\sum_{i=1}^{M}w_{i}\\log\\frac{p_{i}(y|x)}{\\pi_{i}(y|x)}\\right)}}\\\\ &{}&{\\le\\frac{1}{Z^{*}(x)}\\sum_{y\\in\\mathcal{Y}}\\left(\\sum_{i=1}^{M}w_{i}p_{i}(y|x)\\cdot\\sum_{i=1}^{M}w_{i}\\log\\frac{p_{i}(y|x)}{\\pi_{i}(y|x)}\\right)}\\\\ &{}&{=\\frac{1}{Z^{*}(x)}\\left(\\sum_{i=1}^{M}w_{i}^{2}\\mathrm{KL}\\left(p_{i}(\\cdot|x)\\|\\pi_{i}(\\cdot|x)\\right)+\\sum_{i\\ne j}w_{i}w_{j}\\big(\\mathrm{KL}\\left(p_{i}(\\cdot|x)\\|\\pi_{j}(\\cdot|x)\\right)-\\mathrm{KL}\\left(p_{i}(\\cdot|x)\\|p_{j}(\\cdot|x)\\right)\\right.}\\\\ &{}&{\\le\\exp\\left(C\\right)\\cdot\\mathcal{L}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now apply Lemma 2, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{Z_{w}(x)}{Z^{\\star}(x)}=\\sum_{y\\in\\mathcal{Y}}\\left(\\frac{\\prod_{i=1}^{M}p_{i}^{w_{i}}(y|x)}{Z^{\\star}(x)}\\cdot\\exp\\left(-\\sum_{i=1}^{M}w_{i}\\log\\frac{p_{i}(y|x)}{\\pi_{i}(y|x)}\\right)\\right)}\\\\ &{\\qquad\\quad\\geq\\exp\\left(-\\exp(C)\\cdot\\mathcal{L}\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\displaystyle\\frac{1}{Z_{w}(x)}\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(\\cdot|x)\\|\\frac{1}{Z^{\\star}(x)}\\prod_{i=1}^{M}p_{i}^{w_{i}}(\\cdot|x)\\right)}\\\\ &{=\\log Z^{\\star}(x)-\\log Z_{w}(x)+\\displaystyle\\frac{1}{Z_{w}(x)}\\cdot\\sum_{y\\in\\mathcal{Y}}\\left(\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(y|x)\\displaystyle\\sum_{j=1}^{M}w_{j}\\log\\frac{\\pi_{j}(y|x)}{p_{j}(y|x)}\\right)}\\\\ &{\\le\\log Z^{\\star}(x)-\\log Z_{w}(x)+\\displaystyle\\frac{1}{Z_{w}(x)}\\cdot\\left(\\sum_{i=1}^{M}w_{i}^{2}\\mathrm{KL}\\left(\\pi_{i}\\|p_{i}\\right)+\\displaystyle\\sum_{i\\neq j}w_{i}w_{j}\\left(\\mathrm{KL}\\left(\\pi_{i}\\|p_{j}\\right)-\\mathrm{KL}\\left(\\pi_{i}\\|\\pi_{j}\\right)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u22642 exp(C) \u00b7 L . ", "page_idx": 25}, {"type": "text", "text": "Finally we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\star}-V=\\underset{x\\sim\\mathcal{X}}{\\mathbb{E}}\\mathrm{KL}\\left(\\frac{1}{Z_{w}(x)}\\prod_{i=1}^{M}\\pi_{i}^{w_{i}}(\\cdot|x)\\|\\frac{1}{Z^{\\star}(x)}\\prod_{i=1}^{M}p_{i}^{w_{i}}(\\cdot|x)\\right)}\\\\ &{\\qquad\\qquad\\le2\\exp(C)\\cdot\\mathcal{L}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 3 (Theorem 2 in [47]). Suppose $\\pi_{1}(\\cdot|x)$ and $\\pi_{2}(\\cdot|x)$ be two policies, then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{ECE}(\\pi_{1})-\\mathrm{ECE}(\\pi_{2})\\leq\\underset{x\\sim\\mathcal{X}}{\\mathbb{E}}\\left[2\\sqrt{2\\operatorname{KL}\\left(\\pi_{1}(\\cdot|x)\\Vert\\pi_{2}(\\cdot|x)\\right)}\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proposition 4 (Calibration error perspective). The expected calibration error (see Definition 3) of $\\pi_{w}$ can be bounded as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{ECE}(\\pi_{w})\\leq\\operatorname{ECE}(\\pi_{\\mathrm{opt}})+4{\\sqrt{\\exp(C)\\cdot{\\mathcal{L}}}}~.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. This proposition directly comes from combining Lemma 3 with Theorem 4. ", "page_idx": 25}, {"type": "text", "text": "D.5 Proofs of subsection 5.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 2. For any $f$ -divergence satisfying one of the following conditions: $(i)~f$ is not a barrier function; $(i i)\\;I_{f}$ is Reverse $K L$ -divergence; (iii) $f$ is a strong-barrier function, with finite roots of ", "page_idx": 25}, {"type": "equation", "text": "$$\n2\\nabla f\\left(\\frac{3\\sqrt{1-2x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-2\\nabla f\\left(\\frac{3\\sqrt{x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-\\nabla f(3-6x)+\\nabla f(3x)\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\exists N,M\\in\\mathbb{N},\\,\\mathcal{V}=\\{y_{i}\\}_{i=1}^{N},\\,\\beta\\in\\mathbb{R}_{+},$ , a neural network $n n=\\mathrm{softmax}(h_{\\theta}(z_{0}))$ where $z_{0}\\in\\mathbb{R}^{n}$ and $h_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{N}$ is a continuous mapping, preference weightings $w\\in\\Delta^{M-1}$ , reference policy $\\pi_{\\mathrm{ref}}$ and the objectives $J_{1},J_{2},\\dots,J_{M}$ representing reward functions $\\mathcal{R}_{1},\\mathcal{R}_{2},\\ldots,\\mathcal{R}_{M}\\,w.r.t$ . $\\beta{\\cdot}I_{f}({\\cdot}\\|\\pi_{\\mathrm{ref}})$ , s.t. Hypothesis $I$ does not hold. ", "page_idx": 25}, {"type": "text", "text": "Proof. (i) If $f$ is not a barrier function, Hypothesis 1 does not hold immediately from Theorem 3. ", "page_idx": 25}, {"type": "text", "text": "(ii) If $I_{f}$ is Reverse KL-divergence, we let $N=3$ , $M=3$ , and $h_{\\theta}(z_{0})=W_{\\theta}^{(2)}\\sigma\\left(W_{\\theta}^{(1)}z_{0}\\right)$ , where $\\sigma$ is ReLU $(\\cdot)$ . We set $\\mathcal{R}_{i}(y_{j})=\\delta_{i j}$ , $\\pi_{\\mathrm{ref}}(y_{i})=1/3$ for $\\forall i,j\\in$ [3], $z_{0}=1$ and $\\beta=1$ . Then the optimal policies are $W_{\\theta_{1}}^{(1)}=e_{1}$ = e1, W $W_{\\theta_{1}}^{(2)}={\\binom{100}{000}}$ 0 for R1 w.r.t. KL (\u00b7\u2225\u03c0ref), W \u03b8(21) $\\mathrm{KL}\\left(\\cdot\\|\\pi_{\\mathrm{ref}}\\right)\\!,W_{\\theta_{2}}^{(1)}=e_{2},W_{\\theta_{2}}^{(2)}=\\binom{000}{000}$ for R2 w.r.t. KL (\u00b7\u2225\u03c0ref), and W \u03b8(31) $W_{\\theta_{3}}^{(1)}=e_{3}$ = e3, W \u03b8(2) $W_{\\theta_{3}}^{(2)}={\\binom{000}{000}}$ for ${\\mathcal{R}}_{3}$ w.r.t. $\\mathrm{KL}\\left(\\cdot\\Vert\\pi_{\\mathrm{ref}}\\right)$ . Thus we have $h_{\\sum_{j=1}^{3}\\lambda_{j}\\theta_{j}}(z_{0})\\,=\\,\\bigl(\\lambda_{1}^{2},\\lambda_{2}^{2},\\lambda_{3}^{2}\\bigr)^{\\top}$ . Given $w\\,=\\,(0,1/3,2/3)$ , the optimal policy $\\pi^{\\star}$ should output $\\begin{array}{r}{\\pi^{\\star}(y_{1})\\,=\\,\\frac{1}{1+\\exp(1/3)+\\exp(2/3)}}\\end{array}$ , $\\begin{array}{r}{\\pi^{\\star}(y_{2})\\,=\\,\\frac{\\exp(1/3)}{1+\\exp(1/3)+\\exp(2/3)}}\\end{array}$ and $\\begin{array}{r}{\\pi^{\\star}(y_{3})\\,=\\,\\frac{\\exp(2/3)}{1+\\exp(1/3)+\\exp(2/3)}}\\end{array}$ Note that ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sqrt{t}+\\sqrt{t+1/3}+\\sqrt{t+2/3}>1\\ ,\\ \\forall t\\in{\\mathbb R}_{+}\\ ,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "thus there is no solution $\\lambda\\in\\Delta^{2},t\\in\\mathbb{R}_{+}$ for $\\left(\\lambda_{1}^{2},\\lambda_{2}^{2},\\lambda_{3}^{2}\\right)^{\\top}=\\left(t,t+\\frac{1}{3},t+\\frac{2}{3}\\right)^{\\top}$ , i.e. there is no $\\lambda$ s.t. softmax h j3=1 \u03bbj\u03b8j(z0) = \u03c0\u22c6(y1), \u03c0\u22c6(y2), \u03c0\u22c6(y3) , i.e. Hypothesis 1 does not hold. (iii) If $f$ is a strong-barrier function, with finite roots of ", "page_idx": 26}, {"type": "equation", "text": "$$\n2\\nabla f\\left(\\frac{3\\sqrt{1-2x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-2\\nabla f\\left(\\frac{3\\sqrt{x}}{2\\sqrt{1-2x}+\\sqrt{x}}\\right)-\\nabla f(3-6x)+\\nabla f(3x)\\;,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we let $N=3$ , $M=2$ $=2,h_{\\theta}(z_{0})=W_{\\theta}(z_{0}),z_{0}=1,\\mathcal{R}_{1}(y_{i})=\\delta_{1i},\\mathcal{R}_{2}(y_{i})=\\delta_{2i}$ and $\\pi_{\\mathrm{ref}}(y_{i})=1/3$ , for $\\forall i\\in$ [3]. From Eq. (3) the optimal policy for $J_{1}$ is $\\begin{array}{r}{\\pi_{\\theta_{1}}(y_{i})=\\frac{1}{3}(\\nabla f)^{(-1)}\\left(\\frac{1}{\\beta}\\delta_{1i}-Z\\right)}\\end{array}$ , and the optimal policy for $J_{2}$ is $\\begin{array}{r}{\\pi_{\\theta_{2}}(y_{i})\\,=\\,\\frac13(\\nabla f)^{(-1)}\\left(\\frac1\\beta\\delta_{2i}-Z\\right)}\\end{array}$ , where $Z$ is the normalization factor. And these policies can be learned by setting $\\dot{W_{\\theta_{i}}}=\\left(\\log\\tau_{\\theta_{i}}(y_{1}),\\log\\pi_{\\theta_{i}}(y_{2}),\\log\\pi_{\\theta_{i}}(y_{3})\\right)^{\\top}$ . We set $\\begin{array}{r}{a:=\\pi_{\\theta_{1}}(y_{1})=\\frac{1}{3}(\\nabla f)^{(-1)}(\\frac{1}{\\beta}-Z),b:=\\pi_{\\theta_{1}}(y_{2})=\\pi_{\\theta_{1}}(y_{3})=\\frac{1}{3}(\\nabla f)^{(-1)}(-Z)}\\end{array}$ . Thus we have ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla f(3a)-\\nabla f(3b)=\\frac{1}{\\beta}\\;,}\\\\ {a+2b=1\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Proposition 1, the optimal policy for $w_{1}\\cdot J_{1}+w_{2}\\cdot J_{2}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pi_{w}^{\\star}(y_{i})=\\frac{1}{3}(\\nabla f)^{(-1)}\\left(-Z_{w}^{\\star}+\\frac{w_{1}}{\\beta}\\delta_{1i}+\\frac{w_{2}}{\\beta}\\delta_{2i}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $Z_{w}^{\\star}$ is the normalization factor. By linearly merging the weights of $\\pi_{\\theta_{1}}$ and $\\pi_{\\theta_{2}}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\lambda_{1}\\theta_{1}+\\lambda_{2}\\theta_{2}}(y_{i})=\\mathrm{softmax}\\left(\\lambda_{1}W_{\\theta_{1}}(z_{0})+\\lambda_{2}W_{\\theta_{2}}(z_{0})\\right)(y_{i})}\\\\ &{\\phantom{\\pi_{\\lambda_{1}\\theta_{1}+\\lambda_{2}\\theta_{2}}}=\\frac{1}{Z_{\\lambda}}\\left((\\nabla f)^{(-1)}\\left(\\frac1{\\beta}\\delta_{1i}-Z\\right)\\right)^{\\lambda_{1}}\\left((\\nabla f)^{(-1)}\\left(\\frac1{\\beta}\\delta_{2i}-Z\\right)\\right)^{\\lambda_{2}}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $Z_{\\lambda}$ is the normalization factor. ", "page_idx": 26}, {"type": "text", "text": "With symmetry, Eq. (24), (25) and Hypothesis 1 indicate that \u03c0 21 \u03b81+ 21 \u03b82 = \u03c0(\u22c6 1, 1), thu ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{1}{3}(\\nabla f)^{(-1)}\\left(-Z_{(0.5,0.5)}^{\\star}+\\frac{1}{2\\beta}\\right)=\\frac{\\sqrt{a}}{2\\sqrt{a}+\\sqrt{b}}\\;,}\\\\ &{}&{\\frac{1}{3}(\\nabla f)^{(-1)}\\left(-Z_{(0.5,0.5)}^{\\star}\\right)=\\frac{\\sqrt{b}}{2\\sqrt{a}+\\sqrt{b}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and combining them with Eq. (22) yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n2\\nabla f\\left(\\frac{3\\sqrt{a}}{2\\sqrt{a}+\\sqrt{b}}\\right)-2\\nabla f\\left(\\frac{3\\sqrt{b}}{2\\sqrt{a}+\\sqrt{b}}\\right)=\\nabla f(3a)-\\nabla f(3b)\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Given the condition, the solution set $(a,b)$ to Eq. (23), (26) is finite, thus there exists $\\beta\\in\\mathbb{R}_{+}$ s.t. Eq. (22) does not hold, implying that Hypothesis 1 does not hold. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Codebase. Our codebase is mainly based on trl [46] (https://github.com/ huggingface/trl), MODPO [62] (https://github.com/ZHZisZZ/modpo), RiC [56] (https://github.com/YangRui2015/RiC) and Finegrained RLHF [52] (https: //github.com/allenai/FineGrainedRLHF), and has referred to f-divergence DPO [47] (https://github.com/alecwangcq/f-divergence-dpo), PackLLM [32] (https: //github.com/cmavro/PackLLM), and DPA [48] (https://github.com/Haoxiang-Wang/ directional-preference-alignment). We release the code at https://github.com/srzer. ", "page_idx": 27}, {"type": "text", "text": "Datasets. For Reddit Summary, we adopt the Summarize-from-Feedback dataset (https:// huggingface.co/datasets/openai/summarize_from_feedback); For Helpful Assistant, we adopt the Anthropics-HH dataset (https://huggingface.co/datasets/Anthropic/hh-rlhf); For Safety Alignment, we adopt a $\\mathrm{10-k}$ subset (https://huggingface.co/datasets/ PKU-Alignment/PKU-SafeRLHF-10K); For Helpsteer, we adopt the Helpsteer dataset (https: //huggingface.co/datasets/nvidia/HelpSteer). ", "page_idx": 27}, {"type": "text", "text": "SFT. For Reddit Summary and Helpful Assistant, we supervisedly fine-tune the LLAMA2-7B models on the Summarize-from-Feedback dataset, following the practice of [46, 56]; For Safety Alignment, we directly deploy a reproduced model (https://huggingface.co/PKU-Alignment/ alpaca-7b-reproduced); For HelpSteer, we supervisedly fine-tune a MISTRAL-7B model on the HelpSteer dataset, following the practice of [62]. ", "page_idx": 27}, {"type": "text", "text": "Reward models. We deploy off-shelf reward models for RLHF (PPO) training and evaluations. For Reddit Summary, we use https://huggingface.co/Tristan/ gpt2_reward_summarization for summary and https://huggingface.co/CogComp/ bart-faithful-summary-detector for faith; For Helpful Assistant, we use https: //huggingface.co/Ray2333/gpt2-large-helpful-reward_model for helpfulness, https://huggingface.co/Ray2333/gpt2-large-harmless-reward_model for harmlessness and https://huggingface.co/mohameddhiab/humor-no-humor for humor; For Safety Alignment, we use https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward for helpfulness and https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-cost for harmlessness; For HelpSteer, we use https://huggingface.co/Haoxiang-Wang/ RewardModel-Mistral-7B-for-DPA-v1 for all attributes of rewards, including helpfulness, correctness, coherence, complexity and verbosity. ", "page_idx": 27}, {"type": "text", "text": "Training hyper-parameters. For PPO, we follow the settings of [56] and train for 100 batches; for DPO, we follow [62] with minimal modifications as BATCH_ $\\mathtt{S I Z E=1}$ and MAX_LENGTH $=256$ . ", "page_idx": 27}, {"type": "text", "text": "Inference hyper-parameters. For PPO, we follow the settings of [56] with NUM_BEAMS $=1$ ; for DPO, we follow [62] with BATCH_SIZE $=4$ , MAX_LENGTH $=200$ and NUM_BEAMS $=1$ . ", "page_idx": 27}, {"type": "text", "text": "Inference code. Here we provide the inference code. Notably, to prevent potential precision explosion, we approximate the solution for JSD same as Reverse KL-divergence, as they are inherently similar. ", "page_idx": 27}, {"type": "text", "text": "if f_type $==$ \"reverse_kld\" or f_type $==$ \"jsd\": return torch.sum(torch.stack ([ weights[idx]\\* logp[idx] for idx in range(n)]), $\\mathtt{d i m}\\!=\\!0$ )   \nelif f_type $==$ \"forward_kld\": lst $=$ [] for idx in range(n): if weights[idx] != 0: lst.append(-logp[idx]+np.log(weights[idx])) return -torch.logsumexp(torch.stack(lst), dim $=\\!0$ )   \nelif \"-divergence\" in f_type: parts $=$ f_type.split(\"-\") alpha $=$ float(parts [0]) if parts else None lst $=$ [] for idx in range(n): if weights[idx] != 0: lst.append(-logp[idx]\\* alpha+np.log(weights[idx])) return -torch.logsumexp(torch.stack(lst), dim $=\\!0$ ) ", "page_idx": 27}, {"type": "text", "text": "Evaluation setups. The evaluation scores are calculated on a down-sampled dataset, by off-shelf reward models. For Reddit Summary and Helpfull Assistant, we uniformly sample a subset of $2\\mathbf{k}$ prompts from the test set, following [56]; for Safety Alignment and HelpSteer, we randomly sample of subset of 200 prompts from the validation set. The generation configurations are set as identical for all algorithms. ", "page_idx": 28}, {"type": "text", "text": "Compute resources. Our main experiments are conducted on NVIDIA RTX A6000. For training RLHF, MORLHF models, the number of workers are set as 3, each taking up 20, 000M of memory, running for 18 hours; for training DPO, MODPO models, the number of workers are set as 2, each taking up 40, 000M of memory, running for 3 hours. ", "page_idx": 28}, {"type": "text", "text": "F Supplementary Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide additional experimental results for supplementation. ", "page_idx": 28}, {"type": "text", "text": "F.1 Motivating example ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This motivating experiment is based on FineGrainedRLHF [52]. We tune two T5-LARGE models $\\mathcal{M}_{1},\\mathcal{M}_{2}$ for relevance and factuality respectively, based on a reproduced SFT model and pretrained reward models, following the instructions of [52]. And we obtain $\\mathcal{M}_{2}$ via reversing the sign of $Q,K$ matrices of the last two layers of $\\mathcal{M}_{1}$ . The preference weightings are set as $w\\in$ $\\{(\\bar{i}/10,1-i/10):i\\in\\{0,1,\\ldots,10\\}\\}$ . As Figure 5 shows, though the performance is comparable based on normally trained models, a noticeable lag in the performance of RS emerges after a simple reversal of certain parameters. ", "page_idx": 28}, {"type": "image", "img_path": "3csuL7TVpV/tmp/cefc60adf09a7a64e4ba55b9ad8afc8cae129127c18902a0e90de7071d3b2d02.jpg", "img_caption": ["Figure 5: Finegrained RLHF. The left figure illustrates the performance of MOD and RS on $\\mathcal{M}_{1},\\mathcal{M}_{2}$ , and the right one illustrates the performance on $\\mathcal{M}_{1}^{\\star},\\mathcal{M}_{2}$ , where $\\mathcal{M}_{1}^{\\star}$ is obtained via reversing the sign of $Q,K$ matrices of the last two layers of $\\mathcal{M}_{1}$ . "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.2 Additional results for Helpful Assistant ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For 3-reward setting in Helpful Assistant task, we provide the 3d-visualization and numerical results of MOD and RS for many configurations of preference weightings in Figure 6, Table 6, showing that MOD generally beats $R S$ . ", "page_idx": 28}, {"type": "text", "text": "F.3 Additional results for BeaverTails ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For MOD, the effect of harmfulness can be obtained from a harmless model by setting the preference weighting as a negative value. In contrast, RS fails to generate meaningful responses under this setting. Example generations are provided in Table 7. ", "page_idx": 28}, {"type": "image", "img_path": "3csuL7TVpV/tmp/8f8583a06be17c487f35f73d2965b8436ce54f356f65544345280c368ae9d3f5.jpg", "img_caption": ["Figure 6: 3D visualization of Pareto frontiers on Helpful Assistant task. In general, MOD lies over RS. preference weightings are set as $w\\in\\{(0.0,0.0,1.0)$ ), (0.0, 1.0, 0.0), (0.1, 0.1, 0.8), (0.1, 0.8, 0.1), (0.2, 0.2, 0.6), (0.2, 0.4, 0.4), (0.2, 0.6, 0.2), (0.33, 0.33, 0.33), (0.4, 0.4, 0.2), (0.4, 0.2, 0.4), (0.6, 0.2, 0.2), (0.8, 0.1, 0.1), (1.0, 0.0, 0.0)}. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "3csuL7TVpV/tmp/a477427c4c9572bc6e982fbf83edf7a310ff215c448d4056e4be853ec8e3fb5b.jpg", "table_caption": ["Table 6: Results on 3-objective Helpful Assistant. We present $w$ -weighted score as $w_{1}$ \u00b7Helpfulness $^+$ $w_{2}$ \u00b7 Harmlessness $+\\;w_{3}\\;\\cdot$ Humor. Compared to parameter-merging baseline, our algorithm achieves $12.8\\%$ overall improvement when equally optimizing towards 3 objectives. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "3csuL7TVpV/tmp/bdbf6fafc367837faa88f89831813bddfaba3704d14287e1443d22ddfed9a9cf.jpg", "table_caption": ["Table 7: Examples of Safety Alignment. The example generations of MOD and RS when $w_{2}<0$ . The latter fails to generate meaningful responses when $w_{2}\\leq-2$ . "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "F.4 Additional results for HelpSteer ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "By supervisedly fine-tuning a MISTRAL-7B model on HelpSteer dataset, we obtain the reference policy $\\pi_{\\mathrm{ref}}$ . And then we tune models $\\pi_{1f},\\pi_{2f},\\pi_{3f}$ using $f$ -DPO on three pair-comparison datasets for helpfulness, complexity and verbosity. Specifically, we early-stop (3 epochs) the tuning process, to examine the performance when base policies are sub-optimal. For $f$ -DPO models trained w.r.t. Reverse KL-divergence, JSD, 0.3-divergence and 0.5-divergence, we present the score for each attribute of MOD and RS, with weightings set as $w=(0.33,0.33,0.33)$ , as shown in Table 8, 9, 10, 11. It can be observed that MOD still successfully combines their advantages and generally achieves stronger performance than RS. ", "page_idx": 31}, {"type": "table", "img_path": "3csuL7TVpV/tmp/e32415f8ce0c45a9d2380df3373a49ea5c35ecdc1c1a45b0b0a3cac46ef82d9f.jpg", "table_caption": ["Table 8: Results on HelpSteer. $f$ -DPO w.r.t. Reverse KL-divergence. Preference weightings set as $w=(0.33,0.33,0.33)$ . Top-2 scores are highlighted. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "3csuL7TVpV/tmp/d41ca25fbb0ed494f9e44ebc98f4c7df935ebdeeba0f86c5ffd0dec81dc6cca0.jpg", "table_caption": ["Table 9: Results on HelpSteer. $f$ -DPO w.r.t. JSD. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "3csuL7TVpV/tmp/0531a2c6dee3adfee0e7d42cda4f68d080aec0427def5563b261175e3ac92dc5.jpg", "table_caption": ["Table 10: Results on HelpSteer. $f$ -DPO w.r.t. 0.3-divergence. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "3csuL7TVpV/tmp/9b09df7306450f5bd26814644c73b48038145a614dc0f865ef203c519cf09eaf.jpg", "table_caption": ["Table 11: Results on HelpSteer. $f$ -DPO w.r.t. 0.5-divergence. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.5 Additional results for Open Instruction-Following ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Additional numerical results of combining 2 T\u00dcLU models are provided in Table 12. ", "page_idx": 32}, {"type": "text", "text": "Table 12: Results of MOD combining T\u00dcLU-2-HH-13B and CODET\u00dcLU-2-7B, achieving precise control over general capabilities, including safety (Toxigen), coding (Codex) and reasoning $(*\\,\\mathrm{COT})$ . ", "page_idx": 32}, {"type": "table", "img_path": "3csuL7TVpV/tmp/29e0e1ac5bdedee977a7f6ad267da418c0a3d4f1a0bb92e6082b432b7db96a8a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.6 Example generations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Example generations for each dataset are shown in Table 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23. For each dataset, we show a representative prompt in the down-sampled dataset, and one generated response for each model/algorithm, with preference weightings set as $w=(0.5,0.5)$ for MOD and RS. ", "page_idx": 33}, {"type": "table", "img_path": "3csuL7TVpV/tmp/ca90479f198fa63b0ec0de33ebd80b02f9a20dfd965cd51e9d6c83c19cf6f279.jpg", "table_caption": ["Table 13: Examples of Reddit Summary. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "3csuL7TVpV/tmp/0ea0941b7b42968cde741b2e367365a77cb072913d321d816295430628b923b9.jpg", "table_caption": ["Table 14: Examples of Helpful Assistants. Helfulness & Humor. "], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "3csuL7TVpV/tmp/1224b0eac0c8f5c51914e1a632a9899f4e0f610707f24edf2d21e7f458baf51f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "3csuL7TVpV/tmp/b899ed07d2829dcfb39bc6253e04e3c3c97f56cc1478653ca8d8f2bc7b3f112c.jpg", "table_caption": ["Table 16: Examples of Safety Alignment. $f$ -DPO w.r.t. KL-divergence. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "3csuL7TVpV/tmp/decd0217c4c03ca92daacd83dad3b82efc5dda0312ecf2f1ecea15ad3fb9755e.jpg", "table_caption": ["Table 17: Examples of Safety Alignment. $f$ -DPO w.r.t. JSD. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "3csuL7TVpV/tmp/8ad2b00b8d20557f61eb38d0730c2a6714fe8a964e975f40faaf296e0358cdae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "3csuL7TVpV/tmp/eabbc33b01bce312f28f40e9a156f90fdfd202ddae3e674f42e7c311245c5d31.jpg", "table_caption": ["Table 19: Examples of Safety Alignment. $f$ -DPO w.r.t. 0.5-divergence. "], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "3csuL7TVpV/tmp/6259f3a3b6105f7bcac3924940ec1d01cc4b6205f6db436ff5c377993b16f228.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "3csuL7TVpV/tmp/10a3fbfeae798c6d08f0bac079ea533220e8c113db529aefbe2ab4009caddf78.jpg", "table_caption": ["Table 21: Examples of HelpSteer. $f$ -DPO w.r.t. JSD. "], "table_footnote": [], "page_idx": 36}, {"type": "table", "img_path": "3csuL7TVpV/tmp/1d8e84bff1ef5956ae3c4a86e71580e809031ca10b7981f509274ff249db0bbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "3csuL7TVpV/tmp/5b2fca633cd2a818758258440936d768065cdd3249db4d83a0ff4a5f3767fd56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 39}, {"type": "text", "text": "Justification: Both the abstract and introduction reflect our paper\u2019s contributions (as listed in section 1) and scope. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We discuss the limitations of this work in section 7. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Proofs are provided in Appendix D. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide implementation details in Appendix E, including datasets, models, hyper-parameters, etc. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The dataset is accessible, and we have provided the urls in Appendix E. We release the code at https://github.com/srzer/MOD. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We sincerely follow the training details of [56, 62], and state this point in Appendix E. The test details are specified in Appendix E as well. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Since our work focus on inference of language models, the results are provided as evaluation scores on validation/test set. And we have explained how they are calculated in Appendix E. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide details of computing resources like compute device, number of workers, memory and time of execution in Appendix E. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics, and strictly followed it in this work. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Yes, we talk about boarder impacts in Appendix A. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper focus on inference of language models, and thus there is no data or models to release. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Yes. We acknowledge the codebases we have referred to, cite and provide the urls of datasets and models in Appendix E. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}]