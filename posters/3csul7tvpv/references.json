{"references": [{"fullname_first_author": "S.M. Ali", "paper_title": "A general class of coefficients of divergence of one distribution from another", "publication_date": "1966-01-01", "reason": "This paper introduces a general class of f-divergences, which is the theoretical foundation of the proposed MOD algorithm."}, {"fullname_first_author": "Paul F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-12-04", "reason": "This paper introduces deep reinforcement learning from human preferences, which is the basis of many existing single-objective alignment methods, and the MOD algorithm builds upon this foundation."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-10", "reason": "This paper proposes direct preference optimization (DPO), which is a more efficient single-objective alignment approach than RLHF, and MOD is directly inspired by this work."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-07-23", "reason": "This paper analyzes the phenomenon of overoptimization in reward model-based alignment, which is a key problem addressed by MOD's training-free approach."}, {"fullname_first_author": "Chaoqi Wang", "paper_title": "Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints", "publication_date": "2024-01-01", "reason": "This paper generalizes existing single-objective alignment approaches using f-divergence regularization, which provides the theoretical basis for MOD's optimality guarantees."}]}