[{"figure_path": "mCWZj7pa0M/figures/figures_8_1.jpg", "caption": "Figure 1: Test loss and c estimate across four sample sizes and for two levels of noise \u03c3. On the left: MAE for the three first average spike times on a hold out test set. On the right: estimated value of c at the current step.", "description": "This figure shows the results of an experiment to estimate the input current (c) of a single stochastic leaky integrate-and-fire (SLIF) neuron.  The left panel displays the mean absolute error (MAE) for the first three average spike times on a hold-out test set, and the right panel shows the estimated value of c at each step during stochastic gradient descent.  Multiple lines represent different sample sizes (16, 32, 64, 128) and both panels show results for two different noise levels (\u03c3 = 0.25 and \u03c3 = 0.5).", "section": "4.2 Input current estimation"}, {"figure_path": "mCWZj7pa0M/figures/figures_26_1.jpg", "caption": "Figure 2: We estimate the synaptic weights w across three different sample sizes using the signature kernel MMD truncated at depth 3 and stochastic gradient descent with a batch size of 128. On the left we report the loss on a hold out test set. On the right is the mean absolute error between the entries of the currently estimated weight matrix \u0175step and the true weight matrix Wtrue.", "description": "This figure shows the results of estimating synaptic weights in a feed-forward spiking neural network (SSNN).  The experiment uses a signature kernel maximum mean discrepancy (MMD) as a loss function and stochastic gradient descent with a batch size of 128. Three different sample sizes (256, 512, and 1024) were tested. The left panel displays the test loss (mean absolute error) on a held-out test set over 1500 training steps. The right panel shows the mean absolute error between the estimated and true weight matrices during training.  The results illustrate the impact of sample size on learning performance. ", "section": "4.3 Synaptic weight estimation"}]