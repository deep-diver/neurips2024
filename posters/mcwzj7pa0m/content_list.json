[{"type": "text", "text": "Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Christian Holberg Department of Mathematics University of Copenhagen c.holberg@math.ku.dk ", "page_idx": 0}, {"type": "text", "text": "Cristopher Salvi Department of Mathematics Imperial College London c.salvi@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a mathematically rigorous framework based on rough path theory to model stochastic spiking neural networks (SSNNs) as stochastic differential equations with event discontinuities (Event SDEs) and driven by c\u00e0dl\u00e0g rough paths. Our formalism is general enough to allow for potential jumps to be present both in the solution trajectories as well as in the driving noise. We then identify a set of sufficient conditions ensuring the existence of pathwise gradients of solution trajectories and event times with respect to the network\u2019s parameters and show how these gradients satisfy a recursive relation. Furthermore, we introduce a general-purpose loss function defined by means of a new class of signature kernels indexed on c\u00e0dl\u00e0g rough paths and use it to train SSNNs as generative models. We provide an end-to-end autodifferentiable solver for Event SDEs and make its implementation available as part of the diffrax library. Our framework is, to our knowledge, the first enabling gradient-based training of SSNNs with noise affecting both the spike timing and the network\u2019s dynamics. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic differential equations exhibiting event discontinuities (Event SDEs) and driven by noise processes with jumps are an important modelling tool in many areas of science. One of the most notable examples of such systems is that of stochastic spiking neural networks (SSNNs). Several models for neuronal dynamics have been proposed in the computational neuroscience literature with the stochastic leaky integrate-and-fire (SLIF) model being among the most popular choices [19, 56]. In its simplest form, given some continuous input current $i_{t}$ on $[0,T]$ , the dynamics of a single SLIF neuron consist of an Ornstein-Uhlenbeck process describing the membrane potential as well as a threshold for spike triggering and a resetting mechanism [33]. In particular, between spikes, the dynamics of the membrane potential $v_{t}$ is given by the following SDE ", "page_idx": 0}, {"type": "equation", "text": "$$\nd v_{t}=\\mu\\left(i_{t}-v_{t}\\right)d t+\\sigma d B_{t},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mu>0$ is a parameter and $B_{t}$ is a standard Brownian motion. The neuron spikes whenever the membrane potential $v$ hits the threshold $\\psi>0$ upon which $v$ is reset to 0. Alternatively, one can model the spike times as a Poisson process with intensity $\\lambda:\\mathbb{R}\\to\\mathbb{R}_{+}$ depending on the membrane potential $v_{t}$ . A common choice is $\\bar{\\lambda(\\boldsymbol{v})}=\\exp((\\boldsymbol{v}-\\boldsymbol{\\psi})/\\beta)$ [50, 26, 27, 24]. ", "page_idx": 0}, {"type": "text", "text": "A notorious issue for calibrating Event SDEs such as SSNNs is that the implicitly defined event discontinuities, e.g., the spikes, make it difficult to define derivatives of the solution trajectories and of the event times with respect to the network\u2019s parameters using classical calculus rules. This issue is exacerbated when the dynamics are stochastic in which case the usual argument relying on the implicit function theorem, used for instance in [6, 25], is no longer valid. ", "page_idx": 0}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a mathematically rigorous framework to model SSNNs as SDEs with event discontinuities and driven by c\u00e0dl\u00e0g rough paths, without any prior knowledge of the timing of events. The mathematical formalism we adopt is that of rough path theory [38], a modern branch of stochastic analysis providing a robust solution theory for stochastic dynamical systems driven by noisy, possibly discontinuous, rough signals. Although Brownian motion is a prototypical example, these signals can be far more irregular (or rougher) than semimartingales [17, 16, 39]. ", "page_idx": 1}, {"type": "text", "text": "Equipped with this formalism, we proceed to identify sufficient conditions under which the solution trajectories and the event times are differentiable with respect to the network\u2019s parameters and obtain a recursive relation for the exact pathwise gradients in Theorem 3.2. This is a strict generalization of the results presented in [6] and [25] which only deal with ordinary differential equations (ODEs). Furthermore, we define Marcus signature kernels as extensions of continuous signature kernels [52] to c\u00e0dl\u00e0g rough paths and show their characteristicness. We then make use of this class of kernels indexed on discontinuous trajectories to define a general-purpose loss function enabling the training of SSNNs as generative models. We provide an end-to-end autodifferentiable solver for Event SDEs (Algorithm 1) and make its implementation available as part of the diffrax library [28]. ", "page_idx": 1}, {"type": "text", "text": "Our framework is, to our knowledge, the first allowing for gradient-based training of a large class of SSNNs where a noise process can be present in both the spike timing and the network\u2019s dynamics. In addition, we believe this work is the first enabling the computation of exact gradients for classical SNNs whose solutions are approximated via a numerical solver (not necessarily based on a Euler scheme). In fact, previous solutions are based either on surrogate gradients [46] or follow an optimisethen-discretise approach deriving adjoint equations [56], the latter yielding exact gradients only in the scenario where solutions are available in closed form and not approximated via a numerical solver.1 Finally, we discuss how our results lead to bioplausible learning algorithms akin to $e$ -prop [2]. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural stochastic differential equations (NSDEs) The intersection between differential equations and deep learning has become a topic of great interest in recent years. A neural ordinary differential equation (NODE) is an ODE of the form $\\mathrm{d}y_{t}\\,=\\,f_{\\theta}(y_{t})d t$ started at $y_{0}\\,\\in\\,\\mathbb{R}^{e}$ using a parametric Lipschitz vector field $f_{\\theta}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e}$ , usually given by a neural network [5]. Similarly, a neural stochastic differential equation (NSDE) is an SDE of the form $\\mathrm{d}y_{t}=\\mu_{\\theta}(y_{t})d t+\\sigma_{\\theta}(y_{t})d B_{t}$ driven by a $d$ -dimensional Brownian motion $B$ , started at $y_{0}~\\in~\\mathbb{R}^{e}$ , and with parametric vector field $\\mu_{\\theta}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e}$ and $\\sigma_{\\theta}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e\\times d}$ that are $\\mathrm{Lip^{1}}$ and $\\mathrm{Lip}^{2+\\epsilon}$ continuous respectively2. Rough path theory offers a way of treating ODEs, SDEs, and more generally differential equations driven by signals or arbitrary (ir)regularity, under the unified framework of rough differential equations (RDEs) [44, 22]. For an account on applications of rough path theory to machine learning see [4, 14, 51]. ", "page_idx": 1}, {"type": "text", "text": "Training techniques for NSDEs Training a NSDE amounts to minimising over model parameters an appropriate notion of statistical divergence between a distribution of continuous trajectories generated by the NSDE and an empirical distribution of observed sample paths. Several approaches have been proposed in the literature, differing mostly in the choice of discriminating divergence. SDEGANs, introduced in [29], use the 1-Wasserstein distance to train a NSDE as a Wasserstein-GAN [1]. Latent SDEs [36] train a NSDE with respect to the KL divergence via variational inference and can be interpreted as variational autoencoders. In [23] the authors propose to train NSDEs non-adversarially using a class of maximum mean discrepancies (MMD) endowed with signature kernels [30, 52]. Signature kernels are a class of characteristic kernels indexed on continuous paths that have received increased attention in recent years thanks to their efficiency for handling path-dependent problems [35, 54, 10, 53, 9, 48, 41]. For a treatment of this topic we refer the interested reader to [4, Chapter 2]. These kernels are not applicable to sample trajectories of SSNNs because of the lack of continuity. ", "page_idx": 1}, {"type": "text", "text": "Backpropagation through NSDEs Once a choice of discriminator has been made, training NSDEs amounts to perform backpropagation through the SDE solver. There are several ways to do this. The first option is simply to backpropagate through the solver\u2019s internal operations. This method is known as discretise-then-optimise; it is generally speaking fast to evaluate and produces accurate gradients, but it is memory-inefficient, as every internal operation of the solver must be recorded. A second approach, known as optimise-then-discretise, computes gradients by deriving a backwards-in-time differential equation, the adjoint equation, which is then solved numerically by another call to the solver. Not storing intermediate quantities during the forward pass enables model training at a memory cost that is constant in depth. Nonetheless, this approach produces less accurate gradients and is usually slower to evaluate because it requires recalculating the forward solutions to perform the backward pass. A third way of backpropagating through NDEs is given by algebraically reversible solvers, offering both memory and accuracy efficiency. We refer to [28] for further details. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Differential equations with events Many systems are not adequately modelled by continuous differential equations because they experience jump discontinuities triggered by the internal state of the system. Examples include a bouncing ball or spiking neurons. Such systems are often referred to as (stochastic) hybrid systems [21, 37]. When the differential equation is an ODE, there is a rich literature on sensitivity analysis aimed at computing derivatives using the implicit function theorem [11, 12]. If, additionally, the vector fields describing the hybrid system are neural networks, [6] show that NODEs solved up until first event time can be implemented as autodifferentiable blocks and [25] derive the corresponding adjoint equations. Nonetheless, none of these works cover the more general setting of SDEs. The only work, we are familiar with, dealing with sensitivity analysis in this setting is [47], although focused on the problem of optimal control. ", "page_idx": 2}, {"type": "text", "text": "Training techniques for SNNs Roughly speaking, these works can be divided into two strands. The first, usually referred to as backpropagation through time (BPTT), starts with a Euler approximation of the SNN and does backpropagation by unrolling the computational graph over time; it then uses surrogate gradients as smooth approximations of the gradients of the non-differentiable terms. [58, 46, 40]. This approach is essentially analogous to discretise-then-optimise where the backward pass uses custom gradients for the non-differentiable terms. The second strand computes exact gradients of the spike times using the implicit function theorem. These results are equivalent to optimise-then-discretise and can be used to define adjoint equations as in [56] or to derive forward sensitivities [34]. However, we note that, unless solution trajectories and spike times of the SNN are computed exactly, neither method provides the actual gradients of the implemented solver. Furthermore, the BPTT surrogate gradient approach only covers the Euler approximation whereas many auto-differentiable differential equation solvers are available nowadays, e.g. in diffrax. Finally, there is a lot of interest in developing bioplausible learning algorithms where weights can be updated locally and in an online fashion. Notable advances in this direction include [2, 57]. To the best of our knowledge, none of these works cover the case of stochastic SNNs where the neuronal dynamics are modeled as SDEs instead of ODEs. ", "page_idx": 2}, {"type": "text", "text": "3 Stochastic spiking neural networks as Event SDEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We shall in this paper be concerned with SDEs where solution trajectories experience jumps triggered by implicitly defined events, dubbed Event SDEs. The prototypical example that we come back to throughout is the SNN model composed of SLIF neurons. Here the randomness appears both in the inter-spike dynamics as well as in the firing mechanism. To motivate the general definitions and concepts we start with an informal introduction of SSNNs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Stochastic spiking neural networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To achieve a more bioplausible model of neuronal behaviour, one can extend the simple deterministic LIF model by adding two types of noise: a diffusion term in the differential equation describing inter-spike behaviour [33] and stochastic firing [50, 27]. That is, the potential is modelled by eq. (1). Instead of firing exactly when the membrane potential hits a set threshold, we model the spike times (event times) by an inhomogenous Poisson process with intensity $\\lambda:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}_{+}$ which is assumed to be bounded by some constant $C>0$ . This can be phrased as an Event SDE (note that this is essentially the reparameterisation trick) by introducing the additional state variable $s_{t}$ satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\nd s_{t}=\\lambda(v_{t-})d t,\\quad s_{0}=\\log u\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u\\sim\\mathrm{Unif}(0,1)$ . The neuron spikes whenever $s_{t}$ hits 0 from below at which point the membrane potential is reset to a resting level and we sample a new initial condition for $s_{t}$ . We can denote this first spike time by $\\tau_{1}$ and repeat the procedure to generate a sequence of spike times $\\tau_{1}<\\tau_{2}<...$ In practice, we reinitialize $s_{t}$ at $\\log u-\\alpha$ for some $\\alpha>0$ . It can then be shown that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(t<\\tau_{n+1}|\\mathcal{F}_{\\tau_{n}}\\right)=\\operatorname*{min}\\left\\{1,\\exp\\left(\\alpha-\\int_{\\tau_{n}}^{t}\\lambda(v_{t-})d t\\right)\\right\\}\\quad\\mathrm{for}\\;t\\in[\\tau_{n},\\tau_{n+1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It follows that $\\tau_{n+1}-\\tau_{n}\\geq\\alpha/C$ a.s., i.e. $\\alpha$ controls the refractory period after spikes, a large value indicating a long resting period. ", "page_idx": 3}, {"type": "text", "text": "We can then build a SSNN by connecting such SLIF neurons in a network. In particular, apart from the membrane potential, we now also model the input current of each neuron as affected by the other neurons in the network. Let $K\\geq1$ denote the total number of neurons. We model neuron $k\\in[K]$ be the three dimensional vector $y^{k}=(v^{k},i^{k},s^{k})$ the dynamic of which in between spikes is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nd v_{t}^{k}=\\mu_{1}\\left(i_{t}^{k}-v_{t}^{k}\\right)d t+\\sigma_{1}d B_{t}^{k},\\quad d i_{t}^{k}=-\\mu_{2}i_{t}^{k}d t+\\sigma_{2}d B_{t}^{k},\\quad d s_{t}^{k}=\\lambda(v_{t}^{k};\\xi)d t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B^{k}$ is a standard two-dimensional Brownian motion, $\\sigma=(\\sigma_{1},\\sigma_{2})\\in\\mathbb{R}^{2\\times2}$ , $\\mu=(\\mu_{1},\\mu_{2})\\in$ $\\mathbb{R}^{2}$ , and $\\lambda(\\cdot;\\xi):\\mathbb{R}\\to\\mathbb{R}_{+}$ is an intensity function. As before, neuron $k$ fires (or spikes) whenever $s^{k}$ hits zero from below. Apart from resetting the membrane potential, this event also causes spikes to propagate through the network in a such a way that a spike in neuron $k$ will increment the input current of neuron $j$ by $w_{k j}$ . Here $\\boldsymbol{w}\\,\\in\\,\\mathbb{R}^{K\\times K}$ is a matrix of weights representing the synaptic weights in the neural network. If one is only interested in specific network architectures such as, e.g., feed-forward, this can be achieved by fixing the appropriate entries in $w$ at 0. ", "page_idx": 3}, {"type": "text", "text": "As presented here, there is no way to model information coming into the network. But this would only require a minor change. Indeed, by adding a suitable control term to eq. (2) we can model all relevant scenarios. Since this does not change the theory in any meaningful way (the general theory in Appendix B covers RDEs so an extra smooth control is no issue), we only discuss the more simple model given without any additional input currents. ", "page_idx": 3}, {"type": "text", "text": "3.2 Model definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Event SDE). Let $N\\in\\mathbb{N}$ be the number of events. Let $y_{0}\\in\\mathbb{R}^{e}$ be an initial condition. Let $\\mu:\\mathbb{R}^{e}\\to\\mathbb{R}^{e}$ and $\\sigma:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e\\times d}$ be the drift and diffusion vector fields. Let $\\mathcal{E}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}$ and $\\mathcal{T}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e}$ be an event and transition function respectively. We say that $\\left(y,\\left(\\tau_{n}\\right)_{n=1}^{N}\\right)$ is a solution to the Event SDE parameterised by $(y_{0},\\mu,\\sigma,\\mathcal{E},\\mathcal{T},N)$ if $y_{T}=y_{T}^{N}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{t}=\\sum_{n=0}^{N}y_{t}^{n}\\mathbf{1}_{[\\tau_{n},\\tau_{n+1})}(t),\\quad\\tau_{n}=\\operatorname*{inf}\\left\\lbrace t>\\tau_{n-1}:\\mathcal{E}(y_{t}^{n-1})=0\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\mathcal{E}(y_{\\tau_{n}}^{n})\\neq0$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d y_{t}^{0}=\\mu(y_{t}^{0})d t+\\sigma(y_{t}^{0})d B_{t},\\quad s t a r t e d\\,a t\\ y_{0}^{0}=y_{0},}\\\\ &{d y_{t}^{n}=\\mu(y_{t}^{n})d t+\\sigma(y_{t}^{n})d B_{t},\\quad s t a r t e d\\,a t\\ y_{\\tau_{n}}^{n}=7\\left(y_{\\tau_{n}}^{n-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B_{t}$ is a $d$ -dimensional Brownian motion and (4), (5) are Stratonovich SDEs. ", "page_idx": 3}, {"type": "text", "text": "In words, we initialize the system at $y_{0}$ , evolve it using (4) until the first time $\\tau_{1}$ at which an event happens $\\mathcal{E}(y_{\\tau_{1}}^{0})\\,=\\,0$ . We then transition the system according to $y_{\\tau_{1}}^{1}\\,=\\,\\mathcal{T}\\left(y_{\\tau_{1}-}^{0}\\right)$ and evolve it according to (5) until the next event is triggered. We note that Definition 3.1 can be generalised to multiple event and transition functions. Also, the transition function can be randomised by allowing it to have an extra argument $u\\sim\\mathrm{Unif}([0,1])$ . As part of the definition we require that there are only finitely many events and that an event is not immediately triggered upon transitioning. ", "page_idx": 3}, {"type": "text", "text": "Existence of strong solutions to Event SDEs driven by continuous semimartingales has been studied in [31, Theorem 5.2] and [32]. Under sufficient regularity of $\\mu$ and $\\sigma$ , a unique solution to (4) exists. We need the following additional assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. There exists $c~>~0$ such that for all $s\\ \\in\\ (0,T)$ and $a\\ \\in\\ \\mathrm{im}\\ 7\\$ it holds that $\\operatorname*{inf}\\{t>s\\ :\\ \\mathcal{E}(y_{t})=0\\}>c$ where $y_{t}$ is the solution to 4 started at $y_{s}=a$ ", "page_idx": 3}, {"type": "text", "text": "Assumptions 3.1 and 3.2 simply ensure that an event cannot be triggered immediately upon transitioning. This holds in most settings of interest. For example, for the usual deterministic LIF neuron im $\\tau=0$ and ker $\\mathcal E=1$ and the duration of the refractory period is directly linked to $c$ in Assumption 3.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Theorem 5.2, [31]). Under Assumptions 3.1-3.2 and with $\\mu\\in\\mathrm{Lip}^{1}$ and $\\sigma\\in\\mathrm{Lip}^{\\gamma}$ for $\\gamma>2$ , there exists a unique solution $(y,(\\tau_{n})_{n=1}^{N})$ to the Event SDE of Definition 3.1. ", "page_idx": 4}, {"type": "text", "text": "The definitions and results of this section can be extended to differential equations driven by random rough paths, and in particular, to cases where the driving noise exhibits jumps. In the latter case, it is important to note that the resulting Event SDE will exhibit two types of jumps: the ones given apriori by the driving noise and the ones that are implicitly defined through the solution (what we call events). In fact, we develop the main theory of Event RDEs in Appendix A in the more general setting of RDEs driven by c\u00e0dl\u00e0g rough paths. The rough path formalism enables a unified treatment of differential equations driven by noise signals of arbitrary (ir-)regularity, and makes all proofs simple and systematic. In particular, it allows us to handle cases where the diffusion term is driven by a finite activity L\u00e9vy process (e.g, a homogeneous Poisson processes highly relevant in the context of SNNs). ", "page_idx": 4}, {"type": "text", "text": "3.3 Backpropagation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We are interested in optimizing a continuously differentiable loss function $L$ whose input is the solution of a parameterised Event SDE. As for Neural ODEs, the vector fields, $\\mu,\\sigma$ , and the event and transition functions $\\mathcal{E},\\mathcal{T}$ , might depend on some learnable parameters $\\theta$ . We can move the parameters $\\theta$ of the Event RDE inside the initial condition $y_{0}$ by augmenting the dynamics with the additional state variable $\\theta_{t}$ satisfying $d\\theta_{t}=0$ and $\\theta_{0}=\\theta$ . Thus, as long as we can compute gradients with respect to $y_{0}$ , these will include gradients with respect to such parameters. We then require the gradients $\\partial_{y_{0}}L$ , if they exist. For this, we need to be able to compute the Jacobians $\\partial y_{t}^{n}:=\\partial_{y_{0}}y_{t}^{n}$ of the inter-event flows associated to the dynamics of $y_{t}^{n}$ and the derivatives $\\partial\\tau_{n}:=\\partial_{y_{0}}\\tau_{n}$ . We assume that the event and transition functions $\\mathcal{E}$ and $\\tau$ are continuously differentiable. ", "page_idx": 4}, {"type": "text", "text": "Apriori, it is not clear under what conditions such quantities exist and even less how to compute them.   \nThis shall be the focus of the present section. We will need the following running assumptions. Assumption 3.3. $\\sigma(\\mathcal{T}(y))-\\nabla\\mathcal{T}(y)\\sigma(y)=0$ for all $y\\in\\ker\\mathcal{E}$ .   \nAssumption 3.4. $\\nabla\\mathcal{E}(y)\\sigma(y)=0$ for all $y\\in\\ker\\mathcal{E}$ .   \nAssumption 3.5. $\\nabla\\mathcal{E}(y)\\mu(y)\\neq0$ for all $y\\in\\ker\\mathcal{E}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Assumption 3.4 and 3.5 ensure that the event times are differentiable. Intuitively, they state that the event condition is hit only by the drift part of the solution. Assumption 3.4 holds for example if the event functions depend only on a smooth part of the system. Assumption 3.3 is what allows us to differentiate through the event transitions. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let Assumptions 3.1-3.5 be satisfied and $(y,(\\tau_{n})_{n=1}^{N})$ the solution to the Event SDE parameterized by $(y_{0},\\mu,\\sigma,\\mathcal{E},\\mathcal{T},N)$ . Then, almost surely, for any $n\\in[N],$ , the derivatives $\\partial\\tau_{n}$ and the Jacobians $\\partial y_{t}^{n}$ exist and admit the following recursive expressions ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial\\tau_{n}=-\\frac{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\partial y_{\\tau_{n}}^{n-1}}{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\mu(y_{\\tau_{n}}^{n-1})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial y_{t}^{n}=(\\partial_{y_{\\tau_{n}}^{n}}y_{t}^{n})\\left[\\nabla T(y_{\\tau_{n}}^{n-1})\\partial y_{\\tau_{n}}^{n-1}-\\left(\\mu(y_{\\tau_{n}}^{n})-\\nabla T(y_{\\tau_{n}}^{n-1})\\mu(y_{\\tau_{n}}^{n-1})\\right)\\partial\\tau_{n}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\partial y_{t}^{n}$ and $\\partial\\tau_{n}$ are the total derivatives of $y_{n}^{t}$ and $\\tau_{n}$ with respect to the initial condition $y_{0}$ $\\partial_{y_{\\tau_{n}}^{n}}y_{t}^{n}$ denotes the partial derivative of the flow map of eq. (5) with respect to its initial condition, and $\\nabla\\mathcal{T}\\in\\mathbb{R}^{e\\times e}$ and $\\nabla\\mathcal{E}\\in\\mathbb{R}^{1\\times e}$ are the Jacobians of $\\tau$ and $\\mathcal{E}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. If the diffusion term is absent we recover the gradients in [6]. In this case, the assumptions of the theorem are trivially satisfied. Note however, that the result, as stated here, is slightly different since we are considering repeated events. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. The recursive nature of (6) - (7) suggest a way to update gradients in an online fashion by computing the forward sensitivity along with the state of the Event SDE. In traditional machine ", "page_idx": 4}, {"type": "text", "text": "learning applications (e.g. NDEs) forward mode automatic differentiation is usually avoided due to the fact that the output dimension tends to be orders of magnitude smaller than the number of parameters [28]. However, for (S)SNNs this issue can be partly avoided as discussed in Section 4.4. ", "page_idx": 5}, {"type": "text", "text": "Returning now to the SSNN model introduced in Section 3.1 we find that it is an Event SDE with $K$ different event functions given by $\\mathcal{E}_{k}(y)=s^{k}$ and corresponding transition functions given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{T}_{k}(y)=\\big(\\mathcal{T}_{k}^{1}(y^{1}),\\dots,\\mathcal{T}_{k}^{K}(y^{K})\\big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T_{k}^{j}(y^{j})\\,=\\,(v^{j},i^{j}+w_{k j},s^{j})$ if $j\\neq k$ and $\\mathcal{T}_{k}^{k}(y^{k})\\,=\\,(v^{k}\\,-\\,v_{r e s e t},i^{k},\\log u\\,-\\,\\alpha)$ where $v_{r e s e t}>0$ is a constant determining by what amount the membrane potential is reset. The addition of the constant $\\alpha>0$ controlling the refractory period ensures that Assumption 3.2 and 3.2 are satisfied. Stochastic firing smooths out the event triggering so that Assumption 3.5 and 3.4 hold. Finally, one can check that the combination of constant diffusion terms and the given transition functions satisfies Assumptions 3.3. Note that setting $v_{t}^{k}$ exactly to 0 upon spiking would break Assumption 3.3. If one is interested in such a resetting mechanism it suffices to pick a diffusion term $\\sigma_{1}(y^{k})$ that satisfies $\\sigma(0)=0$ . To sum up, solutions (in the sense of Def. 3.1) of the SSNNs exist and are unique. In addition, the trajectories and spike times are almost surely differentiable satisfying (6) and (7). ", "page_idx": 5}, {"type": "text", "text": "3.4 Numerical solvers ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 gives an expression for the gradients of the event times as well as the Event SDE solution. In practice, analytical expressions for gradients are often not available and one has to resort to numerical solvers. Three solutions suggest themselves: ", "page_idx": 5}, {"type": "text", "text": "1. There are multiple autodifferentiable differential equation solvers (such as diffrax [28]) that provide differentiable numerical approximations of the flows $\\partial_{y_{\\tau_{n}}^{n}}y_{t}^{n}$ . We shall write SDESolve $(y_{0},\\mu,\\sigma,s,t)$ for a generic choice of such a solver. Furthermore, if $\\mathtt{R o o t F i n d}(y_{0},f)$ is a differentiable root finding algorithm (here $f:(y,t)\\mapsto\\mathbb{R}$ should be differentiable in both arguments and RootFind $(y_{0},f)$ returns $t^{*}\\in\\mathbb{R}$ such that $f(y_{0},t^{*})=0)$ ), then we can define a differentiable map $E:y_{0}\\mapsto y^{*}$ by $t^{*}=\\operatorname{RootFind}\\left(y_{0},\\mathcal{E}(\\operatorname{SDESolve}\\left(\\cdot,\\mu,\\sigma,s,\\cdot\\right))\\right),\\quad y^{*}=\\operatorname{SDESolve}\\left(y_{0},\\mu,\\sigma,s,t^{*}\\right).$ Consequently, EventSDESolve $(y_{0},\\mu,\\sigma,\\mathcal{E},\\mathcal{T},N)$ can be implemented as subsequent compositions of $\\mathcal{T}\\circ E$ (see Algorithm 1). This is a discretise-then-optimise approach [28].   \n2. Alternatively, one can use the formulas (6) and (7) directly as a replacement of the derivatives. This is the approach taken in e.g. [6]. To be precise, one would replace all the derivatives of the flow map (terms of the sort $\\partial_{y_{\\tau_{n}}^{n}}y_{t}^{n})$ with the derivatives of the given numerical solver. This approach is a solution between discretise-then-optimise and optimise-then-discretise.   \n3. Finally, one could apply the adjoint method (or optimise-then-discretise) as done for deterministic SNNs in [56] by deriving the adjoint equations. These adjoint equations define another SDE with jumps which is solved backwards in time. Between events the dynamics are exactly as in the continuous case so one just needs to specify the jumps of the adjoint process. This can be done by referring to (6) and (7). ", "page_idx": 5}, {"type": "text", "text": "Remark 3.3. One thing to be careful of with the discretise-then-optimise approach is that the SDE solver will compute time derivatives in the backward pass, although the modelled process is not time differentiable. Assumptions 3.4 and 3.3 should in principle guarantee that these derivatives cancel out (see Appendix B), yet this might not necessarily happen at the level of the numerical solver because of precision issues. This is essentially due to the fact that approximate solutions provided by numerical solvers are in general not flows. Thus, when the path driving the diffusion term is very irregular, the gradients can become unstable. In practice we found this could be fixed by setting the gradient with respect to time of the driving Brownian motion to 0 and picking a step size sufficiently small. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.4. In the context of SNNs, Algorithm 1 is actually a version of exact backpropagation through time (BPTT) of the unrolled numerical solution. Contrary to popular belief, this illustrates that one can compute exact gradients of numerical approximations of SNNs without the need to resort to surrogate gradient functions. Of course, this does not alleviate the so-called dead neuron problem. However, this ceases to be a problem when stochastic firing is introduced. In fact, surrogate gradients can be related to stochastic firing mechanisms and expected gradients [20]. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.5. One the one hand, the EventSDESolve algorithm as presented here scales poorly in the number of events since it requires doing a full SDESolve and an additional RootFind each time an event occurs. This problem becomes especially prevalent for SSNNs with a large number of neurons since in this case an event is triggered every time a single neuron spikes and the inter-spike SDE that needs to be solved is high-dimensional. On the other hand, there are multiple ways to mitigate this issue. Firstly, one could relax the root-finding step and simply trigger a spike as soon as $e\\geq0$ and take this as the spike time. For the backward pass one could then solve the adjoint equations (for which you need need to store the spike times in the forward pass). The resulting algorithm would be similar to the one derived in [55] for deterministic SNNs. Secondly, for special architectures such as a feed-forward network, given the spikes from the previous layer, one could solve the EventSDE for each neuron in the current layer independently of all other neurons. This would imply that a forward (or backward) pass of the entire SSNN scales as $O(K S)$ where $S$ is the cost of the forward (or backward) pass of a single neuron and $K$ is the number of neurons. ", "page_idx": 6}, {"type": "table", "img_path": "mCWZj7pa0M/tmp/5d079ba66b4ed7a7d1c9040fe1392a004fabd0645077d5894153f06c13b8c0f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Training stochastic spiking neural networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 A loss function based on signature kernels for c\u00e0dl\u00e0g paths ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To train SSNNs we will adopt a similar technique as in [23], where the authors propose to train NSDEs non-adversarially using a class of maximum mean discrepancies (MMD) endowed with signature kernels [52] indexed on spaces of continuous paths as discriminators. However, as we mentioned in the introduction, classical signature kernels are not directly applicable to the setting of SSNNs as the solution trajectories not continuous. To remedy this issue, in Appendix C, we generalise signature kernels to Marcus signature kernels indexed on discontinuous (or c\u00e0dl\u00e0g) paths. We note that our numerical experiments only concern learning from spike trains, which are c\u00e0dl\u00e0g paths of bounded variation. Yet, the Marcus signature kernel defined in Appendix C can handle more general c\u00e0dl\u00e0g rough paths. ", "page_idx": 6}, {"type": "text", "text": "The main idea goes as follows. If $x$ is a c\u00e0dl\u00e0g path, one can define the Marcus signature $S(x)$ in the spirit of Marcus SDEs [42, 43] as the signature of the Marcus interpolation of $x$ . The general construction is given in Appendix A. The Marcus signature kernel is defined as the inner product $k(x,y)=\\langle S(x\\bar{)},S(y)\\rangle$ of Marcus signatures $S(x),\\bar{S}(y)$ of two c\u00e0dl\u00e0g paths $x,y$ . As stated in the first part of Theorem C.1, this kernel is characteristic on regular Borel measures supported on compact sets of c\u00e0dl\u00e0g paths. In particular, this implies that the resulting maximum mean discrepancy (MMD) ", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{k}(\\mu,\\nu)^{2}=\\mathbb{E}_{x,x^{\\prime}\\sim\\mu}k(x,x^{\\prime})-2\\mathbb{E}_{x,y\\sim\\mu\\times\\nu}k(x,x^{\\prime})+\\mathbb{E}_{y,y^{\\prime}\\sim\\nu}k(y,y^{\\prime})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "satisfies the property $d_{k}(\\mu,\\nu)^{2}=0\\iff\\mu=\\nu$ for any two compactly supported measures $\\mu,\\nu$ ", "page_idx": 6}, {"type": "text", "text": "Nonetheless, characteristicness ceases to hold when one considers measures on c\u00e0dl\u00e0g paths that are not compactly supported. In [8] the authors address this issue for continuous paths by using the so-called robust signature. They introduce a tensor normalization $\\Lambda$ ensuring that the range of the robust signature $\\Lambda\\circ S$ remains bounded. The robust signature kernel is then defined as the inner product $\\bar{k}_{\\Lambda}(x,y)\\,=\\,\\langle\\Lambda\\circ S(x),\\Lambda\\circ S(y)\\rangle$ . This normalization can be applied analogously to the Marcus signature resulting in a robust Marcus signature kernel. In the second part of Theorem C.1, we prove characteristicness of $k_{\\Lambda}$ for possibly non-compactly supported Borel measures on c\u00e0dl\u00e0g paths. The resulting MMD is denoted by $d_{k_{\\Lambda}}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "There are several ways of evaluating signature kernels. The most naive is to simply truncate the signatures at some finite level and then take their inner product. Another amounts to solve a pathdependent wave equation [52]. Our experiments are compatible with both of these methods. ", "page_idx": 7}, {"type": "text", "text": "Given a collection of observed c\u00e0dl\u00e0g trajectories $\\{x^{i}\\}_{i=1}^{m}\\sim\\mu^{\\mathrm{{true}}}$ sampled from an underlying   \ntuonrkienso suusrien $\\mu^{\\mathrm{true}}$ ,u wnbei acsaen dt reaimnp iarni cEalv eesntti SmDatEo rb oyf t(cohri h),e  i.gee. nemriantiemd icsi\u00e0ndgl\u00e0 og vterra jtehce$\\{y^{i}\\}_{i=1}^{n}\\sim\\mu^{\\theta}$ $d_{k}$ $d_{k_{\\Lambda}}$   \nparameters $\\theta$ of the Event SDE the following loss function ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{m(m-1)}\\sum_{j\\neq i}k(x^{i},x^{j})-\\frac{2}{m n}\\sum_{i,j}k(x^{i},y^{j})+\\frac{1}{n(n-1)}\\sum_{j\\neq i}k(y^{i},y^{j}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the context of SSNNs, the observed and generated trajectories $x^{i}$ \u2019s and $y^{i}$ \u2019s correspond to spike trains, which are c\u00e0dl\u00e0g paths of bounded variation. ", "page_idx": 7}, {"type": "text", "text": "4.2 Input current estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The first example is the simple problem of estimating the constant input current $c>0$ based on a sample of spike trains in the single SLIF neuron model, ", "page_idx": 7}, {"type": "equation", "text": "$$\nd v_{t}=\\mu(c-v_{t})d t+\\sigma d B_{t},\\quad d s_{t}=\\lambda(v_{t})d t,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda(v)\\,=\\,\\exp(5(v\\mathrm{~-~}1)$ , $\\mu\\,=\\,15$ and $\\sigma$ varies. Throughout we fix the true $c=1.5$ and set $v_{r e s e t}=1.4$ and $\\alpha=0.03$ . We run stochastic gradient descent for 1500 steps for two choices of the diffusion constant $\\sigma$ . The loss function is the signature kernel MMD between a simulated batch and the sample of spike trains.3. The test loss is the mean absolute error between the first three average spike times. Results are given in Fig. 1. For additional details regarding the experiments, we refer to Appendix E. ", "page_idx": 7}, {"type": "text", "text": "In all cases backpropagation through Algorithm 1 is able to learn the underlying input current after around 600 steps up to a small estimation error. In particular, the convergence is fastest for the largest sample size and the true $c$ is recovered for both levels of noise. ", "page_idx": 7}, {"type": "text", "text": "4.3 Synaptic weight estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next we consider the problem of estimating the weight matrix in a feed-forward SSNN with input dimension 4, 1 hidden layer of dimension 16, and output dimension 2. The rest of the parameters are fixed throughout. We run stochastic gradient descent for 1500 steps with a batch size of 128 and for a sample size of 256, 512, and 1024 respectively. Learning rate is decreased from 0.003 to 0.001 after 1000 steps. The results are given in Fig. 2 in Appendix E. For a sample size of 512 and 1024 we are able to reach a test loss of practically 0, that is, samples from the learned model and the underlying model are more or less indistinguishable. Also, in all cases the estimated weight matrix approaches the true weight matrix. Interestingly, for the largest sample size, the model reaches the same test loss as the model trained on a sample size of 512, but their estimated weight matrices differ significantly. ", "page_idx": 7}, {"type": "text", "text": "4.4 Online learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the case of SSNNs, equations (6)-(7) lead to a formula for the forward sensitivity where any propagation of gradients between neurons only happens at spike times and only between connected neurons (see Proposition D.1). Since the forward sensitivities are computed forward in time together with the solution of the SNN, gradients can be updated online as new data appears. As a result, between spikes of pre-synaptic neurons, we can update the gradient flow of the membrane potential and input current of each neuron using information exclusively from that neuron. For general network structures and loss functions, however, this implies that each neuron needs to store on the order of $K^{2}$ gradient flows (one for each weight in the network). ", "page_idx": 7}, {"type": "image", "img_path": "mCWZj7pa0M/tmp/a765a6ae0d123f235dbf41bf41f12f35b7f1d4ae38a830a2698ad4d54e0d2df8.jpg", "img_caption": ["Figure 1: Test loss and $c$ estimate across four sample sizes and for two levels of noise $\\sigma$ . On the left: MAE for the three first average spike times on a hold out test set. On the right: estimated value of $c$ at the current step. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "On the other hand, if the adjacency matrix of the weight matrix forms a directed acyclic graph (DAG), three-factor Hebbian learning rules like those in [57, 2] are easily derived from Proposition D.1. For simplicity, consider the SNN consisting of deterministic LIF neurons and let $N_{t}^{k}$ denote the spike train of neuron $k$ , i.e., $N_{t}^{k}$ is c\u00e0dl\u00e0g path equal to the number of spikes of neuron $k$ at time $t$ . We let $\\tau^{k}(t)$ (or $\\tau^{k}$ for short) denote the last spike of neuron $k$ before time $t$ . We shall assume that the instantaneous loss function $L_{t}$ depends only on the most recent spike times $\\tau^{1},\\dots,\\tau^{K}$ . Then, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\partial_{w_{j k}}L_{t}=\\partial_{\\tau^{k}}L_{t}\\frac{a_{\\tau^{k}}^{j k}}{\\mu_{1}(v_{\\tau^{k}}^{k}-i_{\\tau^{k}}^{k})}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $a_{t}^{j k}$ is the eligibility trace and the first term can be viewed as a global modulator, that is, a top-down learning signal propagating the error from the output neurons.4The eligibility trace satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nd a_{t}^{j k}=\\mu_{1}\\left(b_{t}^{j k}-a_{t}^{j k}\\right)d t+\\frac{v_{r e s e t}a_{t}^{j k}}{\\mu_{1}\\big(i_{t}^{k}-v_{t}^{k}\\big)}d N_{t}^{k},\\quad d b_{t}^{j k}=-\\mu_{2}b_{t}^{j k}+d N_{t}^{j},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the $d N$ terms are to be understood in the Riemann-Stieltjes sense. In other words, the eligibility trace can be updated exclusively from the activity of the pre- and post-synaptic neurons. We note the similarity to the results derived in [2] only our result gives the exact gradients with no need to introduce surrogate gradient functions. A similar equation for deterministic SNNs was derived in [49] (see, in particular, Chapter 5). For general network structures one can use the eligibility traces as proxies for the the true derivatives $\\partial_{w_{i j}}\\bar{\\tau}^{k}$ . ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We introduced a mathematical framework based on rough path theory to model SSNNs as SDEs exhibiting event discontinuities and driven by c\u00e0dl\u00e0g rough paths. After identifying sufficient conditions for differentiability of solution trajectories and event times, we obtained a recursive relation for the pathwise gradients in Theorem 3.2, generalising the results presented in [6] and [25] which only deal with the case of ODEs. Next, we introduced Marcus signature kernels as extensions of continuous signature kernels from [52] to c\u00e0dl\u00e0g rough paths and used them to define a general-purpose loss function on the space of c\u00e0dl\u00e0g rough paths to train SSNNs where noise is present in both the spike timing and the network\u2019s dynamics. Based on these results, we also provided an end-to-end autodifferentiable solver for SDEs with event discontinuities (Algorithm 1) and made its implementation available as part of the diffrax repository. Finally, we discussed how our results lead to bioplausible learning algorithms akin to $e$ -prop [2] but in the context of spike time gradients. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The primary objective of the paper was to lay out the theoretical foundations of gradient-based learning with stochastic SNNs. Although we provided an initial implementation, which is well-suited for low dimensional examples, a robust version that scales to a high number of neurons is beyond the scope of the paper. Examples that require a much higher number of neurons than the two examples already discussed will be hard to handle with the discretize-then-optimize approach for the reasons given in Remark 3.5. ", "page_idx": 9}, {"type": "text", "text": "We think there are still many interesting research directions left to explore. For instance, it would be of interest to implement the adjoint equations or to use reversible solvers and compare the results. Similarly, since our Algorithm 1 differs from the usual approach with surrogate gradients even in the deterministic setting, questions remain on how these methods compare for training SNNs. Furthermore, it would be interesting to understand to what extent the inclusion of different types of driving noises in the dynamics of SSNNs would be beneficial for learning tasks compared to deterministic SNNs. Finally, it remains to be seen whether the discussion in Section 4.4 could lead to a bio-plausible learning algorithm with comparable performance to state-of-the-art backpropagation methods and implementable on neuromorphic hardware. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. Christian Holberg gratefully acknowledges financial support from Novo Nordisk Foundation through Grant NNF20OC0062958 and from Independent Research Fund Denmark | Natural Sciences through Grant 9040-00215B. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n[2] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature communications, 11(1):3625, 2020.   \n[3] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011.   \n[4] Thomas Cass and Cristopher Salvi. Lecture notes on rough paths and applications to machine learning. arXiv preprint arXiv:2404.06583, 2024.   \n[5] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[6] Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Learning neural event functions for ordinary differential equations. In International Conference on Learning Representations, 2020.   \n[7] Ilya Chevyrev and Peter K Friz. Canonical rdes and general semimartingales as rough paths. The Annals of Probability, 47(1):420\u2013463, 2019.   \n[8] Ilya Chevyrev and Harald Oberhauser. Signature moments to characterize laws of stochastic processes. Journal of Machine Learning Research, 23(176):1\u201342, 2022.   \n[9] Nicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as infinitewidth-depth-limits of controlled resnets. In International Conference on Machine Learning, pages 25358\u201325425. PMLR, 2023.   \n[10] Thomas Cochrane, Peter Foster, Varun Chhabra, Maud Lemercier, Terry Lyons, and Cristopher Salvi. Sk-tree: a systematic malware detection algorithm on streaming trees via the signature kernel. In 2021 IEEE international conference on cyber security and resilience (CSR), pages 35\u201340. IEEE, 2021.   \n[11] Sebastien Corner, Corina Sandu, and Adrian Sandu. Modeling and sensitivity analysis methodology for hybrid dynamical system. Nonlinear Analysis: Hybrid Systems, 31:19\u201340, 2019.   \n[12] Sebastien Corner, Adrian Sandu, and Corina Sandu. Adjoint sensitivity analysis of hybrid multibody dynamical systems. Multibody System Dynamics, 49:395\u2013420, 2020.   \n[13] Christa Cuchiero, Francesca Primavera, and Sara Svaluto-Ferro. Universal approximation theorems for continuous functions of c\\adl\\ag paths and l\\\u2019evy-type signature models. arXiv preprint arXiv:2208.02293, 2022.   \n[14] Adeline Fermanian, Terry Lyons, James Morrill, and Cristopher Salvi. New directions in the applications of rough path theory. IEEE BITS the Information Theory Magazine, 2023.   \n[15] Peter Friz and Atul Shekhar. General rough integration, l\u00e9vy rough paths and a l\u00e9vy\u2013kintchinetype formula. Annals of probability: An official journal of the Institute of Mathematical Statistics, 45(4):2707\u20132765, 2017.   \n[16] Peter K Friz and Martin Hairer. A course on rough paths. Springer, 2020.   \n[17] Peter K Friz and Nicolas B Victoir. Multidimensional stochastic processes as rough paths: theory and applications, volume 120. Cambridge University Press, 2010.   \n[18] Peter K Friz and Huilin Zhang. Differential equations driven by rough paths with jumps. Journal of Differential Equations, 264(10):6226\u20136301, 2018.   \n[19] Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002.   \n[20] Julia Gygax and Friedemann Zenke. Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks. arXiv preprint arXiv:2404.14964, 2024.   \n[21] Thomas A Henzinger. The theory of hybrid automata. In Proceedings 11th Annual IEEE Symposium on Logic in Computer Science, pages 278\u2013292. IEEE, 1996.   \n[22] Melker H\u00f6glund, Emilio Ferrucci, Camilo Hern\u00e1ndez, Aitor Muguruza Gonzalez, Cristopher Salvi, Leandro S\u00e1nchez-Betancourt, and Yufei Zhang. A neural rde approach for continuoustime non-markovian stochastic control problems. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \n[23] Zacharia Issa, Blanka Horvath, Maud Lemercier, and Cristopher Salvi. Non-adversarial training of neural sdes with signature kernel scores. Advances in Neural Information Processing Systems, 2023.   \n[24] Hyeryung Jang and Osvaldo Simeone. Multisample online learning for probabilistic spiking neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(5):2034\u2013 2044, 2022.   \n[25] Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. Advances in Neural Information Processing Systems, 32, 2019.   \n[26] Danilo Jimenez Rezende and Wulfram Gerstner. Stochastic variational learning in recurrent spiking networks. Frontiers in computational neuroscience, 8:38, 2014.   \n[27] Hiroshi Kajino. A differentiable point process with its application to spiking neural networks. In International Conference on Machine Learning, pages 5226\u20135235. PMLR, 2021.   \n[28] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.   \n[29] Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, and Terry Lyons. Neural sdes as infinite-dimensional gans. arXiv preprint arXiv:2102.03657, 2021.   \n[30] Franz J Kir\u00e1ly and Harald Oberhauser. Kernels for sequentially ordered data. Journal of Machine Learning Research, 20(31):1\u201345, 2019.   \n[31] Jaroslav Krystul and HAP Blom. Generalised stochastic hybrid processes as strong solutions of stochastic differential equations. Hybridge report D, 2, 2005.   \n[32] Jaroslav Krystul, Henk AP Blom, and Arunabha Bagchi. Stochastic differential equations on hybrid state spaces. Stochastic Hybrid Systems, 24(15-45):170, 2006.   \n[33] Petr Lansky and Susanne Ditlevsen. A review of the methods for signal estimation in stochastic diffusion leaky integrate-and-fire neuronal models. Biological cybernetics, 99(4-5):253\u2013262, 2008.   \n[34] Jane H Lee, Saeid Haghighatshoar, and Amin Karbasi. Exact gradient computation for spiking neural networks via forward propagation. In International Conference on Artificial Intelligence and Statistics, pages 1812\u20131831. PMLR, 2023.   \n[35] Maud Lemercier, Cristopher Salvi, Theodoros Damoulas, Edwin Bonilla, and Terry Lyons. Distribution regression for sequential data. In International Conference on Artificial Intelligence and Statistics, pages 3754\u20133762. PMLR, 2021.   \n[36] Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David K Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In Symposium on Advances in Approximate Bayesian Inference, pages 1\u201328. PMLR, 2020.   \n[37] John Lygeros and Maria Prandini. Stochastic hybrid systems: a powerful framework for complex, large scale applications. European Journal of Control, 16(6):583\u2013594, 2010.   \n[38] Terry J Lyons. Differential equations driven by rough signals. Revista Matem\u00e1tica Iberoamericana, 14(2):215\u2013310, 1998.   \n[39] Terry J Lyons, Michael Caruana, and Thierry L\u00e9vy. Differential equations driven by rough paths. Springer, 2007.   \n[40] Gehua Ma, Rui Yan, and Huajin Tang. Exploiting noise as a resource for computation and learning in spiking neural networks. Patterns, 4(10), 2023.   \n[41] Georg Manten, Cecilia Casolo, Emilio Ferrucci, S\u00f8ren Wengel Mogensen, Cristopher Salvi, and Niki Kilbertus. Signature kernel conditional independence tests in causal discovery for stochastic processes. arXiv preprint arXiv:2402.18477, 2024.   \n[42] Steven Marcus. Modeling and analysis of stochastic differential equations driven by point processes. IEEE Transactions on Information theory, 24(2):164\u2013172, 1978.   \n[43] Steven I Marcus. Modeling and approximation of stochastic differential equations driven by semimartingales. Stochastics: An International Journal of Probability and Stochastic Processes, 4(3):223\u2013245, 1981.   \n[44] James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829\u20137838. PMLR, 2021.   \n[45] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Sch\u00f6lkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1\u2013141, 2017.   \n[46] Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51\u201363, 2019.   \n[47] Ali Pakniyat and Peter E Caines. On the stochastic minimum principle for hybrid systems. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 1139\u20131144. IEEE, 2016.   \n[48] Alexandre Pannier and Cristopher Salvi. A path-dependent pde solver based on signature kernels. arXiv preprint arXiv:2403.11738, 2024.   \n[49] Christian Pehle. Adjoint equations of spiking neural networks. PhD thesis, Universit\u00e4t Heidelberg, 2021.   \n[50] Jean-Pascal Pfister, Taro Toyoizumi, David Barber, and Wulfram Gerstner. Optimal spiketiming-dependent plasticity for precise action potential firing in supervised learning. Neural computation, 18(6):1318\u20131348, 2006.   \n[51] Cristopher Salvi. Rough paths, kernels, differential equations and an algebra of functions on streams. PhD thesis, University of Oxford, 2021.   \n[52] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and W. Y. The signature kernel is the solution of a Goursat PDE. SIAM Journal on Mathematics of Data Science, 3(3):873\u2013899, 2021.   \n[53] Cristopher Salvi, Maud Lemercier, Thomas Cass, Edwin V Bonilla, Theodoros Damoulas, and Terry J Lyons. Siggpde: Scaling sparse gaussian processes on sequential data. In International Conference on Machine Learning, pages 6233\u20136242. PMLR, 2021.   \n[54] Cristopher Salvi, Maud Lemercier, Chong Liu, Blanka Horvath, Theodoros Damoulas, and Terry Lyons. Higher order kernel mean embeddings to capture flitrations of stochastic processes. Advances in Neural Information Processing Systems, 34:16635\u201316647, 2021.   \n[55] Bernhard Sch\u00f6lkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.   \n[56] Timo C Wunderlich and Christian Pehle. Event-based backpropagation can compute exact gradients for spiking neural networks. Scientific Reports, 11(1):12829, 2021.   \n[57] Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, and Zhouchen Lin. Online training through time for spiking neural networks. Advances in neural information processing systems, 35:20717\u201320730, 2022.   \n[58] Friedemann Zenke and Tim P Vogels. The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks. Neural computation, 33(4):899\u2013925, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is structured as follows. Section A covers the basic concepts of c\u00e0dl\u00e0g rough paths based on [7] extended with a few of our own definitions and results. It culminates with the definition of Event RDEs which can be viewed as generalizations of Event SDEs. Section B covers the proof of the main result, Theorem 3.2, but in the setting of Event RDEs as well as some preliminary technical lemmas needed for the proof. Section C gives a brief overview of the main concepts in kernel learning and presents our results on Marcus signature kernels along with their proofs. Section D derives the forward sensitivities of a SSNN. Finally, Section E covers all the technical details of the simulation experiments that were not discussed in the main body of the paper. ", "page_idx": 13}, {"type": "text", "text": "A C\u00e0dl\u00e0g rough paths ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Marcus integration developed in [7] preserves the chain rule and thus serves as an analog to Stratonovich integration for semi-martingales with jump discontinuities. In particular, it allows to define a canonical lift under which c\u00e0dl\u00e0g semi-martingales are a.s. geometric rough paths and many of the results from the continuous case, such as universal limit theorems and stability results, carry over under suitably defined metrics.We briefly review some of the important concepts here by following the same setup as in [7]. ", "page_idx": 13}, {"type": "text", "text": "Let $C([0,T],E)$ and $D([0,T],E)$ be the space of continuous and c\u00e0dl\u00e0g paths respectively on $[0,T]$ with values in a metric space $(E,d)$ . For $p\\geq1$ , let $C_{p}([0,T],E)$ and $D_{p}([0,T],E)$ be the corresponding subspaces of paths with finite $p$ -variation. For any $N\\ge1$ , Let $G^{N}(\\mathbb R^{d})$ be the step- $N$ free nilpotent Lie group over $\\mathbb{R}^{d}$ endowed with the Carnot-Carath\u00e9odory metric $d$ . Let $\\Omega_{p}^{\\bar{C}}(\\mathbb{R}^{d})\\,:=\\,C_{p}([0,T],G^{\\lfloor\\bar{p}\\rfloor}(\\mathbb{R}^{\\bar{d}}))$ and $\\Omega_{p}^{D}(\\mathbb{R}^{d})\\,:=\\,D_{p}([0,T],G^{\\lfloor p\\rfloor}(\\mathbb{R}^{d}))$ be the space of weakly geometric continuous and c\u00e0dl\u00e0g $p$ -rough paths respectively with the homogeneous $p$ -variation metric ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{p}(\\mathbf{x},\\mathbf{y})=\\operatorname*{max}_{1\\leq k\\leq\\lfloor p\\rfloor}\\operatorname*{sup}_{\\mathcal{D}\\subset[0,T]}\\left(\\sum_{\\mathcal{D}}d(\\mathbf{x}_{t_{i},t_{i+1}},\\mathbf{y}_{t_{i},t_{i+1}})^{\\frac{p}{k}}\\right)^{\\frac{k}{p}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Define the log-linear path function ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi:G^{N}(\\mathbb{R}^{d})\\times G^{N}(\\mathbb{R}^{d})\\to C([0,1],G^{N}(\\mathbb{R}^{d}))}\\\\ &{\\qquad\\qquad\\qquad({\\bf a},{\\bf b})\\mapsto\\exp((1-\\cdot)\\log{\\bf a}+\\cdot\\log{\\bf b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where log and exp are the (truncated) tensor logarithm and exponential maps on $G^{N}(\\mathbb R^{d})$ . If $N=1$ , then $G^{N}(\\mathbb{R}^{d})\\cong\\mathbb{R}\\oplus\\mathbb{R}^{d}$ and $\\phi(a,b)_{t}=(1,(1-t)a+t b)$ is a straight line connecting $a$ to $b$ in unit time. For any $\\mathbf{x}\\in D([0,T],G^{N}(\\mathbb{R}^{d}))$ we can construct a continuous path $\\hat{\\mathbf{x}}\\in C([0,T],G^{N}(\\mathbb{R}^{d}))$ by adding fictitious time and interpolating through the jumps using the log-linear path function according to the following definition. ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Marcus interpolation). Let $N\\geq1$ . For $\\mathbf{x}\\in D([0,T],G^{N}(\\mathbb{R}^{d}))$ , let $\\tau_{1},\\tau_{2},\\dots,\\tau_{m}$ be the jump times of $\\mathbf{x}$ ordered such that $d(\\mathbf{x}_{\\tau_{1}-},\\mathbf{x}_{\\tau_{1}})\\;\\geq\\;d(\\mathbf{x}_{\\tau_{2}-},\\mathbf{x}_{\\tau_{2}})\\;\\geq\\;\\cdots\\;\\geq\\;d(\\mathbf{x}_{\\tau_{m}-},\\mathbf{x}_{\\tau_{m}}),$ , where $0\\leq m\\leq\\infty$ is the number of jumps. Let $\\left(r_{k}\\right)$ be a sequence of positive scalars $r_{k}>0$ such that $\\begin{array}{r}{r=\\sum_{k=1}^{m}r_{k}<+\\infty}\\end{array}$ . Define the discontinuous reparameterisation $\\eta:[0,T]\\rightarrow[0,T+r]$ by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta(t)=t+\\sum_{k=1}^{m}r_{k}\\mathbf{1}_{\\{\\tau_{k}\\leq t\\}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Marcus augmentation $\\mathbf{x}^{M}\\in C([0,T+r],G^{N}(\\mathbb{R}^{d}))$ of $\\mathbf{x}$ is the path ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{x}_{s}^{M}=\\left\\{\\!\\!\\begin{array}{l l}{\\mathbf{x}_{t},\\quad}&{i f s=\\eta(t)\\,f o r\\,s o m e\\,t\\in[0,T],}\\\\ {\\phi(\\mathbf{x}_{\\tau_{k}-},\\mathbf{x}_{\\tau_{k}})_{(s-\\eta(\\tau_{k}-))/r_{k}},\\quad}&{i f s\\in[\\eta(\\tau_{k}-),\\eta(\\tau_{k}))\\,f o r\\,1\\leq k<m+1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Marcus interpolation $\\hat{\\mathbf{x}}\\,\\in\\,C([0,T],G^{N}(\\mathbb{R}^{d}))$ of $\\mathbf{x}$ is the path $\\hat{\\mathbf{x}}\\,=\\,\\mathbf{x}^{M}\\circ\\eta_{r}$ where $\\eta_{r}(t)\\,=$ $t(T+r)/T$ is a reparameterisation from $[0,T]$ to $[0,T+r]$ . We can recover x from $\\hat{\\bf x}$ via $\\mathbf{x}=\\hat{\\mathbf{x}}\\circ\\eta_{\\mathbf{x}}$ by considering the reparameterisation $\\bar{\\eta_{\\ast}}=\\bar{\\eta_{r}^{-1}}\\,\\bar{\\circ}\\,\\eta$ . ", "page_idx": 13}, {"type": "text", "text": "Once the Marcus interpolation is defined we can state what we mean by a solution to a differential equation driven by a geometric c\u00e0dl\u00e0g rough path. ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Marcus RDE). Let $\\mathbf{x}\\in\\Omega_{p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ and $\\boldsymbol{f}=\\left(f_{1},\\ldots,f_{d}\\right)$ be $\\mathrm{Lip^{\\gamma}}$ vector fields on $\\mathbb{R}^{e}$ with $\\gamma>p$ . For an initial condition $a\\in\\mathbb{R}^{e}$ , let $\\hat{y}\\in C_{p}([0,T],\\mathbb{R}^{e})$ be the solution to the classical RDE driven by the Marcus interpolation $\\hat{\\mathbf{x}}\\in\\Omega_{p}^{C}(\\ensuremath{\\mathbb{R}^{d}})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nd{\\hat{y}}_{t}=f({\\hat{y}}_{t})d{\\hat{\\mathbf{x}}}_{t},\\quad{\\hat{y}}_{0}=a.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Define the solution $y\\in D_{p}([0,T],\\mathbb{R}^{e})$ to the Marcus RDE ", "page_idx": 14}, {"type": "equation", "text": "$$\nd y_{t}=f(y_{t})\\diamond d{\\mathbf{x}}_{t},\\quad y_{0}=a\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to be $y=\\hat{y}\\circ\\eta_{\\mathbf{x}},$ , where $\\eta_{\\bf x}$ is the reparameterisation introduced in Definition A.1. ", "page_idx": 14}, {"type": "text", "text": "A.1 Metrics on the space of c\u00e0dl\u00e0g rough paths ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Chevyrev and Friz [7] introduce a metric $\\alpha_{p}$ on $\\Omega_{p}^{D}(\\mathbb{R}^{d})$ with respect to which 1) geometric c\u00e0dl\u00e0g rough paths can be approximated with a sequence of continuous paths [7, Section 3.2] and 2) the solution map $(y_{0},\\mathbf{x})\\mapsto(\\mathbf{x},y)$ of the Marcus RDE (8) is locally Lipschitz continuous [7, Theorem 3.13]. ", "page_idx": 14}, {"type": "text", "text": "We write $\\Lambda$ for the set of increasing bijections from $[0,T]$ to itself. For a $\\lambda\\\\ \\in\\ \\Lambda$ we let $|\\lambda|\\,=$ $\\operatorname*{sup}_{t\\in[0,T]}|\\lambda(t)-t|$ . We first define the Skorokhod metric as well as a Skorokhod version of the usual $p$ -variation metric. ", "page_idx": 14}, {"type": "text", "text": "Definition A.3. For $p\\geq1$ and $\\mathbf{x},\\mathbf{y}\\in D_{p}([0,T],E)$ , we define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma_{\\infty}({\\bf x},{\\bf y})=\\operatorname*{inf}_{\\lambda\\in\\Lambda}\\operatorname*{max}\\left\\{|\\lambda|,\\ \\operatorname*{sup}_{t\\in[0,T]}d(({\\bf x}\\circ\\lambda)_{t},{\\bf y}_{t})\\right\\},}\\\\ {\\displaystyle\\sigma_{p}({\\bf x},{\\bf y})=\\operatorname*{inf}_{\\lambda\\in\\Lambda}\\operatorname*{max}\\left\\{|\\lambda|,d_{p}\\left({\\bf x}\\circ\\lambda,{\\bf y}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It turns out that the topology induced by $\\sigma_{p}$ is too strong. In particular, it is not possible to approximate paths with jump discontinuities with a sequence of continuous paths (see Section 3.2 in [7]). For $\\textbf{x}\\in\\,\\Omega_{p}^{D}(\\mathbb{R}^{d})$ and $\\boldsymbol{f}~=~(f_{1},\\ldots,f_{d})$ a family of vector fields in $\\mathrm{Lip}^{\\gamma-1}(\\mathbb{R}^{e})$ with $\\gamma~>~p$ , let $\\Phi_{f}(y,s,t;\\mathbf{x})$ denote the solution to the Marcus RDE $d y_{t}\\,=\\,f(y_{t})\\diamond d{\\bf x}_{t}$ initialized at $y_{s}=y$ and evaluated at time $t$ . We define the set ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ_{f}=\\left\\{((\\mathbf{a},b),(\\mathbf{a}^{\\prime},b^{\\prime}))\\mid\\mathbf{a},\\mathbf{a}^{\\prime}\\in G^{\\lfloor p\\rfloor}(\\mathbb{R}^{e}),\\Phi_{f}(b,0,1;\\phi(\\mathbf{a},\\mathbf{a}^{\\prime}))=b^{\\prime}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and, on it, the path function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{f}((\\mathbf{a},b),(\\mathbf{a}^{\\prime},b^{\\prime}))_{t}=(\\phi(\\mathbf{a},\\mathbf{a}^{\\prime}),\\Phi_{f}(b,0,1;\\phi(\\mathbf{a},\\mathbf{a}^{\\prime})_{t}))\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, we let $D_{p}^{f}([0,T],G^{\\lfloor p\\rfloor}(\\mathbb{R}^{d})\\times\\mathbb{R}^{e})$ be the space of c\u00e0dl\u00e0g paths $\\mathbf{z}=(\\mathbf{x},y)$ on $G^{\\lfloor p\\rfloor}(\\mathbb{R}^{d})\\times\\mathbb{R}^{e}$ of bounded $p$ -variation such that $(\\mathbf{z}_{t}-,\\mathbf{z}_{t})\\in J_{f}$ for all jump times $t$ of $\\mathbf{z}$ . To keep notation simple, we shall write $D_{p}^{f}$ when this does not cause any confusion. Naturally, if $y$ is the solution to the Marcus RDE $d y_{t}=f(y_{t})\\diamond d\\mathbf{x}_{t}$ , we have $(\\mathbf{x},y)\\in D_{p}^{f}$ . For a $\\mathbf{z}=(\\mathbf{x},y)\\in D_{p}^{f}$ we may define the Marcus interpolation by interpolating the jumps using $\\phi_{f}$ . Let $\\hat{\\mathbf{z}}^{\\delta}$ denote this interpolation but with $r_{k}$ replaced by $\\delta r_{k}$ for $\\delta>0$ and similarly for $\\hat{\\mathbf{x}}^{\\delta}$ with $\\mathbf{x}\\in\\Omega_{p}^{D}(\\mathbb{R}^{d})$ . ", "page_idx": 14}, {"type": "text", "text": "Definition A.4. For $f\\;=\\;(f_{1},\\ldots,f_{d})$ $a$ family of vector fields in $\\mathrm{Lip}^{\\gamma-1}(\\mathbb{R}^{e})$ with $\\gamma\\,>\\,p_{\\mathrm{{r}}}$ , let $\\mathbf{z},\\mathbf{z}^{\\prime}\\in D_{p}^{f}$ with $\\mathbf{z}=(\\mathbf{x},y)$ and $\\mathbf{z}^{\\prime}=(\\mathbf{x}^{\\prime},y^{\\prime})$ and define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{p}(\\mathbf{x},\\mathbf{x}^{\\prime})=\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\sigma_{p}(\\hat{\\mathbf{x}}^{\\delta},\\hat{\\mathbf{x}}^{'\\delta}),}\\\\ {\\alpha_{p}(\\mathbf{z},\\mathbf{z}^{\\prime})=\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\sigma_{p}(\\hat{\\mathbf{z}}^{\\delta},\\hat{\\mathbf{z}}^{'\\delta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Remark A.1. It is proven in [7] that in both cases the limit in $\\alpha_{p}$ exists, is independent of the choice of $r_{k}$ , and that it is indeed a metric on $\\Omega_{p}^{D}(\\mathbb{R}^{d})$ resp. $D_{p}^{f}$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1 (Theorem $3.13+$ Proposition 3.18, [7]). Let $\\boldsymbol{f}=\\left(f_{1},\\ldots,f_{d}\\right)$ be a family of vector fields in Lip $\\gamma^{-1}(\\mathbb{R}^{e})$ with $\\gamma>p$ . Then, ", "page_idx": 14}, {"type": "text", "text": "1. The solution map ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{R}^{e}\\times(\\Omega_{p}^{D}(\\mathbb{R}^{d}),\\alpha_{p})\\rightarrow(D_{p}^{f},\\alpha_{p})}}\\\\ &{}&{(y_{0},\\mathbf{x})\\mapsto\\mathbf{z}=(\\mathbf{x},y)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "of the Marcus RDE $d y_{t}=f(y_{t})\\diamond d\\mathbf{x}_{t}$ initialized at $y_{0}\\in\\mathbb{R}^{e}$ is locally Lipschitz. ", "page_idx": 15}, {"type": "text", "text": "2. On sets of bounded $p$ -variation, the solution map ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}^{e}\\times(\\Omega_{p}^{D}(\\mathbb{R}^{d}),\\sigma_{\\infty})\\rightarrow(D_{p}([0,T],\\mathbb{R}^{e}),\\sigma_{\\infty})}\\\\ &{\\qquad\\qquad\\qquad(y_{0},\\mathbf{x})\\mapsto y}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "of the Marcus RDE $d y_{t}=f(y_{t})\\diamond d\\mathbf{x}_{t}$ initialized at $y_{0}\\in\\mathbb{R}^{e}$ is continuous. ", "page_idx": 15}, {"type": "text", "text": "Now, let $C_{0}^{1}(\\mathbb{R}^{d})$ be the space of absolutely continuous functions on $\\mathbb{R}^{d}$ ", "page_idx": 15}, {"type": "text", "text": "Definition A.5. We define the space of geometric c\u00e0dl\u00e0g $p$ -rough paths $\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ as the closure of $C_{0}^{1}(\\mathbb{R}^{d})$ in $\\Omega_{p}^{D}(\\mathbb{R}^{d})$ under the metric $\\alpha_{p}$ . ", "page_idx": 15}, {"type": "text", "text": "Remark A.2. A c\u00e0dl\u00e0g semi-martingale $x\\in D_{p}([0,T],\\mathbb{R}^{d})$ can be canonically lifted to a geometric c\u00e0dl\u00e0g $p$ -rough path, with $p\\in[2,3)$ , by enhancing it with its two-fold iterated Marcus integrals, i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}_{s,t}=\\left(1,x_{s,t},\\int_{s,t}(x_{s}-x_{u})\\otimes\\diamond d x_{u}\\right)\\in G^{2}(\\mathbb{R}^{d})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the integral is defined in a similar spirit to Definition A.2 (see, for example, [18] for more information). The solution to the corresponding Marcus RDE agrees a.s. with the solution to the usual c\u00e0dl\u00e0g Marcus SDE which, in turn, if $x$ has a.s. continuous sample paths, agrees a.s. with the solution to the Stratonovich SDE. See, e.g., Proposition 4.16 in [7]. ", "page_idx": 15}, {"type": "text", "text": "A.2 Signature ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The extended tensor algebra over $\\mathbb{R}^{d}$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nT\\left(\\left(\\mathbb{R}^{d}\\right)\\right)=\\prod_{n=0}^{\\infty}\\left(\\mathbb{R}^{d}\\right)^{\\otimes n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "equipped with the usual addition $^+$ and tensor multiplication $\\otimes$ . An element $\\textbf{a}\\in\\,T\\left(\\left(\\mathbb{R}^{d}\\right)\\right)$ is a formal series of tensors ${\\mathbf a}\\,=\\,({\\mathbf a}^{0},{\\mathbf a}^{1},\\dots)$ such that $\\mathbf{a}^{n}\\,\\in\\,(\\mathbb{R}^{d})^{\\otimes n}$ . We define the projections $\\pi_{n}:T\\left(\\left(\\mathbb{R}^{d}\\right)\\right)\\rightarrow(\\mathbb{R}^{d})^{\\otimes n}$ given by $\\pi_{n}(\\mathbf{a})=\\mathbf{a}^{n}$ . Let $\\tilde{T}\\big((\\mathbb{R}^{d})\\big)$ be the subset of $T((\\mathbb{R}^{d}))$ such that the $\\pi_{0}(\\mathbf{a})=1$ for all $\\mathbf{a}\\in\\tilde{T}((\\mathbb{R}^{d}))$ . Finally, we define the set of group-like elements, ", "page_idx": 15}, {"type": "equation", "text": "$$\nG^{(*)}=\\left\\{\\mathbf{a}\\in\\tilde{T}\\left(\\left(\\mathbb{R}^{d}\\right)\\right)\\ |\\ \\pi_{n}(\\mathbf{a})\\in G^{N}\\left(\\mathbb{R}^{d}\\right)\\ \\mathrm{for}\\,\\mathrm{all}\\;n\\geq0\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition A.6. Let $p\\geq1$ and $\\mathbf{x}\\in\\Omega_{p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ . The signature of $\\mathbf{x}$ is the path $S(\\mathbf{x}):[0,T]\\mapsto G^{(*)}$ such that, for each $N\\geq0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nd S(\\mathbf{x})_{t}^{N}=S(\\mathbf{x})_{t}^{N}\\otimes\\diamond d\\mathbf{x}_{t},\\quad S(\\mathbf{x})_{0}^{N}=\\mathbf{1}\\in G^{N}\\left(\\mathbb{R}^{d}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Remark A.3. Uniqueness and existence of the signature follow from the continuous analog. Indeed, by definition, (9) is equivalent to a continuous linear RDE. ", "page_idx": 15}, {"type": "text", "text": "Remark A.4. The signature, as defined here, is also known as the minimal jump extension of $\\mathbf{x}$ and was first introduced in [15]. It was further explored in [13] where it was also shown that it acts as a universal feature map. ", "page_idx": 15}, {"type": "text", "text": "In the continuous case, it is well known that the signature characterizes paths up to tree-like equivalence. Two continuous paths $\\mathbf x,\\mathbf y$ are said to be tree-like equivalent if there exists a continuous non-negative map $h:[0,T]\\to\\mathbb{R}_{+}$ such that $h(0)=h(T)$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{s,t}-\\mathbf{y}_{s,t}\\|\\leq h(s)+h(t)-2\\operatorname*{inf}_{u\\in[s,t]}h(u).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This can be generalized to c\u00e0dl\u00e0g paths in the following way. We say that two c\u00e0dl\u00e0g paths $\\mathbf x,\\mathbf y$ are tree-like equivalent, or $\\textbf{x}\\sim_{t}\\textbf{y}$ , if their their Marcus interpolations (see Def. A.1), $\\hat{\\bf x}$ and $\\hat{\\mathbf{y}}$ , are tree-like equivalent. It is straightforward to check that this indeed is an equivalence relation on $\\Omega_{p}^{D}(\\mathbb{R}^{d})$ . Perhaps more interestingly, we obtain the following result. For ease of notation we shall henceforth mean $S(\\mathbf{x})_{T}$ when omitting the subscript from the signature. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1. Let $p\\geq1$ . The map $S(\\cdot):\\Omega_{p}^{D}(\\mathbb{R}^{d})\\to G^{(*)}$ is injective up to tree-like equivalence, i.e., $S(\\mathbf{x})=S(\\mathbf{y})$ iff $\\mathbf x\\sim_{t}\\mathbf y$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The result follows from the continuous case upon realizing that $S(\\mathbf{x})=S(\\hat{\\mathbf{x}})$ and analogously for y. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3 Young pairing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In many cases, given a geometric c\u00e0dl\u00e0g rough path $\\textbf{x}\\in~\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ with $p\\ \\in\\ [2,3)$ and a path $h\\in D_{1}([0,T],\\mathbb{R}^{e})$ of bounded variation one is interested in constructing a new rough path $\\textbf{y\\in}$ $\\Omega_{0,p}^{D}(\\mathbb{R}^{d+e})$ such that the first level of $\\mathbf{y}$ is given by $y=(x,h)$ . In the continuous case this can be done by using the level two information $\\mathbf{x}^{2}$ and $\\int d h\\otimes d h$ to fill in the corresponding terms in $\\mathbf{y}^{2}$ and using the well-defined Young cross-integrals to flil in the rest. The resulting level 2 rough path is called the Young pairing of $\\mathbf{x}$ and $h$ and we will denote it by $\\mathbf{y}=P(\\mathbf{x},h)$ . The canonical example to keep in the mind is when $h_{t}=t$ , that is, we want to augment the rough path with an added time coordinate (see Def. A.8). In the c\u00e0dl\u00e0g case one needs to be more careful in defining the appropriate Marcus lift. ", "page_idx": 16}, {"type": "text", "text": "Definition A.7 (Definition 3.21 [7]). Let $\\mathbf{x}\\in\\Omega_{0,p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ with $p\\geq1$ and $h\\in D_{1}([0,T],\\mathbb{R}^{e})$ . Define the path $\\mathbf{z}=(\\mathbf{x},h)$ and the corresponding Marcus lift $\\hat{\\mathbf{z}}=(\\hat{\\mathbf{x}},\\hat{h})$ . The Young pairing of x and $h$ is the $p$ -rough path $\\dot{P}(\\mathbf{x},h)\\in\\Omega_{p}^{D}(\\mathbb{R}^{d+e})$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\mathbf{x},h)=P(\\hat{\\mathbf{x}},\\hat{h})\\circ\\eta_{\\mathbf{z}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P(\\hat{\\mathbf{x}},\\hat{h})$ is the usual Young pairing of a continuous rough path and a continuous bounded variation path (see Def. 9.27 in $I I7I)$ ). ", "page_idx": 16}, {"type": "text", "text": "We can then construct the time augmented rough path as the rough path obtained by the Young pairing with the simple continuous bounded variation path $h_{t}=t$ . It turns out that this pairing is continuous as a map from $\\Omega_{0,p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ to $\\Omega_{0,p}^{D}(\\mathbb{R}^{d+1})$ . ", "page_idx": 16}, {"type": "text", "text": "Definition A.8. Let $\\mathbf{x}\\,\\in\\,\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ . The time augmented version of $\\mathbf{x}$ is the unique rough path $\\tilde{\\mathbf{x}}\\in\\Omega_{0,p}^{D}(\\mathbb{R}^{d+1})$ obtained by the Young pairing $P(\\mathbf{x},h)$ of x with the continuous bounded variation path $h_{t}=t$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition A.2. Let $p\\in[1,3)$ . Then, the map $\\mathbf{x}\\mapsto\\tilde{\\mathbf{x}}$ is continuous and injective as a map from $\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ to $\\Omega_{0,p}^{D}(\\mathbb{R}^{d+1})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\mathcal{X}=\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ be a metric space when equipped with $\\alpha_{p}$ . Fix $\\mathbf{x}\\in\\mathcal{X}$ and let $x^{n}$ be a sequence of absolutely continuous paths converging in $\\mathcal{X}$ to $\\mathbf{x}$ . We shall first show that $\\tilde{x}^{n}$ then converges to $\\tilde{\\bf x}$ . Since $x^{n}$ does not have any jumps and any reparameterisation of $x^{n}$ is still absolutely continuous, we may assume that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{p}({\\bf x},x^{n})=\\operatorname*{lim}_{\\delta\\to0}d_{p}(\\hat{{\\bf x}}^{\\delta},x^{n})\\to0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $n\\to\\infty$ . Define $\\mathbf{z}=(\\mathbf{x},h)$ and $\\hat{\\mathbf{z}}^{d}=(\\hat{\\mathbf{x}}^{\\delta},\\hat{h}^{\\delta})$ the Marcus interpolation with $\\eta_{\\mathbf{x},\\delta}$ the reparameterisation such that $\\mathbf{z}=\\hat{\\mathbf{z}}^{\\delta}\\circ\\eta_{\\mathbf{x},\\delta}$ . Furthermore, let $P(\\mathbf{x},h)$ be the Young pairing of $\\mathbf{x}$ and $h$ . By definition, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{p}\\left(P(\\mathbf{x},h),P(x^{n},h)\\right)=\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\sigma_{p}\\left(P(\\hat{\\mathbf{x}}^{\\delta},\\hat{h}^{\\delta}),P(x^{n},h)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{lim}_{\\delta\\to0}d_{p}\\left(P(\\hat{\\mathbf{x}}^{\\delta},\\hat{h}^{\\delta}),P(x^{n},h)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{lim}_{\\delta\\to0}C\\left(d_{p}(\\hat{\\mathbf{x}}^{\\delta},x^{n})+d_{1}(\\hat{h}^{\\delta},h)\\right)\\to0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $n\\to\\infty$ where $C$ is just some generic constant depending only on $p$ . The last inequality follows from 9.32 in [17]. Thus, if $\\mathbf y\\in\\mathcal X$ is such that $\\alpha_{p}(\\mathbf{x},\\mathbf{y})<\\epsilon$ , we can choose another sequence $y^{n}$ of absolutely continuous paths and $N\\geq1$ large enough so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{p}\\left(P(\\mathbf{x},h),P(\\mathbf{y},h)\\right)\\leq2\\epsilon+\\alpha_{p}(P(x^{n},h),P(y^{n},h)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{p}(x^{n},y^{n})\\leq2\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $n\\geq N$ . By Remark 3.6 in [7], we then have that, up to choosing a large $N,d_{p}(x^{n},y^{n})\\leq\\epsilon$ for all $n\\geq N$ and therefore, once more appealing to Theorem 9.32 in [17], ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{p}\\left(P(x^{n},h),P(y^{n},h)\\right)\\leq2C\\epsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In conclusion, $\\alpha_{p}\\left(P(\\mathbf{x},h),P(\\mathbf{y},h)\\right)\\leq2(1+C)\\epsilon$ which proves the result.   \nInjectivity follows from [13]. ", "page_idx": 17}, {"type": "text", "text": "A.4 Event RDEs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The results of Section 3.3 hold in more generality. In fact, we can define Event RDEs similar to Definition 3.1 where the inter-event dynamics are given by Marcus RDEs driven by c\u00e0dl\u00e0g rough paths. Utilizing the correspondence between solutions to Marcus RDEs and Marcus SDEs, it then follows that the results in the main body of the paper are a special case of the results given below. ", "page_idx": 17}, {"type": "text", "text": "Definition A.9 (Event RDE). Let $p\\geq1$ and $N\\in\\mathbb{N}$ be the number of events. Let $\\mathbf{x}\\in\\Omega_{p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ and $\\boldsymbol{f}=\\left(f_{1},\\ldots,f_{d}\\right)$ be a family of $\\mathrm{Lip^{\\gamma}}$ on $\\mathbb{R}^{e}$ with $\\gamma>p$ . Let $\\mathcal{E}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}$ and $\\mathcal{T}:\\mathbb{R}^{e}\\rightarrow\\mathbb{R}^{e}$ be an event and transition function respectively. We say that $(y,(\\tau_{n})_{n=1}^{N})$ is a solution to the Event $R D E$ parameterised by $(y_{0},\\mathbf{x},f,\\mathcal{E},\\mathcal{T},N)$ if $y_{T}=y_{T}^{N}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{t}=\\sum_{n=0}^{N}y_{t}^{n}\\mathbf{1}_{[\\tau_{n},\\tau_{n+1})}(t),\\quad\\tau_{n}=\\operatorname*{inf}\\left\\lbrace t>\\tau_{n-1}:\\mathcal{E}(y_{t-}^{n-1})=0\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\mathcal{E}(y_{\\tau_{n}}^{n})\\neq0$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d y_{t}^{0}=f(y_{t}^{0})\\diamond d\\mathbf{x}_{t},\\quad s t a r t e d\\,a t\\ \\,y_{0}^{0}=y_{0},}\\\\ &{d y_{t}^{n}=f(y_{t}^{n})\\diamond d\\mathbf{x}_{t},\\quad s t a r t e d\\,a t\\ \\,y_{\\tau_{n}}^{n}=\\mathcal{T}\\left(y_{\\tau_{n}-}^{n-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Existence and uniqueness of solutions to Event RDEs is proven in the same way as for Event SDEs. Indeed, under the usual assumption that the vector fields $f$ are $\\mathrm{Lip^{\\gamma}}$ , for $\\gamma>p$ , a unique solution to (11) exists. In fact, the solution map $y_{s}\\,\\times\\,(s,t)\\,\\mapsto\\,y_{t}$ is a diffeomorphism for every fixed $0\\leq s<t\\leq T$ (see, e.g., Theorem 3.13 in [7]). It follows that we can iteratively define a unique sequence of solutions $\\bar{y^{n}}\\in D_{p}([t_{n},T],\\mathbb{R}^{d})$ . Finally, as mentioned in Remark A.2, if the driving rough path $\\mathbf{x}$ is the Marcus lift of a semi-martingale, the inter-event solutions agree almost surely with the solutions to the corresponding Marcus SDE. ", "page_idx": 17}, {"type": "text", "text": "Theorem A.2. Under Assumptions 3.1-3.2, there exists a unique solution $(y,(\\tau_{n})_{n=1}^{N})$ to the Event RDE of Definition A.9. Furthermore, if x is the Marcus lift of a Brownian motion, the solution coincides almost surely with the solution to the corresponding Event SDE as given in Def. 3.1. ", "page_idx": 17}, {"type": "text", "text": "Hence, the Event SDEs considered in the main text are special cases of Event RDEs driven by the Marcus lift of a Brownian motion. Yet, the more general formulation of Event RDEs allows to treat, using the same mathematical machinery of rough path theory a much larger family of driving noises such as fractional Brownian motion or even smooth controls. Also, since the driving rough path is allowed to be c\u00e0dl\u00e0g, the model class given by Def. A.9 includes cases where the inter-event dynamics are given by Marcus SDEs driven by general semi-martingales. ", "page_idx": 17}, {"type": "text", "text": "B Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proof of Theorem 3.2 presented below covers the case where $(y,(\\tau_{n})_{n=1}^{N})$ is the solution to an Event RDE. Throughout we consider vector fields $\\mu\\in\\mathrm{Lip}^{1},\\sigma\\in\\mathrm{Lip}^{2+\\dot{\\epsilon}}$ and specialise to Event RDEs where the inter-event dynamics are given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nd y_{t}^{n}=\\mu(y_{t}^{n})d t+\\sigma(y_{t}^{n})\\diamond d\\mathbf{x}_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{x}\\in\\Omega_{p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ . The notation above deserves some clarification. One can define the vector field $f=(\\mu,\\sigma)$ and the Young pairing $\\tilde{{\\bf x}}_{t}$ of $\\mathbf{x}$ and $h_{t}=t$ . Assuming $\\mu\\in\\mathrm{Lip}^{2+\\epsilon}$ we can then view $y_{t}^{n}$ as the unique solution to the Marcus RDE ", "page_idx": 17}, {"type": "equation", "text": "$$\nd y_{t}^{n}=f(y_{t}^{n})\\diamond\\tilde{\\mathbf{x}}_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Alternatively, if one is not ready to impose the added regularity on the drift $\\mu$ , one can view 13 as a RDE with drift as in Ch. 12 in [17]. To accommodate this more general case where the path driving the diffusion term might be 1) c\u00e0dl\u00e0g and 2) is not restricted to be the rough path lift of a semi-martingale, we shall need the following two additional assumptions: ", "page_idx": 18}, {"type": "text", "text": "Assumption B.1. For any $n\\in[N]$ , there exists a non-empty interval $I_{n}=(\\tau_{n}-\\delta_{n},\\tau_{n}+\\delta_{n})$ such that $\\mathbf{x}$ is continuous over $I_{n}$ . In other words, the c\u00e0dl\u00e0g rough path $\\mathbf{x}$ , does not jump in small intervals around the event times $\\left(\\tau_{n}\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "Assumption B.2. For all $0\\leq n\\leq N$ we define $s_{n}=\\tau_{n}-\\delta_{n}/2$ and $t_{n}=\\tau_{n+1}+\\delta_{n+1}/2$ . It holds that $\\mathbf{x}\\in\\Omega_{0,p}^{D}([s_{n},t_{n}],\\mathbb{R}^{d})$ , i.e., $\\mathbf{x}$ is a geometric $p$ -rough path on the intervals $[s_{n},t_{n}]$ . ", "page_idx": 18}, {"type": "text", "text": "Remark B.1. Note that Assumption B.1 trivially holds if $\\mathbf{x}$ is continuous. Otherwise, it is enough to assume, e.g., that $\\mathbf{x}$ is the Marcus lift of a finite activity L\u00e8vy process. Furthermore, by the properties of the metric $\\alpha_{p}$ , if $\\mathbf{x}$ is the canonical Marcus lift of a semi-martingale $x\\in D_{p}([s,\\^t],\\mathbb{R}^{\\vec{d}-1})$ , then there exists a sequence $(x^{m})$ of piece-wise linear paths $x^{m}\\in C_{0}^{1}([0,T],\\mathbb{R}^{d-1})$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{p,[s_{n},t_{n}]}(x^{m},\\mathbf{x})\\to0\\quad{\\mathrm{as~}}\\ m\\to\\infty\\quad{\\mathrm{a.s.}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "See, e.g. [7, Example 4.21]. The setting of Section 3.3 is therefore a special case of the setting considered here and Theorem 3.2 follows from the proof below. ", "page_idx": 18}, {"type": "text", "text": "We shall need two technical lemmas for the proof of 3.2 ", "page_idx": 18}, {"type": "text", "text": "Lemma B.1. Assume that Assumptions 3.1-3.5 and B.1-B.2 are satisfied. Then, there exists an open ball $B_{0}\\subset O$ such that the following holds: ", "page_idx": 18}, {"type": "text", "text": "2. For any $n\\in[N]$ , the maps ", "page_idx": 18}, {"type": "equation", "text": "$$\nB_{0}\\ni a\\mapsto\\Big(\\tau_{n}(a),y_{\\tau_{n}(a)}^{n-1}(a)\\Big)\\quad a r e\\;c o n t i n u o u s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. For the sequence $(x^{m})$ as given in Assumption $B.2$ and $(y^{m},(\\tau_{n}^{m})_{n=1}^{N})$ the corresponding Event RDE solution, for all $n\\in[N]$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\operatorname*{sup}_{a\\in B_{0}}\\left(|\\tau_{n}^{m}(a)-\\tau_{n}(a)|+\\left|y_{\\tau_{n}^{m}(a)}^{m,n-1}(a)-y_{\\tau_{n}(a)}^{n-1}(a)\\right|\\right)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Recall that $\\Phi(y,s,t;\\mathbf{x})$ is the solution map or flow of the differential equation ", "page_idx": 18}, {"type": "equation", "text": "$$\nd y_{u}=f(y_{u})\\diamond d\\tilde{\\mathbf{x}}_{u},\\quad y_{s}=y\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "evaluated at time $t$ . The first step will be to prove continuity at $y_{0}$ . In particular, let $y_{0}^{m}\\;\\in\\;{\\cal O}$ approach $y_{0}$ for $m$ going to infinity and denote the solutions to the corresponding Event RDEs by $\\left(y^{m},(\\tau_{n}^{m})_{n=1}^{N_{m}}\\right)$ . We claim that $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}N_{m}=N}\\end{array}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\tau_{n}^{m}=\\tau_{n},\\quad\\operatorname*{lim}_{m\\to\\infty}y_{\\tau_{n}^{m}}^{m,n-1}=y_{\\tau_{n}}^{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To see this, note that, by Theorem A.1, there exists a sequence $\\lambda_{m}\\in\\Lambda$ of continuous reparameterisations such that $|\\lambda_{m}|\\to0$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(s,t)\\in\\Delta_{T}}|\\Phi(y_{0},s,t;\\mathbf{x})-\\Phi(y_{0}^{m},\\lambda^{m}(s),\\lambda^{m}(t);\\mathbf{x})|\\to0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $m\\rightarrow\\infty$ . Note, furthermore, that $\\Phi(y_{0}^{m},s,t;{\\mathbf x}\\circ\\lambda_{m})=\\Phi(y_{0}^{m},\\lambda_{m}(s),\\lambda_{m}(t);{\\mathbf x})$ for all $(s,t)\\in$ $\\Delta_{T}$ . We let $\\left(\\tilde{y}^{m},(\\tilde{\\tau}_{n}^{m})_{n=1}^{N_{m}}\\right)$ be the solution to the Event RDE where $(y_{0},\\mathbf x)$ is replaced by $(y_{0}^{m},\\mathbf{x}\\circ$ ${\\lambda_{m}},$ ). It suffices to prove that, for all $1\\leq n\\leq N$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\tilde{\\tau}_{n}^{m}=\\tau_{n},\\quad\\operatorname*{lim}_{m\\to\\infty}\\tilde{y}_{\\tilde{\\tau}_{n}^{m}}^{m,n-1}=y_{\\tau_{n}}^{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Indeed, since $\\tilde{\\tau}_{n}^{m}=\\lambda_{m}^{-1}(\\tau_{n}^{m})$ and $|\\lambda_{m}|\\to0$ , it then follows that $\\tau_{n}^{m}\\to\\tau_{n}$ for $m\\rightarrow\\infty$ . Furthermore, we have y\u02dc\u03c4\u02dcm m,n $\\tilde{y}_{\\tilde{\\tau}_{n}^{m}}^{m,n-1}=y_{\\tau_{n}^{m}}^{m,n-1}$ ", "page_idx": 18}, {"type": "text", "text": "We shall proof (15) using an inductive argument. We have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{t}^{0}=\\Phi(y_{0},0,t;\\mathbf{x}),\\quad\\forall t\\in[0,\\tau_{1}],}\\\\ &{\\tilde{y}_{t}^{m,0}=\\Phi(y_{0}^{m},0,t;\\mathbf{x}\\circ\\lambda_{m}),\\quad\\forall t\\in[0,\\tilde{\\tau}_{1}^{m}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now fix some $0\\,<\\,\\epsilon\\,<\\,\\delta_{1}$ where $\\delta_{1}$ is given in Assumption B.1. Note that $|\\mathcal{E}(y_{t}^{0})|\\,>\\,0$ for all $t\\in[0,\\tau_{1}-\\epsilon]$ and therefore, by (14), it follows that there exists an $m_{0}\\in\\mathbb{N}$ such that, for all $m\\geq m_{0}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t\\in[0,\\tau_{1}-\\epsilon]}|\\mathcal{E}(\\Phi(y_{0}^{m},0,t;\\mathbf{x}\\circ\\lambda_{m}))|>0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so that $\\tilde{\\tau}_{1}^{m}\\ge\\tau_{1}-\\epsilon$ . Next, for some small $0<\\eta<\\epsilon$ , Assumption 3.4 and the Mean Value Theorem imply th1e  e\u2265xiste n\u2212ce of $a_{\\eta}^{+}\\,=\\,r_{\\eta}^{+}y_{\\tau_{1}}^{0}\\,-\\,(1\\,-\\,r_{\\eta}^{\\dot{+}})y_{\\tau_{1}+\\eta}^{0}.$ , and $a_{\\eta}^{-}\\,=\\,r_{\\eta}^{-}y_{\\tau_{1}-\\eta}^{0}+(1-r_{\\eta}^{-})y_{\\tau_{1}}^{0}$ with $r_{\\eta}^{+},r_{\\eta}^{-}\\in(0,1)$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}\\left(y_{\\tau_{1}+\\eta}^{0}\\right)=\\mathcal{E}\\left(y_{\\tau_{1}}^{0}\\right)+\\nabla\\mathcal{E}(a_{\\eta}^{+})\\int_{\\tau_{1}}^{\\tau_{1}+\\eta}\\mu(y_{s}^{0})d y_{s},}\\\\ &{\\mathcal{E}\\left(y_{\\tau_{1}-\\eta}^{0}\\right)=\\mathcal{E}\\left(y_{\\tau_{1}}^{0}\\right)-\\nabla\\mathcal{E}(a_{\\eta}^{-})\\int_{\\tau_{1}-\\eta}^{\\tau_{1}}\\mu(y_{s}^{0})d y_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "But then, by Assumption 3.5 and the fact that $\\mathcal{E}(y_{\\tau_{1}}^{0})=0$ , for $\\eta$ small enough, $\\mathcal{E}(y_{\\tau_{1}+\\eta}^{0})$ and $\\mathcal{E}(y_{\\tau_{1}-\\eta}^{0})$ must lie on different sides of 0. Assumption B.1 and eq. (14) then yield the existence of a $m_{1}\\geq m_{0}$ such that $\\tilde{\\tau}_{1}^{m}\\,\\leq\\,\\tau_{1}+\\eta\\,\\leq\\,\\tau_{1}+\\epsilon$ and $\\mathrm{inf}_{t\\in[0,\\tau_{1}+\\eta]}\\,|\\mathcal{E}(\\tilde{y}_{t}^{m,0})|>0$ for all $m\\geq m_{1}$ . It follows that $\\tilde{\\tau}_{1}^{m}\\to\\tau_{1}$ . Finally, note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|{\\tilde{y}}_{\\tilde{\\tau}_{1}^{m}}^{m,0}-y_{\\tau_{1}}^{0}\\right|\\leq\\left|{\\tilde{y}}_{\\tilde{\\tau}_{1}^{m}}^{m,0}-y_{\\tilde{\\tau}_{1}^{m}}^{0}\\right|+\\left|y_{\\tilde{\\tau}_{1}^{m}}^{0}-y_{\\tau_{1}}^{0}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Another application of (14) shows that the first term on the right hand side goes to 0 for $m\\rightarrow\\infty$ and second term vanishes by Assumption B.1. ", "page_idx": 19}, {"type": "text", "text": "To prove the inductive step, assume that (15) holds for $i\\leq n$ . For all $t\\in[\\tau_{n},\\tau_{n+1}]$ it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\tilde{y}}_{t}^{m,n}=\\Phi\\left({\\mathcal{T}}\\left({\\tilde{y}}_{\\tilde{\\tau}_{n}^{m}}^{m,n-1}\\right),{\\tilde{\\tau}}_{n}^{m},t;\\mathbf{x}\\circ\\lambda_{m}\\right),\\quad y_{t}^{n}=\\Phi\\left({\\mathcal{T}}\\left(y_{\\tau_{n}}^{n-1}\\right),\\tau_{n},t;\\mathbf{x}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and, since $\\tilde{y}_{\\tilde{\\tau}_{n}^{m}}^{m,n-1}\\to y_{\\tau_{n}}^{n-1}$ , $\\tilde{\\tau}_{n}^{m}\\to\\tau_{n}$ , and $\\tau$ is continuous, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\operatorname*{sup}_{t\\in\\left[\\tau_{n},T\\right]}\\left|\\Phi\\left(\\mathcal{T}\\left(\\tilde{y}_{\\tilde{\\tau}_{n}^{m}}^{m,n-1}\\right),\\tilde{\\tau}_{n}^{m},t;\\mathbf{x}\\circ\\lambda_{m}\\right)-\\Phi\\left(\\mathcal{T}\\left(y_{\\tau_{n}}^{n-1}\\right),\\tau_{n},t;\\mathbf{x}\\right)\\right|=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "whence the same argument as above proves that (15) also holds for $n+1$ . This completes the proof of the claim. ", "page_idx": 19}, {"type": "text", "text": "Now, by continuity at $y_{0}$ , it follows that there exists some small $r>0$ such that for all $a\\in B_{r}(y_{0})$ it holds that $|\\tau(a)|\\,=\\,N$ and $\\tau_{n}(a)\\,\\in\\,(\\tau_{n}-\\delta_{n}/2,\\tau_{n}+\\delta_{n}/2)$ for all $n\\,\\in\\,[N]$ where $\\delta_{n}$ is as in Assumption B.1. Furthermore, since Assumption 3.1-3.5 and B.1-B.2 still hold for $a\\in B_{r}(y_{0})$ , the same argument as above can be applied to show that $\\tau_{n}(a)$ and $\\boldsymbol{y}_{\\tau_{n}(a)}^{n-1}(a)$ are continuous at $a$ . This proves parts 1 and 2. ", "page_idx": 19}, {"type": "text", "text": "To prove part 3 we employ a similar induction argument to the one above. First, note that, by Theorem A.1, there exists a constant $C>0$ not depending on $x$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha_{p,[0,t_{0}]}\\left(y^{m,0}(a),y^{0}(a)\\right)\\leq C\\alpha_{p,[0,t_{0}]}\\left(x^{m},\\mathbf{x}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the latter term does not depend on $y$ and goes to 0 for $m$ going to infinity, we find that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\operatorname*{sup}_{a\\in B_{r}(y_{0})}\\alpha_{p,[0,t_{1}]}\\left(y^{m,0}(a),y^{0}(a)\\right)=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall, $y^{\\delta,0}(a)$ is the continuous path obtained by the Marcus interpolation with $\\delta r_{k}$ instead of $r_{k}$ and similarly for $y^{m,\\delta,0}(a)$ . Note that $y^{m,\\delta,0}(a)=y^{m,0}(a)$ by continuity. Letting $\\tau_{1}^{m}(a)$ and $\\tau_{1}^{\\delta}(a)$ denote the first event time of $y^{m,0}(a)$ and $y^{\\delta,0}(a)$ respectively, we have, for all $m\\in\\mathbb{N}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{a\\in B_{r}(x_{0})}|\\tau_{1}^{m}(a)-\\tau_{1}(a)|\\leq\\operatorname*{sup}_{a\\in B_{r}(y_{0})}\\operatorname*{lim}_{\\delta\\to0}\\left(\\left|\\tau_{1}^{m}(a)-\\tau_{1}^{\\delta}(a)\\right|+\\left|\\tau_{1}^{\\delta}(a)-\\tau_{1}(a)\\right|\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, let $B_{0}=B_{r}(y_{0})$ . Since $\\tau_{1}(a)\\in(\\tau_{1}-\\delta_{1}/2,\\tau_{1}+\\delta_{1}/2)$ for all $a\\in B_{0}$ and $\\mathbf{x}$ is continuous over this interval, it follows that $\\left|\\tau_{1}^{\\delta}(a)-\\tau_{1}(a)\\right|$ goes to 0 as $\\delta\\rightarrow0$ for each $a\\in B_{0}$ . Furthermore, by definition of the metric $\\alpha_{p}$ , eq. (16), and the fact that $y_{0}^{m,0}(a)=a=y_{0}^{0}(a)$ , for each $a\\in B_{0}$ , a similar argument as the one employed in the beginning of the proof then shows that $|\\tau_{1}^{m}(a)-\\tau_{1}^{\\delta}(a)|\\rightarrow0$ as $\\delta\\rightarrow0$ and, thus, $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\operatorname*{sup}_{a\\in B_{0}}|\\tau_{1}^{m}(a)-\\tau_{1}(a)|=0}\\end{array}$ . Finally, starting from the inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|y_{\\tau_{1}^{m}(a)}^{m,0}(a)-y_{\\tau_{1}(a)}^{0}(a)\\right|\\leq\\left|y_{\\tau_{1}^{m}(a)}^{m,0}(x)-y_{\\tau_{1}^{\\delta}(a)}^{\\delta,0}(a)\\right|+\\left|y_{\\tau_{1}^{\\delta}(a)}^{\\delta,0}(a)-y_{\\tau_{1}(a)}^{0}(a)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and taking the limit as $\\delta\\rightarrow0$ and then the supremum over $x\\in B_{0}$ on both sides, we can argue in exactly the same way to show that part 3 holds for $n=1$ . We can then argue by induction, just as in the first part of the proof, to show that it holds for all subsequent event times as well. Thus, the set $B_{0}$ satisfies all the stated requirements. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma B.2. Let Assumption $B.1$ hold and $x^{m}$ be as in Assumption B.2. Then, for all $n\\in[N]$ and $p^{\\prime}>p_{\\!}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}d_{p^{\\prime},[s,t]}\\left(x^{m},\\mathbf{x}\\right)=0,\\ \\ \\,f o r\\,a n y\\ \\,\\tau_{n}-\\delta_{n}/2\\leq s<t\\leq\\tau_{n}+\\delta_{n}/2.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Fix some $n\\in[N]$ , $p^{\\prime}>p$ and $\\tau_{n}-\\delta_{n}/2\\leq s<t\\leq\\tau_{n}+\\delta_{n}/2.$ . Note that, for any continuous reparameterisation $\\lambda\\in\\Lambda$ , $m\\in\\mathbb{N}$ , and $\\delta>0$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\nd_{p^{\\prime},[s,t]}(x^{m},\\mathbf{x})\\leq d_{p^{\\prime},[s,t]}(x^{m},x^{m}\\circ\\lambda)+d_{p^{\\prime},[s_{n},t_{n}]}(x^{m}\\circ\\lambda,\\hat{\\mathbf{x}}^{\\delta})+d_{p^{\\prime},[s,t]}(\\hat{\\mathbf{x}}^{\\delta},\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}^{\\delta}$ is the Marcus interpolation of $\\mathbf{x}$ over the interval $[s_{n},t_{n}]$ . Taking the infimum over $\\lambda\\in\\Lambda$ and the limit as $\\delta\\rightarrow0$ on both sides, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\nd_{p^{\\prime},[s,t]}(x^{m},\\mathbf{x})\\leq\\alpha_{p^{\\prime},[s_{n},t_{n}]}(x^{m},\\mathbf{x})+\\operatorname*{lim}_{\\delta\\to0}d_{p^{\\prime},[s,t]}(\\hat{\\mathbf{x}}^{\\delta},\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first term on the right hand side goes to 0 as $m\\rightarrow\\infty$ by Assumption B.2. Furthermore, since, by Assumption B.1, $\\mathbf{x}$ is continuous on $(\\tau_{n}-\\delta_{n},\\tau_{n}+\\delta_{n})$ , it follows that $d_{\\infty,[s,t]}(\\hat{\\mathbf{x}}^{\\delta},\\mathbf{x})$ goes to 0 for $\\delta\\rightarrow\\infty$ . But the result then follows from Proposition 8.15 and Lemma 8.16 in [17]. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3.2. Step 1: Assume that $x\\ \\in\\ C_{1}([0,T],\\mathbb{R}^{d-1})$ . By [17, Theorem 4.4], the Jacobian $\\partial y_{t}^{0}$ exists and satisfies (7) for all $t\\,\\in\\,[0,\\tau_{1})$ . We shall prove that relations (6) and (7) hold for all $n\\in[N]$ by induction. Thus, assume that $\\partial y_{t}^{k}$ and $\\partial\\tau_{k}$ exist for all $t\\in[\\tau_{k},\\tau_{k+1})$ and $k\\leq n-1$ and satisfy the stated relations. To emphasise the dependence on the initial condition, we will sometimes use the notation $y^{n}=y^{n}(y_{0})$ and $\\tau_{n}=\\tau_{n}(y_{0})$ for the solution of the Event RDE started at $y_{0}$ . We want to show that, for arbitrary $h\\in\\mathbb{R}^{e}$ , the following limits ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\frac{\\tau_{n}^{\\epsilon}-\\tau_{n}}{\\epsilon}\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{\\epsilon\\to0}\\frac{y_{t}^{n,\\epsilon}-y_{t}^{n}}{\\epsilon}\\quad\\mathrm{for}\\,t\\in[\\tau_{n},\\tau_{n+1})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "exist and satisfy the stated expressions, where $\\tau_{n}^{\\epsilon}=\\tau_{n}(y_{0}+h\\epsilon)$ and $y^{n,\\epsilon}=y^{n}(y_{0}+h\\epsilon)$ . ", "page_idx": 20}, {"type": "text", "text": "For any $\\epsilon>0$ , because $\\mathcal{E}$ is continuously differentiable, the Mean Value Theorem implies that there exists $c_{\\epsilon}\\,\\in\\,\\mathbb{R}^{e}$ on the line connecting $y_{\\tau_{n}}^{n-1}$ to $y_{\\tau_{n}^{\\epsilon}}^{n-1}$ and another $c_{\\epsilon}^{\\prime}\\in\\mathbb{R}^{e}$ on the line connecting $y_{\\tau_{n}^{\\epsilon}}^{n-1,\\epsilon}$ to $y_{\\tau_{n}^{\\epsilon}}^{n-1}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}\\left(y_{\\tau_{n}}^{n-1}\\right)=\\mathcal{E}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)+\\nabla\\mathcal{E}(c_{\\epsilon})\\left(y_{\\tau_{n}}^{n-1}-y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)}\\\\ &{\\qquad\\qquad=\\mathcal{E}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)+\\nabla\\mathcal{E}\\left(c_{\\epsilon}\\right)\\left(\\mu(y_{\\tau_{n}}^{n-1})(\\tau_{n}-\\tau_{n}^{\\epsilon})+\\sigma\\left(y_{\\tau_{n}}^{n-1}\\right)(x_{\\tau_{n}}-x_{\\tau_{n}^{\\epsilon}})+o(|\\tau_{n}-\\tau_{n}^{\\epsilon}|)\\right),}\\\\ &{\\qquad\\qquad:\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1,\\epsilon}\\right)=\\mathcal{E}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)+\\nabla\\mathcal{E}(c_{\\epsilon}^{\\prime})\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1,\\epsilon}-y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)}\\\\ &{\\qquad\\qquad=\\mathcal{E}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)+\\nabla\\mathcal{E}(c_{\\epsilon}^{\\prime})\\left(\\epsilon\\left(\\partial y_{\\tau_{n}}^{n-1}\\right)h+o(\\epsilon)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last equality follows from the induction hypothesis. We have $\\mathcal{E}(y_{\\tau_{n}}^{n-1})=0=\\mathcal{E}(y_{\\tau_{n}^{\\epsilon}}^{n-1,\\epsilon})$ . Thus, by rearranging, we find that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tau_{n}^{\\epsilon}-\\tau_{n}}{\\epsilon}=-\\frac{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\partial y_{\\tau_{n}}^{n-1}h}{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\left(\\mu(y_{\\tau_{n}}^{n-1})+\\sigma(y_{\\tau_{n}}^{n-1})\\frac{x_{\\tau_{n}}-x_{\\tau_{n}^{\\epsilon}}}{\\tau_{n}-\\tau_{n}^{\\epsilon}}\\right)}+o(1)}\\\\ &{\\qquad\\qquad=-\\frac{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\partial y_{\\tau_{n}}^{n-1}h}{\\nabla\\mathcal{E}(y_{\\tau_{n}}^{n-1})\\mu(y_{\\tau_{n}}^{n-1})}+o(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second equality follows from Assumptions 3.4 and 3.5. ", "page_idx": 21}, {"type": "text", "text": "Assume for now that $\\tau_{n}^{\\epsilon}<\\tau_{n}$ . By another application of the Mean Value Theorem, there exists $c_{\\epsilon}\\in\\mathbb{R}^{e}$ on the line connecting $y_{\\tau_{n}}^{n-1}$ to $y_{\\tau_{n}^{\\epsilon}}^{n-1}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\tau_{n}}^{n,\\epsilon}-y_{\\tau_{n}}^{n}=y_{\\tau_{n}}^{n,\\epsilon}-\\mathcal{T}\\left(y_{\\tau_{n}}^{n-1}\\right)}\\\\ &{\\quad\\quad\\quad=y_{\\tau_{n}}^{n,\\epsilon}-\\mathcal{T}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)-\\nabla\\mathcal{T}(c_{\\epsilon})(y_{\\tau_{n}}^{n-1}-y_{\\tau_{n}^{\\epsilon}}^{n-1})}\\\\ &{\\quad\\quad\\quad=y_{\\tau_{n}^{\\epsilon}}^{n,\\epsilon}+\\mu(y_{\\tau_{n}}^{n,\\epsilon})(\\tau_{n}-\\tau_{n}^{\\epsilon})+\\sigma\\left(y_{\\tau_{n}}^{n,\\epsilon}\\right)(x_{\\tau_{n}}-x_{\\tau_{n}^{\\epsilon}})-\\mathcal{T}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)}\\\\ &{\\quad\\quad\\quad\\quad-\\nabla\\mathcal{T}(c_{\\epsilon})\\left(\\mu(y_{\\tau_{n}}^{n-1})(\\tau_{n}-\\tau_{n}^{\\epsilon})+\\sigma\\left(y_{\\tau_{n}}^{n-1}\\right)(x_{\\tau_{n}}-x_{\\tau_{n}^{\\epsilon}})+o(\\lvert\\tau_{n}-\\tau_{n}^{\\epsilon}\\rvert)\\right)}\\\\ &{\\quad\\quad\\quad=\\mathcal{T}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1,\\epsilon}\\right)-\\mathcal{T}\\left(y_{\\tau_{n}^{\\epsilon}}^{n-1}\\right)+\\left(\\mu(y_{\\tau_{n}}^{n,\\epsilon})-\\nabla\\mathcal{T}(c_{\\epsilon})\\mu(y_{\\tau_{n}^{\\epsilon}}^{n-1})\\right)\\left(\\tau_{n}-\\tau_{n}^{\\epsilon}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left(\\sigma(y_{\\tau_{n}}^{n})-\\nabla\\mathcal{T}(c_{\\epsilon})\\sigma(y_{\\tau_{n}}^{n-1})\\right)\\left(x_{\\tau_{n}}-x_{\\tau_{n}^{\\epsilon}}\\right)+o(\\lvert\\tau_{n}-\\tau_{n}^{\\epsilon}\\rvert)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{y_{\\tau_{n}}^{n,\\epsilon}-y_{\\tau_{n}}^{n}}{\\epsilon}=\\nabla{\\mathcal{T}}(y_{\\tau_{n}}^{n-1})\\partial y_{\\tau_{n}}^{n-1}h+\\left(\\mu(y_{\\tau_{n}}^{n})-\\nabla{\\mathcal{T}}(y_{\\tau_{n}}^{n-1})\\mu(y_{\\tau_{n}}^{n-1})\\right)\\partial\\tau_{n}h+o(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used Assumption 3.3, the chain rule and the existence of $\\partial\\tau_{n}$ . Finally, for any $t\\in(\\tau_{n},\\tau_{n+1}]$ , equation (7) follows from the fact that we can write $y_{t}^{n}\\,=\\,\\Phi(y_{s}^{n},s,t,x)$ for all $\\tau_{n}\\leq\\ s\\,<\\,t$ . In particular, by the chain rule, we find that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\partial y_{t}^{n}=\\left[\\partial_{y_{s}^{n}}\\Phi(y_{s}^{n},s,t)\\partial y_{s}^{n}\\right]_{s=\\tau_{n}}=\\partial_{y_{\\tau_{n}}^{n}}y_{t}^{n}\\left[\\partial y_{s}^{n}\\right]_{s=\\tau_{n}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 2: Consider now the general case of $\\mathbf{x}\\in\\Omega_{p}(\\mathbb{R}^{e})$ and let $(y^{m},(\\tau_{n_{m}}^{m})_{n_{m}}^{N_{m}})$ denote the solution to the Event RDE where $\\mathbf{x}$ is replaced by the piece-wise linear approximation $x^{m}$ . With $\\partial y_{t}^{n,m}$ and $\\partial\\tau_{n}^{m}$ denoting the corresponding derivatives, we saw in the previous step that both exist and satisfy (6)-(7). We let $R_{t}^{n}$ and $\\rho_{n}$ denote the right hand side of (7) and (6) respectively. This step consists of proving that, for $n\\in[N]$ and $t\\in(\\tau_{n},t_{n})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\left\\{|\\tau_{n}^{m}-\\tau_{n}|+|y_{t}^{m,n}-y_{t}^{n}|\\right\\}=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and for some open ball $B_{0}$ around $y_{0}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\operatorname*{sup}_{a\\in B_{0}}\\left\\{|\\partial\\tau_{n}^{m}(a)-\\rho_{n}(a)|+\\|\\partial y_{t}^{m,n}(a)-R_{t}^{n}(a)\\|\\right\\}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma B.1 and continuity of $\\tau$ we have that $\\tau(y_{\\tau_{n}^{m}}^{m,n-1})$ converges to $\\mathcal{T}(y_{\\tau_{n}}^{n-1})$ and $\\tau_{n}^{m}$ converges to $\\tau_{n}$ as $m\\ \\rightarrow\\ +\\infty$ . Then, because $y_{t}^{n}\\;\\;\\stackrel{\\cdot\\cdot}{=}\\;\\;\\Phi(T(y_{\\tau_{n}}^{n-1}),\\tau_{n},t;{\\bf x})$ and $y_{t}^{m,n}=$ \u03a6(T (y\u03c4m nm,n\u22121), $\\Phi({\\mathcal T}(y_{\\tau_{n}^{m}}^{m,n-1}),\\tau_{n}^{m},t;x^{m})$ , equation (17) follows from, Lemma B.2 and Corollary 11.16 in [17]. In fact, since $B_{0}$ was constructed in Lemma B.1 in such a way that $\\tau_{n}(a)<t_{n}$ for all $a\\in B_{0}$ we also get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\operatorname*{sup}_{a\\in B_{0}}\\left\\|\\partial_{y_{\\tau_{n}(a)}^{n}(a)}\\Phi\\left(y_{\\tau_{n}(a)}^{n}(a),\\tau_{n}(a),t;\\mathbf{x}\\right)-\\partial_{y_{\\tau_{n}^{m}(a)}^{m,n}(a)}\\Phi\\left(y_{\\tau_{n}^{m}(a)}^{m,n}(a),\\tau_{n}^{m}(a),t;x^{m}\\right)\\right\\|=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by the same corollary in [17]. Thus, to prove (18), it suffices to show that, for all $n\\in\\{1,...,N\\}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\operatorname*{sup}_{a\\in B_{0}}\\left\\|\\partial y_{\\tau_{n}^{m}(a)}^{m,n-1}(a)-R_{\\tau_{n}(a)}^{n-1}(a)\\right\\|=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We shall prove it using another inductive argument starting with $n=1$ . In this case it suffices to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname*{sup}_{a\\in B_{0}}\\|\\partial_{a}\\Phi\\left(a,0,\\tau_{1}(a);\\mathbf{x}\\right)-\\partial_{a}\\Phi\\left(a,0,\\tau_{1}^{m}(a);x^{m}\\right)\\|=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By [7, Theorem 3.3] we know that the above holds if $\\tau_{1}^{m}(a)$ and $\\tau_{1}(a)$ are replaced by $\\tau_{1}+\\delta_{1}/2$ . Now let $\\Phi^{-1}$ be the reverse of the flow map $\\Phi$ , that is, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi^{-1}(a_{1},s,t;{\\bf x})=a_{0}\\Leftrightarrow\\Phi(a_{0},s,t;{\\bf x})=a_{1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Lemma B.1 it follows that $y_{\\tau_{1}(a)}^{0}(a)\\,=\\,\\Phi^{-1}(y_{t_{0}}^{0}(a),\\tau_{1}(a),t_{0};{\\bf x})$ and, for $m$ large enough, $y_{\\tau_{1}^{m}(a)}^{m,0}(a)=\\Phi^{-1}(y_{t_{0}}^{m,0}(a),\\tau_{1}^{m}(a),t_{0};x^{m})$ . But the result then follows from Lemma B.2 and [17, Corollary 11.16]. To prove the inductive step, assume that (18) holds for all $i\\leq n-1$ . Again, by inspecting (6) and (7) and using the inductive assumption, one finds that it is enough to show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\operatorname*{sup}_{a\\in B_{0}}\\left\\|\\partial_{y_{\\tau_{n-1}}^{n-1}}\\Phi\\left(y_{\\tau_{n-1}}^{n-1},\\tau_{n-1},\\tau_{n};\\mathbf{x}\\right)-\\partial_{y_{\\tau_{n-1}^{m}}^{m,n-1}}\\Phi\\left(y_{\\tau_{n-1}^{m}}^{m,n-1},\\tau_{n-1}^{m},\\tau_{n}^{m};x^{m}\\right)\\right\\|=0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we suppressed the dependence on $a$ for notational simplicity. This is done exactly as for $y^{0}$ and completes the proof of Step 2. ", "page_idx": 22}, {"type": "text", "text": "Step 3: The third and final step is to combine Step 1 and 2 to finish the proof. So far we have proven that 1) the theorem holds for continuous paths of bounded variation and 2) $(\\tau_{n}^{m},y_{t}^{m,n})$ converges to $(\\tau_{n},y_{t}^{n})$ and $(\\partial\\tau_{n}^{m}(a),\\partial y_{t}^{m,n}(a))$ converges uniformly to $(\\rho_{n}(a),R_{t}^{n}(a))$ over $a\\in B_{0}$ for all $t\\in(\\tau_{n},t_{n})$ and $n\\in[N]$ . From these results it immediately follows that $(\\tau_{n}(a),y_{t}^{n}(a))$ is differentiable at $a=y_{0}$ with derivatives given by $(\\rho_{n}(y_{0}),R_{t}(y_{0}))$ for all $t\\in(\\tau_{n},\\dot{t}_{n})$ . What is left to show then, is that this also holds for all other $t$ . But this follows immediately from the chain rule upon realizing that, for any $\\tau_{n}<s<\\tau_{n}+\\delta_{n}/2<t<\\tau_{n+1}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\ny_{t}^{n}=\\Phi(y_{s}^{n},s,t;\\mathbf{x})\\Rightarrow\\partial y_{t}^{n}=\\partial_{y_{s}^{n}}\\Phi(y_{s}^{n},s,t;\\mathbf{x})R_{s}^{n}(y_{0})=R_{t}^{n}(y_{0}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C Kernel methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We give here a brief outline of some of the most central concepts related to kernel methods. For a more in-depth introduction we refer the reader to [45, 55, 3]. Let $\\mathcal{X}$ be a topological space. We shall in this paper only be concerned with positive definite kernels, that is, symmetric functions $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ for which the Gram matrix is positive definite. To such a kernel one may associate a feature map $\\mathcal{X}\\rightarrow\\mathbb{R}^{\\mathcal{X}}$ such that $x\\mapsto k_{x}=k(x,\\cdot)$ . A reproducing kernel Hilbert space (RKHS) is a Hilbert space $\\mathcal{H}\\subset\\mathbb{R}^{\\mathcal{X}}$ such that the evaluation functionals, $e v_{x}\\,\\colon f\\mapsto f(x)$ , are bounded for each $x\\in\\mathscr{X}$ . For all positive definite kernels there is a unique RKHS $\\mathcal{H}\\subset\\mathbb{R}^{\\mathcal{X}}$ such that $f(x)=\\langle k_{x},f\\rangle_{\\mathcal{H}}$ for all $f\\in\\mathcal H$ and $x\\in\\mathscr{X}$ . This is also known as the reproducing property. Furthermore, with $H$ denoting the linear span of $\\{k_{x}\\mid x\\in\\mathcal{X}\\}$ , it holds that $\\bar{H}=\\mathcal{H}$ , i.e., $H$ is dense in $\\mathcal{H}$ . Two important properties of kernels are characteristicness and universality. ", "page_idx": 22}, {"type": "text", "text": "Definition C.1. Let $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be a positive definite kernel. Denote by $H$ the linear span of $\\{k_{x}\\mid x\\in\\mathcal{X}\\}$ and let $\\mathcal{F}\\subset\\mathbb{R}^{\\mathcal{X}}$ be a topological vector space containing $H$ and such that the inclusion map $\\iota:H\\to\\mathcal{F}$ is continuous. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 We say that $k$ is characteristic to ${\\mathcal{F}}^{\\prime}$ if the embedding $\\mu:{\\mathcal{F}}^{\\prime}\\to H^{\\prime}$ , $D\\mapsto D|_{H}$ is injective Remark C.1. This definition is the one used in [8] and is more general then the one usually encountered. Note that in many cases (all the cases considered here, in fact) ${\\mathcal{F}}^{\\prime}$ will contain the set of probability measures on $\\mathcal{X}$ in which case $k$ being characteristic implies that the kernel mean embedding $\\mu\\mapsto$ $\\mathbb{E}_{X\\sim\\mu}k_{X}(\\cdot)$ is injective. ", "page_idx": 22}, {"type": "text", "text": "Remark C.2. Often times, instead of starting with the kernel function $k$ and then obtaining the RKHS, one starts with a feature map $F:\\mathcal{X}\\to\\mathcal{H}$ into a RKHS and then defines the kernel as the inner product in that Hilbert space, i.e., $k(x,y)=\\langle F(x),F(y)\\rangle_{\\mathcal{H}}$ . In such cases, it makes sense to ask whether there are equivalent notions of $F$ being universal and characteristic. This is indeed the case and the definition is almost the same as above. We refer to Definition 6 in [8] for a precise statement. ", "page_idx": 22}, {"type": "text", "text": "C.1 Marcus signature kernel ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The definition of the signature kernel requires an initial algebraic setup. Let $\\langle\\cdot,\\cdot\\rangle_{1}$ be the Euclidean inner product on $\\mathbb{R}^{d}$ . Denote by $\\otimes$ the standard outer product of vector spaces. For any $n\\,\\in\\,\\mathbb{N}$ , we denote by $\\langle\\cdot,\\cdot\\rangle_{n}$ on $(\\mathbb{R}^{d})^{\\otimes n}$ the canonical Hilbert-Schmidt inner product defined for any $\\mathbf{a}=$ $(a_{1},\\ldots,a_{n})$ and $\\mathbf{b}=(b_{1},\\ldots,b_{n})$ in $(\\mathbb{R}^{d})^{\\otimes n}$ as $\\begin{array}{r}{\\langle\\mathbf{a},\\mathbf{b}\\rangle_{n}=\\prod_{i=1}^{n}\\left\\langle a_{i},b_{i}\\right\\rangle_{1}}\\end{array}$ . The inner product $\\langle\\cdot,\\cdot\\rangle_{n}$ on $(\\mathbb{R}^{d})^{\\otimes n}$ can then be extended by linearity to an inner product $\\langle\\cdot,\\cdot\\rangle$ on $\\tilde{T}\\big((\\mathbb{R}^{d})\\big)$ defined for any $\\mathbf{a}=(1,a_{1},\\ldots)$ and ${\\bf b}=(1,b_{1},\\ldots)$ in $\\tilde{T}\\big((\\mathbb{R}^{d})\\big)$ as $\\begin{array}{r}{\\left\\langle\\mathbf{a},\\mathbf{b}\\right\\rangle=1+\\sum_{n=1}^{\\infty}\\left\\langle a_{n},b_{n}\\right\\rangle_{n}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "To begin with, let $\\mathcal{X}=D_{1}([0,T],\\mathbb{R}^{d})$ . If $x\\in\\mathscr{X}$ is c\u00e0dl\u00e0g path, we can define the Marcus signature in the spirit of Marcus SDEs [42, 43] as the signature of the Marcus interpolation of $x$ . This interpolation, denoted by $\\hat{x}$ , is the continuous path on $[0,T]$ obtained from $x$ by linearly traversing the jumps of $x$ over added fictitious time $r>0$ and then reparameterising so that the path runs over $[0,T]$ instead of $[0,T+r]$ . The general construction is given in Appendix A. If $x$ is continuous, $x$ and $\\hat{x}$ coincide; thus, without any ambiguity, we can define the Marcus signature $S(x)$ of a general bounded variation c\u00e0dl\u00e0g path as the tensor series described above, but replacing $x$ with $\\hat{x}$ (see also the definition in A.2). ", "page_idx": 23}, {"type": "text", "text": "Since the signature is invariant to certain reparameterisations (Proposition A.1), it is not an injective map. Injectivity is a crucial property required to ensure characteristicness of the resulting signature kernel that we will introduce next. One way of overcome this issue is to to augment a path $x$ with a time coordinate resulting in the path $\\tilde{x}=(x,t)^{5}$ . The Marcus signature kernel is then naturally defined as the map $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ such that $k(x,y)=\\langle S(\\tilde{x}),S(\\tilde{y})\\rangle$ for any $x,y\\in\\mathcal{X}$ . As stated in Theorem C.1, this kernel is universal on compact subsets $K\\subset\\mathcal{X}$ and, equivalently, characteristic to the space of regular Borel measures on $K$ . However, these properties do not generalize to the whole space $C_{b}(\\mathcal{X},\\bar{\\mathbb{R}})$ of bounded continuous functions from $\\mathcal{X}$ to $\\mathbb{R}$ . ", "page_idx": 23}, {"type": "text", "text": "In [8] the authors address this issue in the case of continuous paths by introducing the so-called robust signature. They define a tensor normalization as a continuous injective map ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda:\\tilde{T}\\left(\\left(\\mathbb{R}^{d}\\right)\\right)\\rightarrow\\left\\{\\mathbf{a}\\in\\tilde{T}\\left(\\left(\\mathbb{R}^{d}\\right)\\right)\\ |\\ |\\mathbf{||a|}\\leq R\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some $R>0$ and such that $\\Lambda(\\mathbf{a})=(\\mathbf{a}^{0},\\lambda(\\mathbf{a})a_{1},\\lambda(\\mathbf{a})^{2}a_{2},\\dots)$ for some $\\lambda:\\tilde{T}\\left(\\left(\\mathbb{R}^{d}\\right)\\right)\\rightarrow\\left(0,\\infty\\right)$ . ", "page_idx": 23}, {"type": "text", "text": "Now, let $p\\,\\in\\,[1,3)$ and take $C_{0}^{1}(\\mathbb{R}^{d})$ to be the space of absolutely continuous functions on $\\mathbb{R}^{d}$ . Recall that $\\Omega_{0,p}^{D}(\\ensuremath{\\mathbb{R}}^{d})$ is the closure of $C_{0}^{1}(\\mathbb{R}^{d})$ in $\\Omega_{p}^{D}(\\mathbb{R}^{d})$ under the metric $\\alpha_{p}$ . Throughout we let $\\mathcal{X}=\\Omega_{0,p}^{D}(\\mathbb{R}^{d})$ be a metric space equipped with $\\alpha_{p}$ . Naturally, we can then define the signature kernel on $\\mathcal{X}$ by $k(\\mathbf{x},\\mathbf{y})\\,=\\,\\langle S(\\tilde{\\mathbf{x}}),S(\\tilde{\\mathbf{y}})\\rangle$ and, similarly, the robust signature kernel $k_{\\Lambda}({\\bf x},{\\bf y})\\;=\\;$ $\\langle\\Lambda(S(\\tilde{\\mathbf{x}})),\\Lambda(S(\\tilde{\\mathbf{y}}))\\rangle$ where $\\Lambda$ is a tensor normalisation. ", "page_idx": 23}, {"type": "text", "text": "Theorem C.1. Let $p\\geq1$ , $\\Lambda$ a tensor normalization, and $K\\subset\\mathcal{X}$ compact under $\\alpha_{p}$ . Then, ", "page_idx": 23}, {"type": "text", "text": "(i) The signature kernel $k$ is universal to $\\mathcal{F}=C(K,\\mathbb{R})$ equipped with the uniform topology and characteristic to the dual ${\\mathcal{F}}^{\\prime}$ , the space of regular Borel measures on $K$ .   \n(ii) The robust signature kernel $k_{\\Lambda}$ is universal to $\\mathcal{F}=\\,C_{b}(\\mathcal{X},\\mathbb{R})$ equipped with the strict topology and characteristic to the dual ${\\mathcal{F}}^{\\prime}$ , the space of all finite Borel measures on $\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem C.1. Part $(i)$ follows directly from the proof of Proposition 3.6 in [13]. For part $(i i)$ we shall proof that the feature map $F\\,=\\,\\Lambda\\circ S$ is universal and characteristic. The result then follows from Proposition 29 in [8]. We start by defining $\\mathcal{P}=\\mathcal{X}/\\sim_{t}$ where the equivalence relation $\\sim_{t}$ is defined in Appendix A.2. We equip $\\mathcal{P}$ with the topology induced by the embedding $S:{\\mathcal{P}}\\to{\\tilde{T}}((\\mathbb{R}^{d+1}))$ . By Proposition A.1, $F$ is a continuous and injective map from $\\mathcal{P}$ into a bounded subset of $\\tilde{T}^{}((\\mathbb{R}^{d+1}))$ . Thus, $\\begin{array}{r}{\\mathcal{H}=\\{\\langle\\ell,F\\rangle\\;\\vert\\;\\ell\\in T((\\mathbb{R}^{d+1}))^{\\prime}\\}}\\end{array}$ is a subset of $\\mathcal{F}$ that separates points. Furthermore, since $F$ takes values in the set of group-like elements, $\\mathcal{H}$ is a subalgebra of $\\mathcal{F}$ (under the shuffle product). It then follows from Theorem 7 and Theorem 9 in [8] that $F$ is universal and characteristic. The fact that ${\\mathcal{F}}^{\\prime}$ is the space of all finite Borel measures on $\\mathcal{X}$ is part (iii) of Theorem 9 in the same paper. Finally, as per Appendix A.3, the map $\\mathbf{x}\\mapsto\\tilde{\\mathbf{x}}$ is a continuous and injective embedding of $\\mathcal{X}$ into $\\mathcal{P}$ from which the result then follows. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "With $d_{k}$ denoting the MMD for a given kernel $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ , the following is a direct consequence of Theorem C.1. ", "page_idx": 23}, {"type": "text", "text": "Corollary C.1. Let $p\\geq1$ , $\\Lambda$ a tensor normalization, and $K\\subset\\mathcal{X}$ compact under $\\alpha_{p}$ . Then, $d_{k}$ is $a$ metric on $\\mathcal{M}(K)$ and $d_{k_{\\Lambda}}$ is a metric on $\\mathcal{M}(\\mathcal{X})$ . ", "page_idx": 23}, {"type": "text", "text": "D Forward sensitivities for SLIF network ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the general SSNN model, Theorem 3.2 gives the following result. ", "page_idx": 24}, {"type": "text", "text": "Proposition D.1. Fix some weight $w_{i j}\\in w$ , a neuron $k\\in[K]$ and let $\\mathcal{G}_{t}^{k}$ denote the gradient of $(v^{k},i^{k})$ wrt. $w_{i j}$ at time $t.$ . Furthermore, define $\\gamma:\\{0,1\\}\\,\\rightarrow\\,\\mathbb{R}^{2}$ such that $\\gamma_{0}\\,=\\,(\\mu_{1},-\\mu_{2})w_{l k}$ , $\\gamma_{1}=(\\mu_{1},0)v_{r e s e t}$ , and let $\\Gamma\\in\\mathbb{R}^{2\\times2}$ be the drift matrix in the inter-spike SDE of $(v^{k},i^{k})$ . Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}^{k}=e^{\\Gamma(t-s)}\\left(\\mathcal{G}_{s}^{k}-\\gamma_{\\delta_{l k}}\\partial_{w_{i j}}s+\\delta_{i l}\\delta_{j k}e_{2}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $e_{n}\\in\\mathbb{R}_{2}$ is the n\u2019th unit vector, $l$ is the neuron in $\\mathrm{Pa}_{k}\\cup\\{k\\}$ with the most recent spike time before $t$ , and we denote this spike time by s. If $t$ is a spike time of neuron $k$ it therefore follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\partial_{w_{i j}}t=\\frac{\\lambda(v_{{t_{p r e v}}}^{k})\\partial_{w_{i j}}t_{p r e v}-\\int_{t_{p r e v}}^{t}\\nabla\\lambda(v_{r}^{k})e_{1}^{T}\\mathcal{G}_{r}^{k}d r}{\\lambda(v_{t}^{k})},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $t_{p r e v}$ is the previous spike time of neuron $k$ . In the case of a deterministic SNN, formula (20) is replaced by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\partial_{w_{i j}}t=-\\frac{e_{1}^{T}\\mathcal{G}_{t}^{k}}{\\mu_{1}(i_{t}^{k}-v_{t}^{k})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Throughout we fix some $t>0$ and let $s<t$ denote most recent event time preceding $t$ with $l$ the index of the neuron firing at time $s$ . We define the process $d w_{t}=0d t$ with $w_{0}=w_{i j}$ and with a slight abuse of notation we shall write $y_{t}^{k}=\\left(v_{t}^{k},i_{t}^{k},s_{t}^{k},w_{t}\\right)$ . We will leave out the event index $n$ for notational simplicity. Since $y_{t}^{k}$ depends on $y_{s}$ only through $y_{s}^{k}$ and $\\nabla\\tau_{l}$ is block diagonal, a direct consequence of eq. (7) is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{G}_{t}^{k}=(I}&{{}0)\\,\\partial_{y_{s}^{k}}y_{t}^{k}\\left(\\nabla\\mathcal{T}_{j}^{l}(y_{s-}^{k})\\partial_{w_{i j}}y_{s-}^{k}-\\left(\\mu(y_{s}^{k})-\\nabla\\mathcal{T}_{l}^{k}(y_{s-}^{k})\\mu(y_{s-}^{k})\\right)\\partial_{w_{i j}}s\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mu(v,i,s,w)\\,=\\,(\\mu_{1}(i\\,-\\,v),-\\mu_{2}i,\\lambda(v),0)$ . If $l\\,\\in\\,{\\mathrm{Pa}}_{k}\\cup\\{k\\}$ , then $\\mathcal{T}_{l}^{k}\\,=\\,\\mathrm{id}$ and therefore $\\begin{array}{r}{\\mathcal{G}_{t}^{k}=\\left(I\\quad0\\right)\\partial_{y_{s}^{k}}y_{t}^{k}\\partial_{w_{i j}}y_{s}^{k}}\\end{array}$ . One can then reapply the formula above until $l\\in\\mathsf{P a}_{k}\\cup\\{k\\}$ . By the flow property, it follows that we may assume without loss of generality that $l\\in\\mathsf{P a}_{k}\\cup\\{k\\}$ . This leaves us with two cases. We define $z_{t}^{k}=(v_{t}^{k},i_{t}^{k})$ so that $\\begin{array}{r l}{(I}&{{}0)\\,\\partial_{y_{s}^{k}}y_{t}^{k}=\\partial_{z_{s}^{k}}z_{t}^{k}}\\end{array}$ and $\\partial_{w_{i j}}z_{t}^{k}=\\mathcal{G}_{t}^{k}$ . Furthermore, let $a=\\delta_{i l}\\delta_{j k}$ . ", "page_idx": 24}, {"type": "text", "text": "Case 1, $l\\in{\\mathrm{Pa}}_{k}$ : In this case $\\begin{array}{r}{\\mathcal{T}_{l}^{k}(v,i,s,w)=(v,i+a w+(1-a)c,s,w)}\\end{array}$ where $c$ is a constant. As a result ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\partial_{z_{s}^{k}}z_{t}^{k}\\nabla\\mathcal{T}_{l}^{k}(y_{s-}^{k})\\partial y_{s-}^{k}=\\partial_{z_{s}^{k}}z_{t}^{k}\\mathcal{G}_{t}^{k}+a\\partial_{i_{s}^{k}}z_{t}^{k},}\\\\ {\\partial_{z_{s}^{k}}z_{t}^{k}\\left(\\mu(y_{s}^{k})-\\nabla\\mathcal{T}_{l}^{k}(y_{s-}^{k})\\mu(y_{s-}^{k})\\right)=\\partial_{z_{s}^{k}}z_{t}^{k}\\gamma_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In total, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}^{k}=\\partial_{z_{s}^{k}}z_{t}^{k}\\left(\\mathcal{G}_{t}^{k}-\\gamma_{0}\\partial_{w_{i j}}s+a e_{2}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Case 2, $l=k$ : In this case $\\mathcal{T}_{l}^{k}(v,i,s,w)=(v-v_{r e s e t},i,\\log u-\\alpha,w)$ so that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{z_{s}^{k}}z_{t}^{k}\\nabla\\mathcal{T}_{l}^{k}(y_{s-}^{k})\\partial y_{s-}^{k}=\\partial_{z_{s}^{k}}z_{t}^{k}\\mathcal{G}_{t}^{k},}\\\\ {\\partial_{z_{s}^{k}}z_{t}^{k}\\left(\\mu(y_{s}^{k})-\\nabla\\mathcal{T}_{l}^{k}(y_{s-}^{k})\\mu(y_{s-}^{k})\\right)=\\partial_{z_{s}^{k}}z_{t}^{k}\\gamma_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}^{k}=\\partial_{z_{s}^{k}}z_{t}^{k}\\mathcal{G}_{t}^{k}-\\partial_{z_{s}^{k}}z_{t}^{k}\\gamma_{0}\\partial_{w_{i j}}s.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $z_{t}^{k}$ is an Ornstein-Uhlenbeck process initialized at $z_{s}^{k}$ and with drift and diffusion matrices ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Gamma=\\left(\\!\\!\\begin{array}{c c}{{-\\mu_{1}}}&{{\\mu_{1}}}\\\\ {{0}}&{{-\\mu_{2}}}\\end{array}\\!\\!\\right),\\quad\\Sigma=\\left(\\!\\!\\begin{array}{c c}{{\\sigma_{1}}}&{{0}}\\\\ {{0}}&{{\\sigma_{2}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As a result, we can directly compute $\\partial_{z_{s}^{k}}z_{t}^{k}=e^{(t-s)\\Gamma}$ . This proves that eq. (19) holds. Eq. (20) then follows directly from (6) and the fact that $\\mathcal{E}_{k}(y)=s^{k}$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "From this the results of Section 4.4 follow since the terms $\\partial_{w_{i j}s}$ vanish whenever $s$ is the spike time of a neuron $l$ that is not a descendant of neuron $j$ . Thus, equation (19) only includes terms depending on the activity of the pre- and post-synaptic neuron. In particular, there is no need to store the gradient path $\\mathcal{G}_{t}^{k}$ for each combination of neuron $k$ and synapse $i j$ , but each neuron only needs to keep track of the paths for its incoming synapses. This reduces the memory requirements from the order of $K^{3}$ to only $K^{2}$ (which is needed anyway to store the weight matrix). In general, the gradient paths can be approximated by simply omitting the terms $\\partial_{w_{i j}\\,s}$ . ", "page_idx": 25}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 Input current estimation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "6For each combination of sample size and $\\sigma$ we sample a data set of spike trains using Algorithm 1 with $N=3$ , i.e., up until the first three spikes are generated. We use diffrax to solve the inter-Event SDE with a step size of 0.01 and the numerical solver is the simple Euler-Maruyama method. We then sample an initial guess $c\\sim\\mathrm{Unif}([0.5,2.5])$ and run stochastic gradient descent using the approach described in 4.1. That is, for each step, we generate a batch of the same size as the sample size and use $d_{k}$ to compare the generated batch to the data. For each step we also compare the absolute error between the average spike time of the first three spikes of the generated sample to a hold a out test set of the same size as the sample. We use the RMSProp algorithm with a decay rate of 0.7 and a momentum of 0.3 which we found to work well in practice. The learning rate is 0.001. The experiment was run locally on CPU with an Apple M1 Pro chip with 8 cores and 32 GB of ram. The entire experiment took approximately 3-6 hours to run. For the exact details of this experiment we refer to the notebook snnax/notebooks/single_neuron.ipynb in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "E.2 Synaptic weight estimation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As above, for each sample size $D\\in\\{256,512,1024\\}$ we sample a data set of spike trains using Algorithm 1 with $T=1$ and with the same differential equation solver setup as above. Thus, in this case, the number of spikes varies across each sample path. The parameters are chosen as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,v_{r e s e t}=1.2}\\\\ &{\\bullet\\,\\,\\lambda(v)=\\exp(5(v-1)}\\\\ &{\\bullet\\,\\,\\mu=(6,5)}\\\\ &{\\bullet\\,\\,\\sigma=I_{2}/4}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For each sample size the data was generated using the same randomly sampled weight matrix $w$ which represents a feed-forward network of the dimensions described in Section 4 and which was constructed as follows: for the weight matrix from layer $l$ to layer $l+1$ , say $w^{l}$ , we sample each entry from $\\mathrm{Unif}([0.5,1.5])$ and then normalize by $3/K_{l}$ where $K_{l}$ is the number of neurons in layer $l$ The normalisation makes sure that the spike rate for the neurons in each layer is appropriate. ", "page_idx": 25}, {"type": "text", "text": "For each data set (each sample size) we then train a spiking neural net of the same network structure to match the observed spike trains. This is done using stochastic gradient descent with a batch size of $B=128$ and by computing $d_{k}$ on a generated batch and a batch sampled from the data set at each step. In order to avoid local minimums7 we match the number of spikes between the generated spike trains and the ones sampled from the data set. Also, we sample from the data set without replacement so that we loop through the whole data set every $D/B$ steps. We run RMSProp for 1500 steps with a momentum of 0.3 and a learning rate of 0.003 for the first 1000 steps and 0.001 for the last 500 steps. ", "page_idx": 25}, {"type": "text", "text": "This experiment was run in the cloud using Azure AI Machine Learning Studio on a NVIDIA Tesla V100 GPU with 6 cores and $112\\,\\mathrm{GB}$ of RAM. The entire experiment took around 12-16 hours to run. For the exact details we refer to the notebook snnax/notebooks/spiking_neural_net.ipynb in the supplementary material. ", "page_idx": 25}, {"type": "image", "img_path": "mCWZj7pa0M/tmp/b4bfc2e01cca265586b842f32ed388beb78c74e529d4ce93b41c36cc08affb67.jpg", "img_caption": ["Figure 2: We estimate the synaptic weights $w$ across three different sample sizes using the signature kernel MMD truncated at depth 3 and stochastic gradient descent with a batch size of 128. On the left we report the loss on a hold out test set. On the right is the mean absolute error between the entries of the currently estimated weight matrix $\\hat{w}_{s t e p}$ and the true weight matrix $w_{t r u e}$ . "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The claims are clearly stated in Section 1.1 which also references where to find the key results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is a dedicated section (Section 5) clearly discussing what is lacking and what can be explored in future work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Rigorous proofs are provided in Appendix B and the assumptions are an integral part of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We clearly describe the main algorithm used in both experiments (Algorithm 1) as well as the experimental setups. Additionally, the code is provided in the supplementary material with clear instructions on how to setup the correct environment. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed ", "page_idx": 27}, {"type": "text", "text": "instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Code is provided in the supplementary material. The two main experiments can be reproduced by running the two notebooks in the snnax folder as referenced in Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimiser, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide the most important aspects in Section 4. The full details are given in Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We did not prioritise running multiple iterations of the experiments since the main contributions are of a more theoretical nature. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This information is also given in Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have reviewed and made sure that our paper conforms with the code of ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The work in the paper is of a theoretical nature and there are no clear links to immediate applications with broader societal impacts. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 30}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have followed the citation guidelines for referencing diffrax which is the main asset used for the current work. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]