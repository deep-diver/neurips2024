[{"type": "text", "text": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Self-Supervised Transformer Models are the backbone of much of the recent   \n2 progress in deep learning. However, these models require their inputs to be tok  \n3 enized, and tokenization strategies for continuous data like audio and vision are   \n4 often based on simple heuristics such as fixed sized convolutions or discrete clus  \n5 tering. For speech and audio models in particular, the high resolution of waveforms   \n6 (16,000 samples/second or more) presents a significant challenge, as several times   \n7 more tokens are used per word than in textual language modeling. In this work,   \n8 we introduce a controllable, fully-self-supervised technique to dynamically merge   \n9 speech representations across time to as low as $5\\:\\mathrm{Hz}$ at 60 bits per second while   \n10 still preserving semantic information. We do this by 1) extracting noisy bound  \n11 aries through analyzing correlations between mask spans and model losses and 2)   \n2 iteratively improving these representations with a novel agglomeration technique.   \n3 Using these new feature representations, we successfully train SyllableLM, a Neu  \n4 ral Codec Language Model (NCLM) competitive with current SoTA NCLMs on   \n15 a range of common benchmarks with a $30\\mathrm{x}$ reduction in pretraining compute, 5x   \n16 reduction in inference compute, and $2.5\\mathrm{x}$ reduction in bitrate. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Self-Supervised Learning (SSL) seeks to learn powerful, abstract representations of data without   \n19 external labels. These representations can then be used in downstream tasks to achieve high perfor  \n20 mance even when modest amounts of supervised fine-tuning data are available. In audio and speech   \n21 processing, a key motivation for this learning paradigm is the fact that young children learn to listen   \n22 and speak well before they can read or write. While current textual language models [52, 59, 9] can   \n23 compose highly realistic text, the research community has not yet developed similarly performant   \n24 models that learn solely from spoken language. An increasing focus has coalesced around Generative   \n25 Spoken Language Modeling (GSLM) [34], which sets out to achieve this goal.   \n26 The most successful of these approaches are autoregressive decoder transformer models [53] such as   \n27 AudioLM [8] and TWIST [26], which operate on tokens learned through quantizing the output of   \n28 SSL encoder models [28, 14]. However, these self-supervised tokenizations are much denser than   \n29 their textual counterparts with the token rates typically between 25 and 50 tokens per second for   \n30 speech models, as opposed to the typical human speaking rate of 2-5 words per second. The long   \n31 context lengths that result from high temporal resolution tokenizations in speech models substantially   \n32 impair both pretraining and inference speed, and it is additionally unclear to what extent modeling   \n33 speech with a high granularity harms more abstract semantic understanding.   \n34 Very recently, there has been significant progress in extracting coarser speech unit representations   \n35 from raw audio. In particular, SD-HuBERT [12] distills HuBERT [28] using only audio with a DINO  \n36 like distillation objective, and VG-HuBERT [45, 46] uses a contrastive loss against cross-modal   \n37 visual inputs. We continue and significantly improve upon this line of research, resulting in the   \n38 first syllable-like units suitable for high-quality GSLM. Specifically, we demonstrate breakthrough   \n39 improvements in textual reconstruction from low-bitrate units of SSL models, reducing the word  \n40 error-rate (WER) from $37\\%$ using SD-HuBERT units to $7\\%$ , and more than halving realized bitrate   \n41 of previous SpeechLM units from 175Bps to as low as 60Bps. We additionally find that our units   \n42 correlate strongly with syllables both in boundary detection and in cluster quality.   \n43 Furthermore, we evaluate the effects of training SpeechLMs on these new units and obtain state-of  \n44 the-art results across a wide-variety of metrics, competitive with or outperforming AudioLM (350M   \n45 parameters) and all TWIST model sizes (125M-13B parameters) with fewer parameters and fewer   \n46 GPU-Hours. We commit to making our code open-source and plan to release our tokenizer and   \n47 SpeechLM parameters. Our contributions are as follows:   \n48 1. We propose a novel training-free algorithm named LossPred that reveals noisy syllabic  \n49 like segmentation of unannotated speech signals by analyzing the loss of a pretrained   \n50 self-supervised model (e.g. HuBERT) across different masking spans.   \n51 2. We propose a novel bootstrapping framework for speech unit quantization named SylBoost   \n52 that achieves SotA unsupervised syllabic segmentation, categorization, and low-bitrate   \n53 unit-to-audio resynthesis.   \n54 3. Using quantized SylBoost units as a basis for tokenization, we train SyllableLM, a generative   \n55 spoken language model that outperforms or matches AudioLM and TWIST on a range of   \n56 tasks while being $30\\mathrm{x}$ faster to train, $5\\mathrm{x}$ faster for inference, and having a $2.5\\mathrm{x}$ reduction in   \n57 unit bitrate. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "tVO3b68Oyp/tmp/a785710760a9838e34490edda369f816cc34865eb353b56c0e4b5f9fee689ae8.jpg", "img_caption": ["Figure 1: Left-Top: The loss prediction matrix $C$ , where brighter is higher likelihood placed on the teacher label. A time-aligned transcript is on the bottom, and predicted cluster unit boundaries span vertically as dashed-lines. Left-Bottom: A Mel-Spectrogram of the input waveform with an example masked timespan in gray. The losses on tokens at timesteps covered by the solid blue and dotted red spans are mapped to their corresponding rows and columns in $C$ as described in Section 3.1. Right: Visual of our agglomeration procedure. We train a student to match intermediate teacher features pooled over regions generated by pseudo-syllable-boundaries. We use a min-cut algorithm to extract boundaries, and then apply K-Means and Agglomerative clustering to obtain discrete units. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 Self-Supervised Encoder Models There has been a great amount of work in learning high-level   \n60 representations from data by reconstructing corrupted inputs across speech [3, 28, 6], audio [24], text   \n61 [20, 15], and vision [10, 27]. To navigate the lack of simple discrete targets in speech, much work   \n62 has been placed in finding high-quality targets, such as iterative clustering [28] and by predicting the   \n63 feature representations of a teacher network based on a running average of student model weights   \n64 [5, 6]. An alternate but similar line of work has been placed into learning low-bitrate units for the   \n65 task of resynthesis [19, 58, 56, 33, 60, 21], which include losses focused on reconstruction and use   \n66 an information bottleneck to enforce compression.   \n67 Applications of Neural Codecs The discrete units generated by these self-supervised encoders are   \n68 versatile and fundamental to much of the recent progress in speech research such as Text-To-Speech   \n69 [54, 29, 50, 47], joint audio-text foundation models [57, 13, 38], unsupervised speech recognition   \n70 [4], discrete unit resynthesis [48, 19, 58], text-to-audio [32, 1, 17], and generative spoken language   \n71 modeling [8, 26, 34]. Each of these methods operates on audio units exclusively greater than or equal   \n72 to $25\\mathrm{Hz}$ , which has been a frequently cited area for future work to improve on [26]. Recent work   \n73 [22] has also explored training speech encoder models with coarser units as targets.   \n74 Extracting Semantic Units from Raw Data Also relevant to our work are several approaches,   \n75 particularly in vision and audio, that generate emergent semantic clusterings from self-supervised   \n76 transformer [53] models. In particular, the DINO approach in Caron et al. [10] observes object   \n77 representations in attention maps through student-teacher distillation. Similar techniques have been   \n78 also applied to audio to discover emergent syllable boundaries [12, 46]. These behaviors can vary   \n79 heavily with small changes in pretraining strategy as explored in Darcet et al. [18]. Merging similar   \n80 features has also been shown to produce significant vision model speedups such as in Bolya et al. [7].   \n81 Most similar to our work, Algayres et al. [2] extracted coarse continuous representations for GSLM,   \n82 however these results trail behind NCLM-based approaches. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "83 3 Learning Self-Supervised, Syllable-Like Representations from Raw Speech ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "84 In this section, we describe the bootstrapping process by which we extract low-bitrate speech units.   \n85 We first describe LossPred, our algorithm to analyze outputs of self-supervised speech model loss   \n86 functions to generate initial unit boundaries. Following this, we define SylBoost, an agglomeration   \n87 procedure to iteratively refine these boundaries with student-teacher distillation. We also propose a   \n88 new algorithm for the efficient extraction of boundaries from feature self-similarity matrices to fix   \n89 the bottleneck slowing down VG-HuBERT and SD-HuBERT extraction. ", "page_idx": 2}, {"type": "text", "text": "90 3.1 LossPred: Extracting Syllable-like Segmentation from Relations in HuBERT\u2019s Loss ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "91 HuBERT has previously been shown to learn phone-like units with its K-means clusterings [28] which   \n92 have formed the basis of subsequent works on GSLM and unsupervised ASR [4, 34, 26]. However,   \n93 other work [42, 43] has shown that the representations learned by these models also correlate with   \n94 higher level structure such as words, despite these structures not immediately appearing during   \n95 clustering. Our goal in this section is to propose a method that can be applied to a pre-trained   \n96 HuBERT model in order to automatically extract unit boundaries at the level of syllables or words,   \n97 rather than phones. Although we apply our method to HuBERT, we expect that it could also be   \n98 applied to other SSL speech models that utilize a similar loss function such as WavLM [11] or   \n99 wav2vec2.0 [3]. The crucial commonality between these models is that they all utilize a masked   \n100 language modeling (MLM) training objective, whereby input speech tokens are randomly masked   \n101 and the model is trained to predict the masked inputs conditioned on the unmasked inputs.   \n102 We ground our intuition with the following thought experiment: If the input tokens corresponding   \n103 to an entire word were replaced with mask tokens, we would expect the HuBERT model loss at   \n104 these timesteps to be relatively high, as HuBERT would have to jointly predict word identity and   \n105 the underlying acoustics to predict the missing span. On the other hand, if only the latter portion   \n106 of a word were masked out, infilling this masked region given the word prefix may be easier by   \n107 comparison. With this, if we iteratively shift a contiguous mask over a span of tokens and look at the   \n108 loss, we would suspect to see a strong decrease in the loss throughout the timesteps corresponding to   \n109 a masked semantic unit (word, syllable, or otherwise) as the beginning or end of the unit was partially   \n110 revealed to the model. In our experiments, we find that semantic units extracted by this method tend   \n111 to be syllable-like (both via inspection, and also confirmed experimentally in our segmentation and   \n112 clustering experiments) and so we focus on these units for the rest of the paper.   \n113 We consider the setting of having a pretrained HuBERT teacher model and a HuBERT student model   \n114 trained to predict the quantized contextualized representations generated by the teacher at layer $L$ , as   \n115 described in Hsu et al. [28]. Formally, given an input waveform $W$ , we extract the teacher labels used   \n116 to train the student by passing $W$ unmodified into the frozen HuBERT teacher and then quantizing the   \n117 contextualized representations of layer $L$ with K-Means. We denote these teacher labels as $Y_{\\{1...T\\}}$ ,   \n118 where $T$ is the number of tokens outputted by the CNN feature encoder stage of HuBERT. During   \n119 pretraining, the student model is given a corrupted version of $W$ where tokens after CNN extraction   \n120 at select times are replaced with a learned \u2018mask\u2019 embedding. We denote these tokens input to the   \n121 student as X{M1.. $X_{\\{1\\dots T\\}}^{M}$ where $M=\\{t_{1},\\dots t_{m}\\}$ is a contiguous span of masked timesteps. The student   \n122 is then trained to predict these teacher labels at masked timesteps using a cross-entropy loss, which   \n123 we denote as EtX for the loss on $Y_{t},t\\in M$ given $X^{M}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nE_{t}^{X^{M}}:=-\\log p(Y_{t}\\mid X^{M})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "124 We look at the losses of the student model at the end of pretraining, and define the loss prediction   \n125 matrix $C$ with mask span size parameter $s$ to capture the raw probabilities of the losses that would   \n126 result from all possible temporal locations of the mask span $M$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{r,c}\\in\\mathbb{R}_{+}^{T\\times T}=\\left\\{\\begin{array}{l l}{p(Y_{t}\\ |\\ X^{M})\\ |\\ M=\\{r+1,r+2,\\dots r+s\\}}&{\\mathrm{if}\\ r<c,|r-c|\\le\\left\\lfloor\\frac{s}{2}\\right\\rfloor,}\\\\ {p(Y_{t}\\ |\\ X^{M})\\ |\\ M=\\{r-1,r-2,\\dots r-s\\}}&{\\mathrm{if}\\ r>c,|r-c|\\le\\left\\lfloor\\frac{s}{2}\\right\\rfloor,}\\\\ {0}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "127 We separately calculate the upper and lower triangles of $C$ , relating to the observed waveform being   \n128 before the mask and after the mask respectively. In the upper triangle, each entry $C_{r,c}$ at row $r$   \n129 column $c$ is equal to $p(Y_{t}\\mid X^{M})$ given that the mask span in $X^{M}$ starts just after time $r$ . Inversely,   \n130 in the lower triangle, $C_{r,c}$ is equal to $p(Y_{t}\\mid X^{M})$ given that the mask span in ends just before   \n131 time $r$ . We use a span size $s\\,=\\,50$ corresponding to 1 second as this duration is long enough to   \n132 mask the majority of spoken words, and calculate the upper triangle based on the first 25 tokens   \n133 of the mask span, and the lower triangle based on the last 25. We choose to use a span of tokens   \n134 instead of masking all information after a timestep to prevent global information such as speaker   \n135 information available to the model changing with respect to mask location. However, this limits us to   \n136 only generating a diagonal span of probabilties as seen in 1. To extract $k$ regions with boundaries   \n137 $B=\\bar{\\{b_{1}=1\\,\\bar{<}\\,b_{2}<\\bar{\\,}...<\\bar{b}_{k}=\\bar{T^{+}}1\\}}$ from $C$ , we adopt the min-cut algorithm discussed in Peng   \n138 et al. [46], treating $C$ as the input feature-similarity matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\nB:=\\underset{\\{b_{1}=1<b_{2}\\ldots<b_{k+1}=T+1\\}}{\\arg\\operatorname*{min}}\\sum_{t=1}^{k}\\frac{\\underset{i=b_{t}}{\\sum}\\underset{j=1}{\\overset{T}{\\sum}}(C_{i,j}+C_{j,i})-2\\underset{i,j=b_{t}}{\\sum}C_{i,j}}{\\underset{i=b_{t}}{\\sum}\\underset{j=1}{\\overset{T}{\\sum}}(C_{i,j}+C_{j,i})-\\underset{i,j=b_{t}}{\\sum}C_{i,j}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 By choosing $k$ to be proportional to the length of the utterance, we can control the sample rate of our   \n140 boundaries. We explore modifying this parameter in-depth throughout our experiments.   \n141 LossPred is expensive to run due to having repeat forward passes for sliding windows. To make this   \n142 efficient, we extract multiple masked spans simultaneously with a gap between spans of three seconds.   \n143 This results in roughly 200 forward passes of the student model to calculate $C$ on an arbitrarily-sized   \n144 audio. We also preprocess the audio using a unsupervised voice activity dection model [51]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "145 3.2 SylBoost: Bootstrapping Pesudo-Syllabic Units with Iterative Distillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "146 Given the initial boundaries predicted by LossPred, we follow the paradigm of noisy-student-teacher   \n147 learning [55] to iterate and extract better representations. Our goal is to \u201csharpen\u201d the syllabic   \n148 organization in the feature space of an input student model that initially results from LossPred, as   \n149 seen on the right of Figure 1. We choose a pretrained HuBERT [28] or Data2Vec2 [6] to initialize our   \n150 student and teacher models, with the teacher model parameters held constant.   \n151 For a set of hypothesized speech segment boundaries $B=\\{b_{1}=1<b_{2}<...<b_{k+1}=T+1\\}$ ,   \n152 we group together all temporal tokens between two boundaries into disjoint groups $G_{i}=\\{t\\mid b_{i}\\leq$   \n153 $t<\\bar{b}_{i+1}\\bar{\\}$ . For notation, we let $H_{t}$ map from $t$ to its corresponding group: $t\\in G_{H_{t}}$ . We apply our   \n154 loss to the features at layer $L$ , which we select based on syllabic correlation as explored in detail in   \n155 Pasad et al. [43]. This results in student features $X_{\\{1\\dots T\\}}^{(L)}\\in\\mathbb{R}^{d}$ } \u2208Rd and teacher features Y {(1L..).T $Y_{\\{1\\dots T\\}}^{(L)}\\in\\mathbb{R}^{d}$   \n156 where $d$ is the feature dimension.   \n157 Then the loss, which is applied to each token of the student model, is the mean squared error between   \n158 the student features Xt(L) and the mean of the teacher features in the token\u2019s corresponding group: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nZ:=\\frac{1}{T}{\\sum_{t=1}^{T}{\\left({X_{t}^{(L)}}-\\frac{1}{|G_{H_{i}}|}{\\sum_{s\\in G_{H_{i}}}{Y_{s}^{(L)}}}\\right)^{2}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "159 This results in a model with a mean-squared-error feature similarity matrix as depicted in the right   \n160 side of Figure 1. We then extract boundaries using a cut algorithm described later in Sec. 3.3, although   \n161 the cut algorithm from Peng et al. [46] also works. With this, we can generate new pseudolabels and   \n162 iterate the process again to extract better boundaries, which we perform twice. ", "page_idx": 4}, {"type": "text", "text": "163 3.3 Efficient Extraction of Unit Boundaries with SylBoost ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 To extract boundary indices from learned feature representations Peng et al. [46] proposed adapting   \n165 the mincut approach in Malioutov and Barzilay [35]. However, for speech this approach is slow in   \n166 practice and difficult to parallelize, bottlenecking our ability to extract boundaries in bulk across the   \n167 large corpora necessary for downstream language modeling. Inspired by the SylBoost objective, we   \n168 propose a more efficient approach for extraction: given $k+1$ potential boundaries, we seek to choose   \n169 groups that minimize the sum of the distances from each unit to the mean of its assigned group: ", "page_idx": 4}, {"type": "equation", "text": "$$\nB:=\\underset{\\{b_{1}=1<b_{2}...<b_{k+1}=T+1\\}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{k}\\sum_{j=b_{i}}^{b_{i+1}-1}\\left(X_{j}^{(L)}-\\frac{1}{b_{i+1}-b_{i}}\\sum_{l=b_{i}}^{b_{i+1}-1}X_{l}^{(L)}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 We further restrict the setting by choosing a maximum group length of $G$ tokens, where we choose   \n171 $G=50$ to correspond to one second of tokens, as syllables or words longer than this are fairly rare.   \n172 With this, we can then split our algorithm into 1) calculating a distance array $D\\in\\mathbb{R}^{T\\times G}$ , where $D_{t,g}$   \n173 is the cost of the group of length $g$ ending at token $t$ and then 2) solving the minimal interval cover   \n174 from this distance array with dynamic programming. An efficient implementation using PyTorch   \n175 [44] on CUDA [40] runs in $O(k)$ data-aware sequential steps. ", "page_idx": 4}, {"type": "text", "text": "176 4 Syllable-LM: Speech Unit Language Modeling Over Syllable-Like Units ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 4.1 Language Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "178 GSLM [34] defines a pipeline for modeling raw audio as three stages: 1) Audio-to-unit Tokenization,   \n179 2) Running a decoder transformer model on these units, and 3) Decoding the tokens back into a   \n180 waveform. Like AudioLM and TWIST, we use an autoregressive transformer decoder language   \n181 model to approximate $p(x_{t}\\mid x_{t-1},...,x_{1})$ given an input token sequence $x_{1},\\dots,x_{T}$ . We refer to   \n182 this model as SpeechLM. We train it on clusters which we extract by mean pooling features at layer   \n183 $L$ , chosen as before, over their boundary groups, followed by K-Means and Agglomerative Clustering   \n184 to a desired number of discrete units. Like TWIST, we prepend a $\\tt{<B O S>}$ token and make no other   \n185 special changes. Due to current prevalence of this architecture, we refer to [26] for additional details. ", "page_idx": 4}, {"type": "text", "text": "186 4.2 Resynthesis and the Vocoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "187 For resynthesis, we adopt the interleaved decoding strategy from Song et al. [50] to output the   \n188 mHuBERT units from TWIST [26], obtaining a waveform by cascading this output into their provided   \n189 mHuBERT-to-speech vocoder. This interleaving strategy demonstrates superior performance in high  \n190 difficulty settings compared to other Neural Codec Lanaugae Models like VALL-E [54], and so we   \n191 use it for all resynthesis experiments. Although the cascading procedure may produce additional   \n192 errors, we choose this approach for the following reasons:   \n193 1. Text-to-speech systems like VALL-E traditionally start by converting text units into phones   \n194 using rule-based strategies to improve quality. This indicates that traditional unit-to-speech   \n195 resynthesis methods might be challenging for our low-bitrate units.   \n196 2. This pipeline allows for fast experimentation as we can precompute the mHuBERT 25hz   \n197 units once for all training runs.   \n198 3. Using the same Vocoder allows for fairer comparisons against TWIST.   \n199 To interleave our units, we sort on the start-timestep of every pseudo-syllable unit and mHuBERT-unit   \n200 in ascending order. To decrease the odds of mHuBERT units appearing before the pseudo-syllable   \n201 unit corresponding to the same ground truth syllable due to errant SylBoost boundaries, we subtract   \n202 0.08s (the length of two mHuBERT frames) from each pseudo-syllable start time before sorting. For   \n203 the rest of the pipeline, we follow [50] with our syllables as a drop-in replacement for phones. We note   \n204 that although our interleaved resynthesis model slows down generation compared to TWIST, most   \n205 model parameter scaling happens in the SpeechLM. For example, the TWIST paper still observes   \n206 scaling improvements at 13B parameters while current SOTA TTS models such as [29] operate well   \n207 with fewer than 1B parameters.   \n208 We then generate continuations for a sample by 1) Extracting syllable-unit and mHuBERT units from   \n209 the sample, 2) Sampling syllable-unit continuations from the SpeechLM, 3) Continuing mHuBERT   \n210 units with our interleaved model conditioned on sample mHuBERT units, sample syllable-units, and   \n211 continued syllable-units, and 4) Resynthesizing these into speech using the vocoder. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/fddc5be0ef9ebdbae4029c77b86d0f91f87fa268d4220593b02615580eb2e6c2.jpg", "table_caption": ["Table 1: Unsupervised Syllable Boundary Detection and Clustering Accuracy on LibriSpeech [41] Test. For F1 scores, the superscript is tolerance threshold in ms. All other metrics use $50\\mathrm{ms}$ . Higher is better. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "212 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "213 5.1 Training Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 We train our tokenizer using LibriSpeech [41], which contains 960 hours of audio books. We noticed   \n215 that the agglomeration procedure described in 3.2 converges before all data is used, and so we   \n216 randomly subsample LibriSpeech to a 100 hour train set and train for five epochs and two iterations   \n217 for all experiments. We train our SpeechLMs using all of LibriLight [30], which provides roughly   \n218 $55\\mathrm{k}$ hours of speech. As a note on fair comparison, although AudioLM uses exactly this split of   \n219 LibriLight, TWIST collects an additional $100\\mathrm{k}$ hours of data, totaling to $155\\mathrm{k}$ hours. ", "page_idx": 5}, {"type": "text", "text": "220 5.2 Model Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "221 We implement using the OPT [59] flavor of models and default to using 12 layers, an embedding   \n222 dimension of 768, and learned positional embeddings for both our SpeechLM and our Interleaved  \n223 Vocoder-LM. This totals to 90M non-embedding parameters, the same as TWIST-125M. We also   \n224 experiment with a larger 24 layer 1024 dimension model totaling to 300M non-embedding parameters,   \n225 the same as AudioLM and TWIST-350M. For all pretraining experiments we randomly crop flies to   \n226 25 seconds, use a batch size of 80000 tokens, and train for $200\\mathbf{k}$ steps, which amounts to the same   \n227 compute as in TWIST. To make our approach entirely textless, we do not use TWIST initialization.   \n228 Additional hyperparameters and hardware details are in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "229 5.3 Tokenizer Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 By varying the number of boundaries input to our cut algorithm at each stage in the agglomeration   \n231 pipeline, we can arbitrarily control our rate of temporal tokenization. We evaluate three main unit  \n232 rates at $8.33\\mathrm{Hz}$ , $6.25\\mathrm{Hz}$ , and $5.00\\mathrm{Hz}$ , the latter which matches the empirical rate of SD-HuBERT   \n233 units on LibriSpeech dev-clean. Combining unit-rates with changing the number of clusters generated   \n234 by K-Means and Agglomeration gives us fine-grained control of the model bitrate. We note that   \n235 although SD-HuBERT applies a cut algorithm, this is done after thresholding low-magnitude features   \n236 that emerge from pretraining. As a result, we find that we cannot control the frequency of SD  \n237 HuBERT units by changing parameters of its mincut algorithm becuase additional cuts result in   \n238 close-to-identical representations that map to the same quantized clusters.   \n239 From prior work, we compare against the AudioLM tokenizer w2v-BERT [14], and the tokenizer   \n240 from TWIST which is an open-source HuBERT model pretrained for an additional iteration on a large   \n241 and diverse set of multilingual data, henceforth mHuBERT. Both of these tokenizers operate at $25\\mathrm{Hz}$   \n242 followed by Run Length Encoding, which deduplicates repeated units. We additionally reimplement   \n243 Byte Pair Encoding as done in Shen et al. [49] on the deduplicated mHuBERT units, resulting in the   \n244 lowest bitrate encoding of speech outside of our model. We grid search and find that the minimum   \n245 bitrate from BPE is obtained from $4\\mathrm{k-}16\\mathrm{k}$ units and choose 4k units for all experiments (Shen et al.   \n246 [49] originally operated on $50\\mathrm{Hz}$ units, meaning that the 117bps rate obtained here is also new).   \n247 Because we want to use a $50\\mathrm{Hz}$ base encoder to match SD-HuBERT and have fine-grained boundary   \n248 control during syllable segmentation, we cannot use the 25hz mHuBERT encoder from TWIST.   \n249 Unfortunately, this means that the quality of the base encoder may be a confounding factor in our   \n250 SpeechLM evaluation. We choose Data2Vec2-base [6] as a middleground for training SpeechLMs on   \n251 syllable-like units because we find its quality enables lower bitrates than HuBERT, but it is older and   \n252 trains on less-data than mHuBERT from TWIST, and it has 6x fewer parameters than w2v-BERT,   \n253 used by AudioLM. We suspect that applying newer encoders like w2v-BERT 2 from Communication   \n254 et al. [16] could enable even better performance, which we leave to future work. We initialize   \n255 Data2Vec2 SylBoost from the same HuBERT loss boundaries as discussed in 3.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/434b44342cd2b844547518450bbc3aebe005d3f51f2b09ceaf3d57d8a7ff2cb7.jpg", "table_caption": ["Table 2: Unit Resynthesis. WER/CER results on 4-10 second examples on LibriSpeech [41] test-clean. Hz and Bitrate are measured post Run-Length-Encoding (RLE) on LibriSpeech dev-clean. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/e05e8bf4d51f60643574ff7f92f1fd43f6ee65ec5d1d527355d9355dfb6a9379.jpg", "table_caption": ["Table 3: Main SyllableLM results. We evaluate on sWUGGY (In-Vocab, All, Out-of-Vocab), sBLIMP from ZeroSpeech [39], and tStoryCloze from Hassid et al. [26]. Higher is better. \\*Estimated. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "256 5.4 Results: Evaluating Unit Quality ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "257 We evaluate the quality of our semantic units with two approaches 1) measuring correspondence   \n258 with syllables and 2) running speech resynthesis followed by ASR. To measure correspondence with   \n259 syllables, we use the development and test sets of LibriSpeech [41] and follow the approach from   \n260 Peng et al. [46], extracting timesteps for phones on using the Montreal Forced Aligner [36] and then   \n261 converting these phones into syllables with a rule-based method [25]. We evaluate the quality of   \n262 syllable boundary detection with a ground truth boundary marked as hit if a proposed boundary is   \n263 present within a tolerance threshold. We report F1, Precision, Recall, and R score. We ablate F1   \n264 scores with tolerance windows of $20\\mathrm{ms}$ and $50\\mathrm{ms}$ . Given boundaries, we also evaluate the purity of   \n265 our clusters with 4096 units, with Syllable Purity measuring the probability that a syllable is mapped   \n266 to its most corresponding cluster unit, and Cluster Purity measuring the probability that a cluster is   \n267 mapped to its most corresponding syllable unit.   \n268 Even if units do not correspond with syllables, they can still be of great use to SpeechLMs if they   \n269 can resynthesize back into speech that matches the original text. Additionally, training a resynthesis   \n270 model provides a stronger description of the semantic information contained in units than purity   \n271 metrics, which are especially problematic because SD-HuBERT does not provide a unit at every   \n272 timestep while our methods do, possibly making cluster and syllable purity evaluation unreliable.   \n273 To evaluate resynthesized speech, we follow AudioLM and measure Word Error Rate (WER) and   \n274 Character Error Rate (CER) on the set of 4-10 second segments from LibriSpeech test-clean. For   \n275 ASR, we follow VALL-E [54] and use the public HuBERT-base CTC ASR model provided by [28].   \n276 Table 1 shows our syllabic correspondence results against the prior-state-of-the-art SD-HuBERT [12]   \n277 and the HuBERT-based feature similarity strategy from [46]. Applying our LossPred followed by   \n278 agglomeration strategy on either HuBERT or Data2Vec2 improves performance across-the-board   \n279 except for in cluster purity. Although it LossPred SD-HuBERT in performance, it pushes the boundary   \n280 for syllable recognition using HuBERT without additional training. We justify using LossPred as a   \n281 bootstrapping source instead of a HuBERT similarity metric [46, 43] in Table 4, which we discuss   \n282 more in Appendix A.4. Improvement across iterations and with different loss initialization can be   \n283 found in Table 4. We explore the effects of changing the unit rate on boundary predictions in Table 5.   \n284 We compare against prior SpeechLMs and demonstrate the step-by-step changes used to improve   \n285 unit cluster re-synthesis quality as compared to SD-HuBERT in table 2. We observe over a $50\\%$   \n286 decrease in WER and CER by applying our method using the SD-HuBERT base parameters. We   \n287 further decrease WER by a third by using Data2Vec2, and from there by modifying the unit sample   \n288 rate and number of clusters can reach as low as 2048 clusters and a WER of $7\\%$ . These results   \n289 demonstrate by far the lowest bitrate we are aware of for \u2018reasonable-quality\u2019 self-supervised-unit   \n290 resynthesis. Resynthesis for all models we train is done back into mHuBERT- $.25\\mathrm{Hz}$ units, bounding   \n291 potential quality at a WER of $6.3\\%$ . ", "page_idx": 6}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/b29c34ba588305d8a6e94ff41560adde52c93c99cf6ab23cbb75ac4fa8c45c17.jpg", "table_caption": ["Table 4: Boundary detaction with different initialization using HuBERT on LS dev-clean ", "Table 5: Controllability of unit rate measured on LibriSpeech dev-clean boundary detection. D2V2, 50ms threshold. P:Phone, "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 6: Holding number of units and unit rate constant. ZeroSpeech development set. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/d869bdc4e4da3439005caddd84eacb6faafeac6e1ac87a698fc13462e6b7a19d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "292 5.5 Results: Generative Spoken Lanauage Modeling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "293 The end-to-end GSLM pipeline is deep, and so it is essential to have metrics to independently   \n294 evaluate different stages. To evaluate our SpeechLM stage, we follow Lakhotia et al. [34] and use   \n295 the ZeroSpeech [39] sWUGGY and sBLIMP evaluation. The sWUGGY dataset tasks the model   \n296 with outputting a higher perplexity on similar but fake spoken words (e.g. brick vs blick). Similarly,   \n297 the sBLIMP dataset checks syntactic correctness (e.g. the dog sleeps vs the dogs sleeps). We also   \n298 evaluate the SpeechLM on the tSC set from Hassid et al. [26], which operates like the ZeroSpeech   \n299 metrics on a spoken version of the StoryCloze dataset [37] with the last sentence in negative samples   \n300 randomly chosen. For all metrics we follow prior work and output the mean perplexity per token.   \n301 The results for SpeechLM metrics are in 3. We reimplement a 90M parameter model using the   \n302 TWIST mHuBERT units without textually-pretrained initialization (Cold-Init in the TWIST paper)   \n303 on our data split for an all-else-held equal comparison on unit type. We also train on BPE units as   \n304 described in 5.3, the next-lowest bitrate units outside of our model. For textual toplines, we train   \n305 on corresponding LibriLight text transcripts from Kang et al. [31] and convert text to phones and   \n306 syllables using the same methods as in Section 5.4. We find that training with each of our syllable   \n307 units improves perfromance across-the-board on sBLIMP and tSC versus comparably-sized models   \n308 and is competitive against larger models. In fact, with under 90 hours of training, SyllableLM   \n309 outperforms even the 13B parameter TWIST on sBLIMP. We also beat AudioLM on the full split   \n310 of sWUGGY with $30\\mathrm{x}$ less GPU compute and TWIST model sizes up to 1.3B parameters. On tSC,   \n311 we observe that SyllableLM large approaches performance of the textual topline, outperforming all   \n312 models except for TWIST 13B. Due to compute requirements, we are unable to scale further.   \n313 We notice a decrease in sWUGGY quality with our   \n314 $5.0\\mathrm{Hz}$ units, which we suspect is in part caused by   \n315 the short length of the dataset audios making input to  \n316 kenization excessively short. We further ablate these   \n317 differences in table 6. We also find that BPE, despite   \n318 having the lowest bitrate outside of our approach,   \n319 does not approach the quality gains created by our   \n320 syllable-like units.   \n321 To measure the quality of end-to-end continuations,   \n322 we use the VERT $@$ O-PPX and PPX $@$ O-VERT met  \n323 rics proposed in Lakhotia et al. [34], which are shown   \n324 to be the automatic metrics correlating best with hu  \n$@$   \n325 man meaningfulness judgements. VERT O-PPX measures the diversity of output at the sampling   \n326 temperature where the perplexity of generated audio transcriptions matches that of the ground truth   \n327 text, and PPX $@$ O-VERT performs the inverse. Like Lakhotia et al. [34], we generate 10-second   \n328 continuations from 1000 randomly sampled 3-second crops from LibriSpeech test-clean, and measure   \n329 results using their provided environment and parameters. We report these in Table 7 with two sigma   \n330 error bars, outperforming TWIST 300M and 1.3B. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/e343eea22365590a93da5f86afdc1987827014c761eac66b4350fe2d9ae2042f.jpg", "table_caption": ["Table 7: Continuation Metrics. We measure PPX $@$ Oracle-VERT and VERT $@$ OraclePPX as implemented in Lakhotia et al. [34] "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "331 6 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "332 Though speech is a very general medium, there are a number of challenges in adapting our methods   \n333 to generate low-bitrate units angled towards other audio tasks or other domains such as vision.   \n334 Our LossPred technique assumes that the semantic units to learn are separable across time, one  \n335 dimensional, and contiguous. In audio tasks or settings with multiple speakers, sounds or words   \n336 can occur simultaneously and can\u2019t be separated across the time dimension. Images and video   \n337 are multi-dimensional, not allowing a trivial sliding window approach. Images and video can also   \n338 have partially occluded or overlapping objects, violating continuity. Furthermore, it is still unclear   \n339 whether our longer units will be better at scaling to larger datasets, such as the $4.5\\mathrm{M}$ hours used by   \n340 Communication et al. [16]. For example, our semantic units may be losing out on useful paralinguistic   \n341 features like tone whose impact is only salient on non-audiobooks or at scale. It is also important to   \n342 note that large textual language models can have harmful effects, such as enabling the generation of   \n343 misinformation in mass. Although generative spoken language models have not yet caught up to their   \n344 textual counterparts, it is still necessary to be aware of potential misuses that could arise in the future. ", "page_idx": 8}, {"type": "text", "text": "345 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "346 We introduce a new method to tokenize speech for use in GSLMs. We do this by proposing a method   \n347 to elicit syllabic organization in pretrained speech encoder models, bootstrapping a feature-space   \n348 agglomeration algorithm from a static analysis of correlations in off-the-shelf teacher and student   \n349 model losses across time. We demonstrate the success of our technique both in having strong   \n350 associations with syllables and as an extremlely low-bitrate codec for speech resynthesis. Using this   \n351 tokenization strategy, we successfully train SyllableLM, a SpeechLM that out-performs comparable   \n352 state-of-the-art approaches across a diverse range of metrics with a significant inference speedup.   \n353 We further ablate several design decisions such as quantitization strategy, loss initialization, and   \n354 the effects of controllability for downstream usecases. Compression is a crucial aspect of learning,   \n355 and we hope that these significant improvements in the unsupervised learning of low-bitrate speech   \n356 units can serve as a foundation for approaches towards understanding spoken language and general   \n357 representation learning. ", "page_idx": 8}, {"type": "text", "text": "358 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "359 [1] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts,   \n360 M. Tagliasacchi, M. Sharif,i N. Zeghidour, and C. Frank. Musiclm: Generating music from text, 2023.   \n361 [2] R. J. Algayres, Y. Adi, T. A. Nguyen, J. Copet, G. Synnaeve, B. Sagot, and E. Dupoux. Generative spoken   \n362 language model based on continuous word-sized audio tokens, 2023. URL https://openreview.net/   \n363 forum?id=a0e7x2EuFO.   \n364 [3] A. Baevski, H. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: a framework for self-supervised learning   \n365 of speech representations. In Proceedings of the 34th International Conference on Neural Information   \n366 Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.   \n367 [4] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli. Unsupervised speech recognition. In A. Beygelzimer,   \n368 Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems,   \n369 2021. URL https://openreview.net/forum?id=QmxFsofRvW9.   \n370 [5] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. data2vec: A general framework for   \n371 self-supervised learning in speech, vision and language, 2022.   \n372 [6] A. Baevski, A. Babu, W.-N. Hsu, and M. Auli. Efficient self-supervised learning with contextualized target   \n373 representations for vision, speech and language. In Proceedings of the 40th International Conference on   \n374 Machine Learning, ICML\u201923. JMLR.org, 2023.   \n375 [7] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman. Token merging: Your vit   \n376 but faster. In The Eleventh International Conference on Learning Representations, 2023. URL https:   \n377 //openreview.net/forum?id=JroZRaRw7Eu.   \n378 [8] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharif,i D. Roblek, O. Teboul,   \n379 D. Grangier, M. Tagliasacchi, and N. Zeghidour. Audiolm: a language modeling approach to audio   \n380 generation, 2023.   \n381 [9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,   \n382 A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,   \n383 C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,   \n384 A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ran  \n385 zato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,   \n386 volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.   \n387 cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.   \n388 [10] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in   \n389 self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision   \n390 (ICCV), 2021.   \n391 [11] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou,   \n392 S. Ren, Y. Qian, Y. Qian, M. Zeng, and F. Wei. Wavlm: Large-scale self-supervised pre-training for full   \n393 stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16:1505\u20131518, 2021. URL   \n394 https://api.semanticscholar.org/CorpusID:239885872.   \n395 [12] C. J. Cho, A. Mohamed, S.-W. Li, A. W. Black, and G. K. Anumanchipalli. Sd-hubert: Sentence-level   \n396 self-distillation induces syllabic organization in hubert. 2024.   \n397 [13] J.-C. Chou, C.-M. Chien, W.-N. Hsu, K. Livescu, A. Babu, A. Conneau, A. Baevski, and M. Auli. Toward   \n398 joint language modeling for speech units and text. In Conference on Empirical Methods in Natural   \n399 Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:264128173.   \n400 [14] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. w2v-bert: Combining contrastive   \n401 learning and masked language modeling for self-supervised speech pre-training. 2021 IEEE Automatic   \n402 Speech Recognition and Understanding Workshop (ASRU), pages 244\u2013250, 2021. URL https://api.   \n403 semanticscholar.org/CorpusID:237048255.   \n404 [15] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. Electra: Pre-training text encoders as discriminators   \n405 rather than generators. In International Conference on Learning Representations, 2020. URL https:   \n406 //openreview.net/forum?id=r1xMH1BtvB.   \n407 [16] S. Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler,   \n408 P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim, J. Hoffman, M.-J. Hwang, H. Inaguma, C. Klaiber,   \n409 I. Kulikov, P. Li, D. Licht, J. Maillard, R. Mavlyutov, A. Rakotoarison, K. R. Sadagopan, A. Ramakrishnan,   \n410 T. Tran, G. Wenzek, Y. Yang, E. Ye, I. Evtimov, P. Fernandez, C. Gao, P. Hansanti, E. Kalbassi, A. Kallet,   \n411 A. Kozhevnikov, G. M. Gonzalez, R. S. Roman, C. Touret, C. Wong, C. Wood, B. Yu, P. Andrews,   \n412 C. Balioglu, P.-J. Chen, M. R. Costa-juss\u00e0, M. Elbayad, H. Gong, F. Guzm\u00e1n, K. Heffernan, S. Jain, J. Kao,   \n413 A. Lee, X. Ma, A. Mourachko, B. Peloquin, J. Pino, S. Popuri, C. Ropers, S. Saleem, H. Schwenk, A. Sun,   \n414 P. Tomasello, C. Wang, J. Wang, S. Wang, and M. Williamson. Seamless: Multilingual expressive and   \n415 streaming speech translation, 2023.   \n416 [17] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D\u2019efossez. Simple and   \n417 controllable music generation. ArXiv, abs/2306.05284, 2023. URL https://api.semanticscholar.   \n418 org/CorpusID:259108357.   \n419 [18] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski. Vision transformers need registers. In The Twelfth   \n420 International Conference on Learning Representations, 2024. URL https://openreview.net/forum?   \n421 id=2dnO3LLiJ1.   \n422 [19] A. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. Transactions   \n423 on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=   \n424 ivCd8z8zR2. Featured Certification, Reproducibility Certification.   \n425 [20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers   \n426 for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019   \n427 Conference of the North American Chapter of the Association for Computational Linguistics: Human   \n428 Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota,   \n429 June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:   \n430 //aclanthology.org/N19-1423.   \n431 [21] Z. Du, S. Zhang, K. Hu, and S. Zheng. Funcodec: A fundamental, reproducible and integrable open-source   \n432 toolkit for neural speech codec. ArXiv, abs/2309.07405, 2023. URL https://api.semanticscholar.   \n433 org/CorpusID:261823065.   \n434 [22] A. Elkahky, W.-N. Hsu, P. Tomasello, T.-A. Nguyen, R. Algayres, Y. Adi, J. Copet, E. Dupoux, and   \n435 A. Mohamed. Do coarser units benefit cluster prediction-based speech pre-training? In ICASSP 2023 -   \n436 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135,   \n437 2023. doi: 10.1109/ICASSP49357.2023.10096788.   \n438 [23] T. S. Fuchs and Y. Hoshen. Unsupervised word segmentation using temporal gradient pseudo-labels. In   \n439 ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),   \n440 pages 1\u20135, 2023. doi: 10.1109/ICASSP49357.2023.10095363.   \n441 [24] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. R. Glass. Ssast: Self-supervised audio spectrogram transformer.   \n442 ArXiv, abs/2110.09784, 2021. URL https://api.semanticscholar.org/CorpusID:239024736.   \n443 [25] K. Gorman. Syllabify. https://github.com/kylebgorman/syllabify/tree/master, 2014.   \n444 [26] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk, J. Copet, A. D\u00e9fossez, G. Synnaeve,   \n445 E. Dupoux, R. Schwartz, and Y. Adi. Textually pretrained speech language models. In Thirty-seventh   \n446 Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?   \n447 id=UlHueVjAKr.   \n448 [27] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners.   \n449 arXiv:2111.06377, 2021.   \n450 [28] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert: Self  \n451 supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Trans. Audio,   \n452 Speech and Lang. Proc., 29:3451\u20133460, oct 2021. ISSN 2329-9290. doi: 10.1109/TASLP.2021.3122291.   \n453 URL https://doi.org/10.1109/TASLP.2021.3122291.   \n454 [29] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang, Z. Wu, T. Qin,   \n455 X.-Y. Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao. Naturalspeech 3: Zero-shot speech   \n456 synthesis with factorized codec and diffusion models. ArXiv, abs/2403.03100, 2024. URL https:   \n457 //api.semanticscholar.org/CorpusID:268248388.   \n458 [30] J. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar\u00e9, J. Karadayi, V. Liptchinsky,   \n459 R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.   \n460 Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE Inter  \n461 national Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669\u20137673, 2020.   \n462 https://github.com/facebookresearch/libri-light.   \n463 [31] W. Kang, X. Yang, Z. Yao, F. Kuang, Y. Yang, L. Guo, L. Lin, and D. Povey. Libriheavy: a 50,000 hours   \n464 asr corpus with punctuation casing and context, 2023.   \n465 [32] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u2019efossez, J. Copet, D. Parikh, Y. Taigman, and   \n466 Y. Adi. Audiogen: Textually guided audio generation. ArXiv, abs/2209.15352, 2022. URL https:   \n467 //api.semanticscholar.org/CorpusID:252668761.   \n468 [33] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with im  \n469 proved rvqgan. ArXiv, abs/2306.06546, 2023. URL https://api.semanticscholar.org/CorpusID:   \n470 259138883.   \n471 [34] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski,   \n472 A. Mohamed, and E. Dupoux. On generative spoken language modeling from raw audio. Transactions   \n473 of the Association for Computational Linguistics, 9:1336\u20131354, 2021. doi: 10.1162/tacl_a_00430. URL   \n474 https://aclanthology.org/2021.tacl-1.79.   \n475 [35] I. Malioutov and R. Barzilay. Minimum cut model for spoken lecture segmentation. In N. Calzolari,   \n476 C. Cardie, and P. Isabelle, editors, Proceedings of the 21st International Conference on Computational   \n477 Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 25\u201332,   \n478 Sydney, Australia, July 2006. Association for Computational Linguistics. doi: 10.3115/1220175.1220179.   \n479 URL https://aclanthology.org/P06-1004.   \n480 [36] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal Forced Aligner:   \n481 Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech 2017, pages 498\u2013502, 2017. doi:   \n482 10.21437/Interspeech.2017-1386.   \n483 [37] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A   \n484 corpus and cloze evaluation for deeper understanding of commonsense stories. In K. Knight, A. Nenkova,   \n485 and O. Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the   \n486 Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego,   \n487 California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL   \n488 https://aclanthology.org/N16-1098.   \n489 [38] T. Nguyen, B. Muller, B. Yu, M. R. Costa-juss\u00e0, M. Elbayad, S. Popuri, P.-A. Duquenne, R. Algayres,   \n490 R. Mavlyutov, I. Gat, G. Synnaeve, J. Pino, B. Sagot, and E. Dupoux. Spirit-lm: Interleaved spoken and   \n491 written language model. ArXiv, abs/2402.05755, 2024. URL https://api.semanticscholar.org/   \n492 CorpusID:267547793.   \n493 [39] T. A. Nguyen, M. de Seyssel, P. Roz\u00e9, M. Rivi\u00e8re, E. Kharitonov, A. Baevski, E. Dunbar, and E. Dupoux.   \n494 The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language   \n495 modeling, 2020.   \n496 [40] NVIDIA, P. Vingelmann, and F. H. Fitzek. Cuda, release: 10.2.89, 2020. URL https://developer.   \n497 nvidia.com/cuda-toolkit.   \n498 [41] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: An asr corpus based on public domain   \n499 audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),   \n500 pages 5206\u20135210, 2015. doi: 10.1109/ICASSP.2015.7178964.   \n501 [42] A. Pasad, B. Shi, and K. Livescu. Comparative layer-wise analysis of self-supervised speech models. pages   \n502 1\u20135, 06 2023. doi: 10.1109/ICASSP49357.2023.10096149.   \n503 [43] A. Pasad, C.-M. Chien, S. Settle, and K. Livescu. What Do Self-Supervised Speech Models Know About   \n504 Words? Transactions of the Association for Computational Linguistics, 12:372\u2013391, 04 2024. ISSN   \n505 2307-387X. doi: 10.1162/tacl_a_00656. URL https://doi.org/10.1162/tacl_a_00656.   \n506 [44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,   \n507 L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,   \n508 B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance   \n509 deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and   \n510 R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Asso  \n511 ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/   \n512 bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.   \n513 [45] P. Peng and D. Harwath. Word discovery in visually grounded, self-supervised speech models. In   \n514 Interspeech, pages 2823\u20132827, 09 2022. doi: 10.21437/Interspeech.2022-10652.   \n515 [46] P. Peng, S.-W. Li, O. R\u00e4s\u00e4nen, A. Mohamed, and D. Harwath. Syllable segmentation and cross-lingual   \n516 generalization in a visually grounded, self-supervised speech model. In Interspeech, 2023.   \n517 [47] P. Peng, P.-Y. B. Huang, D. Li, A. Mohamed, and D. F. Harwath. Voicecraft: Zero-shot speech editing and   \n518 text-to-speech in the wild. ArXiv, abs/2403.16973, 2024. URL https://api.semanticscholar.org/   \n519 CorpusID:268681356.   \n520 [48] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux. Speech   \n521 Resynthesis from Discrete Disentangled Self-Supervised Representations. In Proc. Interspeech 2021,   \n522 2021.   \n523 [49] F. Shen, Y. Guo, C. Du, X. Chen, and K. Yu. Acoustic bpe for speech generation with discrete tokens. In   \n524 ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),   \n525 pages 11746\u201311750, 2024. doi: 10.1109/ICASSP48485.2024.10446063.   \n526 [50] Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen. Ella-v: Stable neural codec language modeling with   \n527 alignment-guided sequence reordering, 2024.   \n528 [51] Z.-H. Tan, A. kr. Sarkar, and N. Dehak. rvad: An unsupervised segment-based robust voice activ  \n529 ity detection method. Computer Speech Language, 59:1\u201321, 2020. ISSN 0885-2308. doi: https:   \n530 //doi.org/10.1016/j.csl.2019.06.005. URL https://www.sciencedirect.com/science/article/   \n531 pii/S0885230819300920.   \n532 [52] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,   \n533 F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation lan  \n534 guage models. ArXiv, abs/2302.13971, 2023. URL https://api.semanticscholar.org/CorpusID:   \n535 257219404.   \n536 [53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin.   \n537 Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,   \n538 and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran As  \n539 sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/   \n540 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n541 [54] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and   \n542 F. Wei. Neural codec language models are zero-shot text to speech synthesizers, 2023.   \n543 [55] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet   \n544 classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n545 10684\u201310695, 2020. doi: 10.1109/CVPR42600.2020.01070.   \n546 [56] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y. Zou. Hifi-codec: Group-residual vector quantization   \n547 for high fidelity audio codec. ArXiv, abs/2305.02765, 2023. URL https://api.semanticscholar.   \n548 org/CorpusID:258479750.   \n549 [57] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, Z. Zhao, S. Watanabe,   \n550 and H. Meng. Uniaudio: An audio foundation model toward universal audio generation, 2023.   \n551 [58] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural   \n552 audio codec, 2021.   \n553 [59] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V.   \n554 Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and   \n555 L. Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022. URL   \n556 https://api.semanticscholar.org/CorpusID:248496292.   \n557 [60] X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu. Speechtokenizer: Unified speech tokenizer for speech   \n558 large language models. ArXiv, abs/2308.16692, 2023. URL https://api.semanticscholar.org/   \n559 CorpusID:261394297. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "560 A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "561 A.1 Randomly Sampled Example Segmentations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "562 We provide randomly sampled example segmentations from the LibriSpeech [41] dev-clean set.   \n563 All models are the second iteration of Data2Vec2, which we use for our SyllableLM experiments   \n564 in Section 5.5. Top: Feature Self-Similarity matrix, darker green is closer. Segmented cuts span   \n565 vertically in blue from the top, ground truth boundaries span vertically in red at the bottom. Bottom:   \n566 time-aligned Mel-Spectrogram. We call attention to the interesting behavior of global correspondences ", "page_idx": 13}, {"type": "image", "img_path": "tVO3b68Oyp/tmp/e22651319036947a6e793e9e5aa3c7409f87fc852aa9e304d03e5298339d4bce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "tVO3b68Oyp/tmp/1814130c33a89d2afc16ed85cb99f95d705634f14f72ed6b31f4deef8e8ccbb9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "tVO3b68Oyp/tmp/6ad7e975834cdc02d9c760557ea2fbc2ac564c5611f3abd99d8826a0370f8820.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "tVO3b68Oyp/tmp/9cc90a1081488364c08e9b46146c91db3f6a7bf6c1694a0ffc0d496be353ff80.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "568 A.2 Hardware And Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "569 We implement all experiments using NVIDIA A40 46GB GPUS with a Intel Xeon Gold 6226R CPU   \n570 $@$ 2.90GHz. Estimated speeds are made using these results as well as scaling from Zhang et al. [59].   \n571 Hyperparameters for pretraining our models are below. We note that the Batch Size is in terms of   \n572 tokens, which means that higher unit rates will have fewer seconds of raw audio per batch to keep   \n573 GPU compute roughly equal per model. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/2e21517bac24760c7e9f6b7302167dc5418ff540a5e4c5cb2f9ec47ea076aaa2.jpg", "table_caption": ["Table 8: Speech pre-training hyper-parameters. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "574 A.3 Speedup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 9: Inference speed results, measured in Real-Time-Factor, the processed seconds per second. We use 32 Batches with 25 seconds of audio each, which matches the length of our training data. 1 GPU, 16 Cores. Standard error less than 1 sec/sec ", "page_idx": 15}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/4f34a250b396ba53171f2af806e5d14a988a4251228642ca9e213eb7a4d2d43b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "575 A.4 Discussion: Other Bootstrapping Strategies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "576 Of course, there already exist several strategies for unsupervised syllable and word segmentation such   \n577 as Fuchs and Hoshen [23] and Pasad et al. [43] that could be used to bootstrap our first pseudolabels.   \n578 We find however in our experiments that these approaches, which are calculated using the similarity or   \n579 dissimilarity of HuBERT embeddings across time, converge to a lower quality in bootstrapping than   \n580 our proposed method. We suspect that this may be caused by the fact that although the representations   \n581 of these models correlate with boundaries, there is no modeling in the pretraining loss pushing the   \n582 representations to linearly separate across semantic differences. Meanwhile, the loss is forced to   \n583 change across semantic boundaries due to the difficulty of language modeling, albeit noisily.   \n585 Below are sample continuations generated with temperature sampling parameter chosen to best match   \n586 Oracle VERT diversity scores. We provide continuations of roughly 3 seconds of audio, sampled   \n587 randomly from LibriSpeech test-clean. This text is given as output by our HuBERT ASR Model from   \n588 [28], with transcription errors present and with no additional modifications. The source text is bolded,   \n589 and sometimes cuts off mid-word, which can behave differently per sample based on unit rate and   \n590 quantization artifacts. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/33a9587e012b95fe263fa4c54fb5902e6316c54b868af21833ca7d565fcf05f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "tVO3b68Oyp/tmp/75f3e3c70c85100766864c41553250c9c583261423d19ccdf2f8f81a394a908a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "592 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Justification: We make concrete claims in the abstract and introduction regarding the performance of our low bitrate units and SpeechLM results. We address each of these explicitly within the experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide a Limitations Section to address the assumptions we make for our LossPred algorithm and on the nascent nature of SpeechLMs and their relation to scaling. We explicitly describe why we choose the models we do, and acknowledge where in the field it is hard to draw all-else-held-equal experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "644 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "661 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Justification: We commit throughout our work to being entirely reproducable and open source. We explicitly cite all architectures used, their hyperparameters, all datasets used, and all models used to calculate metrics. We plan on releasing all SyllablLM model checkpoints and code, and all datasets used are fully open-source. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 19}, {"type": "text", "text": "697 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n698 authors are welcome to describe the particular way they provide for reproducibility.   \n699 In the case of closed-source models, it may be that access to the model is limited in   \n700 some way (e.g., to registered users), but it should be possible for other researchers   \n701 to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Justification: We plan on open sourcing all code for training and evaluation, including model parameter checkpoints. All datasets used are open source, and instructions for downloading them can be found as cited. For training transformers, we use FAIRSEQ https://github.com/facebookresearch/fairseq. For Unit Resynthesis, we adapt the code from https://github.com/jasonppy/syllable-discovery. For SpeechLM Evaluation, we use methods described at ? ]. As with most in-reserach code, absolute file paths and local environment modifications prohibit the code from being both deanonymized and in a runnable state at the current time as suggested by https://nips.cc/public/ guides/CodeSubmissionPolicy. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "736 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We specify all hyperparameters for model training and evaluation throughout the paper and in the appendix. All data splits are explicit and referenced in their tables. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "749 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "750 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n751 information about the statistical significance of the experiments?   \n52 Answer: [No]   \n753 Justification: All datasets experimented on contain at least several thousand examples,   \n754 meaning that experimental significance results would be redundant and insignificant relative   \n755 to their metrics. Because of this, we follow prior published work for each experiment and   \n756 do not report these significance metrics. We also do not have the GPU resources to attempt   \n757 significance errors across multiple training runs or random seeds, as even our smallest   \n758 models taking 80 hours to train.   \n759 Guidelines:   \n60 \u2022 The answer NA means that the paper does not include experiments.   \n761 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n762 dence intervals, or statistical significance tests, at least for the experiments that support   \n763 the main claims of the paper.   \n64 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n65 example, train/test split, initialization, random drawing of some parameter, or overall   \n66 run with given experimental conditions).   \n767 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n768 call to a library function, bootstrap, etc.)   \n69 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n770 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n771 of the mean.   \n772 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n773 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n774 of Normality of errors is not verified.   \n775 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n776 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n777 error rates).   \n778 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n779 they were calculated and reference the corresponding figures or tables in the text.   \n780 8. Experiments Compute Resources   \n781 Question: For each experiment, does the paper provide sufficient information on the com  \n782 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n783 the experiments?   \n784 Answer: [Yes]   \n785 Justification: We mention our system setup in the Appendix in A.2. We include GPU hours   \n786 used for all main SpeechLM results, and make experiments significantly cheaper than prior   \n787 work like Borsos et al. [8], Hassid et al. [26].   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper does not include experiments.   \n790 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n791 or cloud provider, including relevant memory and storage.   \n92 \u2022 The paper should provide the amount of compute required for each of the individual   \n793 experimental runs as well as estimate the total compute.   \n794 \u2022 The paper should disclose whether the full research project required more compute   \n795 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n796 didn\u2019t make it into the paper).   \n797 9. Code Of Ethics ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "801 Justification: All datasets used for training and experiments use open-source and licensed   \n802 data. We do not use Human Judges. We focus purely on generating semantic continuations,   \n803 making our approach entirely orthogonal to generating realistic data that can mimic speaker   \n804 voices such as Wang et al. [54]   \n805 Guidelines:   \n806 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n807 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n808 deviation from the Code of Ethics.   \n809 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n810 eration due to laws or regulations in their jurisdiction).   \n811 10. Broader Impacts   \n812 Question: Does the paper discuss both potential positive societal impacts and negative   \n813 societal impacts of the work performed?   \n814 Answer: [Yes]   \n815 Justification: Generative Spoken Language Models have not yet caught up to their textual   \n816 counterparts, however it is still important to note that with increased scaling and methods   \n817 research GSLM systems may eventually be able to reach parity with today\u2019s systems in   \n818 terms of quality. Because of this, we address the ethical considerations of generative models   \n819 in ??   \n820 Guidelines:   \n821 \u2022 The answer NA means that there is no societal impact of the work performed.   \n822 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n823 impact or why the paper does not address societal impact.   \n824 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n825 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n826 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n827 groups), privacy considerations, and security considerations.   \n828 \u2022 The conference expects that many papers will be foundational research and not tied   \n829 to particular applications, let alone deployments. However, if there is a direct path to   \n830 any negative applications, the authors should point it out. For example, it is legitimate   \n831 to point out that an improvement in the quality of generative models could be used to   \n832 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n833 that a generic algorithm for optimizing neural networks could enable people to train   \n834 models that generate Deepfakes faster.   \n835 \u2022 The authors should consider possible harms that could arise when the technology is   \n836 being used as intended and functioning correctly, harms that could arise when the   \n837 technology is being used as intended but gives incorrect results, and harms following   \n838 from (intentional or unintentional) misuse of the technology.   \n839 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n840 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n841 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n842 feedback over time, improving the efficiency and accessibility of ML).   \n843 11. Safeguards   \n844 Question: Does the paper describe safeguards that have been put in place for responsible   \n845 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n846 image generators, or scraped datasets)?   \n847 Answer: [NA]   \n848 Justification: The current quality of GSLM systems trails far behind language models in   \n849 terms of their capabilities for misuse, and this work instead focuses toward the direction of   \n850 discovering better representation learning algorithms. All output audio comes from a single   \n851 speaker, and our approach is orthogonal to voice conversion methods.   \n852 Guidelines:   \n853 \u2022 The answer NA means that the paper poses no such risks.   \n854 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n855 necessary safeguards to allow for controlled use of the model, for example by requiring   \n856 that users adhere to usage guidelines or restrictions to access the model or implementing   \n857 safety filters.   \n858 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n859 should describe how they avoided releasing unsafe images.   \n860 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n861 not require this, but we encourage authors to take this into account and make a best   \n862 faith effort. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "863 12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "864 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n865 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n866 properly respected?   \n867 Answer: Yes   \n868 Justification: All works used have licenses that do not require citing and are available for   \n69 both research and commercial use. We properly cite every dataset used when mentioned in   \n870 our work. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All models created and trained are standard transformer models, which have been robustly documented for ease-of-use. Our model parameters can be dropped in to existing model pipelines such as that of Zhang et al. [59] on online distribution services such as https://huggingface.co/ ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "904 14. Crowdsourcing and Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]