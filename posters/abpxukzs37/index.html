<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Diffusion PID: Interpreting Diffusion via Partial Information Decomposition &#183; NeurIPS 2024</title>
<meta name=title content="Diffusion PID: Interpreting Diffusion via Partial Information Decomposition &#183; NeurIPS 2024"><meta name=description content="DiffusionPID unveils the secrets of text-to-image diffusion models by decomposing text prompts into unique, redundant, and synergistic components, providing insights into how individual words and thei..."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ Carnegie Mellon University,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/abpxukzs37/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/abpxukzs37/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="Diffusion PID: Interpreting Diffusion via Partial Information Decomposition"><meta property="og:description" content="DiffusionPID unveils the secrets of text-to-image diffusion models by decomposing text prompts into unique, redundant, and synergistic components, providing insights into how individual words and thei‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ Carnegie Mellon University"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/abpxukzs37/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/abpxukzs37/cover.png"><meta name=twitter:title content="Diffusion PID: Interpreting Diffusion via Partial Information Decomposition"><meta name=twitter:description content="DiffusionPID unveils the secrets of text-to-image diffusion models by decomposing text prompts into unique, redundant, and synergistic components, providing insights into how individual words and thei‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"Diffusion PID: Interpreting Diffusion via Partial Information Decomposition","headline":"Diffusion PID: Interpreting Diffusion via Partial Information Decomposition","abstract":"DiffusionPID unveils the secrets of text-to-image diffusion models by decomposing text prompts into unique, redundant, and synergistic components, providing insights into how individual words and thei\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/abpxukzs37\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ Carnegie Mellon University"],"mainEntityOfPage":"true","wordCount":"5438"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/abpxukzs37/cover_hu14929000408269002535.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/abpxukzs37/>Diffusion PID: Interpreting Diffusion via Partial Information Decomposition</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Diffusion PID: Interpreting Diffusion via Partial Information Decomposition</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>5438 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">26 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/aBpxukZS37/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/aBpxukZS37/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-carnegie-mellon-university/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Carnegie Mellon University</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#diffusionpid-intro>DiffusionPID Intro</a></li><li><a href=#pid-methodology>PID Methodology</a></li><li><a href=#bias-experiments>Bias Experiments</a></li><li><a href=#future-of-pid>Future of PID</a></li><li><a href=#limitations>Limitations</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#diffusionpid-intro>DiffusionPID Intro</a></li><li><a href=#pid-methodology>PID Methodology</a></li><li><a href=#bias-experiments>Bias Experiments</a></li><li><a href=#future-of-pid>Future of PID</a></li><li><a href=#limitations>Limitations</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>aBpxukZS37</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Shaurya Rajat Dewan et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=aBpxukZS37" target=_blank role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/94580 target=_blank role=button>‚Üó NeurIPS Homepage
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://huggingface.co/spaces/huggingface/paper-central?tab=tab-chat-with-paper&amp;paper_id=aBpxukZS37&amp;paper_from=neurips" target=_blank role=button>‚Üó Chat</a></p><audio controls><source src=https://ai-paper-reviewer.com/aBpxukZS37/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Text-to-image diffusion models, while powerful, lack transparency, making bias detection and control difficult. Understanding how these models process visual-semantic relationships is crucial for improvement. Existing methods offer limited insight into the complex interplay between individual words and their combined effect on image generation.</p><p>DiffusionPID addresses this by applying partial information decomposition, breaking down prompts into their unique, redundant, and synergistic components. This novel technique helps pinpoint how individual tokens and their interactions influence the final image, revealing biases, ambiguity, and synergy within the model. It provides a fine-grained analysis of model characteristics and enables effective prompt intervention, leading to more reliable and controllable image generation.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-aedd75fdb34ab5a20fc137fb2c55aa2d></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-aedd75fdb34ab5a20fc137fb2c55aa2d",{strings:[" DiffusionPID uses partial information decomposition to analyze how individual words and their relationships influence image generation. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-27a9d5954c10341b4f13c11d6a5cbc79></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-27a9d5954c10341b4f13c11d6a5cbc79",{strings:[" The method effectively reveals biases (gender, ethnicity) and ambiguity in the models' processing of text. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-9ac5ef6b92afd19236a5b42363a4eed2></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-9ac5ef6b92afd19236a5b42363a4eed2",{strings:[" DiffusionPID enables prompt intervention by identifying and removing redundant words to refine image outputs. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers working with text-to-image diffusion models because <strong>it introduces DiffusionPID, a novel technique for interpreting these models&rsquo; inner workings.</strong> This method enhances our understanding of how these models handle complex prompts, visualize biases, and improve model control, paving the way for building more robust and interpretable AI systems. Its information-theoretic approach offers a novel perspective for analyzing model dynamics, which is highly relevant to current interpretability research.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_0_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the effectiveness of the DiffusionPID method in interpreting the relationships between different components of text prompts and the resulting images. It shows that DiffusionPID successfully identifies visual similarities, such as the seam of a tennis ball being similar to that of a baseball (Uniqueness), the contextual relationships between words (Synergy), and redundant information (Redundancy) within a prompt, providing insights into how the diffusion model processes the information to generate the final image.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/tables_5_1.jpg alt></figure></p><blockquote><p>This table presents the results of an experiment investigating gender bias in a diffusion model. For a set of 188 common occupations, the model generated images using prompts that included the occupation paired with either &lsquo;male&rsquo; or &lsquo;female.&rsquo; The table shows the redundancy values, which are normalized across the entire dataset to the range [0, 1], for each gender and occupation. High redundancy indicates that the model strongly associates a particular gender with a given occupation, suggesting gender bias. Lower redundancy suggests less association.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">DiffusionPID Intro<div id=diffusionpid-intro class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#diffusionpid-intro aria-label=Anchor>#</a></span></h4><p>The hypothetical &ldquo;DiffusionPID Intro&rdquo; section would likely introduce the core concept of DiffusionPID, a novel technique for interpreting the inner workings of text-to-image diffusion models. It would emphasize the <strong>lack of transparency</strong> in these models, highlighting the need for methods to understand their decision-making processes. The introduction would then position DiffusionPID as a solution, emphasizing its use of <strong>partial information decomposition (PID)</strong> from information theory. This section would likely also briefly describe how DiffusionPID analyzes the input text prompt, decomposing it into its constituent parts to reveal the individual and combined contributions of different tokens to the generation of the image. <strong>Uniqueness, redundancy, and synergy</strong> between tokens would be mentioned as key aspects of the analysis. Finally, a high-level overview of the paper&rsquo;s structure and the benefits of using DiffusionPID, such as improved model evaluation and bias detection, would likely be presented, <strong>piquing the reader&rsquo;s interest</strong> and setting the stage for the detailed methodology and experimental results to come.</p><h4 class="relative group">PID Methodology<div id=pid-methodology class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pid-methodology aria-label=Anchor>#</a></span></h4><p>A PID (Partial Information Decomposition) methodology for interpreting diffusion models would involve using PID to analyze the informational relationships between input text prompts and generated images. <strong>This would go beyond simply measuring mutual information</strong>, offering a nuanced understanding of how individual words, phrases, and their interactions contribute to the final output. The core of the method would likely involve decomposing the mutual information into unique, redundant, and synergistic components at both the image and pixel levels. <strong>Unique information</strong> would highlight aspects of the image uniquely explained by specific text elements, <strong>redundancy</strong> would identify overlapping information across different parts of the prompt, and <strong>synergy</strong> would reveal how interactions between prompt elements generate novel visual information. By visualizing these components, one can gain crucial insights into the model&rsquo;s internal reasoning process and <strong>identify potential sources of bias or ambiguity</strong>. A crucial aspect would be considering context; how the presence or absence of certain words in the prompt affects the contributions of other words. This technique could offer valuable insights into the model&rsquo;s behavior, aiding in its evaluation, diagnosis, and improvement. It also <strong>has the potential to guide prompt engineering</strong> by identifying redundant or underutilized elements.</p><h4 class="relative group">Bias Experiments<div id=bias-experiments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bias-experiments aria-label=Anchor>#</a></span></h4><p>The section on bias experiments in this research paper is crucial for evaluating the fairness and trustworthiness of the studied diffusion models. The experiments likely involve analyzing the model&rsquo;s output for systematic biases related to gender and ethnicity. <strong>Careful selection of prompts and occupations</strong> associated with stereotypical gender roles would be critical. <strong>Quantitative measures like redundancy, uniqueness, and synergy</strong> (derived using Partial Information Decomposition - PID) will likely be used to assess how strongly the model associates certain attributes with specific genders or ethnicities. The results might reveal imbalances, highlighting <strong>the extent to which the models perpetuate societal biases</strong>. This evaluation should not only identify the presence of bias but also investigate its magnitude and its effect on image generation. <strong>Visualizations of PID components (heatmaps)</strong> might be included to illustrate the model‚Äôs biases in a more intuitive way. Furthermore, the study might also assess whether prompt engineering techniques are able to mitigate or exacerbate these biases.</p><h4 class="relative group">Future of PID<div id=future-of-pid class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-of-pid aria-label=Anchor>#</a></span></h4><p>The future of Partial Information Decomposition (PID) is bright, promising deeper insights into complex systems. <strong>Methodological advancements</strong> are crucial; refining existing PID measures to handle high-dimensional data and non-linear relationships will unlock applications in diverse fields. <strong>Expanding beyond pairwise interactions</strong> to analyze the information dynamics of larger ensembles of variables is vital, requiring new theoretical frameworks and computational techniques. <strong>Developing efficient algorithms</strong> is paramount to enable large-scale applications of PID. <strong>Bridging PID with other information-theoretic tools</strong> such as causal inference and network analysis will provide a holistic perspective on system behavior, enabling more comprehensive understanding. <strong>Interdisciplinary collaborations</strong> will be essential for successfully applying PID across various scientific domains, from neuroscience and ecology to economics and climate science. <strong>Focus on interpretability</strong> will enhance PID&rsquo;s adoption and impact, translating complex information-theoretic results into actionable knowledge.</p><h4 class="relative group">Limitations<div id=limitations class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#limitations aria-label=Anchor>#</a></span></h4><p>A critical analysis of the &lsquo;Limitations&rsquo; section in a research paper necessitates a nuanced understanding of its role. This section isn&rsquo;t merely a list of shortcomings; it&rsquo;s a demonstration of <strong>intellectual honesty</strong> and <strong>rigor</strong>. A well-written limitations section acknowledges methodological constraints, such as sample size or data biases. It also addresses potential limitations in generalizability, acknowledging that the findings might not apply universally across all contexts or populations. Furthermore, a strong limitations section preemptively addresses potential criticisms by acknowledging the study&rsquo;s scope, and the boundaries of its claims. <strong>Transparency</strong> regarding limitations builds trust with the reader. A thoughtful analysis also assesses whether the limitations are acknowledged appropriately (i.e., are they appropriately contextualized and weighted relative to the paper&rsquo;s contributions?) and whether the authors demonstrate awareness of how these limitations might impact the interpretation of their results and guide future research.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_5_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to highlight different aspects of semantic relationships between words in image generation. The left panel shows the uniqueness map for &lsquo;baseball,&rsquo; emphasizing the visual similarity between a baseball and a tennis ball. The center panel displays the synergy map between &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; illustrating how the model uses contextual information for image generation. The right panel shows the redundancy map for &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; highlighting the overlapping regions where both concepts are represented.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_6_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the application of the DiffusionPID method to analyze homonyms (words with multiple meanings) in text-to-image generation. The left panel shows a successful case where the model correctly interprets the context of the word &lsquo;bowl&rsquo; (in &lsquo;ceramic bowl&rsquo; and &lsquo;rose bowl stadium&rsquo;) based on its synergy with other words, leading to appropriate image generation. The right panel showcases a failure case where the model fails to distinguish between the different meanings of &lsquo;mole&rsquo;, generating an image of an animal (&lsquo;mole&rsquo; as a creature) instead of a person (&lsquo;mole&rsquo; as an undercover agent) due to lack of contextual synergy with words like &lsquo;coworker&rsquo;. The synergy maps visually highlight the model&rsquo;s interpretation of word relationships.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_6_2.jpg alt></figure></p><blockquote><p>This figure showcases the results of applying the proposed DiffusionPID method to analyze the semantic similarity between synonym pairs: &lsquo;bed&rsquo; and &lsquo;mattress&rsquo;, and &lsquo;cube&rsquo; and &lsquo;cuboid&rsquo;. The redundancy map, a key output of DiffusionPID, highlights the regions in the generated images where the model associates both words strongly. For &lsquo;bed&rsquo; and &lsquo;mattress&rsquo;, the redundancy is concentrated on the bed region, indicating the model&rsquo;s understanding of their semantic overlap. Similarly, for &lsquo;cube&rsquo; and &lsquo;cuboid&rsquo;, the redundancy map strongly activates around the cuboid, highlighting the shared semantic space. The visualization is compared against mutual information (MI), conditional mutual information (CMI), and Discriminator Attention Maps (DAAM) methods. The figure demonstrates the superior performance of DiffusionPID&rsquo;s redundancy map in precisely highlighting the semantically similar regions compared to the other methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_7_1.jpg alt></figure></p><blockquote><p>This figure shows the results of applying the DiffusionPID method to analyze the redundancy between co-hyponym pairs from the COCO dataset. The left image shows a generated image of both a sandwich and a pizza. This indicates that the model fails to distinguish between the two and treat them as redundant rather than distinct concepts. The right shows a similar situation with a generated image of both an elephant and a cat, despite them being co-hyponyms. The redundancy maps highlight the regions of the image where the model conflates the meanings of the two words. This supports the hypothesis that the model struggles with co-hyponyms due to associating them with the same semantic meaning.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_7_2.jpg alt></figure></p><blockquote><p>This figure shows the redundancy maps produced by the DiffusionPID method for two pairs of co-hyponyms (words with similar meanings but not identical) from the Wordnet dataset. The left panel shows the results for the pair &lsquo;barrier&rsquo; and &lsquo;railing&rsquo;, while the right panel shows the results for the pair &lsquo;chair&rsquo; and &lsquo;sofa&rsquo;. The redundancy maps highlight the regions where the model confuses the two co-hyponyms, resulting in images that fuse features from both objects or generate only one of the objects. This confusion is because the model does not adequately differentiate between the co-hyponyms&rsquo; semantic meaning, treating them as if they were synonyms.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_8_1.jpg alt></figure></p><blockquote><p>This figure shows the results of a prompt intervention experiment. The top row shows the original image generated with the prompt &lsquo;a cow and a giraffe&rsquo;, along with the generated image after removing the word &lsquo;cow&rsquo; from the prompt, and several information maps (redundancy, uniqueness, CMI, MI, DAAM). The bottom row shows the same analysis but removing the word &lsquo;giraffe&rsquo; instead. The results demonstrate that removing the word &lsquo;cow&rsquo; (redundant word) has minimal effect on the generated image, because its information is already captured by the presence of the word &lsquo;giraffe&rsquo;. This experiment highlights the effectiveness of DiffusionPID in identifying and removing redundant words in prompts, which allows for more concise prompts with minimal impact on the generated image.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_8_2.jpg alt></figure></p><blockquote><p>This figure shows the result of prompt intervention experiments. By removing the word &lsquo;mouse&rsquo; from the prompt, the generated image remains almost the same. The redundancy map highlights the mouse area, indicating that the word &lsquo;mouse&rsquo; is redundant in the prompt. The uniqueness map for the mouse is low and spread out, further supporting this. This experiment validates that removing redundant words from prompts has minimal impact on the generated image.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_9_1.jpg alt></figure></p><blockquote><p>This figure visualizes the uniqueness maps generated by the DiffusionPID method for two different prompts: &lsquo;A hairdryer and a toothbrush&rsquo; and &lsquo;A horse and a bear&rsquo;. The uniqueness map highlights the regions of the image that are most uniquely associated with a specific concept in the prompt. In the left panel, the uniqueness map for &rsquo;toothbrush&rsquo; strongly emphasizes the bristles, showing that these are the most distinctive characteristics of the toothbrush as perceived by the model. Similarly, in the right panel, the uniqueness map for &lsquo;bear&rsquo; focuses on the bear&rsquo;s face, indicating the model&rsquo;s recognition of the facial features as the key identifier for the bear.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_9_2.jpg alt></figure></p><blockquote><p>This figure shows the results of applying Partial Information Decomposition (PID) and Conditional PID (CPID) to complex prompts. The prompts describe scenes with multiple objects and attributes, such as &lsquo;An American woman drinking black coffee. She has a red bag in her hand. That bag has golden stars on it and is the size of her face.&rsquo; and &lsquo;A Japanese girl with long, light purple hair, wearing a white vest and denim trousers with holes. She holds a bamboo sword and shoots radio waves at a bamboo plant.&rsquo;. The figure displays the image, redundancy (R), uniqueness for the first word (U1), uniqueness for the second word (U2), and synergy (S) for both PID and CPID analyses. This visualization helps understand how individual words and their interactions contribute to the generated image by the diffusion model. The difference in results between PID and CPID highlights the impact of context on the model&rsquo;s interpretation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_17_1.jpg alt></figure></p><blockquote><p>This figure presents a comparison of MMSE curves obtained using two different estimators for mutual information: the standard estimator (Equation 2) and the orthogonal estimator (Equation 3). The x-axis represents the noise level (Œ±), while the y-axis represents the value of the MMSE (Mean Squared Error). The figure shows that the orthogonal estimator provides more stable and consistent results compared to the standard estimator, which exhibits significant zigzag patterns, particularly at lower noise levels. This comparison highlights the advantage of using the simplified orthogonal estimator for practical applications. The comparison is shown for several words from the dataset: cat, elephant, ceramic, bowl, game, bowl, bed, and mattress.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_17_2.jpg alt></figure></p><blockquote><p>This figure displays redundancy maps generated at different noise levels (SNR) and with varying numbers of samples. The maps illustrate how the redundancy estimations change as more samples and varying SNRs are incorporated into the calculation. This helps to understand the uncertainty and stability of the redundancy measure.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_19_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to analyze text-to-image diffusion models. The left panel shows how the model identifies visual similarities (uniqueness) between a tennis ball and baseball, highlighting the seam region as a shared feature. The center panel illustrates how the model uses contextual cues (synergy) to generate images, showing a strong relationship between &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead.&rsquo; The right panel showcases how the model identifies redundancy between &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; correctly focusing on the crown and facial area.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_19_2.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to analyze the contribution of individual words and their interactions in generating images from text prompts. The left panel shows how the model associates visual similarities (seam of tennis ball and baseball), the middle panel displays how the model uses contextual information (bat, baseball, overhead) for appropriate image generation, and the right panel illustrates the redundancy between related concepts (queen, crown).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_19_3.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to analyze text-to-image diffusion models. The left panel shows a uniqueness map highlighting the visual similarity between a tennis ball and a baseball, indicating that the model uses shared visual features. The center panel demonstrates synergy, showing how the model leverages contextual cues (&lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; &lsquo;overhead&rsquo;) to generate appropriate imagery. The right panel showcases redundancy, with the model appropriately focusing on the crown and facial region when provided the prompt words &lsquo;queen&rsquo; and &lsquo;crown.&rsquo;</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_19_4.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to interpret text prompts in image generation. The left panel shows a uniqueness map for the word &lsquo;baseball,&rsquo; highlighting the similarity between a baseball and a tennis ball. The center panel illustrates synergy between &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; indicating how contextual cues influence image generation. The right panel shows a redundancy map for &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; accurately focusing on the relevant image regions.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_20_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to highlight specific visual-semantic relationships within generated images. The left panel shows how the model recognizes visual similarity (uniqueness) between a baseball and a tennis ball, focusing on their seam. The center panel illustrates how contextual cues, such as the words &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; synergistically contribute to the generation of a coherent scene. Finally, the right panel showcases the redundancy detected by the model between the words &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; correctly highlighting the relevant image regions.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_20_2.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to analyze the information contribution of different text tokens in generating images from text prompts using three examples. The left panel shows a uniqueness map for the word &lsquo;baseball,&rsquo; highlighting the tennis ball&rsquo;s seam, which is visually similar to a baseball. This indicates that the model identifies this visual feature as unique to the concept of a baseball. The center panel illustrates a synergy map between the words &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; demonstrating the model&rsquo;s use of contextual cues to correctly generate a bat in the appropriate setting. The right panel displays a redundancy map for &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; correctly focusing on the overlapping visual aspects of the crown and the queen&rsquo;s face.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_20_3.jpg alt></figure></p><blockquote><p>This figure demonstrates the effectiveness of the DiffusionPID method in identifying unique, redundant, and synergistic information within text prompts used for image generation. The left panel shows the uniqueness of the word &lsquo;baseball&rsquo;, highlighting similarities between a baseball and a tennis ball. The center panel illustrates synergy between &lsquo;bat&rsquo;, &lsquo;baseball&rsquo;, and &lsquo;overhead&rsquo;, showcasing how these words work together to generate a coherent image. The right panel shows the redundancy between &lsquo;queen&rsquo; and &lsquo;crown&rsquo;, accurately highlighting the overlapping visual features.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_21_1.jpg alt></figure></p><blockquote><p>This figure shows two examples of how diffusion models handle homonyms, which are words with multiple meanings. The left side demonstrates a successful generation where the model uses contextual cues to select the appropriate meaning of the homonym &lsquo;bowl&rsquo; (a container or a stadium). The synergy map shows high synergy between &lsquo;bowl&rsquo; and the modifiers &lsquo;ceramic&rsquo; and &lsquo;game,&rsquo; indicating that the model leveraged these contextual cues effectively. The right side shows a failure case, where the model fails to differentiate between the meanings of the homonym &lsquo;mole&rsquo; (a spy or a small animal), generating the animal instead of the intended meaning of a spy. The synergy map shows low synergy between &lsquo;mole&rsquo; and the contextual cues &lsquo;coworker&rsquo; and &lsquo;searching,&rsquo; explaining the model&rsquo;s failure to utilize the contextual information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_21_2.jpg alt></figure></p><blockquote><p>This figure demonstrates the effectiveness of the DiffusionPID method in identifying visual-semantic relationships in images generated by text-to-image diffusion models. The three subfigures illustrate the three core concepts of the PID framework: Uniqueness, Synergy, and Redundancy. The left panel shows that the model identifies similar visual features (the seams of a baseball and tennis ball) as unique aspects of the &lsquo;baseball&rsquo; prompt. The center panel highlights the synergistic relationship between prompts (&lsquo;bat&rsquo;, &lsquo;baseball&rsquo;, &lsquo;overhead&rsquo;), demonstrating the model&rsquo;s ability to combine contextual cues to generate coherent scenes. Finally, the right panel illustrates redundancy between the prompts (&lsquo;queen&rsquo; and &lsquo;crown&rsquo;), showing how the model focuses on overlapping regions (the crown and queen&rsquo;s face) in the generated image. This visual representation highlights the capacity of DiffusionPID to unpack the complex interplay between textual prompts and image generation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_21_3.jpg alt></figure></p><blockquote><p>This figure shows the results of applying both Partial Information Decomposition (PID) and Conditional PID (CPID) to analyze complex image generation prompts. Two examples are provided, demonstrating how the methods decompose the prompt into uniqueness, redundancy, and synergy components and highlighting the differences between PID and CPID in capturing contextual information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_21_4.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to highlight specific visual-semantic relationships within generated images. The left panel shows the uniqueness of the word &lsquo;baseball,&rsquo; emphasizing a tennis ball&rsquo;s seam, which is similar to a baseball. The center panel shows the synergy between &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; illustrating how contextual cues influence generation. The right panel showcases the redundancy of &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; focusing on overlapping features.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_22_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the DiffusionPID method&rsquo;s ability to analyze the uniqueness, synergy, and redundancy of different words in a text prompt. The left panel shows the uniqueness map for &lsquo;baseball,&rsquo; highlighting similar features between a baseball and a tennis ball. The center panel illustrates the synergy between &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; showcasing the model&rsquo;s contextual understanding. Finally, the right panel depicts the redundancy map between &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; accurately identifying overlapping semantic information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_22_2.jpg alt></figure></p><blockquote><p>This figure shows how the model uses context to disambiguate homonyms. In the first example, the word &lsquo;bat&rsquo; is successfully identified as a baseball bat because of the synergistic context provided by the words &lsquo;baseball&rsquo; and &lsquo;swing&rsquo;. However, in the second example, without sufficient context, the model fails to correctly interpret the meaning of &lsquo;bat&rsquo;, leading to an incorrect generation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_22_3.jpg alt></figure></p><blockquote><p>This figure shows two examples of the model&rsquo;s performance when dealing with homonyms (words with multiple meanings). The left shows that with the proper contextual modifiers, the model generates the correct image, while the right side shows failure where despite proper modifiers, the model fails to generate the correct image and maintains the default meaning of the homonym.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_22_4.jpg alt></figure></p><blockquote><p>This figure shows the synergy maps for two homonyms, &lsquo;bowl&rsquo; and &lsquo;mole&rsquo;, used in different contexts. The left panel shows that the model successfully generates the correct image for the &lsquo;bowl&rsquo; homonym, indicating the ability to utilize context. The right panel demonstrates the model&rsquo;s failure with the &lsquo;mole&rsquo; homonym, showing the inability to use contextual information to correctly generate the image. This highlights how the model sometimes fails to capture the nuances of meaning in context.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_23_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the application of the DiffusionPID method to analyze how diffusion models handle homonyms (words with multiple meanings) in different contexts. The left panel shows a successful case where the model correctly generates different visuals for the homonym &lsquo;bowl&rsquo; based on the provided context (&lsquo;ceramic bowl&rsquo; vs. &lsquo;rose bowl stadium&rsquo;), evidenced by the high synergy between the homonym and the modifiers. The right panel, however, illustrates a failure case. Despite providing context (&lsquo;coworker&rsquo; and &lsquo;searching&rsquo;), the model fails to correctly disambiguate the homonym &lsquo;mole&rsquo;, generating an image of an animal instead of a person who might be secretly leaking information. This failure is highlighted by the low synergy scores observed in the corresponding synergy map. The results show that the synergistic relationship between the homonym and the modifiers is critical for the correct interpretation of homonyms.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_23_2.jpg alt></figure></p><blockquote><p>This figure shows the results of applying both Partial Information Decomposition (PID) and Conditional PID (CPID) to complex prompts. The two example prompts are shown above the images, illustrating how the methods decompose the semantic information within the prompts in order to provide insight into how the diffusion model processes them. Each sub-figure shows the image generated from the prompt, along with the Redundancy, Uniqueness for each term in the prompt, and Synergy maps from both the PID and CPID methods. This allows for a comparison of the two methods for analyzing the information contribution of the different elements within the prompts to the final image. By comparing the visualizations from PID and CPID, it&rsquo;s possible to understand the difference that the context of the rest of the prompt makes on the individual terms within the prompt.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_23_3.jpg alt></figure></p><blockquote><p>This figure demonstrates the effectiveness of the DiffusionPID method in interpreting text-to-image diffusion models by visualizing uniqueness, synergy, and redundancy between different words in a prompt. The left image shows how the model recognizes the visual similarity between a baseball and a tennis ball, highlighting the seam region as unique to &lsquo;baseball&rsquo;. The center image illustrates the synergistic relationship between &lsquo;bat&rsquo;, &lsquo;baseball&rsquo;, and &lsquo;overhead&rsquo;, demonstrating how the model utilizes these contextual cues. The right image shows redundancy between &lsquo;queen&rsquo; and &lsquo;crown&rsquo;, focusing on the crown and facial region as overlapping concepts.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_23_4.jpg alt></figure></p><blockquote><p>This figure shows the results of applying the DiffusionPID method to analyze the semantic similarity of synonym pairs. The redundancy maps, generated by DiffusionPID, highlight the regions in the generated images that are semantically related to both synonyms. For example, in the left image the redundancy map highlights the bed region for both the words &lsquo;bed&rsquo; and &lsquo;mattress&rsquo;, indicating a high degree of semantic overlap. This suggests that the model correctly identifies the semantic similarity between the synonym pairs.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_24_1.jpg alt></figure></p><blockquote><p>This figure visualizes redundancy maps generated using different numbers of samples and signal-to-noise ratios (SNRs). It aims to demonstrate the impact of these parameters on the accuracy and consistency of the redundancy maps, highlighting the trade-off between computational cost and the precision of the results. The consistency of highlighted regions across varying parameters suggests robustness of the method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_24_2.jpg alt></figure></p><blockquote><p>This figure shows the results of applying Partial Information Decomposition (PID) and Conditional PID (CPID) to complex prompts containing multiple objects and their attributes. The image shows that CPID provides slightly better localized results than PID, likely due to its consideration of the contextual contribution of the rest of the prompt in the image generation process.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_24_3.jpg alt></figure></p><blockquote><p>This figure shows three examples of how the DiffusionPID method highlights different aspects of visual-semantic relationships learned by diffusion models. The left panel illustrates the uniqueness of the word &lsquo;baseball&rsquo; by focusing on the seam of a tennis ball, which visually resembles a baseball. The center panel demonstrates the synergy between words like &lsquo;bat,&rsquo; &lsquo;baseball,&rsquo; and &lsquo;overhead,&rsquo; showing how the model uses contextual clues to generate appropriate images. The right panel showcases the redundancy between &lsquo;queen&rsquo; and &lsquo;crown,&rsquo; correctly identifying the overlapping visual features.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_24_4.jpg alt></figure></p><blockquote><p>This figure shows the results of applying the DiffusionPID method to analyze the generation of images from prompts containing co-hyponyms (words with similar meanings) from the COCO dataset. The left panel shows a prompt with &lsquo;sandwich&rsquo; and &lsquo;pizza&rsquo;, and the right panel shows a prompt with &rsquo;elephant&rsquo; and &lsquo;cat&rsquo;. In both cases, the redundancy map (a visualization of the redundancy term in PID) highlights that the model treats the co-hyponyms as highly similar. This similarity leads to the model&rsquo;s failure to generate both objects in the image, instead generating a fused version or just one of the objects. This finding indicates that the model may have difficulties distinguishing between closely related concepts, highlighting a limitation in its ability to handle subtle semantic differences.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_25_1.jpg alt></figure></p><blockquote><p>This figure visualizes redundancy maps generated using different numbers of samples and signal-to-noise ratios (SNRs). It demonstrates how the quality of the redundancy maps improves with increasing sample size and higher SNRs, suggesting better reliability of the method with improved data quality.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_25_2.jpg alt></figure></p><blockquote><p>This figure shows the results of applying both PID and CPID to complex prompts. Two example prompts are used, each with associated images and heatmaps illustrating redundancy, uniqueness (for two terms in each prompt), and synergy. The heatmaps visually represent the information theoretic concepts calculated by PID and CPID at the pixel level, showcasing how different parts of the prompt contribute to the final image generation. CPID, an extension of PID that accounts for context, is compared to standard PID to demonstrate its potential advantages in interpreting complex prompts.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_25_3.jpg alt></figure></p><blockquote><p>This figure shows the results of applying the proposed method, DiffusionPID, to analyze co-hyponyms from the Wordnet dataset. Co-hyponyms are words with similar but not identical meanings. The redundancy maps highlight regions where the model conflates the meanings of the co-hyponyms, leading to errors in the generated images. The figure compares the redundancy map produced by DiffusionPID with maps produced by other methods (CMI, MI, and DAAM) which don&rsquo;t show as much relevant information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_26_1.jpg alt></figure></p><blockquote><p>This figure shows a comparison of redundancy maps generated using the proposed DiffusionPID method and three other methods (CMI, MI, and DAAM) for two sets of co-hyponyms from WordNet: (&lsquo;barrier&rsquo;, &lsquo;railing&rsquo;) and (&lsquo;chair&rsquo;, &lsquo;sofa&rsquo;). The redundancy maps highlight regions where the model confuses the two concepts in each pair, indicating that the model does not fully differentiate them semantically. The other methods show far less localized results, suggesting that DiffusionPID is better suited to capture this type of semantic confusion.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_26_2.jpg alt></figure></p><blockquote><p>This figure demonstrates the use of uniqueness maps generated by the DiffusionPID method to highlight the most representative features of objects from the model&rsquo;s perspective. The left panel shows a generated image of a hairdryer and a toothbrush, with the uniqueness map focusing strongly on the bristles of the toothbrush, indicating that the model considers the bristles to be the most distinctive characteristic. The right panel shows a generated image of a horse and a bear, with the uniqueness map clearly highlighting the bear&rsquo;s face as its most defining feature. This illustrates how DiffusionPID can help identify uniquely defining characteristics of objects, separating them from potentially redundant visual information.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_27_1.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention using the DiffusionPID method. The original prompt was &lsquo;a cat and a sheep&rsquo;. The image generated by Stable Diffusion shows a sheep whose face appears to be that of a cow. The redundancy map highlights the face region, indicating redundancy between &lsquo;cow&rsquo; and &lsquo;sheep&rsquo;. After removing the word &lsquo;cow&rsquo; from the prompt (intervention), the generated image is almost identical, confirming that &lsquo;cow&rsquo; is a redundant word in this context. The uniqueness, CMI, MI, and DAAM maps are also shown for comparison.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_27_2.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention. The original prompt was &lsquo;a cat and a sheep.&rsquo; The image generated shows a sheep with a cat&rsquo;s face. The redundancy map highlights the cat&rsquo;s facial features, suggesting that &lsquo;cat&rsquo; is redundant to the model&rsquo;s generation process. An intervention was performed by removing the word &lsquo;cat&rsquo; from the prompt resulting in a much more accurate image of only a sheep. This demonstrates how PID can highlight redundancy and help correct model outputs via prompt engineering.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_27_3.jpg alt></figure></p><blockquote><p>This figure shows the results of a prompt intervention experiment. The original prompt included the objects &rsquo;laptop&rsquo; and &lsquo;mouse&rsquo;. The researchers removed the word &lsquo;mouse&rsquo; from the prompt and generated a new image. The redundancy map (showing shared information between the two concepts) shows high activation in the mouse region indicating that the information about the mouse is redundant with the laptop in this case. The uniqueness map (showing unique information for each concept) shows low activation for &lsquo;mouse&rsquo;, confirming that the mouse is not a unique element for the model. The overall visual similarity between the original and the intervention images supports that removing the word &lsquo;mouse&rsquo; had little effect on the image generated by the model, thus confirming its redundancy.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_27_4.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention using the DiffusionPID method. The original prompt contained the phrase &lsquo;a cow and a giraffe.&rsquo; The redundancy map highlights a significant overlap between the cow and giraffe features, particularly focusing on the giraffe&rsquo;s face, which is actually a cow&rsquo;s face. When the word &lsquo;cow&rsquo; was removed from the prompt, the generated image showed little change. This demonstrates that &lsquo;cow&rsquo; is a redundant element in this specific context because the model already uses its features to represent the giraffe (the cow&rsquo;s face features being mistaken for a giraffe&rsquo;s features).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_28_1.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention. The original image contains a cow and a giraffe. The word &lsquo;cow&rsquo; is identified as redundant based on the redundancy map (which is highly activated in the giraffe&rsquo;s face, showing cow-like features). Removing &lsquo;cow&rsquo; from the prompt results in only a minor change to the generated image, confirming the redundancy of this word.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_28_2.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention using the DiffusionPID method. The original prompt includes the words &lsquo;cow&rsquo; and &lsquo;giraffe&rsquo;. The redundancy map highlights the face region of the generated giraffe image, indicating that the model considers the &lsquo;cow&rsquo; aspect to be redundant in this context. The intervention removes the word &lsquo;cow&rsquo; from the prompt, resulting in only a slight change in the generated image, confirming that &lsquo;cow&rsquo; was redundant to the model.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_28_3.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention using the DiffusionPID method. The original image contains a cow and a giraffe. The redundancy map highlights the face of the giraffe, which visually resembles a cow. When the word &lsquo;cow&rsquo; is removed from the prompt, the generated image remains largely unchanged, confirming the redundancy of the term &lsquo;cow&rsquo;. This demonstrates that DiffusionPID can identify and remove redundant words in prompts, leading to minimal changes in the generated images. The other maps (Uniqueness, CMI, MI, DAAM) are provided for comparison and illustrate that DiffusionPID provides more targeted insights into the impact of specific words on the image generation process.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/aBpxukZS37/figures_28_4.jpg alt></figure></p><blockquote><p>This figure shows an example of prompt intervention using the DiffusionPID method. The original prompt included the words &lsquo;cow&rsquo; and &lsquo;giraffe&rsquo;. The resulting image shows a giraffe whose face resembles a cow, indicating that the word &lsquo;cow&rsquo; was redundant in the prompt. When the word &lsquo;cow&rsquo; is removed from the prompt, the generated image is nearly identical to the original, confirming the redundancy analysis. The figure also displays the redundancy, uniqueness, CMI, MI, and DAAM maps which support this observation.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-3676db6f0275c26f6b6b4ccd99f08024 class=gallery><img src=https://ai-paper-reviewer.com/aBpxukZS37/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/aBpxukZS37/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/abpxukzs37/&amp;title=Diffusion%20PID:%20Interpreting%20Diffusion%20via%20Partial%20Information%20Decomposition" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/abpxukzs37/&amp;text=Diffusion%20PID:%20Interpreting%20Diffusion%20via%20Partial%20Information%20Decomposition" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/abpxukzs37/&amp;subject=Diffusion%20PID:%20Interpreting%20Diffusion%20via%20Partial%20Information%20Decomposition" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/aBpxukZS37/index.md",oid_likes="likes_posters/aBpxukZS37/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/1l5vaniok5/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/wgp1tbcp1e/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Diffusion Models are Certifiably Robust Classifiers</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>