[{"figure_path": "CIRPE1bSmV/tables/tables_7_1.jpg", "caption": "Table 1: POPE Results. acc: accuracy. f1: f1 score, measured by precision and recall. Baseline and VCD results are reported by paper [34].", "description": "This table presents the results of the Polling-based Object Probing Evaluation (POPE) experiment, which evaluates object hallucination in Large Vision-Language Models (LVLMs).  It shows the accuracy (acc) and F1 score (f1) of different models on three datasets (MSCOCO, A-OKVQA, and GQA) across three negative sampling methods (random, popular, and adversarial). The table compares the performance of the proposed CCA-LLaVA model against baseline LLaVA, VCD (a previous hallucination mitigation method), and LLaVA-RLHF (a model with human feedback). The results are broken down for each dataset and negative sampling method, allowing for a detailed analysis of the model's performance in different scenarios.", "section": "5.2 POPE"}, {"figure_path": "CIRPE1bSmV/tables/tables_8_1.jpg", "caption": "Table 2: CHAIR results. For evaluation setups, 512 and 64 refer to a hyperparater that relates to the length of LVLM repsonses, corresponding to long-text and short-text generation, respectively.", "description": "This table presents the results of the CHAIR (Caption Hallucination Assessment with Image Relevance) metric, which evaluates the factuality of image captions generated by different models.  The evaluation is performed using two different text lengths (512 and 64 tokens) to examine hallucination in both short and long responses.  The results are shown for both greedy and beam search decoding methods, and include comparisons with baseline and other approaches.  The table shows that CCA-LLaVA consistently improves the accuracy of captions generated by mitigating object hallucination.", "section": "5.3 CHAIR"}, {"figure_path": "CIRPE1bSmV/tables/tables_8_2.jpg", "caption": "Table 3: MME results.", "description": "This table presents the results of the MME (Multimodal Hallucination Evaluation) benchmark.  MME assesses four perception sub-tasks related to object hallucination: object existence, object count, object position, and object color, both at the object level and attribute level. The table compares the performance of the baseline LLaVA model against three other hallucination mitigation methods: OPERA, VCD, and CCA-LLaVA. The total score is the sum of the four sub-task scores.  CCA-LLaVA shows significant improvement over the other methods, indicating its effectiveness in mitigating object-level and attribute-level hallucinations.", "section": "5.4 MME"}, {"figure_path": "CIRPE1bSmV/tables/tables_8_3.jpg", "caption": "Table 4: LLaVA Bench (In-the-Wild) results.", "description": "This table presents the results of evaluating the CCA-LLaVA model and other baselines on the LLaVA Bench (In-the-Wild) dataset.  The LLaVA Bench is a benchmark for evaluating multimodal language models. The table shows the performance of each model on different aspects of the benchmark including complex, detail, conversation, and overall scores.  The results illustrate CCA-LLaVA's performance in comparison to other models like OPERA and VCD, highlighting its improvements in generating detailed and comprehensive responses.", "section": "5.4 MME"}, {"figure_path": "CIRPE1bSmV/tables/tables_16_1.jpg", "caption": "Table 5: Evaluation on Multiple-Choice Benchmarks. Baseline results are reported by paper [45], except for MMStar reported by [8]. SEED<sup>A</sup>, SEED<sup>I</sup> and SEED<sup>V</sup> refers to all, image and video evaluation, respectively. SeVa results are reported by [85].", "description": "This table presents the results of evaluating the proposed Concentric Causal Attention (CCA) method and several baseline methods on six different multiple-choice benchmarks that assess the general visual perception capabilities of Large Vision-Language Models (LVLMs). The benchmarks include SEED-Bench, ScienceQA, GQA, VizWiz, MMBench, and MMStar.  The table compares the accuracy of CCA-LLaVA against LLaVA, LLaVA with VCD (a previous hallucination mitigation method), and two variants of SeVa.  The results demonstrate the improvement in performance achieved by the CCA method across various benchmarks and evaluation dimensions.", "section": "D More Results"}]