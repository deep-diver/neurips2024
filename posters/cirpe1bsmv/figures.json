[{"figure_path": "CIRPE1bSmV/figures/figures_1_1.jpg", "caption": "Figure 1: Long-term decay of RoPE [61] in Large Vision Language Models (LVLMs). (a) a schematic view of inference in LVLMs, typically involving a pre-trained vision encoder, a large language model and a projector to map visual tokens to textual space. For each of V visual tokens Svision, we aggregate its information flow to instruction tokens Sinstruct and reshape the aggregation results to 2-D (\u221aV by \u221aV). Applying RoPE on visual tokens introduces long-term decay as illustrated in (c), referring to the phenomenon where information flowing from visual tokens to instruction tokens gradually decays from lower-right region (rightmost visual tokens in the 1-D sequence) to upper-left region (leftmost visual tokens). For instruction tokens, they have much less direct interaction with leftmost visual tokens as compared with rightmost visual tokens, leading to inferior multimodal alignment in the trained LVLMs. (b) and (c) are derived from the adversarial subset of the 3k POPE [41] image-instruction pairs. Best viewed in color.", "description": "This figure illustrates the long-term decay effect of Rotary Position Encoding (RoPE) in Large Vision Language Models (LVLMs). It shows a schematic of LVLM inference, highlighting the role of RoPE in mapping visual and instruction tokens into a shared textual space.  The heatmaps (b) and (c) visualize the information flow from visual tokens to instruction tokens, demonstrating how RoPE's long-term decay weakens the interaction between visual and instruction tokens when they are far apart in the input sequence. This decay is more pronounced when visual tokens are distant from instruction tokens in the sequence.", "section": "1 Introduction"}, {"figure_path": "CIRPE1bSmV/figures/figures_4_1.jpg", "caption": "Figure 2: Motivation Experiment. Given an image I with object O<sub>r</sub>, we crop O<sub>r</sub> and paste it to various spatial positions {v<sub>1</sub>, ..., v<sub>k</sub>} within a pre-defined template. For every pasting position, we ask two LVLMs (F<sub>b</sub> and F<sub>r</sub>) if object O<sub>r</sub> is in this template, where F<sub>b</sub> refers to a baseline model that follows raster-scan positional alignment strategy and F<sub>r</sub> refers to a model that resorts to reversal raster-scan position alignment strategy. The total number of correct responses at different pasting positions {v<sub>1</sub>, ..., v<sub>k</sub>} is reported in (a) and (b), which refers to results from model F<sub>b</sub> and F<sub>r</sub>, respectively. We observe that LVLM F<sub>b</sub> are more likely to generate correct responses when pasting object O<sub>r</sub> to lower region, while F<sub>r</sub> are less hallucinated when pasting object O<sub>r</sub> to upper region. Pasting positions with the most and the least correct responses are highlighted in solid-line and dotted-line red boxes. More details are provided in Appendix C.1. Best viewed in color.", "description": "This figure presents a motivation experiment to show the impact of RoPE long-term decay on object hallucination in LVLMs.  Two models, one using raster-scan positional alignment and the other using reverse raster-scan, are tested by pasting a cropped object to various positions on a template image. The results show a clear positional bias in the models' accuracy, suggesting that RoPE's long-term decay affects object hallucination in LVLMs.", "section": "3 Motivation"}, {"figure_path": "CIRPE1bSmV/figures/figures_5_1.jpg", "caption": "Figure 3: An overview for Concentric Causal Attention. Left: Visual Token Re-organization. In comparison to raster-scan positional alignment in (a), we design concentric position alignment in (b) which shortens visual-instruction distance and retains spatial locality for 2-D data like images. Right: Concentric Causal Masking. By default as in (c), a visual token attends to all preceding visual tokens in a 1-D sequence. In contrast, our concentric causal attention in (d) models 2-D continuous positional dependencies among visual tokens, where center visual tokens attend to peripheral ones. Causal masks are V by V where in this case V is 36 for demonstration purpose. Best viewed in color.", "description": "This figure illustrates the proposed Concentric Causal Attention (CCA) method. The left side shows how CCA re-organizes visual tokens in a concentric manner, reducing the distance between visual and instruction tokens compared to the traditional raster-scan approach. The right side demonstrates the CCA's causal attention mask, which models 2D spatial relationships between visual tokens, unlike the traditional 1D causal mask.", "section": "4 Concentric Causal Attention"}, {"figure_path": "CIRPE1bSmV/figures/figures_14_1.jpg", "caption": "Figure 4: ROPE in LLaMA. A schematic view for LLaMA where RoPE is highlighted, and an example illustration on how RoPE is applied over query or key feature. We use a short input sequence with length of 4 and feature dimension of 4 for demonstration purpose. Input tokens are rotated with angles, subject to token positions. For mathematical definition, please refer to Sec. 3.", "description": "This figure shows how Rotary Position Encoding (RoPE) is implemented in the LLaMA architecture.  It illustrates RoPE's application to query and key features using a simplified example of a short input sequence. The schematic diagram demonstrates the integration of RoPE within the LLaMA layers. The detailed mathematical explanation is provided in Section 3 of the paper.", "section": "B ROPE in LLAMA"}, {"figure_path": "CIRPE1bSmV/figures/figures_15_1.jpg", "caption": "Figure 5: Workflow illustration on how we synthesize testing data. Given an image and box annotation for one object instance, we crop it and paste it on a template image, initialized with ImageNet mean pixel values. We paste every cropped region on every spatial position. Resulting data constitutes a large amount of questions about object existence, diverse in spatial positions.", "description": "This figure illustrates the process of creating synthetic data for testing.  An object is cropped from a real image and pasted onto a template image at various locations.  This creates a dataset of images with the object at different positions, allowing for the evaluation of the model's ability to detect objects at varying distances from instruction tokens.", "section": "C Implementation Details"}, {"figure_path": "CIRPE1bSmV/figures/figures_15_2.jpg", "caption": "Figure 3: An overview for Concentric Causal Attention. Left: Visual Token Re-organization. In comparison to raster-scan positional alignment in (a), we design concentric position alignment in (b) which shortens visual-instruction distance and retains spatial locality for 2-D data like images. Right: Concentric Causal Masking. By default as in (c), a visual token attends to all preceding visual tokens in a 1-D sequence. In contrast, our concentric causal attention in (d) models 2-D continuous positional dependencies among visual tokens, where center visual tokens attend to peripheral ones. Causal masks are V by V where in this case V is 36 for demonstration purpose. Best viewed in color.", "description": "This figure illustrates the core idea of the Concentric Causal Attention (CCA) method.  The left side shows how CCA reorganizes the positions of visual tokens in a 2D concentric manner, reducing the distance between visual and instruction tokens compared to the traditional raster-scan approach. The right side depicts the modified causal attention mask that reflects this concentric arrangement, enabling visual tokens to interact with more relevant instruction tokens.", "section": "4 Concentric Causal Attention"}, {"figure_path": "CIRPE1bSmV/figures/figures_16_1.jpg", "caption": "Figure 2: Motivation Experiment. Given an image I with object Or, we crop Or and paste it to various spatial positions {v\u2081, ..., vk } within a pre-defined template. For every pasting position, we ask two LVLMS (Fb and Fr) if object Or is in this template, where F\u2081 refers to a baseline model that follows raster-scan positional alignment strategy and Fr refers to a model that resorts to reversal raster-scan position alignment strategy. The total number of correct responses at different pasting positions {v1, ..., vk } is reported in (a) and (b), which refers to results from model F\u266d and Fr, respectively. We observe that LVLM F\u266d are more likely to generate correct responses when pasting object O\u2082 to lower region, while F\u2081 are less hallucinated when pasting object O\u2082 to upper region. Pasting positions with the most and the least correct responses are highlighted in solid-line and dotted-line red boxes. More details are provided in Appendix C.1. Best viewed in color.", "description": "This figure shows an experiment designed to demonstrate the effect of positional alignment strategies on object hallucination in Large Vision-Language Models (LVLMs).  Two models, one using a standard raster-scan alignment (Fb) and the other using a reversed raster-scan alignment (Fr), were tested.  The experiment involved cropping an object from an image, pasting it into different locations within a template image, and then querying the models about the object's presence. The results show that the models' performance varies significantly depending on the object's position and the alignment strategy used, highlighting the impact of long-term decay in RoPE on object hallucination.", "section": "3 Motivation"}, {"figure_path": "CIRPE1bSmV/figures/figures_16_2.jpg", "caption": "Figure 7: Qualitative comparison of open-ended generation between baseline and our method.", "description": "This figure compares the image captioning results of the baseline LLaVA model and the CCA-LLaVA model.  The input is an image of a pizza in a box with a bottle of beer on the side.  The baseline model hallucinates a knife and a cup that are not present in the image.  The CCA-LLaVA model produces a more accurate and less hallucinatory caption.", "section": "D More Results"}, {"figure_path": "CIRPE1bSmV/figures/figures_17_1.jpg", "caption": "Figure 8: Case Study where question is sampled from LLaVA-Bench [46]. LLaVA hallucinates hat in its long response, while CCA answers correctly without hallucination.", "description": "This figure presents a case study comparing the responses of the baseline LLaVA model and the CCA-LLaVA model to a question from the LLaVA-Bench dataset.  The question asks about the intended effect of a painting depicting a dog in Renaissance-style clothing. The LLaVA response hallucinates details (mentions a hat which is not in the image), while the CCA-LLaVA response accurately describes the painting's intent without hallucination, highlighting the improved factual accuracy of the CCA method.", "section": "D More Results"}, {"figure_path": "CIRPE1bSmV/figures/figures_17_2.jpg", "caption": "Figure 9: Case Study where question is sampled from LLaVA-Bench [46]. CCA-LLaVA outperforms LLaVA on optical character recognition (left) and numerical prediction in given cases.", "description": "This figure shows two examples where CCA-LLaVA outperforms the baseline LLaVA model in handling questions that require OCR and numerical reasoning.  The left example involves identifying the brand of yogurt in an image, and the right example involves counting the number of uncut fruits. In both cases, CCA-LLaVA produces correct answers, while LLaVA generates incorrect answers showing the effectiveness of CCA in improving the model's accuracy and reducing hallucinations.", "section": "D More Results"}]