[{"type": "text", "text": "Learning to Embed Distributions via Maximum Kernel Entropy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Oleksii Kachaiev Dipartimento di Matematica, Universit\u00e0 degli Studi di Genova, Genoa, Italy oleksii.kachaiev@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Stefano Recanatesi   \nTechnion Israel Institute of Technology, Haifa, Israel   \nAllen Institute for Neural Dynamics, Seattle, USA stefano.recanatesi@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Empirical data can often be considered as samples from a set of probability distributions. Kernel methods have emerged as a natural approach for learning to classify these distributions. Although numerous kernels between distributions have been proposed, applying kernel methods to distribution regression tasks remains challenging, primarily because selecting a suitable kernel is not straightforward. Surprisingly, the question of learning a data-dependent distribution kernel has received little attention. In this paper, we propose a novel objective for the unsupervised learning of data-dependent distribution kernel, based on the principle of entropy maximization in the space of probability measure embeddings. We examine the theoretical properties of the latent embedding space induced by our objective, demonstrating that its geometric structure is well-suited for solving downstream discriminative tasks. Finally, we demonstrate the performance of the learned kernel across different modalities. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most discriminative learning methods conventionally assume that each data point is represented as a real-valued vector. In practical scenarios, however, data points often manifest as a \u2019set\u2019 of features or a \u2019group\u2019 of objects. A quintessential example is the task of predicting a health indicator based on multiple blood measurements. In this case, the single data point of a patient has multiple, or a distribution of, measurements. One approach to accommodate such cases involves representing each input point as a probability distribution. Beyond mere convenience, it is more appropriate to model input points as distributions when dealing with missing data or measurement uncertainty, as often encountered when facing the abundance of data, which commonly presents a challenge for data-rich fields such as genetics, neuroscience, meteorology, astrophysics, or economics. ", "page_idx": 0}, {"type": "text", "text": "The task of regressing a mapping of probability distributions to a real-valued response is known as distribution regression. Distribution regression has been successfully applied in various fields, such as voting behavior prediction [13], dark matter halo mass learning [43], human cancer cells detection [44], brain-age prediction [6], among others [38, 33, 68]. The versatility and effectiveness of this framework underscore its power in solving complex problems [69, 29]. Kernel methods have become a widely used approach for solving distribution regression tasks by exploiting a kernel between distributions referred to as a distribution kernel. Despite the multitude of proposed kernels, the practical application of kernel methods remains challenging due to the nontrivial choice of the appropriate kernel. While some efforts focus on identifying kernels with broad applicability and favorable statistical properties [54], others aim to tailor kernels to the geometric characteristics of specific input spaces [6]. Remarkably, the question of learning data-dependent kernels has received limited attention. This study is thus driven by a fundamental question: What are the underlying principles that facilitate the unsupervised learning of an effective kernel, one that optimally encapsulates the data properties and is well suited for discriminative learning on distributions? ", "page_idx": 0}, {"type": "image", "img_path": "A0cok1GK9c/tmp/3783d5925afe54225f3dc65f08c479da741b2dca02a7649547f2a973b62e070b.jpg", "img_caption": ["Figure 1: Learning to embed distributions. (a) Example of multiple distributions over the input space. (b) The trainable function $f_{\\theta}$ encodes the input dataset into a compact latent space, in our case $\\dot{\\mathcal Z}={S}^{d-1}$ . (c) The first-level embedding kernel $k$ induces kernel mean embedding map to $\\mathcal{H}$ . The encoder is optimized to maximize the entropy of the covariance operator embedding of the dataset w.r.t. the second-level distribution kernel $K$ between kernel mean embeddings in $\\mathcal{H}$ . (d) Utilizing learned data-dependent kernel, downstream classification tasks can be solved using tools such as Kernel SVM or Kernel Ridge Regression. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we leverage a key insight: an appropriate selection of the distribution kernel enables the embedding of a set of distributions into the space of covariance operators. Building on this theoretical idea, we claim that quantum entropy maximization of the corresponding covariance operator is a suitable guiding principle to learn data-dependent kernel. This, combined with a careful design of kernel parametrization, let us to devise a differentiable optimization objective for learning data-specific distribution embedding kernels from unlabeled datasets (i.e. unsupervised) (Fig. 1). We show that the entropy maximization principle facilitates learning of the latent space with geometrical configuration suitable for solving discriminative tasks [3, 20]. We empirically demonstrate the performance of our method by performing classification tasks in multiple modalities. ", "page_idx": 1}, {"type": "text", "text": "In summary, our unsupervised data-dependent distribution kernel learning framework introduces a theoretically grounded alternative to the common practice of hand-picking kernels. Such framework could be further leveraged for generalizing existing learning approaches [61, 32] catalyzing the use of distribution-based representations within the broader scientific community. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first introduce the main concepts necessary to formalize our learning framework: kernel mean embeddings and covariance operator embeddings. ", "page_idx": 1}, {"type": "text", "text": "2.1 Kernel Embeddings of Distributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider an input space $\\mathcal{X}$ and a positive-definite (p.d.) kernel $k\\,:\\,\\mathcal{X}\\,\\times\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ . Let $\\mathcal{H}$ be the corresponding reproducible kernel Hilbert space (RKHS) induced by such kernel. Consider a probability distribution $P\\in\\mathcal P(\\mathcal X)$ . The kernel mean embedding map embeds the distribution $P$ as a function in Hilbert space: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mu_{P}\\equiv\\mu(P):=\\int_{\\mathcal{X}}k(x,\\cdot)\\,d P(x)=\\int_{\\mathcal{X}}\\phi(x)\\,d P(x)\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\phi:\\mathcal{X}\\to\\mathcal{H}$ is a feature map such that $\\phi(x)=k(x,\\cdot)$ . ", "page_idx": 1}, {"type": "text", "text": "Importantly, if the kernel $k$ is characteristic [50], the mapping $\\mu:\\mathcal{P}(\\mathcal{X})\\rightarrow\\mathcal{H}$ is injective, implying that all information about the original distribution is preserved in $\\mathcal{H}$ . This last property underscores much of power under recent applications of kernel mean embeddings [41, 48, 14]. The natural empirical estimator for the kernel mean embedding approximates the true distribution with a finite ", "page_idx": 1}, {"type": "text", "text": "sum of Dirac delta functions: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{P}:=\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_{i})\\in\\mathcal{H}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{1},\\ldots,x_{N}\\sim P$ are $N$ empirical i.i.d. samples. The estimator has a dimension-free sample complexity error rate of $\\mathcal{O}(N^{-\\frac{1}{2}})$ [51]. ", "page_idx": 2}, {"type": "text", "text": "Additionally, we denote $d_{k}:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}_{\\geq0}$ a kernel metric induced by a kernel $k$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{k}(x,x^{\\prime})=\\lVert\\phi(x)-\\phi(x^{\\prime})\\rVert_{\\mathcal{H}}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that $d_{k}$ is a metric on $\\mathcal{X}$ if feature map $\\phi$ is injective. ", "page_idx": 2}, {"type": "text", "text": "2.2 Covariance Operators and Entropy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A second way of mapping a probability distribution to a Hilbert space can be defined by means of a covariance operators. For a given feature map $\\phi(x)=k(x,\\cdot):\\dot{x}\\rightarrow\\mathcal{H}^{1}$ , and a given probability distribution $P\\in\\mathcal P(\\mathcal X)$ , the covariance operator embedding is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Sigma_{P}:=\\int_{\\mathcal{X}}\\phi(x)\\otimes\\phi(x)\\,d P(x)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\otimes$ is a tensor product. $\\Sigma_{P}$ is a self-adjoint positive semi-definite (p.s.d.) operator acting on $\\mathcal{H}$ . Such operator can be seen as a mean embedding w.r.t. the feature map $x\\mapsto{\\dot{\\phi}}(x)\\otimes\\phi(x)$ and therefore, for a universal kernel $k$ , the map $P\\mapsto\\Sigma_{P}$ is injective (see Bach [1]). ", "page_idx": 2}, {"type": "text", "text": "Similarly to Eq. (2), the natural empirical estimator is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}_{P}=\\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_{i})\\otimes\\phi(x_{i})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{1},\\ldots,x_{N}\\sim P$ are $N$ i.i.d. samples. ", "page_idx": 2}, {"type": "text", "text": "For a translation-invariant kernel $k(x,x^{\\prime})\\,=\\,\\psi(x\\,-\\,x^{\\prime})$ normalized such that $k(x,x)\\,=\\,1$ , the covariance operator $\\Sigma_{P}$ is a density operator [1]. Henceforth, entropy measures can be applied to it, and the quantum R\u00e9nyi entropy of the order $\\alpha$ can be defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{\\alpha}(\\Sigma_{P}):=\\frac{1}{1-\\alpha}\\log\\mathrm{tr}\\left[(\\Sigma_{P})^{\\alpha}\\right]=\\frac{1}{1-\\alpha}\\log\\sum_{i}\\lambda_{i}^{\\alpha}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\lambda_{i}\\}_{i}$ are the eigenvalues of $\\Sigma_{P}$ . The Von Neumann entropy can be seen as a special case of R\u00e9nyi entropy in the limit $\\alpha\\rightarrow1$ . However, in our work, we focus primarily on the second-order case of R\u00e9nyi entropy, i.e. $\\alpha=2$ (see Carlen [9], M\u00fcller-Lennert et al. [42], Wilde [63], Giraldo et al. [15] for an in-depth overview of the properties and theory of quantum entropies). ", "page_idx": 2}, {"type": "text", "text": "3 Unsupervised Distribution Kernel Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Distribution Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss the key topics in distribution regression, including problem setup, the notion of a 2-stage sampling process, and the common solutions to regression employing kernel methods. ", "page_idx": 2}, {"type": "text", "text": "Distribution regression extends the common regression framework to the setup where covariates are given as probability distributions available only through samples. Formally, consider the task of finding a regressor $f:\\mathcal{P}(\\mathcal{X})\\to\\mathcal{Y}$ from the dataset of samples $\\Gamma_{M}\\,=\\,\\dot{\\{}(P_{i},y_{i})\\}_{i=1}^{M}$ where $P_{i}\\in\\mathcal{P}(\\bar{\\boldsymbol{\\chi}})$ are distributions provided as a set of i.i.d. empirical samples $x_{1},\\ldots,x_{N_{i}}\\sim P_{i}$ (see Poczos et al. [45], Szab\u00f3 et al. [53, 54] for a comprehensive analysis). A viable approach to solving this problem is to define a kernel $K:\\mathcal{P}(\\mathcal{X})\\times\\bar{\\mathcal{P}}(\\mathcal{X})\\rightarrow\\mathbb{R}$ that is universal in $\\mathcal{P}(\\mathcal{X})$ . By setting up such a kernel $K$ , we can utilize kernel-based regression techniques, with SVM often being the preferred method for classification tasks [39] or Kernel Ridge Regression (KRR) when the space $\\boldsymbol{\\wp}$ is continuous [37] 2. To this end, several kernels have been proposed over time (see details in Sec. 4). ", "page_idx": 2}, {"type": "image", "img_path": "A0cok1GK9c/tmp/172c03957c2ecdd3ec7cfd386620975c5c711583bdbbf9fc2310393952b9a02e.jpg", "img_caption": ["Figure 2: Properties of the entropy on the toy example. (a) Entropy and Distributional Variance for 6 distributions on a sphere as a function of their geometrical arrangement parametrized by $\\gamma$ . (b) Kernel norms that enter the distributional variance bound. The blue shaded area (difference between blue and red lines) corresponds to the dotted red line in (a) (up to multiplicative factor). (c) Flattening of Gram matrix eigenvalues as a function of $\\gamma$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "One possibility, proposed by Muandet et al. [39], is to introduce a kernel in the input space, the so called embedding kernel $k_{\\mathrm{emb}}:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ and exploit the induced mean embeddings $\\mu_{\\mathrm{emb}}:$ $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})\\rightarrow\\mathcal{H}_{\\mathrm{emb}}$ to map input distributions to points in RKHS. Subsequently, to define a second level kernel, distribution kernel $K_{\\mathrm{distr}}:\\mathcal{H}_{\\mathrm{emb}}\\times\\mathcal{H}_{\\mathrm{emb}}\\rightarrow\\mathbb{R}$ between points in the RKHS $\\mathcal{H}_{\\mathrm{emb}}$ . The simplest choice for such a distribution kernel is the linear kernel: ", "page_idx": 3}, {"type": "equation", "text": "$$\nK_{l}(\\mu_{P},\\mu_{Q}):=\\langle\\mu_{P},\\mu_{Q}\\rangle_{\\mathcal{H}_{\\mathrm{cmb}}}=\\iint_{\\mathcal{X}\\times\\mathcal{X}}k(x,x^{\\prime})\\,d P(x)\\,d Q(x^{\\prime})\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A standard alternative to the linear kernel is a Gaussian kernel with bandwidth parameter $\\lambda>0$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nK_{\\tt R B F}(\\mu_{P},\\mu_{Q}):=\\exp\\left(-\\frac{\\lambda}{2}\\|\\mu_{P}-\\mu_{Q}\\|_{\\mathcal H_{\\mathrm{cmb}}}^{2}\\right)=\\exp\\left(-\\frac{\\lambda}{2}d_{k_{\\mathrm{cmb}}}(\\mu_{P},\\mu_{Q})^{2}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which was shown to be universal in $\\mathcal{P}(\\mathcal{X})$ [11]. The Gaussian kernel $K_{\\mathrm{RBF}}$ can be computed from the linear kernel $K_{l}$ as $d_{k_{\\mathrm{cmb}}}(\\mu_{P},\\mu_{Q})^{2}=K_{l}(P,P)+K_{l}(Q,Q)-2K_{l}(P,Q)$ . ", "page_idx": 3}, {"type": "text", "text": "In practice, we only have access to a finite number of samples of each input distribution, thus the distribution kernel is approximated using the natural estimator for the kernel mean embedding. The related excess risk for the regression solution is analyzed in Szab\u00f3 et al. [53]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Dataset Embedding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Instead of using standard kernels designed to encapsulate the geometry of the input space, we consider learning a data-dependent kernel, tailored to the specific properties of the dataset. In a similar vein, Yoshikawa et al. [68] proposed learning an optimal kernel (or equivalently, a feature map) jointly with the classifier to address the text modality. In this work, we focus on an unsupervised problem, aiming to learn a data-dependent kernel between probability distributions without access to classification labels. ", "page_idx": 3}, {"type": "text", "text": "We first introduce proper parametrization to ensure both expressivity and robustness followed by the definition of the optimization objective. Leveraging the idea of 2-level kernel setup, we define the embedding kernel as ", "page_idx": 3}, {"type": "equation", "text": "$$\nk_{\\theta}:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}=k_{\\mathrm{emb}}(f_{\\theta}(x),f_{\\theta}(x^{\\prime}))\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{\\theta}$ is a trainable encoder function $f_{\\theta}:\\mathcal{X}\\to\\mathcal{Z},$ $\\mathcal{Z}$ is a latent encoding space, and $k_{\\mathrm{emb}}$ is a kernel defined on the latent space $k_{\\mathrm{emb}}:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ . The encoder function $f_{\\theta}$ transforms every ", "page_idx": 3}, {"type": "text", "text": "input probability distribution $P\\,\\in\\,\\mathcal P(\\mathcal X)$ into a distribution over the latent space $\\mathbb{P}_{\\theta}\\,\\in\\,\\mathcal{P}(\\mathcal{Z})$ 3 (Fig. 1a). Furthermore, we denote RKHS corresponding to the kernel $k_{\\mathrm{emb}}$ as $\\mathcal{H}_{\\mathrm{emb}}$ and the kernel mean embedding map as $\\mu_{\\mathrm{emb}}$ (see Eq. (1)). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P\\in\\mathcal{P}(\\mathcal{X})\\,\\xrightarrow{f_{\\theta}}\\,P_{\\theta}\\in\\mathcal{P}(\\mathcal{Z})\\,\\xrightarrow{k_{\\mathrm{cmb}}}\\,\\mu_{\\mathrm{emb}}(P)\\in\\mathcal{H}_{\\mathrm{emb}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These transformations define mean embeddings for each input probability distributions through the first level, embedding kernel $k_{\\mathrm{emb}}$ (Fig. 1b). The second level, distribution kernel $K_{\\mathrm{distr}}\\ :$ $\\mathcal{H}_{\\mathrm{emb}}\\times\\mathcal{H}_{\\mathrm{emb}}\\rightarrow\\mathbb{R}$ is defined over the mean embeddings $\\mu_{\\mathrm{emb}}(P)$ \u2019s. We can now consider to embed dataset $\\mathcal{D}_{M}=\\{P_{i}\\}_{i=1}^{M}$ as an empirical covariance operator (see Eq. (5)), i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{D}}_{M}\\xrightarrow{\\mu_{\\mathrm{emb}},K_{\\mathrm{dist}}}{\\hat{\\Sigma}}_{\\mathcal{D}}=\\frac{1}{M}\\sum_{P\\in{\\mathcal{D}}_{M}}K_{\\mathrm{distr}}(\\mu_{\\mathrm{emb}}(P),\\cdot)\\otimes K_{\\mathrm{distr}}(\\mu_{\\mathrm{emb}}(P),\\cdot)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As $\\hat{\\Sigma}_{\\mathcal{D}}$ encapsulates information about the entire dataset, we term it dataset embedding. With the assumption that the dataset $\\mathcal{D}_{M}$ is sampled i.i.d. from the (unknown) true meta-distribution $\\mathcal{D}$ , $\\hat{\\Sigma}_{\\mathcal{D}}$ is a natural estimator to approximate the true covariance operator $\\Sigma_{\\mathcal{D}}$ . To simplify the notation we use $\\Sigma_{\\mathcal{D}}$ in place of $\\Sigma_{\\mathcal{D}}$ unless required by the context. ", "page_idx": 4}, {"type": "text", "text": "Both the embedding kernel $k_{\\mathrm{emb}}$ and the distribution kernel $K_{\\mathrm{distr}}$ remain fixed throughout the training, learning happens by adjusting the parametrization of the latent space encoder $f_{\\theta}$ . Such separation ensures expressivity while conforming to all technical requirements for a distribution kernel. Throughout the paper, we make the following assumptions on the latent space $\\mathcal{Z}$ and embedding kernel $k$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1. Latent space $\\mathcal{Z}$ is a compact subset of $\\mathbb{R}^{d}$ . Kernel $k_{\\mathrm{emb}}:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ is a p.d. characteristic translation-invariant kernel $\\bar{k_{\\mathrm{emb}}}(z,z^{\\prime})=f(\\|z-z^{\\prime}\\|^{2})$ such that $-f^{\\prime}$ is completely monotone on $(0,\\infty)$ (see Definition 2.2.4 of Borodachov et al. [7]) and $\\forall z\\in\\mathcal{Z}:\\,k_{\\mathrm{emb}}(z,z)=1$ . ", "page_idx": 4}, {"type": "text", "text": "The choice of a kernel based on Euclidean distance in the latent space makes its definition similar to that in Weinberger and Tesauro [62], though the parametrization of the encoding process differs. Learning kernels by explicitly learning feature maps has been explored in a wide range of settings [66, 57, 58, 67]. In contrast, the parametrization proposed in our study applies a known characteristic kernel to a learned latent representation. To facilitate our optimization process (which will be explained shortly), we opt for $\\mathcal{Z}=\\mathbb{S}^{d-1}$ (the d-dimensional hypersphere) and Gaussian kernel both for $k_{\\mathrm{emb}}$ and $K_{\\mathrm{distr}}$ (see Eq. (8)). We retain other suitable choices as potential avenues for future research. ", "page_idx": 4}, {"type": "text", "text": "3.3 Unsupervised Optimization Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This dataset level representation depends on the choice of first and second level kernels $k,K$ and, in turn, on the trainable function $f_{\\theta}$ parameterized by the set of parameters $\\theta$ . In this work, we propose learning the parameters $\\theta$ to maximize quantum entropy of the dataset embedding, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta=\\arg\\operatorname*{max}\\left\\{S_{2}(\\Sigma_{\\mathcal{D}}):=-\\log\\mathrm{tr}\\left[(\\Sigma_{\\mathcal{D}})^{2}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As we will describe in brief, optimizing this target has clear benefits inherited from the underlying geometry of the setup. But, first, we show how to empirically compute $S_{2}(\\Sigma_{\\mathcal{D}})$ . Building upon previous work $[1]^{4}$ , we exploit the following property of the covariance estimator: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{tr}\\left[\\left(\\Sigma_{\\mathcal{D}}\\right)^{2}\\right]=\\mathbf{tr}\\left[\\left(\\frac{1}{M}K_{\\mathcal{D}}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K_{\\mathcal{D}}\\in\\mathbb{R}^{M\\times M}$ is the distribution kernel matrix, with $[K_{\\mathscr D}]_{i j}=K_{\\mathrm{distr}}(\\mu_{P_{i}},\\mu_{P_{j}})$ . This equation follows directly from the fact that $\\Sigma_{\\mathcal{D}}$ and $\\scriptstyle{\\frac{1}{M}}K_{D}$ share the same set of eigenvalues. Leveraging this ", "page_idx": 4}, {"type": "text", "text": "relationship, we can define tractable unsupervised training loss, which we term Maximum Distribution Kernel Entropy (MDKE), with respect to the parameters of the encoder $f_{\\theta}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{MDKE}}(\\theta):=-{\\mathcal{S}}_{2}(\\Sigma_{\\mathcal{D}})=\\log\\operatorname{tr}\\left[\\left({\\frac{1}{M}}K_{\\mathcal{D}}\\right)^{2}\\right]=\\log\\sum_{i=1}^{M}\\lambda_{i}^{2}\\left({\\frac{1}{M}}K_{\\mathcal{D}}\\right)=\\log\\|{\\frac{1}{M}}K_{\\mathcal{D}}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the latter relies on the fact that the Frobenius norm $\\begin{array}{r}{\\|A\\|_{F}^{2}=\\sum_{i}\\lambda_{i}^{2}(A)}\\end{array}$ , where $\\lambda_{i}(A)$ are eigenvalues $A$ . ", "page_idx": 5}, {"type": "text", "text": "The MDKE objective is differentiable w.r.t. $\\theta$ for commonly used kernels, provided that the encoder $f_{\\theta}$ is differentiable as well. While the entropy estimator $S_{2}(\\Sigma_{\\mathcal{D}})$ is convex in the kernel matrix $K_{D}$ , the objective as a whole is generally not convex in $\\theta$ . However, in practice, as we show in Sec. 5, mini-batch Stochastic Gradient Descent (SGD) proves to be an effective method for optimizing this objective. The effectiveness of this optimization process is significantly influenced by the parameters of the Gaussian kernels. We elaborate on the methodologies for kernel bandwidth selection in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "The Frobenius norm formulation in the loss Eq. (14) significantly reduces computational complexity. However, as we have observed in some of our experiments, it can lead to the collapse of small eigenvalues of $K_{D}$ , particularly near the optimal value of the objective. To address this challenge we introduced a regularized version of the loss $\\mathcal{L}_{\\mathrm{MDKE-R}}$ that incorporates optional regularization, based on the determinant $K_{D}$ inspired by the connection with Fekete points (see details in Appendix B.2). ", "page_idx": 5}, {"type": "text", "text": "3.4 Geometrical Interpretation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The optimization objective is specifically designed to minimize the variance within each distribution (inner-distribution variance) while simultaneously maximizing the spread of distributions over the compact latent space $\\mathcal Z=\\mathop{\\mathcal S}^{d-1}$ . This shaping of the distributions embeddings in the latent space facilitates easier separation in downstream tasks. In this section we show that the geometry of the optimal (w.r.t. the MDKE loss) configuration of mean embeddings in the RKHS attains describe properties. For doing so we leverage the notion of distributional variance $\\mathbb{V}_{\\mathcal{H}}$ (Definition 1 in Muandet et al. [40]). ", "page_idx": 5}, {"type": "text", "text": "Definition 3.2. For a set of $M$ probability distributions $\\mathcal{D}_{M}$ , distributional variance $\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})$ of the mean embeddings in the RKHS $\\mathcal{H}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M}):=\\frac{1}{M}\\mathrm{tr}\\left[G\\right]-\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}G_{i j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $G$ is the $M\\times M$ Gram matrix of mean embeddings in $\\mathcal{H}$ , i.e. $G_{i j}=\\langle\\mu_{P_{i}},\\mu_{P_{j}}\\rangle_{\\mathcal{H}}$ [40]. ", "page_idx": 5}, {"type": "text", "text": "Here we show that the distributional variance $\\mathbb{V}_{\\mathcal{H}}$ can be equally reformulated into two separate contributions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})\\equiv\\frac{1}{M}\\sum_{i=1}^{M}\\|\\mu_{P_{i}}\\|_{\\mathcal{H}}^{2}-\\|\\mu_{\\bar{P}}\\|_{\\mathcal{H}}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{P}$ denotes mixture distribution with elements of $\\mathcal{D}_{M}$ being uniformly weighted mixture components (see proof in the Appendix A.1). ", "page_idx": 5}, {"type": "text", "text": "The relevance of distributional variance for MDKE objective is established by the following result. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. For a set of $M$ probability distributions $\\mathcal{D}_{M}$ , the second-order R\u00e9nyi entropy $S_{2}$ of the empirical covariance operator embedding $\\hat{\\Sigma}_{\\mathcal{D}}$ induced by the choice of Gaussian distribution kernel $K_{R B F}$ over points in the RKHS $\\mathcal{H}_{e m b}$ , - as defined in Eq. (8), - is upper bounded by the distributional variance $\\mathbb{V}_{\\mathcal{H}_{e m b}}(\\mathcal{D}_{M})$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{2\\gamma}S_{2}(\\hat{\\Sigma}_{\\mathcal{D}})\\leq\\mathbb{V}_{\\mathcal{H}_{e m b}}(\\mathcal{D}_{M})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma$ is the bandwidth of the distribution kernel $K_{R B F}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of this proposition is provided in Appendix A.3. This result formalizes the fact that our objective increases distributional variance, pushing up the average squared norm of mean embedding ", "page_idx": 5}, {"type": "text", "text": "of input distributions while minimizing squared norm of the mean embedding of the mixture. We further explore the geometrical implications of such optimization by formalizing connection between the variance of the distribution and the squared norm of the its mean embedding in RKHS. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.4. Under Assumption 3.1, the maximum norm of kernel mean embedding is attained by Dirac distributions $\\{\\delta_{z}\\}_{z\\in\\mathcal{Z}}$ . ", "page_idx": 6}, {"type": "text", "text": "This result is trivial due to the fact that the set of mean embeddings is contained in the convex hull of $\\{k_{\\mathrm{emb}}(z,\\cdot)\\}_{z\\in\\mathcal{Z}}$ , and, under Assumption 3.1, $\\forall z\\in\\mathcal{Z}:\\|k_{\\mathrm{emb}}(z,\\bar{\\cdot})\\|_{\\mathcal{H}_{\\mathrm{emb}}}^{2}=k_{\\mathrm{emb}}(z,z)=1.$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.5. Under Assumption 3.1, uniform distribution $\\mathcal{U}(\\mathcal{Z})$ is a unique solution of ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{P\\in\\mathcal{P}(\\mathcal{Z})}{\\arg\\operatorname*{min}}\\bigg\\{\\|\\mu_{e m b}(P)\\|_{\\mathcal{H}_{e m b}}^{2}\\equiv\\iint_{\\mathcal{Z}\\times\\mathcal{Z}}k_{e m b}(z,z^{\\prime})\\,d P(z)\\,d P(z^{\\prime})\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The key intuition here comes from the fact that minimization of the squared norm of mean embedding in RKHS could be seen as minimization of total interaction energy over the given surface where the potential is defined by the kernel $k$ . Thus Proposition 3.5 is a special case of Theorem 6.2.1 of Borodachov et al. [7]. Similar setup was used w.r.t. Gaussian potential over the unit hypersphere in Proposition 1 from Wang and Isola [61]. The reformulation in Eq. (16) together with Propositions 3.4 and 3.5 immediately suggests that the framework could be seen as an extension of the Hyperspherical Uniformity Gap [32] to infinite-dimensional spaces of probability distributions This extension maintains the goal of reducing variance among input distributions while maximizing the separability between their means. See Appendix C.2 for a broader explanation of the connection. ", "page_idx": 6}, {"type": "text", "text": "More generally, utilizing the fact that under Assumption 3.1, the kernel metric $d_{k_{\\mathrm{cmb}}}$ (see Eq. (3)) is a monotonically increasing function of Euclidean metric on the latent space, we establish a precise connection between the generalized variance [60] and the norm of the mean embedding. Further details can be found in Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "An attempt to directly optimize $\\mathbb{V}_{\\mathcal{H}_{\\mathrm{cmb}}}$ using SGD resulted in significantly weaker outcomes. While a thorough mathematical explanation necessitates further investigation, we contend that this issue aligns with the recurring challenge reported across various studies regarding direct optimization over Maximum Mean Discrepancy (MMD). We hypothesize that optimal solutions exhibit similar geometric configurations, while the entropy of the covariance operator providing a smoother objective. Nonetheless, $\\mathbb{V}_{\\mathcal{H}_{\\mathrm{cmb}}}$ retains its value as an insightful and intuitive measure for describing the geometric configuration of the learned system. ", "page_idx": 6}, {"type": "text", "text": "3.5 An Illustrative Example ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use a simple example to illustrate the connection between geometrical configurations of embedded distributions and distribution kernel entropy $S_{2}(\\Sigma_{\\mathcal{D}})$ (see Fig. 2). We sample a number of points from 6 different Gaussian distributions and project on a sphere $\\mathbb{S}^{2}$ varying their projected variance $\\gamma$ . As $\\gamma$ decreases, the distributional variance of the overall distribution of Gaussians increases (Fig. 2a). For very small $\\gamma$ each distribution converges to a point (a Dirac distribution). This results in the entropy interpolating between lower and upper bounds, demonstrating how entropy behaves in response to changes in distribution variance. Fig. 2b showcases the behavior of two terms comprising distributional variance (Eq. (16)): the average kernel norm of the distributions alongside the kernel norm of the mixture. The increase in entropy and variance corresponds to a \u2019flattening\u2019 effect on the spectrum of the distribution kernel matrix. This example provides a simplified picture of how input distributions configurations influence kernel entropy. ", "page_idx": 6}, {"type": "text", "text": "3.6 Limitations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Runtime complexity. The applicability of a data-dependent distribution kernel to solving discriminative tasks relies on the structure of the dataset being well-suited for distribution regression modeling. The model performs best when the number of input distributions is relatively small (e.g., thousands rather than millions), while the number of samples per distribution is large. It is crucial to note that the computational complexity of the proposed method, which is a common concern in practical applications, is most favorable for the tasks described. A detailed analysis of runtime complexity can be found in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "Broader impact. We wish to emphasize that the distribution regression framework has emerged as a powerful tool for analysis and predictive modeling, especially in domains where traditional methods face challenges, including social science, economics, and medical studies. We urge researchers and practitioners applying distribution regression in these areas to give special consideration to issues such as bias, fairness, data quality, and interpretability, - aspects that are currently under-researched in the context of distributional regression, largely due to the relative novelty. ", "page_idx": 7}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Distribution Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Distribution regression was introduced in Poczos et al. [45], while the seminal work of Szab\u00f3 et al. [54] provides a comprehensive theoretical analysis of this regression technique. A natural approach to solving distribution regression problems involves using kernels between measures. Notable examples include the Fisher kernel [17], Bhattacharyya kernel [18], Probability product kernel [19], kernels based on nonparametric divergence estimates [52], and Sliced Wasserstein kernels [23, 37]. Muandet et al. [39] proposed leveraging the mean embedding of measures in RKHS, and Szab\u00f3 et al. [53] provided theoretical guarantees for learning a ridge regressor from distribution embeddings in Hilbert space to the outputs. Distribution kernels have been successfully applied in various kernel-based methods, such as SVM [39], Ridge Regression [37], Bayesian Regression [28], and Gaussian Process Regression [2]. They have also been adapted for different modalities like distribution to distribution regression [44], sequential data [29], and more. Some learning paradigms can be considered closely related to distributional classification settings, such as multiple instance learning, where group-level information (i.e., labels) is available during training [70, 26, 25]. For an in-depth exploration of the diverse methodologies employed in distributional regression settings, we invite readers to consult Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "4.2 Matrix Information Theory ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantum entropy, including R\u00e9nyi entropy, is a powerful metric to describe information in a unique way (see M\u00fcller-Lennert et al. [42] for foundational insights). Giraldo et al. [15] designed the measure of entropy using operators in RKHS to mimic R\u00e9nyi entropy\u2019s behavior, offering the advantage of direct estimation from data. Bach [1] applied von Neumann entropy of the density matrix to the covariance operator embedding of probability distributions, thereby defining an information-theoretic framework utilizing kernel methods. In machine learning, especially within self-supervised learning (SSL) setups, entropy concepts have recently found novel applications. Our study builds on most recent developments [49, 21, 55] by applying quantum R\u00e9nyi entropy to the covariance operator in RKHS. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We here demonstrate that our proposed method successfully performs unsupervised learning of data-dependent distribution kernel across different modalities. The experimental setup is divided into two phases: unsupervised pre-training and downstream regression classification using the learned kernel. ", "page_idx": 7}, {"type": "text", "text": "For each dataset, we select a hold-out validation subset with balanced classes, while the remainder of the dataset is utilized for unsupervised pre-training. We use mini-batch ADAM [22] with a static learning rate of 0.0005. We report mini-batch based (instead of epoch based) training dynamics as our tasks do not require cycling over the entire dataset to converge to the optimal loss value. All experiments use Gaussian kernel both as an embedding kernel and distribution kernel, the hyperparameter selection is performed as described in Appendix B.1. ", "page_idx": 7}, {"type": "text", "text": "Once the samples encoder $f_{\\theta}$ is learned, we employ it to compute distribution kernel Gram matrix, used as an input to the Support Vector Machine (SVM) for solving downstream classification tasks. A grid search with 5 splits (70/30) is conducted to optimize the strength of the squared $l_{2}$ regularization penalty $C$ , exploring 50 values over the log-spaced range $\\{10^{-\\bar{7}},\\dots,10^{5}\\}$ . The best estimator is then applied to evaluate classification accuracy on the validation subset, which we report. ", "page_idx": 7}, {"type": "text", "text": "Additional experiments exploring the application of data-dependent distribution kernels in domains where distribution regression models are less common, such as image and text, are presented in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "5.1 Flow Cytometry ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Flow Cytometry (FC) is a widely used technique for measuring chemical characteristics of mixed cell population. Because population-level properties are described through (randomized) sampling of cells, FC is used as a canonical setup of distribution regression. For this study we used a dataset [56] where more than 100.000 cells are measured per each patient (subject). For each cell a total of ten parameters are reported, hence, we treated each subject as an empirical distribution over $\\mathbb{R}^{10^{\\mathbf{\\alpha}}}$ . We considered downstream classification tasks on two different sets of labels. The first (\u2019Tissue\u2019 classification) contains peripheral blood (pB) and bone marrow (BM) samples from $N~=~44$ subjects. The second (\u2019Leukemia\u2019 classification) presents healthy and leukemia BM cell samples, $N\\,=\\,50$ . Classes were balanced in both cases. ", "page_idx": 8}, {"type": "table", "img_path": "A0cok1GK9c/tmp/b0235f74e98accf75a9a68d914c71baf5ee9d1c618c60cf06b7ba57def7ebfa0.jpg", "table_caption": ["Table 1: Distribution regression accuracy on Flow Cytometry datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We sampled 16 subjects for Tissue classification and 20 subjects for Leukemia for training. Unsupervised learning was performed over the entire dataset. The encoder $f_{\\theta}$ was parametrized by a 2-layers neural network (NN) with ReLU nonlinearity and $l_{2}$ normalized output (on the unit hypersphere $\\mathbb{S}^{9}$ ). Per each subject we sampled a small percentage of cells, and we report performance for the sample size of 200. We repeated each training and testing phase for 100 times to track the variance induced by this aggressive subsampling. ", "page_idx": 8}, {"type": "text", "text": "To demonstrate the impact of unsupervised pre-training, we compared several methods across multiple configu", "page_idx": 8}, {"type": "text", "text": "rations (Table 1): ", "page_idx": 8}, {"type": "text", "text": "a) Kernels on distributions. This group includes Fisher kernels applied to parametric Gaussian Mixture Model (GMM) estimates, as suggested in Krapac et al. [24], along with Sliced Wasserstein-1 and Sliced Wasserstein-2 kernels [37]. ", "page_idx": 8}, {"type": "text", "text": "b) MMD kernels. Here, we employ a Gaussian embedding kernel for mean embeddings, marked as \u2019MMD\u2019 (Maximum Mean Discrepancy) in the table. This category includes various options for the distribution kernel, such as linear, Gaussian, Cauchy, and inverse multiquadrics. ", "page_idx": 8}, {"type": "text", "text": "c) Distributional Variance. An ablation study is conducted to demonstrate the results of directly optimizing the distributional variance defined in Eq. (15). ", "page_idx": 8}, {"type": "text", "text": "d) MDKE. We explore various configurations of the encoder optimized with the MDKE objective. We report performance for randomly initialized encoder, and for unsupervised pre-trained encoder with and without regularization. Random initialization happens only once, and all subsequent accuracy measurements are taken using the same encoder. ", "page_idx": 8}, {"type": "text", "text": "The variance reported for each model is measured across multiple runs to demonstrate the effect of the sampling. Importantly, the optimization of the MDKE objective results in embedding kernels with significantly lower variance compared to non-data-specific kernels. ", "page_idx": 8}, {"type": "text", "text": "5.2 Image and Text Modalities ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the following section, we present additional experiments on learning data-dependent distribution kernels in domains not typically considered distributional regression tasks, specifically the image and text domains. While we acknowledge the existence of more powerful domain-specific models and methods for both modalities, we provide these results to demonstrate the framework\u2019s applicability across a wide range of settings under appropriate choice of the representation model. Representing text as empirical samples from the finite space of tokens (i.e. words from the dictionary) is quite common, while the choice to model images as histograms over pixel positions is more subtle. We demonstrate that, in both scenarios, unsupervised pre-training of the encoder yields distribution kernel that achieves strong performance on downstream classification tasks, showcasing the versatility of the proposed learning framework in scenarios where distribution regression formulations are uncommon. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Images. MNIST [12] and Fashion-MNIST [65] consist of $28\\times28$ pixel grayscale images divided into 10 classes. We considered each individual image to be a probability distribution (via rescaling pixel intensities so that $l_{1}(\\mathrm{image})=1)$ over the discrete space of pixel positions (i.e., histogram). Given this support space, the encoder $f_{\\theta}$ is a discrete map which we implemented as a table lookup (i.e., embeddings) from pixel indices to the points on the hypersphere $S^{31}$ . Embeddings were initialized by sampling points uniformly. Gradients of the MDKE objective with respect to the embeddings parameters were computed via automatic differentiation using projected gradient steps to ensure that the embeddings remain on the hypersphere. The small size of the support space enables the exact computation of the inner product between kernel mean embeddings of input distributions Eq. (7) and, subsequently, the distribution kernel Eq. (8) during both training and evaluation. This ensured a lower variance of the accuracy for the downstream classification. Performing MNIST classification upon pre-training with our unsupervised encoder significantly improves the baseline (random initialization of latent embeddings) accuracy of $85.0\\%$ by reaching a plateau at $92.15\\%$ . Refer to the detailed analysis of the latent spaces of the trained encoders in Appendix D.1. ", "page_idx": 9}, {"type": "text", "text": "Text. To assess our method\u2019s performance in a larger discrete support space, we utilized the \"20 Newsgroups\" [27], a multi-class text classification dataset. We reduced the size of the dataset to 5 classes (resulting in 2, 628 sentences and 38, 969 unique words) by subsampling both training and test subsets. We treated sentences as empirical distributions over words, assuming word sets to be enough for topic classification, despite no positional info. The encoder $f_{\\theta}$ mirrored the setup used in the MNIST case (Appendix D.1), with $l_{2}$ normalized word embeddings on $\\mathbb{S}^{31}$ . However, while in the MNIST case embeddings computations were performed exactly, here considering the entire embedding kernel Gram matrix is impractical due to its large size. Instead, we optimized embeddings by randomly sampling 20 words per sentence, making the inner product between embeddings a stochastic approximation. This setup is meant to confirm that the optimization of the proposed MDKE objective yields a solution that is robust w.r.t. the excessive risk induced by first-level subsampling. Detailed results can be found in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we presented an unsupervised way of learning data-dependent distribution kernel. While previous studies in distribution regression predominantly relied on hand-crafted kernels, our work, in contrast, demonstrates that entropy maximization can serve as a powerful guiding principle for learning adaptable, data-dependent kernel in the space of distributions. Our empirical findings show that this technique can not only serve as a pre-training step to enhance the performance of downstream distribution regression tasks, but also facilitate complex analyses of the input space. The interpretation of the learning dynamics induced by the proposed objective relies on a theoretical link between the quantum entropy of the dataset embedding and distributional variance. This theoretical link, which we have proven, enables us to approach the optimization from a geometrical perspective, providing crucial insights into the flexibility of the learned latent space encoding. ", "page_idx": 9}, {"type": "text", "text": "We hope that theoretically grounded way of learning data-dependent kernel for distribution regression tasks will become a strong alternative to the common practice of hand-picking kernels. More broadly, our results present a methodology for leveraging the distributional nature of input data along side the novel perspective on the encoding of complex input spaces. This highlights the potential to extend the application of more advanced learning methods, embracing the ever-increasing complexity of data by going beyond more conventional vector-based representations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We thank the Allen Institute for Brain Science founder, Paul G. Allen, for his vision, encouragement, and support. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] F. Bach. Information theory with kernel methods. IEEE Transactions on Information Theory, 69(2):752\u2013775, 2022.   \n[2] F. Bachoc, F. Gamboa, J.-M. Loubes, and N. Venet. A gaussian process regression model for distribution inputs. IEEE Transactions on Information Theory, 64(10):6620\u20136637, 2017.   \n[3] R. Berman, S. Boucksom, and D. W. Nystr\u00f6m. Fekete points and convergence towards equilibrium measures on complex manifolds. 2011.   \n[4] G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011. [5] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-neighbor based image classification. In 2008 IEEE conference on computer vision and pattern recognition, pages 1\u20138. IEEE, 2008.   \n[6] C. Bonet, B. Mal\u00e9zieux, A. Rakotomamonjy, L. Drumetz, T. Moreau, M. Kowalski, and N. Courty. Sliced-wasserstein on symmetric positive definite matrices for m/eeg signals. In International Conference on Machine Learning, pages 2777\u20132805. PMLR, 2023.   \n[7] S. V. Borodachov, D. P. Hardin, and E. B. Saff. Discrete energy on rectifiable sets. Springer, 2019.   \n[8] C. Boutsidis, M. W. Mahoney, and P. Drineas. An improved approximation algorithm for the column subset selection problem. In Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms, pages 968\u2013977. SIAM, 2009. [9] E. Carlen. Trace inequalities and quantum entropy: an introductory course. Entropy and the quantum, 529:73\u2013140, 2010.   \n[10] B. Charlier, J. Feydy, J. A. Glaunes, F.-D. Collin, and G. Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1\u20136, 2021.   \n[11] A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. Advances in neural information processing systems, 23, 2010.   \n[12] L. Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[13] S. Flaxman, D. J. Sutherland, Y.-X. Wang, and Y. W. Teh. Understanding the 2016 us presidential election using ecological inference and distribution regression with census microdata. arXiv preprint arXiv:1611.03787, 2016.   \n[14] B. Ghojogh, A. Ghodsi, F. Karray, and M. Crowley. Reproducing kernel hilbert space, mercer\u2019s theorem, eigenfunctions, nystr\u00f6m method, and use of kernels in machine learning: Tutorial and survey. arXiv preprint arXiv:2106.08443, 2021.   \n[15] L. G. S. Giraldo, M. Rao, and J. C. Principe. Measures of entropy from data using infinitely divisible kernels. IEEE Transactions on Information Theory, 61(1):535\u2013548, 2014.   \n[16] K. Grauman and T. Darrell. The pyramid match kernel: Efficient learning with sets of features. Journal of Machine Learning Research, 8(4), 2007.   \n[17] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. Advances in neural information processing systems, 11, 1998.   \n[18] T. Jebara and R. Kondor. Bhattacharyya and expected likelihood kernels. In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings, pages 57\u201371. Springer, 2003.   \n[19] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. The Journal of Machine Learning Research, 5:819\u2013844, 2004.   \n[20] T. Karvonen, S. S\u00e4rkk\u00e4, and K. Tanaka. Kernel-based interpolation at approximate fekete points. Numerical Algorithms, 87:445\u2013468, 2021.   \n[21] J. Kim, S. Kang, D. Hwang, J. Shin, and W. Rhee. Vne: An effective method for improving deep representation by manipulating eigenvalue distribution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3799\u20133810, 2023.   \n[22] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.   \n[23] S. Kolouri, Y. Zou, and G. K. Rohde. Sliced wasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5258\u20135267, 2016.   \n[24] J. Krapac, J. Verbeek, and F. Jurie. Modeling spatial layout with fisher vectors for image categorization. In 2011 International Conference on Computer Vision, pages 1487\u20131494. IEEE, 2011.   \n[25] G. Krummenacher, C. S. Ong, and J. Buhmann. Ellipsoidal multiple instance learning. In International Conference on Machine Learning, pages 73\u201381. PMLR, 2013.   \n[26] H. Kuck and N. de Freitas. Learning about individuals from group statistics, 2012. URL https://arxiv.org/abs/1207.1393.   \n[27] K. Lang. Newsweeder: Learning to fliter netnews. In Proceedings of the Twelfth International Conference on Machine Learning, pages 331\u2013339, 1995.   \n[28] H. C. L. Law, D. J. Sutherland, D. Sejdinovic, and S. Flaxman. Bayesian approaches to distribution regression. In International Conference on Artificial Intelligence and Statistics, pages 1167\u20131176. PMLR, 2018.   \n[29] M. Lemercier, C. Salvi, T. Damoulas, E. Bonilla, and T. Lyons. Distribution regression for sequential data. In International Conference on Artificial Intelligence and Statistics, pages 3754\u20133762. PMLR, 2021.   \n[30] T. Leung and J. Malik. Representing and recognizing the visual appearance of materials using three-dimensional textons. International journal of computer vision, 43:29\u201344, 2001.   \n[31] F. Liu, X. Huang, Y. Chen, and J. A. Suykens. Random features for kernel approximation: A survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7128\u20137148, 2021.   \n[32] W. Liu, L. Yu, A. Weller, and B. Sch\u00f6lkopf. Generalizing and decoupling neural collapse via hyperspherical uniformity gap, 2023.   \n[33] D. Lopez-Paz, K. Muandet, B. Sch\u00f6lkopf, and I. Tolstikhin. Towards a learning theory of cause-effect inference. In International Conference on Machine Learning, pages 1452\u20131461. PMLR, 2015.   \n[34] S. Lyu. Mercer kernels for object recognition with local features. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), volume 2, pages 223\u2013229. IEEE, 2005.   \n[35] J. Marzo and J. Ortega-Cerd\u00e0. Equidistribution of fekete points on the sphere. Constructive Approximation, 32(3):513\u2013521, 2010.   \n[36] G. Meanti, L. Carratino, L. Rosasco, and A. Rudi. Kernel methods through the roof: handling billions of points efficiently. Advances in Neural Information Processing Systems, 33:14410\u2013 14422, 2020.   \n[37] D. Meunier, M. Pontil, and C. Ciliberto. Distribution regression with sliced wasserstein kernels. In International Conference on Machine Learning, pages 15501\u201315523. PMLR, 2022.   \n[38] J. Mitrovic, D. Sejdinovic, and Y.-W. Teh. Dr-abc: Approximate bayesian computation with kernel-based distribution regression. In International Conference on Machine Learning, pages 1482\u20131491. PMLR, 2016.   \n[39] K. Muandet, K. Fukumizu, F. Dinuzzo, and B. Sch\u00f6lkopf. Learning from distributions via support measure machines. Advances in neural information processing systems, 25, 2012.   \n[40] K. Muandet, D. Balduzzi, and B. Sch\u00f6lkopf. Domain generalization via invariant feature representation. In International conference on machine learning, pages 10\u201318. PMLR, 2013.   \n[41] K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Sch\u00f6lkopf. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2): 1\u2013141, 2017. doi: 10.1561/2200000060. URL https://doi.org/10.1561%2F2200000060.   \n[42] M. M\u00fcller-Lennert, F. Dupuis, O. Szehr, S. Fehr, and M. Tomamichel. On quantum r\u00e9nyi entropies: A new generalization and some properties. Journal of Mathematical Physics, 54(12), 2013.   \n[43] M. Ntampaka, H. Trac, D. J. Sutherland, S. Fromenteau, B. P\u00f3czos, and J. Schneider. Dynamical mass measurements of contaminated galaxy clusters using machine learning. The Astrophysical Journal, 831(2):135, 2016.   \n[44] J. Oliva, B. P\u00f3czos, and J. Schneider. Distribution to distribution regression. In International Conference on Machine Learning, pages 1049\u20131057. PMLR, 2013.   \n[45] B. Poczos, A. Rinaldo, A. Singh, and L. Wasserman. Distribution-free distribution regression, 2013.   \n[46] A. Rakotomamonjy, A. Traore, M. Berar, R. Flamary, and N. Courty. Distance measure machines. arXiv preprint arXiv:1803.00250, 2018.   \n[47] A. Rudi, R. Camoriano, and L. Rosasco. Less is more: Nystr\u00f6m computational regularization. Advances in neural information processing systems, 28, 2015.   \n[48] C.-J. Simon-Gabriel and B. Sch\u00f6lkopf. Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions. The Journal of Machine Learning Research, 19(1):1708\u20131736, 2018.   \n[49] O. Skean, A. Dhakal, N. Jacobs, and L. G. S. Giraldo. Frossl: Frobenius norm minimization for self-supervised learning. arXiv preprint arXiv:2310.02903, 2023.   \n[50] B. K. Sriperumbudur, K. Fukumizu, and G. R. Lanckriet. Universality, characteristic kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7), 2011.   \n[51] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Sch\u00f6lkopf, and G. R. Lanckriet. On the empirical estimation of integral probability metrics. 2012.   \n[52] D. J. Sutherland, L. Xiong, B. P\u00f3czos, and J. Schneider. Kernels on sample sets via nonparametric divergence estimates. arXiv preprint arXiv:1202.0302, 2012.   \n[53] Z. Szab\u00f3, A. Gretton, B. P\u00f3czos, and B. Sriperumbudur. Two-stage sampled learning theory on distributions. In Artificial Intelligence and Statistics, pages 948\u2013957. PMLR, 2015.   \n[54] Z. Szab\u00f3, B. K. Sriperumbudur, B. P\u00f3czos, and A. Gretton. Learning theory for distribution regression. The Journal of Machine Learning Research, 17(1):5272\u20135311, 2016.   \n[55] Z. Tan, J. Yang, W. Huang, Y. Yuan, and Y. Zhang. Information flow in self-supervised learning. arXiv preprint arXiv:2309.17281, 2023.   \n[56] M. C. Thrun, J. Hoffmann, M. R\u00f6hnert, M. von Bonin, U. Oelschl\u00e4gel, C. Brendel, and A. Ultsch. Flow cytometry datasets consisting of peripheral blood and bone marrow samples for the evaluation of explainable artificial intelligence methods. Data in Brief, 43:108382, 2022.   \n[57] J.-F. Ton, C. Lucian, Y. W. Teh, and D. Sejdinovic. Noise contrastive meta-learning for conditional density estimation using kernel mean embeddings. In International Conference on Artificial Intelligence and Statistics, pages 1099\u20131107. PMLR, 2021.   \n[58] J.-F. Ton, D. Sejdinovic, and K. Fukumizu. Meta learning for causal direction. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 9897\u20139905, 2021.   \n[59] T. Tuytelaars, M. Fritz, K. Saenko, and T. Darrell. The nbnn kernel. In 2011 International Conference on Computer Vision, pages 1824\u20131831. IEEE, 2011.   \n[60] N. Vakhania, V. Tarieladze, and S. Chobanyan. Probability distributions on Banach spaces, volume 14. Springer Science & Business Media, 2012.   \n[61] T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere, 2022.   \n[62] K. Q. Weinberger and G. Tesauro. Metric learning for kernel regression. In Artificial intelligence and statistics, pages 612\u2013619. PMLR, 2007.   \n[63] M. M. Wilde. Quantum information theory. Cambridge university press, 2013.   \n[64] C. Williams and M. Seeger. Using the nystr\u00f6m method to speed up kernel machines. Advances in neural information processing systems, 13, 2000.   \n[65] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.   \n[66] L. Xu, Y. Chen, S. Srinivasan, N. de Freitas, A. Doucet, and A. Gretton. Learning deep features in instrumental variable regression. arXiv preprint arXiv:2010.07154, 2020.   \n[67] L. Xu, H. Kanagawa, and A. Gretton. Deep proxy causal learning and its application to confounded bandit policy evaluation. Advances in Neural Information Processing Systems, 34: 26264\u201326275, 2021.   \n[68] Y. Yoshikawa, T. Iwata, and H. Sawada. Latent support measure machines for bag-of-words data classification. Advances in neural information processing systems, 27, 2014.   \n[69] Z. Yu, D. W. Ho, Z. Shi, and D.-X. Zhou. Robust kernel-based distribution regression. Inverse Problems, 37(10):105014, 2021.   \n[70] Z.-H. Zhou, Y.-Y. Sun, and Y.-F. Li. Multi-instance learning by treating instances as non-i.i.d. samples, 2009. URL https://arxiv.org/abs/0807.1997. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the proofs for the propositions outlined in our study. To ensure clarity, we will first restate the setup and introduce necessary concepts. ", "page_idx": 14}, {"type": "text", "text": "We define the input space as $\\mathcal{X}$ , and $\\mathcal{P}(\\mathcal{X})$ represents the space of probability distributions over $\\mathcal{X}$ . Consider a dataset of $M$ probability distributions, denoted as $\\bar{D_{M}}\\bar{=}\\,\\{P_{i}\\bar{\\in}\\,\\mathcal{P}(\\boldsymbol{\\chi})\\}_{i=1}^{M}$ . With a p.d. characteristic embedding kernel $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ , the corresponding RKHS $\\mathcal{H}$ , and the feature map $\\phi:\\mathcal{X}\\to\\mathcal{H}$ , we define the mean embedding map $\\mu:\\mathcal{P}(\\mathcal{X})\\rightarrow\\mathcal{H}$ such that $\\mu_{P}=\\mu(P):=$ $\\textstyle\\int_{\\mathcal{X}}\\phi({\\boldsymbol{x}})\\,d P({\\boldsymbol{x}})$ . A p.d. translation-invariant characteristic distribution kernel $K:{\\mathcal{P}}({\\mathcal{X}})\\times{\\mathcal{P}}({\\mathcal{X}})\\to$ $\\mathbb{R}$ is defined using the mean embeddings of corresponding distributions. For simplicity, $\\mu_{i}$ denotes $\\mu(P_{i})$ . ", "page_idx": 14}, {"type": "text", "text": "Additional concepts essential for our proofs include the Gram matrix of mean embeddings $G\\in$ $\\mathbb{R}^{M\\times M}$ , representing the inner products of mean embeddings in the dataset, i.e., $G:=[\\langle\\mu_{i},\\bar{\\mu}_{j}\\rangle_{\\mathcal{H}}]_{i j}$ . The kernel matrix $K_{\\mathcal{D}}\\,\\in\\,\\mathbb{R}^{M\\times M}$ with respect to the distribution kernel $K$ is denoted as $K_{D}:=$ $[K(P_{i},P_{j})]_{i j}$ . We also recall the definition of distributional variance $\\mathbb{V}_{\\mathcal{H}}$ (see Eq. (15)): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M}):=\\frac{1}{M}\\mathrm{tr}[G]-\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}G_{i j}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "These definitions and notations will be referenced throughout the proofs. ", "page_idx": 14}, {"type": "text", "text": "A.1 Kernel Norms Gap and Distributional Variance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For both cases of input distributions being empirical probability distributions or continuous densities, we define mixture distribution $\\bar{P}$ , with a slight abuse of notation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bar{P}(x):=\\frac{1}{M}\\sum_{i=1}^{M}P_{i}(x)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. For the mixture distribution $\\bar{P}$ and the Gram matrix $G$ , the following relationship holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\mu_{\\bar{P}}\\|_{\\mathcal{H}}^{2}=\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}G_{i j}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We begin by recalling that the inner product between mean embeddings $\\mu_{i}$ and $\\mu_{j}$ in $\\mathcal{H}$ is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle\\mu_{i},\\mu_{j}\\rangle_{\\mathcal{H}}=\\iint_{\\mathcal{X}\\times\\mathcal{X}}k(x,x^{\\prime})\\,d P_{i}(x)\\,d P_{j}(x^{\\prime})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Substituting this into the expression for the Gram matrix, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}G_{i j}=\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\iint_{\\mathcal{X}\\times\\mathcal{X}}k(x,x^{\\prime})\\,d P_{i}(x)\\,d P_{j}(x^{\\prime})}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\iint_{\\mathcal{X}\\times\\mathcal{X}}k(x,x^{\\prime})\\,d\\left(\\frac{1}{M}\\sum_{i=1}^{M}P_{i}(x)\\right)\\,d\\left(\\frac{1}{M}\\sum_{j=1}^{M}P_{j}(x^{\\prime})\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\iint_{\\mathcal{X}\\times\\mathcal{X}}k(x,x^{\\prime})\\,d\\Bar{P}(x)\\,d\\Bar{P}(x^{\\prime})=\\|\\mu_{\\Bar{P}}\\|_{\\mathcal{H}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "By incorporating Eq. (19) into the definition of distributional variance (see Eq. (15)), and noting that the trace of $G$ is the sum of squared norms of input distributions (i.e., $\\begin{array}{r}{\\mathrm{tr}[G]=\\sum_{i=1}^{M}\\|\\mu_{P_{i}}\\|_{\\mathcal{H}}^{2})}\\end{array}$ , we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})\\equiv\\frac{1}{M}\\sum_{i=1}^{M}\\|\\mu_{P_{i}}\\|_{\\mathcal{H}}^{2}-\\|\\mu_{\\bar{P}}\\|_{\\mathcal{H}}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This result offers a new and intuitive perspective on distributional variance, conceptualizing it as the difference between the average squared norm of individual input distributions and the squared norm of the mixture distribution. ", "page_idx": 15}, {"type": "text", "text": "A.2 Pairwise Distance and Distributional Variance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Definition A.2. For a dataset $\\mathcal{D}_{M}$ of $M$ probability distributions, we define the average pairwise distance between kernel mean embeddings in $\\mathcal{H}$ as $\\dot{\\mathbb{J}}_{\\mathcal{H}}(\\mathcal{D}_{M})$ , given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{J}_{\\mathcal{H}}(\\mathcal{D}_{M}):=\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}||\\mu_{i}-\\mu_{j}||_{\\mathcal{H}}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. For the distributional variance $\\mathbb{V}_{\\mathcal{H}}$ of the dataset $\\mathcal{D}_{M}$ , the following relationship holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})\\equiv\\frac{1}{2}\\cdot\\mathbb{J}_{\\mathcal{H}}(\\mathcal{D}_{M})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Starting with the definition of $\\mathbb{J}_{\\mathcal{H}}(\\mathcal{D}_{M})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{\\mu}(D_{M})=\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\|\\mu_{i}-\\mu_{j}\\|_{W}^{2}}&{}\\\\ &{=\\displaystyle\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\|\\mu_{i}-\\mu_{j},\\mu_{i}-\\mu_{j}\\rangle_{W}}\\\\ &{=\\displaystyle\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\sum_{j=1}^{M}(\\langle\\mu_{i},\\mu_{i}\\rangle_{W}+\\langle\\mu_{j},\\mu_{j}\\rangle_{W}-2\\langle\\mu_{i},\\mu_{j}\\rangle_{W})}\\\\ &{=\\displaystyle\\frac{1}{M^{2}}\\left(2\\Lambda\\displaystyle\\sum_{i=1}^{M}\\langle\\mu_{i},\\mu_{i}\\rangle_{W}-2\\sum_{\\mathrm{trim}}^{M}\\sum_{j=1}^{M}\\langle\\mu_{i},\\mu_{j}\\rangle_{W}\\right)}\\\\ &{=2\\left(\\frac{1}{M}\\displaystyle\\sum_{i=1}^{M}\\langle\\mu_{i},\\mu_{i}\\rangle_{W}-\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\langle\\mu_{i},\\mu_{j}\\rangle_{W}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recognizing that $\\begin{array}{r}{\\sum_{i=1}^{M}\\left\\langle\\mu_{i},\\mu_{i}\\right\\rangle_{\\mathcal{H}}=\\mathbf{tr}[G]}\\end{array}$ , we can equate expression in the brackets to $\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})$ , leading to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D}_{M})\\equiv\\frac{1}{2}\\cdot\\mathbb{J}_{\\mathcal{H}}(\\mathcal{D}_{M}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 will be instrumental in further proofs, establishing a crucial link between distributional variance in $\\mathcal{H}$ and quantum entropy of the covariance operator embedding $\\Sigma_{\\mathcal{D}}$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Distribution Kernel Entropy Upper-Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the proof for the key theoretical result stated in Proposition 3.3. ", "page_idx": 15}, {"type": "text", "text": "Consider a dataset $\\mathcal{D}$ consisting of probability distributions $\\{P\\in\\mathcal P(\\mathcal X)\\}_{i}$ sampled i.i.d. from an unknown meta-distribution $\\mathbb{D}$ . We assert that the second-order R\u00e9nyi entropy $S_{2}$ of the empirical covariance operator embedding $\\Sigma_{\\mathcal{D}}$ , induced by the choice of Gaussian distribution kernel $K_{\\mathrm{RBF}}$ over points in the RKHS $\\mathcal{H}$ , is upper-bounded by the distributional variance $\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2\\gamma}S_{2}(\\hat{\\Sigma}_{\\mathcal{D}})\\leq\\mathbb{V}_{\\mathcal{H}}(\\mathcal{D})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\gamma$ is the bandwidth of the distribution kernel $K_{\\mathrm{RBF}}$ . ", "page_idx": 16}, {"type": "text", "text": "Starting from the properties of $S_{2}(\\hat{\\Sigma}_{\\mathcal{D}})$ stated in Eq. (13): ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\mathcal{S}}_{2}(\\hat{\\Sigma}_{\\mathcal{D}})={\\mathcal{S}}_{2}\\left(\\frac{1}{M}K_{\\mathcal{D}}\\right)=-\\log\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\left(\\frac{1}{M}K_{i j}\\right)^{2}=-\\log\\left(\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}K_{i j}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Jensen\u2019s inequality, considering the concavity of the log, to Eq. (22), we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{2}(\\widehat{\\Sigma}\\boldsymbol{v})\\leq-\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\displaystyle\\sum_{j=1}^{M}\\log K_{i j}^{2}}\\\\ &{\\qquad=-\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\log\\exp\\left(-\\frac{\\gamma}{2}\\|\\mu_{i}-\\mu_{j}\\|_{\\mathcal{H}}^{2}\\right)^{2}}\\\\ &{\\qquad=-\\displaystyle\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\sum_{j=1}^{M}\\log\\exp\\left(-\\gamma\\|\\mu_{i}-\\mu_{j}\\|_{\\mathcal{H}}^{2}\\right)}\\\\ &{\\qquad=\\displaystyle\\gamma\\left(\\frac{1}{M^{2}}\\displaystyle\\sum_{i=1}^{M}\\|\\mu_{i}-\\mu_{j}\\|_{\\mathcal{H}}^{2}\\right)}\\\\ &{\\qquad=\\gamma\\cdot\\mathcal{H}(D)}\\\\ &{\\qquad=2\\gamma\\cdot\\mathcal{V}_{\\mathbb{H}}(D)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Eq. (23) uses the definition of $K_{\\mathrm{RBF}}$ from Eq. (8), Eq. (24) is derived from the definition of the average pairwise distance in Eq. (20), and Eq. (25) follows from Lemma A.3. This completes the proof of Proposition 3.3, establishing the upper bound for the second-order quantum R\u00e9nyi entropy of the covariance operator embedding $\\Sigma_{\\mathcal{D}}$ in terms of the distributional variance in RKHS $\\mathcal{H}$ \uff0e\u53e3 Remark A.4. It is important to note that $S_{2}$ is typically measured using the base-2 logarithm $(\\mathrm{log_{2}})$ rather than the natural logarithm. However, the proof remains accurate with a proper re-scaling to account for the change in the logarithmic base. ", "page_idx": 16}, {"type": "text", "text": "A.4 Generalized Variance in RKHS ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we prove the connection between generalized variance and norm of the kernel mean embedding. ", "page_idx": 16}, {"type": "text", "text": "So far we established the squared norm of mean embedding is maximized by Dirac points (zero variance) and is minimized by uniform distribution (max variance). We now formalize the intuition that larger squared norm in embedding kernel RHKS corresponds to smaller variance in the latent space. We first note that under Assumption 3.1, the kernel metric $d_{k_{\\mathrm{cmb}}}$ (see Eq. (3)) induced by the choice of $k_{\\mathrm{emb}}$ is a monotonically increasing function of Euclidean metric on the latent space, thus is representative of the geometry of encoded input distributions. Which let us use a generalized notion of variance here. ", "page_idx": 16}, {"type": "text", "text": "Definition A.5. (Vakhania et al. [60]) Let $X$ be a random variable which takes values in a Fr\u00e9chet space $\\mathcal{F}$ equipped with seminorm $\\|\\cdot\\|_{\\alpha}$ . And suppose that $X$ is square-integratable, in a sense that $\\bar{\\mathbb{E}}\\|X\\|_{\\alpha}^{2}<\\infty$ . Let $\\mu\\in{\\mathcal{F}}$ be a Pettis integral of $X$ (i.e. generalization of the mean). Generalized variance of $X$ w.r.t. seminorm $\\|\\cdot\\|_{\\alpha}$ is defined as following ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\alpha}[X]:=\\mathbb{E}\\|X-\\mu\\|_{\\alpha}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that for a random variable in RKHS defined as a push-forward of a probability distribution $P\\in\\mathcal{P}(\\mathcal{Z})$ over the latent space, i.e. $X=z\\sim\\phi{\\#}P$ satisfies conditions of Definition A.5 with ${\\mathcal{F}}={\\mathcal{H}}$ , $\\|\\cdot\\|_{\\alpha}=\\|\\cdot\\|_{\\mathcal{H}_{\\mathrm{cmb}}}$ , and a Pettis integral being a kernel mean embedding. Denote the described variance as $\\operatorname{Var}_{\\mathcal{H}_{\\mathrm{cmb}}}[P]$ . We now show that ", "page_idx": 16}, {"type": "text", "text": "Proposition A.6. Under Assumption 3.1, for every $P\\in\\mathcal{P}(\\mathbb{Z})$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{V a r}_{\\mathcal{H}_{e m b}}[P]=1-\\|{\\mu}_{e m b}(P)\\|_{\\mathcal{H}_{e m b}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. From the definition of generalized variance we have the following ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Var}_{h o n}\\big[P\\big]=\\mathbb{E}_{x\\sim P}\\|\\phi(x)-\\mu\\,P\\|_{\\mathcal{H}_{\\omega}}^{2}}\\\\ {=}&{\\int\\|\\phi(x)-\\mu\\{P\\}\\|_{\\mathcal{H}_{\\omega}}^{2}\\,d P(x)}\\\\ {=}&{\\int\\Big(\\phi(x)-\\mu\\,P,\\phi(x)-\\mu\\,P\\big)_{\\mathcal{H}_{\\omega}}\\,d P(x)}\\\\ {=}&{\\int\\big(\\|\\phi(x)\\|_{\\mathcal{H}_{\\omega}}^{2}+\\|\\mu\\|_{\\mathcal{H}_{\\omega}}^{2}-2(\\phi(x),\\mu_{P})_{\\mathcal{H}_{\\omega}}\\big)\\,d P(x)}\\\\ {=}&{\\int\\|\\phi(x)\\|_{\\mathcal{H}_{\\omega}}^{2}\\,d P(x)+\\int\\|\\mu\\,P\\|_{\\mathcal{H}_{\\omega}}^{2}\\,d P(x)-2\\int\\big(\\phi(x),\\mu\\,P\\big)_{\\mathcal{H}_{\\omega}}\\,d P(x)}\\\\ {=}&{\\int\\|\\phi(x)\\|_{\\mathcal{H}_{\\omega}}^{2}\\,d P(x)+\\int\\|\\mu\\,P\\|_{\\mathcal{H}_{\\omega}}^{2}\\,d P(x)-2\\left\\langle\\int\\phi(x)\\,d P(x),\\mu_{P}\\right\\rangle_{\\mathcal{H}_{\\omega}}\\,d P(x)}\\\\ {=}&{1+\\|\\mu\\,P\\|_{\\mathcal{H}_{\\omega}}^{2}-2\\left\\|\\mu\\,P\\right\\|_{\\mathcal{H}_{\\omega}}^{2}}\\\\ &{=1-\\|\\mu\\,P\\|_{\\mathcal{H}_{\\omega}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "The proposition demonstrates the motivation behind training the encoder to minimize the variance in the latent space by maximizing the (average) squared norm of the mean embeddings. ", "page_idx": 17}, {"type": "text", "text": "B Practical Aspects of Learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Kernel Hyperparameter Selection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In practice, the effectiveness of the optimization process is significantly influenced by the parameters of the Gaussian kernels. Setting the bandwidth parameter $\\gamma$ either too low or too high can hinder the model\u2019s ability to learn effectively. This issue is a well-known challenge in working with kernel methods, where determining the optimal kernel bandwidth has been an area of extensive study. ", "page_idx": 17}, {"type": "text", "text": "In our work, we have employed an empirical approach, which involves adjusting $\\gamma$ based on the idealized structure of the dataset after its projection onto the latent space. Specifically, we set $\\gamma$ such that $1/\\gamma$ equals 10 times the average distance to the nearest neighbor in the set of points sampled uniformly on $\\mathbb{S}^{d-1}$ (inspired by the experimental approach in Blanchard et al. [4]). The number of points is chosen to be the number of distributions in the training set for the distribution kernel. For the embedding kernel, it is set to the number of distributions in a batch multiplied by the number of samples per distribution used for unsupervised training. ", "page_idx": 17}, {"type": "text", "text": "B.2 Optional Regularization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As the theoretical maximum of $S_{2}$ entropy is attained when the spectrum of $\\scriptstyle{\\frac{1}{M}}K_{D}$ is uniform, the optimal encoder with respect to $\\mathcal{L}_{\\mathrm{MDKE}}$ also tends to maximize the determinant of the kernel Gram matrix $K_{D}$ . This relationship is intuitive when considering that the determinant of a matrix is the product of its eigenvalues. The points that maximize such determinant $K_{D}$ are theoretically known as Fekete points [35, 3] and, in our case, are relative to the distribution kernel. Fekete points have been shown by Karvonen et al. [20] to be an optimal configuration for learning kernel interpolants, making them particularly suitable for downstream tasks framed as kernel regression. As such, encoders optimized under the $\\mathcal{L}_{\\mathrm{MDKE}}$ objective facilitate more accurate and robust performance in subsequent regression tasks. ", "page_idx": 17}, {"type": "text", "text": "In practice, we found that optimizing the MDKE objective posed certain numerical challenges, particularly due to the tendency of too small eigenvalues in the distribution kernel matrix $K_{D}$ to collapse near the optimal value of the objective when using large batch size for training. To mitigate this issue and prevent undesirable optimization behavior, we have introduced a regularization term to the original objective. This term is inspired by the concept of Fekete points configuration, leading to ", "page_idx": 17}, {"type": "text", "text": "the following loss function: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{MDKE-R}}(\\boldsymbol{\\theta}):=-S_{2}(\\Sigma_{\\mathcal{D}})+\\epsilon\\cdot\\Omega(\\Sigma_{\\mathcal{D}})}\\\\ &{\\qquad\\qquad\\qquad=\\log\\|\\frac{1}{M}K_{\\mathcal{D}}\\|_{F}^{2}-\\epsilon\\cdot\\log\\operatorname*{det}\\left|\\frac{1}{M}K_{\\mathcal{D}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\epsilon$ serves as a hyperparameter to control the strength of regularization. The regularization term $\\Omega(\\Sigma_{\\mathcal{D}})$ is designed to stabilize the optimization process by counteracting the effects of the collapse of small eigenvalues. The empirical evidence supporting the effectiveness of regularization in stabilizing the training dynamics for the sentence representation learning experiment (as detailed in Appendix D.1) is showcased in Fig. 3. ", "page_idx": 18}, {"type": "text", "text": "B.3 Runtime Complexity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Scalability of the kernel methods is typically a key concern when it comes to practical applications. The runtime complexity of the proposed method could be decomposed into two components: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Computation of the distribution kernel Gram matrix. This involves computing $O(N^{2})$ inner products between distributions where $N$ is a number of distributions. Each inner product involves computing matrix of pairwise distances between samples from each distribution, in the case of computing the Gaussian kernel over points on the hypersphere, the runtime complexity is the complexity of multiplying matrices $\\mathbb{R}^{M\\times d}$ where $M$ is a number of samples. All computations could be efficiently parallelized.   \n\u2022 Solving regression using distribution kernel Gram matrix. The complexity comes from matrix inverse and is typically estimated to be $O(N^{3})$ with a range of methods proposed to reduce the complexity [8, 47, 64, 36, 31]. ", "page_idx": 18}, {"type": "text", "text": "With this being said, we want to highlight that the runtime complexity of the proposed method is most favorable exactly for those tasks and datasets where distributional regression is an appropriate modeling approach. A significant number of samples per distribution ensures a high accuracy in approximating the kernel between a pair of distributions, and, at the same time, a relatively small number of distributions in the dataset alleviates issues related to storing the distribution kernel Gram matrix in memory and performing computations on it ", "page_idx": 18}, {"type": "image", "img_path": "A0cok1GK9c/tmp/83e8fbebad874638bd37924ca09e6594c430828f1dceb5e88412a29af22c7d63.jpg", "img_caption": ["Figure 3: The effect or regularization on the training dynamics. The distribution of the eigenvalues of the distribution kernel Gram matrix, calculated for 2,000 sentences sampled from $^{\\circ}20$ Newsgroups\u2019 dataset (details in Appendix D.2), is observed throughout the training. (a) Training with no regularization leads to the collapse of smaller eigenvalues. (b) The regularization stabilizes the training by preventing eigenvalues from collapsing. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Connection to Other Frameworks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Distributional Regression Landscape ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As the distributional regression task differs from common Machine Learning (ML) setups where inputs are given as vectors, the effective practical solution requires unique considerations. ", "page_idx": 19}, {"type": "text", "text": "The most obvious approach would be to ignore the fact that inputs are given as distributions and learn classifier on the space of samples, with a proper aggregation of posteriors (e.g. with a simple sum over histograms). This approach, while being simple, has been shown not to yield practically useful results, and was explicitly excluded from reported performance on different tasks by different authors [39, 46]. ", "page_idx": 19}, {"type": "text", "text": "Viable approaches to solving distributional regression could be, approximately, split up into the following categories. ", "page_idx": 19}, {"type": "text", "text": "Discriminate generative models. The idea is to fit each input distribution to a parametric family, e.g. Gaussian Mixture Model (GMM), to use available closed-form solutions to compute kernel or similarity or distance between distributions. This group of methods originated from the work on hidden Markov models and it\u2019s applications to processing sequence modalities, like text, DNA, proteins, and more. Early work [17] on learning discriminative classifier for generative models leveraged the fact that parametric models forms a Riemannian manifold with local metric given by Fisher information, they derived kernel function termed Fisher kernel suitable for running SVM between generative models. Driving motivation was classification between hidden Markov models, with the developed method being applied to DNA and protein sequence analysis. Following the same modeling approach of analysing DNA sequences with discriminative models between Markov models, Jebara and Kondor [18] proposed to use Bhattacharyya distance between distributions from exponential families to derived so-called Expected Likelihood Kernel. Jebara et al. [19] explored the method of computing kernel between distributions as the integral of the product of pairs of distributions, termed Probability Product Kernel. A new family of kernels was applied to the same setup of discriminating task on text modality, hidden Markov models for biological data. The model was also successfully applied for analysis of linear dynamical systems for time series data. Critical advantage of this work was access to computationally effective way of computing kernel for the distributions without having access to analytical closed-form parametrization (only relying on samples). In Sec. 5.1 we used fitting distributions to GMM with Fisher Kernel applied to learn parameters as a baseline for the performance on the task. ", "page_idx": 19}, {"type": "text", "text": "Point clouds. This group includes methods that model each input distribution as a set of points (also known as \u2019point cloud\u2019 or \u2019feature group\u2019 in computer vision) and use kernel or similarity function defined on sets. Large portion of the methods in this group arised in computer vision (CV) field when local features extractor were widely employed to pre-process images yielding either per-image histograms or sets of low dimensional vectors. The group includes kernels based on nonparametric divergence estimates, quantized set kernels, and so-called \u2019nearest-neighbor\u2019 set kernels to name a few [52, 30, 34, 16, 5, 5, 59]. Such kernels were employed to many CV tasks though the successfully application required not only a good kernel but also a high-quality feature extractor. Special attention goes to methods leveraging Wasserstein distance (including both kernel-based and similarity-based solutions). Wasserstein distance as a metric-aware discrepancy measure for probability distributions is a natural choice of deriving kernels or similarity functions between point clouds [46]. While being computationally problematic for large-scale problems, Sliced Wasserstein kernels were succesfully adopted [23, 37] as they provided both reliable way for the point set comparison with a reduced cost leveraging sliced formulation. In Sec. 5.1 we compared performance of Sliced Wasserstein-1 and Sliced Wasserstein-2 kernels, with the latter yielding significantly lower performance. Such a behavior is consistent with theoretical analysis stating that Sliced Wasserstein-1 has favoriable properties when compared to kernel based on Wasserstein-2 distance. ", "page_idx": 19}, {"type": "text", "text": "Kernel Mean Embeddidngs. Muandet et al. [39] proposed to leverage kernel mean embedding of measures in RKHS, so that the distributional regression could now be casted to a regression the corresponding Hilbert space. While the original work leveraged the kernel between the RKHS embeddings to train SVM, Szab\u00f3 et al. [53] provided theoretical guarantees for the for learning Ridge regressor from distribution embeddings in the Hilbert space to the outputs. Law et al. [28] applied the same approach in Bayesian regression settings, and [2] used it for Gaussian Process Regression. The setup gives a great deal of flexibility by choosing the kernel in RKHS, with a few being tested in practice. Gaussian kernel is a common choice, due to its universality [11]. Among others, inverse multiquadric (IMQ) is a popular choice, typically paired with random features to improve the runtime complexity of the algorithm. Linear kernel in RKHS, despite not being universal, was reported to produce competitive results in multiple practical setups. In Sec. 5.1 we compared performance for 4 different kernels in RKHS, namely linear, Gaussian, Cauchy, and IMQ, all defined as functions of Maximum Mean Discrepancy (MMD), which gives as a way to compute the value of the kernel from finite number of samples with high accuracy. While all kernels demonstrated different performance in terms of the test accuracy, the difference reported is not that substantial. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Distribution kernels were successfully applied in different kernel-based methods and in different modalities, like distribution to distribution regression [44], distribution regression for sequential data [29], and more. In cases where input data is naturally represented as a probability distribution but not immediately applicable for existing distribution regression solutions, pre-processing or encoding of inputs is required. Yoshikawa et al. [68] proposed learning a latent space representation for input data to apply distribution regression in the text modality, with the encoder being trained jointly with the classifier. To our best knowledge, methods for learning data-dependent kernel for solving distributional regressions were not previously reported. ", "page_idx": 20}, {"type": "text", "text": "C.2 Hyperspherical Uniformity Gap ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The link between entropy maximization in the space of distribution, the gap between average norm and norm of the mixture distribution marginalized over the dataset (i.e. \u2019average\u2019 distribution), and properties of distributions that minimize and maximize kernel mean embedding norm (as described in Sec. 3.4) unveils a subtle yet significant connection to the concept of the Hyperspherical Uniformity Gap (HUG), introduced in [32]. HUG has been developed to generalize the phenomenon of neural collapse observed in supervised classification settings. In our approach, working directly with samples from input distributions grants us explicit access to the \u2019grouping\u2019 of points in the input space. Even though the HUG framework setup does not have a notion of \u2019grouping\u2019, we can close the gap by noting that class labels provided with the dataset implicitly create \u2019groupings\u2019 of points, which can be interpreted as empirical samples drawn from latent probability distributions (one for each class). ", "page_idx": 20}, {"type": "text", "text": "However, a notable distinction lies in the dimensional aspect: HUG focuses on the distribution of points on the surface of a finite-dimensional hypersphere, whereas our work encompasses an infinite-dimensional hyperball setting, given the use of points in kernel-induced RKHSs. Furthermore, the loss function introduced in our study presents a unified optimization objective that weaves together both the inner-group and inter-group dynamics while being derived from first principles. In the HUG framework these two aspects are addressed separately. ", "page_idx": 20}, {"type": "text", "text": "Establishing a more formal connection between these frameworks emerges as a promising direction for future research. Such an endeavor could offer a novel perspective on supervised classification, particularly by conceptualizing class prototypes as probability distributions rather than mere vectors. This exploration might bridge the gap between these distinct approaches, enriching our understanding of classification paradigms in high-dimensional spaces. ", "page_idx": 20}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Image Classification Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Performing MNIST classification upon pre-training with our unsupervised encoder significantly improves the baseline (random initialization of latent embeddings) accuracy of $85.0\\%$ by reaching a plateau at $92.15\\%$ . ", "page_idx": 20}, {"type": "text", "text": "To understand the data-dependency of our encoding procedure, we analyzed the latent spaces of MNIST and Fashion-MNIST datasets. The visualization of pixel-level interaction, computed using a Gaussian kernel Gram matrix, reveals complex, dataset-specific interactions (Fig. 4a, 4c). Spectral clustering using the kernel Gram matrix provides deeper insight into the pixel interaction landscape. The chart shows clusters of pixels with correlated intensities (Fig. 4b, 4d), with the number of clusters set to 10 empirically. ", "page_idx": 20}, {"type": "image", "img_path": "A0cok1GK9c/tmp/db365fbbb918dfcc8b8ed083d96202dc9211d81c0f427107b1a74d6e020f53ea.jpg", "img_caption": ["Figure 4: Unsupervised encoding of Images. Unsupervised learning of image embeddings as finite-support distributions (i.e., histograms) of pixel intensities. For every pixel position we assign a point location on the unit hypersphere and optimize such locations via the covariance operator dataset embedding w.r.t. the MDKE objective. (a) Samples from the MNIST dataset and learned pixel-to-pixel interaction kernel Gram matrix. (b) Spectral clustering of pixels based on the learned kernel Gram matrix. (c) and (d) same as (a) and (b) for Fashion-MNIST dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "A0cok1GK9c/tmp/6da51606f41cd96a9d1f2df5f63a7e90dd5d49f558af78735ee733078bbf68b4.jpg", "img_caption": ["Figure 5: Unsupervised encoding of Text. Unsupervised learning of sentences embeddings as empirical distributions of words on the \u201920 Newsgroup\u2019 dataset. Goodness of the learned embeddings is evaluated by performing sentence-to-topic classification. (a) Distribution kernel entropy, distributional variance, and validation accuracy throughout training. (b) Kernel norms Eq. (16) throughout training. Shaded blue area (the difference between the blue and red lines) corresponds to the blue dotted line in panel (a) (up to a multiplicative factor). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.2 Text Classification Tasks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "During the unsupervised pre-training phase, we observed a steady decrease in training loss (Fig. 5a) despite the small batch size (50 sentences with a total of $1,000$ words). Importantly, for this experiment we employed a regularized version of the objective (MDKE-R). We discuss the rationale and empirical evidence supporting the use of this regularization scheme in Appendix B.2. During evaluation, to reduce computational complexity, we sample 2, 000 sentences for the train and 1, 000 for the test split, keeping the classes balanced. The maximum classification accuracy achieved was approximately $89.3\\%$ , while the random initialization performance averaged at $37.5\\%$ (Fig. 5). The framework\u2019s high accuracy in downstream classification showcases its prowess in learning potent latent representations, even when dealing with input distributions with large finite support. ", "page_idx": 21}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide an example that illustrates the implementation of the proposed method using the PyTorch framework. All experiments were performed on a single machine with 1 GPU and 6 CPUs. ", "page_idx": 22}, {"type": "text", "text": "Functions to compute distribution kernel Gram matrix: ", "page_idx": 22}, {"type": "text", "text": "def pairwise_kernel (x, gamma1): B, $\\textbf{T}=\\textbf{x}$ .size(0), x.size(1) X_unroll $=$ x.reshape(B\\*T, -1) dist $=$ (X_unroll[:, None , :] - X_unroll[None , :, :]) \\*\\* 2 dist $=$ torch.sum(dist , dim $^{=2}$ ) G = F.avg_pool2d( dist[None , : , :], kernel_size $=$ (T, T), stride $=$ (T, T) return torch.exp(-(gamma1/2.) \\* G.squeeze(0))   \ndef distribution_kernel_gram (x, gamma1 , gamma2): $\\begin{array}{r l}{\\mathbb{G}\\mathbf{x}\\mathbf{y}}&{{}=}\\end{array}$ pairwise_kernel (x, gamma1=gamma1) $\\begin{array}{r l r}{{\\sf G}{\\bf\\Sigma}}&{{}=}&{{\\sf G}{\\bf\\Sigma}=}\\end{array}$ torch.diag(Gxy) G = Gx[:, None] + Gy[None , :] - 2\\*Gxy $\\texttt{K}=$ torch.exp(-(gamma2/2.) \\* G) return Gxy , K ", "page_idx": 22}, {"type": "text", "text": "Distribution kernel entropy estimator and the MKDE loss: ", "page_idx": 22}, {"type": "text", "text": "def distribution_kernel_entropy (K): $\\mathrm{~\\textsf~{~B~}~}=\\mathrm{~\\textsf~{~K~}~}$ .size(0) Cov = (1/B) \\* K return -(Cov \\*\\* 2).sum().log2 ()   \ndef mkde_loss(encoder , X, gamma1 , gamma2): # X.shape is (n_distributions , n_samples , d_input) $\\begin{array}{r l}{{\\bf Z}}&{{}=}\\end{array}$ encoder(X) # Z.shape is (n_distributions , n_samples , d_latent) _, $\\texttt{K}=$ distribution_kernel_gram (Z, gamma1 , gamma2) # K.shape is (n_distributions , n_distributions ) loss $=$ - distribution_kernel_entropy (K) return loss ", "page_idx": 22}, {"type": "text", "text": "The code presented here follows the setup presented in Sec. 5 using Gaussian kernel both as embedding and as a distribution kernel. Other kernels could be used by adjusting implementation of both helper functions accordingly. When training on large datasets, Charlier et al. [10] might be used to avoid memory overflow in average pooling. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: the claim is outlined in the introduction, and it matches theoretical and experimental results presented in the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 3.6 for details. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 3.4 and Appendix A for details. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] Justification: See Sec. 5 and Appendix D for details. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code for the implementation of the proposed loss is provided in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 5 and Appendix D for details. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We report the variance of accuracy measurements that is unique to our data/- model setup, specifically the resampling variance. Detailed explanations are provided in Sec. 5. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix E for implementational details. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Sec. 3.6 for details. ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Only publicly released datasets were used, with the citations provided for each experiment. See Sec. 5 and Appendix D for details. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}]