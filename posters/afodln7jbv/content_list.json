[{"type": "text", "text": "An Accelerated Gradient Method for Convex Smooth Simple Bilevel Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jincheng Cao ECE Department UT Austin jinchengcao@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Ruichen Jiang   \nECE Department   \nUT Austin   \nrjiang@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Erfan Yazdandoost Hamedani SIE Department The University of Arizona erfany@arizona.edu ", "page_idx": 0}, {"type": "text", "text": "Aryan Mokhtari ECE Department UT Austin mokhtari@austin.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\\mathcal{O}(\\operatorname*{max}\\{1/\\sqrt{\\epsilon_{f}},1/\\epsilon_{g}\\})$ iterations to find a solution that is $\\epsilon_{f}$ -suboptimal and $\\epsilon_{g}$ -infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$ -th H\u00f6lderian error bound, we show that our method achieves an iteration complexity of O\u02dc(max{\u03f5f\u2212 $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$ , which matches the optimal complexity of single-level convex constrained optimization when $r=1$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate a class of bilevel optimization problems known as simple bilevel optimization, aiming to minimize an upper-level objective function over the solution set of a corresponding lower-level problem. This class has recently gained attention due to its broad applications in continual learning [1], hyper-parameter optimization [2, 3], meta-learning [4, 5], and over-parameterized machine learning [6\u20138]. Specifically, we focus on the following bilevel optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\;f(\\mathbf{x})\\qquad\\mathrm{s.t.}\\quad\\mathbf{x}\\in\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\mathrm{argmin}}\\;g(\\mathbf{z}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where, $\\mathcal{Z}$ is a convex set, and $f,g:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ are convex, continuously differentiable functions on an open set containing $\\mathcal{Z}$ . We assume that the lower-level objective function $g$ is convex but not strongly convex, so the lower-level problem may have multiple optimal solutions. Throughout the paper, we use $\\mathbf{x}^{*}$ to denote an optimal solution of problem (1). We define $f^{*}\\triangleq f(\\mathbf{x}^{*})$ and $g^{*}\\triangleq g(\\mathbf{x}^{*})$ , representing the optimal value of problem (1) and the optimal value of the lower-level objective $g$ , respectively. This class of problems is referred to as the \u201csimple bilevel problem\u201d [9\u201311] to distinguish it from more general settings with parameterized lower-level problems. ", "page_idx": 0}, {"type": "table", "img_path": "aFOdln7jBV/tmp/46955259c48e1fe841479c5718455d0a9b51f14db89c09f600a0a58cb31aa19d.jpg", "table_caption": ["Table 1: Non-asymptotic results on simple bilevel optimization. ( $\\Phi$ : with a first-order H\u00f6lderian error bound assumption on $g$ ; $\\textcircled{\\mathrm{r}}$ : with an $r$ th-order $r\\geq1$ ) H\u00f6lderian error bound assumption on $g$ ; $\\textsuperscript{\\textregistered}$ : additional assumption implying that the projection onto the sublevel set of $f$ is easy to compute.) "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The main challenge in solving problem (1) is that the feasible set, i.e., the optimal solution set of the lower-level problem, lacks a simple characterization and is not explicitly provided. This makes direct application of projection-based or projection-free methods infeasible, as projecting onto or solving a linear minimization problem over such an implicitly defined feasible set is intractable. Instead, our approach constructs an approximation set with specific properties, serving as a surrogate for the true feasible set. In Section 3, we detail how this set is constructed. Using this technique and building on the projected accelerated gradient method, we establish the best-known complexity bounds for solving problem (1). ", "page_idx": 1}, {"type": "text", "text": "To provide context, the best-known complexity bound for achieving an $\\epsilon_{}$ -accurate solution in singlelevel convex constrained optimization is $\\bar{O}(\\epsilon^{-0.5})$ , as demonstrated in [17]. This optimal bound was achieved using the accelerated proximal method or FISTA (Fast Iterative Shrinkage-Thresholding Algorithm), which also influenced the development of our algorithm. While the literature on bilevel optimization is not as extensive as that for single-level optimization, there have been recent nonasymptotic results for solving this class of problems, which we summarize in Table 1. ", "page_idx": 1}, {"type": "text", "text": "Specifically, these results aim to establish convergence rates on the infeasibility gap $g(\\mathbf{x}_{k})-g^{*}$ and the suboptimality gap $f(\\mathbf{x}_{k})-f^{*}$ after $k$ iterations. In [12], an iterative regularization-based method demonstrated a convergence rate of ${\\mathcal{O}}(1/k^{0.5-b})$ in terms of suboptimality and a rate of ${\\mathcal{O}}(1/k^{b})$ in terms of infeasibility, where $b\\in(0,0.5)$ is a user-defined parameter. Setting $b=0.25$ to balance these rates requires an iteration complexity of $\\mathcal{O}(\\operatorname*{max}\\{1/\\dot{\\epsilon}_{f}^{4},1/\\epsilon_{g}^{4}\\})$ to find a solution that is $\\epsilon_{f}$ -optimal and $\\epsilon_{g}$ -infeasible. Later, the Bi-Sub-Gradient (Bi-SG) algorithm was proposed in [13] to address convex simple bilevel optimization problems with nonsmooth upper-level objective functions. It showed convergence rates of $\\mathcal{O}(1/k^{1-\\dot{\\alpha}})$ and $\\mathcal{O}(1/k^{\\alpha})$ in terms of suboptimality and infeasibility, respectively, where $\\alpha\\in(0.5,1)$ serves as a hyper-parameter. Balancing the rates by setting $\\alpha=0.5$ results in an iteration complexity of $\\mathcal{O}(\\operatorname*{max}\\{\\bar{1}/\\epsilon_{f}^{\\bar{2}},1/\\epsilon_{g}^{2}\\})$ . Additionally, a structureexploiting method introduced in [14] achieved an iteration complexity of $\\mathcal{O}(\\operatorname*{max}\\{1/\\epsilon_{f}^{2},1/\\epsilon_{g}^{2}\\})$ when the upper-level objective is convex and the lower-level objective is convex and smooth. Imposing additional assumptions on the upper-level function, such as smoothness or strong convexity, does not result in faster rates for this method. ", "page_idx": 1}, {"type": "text", "text": "Recently, [6] presented a projection-free conditional gradient method (CG-BiO) that uses a cutting plane to approximate the solution set of the lower-level problem. Assuming both upperand lower-level objective functions are convex and smooth, CG-BiO achieves a complexity of $\\mathcal{O}(\\operatorname*{max}\\{1/\\epsilon_{f},1/\\epsilon_{g}\\})$ . Since the suboptimality gap $f(\\hat{\\mathbf{x}})-f^{*}$ may be negative for an infeasible point $\\hat{\\bf x}$ , a more desirable metric is the absolute suboptimality gap $\\bar{|f(\\hat{\\mathbf{x}})-\\bar{f}^{*}|}$ . To ensure this, [6] introduced the H\u00f6lderian error bound condition on $g$ . Specifically, under the $r$ -th order H\u00f6lderian error bound condition, CG-BiO finds a solution $\\hat{\\bf x}$ with $|f(\\hat{\\mathbf{x}})\\,-\\,f^{*}|\\,\\leq\\,\\epsilon_{f}$ and $g(\\hat{\\mathbf{x}})\\gets g^{*}\\,\\leq\\,\\epsilon_{g}$ after $\\mathcal{O}(\\operatorname*{max}\\{1/\\epsilon_{f}^{r},1/\\epsilon_{g}\\})$ iterations. More recently, [8] introduced the regularized proximal accelerated method (R-APM), which runs the proximal accelerated gradient method on a weighted sum of the upper- and lower-level objective functions. Assuming both functions are convex and smooth, they established a complexity bound of $\\mathcal{O}(\\operatorname*{max}\\{1/\\epsilon_{f},1/\\epsilon_{g}\\})$ to find an $(\\epsilon_{f},\\epsilon_{g})$ solution. This bound is worse than the $\\mathcal{O}(\\operatorname*{max}\\{1/\\sqrt{\\epsilon_{f}},1/\\epsilon_{g}\\})$ complexity achieved by our proposed AGMBiO method, assuming the feasible set $\\mathcal{Z}$ is compact. Additionally, [8] showed that when the lower-level objective function $g$ satisfies the weak sharpness property (equivalent to the H\u00f6lderian error bound cond\u221aition wi\u221ath $r\\,=\\,1\\,\\$ ), R-APM finds an $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution after at most $\\mathcal{O}(\\operatorname*{max}\\{1/\\sqrt{\\epsilon_{f}},{1}/\\sqrt{\\epsilon_{g}}\\})$ iterations. This result is comparable to our convergence result for AGM-BiO, which considers a more general H\u00f6lderian error bound condition. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Contributions. In this paper, we present a novel accelerated gradient-based bilevel optimization method, AGM-BiO, which offers state-of-the-art non-asymptotic guarantees for both suboptimality and infeasibility. At each iteration, AGM-BiO uses a cutting plane to linearly approximate the solution set of the lower-level problem, followed by a variant of the projected accelerated gradient update on the upper-level objective function. Below, we summarize our theoretical guarantees: ", "page_idx": 2}, {"type": "text", "text": "\u2022 When the feasible set $\\mathcal{Z}$ is compact, we s\u221ahow that AGM-BiO finds $\\hat{\\bf x}$ that satisfies $f(\\hat{\\mathbf{x}})\\!-\\!f^{*}\\le\\epsilon_{f}$ and $g(\\hat{\\mathbf{x}})-g^{*}\\leq\\epsilon_{g}$ within $\\mathcal{O}(\\operatorname*{max}\\{1/\\sqrt{\\epsilon_{f}},1/\\epsilon_{g}\\})$ iterations, where $f^{*}$ is the optimal value of problem (1) and $g^{\\ast}$ is the optimal value of the lower-level problem. \u2022 With an additional $r$ -th-order $(r\\ \\geq\\ 1)$ H\u00f6lderian error bound assumption on the lowerlevel problem, AGM-BiO finds $\\hat{\\bf x}$ satisfying $f(\\hat{\\mathbf{x}})\\,-\\,f^{*}\\,\\leq\\,\\epsilon_{f}$ and $g(\\hat{\\mathbf{x}})\\,-\\,g^{*}\\,\\leq\\,\\epsilon_{g}$ within $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$ iterations. Moreover, it achieves the stronger guarantee that $|f(\\hat{\\mathbf{x}})-f^{*}|\\leq\\epsilon_{f}$ and $g(\\hat{\\mathbf{x}})-g^{*}\\leq\\epsilon_{g}$ within $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-\\frac{2r-1}{2}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$ iterations. ", "page_idx": 2}, {"type": "text", "text": "These bounds all achieve the best-known complexity bounds in terms of both suboptimality and infeasibility guarantees for the considered settings. All the non-asymptotic results are summarized and compared in Table 1. ", "page_idx": 2}, {"type": "text", "text": "Discussions on two concurrent works. The authors in [15] proposed a bisection algorithm with a total operation complexity of $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-0.5},\\epsilon_{g}^{-0.5}\\})$ to find an $(\\epsilon_{f},\\epsilon_{g})$ -optimal solution, assuming the upper-level objective $f$ meets specific criteria. Specifically, Assumption 1(iv) in [15] implies the ability to compute the projection onto the sublevel set of the upper-level function $f$ . However, this assumption may not hold for general functions, such as the mean squared loss function in our over-parameterized regression example in Section 5. In [16], the authors introduced the penalty-based accelerated proximal gradient method (PB-APG) for solving simple bilevel optimization problems with the $r$ -th order H\u00f6lderian error bound assumption on the lower-level objective $g$ . Their algorithm, similar to [8], runs the accelerated proximal gradient method on a weighted sum of the upper and lower-level objective functions. PB-APG achieves a complexity of $\\mathcal{O}\\bar{(}\\epsilon_{f}^{-0.5r})+\\mathcal{O}(\\epsilon_{g}^{-0.5})$ to find an $(\\epsilon_{f},\\epsilon_{g})$ -optimal solution. The term $\\mathcal{O}(1/\\epsilon_{f}^{0.5r})$ can become significantly large as the order of the H\u00f6lderian error bound $r$ increases. In contrast, our algorithm, AGM-BiO, avoids this issue, requiring at most O\u02dc(max{\u03f5f\u2212 $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$ iterations to achieve an $(\\epsilon_{f},\\epsilon_{g})$ -optimal solution. Therefore, regardless of how large $r$ is, the worst-case complexity for AGM-BiO is $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-1},\\epsilon_{g}^{-1}\\})$ . Thus, our method achieves a better rate than PB-APG when $r>1$ . ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we state the assumptions and introduce the notions of optimality used in the paper. ", "page_idx": 2}, {"type": "text", "text": "2.1 Assumptions and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on the case where both the upper and lower-level functions $f$ and $g$ are convex and smooth.   \nFormally, we make the following assumptions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. Let $\\|\\cdot\\|$ be an arbitrary norm on $\\mathbb{R}^{n}$ and $\\Vert\\cdot\\Vert_{*}$ be its dual norm. We assume these conditions hold: ", "page_idx": 2}, {"type": "text", "text": "(i) $\\mathcal{Z}\\subset\\mathbb{R}^{n}$ is convex and compact with diameter $D$ , i.e., $\\|\\mathbf x-\\mathbf y\\|\\le D$ for all $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{Z}}$ . ", "page_idx": 2}, {"type": "text", "text": "(ii) $g$ is convex and continuously differentiable on an open set containing $\\mathcal{Z}$ , and its gradient is $L_{g}$ -Lipschitz, i.e., $\\|\\nabla g(\\mathbf{x})-\\nabla g(\\mathbf{y})\\|_{*}\\leq L_{g}\\|\\mathbf{x}-\\mathbf{y}\\|$ for all $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{Z}}$ . ", "page_idx": 3}, {"type": "text", "text": "(iii) $f$ is convex and continuously differentiable and its gradient is Lipschitz with constant $L_{f}$ . ", "page_idx": 3}, {"type": "text", "text": "In this paper, we denote the optimal value and the optimal solution set of the lower-level problem as $g^{*}\\triangleq\\operatorname*{min}_{\\mathbf{z}\\in\\mathcal{Z}}g(\\mathbf{z})$ and $\\begin{array}{r}{\\mathcal{X}_{g}^{*}\\triangleq\\mathrm{argmin}_{\\mathbf{z}\\in\\mathcal{Z}}\\,g(\\mathbf{z})}\\end{array}$ , respectively. By Assumption 2.1, the set $\\mathcal{X}_{g}^{*}$ is nonempty, compact, and convex, but in general, not a singleton since $g$ could have multiple optimal solutions on $\\mathcal{Z}$ , as $g$ is only convex but not strongly convex. Moreover, we use $f^{*}$ to denote the optimal value and $\\mathbf{x}^{*}$ to denote an optimal solution of problem (1). ", "page_idx": 3}, {"type": "text", "text": "In the simple bilevel problem, the suboptimality of a solution $\\hat{\\bf x}$ is measured by $f(\\hat{\\mathbf{x}})-f^{*}$ . Similarly, its infeasibility is indicated by $g(\\hat{\\mathbf{x}})-g^{*}$ . To ensure minimal suboptimality and infeasibility, we formally define an $(\\epsilon_{f},\\epsilon_{g})$ -optimal solution as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1. $((\\epsilon_{f},\\epsilon_{g})$ -optimal solution). $A$ point $\\hat{\\mathbf{x}}\\in\\mathcal{Z}$ is $(\\epsilon_{f},\\epsilon_{g})$ -optimal for problem (1) if $f(\\hat{\\mathbf{x}})-f^{*}\\leq\\epsilon_{f}$ and $g(\\hat{\\mathbf{x}})-g^{*}\\leq\\epsilon_{g}$ . ", "page_idx": 3}, {"type": "text", "text": "This definition is commonly used in bilevel optimization literature [6\u20138, 13]. Due to the unique structure of bilevel optimization, it is not guaranteed that $f(\\hat{\\mathbf{x}})-f^{*}$ will always be positive. To address this, we propose using $|f(\\hat{\\mathbf{x}})-f^{*}|$ as the absolute optimal criterion. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2. $\\bigcup(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution). $A$ point $\\hat{\\mathbf{x}}\\in\\mathcal{Z}$ is $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal for problem (1) $i f|f(\\hat{\\mathbf{x}})-f^{*}|\\leq\\epsilon_{f}$ and $|g(\\hat{\\mathbf{x}})-g^{*}|\\leq\\epsilon_{g}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before presenting our method, we first introduce a conceptual accelerated gradient method for solving the simple bilevel problem in (1). The first step is to recast it as a constrained optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\;f(\\mathbf{x})\\quad\\mathrm{s.t.}\\quad\\mathbf{x}\\in\\mathcal{X}_{g}^{*},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{X}_{g}^{*}\\triangleq\\,\\mathrm{argmin}_{\\mathbf{z}\\in\\mathcal{Z}}\\ g(\\mathbf{z})}\\end{array}$ is the solution set of the lower-level objective. Conceptually, we apply Nesterov\u2019s accelerated gradient method (AGM) to achieve a rate of ${\\mathcal{O}}(1/k^{2})$ on the upper-level objective $f$ . Several variants of AGM have been proposed; see, e.g., [18]. Here, we consider a variant proposed in [19]. It involves three intertwined sequences of iterates $\\{\\mathbf{x}_{k}\\}_{k\\ge0},\\{\\mathbf{y}_{k}\\}_{k\\ge0},\\{\\mathbf{z}_{k}\\}_{k\\ge0}$ and the scalar variables $\\{a_{k}\\}_{k\\ge0}$ and $\\{A_{k}\\}_{k\\ge0}$ . In the first step, we compute the auxiliary iterate $\\mathbf{y}_{k}$ $\\begin{array}{r}{{\\bf y}_{k}=\\frac{A_{k}}{A_{k}+a_{k}}{\\bf x}_{k}+\\frac{a_{k}}{A_{k}+a_{k}}\\overline{{{\\bf z}_{k}}}}\\end{array}$ . Then in the second step, we update $\\mathbf{z}_{k+1}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{k+1}=\\Pi_{\\mathcal{X}_{g}^{\\ast}}(\\mathbf{z}_{k}-a_{k}\\nabla f(\\mathbf{y}_{k})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi_{\\mathcal{X}_{g}^{*}}(\\cdot)$ denotes the Euclidean projection onto the set $\\mathcal{X}_{g}^{*}$ . Finally, in the third step, we compute $\\begin{array}{r}{{\\bf x}_{k+1}\\,=\\,\\frac{A_{k}}{A_{k}+a_{k}}{\\bf x}_{k}\\,+\\,\\frac{a_{k}}{A_{k}+a_{k}}{\\bf z}_{k+1}}\\end{array}$ and $A_{k+1}\\,=\\,A_{k}\\,+\\,a_{k}$ . It can be shown that if the stepsize is selected as ak = k4L+f1 , then the suboptimality gap $f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})$ of the iterates generated by the method above converges to zero at the optimal rate of ${\\mathcal{O}}(1/k^{2})$ . In this case, indeed all the iterates are feasible as it is possible to project onto the set $\\X_{g}^{*}$ . However, the conceptual method above is not directly implementable for the simple bilevel problem considered in this paper, as the constraint set $\\mathcal{X}_{g}^{*}$ is not explicitly given. As a result projection onto the set $\\mathcal{X}_{g}^{*}$ is not computationally tractable. ", "page_idx": 3}, {"type": "text", "text": "To address this issue, we replace the implicit set $\\scriptstyle{\\mathcal{X}}_{a}^{*}$ in (3) with $\\scriptstyle{\\mathcal{X}}_{k}$ , which can be explicitly characterized, making the Euclidean projection onto $\\vec{\\mathcal{X}}_{k}$ feasible. Additionally, $\\scriptstyle{\\mathcal{X}}_{k}$ must encompass the optimal solution set $\\mathcal{X}_{g}^{*}$ . Inspired by the cutting plane approach in [6], we define $\\scriptstyle{\\mathcal{X}}_{k}$ as the intersection of $\\mathcal{Z}$ and a halfspace: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{X}_{k}\\triangleq\\{\\mathbf{z}\\in\\mathcal{Z}:g(\\mathbf{y}_{k})+\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{z}-\\mathbf{y}_{k}\\rangle\\leq g_{k}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, the auxiliary sequence $\\{g_{k}\\}_{k\\ge0}$ should be selected such that $g_{k}\\ \\geq\\ g^{*}$ and $g_{k}\\,\\rightarrow\\,g^{*}$ . One straightforward way to generate this sequence is by applying an accelerated projected gradient method to the lower-level objective $g$ separately. The loss function of the iterates generated by this algorithm can be considered as $\\{g_{k}\\}$ for the above halfspace. Note that in this case, it is known that ", "page_idx": 3}, {"type": "equation", "text": "$$\n0\\leq g_{k}-g^{*}\\leq\\frac{2L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{(k+1)^{2}},\\quad\\forall k\\geq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Accelerated Gradient Method for Bilevel Optimization (AGM-BiO) ", "page_idx": 4}, {"type": "text", "text": "1: Input: A sequence $\\{g_{k}\\}_{k=0}^{K}$ , a scalar $\\gamma\\in(0,1]$   \n2: Initialization: $A_{0}=0$ , $\\mathbf{x}_{0}=\\mathbf{z}_{0}\\in\\mathcal{Z}$   \n3: for $k=0,\\ldots,K$ do   \n4: Set $\\begin{array}{r}{a_{k}=\\gamma\\frac{k+1}{4L_{f}}}\\end{array}$   \n5: Compute $\\begin{array}{r}{\\mathbf{y}_{k}=\\frac{A_{k}}{A_{k}+a_{k}}\\mathbf{x}_{k}+\\frac{a_{k}}{A_{k}+a_{k}}\\mathbf{z}_{k}}\\end{array}$   \n6: Compute $\\begin{array}{r}{\\mathbf{z}_{k+1}=\\Pi_{\\mathcal{X}_{k}}(\\mathbf{z}_{k}-a_{k}\\nabla f(\\mathbf{y}_{k}))}\\end{array}$ , where   \n7: Compute $\\begin{array}{r l}&{\\qquad\\quad\\mathcal{X}_{k}\\triangleq\\{\\mathbf{z}\\in\\mathcal{Z}:g(\\mathbf{y}_{k})+\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{z}-\\mathbf{y}_{k}\\rangle\\leq g_{k}\\}}\\\\ &{\\mathbf{x}_{k+1}=\\frac{A_{k}}{A_{k}+a_{k}}\\mathbf{x}_{k}+\\frac{a_{k}}{A_{k}+a_{k}}\\mathbf{z}_{k+1}}\\\\ &{\\mathbf{\\phi}_{k+1}=A_{k}+a_{k}}\\end{array}$   \n8: Update   \n9: end for   \n10: Return: $\\mathbf{x}_{K}$ ", "page_idx": 4}, {"type": "text", "text": "Hence, the above requirements on the sequence $\\{g_{k}\\}$ are satisfied. Two remarks on the set $\\scriptstyle{\\mathcal{X}}_{k}$ are in order. First, the set $\\scriptstyle{\\mathcal{X}}_{k}$ in (4) has an explicit form, making the Euclidean projection onto $\\scriptstyle{\\mathcal{X}}_{k}$ tractable. It can also be verified that $\\scriptstyle{\\mathcal{X}}_{k}$ always contains the lower-level problem solution set $\\mathcal{X}_{g}^{*}$ . To prove this, let $\\hat{\\mathbf{x}}^{*}$ be any point in $\\mathcal{X}_{g}^{*}$ . By using the convexity of $g$ , we obtain $\\begin{array}{r}{g(\\mathbf{y}_{k})+\\langle\\nabla g({\\grave{\\mathbf{y}_{k}}}),{\\hat{\\mathbf{x}}}^{*}-\\mathbf{y}_{k}\\rangle\\leq}\\end{array}$ $g(\\hat{\\mathbf{x}}^{*})=g^{*}\\leq g_{k}$ . Thus, $\\hat{\\mathbf{x}}^{*}$ satisfies both constraints in (4), so $\\hat{\\mathbf{x}}^{*}\\in\\mathcal{X}_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "Now that we have identified an appropriate replacement for the set $\\chi_{g}^{*}$ , we can easily implement a variant of the projected accelerated gradient method for the bilevel problem using the surrogate set $\\scriptstyle{\\mathcal{X}}_{k}$ . We refer to our method as the Accelerated Gradient Method for Bilevel Optimization (AGM-BiO) and its steps are outlined in Algorithm 1. It is important to note that the iterates, when projected onto the set $\\scriptstyle{\\mathcal{X}}_{k}$ , may not belong to the set $\\mathcal{X}_{g}^{*}$ , as $\\scriptstyle{\\mathcal{X}}_{k}$ is an approximation of the true solution set. Consequently, the iterates might be infeasible. However, the design of $\\scriptstyle{\\mathcal{X}}_{k}$ allows us to control the infeasibility of the iterates, as we will demonstrate in the convergence analysis section. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. The design of the halfspace as specified in (4) should be recognized as a nuanced task. Various alternative formulations of halfspaces could fulfill the same primary conditions, such as $\\left\\{\\mathbf{z}\\in\\mathcal{Z}:g(\\mathbf{x}_{k})\\!+\\!\\left\\langle\\nabla g(\\mathbf{x}_{k}),\\mathbf{z}-\\mathbf{x}_{k}\\right\\rangle\\leq g_{k}\\right\\}$ and $\\{\\mathbf{z}\\in\\mathcal{Z}:g(\\mathbf{z}_{k})\\!+\\!\\langle\\nabla g(\\bar{\\mathbf{z}}_{k}),\\mathbf{z}\\bar{-}\\mathbf{z}_{k}\\rangle\\leq g_{k}\\}$ . However, the selection of the gradient at $\\mathbf{y}_{k}$ for constructing the halfspace is not arbitrary but essential as we characterize in the convergence analysis of our method. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. How to project onto the set $\\chi_{k}?$ In some cases, such as our over-parameterized regression problem, $\\scriptstyle{\\mathcal{X}}_{k}$ is the intersection of an $L_{2}$ ball and a half-space, for which a closed-form solution exists to find the projected iterates. In other cases, such as our linear inverse problem, we may not be able to find $\\scriptstyle{\\mathcal{X}}_{k}$ directly. Instead, we can solve the projection subproblem using Dykstra\u2019s projection algorithm [20]. In this case, an additional loop is needed to solve the subproblem. ", "page_idx": 4}, {"type": "text", "text": "3.1 Algorithm for the Composite Setting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While our paper focuses on the smooth setting, our proposed method can be also extended to the composite setting. Let us consider the composite counterpart of Problem (1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\ f(\\mathbf{x}):=f_{1}(\\mathbf{x})+f_{2}(\\mathbf{x})\\qquad\\mathrm{s.t.}\\quad\\mathbf{x}\\in\\underset{\\mathbf{z}\\in\\mathbb{R}^{n}}{\\mathrm{argmin}}\\ g(\\mathbf{z}):=g_{1}(\\mathbf{z})+g_{2}(\\mathbf{z}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{1},g_{1}:\\mathbb{R}^{n}\\to\\mathbb{R}$ are smooth convex functions and $f_{2},g_{2}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ are nonsmooth convex functions, respectively. To analyze and implement the proximal gradient-based methods, we need the following definition concerning the property of the proximal mapping. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. Given $h:\\mathbb{R}^{n}\\to(-\\infty,+\\infty]$ and $\\eta>0$ , the proximal map of $h$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nP r o x_{\\eta h}(\\mathbf{x})\\triangleq\\underset{\\mathbf{u}\\in\\mathbb{R}^{n}}{\\mathrm{argmin}}\\{\\frac{1}{2\\eta}\\|\\mathbf{u}-\\mathbf{x}\\|^{2}+h(\\mathbf{u})\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To handle the upper-level nonsmooth part $f_{2}$ , we change the projection step in Step 6 of Algorithm 1 to a proximal update, which is similar to the accelerated proximal gradient method for single-level problems in [17]. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Proximal Accelerated Gradient Method for Bilevel Optimization (P-AGM-BiO) ", "page_idx": 5}, {"type": "text", "text": "1: Input: A sequence $\\{g_{k}\\}_{k=0}^{K}$ , a scalar $\\gamma\\in(0,1]$   \n2: Initialization: $A_{0}=0$ , $\\mathbf{x}_{0}=\\mathbf{z}_{0}\\in\\mathbb{R}^{n}$   \n3: for $k=0,\\ldots,K$ do   \n4: Set $a_{k}=\\gamma\\frac{k+1}{4L_{f}}$   \n5: Compute $\\mathbf{y}_{k}={\\frac{A_{k}}{A_{k}+a_{k}}}\\mathbf{x}_{k}+{\\frac{a_{k}}{A_{k}+a_{k}}}\\mathbf{z}_{k}$   \n6: Compute $\\begin{array}{r}{\\mathbf{z}_{k+1}=P r o x_{a_{k}(f_{2}+\\delta_{\\mathcal{X}_{k}})}(\\mathbf{z}_{k}-a_{k}\\nabla f_{1}(\\mathbf{y}_{k}))}\\end{array}$ , where   \n7: Compute xk+1 = $\\begin{array}{l}{{\\displaystyle{\\mathcal{X}}_{k}\\triangleq\\{{\\mathbf{z}}\\in{\\mathbb{R}}^{n}:g_{1}({\\mathbf{y}}_{k})+\\langle\\nabla g_{1}({\\mathbf{y}}_{k}),{\\mathbf{z}}-{\\mathbf{y}}_{k}\\rangle+g_{2}({\\mathbf{z}})\\leq g_{k}\\}}}\\\\ {{\\displaystyle{\\mathbf{x}}_{k+1}=\\frac{A_{k}}{A_{k}+a_{k}}{\\mathbf{x}}_{k}+\\frac{a_{k}}{A_{k}+a_{k}}{\\mathbf{z}}_{k+1}}}\\\\ {{\\displaystyle1_{k+1}=A_{k}+a_{k}}}\\end{array}$   \n8: Update   \n9: end for   \n10: Return: $\\mathbf{x}_{K}$ ", "page_idx": 5}, {"type": "text", "text": "On the other hand, to deal with the lower-level nonsmooth part $g_{2}$ , it is necessary to modify the approximated lower-level solution set $\\scriptstyle{\\mathcal{X}}_{k}$ . Specifically, we keep the linear approximation of the smooth part of the lower-level objective function $g_{1}$ while adding the nonsmooth part $g_{2}$ as a lower bound of $g_{k}$ to construct $\\scriptstyle{\\mathcal{X}}_{k}$ . Note that the constructed set $\\scriptstyle{\\mathcal{X}}_{k}$ is no longer a halfspace in this setting due to the possibly non-linear nature of $g_{2}$ . We refer to our method as the Proximal Accelerated Gradient Method for Bilevel Optimization (P-AGM-BiO) and its steps are outlined in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Different proximal-friendly assumptions are commonly used in the literature of composite singlelevel/bilevel optimization [8, 15\u201317]. The following proximal-friendly assumption is necessary for our method in the composite setting. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1. The function $f_{2}+\\delta_{\\mathcal{X}_{k}}$ in the Step 6 of Algorithm 2 is proximal-friendly, i.e. the proximal mapping in Definition 3.1 is easy to compute, where $\\delta_{\\mathcal{X}_{k}}(\\cdot)$ is the indicator function. ", "page_idx": 5}, {"type": "text", "text": "This assumption implies that $f_{2}$ is proximal-friendly and that projecting onto the constructed set $\\scriptstyle{\\mathcal{X}}_{k}$ can be done efficiently. Moreover, the function $f_{2}+\\delta_{\\mathcal{X}_{k}}$ is the sum of two convex functions, and the study of proximal mappings for such sums is well-documented in the literature [21\u201324]. Under this assumption, all analysis for the smooth case can be extended to the composite setting. The details are provided in Section B of the Appendix. ", "page_idx": 5}, {"type": "text", "text": "4 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the convergence rate and iteration complexity of our proposed AGM-BiO method for convex simple bilevel optimization problems. We choose the stepsize $\\begin{array}{r}{\\bar{a}_{k}=\\frac{k+1}{4L_{f}}}\\end{array}$ , which is inspired from our theoretical analysis. The main theorem is as follows, ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Suppose Assumption 2.1 holds. Let $\\{{\\bf x}_{k}\\}_{k\\ge0}$ be the sequence of iterates generated by Algorithm 1 with stepsize $\\textstyle a_{k}={\\frac{k+1}{4L_{f}}}$ for $k\\geq0$ and suppose the sequence $g_{k}$ used for generating the cutting plane satisfies (5). Then, for any $k\\geq0$ we have, ", "page_idx": 5}, {"type": "text", "text": "(i) The function suboptimality is bounded above by $\\begin{array}{r}{f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\le\\frac{4L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}.}\\end{array}$ (ii) The infeasibility term is bounded above by g(xk) \u2212g(x\u2217) \u22644Lg\u2225x0k\u2212(xk\u2217+\u222512 )ln(k+1)+ 2Lkg+D12 . (iii) Furthermore, if the condition $f(\\mathbf{x}_{k})\\geq f(\\mathbf{x}^{*})$ holds, then the infeasibility term is bounded above by g(xk) \u2212g(x\u2217) \u22648Lg\u2225x0\u2212x\u2217\u22252 ln(k+1). ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 shows the upper-level objective function gap is upper bounded by ${\\mathcal{O}}(1/k^{2})$ , which matches the convergence rate of the accelerated gradient method for single-level optimization problems. On the other hand, the suboptimality of the lower-level objective which measures infeasibility for the bilevel problem in the worst case is bounded above by ${\\mathcal{O}}(1/k)$ . In the case where $f(\\mathbf{x}_{k})\\geq f^{*}$ , this upper bound improves to ${\\mathcal{O}}(1/k^{2})$ . As a corollary of the worst-case bounds, Algorithm 1 will return an $(\\epsilon_{f},\\epsilon_{g})$ -optimal solution after at most the following number of iterations $\\begin{array}{r}{\\mathcal{O}(\\operatorname*{max}\\{\\frac{1}{\\sqrt{\\epsilon_{f}}},\\frac{1}{\\epsilon_{g}}\\})}\\end{array}$ . We should emphasize that, under the assumptions being considered, this complexity bound represents the best-known bound among all previous works summarized in Table 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Remark 4.1 (The necessity of compactness of $\\mathcal{Z}$ ). For the lower-level objective, we show that $\\begin{array}{r}{A_{k}(g({\\mathbf x}_{k})-g({\\mathbf x}^{*}))\\le\\sum_{i=0}^{{k-1}}a_{i}(g_{i}-g^{*})+\\frac{L_{g}}{4L_{f}}\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}}\\end{array}$ ((24) in Section A). The main cWhiatlhloeuntg ae  lionw oerb tbaionuinndg  oann a, ctcheisl etreartme dc arantneo to fb ${\\mathcal{O}}(1/k^{2})$ d  fboyr $g$ ei su pcpoenrt-rloelvlienl gs $\\begin{array}{r}{\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}}\\end{array}$ $f$   \n$f(\\mathbf{x}_{k})\\geq f(\\mathbf{x}^{*})$ , we can achieve the rate of ${\\mathcal{O}}(1/k^{2})$ for $g$ . Otherwise, we use the compactness of $\\mathcal{Z}$ to achieve $\\mathcal{O}(1/k)$ for $g$ . Please refer to Section A for more details. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.2 (Removable log terms). The log terms in all the complexity results can be removed by choosing the auxiliary sequence $g_{k}=g_{K}$ for all $0\\le k\\le K$ , which satisfies the condition (5). This eliminates the log term in (24) and all subsequent results. However, this choice of $\\{g_{k}\\}_{k\\ge0}$ requires predetermining the total number of iterations $K$ . ", "page_idx": 6}, {"type": "text", "text": "Since the algorithm\u2019s output $\\hat{\\bf x}$ may fall outside the feasible set $\\mathcal{X}_{g}^{*}$ , the expression $f(\\hat{\\mathbf{x}})-f^{*}$ may not necessarily be non-negative. On the other hand, under the considered assumptions, proving convergence in terms of $|f(\\hat{\\mathbf{x}})-f^{*}|$ is known to be impossible due to a negative result presented by [25]. Specifically, for any first-order method and a given number of iterations $k$ , they demonstrated the existence of an instance of Problem (1) where $|\\bar{f}(\\mathbf{x}_{k})-f^{*}|\\geq1$ for all $k\\geq0$ . Thus, to provide any form of guarantee in terms of the absolute value of the suboptimality, i.e., $|f(\\hat{\\mathbf{x}})-f^{*}|$ , we need an additional assumption to obtain a lower bound on suboptimality and to provide a convergence bound for $|f(\\hat{\\mathbf{x}})-\\bar{f}^{*}|$ . We will address this point in the following section. ", "page_idx": 6}, {"type": "text", "text": "4.1 Convergence under H\u00f6lderian Error Bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we introduce an additional regularity condition on $g$ to establish a lower bound for $f(\\hat{\\mathbf{x}})-f^{*}$ . Specifically, we assume that the lower-level objective function $g$ satisfies the H\u00f6lderian Error Bound condition, which governs how $g\\mathbf{(x)}$ grows as $\\mathbf{x}$ moves away from the optimal solution set $\\X_{g}^{*}$ . Intuitively, since our method\u2019s output $\\hat{\\bf x}$ is $\\epsilon_{g}$ -optimal for the lower-level problem, it should be close to $\\mathcal{X}_{g}^{*}$ under this regularity condition. We can then use this proximity and the smoothness property of $f$ to establish a lower bound for $f(\\hat{\\mathbf{x}})-f^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. The function $g$ satisfies the H\u00f6lderian error bound for some $\\alpha>0$ and $r\\geq1$ , i.e, ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\frac{\\alpha}{r}}\\operatorname{dist}\\left(\\mathbf{x},{\\boldsymbol{\\mathcal{X}}}_{g}^{*}\\right)^{r}\\leq g(\\mathbf{x})-g^{*},\\quad\\forall\\mathbf{x}\\in{\\boldsymbol{\\mathcal{Z}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where dist $\\left(\\mathbf{x},\\mathcal{X}_{g}^{*}\\right)\\triangleq\\operatorname*{inf}_{\\mathbf{x}^{\\prime}\\in\\mathcal{X}_{g}^{*}}\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|.$ . ", "page_idx": 6}, {"type": "text", "text": "We note that the H\u00f6lderian error bound condition in (8) is well-studied in the optimization literature [26\u201328] and is known to hold in general when function $g$ is analytic and the set $\\mathcal{Z}$ is bounded [29]. There are two important special cases of the H\u00f6lderian error bound condition: 1) $g$ satisfies (8) with $r\\,=\\,1$ known as the weak sharpness condition [30, 31]; 2) $g$ satisfies (8) with $r\\,=\\,2$ known as the quadratic functional growth condition [32]. By using the H\u00f6lderian error bound condition, [6] established a stronger relation between suboptimality and infeasibility, as shown next. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 ([6, Proposition 1]). Assume that $f$ is convex and $g$ satisfies Assumption 4.1, and define $\\begin{array}{r}{M=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{g}^{*}}\\|\\nabla f(\\mathbf{x})\\|_{*}}\\end{array}$ . Then $\\begin{array}{r}{f(\\hat{\\mathbf{x}})-f^{*}\\ge-M(\\frac{r(g(\\hat{\\mathbf{x}})-\\bar{g}^{*})}{\\alpha})^{\\frac{1}{r}}}\\end{array}$ for any $\\hat{\\mathbf{x}}\\in\\mathcal{Z}$ . ", "page_idx": 6}, {"type": "text", "text": "Hence, under Assumption 4.1, Proposition 4.2 shows that the suboptimality $f(\\hat{\\mathbf{x}})-f^{*}$ can also be bounded from below when $\\hat{\\bf x}$ is an approximate solution of the lower-level problem. As a result, we can establish a convergence bound on $\\left|f(\\mathbf{x}_{k})\\right.-\\left.f^{*}\\right|$ by combining Proposition 4.2 with the upper bounds in Theorem 4.1. Moreover, it also allows us to improve the convergence rate for the lower-level problem. To prove this claim, we first introduce the following lemma which establishes an upper bound on the weighted sum of upper and lower-level objectives. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3. Suppose conditions $(i i)$ and (iii) in Assumption 2.1 hold. Let $\\{{\\bf x}_{k}\\}$ be the sequence of iterates generated by Algorithm 1 with stepsize ak = \u03b3 k4L+f1 , where $0<\\gamma\\leq1$ . If the sequence $g_{k}$ used for generating the cutting plane satisfies (5), then for any \u03bb \u2265(2/\u03b3L\u2212g1)Lf and $k\\geq0$ we have ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}\\ln(k+1)}{k(k+1)}+\\frac{4\\lambda L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma k(k+1)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This result characterizes and upper bound of $\\tilde{\\mathcal{O}}(1/k^{2})$ on the expression $\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-$ $g(\\mathbf{x}^{*})$ . That said, the first term in this expression, a.k.a., $\\lambda(f(\\bar{\\mathbf{x}_{k}})-f(\\mathbf{x}^{*}))$ may not be non-negative for a bilevel problem as discussed earlier. Hence, we cannot simply eliminate $\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))$ to show an upper bound of ${\\mathcal{O}}(1/k^{2})$ on infeasibility, a.k.a., $g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})$ . Instead, we leverage the H\u00f6lderian error bound on $g$ and apply Proposition 4.2 to the first term. As a result, we can eliminate the dependence on $f$ in (9). In this case, we can establish an upper bound on infeasibility. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. Suppose conditions (ii) and (iii) in Assumption 2.1 hold and the lower-level function $g$ satisfies the H\u00f6lderian error bound with $r\\!>\\!1$ . Let $\\{{\\bf{x}}_{k}\\}$ be the iterates generated by Algorithm 1 with stepsize ak = \u03b3 k4L+f1 , where $\\begin{array}{r}{\\gamma=1/(\\frac{2L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+2)}\\end{array}$ and $K$ is the total number of iterations. Moreover, suppose the sequence $g_{k}$ used for generating the cutting plane satisfies (5). If we define the constants $C_{f}\\triangleq8L_{f}\\Vert\\mathbf{x}_{0}-\\mathbf{x}^{*}\\Vert^{2}$ , $C_{g}\\triangleq12L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}$ and $C\\triangleq M({\\frac{r}{\\alpha}})^{\\frac{1}{r}}$ , where $M\\triangleq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{g}^{*}}\\|\\nabla f(\\mathbf{x})\\|$ $\\alpha$ and $r$ are the parameters in Assumption 4.1, then the following results hold: ", "page_idx": 7}, {"type": "text", "text": "(i) The function suboptimality is bounded above by ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})\\leq\\frac{C_{g}(\\ln K+1)}{K^{\\frac{2r}{2r-1}}}+\\frac{C_{f}}{K^{2}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "(ii) The function suboptimality is bounded below by ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})\\geq-C\\operatorname*{max}\\left\\{\\frac{(2C_{g}(\\ln K+1))^{\\frac{1}{r}}}{K^{\\frac{2}{2r-1}}}+\\frac{(2C_{f})^{\\frac{1}{r}}}{K^{\\frac{2}{r}}},\\frac{(2C)^{\\frac{1}{r-1}}}{K^{\\frac{2}{2r-1}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "(iii) The infeasibility term is bounded above by ", "page_idx": 7}, {"type": "equation", "text": "$$\ng(\\mathbf x_{K})-g(\\mathbf x^{*})\\leq\\operatorname*{max}\\left\\{\\frac{2C_{g}(\\ln K+1)}{K^{\\frac{2r}{2r-1}}}+\\frac{2C_{f}}{K^{2}},\\frac{(2C)^{\\frac{r}{r-1}}}{K^{\\frac{2r}{2r-1}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Before unfolding this result, we would like to highlight that unlike the result in Theorem 4.1, the above bounds in Theorem 4.4 do not require the feasible set to be compact. Since $r>1$ , the first result shows $f(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})$ has an upper bound of $\\tilde{\\mathcal{O}}\\big(\\big(\\frac{1}{K}\\big)^{\\frac{2r}{2r-1}}\\big)$ and the second result guarantees a lower bound of $\\textstyle-{\\tilde{O}}\\big(\\big(\\frac{1}{K}\\big)^{\\frac{2}{2r-1}}\\big)$ . These two bounds together lead to an upper bound of $\\tilde{\\mathcal{O}}\\big(\\big(\\frac{1}{K}\\big)^{\\frac{2}{2r-1}}\\big)$ for the absolute error $|\\dot{f}(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})|$ . Moreover, the third result implies that the lower-level problem suboptimality which measures infeasibility is bounded above by $\\tilde{\\mathcal{O}}\\big(\\big(\\frac{1}{K}\\big)^{\\frac{2r}{2r-1}}\\big)$ . ", "page_idx": 7}, {"type": "text", "text": "The previous result presented in Theorem 4.4 is applicable when $r>1$ . However, for the case that 1st-order H\u00f6lderian error bound condition on $g$ holds (i.e., weak sharpness condition), we require a distinct analysis and a different choice of $\\gamma$ to achieve the tightest bounds. In the subsequent theorem, we present our findings for this specific scenario. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.5. Suppose conditions (ii) and (iii) in Assumption 2.1 are met and that the lower-level objective function $g$ satisfies the H\u00f6lderian error bound with $r=1$ . Let $\\{{\\bf x}_{k}\\}$ be the sequence of iterates generated by Algorithm 1 with stepsize ak = \u03b3 k4L+f1 , where $\\begin{array}{r}{0<\\gamma\\le\\operatorname*{min}\\{\\frac{2\\alpha L_{f}}{2M L_{g}+\\alpha L_{f}},1\\}}\\end{array}$ Moreover, suppose the sequence $g_{k}$ used for generating the cutting plane satisfies (5), and recall $M\\triangleq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{g}^{*}}\\|\\nabla f(\\mathbf{x})\\|$ and $\\alpha$ in Assumption 4.1. If we define the constants $C_{f}\\triangleq4L_{f}\\Vert\\mathbf{x}_{0}-\\mathbf{x}^{*}\\Vert^{2}$ and $C_{g}\\triangleq8L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}$ , then for any $k\\geq0$ : ", "page_idx": 7}, {"type": "text", "text": "(i) The function suboptimality is bounded above by $\\begin{array}{r}{f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\le\\frac{C_{f}}{\\gamma k(k+1)}}\\end{array}$ . (ii) The function suboptimality is bounded below by $\\begin{array}{r}{f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\ge-\\frac{\\dot{C}_{g}M\\left(\\ln k+1\\right)}{\\alpha k(k+1)}-\\frac{C_{f}}{\\gamma k(k+1)}.}\\end{array}$ (iii) The infeasibility term is bounded above by $\\begin{array}{r}{g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{C_{g}(\\ln k+1)}{k(k+1)}+\\frac{\\'\\alpha C_{f}}{\\gamma M k(k+1)}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.5 shows that under the H\u00f6lderian error bound with $r=1$ , also known as weak sharpness condition, the absolute value of the function suboptimality $|f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})|$ approaches zero at a rate of ${\\mathcal{O}}(1/k^{2})$ \u2013 ignoring the log term. The lower-level error $g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})$ , capturing the infeasibility of the iterates, also approaches zero at a rate of ${\\mathcal{O}}(1/k^{2})$ . As a corollary, Algorithm 1 returns an $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solution after $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\frac{1}{\\sqrt{\\epsilon_{f}}},\\frac{1}{\\sqrt{\\epsilon_{g}}}\\})$ iterations. ", "page_idx": 7}, {"type": "image", "img_path": "aFOdln7jBV/tmp/5691d3978ed75e5a055eabe173940868d4351f1b82a3ee5efa354280f91865b5.jpg", "img_caption": ["Figure 1: Comparison of a-IRG, CG-BiO, Bi-SG, SEA, R-APM, PB-APG, and AGM-BiO for solving the over-parameterized regression problem. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate our AGM-BiO method on two different bilevel problems using real and synthetic datasets. We compare its runtime and iteration count with other methods, including a-IRG [12], CG-BiO [6], Bi-SG [13], SEA [14], R-APM [8], PB-APG [16], and Bisec-BiO [15]. ", "page_idx": 8}, {"type": "text", "text": "Over-parameterized regression. We examine problem (1) where the lower-level problem corresponds to training loss, and the upper-level pertains to validation loss. The objective is to minimize the validation loss by selecting an optimal training loss solution. This method is also referred to as lexicographic optimization [33]. A common example of that is the constrained regression problem, where we aim to find an optimal parameter vector $\\boldsymbol{\\beta}\\in\\mathbb{R}^{d}$ for the validation loss that minimizes the loss $\\ell_{\\mathrm{tr}}(\\beta)$ over the training dataset $\\mathcal{D}_{\\mathrm{tr}}$ . To represent some prior knowledge, we constrain $\\beta$ to be in some subset $\\mathcal{Z}\\subseteq\\mathbb{R}^{d}$ , e.g., ${\\mathcal{Z}}=\\{\\beta\\mid\\beta_{1}\\leq\\cdots\\leq\\beta_{d}\\}$ in isotonic regression and $\\mathcal{Z}=\\{\\beta\\mid\\|\\beta\\|_{p}\\leq\\lambda\\}$ in $L_{p}$ constrained regression. Without explicit regularization, an over-parameterized regression over the training dataset has multiple global minima, but not all these optimal regression coefficients perform equally on validation or testing datasets. Thus, the upper-level objective serves as a secondary criterion to ensure a smaller error on the validation dataset $\\mathcal{D}_{\\mathrm{val}}$ . The problem can be cast as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta\\in\\mathbb{R}^{d}}f(\\beta)\\triangleq\\ell_{\\mathrm{val}}(\\beta)\\quad\\mathrm{~s.t.~}\\quad\\beta\\in\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\mathrm{argmin}}\\;g(\\mathbf{z})\\triangleq\\ell_{\\mathrm{tr}}(\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In this case, both upper-level and lower-level objectives are convex and smooth if the loss $\\ell$ is smooth and convex. Since projections onto the sublevel set of $f$ are difficult to compute, Bisec-BiO is excluded from this experiment. ", "page_idx": 8}, {"type": "text", "text": "We apply the Wikipedia Math Essential dataset [34] which is composed of a data matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ with $n=1068$ samples and $d=730$ features and an output vector $\\mathbf{b}\\in\\mathbb{R}^{n}$ . We use $75\\%$ of the dataset as the training set $(\\mathbf{A}_{\\mathrm{tr}},\\mathbf{b}_{\\mathrm{tr}})$ and $25\\%$ as the validation set $(\\mathbf{A}_{\\mathrm{val}},\\mathbf{b}_{\\mathrm{val}})$ . For both upperand lower-level loss functions, we use the least squared loss. Then the lower-level objective is $\\begin{array}{r}{g(\\beta)=\\frac12\\|{\\bf A}_{t r}\\beta-{\\bf b}_{t r}\\|_{2}^{2}}\\end{array}$ , the upper-level objective is $\\begin{array}{r}{f(\\beta)=\\frac{1}{2}||{\\bf A}_{v a l}\\beta-{\\bf b}_{v a l}||_{2}^{2}}\\end{array}$ , and the constraint set is chosen as the unit $L_{2}$ -ball $\\mathcal{Z}=\\{\\beta\\,\\vert\\,\\,\\vert\\vert\\beta\\vert\\vert_{2}\\leq1\\}$ . Note that this regression problem is overparameterized since the number of features $d$ is larger than the number of data points in both the training set and validation set. ", "page_idx": 8}, {"type": "text", "text": "In Figures 1(a) and 1(c), we observe that the three accelerated gradient-based methods (R-APM, PB-APG, and AGM-BiO) converge faster in reducing infeasibility, both in terms of runtime and number of iterations. In terms of absolute suboptimality, shown in Figures 1(b) and 1(d), AGM-BiO achieves the smallest absolute suboptimality gap among all algorithms. Unlike the infeasibility plots, R-APM and PB-APG underperform compared to AGM-BiO. Note that the lower-level objective in this problem does not satisfy the weak sharpness condition, so the regularization parameter $\\eta$ in R-APM is set as $1/(K+1)$ . Consequently, the suboptimality for R-APM converges slower than AGM-BiO, as suggested by the theoretical results in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Linear inverse problems. In the next experiment, we concentrate on a problem that fulfills the H\u00f6lderian Error Bound condition for some $r\\,>\\,1$ . We aim to evaluate the performance of our method in this specific context and verify the validity of our theoretical results for this scenario. Specifically, we focus on the so-called linear inverse problems, commonly used to evaluate convex bilevel optimization algorithms, which originate from [35]. The goal of linear inverse problems is to obtain a solution $\\mathbf{x}\\in\\mathbb{R}^{n}$ to the system of linear equation $\\mathbf{Ax}=\\mathbf{b}$ . Note that if $\\mathbf{A}$ is rank-deficient, there can be multiple solutions, or there might be no exact solution due to noise. To address this issue, we chase a solution that has the smallest weighted norm with respect to some positive definite matrix $\\mathbf{Q}$ , i.e., $\\|\\mathbf{x}\\|_{\\mathbf{Q}}:=\\sqrt{\\mathbf{x}^{\\top}\\mathbf{Qx}}$ . This problem can be also cast as the following simple bilevel problem: ", "page_idx": 8}, {"type": "image", "img_path": "aFOdln7jBV/tmp/7aed1bc45a82ad0f7d8a2a1bf0af6f980f84e3c73556c8d06e9f6405d8202717.jpg", "img_caption": ["Figure 2: Comparison of a-IRG, Bi-SG, SEA, R-APM, PB-APG, Bisec-BiO, and AGM-BiO for solving the linear inverse problem. ", ""], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{n}}f(\\mathbf{x})\\triangleq\\frac{1}{2}\\|\\mathbf{x}\\|_{\\mathbf{Q}}^{2}\\quad\\mathrm{~s.t.~}\\quad\\mathbf{x}\\in\\underset{\\mathbf{z}\\in\\mathcal{Z}}{\\mathrm{argmin}}g(\\mathbf{z})\\triangleq\\frac{1}{2}\\|\\mathbf{A}\\mathbf{z}-\\mathbf{b}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "For this class of problem, if $\\mathbf{Q}$ , A, and $\\mathbf{b}$ are generated randomly or by the \u201cregularization tools\u201d like [14, 35], we are not able to obtain the exact optimal value $f^{*}$ . To the best of our knowledge, no existing solver could obtain the exact optimal value $f^{*}$ for this bilevel problem. Specifically, the existing solvers either fail to solve this bilevel problem or return an inaccurate solution by solving a relaxed version of the problem. Hence, in [14, 35] they only reported the upper-level function value. However, in this paper, we intend to obtain the complexity bounds for finding $(\\epsilon_{f},\\epsilon_{g})$ -optimal and $(\\epsilon_{f},\\epsilon_{g})$ -absolute optimal solutions. Without knowing $f^{*}$ , we can not characterize the behavior of $|\\bar{f}(\\bar{\\mathbf{x}_{k}})-f^{*}|$ . Therefore, we choose an example where we can obtain the exact solution. Specifically, we set $\\mathbf{Q}=\\mathbf{I}_{n}$ , $\\mathbf{A}=\\mathbf{1}_{n}^{\\top}$ , $\\mathbf{b}=1$ , and the constraint set $\\mathcal{Z}=\\mathbb{R}_{+}^{n}$ . In this case, the optimal solution $\\begin{array}{r}{{\\bf x}^{*}=\\,\\frac{1}{n}{\\bf1}_{n}}\\end{array}$ and optimal value $\\textstyle f^{*}\\,=\\,{\\frac{1}{2n}}$ . This specific example essentially involves seeking the minimum norm for an under-determined system. Note that the lower-level objective in this problem satisfies the H\u00f6lderian Error Bound condition with order $r\\,=\\,2$ [36]. Hence, we do not need the constraint set $\\mathcal{Z}$ to be compact as shown in Theorem 4.4. Due to the unbounded nature of the constraint set, Frank-Wolfe-type methods are not viable options. Consequently, we have opted not to incorporate CG-BiO in this experiment. ", "page_idx": 9}, {"type": "text", "text": "We explored examples with two distinct dimensions: $n=3$ and $n=100$ , evaluating a total of 2000 gradients. In Figures 2(a) and 2(c), AGM-BiO shows superior performance in terms of infeasibility. In Figures 2(b) and 2(d), we compare methods in terms of absolute error of suboptimality. The gap between R-APM and AGM-BiO is smaller for $n\\,=\\,3$ , but for $n=100$ , AGM-BiO significantly outperforms all other methods, including R-APM. Since the regularization and penalty parameters in R-APM and PB-APG are fixed, they might get stuck at a certain accuracy level, as seen in Figures 2(a) and 2(c). In contrast, AGM-BiO uses a dynamic framework for minimizing the upper and lower-level functions, consistently reducing both suboptimality and infeasibility. Although Bisec-BiO theoretically has the best complexity results due to the ease of projecting onto the sublevel set of $f$ , its performance in the last iteration is inconsistent, as shown in Figure 2. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced an accelerated gradient-based algorithm for solving a specific class of bilevel optimization problems with convex objective functions in both the upper and lower levels. Our proposed algorithm achieves a computational complexity of $\\mathcal{O}(\\operatorname*{max}\\{\\epsilon_{f}^{\\bar{-}\\bar{0}.5},\\epsilon_{g}^{-1}\\})$ . When an additional weak sharpness condition is applied to the lower-level function $g$ , the iteration complexity improves to $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-0.5},\\epsilon_{g}^{-0.5}\\})$ , matching the well-known fastest convergence rate for singlelevel convex optimization problems. We further extended this result to an iteration complexity of $\\tilde{\\mathcal{O}}(\\operatorname*{max}\\{\\epsilon_{f}^{-\\frac{2r-\\bar{1}}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$ when the lower-level loss satisfies the H\u00f6lderian error bound assumption. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research of J. Cao, R. Jiang, and A. Mokhtari is supported in part by NSF Grant 2127697 and the NSF AI Institute for Foundations of Machine Learning (IFML) at UT Austin. The research of E. Yazdandoost Hamedani is supported by NSF Grant 2127696. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. Advances in Neural Information Processing Systems, 33:14879\u2013 14890, 2020.   \n[2] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pages 1568\u20131577. PMLR, 2018.   \n[3] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723\u20131732. PMLR, 2019.   \n[4] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \n[5] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.   \n[6] Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani. A conditional gradient-based method for simple bilevel optimization with convex lower-level problem. In International Conference on Artificial Intelligence and Statistics, pages 10305\u2013 10323. PMLR, 2023.   \n[7] Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, and Aryan Mokhtari. Projection-free methods for stochastic simple bilevel optimization with convex lower-level problem. Advances in Neural Information Processing Systems, 36, 2024.   \n[8] Sepideh Samadi, Daniel Burbano, and Farzad Yousefian. Achieving optimal complexity guarantees for a class of bilevel convex optimization problems. arXiv preprint arXiv:2310.12247, 2023.   \n[9] Stephen Dempe, Nguyen Dinh, and Joydeep Dutta. Optimality conditions for a simple convex bilevel programming problem. Variational Analysis and Generalized Differentiation in Optimization and Control: In Honor of Boris S. Mordukhovich, pages 149\u2013161, 2010.   \n[10] Joydeep Dutta and Tanushree Pandit. Algorithms for simple bilevel programming. Bilevel Optimization: Advances and Next Challenges, pages 253\u2013291, 2020.   \n[11] Yekini Shehu, Phan Tu Vuong, and Alain Zemkoho. An inertial extrapolation method for convex simple bilevel optimization. Optimization Methods and Software, 36(1):1\u201319, 2021.   \n[12] Harshal D Kaushik and Farzad Yousefian. A method with convergence rates for optimization problems with variational inequality constraints. SIAM Journal on Optimization, 31(3):2171\u2013 2198, 2021.   \n[13] Roey Merchav and Shoham Sabach. Convex bi-level optimization problems with non-smooth outer objective function. arXiv preprint arXiv:2307.08245, 2023.   \n[14] Lingqing Shen, Nam Ho-Nguyen, and Fatma K\u0131l\u0131n\u00e7-Karzan. An online convex optimizationbased framework for convex bilevel optimization. Mathematical Programming, 198(2):1519\u2013 1582, 2023.   \n[15] Jiulin Wang, Xu Shi, and Rujun Jiang. Near-optimal convex simple bilevel optimization with a bisection method. arXiv preprint arXiv:2402.05415, 2024.   \n[16] Pengyu Chen, Xu Shi, Rujun Jiang, and Jiulin Wang. Penalty-based methods for simple bilevel optimization under $\\mathrm{h}\\backslash\"\\{\\mathrm{o}\\}$ lderian error bounds. arXiv preprint arXiv:2402.02155, 2024.   \n[17] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.   \n[18] Alexandre d\u2019Aspremont, Damien Scieur, Adrien Taylor, et al. Acceleration methods. Foundations and Trends\u00ae in Optimization, 5(1-2):1\u2013245, 2021.   \n[19] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to SIAM J. Optim., 2008.   \n[20] Heinz H Bauschke, Regina S Burachik, Patrick L Combettes, Veit Elser, D Russell Luke, and Henry Wolkowicz. Fixed-point algorithms for inverse problems in science and engineering, volume 49. Springer Science & Business Media, 2011.   \n[21] Yao-Liang Yu. On decomposing the proximal map. Advances in neural information processing systems, 26, 2013.   \n[22] Nelly Pustelnik and Laurent Condat. Proximity operator of a sum of functions; application to depth map estimation. IEEE Signal Processing Letters, 24(12):1827\u20131831, 2017.   \n[23] Heinz H Bauschke, Minh N Bui, and Xianfu Wang. Projecting onto the intersection of a cone and a sphere. SIAM Journal on Optimization, 28(3):2158\u20132188, 2018.   \n[24] Samir Adly, Lo\u00efc Bourdin, and Fabien Caubet. On a decomposition formula for the proximal operator of the sum of two convex functions. Journal of Convex Analysis, 26(2):699\u2013718, 2019.   \n[25] Lesi Chen, Jing Xu, and Jingzhao Zhang. On bilevel optimization without lower-level strong convexity. arXiv preprint arXiv:2301.00712, 2023.   \n[26] Jong-Shi Pang. Error bounds in mathematical programming. Mathematical Programming, 79(1-3):299\u2013332, 1997.   \n[27] J\u00e9r\u00f4me Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W Suter. From error bounds to the complexity of first-order descent methods for convex functions. Mathematical Programming, 165:471\u2013507, 2017.   \n[28] Vincent Roulet and Alexandre d\u2019Aspremont. Sharpness, restart and acceleration. Advances in Neural Information Processing Systems, 30, 2017.   \n[29] Zhi-Quan Luo and Jong-Shi Pang. Error bounds for analytic systems and their applications. Mathematical Programming, 67(1-3):1\u201328, 1994.   \n[30] James V Burke and Michael C Ferris. Weak sharp minima in mathematical programming. SIAM Journal on Control and Optimization, 31(5):1340\u20131359, 1993.   \n[31] James V Burke and Sien Deng. Weak sharp minima revisited, part ii: application to linear regularity and error bounds. Mathematical programming, 104:235\u2013261, 2005.   \n[32] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. Mathematics of Operations Research, 43(3):919\u2013948, 2018.   \n[33] Chengyue Gong and Xingchao Liu. Bi-objective trade-off with dynamic barrier gradient descent. NeurIPS 2021, 2021.   \n[34] Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzm\u00e1n L\u00f3pez, Nicolas Collignon, et al. Pytorch geometric temporal: Spatiotemporal signal processing with neural machine learning models. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 4564\u20134573, 2021.   \n[35] Shoham Sabach and Shimrit Shtern. A first order method for solving convex bilevel optimization problems. SIAM Journal on Optimization, 27(2):640\u2013660, 2017.   \n[36] Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for non-strongly convex optimization. Mathematical Programming, 175:69\u2013107, 2019.   \n[37] Andre\u0131\u02d8 Nikolaevich Tikhonov and VIAK Arsenin. Solutions of ill-posed problems. (No Title), 1977.   \n[38] Mikhail Solodov. An explicit descent method for bilevel convex optimization. Journal of Convex Analysis, 14(2):227, 2007.   \n[39] Mikhail V Solodov. A bundle method for a class of bilevel nonsmooth convex minimization problems. SIAM Journal on Optimization, 18(1):242\u2013259, 2007.   \n[40] Elias S Helou and Lucas EA Sim\u00f5es. \u03f5-subgradient algorithms for bilevel convex optimization. Inverse Problems, 33(5):055020, 2017.   \n[41] Amir Beck and Shoham Sabach. A first order method for finding minimal norm-like solutions of convex optimization problems. Mathematical Programming, 147(1-2):25\u201346, 2014.   \n[42] Yura Malitsky. Chambolle-pock and tseng\u2019s methods: relationship and extension to the bilevel optimization. arXiv preprint arXiv:1706.02602, page 3, 2017.   \n[43] Boris T Polyak. Introduction to optimization. 1987. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of the Main Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To prove Theorem 4.1, we start with the following general lemma that holds for any choice of the step sizes $\\left\\{a_{k}\\right\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Let $\\{{\\bf x}_{k}\\}$ be the sequence of iterates generated by Algorithm $^{\\,l}$ with stepsize $a_{k}>0$ for $k\\geq0$ . Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}\\!-\\!\\Big(A_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Big)}\\\\ &{\\phantom{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}}\\leq\\left(\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}-\\displaystyle\\frac{1}{2}\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2},}\\\\ &{\\phantom{A_{k+1}(f(\\mathbf{x}_{k+1})-g(\\mathbf{x}^{*}))-A_{k}(g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))\\leq a_{k}(g_{k}-g(\\mathbf{x}^{*}))+\\displaystyle\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.1. Let $\\mathbf{x}^{*}$ be any optimal solution of (1). We first consider the upper-level objective $f$ . Since $f$ is convex, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\mathbf{y}_{k})-f(\\mathbf{x}^{*})\\leq\\left\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}^{*}\\right\\rangle,\\quad f(\\mathbf{y}_{k})-f(\\mathbf{x}_{k})\\leq\\left\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now given the update rule $A_{k+1}=A_{k}+a_{k}$ , we can write ", "page_idx": 13}, {"type": "text", "text": "$A_{k+1}{\\big(}f(\\mathbf{y}_{k})-f(\\mathbf{x}^{*}){\\big)}-A_{k}{\\big(}f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}){\\big)}=a_{k}{\\big(}f(\\mathbf{y}_{k})-f(\\mathbf{x}^{*}){\\big)}+A_{k}{\\big(}f(\\mathbf{y}_{k})-f(\\mathbf{x}_{k}){\\big)}$ (13) Combining (12) and (13), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(f(\\mathbf{y}_{k})-f(\\mathbf{x}^{*}))-A_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))}\\\\ &{\\le a_{k}(\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}^{*}\\rangle)+A_{k}(\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\rangle)}\\\\ &{=\\langle\\nabla f(\\mathbf{y}_{k}),a_{k}\\mathbf{y}_{k}+A_{k}(\\mathbf{y}_{k}-\\mathbf{x}_{k})-a_{k}\\mathbf{x}^{*}\\rangle}\\\\ &{=a_{k}\\left\\langle\\nabla f(\\mathbf{y}_{k}),\\mathbf{z}_{k}-\\mathbf{x}^{*}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equality follows from the definition of $\\mathbf{y}_{k}$ . Furthermore, since $f$ is $L_{f}$ -smooth, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nf({\\mathbf x}_{k+1})\\leq f({\\mathbf y}_{k})+\\langle\\nabla f({\\mathbf y}_{k}),{\\mathbf x}_{k+1}-{\\mathbf y}_{k}\\rangle+\\frac{L_{f}}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf y}_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If we multiply both sides of (15) by $A_{k+1}$ and combine the resulting inequality with (14), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(f({\\bf x}_{k+1})-f({\\bf x}^{*}))-A_{k}(f({\\bf x}_{k})-f({\\bf x}^{*}))}\\\\ &{\\le a_{k}\\left\\langle\\nabla f({\\bf y}_{k}),{\\bf z}_{k}-{\\bf x}^{*}\\right\\rangle+A_{k+1}\\left\\langle\\nabla f({\\bf y}_{k}),{\\bf x}_{k+1}-{\\bf y}_{k}\\right\\rangle+\\displaystyle\\frac{L_{f}A_{k+1}}{2}\\|{\\bf x}_{k+1}-{\\bf y}_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla f({\\bf y}_{k}),{\\bf z}_{k}-{\\bf x}^{*}\\right\\rangle+a_{k}\\left\\langle\\nabla f({\\bf y}_{k}),{\\bf z}_{k+1}-{\\bf z}_{k}\\right\\rangle+\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}\\|{\\bf z}_{k+1}-{\\bf z}_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla f({\\bf y}_{k}),{\\bf z}_{k+1}-{\\bf x}^{*}\\right\\rangle+\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}\\|{\\bf z}_{k+1}-{\\bf z}_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we used the fact that $a_{k}({\\bf z}_{k+1}-{\\bf z}_{k})=A_{k+1}({\\bf x}_{k+1}-{\\bf y}_{k})$ in the first equality. Moreover, since $\\mathbf{x}^{\\ast}\\in\\mathcal{X}_{k}$ , we obtain from the update rule in (3) that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf z_{k+1}-\\mathbf z_{k}+a_{k}\\nabla f(\\mathbf y_{k}),\\mathbf x^{*}-\\mathbf z_{k+1}\\rangle\\ge0}\\\\ {\\Leftrightarrow}&{a_{k}\\left\\langle\\nabla f(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf x^{*}\\right\\rangle\\le\\langle\\mathbf z_{k+1}-\\mathbf z_{k},\\mathbf x^{*}-\\mathbf z_{k+1}\\rangle}\\\\ {\\Leftrightarrow}&{a_{k}\\left\\langle\\nabla f(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf x^{*}\\right\\rangle\\leq\\frac{1}{2}\\|\\mathbf z_{k}-\\mathbf x^{*}\\|^{2}-\\frac{1}{2}\\|\\mathbf z_{k+1}-\\mathbf x^{*}\\|^{2}-\\frac{1}{2}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining (16) and (17) leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*})\\big)+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\Bigg(A_{k}\\big(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\big)+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{2}\\left(\\frac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which proves the claim in (10). ", "page_idx": 14}, {"type": "text", "text": "Next, we proceed to prove the claim in (11). To do so, we first leverage the convexity of $g$ which leads to ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(\\mathbf{y}_{k})-g(\\mathbf{x}_{k})\\leq\\left\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Also, since $g$ is $L_{g}$ -smooth, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(\\mathbf x_{k+1})\\leq g(\\mathbf y_{k})+\\langle\\nabla g(\\mathbf y_{k}),\\mathbf x_{k+1}-\\mathbf y_{k}\\rangle+\\frac{L_{g}}{2}\\|\\mathbf x_{k+1}-\\mathbf y_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By multiplying both sides of (18) and (19) by $A_{k}$ and $A_{k+1}$ , respectively, and adding the resulted inequalities we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(g(\\mathbf x_{k+1})-g(\\mathbf y_{k}))+A_{k}(g(\\mathbf y_{k})-g(\\mathbf x_{k}))}\\\\ &{\\le A_{k+1}\\left\\langle\\nabla g(\\mathbf y_{k}),\\mathbf x_{k+1}-\\mathbf y_{k}\\right\\rangle+A_{k}\\left\\langle\\nabla g(\\mathbf y_{k}),\\mathbf y_{k}-\\mathbf x_{k}\\right\\rangle+\\frac{L_{g}A_{k+1}}{2}\\|\\mathbf x_{k+1}-\\mathbf y_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla g(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf z_{k}\\right\\rangle+A_{k}\\left\\langle\\nabla g(\\mathbf y_{k}),\\mathbf y_{k}-\\mathbf x_{k}\\right\\rangle+\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla g(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf y_{k}\\right\\rangle+\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first equality holds since $a_{k}({\\mathbf{z}}_{k+1}-{\\mathbf{z}}_{k})=A_{k+1}({\\mathbf{x}}_{k+1}-{\\mathbf{y}}_{k})$ , and the second equality holds since $a_{k}({\\mathbf z}_{k}-{\\mathbf y}_{k})=A_{k}({\\mathbf y}_{k}-{\\mathbf x}_{k})$ . Lastly, by the definition of the constructed cutting plane, we know that $g(\\mathbf{y}_{k})+\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{z}-\\mathbf{y}_{k}\\rangle\\,\\leq\\,g_{k}$ for any $\\textbf{z}\\in\\mathcal{X}_{k}$ . Hence, $\\langle\\nabla g(\\mathbf{y}_{k}),\\mathbf{z}_{k+1}-\\mathbf{\\bar{y}}_{k}\\rangle$ is upper bounded by $g_{k}-g(\\mathbf{y}_{k})$ . Applying this substitution into to the above expression would lead to the claim in (11). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Now we are ready to prove Theorem 4.1. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 4.1. To begin with, note that by our choice of $a_{k}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\na_{k}={\\frac{k+1}{4L_{f}}}\\quad{\\mathrm{and}}\\quad A_{k+1}={\\frac{(k+1)(k+2)}{8L_{f}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, it can be verified that $L_{f}a_{k}^{2}\\leq\\textstyle{\\frac{1}{2}}A_{k+1}$ . Then it follows from Lemma A.1 that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\Big(A_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Big)}\\\\ &{\\phantom{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}}\\leq\\left(\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}-\\displaystyle\\frac{1}{2}\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2},}\\\\ &{A_{k+1}(g(\\mathbf{x}_{k+1})-g(\\mathbf{x}^{*}))-A_{k}(g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))\\leq a_{k}(g_{k}-g(\\mathbf{x}^{*}))+\\displaystyle\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We first prove the convergence guarantee for the upper-level objective. By using induction on (21), we obtain that for any $k\\geq0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nA_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\leq A_{0}(f(\\mathbf{x}_{0})-f(\\mathbf{x}^{*}))+\\frac{1}{2}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}=\\frac{1}{2}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\leq\\frac{\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{2A_{k}}=\\frac{4L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We proceed to establish an upper bound on $g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})$ . By summing the inequality in (22) from 0 to $k-1$ we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{A_{k}(g(\\mathbf x_{k})-g(\\mathbf x^{*}))\\leq\\sum_{i=0}^{k-1}a_{i}(g_{i}-g^{*})+\\frac{L_{g}}{4L_{f}}\\sum_{i=0}^{k-1}\\|\\mathbf z_{i+1}-\\mathbf z_{i}\\|^{2}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{i=0}^{k-1}\\frac{i+1}{4L_{f}}\\frac{2L_{g}\\|\\mathbf x_{0}-\\mathbf x^{*}\\|^{2}}{(i+1)^{2}}+\\frac{L_{g}}{4L_{f}}\\sum_{i=0}^{k-1}D^{2}}\\\\ &{}&{\\leq\\displaystyle\\frac{L_{g}}{2L_{f}}\\|\\mathbf x_{0}-\\mathbf x^{*}\\|^{2}(\\ln k+1)+\\frac{L_{g}}{4L_{f}}D^{2}k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that the second inequality holds due to the condition in (5). Thus, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln k+1)}{k(k+1)}+\\frac{2L_{g}D^{2}}{k+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The above upper bound on $g(\\mathbf{x}_{k})\\,-\\,g(\\mathbf{x}^{*})$ without any additional condition, but next we show that if $f(\\mathbf{x}_{k})~\\geq~f(\\mathbf{x}^{*})$ the above upper bound can be further improved as we can upper bound $\\begin{array}{r}{\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}}\\end{array}$ )b fyr oa mc o0n sttoa epween odbetnati nof $k$ instead of $k D^{2}$ . To prove this claim, by summing $k-1$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}\\leq\\frac{1}{2}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}-\\bigg(A_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\bigg)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, if $f(\\mathbf{x}_{k})\\geq f(\\mathbf{x}^{*})$ , then it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}\\leq2\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus if replace $\\begin{array}{r}{\\sum_{i=0}^{k-1}\\|\\mathbf{z}_{i+1}-\\mathbf{z}_{i}\\|^{2}}\\end{array}$ in (24) by $2\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}$ , we would obtain the following improve bound: ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln k+1)}{k(k+1)}+\\frac{4L_{g}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall Remark 4.1. The main difficulty in obtaining an accelerated rate of $\\mathcal{O}(1/K^{2})$ for $g$ is that it is unclear how to control $\\begin{array}{r}{\\sum_{k=0}^{K-1}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}\\end{array}$ . This, in turn, is because we don\u2019t know how to prove a lower bound on $f$ . In stead, we used the compactness of $\\mathcal{Z}$ to achieve the ${\\mathcal{O}}(1/K)$ for $g$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4.3. Note that by multiplying both sides of (10) by $\\lambda>0$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A_{k+1}\\big(\\lambda(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))\\big)+\\cfrac{\\lambda}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\bigg(A_{k}\\big(\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))\\big)+\\cfrac{\\lambda}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\bigg)}&{{}\\leq\\cfrac{\\lambda}{2}\\left(\\cfrac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}\\\\ {\\leq\\cfrac{\\lambda}{2}\\left(\\cfrac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}&{{}\\leq\\cfrac{\\lambda}{2}\\left(\\cfrac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Further note that in this case we have $\\begin{array}{r}{a_{k}=\\frac{\\gamma(k+1)}{4L_{f}}}\\end{array}$ $\\begin{array}{r}{A_{k+1}=\\gamma\\frac{(k+1)(k+2)}{8L_{f}}}\\end{array}$ (k+1)(k+2). Hence, a2k/Ak+1 is bounded above by $\\frac{\\gamma}{2L_{f}}$ . Therefore, we can replace $a_{k}^{2}/A_{k+1}$ in the above expression by $\\frac{\\gamma}{2L_{f}}$ to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(\\lambda(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))\\big)+\\displaystyle\\frac{\\lambda}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\bigg(A_{k}\\big(\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))\\big)+\\frac{\\lambda}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\lambda}{2}\\left(\\frac{\\gamma}{2}-1\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we can replace $a_{k}^{2}/A_{k+1}$ in (11) by $\\frac{\\gamma}{2L_{f}}$ to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{k+1}{\\big(}g(\\mathbf{x}_{k+1})-g(\\mathbf{x}^{*}){\\big)}-A_{k}{\\big(}g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}){\\big)}\\leq a_{k}{\\big(}g_{k}-g(\\mathbf{x}^{*}){\\big)}+{\\frac{\\gamma L_{g}}{4L_{f}}}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note if we sum up the two inequalities above, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{A_{k+1}(\\lambda(f({\\mathbf x}_{k+1})-f({\\mathbf x}^{*}))+g({\\mathbf x}_{k+1})-g({\\mathbf x}^{*}))+\\frac{\\lambda}{2}\\|{\\mathbf z}_{k+1}-{\\mathbf x}^{*}\\|^{2}}\\\\ &{}&{\\quad-\\left(A_{k}(\\lambda(f({\\mathbf x}_{k})-f({\\mathbf x}^{*}))+g({\\mathbf x}_{k})-g({\\mathbf x}^{*}))+\\frac{\\lambda}{2}\\|{\\mathbf z}_{k}-{\\mathbf x}^{*}\\|^{2}\\right)}\\\\ &{}&{\\quad\\leq(\\lambda(\\frac{\\gamma}{4}-\\frac{1}{2})+\\frac{\\gamma}{4}\\frac{L_{g}}{L_{f}})\\|{\\mathbf z}_{k+1}-{\\mathbf z}_{k}\\|^{2}+a_{k}(g_{k}-g({\\mathbf x}^{*}))\\leq a_{k}(g_{k}-g({\\mathbf x}^{*}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the last inequality holds since the first term is negative due to the choice of $\\lambda$ . By summing the inequalities from 0 to $k-1$ we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{k}(\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))+\\frac{1}{2}\\lambda\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\leq\\sum_{i=0}^{k-1}a_{i}(g_{i}-g(\\mathbf{x}^{*}))+\\frac{1}{2}\\lambda\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now using the condition on $g_{i}$ in (5) and the definition of $a_{i}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}a_{i}(g_{i}-g(\\mathbf{x}^{*}))\\leq\\sum_{i=0}^{k-1}\\frac{\\gamma(i+1)}{4L_{f}}\\frac{2L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{(i+1)^{2}}\\leq\\frac{\\gamma L_{g}}{2L_{f}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln k+1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By applying this upper bound into (31) we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n4_{k}(\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))+\\frac{1}{2}\\lambda\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\leq\\frac{\\gamma L_{g}}{2L_{f}}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln k+1)+\\frac{1}{2}\\lambda\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If we drop the $\\begin{array}{r}{\\frac{1}{2}\\lambda\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}}\\end{array}$ in the left-hand side and divide both sides of the resulted inequality by Ak which is equal to Ak = \u03b3 k(8kL+f1) we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln k+1)}{k(k+1)}+\\frac{4\\lambda L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma k(k+1)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the proof is complete. ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4.4. Recall the result of Lemma 4.3 that if $a_{k}=\\gamma(k\\!+\\!1)/(4L_{f})$ , where $0<\\gamma\\leq1$ and \u03bb \u2265(2/\u03b3\u22121)Lf Lg then after $K$ iterations we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda(f(\\mathbf{x}_{K})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K(K+1)}+\\frac{4\\lambda L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma K(K+1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now if we replace $\\gamma$ by $1/(\\frac{2L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+2)$ as suggested in the statement of the theorem, we would obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda(f(\\mathbf{x}_{K})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})}\\\\ &{\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K(K+1)}+\\frac{8(\\frac{L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+1)\\lambda L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we proceed to prove the first claim which is an upper bound on $f(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})$ . Note that given the fact that $g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})>0$ and $\\lambda>0$ we can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{\\lambda K(K+1)}+\\frac{8((\\frac{L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+1))L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If we select \u03bb which is a free parameter as \u03bb = K\u221222rr\u2212\u221221 \u2265(2/\u03b3L\u2212g1)Lf then we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K^{\\frac{1}{2r-1}}(T+1)}+\\frac{8L_{g}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{\\frac{1}{2r-1}}(T+1)}+\\frac{8L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the fact that $\\mathbf{x}_{0}=\\mathbf{z}_{0}$ we can simplify the upper bound to ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f(\\mathbf{x}^{*})\\leq\\frac{12L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K^{\\frac{2r}{2r-1}}}+\\frac{8L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we proceed to establish an upper bound on $g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})$ . We will use the following inequality that holds due to the HEB condition and formally stated in Proposition 4.2: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{K})-f^{*}\\geq-M\\left(\\frac{r(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})))}{\\alpha}\\right)^{\\frac{1}{r}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now if we replace this lower bound into (36) we would obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\,\\lambda M(\\frac{r}{\\alpha})^{\\frac{1}{r}}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))^{\\frac{1}{r}}+g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})}\\\\ &{\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K(K+1)}+\\frac{8((\\frac{L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+1))\\lambda L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next we consider two different cases: In the first case we assume $\\begin{array}{r l r}{\\lambda M(\\frac{r}{\\alpha})^{\\frac{1}{r}}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))^{\\frac{1}{r}}\\leq}&{{}}&{}\\end{array}$ $\\begin{array}{r}{\\frac{1}{2}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))}\\end{array}$ holds and in the second case we assume the opposite of this inequality holds. If we are in the first case and $\\begin{array}{r}{\\lambda M(\\frac{r}{\\alpha})^{\\frac{1}{r}}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))^{\\frac{1}{r}}\\leq\\frac{1}{2}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))}\\end{array}$ , then the inequality in (41) leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{2}(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K(K+1)}+\\frac{8((\\frac{L_{g}}{L_{f}}K^{\\frac{2r-2}{2r-1}}+1))\\lambda L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\lambda=K^{-\\frac{2r-2}{2r-1}}$ , it further leads to the following upper bound ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})\\leq\\frac{8L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K(K+1)}+\\frac{16L_{g}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K(K+1)}+\\frac{16L_{f}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{\\frac{1}{2r-1}}(K+1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now given the fact that $\\mathbf{x}_{0}=\\mathbf{z}_{0}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})\\leq\\frac{24L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K^{2}}+\\frac{16L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{\\frac{2r}{2r-1}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If we are in the second case and $\\begin{array}{r}{\\lambda M(\\frac{r}{\\alpha})^{\\frac{1}{r}}(g(\\mathbf{x}_{K})\\!-\\!g(\\mathbf{x}^{*}))^{\\frac{1}{r}}>\\frac{1}{2}(g(\\mathbf{x}_{K})\\!-\\!g(\\mathbf{x}^{*}))}\\end{array}$ then this inequality is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n(g(\\mathbf{x}_{K})-g(\\mathbf{x}^{*}))^{1-1/r}\\leq2\\lambda M(\\frac{r}{\\alpha})^{\\frac{1}{r}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "leading to ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})\\leq\\left(2K^{-\\frac{2r-2}{2r-1}}M(\\frac{r}{\\alpha})^{\\frac{1}{r}}\\right)^{\\frac{r}{r-1}}=\\frac{(2M)^{\\frac{r}{r-1}}(\\frac{r}{\\alpha})^{\\frac{1}{r-1}}}{K^{\\frac{2r}{2r-1}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By combining the bounds in (44) and (46) we realize that ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{K})-g(\\mathbf{x}^{*})\\leq\\operatorname*{max}\\{\\frac{24L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K^{2}}+\\frac{16L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{\\frac{2r}{2r-1}}},\\frac{(2M)^{\\frac{r}{r-1}}(\\frac{r}{\\alpha})^{\\frac{1}{r-1}}}{K^{\\frac{2r}{2r-1}}}\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally by using the above bound in (47) and the result of Proposition 4.2 we can prove the the second claim and establish a lower bound on $f(\\mathbf{x}_{K})-f^{*}$ which is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{K})-f^{*}\\geq}\\\\ &{\\,\\,-\\,M\\left(\\frac{r}{\\alpha}\\right)^{\\frac{1}{r}}\\left(\\operatorname*{max}\\{\\frac{24L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1)}{K^{2}}+\\frac{16L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{K^{\\frac{2r}{2r-1}}},\\frac{(2M)^{\\frac{r}{r-1}}\\left(\\frac{r}{\\alpha}\\right)^{\\frac{1}{r-1}}}{K^{\\frac{2r}{2r-1}}}\\}\\right)^{1/r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "leading to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{K})-f^{*}\\geq-M\\left(\\frac{r}{\\alpha}\\right)^{\\frac{1}{r}}}\\\\ &{\\left(\\operatorname*{max}\\{\\frac{(24L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}(\\ln K+1))^{1/r}}{K^{2/r}}+\\frac{(16L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2})^{1/r}}{K^{\\frac{2}{r r-1}}},\\frac{\\left((2M)^{\\frac{r}{r-1}}\\left(\\frac{r}{\\alpha}\\right)^{\\frac{1}{r-1}}\\right)^{1/r}}{K^{\\frac{2}{r-1}}}\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 4.5. To upper bound $f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})$ , we follow a similar analysis as in Theorem 4.1. Specifically, first note that by our choice of ak, we have ak = \u03b3 k4L+f1 and Ak+1 = \u03b3 $\\begin{array}{r}{A_{k+1}=\\gamma\\frac{(k+1)(k+2)}{8L_{f}}}\\end{array}$ (k+1)(k+2), where $\\gamma\\,\\in\\,(0,1)$ . Hence, we can obtain that $\\begin{array}{r}{L_{f}a_{k}^{2}\\,\\leq\\,\\frac{\\gamma}{2}A_{k+1}}\\end{array}$ . By using Lemma A.1 and the fact that $\\gamma\\in(0,1)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*})\\big)+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}\\!-\\!\\Big(A_{k}\\big(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\big)+\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\frac{\\gamma}{4}-\\frac{1}{2}\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By using induction, we obtain that for any $k\\geq0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\leq A_{0}(f(\\mathbf{x}_{0})-f(\\mathbf{x}^{*}))+\\frac{1}{2}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}=\\frac{1}{2}\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\begin{array}{r}{A_{k}=\\gamma\\frac{k(k+1)}{8L_{f}}}\\end{array}$ and $\\mathbf{z}_{0}=\\mathbf{x}_{0}$ , this further implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\leq\\frac{\\|\\mathbf{z}_{0}-\\mathbf{x}^{*}\\|^{2}}{2A_{k}}=\\frac{4L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma k(k+1)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we will prove the upper bound on $g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})$ . By Lemma 4.3, we have for any $k\\geq0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}(\\ln k+1)+\\frac{4\\lambda L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma k(k+1)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, since $g$ satisfies the weak sharpness condition, we can use Proposition 4.2 with $r=1$ to write ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k})-f^{*}\\geq-\\frac{M}{\\alpha}(g(\\mathbf{x}_{k})-g^{*}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (51) and (52) leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\lambda\\frac{M}{\\alpha}(g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))+g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{4L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}(\\ln k+1)+\\frac{4\\lambda L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma k(k+1)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that we can choose \u03bb to be any number satisfying \u03bb \u2265 $\\begin{array}{r}{\\lambda\\geq\\frac{L_{g}}{(2/\\gamma-1)L_{f}}}\\end{array}$ (cf. Lemma 4.3). Specifically, since $\\begin{array}{r}{\\gamma\\le\\frac{2\\alpha L_{f}}{2M L_{g}+\\alpha L_{f}}}\\end{array}$ , we can set $\\lambda=\\alpha/(2M)$ and accordingly (53) can be simplified to ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})\\leq\\frac{8L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{k(k+1)}(\\ln k+1)+\\frac{4\\alpha L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{*}\\|^{2}}{\\gamma M k(k+1)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we use (52) again together with the above upper bound on $g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*})$ to obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n^{\\mathrm{\\tiny~\\sharp~}}(\\mathbf{x}_{k})-f(\\mathbf{x}^{\\mathrm{\\tiny~*}})\\geq-\\frac{M}{\\alpha}(g(\\mathbf{x}_{k})-g(\\mathbf{x}^{\\mathrm{\\tiny~*}}))\\geq-\\left(\\frac{8M L_{g}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{\\tiny~*}}\\|^{2}}{\\alpha k(k+1)}(\\ln k+1)+\\frac{4L_{f}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{\\tiny~*}}\\|^{2}}{\\gamma k(k+1)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B Extension to the Non-smooth/Composite Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we would like to mention the possible extension to the non-smooth/composite setting. In the general non-smooth settings, we believe it is not possible to extend our results and achieve the purpose of the acceleration. This is because\u221a, even in the single-level setting, the best achievable rate in the general non-smooth setting is $\\mathcal{O}(1/\\sqrt{K})$ achieved by sub-gradient method. That said, it should be possible to extend our accelerated bilevel framework to a special non-smooth setting where the upper- and lower-level objective functions have a composite structure, i.e., they can be written as the sum of a convex smooth function and a convex non-smooth function that is easy to compute its proximal operator. ", "page_idx": 18}, {"type": "text", "text": "Note that the properties of smoothness of $f$ and $g$ have only been used in the proof of Lemma A.1 in Section A. None of the other results will break if (10) and (11) in Lemma A.1 still hold in the composite setting. Now, we present and prove the counterpart of Lemma A.1 in the composite setting. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.1. Suppose $f_{1},f_{2},g_{1},g_{2}$ are convex and $f_{1},g_{1}$ are $L_{f}$ -smooth and $L_{g}$ -smooth, respectively. Let $\\{{\\bf x}_{k}\\}$ be the sequence of iterates generated by Algorithm 2 with stepsize $a_{k}\\,>\\,0$ for $k\\,\\geq\\,0$ . Moreover, suppose Assumption 3.1 holds. Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\Big(A_{k}(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Big)}\\\\ &{\\phantom{A_{k+1}(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*}))+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}}\\leq\\left(\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}-\\displaystyle\\frac{1}{2}\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2},}\\\\ &{\\phantom{A_{k+1}(g(\\mathbf{x}_{k+1})-g(\\mathbf{x}^{*}))-A_{k}(g(\\mathbf{x}_{k})-g(\\mathbf{x}^{*}))\\leq a_{k}(g_{k}-g(\\mathbf{x}^{*}))+\\displaystyle\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma B.1. Let $\\mathbf{x}^{*}$ be any optimal solution of (6). ", "page_idx": 19}, {"type": "text", "text": "We first consider the upper-level objective $f$ . Since $f_{1}$ is convex, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}^{*})\\leq\\left\\langle\\nabla f_{1}(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}^{*}\\right\\rangle,\\quad f_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}_{k})\\leq\\left\\langle\\nabla f_{1}(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now given the update rule $A_{k+1}=A_{k}+a_{k}$ , we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{k+1}(f_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}^{*}))-A_{k}(f_{1}(\\mathbf{x}_{k})-f_{1}(\\mathbf{x}^{*}))=a_{k}(f_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}^{*}))+A_{k}(f_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}_{k}))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (56) and (57), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(f_{1}(\\mathbf{y}_{k})-f_{1}(\\mathbf{x}^{*})\\big)-A_{k}\\big(f_{1}(\\mathbf{x}_{k})-f_{1}(\\mathbf{x}^{*})\\big)}\\\\ &{\\leq a_{k}\\big(\\langle\\nabla f_{1}(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}^{*}\\rangle\\big)+A_{k}\\big(\\langle\\nabla f_{1}(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\rangle\\big)}\\\\ &{=\\langle\\nabla f_{1}(\\mathbf{y}_{k}),a_{k}\\mathbf{y}_{k}+A_{k}(\\mathbf{y}_{k}-\\mathbf{x}_{k})-a_{k}\\mathbf{x}^{*}\\rangle}\\\\ &{=a_{k}\\left\\langle\\nabla f_{1}(\\mathbf{y}_{k}),\\mathbf{z}_{k}-\\mathbf{x}^{*}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality follows from the definition of $\\mathbf{y}_{k}$ . Furthermore, since $f_{1}$ is $L_{f}$ -smooth, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{1}({\\bf x}_{k+1})\\leq f_{1}({\\bf y}_{k})+\\langle\\nabla f_{1}({\\bf y}_{k}),{\\bf x}_{k+1}-{\\bf y}_{k}\\rangle+\\frac{L_{f}}{2}\\|{\\bf x}_{k+1}-{\\bf y}_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If we multiply both sides of (59) by $A_{k+1}$ and combine the resulting inequality with (58), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(f_{1}(\\mathbf x_{k+1})-f_{1}(\\mathbf x^{*}))-A_{k}(f_{1}(\\mathbf x_{k})-f_{1}(\\mathbf x^{*}))}\\\\ &{\\le a_{k}\\left\\langle\\nabla f_{1}(\\mathbf y_{k}),\\mathbf z_{k}-\\mathbf x^{*}\\right\\rangle+A_{k+1}\\left\\langle\\nabla f_{1}(\\mathbf y_{k}),\\mathbf x_{k+1}-\\mathbf y_{k}\\right\\rangle+\\displaystyle\\frac{L_{f}A_{k+1}}{2}\\|\\mathbf x_{k+1}-\\mathbf y_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla f_{1}(\\mathbf y_{k}),\\mathbf z_{k}-\\mathbf x^{*}\\right\\rangle+a_{k}\\left\\langle\\nabla f_{1}(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf z_{k}\\right\\rangle+\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla f_{1}(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf x^{*}\\right\\rangle+\\displaystyle\\frac{L_{f}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the fact that $a_{k}({\\bf z}_{k+1}-{\\bf z}_{k})=A_{k+1}({\\bf x}_{k+1}-{\\bf y}_{k})$ in the first equality. Moreover, from the step 6 in Algorithm 2, we have zk \u2212 $\\begin{array}{r}{\\i_{k}\\nabla f_{1}(\\mathbf{y}_{k})-\\mathbf{z}_{k+1}\\in a_{k}\\partial(f_{2}(\\mathbf{z}_{k+1})+\\delta_{\\mathcal{X}_{k}}(\\mathbf{z}_{k+1})}\\end{array}$ ). Using this, from the definition of subgradients for $f_{2}+\\delta_{\\mathcal{X}_{k}}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\langle\\mathbf x^{*}-\\mathbf z_{k+1},\\mathbf z_{k}-a_{k}\\nabla f_{1}(\\mathbf y_{k})-\\mathbf z_{k+1}\\rangle\\le a_{k}\\bigl(f_{2}(\\mathbf x^{*})+\\delta_{X_{k}}(\\mathbf x^{*})-f_{2}(\\mathbf z_{k+1})-\\delta_{X_{k}}(\\mathbf z_{k+1})\\bigr)}&\\\\ {\\Leftrightarrow}&{\\langle\\mathbf z_{k+1}-\\mathbf z_{k}+a_{k}\\nabla f(\\mathbf y_{k}),\\mathbf x^{*}-\\mathbf z_{k+1}\\rangle\\ge a_{k}f_{2}(\\mathbf z_{k+1})-a_{k}f_{2}(\\mathbf x^{*})}&\\\\ {\\Leftrightarrow}&{a_{k}\\left\\langle\\nabla f(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf x^{*}\\right\\rangle\\le\\langle\\mathbf z_{k+1}-\\mathbf z_{k},\\mathbf x^{*}-\\mathbf z_{k+1}\\rangle-a_{k}f_{2}(\\mathbf z_{k+1})+a_{k}f_{2}(\\mathbf x^{*})}&\\\\ {\\Leftrightarrow}&{a_{k}\\left\\langle\\nabla f(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf x^{*}\\right\\rangle\\leq\\frac{1}{2}\\bigl\\|\\mathbf z_{k}-\\mathbf x^{*}\\bigr\\|^{2}-\\frac{1}{2}\\bigl\\|\\mathbf z_{k+1}-\\mathbf x^{*}\\bigr\\|^{2}-\\frac{1}{2}\\bigl\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\bigr\\|^{2}}&\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-a_{k}f_{2}(\\mathbf z_{k+1})+a_{k}f_{2}(\\mathbf x^{*}).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The first step holds since $\\mathbf{x}^{*}$ , $\\mathbf{z}_{k+1}\\in\\mathcal{X}_{k}$ , i.e. $\\delta_{\\mathcal{X}_{k}}(\\mathbf{x}^{*})=\\delta_{\\mathcal{X}_{k}}(\\mathbf{z}_{k+1})=0$ . Combining (60) and (61) leads to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(f_{1}\\big(\\mathbf x_{k+1}\\big)-f_{1}\\big(\\mathbf x^{*}\\big)\\big)+\\displaystyle\\frac12\\|\\mathbf z_{k+1}-\\mathbf x^{*}\\|^{2}-\\bigg(A_{k}\\big(f_{1}\\big(\\mathbf x_{k}\\big)-f_{1}\\big(\\mathbf x^{*}\\big)\\big)+\\frac12\\|\\mathbf z_{k}-\\mathbf x^{*}\\|^{2}\\bigg)}\\\\ &{\\leq\\displaystyle\\frac12\\left(\\frac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}-a_{k}f_{2}\\big(\\mathbf z_{k+1}\\big)+a_{k}f_{2}\\big(\\mathbf x^{*}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we add $\\begin{array}{r}{\\left(A_{k+1}f_{2}(\\mathbf{x}_{k+1})-a_{k}f_{2}(\\mathbf{x}^{*})-A_{k}f_{2}(\\mathbf{x}_{k})\\right)}\\end{array}$ on both sides to obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{k+1}\\big(f(\\mathbf{x}_{k+1})-f(\\mathbf{x}^{*})\\big)+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k+1}-\\mathbf{x}^{*}\\|^{2}-\\Bigg(A_{k}\\big(f(\\mathbf{x}_{k})-f(\\mathbf{x}^{*})\\big)+\\displaystyle\\frac{1}{2}\\|\\mathbf{z}_{k}-\\mathbf{x}^{*}\\|^{2}\\Bigg)}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\left(\\frac{L_{f}a_{k}^{2}}{A_{k+1}}-1\\right)\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}-a_{k}f_{2}\\big(\\mathbf{z}_{k+1}\\big)+A_{k+1}f_{2}\\big(\\mathbf{x}_{k+1}\\big)-A_{k}f_{2}\\big(\\mathbf{x}_{k}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, by the convexity of $f_{2},\\;A_{k+1}\\;=\\;A_{k}\\,+\\,a_{k}$ , and $\\begin{array}{r}{\\mathbf{x}_{k+1}\\:=\\:\\frac{A_{k}}{A_{k}+a_{k}}\\mathbf{x}_{k}\\:+\\:\\frac{a_{k}}{A_{k}+a_{k}}\\mathbf{z}_{k+1}}\\end{array}$ , i.e. $-a_{k}f_{2}(\\mathbf{z}_{k+1})+A_{k+1}f_{2}(\\mathbf{x}_{k+1})-A_{k}f_{2}(\\mathbf{x}_{k})\\leq0,$ , the first inequality of this Lemma can be obtained. Next, we proceed to prove the claim for the lower-level objective $g$ . To do so, we first leverage the convexity of the smooth part $g_{1}$ which leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{1}(\\mathbf{y}_{k})-g_{1}(\\mathbf{x}_{k})\\leq\\left\\langle\\nabla g_{1}(\\mathbf{y}_{k}),\\mathbf{y}_{k}-\\mathbf{x}_{k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, since $g_{1}$ is $L_{g}$ -smooth, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{1}(\\mathbf x_{k+1})\\leq g_{1}(\\mathbf y_{k})+\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf x_{k+1}-\\mathbf y_{k}\\rangle+\\frac{L_{g}}{2}\\|\\mathbf x_{k+1}-\\mathbf y_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By multiplying both sides of (64) and (65) by $A_{k}$ and $A_{k+1}$ , respectively, and adding the resulted inequalities we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(g_{1}(\\mathbf x_{k+1})-g_{1}(\\mathbf y_{k}))+A_{k}(g_{1}(\\mathbf y_{k})-g_{1}(\\mathbf x_{k}))}\\\\ &{\\le A_{k+1}\\left\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf x_{k+1}-\\mathbf y_{k}\\right\\rangle+A_{k}\\left\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf y_{k}-\\mathbf x_{k}\\right\\rangle+\\displaystyle\\frac{L_{g}A_{k+1}}{2}\\|\\mathbf x_{k+1}-\\mathbf y_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf z_{k}\\right\\rangle+A_{k}\\left\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf y_{k}-\\mathbf x_{k}\\right\\rangle+\\displaystyle\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}}\\\\ &{=a_{k}\\left\\langle\\nabla g_{1}(\\mathbf y_{k}),\\mathbf z_{k+1}-\\mathbf y_{k}\\right\\rangle+\\displaystyle\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first equality holds since $a_{k}({\\mathbf{z}}_{k+1}-{\\mathbf{z}}_{k})=A_{k+1}({\\mathbf{x}}_{k+1}-{\\mathbf{y}}_{k})$ , and the second equality holds since $a_{k}({\\bf z}_{k}-{\\bf y}_{k})\\,=\\,A_{k}({\\bf y}_{k}-{\\bf\\dot{x}}_{k})$ . Lastly, by the definition of the constructed approximated set $\\scriptstyle{\\mathcal{X}}_{k}$ , we know that $g_{1}(\\dot{\\mathbf{y}}_{k})+\\langle\\nabla g_{1}(\\mathbf{y}_{k}),\\mathbf{z}-\\mathbf{y}_{k}\\rangle+g_{2}(\\mathbf{z})\\,\\leq\\,g_{k}$ for any $\\textbf{z}\\in\\mathcal{X}_{k}$ . Hence, $\\langle\\nabla g_{1}(\\mathbf{y}_{k}),\\mathbf{z}_{k+1}-\\mathbf{y}_{k}\\rangle$ is upper bounded by $g_{k}-g_{1}(\\mathbf{y}_{k})-g_{2}(\\mathbf{z}_{k+1})$ . Applying this substitution into to the above expression to obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A_{k+1}\\big(g_{1}\\big(\\mathbf x_{k+1}\\big)-g_{1}\\big(\\mathbf y_{k}\\big)\\big)+A_{k}\\big(g_{1}\\big(\\mathbf y_{k}\\big)-g_{1}\\big(\\mathbf x_{k}\\big)\\big)}\\\\ {\\displaystyle\\leq a_{k}g_{k}-a_{k}g_{1}\\big(\\mathbf y_{k}\\big)-a_{k}g_{2}\\big(\\mathbf z_{k+1}\\big)+\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By adding $a_{k}g_{1}(\\mathbf y_{k})-a_{k}g_{1}(\\mathbf x^{*})$ on both sides, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A_{k+1}\\big(g_{1}\\big(\\mathbf x_{k+1}\\big)-g_{1}\\big(\\mathbf x^{*}\\big)\\big)+A_{k}\\big(g_{1}\\big(\\mathbf x^{*}\\big)-g_{1}\\big(\\mathbf x_{k}\\big)\\big)}\\\\ {\\displaystyle\\leq a_{k}g_{k}-a_{k}g_{1}\\big(\\mathbf x^{*}\\big)-a_{k}g_{2}\\big(\\mathbf z_{k+1}\\big)+\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf z_{k+1}-\\mathbf z_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lastly, we add $\\begin{array}{r}{\\left(A_{k+1}g_{2}(\\mathbf{x}_{k+1})-a_{k}g_{2}(\\mathbf{x}^{*})-A_{k}g_{2}(\\mathbf{x}_{k})\\right)}\\end{array}$ on both sides to obtain, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{k+1}(g(\\mathbf{x}_{k+1})-g(\\mathbf{x}^{*}))+A_{k}(g(\\mathbf{x}^{*})-g(\\mathbf{x}_{k}))}\\\\ &{\\le a_{k}g_{k}-a_{k}g(\\mathbf{x}^{*})+A_{k+1}g_{2}(\\mathbf{x}_{k+1})-A_{k}g_{2}(\\mathbf{x}_{k})-a_{k}g_{2}(\\mathbf{z}_{k+1})+\\frac{L_{g}a_{k}^{2}}{2A_{k+1}}\\|\\mathbf{z}_{k+1}-\\mathbf{z}_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the convexity of g2, Ak+1 = Ak + ak, and xk+1 = AkA+kak $\\begin{array}{r}{\\mathbf{x}_{k+1}\\:=\\:\\frac{A_{k}}{A_{k}+a_{k}}\\mathbf{x}_{k}\\:+\\:\\frac{a_{k}}{A_{k}+a_{k}}\\mathbf{z}_{k+1}}\\end{array}$ (outlined in Algorithm 2), i.e. $A_{k+1}g_{2}(\\mathbf{x}_{k+1})-A_{k}g_{2}(\\mathbf{x}_{k})-a_{k}g_{2}(\\mathbf{z}_{k+1})\\stackrel{*}{\\leq}\\stackrel{*}{0}$ , the second inequality of this Lemma can be achieved. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Hence, with the additional Assumption 3.1, by replicating the analysis outlined in Section A, we can derive identical complexity results for Algorithm 2 in either the compact domain setting or with the H\u00f6lderian error bounds on $g$ . ", "page_idx": 20}, {"type": "text", "text": "C Additional related work ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Previous work has explored \u201casymptotic\" results for simple bilevel problems, dating back to Tikhonovtype regularization introduced in [37]. In this approach, the objectives of both levels are combined into a single-level problem using a regularization parameter $\\sigma>0$ and as $\\sigma\\rightarrow0$ the solutions of the regularized single-level problem approaches a solution to the bilevel problem in (1). Further, the authors in [38] proposed the explicit descent method that solves problem (1) when upper and lower-level functions are smooth and convex. This result was further extended to a non-smooth setting in [39]. The results in both [38] and [39] only indicated that both upper and lower-level objective functions converge asymptotically. Moreover, the authors in [40] proposed the $\\epsilon_{}$ -subgradient method to solve simple bilevel problems and showed its asymptotic convergence. Specifically, they assumed the upper-level objective function to be convex and utilized two different algorithms, namely, the Fast Iterative Bilevel Algorithm (FIBA) and Incremental Iterative Bilevel Algorithm (IIBA), that consider smooth and non-smooth lower-level objective functions, respectively. ", "page_idx": 21}, {"type": "text", "text": "Some studies have only established non-asymptotic convergence rates for the lower-level problem. One of the pioneering methods in this category is the minimal norm gradient (MNG) method, introduced in [41]. This method assumes that the upper-level objective function is smooth and strongly convex, while the lower-level objective function is smooth and convex. The authors showed that the lower-level objective function reaches an iteration complexity of ${\\mathcal{O}}(1/\\epsilon^{2})$ . Subsequently, the Bilevel Gradient SAM (BiS-SAM) method was introduced in [35], and it was proven to achieve a complexity of ${\\mathcal O}(1/\\epsilon)$ for the lower-level problem. A similar rate of convergence was also attained in [42]. ", "page_idx": 21}, {"type": "text", "text": "D Connection with the Polyak Step Size ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we would like to highlight the connection between our algorithm\u2019s projection step (outlined in Step 6 of Algorithm 1) and the Polyak step size. To make this connection, we first without loss of generality, replace $g_{k}$ with $g^{\\ast}$ . It is a reasonable argument, as $g_{k}$ values are close to $g^{\\ast}$ , a point highlighted in (5). In addition, we further assume that the set $\\mathcal{Z}=\\mathbb{R}^{n}$ to simplify the expressions. Given these substitutions, the projection step in our AGM-BiO method is equivalent to solving the following problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}\\quad\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}}\\\\ &{\\mathrm{s.t.}\\quad g(\\mathbf{x}_{k})+\\langle\\nabla g(\\mathbf{x}_{k}),\\mathbf{x}-\\mathbf{x}_{k}\\rangle\\leq g^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In other $\\operatorname{words},\\mathbf{x}_{k+1}$ is the unique solution of the above quadratic program with a linear constraint. By writing the optimality conditions for the above problem and considering $\\lambda$ as the Lagrange multipliers associated with the linear constraint, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\lambda\\nabla g(\\mathbf{x}_{k})}\\\\ {\\lambda(g(\\mathbf{x}_{k})+\\langle\\nabla g(\\mathbf{x}_{k}),\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\rangle-g^{*})=0}\\\\ {\\lambda\\geq0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given the fact that $\\mathbf{x}_{k+1}\\neq\\mathbf{x}_{k}$ , we can conclude that $\\lambda\\neq0$ , and hence we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\lambda\\nabla g(\\mathbf{x}_{k})}\\\\ {g(\\mathbf{x}_{k})+\\langle\\nabla g(\\mathbf{x}_{k}),\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\rangle-g^{*}=0}\\\\ {\\lambda>0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By replacing $\\mathbf{x}_{k+1}$ in the second expression with its expression in the first equation we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda=\\frac{g(\\mathbf{x}_{k})-g^{*}}{\\|\\nabla g(\\mathbf{x}_{k})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is exactly the Polyak step size in the literature [43]. To solve a bilevel optimization problem, we intend to do gradient descent for both upper- and lower-level functions. Tuning the ratio of upperand lower-level step size is generally hard. However, by connecting the projection step with the Polyak step size, we observe that the stepsize for the lower-level objective is auto-selected as the Polyak stepsize in our method. In other words, it is one of the advantages of our algorithm that we do not need to choose the lower-level stepsize or ratio of the upper- and lower-level stepsize theoretically or empirically. ", "page_idx": 21}, {"type": "text", "text": "E Experiment Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we include more details of the numerical experiments in Section 5. All simulations are implemented using MATLAB R2022a on a PC running macOS Sonoma with an Apple M1 Pro chip and 16GB Memory. ", "page_idx": 22}, {"type": "text", "text": "E.1 Over-parametrized Regression ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Dataset generation. The original Wikipedia Math Essential dataset [34] composes of a data matrix of size $1068\\times731$ . We randomly select one of the columns as the outcome vector $\\mathbf{b}\\in\\mathbb{R}^{1068}$ and the rest to be a new matrix $\\mathbf{A}\\in\\mathbb{R}^{\\bar{10}68\\times730}$ . We set the constraint parameter $\\lambda=1$ in this experiment, i.e., the constraint set is given by $\\mathcal{Z}=\\{\\beta\\;|\\;\\|\\beta\\|_{2}\\leq1\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Implementation details. To be fair, all the algorithms start from the origin as the initial point. For our AGM-BiO method, we set the target tolerances for the absolute suboptimality and infeasibility to $\\epsilon_{f}=10^{-4}$ and $\\epsilon_{g}=10^{-4}$ , respectively. We choose the stepsizes as $\\bar{a}_{k}=10^{-2}(k+1)/(4L_{f}\\dot{)}$ . In each iteration, we need to do a projection onto an intersection of a $L_{2}$ -ball and a halfspace, which has a closed-form solution. For a-IRG, we set $\\eta_{0}\\,=\\,10^{-3}$ and $\\gamma_{0}\\,=\\,10^{-3}$ . For CG-BiO, we obtain an initial point with FW gap of the lower-level problem less than $\\epsilon_{g}/2=5\\times10^{-5}$ and choose stepsize $\\gamma_{k}=10^{-2}/(k+2)$ . For Bi-SG, we set $\\eta_{k}=10^{-2}/(k+1)^{0.75}$ and $t_{k}=1/L_{g}=$ $1/\\lambda_{m a x}(\\mathbf{A}_{t r}^{\\top}\\mathbf{A}_{t r})=1.5\\times10^{-4}$ . For SEA, we set both the lower- and upper-level stepsizes to be $10^{-4}$ . For R-APM, since the lower-level problem does not satisfy the weak sharpness condition, we set $\\eta=1/(K+1)=1.25\\times10^{-5}$ and $\\gamma^{'}\\!=10^{-4}\\leq1/(L_{g}+\\eta L_{f})$ . For PB-APG, we set the penalty parameter $\\gamma=10^{4}$ . Note that the lower-level problem in this experiment does not satisfy H\u00f6derian error bound assumption, so there is no theoretical guarantee for PB-APG. ", "page_idx": 22}, {"type": "text", "text": "E.2 Linear inverse problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Dataset generation. We set $\\mathbf{Q}=\\mathbf{I}_{n}$ , $\\mathbf{A}=\\mathbf{1}_{n}^{\\top}$ , and $\\mathbf{b}=1$ . The constraint set is selected as $\\mathcal{Z}=\\mathbb{R}_{+}^{n}$ . We choose a low dimensional $\\ n=3$ ) and a high dimensional $\\left[n=100\\right]$ ) example and run $K=10^{3}$ number of iterations to compare the numerical performance of these algorithms, respectively. ", "page_idx": 22}, {"type": "text", "text": "Implementation details. To be fair, all the algorithms start from the same initial point randomly chosen from $\\mathbb{R}_{+}^{n}$ . For our AGM-BiO method, we set the stepsizes as $a_{k}=\\gamma(k+1)/(4L_{f})$ , where $\\begin{array}{r}{\\gamma=1/(\\frac{2L_{g}}{L_{f}}K^{2/3}+2)}\\end{array}$ aacs es uangdg .d  iSni nTche ehoarlefsmp a4.c4e.s  Iann ed itaerrea tbiootnh,  cwoen nveeex da tnod  pcrloojseecdt  osentt,o t ahne $\\mathbb{R}_{+}^{n}$ $\\mathbb{R}_{+}^{n}$   \nprojection subproblem can be solved by Dykstra\u2019s projection algorithm in [20]. For a-IRG, we set $\\bar{\\eta}_{0}\\doteq10^{-2}$ and $\\gamma_{0}=10^{-2}$ . For Bi-SG, we set $\\eta_{k}\\dot{=}\\;\\dot{1}/(k+1)^{\\frac{5}{0.75}}$ and $t_{k}=1/L_{g}$ . For SEA, we set the lower-level stepsize to be $10^{-2}$ and the upper-level stepsize to be $10^{-2}$ . For R-APM, since the lower-level problem does not satisfy the weak sharpness condition, we set $\\eta=1/(K+1)$ and $\\gamma=1/(L_{g}+\\eta\\bar{L}_{f})$ . For PB-APG, we set the penalty parameter $\\gamma=10^{4}$ . For Bisec-BiO, we choose the target tolerances to $\\epsilon_{f}=\\epsilon_{g}=10^{-4}$ . For comparison purposes, we limit the maximum number of gradient evaluations for each APG call to $10^{2}$ . In this experiment, $L_{f}=1$ and $L_{g}=n$ , where $n$ is the number of dimensions. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We clearly stated our contributions in the introduction aligned with the main claims in the abstract. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The main limitation is that our algorithm requires the compact domain as we stated in Section 3 and 4. We also explained why such an assumption is necessary in Remark 4.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper provides the full set of assumptions in Section 2.1 and a complete proof in Section A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experimental results are stated in Section 5. The implementation details are included in Section E. The code and data are attached in the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The code and data are attached in the supplementary material. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We explained how we performed the experiments in Section 5. Moreover, the implementation details are included in Section E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our algorithm is designed for deterministic simple bilevel optimization, which does not include any randomness. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The compute resources we used are stated in Section E. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper conforms, in every respect, with the NeurIPS Code of Ethics Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no societal impact of the work performed in this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The data used in the paper are properly credited and cited in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]