[{"type": "text", "text": "Fast Samplers for Inverse Problems in Iterative Refinement Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kushagra Pandey\u2217 Department of Computer Science University of California Irvine pandeyk1@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Ruihan Yang\u2217 Department of Computer Science University of California Irvine ruihan.yang@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Stephan Mandt Department of Computer Science University of California Irvine mandt@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-andplay framework for constructing efficient samplers for inverse problems, requiring only pre-trained diffusion or flow-matching models. We present Conditional Conjugate Integrators, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method\u2019s performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like $4\\times$ super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Iterative refinement models, such as diffusion generative models and flow matching methods [SohlDickstein et al., 2015, Ho et al., 2020, Song et al., 2020, Lipman et al., 2023, Albergo and VandenEijnden, 2023], have seen increasing popularity in recent months, and much effort has been invested in accelerating unconditional sampling in these models [Pandey et al., 2024, Shaul et al., 2024, Sauer et al., 2024, Karras et al., 2022, Salimans and Ho, 2022, Zhang and Chen, 2023, Lu et al., 2022, Song et al., 2021]. However, while most efficient samplers have been designed in the unconditional setup, current methods for solving inverse problems, such as deblurring, inpainting, or super-resolution, still require hundreds to thousands of neural network evaluations to achieve the highest perceptual quality. Moreover, in addition to a score function evaluation, a class of existing methods for solving inverse problems using pre-trained unconditional iterative refinement models often involves expensive Jacobian-vector products [Song et al., 2022, Chung et al., 2022a], making a single sampling step quite expensive and therefore, intolerably slow for most practical applications. ", "page_idx": 0}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/0cc76b7600fddb848ba4e91a2489dd116659582486f34126eb08ddd4f847c565.jpg", "img_caption": ["Figure 1: Illustration of Conditional Conjugate Integrators for Fast Sampling in Inverse Problems. Given an initial sampling latent ${\\bf x}_{t_{s}}$ at time $t_{s}$ , our sampler projects the diffusion/flow dynamics to a more amenable space for sampling using a projector operator $\\Phi$ which is conditioned on the degradation operator $\\pmb{H}$ and the sampling guidance scale $w$ . The diffusion/flow sampling is then performed in the projected space. Post completion, the generated sample in the projected space is transformed back into the original space using the inverse of the projection operator, yielding the final generated sample. We define the form of the operator $\\Phi$ in Section 2.2. Conditional Conjugate Integrators can significantly speed up sampling in challenging inverse problems and can generate high-quality samples in as few as 5 NFEs as compared to existing baselines, which require from 20-1000 NFEs (see Section 3). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This paper presents a principled framework for designing efficient samplers for guided sampling in iterative refinement models, accelerating existing samplers like \u03a0GDM by an order of magnitude. We present our framework for inverse problems where the degradation operator is known and might be corrupted with additional noise. Crucially, our transformations do not require any re-training and merely rely on some algebraic manipulations of the equations to be simulated. ", "page_idx": 1}, {"type": "text", "text": "Intuitively, we expand on the concept of Conjugate Integrators [Pandey et al., 2024] by projecting the conditional generation process in inverse problems to another space that might be better conditioned for faster sampling (See Figure 1). To this end, we separate the linear and non-linear components in the generation process and parameterize the transformation by analytically solving the linear coefficients. By the end of the sampling procedure, we map back to the original sampling space, leading to the concept of Conditional Conjugate Integrators that apply to various iterative refinement models such as diffusion models, flows, and interpolants. ", "page_idx": 1}, {"type": "text", "text": "In more detail, our main contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Conditional Conjugate Integrators: We repurpose the recently proposed Conjugate Integrator framework [Pandey et al., 2024] for fast guided sampling in iterative refinement models (diffusions and flows) for linear inverse problems and refer to it as Conditional Conjugate Integrators. Next, we design a specific parameterization of the proposed framework, which encodes the structure of the linear inverse problem in the sampler design itself. \u2022 Theoretical Analysis: Our parameterization exhibits theoretical properties that help us identify key parameters for sampler design. More specifically, we show that our parameterization (by design) enables recovering high-frequency details early on during sampling. This further enables fast-guided sampling while maintaining good sample quality in the context of inverse problems. \u2022 Empirical Results. Empirically, we show that our proposed sampler significantly improves over baselines in terms of sampling efficiency on challenging benchmarks across inverse problems like super-resolution, inpainting, and Gaussian deblurring. For instance, on a challenging $4\\mathtt{x}$ superresolution task on the ImageNet dataset, our proposed sampler achieves better sample quality at 5 steps, compared to 20-1000 steps required by competing baselines. ", "page_idx": 1}, {"type": "text", "text": "Additionally, we extend the proposed framework for noisy and non-linear inverse problems with qualitative demonstrations. ", "page_idx": 2}, {"type": "text", "text": "2 Fast Samplers for Inverse Problems using Diffusions/Flows. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Background and Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models define a continuous-time forward process (usually with an affine drift) to convert data $\\mathbf{x}_{0}\\in\\mathbb{R}^{d}$ into noise. A learnable reverse process is trained to generate data from noise. In this work, we only consider deterministic reverse processes specified as an ODE [Song et al., 2020], ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{x}_{t}=\\left[F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p_{t}(\\mathbf{x}_{t})\\right]\\,d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The score is usually intractable and is approximated using a parametric estimator $\\mathbf{\\boldsymbol{s}}_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)$ , trained using denoising score matching [Vincent, 2011, Song and Ermon, 2019, Song et al., 2020]. Analogously, one-sided stochastic interpolants [Albergo and Vanden-Eijnden, 2023] define an interpolant2 $\\mathbf{x}_{t}\\,=\\,\\alpha_{t}\\mathbf{x}_{1}+\\gamma_{t}\\mathbf{z},$ , where $\\mathbf{x}_{1}\\,\\sim\\,p_{\\mathrm{data}}$ ,and $\\textbf{z}\\sim\\mathcal{N}(0,I)$ to define a transport map between the generative prior (typically an isotropic Gaussian) and the data distribution. Interestingly, the one-sided interpolant induces a vector field $b({\\bf x}_{t},t)=\\mathbb{E}[\\dot{\\alpha}_{t}{\\bf x}_{1}+\\dot{\\gamma}_{t}{\\bf z}|{\\bf x}_{t}]$ , where $\\dot{\\alpha}_{t}$ , ${\\dot{\\gamma}}_{t}$ represent the time derivatives of $\\alpha_{t}$ and $\\gamma_{t}$ , respectively. The vector field $b(.)$ is typically learned using a neural network approximation $\\boldsymbol{b}_{\\theta}(\\mathbf{x}_{t},t)$ . The deterministic interpolant process can then be specified as $d\\mathbf{x}_{t}=b_{\\theta}\\bar{(\\mathbf{x}_{t}},t)\\,d t$ . Numerically solving these deterministic generative processes with a sufficient sampling budget can generate plausible samples from noise. ", "page_idx": 2}, {"type": "text", "text": "Problem Statement. Given a noisy linear degradation process (we will consider non-linear processes later) with a degradation operator $\\pmb{H}$ specified over an unobserved data point $\\mathbf{x}_{\\mathrm{0}}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{y}=H\\mathbf{x}_{0}+\\sigma_{y}\\mathbf{z},\\quad\\mathbf{z}\\sim\\mathcal{N}(0,I),\\ \\mathbf{x}_{0}\\sim p_{\\mathrm{data}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "the goal is to recover the original signal $\\mathbf{x}_{\\mathrm{0}}$ . Additionally, given an unconditional pre-trained diffusion or flow matching model, one approach for solving inverse problems is to infer the posterior distribution over the data given the degraded observation, i.e., $p(\\mathbf{x}_{0}|\\mathbf{y}\\bar{\\mathbf{\\alpha}})\\propto p(\\mathbf{y}|\\mathbf{x}_{0})p(\\mathbf{x}_{0})$ by simulating the conditional reverse process dynamics i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Diffusion}\\colon\\qquad}&{d\\mathbf{x}_{t}=\\Big[F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}|y)\\Big]d t,}\\\\ {\\mathrm{Flows}\\colon\\qquad}&{d\\mathbf{x}_{t}=b(\\mathbf{x}_{t},y,t)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}|\\pmb{y})$ and $\\mathbf{b}(\\mathbf{x}_{t},\\mathbf{y},t)$ are the conditional score and velocity estimates, respectively. One approach could be to directly model the conditional score or velocity estimates using a conditional iterative refinement model [Saharia et al., 2022a,b]. However, such approaches are problem-dependent, requiring expensive training pipelines to account for the lack of generalization across inverse problems. Additionally, such methods rely on the availability of paired $\\left(\\mathbf{x}_{t},\\pmb{y}\\right)$ measurements, which can be expensive to acquire. Alternatively, problem-agnostic methods leverage pre-trained unconditional iterative refinement models to estimate the conditional score or velocity fields and can generalize to different inverse problems without extra training. In this work, we restrict our discussion to the latter and discuss estimating conditional score/velocity fields next. ", "page_idx": 2}, {"type": "text", "text": "Estimating Conditional Score/Velocity from Pretrained Models: For diffusion models, approximating the conditional score follows directly from Bayes Rule, i.e. $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}|\\pmb{y})\\;\\stackrel{\\cdot}{\\approx}$ $s_{\\theta}(\\mathbf{x}_{t},t)+w_{t}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})$ where $w_{t}$ is the guidance weight (or temperature) of the distribution $p(\\mathbf{y}|\\mathbf{x}_{t})$ . Analogously for interpolants (or flows), Pokle et al. [2023] propose the conditional flow dynamics, ", "page_idx": 2}, {"type": "equation", "text": "$$\nb(\\mathbf{x}_{t},\\mathbf{y},t)\\approx b_{\\theta}(\\mathbf{x}_{t},t)+w_{t}\\frac{\\gamma_{t}}{\\alpha_{t}}\\Big[\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We include a formal proof for the result in Eqn. 4 from an interpolant perspective in Appendix A.1. Since the conditional score and velocity estimates require approximating the term $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ , we discuss its estimation next. ", "page_idx": 2}, {"type": "text", "text": "Estimation of the Noise Conditional Score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ : The noise conditional distribution $p(\\pmb{y}|\\mathbf{x}_{t})$ can be represented as $\\begin{array}{r}{p(\\mathbf{y}|\\mathbf{x}_{t})=\\int p(\\mathbf{y}|\\dot{\\mathbf{x}_{0}})p(\\mathbf{x}_{0}|\\dot{\\mathbf{x}_{t}})d\\mathbf{x}_{0}}\\end{array}$ . For problem-agnostic models, it is common to approximate the posterior $p\\big(\\mathbf{x}_{0}\\big|\\mathbf{x}_{t}\\big)$ using an unimodal Gaussian distribution [Chung et al., 2022a, Song et al., 2022]. In this work, we restrict our discussion to the posterior approximation in \u03a0GDM [Song et al., 2022] and its flow variant [Pokle et al., 2023] (named as \u03a0GFM in our work), $p(\\mathbf{x}_{0}|\\mathbf{x}_{t})\\approx\\mathcal{N}(\\bar{\\mathbf{x}}_{0},r_{t}^{2}I_{d})$ , which yields the following estimate of the conditional score: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})=\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}H^{\\top}(r_{t}^{2}H H^{\\top}+\\sigma_{y}^{2}I_{d})^{-1}(y-H\\hat{\\mathbf{x}}_{0}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ is the first-order Tweedie\u2019s moment estimate [Stein, 1981]. Our choice of using the \u03a0GDM approximation is motivated by its expressive posterior approximation $p(x_{0}|x_{t})$ compared to other methods such as DPS or MCG. This makes it an excellent starting point for low-budget sampling. ", "page_idx": 3}, {"type": "text", "text": "2.2 Conditional Conjugate Integrators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Conjugate Integrators The main idea in conjugate integrators [Pandey et al., 2024] is to project the diffusion dynamics in Eqn. 1 into another space where sampling might be more efficient. The projected diffusion dynamics can then be solved using any numerical ODE solver. On completion, the dynamics can be projected back to the original space to generate samples from the data distribution. To this end, Pandey et al. [2024] introduce an invertible time-dependent affine transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ Interestingly, conjugate samplers have theoretical connections to prior work in fast sampling for unconditional diffusion models [Song et al., 2021, Zhang and Chen, 2023, Lu et al., 2022]. We refer the readers to Pandey et al. [2024] for exact details. ", "page_idx": 3}, {"type": "text", "text": "2.2.1 Conjugate Integrators for Inverse Problems ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Next, we design conjugate integrators for linear inverse problems. For simplicity, we discuss noiseless inverse problems, $\\sigma_{y}=0$ , and defer the discussion of noisy inverse problems to Section 2.4. Furthermore, due to space constraints, we present our analysis for diffusion models and defer the discussion of flows to Appendix B. Lastly, without loss of generality, we assume the standard score network parameterization, $\\begin{array}{r}{\\pmb{s}_{\\theta}(\\mathbf{x}_{t},t)=\\dot{C_{\\mathrm{out}}}(t)\\pmb{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}\\end{array}$ where $C_{\\mathrm{out}}(t)$ is the notation from the score precondition defined in Karras et al. [2022]. ", "page_idx": 3}, {"type": "text", "text": "A straightforward way to define conditional conjugate integrators is to treat the score estimate $\\nabla_{\\mathbf{x}_{t}}\\log{p(\\mathbf{y}|\\mathbf{x}_{t})}$ as a black-box i.e., ignore the structure of the inverse problem. For this case, we formally specify the conjugate integrator formulation as, ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. (Extended \u03a0GDM) For the conditional diffusion dynamics defined in Eqn. 3, introducing a diffeomorphism, $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ , where, ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{t}=\\exp{\\left(\\int_{0}^{t}B_{s}-F_{s}d s\\right)},\\qquad\\Phi_{t}=-\\int_{0}^{t}\\frac12A_{s}G_{s}G_{s}^{\\top}C_{o u t}(s)d s,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "induces the following projected diffusion dynamics, ", "page_idx": 3}, {"type": "equation", "text": "$$\nd\\hat{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\hat{\\mathbf{x}}_{t}d t+d\\Phi_{t}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)-\\frac{w_{t}r_{t}^{-2}}{2}G_{t}G_{t}^{\\top}\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{0})d t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H^{\\dag}\\,=\\,H^{\\top}(H H^{\\top})^{-1}$ and $P\\,=\\,H^{\\top}(H H^{\\top})^{-1}H$ represent the pseudoinverse and the orthogonal projector operators for the degradation operator $\\pmb{H}$ . (Proof in Appendix A.2) ", "page_idx": 3}, {"type": "text", "text": "Similar to Pandey et al. [2024], the matrix $B_{t}$ is a design choice. We refer to the formulation in Eqn. 7 as Extended \u03a0GDM since for $B_{t}=0$ , the ODE in Eqn. 7 becomes equivalent to the \u03a0GDM formulation proposed in Song et al. [2022]. This is because, for $B_{t}=0$ , Conjugate Integrators are equivalent to DDIM [Song et al., 2021] (See Pandey et al. [2024] for proof). Therefore, the projected diffusion dynamics in Eqn. 7 already present a more generic framework for designing samplers for inverse problems over \u03a0GDM. In this work, we only explore the parameterization in Eqn. 7 for $B_{t}=0$ and hence refer to it simply as \u03a0GDM (analogously \u03a0GFM for flows; see Appendix B). ", "page_idx": 3}, {"type": "text", "text": "One characteristic of the formulation in Eqn. 7 is the black-box nature of the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ . However, the inherent linearity in the conditional score can be used to design better conditioned (more on this in Section 2.3) conjugate integrators, which we illustrate formally in the form of the following result. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. (Conjugate \u03a0GDM) Given a noiseless linear inverse problem with $\\sigma_{y}=0$ , a design matrix $B:[0,1]\\rightarrow\\mathbb{R}^{d\\times d}$ , and the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ approximated using Eqn. 5, introducing the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}B_{s}-\\Big(F_{s}+\\frac{w_{s}r_{s}^{-2}}{2\\mu_{s}^{2}}G_{s}G_{s}^{\\top}P\\Big)d s\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "induces the following projected diffusion dynamics: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\bar{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\bar{\\mathbf{x}}_{t}d t+d\\Phi_{y}y+d\\Phi_{s}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\mathbf{x}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{0})\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\exp(.)$ denotes the matrix exponential, $H^{\\dagger}$ , and $_{P}$ are the pseudoinverse and projector operators (as defined previously). Proof in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "In this case, the coefficients $\\Phi_{y},\\,\\Phi_{j}$ , and $\\Phi_{s}$ depend on time $t$ and the degradation operator $\\pmb{H}$ (See Appendix A.3 for full definitions). Intuitively, by including information about the degradation operator $\\pmb{H}$ and the guidance scale in the transformation $\\boldsymbol{A}_{t}$ in Eqn. 8, we incorporate the specific structure of the inverse problem in the sampler design, which can have several advantages (more on this in Section 2.3). Moreover, the matrix $B_{t}$ is a design choice of our parameterization (we will discuss exact choices in Section 2.2.2). We refer to this parameterization as $C$ -\u03a0GDM (analogously $C_{}$ -\u03a0GFM for flows; see Appendix $B$ ). In this work, we restrict our discussion to this parameterization and discuss some practical and theoretical aspects next. ", "page_idx": 4}, {"type": "text", "text": "2.2.2 Practical Design Choices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Choice of Diffusions and Flows: While our proposed integrators are applicable to generic diffusion processes [Dockhorn et al., 2022, Pandey and Mandt, 2023] and flows [Ma et al., 2024], we r\u221aestrict follow-up discussion to VP-SDE [Song et al., 2020] diffusion for which $\\begin{array}{r}{\\dot{\\bf F}_{t}=-\\frac{1}{2}\\beta_{t}{\\cal I}_{d},\\dot{\\bf G}_{t}=\\sqrt{\\beta_{t}}{\\cal I}_{d}}\\end{array}$ and OT-flows [Liu et al., 20223, Lipman et al., 2023] for which $\\alpha_{t}=t,\\gamma_{t}=\\bar{1}-t$ . For our score network parameterization, we set $C_{\\mathrm{out}}(t)=-1/\\sigma_{t}$ , corresponding to the standard $\\epsilon$ -prediction $[\\mathrm{Ho}$ et al., 2020, Song et al., 2020] parameterization in diffusion models. ", "page_idx": 4}, {"type": "text", "text": "Choice of $B_{t}$ : Similar to Pandey et al. [2024], we set $B_{t}=\\lambda I_{d}$ , where $\\lambda$ is a time-invariant scalar hyperparameter tuned during inference for optimal sample quality. ", "page_idx": 4}, {"type": "text", "text": "Choice of $w_{t}$ : Similar to prior work [Song et al., 2022, Pokle et al., 2023], we use an adaptive guidance weight schedule. For diffusion models, we use $w_{t}=w\\mu_{t}^{2}r_{t}^{2}$ where $\\begin{array}{r}{r_{t}^{2}=\\frac{\\sigma_{t}^{2}}{\\mu_{t}^{2}+\\sigma_{t}^{2}}}\\end{array}$ Analogously, for flows, we set $w_{t}=w\\alpha_{t}^{2}r_{t}^{2}$ where $\\begin{array}{r}{r_{t}^{2}=\\frac{\\gamma_{t}^{2}}{\\alpha_{t}^{2}+\\gamma_{t}^{2}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Having an extra multiplicative factor of $\\mu_{t}^{2}$ (for VP-SDE) or $\\alpha_{t}^{2}$ (for flows) stabilizes the numerical computation of coefficients in Eqn. 9 before sampling. We tune the static guidance weight $w$ during inference for optimal sample quality. ", "page_idx": 4}, {"type": "text", "text": "Choice of Start Time: Given a degradation output $\\textit{\\textbf{y}}$ , it is common to start diffusion or flow sampling at $\\tau<T$ or $\\tau>0$ , respectively [Chung et al., 2022b, Song et al., 2022, Pokle et al., 2023]. Consequently, we initialize the diffusion sampling process as $\\mathbf{x}_{\\tau}=\\bar{\\mu}_{\\tau}\\pmb{H}^{\\dagger}\\pmb{y}+\\sigma_{\\tau}\\mathbf{z}$ . Analogously for flows, we initialize sampling at $\\mathbf{x}_{\\tau}=\\alpha_{\\tau}H^{\\dagger}\\mathbf{\\dot{y}}+\\gamma_{\\tau}\\mathbf{z}$ . ", "page_idx": 4}, {"type": "text", "text": "Choice of the ODE Solver: Unless specified otherwise, we use the Euler discretization scheme for C-\u03a0G(D/F)M samplers. ", "page_idx": 4}, {"type": "text", "text": "We illustrate a generic C-\u03a0GDM sampling routine in Algorithm 1 and include additional implementation details in Appendix D. Next, we present some theoretical aspects of our proposed method. ", "page_idx": 4}, {"type": "text", "text": "2.3 Theoretical Aspects ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the simplifications in Section 2.2.2, the transformation $A_{t}$ in Eqn. 8 simplifies to: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s I_{d}-\\frac{w}{2}\\Big(\\int_{0}^{t}\\beta_{s}d s\\Big){\\cal P}\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P=H^{\\top}(H H^{\\top})^{-1}H$ is an orthogonal projection operator. ", "page_idx": 4}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/cd7af07f8ff0e8861ec48c3c3051978db9e07c3ce8438cb46651dab4cd951c2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Computing $\\boldsymbol{A}_{t}$ : While computing the matrix exponential in Eqn. 10 might seem non-trivial, it has several interesting properties that make it tractable to compute. More specifically, the matrix exponential in Eqn. 10 can be simplified as (Proof in Appendix A.4), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{4}_{t}=\\exp(\\kappa_{1}(t))\\left[I_{d}+(\\exp(\\kappa_{2}(t))-1)P\\right],\\quad\\kappa_{1}(t)=\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s,\\quad\\kappa_{2}(t)=-\\frac{w}{2}\\int_{0}^{t}\\beta_{s}d s,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\exp(.)$ in Eqn. 11 represents the scalar exponential. Furthermore, the integrals in Eqn. 11 are trivial to compute analytically or numerically, making $A_{t}$ easier to compute. Moreover, $A_{t}^{-1}$ can also be compactly represented as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}_{t}^{-1}=\\exp(-\\kappa_{1}(t))\\Big[\\pmb{I}_{d}+(\\exp(-\\kappa_{2}(t))-1)\\pmb{P}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and is also tractable to compute. Due to the tractability of $\\boldsymbol{A}_{t}$ and $A_{t}^{-1}$ , the projected diffusion dynamics in C-\u03a0GDM are straightforward to simulate numerically. ", "page_idx": 5}, {"type": "text", "text": "Intuition behind $\\boldsymbol{A}_{t}$ : Next, we analyze several theoretical properties of the transformation matrix $\\scriptstyle A_{t}$ in Eqn. 11. More specifically, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}=\\exp(\\kappa_{1}(t))\\Big[\\mathbf{x}_{t}-(1-\\exp(\\kappa_{2}(t)))P\\mathbf{x}_{t}\\Big],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $P=H^{\\top}(H H^{\\top})^{-1}H$ is an orthogonal projector, the matrix ${\\cal I}_{d}-{\\cal P}$ is also an orthogonal projector which projects any vector $\\pmb{v}$ in the nullspace of $_{P}$ . Therefore, we can decompose the state $\\mathbf{x}_{t}$ into two orthogonal components ${\\bf x}_{t}=P{\\bf x}_{t}+({\\cal I}_{d}-P){\\bf x}_{t}$ . Plugging this form in Eqn. 13, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}_{t}=\\mathrm{exp}(\\kappa_{1}(t))\\Big[(I_{d}-P)\\mathbf{x}_{t}+\\mathrm{exp}(\\kappa_{2}(t))P\\mathbf{x}_{t}\\Big],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Intuitively, near $t=T$ (i.e., at the start of reverse diffusion sampling), for a large static guidance weight $w$ , $\\exp(\\kappa_{2}(t))\\to0$ . In this limit, from eqn. 14, $\\bar{\\mathbf{x}}_{t}\\approx(I_{d}-P)\\mathbf{x}_{t}$ . This implies that for a large guidance weight $w$ , the diffusion dynamics are projected into the nullspace of the projection operator $_{P}$ . Intuitively, for an inverse problem like superresolution, this implies that near the start of the diffusion process, the projected diffusion dynamics correspond to the denoising of the highfrequency details missing in $\\mathbf{\\mathit{Px}}_{t}$ . This is because the projector operation, $P\\mathbf{x}_{t}=H^{\\dagger}H\\mathbf{x}_{t}$ can be interpreted as the pseudoinverse of the noisy degraded state $\\mathbf{x}_{t}$ , and, therefore, $(I_{d}-P)\\mathbf{x}_{t}$ represents the high-frequency details missing from the signal component in $\\mathbf{\\mathit{Px}}_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "Moreover, near $t=0$ (i.e., near the end of reverse diffusion sampling), assuming the guidance weight $w$ is not too large, both coefficients $\\exp(\\kappa_{1}(t))$ and $\\exp(\\kappa_{2}(t)\\bar{)}\\to1$ , which implies $\\bar{\\mathbf{x}}_{t}\\approx\\mathbf{x}_{t}$ . This implies that near $t=0$ , diffusion happens in the original space, which can prevent over-sharpening artifacts towards the end of sampling. Therefore, we hypothesize that a large $w$ can also lead to over-sharpened results near the end of sampling, resulting in artifacts in the generated samples. Therefore, introducing the projection $\\boldsymbol{A}_{t}$ as defined in Eqn. 10, introduces a tradeoff in the choice of $w$ to control for sample quality. Lastly, since the parameter $\\lambda$ controls the magnitude of $\\bar{\\bf x}_{t}$ , it exhibits a similar tradeoff. Indeed, we will empirically demonstrate these tradeoffs in Section 3.3. While our discussion has been limited to diffusion models, a similar theoretical intuition also holds for flows (See Appendix B for proof). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "2.4 Extension to Noisy and Non-Linear Inverse Problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While our discussion has been primarily in the context of noiseless linear inverse problems, the conditional Conjugate Integrator framework can also be extended to develop samplers for noisy linear and non-linear inverse problems. We provide a more detailed explanation for the same in App. C. ", "page_idx": 6}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we empirically demonstrate that our proposed samplers C-\u03a0GDM/GFM outperform recent baselines on linear image restoration tasks regarding sampling speed vs. quality tradeoff. We then present ablation experiments highlighting the key parameters of our samplers. Lastly, we present design choices for solving noisy and non-linear inverse problems using our proposed framework. ", "page_idx": 6}, {"type": "text", "text": "Models and Dataset: For diffusion models, we utilize an unconditional pre-trained ImageNet [Deng et al., 2009] checkpoint at $256\\!\\times\\!256$ resolution from OpenAI [Dhariwal and Nichol, 2021]3. For evaluations on the FFHQ dataset Karras et al. [2019], we use a pre-trained checkpoint from Choi et al. [2021] also at $256\\!\\times\\!256$ resolution. For flow model comparisons, we utilize three publicly available model checkpoints from Liu et al. $[20223]^{4}$ , trained on the AFHQ-Cat [Choi et al., 2020], LSUN-Bedroom Yu et al. [2015], and CelebA-HQ [Karras et al., 2018] datasets. Each flow model was trained at a pixel resolution of $256\\times256$ . For diffusion models, we conduct evaluations on a 1k subset of the evaluation set. For flows, we conduct evaluations on the entire validation set. ", "page_idx": 6}, {"type": "text", "text": "Tasks and Metrics: We evaluate our samplers qualitatively (see Figure 2) and quantitatively on three challenging linear inverse problems under the noiseless setting. Firstly, we test Image Super-Resolution, enhancing images from bicubic-downsampled $64\\times64$ pixels to $256\\times256$ pixels. Secondly, we assess Image Inpainting performance on images with a fixed free-form center mask. Lastly, we evaluate our samplers on Gaussian Deblurring, applying a Gaussian kernel with $\\sigma=3.0$ across a $61\\times61$ window. We evaluate the performance of each task based on three perceptual metrics: FID [Heusel et al., 2017], KID [Bin\u00b4kowski et al., 2018] and LPIPS [Zhang et al., 2018]. ", "page_idx": 6}, {"type": "text", "text": "Methods and Baselines: We assess the sample quality of our proposed C-\u03a0GDM and C-\u03a0GFM samplers using 5, 10, and 20 sampling steps (denoted as Number of Function Evaluations (NFE)). We conduct an extensive search to optimize the parameters $w$ , $\\lambda$ and $\\tau$ to identify the best-performing configuration based on sample quality. For diffusion baselines, we include DDRM [Kawar et al., 2022], DPS [Chung et al., 2022a], and \u03a0GDM [Song et al., 2022]. As recommended for DPS [Chung et al., 2022a], we use $\\mathrm{NFE=}1000$ for all tasks. For DDRM, we adhere to the original implementation and run it with $\\eta_{b}\\,=\\,1.0$ and $\\eta=0.85$ at $\\mathrm{NFE}{=}20$ . We test our implementation of \u03a0GDM (see Section 2.2), with NFE values of 5, 10, and 20 and use the recommended guidance schedule of $w_{t}=r_{t}^{2}$ across all tasks. For flow models, we consider the recently proposed method inspired by \u03a0GDM running on OT-ODE path by Pokle et al. [2023] (which we refer to as \u03a0GFM; see Appendix B), and similarly run it with NFE values of 5, 10, and 20. We optimize all baselines by conducting an extensive grid search over $w$ and $\\tau$ for the best performance (in terms of sample quality). ", "page_idx": 6}, {"type": "text", "text": "3.1 Quantitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the results of our method applied to inverse problems in Table 1, specifically using the CelebA-HQ dataset for flow-based models and the ImageNet dataset for diffusion-based models. For a comprehensive review of additional results across different datasets, please refer to Appendix E. Our method consistently surpasses other approaches across all sampling budgets (indicated by NFE) for the inpainting task. Similarly, our flow-based sampler (C-\u03a0GFM) exhibits superior perceptual quality for image super-resolution at NFEs of 5 and 10. The \u03a0GFM model only reaches comparable performance at higher NFEs. Remarkably, our diffusion-based sampler C-\u03a0GDM outperforms all baselines across the entire range of NFEs. Notably, C-\u03a0GDM outperforms competing baselines requiring 20-1000 NFEs in just 5 sampling steps on the challenging ImageNet dataset, demonstrating a significant speedup in sampling speed while preserving sample quality. A similar pattern is observed in the image deblurring task, where the performance of \u03a0GDM/\u03a0GFM approaches that of our method only when the NFE is increased to 20 steps. ", "page_idx": 6}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/8f8cb4e853419c94cd1e60720db0d633bf102d96a872a34ca583771ae6e6bc67.jpg", "table_caption": ["Table 1: Comparison between Conjugate $\\overline{{\\Pi\\mathrm{G}(\\mathrm{D/F})\\mathrm{M}}}$ and other baselines for noiseless linear inverse problems. Top: Flow models (CelebA-HQ) and Bottom: Diffusion Models (ImageNet). Entries in bold show the best performance for a given sampling budget. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Interestingly, we observe a plateau in performance improvements at $\\mathrm{NFE}{=}20$ for both super-resolution and deblurring tasks using our method. This suggests that while our method efficiently utilizes the iterative model under a deterministic path with an Euler solver, further enhancements in performance, particularly at higher NFEs, might require integrating stochastic sampling techniques or more advanced solvers. This potential next step could unlock further gains from our approach in complex image processing tasks. ", "page_idx": 7}, {"type": "text", "text": "3.2 Qualitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 2 presents a qualitative comparison between our proposed method and the \u03a0G(D/F)M baseline. The inpainting results in the first column reveal that \u03a0GFM tends to introduce gray artifacts within the inpainted areas. This issue may stem from the initialization of the parameter $\\tau$ ; optimal performance is achieved when $\\tau\\geq0.2$ , as established during our parameter tuning phase and corroborated by Pokle et al. [2023]. Consequently, insufficient NFE means \u03a0GFM cannot effectively eliminate the artifacts associated with the inpainting mask in our experiments. For image super-resolution, our method excels in restoring fine details, particularly evident in high-frequency image components such as human hair and wheat ears. Similarly, for the deblurring task, our method qualitatively outperforms the baseline, especially in mitigating the over-smoothing artifacts (Figure 2, last column). Additional examples are provided in Appendix E.4. ", "page_idx": 7}, {"type": "text", "text": "3.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we further explore the impact of the hyperparameters $w,\\,\\lambda$ , and $\\tau$ , which were identified during our tuning phase and link to the theoretical insights discussed in Section 2.3. We recognize that $\\tau$ is particularly task-specific and relatively straightforward to adjust. For instance, tasks such as inpainting require a smaller $\\tau$ to prevent masking artifacts, whereas tasks like superresolution or deblurring benefit from a larger $\\tau$ to ensure effective initialization. Consequently, our discussion will primarily focus on the effects of $w$ and $\\lambda$ . Figure 3 illustrates the impact of varying $w$ and $\\lambda$ on sample quality for image super-resolution on the CelebA-HQ and ImageNet datasets. ", "page_idx": 7}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/d720ee79dff65dedb8636c709401deb659577ce898b463e1a07fd930c54bdd0e.jpg", "img_caption": ["Figure 2: Qualitative comparison between C-\u03a0G(D/F)M and \u03a0G(D/F)M baselines on five different datasets. (a, b, c) Inpainting, De-blurring, and $4\\mathtt{x}$ Super-resolution with C-\u03a0GFM, respectively. (d,e) ${4\\bf{x}}$ Image Super-resolution and De-blurring with C-\u03a0GDM, respectively. ( $\\sigma_{y}=0$ , $\\mathrm{NFE}{=}51$ ) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "From Figure 3 we make the following observations. Firstly, for both C-\u03a0GDM and C-\u03a0GFM samplers, we observe that the optimal value of $\\lambda$ can differ from $\\lambda\\:=\\:0$ . This illustrates the usefulness of parameterizing $B_{t}$ in our sampler design. On the contrary, \u03a0GDM or \u03a0GFM samplers do not have this flexibility and, therefore, yield sub-optimal sample quality at different sampling budgets. Secondly, we observe that deviating from the optimal $\\lambda$ can lead to degradation in sample quality. More specifically, we observed that deviating from our tuned value of $\\lambda$ leads to either over-sharpening artifacts or blurry samples (See Figs. 7, 11). This is intuitive since $\\lambda$ controls the scale of the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ (see Eqn. 14) and thus plays a significant role in conditioning the projected diffusion dynamics. We observe a similar tradeoff on varying the static guidance weight $w$ where a large magnitude of $w$ can lead to over-sharpened artifacts while a very small guidance weight can lead to blurry samples (See Figs. 6, 10). These empirical observations are consistent with our theoretical analysis in Section 2.3, confirming our theoretical intuition on the role of the sampler parameters $w$ and $\\lambda$ . ", "page_idx": 8}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fast Unconditional Sampling: Recent research has significantly advanced the efficiency of the sampling process in unconditional diffusion/flow models [Song et al., 2020, Lipman et al., 2023, Manduchi et al., 2024]. One line of research involves designing efficient diffusion models to improve sampling by design [Karras et al., 2022, Dockhorn et al., 2022, Pandey and Mandt, 2023, Song et al., 2023]. Since our treatment of conditional Conjugate Integrators is quite generic, our method is readily compatible with most advancements in diffusion model design. Another line of work focuses on distilling a student model from a teacher model, enabling sampling in even a single step [Salimans and Ho, 2022, Meng et al., 2023, Sauer et al., 2024]. However, since these methods require expensive re-training, there has been a significant interest in the development of fast samplers applicable to pretrained diffusion/flow models [Liu et al., 2022, Pandey et al., 2024, Shaul et al., 2024, Zhang and Chen, 2023, Lu et al., 2022, Song et al., 2021, Gonzalez et al., 2023]. Our work falls under the latter line of research, where we develop fast conditional samplers that can be applied to pretrained diffusion models. ", "page_idx": 8}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/d3d41981f1d8473b233c48495df5f6677fe90b9050c56b42ed96d870d5511500.jpg", "img_caption": ["Figure 3: Impact of $\\lambda$ and $w$ on sampling quality. Red curves and labels represent the LPIPS scores, while blue curves and labels indicate the FID scores. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Conditional Iterative Refinement Models have become prevalent for tasks requiring controlled generation. These models often involve training specialized conditional diffusion models [Saharia et al., 2022b, Yang and Mandt, 2023, Kong et al., 2021, Pandey et al., 2022, Preechakul et al., 2022, Rombach et al., 2022, Podell et al., 2023, Ramesh et al., 2022, Peebles and Xie, 2023, Ma et al., 2024, Esser et al., 2024, Chen et al., 2023] and may incorporate classifier-free guidance [Ho and Salimans, 2021] or classifier guidance [Dhariwal and Nichol, 2021, Song et al., 2020] for conditional sampling. These approaches have also spurred research into solving inverse problems related to various image degradation transformations, such as inpainting and super-resolution [Kawar et al., 2022, Chung et al., 2022a, Song et al., 2022, Mardani et al., 2023, Pokle et al., 2023]. Although these methods demonstrate promising outcomes, they are typically bottlenecked by a costly sampling process, emphasizing the need for a fast sampler to address inverse problems efficiently. Recent work Xu et al. [2024] employs a consistency model Song et al. [2023] to enhance posterior approximation, but incorporating an additional model may deviate from our proposal of using a single pre-trained model. DPM-Solver $^{++}$ [Lu et al., 2023] also tackles the problem of accelerating guided sampling in diffusion models. However, unlike [Lu et al., 2023], we incorporate the structure of the inverse problem in the sampler design. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a generic framework for designing samplers for accelerating guided sampling in iterative refinement models. In this work, we explore a specific parameterization of this framework, which incorporates the structure of the inverse problem in sampler design. We provide a theoretical intuition behind our design choices and empirically justify its effectiveness in solving linear inverse problems in as few as 5 sampling steps compared to 20-1000 NFEs required by competing baselines. While our method can serve as an important step toward designing fast-guided samplers, there are several important future directions. Firstly, our parameterization of the transform $\\boldsymbol{A}_{t}$ can be more expressive by learning it directly during the sampling stage. Secondly, in this work, we consider inverse problems with a known degradation operator. Extending our framework for solving blind inverse problems could be an important research direction. Lastly, it would be interesting to adapt our solvers to techniques for solving inverse problems in latent diffusion models [Rout et al., 2024] to enhance sampling efficiency further. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact: While our work has the potential to make synthetic data generation accessible, the techniques presented in this work should be used responsibly. Moreover, despite good sample quality in a limited sampling budget, restoration can sometimes lead to artifacts in the generated sample which can be undesirable in some domains like medical image analysis. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank Justus Will for providing valuable feedback on our manuscript. KP acknowledges support from the HPI Research Center in Machine Learning and Data Science at UC Irvine. SM acknowledges support from the National Science Foundation (NSF) under an NSF CAREER Award IIS-2047418 and IIS-2007719, the NSF LEAP Center, by the Department of Energy under grant DE-SC0022331, the IARPA WRIVA program, the Hasso Plattner Research Center at UCI, and by gifts from Qualcomm and Disney. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015. ", "page_idx": 10}, {"type": "text", "text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\cdot$ PqvMRDCJT9t.   \nMichael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR 2023 Conference, 2023.   \nKushagra Pandey, Maja Rudolph, and Stephan Mandt. Efficient integrators for diffusion generative models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot^{=}$ qA4foxO5Gf.   \nNeta Shaul, Juan Perez, Ricky TQ Chen, Ali Thabet, Albert Pumarola, and Yaron Lipman. Bespoke solvers for generative flow models. In The Twelfth International Conference on Learning Representations, 2024.   \nAxel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024.   \nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id $\\mathbf{\\mu=}$ TIdIXIpzhoI.   \nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=Loek7hfb46P.   \nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id $\\equiv$ St1giarCHLP.   \nJiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2022.   \nHyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2022a.   \nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661\u20131674, 2011. doi: 10.1162/NECO_a_00142.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022a.   \nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022b.   \nAshwini Pokle, Matthew J Muckley, Ricky TQ Chen, and Brian Karrer. Training-free linear image inversion via flows. arXiv preprint arXiv:2310.04432, 2023.   \nCharles M Stein. Estimation of the mean of a multivariate normal distribution. The annals of Statistics, pages 1135\u20131151, 1981.   \nTim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with criticallydamped langevin diffusion. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ CzceR82CYc.   \nKushagra Pandey and Stephan Mandt. Generative diffusions in augmented spaces: A complete recipe, 2023.   \nNanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boff,i Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers, 2024.   \nXingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 20223.   \nHyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction, 2022b.   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \nJooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models, 2021.   \nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a largescale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.   \nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \nMiko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In International Conference on Learning Representations, 2018.   \nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \nBahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022.   \nLaura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina D\u00e4ubener, Sophie Fellenz, Asja Fischer, Thomas G\u00e4rtner, Matthias Kirchler, Marius Kloft, et al. On the challenges and opportunities in generative ai. arXiv preprint arXiv:2403.00025, 2024.   \nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, pages 32211\u201332252, 2023.   \nChenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \nLuping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2022.   \nMartin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, Hatem Hajri, Nader Masmoudi, et al. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. Advances in Neural Information Processing Systems, 36, 2023.   \nRuihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models. Advances in Neural Information Processing Systems, 36, 2023.   \nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.   \nKushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. Transactions on Machine Learning Research, 2022.   \nKonpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629, 2022.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.   \nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents, 2022. URL https://arxiv.org/abs/2204. 06125.   \nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4195\u20134205, October 2023.   \nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.   \nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.   \nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \nMorteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2023.   \nTongda Xu, Ziran Zhu, Dailan He, Yuanyuan Wang, Ming Sun, Ning Li, Hongwei Qin, Yan Wang, Jingjing Liu, and Ya-Qin Zhang. Consistency models improve diffusion inverse solvers. arXiv preprint arXiv:2403.12063, 2024.   \nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver $^{++}$ : Fast solver for guided sampling of diffusion probabilistic models, 2023.   \nLitu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \nMichael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \nRicky T. Q. Chen. torchdiffeq, 2018. URL https://github.com/rtqichen/torchdiffeq.   \nJ.R. Dormand and P.J. Prince. A family of embedded runge-kutta formulae. Journal of Computational and Applied Mathematics, 6(1):19\u201326, 1980. ISSN 0377-0427. doi: https://doi.org/10.1016/ 0771-050X(80)90013-3. URL https://www.sciencedirect.com/science/article/pii/ 0771050X80900133.   \nAlexia Jolicoeur-Martineau, R\u00e9mi Pich\u00e9-Taillefer, Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and improved sampling for image generation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $=$ eLfqMl3z3lq.   \nAnton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance metrics for generative models in pytorch, 2020. URL https: //github.com/toshas/torch-fidelity. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.   \nDavid Minnen, Johannes Ball\u00e9, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_ files/paper/2018/file/53edebc543333dfbf7c5933af792c9c4-Paper.pdf. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction ", "page_idx": 14}, {"type": "text", "text": "2 Fast Samplers for Inverse Problems using Diffusions/Flows. 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "2.1 Background and Problem Statement 3   \n2.2 Conditional Conjugate Integrators . . . 4   \n2.2.1 Conjugate Integrators for Inverse Problems . . 4   \n2.2.2 Practical Design Choices . . . . 5   \n2.3 Theoretical Aspects . . . . 5   \n2.4 Extension to Noisy and Non-Linear Inverse Problems . . 7 ", "page_idx": 14}, {"type": "text", "text": "3 Experiments 7 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3.1 Quantitative Results 7   \n3.2 Qualitative Results 8   \n3.3 Ablation Studies . . 8 ", "page_idx": 14}, {"type": "text", "text": "4 Related Works 9 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "5 Discussion 10 ", "page_idx": 14}, {"type": "text", "text": "A Proofs 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Conditional flow dynamics 16   \nA.2 Proof of Proposition 1 . . . . 17   \nA.3 Proof of Proposition 2 . . . . 18   \nA.4 Simplification in Eqn. 11 . . . 20   \nA.5 Proof of Proposition for Solving Noisy Inverse Problems . 21 ", "page_idx": 14}, {"type": "text", "text": "B Conditional Conjugate Integrators: Flows 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Background 22   \nB.2 Conditional Conjugate Integrators for Flows . . 22   \nB.2.1 Conjugate-\u03a0GFM (C-\u03a0GFM) . . . 22 ", "page_idx": 14}, {"type": "text", "text": "C Extension to Noisy and Non-linear Inverse Problems 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D Implementation Details 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 C-\u03a0GDM: Practical Aspects . 24   \nD.1.1 VP-SDE . . 24   \nD.1.2 C-\u03a0GDM - Simplified Expressions . . 24   \nD.2 C-\u03a0GFM: Practical Aspects . . 25   \nD.2.1 OT-Flows . . 25   \nD.2.2 C-\u03a0GFM: Simplified Expressions . . 25   \nD.3 Coefficient Computation 26   \nD.4 Choice of Numerical Solver 27   \nD.5 Timestep Selection during Sampling . . . 27   \nD.6 Last-Step Denoising . . . . 27   \nD.7 Evaluation Metrics 27   \nD.8 Baseline Hyperparameters 27 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Additional Baseline Comparisons 27   \nE.2 Comparison of Perceptual vs Recovery Metrics 28   \nE.3 Traversing the Recovery vs Perceptual trade-off 28   \nE.4 Qualitative Results . . 29 ", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Conditional flow dynamics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Given a one-sided interpolant, $\\mathbf{x}_{t}\\,=\\,\\alpha_{t}\\mathbf{x}_{1}\\,+\\,\\gamma_{t}\\mathbf{z},\\quad\\mathbf{x}_{1}\\,\\sim\\,p_{\\mathrm{data}},\\ \\ \\mathbf{z}\\,\\sim\\mathcal{N}(0,I)$ satisfying regularity conditions as stated in [Albergo et al., 2023], and a degraded signal y generated using Eqn. 2, the conditional velocity field $\\pmb{b}_{(\\mathbf{X}_{t},\\mathbf{y},t)}$ can be approximately estimated from the unconditional velocity field $\\boldsymbol{b}_{\\theta}(\\mathbf{x}_{t},t)$ as [Pokle et al., 2023], ", "page_idx": 15}, {"type": "equation", "text": "$$\nb(\\mathbf{x}_{t},\\mathbf{y},t)\\approx b_{\\theta}(\\mathbf{x}_{t},t)+w_{t}\\frac{\\gamma_{t}}{\\alpha_{t}}\\Big[\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $w_{t}$ represents a time-dependent scalar guidance schedule and $\\dot{\\sigma}_{t},\\dot{\\alpha}_{t}$ represent the first-order time derivatives of $\\sigma_{t}$ and $\\alpha_{t}$ , respectively. Our proof consists of two parts: Firstly, we establish the connection between the unconditional velocity field $\\boldsymbol{b}(\\mathbf{x}_{t},t)$ for the one-sided interpolant and the score function $s(\\mathbf{x}_{t},t)$ associated with the marginal distribution $p(\\mathbf{x}_{t})$ . Secondly, we use this connection to estimate the conditional velocity $\\pmb{b}(\\mathbf{x}_{t},\\pmb{y},t)$ in terms of $\\boldsymbol{b}(\\mathbf{x}_{t},t)$ to establish the required result. ", "page_idx": 15}, {"type": "text", "text": "Connection between $\\boldsymbol{b}(\\mathbf{x}_{t},t)$ and $\\boldsymbol{s}(\\mathbf{x}_{t},t)$ : For the one-sided interpolant, by definition, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{1}+\\gamma_{t}\\mathbf{z}\\quad\\mathbf{x}_{1}\\sim p_{\\mathrm{data}},\\ \\ \\mathbf{z}\\sim\\mathcal{N}(0,I)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking the expectation w.r.t $p(\\mathbf{x}_{1},\\mathbf{z})$ on both sides conditioned on the noisy state $\\mathbf{x}_{t}$ , we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\alpha_{t}\\mathbb{E}[\\mathbf{x}_{1}|\\mathbf{x}_{t}]+\\gamma_{t}\\mathbb{E}[\\mathbf{z}|\\mathbf{x}_{t}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, we have the following result from Albergo et al. [2023], ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{z}|\\mathbf{x}_{t}]=-\\gamma_{t}\\mathbf{\\mathcal{s}}(\\mathbf{x}_{t},t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $s(\\mathbf{x}_{t},t)$ represents the score function. From Eqns. 17, 18, it follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf x}_{1}|{\\bf x}_{t}]=\\frac{1}{\\alpha_{t}}\\Big[{\\bf x}_{t}+\\gamma_{t}^{2}s({\\bf x}_{t},t)\\Big]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Intuitively, the above result represents Tweedie\u2019s estimate [Stein, 1981] for estimating $\\hat{\\mathbf{x}}_{1}=\\mathbb{E}[\\mathbf{x}_{1}|\\mathbf{x}_{t}]$ in the context of one-sided stochastic interpolants. Next, the one-sided interpolant also induces an unconditional velocity field specified as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{b}(\\mathbf{x}_{t},t)=\\dot{\\alpha_{t}}\\mathbb{E}[\\mathbf{x}_{1}|\\mathbf{x}_{t}]+\\dot{\\gamma_{t}}\\mathbb{E}[\\mathbf{z}|\\mathbf{x}_{t}]}\\\\ &{~~~~~~~~~~~~=\\dot{\\alpha_{t}}\\mathbb{E}[\\mathbf{x}_{1}|\\mathbf{x}_{t}]-\\dot{\\gamma_{t}}\\gamma_{t}s(\\mathbf{x}_{t},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\dot{\\gamma}_{t},\\,\\dot{\\alpha}_{t}$ represent the first-order time derivatives of $\\gamma_{t}$ and $\\alpha_{t}$ , respectively. Substituting the result from Eqn. 19 into Eqn. 20, we have the following result, ", "page_idx": 15}, {"type": "equation", "text": "$$\nb({\\bf x}_{t},t)=\\frac{\\dot{\\alpha_{t}}}{\\alpha_{t}}{\\bf x}_{t}+\\frac{\\gamma_{t}}{\\alpha_{t}}\\Big[\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big]s({\\bf x}_{t},t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This concludes the first part of the proof. ", "page_idx": 16}, {"type": "text", "text": "Estimating the conditional velocity $\\mathbf{\\boldsymbol{b}}(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{y}},t)$ in terms of $\\boldsymbol{b}(\\mathbf{x}_{t},t)$ . For transport conditioned on $\\mathbf{y}$ , the conditional velocity can be expressed as (following the result in Eqn. 21): ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{b(\\mathbf{x}_{t},y,t)={\\frac{{\\dot{\\alpha_{t}}}}{\\alpha_{t}}}\\mathbf{x}_{t}+{\\frac{\\gamma_{t}}{\\alpha_{t}}}\\Bigl[\\gamma_{t}{\\dot{\\alpha}}_{t}-{\\dot{\\gamma}}_{t}\\alpha_{t}\\Bigr]s(\\mathbf{x}_{t},y,t)}\\\\ &{\\qquad\\qquad={\\frac{{\\dot{\\alpha_{t}}}}{\\alpha_{t}}}\\mathbf{x}_{t}+{\\frac{\\gamma_{t}}{\\alpha_{t}}}\\Bigl[\\gamma_{t}{\\dot{\\alpha}}_{t}-{\\dot{\\gamma}}_{t}\\alpha_{t}\\Bigr]\\Bigl[s(\\mathbf{x}_{t},t)+w_{t}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\\Bigr]}\\\\ &{\\qquad={\\frac{{\\dot{\\alpha_{t}}}}{\\alpha_{t}}}\\mathbf{x}_{t}+{\\frac{\\gamma_{t}}{\\alpha_{t}}}\\Bigl[\\gamma_{t}{\\dot{\\alpha}}_{t}-{\\dot{\\gamma}}_{t}\\alpha_{t}\\Bigr]s(\\mathbf{x}_{t},t)+w_{t}{\\frac{\\gamma_{t}}{\\alpha_{t}}}\\Bigl[\\gamma_{t}{\\dot{\\alpha}}_{t}-{\\dot{\\gamma}}_{t}\\alpha_{t}\\Bigr]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})}\\\\ &{\\qquad=b(\\mathbf{x}_{t},t)+w_{t}{\\frac{\\gamma_{t}}{\\alpha_{t}}}\\Bigl[\\gamma_{t}{\\dot{\\alpha}}_{t}-{\\dot{\\gamma}}_{t}\\alpha_{t}\\Bigr]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Approximating the unconditional velocity $\\boldsymbol{b}(\\mathbf{x}_{t},t)$ using a parametric estimator $\\boldsymbol{b}_{\\theta}(\\mathbf{x}_{t},t)$ , we get the required result. ", "page_idx": 16}, {"type": "equation", "text": "$$\nb(\\mathbf{x}_{t},y,t)\\approx b_{\\theta}(\\mathbf{x}_{t},t)+w_{t}\\frac{\\gamma_{t}}{\\alpha_{t}}\\Big[\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We restate Proposition 1 for convenience, ", "page_idx": 16}, {"type": "text", "text": "Proposition. For the conditional diffusion dynamics defined in Eqn. 3, introducing the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ induces the following projected diffusion dynamics. ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\hat{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\hat{\\mathbf{x}}_{t}d t+d\\Phi_{t}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)-\\frac{w_{t}r_{t}^{-2}}{2}G_{t}G_{t}^{\\top}\\frac{\\partial\\hat{\\mathbf{x}}_{1}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{1})d t\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{t}=\\exp{\\left(\\int_{0}^{t}B_{s}-F_{s}d s\\right)},\\qquad\\Phi_{t}=-\\int_{0}^{t}{\\frac{1}{2}}A_{s}G_{s}G_{s}^{\\top}C_{o u t}(s)d s,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $H^{\\dag}\\,=\\,H^{\\top}(H H^{\\top})^{-1}$ and $P\\,=\\,H^{\\top}(H H^{\\top})^{-1}H$ represent the pseudoinverse and the orthogonal projector operators for the degradation operator $\\pmb{H}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We have the following form of the conditional diffusion dynamics ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\mathbf{x}_{t}}{d t}=F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}|\\boldsymbol{y})}\\\\ {\\displaystyle\\qquad=F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}s_{\\theta}(\\mathbf{x}_{t},t)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given an affine transformation which projects the state $\\mathbf{x}_{t}$ to $\\hat{\\mathbf{x}}_{t}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by the Chain Rule of calculus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d\\hat{\\mathbf{x}}_{t}}{d t}=\\frac{d\\mathbf{A}_{t}}{d t}\\mathbf{x}_{t}+\\mathbf{A}_{t}\\frac{d\\mathbf{x}_{t}}{d t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting the ODE in Eqn. 30 in Eqn. 32 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\hat{\\bf x}_{t}}{d t}=\\frac{d A_{t}}{d t}{\\bf x}_{t}+A_{t}\\Big[F_{t}{\\bf x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\nabla_{{\\bf x}_{t}}s_{\\theta}\\big({\\bf x}_{t},t\\big)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{{\\bf x}_{t}}\\log p\\big({\\bf y}\\vert{\\bf x}_{t}\\big)\\Big]}\\\\ {\\displaystyle\\ ~~~=\\Big[\\frac{d A_{t}}{d t}+A_{t}F_{t}\\Big]{\\bf x}_{t}-\\frac{1}{2}A_{t}G_{t}G_{t}^{\\top}s_{\\theta}\\big({\\bf x}_{t},t\\big)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{{\\bf x}_{t}}\\log p\\big({\\bf y}\\vert{\\bf x}_{t}\\big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since, we have $\\begin{array}{r}{\\pmb{s}_{\\theta}(\\mathbf{x}_{t},t)={C}_{\\mathrm{out}}(t)\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}\\end{array}$ , the above equation can be simplified as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{d{\\hat{\\mathbf{x}}}_{t}}{d t}}=\\left[{\\frac{d A_{t}}{d t}}+A_{t}F_{t}\\right]\\mathbf{x}_{t}-{\\frac{1}{2}}A_{t}G_{t}G_{t}^{\\top}C_{\\mathrm{out}}(t)\\epsilon_{\\theta}(\\mathbf{x}_{t},t)-{\\frac{1}{2}}w_{t}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further parameterizing, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d A_{t}}{d t}+A_{t}F_{t}=A_{t}B_{t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d\\Phi_{t}}{d t}=-\\frac{1}{2}A_{t}G_{t}G_{t}^{\\top}C_{\\mathrm{out}}(t)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which yields the required diffusion ODE in the projected space: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\hat{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\hat{\\mathbf{x}}_{t}d t+d\\Phi_{t}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})d t\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have the following approximation for the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ (for the noiseless case $\\sigma_{y}=0$ ) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})=r_{t}^{-2}\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}H^{\\top}(H H^{\\top})^{-1}(y-H\\hat{\\mathbf{x}}_{0})}\\\\ &{\\qquad\\qquad\\qquad=r_{t}^{-2}\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\top}(H H^{\\top})^{-1}y-H^{\\top}(H H^{\\top})^{-1}H\\hat{\\mathbf{x}}_{0})}\\\\ &{\\qquad\\qquad\\qquad=r_{t}^{-2}\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\top}y-P\\hat{\\mathbf{x}}_{0})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $H^{\\dag}=H^{\\top}(H H^{\\top})^{-1}$ and $P=H^{\\top}(H H^{\\top})^{-1}H$ represent the pseudoinverse and the orthogonal projector operators for the degradation operator $\\pmb{H}$ . Substituting this form of the conditional score in projected diffusion dynamics, we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\hat{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\hat{\\mathbf{x}}_{t}d t+d\\Phi_{t}\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t\\right)-\\frac{w_{t}r_{t}^{-2}}{2}G_{t}G_{t}^{\\top}\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{0})d t\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Proposition 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We restate the theorem here for convenience. ", "page_idx": 17}, {"type": "text", "text": "Proposition. For a noiseless linear inverse problem with $\\sigma_{y}=0$ and the conditional score approximated using Eqn. 5, introducing the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ also induces the following projected diffusion dynamics. ", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\bar{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\bar{\\mathbf{x}}_{t}+d\\Phi_{y}y+d\\Phi_{s}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\mathbf{x}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{1})\\Big]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\bigg[\\int_{0}^{t}B_{s}-\\Big(F_{s}+\\frac{w_{s}r_{s}^{-2}}{2\\mu_{s}^{2}}G_{s}G_{s}^{\\top}P\\Big)d s\\bigg]\\qquad d\\Phi_{y}=-\\frac{w_{t}r_{t}^{-2}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}H^{\\dagger}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nd\\pmb{\\Phi}_{s}=-\\frac{1}{2}A_{t}G_{t}G_{t}^{\\top}\\Big[I_{d}-\\frac{w_{t}r_{t}^{-2}\\sigma_{t}^{2}}{\\mu_{t}^{2}}A_{t}P\\Big]C_{o u t}(t)\\qquad d\\pmb{\\Phi}_{j}=-\\frac{w_{t}r_{t}^{-2}\\sigma_{t}^{2}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}C_{o u t}(t)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\exp(.)$ denotes the matrix exponential, $H^{\\dagger}$ , and $_{P}$ are the pseudoinverse and projector operators (as defined previously). ", "page_idx": 17}, {"type": "text", "text": "Proof. The proof consists of two parts. Firstly, we simplify the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ . Secondly, we plug the simplified form of the conditional score into the conditional diffusion dynamics and develop conjugate integrators. ", "page_idx": 17}, {"type": "text", "text": "Exploiting the linearity in $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ : From the definition of the score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})=\\frac{\\partial\\hat{\\mathbf{x}}_{0}}{\\partial\\mathbf{x}_{t}}^{\\top}H^{\\top}\\Sigma_{t}^{-1}({\\pmb y}-H\\hat{\\mathbf{x}}_{0})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{0}$ is the Tweedie\u2019s estimate of $\\mathbb{E}(\\mathbf{x}_{1}|\\mathbf{x}_{t})$ given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{0}=\\frac{1}{\\mu_{t}}(\\mathbf{x}_{t}+\\sigma_{t}^{2}s_{\\theta}(\\mathbf{x}_{t},t))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mu_{t},\\,\\sigma_{t}$ are the mean coefficient and standard deviation of the perturbation kernel $p(\\mathbf{x}_{t}|\\mathbf{x}_{1})=$ $\\mathcal{N}(\\mu_{t}\\mathbf{x}_{1}\\sigma_{t}^{2}I_{d})$ , respectively, and $\\Sigma_{t}=r_{t}^{2}(H H^{\\top})$ is the variance of the \u03a0GDM approximation of $p(\\mathbf{y}|\\mathbf{x}_{t})$ (for the noiseless case i.e. $\\sigma_{y}=0$ ). Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\mathbf x_{i}}\\log(y\\|\\mathbf x_{i})=}&{\\frac{\\partial\\mathbf{\\hat{x}_{o}}}{\\partial x_{i}}^{-1}H^{\\top}\\mathbf{\\mathcalZ}_{i}^{-1}(y-H\\delta_{o})}&{}&{\\mathrm{(45)}}\\\\ &{=}&{\\frac{1}{\\mu_{d}}\\left(L_{4}\\cdot\\sigma_{i}^{2}\\frac{\\nabla_{\\mathbf x_{i}}\\delta_{o}(\\mathbf x_{i},t)}{\\nabla_{\\mathbf x_{i}}\\delta_{o}(\\mathbf x_{i},t)}\\right)H^{\\top}\\Sigma_{i}^{-1}(y-H\\delta_{o})}&{\\mathrm{(45)}}\\\\ &{=}&{\\frac{1}{\\mu_{d}}\\left[H^{\\top}\\Sigma_{i}^{-1}(y-H\\delta_{o})+\\sigma_{i}^{2}S_{(\\mathbf x_{i},t)}H^{\\top}\\Sigma_{i}^{-1}(y-H\\delta_{o})\\right]}&{}&{\\mathrm{(57)}}\\\\ &{=}&{\\frac{1}{\\mu_{d}}\\left[H^{\\top}\\Sigma_{i}^{-1}(y-\\frac{1}{\\mu_{d}}H(\\mathbf x_{i}+\\sigma_{i}^{2}s_{o}(\\mathbf x_{i},t)))+\\sigma_{i}^{2}S_{(\\mathbf x_{i},t)}H^{\\top}\\Sigma_{i}^{-1}(y-H\\delta_{o})\\right.}&{}&{\\mathrm{(57)}}\\\\ &{=}&{\\frac{1}{\\mu_{d}}H^{\\top}\\Sigma_{i}^{-1}y-\\frac{1}{\\mu_{d}}H^{\\top}\\Sigma_{i}^{-1}H\\mathbf x_{i}}&{}&{\\mathrm{(5)}}\\\\ &{\\left.=\\underbrace{\\frac{1}{\\mu_{d}}H^{\\top}\\Sigma_{i}^{-1}y-\\frac{1}{\\mu_{d}}H^{\\top}\\Sigma_{i}^{-1}H\\mathbf x_{i}}_{=\\mathrm{~\\mu_{d}~}H^{\\top}\\Sigma_{i}^{-1}H\\delta_{o}}\\right.}&{}&{\\mathrm{(5)}}\\\\ &{\\quad\\left.\\quad-\\underbrace{\\frac{\\sigma_{i}^{2}}{\\mu_{d}^{2}}H^{\\top}\\Sigma_{i}^{-1}H s_{o}(\\mathbf x_{i},t)}_{\\mathrm{~\\mu_{d}~}H}+\\underbrace{\\sigma \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\scriptstyle S_{\\theta}$ denotes the second-order derivative of the score function $\\mathbf{\\boldsymbol{s}}_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)$ . Therefore, the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ , can be decomposed into a combination of linear and non-linear terms. Next, we use this decomposition to design conjugate integrators for noiseless linear inverse problems. ", "page_idx": 18}, {"type": "text", "text": "Conjugate Integrator Design: From Eqn. 30, the conditional reverse diffusion dynamics can be specified as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{x}_{t}}{d t}=F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}s_{\\theta}(\\mathbf{x}_{t},t)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging in the form of the conditional score in Eqn. 53 in the above equation, we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{i\\mathbf{X}_{t}}{d t}=F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})}}\\\\ &{}&{=F_{t}\\mathbf{x}_{t}-\\frac{1}{2}G_{t}G_{t}^{\\top}\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)-\\frac{1}{2}w_{t}G_{t}G_{t}^{\\top}\\Big[\\frac{1}{\\mu_{t}}H^{\\top}\\Sigma_{t}^{-1}y-\\frac{1}{\\mu_{t}^{2}}H^{\\top}\\Sigma_{t}^{-1}H\\mathbf{x}_{t}\\quad\\qquad\\qquad\\quad(56)}\\\\ &{}&{\\qquad-\\left.\\frac{\\sigma_{t}^{2}}{\\mu_{t}^{2}}H^{\\top}\\Sigma_{t}^{-1}H\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)\\right)+\\frac{\\sigma_{t}^{2}}{\\mu_{t}}S_{\\theta}(\\mathbf{x}_{t},t)H^{\\top}\\Sigma_{t}^{-1}(y-H\\mathbf{\\hat{x}}_{0})\\Big]}\\\\ &{}&{=\\Big[F_{t}+\\frac{w_{t}}{2\\mu_{t}^{2}}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma_{t}^{-1}H\\Big]\\mathbf{x}_{t}-\\frac{w_{t}}{2\\mu_{t}}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma_{t}^{-1}y\\quad\\qquad\\qquad\\quad(58)}\\\\ &{}&{\\qquad-\\left.\\frac{1}{2}G_{t}G_{t}^{\\top}\\Big[I_{d}-\\frac{w_{t}\\sigma_{t}^{2}}{\\mu_{t}^{2}}H^{\\top}\\Sigma_{t}^{-1}H\\Big]s_{\\theta}(\\mathbf{x}_{t},t)-\\frac{w_{t}\\sigma_{t}^{2}}{2\\mu_{t}}G_{t}G_{t}^{\\top}\\Big[S_{\\theta}(\\mathbf{x}_{t},t)H^{\\top}\\Sigma_{t}^{-1}(y-H\\mathbf{\\hat{x}}_{0})\\Big]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given an affine transformation which projects the state $\\mathbf{x}_{t}$ to $\\hat{\\mathbf{x}}_{t}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the projected diffusion dynamics can be specified as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{i\\overline{{\\mathbf{x}}}_{t}}{d t}=\\left[\\frac{d A_{t}}{d t}+A_{t}(F_{t}+\\frac{w_{t}}{2\\mu_{t}^{2}}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma_{t}^{-1}H)\\right]\\mathbf{x}_{t}-\\frac{w_{t}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma_{t}^{-1}y\\qquad\\qquad\\qquad(61)}\\\\ &{\\qquad\\quad-\\frac{1}{2}A_{t}G_{t}G_{t}^{\\top}\\Big[I_{d}-\\frac{w_{t}\\sigma_{t}^{2}}{\\mu_{t}^{2}}A_{t}H^{\\top}\\Sigma_{t}^{-1}H\\Big]s_{\\theta}(\\mathbf{x}_{t},t)-\\frac{w_{t}\\sigma_{t}^{2}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}\\Big[S_{\\theta}(\\mathbf{x}_{t},t)H^{\\top}\\Sigma_{t}^{-1}(\\mathbf{x}_{t},t)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the score network is parameterized as $\\begin{array}{r}{\\pmb{s}_{\\theta}(\\mathbf{x}_{t},t)\\,=\\,\\pmb{C}_{\\mathrm{out}}(t)\\pmb{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)}\\end{array}$ . Consequently, $S_{\\theta}({\\bf x}_{t},t)=C_{\\mathrm{out}}(t)\\partial_{t}\\epsilon_{\\theta}({\\bf x}_{t},t)$ . Lastly, we reparameterize $\\pmb{\\Sigma}_{t}=r_{t}^{2}\\pmb{\\Sigma}$ where $\\pmb{\\Sigma}=\\pmb{H}\\pmb{H}^{\\top}$ . Plugging ", "page_idx": 18}, {"type": "text", "text": "these parameterizations in the projected diffusion dynamics, we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\frac{d\\overline{{{\\mathbf{x}}}}_{t}}{d t}=\\Big[\\frac{d A_{t}}{d t}+A_{t}(F_{t}+\\frac{w_{t}r_{t}^{-2}}{2\\mu_{t}^{2}}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma^{-1}H)\\Big]\\mathbf{x}_{t}-\\frac{w_{t}r_{t}^{-2}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}H^{\\top}\\Sigma^{-1}y}\\\\ {\\quad\\quad\\quad-\\frac{1}{2}A_{t}G_{t}G_{t}^{\\top}\\Big[I_{d}-\\frac{w_{t}r_{t}^{-2}\\sigma_{t}^{2}}{\\mu_{t}^{2}}A_{t}H^{\\top}\\Sigma^{-1}H\\Big]C_{\\mathrm{out}}(t)\\epsilon_{\\theta}(\\mathbf{x}_{t},t)}\\\\ {\\quad\\quad\\quad-\\frac{w_{t}r_{t}^{-2}\\sigma_{t}^{2}}{2\\mu_{t}}A_{t}G_{t}G_{t}^{\\top}C_{\\mathrm{out}}(t)\\Big[\\partial_{\\mathbf{x}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)H^{\\top}\\Sigma^{-1}(y-H\\hat{\\mathbf{x}}_{0})\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We then parameterize, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d\\pmb{A}_{t}}{d t}+\\pmb{A}_{t}\\Big(\\pmb{F}_{t}+\\frac{w_{t}r_{t}^{-2}}{2\\mu_{t}^{2}}\\pmb{G}_{t}\\pmb{G}_{t}^{\\top}\\pmb{H}^{\\top}\\pmb{\\Sigma}_{t}^{-1}\\pmb{H}\\Big)=\\pmb{A}_{t}\\pmb{B}_{t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This implies, ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}B_{s}-\\Big(F_{s}+\\frac{w_{s}r_{s}^{-2}}{2\\mu_{s}^{2}}G_{s}G_{s}^{\\top}P\\Big)d s\\Big]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\exp(.)$ denotes the matrix exponential. Furthermore, we parameterize, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\Phi_{y}=-\\int_{0}^{t}\\frac{w_{s}r_{s}^{-2}}{2\\mu_{s}}A_{s}G_{s}G_{s}^{\\top}H^{\\dagger}d s}\\\\ {\\displaystyle\\Phi_{s}=-\\int_{0}^{t}\\frac{1}{2}A_{s}G_{s}G_{s}^{\\top}\\Big[I_{d}-\\frac{w_{s}r_{s}^{-2}\\sigma_{s}^{2}}{\\mu_{s}^{2}}A_{s}P\\Big]C_{\\mathrm{out}}(s)d s}\\\\ {\\displaystyle\\Phi_{j}=-\\int_{0}^{t}\\frac{w_{s}r_{s}^{-2}\\sigma_{s}^{2}}{2\\mu_{s}}A_{s}G_{s}G_{s}^{\\top}C_{\\mathrm{out}}(s)d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With this parameterization, the projected diffusion dynamics can be compactly specified as follows: ", "page_idx": 19}, {"type": "text", "text": "$d\\bar{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\bar{\\mathbf{x}}_{t}+d\\Phi_{y}y+d\\Phi_{s}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\mathbf{x}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{0})\\Big]$ This concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "A.4 Simplification in Eqn. 11 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We restate the result for convenience. The matrix exponential in Eqn. 10 ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s\\,I_{d}-\\frac{w}{2}\\Big(\\int_{0}^{t}\\beta_{s}d s\\Big)P\\Big]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "can be simplified as, ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t}=\\exp(\\kappa_{t}^{1})\\Big[I_{d}+(\\exp(\\kappa_{t}^{2})-1)P\\Big],\\quad\\kappa_{t}^{1}=\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s,\\quad\\kappa_{t}^{2}=-\\frac{w}{2}\\int_{0}^{t}\\beta_{s}d s\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $_{P}$ is the orthogonal projector corresponding to the degradation operator $\\pmb{H}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{A}_{t}=\\exp(\\kappa_{t}^{1}\\pmb{I}_{d}+\\kappa_{t}^{2}\\pmb{P})=\\exp(\\kappa_{t}^{1}\\,\\pmb{I}_{d})\\exp(\\kappa_{t}^{2}\\pmb{P})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The above result follows since $I_{d}P=P I_{d}$ (commutative under multiplication). Moreover, we can further simplify the matrix exponential in $_{P}$ as follows, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp(\\kappa_{t}^{2}P)=\\displaystyle\\sum_{i=0}^{\\infty}\\frac{(\\kappa_{t}^{2})^{i}P^{i}}{i!}}\\\\ &{\\qquad\\qquad=I_{d}+\\displaystyle\\sum_{i=1}^{\\infty}\\frac{(\\kappa_{t}^{2})^{i}P^{i}}{i!}=I_{d}+\\Big[\\displaystyle\\sum_{i=1}^{\\infty}\\frac{(\\kappa_{t}^{2})^{i}}{i!}\\Big]P}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The above result follows from the property of orthogonal projectors $P^{2}=P$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\exp(\\kappa_{t}^{2}P)=I_{d}+\\Big[\\displaystyle\\sum_{i=1}^{\\infty}\\frac{(\\kappa_{t}^{2})^{i}}{i!}\\Big]P=I_{d}+\\Big[\\displaystyle\\sum_{i=0}^{\\infty}\\frac{(\\kappa_{t}^{2})^{i}}{i!}-1\\Big]P}\\\\ {\\displaystyle\\exp(\\kappa_{t}^{2}P)=I_{d}+\\Big[\\exp(\\kappa_{t}^{2})-1\\Big]P}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "A.5 Proof of Proposition for Solving Noisy Inverse Problems ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We restate the result here for convenience. Proposition. For the noisy inverse problem, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}=H\\mathbf{x}_{0}+\\sigma_{y}\\mathbf{z},\\quad\\mathbf{z}\\sim\\mathcal{N}(0,\\pmb{I}_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "given the transformation $A_{t}$ for the noiseless case as defined in Eqn. $_{l l}$ , the corresponding noisy transformation can be approximated as, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\boldsymbol{A}}_{t}^{\\sigma_{y}}=\\mathbf{\\boldsymbol{A}}_{t}+\\kappa_{3}(t)\\mathbf{\\boldsymbol{H}}^{\\dagger}(\\mathbf{\\boldsymbol{H}}^{\\dagger})^{\\top}+\\mathcal{O}(\\sigma_{y}^{4})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa_{3}(t)=\\frac{w\\sigma_{y}^{2}}{2}\\Big(\\int_{0}^{t}\\frac{\\beta_{s}}{r_{s}^{2}}d s\\Big)\\Big[\\exp\\Big(\\kappa_{1}(t)+\\kappa_{2}(t)\\Big)-1\\Big]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, the inverse of the transformation $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ can be approximated as, ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\pmb{A}_{t}^{\\sigma_{y}})^{-1}\\approx\\pmb{A}_{t}^{-1}-\\kappa_{3}(t)\\pmb{A}_{t}^{-1}\\pmb{H}^{\\dagger}(\\pmb{H}^{\\dagger})^{\\top}\\pmb{A}_{t}^{-1}+\\mathcal{O}(\\sigma_{y}^{4})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We have, ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t}^{\\sigma_{y}}=\\exp\\Big[\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s-\\frac{w}{2}\\Big(\\int_{0}^{t}\\beta_{s}H^{\\top}(H H^{\\top}+\\frac{\\sigma_{y}^{2}}{r_{t}^{2}}I_{d})^{-1}H d s\\Big)\\Big]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From perturbation analysis, we introduce the following first-order approximation, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{\\top}(H H^{\\top}+\\frac{\\sigma_{y}^{2}}{r_{t}^{2}}I_{d})^{-1}H\\approx H^{\\top}\\Big[(H H^{\\top})^{-1}-\\frac{\\sigma_{y}^{2}}{r_{t}^{2}}(H H^{\\top})^{-2}\\Big]H+\\mathcal{O}(\\sigma_{y}^{4})}\\\\ &{\\qquad\\qquad\\qquad\\approx H^{\\top}(H H^{\\top})^{-1}H-\\frac{\\sigma_{y}^{2}}{r_{t}^{2}}H^{\\top}(H H^{\\top})^{-2}H+\\mathcal{O}(\\sigma_{y}^{4})}\\\\ &{\\qquad\\qquad\\qquad\\approx P-\\frac{\\sigma_{y}^{2}}{r_{t}^{2}}H^{\\dagger}(H^{\\dagger})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting this approximation in the expression for $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ (and ignoring terms in $\\mathcal{O}(\\sigma_{y}^{4}),$ ), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{t}^{\\sigma_{y}}\\approx\\exp\\Big[\\displaystyle\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s I_{d}-\\frac{w}{2}\\Big(\\displaystyle\\int_{0}^{t}\\beta_{s}\\Big[P-\\frac{\\sigma_{y}^{2}}{r_{s}^{2}}H^{\\dagger}(H^{\\dagger})^{\\top}\\Big]d s\\Big)\\Big]}\\\\ &{\\quad\\quad=\\exp\\Big[\\displaystyle\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s\\,I_{d}\\underbrace{-\\frac{w}{2}\\Big(\\displaystyle\\int_{0}^{t}\\beta_{s}d s\\Big)P+\\frac{w\\sigma_{y}^{2}}{2}\\Big(\\displaystyle\\int_{0}^{t}\\frac{\\beta_{s}}{r_{s}^{2}}d s\\Big)H^{\\dagger}(H^{\\dagger})^{\\top}\\Big]}_{=\\kappa_{2}(t)}}\\\\ &{\\quad\\quad=\\exp\\Big[\\kappa_{1}(t)I_{d}+\\kappa_{2}(t)P+\\frac{w\\sigma_{y}^{2}}{2}\\Big(\\displaystyle\\int_{0}^{t}\\frac{\\beta_{s}}{r_{s}^{2}}d s\\Big)H^{\\dagger}(H^{\\dagger})^{\\top}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the definition of the matrix exponential, it can be shown that the $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ in Eqn. 89 can be approximated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{1}_{t}^{\\sigma_{y}}\\approx\\exp\\Big[\\kappa_{1}(t)I_{d}+\\kappa_{2}(t)P\\Big]+\\underbrace{\\frac{w\\sigma_{y}^{2}}{2}\\Big(\\int_{0}^{t}\\frac{\\beta_{s}}{r_{s}^{2}}d s\\Big)\\Big[\\exp\\Big(\\kappa_{1}(t)+\\kappa_{2}(t)\\Big)-1\\Big]}_{=\\kappa_{3}(t)}H^{\\dagger}(H^{\\dagger})^{\\top}+O(\\sigma_{y}^{4}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Ignoring the higher-order terms, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{\\cal{A}}_{t}^{\\sigma_{y}}\\approx\\mathbf{\\cal{A}}_{t}+\\kappa_{3}(t)H^{\\dagger}(H^{\\dagger})^{\\top}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, we can also approximate the inverse of $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ , as follows, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A_{t}^{\\sigma_{y}})^{-1}=[A_{t}+\\kappa_{3}(t)H^{\\dagger}(H^{\\dagger})^{\\top}]^{-1}}\\\\ &{\\qquad\\qquad\\approx A_{t}^{-1}-\\kappa_{3}(t)A_{t}^{-1}H^{\\dagger}(H^{\\dagger})^{\\top}A_{t}^{-1}+\\mathcal{O}(\\sigma_{y}^{4})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "B Conditional Conjugate Integrators: Flows ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Background ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section discusses conditional conjugate integrators in the context of flows. For brevity, we skip deriving our results for flows since the derivations can be similar to the analysis of diffusion models with minor parameterization changes. Recall that the conditional dynamics for flows are specified as follows [Pokle et al., 2023]: ", "page_idx": 21}, {"type": "equation", "text": "$$\nb(\\mathbf{x}_{t},\\mathbf{y},t)\\approx b_{\\theta}(\\mathbf{x}_{t},t)+w_{t}\\frac{\\gamma_{t}}{\\alpha_{t}}\\Big[\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big]\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{b}_{\\theta}(\\mathbf{x}_{t},t)$ represents the pre-trained velocity field for a flow. Moreover, we restate the form of the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ for convenience. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{y}|\\mathbf{x}_{t})=\\frac{\\partial\\hat{\\mathbf{x}}_{1}}{\\partial\\mathbf{x}_{t}}^{\\top}H^{\\top}(r_{t}^{2}H H^{\\top}+\\sigma_{y}^{2}I_{d})^{-1}(y-H\\hat{\\mathbf{x}}_{1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{1}$ represents the Tweedie\u2019s estimate of the first moment of $\\mathbb{E}(\\mathbf{x}_{t}|\\mathbf{x}_{1})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{1}=\\mathbb{E}[\\mathbf{x}_{1}|\\mathbf{x}_{t}]=\\frac{1}{\\alpha_{t}}\\Big[\\mathbf{x}_{t}+\\gamma_{t}^{2}s(\\mathbf{x}_{t},t)\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{s}(\\mathbf{x}_{t},t)$ represents the score function associated with the marginal distribution $p(\\mathbf{x}_{t})$ . It can be shown that $\\hat{\\mathbf{x}}_{1}$ can also be expressed in terms of the pre-trained velocity field $\\boldsymbol{b}_{\\theta}(\\mathbf{x}_{t},t)$ as follows, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{1}=\\frac{1}{\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}}\\Big[-\\dot{\\gamma}_{t}\\mathbf{x}_{t}+\\gamma_{t}b_{\\theta}(\\mathbf{x}_{t},t)\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.2 Conditional Conjugate Integrators for Flows ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Analogous to diffusion models, we can design conditional conjugate samplers for flows that treat the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ as a black box. Similar to Proposition 1, by introducing the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ , we have the projected flow dynamics, ", "page_idx": 21}, {"type": "equation", "text": "$$\nd\\hat{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\hat{\\mathbf{x}}_{t}d t+d\\Phi_{t}b_{\\theta}\\left(\\mathbf{x}_{t},t\\right)+w_{t}r_{t}^{-2}\\frac{\\partial\\hat{\\mathbf{x}}_{1}}{\\partial\\mathbf{x}_{t}}^{\\top}(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{1})d t\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\left(\\int_{0}^{t}B_{s}d s\\right),\\qquad\\Phi_{t}=\\int_{0}^{t}A_{s}d s,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $H^{\\dag}\\,=\\,H^{\\top}(H H^{\\top})^{-1}$ and $P\\,=\\,H^{\\top}(H H^{\\top})^{-1}H$ represent the pseudoinverse and the orthogonal projector operators for the degradation operator $\\pmb{H}$ . For $B_{t}=0$ , the formulation in Eqn. 98 becomes equivalent to the \u03a0GDM formulation proposed for OT-flows in Pokle et al. [2023]. For simplicity, since in this work, we only explore the parameterization in Eqn. 98 for $B_{t}=0$ , we refer to this parameterization as \u03a0GFM. ", "page_idx": 21}, {"type": "text", "text": "B.2.1 Conjugate-\u03a0GFM (C-\u03a0GFM) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Analogous to the discussion of C-\u03a0GDM samplers in Section 2.2. More specifically, given a noiseless linear inverse problem with $\\sigma_{y}=0$ , and the conditional score $\\nabla_{{\\bf x}_{t}}\\log p({\\bf y}|{\\bf x}_{t})$ , introducing the transformation $\\bar{\\mathbf{x}}_{t}=A_{t}\\mathbf{x}_{t}$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}B_{s}+\\frac{w_{s}r_{s}^{-2}\\gamma_{t}\\dot{\\gamma}_{t}^{2}}{2\\alpha_{t}\\Big(\\gamma_{t}\\dot{\\alpha}_{t}-\\dot{\\gamma}_{t}\\alpha_{t}\\Big)}P d s\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "induces the following projected flow dynamics. ", "page_idx": 21}, {"type": "equation", "text": "$$\nd\\bar{\\mathbf{x}}_{t}=A_{t}B_{t}A_{t}^{-1}\\bar{\\mathbf{x}}_{t}d t+d\\Phi_{y}y+d\\Phi_{b}b_{\\theta}(\\mathbf{x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\mathbf{x}_{t}}b_{\\theta}(\\mathbf{x}_{t},t)(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{1})\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi_{y}=-\\int_{0}^{t}\\frac{w_{s}r_{t}^{-2}\\gamma_{s}\\dot{\\gamma}_{s}}{\\alpha_{s}}A_{s}H^{\\dagger}d s\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/0994f3673496d9ff9d33b4a017bd4ce34a1638f49857a7a10098837b0bfeecf6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi_{b}=\\int_{0}^{t}A_{s}\\Big[I_{d}+\\frac{w_{s}r_{s}^{-2}\\gamma_{s}^{2}\\dot{\\gamma}_{s}}{\\alpha_{s}(\\gamma_{s}\\dot{\\alpha}_{s}-\\dot{\\gamma}_{s}\\alpha_{s})}P\\Big]d s\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi_{j}=\\int_{0}^{t}\\frac{w_{s}r_{s}^{-2}\\gamma_{s}^{2}}{\\alpha_{s}}A_{s}d s\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{exp}(.)$ denotes the matrix exponential, $H^{\\dagger}$ , and $_{P}$ are the pseudoinverse and projector operators (as defined previously). Lastly, the matrix $B_{t}$ is a design choice of our method. We specify a recipe for C-\u03a0GFM sampling in Algorithm 2. ", "page_idx": 22}, {"type": "text", "text": "C Extension to Noisy and Non-linear Inverse Problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we discuss an extension of Conditional Conjugate Integrators to noisy and non-linear inverse problems. While our discussion is primarily in the context of diffusion models, similar theoretical arguments also apply to Flows. ", "page_idx": 22}, {"type": "text", "text": "Noisy Linear Inverse Problems: For noisy linear inverse problems of the form, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{y}=H\\mathbf{x}_{0}+\\sigma_{y}\\mathbf{z},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for VPSDE diffusion, the noisy transformation $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ can be approximated from the transformation $\\boldsymbol{A}_{t}$ for the noiseless case (i.e., $\\sigma_{y}=0$ ) as illustrated in the following result (Proof in Appendix A.5): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}_{t}^{\\sigma_{y}}=\\pmb{A}_{t}+\\kappa_{3}(t)\\pmb{H}^{\\dag}(\\pmb{H}^{\\dag})^{\\top}+\\mathcal{O}(\\sigma_{y}^{4})\\approx\\pmb{A}_{t}+\\kappa_{3}(t)\\pmb{H}^{\\dag}(\\pmb{H}^{\\dag})^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\kappa_{3}(t)=\\frac{w\\sigma_{y}^{2}}{2}\\Big(\\int_{0}^{t}\\frac{\\beta_{s}}{r_{s}^{2}}d s\\Big)\\Big[\\exp\\Big(\\kappa_{1}(t)+\\kappa_{2}(t)\\Big)-1\\Big].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, the inverse projection $(A_{t}^{\\sigma_{y}})^{-1}$ can be approximated from $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ from perturbation analysis. ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\pmb{A}_{t}^{\\sigma_{y}})^{-1}\\approx\\pmb{A}_{t}^{-1}-\\kappa_{3}(t)\\pmb{A}_{t}^{-1}\\pmb{H}^{\\dagger}(\\pmb{H}^{\\dagger})^{\\top}\\pmb{A}_{t}^{-1}+\\mathcal{O}(\\sigma_{y}^{4})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the transformation matrix $\\boldsymbol{A}_{t}^{\\sigma_{y}}$ and its inverse (see Appendix A.5) can also be computed tractably for the noisy case. Since, for most practical purposes, $\\sigma_{y}$ is pretty small, higher order terms in $\\sigma_{y}^{4}$ can be safely ignored, making our approximation accurate. We include qualitative examples for ${4\\bf{x}}$ super-resolution with $\\sigma_{y}=0.05$ for the ImageNet dataset in Figure 8 ", "page_idx": 22}, {"type": "text", "text": "Non-Linear Inverse Problems: For non-linear inverse problems of the form, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}=h(\\mathbf{x}_{0})+\\sigma_{y}\\mathbf{z},\\quad\\mathbf{z}\\sim\\mathcal{N}(0,\\pmb{I}_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "similar to Song et al. [2022], we heuristically re-define linear operations like ${\\cal{H}}^{\\dagger}{\\bf{x}}_{t}$ , ${\\mathbf{}}H{\\mathbf{x}}_{t}$ and $\\mathbf{\\nabla}P\\mathbf{x}_{t}$ by their non-linear equivalents $h^{\\dagger}(\\mathbf{x}_{t})$ , $h(\\mathbf{x}_{t})$ and $h^{\\dagger}(h(\\mathbf{x}_{t}))$ respectively. Consequently, analogous to Eqn. 11 the projection operator for a noiseless non-linear inverse problem, in this case, can be defined as, ", "page_idx": 23}, {"type": "equation", "text": "$$\n4_{t}=\\exp(\\kappa_{1}(t))\\Big[I_{d}+(\\exp(\\kappa_{2}(t))-1)P\\Big],\\quad\\kappa_{1}(t)=\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s,\\quad\\kappa_{2}(t)=-\\frac{w}{2}\\int_{0}^{t}\\beta_{s}d s,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P=h^{\\dagger}(h(.))$ is non-linear \u2018projector\" operator. For instance, in non-linear inverse problems like compression artifact removal, $h(\\mathbf{x}_{t})$ and $h^{\\dagger}(\\mathbf{x}_{t})$ can realized by encoders and decoders. We illustrate some qualitative examples in Figure 12. It is worth noting that this is a purely heuristic approximation, and developing a more principled framework for non-linear inverse problems within our framework remains an interesting direction for further work. ", "page_idx": 23}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we include additional practical implementation details for both C-\u03a0GDM and C\u03a0GFM formulations. ", "page_idx": 23}, {"type": "text", "text": "D.1 C-\u03a0GDM: Practical Aspects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1.1 VP-SDE ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We work with the VP-SDE diffusion [Song et al., 2020] with the forward process specified as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\mathbf{x}_{t}=-\\frac{1}{2}\\beta_{t}\\mathbf{x}_{t}\\,d t+\\sqrt{\\beta_{t}}\\,d\\mathbf{w}_{t},\\quad t\\in[0,T],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This implies, $\\begin{array}{r}{\\pmb{F}_{t}=-\\frac{1}{2}\\beta_{t}}\\end{array}$ and $G_{t}=\\sqrt{\\beta_{t}}$ . For the VP-SDE the perturbation kernel is given by, ", "page_idx": 23}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mu_{t}\\mathbf{x}_{0},\\sigma_{t}^{2}I_{d})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu_{t}=\\exp\\Big(-\\frac{1}{2}\\int_{0}^{s}\\beta_{s}d s\\Big)\\qquad\\sigma_{t}^{2}=\\Big[1-\\exp\\Big(-\\int_{0}^{s}\\beta_{s}d s\\Big)\\Big]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The corresponding deterministic reverse process is parameterized as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\mathbf{x}_{t}=-\\frac{\\beta_{t}}{2}\\left[\\mathbf{x}_{t}+s_{\\theta}(\\mathbf{x}_{t},t)\\right]\\,d t.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, we adopt the standard $\\epsilon$ -prediction parameterization which implies $C_{\\mathrm{out}}(t)\\,=\\,-1/\\sigma_{t}$ . Lastly, the Tweedies estimate $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ can be specified as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{0}=\\frac{1}{\\mu_{t}}\\Bigl[\\mathbf{x}_{t}+\\sigma_{t}^{2}s_{\\theta}(\\mathbf{x}_{t},t)\\Bigr]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.1.2 C-\u03a0GDM - Simplified Expressions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We choose the parameterization $B_{t}=\\lambda I_{d}$ and set the adaptive guidance weight as $w_{t}=w\\mu_{t}^{2}r_{t}^{2}$ , where rt =\u03c3t2 +t\u00b5t2 . The projected diffusion dynamics are then specified as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\bar{\\mathbf{x}}_{t}=\\lambda\\bar{\\mathbf{x}}_{t}d t+d\\Phi_{y}y+d\\Phi_{s}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\mathbf{x}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)(H^{\\dagger}y-P\\hat{\\mathbf{x}}_{0})\\Big]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nA_{t}=\\exp\\Big[\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s I_{d}-\\frac{w}{2}\\Big(\\int_{0}^{t}\\beta_{s}d s\\Big)P\\Big]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which further simplifies to, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{4}_{t}=\\exp(\\kappa_{1}(t))\\left[I_{d}+(\\exp(\\kappa_{2}(t))-1)P\\right],\\quad\\kappa_{1}(t)=\\int_{0}^{t}\\Big(\\lambda+\\frac{1}{2}\\beta_{s}\\Big)d s,\\quad\\kappa_{2}(t)=-\\frac{w}{2}\\int_{0}^{t}\\beta_{s}d s\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{y}=-\\int_{0}^{t}\\frac{w_{S}r_{S}^{-2}}{2\\mu_{S}}A_{s}G_{s}G_{s}^{\\top}H^{\\mathrm{i}}d s}\\\\ {=-\\int_{0}^{t}\\frac{w_{S}\\beta_{\\mathrm{f}}\\mu_{S}}{2}A_{s}M^{\\top}d s}\\\\ {=-\\int_{0}^{t}\\frac{w_{S}\\beta_{\\mathrm{f}}\\mu_{S}}{2}\\Big[\\exp(\\kappa_{1}(s))\\Big[H+(\\exp(\\kappa_{2}(s))-1)P\\Big]\\Big]H^{\\dagger}d s}\\\\ {=-\\int_{0}^{t}\\frac{w_{\\mathrm{f}}\\beta_{\\mathrm{f}}\\mu_{S}}{2}\\exp(\\kappa_{1}(s))\\Big[H^{\\dagger}+(\\exp(\\kappa_{2}(s))-1)P H^{\\dagger}\\Big]d s}\\\\ {=-\\int_{0}^{t}\\frac{w_{\\mathrm{f}}\\beta_{\\mathrm{f}}\\mu_{S}}{2}\\exp(\\kappa_{1}(s))\\Big[H^{\\dagger}+(\\exp(\\kappa_{2}(s))-1)H^{\\dagger}\\Big]d s}\\\\ {=-\\int_{0}^{t}\\frac{w_{\\mathrm{f}}\\beta_{\\mathrm{f}}\\mu_{S}}{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]H^{\\dagger}}\\\\ {=-\\left[\\int_{0}^{t}\\frac{w_{\\mathrm{f}}\\beta_{\\mathrm{f}}\\mu_{S}}{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\right]H^{\\dagger}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Phi_{s}=-\\int_{0}^{t}\\frac{1}{2}A_{s}G_{s}G_{s}^{\\top}\\Big[I_{d}-\\frac{w_{s}r_{s}^{-2}\\sigma_{s}^{2}}{\\mu_{s}^{2}}P\\Big]C_{\\mathrm{out}}(s)d s}}\\\\ &{}&{=\\int_{0}^{t}\\frac{\\beta_{s}}{2\\sigma_{s}}A_{s}\\Big[I_{d}-w\\sigma_{s}^{2}P\\Big]d s}\\\\ &{}&{=\\int_{0}^{t}\\frac{\\beta_{s}}{2\\sigma_{s}}A_{s}d s-\\Big[\\int_{0}^{t}\\frac{w\\beta_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]P}\\\\ &{}&{=\\int_{0}^{t}\\frac{\\beta_{s}}{2\\sigma_{s}}\\exp(\\kappa_{1}(s))\\Big[I_{d}+(\\exp(\\kappa_{2}(s))-1)P\\Big]d s-\\Big[\\int_{0}^{t}\\frac{w\\beta_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(}\\\\ &{}&{=\\int_{0}^{t}\\frac{\\beta_{s}}{2\\sigma_{s}}\\exp(\\kappa_{1}(s))d s+\\Big[\\int_{0}^{t}\\frac{\\beta_{s}}{2\\sigma_{s}}\\exp(\\kappa_{1}(s))(\\exp(\\kappa_{2}(s))-1)-\\frac{w\\beta_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi_{j}=-\\int_{0}^{t}\\frac{w_{s}r_{s}^{-2}\\sigma_{s}^{2}}{2\\mu_{s}}A_{s}G_{s}G_{s}^{\\top}C_{\\mathrm{out}}(s)d s=\\int_{0}^{t}\\frac{w\\beta_{s}\\mu_{s}\\sigma_{s}}{2}A_{s}d s}\\\\ {\\displaystyle\\quad=\\int_{0}^{t}\\frac{w\\beta_{s}\\mu_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s))\\Big[I_{d}+(\\exp(\\kappa_{2}(s))-1)P\\Big]d s}\\\\ {\\displaystyle\\quad=\\int_{0}^{t}\\frac{w\\beta_{s}\\mu_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s))d s+\\Big[\\int_{0}^{t}\\frac{w\\beta_{s}\\mu_{s}\\sigma_{s}}{2}\\exp(\\kappa_{1}(s))(\\exp(\\kappa_{2}(s))-1)d s\\Big]P}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2 C-\u03a0GFM: Practical Aspects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.2.1 OT-Flows ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We work with OT-Flows [Albergo et al., 2023, Lipman et al., 2023, Liu et al., 20223] due to its wide adoption. More specifically, the corresponding interpolant can be specified as, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=(1-t)\\mathbf{z}+t\\mathbf{x}_{1},\\quad\\mathbf{z}\\sim\\mathcal{N}(0,I_{d})\\quad\\mathbf{x}_{1}\\sim p_{\\mathrm{data}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For this case $\\alpha_{t}=t$ and $\\gamma_{t}=1-t$ . Therefore, the Tweedie\u2019s estimate of $\\mathbb{E}(\\mathbf{x}_{t}|\\mathbf{x}_{1})$ can be specified as (from Eqn. 97): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{1}=\\mathbf{x}_{t}+(1-t)\\pmb{b}_{\\theta}(\\mathbf{x}_{t},t)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2.2 C-\u03a0GFM: Simplified Expressions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We choose the parameterization $B_{t}=\\lambda I_{d}$ and set the adaptive guidance weight as $w_{t}=w\\alpha_{t}^{2}r_{t}^{2}$ where $\\begin{array}{r}{r_{t}^{2}=\\frac{\\gamma_{t}^{2}}{\\alpha_{t}^{2}+\\gamma_{t}^{2}}}\\end{array}$ . The projected diffusion dynamics are then specified as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nd\\bar{\\bf x}_{t}=\\lambda\\bar{\\bf x}_{t}d t+d\\Phi_{y}y+d\\Phi_{b}b_{\\theta}({\\bf x}_{t},t)+d\\Phi_{j}\\Big[\\partial_{\\bf x}b_{\\theta}({\\bf x}_{t},t)(H^{\\dagger}y-P\\hat{\\bf x}_{1})\\Big]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where, ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\bf\\cal A}_{t}={\\bf e x p}\\left[\\int_{0}^{t}\\lambda I_{d}+{\\frac{w t(1-t)}{2}}{\\cal P}d s\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which further simplifies to, ", "page_idx": 25}, {"type": "equation", "text": "$$\nA_{t}=\\exp(\\kappa_{1}(t))\\Big[I_{d}+(\\exp(\\kappa_{2}(t))-1)P\\Big],\\quad\\kappa_{1}(t)=\\int_{0}^{t}\\lambda d s,\\quad\\kappa_{2}(t)=\\frac{w}{2}\\int_{0}^{t}s(1-s)d s.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Phi_{y}=-\\int_{0}^{t}\\frac{w_{s}r_{t}^{-2}\\gamma_{s}\\dot{\\gamma}_{s}}{\\alpha_{s}}A_{s}H^{\\dagger}d s}}\\\\ &{=-\\int_{0}^{t}w\\alpha_{s}\\gamma_{s}\\dot{\\gamma}_{s}A_{s}H^{\\dagger}d s=\\int_{0}^{t}w s(1-s)A_{s}H^{\\dagger}d s}\\\\ &{=\\displaystyle\\int_{0}^{t}w s(1-s)\\exp(\\kappa_{1}(s))\\Big[I_{d}+(\\exp(\\kappa_{2}(s))-1)P\\Big]H^{\\dagger}d s}\\\\ &{=\\Big[\\displaystyle\\int_{0}^{t}w s(1-s)\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]H^{\\dagger}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{b}=\\int_{0}^{t}A_{s}\\Big[I_{d}+\\frac{w_{s}r_{s}^{2}\\sigma_{s}^{2}\\bar{r}_{s}^{2}\\bar{r}_{s}}{\\alpha_{s}(\\gamma_{s}\\bar{u}_{s}-\\gamma_{s}\\alpha_{s})}P\\Big]d s}\\\\ &{\\quad=\\int_{0}^{t}A_{s}\\Big[I_{d}+\\frac{w\\alpha_{s}\\gamma_{s}^{2}\\bar{r}_{s}}{(\\gamma_{s}\\bar{u}_{s}-\\gamma_{s}\\alpha_{s})}P\\Big]d s}\\\\ &{\\quad=\\int_{0}^{t}A_{s}\\Big[I_{d}-w s(1-s)^{2}P\\Big]d s}\\\\ &{\\quad=\\int_{0}^{t}A_{s}d s-\\int_{0}^{t}w s(1-s)^{2}A_{s}P\\Big]d s}\\\\ &{\\quad=\\int_{0}^{t}A_{s}d s-\\int_{0}^{t}w s(1-s)^{2}\\Phi_{s}P\\Big[d s}\\\\ &{\\quad=\\int_{0}^{t}A_{s}d s-\\Big[\\int_{0}^{t}w s(1-s)^{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]P}\\\\ &{\\quad=\\int_{0}^{t}\\exp(\\kappa_{1}(s))d s+\\Big[\\int_{0}^{t}\\exp(\\kappa_{1}(s))(\\exp(\\kappa_{2}(s))-1)-w s(1-s)^{2}\\exp(\\kappa_{1}(s)+\\kappa_{2}(s))d s\\Big]+}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Phi_{j}=\\displaystyle\\int_{0}^{t}\\frac{w_{s}r_{s}^{-2}\\gamma_{s}^{2}}{\\alpha_{s}}A_{s}d s=\\displaystyle\\int_{0}^{t}w\\alpha_{s}\\gamma_{s}^{2}A_{s}d s}\\\\ {\\displaystyle\\quad=\\displaystyle\\int_{0}^{t}w s(1-s)^{2}\\exp(\\kappa_{1}(s))\\Big[I_{d}+(\\exp(\\kappa_{2}(s))-1)P\\Big]d s}\\\\ {\\displaystyle\\quad=\\displaystyle\\int_{0}^{t}w s(1-s)^{2}\\exp(\\kappa_{1}(s))d s+\\Big[\\displaystyle\\int_{0}^{t}w s(1-s)^{2}\\exp(\\kappa_{1}(s))(\\exp(\\kappa_{2}(s))-1)d s\\Big]P}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.3 Coefficient Computation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "From the above analysis, most integrals are one-dimensional and can be computed in closed form or numerically with high precision. To clarify, with a predetermined timestep schedule $\\{t_{i}\\}$ , the coefficients $\\Phi$ can be calculated offilne just once and then reused across various samples. Therefore, this computation must only be done once offilne for each sampling run. For numerical approximation of these integrals, we use the odeint method from the torchdiffeq package [Chen, 2018] with parameters atol $\\cdot=$ 1e-5, rtol $=$ 1e-5 and the RK45 solver [Dormand and Prince, 1980]. We set the initial value $\\Phi_{\\mathrm{init}}=\\mathbf{0}$ for all coefficients $\\Phi$ as an initial condition for both C-\u03a0GDM and C-\u03a0GFM samplers. ", "page_idx": 25}, {"type": "text", "text": "D.4 Choice of Numerical Solver ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We use the Euler method to simulate projected diffusion/flow dynamics for simplicity. However, using higher-order numerical solvers within our framework is also possible. We leave this exploration to future work. ", "page_idx": 26}, {"type": "text", "text": "D.5 Timestep Selection during Sampling ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": ": We use uniform spacing for timestep discretization during sampling. We hypothesize our sampler can also benefti from more advanced timestep discretization techniques Karras et al. [2022] commonly used for sampling in unconditional diffusion models in the low NFE regime. ", "page_idx": 26}, {"type": "text", "text": "D.6 Last-Step Denoising ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "It is common to add an Euler-based denoising step from a cutoff $\\epsilon$ to zero to optimize for sample quality [Song et al., 2020, Dockhorn et al., 2022, Jolicoeur-Martineau et al., 2021] at the expense of another sampling step. In this work, we do not use last-step denoising for our samplers. ", "page_idx": 26}, {"type": "text", "text": "D.7 Evaluation Metrics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We use the network function evaluations (NFE) to assess sampling efficiency and perceptual metrics KID [Bin\u00b4kowski et al., 2018], LPIPS [Zhang et al., 2018] and FID [Heusel et al., 2017] to assess sample quality. In practice, we use the torch-fidelity[Obukhov et al., 2020] package for computing all FID and KID scores reported in this work. For LPIPS, we use the torchmetrics package with Alexnet embedding. ", "page_idx": 26}, {"type": "text", "text": "D.8 Baseline Hyperparameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Diffusion Baselines: For DPS [Chung et al., 2022a], we set $\\mathrm{NFE}{=}1000$ and set the step size for each task to the value recommended in Appendix D in Chung et al. [2022a]. For DDRM [Kawar et al., 2022], we set the number of sampling steps to $\\mathrm{NFE}{=}20$ with parameters $\\eta_{b}=1.0$ and $\\eta=0.85$ as recommended in Kawar et al. [2022]. For both DPS and DDRM we start diffusion sampling from $t\\,=\\,T$ . For our implementation of $\\Pi$ -GDM, we set the start time parameter $\\tau$ to 0.6 for super-resolution and deblurring. We set the guidance weight $w_{t}\\,=\\,w r_{t}^{2}$ where $w$ is tuned using grid search between 1.0 and 10.0 for best sample quality for super-resolution and deblurring. For implementation of all diffusion-based baselines, we use the official code for RED-Diff [Mardani et al., 2023] at https://github.com/NVlabs/RED-diff. ", "page_idx": 26}, {"type": "text", "text": "Flow Baselines: In developing our flow-based baseline, we adhere to the approach outlined in \u03a0GFM (Pokle et al., 2024), which advocates for a consistent guidance schedule characterized by $w_{t}\\;=\\;w$ and $\\begin{array}{r}{r_{t}\\,=\\,\\frac{\\gamma_{t}^{2}}{\\gamma_{t}^{2}+\\alpha_{t}^{2}}}\\end{array}$ . For each task, we perform a comprehensive grid search over the parameters $\\alpha_{\\tau}~=~\\{0.1,0.2,\\ldots,0.7\\}$ and $w\\;=\\;\\{1,2,\\ldots,5\\}$ (35 combinations in total) across different datasets to identify the optimal configuration that minimizes the LPIPS score. For the implementation of Flows, we use the official implementation of Rectified Flows [Liu et al., 20223] at https://github.com/gnobitab/RectifiedFlow. ", "page_idx": 26}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Additional Baseline Comparisons ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We include additional comparisons between our proposed samplers and competing baselines on the AFHQ-Cat (see Table 2), LSUN Bedroom (see Table 3), and the FFHQ (see Table 4) datasets. ", "page_idx": 26}, {"type": "text", "text": "A note on Inpainting evaluations for ImageNet. We find that for diffusion model evaluations, the continuous sampler for \u03a0GDM suffers from noisy artifacts for the inpainting task. Consequently, Conjugate \u03a0GDM suffers from similar artifacts. Therefore, we do not report results on this task for the ImageNet dataset. ", "page_idx": 26}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/5cc47327d6ffb62bbcccb3609c5b0e5861ae6cea271f49a848b40abe4105e205.jpg", "table_caption": [], "table_footnote": ["Table 2: Quantitative evaluation on 4x superresolution, inpainting, and Gaussian deblurring on the AFHQ-Cat dataset. "], "page_idx": 27}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/34b3bb54582e9eb85ed108ef4fffe850e68bf662cddb8824a1dbd3a2553e31d2.jpg", "table_caption": [], "table_footnote": ["Table 3: Quantitative evaluation on 4x superresolution, inpainting, and Gaussian deblurring on the LSUN-Bedroom dataset. We note that \u03a0GFM fails to generate reasonable texture in the masked region even with the maximum $\\mathrm{NFE}{=}20$ , so we choose not to report the results here. (See qualitative examples in Figure 9) "], "page_idx": 27}, {"type": "text", "text": "E.2 Comparison of Perceptual vs Recovery Metrics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, we highlight the robustness of C-\u03a0GDM in both perceptual and recovery metrics in the context of inverse problems. For completeness, we provide a comparison between DPS, \u03a0GDM, and C\u03a0GDM in terms of PSNR, SSIM, FID, and LPIPS in Tables 5 and 6 on the ImageNet-256 and FFHQ-256 datasets on the 4x super-resolution task. It is worth noting that the PSNR and SSIM scores for all methods correspond with the best FID/LPIPS scores presented in the main text for these methods. Our method achieves competitive PSNR and SSIM scores for better perceptual quality than competing baselines like DPS/\u03a0-GDM, even for very small sampling budgets. For instance, on the FFHQ dataset, our method achieves a PSNR of 28.97 compared to 28.49 for DPS while achieving better perceptual sample quality (LPIPS: 0.095 for ours vs 0.107 for DPS) and requiring around 200 times less sampling budget $\\mathrm{NFE}{=}5$ for our method vs 1000 for DPS). Therefore, we argue that our perceptual quality to recovery trade-off is better than competing baselines. ", "page_idx": 27}, {"type": "text", "text": "E.3 Traversing the Recovery vs Perceptual trade-off ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In addition to the guidance weight $w$ , our method also allows tuning an additional hyperparameter $\\lambda$ , which controls the dynamics of the projection operator (See Sections 2.3 and 3.2 for more intuition). Therefore, tuning $w$ and $\\lambda$ can help traverse the trade-off curve between perceptual quality and distortion for a fixed NFE budget. We illustrate this aspect in Table 7 (fixed $\\lambda$ with varying $w$ ) and Table 8 (fixed $w$ with varying $\\lambda$ ) for the $\\mathrm{SR}(\\mathrm{x}4)$ task on the ImageNet-256 dataset using the PSNR, LPIPS, and FID metrics. Therefore, our method offers greater flexibility to tune the sampling process towards either good perceptual quality or good recovery for a given application while maintaining the same number of sampling steps. In contrast, other methods like DPS or $\\Pi$ -GDM do not offer such flexibility. Moreover, tuning the guidance weight in methods like DPS could be very expensive due to its high sampling budget requirement (around 1000 NFE). ", "page_idx": 27}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/93f6e61b57a65b67222147ae4e32d49a383f17b0f608166844f2b9eb342fac2e.jpg", "table_caption": [], "table_footnote": ["Table 4: Quantitative evaluation on 4x superresolution and Gaussian Deblurring tasks for the FFHQ dataset. DPS was evaluated with $\\mathrm{NFE=}1000$ but failed to perform well on the deblurring task. DDRM was evaluated with $\\mathrm{NFE}{=}20$ . "], "page_idx": 28}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/0c0b2237f07a244dd394cebe3dc0dd5463760080111e768a851cbd004062714a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "E.4 Qualitative Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Diffusion Models: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. We include additional qualitative comparisons between \u03a0-GDM and our proposed C-\u03a0GDM sampler for the ImageNet dataset in Fig. 4.   \n2. We include a qualitative comparison between sample quality at different sampling budgets for the C-\u03a0GDM sampler in Fig.5.   \n3. We qualitatively study the impact of varying $w$ on sample quality in Fig. 6 and the impact of varying $\\lambda$ on sample quality in Fig. 7.   \n4. We qualitatively present the performance of the C-\u03a0GDM sampler for noisy inverse problems in Fig. 8. In just 5 steps, our method can also generate good-quality samples for noisy inverse problems. ", "page_idx": 28}, {"type": "text", "text": "Flow Models: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. We include additional qualitative comparisons between \u03a0-GFM and our proposed C-\u03a0GFM sampler with different sampling budget for the all three datasets in Fig. 9. 2. We qualitatively study the impact of varying $w$ on sample quality in Fig. 10 and the impact of varying $\\lambda$ on sample quality in Fig. 11. ", "page_idx": 28}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/0efa4dbd0ec1d3a470611f41afe75804f507cac58d6e91b08a47424815a9de9b.jpg", "table_caption": [], "table_footnote": ["Table 6: Comparison between C-\u03a0GDM and other baselines in terms of the Recovery (a.k.a distortion) vs Perception tradeoff for FFHQ-256 dataset for the SR(x4) task. "], "page_idx": 29}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/9d4d2bb71b9189894c6c9ce274ab9f32411522bc9527679fc8c80a3416d6f34d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "qxS4IvtLdD/tmp/f942ba62d29797020d36dda7b0a32e7c1750667e9e25c77af84e391f640cb767.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/15c7f23789c3711db7a47c400f2c6bf8b2457f7a604f94d2f85b7360c418a4e1.jpg", "img_caption": ["Figure 4: Qualitative comparison between \u03a0GDM and C-\u03a0GDM at $\\mathrm{NFE}{=}5$ for the ImageNet dataset on the $4\\mathtt{x}$ Superresolution task. C-\u03a0GDM can generate high-frequency details even for a low compute budget as compared to the baseline \u03a0-GDM (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/c60f948a4c33cb5a16cee02cf90eb7394311fbf151c737b0a09fd51a0a8937d0.jpg", "img_caption": ["Figure 5: Qualitative comparison for different sampling budgets for the ImageNet dataset on the 4x Superresolution task. C-\u03a0GDM can generate high-quality samples in just 5 steps (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/82f5bcda16a2084ce220d2c68bae73bc1e35b10fdbb492c9ba924a20f282a69e.jpg", "img_caption": ["Figure 6: Impact of varying C-\u03a0GDM guidance weight $w$ on sample quality for the ImageNet dataset on the 4x Superresolution task. High guidance weight is crucial to generate good quality samples from C-\u03a0GDM (NFE ${}^{=5}$ steps) (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/3c9ba58065f0cf79cd703e873c71f926af78966203d3010294a420ed90880e5a.jpg", "img_caption": ["Figure 7: Impact of varying C-\u03a0GDM $\\lambda$ on sample quality for the ImageNet dataset on the $4\\mathbf{x}$ Superresolution task. High $\\lambda$ can lead to blurry samples while a very low $\\lambda$ can lead to oversharpened artifacts $\\mathrm{NFE}{=}5$ steps) (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/bf0773764ac9ccad772b6999c882f1f1d01e091f91ce4ee4c9b039f5cd53a8c9.jpg", "img_caption": ["Figure 8: C-\u03a0GDM can also generate good quality samples for noisy inverse problems (4x superres with $\\mathrm{NFE}{=}5$ , $\\sigma_{y}=0.05)$ . For this case naively computing the pseudoinverse fails to get rid of the noise. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/e54ac9eaf1a3b6942d3ceb787410efcfdf3018a6531f1066e29aee2388a0b15f.jpg", "img_caption": ["", "Figure 9: Qualitative comparison between \u03a0GFM and C-\u03a0GFM at $\\mathrm{NFE}{=}\\{5,10\\}$ for the 3 datasets on 3 tasks. C-\u03a0GFM can generate high-frequency details even for a low compute budget as compared to the baseline \u03a0-GFM (Best Viewed when zoomed in). We did not report \u03a0GFM inpainting results in Table 3 as it failed to generate \u201creasonable\u201d textures even after extensive hyper-parameter search on $w$ and $\\tau$ . "], "img_footnote": [], "page_idx": 34}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 34}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/f83ee1c3663e16513db3f7bba1261c999f0c159a112b0cba43d95277ad11bb50.jpg", "img_caption": ["Figure 10: Impact of varying C-\u03a0GFM guidance weight $w$ on sample quality for the ImageNet dataset on the 4x Superresolution task. High guidance weight is crucial to generate good quality samples from C-\u03a0GFM $\\mathrm{NFE}{=}5$ steps) (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/60fdd55979c1c960fb66ea207fc97d9118fe46d19ae58ce94f147357c64c77fa.jpg", "img_caption": ["Figure 11: Impact of varying C-\u03a0GFM $\\lambda$ on sample quality for the ImageNet dataset on the $4\\mathbf{x}$ Superresolution task. High $\\lambda$ can lead to blurry samples while a very high $\\lambda$ can lead to oversharpened artifacts $\\mathrm{NFE}{=}5$ steps) (Best Viewed when zoomed in) "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "qxS4IvtLdD/tmp/4d5691e1708a2078bc96989430b497fc32674dfdf09342b660543aadd33d8af2.jpg", "img_caption": ["Figure 12: C-\u03a0GFM for solving compression inverse problem. Top: decoding compressed latents from pretrained mean-scale hyperprior neural codec [Minnen et al., 2018]; Bottom: JPEG image restoration. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 36}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 36}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 36}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 36}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 36}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: See Abstract, Section 1, Section 2 and Section 3 ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See Section 5 ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: See Section 2 and Appendix A ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See the beginning of Section 3 ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The code is not available at the time of submission, but will be published later. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 38}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See the beginning of Section 3 ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: We don\u2019t have any error bar data in our experiments. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our approach utilizes established models for a range of downstream tasks, ensuring that hardware variations do not affect the outcomes. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: the research conducted in the paper conform with the NeurIPS Code of Ethics Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: See Section 5 ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our approach utilizes published models for a range of downstream tasks, so we do not need to add any safeguard. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We cited all the related works and packegs we used ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our approach utilizes published models for a range of downstream tasks. Details are available in Section 3 ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We don\u2019t have experiment with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We don\u2019t have experiment with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]