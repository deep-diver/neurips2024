{"references": [{"fullname_first_author": "Abraham Charnes", "paper_title": "Programming with linear fractional functionals", "publication_date": "1962-00-00", "reason": "This paper introduces linear-fractional programming, a mathematical optimization technique that is fundamental to the paper's ROI maximization framework."}, {"fullname_first_author": "Jongmin Lee", "paper_title": "Optidice: Offline policy optimization via stationary distribution correction estimation", "publication_date": "2021-00-00", "reason": "This paper introduces the DICE-RL framework, which is a core component of the proposed ROIDICE algorithm for offline ROI maximization."}, {"fullname_first_author": "Jongmin Lee", "paper_title": "COptiDICE: Offline constrained reinforcement learning via stationary distribution correction estimation", "publication_date": "2022-00-00", "reason": "This paper extends the DICE-RL framework to handle constrained optimization, which is relevant to the ROI maximization problem with cost constraints."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4rl: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-04-22", "reason": "This paper provides the D4RL benchmark datasets, which are used for evaluating the performance of the proposed algorithm in various continuous control tasks."}, {"fullname_first_author": "Rong-Jun Qin", "paper_title": "Neorl: A near real-world benchmark for offline reinforcement learning", "publication_date": "2022-00-00", "reason": "This paper provides the NeoRL benchmark datasets, which are used for evaluating the performance of the proposed algorithm in a more realistic stock trading environment."}]}