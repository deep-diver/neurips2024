[{"figure_path": "6Kg26g1quR/figures/figures_5_1.jpg", "caption": "Figure 1: Comparison of ROIDICE with other offline algorithms. We average the scores and obtain \u00b12\u00d7 standard error using 1000 seeds. N denotes the number of trajectories within the dataset.", "description": "This figure compares the performance of ROIDICE against other offline reinforcement learning algorithms (offline RL and offline constrained RL).  It shows the trade-off between return and accumulated cost for different numbers of trajectories in the training dataset. Subfigure (a) contrasts ROIDICE with a standard offline RL approach, highlighting ROIDICE's superior efficiency in balancing return and cost. Subfigure (b) compares ROIDICE with an offline constrained RL method, demonstrating ROIDICE's ability to achieve higher ROI even with cost constraints, particularly when the number of trajectories is sufficient.", "section": "5 Experiments"}, {"figure_path": "6Kg26g1quR/figures/figures_7_1.jpg", "caption": "Figure 2: ROI Comparison of ROIDICE and Dataset with varying dataset qualities. We average the each scores and get \u00b12\u00d7 standard error with 5 seeds across 10 episodes. BCn% refers to behavior cloning utilizing the top n% of the offline dataset, ranked by ROI.", "description": "This figure compares the Return on Investment (ROI) performance of ROIDICE against behavior cloning (BC) using different percentages of the dataset. The dataset is ranked by ROI, and BC20%, BC50%, BC80%, and BC100% represent using the top 20%, 50%, 80%, and 100% of the dataset respectively.  The results show how ROIDICE's performance compares to simply cloning the best performing policies from the dataset, illustrating its ability to learn from a diverse set of experiences.", "section": "5.2 Results"}, {"figure_path": "6Kg26g1quR/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of ROIDICE and OptiDICE in Hopper environment.", "description": "This figure compares the qualitative behavior of ROIDICE and OptiDICE in the Hopper environment.  It shows a sequence of states/actions taken by both agents. The visual comparison highlights how ROIDICE, by optimizing for ROI, achieves a higher return with a lower accumulated cost compared to OptiDICE, which focuses solely on maximizing return.  The difference can be seen in the smoother, more efficient movements produced by ROIDICE in contrast to the less efficient jumps by OptiDICE.", "section": "5.2.2 Results"}, {"figure_path": "6Kg26g1quR/figures/figures_15_1.jpg", "caption": "Figure 1: Comparison of ROIDICE with other offline algorithms. We average the scores and obtain \u00b12\u00d7 standard error using 1000 seeds. N denotes the number of trajectories within the dataset.", "description": "The figure compares ROIDICE's performance against other offline algorithms (offline RL and offline constrained RL) across different numbers of trajectories (dataset sizes).  Subfigure (a) contrasts ROIDICE with offline RL, showing that ROIDICE achieves higher ROI (return on investment) by balancing return and cost more effectively than offline RL which prioritizes maximizing return. Subfigure (b) compares ROIDICE against offline constrained RL algorithms. While constrained RL aims to maximize return subject to a cost constraint, ROIDICE consistently achieves higher ROI than constrained RL across various cost constraint thresholds, particularly with larger datasets.  This highlights ROIDICE's unique ability to optimize for a superior trade-off between return and cost, leading to improved policy efficiency.", "section": "5 Experiments"}, {"figure_path": "6Kg26g1quR/figures/figures_16_1.jpg", "caption": "Figure 5: Comparison of different levels of the hyperparameter (\u03b1) of ROIDICE in locomotion environments using expert data quality. We average the scores and obtain \u00b12\u00d7 standard error using 5 seeds across 10 episodes.", "description": "This figure shows the effect of different values of the hyperparameter \u03b1 on the performance of the ROIDICE algorithm across three locomotion tasks (Hopper, Walker2D, and Halfcheetah) using expert-level datasets.  The hyperparameter \u03b1 controls the strength of the regularization in ROIDICE, balancing between return maximization and distribution shift.  The plot displays the average ROI achieved with varying values of \u03b1, along with error bars representing the standard error across 5 seeds and 10 episodes. By examining how ROI changes with \u03b1, we can understand the trade-off between regularization and maximizing return in the algorithm.", "section": "D Impact of Various \u03b1"}, {"figure_path": "6Kg26g1quR/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison of ROIDICE with other offline algorithms. We average the scores and obtain \u00b12\u00d7 standard error using 1000 seeds. N denotes the number of trajectories within the dataset.", "description": "This figure compares the performance of ROIDICE against other offline RL algorithms (OptiDICE) and offline constrained RL algorithms (COptiDICE) across different numbers of trajectories in the dataset.  Subfigure (a) shows ROIDICE achieving a superior trade-off between return and cost compared to OptiDICE, which prioritizes maximizing return.  Subfigure (b) demonstrates that ROIDICE outperforms COptiDICE, which maximizes return under a cost constraint, particularly when the cost threshold is set appropriately. The results highlight ROIDICE's effectiveness in optimizing ROI by efficiently balancing return and cost.", "section": "5 Experiments"}, {"figure_path": "6Kg26g1quR/figures/figures_19_2.jpg", "caption": "Figure 6: For each tasks, we report average ROI, return, and cost return with \u00b12\u00d7 standard error with 5 seeds across 10 episodes.", "description": "This figure compares the performance of ROIDICE, OptiDICE, COptiDICE, and CDT across different tasks (locomotion and finance) and dataset qualities (medium and high). For each task and dataset, the figure shows the average ROI, return, and cost return over 10 episodes, with error bars representing the standard error across 5 seeds. This allows for a visual comparison of the different algorithms' performance in terms of efficiency (ROI), reward, and cost.", "section": "5.2 Results"}]