[{"type": "text", "text": "Regression under demographic parity constraints via unlabeled post-processing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gayane Taturyan   \nIRT SystemX, Universit\u00e9 Gustave Eiffel, Universit\u00e9 Paul-Sabatier   \ngayane.taturyan@univ-eiffel.fr ", "page_idx": 0}, {"type": "text", "text": "Evgenii Chzhen CNRS, Universit\u00e9 Paris-Saclay evgenii.chzhen@cnrs.fr ", "page_idx": 0}, {"type": "text", "text": "Mohamed Hebiri Universit\u00e9 Gustave Eiffel mohamed.hebiri@univ-eiffel.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We address the problem of performing regression while ensuring demographic parity, even without access to sensitive attributes during inference. We present a general-purpose post-processing algorithm that, using accurate estimates of the regression function and a sensitive attribute predictor, generates predictions that meet the demographic parity constraint. Our method involves discretization and stochastic minimization of a smooth convex function. It is suitable for online post-processing and multi-class classification tasks only involving unlabeled data for the post-processing. Unlike prior methods, our approach is fully theory-driven. We require precise control over the gradient norm of the convex function, and thus, we rely on more advanced techniques than standard stochastic gradient descent. Our algorithm is backed by finite-sample analysis and post-processing bounds, with experimental results validating our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithmic fairness is an umbrella term for a subset of machine learning research that aims to better understand, quantify, mitigate, evaluate, and conceptualize negative and/or positive effects of datadriven algorithms on the society. At least one direction in this field falls within theoretical machine learning, where a form of fairness constraint, mainly inspired by common sense and formalized within mathematical framework, is proposed as an arguably reasonable proxy for a definition of ethical and non-discriminatory prediction. Even more particular sub-field of this research direction is formalized within a paradigm of group fairness, that aims at mitigating negative impact (or provide equal treatment to) towards sub-populations that share a common sensitive characteristic. Many works fall within this category (Barocas et al., 2018, Calders et al., 2009, Chiappa et al., 2020, Dwork et al., 2011, Feldman et al., 2015, Gordaliza et al., 2019, Hardt et al., 2016, Jiang et al., 2020, Lum and Johndrow, 2016, Zafar et al., 2017, Zemel et al., 2013, just to name a few). ", "page_idx": 0}, {"type": "text", "text": "Even without going into debates on the relevance of a given definition of fairness, many, purely mathematical and algorithmic questions remain unanswered in this field. The best theoretical understanding of the problem is available for the demographic parity constraint in case of awareness\u2014 the situation when the sensitive attribute is available at inference time (Agarwal et al., 2019, Chiappa et al., 2020, Chzhen and Schreuder, 2020b, Chzhen et al., 2019, Denis et al., 2024, Gaucher et al., 2023, Le Gouic et al., 2020). The latter case is well studies both in classification and regression setups. This is no longer the case for other fairness constraints or the unawareness setup\u2014the situation when the sensitive attribute is not available at inference time. In particular, while the case of classification has been studied before from algorithmic and mathematical perspectives (Chzhen et al., 2019, Gaucher et al., 2023, Gordaliza et al., 2019, Hardt et al., 2016), the regression setup remains largely under explored and many methods lack strong theoretical evidences. In particular, to date, none of previous works effectively build computationally-efficient, fully theory-driven algorithm for the problem of regression under the demographic parity constraint in the case of unawareness. The present work fills this gap. Relying on previous ideas of discretization that goes back to Agarwal et al. (2019), we design a smooth convex objective function whose exact solution yields a fair and optimal prediction function. It turns out that this objective admits a first-order stochastic oracle that can be evaluated using only one independent sample of feature vector, thus allowing for stochastic optimization approach. Furthermore, despite the convexity, we show that the key quantity to control is the gradient (or rather a gradient-map) of this objective function, deviating from the more common setup of controlling the optimization error measured by the objective function. We deploy recent machinery of Allen-Zhu (2021) and Foster et al. (2019) that allows to ach\u221aieve this goal, properly setting all the hyper-parameters and recovering the usual statistical rate $1/\\sqrt{T}$ for both fairness and risk guarantees \u2014 $T$ being the number of samples. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our work falls withing the realm of post-processing methods\u2014another umbrella term that combines all the methods that perform a refitting of a base estimator to satisfy a certain constraint. ", "page_idx": 1}, {"type": "text", "text": "Importantly, due to the careful design of the above mentioned objective function, we can perform this post-processing in an online manner using a stream of i.i.d. unlabeled data without keeping it in memory, making it attractive in practice. Our approach is based on a combination of ideas from previous contributions to fairness from Agarwal et al. (2019) and Chzhen et al. (2020b) and recent stochastic optimization literature (Allen-Zhu, 2021, Foster et al., 2019) that deals with stationary point-type guarantees in the case of convex optimization. ", "page_idx": 1}, {"type": "text", "text": "Contributions Our contribution is three-fold: i) we significantly enhance the discretization strategy of Chzhen et al. (2020b) accommodating multiple sensitive features, relaxed fairness constraints, and unawareness setup; we introduce entropic regularization for this problem and design a dual convex objective from it; ii) we design a semi-supervised post-processing algorithm and show that it enjoys strong theoretical guarantees; iii) we perform numerical simulations demonstrating the relevance of our approach in practice. ", "page_idx": 1}, {"type": "text", "text": "Organization. This paper is organized as follows: in Section 2 we present the problem setup and introduce main problem-related notation; in Section 3 we describe our methodology step-by-step and highlight main challenges and relations to other results; in Section 4 we gives technical details of the proposed approach; Section 5 contains main theoretical results of the work; finally, Section 6 contains empirical evaluation of our method. All the proofs are postponed to the appendix. ", "page_idx": 1}, {"type": "text", "text": "Notation. Let us present generic notation that is used throughout this work. For a positive integer $K$ , we write $[K]$ to denote $\\{1,\\ldots,K\\}$ and $[\\![K]\\!]$ to denote $\\{-K,\\ldots,0,\\ldots,K\\}$ . For $a>0$ denote by $\\lfloor a\\rfloor$ largest non-negative integer that is smaller or equal to $a$ . For a univariate probability measure $\\mu$ , we denote by $\\operatorname{supp}(\\mu)$ its support. For every $\\beta>0,m\\in\\mathbb{N}$ , and $\\pmb{w}=(w_{1},\\ldots,w_{m})^{\\top}\\in\\mathbb{R}^{m}$ , we denote by $\\mathrm{LSE}_{\\beta}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ the log-sum-exp function, defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{LSE}_{\\beta}(\\pmb{w})=\\beta^{-1}\\log\\big(\\sum_{j=1}^{m}\\exp(\\beta w_{j})\\big)\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For every $m\\in\\mathbb{N},\\pmb{w}=(w_{1},\\dots,w_{m})^{\\top}\\in\\mathbb{R}^{m}$ , we denote by $\\pmb{\\sigma}=(\\sigma_{1},\\dots,\\sigma_{m}):\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ the soft-argmax as $\\begin{array}{r}{\\sigma_{j}(\\pmb{w})=\\exp(w_{j})/(\\sum_{i=1}^{m}\\exp(w_{i}))}\\end{array}$ . For any matrix $\\mathbf{A}$ , the notation $\\mathbf A\\geqslant0$ means that $\\mathbf{A}$ is positive coordinate-wise. For any $a\\in\\mathbb{R}$ and $\\pmb{w}\\in\\mathbb{R}^{m}$ we set $(a)_{+}=\\operatorname*{max}\\{0,a\\}$ and $(\\pmb{w})_{+}=((w_{1})_{+},\\dots,(w_{m})_{+})^{\\top}$ . The notationO  hides (unimportant) constants and polylogarithmic factors. For a pair of random elements $(A,B)$ ,  we denote by $\\operatorname{Law}(A)$ , the law of $A$ , by $\\bar{\\operatorname{Law}}(A\\mid B)$ , the conditional law of $A$ given $B$ , and we write $A\\perp\\!\\!\\!\\perp B$ to denote that variables $A$ and $B$ are independent. For two vectors $\\pmb{w},\\pmb{w}^{\\prime}\\,\\in\\,\\mathbb{R}^{m}$ , we write $w/w^{\\prime}=(w_{j}/w_{j}^{\\prime})_{j\\in[m]}\\,\\in\\,\\mathbb{R}^{m}$ to denote element-wise division. The Euclidean norm of a vector and the Frobenius norm of a matrix are denoted by $\\|\\cdot\\|$ , while the spectral norm of a matrix is denoted by $\\|\\cdot\\|_{\\mathrm{op}}$ . We denote by $\\beta(\\mathbb{R})$ , the Borel sigma-algebra on $\\mathbb{R}$ , induced by the usual topology. We write log to denote the natural logarithm and $\\log_{a}$ , the base $a>0$ logarithm. ", "page_idx": 1}, {"type": "text", "text": "2 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $(X,S,Y)$ be a triplet of nominally non-sensitive, nominally sensitive, and output characteristics, taking values in $\\mathbb{R}^{d}\\,\\times\\,[K]\\,\\times\\,\\mathbb{R}$ for some $K\\ \\geqslant\\ 2$ . We assume that $(X,S,Y)\\,\\sim\\,\\mathbb{P}$ , for some unknown distribution $\\mathbb{P}$ . The main quantities of interest are the following: the regression function $\\eta({\\pmb x})~{\\stackrel{\\mathrm{def}}{=}}~\\mathbb{E}[Y~|~X={\\pmb x}]$ ; the marginal distribution of sensitive vectors $p\\ {\\stackrel{\\mathrm{def}}{=}}\\ (p_{s})_{s\\in[K]}$ with $p_{s}~\\stackrel{\\mathrm{def}}{=}$ $\\mathbb{P}(S\\ =\\ s)$ ; the conditional distribution of $S$ given $\\mathbf{\\deltaX}$ , defined as $\\pmb{\\tau}(\\pmb{x})\\ \\stackrel{\\mathrm{def}}{=}\\ (\\tau_{s}(\\pmb{x}))_{s\\in[K]}$ with $\\tau_{s}(\\pmb{x})\\stackrel{\\mathrm{def}}{=}\\mathbb{P}(S=s\\mid\\pmb{X}=\\pmb{x})$ . A randomized prediction function is a map $\\pi:B(\\mathbb{R})\\times\\mathbb{R}^{d}\\to[0,1]$ such that the map $B\\mapsto\\pi(B\\mid x)$ for $B\\,\\in\\,{\\mathcal{B}}(\\mathbb{R})$ is a probability measure on $\\left(\\mathbb{R},\\beta(\\mathbb{R})\\right)$ for all $\\pmb{x}\\in\\mathbb{R}^{d}$ . For any prediction $\\pi$ we define a random variable ${\\widehat{Y}}_{\\pi}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{Law}\\left({\\widehat{Y}}_{\\pi}\\mid X=x,S=s\\right)=\\pi(\\cdot\\mid x)\\quad x\\in\\mathbb{R}^{d},s\\in[K]\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 2.1. Note that if $\\textstyle{\\boldsymbol{\\pi}}(\\cdot\\mid{\\boldsymbol{x}})$ is a Dirac measure for all $\\pmb{x}\\in\\mathbb{R}^{d}$ , the above condition just means that $\\widehat{Y}_{\\pi}=g(X)$ almost surely for some deterministic $g:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ . The above condition is not to be confused with the fairness constraint, which is not formulated point-wise. It is only viewed as an extension of the unawareness framework to the case of randomized predictions. The above condition completely specifies the distribution of the triplet $(\\bar{\\boldsymbol{X}_{}},\\bar{\\boldsymbol{S}_{}},\\widehat{\\boldsymbol{Y}}_{\\pi})$ but leaves the relation between ${\\widehat{Y}}_{\\pi}$ and $Y$ ambiguous. To be more formal, one needs to add the condition $(\\widehat{Y}_{\\pi}\\perp Y)\\mid(X,S),$ , that is, the prediction ${\\widehat{Y}}_{\\pi}$ is independent from the true label $Y$ , conditionally on $(X,S)$ . That would define $a$ complete joint distribution of $(X,S,Y,\\widehat{Y}_{\\pi})\\sim\\mathbb{P}_{\\pi}=\\mathbb{P}_{(X,S)}\\otimes\\mathbb{P}_{Y\\mid(X,S)}\\otimes\\pi(\\cdot\\mid X)$ . ", "page_idx": 2}, {"type": "text", "text": "We consider the following risk of a prediction function $\\pi$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pi)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}[(\\widehat{Y}_{\\pi}-\\eta(\\pmb{X}))^{2}]=\\mathbb{E}\\left[\\int_{\\mathbb{R}}(\\widehat{y}-\\eta(\\pmb{X}))^{2}\\pi(\\mathrm{d}\\,\\widehat{y}\\mid\\pmb{X})\\right]\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A prediction function $\\pi$ is said to satisfy the demographic parity constraint, if $\\widehat{Y}_{\\pi}\\perp\\!\\!\\!\\!\\perp S$ . ", "page_idx": 2}, {"type": "text", "text": "That is, ${\\widehat{Y}}_{\\pi}$ is stochastically independent of $S$ viewed from the perspective of the joint distribution of $(X,S,{\\widehat{Y}}_{\\pi})$ . On the high-level, the goal in this setup is to find a prediction function $\\pi$ , whose risk is small a nd whose violation of the demographic parity constraint is controlled as quantified by some measure of unfairness. The above problem is well understood in the case of awareness\u2014the situation when $\\pi$ is expressed as $\\pi(\\cdot\\mid x,\\bar{s})$ (Chiappa et al., 2020, Chzhen et al., 2020a, 2021, Jiang et al., 2020, Le Gouic et al., 2020)\u2014revealing an intimate connection of this problem with Wasserstein barycenters. Yet, when the sensitive attribute is not an input of the prediction function, the situation is drastically different. Some attempts have been made to either (so far only partially) characterise the optimal prediction function (Chzhen and Schreuder, 2020a, Gaucher et al., 2023, Zhao, 2021) or to design efficient algorithms for this problem (Agarwal et al., 2019, Maheshwari and Perrot, 2022, Narasimhan et al., 2020) that are only partially supported by a sound theory. One of the principal goals of this work is to design a computationally efficient algorithm that admits a (near) end-to-end theoretical guarantees. The main difficulty of the problem lies in very different natures of the risk and the fairness constraint\u2014the latter involves image measures, while the former is a simple linear functional of $\\pi$ . In the case of awareness this issue can be bypassed by lifting the problem in the space of measures, working there directly and, then, returning to the initial space of prediction functions. Crucially, this is achieved only thanks to the fact that $S$ is known at inference time, which is not the case for the considered problem. ", "page_idx": 2}, {"type": "text", "text": "Remark 2.2. In what follows we will exclusively focus on the squared risk and the regression setup. However, one can observe that the proposed methodology can be extended or even simplified for $\\mathcal{R}(\\pi)=\\mathbb{E}[r(\\pmb{X},\\widehat{Y}_{\\pi})]$ and multi-class classification respectively under the demographic parity constraint. Here $r(x,\\widehat{y})$ quantifies fit of $\\widehat{y}$ for an individual $\\textbf{\\em x}$ and can be either known or unknown. ", "page_idx": 2}, {"type": "text", "text": "3 Our methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The starting point of our work is similar to the one of Chzhen et al. (2020b) and relies on a simple observation\u2014if $|\\operatorname{supp}(\\pi(\\cdot\\mid x))|<\\infty$ and stays the same for all $\\textbf{\\em x}$ , the independence constraint is reduced to a finite amount of constraints that only involve the image of $\\pi(\\cdot\\mid x)$ . In particular, assuming that $\\operatorname{supp}(\\pi(\\cdot\\mid x))={\\widehat{\\mathcal{P}}}\\subset\\mathbb{R}$ for all $\\pmb{x}\\in\\mathbb{R}^{d}$ , ${\\widehat{Y}}_{\\pi}$ is independent from $S$ iff $\\mathbb{P}(\\widehat{Y}_{\\pi}=\\widehat{y}\\mid$ $S=s)=\\mathbb{P}(\\widehat{Y}_{\\pi}=\\widehat{y})$ for all $s\\in[K]$ and all $\\widehat{y}\\in\\widehat{\\mathcal{Y}}$ . In view of the definition of ${\\widehat{Y}}_{\\pi}$ , the latter is equivalent to ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\pi(\\widehat{y}\\mid X)\\mid S]=\\mathbb{E}[\\pi(\\widehat{y}\\mid X)]\\qquad s\\in[K],\\,\\widehat{y}\\in\\widehat{\\mathcal{Y}}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which, assuming that $\\widehat{\\boldsymbol{y}}$ is fixed, correspond to linear constraints on $\\pi$ . Combined with the observation that $\\pi\\mapsto\\,{\\mathcal{R}}(\\pi)$ is also linear, we end up with a problem that is significantly easier to handle. Furthermore, again assuming that $\\widehat{\\boldsymbol{y}}$ is fixed, the sketched direction gives a natural way to introduce some slack to the independence co nstraint\u2014simply requiring an approximate equality in (1). Set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{U}_{s}(\\pi,\\widehat{y})\\overset{\\mathrm{def}}{=}|\\mathbb{E}\\left[\\pi(\\widehat{y}\\mid X)\\mid S=s\\right]-\\mathbb{E}\\left[\\pi(\\widehat{y}\\mid X)\\right]|\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $s\\in[K]$ and $\\widehat{y}\\in\\widehat{\\mathcal{Y}}$ . Thus, for a fixed support (whose choice will be discussed in the next paragraph) and a fixed vector $\\boldsymbol{\\varepsilon}\\stackrel{\\mathrm{def}}{=}(\\varepsilon_{1},\\dots,\\varepsilon_{K})^{\\top}$ , our goal is to build an estimator of a solution to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi:B(\\mathbb{R})\\times\\mathbb{R}^{d}\\to[0,1]}\\left\\{\\mathcal{R}(\\pi)\\,:\\,\\operatorname{supp}(\\pi(\\cdot\\,|\\,\\,x))=\\widehat{\\mathcal{V}}\\,\\mathrm{for}\\,x\\in\\mathbb{R}^{d},\\quad\\mathcal{U}_{s}(\\pi,\\widehat{y})\\leqslant\\varepsilon_{s}\\,\\,\\mathrm{for}\\,\\widehat{y}\\in\\widehat{\\mathcal{V}},s\\in[K]\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let us now describe the methodology for selecting $\\widehat{\\boldsymbol{y}}$ and the trade-offs that are introduced. ", "page_idx": 3}, {"type": "text", "text": "Introducing discretization. Having in mind the above discussion, for every integer $L\\geqslant0$ and real $B>0$ , we introduce a uniform grid $\\widehat{\\mathcal{V}}_{L}\\stackrel{\\mathrm{def}}{=}B\\cdot\\mathbb{L}\\mathbb{J}/L$ on $[-B,B]$ , so that $|\\widehat{\\mathcal{D}}_{L}|=2L+1$ , which is viewed as a support of prediction functions $\\pi(\\cdot\\mid x)$ . For the sake of simplicity, we will assume that the regression function $\\eta(\\cdot)$ is bounded in $[-B,B]$ for some known $B>0$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Bounded signal). There exists $B>0$ such that $|\\eta(\\boldsymbol{X})|\\leqslant B$ almost surely. ", "page_idx": 3}, {"type": "text", "text": "Thus, for a given $B$ , the main parameter to tune is $L\\geqslant1$ \u2014the higher the $L$ is, the more accurate prediction functions can be produced, while lower values of $L$ ensure that the demographic parity requirement reduces to a small number of constraints. Thus, there is a trade-off that is introduced by $L$ . A natural attempt to tackle the problem of fairness in this context would be to estimate a solution to (3) with $\\widehat{\\boldsymbol{y}}=\\widehat{\\boldsymbol{y}_{L}}$ . Of course, $L$ needs to be chosen so that the aforementioned solution attains the risk tha t  is cl o se to the risk of some benchmark prediction function that does not involve any discretization. This will be discussed later in the text. For now, let us address another subtle issue. Even assuming a complete knowledge of the underlying distribution $\\mathbb{P}$ , solving (3) requires solving a linear program in dimension $\\Omega(L K)$ which can be infeasible in practice for large values of $L$ and $K$ . Instead of (3), we rather focus on the entropic regularized version of it. For $\\bar{\\beta}>0$ , we consider ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi:B(\\mathbb{R})\\times\\mathbb{R}^{d}\\to[0,1]}\\left\\{\\mathcal{R}_{\\beta}(\\pi)\\,:\\,\\operatorname{supp}(\\pi(\\cdot\\,|\\,\\,x))=\\widehat{\\mathcal{P}}\\,\\mathrm{for}\\,x\\in\\mathbb{R}^{d},\\quad\\mathcal{U}_{s}(\\pi,\\widehat{y})\\leqslant\\varepsilon_{s}\\,\\,\\mathrm{for}\\,\\widehat{y}\\in\\widehat{\\mathcal{Y}},s\\in[K]\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{R}_{\\beta}(\\pi)=\\mathcal{R}(\\pi)+\\frac{1}{\\beta}\\mathbb{E}[\\Psi(\\pi(\\cdot\\mid X))]}\\end{array}$ and for any discrete univariate distribution $\\mu$ , we define its negative entropy $\\begin{array}{r}{\\Psi(\\mu)\\stackrel{\\mathrm{def}}{=}\\sum_{\\widehat{y}\\in\\mathrm{supp}(\\mu)}\\mu(\\widehat{y})\\log(\\mu(\\widehat{y})).}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Remark 3.1 (On abuse of notation). Note that for every $\\widehat{\\boldsymbol{y}}\\in\\widehat{\\mathcal{D}}_{L}$ there is a unique $\\ell\\in[L]$ such that $\\widehat{\\boldsymbol{y}}=\\ell\\boldsymbol{B}/L$ and we will write $\\pi(\\boldsymbol{\\ell}\\mid\\boldsymbol{x})$ instead of $\\pi(\\widehat{\\boldsymbol{y}}\\mid\\boldsymbol{x})$ . Similarly, we write $\\mathcal{U}_{s}(\\pi,\\ell)$ instead of $\\mathcal{U}_{s}(\\pi,\\widehat{y})$ , defined in (2), when no confusion is possible and the support $\\widehat{\\boldsymbol{y}}_{L}$ is fixed. ", "page_idx": 3}, {"type": "text", "text": "An extremely attractive feature of the problem in (4) is the fact that the solution to it can be written explicitly as a function of optimal dual variables, with the latter being a solution of a stochastic convex program with Lipschitz gradient\u2014the main observation of our approach, that shares many similarities with the smoothing technique of Nesterov (2005). This is summarized in the following lemma. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. Let $L\\in\\mathbb N$ and $\\beta>0$ . Let $\\pmb{\\Lambda}^{\\star}=(\\lambda_{\\ell s}^{\\star})_{\\ell\\in[L],s\\in[K]}$ and $\\mathbf{V}^{\\star}=(\\nu_{\\ell s}^{\\star})_{\\ell\\in[L],s\\in[K]}$ be two matrices that are solutions to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda,\\nabla\\geqslant0}\\left\\{F(\\mathbf{\\Lambda},\\mathbf{V})\\stackrel{\\mathrm{def}}{=}\\mathbb{E}\\left[\\mathrm{LSE}_{\\beta}\\left(\\big(\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\,t(\\mathbf{X})\\rangle-r_{\\ell}(\\mathbf{X})\\big)_{\\ell\\in[L]}\\right)\\right]+\\sum_{\\ell\\in[L]}\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\rangle\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{{\\pmb t}({\\pmb x})\\,\\,{\\stackrel{\\mathrm{def}}{=}}\\,1-\\,\\frac{{\\pmb\\tau}({\\pmb x})}{p}}\\end{array}$ , $\\begin{array}{r}{r_{\\ell}(\\pmb{x})\\stackrel{\\mathrm{def}}{=}\\left(\\eta(\\pmb{x})-\\frac{\\ell B}{L}\\right)^{2}}\\end{array}$ , and $\\pmb{\\lambda}_{\\ell}=(\\lambda_{\\ell s})_{s\\in[K]}$ , $\\pmb{\\nu}_{\\ell}=(\\nu_{\\ell s})_{s\\in[K]}$ . Then, (4) admits a solution in the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\Lambda^{\\star},\\mathbf{V}^{\\star}}(\\ell\\mid x)\\overset{\\mathrm{def}}{=}\\sigma_{\\ell}\\left(\\beta\\left(\\langle\\lambda_{\\ell^{\\prime}}^{\\star}-\\nu_{\\ell^{\\prime}}^{\\star},\\,t(x)\\rangle-r_{\\ell^{\\prime}}(x)\\right)_{\\ell^{\\prime}\\in\\left[\\![L]\\!\\right]}\\right)\\,f o r\\,\\ell\\in\\left[\\![L\\!]\\!\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming perfect knowledge of $\\eta$ and $\\tau$ , the above lemma suggests a natural approach to estimating the $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ \u2014we can run a (version of) stochastic gradient descent on $F(\\cdot,\\cdot)$ and then plug-in the resulting dual variables in the formula for $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ . Notably, a stochastic gradient of $F(\\cdot,\\cdot)$ can be obtained by simply sampling one $\\mathbf{\\deltaX}$ from $\\mathbb{P}_{X}$ \u2014it does not require labels for this step. Yet, even in the above idealized case, it is not clear which optimization criteria would allow us to prove that the resulting solution would yield good properties in terms of risk and fairness. As we will see, despite the problem in (5) being convex with Lipschitz gradient, it is crucial to control the norm of the gradient of $F$ for good statisitcal properties of the algorithm. That goes without saying that this relaxation has its price\u2014the smaller the regularization parameter $\\beta$ the less accurate the resulting solution, but the resulting dual optimization problem is easier and vice-versa. ", "page_idx": 4}, {"type": "text", "text": "Properties of $F$ and $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ . Let us summarized key properties of the objects introduced in Lemma 3.1. The first two results concern the population properties of $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ : ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 (Fairness quantification). Let $L\\in\\mathbb N$ , $\\pmb{\\varepsilon}=(\\varepsilon_{s})_{s\\in[K]}\\in[0,1]^{K},\\beta>0,$ , and $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ be defined in Lemma 3.1. Then, $\\mathcal{U}_{s}(\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}},\\ell)\\leqslant\\varepsilon_{s}$ for all $s\\in[K],\\ell\\in[L]$ . ", "page_idx": 4}, {"type": "text", "text": "In words, the optimal entropic-regularized prediction function is feasible for (3), that is, it satisfies the relaxed fairness constraints as quantified by (2). Furthermore, we can show that its risk is also controlled by the regularization parameter $\\beta>0$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3 (Risk gain). Let $L\\;\\in\\;\\mathbb{N},\\beta\\;>\\;0$ , and $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ be defined in Lemma 3.1. For any $\\pi:B(\\mathbb{R})\\times\\mathbb{R}^{d}\\to[0,1]$ that is feasible for (3), we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\pi_{\\Lambda^{\\star},\\mathbf V^{\\star}})\\leqslant\\mathcal{R}(\\pi)+\\frac{\\log|\\widehat{\\mathcal{V}}_{L}|}{\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above result is rather instructive, it quantifies the price of the introduced regularization. Intuitively, one wants to set $\\beta$ high enough, so that the additive term in the above bound is vanishing. Unfortunately, we cannot set it arbitrarily high, since it will introduce instabilities from the optimization perspective\u2014the function $F$ becomes less regular as $\\beta$ growth. This is summarized below. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.4 (Regularity of F). Let \u03c32 d=ef 2  s\u2208[K]1\u2212psp s . The objective function in (5) is convex and its gradient is $(\\beta\\sigma^{2})$ -Lipschitz. ", "page_idx": 4}, {"type": "text", "text": "As mentioned, we see that the larger the $\\beta$ is, the less regular the function $F$ is, making it harder to minimize. Thus, $\\beta\\geqslant0$ controls the trade-off between the optimization error and statistical bias. ", "page_idx": 4}, {"type": "text", "text": "Gradient of $F$ is crucial. Let us show that the control of the gradient of $F$ is the most important and non-trivial part that allows to demonstrate strong statistical properties of the plug-in rule derived from the above strategy. ", "page_idx": 4}, {"type": "text", "text": "To this end, let us introduce parametric family of prediction functions, defined for any $\\mathbf{A},\\mathbf{V}\\geqslant0$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\Lambda,\\mathbf{V}}({\\boldsymbol{\\ell}}\\mid{\\boldsymbol{x}})\\overset{\\mathrm{def}}{=}\\sigma_{\\boldsymbol{\\ell}}\\left(\\beta\\left(\\langle\\lambda_{{\\boldsymbol{\\ell}}^{\\prime}}-\\nu_{{\\boldsymbol{\\ell}}^{\\prime}},\\,t({\\boldsymbol{x}})\\rangle-r_{{\\boldsymbol{\\ell}}^{\\prime}}({\\boldsymbol{x}})\\right)_{{\\boldsymbol{\\ell}}^{\\prime}\\in\\left[L\\right]}\\right)\\;\\mathrm{for}\\;{\\boldsymbol{\\ell}}\\in\\left[L\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We want to show that if $\\mathbf{A},\\mathbf{V}\\geqslant0$ is nearly stationary point of $F$ , then $\\pi_{\\boldsymbol{\\Lambda},\\mathbf{V}}$ is nearly optimal in terms of risk and its violation of the demographic parity constraint is controlled. Note that the optimization problem in (5) is constrained, thus, unless the minimum lies in the interior of the domain, we cannot hope for the gradient of $F$ to go to zero. Instead, we introduce gradient mapping\u2014a quantity that shares many properties of the gradient in the case of constraint optimization problem. For $\\alpha>0$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{G}}_{\\alpha}\\left(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{\\boldsymbol{V}}\\right)\\stackrel{\\mathrm{def}}{=}\\frac{\\left(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{\\boldsymbol{V}}\\right)-\\left(\\left(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{\\boldsymbol{V}}\\right)-\\alpha\\nabla F\\left(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{\\boldsymbol{V}}\\right)\\right)_{+}}{\\alpha}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our main observation is summarized in the next lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.5. Let \u03c32 d=ef 2  s\u2208[K]1\u2212psp s, $L\\in\\mathbb{N},\\,\\Lambda,\\mathbf{V}\\geqslant0,$ , then for any $\\alpha>0,\\beta>0,$ , the unfairness of $\\pi_{\\boldsymbol{\\Lambda},\\mathbf{V}}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in[L]|s\\in[K]}\\left(\\mathcal{U}_{s}\\big(\\pi_{\\Lambda,\\mathbf{V}},\\ell\\big)-\\varepsilon_{s}\\right)_{+}^{2}\\leqslant\\left\\|G_{\\alpha}(\\Lambda,\\mathbf{V})\\right\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pi_{\\mathbf{A},\\mathbf{V}})\\leqslant\\mathcal{R}(\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}})+\\left(\\|(\\mathbf{A},\\mathbf{V})\\|+\\alpha\\left\\{\\sigma+\\|\\varepsilon\\|\\sqrt{2|\\widehat{y}_{L}|}\\right\\}\\right)\\|G_{\\alpha}(\\mathbf{A},\\mathbf{V})\\|+\\frac{\\log|\\widehat{y}_{L}|}{\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 3.5 is very instructive on its own\u2014we can obtain a good estimator of $\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ in terms of risk and unfairness by performing stochastic optimization on $F$ and controlling the norm of gradient mapping for a suitable parameter $\\alpha\\,>\\,0$ . The final choice of the parameter $\\alpha$ will depend on the optimization algorithm used and will be purely theoretical. In particular, for our purposes, it is sufficient to guarantee an existence of some value of $\\alpha>0$ that yields desired statistical properties. A naive approach in doing so relies on a well-known relation between $F(\\mathbf{A},\\mathbf{V})-F(\\mathbf{\\boldsymbol{\\Lambda}}^{\\star},\\bar{\\mathbf{V}}^{\\star})$ and $\\|G_{\\alpha}(\\mathbf{A},\\bar{\\mathbf{V}})\\|^{2}$ using the Lipshitzness of the gradient of $F$ (see e.g., Beck, 2014, Lemma 9.11). More concretely, forgetting about the constraints1, one has ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\mathbf{A},\\mathbf{V})\\|^{2}\\leqslant2M\\big(F(\\mathbf{A},\\mathbf{V})-F(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\big)\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M$ is the Lipschitz constant of $\\nabla F$ . Thus, the above inequality suggests that it is sufficient to control the standard optimization error in order to control the norm of the gradient. Unfortunately this approach is deemed to fail for two reasons: the first being that we control only the squared norm of the gradient map and not the norm itself, thus loosing in the rate of convergence; the second, and more subtle reason, is the separation of the purely \u201cstatistical\u201d rate that depends only on the variance of the stochastic gradient and scales as $1/\\sqrt{T}$ , with $T$ being the number of future samples from $\\mathbb{P}_{X}$ , and \u201coptimization\u201d rate of convergence that depends on $M$ and the diameter of the problem and typically scales as $1/T$ or even $1/\\Bar{T}^{2}$ if acceleration is used. ", "page_idx": 5}, {"type": "text", "text": "Indeed, in our setup, Lipschitz constant $M$ of $\\nabla F$ is not a fixed constant, but a para\u221ameter to be set\u2014it relate\u221as to $\\beta$ (cf. Lemma 3.4). Ideally, seeing Lemma 3.3, \u221awe want to set $\\beta=\\Theta({\\sqrt{T}})$ , leading to $M=\\Omega({\\sqrt{T}})$ . Thus, in view of (9), a term of the form $M/\\sqrt{T}$ appears in the convergence rate, which destroys consistency of the resulting estimator. Arguably, this is less of an issue in case of convex optimization with constant Lipschitz constant $M$ , especially if we only want the norm to go to zero. This discussion highlights that it is crucial to keep the separation between the statistical part of the rate and the optimization part of the rate, while controlling the norm of the gradient. Lucky for us, it is known that for convex problems one can indeed control the gradient mapping keeping this separation of the rate (Allen-Zhu, 2021, Foster et al., 2019). Note that it is not the case for non-convex problems as demonstrated by Arjevani et al. (2023). ", "page_idx": 5}, {"type": "text", "text": "Summary of our approach and why is it different from others. Now, keeping in mind the above, rather long justification, we are in position to sketch our approach and the formal presentation is deferred to the next section. For well selected parameters $\\beta\\,>\\,0$ , $L\\,\\in\\,\\mathbb{N}$ , we are going to perform stochastic optimization of $F$ , relying on the SGD3 algorithm of Allen-Zhu (2021). In order to compute the stochastic gradient of $F$ , we are simply going to sample one $\\mathbb{P}_{X}$ and it appears that this stochastic gradient has a well-behaved variance (see Appendix B-C for details). To make our approach completely data-driven (or at least to understand the order of magnitude of the parameters), we will compute or bound all the oracle quantities that appear in the used optimization algorithm (essentially related to the step-size tuning). We will show that for any sufficiently small $\\alpha\\,>\\,0$ , the term $\\underline{{\\operatorname{\\bar{E}}}}\\|G_{\\alpha}(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\|^{2}$ is controlled and then rely on Lemma 3.5 and some additional results to demonstrate that the  resulting $\\pi_{\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}}}$ possesses good statistical properties. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.2 (On the dynamic of algorithm). Note that for $\\mathbf{A}=\\mathbf{V}=\\mathbf{0}$ , the corresponding ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(\\pi_{\\mathbf{0},\\mathbf{0}}(\\ell\\mid\\mathbf{x})\\right)_{\\ell\\in[L]}=\\pmb{\\sigma}\\left(\\beta\\left(-(\\eta(\\pmb{x})-\\ell^{\\prime}B/L)^{2}\\right)_{\\ell^{\\prime}\\in[L]}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "That is, the above prediction puts the most amount of mass on the atom $\\ell$ which minimizes $(\\eta({\\pmb x})-$ $\\ell B/L)^{2}$ \u2014the most accurate, but unfair prediction. Since our algorithm is based on a SGD-type algorithm, initialized at $\\mathbf{A}_{0}=\\mathbf{V}_{0}=\\mathbf{0}$ , then we expect that during the dynamic of the algorithm, the risk of $\\pi_{\\boldsymbol{\\Lambda}_{t},\\mathbf{V}_{t}}$ increases, while the unfairness decreases. This phenomena coincides with the intuition of post-processing\u2014we want to gain in fairness, while sacrificing some accuracy. ", "page_idx": 5}, {"type": "text", "text": "As it has been already mentioned, the idea of discretizing the image of (randomized) predictions is not novel and has been successfully deployed by Agarwal et al. (2019) for an in-processing estimator and by Chzhen et al. (2020b) for a post-processing estimator. We use this insight as a building block, but significantly deviate from both algorithms. Compared to Agarwal et al. (2019), our algorithm is positioned in the realm of post-processing and even online post-processing, where i.i.d. samples from $\\mathbb{P}_{X}$ comes in a stream and we do not need to store them in memory. Also, while their algorithm is partially inspired by theory, the same theory suggests that this algorithm is not computationally efficient and it relies on some black-box parts that assume perfect solutions to some optimization problems. That being said, the algorithm of Agarwal et al. (2019) seem to be the gold standard method for the generic in-processing method in this problem. Compared to Chzhen et al. (2020b), we have made a sequence of improvements. First, our setup is unawareness, which is not the case in their paper; second, our algorithm is able to handle multiple protected attributes as well as approximate fairness constraints; finally, and most importantly, we do not make black-box assumptions about having access to exact minimizers of convex problems and provide an end-to-end analysis of out approach. Let us also remark that our method cannot be considered as a simple extension of Chzhen et al. (2020b) as we rely on different phenomenons and provide a very different algorithm. On a more subjective note, we believe that our approach is a nice example of a real convex optimization problem, where the norm of the gradient plays the central role, while the optimization error in term of the objective function does not matter2. This is precisely the phenomena highlighted by Nesterov (2012). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Proposed algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{\\mathrm{\\bfAlgorithm\\1!\\cdotDP\\;\\;post-processing}(L,T,\\beta,{\\pmb p},B,\\eta,\\tau)}\\end{array}$   \n1: Input: discretization parameter $L\\geqslant1$ ; regularization $\\beta>0$ , number of stochastic gradient evaluations $T\\geqslant1$ ; marginal distribution $\\pmb{p}$ of $S$ ; regression function $\\eta$ ; conditional distribution $\\tau$ of $S\\mid X$ ; bound $B>0$ on $\\eta$ .   \n2: Build uniform grid $\\widehat{\\boldsymbol{y}}_{L}$ over $[-B,B]$ ;   \n3: Set parameters: \u03c32  = 2 s\u2208[K]1\u2212psp s , $M=\\beta\\sigma^{2}$ ; 4: Set $(\\mathbf{A},\\mathbf{V})\\mapsto F(\\mathbf{A},\\mathbf{V})$ as defined in Lemma 3.1   \n5: Run a black-box optimizer $A(F,\\sigma^{2},M,T)$ on function $F$ having access to $T$ stochastic gradient evaluations (see (11)) with variance $\\sigma^{2}$ and smoothness parameter $M$ to obtain $(\\widehat{\\bf A},\\widehat{\\bf V})$ ;   \n6: return $\\pi_{(\\widehat{\\Lambda},\\widehat{\\mathbf{V}})}(\\cdot\\mid\\cdot)$ as defined in (7); ", "page_idx": 6}, {"type": "text", "text": "In this section, we provide all the details about the proposed algorithm in case $\\eta$ and $\\tau$ are known. If they are unknown, these quantities are replaced by their estimates $\\widehat{\\eta}$ and $\\widehat{\\tau}$ that are constructed on a separate labeled dataset. First, for $\\mathbf{A}=\\bar{(\\lambda_{\\ell s})}_{\\ell\\in[\\![L]\\!],s\\in[K]},\\mathbf{V}=(\\nu_{\\ell s})_{\\ell\\in[\\![L]\\!],s\\in[K]},$ , let us provide the expression for the gradient of $F$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\perp\\varepsilon_{s}}F(\\mathbf{A},\\mathbf{V})=\\triangle\\mathbb{E}\\left[\\sigma_{\\ell}\\left(\\beta\\left(\\langle\\lambda_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\,t(X)\\rangle-r_{\\ell^{\\prime}}(X)\\right)_{\\ell^{\\prime}=-L}^{L}\\right)t_{s}(X)\\right]+\\varepsilon_{s}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\square\\,\\in\\,\\{\\lambda,\\nu\\}$ and $\\triangle\\,=\\,1$ if $\\sqsubseteq=\\lambda$ and $\\triangle\\,=\\,-1$ otherwise. Thus, a stochastic gradient $g(\\Lambda,\\mathbf{V})=(g_{\\lambda\\ell s}(\\Lambda,\\mathbf{V}),g_{\\nu\\ell s}(\\Lambda,\\mathbf{V}))_{\\ell\\in[L],s\\in[K]}$ of $F$ at a point $(\\mathbf{\\cal{A}},\\mathbf{\\cal{V}})$ can be computed by erasing expectation in (10), i.e., by sampling one $\\bar{X}\\sim\\mathbb{P}_{X}$ , using the same convention as above about $\\sqcap,\\triangle$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\ng_{\\Omega_{\\ell s}}(\\mathbf{A},\\mathbf{V})=\\triangle\\sigma_{\\ell}\\left(\\beta\\left(\\langle\\mathbf{\\lambda}\\mathbf{\\lambda}_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\,t(\\mathbf{X})\\rangle-r_{\\ell^{\\prime}}(\\mathbf{X})\\right)_{\\ell^{\\prime}=-L}^{L}\\right)t_{s}(\\mathbf{X})+\\varepsilon_{s}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The next result controls the variance of the above stochastic gradient. ", "page_idx": 6}, {"type": "text", "text": "The proposed method is summarized in Algorithm 1. It uses a black-box stochastic optimization algorithm $\\boldsymbol{\\mathcal{A}}$ , that operates on a convex function $F$ and a stochastic first-order oracle. The stochasticfirst order oracle is implemented by (11) and only requires to sample $X\\sim\\mathbb{P}$ in an i.i.d. manner. We also pass two additional parameters to this algorithm: namely, we pass the variance $\\sigma^{2}$ from Lemma 4.1 and the Lipschitz constant of the gradient of $F$ from Lemma 3.4. Then one can use any such algorithm. However, as shown in Lemma 3.5, those algorithms that are tailored to control expected norm of gradient mapping are preferred. For example, one can use SGD3 of Allen-Zhu (2021) or an improved version of Foster et al. (2019) that relies on restarted accelerated SGD of Ghadimi and Lan (2012). ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical guarantees. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let us first provide main results for Algorithm 1 assuming that $\\eta$ and $\\tau$ are known. Note that Algorithm 1 can rely on any optimization algorithm. We provide a complete analysis using a refined version of SGD3 algorithm of Allen-Zhu (2021) that is due to Foster et al. (2019) with additional modifications taking into account the specific structure of our problem. We state the main result in existential form and postpone all the details on the implementation of the algorithm and a primer on optimization to the supplementary material (Appendix C-D). ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. Let \u03b5 = (\u03b5s)s\u2208[K] \u2208[0, 1]K and \u03c32 = 2 s\u2208[K](1 \u2212ps)/ps. Setting \u03b2 =8 logT2(T ) and $L={\\sqrt{T}}$ , there exists an optimizer $\\boldsymbol{\\mathcal{A}}$ to be used in Algorithm $^{\\,l}$ that, for $T$ larger than some absolute constant, ensures ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{E}^{1/2}\\Bigg[\\sum_{\\ell\\in[L]\\,s\\in[K]}\\left(\\mathcal{U}_{s}\\big(\\pi_{\\Lambda,\\mathbf{V}},\\ell\\big)-\\varepsilon_{s}\\right)_{+}^{2}\\Bigg]\\leqslant\\widetilde{\\mathcal{O}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\left(1+\\frac{\\sigma}{\\sqrt{T}}\\,\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\|\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Furthermore, $i f$ Assumption 3.1 is satisfied and let ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}^{\\star}\\underset{h\\colon\\mathbb{R}^{d}\\rightarrow[-B,B]}{\\mathrm{inf}}\\left\\lbrace\\mathcal{R}(h)\\,:\\,\\underset{t\\in\\mathbb{R}}{\\mathrm{sup}}\\,\\vert\\mathbb{P}(h(X)\\leqslant t\\,|\\,S=s)-\\mathbb{P}(h(X)\\leqslant t)\\vert\\leqslant\\frac{\\varepsilon_{s}}{2},\\quad\\forall s\\in[K]\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $\\mathcal{E}(\\pi_{\\widehat{\\Lambda},\\widehat{\\mathbf{V}}})\\overset{\\mathrm{def}}{=}\\mathbb{E}\\left[\\mathcal{R}(\\pi_{\\widehat{\\Lambda},\\widehat{\\mathbf{V}}})\\right]-\\mathcal{R}^{\\star},$ , then for the same algorithm ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\pi_{\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}}})\\leqslant\\widetilde{\\mathcal{O}}\\left(\\left(\\frac{\\sigma}{\\sqrt{T}}\\mathbf{E}^{1/2}\\left[\\|(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\|^{2}\\right]+\\frac{\\|\\varepsilon\\|}{T^{5/4}}\\right)\\left(1+\\frac{\\sigma}{\\sqrt{T}}\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|\\right)+\\frac{B}{\\sqrt{T}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 gives two results: the first one being on the unfairness of the proposed estimator and the second one on the risk of thereof compared to a benchmark prediction function in (12). The benchmark that we pick is rather natural, we compare to the risk of a deterministic prediction that minimizes the risk and whose unfairness is controlled by a Kolmogorov-\u221aSmirnov distance. One first main observation is that both fairness and risk decrease at the rate $1/\\sqrt{T}$ and $T$ is the number of unlabeled data. From our numerical experiments, we observed that we can keep the number of unlabeled data unchanged and iterate several times through them. As a result, we increase artificially $T$ \u2014without generating new data\u2014which gives a significant empirical improvement. We also remark that $\\sigma$ is the parameter that depends on the number of groups. For example, in the case of uniform distribution of sensitive groups $\\sigma=O(K)$ . We finally remark that both bounds involve a single unknown quantity\u2014 $\\bar{\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\|}$ , which from standard duality argument can be shown to be bounded by $O(1/\\operatorname*{min}_{s\\in[K]}\\{\\varepsilon_{s}\\})$ (see e.g., Nedi\u00b4c and Ozdaglar, 2009, Lemma 3). Thus, having this norm multiplied by $T^{-1/2}$ is a very attractive property of the bound. It allows to set $\\varepsilon\\approx T^{-1/2}$ without damaging the parametric convergence rate. ", "page_idx": 7}, {"type": "text", "text": "To derive the above result, we slightly extend the analysis of Foster et al. (2019), who, relying on the SGD3 algorithm of Allen-Zhu (2021), gave an optimal algorithm that controls the expected norm of the gradient in the convex case. More concretely, we incorporate a projection step into their analysis and extend the control to the squared norm of the gradient map. Interestingly, due to our smoothing step and the choice of the parameter $\\beta$ , we noticed that there is no need to restart the accelerated SGD as it is done by Foster et al. (2019) because it leads to identical statistical convergence rates. The interested reader can take a closer look into the Appendix C, where all the optimization results are either recalled or derived for the sake of completeness. Finally, having a control of the squared norm of the gradient map, the proof of Theorem 5.1 follows from Lemma 3.5 and a careful and practical choice of all the parameters of the algorithm. ", "page_idx": 7}, {"type": "text", "text": "Extension to unknown $\\eta$ and $\\tau$ . In this part we show that if we replace $\\eta$ and $\\tau$ with their estimates $\\widehat{\\eta}$ and $\\widehat{\\tau}$ and run DP post-processin $\\mathtt{g}(L,T,\\beta,\\mathbf{p},B,\\widehat{\\eta},\\widehat{\\pmb{\\tau}})$ algorithm with the same choice of parameters, Theorem 5.1 remains if we pay additional price for the estimation of $\\eta$ and $\\tau$ . From now on, we assume that $\\widehat{\\eta}$ and $\\widehat{\\tau}$ are provided and are trained on its own labeled data sample, while the reftiting is performe d  on a n  independent stream of i.i.d. data from $\\mathbb{P}_{X}$ . So, we essentially treat $\\widehat{\\eta}$ and $\\widehat{\\tau}$ as deterministic functions. Let us introduce a family of prediction functions ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{\\mathbf{A},\\mathbf{V}}(\\boldsymbol{\\ell}\\mid\\boldsymbol{x})\\overset{\\mathrm{def}}{=}\\sigma_{\\boldsymbol{\\ell}}\\left(\\beta\\left(\\left\\langle\\lambda_{\\boldsymbol{\\ell}^{\\prime}}-\\nu_{\\boldsymbol{\\ell}^{\\prime}},\\,\\widehat{t}(\\boldsymbol{x})\\right\\rangle-\\widehat{r}_{\\boldsymbol{\\ell}^{\\prime}}(\\boldsymbol{x})\\right)_{\\boldsymbol{\\ell}^{\\prime}\\in[\\![L]\\!]}\\right)\\,\\mathrm{for}\\;\\boldsymbol{\\ell}\\in\\left[\\![L\\!]\\,\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that for fixed matrices $\\mathbf{\\Delta}_{\\mathbf{\\lambda}}\\mathbf{X},\\mathbf{V}$ , the above prediction function is fully data-driven. With this plug-in strategy, our approach becomes fully data-driven and, in Appendix E, we show that the guarantees presented in the main body still hold paying additional price for estimation of $\\eta$ and $\\tau$ . To be more precise, we consider the plug-in version of (5), defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda,\\nabla\\geqslant0}\\left\\{\\mathbb{E}_{X}\\left[\\mathrm{LSE}_{\\beta}\\left(\\left(\\left\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\widehat{t}(X)\\right\\rangle-\\widehat{r}_{\\ell}(X)\\right)_{\\ell=-L}^{L}\\right)\\right]+\\sum_{\\ell=-L}^{L}\\left\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\right\\rangle\\right\\}\\cdot\\;\\left(\\widehat{\\mathcal{P}}_{L S}\\right)_{\\ell=-L}^{L}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Let us denote by $\\widehat F$ , the objective function of the above problem. Thus, main interesting part is to demonstrate that a control of the gradient map of $\\widehat F$ , denoted by $||G_{\\widehat{F},\\alpha}(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})||$ , gives a control of risk and unfairness of $\\widehat{\\pi}_{\\widehat{\\Lambda},\\widehat{\\mathbf{v}}}$ , quantifying the price induced by the plug-in estimation. This is precisely the purpose of the follow ing two results: ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.1. Let $L\\in\\mathbb{N},\\,\\Lambda,\\mathbf{V}\\geqslant0,$ , then for any $\\alpha>0,\\beta>0$ it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{\\ell\\in[L]\\,s\\in[K]}(\\mathcal{U}_{s}(\\widehat{\\pi}_{\\Lambda},\\mathbf{v},\\ell)-\\varepsilon_{s})_{+}^{2}}\\leqslant\\|G_{\\widehat{F},\\alpha}(\\Lambda,\\mathbf{V})\\|+\\mathbb{E}^{1/2}\\|\\widehat{t}(X)-t(X)\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 5.2. Let\u03c32 = 2 s\u2208[K] EX(ps\u2212p2\u03c4 s(X))2, L \u2208N, \u039b, V \u2a7e0, then for any \u03b1 > 0, \u03b2 > 0 it holds that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\widehat{\\pi}_{\\Lambda,\\mathbf{V}})\\leqslant\\mathcal{R}(\\pi_{\\Lambda^{\\star},\\mathbf{V}^{\\star}})+\\bigg(\\|(\\Lambda,\\mathbf{V})\\|+\\alpha\\left\\{\\widehat{\\sigma}+\\|\\varepsilon\\|\\sqrt{2|\\widehat{\\mathcal{V}}_{L}|}\\right\\}\\bigg)\\left\\|G_{\\widehat{F},\\alpha}(\\Lambda,\\mathbf{V})\\right\\|+\\frac{\\log|\\widehat{\\mathcal{V}}_{L}|}{\\beta}}\\\\ &{\\qquad\\qquad+\\,2\\mathbb{E}_{X}\\left[\\underset{\\ell\\in[L]}{\\operatorname*{max}}\\left|r_{\\ell}(X)-\\widehat{r}_{\\ell}(X)\\right|\\right]+\\sqrt{2}\\|(\\Lambda,\\mathbf{V})\\|\\cdot\\mathbb{E}^{1/2}\\|t(X)-\\widehat{t}(X)\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that the two above results are extensions of Lemma 3.5, where both $\\pmb{t}$ and $\\eta$ were assumed to be known. These results are following the spirit of post-processing bounds\u2014the quality of the final approach depends on the initial estimator and the optimization algorithm used to post-process and the two errors are clearly separated. Proofs of both results with additional details and discussions is provided in the supplementary material. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical illustration ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section we conduct empirical study of the proposed algorithm, denoted by DP-postproc, and demonstrate its relevance in practical problems 3. We have implemented both SGD3 of AllenZhu (2021) and an improved version by Foster et al. (2019), observing that the latter significantly outperforms the former. We also tested the approach that is suggested by the theory\u2014SGD3 and accelerated SGD, without restart and it show nearly identical performance as the restarted version of Foster et al. (2019). Thus, for numerical evaluation, we stick to the latter. ", "page_idx": 8}, {"type": "text", "text": "We conduct our study on two datasets: Law School dataset (Wightman (1998)) and Communities and Crime dataset (Redmond (2009)). In the Law School dataset, the aim is to predict students\u2019 GPA on a scale of 0 to 4, normalized to [0, 1], while in the Communities and Crime dataset, we focus on predicting the normalized number of violent crimes per population within the range of $[0,1]$ . In both datasets, ethnicity is a sensitive attribute, distinguishing between white and non-white individuals or communities (majority-wise). ", "page_idx": 8}, {"type": "text", "text": "Our pipeline is the following: First, we randomly split the data into training, unlabeled and testing sets with proportions of $0.4\\times0.4\\times0.2$ . We use $\\bar{\\mathcal{D}_{\\mathrm{train}}}=\\{(\\pmb{x}_{i},s_{i},y_{i})_{i=1}^{n}\\}$ to train a base (unfair) regressor to estimate $\\eta$ and to train a classifier to estimate $\\tau$ . We use simple LinearRegression and LogisticRegression from scikit-learn for training the regressor and the classifier. Finally, we use the trained regressor and classifier to train the Algorithm 1 with $D_{\\mathrm{unlabeled}}=(\\pmb{x})_{i=n+1}^{n+T}$ for $N$ (note that our theory suggests that $N=T$ is enough, but we have noticed that larger $N$ can be more beneficial in practice) iterations. We use $\\mathcal{D}_{\\mathrm{test}}=\\bar{\\left\\{(\\pmb{x}_{i}^{\\prime},s_{i}^{\\prime},y_{i}^{\\prime})_{i=1}^{m}\\right\\}}$ to collect evaluation statistics. ", "page_idx": 8}, {"type": "text", "text": "In Figure 1 we illustrate the post-processing dynamics of our method. We have 2 plots for each test dataset: the history of risk $({\\mathcal{R}}({\\widehat{\\pi}}))$ and of the unfairness $U_{0}(\\widehat{\\pi})$ and $U_{1}({\\widehat{\\pi}}))\\,w.r.t.$ number of iterations. We illustrate the convergence f or $\\varepsilon=(2^{-8},2^{-8})$ unfairness  threshold.  The explicit formulas of the evaluation measures are provided in Appendix $\\mathrm{G}$ . ", "page_idx": 8}, {"type": "image", "img_path": "UtbjD5LGnC/tmp/965b09efb1b911707f88c9794ede9aa8c34dc5d62b6a0a005677f86ae65849b2.jpg", "img_caption": ["Figure 1: Risk and unfairness of our estimator on Communities and Crime and Law School datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Comparison with Agarwal et al. (2019). Surprisingly, we were unable to find many open source competitors that target regression with demographic parity constraint in unaware setting, even the FairLearn\u2014a popular python package\u2014does not deal with the demographic parity constraint in regression. The only easily accessible algorithm that deals with our problem was kindly provided by Agarwal et al. (2019) (from now on referenced as ADW). We train ADW method in two ways: we use $\\mathcal{D}_{\\mathrm{train}}$ and $\\mathcal{D}_{\\mathrm{unlabeled}}$ as training set for ADW-1, whereas for ADW-2 we use only $\\mathcal{D}_{\\mathrm{train}}$ . The second situation is realistic, when unlabeled data is available and unlike ADW, our approach is able to take advantage of it. We take the set $\\{(2^{-i},2^{-i})_{i\\in\\mathbb{Z}}\\}$ , where $\\mathcal{T}=\\{1,2,4,8,16\\}$ as unfairness thresholds for training both datasets. We train ADW-1 and ADW-2 for each pair of epsilons for 10 times. With our available computing power and the code provided by the authors, the algorithm runs for 13.5 hours (see Appendix G for additional details).4 ", "page_idx": 9}, {"type": "text", "text": "On Figure 2 we illustrate the comparison of risk and unfairness between ADW-1, ADW-2, base (LinearRegression) and our model. We plot the mean and standard deviation of risk and unfairness for each epsilon threshold on both datasets. We observe that our method is competitive or eventually outperforms ADW in both training regimes. ", "page_idx": 9}, {"type": "image", "img_path": "UtbjD5LGnC/tmp/4bb67e3bea609179195714b0a2eb876cbc337f59aef93706300d45838b6520a7.jpg", "img_caption": ["Figure 2: Comparison with ADW model on Communitites and Crime and Law School datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Deriving a dual convex surrogate, we have provided a generic way to build a post-processing estimator of any off-the-shelf method that achieves the demographic parity constraint. Our approach is fully data and theory driven, revealing a key role of stationary point guarantees in stochastic convex optimization. Following Remark 2.2, we intend to extend our approach, which is general enough, to other learning problems, beyond algorithmic fairness. ", "page_idx": 9}, {"type": "text", "text": "Limitations. From the theoretical perspective, the knowledge of $B$ seems to be the main limitation. While it is available for many applications, it does not have to be the case all the time. Replacing this assumption with some tail conditions, could be more realistic. From the applied perspective, it would be beneficial to further investigate stationary point guarantees for convex optimization to yield a better practical performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements The work of Gayane Taturyan has been supported by the French government under the \"France $2030^{\\circ}$ program, as part of the SystemX Technological Research Institute within the Confiance.ai project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. Agarwal, M. Dudik, and Z. S. Wu. Fair regression: Quantitative definitions and reduction-based algorithms. In International Conference on Machine Learning, 2019.   \nZ. Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd, 2021.   \nY. Arjevani, Y. Carmon, J. Duchi, D. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165\u2013214, 2023.   \nS. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning. fairmlbook.org, 2018.   \nA. Beck. Introduction to nonlinear optimization: Theory, algorithms, and applications with MATLAB. SIAM, 2014.   \nR. Bhatia and C. Davis. A better bound on the variance. The American Mathematical Monthly, 107 (4):353\u2013357, 2000. doi: 10.1080/00029890.2000.12005203.   \nS. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.   \nT. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In IEEE international conference on Data mining, 2009.   \nS. Chiappa, R. Jiang, T. Stepleton, A. Pacchiano, H. Jiang, and J. Aslanides. A general approach to fairness with optimal transport. In AAAI, 2020.   \nE. Chzhen and N. Schreuder. An example of prediction which complies with demographic parity and equalizes group-wise risks in the context of regression. arXiv preprint arXiv:2011.07158, 2020a.   \nE. Chzhen and N. Schreuder. A minimax framework for quantifying risk-fairness trade-off in regression. arXiv preprint arXiv:2007.14265v2, 2020b.   \nE. Chzhen, C. Denis, M. Hebiri, L. Oneto, and M. Pontil. Leveraging labeled and unlabeled data for consistent fair binary classification. In Advances in Neural Information Processing Systems, 2019.   \nE. Chzhen, C. Denis, M. Hebiri, L. Oneto, and M. Pontil. Fair regression with wasserstein barycenters. NeurIPS 2020, 2020a.   \nE. Chzhen, M. Denis, C.and Hebiri, L. Oneto, and M. Pontil. Fair regression via plug-in estimator and recalibration. NeurIPS 2020, 2020b.   \nE. Chzhen, C. Denis, and M. Hebiri. Minimax semi-supervised set-valued approach to multi-class classification. Bernoulli, 2021.   \nC. Denis, R. Elie, M. Hebiri, and F. Hu. Fairness guarantees in multi-class classification with demographic parity. Journal of Machine Learning Research, 25(130):1\u201346, 2024.   \nC. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness, 2011. URL https://arxiv.org/abs/1104.3913.   \nM. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In International Conference on Knowledge Discovery and Data Mining, 2015.   \nD. Foster, A. Sekhari, O. Shamir, N. Srebro, K. Sridharan, and B. Woodworth. The complexity of making the gradient small in stochastic convex optimization. In Conference on Learning Theory, pages 1319\u20131345. PMLR, 2019.   \nB. Gao and L. Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017.   \nS. Gaucher, N. Schreuder, and E. Chzhen. Fair learning with wasserstein barycenters for nondecomposable performance measures. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 2436\u20132459. PMLR, 25\u201327 Apr 2023. URL https://proceedings.mlr.press/v206/ gaucher23a.html.   \nS. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 22 (4):1469\u20131492, 2012.   \nP. Gordaliza, E. Del Barrio, G. Fabrice, and J. M. Loubes. Obtaining fairness using optimal transport theory. In International Conference on Machine Learning, 2019.   \nM. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016.   \nR. Jiang, A. Pacchiano, T. Stepleton, H. Jiang, and S. Chiappa. Wasserstein fair classification. Uncertainty in Artificial Intelligence Conference, 2020.   \nT. Le Gouic, J.-M. Loubes, and P. Rigollet. Projection to fairness in statistical learning. arXiv preprint arXiv:2005.11720, 2020.   \nM. Lichman. Adult. UCI Machine Learning Repository, 2013. URL http://archive.ics.uci. edu/ml.   \nK. Lum and J. Johndrow. A statistical framework for fair predictive algorithms. arXiv preprint arXiv:1610.08077, 2016.   \nG. Maheshwari and M. Perrot. Fairgrad: Fairness aware gradient descent. arXiv preprint arXiv:2206.10923, 2022.   \nH. Narasimhan, A. Cotter, M. Gupta, and S. Wang. Pairwise fairness for ranking and regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5248\u20135255, 2020.   \nA Nedic\u00b4 and A Ozdaglar. Subgradient methods for saddle-point problems. Journal of Optimization Theory and Applications, 142(1):205\u2013228, 2009.   \nY. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(3): 127\u2013152, 2005.   \nY. Nesterov. How to make the gradients small. Optima. Mathematical Optimization Society Newsletter, (88):10\u201311, 2012.   \nM. Redmond. Communities and Crime. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C53W3X.   \nL. Wightman. Lsac national longitudinal bar passage study. lsac research report series. 1998. URL https://api.semanticscholar.org/CorpusID:151073942.   \nM. Zafar, I. Valera, M. Gomez Rodriguez, and K. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In International Conference on World Wide Web, 2017.   \nR. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning, 2013.   \nH. Zhao. Costs and benefits of fair regression. arXiv preprint arXiv:2106.08812, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs for results in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "First we explicit the first order optimality conditions for the problem in (5). ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Let $(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\geqslant0$ be any minimizer of (5) and $\\pi^{\\star}=\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}}$ be defined in (6). Then, there exist $\\mathbf{I}=(\\gamma_{\\ell s})_{\\ell\\in[L],s\\in[K]},\\mathbf{P}^{\\prime}=(\\gamma_{\\ell s}^{\\prime})_{\\ell\\in[L],s\\in[K]}.$ \u2014element-wise non-negative matrices such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\begin{array}{r l}{\\mathbb{E}_{X}\\left[\\pi^{\\star}(\\ell\\mid X)t(X)\\right]=-\\varepsilon+\\gamma_{\\ell}}\\\\ {\\mathbb{E}_{X}\\left[\\pi^{\\star}(\\ell\\mid X)t(X)\\right]=\\varepsilon-\\gamma_{\\ell}^{\\prime}}\\\\ {\\gamma_{\\ell s}\\lambda_{\\ell s}^{\\star}=0}\\\\ {\\gamma_{\\ell s}^{\\prime}\\nu_{\\ell s}^{\\star}=0}\\end{array}}&{\\quad\\forall\\ell\\in\\ensuremath{[L]},s\\in\\ensuremath{[K]}\\,,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\gamma_{\\ell}=(\\gamma_{\\ell s})_{s\\in[K]},\\gamma_{\\ell}^{\\prime}=(\\gamma_{\\ell s}^{\\prime})_{s\\in[K]}.$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We first observe that the optimization problem in (5) is convex and smooth. Thus, Karush\u2013Kuhn\u2013Tucker conditions are sufficient for optimally. Furthermore, since Slatter\u2019s condition is satisfied, the latter is also necessary, as the strong duality holds. In particular, there exist $\\mathbf{I}=(\\gamma_{\\ell s})_{\\ell\\in\\mathbb{I}\\!L\\mathbb{I},s\\in[K]},\\mathbf{I}^{\\prime}=(\\gamma_{\\ell s}^{\\prime})_{\\ell\\in\\mathbb{I}\\!L\\mathbb{I},s\\in[K]}.$ \u2014element-wise non-negative matrices such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\nabla_{\\mathbf{\\boldsymbol{A}}}F(\\mathbf{\\boldsymbol{A}}^{\\star},\\mathbf{V}^{\\star})-\\mathbf{\\boldsymbol{\\Gamma}}=\\mathbf{0}}\\\\ {\\nabla_{\\mathbf{\\boldsymbol{V}}}F(\\mathbf{\\boldsymbol{A}}^{\\star},\\mathbf{V}^{\\star})-\\mathbf{\\boldsymbol{\\Gamma}}^{\\prime}=\\mathbf{0}}\\\\ {\\mathbf{\\boldsymbol{A}}^{\\star},\\mathbf{V}^{\\star}\\geqslant0}\\\\ {\\gamma_{\\ell s}\\lambda_{\\ell s}^{\\star}=0}\\\\ {\\gamma_{\\ell s}^{\\prime}\\nu_{\\ell s}^{\\star}=0}\\end{array}\\right.\\qquad\\forall\\ell\\in\\ensuremath{[\\![L]\\!]},s\\in\\ensuremath{[\\![L]\\!]}\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To conclude, it is sufficient to evaluate the gradient on $F$ , whose expression is given in (10) and use the definition of $\\pi^{\\star}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 3.1. To prove this result, we introduce the Lagrangian for the problem in (4). ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pi,\\mathbf{A},\\mathbf{V})=\\mathcal{R}_{\\beta}(\\pi)+\\mathbb{E}_{X}\\left[\\sum_{\\ell\\in[L]}\\left\\langle\\nu_{\\ell}-\\lambda_{\\ell},\\,t(X)\\right\\rangle\\pi(\\ell\\mid X)\\right]-\\sum_{\\ell\\in[L]}\\left\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\right\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we used the fact, that using the definition of $\\mathcal{U}_{s}$ , for any randomized prediction function $\\pi$ , we can write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{U}_{s}(\\pi,\\ell)=\\left|\\frac{\\mathbb{E}_{X}\\left[\\pi(\\ell\\,|\\,X)\\mathbb{I}\\{S=s\\}\\right]}{\\mathbb{P}(S=s)}-\\mathbb{E}_{X}\\left[\\pi(\\ell\\,|\\,X)\\right]\\right|=\\left|\\mathbb{E}_{X}\\left[\\pi(\\ell\\,|\\,X)t_{s}(X)\\right]\\right|\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, denoting by $(\\star)$ the value in (4), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(\\star\\right)=\\operatorname*{min}_{\\pi}\\operatorname*{max}_{\\Lambda,\\mathbf{V}\\geqslant0}\\mathcal{L}(\\pi,\\Lambda,\\mathbf{V})=\\operatorname*{max}_{\\Lambda,\\mathbf{V}\\geqslant0}\\operatorname*{min}_{\\pi}\\mathcal{L}(\\pi,\\Lambda,\\mathbf{V})\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the second equality holds thanks to Sion\u2019s minmax theorem. Let us solve the inner minimization problem on the right-hand-side. We can write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\pi,\\mathbf{A},\\mathbf{V})=\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{\\ell\\in[L]}\\left(r_{\\ell}(X)-\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\,t(X)\\rangle\\right)\\pi(\\ell\\mid X)+\\frac{1}{\\beta}\\Psi(\\pi(\\cdot\\mid X))\\right]}\\\\ {-\\displaystyle\\sum_{\\ell\\in[L]}\\left\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\right\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, using the variational representation of $\\mathrm{LSE}_{\\beta}$ , recalled in Lemma F.1, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\mathcal{L}(\\pi,\\Lambda,\\mathbf{V})=-\\operatorname*{max}_{\\pi}\\left\\{\\mathbb{E}_{X}\\left[\\sum_{\\ell\\in[L]}\\left(\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\,t(X)\\rangle-r_{\\ell}(X)\\right)\\pi(\\ell\\mid X)-\\frac{1}{\\beta}\\Psi(\\pi(\\cdot\\mid X))\\right]\\right.}\\\\ {\\left.\\qquad\\qquad\\qquad+\\sum_{\\ell\\in[L]}\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\rangle\\right\\}}\\\\ {=-\\mathbb{E}_{X}\\left[\\mathrm{LSE}_{\\beta}\\left(\\left(\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\,t(X)\\rangle-r_{\\ell}(X)\\right)_{\\ell\\in[L]}\\right)\\right]-\\sum_{\\ell\\in[L]}\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\rangle\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the optimum in the above problem for every $\\mathbf{A},\\mathbf{V}\\geqslant0$ is achieved by $\\pi_{\\boldsymbol{\\Lambda},\\mathbf{V}}$ , defined in (7). Thus, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{\\Pi}(\\star)=\\operatorname*{max}_{\\mathbf{\\Lambda},\\mathbf{V}\\geqslant0}\\left\\{-F(\\mathbf{\\Lambda},\\mathbf{V})\\right\\}=\\mathcal{R}_{\\beta}(\\pi_{\\mathbf{\\Lambda}^{\\star}},\\mathbf{V}^{\\star})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof is concluded. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.2. As shown in (15), for any randomized prediction function $\\pi$ , we can write ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{U}_{s}(\\pi,\\ell)=|\\mathbb{E}_{\\pmb{X}}\\left[\\pi(\\ell\\mid\\pmb{X})t_{s}(\\pmb{X})\\right]|~.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Our goal is to show that $\\pi^{\\star}$ satisfies the required fairness constraints. We are going to rely on Lemma A.1 Subtracting the first equation in (14) from the second one, we deduce that $\\gamma_{\\ell}+\\gamma_{\\ell}^{\\prime}=2\\varepsilon$ . Since $\\gamma_{\\ell},\\gamma_{\\ell}^{\\prime}\\geqslant0$ , then we conclude that $\\gamma_{\\ell s},\\gamma_{\\ell s}^{\\prime}\\in[0,2\\varepsilon_{s}]$ . The above implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\varepsilon_{s}\\leqslant\\mathbb{E}_{X}\\left[\\pi^{\\star}(\\ell\\mid X)t_{s}(X)\\right]=\\mathcal{U}_{s}(\\pi^{\\star},\\ell)\\leqslant\\varepsilon_{s}\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as claimed. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.3. Fix some randomized prediction function $\\pi$ that is feasible for the problem in (3). In particular, it must be supported on $\\hat{\\mathcal{D}}$ . Then, we can bound its negative risk as follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\widetilde{\\mathbf{R}}(\\pi)=-\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{\\ell\\in[\\ell]}r_{\\ell}(X)\\pi(\\ell\\mid X)\\right]}\\\\ &{\\overset{(a)}{\\leqslant}\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{\\ell\\in[\\ell]}\\left((X\\!\\!\\!\\!\\slash-\\!\\!\\!\\nu_{\\ell}^{\\prime}\\!\\!\\!\\ell\\left(X\\right)\\right)\\!\\!\\!\\!\\cdot\\!r_{\\ell}(X)\\right)\\!\\!\\!\\cdot\\!\\!\\tau(\\mathcal{E}\\mid X)\\right]+\\sum_{\\ell\\in[\\ell]}\\left(\\lambda_{\\ell}^{\\star}+\\nu_{\\ell}^{\\star},\\epsilon\\right)}\\\\ &{\\overset{(b)}{\\leqslant}\\mathbb{E}_{X}\\left[\\big[\\mathrm{E}\\!\\!\\!\\:\\mathrm{E}\\!\\!\\:\\mathrm{E}\\!\\:\\big((X\\!\\!\\!\\!\\slash-\\!\\!\\!\\nu_{\\ell}^{\\prime}\\!\\!\\!\\cdot\\!\\!\\ell(X))\\!\\!\\!\\!\\cdot\\!r_{\\ell}(X)\\big)\\!\\!\\!\\right]+\\sum_{\\ell\\in[\\ell]}\\big(\\lambda_{\\ell}^{\\star}+\\nu_{\\ell}^{\\star},\\epsilon\\big)}\\\\ &{\\overset{(c)}{\\cong}\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{\\ell\\in[\\ell]}\\big((X\\!\\!\\!\\!\\slash-\\!\\!\\!\\nu_{\\ell}^{\\prime}\\!\\!\\!\\cdot\\!\\!\\ell(X)\\big)\\!\\!\\!\\cdot\\!r_{\\ell}(X)\\big)^{\\!\\star}\\!\\!\\!\\cdot\\!\\big\\{\\ell(X)\\big-\\!\\frac{1}{2}\\!\\!\\!\\cdot\\!\\!\\big\\Vert\\nabla\\big(\\tau^{\\star}\\!\\!\\!\\!\\cdot\\!\\!\\big(X\\!\\!\\!\\!\\slash-\\!\\!\\!\\!\\nu_{\\ell}^{\\prime}\\!\\!\\!\\big)\\big)\\right]}+\\sum_{\\ell\\in[\\ell]}\\big(\\lambda_{\\ell}^{\\star}+\\nu_{\\ell}^{\\star},\\epsilon\\big)}\\\\ &{\\overset{(d)}{\\leqslant}\\mathbb{E}_{X}\\left[\\displaystyle\\sum_{\\ell\\in[\\ell]}\\big((X\\!\\!\\!\\!\\slash-\\!\\!\\!\\nu_{\\ell}^{\\prime}\\!\\!\\!\\cdot\\!\\!\\ell(X)\\big)\\!\\!\\!\\cdot\\!r_{\\ell}(X)\\big)^{\\!\\star}\\!\\!\\left(\\ell\\mid X\\big)\\right]+\\sum_{\\ell\\in[\\ell]}\\big(\\lambda_{\\ell}^{\\star}+\\nu_{\\ell}^{\\star\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for (a) we used the assumption that $\\pi$ is fair (i.e., $\\mathcal{U}_{s}(\\pi,\\ell)\\;\\leqslant\\;\\varepsilon_{s})$ , which due to the fact that $\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}\\geqslant0$ implies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in[L]}\\;\\langle\\lambda_{\\ell}^{\\star},\\,\\mathbb{E}_{X}\\left[t(X)\\pi(\\ell\\mid X)\\right]+\\varepsilon\\rangle+\\sum_{\\ell\\in[L]}\\;\\langle\\nu_{\\ell}^{\\star},\\,-\\mathbb{E}_{X}\\left[t(X)\\pi(\\ell\\mid X)\\right]+\\varepsilon\\rangle\\geqslant0\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since every term in the summation is non-negative; (b) uses the fact that $\\mathrm{LSE}_{\\beta}(\\pmb{w})\\geqslant\\langle\\pmb{w},\\pmb{p}\\rangle$ for any probability vector $\\pmb{p}$ (see Lemma F.1 for details); (c) relies on the variational representation of the $\\mathrm{LSE}_{\\beta}$ , recalled in Lemma F.1 and the definition of $\\pi^{\\star}(\\cdot\\mid X)$ ; (d) uses the uniform bound on the entropy; (e) the last equality relies on the complementary slackness condition (14) of Lemma A.1. It ensures that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\lambda_{\\ell s}^{\\star}\\mathbb{E}_{X}\\left[\\pi^{\\star}(\\ell\\mid X)t_{s}(X)\\right]=-\\lambda_{\\ell s}^{\\star}\\varepsilon_{s}\\qquad\\qquad\\forall\\ell\\in\\left[L\\right],s\\in\\left[K\\right],}\\\\ {\\nu_{\\ell s}^{\\star}\\mathbb{E}_{X}\\left[\\pi^{\\star}(\\ell\\mid X)t_{s}(X)\\right]=\\nu_{\\ell s}^{\\star}\\varepsilon_{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "implying that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}\\left[\\sum_{\\ell\\in[L]}\\,\\langle\\lambda_{\\ell}^{\\star}-\\nu_{\\ell}^{\\star},\\,t(X)\\rangle\\,\\pi^{\\star}(\\ell\\mid X)\\right]+\\sum_{\\ell\\in[L]}\\,\\langle\\lambda_{\\ell}^{\\star}+\\nu_{\\ell}^{\\star},\\,\\varepsilon\\rangle=0\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The proof is concluded. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.5. Fix arbitrary $\\mathbf{A},\\mathbf{V}\\geqslant0$ and consider $\\pi_{\\boldsymbol{\\Lambda},\\mathbf{V}}$ , defined in (7). To ease the notation, in this proof, we write $\\pi$ instead of $\\pi_{\\boldsymbol{\\Lambda},\\mathbf{V}}$ . ", "page_idx": 14}, {"type": "text", "text": "Part I. Let us first recall the definition of the gradient map $G_{\\alpha}$ given in (8). We have the following expression ", "page_idx": 14}, {"type": "equation", "text": "$$\nG_{\\alpha}\\left(\\mathbf{A},\\mathbf{V}\\right)=\\frac{\\left(\\mathbf{A},\\mathbf{V}\\right)-\\left(\\left(\\mathbf{A},\\mathbf{V}\\right)-\\alpha\\nabla F\\left(\\mathbf{A},\\mathbf{V}\\right)\\right)_{+}}{\\alpha}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(\\cdot)_{+}$ is to be understood entry-wise. Observing that for any $\\alpha,a\\geqslant0$ and $b\\in\\mathbb{R}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\frac{a-(a-\\alpha b)_{+}}{\\alpha}\\right|=\\left|\\frac{a-\\operatorname*{max}\\{0;a-\\alpha b\\}}{\\alpha}\\right|=\\left|\\operatorname*{min}\\left\\{\\frac{a}{\\alpha};b\\right\\}\\right|\\geqslant\\left|\\operatorname*{min}\\left\\{0;b\\right\\}\\right|\\geqslant(-b)_{+}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we deduce that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigl\\|(-\\nabla F(\\mathbf{A},\\mathbf{V}))_{+}\\bigr\\|\\leqslant\\|G_{\\alpha}(\\mathbf{A},\\mathbf{V})\\|\\qquad\\forall\\mathbf{A},\\mathbf{V}\\geqslant0\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Relying on (10) and the expression for $\\pi$ in (7), we observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\lambda_{\\ell s}}F(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{V})=\\mathbb{E}_{\\boldsymbol{X}}\\left[\\pi(\\boldsymbol{\\ell}\\mid\\boldsymbol{X})t_{s}(\\boldsymbol{X})\\right]+\\varepsilon_{s}}\\\\ &{\\nabla_{\\nu_{\\ell s}}F(\\mathbf{\\boldsymbol{\\Lambda}},\\mathbf{V})=-\\mathbb{E}_{\\boldsymbol{X}}\\left[\\pi(\\boldsymbol{\\ell}\\mid\\boldsymbol{X})t_{s}(\\boldsymbol{X})\\right]+\\varepsilon_{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and that $\\mathcal{U}_{s}(\\pi,\\ell)\\,=\\,|\\mathbb{E}\\left[\\pi(\\ell\\mid X)t_{s}(X)\\right]|$ as it is shown in (15). Using the fact that $(|a|-c)_{+}^{2}=$ $(-a-c)_{+}^{2}+(a-c)_{+}^{2}$ for all $a\\in\\mathbb{R}$ and $c\\geqslant0$ , we deduce from above ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\mathcal{U}_{s}(\\pi_{\\mathbf{A},\\mathbf{V}},\\ell)-\\varepsilon_{s})_{+}^{2}=(-\\nabla_{\\lambda_{\\ell s}}F(\\mathbf{A},\\mathbf{V}))_{+}^{2}+(-\\nabla_{\\nu_{\\ell s}}F(\\mathbf{A},\\mathbf{V}))_{+}^{2}\\qquad\\forall\\ell\\in\\left[L\\right],s\\in\\left[K\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we have shown ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\boldsymbol{\\ell}\\in[L]}\\left(\\mathcal{U}_{s}(\\boldsymbol{\\pi},\\boldsymbol{\\ell})-\\varepsilon_{s}\\right)_{+}^{2}=\\left\\|\\left(-\\nabla F(\\mathbf{A},\\mathbf{V})\\right)_{+}\\right\\|^{2}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and (19) yields the claim. ", "page_idx": 14}, {"type": "text", "text": "Part II. We note that $\\pi_{(\\mathbf{\\Lambda}\\Lambda,\\mathbf{V})}$ is a unique solution to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\mathcal{L}(\\pi,\\Lambda,\\mathbf{V})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{L}$ is the Lagrangian defined in (16). Furthermore, $\\operatorname*{min}_{\\pi}{\\mathcal{L}}(\\pi,\\Lambda,\\mathbf{V})\\ =\\ -F(\\mathbf{\\Lambda},\\mathbf{V})\\ =$ $\\mathcal{L}(\\pi_{(\\mathbf{A},\\mathbf{V})},\\Lambda,\\mathbf{V})$ . Hence, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr R}_{\\beta}(\\pi_{(\\mathbf{A},\\mathbf{V})})+F(\\mathbf{A},\\mathbf{V})=\\displaystyle\\sum_{\\ell\\in[\\![L]\\!],s\\in[\\![K]\\!]}\\lambda_{\\ell s}\\nabla_{\\lambda_{\\ell s}}F(\\mathbf{A},\\mathbf{V})+\\displaystyle\\sum_{\\ell\\in[\\![L]\\!],s\\in[\\![K]\\!]}\\nu_{\\ell s}\\nabla_{\\nu_{\\ell s}}F(\\mathbf{A},\\mathbf{V})}\\\\ &{\\qquad\\qquad\\qquad=\\left\\langle\\left(\\mathbf{A},\\mathbf{V}\\right),\\nabla F\\left(\\mathbf{A},\\mathbf{V}\\right)\\right\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the sake of simplicity let us denote by $u\\overset{\\mathrm{def}}{=}(\\mathbf{A},\\mathbf{V})\\in\\mathbb{R}^{2K(2L+1)}$ and recall the definition of gradient mapping $G_{\\alpha}$ given in (8). For any $j\\in[2K(2L+1)]$ and $\\alpha>0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nG_{\\alpha j}({\\pmb u})=\\left\\{\\begin{array}{l l}{u_{j}/\\alpha}&{\\mathrm{if}\\;\\alpha\\partial_{j}F({\\pmb u})>u_{j}\\,,}\\\\ {\\partial_{j}F({\\pmb u})}&{\\mathrm{if}\\;\\partial_{j}F({\\pmb u})\\leqslant u_{j}\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To bound $\\langle\\boldsymbol{u},\\,\\nabla F(\\boldsymbol{u})\\rangle$ , let us examine each term of the scalar product. Denoting by $\\widetilde{\\textbf{\\em u}}\\,\\frac{\\mathrm{def}}{=}\\textbf{\\em u}-$ $\\alpha\\nabla F(\\pmb{u})\\in\\mathbb{R}^{2K(2L+1)}$ , for any $j\\in[2K(2L+1)]$ and $\\alpha>0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{j}\\partial_{j}F({\\pmb u})=\\alpha\\partial_{j}F({\\pmb u})G_{\\alpha j}({\\pmb u})\\mathbb{I}\\{\\widetilde u_{j}<0\\}+u_{j}G_{\\alpha j}({\\pmb u})\\mathbb{I}\\{\\widetilde u_{j}\\geqslant0\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u,\\,\\nabla F(u)\\!\\!\\!\\rangle\\leqslant\\!\\!\\!\\!\\sum_{j\\in[2K(2L+1)]}\\!\\!\\!\\!\\big(\\alpha|\\partial_{j}F(u)G_{\\alpha j}(u)|+|u_{j}G_{\\alpha j}(u)|)\\leqslant(\\|u\\|+\\alpha\\,\\|\\nabla F(u)\\|)\\,\\|G_{\\alpha}(u)\\|}\\\\ &{\\qquad\\qquad\\leqslant\\!\\left(\\|u\\|+\\alpha\\sigma+\\alpha\\,\\|\\varepsilon\\|\\,\\sqrt{2(2L+1)}\\right)\\|G_{\\alpha}(u)\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality follows from Lemma F.5 that bounds $\\|\\nabla F(\\pmb{u})\\|$ . ", "page_idx": 14}, {"type": "text", "text": "To conclude the proof, we use the fact that It remains to observe that $\\scriptstyle\\operatorname*{min}_{\\mathbf{\\Lambda},\\mathbf{V}}{\\cal F}(\\mathbf{\\Lambda},\\mathbf{V})\\ =$ $-\\mathcal{R}_{\\beta}\\big(\\pi_{(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})}\\big)$ , the bound (19) on $\\|(-\\nabla F(\\mathbf{A},\\mathbf{V}))_{+}\\|$ and the fact that $\\begin{array}{r}{\\mathcal{R}_{\\beta}(\\pi)\\leqslant\\mathcal{R}(\\pi)\\!+\\!\\frac{\\log(2L+1)}{\\beta}}\\end{array}$ for all $\\pi$ . \u25a0 ", "page_idx": 14}, {"type": "text", "text": "B Bound on the variance of the stochastic gradient and its\u2019 smoothness ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4.1. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{{\\boldsymbol X}}\\left\\|g_{{\\boldsymbol A},{\\bf V}}({\\boldsymbol X})-\\nabla_{{\\boldsymbol\\Lambda},{\\bf V}}F\\left({\\boldsymbol\\Lambda},{\\bf V}\\right)\\right\\|^{2}}\\\\ &{\\ \\leqslant2\\mathbb{E}_{{\\boldsymbol X}}\\left\\|\\left(\\sigma_{\\ell}\\left(\\beta\\left(\\left\\langle\\lambda_{\\ell}-\\nu_{\\ell^{\\prime}},t({\\boldsymbol X})\\right\\rangle-r_{\\ell^{\\prime}}({\\boldsymbol X})\\right)_{\\ell^{\\prime}=-L}^{L}\\right)t_{s}({\\boldsymbol X})\\right)_{\\ell\\in[L],s\\in[K]}\\right\\|^{2}}\\\\ &{\\ \\leqslant2\\mathbb{E}_{{\\boldsymbol X}}\\left[\\displaystyle\\sum_{s\\in[K]}t_{s}^{2}({\\boldsymbol X})\\right]\\leqslant2\\displaystyle\\sum_{s\\in[K]}\\displaystyle\\frac{1-p_{s}}{p_{s}}\\,\\frac{\\mathrm{def}}{s}\\,\\sigma^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality follows from the expressions of $g_{\\mathbf{A},\\mathbf{V}}$ and $\\nabla_{\\boldsymbol{\\Lambda},\\mathbf{V}}F\\left(\\boldsymbol{\\Lambda},\\mathbf{V}\\right)$ , and the fact that $\\operatorname{Var}(X+a)=\\operatorname{Var}(X)\\leqslant\\mathbb{E}[X^{2}]$ ; the second inequality follows from the facts that $\\left\\|(a_{i}b_{j})_{i,j}\\right\\|_{2}^{2}=$ $\\left\\|\\pmb{\\mathscr{a}}\\right\\|_{2}^{2}\\left\\|\\pmb{\\mathscr{b}}\\right\\|_{2}^{2}\\leqslant\\left\\|\\pmb{\\mathscr{a}}\\right\\|_{1}^{2}\\left\\|\\pmb{\\mathscr{b}}\\right\\|_{2}^{2}$ and that $\\left\\|\\boldsymbol{\\sigma}(\\cdot)\\right\\|_{1}=1$ ; and the last inequality follows from Lemma F.4. ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3.4. The goal of this proof is to show that the gradient of $(\\mathbf{A},\\mathbf{V})\\mapsto F(\\mathbf{A},\\mathbf{V})$ is $M$ -Lipschitz. To this end, we first introduce some, rather heavy, but convenient, notation which will allow us to derive the announced result. ", "page_idx": 15}, {"type": "text", "text": "Introducing notation. We first vectorize the variables $(\\mathbf{\\cal{A}},\\mathbf{\\cal{V}})$ and express them as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta\\stackrel{\\mathrm{def}}{=}\\left(\\underbrace{\\lambda_{-L1},\\cdots\\lambda_{-L K}}_{=\\lambda_{-L}},\\cdots\\cdots,\\underbrace{\\lambda_{L1},\\cdots\\lambda_{L K}}_{=\\lambda_{L}},\\underbrace{\\nu_{-L1},\\cdots\\nu_{-L K}}_{=\\nu_{-L}},\\cdots\\cdots,\\underbrace{\\nu_{L1},\\cdots\\nu_{L K}}_{=\\nu_{L}}\\right)\\in\\mathbb{R}^{2K(2L+1)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, for each $\\pmb{x}\\in\\mathbb{R}^{d}$ , we introduce a matrix $\\mathbf{A}(\\pmb{x})\\in\\mathbb{R}^{(2L+1)\\times2K(2L+1)}$ defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}(x)\\overset{\\mathrm{def}}{=}\\left(\\begin{array}{c c c c c c c c}{t(x)^{\\top}}&{0\\cdot\\cdot\\cdot0}&{\\cdots}&{0\\cdot\\cdot0}&{-t(x)^{\\top}}&{0\\cdot\\cdot0}&{\\cdots}&{0\\cdot\\cdot0}\\\\ {0\\cdot\\cdot0}&{t(x)^{\\top}}&{\\cdots}&{0\\cdot\\cdot0}&{0\\cdot\\cdot0}&{-t(x)^{\\top}}&{\\cdots}&{0\\cdot\\cdot0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0\\cdot\\cdot0}&{0\\cdot\\cdot0}&{\\cdots}&{t(x)^{\\top}}&{0\\cdot\\cdot0}&{0\\cdot\\cdot0}&{\\cdots}&{-t(x)^{\\top}}\\end{array}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as well as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b(x)\\overset{\\mathrm{def}}{=}\\left(r_{-L}(X),\\cdots,r_{L}(X)\\right)^{\\top}\\in\\mathbb{R}^{2L+1}\\quad\\quad\\mathrm{and}}\\\\ &{\\quad\\quad c\\overset{\\mathrm{def}}{=}\\left(\\varepsilon_{1},\\cdots,\\varepsilon_{K},\\varepsilon_{1},\\cdots,\\varepsilon_{K},\\cdots\\cdots,\\varepsilon_{1},\\cdots,\\varepsilon_{K}\\right)^{\\top}\\in\\mathbb{R}^{2K(2L+1)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hessian of $F$ in the introduced notation. With the above introduce notation, we can express the function $F$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nF\\left(\\mathbf{A},\\mathbf{V}\\right)=F(z)=\\mathbb{E}_{X}\\left[\\mathrm{LSE}_{\\beta}(\\mathbf{A}(X)z-b(X))\\right]+\\left\\langle c,\\,z\\right\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is, $F$ is obtained from the $\\mathrm{LSE}_{\\beta}$ by a point-wise affine transformation of the coordinates plus a linear term. Chain rule yields the following expressions for the Hessian of $F$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla^{2}F(z)=\\mathbb{E}_{X}\\left[{\\bf A}(X)^{\\top}\\nabla^{2}\\,\\mathrm{LSE}_{\\beta}({\\bf A}(X)z-b(X)){\\bf A}(X)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Bounding the operator norm of the Hessian of $F$ . To conclude the proof, we provide a uniform upper bound on the operator (spectral) norm of the Hessian of $F$ . Using the Jensen\u2019s inequality and the fact that the operator norm is subordinate, we deduce that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla^{2}F(z)\\|_{\\mathrm{op}}=\\|\\mathbb{E}_{X}\\left[\\mathbf{A}(X)^{\\top}\\nabla^{2}\\mathrm{LSE}_{\\beta}(\\mathbf{A}(X)z-b(X))\\mathbf{A}(X)\\right]\\|_{\\mathrm{op}}}\\\\ &{~~~~~~~~~~~~~~~~~~\\leqslant\\mathbb{E}_{X}\\left[\\|\\mathbf{A}(X)^{\\top}\\nabla^{2}\\mathrm{LSE}_{\\beta}(\\mathbf{A}(X)z-b(X))\\mathbf{A}(X)\\|_{\\mathrm{op}}\\right]}\\\\ &{~~~~~~~~~~~~~~~~~\\leqslant\\mathbb{E}_{X}\\left[\\|\\mathbf{A}(X)\\|_{\\mathrm{op}}\\|\\nabla^{2}\\mathrm{LSE}_{\\beta}(\\mathbf{A}(X)z-b(X))\\|_{\\mathrm{op}}\\|\\mathbf{A}(X)\\|_{\\mathrm{op}}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma F.2, implies that $\\|\\nabla^{2}\\operatorname{LSE}_{\\beta}(\\mathbf{A}(X)z-b(X))\\|_{\\mathrm{op}}\\leqslant\\beta$ almost surely and for all $_{\\textit{z}}$ . Thus, it remains to bound $\\mathbb{E}_{X}\\|\\mathbf{A}(X)\\|_{\\mathrm{op}}^{2}$ to conclude the proof. To this end, consider a vector $\\textbf{\\em u}$ , expressed \u201cblock-wise\u201d as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota=\\big(\\underbrace{u_{-L1}^{\\lambda},\\cdots u_{-L K}^{\\lambda}}_{\\underset{=}{\\mathrm{def}}_{u_{-L}^{\\lambda}}},\\cdots\\cdots,\\underbrace{u_{L1}^{\\lambda},\\cdots u_{L K}^{\\lambda}}_{\\underset{=}{\\mathrm{def}}_{u_{L}^{\\lambda}}},\\underbrace{u_{-L1}^{\\nu},\\cdots u_{-L K}^{\\nu}}_{\\underset{=}{\\mathrm{def}}_{u_{-L}^{\\nu}}},\\cdots\\cdots,\\underbrace{u_{L1}^{\\nu},\\cdots u_{L K}^{\\nu}}_{\\underset{=}{\\mathrm{def}}_{u_{L}^{\\nu}}}\\big)^{\\top}\\in\\mathbb{R}^{2K(2L+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the definition of the operator norm and the expression for $\\mathbf{A}(X)$ , we deduce that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{X}}\\|\\mathbf{A}(\\boldsymbol{X})\\|_{\\mathrm{op}}^{2}=\\mathbb{E}_{\\boldsymbol{X}}\\underset{\\|\\boldsymbol{u}\\|_{2}^{2}=1}{\\operatorname*{sup}}\\|\\mathbf{A}(\\boldsymbol{X})\\boldsymbol{u}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\boldsymbol{X}}\\underset{\\|\\boldsymbol{u}\\|_{2}^{2}=1}{\\operatorname*{sup}}\\underset{\\ell=-L}{\\overset{L}{\\sum}}\\left(\\left<\\boldsymbol{u}_{\\ell}^{\\lambda}-\\boldsymbol{u}_{\\ell}^{\\nu},\\,t(\\boldsymbol{X})\\right>\\right)^{2}}\\\\ &{\\qquad\\qquad\\leqslant2\\mathbb{E}_{\\boldsymbol{X}}\\left[\\|t(\\boldsymbol{X})\\|_{2}^{2}\\right]\\underset{\\|\\boldsymbol{u}\\|_{2}^{2}=1}{\\operatorname*{sup}}\\underset{\\ell\\in[L]}{\\operatorname*{sup}}\\left(\\left\\|\\boldsymbol{u}_{\\ell}^{\\lambda}\\right\\|_{2}^{2}+\\|\\boldsymbol{u}_{\\ell}^{\\nu}\\|_{2}^{2}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality combines the Cauchy-Schwartz inequality and the fact that $\\lVert\\pmb{v}-\\pmb{w}\\rVert_{2}^{2}\\leqslant$ $2(\\|\\pmb{v}\\|_{2}^{2}+\\|\\pmb{w}\\|_{2}^{2})$ for all $\\pmb{v},\\pmb{w}\\in\\mathbb{R}^{m}$ . The proof is concluded using Lemma F.4 to bound $\\mathbb{E}_{\\pmb{X}}\\left\\|\\pmb{\\operatorname{t}{(X)}}\\right\\|_{2}^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1 (Price of discretization). Let Assumption 3.1 be satisfied. Let $\\beta,B>0,L\\in\\mathbb{N}.$ . Consider ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{L}^{\\star}\\stackrel{\\mathrm{def}}{=}\\operatorname*{inf}_{h:\\mathbb{R}^{d}\\to\\mathbb{R}}\\left\\{\\mathbb{E}(h(X)-\\eta(X))^{2}\\,:\\,\\operatorname*{sup}_{t\\in\\mathbb{R}}\\left|\\mathbb{P}(h(X)\\leqslant t\\mid S=s)-\\mathbb{P}(h(X)\\leqslant t)\\right|\\leqslant\\varepsilon_{s}/2,\\quad\\forall s\\in[h,T]\\,\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pi_{\\Lambda^{\\star},\\mathbf{V}^{\\star}})\\leqslant\\mathcal{R}^{\\star}+\\frac{4B}{L}+\\frac{1}{L^{2}}+\\frac{\\log(2L+1)}{\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let us assume that $\\mathcal{R}^{\\star}=\\mathbb{E}(h^{\\star}(\\pmb{X})-\\eta(\\pmb{X}))$ for some $h^{\\star}:\\mathbb{R}^{d}\\rightarrow[-B,B]$ . If it is not the case, the standard argument based on the minimizing sequence yields the same result. ", "page_idx": 16}, {"type": "text", "text": "Consider an operator $T_{L}$ , which maps a deterministic classifier $h:\\mathbb{R}^{d}\\rightarrow[-B,B]$ onto a deterministic classifier $T_{L}(h):\\mathbb{R}^{d}\\rightarrow\\widehat{\\mathcal{P}}_{L}$ , which is defined point-wise as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n(T_{L}(h))(\\pmb{x})=\\lfloor L h(\\pmb{x})/B\\rfloor B/L\\qquad\\forall\\pmb{x}\\in\\mathbb{R}^{d}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\lfloor a\\rfloor$ is the closest integer smaller or equal to $a\\in\\mathbb R$ in absolute value. Notice, that for any $\\ell\\in\\{-L,\\ldots,L-1\\}$ and any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n(T_{L}(h^{\\star}))({\\pmb x})=\\frac{\\ell B}{L}\\quad\\Longleftrightarrow\\quad h^{\\star}({\\pmb x})\\in\\left[\\frac{\\ell B}{L},\\frac{(\\ell+1)B}{L}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\n(T_{L}(h^{\\star}))({\\pmb x})=B\\quad\\Longleftrightarrow\\quad h^{\\star}({\\pmb x})=B\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $h^{\\star}$ satisfies $(\\varepsilon/2)$ -fairness constraints, one checks that for all $\\ell\\in[L],s\\in[K]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{U}_{s}(T_{L}(h^{\\star}),\\ell)\\leqslant\\varepsilon_{s}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, $T_{L}(h^{\\star})$ is feasible for the problem in (4). Therefore, Lemma 3.3 implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pi_{\\Lambda^{\\star},\\mathbf{V}^{\\star}})\\leqslant\\mathcal{R}(T_{L}(h^{\\star}))+\\frac{\\log(2L+1)}{\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, since $|T_{L}(h^{\\star})({\\pmb x})-h^{\\star}({\\pmb x})|\\leqslant1/L$ and $|\\eta(\\pmb{x})-h^{\\star}(\\pmb{x})|\\leqslant2B$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}(T_{L}(h^{\\star}))=\\mathbb{E}\\left(\\eta(X)-T_{L}(h^{\\star})(X)\\right)^{2}\\leqslant\\mathcal{R}(h^{\\star})+\\frac{4B}{L}+\\frac{1}{L^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof is concluded. ", "page_idx": 16}, {"type": "text", "text": "1: Input: function $F$ ; initial vector $\\pmb{w}_{0}$ ; parameters $\\mu,M\\geqslant0$ ; number of iterations $T\\geqslant1$   \n2: $\\pmb{w}_{0}^{\\bar{a}g}=\\pmb{w}_{0}$   \n3: for $t=1$ to $T$ do   \n4: sample new $z\\sim P$ , independently from the past   \n5: $\\begin{array}{r}{\\alpha_{t}\\gets\\frac{2}{t+1}}\\end{array}$   \n6: $\\begin{array}{r}{\\gamma_{t}\\leftarrow\\frac{4M}{t(t+1)}}\\end{array}$   \n7: $\\begin{array}{r l}&{w_{t}^{m d}\\leftarrow\\frac{(1-\\alpha_{t})(\\mu+\\gamma_{t})}{\\gamma_{t}+(1-\\alpha_{t}^{2})\\mu}w_{t-1}^{a g}+\\frac{\\alpha_{t}((1-\\alpha_{t})\\mu+\\gamma_{t})}{\\gamma_{t}+(1-\\alpha_{t}^{2})\\mu}w_{t-1}}\\\\ &{w_{t}\\leftarrow\\mathrm{Proj}_{W}\\left\\{\\frac{(1-\\alpha_{t})\\mu+\\gamma_{t}}{\\mu+\\gamma_{t}}w_{t-1}+\\frac{\\alpha_{t}\\mu}{\\mu+\\gamma_{t}}w_{t}^{m d}-\\frac{\\alpha_{t}}{\\mu+\\gamma_{t}}\\nabla f_{w}({w}_{t}^{m d},z)\\right\\}}\\\\ &{w_{t}^{a g}\\leftarrow\\alpha_{t}w_{t}+(1-\\alpha_{t})w_{t-1}^{a g}}\\end{array}$   \n8:   \n9:   \n10: end for   \n11: return ${\\pmb w}_{t}^{a g}$ ", "page_idx": 17}, {"type": "text", "text": "C Additional details on the algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this part of the appendix, we provide the analysis for the proposed algorithm. First, we introduce required notation and recall a result of Foster et al. (2019), who provided an algorithm for convex stochastic optimization. The provided algorithm is a refined version of the SDG3 algorithm of Allen-Zhu (2021). We note that Foster et al. (2019) give a control of the expected norm of a gradient, while we require a control of the expected squared norm of the gradient mapping. We introduce projection to the algorithm of Foster et al. (2019) based on the original algorithm of Ghadimi and Lan (2012) and provide a control of the expected squared norm of the gradient mapping of the final estimated solution. ", "page_idx": 17}, {"type": "text", "text": "The setup and notation. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Consider $f:\\mathbb{R}^{d}\\times\\mathcal{Z}\\to\\mathbb{R}$ , such that ${\\pmb w}\\mapsto f({\\pmb w},z)$ is convex for each $z\\in{\\mathcal{Z}}$ . Let $W\\subset\\mathbb{R}^{d}$ be a closed convex set. Let ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(\\pmb{w})\\stackrel{\\mathrm{def}}{=}\\int f(\\pmb{w},z)\\mathrm{d}\\,P(z)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some probability distribution $P$ on $\\mathcal{Z}$ . In what follows, we assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{w}^{\\star}\\in\\arg\\operatorname*{min}_{\\pmb{w}\\in W}\\pmb{F}(\\pmb{w})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "always exists. ", "page_idx": 17}, {"type": "text", "text": "Assumption C.1. We assume that $F$ is $M$ -smooth and the variance of $\\nabla_{\\pmb{w}}f(\\pmb{w},z)$ is bounded. That is, for some $M>0$ and $\\sigma>0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall w,w^{\\prime}\\in W\\qquad\\|\\nabla F({\\pmb w})-\\nabla F({\\pmb w}^{\\prime})\\|\\leqslant M\\,\\|{\\pmb w}-{\\pmb w}^{\\prime}\\|\\qquad a n d}\\\\ &{\\forall{\\pmb w}\\in W\\qquad\\int\\Big[\\|\\nabla_{\\pmb w}f({\\pmb w},z)-\\nabla F({\\pmb w})\\|^{2}\\Big]\\,\\mathrm{d}\\,P(z)\\leqslant{\\pmb\\sigma}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us also define gradient mapping as ", "page_idx": 17}, {"type": "equation", "text": "$$\nG_{F,\\alpha}({\\boldsymbol w})\\stackrel{\\mathrm{def}}{=}\\frac{\\boldsymbol{w}-\\boldsymbol{w}_{+}}{\\alpha}\\quad\\mathrm{with}\\quad\\boldsymbol{w}_{+}\\in\\underset{\\boldsymbol{w}^{\\prime}\\in W}{\\arg\\operatorname*{min}}\\,\\left\\{\\left\\langle\\nabla F({\\boldsymbol w}),\\,{\\boldsymbol w}^{\\prime}\\right\\rangle+\\frac{1}{2\\alpha}\\left\\|{\\boldsymbol w}^{\\prime}-{\\boldsymbol w}\\right\\|^{2}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\mathrm{Proj}_{W}(\\cdot)$ be the Euclidean projection onto closed convex $W$ . ", "page_idx": 17}, {"type": "text", "text": "C.1 Some known results. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start by introducing the original AC-SA algorithm of Ghadimi and Lan (2012) and recall some of their results for the sake of completeness. ", "page_idx": 17}, {"type": "table", "img_path": "UtbjD5LGnC/tmp/6b27da0b63c9f7a47ee0b23ea3bc695b3387a69ef6d72c9bcb95749088e9dfbf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "equation", "text": "$\\mathbf{Algorithm\\4!:}\\mathbf{SGD3-ref\\ined}(F,\\mathbf{w}_{0},\\mu,M,T)$ ", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "1: Input: function $F$ ; initial vector $\\pmb{w}_{0}$ ; parameters $0<\\mu\\leqslant M$ ; number of iterations   \n$\\begin{array}{r}{\\bar{T}\\geqslant\\Omega\\left(\\frac{M}{\\mu}\\log_{2}\\frac{M}{\\mu}\\right)}\\end{array}$   \n2: $\\begin{array}{r}{F^{(0)}({\\pmb w})\\gets F({\\pmb w})+\\frac{\\mu}{2}\\left\\|{\\pmb w}-{\\pmb w}_{0}\\right\\|^{2};\\widehat{{\\pmb w}}_{0}\\gets{\\pmb w}_{0};\\mu_{0}\\gets\\mu}\\end{array}$   \n3: for $j=1$ to $\\begin{array}{r}{J=\\left\\lfloor\\log\\frac{M}{\\mu}\\right\\rfloor}\\end{array}$ do   \n4: $\\begin{array}{r}{\\widehat{\\pmb{w}}_{j}\\gets\\mathtt{A C}\\!-\\!\\mathtt{S A}^{2}(F^{(j-1)},\\widehat{\\pmb{w}}_{j-1},\\mu_{j-1},2(M+\\mu),\\frac{T}{J})}\\end{array}$   \n5: \u00b5j \u21902\u00b5j\u22121   \n6: $\\begin{array}{r}{{\\cal F}^{(j)}({\\pmb w})\\stackrel{\\mathrm{def}}{=}{\\cal F}^{(j-1)}({\\pmb w})+\\frac{\\mu_{j}}{2}\\,\\|{\\pmb w}-\\widehat{\\pmb w}_{j}\\|^{2}}\\end{array}$   \n7: end for   \n8: return $\\widehat{\\pmb{w}}_{J}$ ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. (Ghadimi and Lan, 2012, Proposition 9) Let $\\mathbf{\\Delta}\\cdot\\mathbf{\\mu}\\cdot\\mathbf{\\sigma}\\cdot\\mathbf{\\mu}\\cdot\\mathbf{\\sigma}\\cdot\\mathbf{\\mu}\\cdot\\mathbf{\\sigma}\\cdot\\mathbf{\\mu}$ $w_{0}\\in W\\mathrm{~}a$ starting vector. If $F$ is $\\mu-$ strongly convex and $T\\geqslant1$ then with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{t}=\\frac{2}{t+1}\\;\\;\\;\\;a n d\\;\\;\\;\\gamma_{t}=\\frac{4M}{t(t+1)},\\;\\;\\;\\;\\forall t>1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$A\\,C\\!-\\!S A(F,{\\pmb w}_{0},\\mu,M,T).$ , defined in Algorithm 2, outputs $\\widehat{\\pmb{w}}_{T}$ satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\widehat{\\pmb{w}}_{T})]-F(\\pmb{w}^{*})\\leqslant\\frac{2M\\left\\|\\pmb{w}_{0}-\\pmb{w}^{\\star}\\right\\|^{2}}{T^{2}}+\\frac{8\\sigma^{2}}{\\mu T}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Foster et al. (2019) propose another version of AC-SA, called $\\mathtt{A C-S A^{2}}$ , which resets the stepsize halfway through the process. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1. (Foster et al., 2019, Lemma $^{\\,l}$ ) Let $W=\\mathbb{R}^{d}$ , $\\pmb{w}^{\\star}\\in\\arg\\operatorname*{min}_{\\pmb{w}\\in W}F(\\pmb{w}),\\,\\pmb{w}_{0}\\in\\mathbb{R}^{d}$ starting vector. If $\\mu>0$ , $M\\geqslant0$ and $T\\geqslant1$ then $A\\,C\\!-\\!S A^{2}(F,{\\pmb w}_{0},\\mu,M,T)$ , defined in Algorithm $^3$ , outputs $\\widehat{\\pmb{w}}$ satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\widehat{\\pmb{w}})]-F(\\pmb{w}^{*})\\leqslant\\frac{128M^{2}\\left\\|\\pmb{w}_{0}-\\pmb{w}^{\\star}\\right\\|^{2}}{\\mu T^{4}}+\\frac{256M\\sigma^{2}}{\\mu^{2}T^{3}}+\\frac{16\\sigma^{2}}{\\mu T}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark C.1. Foster et al. (2019) do not consider constrained optimization throughout their work.   \nHowever, the proof of Lemma C.1 follows analogous arguments. ", "page_idx": 18}, {"type": "text", "text": "Foster et al. (2019) also introduce a refined version of algorithm SGD3 of Allen-Zhu (2021). ", "page_idx": 18}, {"type": "text", "text": "In what follows, we will show that Algorithm 4, after $T$ evaluations of the stochastic gradient, produces a point $\\widehat{\\pmb w}$ such that $\\mathbf{E}\\|G_{F,\\alpha}(\\tilde{\\pmb{w}})\\|^{2}$ is controlled. This is a, rather mild, extension of Foster et al. (2019) and Allen-Zhu (2021). ", "page_idx": 18}, {"type": "text", "text": "C.2 Control of the expected squared norm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Most of the proof techniques are already present in the original contribution of Allen-Zhu (2021) and Foster et al. (2019), we slightly extend their proof, introducing modifications related to the control of the squared norm and the projection step. For some $J\\geqslant1$ , to be fixed later on, introduce ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{\\widetilde{\\mu}}(w)\\stackrel{\\mathrm{def}}{=}F(w)+\\sum_{j=1}^{J}\\frac{\\mu_{j}}{2}\\left\\|w-\\widehat{w}_{j}\\right\\|^{2}\\quad\\mathrm{and}\\quad w_{\\widetilde{\\mu}}^{\\star}\\in\\underset{w\\in W}{\\mathrm{arg}\\,\\mathrm{min}}\\ F_{\\widetilde{\\mu}}(w)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By construction, $F_{\\widetilde{\\mu}}$ is $\\begin{array}{r}{\\widetilde{\\mu}\\ \\overset{\\mathrm{def}}{=}\\sum_{j=1}^{J}\\mu_{j}}\\end{array}$ -strongly convex and $(M+{\\widetilde{\\mu}})$ -smooth. Let us also define $F^{(0)}\\ {\\stackrel{\\mathrm{def}}{=}}\\ F(\\pmb{w})$ and $\\begin{array}{r}{{\\cal F}^{(j)}({\\pmb w})\\stackrel{\\mathrm{def}}{=}{\\cal F}^{(j-1)}({\\pmb w})+\\frac{\\mu_{j}}{2}\\,\\|{\\pmb w}-\\widehat{\\pmb w}_{j}\\|^{2}}\\end{array}$ , for $j=1,2,\\dots,J$ . We will use the following results of Allen-Zhu (2021). ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2. (Allen-Zhu, 2021, Lemma 2.3) Let $\\widetilde{F}$ be an $\\widetilde{M}$ -smooth and $\\widetilde{\\mu}$ -strongly convex function. Let ${\\pmb w},{\\pmb w}^{\\prime}\\in W$ and $\\pmb{w}^{+}=\\pmb{w}-\\alpha\\cdot\\pmb{G}_{\\widetilde{F},\\alpha}(\\pmb{w})$ . For any $\\alpha\\in\\left(0,\\frac{1}{\\widetilde{M}}\\right]$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{F}({\\pmb w}^{\\prime})\\geqslant\\widetilde{F}({\\pmb w}^{+})+\\left\\langle{\\pmb G}_{\\widetilde{F},\\alpha}({\\pmb w}),\\,{\\pmb w}^{\\prime}-{\\pmb w}\\right\\rangle+\\frac{\\alpha}{2}\\left\\|{\\pmb G}_{\\widetilde{F},\\alpha}({\\pmb w})\\right\\|^{2}+\\frac{\\widetilde{\\mu}}{2}\\left\\|{\\pmb w}^{\\prime}-{\\pmb w}\\right\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. (Allen-Zhu, 2021, Lemma 5.1) Consider $F_{\\widetilde{\\mu}}$ and $\\pmb{w}_{\\tilde{\\mu}}^{\\star}$ as defined in (22) and $w\\in W$ . For any $\\begin{array}{r}{\\alpha\\in\\left(0,\\frac{1}{M+\\widetilde{\\mu}}\\right]}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|G_{F,\\alpha}(\\pmb{w})\\|\\leqslant\\sum_{j=1}^{J}\\mu_{j}\\,\\big\\|\\pmb{w}_{\\widetilde{\\mu}}^{\\star}-\\widehat{\\pmb{w}}_{j}\\big\\|+3\\,\\big\\|G_{F_{\\widetilde{\\mu}},\\alpha}(\\pmb{w})\\big\\|\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Claim C.1. (Allen-Zhu, 2021, Claim 6.2) Suppose for every $j~=~1,\\ldots,J$ the iterates $\\widehat{\\pmb{w}}_{j}$ of Algorithm 4 satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[F^{(j-1)}\\left(\\widehat{\\pmb{w}}_{j}\\right)-F^{(j-1)}\\left(\\pmb{w}_{j-1}^{\\star}\\right)\\right]\\leqslant\\delta_{j}\\quad w h e r e\\quad\\pmb{w}_{j-1}^{\\star}\\in\\arg\\operatorname*{min}_{w}\\left\\{F^{(j-1)}({\\pmb{w}})\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r\\;e v e r y\\;j\\geqslant1\\;w e\\;h a\\nu e\\;\\mathbf{E}\\left[\\left\\|\\widehat{w}_{j}-\\boldsymbol{w}_{j-1}^{\\star}\\right\\|\\right]^{2}\\leqslant\\mathbf{E}\\left[\\left\\|\\widehat{w}_{j}-\\boldsymbol{w}_{j-1}^{\\star}\\right\\|^{2}\\right]\\leqslant\\frac{2\\delta_{j}}{\\mu_{j-1}};}\\\\ &{r\\;e v e r y\\;j\\geqslant1\\;w e\\;h a\\nu e\\;\\mathbf{E}\\left[\\left\\|\\widehat{w}_{j}-\\boldsymbol{w}_{j}^{\\star}\\right\\|\\right]^{2}\\leqslant\\mathbf{E}\\left[\\left\\|\\widehat{w}_{j}-\\boldsymbol{w}_{j}^{\\star}\\right\\|^{2}\\right]\\leqslant\\frac{\\delta_{j}}{\\mu_{j}};}\\\\ &{\\mu_{j}=2\\mu_{j-1},\\,t h e n f o r\\;a l l\\;j\\geqslant1\\;w e\\;h a\\nu e\\;\\mathbf{E}\\left[\\sum_{j=1}^{J}\\mu_{j}\\left\\|\\widehat{w}_{j}-\\boldsymbol{w}_{j}^{\\star}\\right\\|\\right]\\leqslant4\\sum_{j=1}^{J}\\sqrt{\\delta_{j}{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition to Claim C.1, we prove the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4. Suppose for every $j=1,\\dotsc,J,\\,\\mu_{j}=2\\mu_{j-1}$ and the iterates $\\widehat{\\pmb{w}}_{j}$ of Algorithm $^{4}$ satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[F^{(j-1)}\\left(\\widehat{\\pmb{w}}_{j}\\right)-F^{(j-1)}\\left(\\pmb{w}_{j-1}^{\\star}\\right)\\right]\\leqslant\\delta_{j}\\quad w h e r e\\quad\\pmb{w}_{j-1}^{\\star}\\in\\arg\\operatorname*{min}_{w}\\left\\{F^{(j-1)}({\\pmb{w}})\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left(\\sum_{j=1}^{J}\\mu_{j}\\,\\lVert\\pmb{w}_{J}^{\\star}-\\widehat{\\pmb{w}}_{j}\\rVert\\right)^{2}\\right]\\leqslant16J\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{P_{j}\\stackrel{\\mathrm{def}}{=}\\sum_{t=1}^{j}\\mu_{t}\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{t}\\right\\|}\\end{array}$ , yielding that $\\begin{array}{r}{P_{J}=\\sum_{j=1}^{J}(P_{j}-P_{j-1})}\\end{array}$ , with the agreement that $P_{0}=0$ . Cauchy-Schwartz inequality gives ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{J}^{2}=\\left(\\sum_{j=1}^{J}(P_{j}-P_{j-1})\\right)^{2}\\leqslant J\\sum_{j=1}^{J}(P_{j}-P_{j-1})^{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $P_{j}$ is non-decreasing, to bound the above quantity, it suffices to bound each increment of the form $P_{j}-P_{j-1}$ . One can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{j}-P_{j-1}\\overset{(a)}{\\leqslant}\\mu_{j}\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|+\\displaystyle\\sum_{t=1}^{j-1}\\mu_{t}(\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{t}\\right\\|-\\left\\|\\pmb{w}_{j-1}^{\\star}-\\widehat{\\pmb{w}}_{t}\\right\\|)}\\\\ &{\\overset{(b)}{\\leqslant}\\mu_{j}\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|+\\left(\\displaystyle\\sum_{t=1}^{j-1}\\mu_{t}\\right)\\left\\|\\pmb{w}_{j}^{\\star}-\\pmb{w}_{j-1}^{\\star}\\right\\|}\\\\ &{\\overset{(c)}{\\leqslant}\\mu_{j}(2\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|+\\left\\|\\pmb{w}_{j-1}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (a) follows from the definition of $P_{j}$ , (b) from reverse triangle inequality and (c) uses triangle inequality and the fact that $\\textstyle\\sum_{t=1}^{j-1}\\mu_{t}\\leqslant\\mu_{j}$ as $\\mu_{j}=2\\mu_{j-1}$ . Therefore, using the fact that $(a+b)^{2}\\leqslant$ $2a^{2}+2b^{2}$ , we deduce from the above that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(P_{j}-P_{j-1})^{2}\\leqslant2\\mu_{j}^{2}(4\\left\\|\\pmb{w}_{j}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|^{2}+\\left\\|\\pmb{w}_{j-1}^{\\star}-\\widehat{\\pmb{w}}_{j}\\right\\|^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking the expectation and applying Claim C.1(a) and Claim C.1(b), the latter is bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Im\\left[(P_{j}-P_{j-1})^{2}\\right]\\leqslant8\\mu_{j}^{2}\\mathbf{E}[\\left\\|w_{j}^{\\star}-\\widehat{w}_{j}\\right\\|^{2}]+2\\mu_{j}^{2}\\mathbf{E}[\\left\\|w_{j-1}^{\\star}-\\widehat{w}_{j}\\right\\|^{2}]\\leqslant8\\mu_{j}^{2}\\frac{\\delta_{j}}{\\mu_{j}}+2\\mu_{j}^{2}\\frac{2\\delta_{j}}{\\mu_{j-1}}=16\\mu_{j}\\delta_{j}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging (24) into (23) yields the claimed bound. ", "page_idx": 20}, {"type": "text", "text": "Remark C.2. Notice, that in Algorithm $^{4}$ we apply $A{\\mathcal{C}}{-}S A^{2}$ to $F^{(j-1)}$ with starting point $\\widehat{\\pmb{w}}_{j-1}$ and $T/J$ iterations. Since $F^{(j-1)}$ is $\\begin{array}{r}{M+\\sum_{t=1}^{j-1}\\mu_{t}\\leqslant2M-}\\end{array}$ smooth and $\\mu_{j-1}$ \u2212strongly convex ,  applying Lemma C.1 and Claim $C.I(b)$ , we ge t $\\mathbf{E}\\left[\\bar{F}^{(j-1)}\\left(\\widehat{\\pmb{w}}_{j}\\right)-F^{(j-1)}\\left(\\pmb{w}_{j-1}^{\\star}\\right)\\right]\\leqslant\\delta_{j}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{j}\\leqslant\\frac{128(2M)^{2}\\mathbf{E}\\,\\left\\|\\widehat{w}_{j-1}-\\pmb{w}_{j-1}^{*}\\right\\|^{2}}{\\mu_{j-1}(T/J)^{4}}+\\frac{256(2M)\\sigma^{2}}{\\mu_{j-1}^{2}(T/J)^{3}}+\\frac{16\\sigma^{2}}{\\mu_{j-1}(T/J)}}\\\\ &{\\quad\\leqslant\\frac{2^{9}M^{2}\\delta_{j-1}}{\\mu_{j-1}^{2}(T/J)^{4}}+\\frac{2^{9}M\\sigma^{2}}{\\mu_{j-1}^{2}(T/J)^{3}}+\\frac{2^{4}\\sigma^{2}}{\\mu_{j-1}(T/J)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We are in position to prove the main ingredient of this section. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.2 (Control of the expected squared norm). Let $\\pmb{w}^{\\star}\\in\\arg\\operatorname*{min}_{\\pmb{w}\\in W}F(\\pmb{w}),\\,\\pmb{w}_{0}\\in\\mathbb{R}^{d}$ a starting vector. When $\\mu\\,\\in\\,(0,M]$ and $\\begin{array}{r}{T\\,>\\,2^{11/4}\\sqrt{\\frac{M}{\\mu}}\\left\\lfloor\\log_{2}\\frac{M}{\\mu}\\right\\rfloor$ , then for $\\begin{array}{r}{\\alpha\\:=\\:\\frac{1}{2^{J+2}\\mu}}\\end{array}$ , with $\\begin{array}{r}{J=\\left\\lfloor\\log_{2}\\frac{M}{\\mu}\\right\\rfloor\\!,\\,\\,S G D3\\!-\\!r e f i n e d(F,{\\pmb w}_{0},\\mu,M,T)}\\end{array}$ outputs $\\widehat{\\pmb{w}}$ satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\left[\\left\\|{G_{F,\\alpha}(\\widehat{\\pmb{w}})}\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}\\cdot2^{16}M^{2}}{T^{4}}\\log_{2}^{5}\\frac{M}{\\mu}+2\\mu^{2}\\right)\\left\\|{\\pmb{w}}_{0}-\\pmb{w}_{\\mu}^{*}\\right\\|^{2}}\\\\ {+\\,\\frac{3^{4}\\cdot2^{17}M\\sigma^{2}}{\\mu T^{3}}\\log_{2}^{4}\\frac{M}{\\mu}+\\frac{3^{4}\\cdot2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{M}{\\mu}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Part I. At first, let us assume that $F$ is $\\mu_{0}$ -strongly convex. Since the definition of $F$ satisfies the definition given in (22) with $J\\mathrm{~-~}1$ , applying Lemma C.3 and using the fact that $(a+b)^{2}\\leqslant2a^{2}+2b^{2}$ , we get that for any $\\begin{array}{r}{\\alpha\\in\\big(0,(\\bar{M}+\\sum_{j=1}^{\\bar{J}-1}\\mu_{j})^{-1}\\big]}\\end{array}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left\\|\\boldsymbol{G}_{F,\\alpha}(\\widehat{w}_{J})\\right\\|^{2}\\right]\\leqslant\\mathbf{E}\\left[\\left(3\\left\\|\\boldsymbol{G}_{F^{(J-1)},\\alpha}(\\widehat{w}_{J})\\right\\|+\\displaystyle\\sum_{j=1}^{J-1}\\mu_{j}\\left\\|\\pmb{w}_{J-1}^{\\star}-\\widehat{w}_{j}\\right\\|\\right)^{2}\\right]}\\\\ &{\\leqslant2\\mathbf{E}\\left[9\\left\\|\\pmb{G}_{F^{(J-1)},\\alpha}(\\widehat{w}_{J})\\right\\|^{2}+\\left(\\displaystyle\\sum_{j=1}^{J-1}\\mu_{j}\\left\\|\\pmb{w}_{J-1}^{\\star}-\\widehat{w}_{j}\\right\\|\\right)^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note, that due to definition of $\\mu_{j}$ , we have $\\textstyle\\sum_{j=1}^{J-1}\\mu_{j}\\leqslant M$ , thus, the derived inequality holds for any $\\begin{array}{r}{\\alpha\\in\\left(0,(2M)^{-1}\\right]\\subset\\left(0,(M+\\sum_{j=1}^{J-1}\\mu_{j})^{-1}\\right]}\\end{array}$ . Lemma C.4 provides a control of the second term of (25). To control the first term, let us apply Lemma C.2 with $F^{(J-1)}$ and $\\pmb{w}=\\pmb{w}^{\\prime}=\\pmb{\\hat{w}}_{J}$ , getting $\\begin{array}{r}{\\frac{\\mathtt{x}}{2}\\left\\|G_{F^{(J-1)},\\alpha}(\\widehat{w}_{J})\\right\\|^{2}\\;\\leqslant\\;F^{(J-1)}(\\widehat{w}_{J})\\;-\\;F^{(J-1)}(\\widehat{w}_{J}^{+})\\;\\leqslant\\;F^{(J-1)}(\\widehat{w}_{J})\\;-\\;F^{(J-1)}({\\pmb w}_{J-1}^{*}),\\forall\\alpha\\in\\mathbb{Z}.}\\end{array}$ $\\textstyle(0,{\\frac{1}{2M}}]$ . Meaning, that $\\begin{array}{r}{\\left\\|\\boldsymbol{G}_{F^{(J-1)},\\alpha}(\\widehat{\\pmb{w}}_{J})\\right\\|^{2}\\leqslant\\frac{2\\delta_{J}}{\\alpha}}\\end{array}$ . Let us recall, that $\\begin{array}{r}{J=\\left\\lfloor\\log_{2}\\frac{M}{\\mu_{0}}\\right\\rfloor}\\end{array}$ and $\\mu_{J}=$ $2^{J}\\mu_{0}\\,\\leqslant\\,M\\,\\leqslant\\,2\\mu_{J}$ . Hence, choosing $\\begin{array}{r}{\\alpha\\,=\\,\\frac{1}{4\\mu_{J}}}\\end{array}$ and substituting the derived bound into (25), we deduce that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\pmb{w}}_{J})\\right\\|^{2}\\right]\\leqslant\\frac{36\\delta_{J}}{\\alpha}+32(J-1)\\sum_{j=1}^{J-1}\\mu_{j}\\delta_{j}\\leqslant144J\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, let us substitute the bound on $\\delta_{j}$ from Remark C.2 and replicate the steps of Foster et al. (2019) to control the above. We get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{J}\\mu_{j}\\delta_{j}\\leqslant\\frac{2^{10}M^{2}\\prod\\left\\langle\\ln\\ o-w^{*}\\right\\|^{2}}{(T/J)^{4}}+\\frac{2^{10}M\\sigma^{2}}{\\mu_{0}(T/J)^{3}}+\\frac{2^{5}\\sigma^{2}}{(T/J)}+\\sum_{j=2}^{J}\\left(\\frac{2^{10}M^{2}\\delta_{j-1}}{\\mu_{j-1}(T/J)^{4}}+\\frac{2^{10}M\\sigma^{2}}{\\mu_{j-1}(T/J)^{3}}+\\frac{2^{5}\\sigma^{2}}{(T/J)}\\right)}\\\\ &{\\qquad\\leqslant\\frac{2^{10}M^{2}\\prod\\left\\vert w_{0}-w^{*}\\right\\vert^{2}J^{4}}{T^{4}}+\\frac{2^{10}M\\sigma^{2}J^{3}}{\\mu_{0}T^{3}}\\sum_{j=1}^{J}\\frac{1}{2^{j-1}}+\\frac{2^{5}\\sigma^{2}J^{2}}{T}+\\frac{2^{10}M^{2}J^{4}}{T^{4}}\\sum_{j=2}^{J}\\frac{\\delta_{j-1}}{\\mu_{j-1}}}\\\\ &{\\qquad\\leqslant\\frac{2^{10}M^{2}\\prod\\left\\vert w_{0}-w^{*}\\right\\vert^{2}J^{4}}{T^{4}}+\\frac{2^{11}M\\sigma^{2}J^{3}}{\\mu_{0}T^{3}}+\\frac{2^{5}\\sigma^{2}J^{2}}{T}+\\frac{2^{10}M^{2}J^{4}}{\\mu_{0}^{2}T^{4}}\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality comes from the facts that $\\textstyle\\sum_{j=1}^{J}{\\frac{1}{2^{j-1}}}\\leqslant2$ and $\\begin{array}{r}{\\sum_{j=2}^{J}\\frac{\\delta_{j-1}}{\\mu_{j-1}}\\leqslant\\sum_{j=1}^{J}\\frac{\\delta_{j}}{\\mu_{j}}\\leqslant}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\mu_{0}^{2}}\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}}\\end{array}$ . Rearranging the terms and multiplying both sides by $144J$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n144J\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\leqslant\\frac{9}{1-\\frac{2^{10}M^{2}J^{4}}{\\mu_{0}^{2}T^{4}}}\\left(\\frac{2^{14}M^{2}\\left\\|w_{0}-w^{*}\\right\\|^{2}J^{5}}{T^{4}}+\\frac{2^{15}M\\sigma^{2}J^{4}}{\\mu_{0}T^{3}}+\\frac{2^{9}\\sigma^{2}J^{3}}{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Choosing $\\begin{array}{r}{T>2^{11/4}J\\sqrt{\\frac{M}{\\mu_{0}}}}\\end{array}$ ensures that $\\frac{1}{1-\\frac{2^{10}M^{2}J^{4}}{\\mu_{0}^{2}T^{4}}}\\leqslant2$ . Finally, substituting the derived bounds and the value of $\\begin{array}{r}{J=\\left\\lfloor\\log_{2}\\frac{M}{\\mu_{0}}\\right\\rfloor}\\end{array}$ , we conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\left[\\Vert G_{F,\\alpha}(\\widehat{w}_{J})\\Vert^{2}\\right]\\leqslant\\frac{9\\cdot2^{15}M^{2}\\left\\Vert w_{0}-w^{*}\\right\\Vert^{2}}{T^{4}}\\log_{2}^{5}\\frac{M}{\\mu_{0}}+\\frac{9\\cdot2^{16}M\\sigma^{2}}{\\mu_{0}T^{3}}\\log_{2}^{4}\\frac{M}{\\mu_{0}}+\\frac{9\\cdot2^{10}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{M}{\\mu_{0}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Part II. When $F$ is not strongly convex, let $F_{\\mu}({\\pmb w})\\;\\stackrel{\\mathrm{def}}{=}\\;F({\\pmb w})\\,+\\,\\frac{\\mu}{2}\\,\\|{\\pmb w}-{\\pmb w}_{0}\\|^{2}$ and $w_{\\mu}^{\\star}~\\in$ $\\operatorname*{arg\\,min}_{\\pmb{w}}\\left\\{F_{\\mu}(\\pmb{w})\\right\\}$ . Applying (26) and Lemma C.3 with $J=1$ and $\\hat{\\pmb{w}}_{1}=\\pmb{w}_{0}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\pmb{w}})\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}\\cdot2^{16}M^{2}}{T^{4}}\\log_{2}^{5}\\frac{M}{\\mu}+2\\mu^{2}\\right)\\left\\|\\pmb{w}_{0}-\\pmb{w}_{\\mu}^{*}\\right\\|^{2}}\\\\ {+\\,\\frac{3^{4}\\cdot2^{17}M\\sigma^{2}}{\\mu T^{3}}\\log_{2}^{4}\\frac{M}{\\mu}+\\frac{3^{4}\\cdot2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{M}{\\mu}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{\\mu}{2}\\left\\Vert w^{\\star}-w_{0}\\right\\Vert^{2}-\\frac{\\mu}{2}\\left\\Vert w_{\\mu}^{\\star}-w_{0}\\right\\Vert^{2}=\\left(F_{\\mu}(\\pmb{w}^{\\star})-F(\\pmb{w}^{\\star})\\right)+\\left(F(\\pmb{w}_{\\mu}^{\\star})-F_{\\mu}(\\pmb{w}_{\\mu}^{\\star})\\right)\\geqslant0.}\\end{array}$ , then $\\left\\|\\pmb{w}_{\\mu}^{\\star}-\\pmb{w}_{0}\\right\\|^{2}\\leqslant\\left\\|\\pmb{w}^{\\star}-\\pmb{w}_{0}\\right\\|^{2}$ . The proof is concluded. ", "page_idx": 21}, {"type": "text", "text": "Remark C.3. Notice, that in Algorithm $^{4}$ we apply AC-SA to $F^{(j-1)}$ with starting point $\\widehat{\\pmb{w}}_{j-1}$ and $T/J$ iterations. Since $F^{(j-1)}$ i $\\begin{array}{r}{\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm{~~}\\mathrm{~}\\mathrm{~~}\\mathrm{~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~~}\\mathrm{~~}\\mathrm{~~~}\\mathrm{~~}\\mathrm{~~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm{~~}\\mathrm\\mathrm{~~}\\mathrm{~}\\mathrm{~~}\\mathrm{~}\\mathrm\\mathrm{~{~~}\\mathrm\\mathrm{~}\\mathrm{~~}\\mathrm{~}\\mathrm\\mathrm{~}\\mathrm{~}\\mathrm{~}\\mathrm\\mathrm{{~~}\\mathrm}$ smooth and $\\mu_{j-1}$ \u2212strongly convex ,  applying Lemma C.1 and Claim $C.I(b)$ , we ge t $\\mathbf{E}\\left[F^{(j-1)}\\left(\\widehat{\\pmb{w}}_{j}\\right)-F^{(j-1)}\\left(\\pmb{w}_{j-1}^{\\star}\\right)\\right]\\leqslant\\delta_{j}$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta_{j}\\leqslant\\frac{2(2M)\\mathbf{E}\\left\\|\\hat{\\pmb{w}}_{j-1}-\\pmb{w}_{j-1}^{*}\\right\\|^{2}}{(T/J)^{2}}+\\frac{8\\sigma^{2}}{\\mu_{j-1}(T/J)}\\leqslant\\frac{4M\\delta_{j-1}}{\\mu_{j-1}(T/J)^{2}}+\\frac{8\\sigma^{2}}{\\mu_{j-1}(T/J)}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem C.3 (Control of the expected squared norm). Let $\\pmb{w}^{\\star}\\in\\arg\\operatorname*{min}_{\\pmb{w}\\in W}F(\\pmb{w}),\\,\\pmb{w}_{0}\\in\\mathbb{R}^{d}$ a starting vector. When $\\mu\\,\\in\\,(0,M]$ and $\\begin{array}{r}{T\\,>\\,4\\sqrt{\\frac{M}{\\mu}}\\left\\lfloor\\log_{2}\\frac{M}{\\mu}\\right\\rfloor}\\end{array}$ , then for $\\begin{array}{r}{\\alpha\\,=\\,\\frac{1}{2^{J+2}\\mu}}\\end{array}$ 2J+12\u00b5, with J = $\\begin{array}{r}{\\log_{2}{\\frac{M}{\\mu}}\\,\\Big|,\\,S G D3\\!-\\!r e f i n e d(F,{\\pmb w}_{0},\\mu,M,T)}\\end{array}$ with AC-SA outputs $\\widehat{\\pmb w}$ satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{w})\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}2^{9}M\\mu}{T^{2}}\\log_{2}^{3}\\frac{M}{\\mu}+2\\mu^{2}\\right)\\left\\|w_{0}-w^{*}\\right\\|^{2}+\\frac{3^{4}2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{M}{\\mu}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Part I. At first, let us assume that $F$ is $\\mu_{0}$ -strongly convex. Applying Lemma C.3 and using the fact that $(a+b)^{2}\\leqslant2a^{2}+2b^{2}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left\\|\\boldsymbol{G}_{F,\\alpha}(\\widehat{w}_{J})\\right\\|^{2}\\right]\\leqslant\\mathbf{E}\\left[\\left(3\\left\\|\\boldsymbol{G}_{F^{(J-1)},\\alpha}(\\widehat{w}_{J})\\right\\|+\\displaystyle\\sum_{j=1}^{J-1}\\mu_{j}\\left\\|\\pmb{w}_{J-1}^{\\star}-\\widehat{w}_{j}\\right\\|\\right)^{2}\\right]}\\\\ &{\\leqslant2\\mathbf{E}\\left[9\\left\\|\\pmb{G}_{F^{(J-1)},\\alpha}(\\widehat{w}_{J})\\right\\|^{2}+\\left(\\displaystyle\\sum_{j=1}^{J-1}\\mu_{j}\\left\\|\\pmb{w}_{J-1}^{\\star}-\\widehat{w}_{j}\\right\\|\\right)^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.4 provides a control of the second term of the above inequality. To control the first term, let us apply Lemma C.2 with $F^{(J-1)}$ and $\\pmb{w}=\\pmb{w}^{\\prime}=\\pmb{\\hat{w}}_{J}$ , getting $\\frac{\\alpha}{2}\\left\\lVert G_{F^{(J-1)},\\alpha}(\\widehat{\\pmb{w}}_{J})\\right\\rVert^{2}\\leqslant$ $F^{(J-1)}(\\widehat{\\pmb{w}}_{J})\\,-\\,F^{(J-1)}(\\widehat{\\pmb{w}}_{J}^{+})\\,\\leqslant\\,F^{(J-1)}(\\widehat{\\pmb{w}}_{J})\\,-\\,F^{(J-1)}(\\pmb{w}_{J-1}^{*}),\\forall\\alpha\\,\\in\\,\\widetilde{(0,\\frac{1}{2M}]}$ . Mea n ing, that $\\begin{array}{r}{\\left\\|\\boldsymbol{G}_{F^{(J-1)},\\alpha}\\!\\left(\\widehat{\\pmb{w}}_{J}\\right)\\right\\|^{2}\\leqslant\\frac{2\\delta_{J}}{\\alpha}}\\end{array}$ . Let us recall, that $\\begin{array}{r}{J=\\left\\lfloor\\log_{2}\\frac{M}{\\mu_{0}}\\right\\rfloor}\\end{array}$ and $\\mu_{J}=2^{J}\\mu_{0}\\leqslant M\\leqslant2\\mu_{J}$ . Hence, choosing \u03b1 = $\\begin{array}{r}{\\alpha=\\frac{1}{4\\mu_{J}}}\\end{array}$ and substituting the derived bound into (27), we deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\pmb{w}}_{J})\\right\\|^{2}\\right]\\leqslant\\frac{36\\delta_{J}}{\\alpha}+32(J-1)\\sum_{j=1}^{J-1}\\mu_{j}\\delta_{j}\\leqslant144J\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us substitute the bound on $\\delta_{j}$ from Remark C.2 to control the above. We get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\leqslant\\frac{4M\\,\\|w_{0}-w^{*}\\|^{2}\\,\\mu_{1}}{(T/J)^{2}}+\\frac{8\\sigma^{2}\\mu_{1}}{(T/J)\\mu_{0}}+\\sum_{j=2}^{J}\\bigg(\\frac{8M\\delta_{j-1}}{(T/J)^{2}}+\\frac{16\\sigma^{2}}{(T/J)}\\bigg)}}\\\\ &{\\leqslant\\frac{8M\\mu_{0}\\,\\|w_{0}-w^{*}\\|^{2}\\,J^{2}}{T^{2}}+\\frac{32\\sigma^{2}J^{2}}{T}+\\frac{8M\\,J^{2}}{T^{2}}\\sum_{j=2}^{J}\\delta_{j-1}}\\\\ &{\\leqslant\\frac{8M\\mu_{0}\\,\\|w_{0}-w^{*}\\|^{2}\\,J^{2}}{T^{2}}+\\frac{32\\sigma^{2}J^{2}}{T}+\\frac{8M J^{2}}{\\mu_{0}T^{2}}\\sum_{j=2}^{J}\\mu_{j-1}\\delta_{j-1}}\\\\ &{\\leqslant\\frac{8M\\mu_{0}\\,\\|w_{0}-w^{*}\\|^{2}\\,J^{2}}{T^{2}}+\\frac{32\\sigma^{2}J^{2}}{T}+\\frac{8M\\,J^{2}}{\\mu_{0}T^{2}}\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Rearranging the terms and multiplying both sides by $144J$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n144J\\sum_{j=1}^{J}\\mu_{j}\\delta_{j}\\leqslant\\frac{1152}{1-\\frac{8M J^{2}}{\\mu_{0}T^{2}}}\\left(\\frac{M\\mu_{0}\\left\\|\\pmb{w}_{0}-\\pmb{w}^{*}\\right\\|^{2}J^{3}}{T^{2}}+\\frac{4\\sigma^{2}J^{3}}{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Choosing $T>4J\\sqrt{\\frac{M}{\\mu_{0}}}$ ensures that $\\frac{1}{1-\\frac{8M J^{2}}{\\mu_{0}T^{2}}}\\leqslant2$ . Finally, substituting the derived bounds and the value of $\\begin{array}{r}{J=\\left\\lfloor\\log_{2}\\frac{M}{\\mu_{0}}\\right\\rfloor}\\end{array}$ , we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\pmb{w}}_{J})\\right\\|^{2}\\right]\\leqslant2304\\log_{2}^{3}\\frac{M}{\\mu_{0}}\\left(\\frac{M\\mu_{0}\\left\\|\\pmb{w}_{0}-\\pmb{w}^{*}\\right\\|^{2}}{T^{2}}+\\frac{4\\sigma^{2}}{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Part II. When $F$ is not strongly convex, let $F_{\\mu}({\\pmb w})\\;\\stackrel{\\mathrm{def}}{=}\\;F({\\pmb w})\\,+\\,\\frac{\\mu}{2}\\,\\|{\\pmb w}-{\\pmb w}_{0}\\|^{2}$ and $w_{\\mu}^{\\star}~\\in$ a $\\operatorname{rg\\,min}_{\\pmb{w}}\\left\\{F_{\\mu}(\\pmb{w})\\right\\}$ . Applying (26) and Lemma C.3 with $J=1$ and $\\hat{w}_{1}=w_{0}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{w})\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}2^{9}M\\mu}{T^{2}}\\log_{2}^{3}\\frac{M}{\\mu}+2\\mu^{2}\\right)\\left\\|w_{0}-w_{\\mu}^{*}\\right\\|^{2}+\\frac{3^{4}2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{M}{\\mu}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{\\mu}{2}\\left\\Vert w^{\\star}-w_{0}\\right\\Vert^{2}-\\frac{\\mu}{2}\\left\\Vert w_{\\mu}^{\\star}-w_{0}\\right\\Vert^{2}=\\left(F_{\\mu}(w^{\\star})-F(w^{\\star})\\right)+\\left(F(w_{\\mu}^{\\star})-F_{\\mu}(w_{\\mu}^{\\star})\\right)\\geqslant0,}\\end{array}$ , then $\\left\\|\\pmb{w}_{\\mu}^{\\star}-\\pmb{w}_{0}\\right\\|^{2}\\leqslant\\|\\pmb{w}^{\\star}-\\pmb{w}_{0}\\|^{2}$ . The proof is concluded. ", "page_idx": 22}, {"type": "text", "text": "D Proofs of statistical guarantees ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In order to derive statistical guarantees for the proposed method, we are going to instantiate the extension provided in the previous appendix. ", "page_idx": 23}, {"type": "text", "text": "\u03c32 = 2 s\u2208[S]1\u2212psps Pwreo hoaf voef  tThhate taanntdi $M=\\beta\\sigma^{2}$ . mS eCt.t2i.n g $\\begin{array}{r}{\\beta=\\frac{\\bar{T}}{8\\log_{2}T}}\\end{array}$ Lanedm $\\mu=\\sigma^{2}/\\beta$ ,d  eLnesumremsa  t3h.a4t $\\mu\\leqslant M$ and that $\\begin{array}{r}{T>4\\sqrt{\\frac{M}{\\mu}}\\bigg\\lfloor\\log_{2}\\frac{M}{\\mu}\\bigg\\rfloor\\,=\\,\\frac{T}{\\log_{2}T}\\,\\bigg\\lfloor\\log_{2}\\frac{T}{8\\log_{2}T}\\bigg\\rfloor,\\forall T\\geqslant2}\\end{array}$ . For $T$ larger than some large enough absolute constant, the conditions of Theorem C.2 are satisfied for the function $F$ . ", "page_idx": 23}, {"type": "text", "text": "Fairness guarantee. Theorem C.2 yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\boldsymbol{w}})\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}2^{6}\\sigma^{4}}{T^{2}}\\frac{\\log_{2}^{5}\\frac{T^{2}}{64\\log_{2}^{2}T}}{\\log_{2}^{2}T}+\\frac{2^{7}\\sigma^{4}}{T^{2}}\\log_{2}^{2}T\\right)\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{3^{4}2^{11}\\sigma^{2}}{T\\log_{2}^{2}T}\\log_{2}^{4}\\frac{T^{2}}{64\\log_{2}^{2}T}+\\frac{3^{4}2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{T^{2}}{64\\log_{2}^{2}T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we have shown that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|\\mathbf{G}_{\\alpha}(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\right\\|^{2}\\right]\\leqslant\\widetilde{\\mathcal{O}}\\left(\\frac{\\sigma^{2}}{T}\\left(1+\\frac{\\sigma^{2}}{T}\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|^{2}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the first part of Lemma 3.5 implies the fairness guarantee. ", "page_idx": 23}, {"type": "text", "text": "Fairness guarantee with AC-SA. Theorem C.3 yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left\\|G_{F,\\alpha}(\\widehat{\\boldsymbol{w}})\\right\\|^{2}\\right]\\leqslant\\left(\\frac{3^{4}2^{9}\\sigma^{4}}{T^{2}}\\log_{2}^{3}\\frac{T^{2}}{64\\log_{2}^{2}T}+\\frac{2^{7}\\sigma^{4}}{T^{2}}\\log_{2}^{2}T\\right)\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{3^{4}2^{11}\\sigma^{2}}{T}\\log_{2}^{3}\\frac{T^{2}}{64\\log_{2}^{2}T}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we have shown that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|G_{\\alpha}(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\right\\|^{2}\\right]\\leqslant\\widetilde{\\mathcal{O}}\\left(\\frac{\\sigma^{2}}{T}\\left(1+\\frac{\\sigma^{2}}{T}\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|^{2}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the first part of Lemma 3.5 implies the fairness guarantee. ", "page_idx": 23}, {"type": "text", "text": "Risk guarantee. The second part of Lemma 3.5 states that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathfrak{L}(\\pi_{\\widehat{\\Lambda},\\widehat{\\mathbf{V}}})-\\mathcal{R}(\\pi_{\\mathbf{\\Lambda}^{\\star},\\mathbf{V}^{\\star}})\\leqslant\\left(\\left\\|(\\widehat{\\Lambda},\\widehat{\\mathbf{V}})\\right\\|+\\alpha\\sigma+\\alpha\\left\\|\\varepsilon\\right\\|\\sqrt{2(2L+1)}\\right)\\cdot\\left\\|G_{\\alpha}(\\widehat{\\Lambda},\\widehat{\\mathbf{V}})\\right\\|+\\frac{\\log(2L+1)}{\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the expectation and applying the Cauchy-Schwartz inequality, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\mathcal{R}(\\pi_{\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}}})\\right]-\\mathcal{R}(\\pi_{\\mathbf{A}^{\\star},\\mathbf{V}^{\\star}})\\leqslant\\left(\\sqrt{\\mathbf{E}\\left[\\left\\|(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\right\\|^{2}\\right]}+\\alpha\\sigma+\\alpha\\left\\|\\varepsilon\\right\\|\\sqrt{2(2L+1)}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\cdot\\sqrt{\\mathbf{E}\\left[\\left\\|G_{\\alpha}(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\right\\|^{2}\\right]}+\\frac{\\log\\left(2L+1\\right)}{\\sqrt{T}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recalling the value of $\\begin{array}{r}{\\alpha=\\frac{1}{2^{J+2}\\mu_{J}}\\leqslant\\frac{1}{2M}}\\end{array}$ from Theorems C.2 \u221aand C.3 and the fact that $M=\\beta\\sigma^{2}$ , we get $\\begin{array}{r}{\\alpha\\sigma\\leqslant\\frac{1}{2\\beta\\sigma}}\\end{array}$ . Finally, applying (29)-(30) and setting $L=\\sqrt{T}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\mathcal{R}(\\pi_{\\widehat{\\Lambda}},\\widehat{\\mathbf{v}})\\right]-\\mathcal{R}(\\pi_{\\Lambda^{\\star},\\mathbf{v}^{\\star}})}\\\\ &{\\ \\ \\ \\leqslant\\widetilde{\\mathcal{O}}\\left(\\frac{\\sigma}{\\sqrt{T}}\\left(1+\\frac{\\sigma}{\\sqrt{T}}\\left\\|(\\mathbf{A}^{\\star},\\mathbf{V}^{\\star})\\right\\|\\right)\\left(\\mathbf{E}^{1/2}\\left[\\|(\\widehat{\\mathbf{A}},\\widehat{\\mathbf{V}})\\|^{2}\\right]+\\frac{1}{T\\sigma}+\\frac{\\|\\varepsilon\\|}{T^{3/4}\\sigma}\\right)+\\frac{\\log(\\sqrt{T})}{\\sqrt{T}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Above combined with Lemma B.1 yields the claimed bound. ", "page_idx": 23}, {"type": "text", "text": "E Unknown $\\eta$ and $\\tau$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we consider the case, when $\\eta$ and $\\tau$ are unknown and estimated by $\\widehat{\\eta}$ and $\\widehat{\\tau}$ . We denote by ${\\widehat{\\pmb{t}}}({\\pmb{x}})\\ {\\stackrel{\\mathrm{def}}{=}}\\ 1-\\ {\\frac{{\\widehat{\\pmb{\\tau}}}({\\pmb{x}})}{p}}$ and $\\begin{array}{r}{\\widehat{r}_{\\ell}(\\pmb{x})\\stackrel{\\mathrm{def}}{=}\\left(\\widehat{\\eta}(\\pmb{x})-\\frac{\\ell B}{L}\\right)^{2}}\\end{array}$ . We consider the plug-in versi o n of ( 5 ), defined as $\\operatorname*{min}_{\\Lambda,\\nabla\\geqslant0}\\left\\{\\mathbb{E}_{X}\\left[\\mathrm{LSE}_{\\beta}\\left(\\left(\\left\\langle\\lambda_{\\ell}-\\nu_{\\ell},\\widehat{t}(X)\\right\\rangle-\\widehat{r}_{\\ell}(X)\\right)_{\\ell=-L}^{L}\\right)\\right]+\\sum_{\\ell=-L}^{L}\\left\\langle\\lambda_{\\ell}+\\nu_{\\ell},\\,\\varepsilon\\right\\rangle\\right\\}\\,.\\,\\,\\,\\,\\ell=\\ell,\\,.$ (P LSE) ", "page_idx": 24}, {"type": "text", "text": "Let us denote by $\\widehat F$ , the objective function of the above problem and introduce ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\mathscr{R}}}_{\\beta}(\\pi)\\stackrel{\\mathrm{def}}{=}\\mathbb{E}_{\\pmb{X}}\\left[\\sum_{\\ell\\in[L]}\\widehat{r}_{\\ell}(\\pmb{X})\\pi(\\ell\\mid\\pmb{X})+\\frac{1}{\\beta}\\Psi(\\pi(\\cdot\\mid\\pmb{X}))\\right]\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The gradient of $\\widehat F$ is given for any $\\mathbf{A},\\mathbf{V}\\geqslant0$ by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\lambda\\ell_{s}}\\widehat{F}(\\mathbf{A},\\mathbf{V})=\\mathbb{E}_{X}\\left[\\sigma_{\\ell}\\left(\\beta\\left(\\left\\langle\\lambda_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\widehat{t}(X)\\right\\rangle-\\widehat{r}_{\\ell^{\\prime}}(X)\\right)_{\\ell^{\\prime}=-L}^{L}\\right)\\widehat{t}_{s}(X)\\right]+\\varepsilon_{s}\\,,}\\\\ &{\\nabla_{\\nu_{\\ell s}}\\widehat{F}(\\mathbf{A},\\mathbf{V})=-\\mathbb{E}_{X}\\left[\\sigma_{\\ell}\\left(\\beta\\left(\\left\\langle\\lambda_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\widehat{t}(X)\\right\\rangle-\\widehat{r}_{\\ell^{\\prime}}(X)\\right)_{\\ell^{\\prime}=-L}^{L}\\right)\\widehat{t}_{s}(X)\\right]+\\varepsilon_{s}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\ell\\in[L],s\\in[K]$ . Let us denote by $\\widehat{\\pmb{g}}({\\pmb{\\Lambda}},{\\bf V})$ the stochastic gradient of $\\widehat F$ , defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{g}_{\\lambda_{\\ell s}}(\\mathbf{A},\\mathbf{V})=\\sigma_{\\ell}\\left(\\beta\\left(\\left\\langle\\lambda_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\widehat{t}(\\mathbf{X})\\right\\rangle-\\widehat{r}_{\\ell^{\\prime}}(\\mathbf{X})\\right)_{\\ell^{\\prime}=-L}^{L}\\right)\\widehat{t}_{s}(\\mathbf{X})+\\varepsilon_{s}\\,,}\\\\ &{\\widehat{g}_{\\nu_{\\ell s}}(\\mathbf{A},\\mathbf{V})=-\\sigma_{\\ell}\\left(\\beta\\left(\\left\\langle\\lambda_{\\ell^{\\prime}}-\\nu_{\\ell^{\\prime}},\\widehat{t}(\\mathbf{X})\\right\\rangle-\\widehat{r}_{\\ell^{\\prime}}(\\mathbf{X})\\right)_{\\ell^{\\prime}=-L}^{L}\\right)\\widehat{t}_{s}(\\mathbf{X})+\\varepsilon_{s}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $X\\sim\\mathbb{P}_{X}$ and $\\ell\\in[L],s\\in[K]$ . We also define, by the analogy with the main body, a family of plug-in estimators ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{\\mathbf{A},\\mathbf{V}}(\\boldsymbol{\\ell}\\mid\\boldsymbol{x})\\overset{\\mathrm{def}}{=}\\sigma_{\\boldsymbol{\\ell}}\\left(\\beta\\left(\\left\\langle\\lambda_{\\boldsymbol{\\ell}}-\\nu_{\\boldsymbol{\\ell}},\\,\\widehat{t}(\\boldsymbol{x})\\right\\rangle-\\widehat{r}_{\\boldsymbol{\\ell}}(\\boldsymbol{x})\\right)_{\\boldsymbol{\\ell}=-L}^{L}\\right)\\qquad\\mathbf{A},\\mathbf{V}\\geqslant0\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Our goal is to derive analogous optimization results for the new plug-in objective. Inspecting the proofs of Lemma 4.1 and Lemma 3.4, which bound variance of and the Lipschitz constant of the gradient respectively, we observe that those proofs only depend on the nature oft via Lemma F.4. In particular, the key quantity to control is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\sigma}^{2}=2\\sum_{s\\in[K]}\\frac{\\mathbb{E}_{X}(p_{s}-\\widehat{\\tau}_{s}(X))^{2}}{p_{s}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Before, when we assumed the perfect knowledge of $\\tau$ , the above was controlled by the Bhatia-Davis inequality, leveraging the fact that variance of $\\tau_{s}(X)$ appears in the numerator. It is no longer the case here. However, if one can build calibrated estimators, that is, estimators for which $\\mathbb{E}_{X}[\\widehat{\\tau}_{s}(X)]=p_{s}$ , the same machinery is applicable. In any case, even without requiring calibrated predicti ons, one can have a reasonable control of ${\\widehat{\\sigma}}^{2}$ building sufficiently accurate estimator $\\widehat{\\tau}_{s}$ . ", "page_idx": 24}, {"type": "text", "text": "That being said, results of Lemma 4.1 and Lemma 3.4 generalize line-by-line, replacing $\\sigma^{2}$ by $\\widehat{\\sigma}^{2}$ and give ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\Lambda,\\nabla\\geqslant0}\\mathbb{E}_{X}\\left\\|\\widehat{g}_{\\Lambda,\\mathbf{V}}(X)-\\nabla_{\\Lambda,\\mathbf{V}}\\widehat{F}\\left(\\Lambda,\\mathbf{V}\\right)\\right\\|^{2}\\leqslant\\widehat{\\sigma}^{2}\\quad\\mathrm{and}\\quad\\operatorname*{sup}_{\\Lambda,\\mathbf{V}}\\|\\nabla^{2}\\widehat{F}(\\Lambda,\\mathbf{V})\\|_{\\mathrm{op}}\\leqslant\\beta\\widehat{\\sigma}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, the result of Lemma F.5 generalizes as well and gives $\\begin{array}{r}{\\operatorname*{sup}_{\\Lambda,\\mathbf{V}\\geqslant0}\\left\\|\\nabla_{\\Lambda,\\mathbf{V}}\\widehat{F}\\left(\\mathbf{\\Lambda},\\mathbf{V}\\right)\\right\\|\\leqslant\\widehat{\\sigma}.}\\end{array}$ . As in (19), we can show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigg\\|\\Big(-\\nabla\\widehat{F}(\\mathbf{A},\\mathbf{V})\\Big)_{+}\\bigg\\|\\leqslant\\bigg\\|G_{\\widehat{F},\\alpha}(\\mathbf{A},\\mathbf{V})\\bigg\\|\\qquad\\forall\\mathbf{A},\\mathbf{V}\\geqslant0\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the gradient mapping $G_{\\widehat{F},\\alpha}$ is defined by analogy with $G_{\\alpha}=G_{F,\\alpha}$ , discussed in the main body. ", "page_idx": 24}, {"type": "text", "text": "Considering the SGD3 algorithm with the same choice of parameters, but replacing $\\sigma^{2}$ by ${\\widehat{\\sigma}}^{2}$ , results in a control of $\\left\\|G_{\\widehat{F},\\alpha}(\\widehat{\\boldsymbol{\\Lambda}},\\widehat{\\mathbf{V}})\\right\\|$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 5.1. Fix $\\mathbf{A},\\mathbf{V}\\geqslant0$ . To ease the notation, we write $\\widehat{\\pi}$ to denote $\\widehat{\\pi}_{\\mathbf{A},\\mathbf{V}}$ within this proof. Similarly to the proof of (21) from Lemma 3.5, one shows that  f or all $\\mathbf{A},\\mathbf{V}\\geqslant{\\stackrel{\\cdot}{0}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{\\ell\\in[L]\\,s\\in[K]}\\big(\\big|\\mathbb{E}\\left[\\widehat{\\pi}(\\ell\\mid X)\\widehat{t_{s}}(X)\\right]\\big|-\\varepsilon_{s}\\big)_{+}^{2}}=\\|(-\\nabla\\widehat{F}(\\mathbf{A},\\mathbf{V}))_{+}\\|\\qquad\\forall\\ell\\in[L],s\\in[K]\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recalling that $\\mathcal{U}_{s}(\\widehat{\\pi},\\ell)=|\\mathbb{E}\\left[\\widehat{\\pi}(\\ell\\mid X)t_{s}(X)\\right]|$ , triangle\u2019s inequality combined with the above yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{\\ell\\in[L]\\,s\\in[K]}(\\mathcal{U}_{s}(\\widehat{\\pi},\\ell)-\\varepsilon_{s})_{+}^{2}}\\leqslant\\|(-\\nabla\\widehat{F}(\\mathbf{A},\\mathbf{V}))_{+}\\|+\\sqrt{\\sum_{\\ell\\in[L]\\,s\\in[K]}\\big\\{\\mathbb{E}[\\widehat{\\pi}(\\ell\\,|\\,X)|\\widehat{t}_{s}(X)-t_{s}(X)|]\\big\\}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Cauchy-Schwartz inequality gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\in[L]\\,s\\in[K]}\\left\\{\\mathbb{E}[\\widehat{\\pi}(\\ell\\mid X)|\\widehat{t_{s}}(X)-t_{s}(X)|]\\right\\}^{2}\\leqslant\\sum_{s\\in[K]}\\mathbb{E}\\left[\\left(\\sum_{\\ell\\in[L]}\\widehat{\\pi}(\\ell\\mid X)^{2}\\right)\\Big|\\widehat{t_{s}}(X)-t_{s}(X)\\Big|^{2}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\textstyle\\sum_{\\ell\\in[\\![L]\\!]}{\\widehat{\\pi}}(\\ell\\mid X)=1$ , then $\\begin{array}{r}{\\sum_{\\ell\\in[L]}\\widehat\\pi(\\ell\\mid X)^{2}\\leqslant1}\\end{array}$ . Thus, we have shown that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{\\ell\\in[L]\\,s\\in[K]}(\\mathcal{U}_{s}(\\widehat{\\pi}_{\\Lambda,\\mathbf{V}},\\ell)-\\varepsilon_{s})_{+}^{2}}\\leqslant\\left\\|\\,\\left(-\\nabla\\widehat{F}(\\Lambda,\\mathbf{V})\\right)_{+}\\,\\right\\|+\\mathbb{E}^{1/2}\\|\\widehat{t}(\\pmb{X})-t(\\pmb{X})\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We conclude using (34). ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 5.2. Fix $\\mathbf{A},\\mathbf{V}\\geqslant0$ . To ease the notation, we write $\\widehat{\\boldsymbol{\\pi}}\\overset{\\mathrm{def}}{=}\\widehat{\\boldsymbol{\\pi}}_{\\mathbf{A},\\mathbf{V}}$ and $\\pi^{\\star}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\pi_{\\Lambda^{\\star},\\mathbf{V}^{\\star}}$ , within this proof. ", "page_idx": 25}, {"type": "text", "text": "As in the second part of the proof of Lemma 3.5, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{R}}_{\\beta}(\\widehat{\\boldsymbol{\\pi}})+\\widehat{F}(\\mathbf{A},\\mathbf{V})\\leqslant\\left(\\|(\\mathbf{A},\\mathbf{V})\\|+\\alpha\\widehat{\\sigma}+\\alpha\\left\\|\\varepsilon\\right\\|\\sqrt{2(2L+1)}\\right)\\left\\|G_{\\widehat{F},\\alpha}(\\mathbf{A},\\mathbf{V})\\right\\|\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, since $\\|\\nabla\\mathrm{LSE}_{\\beta}(\\cdot)\\|_{1}\\equiv1$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\widehat{F}(\\mathbf{A},\\mathbf{V})-F(\\mathbf{A},\\mathbf{V})|\\leqslant\\mathbb{E}\\left[\\operatorname*{max}_{\\ell\\in[L]}\\left\\{|r_{\\ell}({\\boldsymbol{X}})-\\widehat{r}_{\\ell}({\\boldsymbol{X}})|+\\|\\mathbf{\\lambda}_{\\ell}-\\nu_{\\ell}\\|\\|t({\\boldsymbol{X}})-\\widehat{t}({\\boldsymbol{X}})\\|\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $\\begin{array}{r}{|\\widehat{\\mathcal{R}}_{\\beta}(\\widehat{\\pi})\\,-\\,\\mathcal{R}_{\\beta}(\\widehat{\\pi})|\\ \\leqslant\\ \\mathbb{E}[\\operatorname*{max}_{\\ell\\in[L]}\\{|r_{\\ell}(X)-\\widehat{r}_{\\ell}(X)|\\}]\\,}\\end{array}$ . The last two displays combined with ( 35),  gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{R}}_{\\beta}(\\widehat{\\boldsymbol{\\pi}})+F(\\mathbf{A},\\mathbf{V})\\leqslant\\mathbb{E}\\left[2\\operatorname*{max}_{\\ell\\in[L]}\\left\\{|r_{\\ell}({\\boldsymbol{X}})-\\widehat{r}_{\\ell}({\\boldsymbol{X}})|+\\|\\lambda_{\\ell}-\\nu_{\\ell}\\|\\|t({\\boldsymbol{X}})-\\widehat{t}({\\boldsymbol{X}})\\|\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\|(\\mathbf{A},\\mathbf{V})\\|+\\alpha\\widehat{\\sigma}+\\alpha\\left\\|\\varepsilon\\right\\|\\sqrt{2(2L+1)}\\right)\\left\\|G_{\\widehat{F},\\alpha}(\\mathbf{A},\\mathbf{V})\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Observe that $\\mathrm{min}_{\\Lambda,\\mathbf{V}\\geqslant0}\\,F(\\mathbf{A},\\mathbf{V})\\ =\\ -\\mathcal{R}_{\\beta}(\\pi^{\\star})$ . Using triangle\u2019s inequality and the fact that $\\begin{array}{r}{\\operatorname*{max}_{\\ell\\in[\\![L]\\!]}\\|\\lambda_{\\ell}\\!-\\!\\pmb{\\nu}_{\\ell}\\|\\leqslant\\sqrt{2}\\|(\\mathbf{A},\\mathbf{V})\\|}\\end{array}$ , we conclude recalling that $\\begin{array}{r}{\\mathcal{R}(\\pi)\\!+\\!\\frac{\\log(2L+1)}{\\beta}\\geqslant\\mathcal{R}_{\\beta}(\\pi)\\geqslant\\mathcal{R}(\\pi)}\\end{array}$ for any randomized prediction function. \u25a0 ", "page_idx": 25}, {"type": "text", "text": "F Auxilliary results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this appendix, we collect some standard auxiliary results, that are used to derive main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "Lemma F.1 (Boyd and Vandenberghe (2004)). It holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{LSE}_{\\beta}({\\pmb w})=\\operatorname*{max}_{{\\pmb p}\\in\\Delta}\\left\\{\\langle{\\pmb w},{\\pmb p}\\rangle-\\frac{1}{\\beta}\\Psi({\\pmb p})\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\Delta$ is the probability simplex in $\\mathbb{R}^{m}$ and $\\begin{array}{r}{\\Psi(\\pmb{p})=\\sum_{i=1}^{m}p_{i}\\log(p_{i})}\\end{array}$ . Furthermore, $-\\Psi(p)\\in$ $[0,\\log(m)]$ and the optimum in the above optimization problem is achieved at $\\pmb{p}^{\\star}=\\sigma(\\beta\\pmb{w})$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma F.2 (Gao and Pavel (2017)). Let $\\pmb{a}=(a_{1},\\cdot\\cdot\\cdot\\,,a_{m})$ and $\\beta>0$ . Define log-sum-exp and softmax functions respectively as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{LSE}_{\\beta}(\\pmb{a})\\stackrel{\\mathrm{def}}{=}\\frac{1}{\\beta}\\log\\left(\\sum_{i=1}^{m}\\exp(\\beta a_{i})\\right)\\,\\,a n d\\,\\sigma_{j}(\\beta\\pmb{a})\\stackrel{\\mathrm{def}}{=}\\frac{\\exp(\\beta a_{j})}{\\sum_{i=1}^{m}\\exp(\\beta a_{i})}\\quad j\\in[m]\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The LSE property is as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{a_{1},\\cdots,a_{m}\\}\\leqslant\\mathrm{LSE}_{\\beta}(a)\\leqslant\\operatorname*{max}\\{a_{1},\\cdots,a_{m}\\}+\\frac{\\log(m)}{\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, $\\sigma(\\beta\\mathbf{a})=\\nabla\\operatorname{LSE}_{\\beta}(\\mathbf{a}),$ , and $\\sigma(\\beta\\pmb{a})$ is $\\beta$ -Lipschitz. ", "page_idx": 26}, {"type": "text", "text": "Lemma F.3 (Bhatia and Davis (2000)). Let m and $M$ be the lower and upper bounds, respectively, for a set of real numbers $a_{1},\\cdot\\cdot\\cdot\\,,a_{n}$ , with a particular probability distribution. Let $\\mu$ and $\\sigma^{2}$ be respectively the expected value and the variance of this distribution. Then the Bhatia\u2013Davis inequality states: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sigma^{2}\\leqslant(M-\\mu)(\\mu-m)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma F.4. It holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}\\left[\\sum_{s\\in[K]}t_{s}^{2}(X)\\right]\\leqslant\\sum_{s\\in[K]}\\frac{1-p_{s}}{p_{s}}\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ts(x) = 1 \u2212\u03c4s(x).", "page_idx": 26}, {"type": "text", "text": "Proof. We have $\\mathbb{E}_{X}[\\tau_{s}(X)]=p_{s}$ and $0\\leqslant\\tau_{s}(X)\\leqslant1$ almost surely. Using Bhatia-Davis inequality written in Lemma F.3, we deduce that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}\\left[\\sum_{s\\in[K]}t_{s}^{2}(X)\\right]=\\sum_{s\\in[K]}\\mathrm{Var}\\left(\\frac{\\tau_{s}(X)}{p_{s}}\\right)=\\sum_{s\\in[K]}\\frac{1}{p_{s}^{2}}\\,\\mathrm{Var}\\left(\\tau_{s}(X)\\right)\\leqslant\\sum_{s\\in[K]}\\frac{1-p_{s}}{p_{s}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof is concluded. ", "page_idx": 26}, {"type": "text", "text": "Lemma F.5. Let \u03c32 d=ef 2 s\u2208[K]1\u2212pp s . It holds that $\\|\\nabla_{\\mathbf{A},\\mathbf{V}}F(\\mathbf{A},\\mathbf{V})\\|\\leqslant\\sigma+{\\sqrt{2(2L+1)}}\\,\\|\\varepsilon\\|.$ ", "page_idx": 26}, {"type": "text", "text": "Proof. By Jensen\u2019s inequality ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{\\mathbf{A},\\mathbf{V}}F(\\mathbf{A},\\mathbf{V})\\right\\|^{2}=\\left\\|\\mathbb{E}_{X}[g_{\\mathbf{A},\\mathbf{V}}(X)]\\right\\|^{2}\\leqslant\\mathbb{E}_{X}[\\|g_{\\mathbf{A},\\mathbf{V}}(X)\\|^{2}]\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recalling the definition of $g_{\\mathbf{A},\\mathbf{V}}$ , given in (11), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}[\\|g_{\\Lambda,\\mathbf{V}}(X)\\|^{2}]=\\mathbb{E}_{X}\\displaystyle\\sum_{\\ell\\in[L]\\,s\\in[K]}\\left((\\sigma_{\\ell}(\\cdot)t_{s}(X)+\\varepsilon_{s})^{2}+(-\\sigma_{\\ell}(\\cdot)t_{s}(X)+\\varepsilon_{s})^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad=2\\mathbb{E}_{X}\\displaystyle\\sum_{\\ell\\in[L]\\,s\\in[K]}\\left(\\sigma_{\\ell}^{2}(\\cdot)t_{s}^{2}(X)+\\varepsilon_{s}^{2}\\right)\\leqslant\\sigma^{2}+2(2L+1)\\left\\|\\varepsilon\\right\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality comes from the proof of Lemma 4.1. ", "page_idx": 26}, {"type": "text", "text": "The proof is concluded. ", "page_idx": 26}, {"type": "text", "text": "G Additional details on experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Evaluation measures. We use $\\mathcal{D}_{\\mathrm{test}}=\\{(\\mathbf{\\boldsymbol{x}}_{i}^{\\prime},\\boldsymbol{s}_{i}^{\\prime},\\boldsymbol{y}_{i}^{\\prime})\\}_{i=1}^{m}$ to collect the following statistics of any (randomized) prediction $\\pi$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{\\mathcal{R}}(\\pi)\\overset{\\mathrm{def}}{=}\\frac{1}{m}\\sum_{i=1}^{m}\\int_{-\\infty}^{+\\infty}\\left(\\widehat{y}-y_{i}^{\\prime}\\right)^{2}\\pi(\\mathrm{d}\\,\\widehat{y}\\mid x_{i}^{\\prime})\\,,}\\\\ {\\displaystyle\\widehat{U}_{s}(\\pi)\\overset{\\mathrm{def}}{=}\\operatorname*{sup}_{t\\in\\mathbb{R}}\\left|\\frac{1}{m_{s}}\\sum_{i=1}^{m}\\int_{-\\infty}^{t}\\pi(\\mathrm{d}\\,\\widehat{y}\\mid x_{i}^{\\prime})\\mathbb{I}\\{s_{i}^{\\prime}=s\\}-\\frac{1}{m}\\sum_{i=1}^{m}\\int_{-\\infty}^{t}\\pi(\\mathrm{d}\\,\\widehat{y}\\mid x_{i}^{\\prime})\\right|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which correspond to the empirical risk and the empirical group-wise unfairness quantified by the Kolmogorov-Smirnov distance of a randomized. We note that our classifier is supported on a finite grid, thus all the integrals involved transform into weighted sums. ", "page_idx": 27}, {"type": "text", "text": "Agarwal et al. (2019) build multi-class classifiers $h_{k}:\\mathbb{R}^{d}\\mapsto\\Theta$ , where $\\Theta$ is some finite grid over $\\mathbb{R}$ and $k=1,\\ldots,K$ , that come with weights $(w_{1},\\hdots,w_{K})^{\\top}$ such that $w_{k}\\geqslant0$ and $\\textstyle\\sum_{k=1}^{K}w_{k}=1$ Then, they build a randomized classifier $\\pi(\\cdot\\mid\\cdot)$ such that $\\operatorname{supp}(\\pi(\\cdot\\mid x))=\\Theta$ and fo r each $\\theta\\in\\Theta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\widehat{Y}_{\\pi}=\\theta\\mid X=x)=\\sum_{k=1}^{K}w_{k}\\mathbb{I}\\{h_{k}({\\pmb x})=\\theta\\}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, integrals appearing inR and $\\widehat{U}_{s}$ reduced to finite sums for both methods. ", "page_idx": 27}, {"type": "text", "text": "Additional details on the experiments on Communities and Crime and Law School datasets. Communities and Crime dataset has 1994 instances, however we use 1984 examples with 120 features after preprocessing. Law School dataset has 20649 instances, thus we use a smaller sub-sample of 2000 points with 11 features after preprocessing. ", "page_idx": 27}, {"type": "text", "text": "We take the sets of unfairness thresholds $\\{(2^{-i},2^{-i})_{i\\in\\mathbb{Z}}\\}$ , where $\\begin{array}{r l r l}{{\\mathcal{T}}}&{{}}&{}&{{}=}\\end{array}$ $\\{1,2,4,5,6,8,16,32,128,512\\}$ for Communities and Crime dataset, and $\\begin{array}{r l r l}{{\\mathcal{T}}}&{{}}&{=}\\end{array}$ $\\{0,1,2,4,5,6,8,16,32,64,128\\}$ for Law School dataset. We train Communities and Crime dataset for $\\scriptstyle\\mathrm{N=}15000$ iterations and Law School dataset for $\\scriptstyle\\mathrm{N}=5000$ iterations for each pair of epsilons. We use parameters $L=\\sqrt{T}$ , $\\beta=\\sqrt{T}\\log{\\sqrt{T}}$ and $B=1$ for both datasets. We repeat the aforementioned pipeline 10 times to ensure more reliable statistical summary. ", "page_idx": 27}, {"type": "text", "text": "Discussion on other algorithms. We conduct additional experiments to observe the behaviors of the more straightforward algorithms discussed in Appendix C. We illustrate the comparison in Figure 3. In conclusion, all of the algorithms perform similarly in the middle to high unfairness regime, while those based on SGD3 are more stable in the low unfairness (high fairness) regime. ", "page_idx": 27}, {"type": "image", "img_path": "UtbjD5LGnC/tmp/a6293f85e8292d3574c5814da016a8c5de7a939135d5f8f67f63d6987f3d0cb8.jpg", "img_caption": ["Figure 3: Comparison of SDG, ACSA, ACSA2, $\\mathtt{S D G3+A C S A}$ and $\\mathtt{S D G3+A C S A2}$ algorithms on Communitites and Crime and Law School datasets. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Additional experiments on Adult dataset. We conduct further experiments on Adult dataset (Lichman (2013)). Classically, Adult is used for classification, however we use it to predict individual\u2019s age on a scale of 0 to 100, normalized to [0, 1]. We factor in sex as a sensitive attribute, distinguishing between male and female individuals. Adult dataset has 48842 instances, however we clean and preprocess it, and use a smaller sub-sample of 2000 points with 8 features throughout our experiments. ", "page_idx": 27}, {"type": "text", "text": "The pipleline of the experiments is the same as the one for Law School and Communities and Crime datasets in the main body. We randomly split the data into training, unlabeled and testing sets with proportions of $0.4\\times0.4\\times0.2$ . We use $\\mathcal{D}_{\\mathrm{train}}\\,=\\,\\{(\\pmb{x}_{i},s_{i},y_{i})_{i=1}^{n}\\}$ to train a base (unfair) regressor to estimate $\\eta$ and to train a classifier to estimate $\\tau$ . We use simple LinearRegression and LogisticRegression from scikit-learn for training the regressor and the classifier, and give them to Algorithm 1 with ${\\cal D}_{\\mathrm{unlabeled}}\\,=\\,({\\pmb x})_{i=n+1}^{n+T}$ for $N\\,=\\,10000$ iterations. We use $\\ensuremath{\\mathcal{D}_{\\mathrm{test}}}=$ $\\{(\\pmb{x}_{i}^{\\prime},s_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m}$ to collect statistics. We take the sets of unfairness thresholds $\\{(2^{-i},2^{-i})_{i\\in\\mathbb{Z}}\\}$ , where\u221a $\\mathcal{T}=\\{0,\\bar{1},2,4,5,6,8,16,32,64,128\\}$ for. As in the experiments in the main body, we set $L=\\sqrt{T},\\beta=\\sqrt{T}/\\log\\sqrt{T}$ and $B=1$ . We repeat the pipeline 10 times. ", "page_idx": 27}, {"type": "table", "img_path": "UtbjD5LGnC/tmp/bdde30c96a3f0c4eb6a9ea9c68628f8af1c07cdaf12d48a09d466b54c446bd9d.jpg", "table_caption": [], "table_footnote": ["Table 1: The average training time (in seconds) for one $\\varepsilon$ threshold. "], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "We compare our method with the ADW method. We train ADW 2 times: we use $\\mathcal{D}_{\\mathrm{train}}$ and $\\mathcal{D}_{\\mathrm{unlabeled}}$ as training set for ADW-1, whereas for ADW-2 we use only $\\mathcal{D}_{\\mathrm{train}}$ . We take the set $\\{(2^{-i},2^{-i})_{i\\in\\mathbb{Z}}\\}$ , where $\\mathcal{T}=\\{1,2,4,8,16\\}$ as unfairness thresholds for training both datasets. We train ADW-1 and ADW-2 for each pair of epsilons for 10 times. ", "page_idx": 28}, {"type": "text", "text": "In Figure 4 we illustrate the convergence of the risk and the unfairness for convergence for $\\varepsilon=$ $(2^{-8},{2^{-8}})$ unfairness threshold. We also illustrate the comparison of our model with ADW-1, ADW-2 and base models. ", "page_idx": 28}, {"type": "image", "img_path": "UtbjD5LGnC/tmp/7dbc6488a5fcd2cdd26344ea01327b133725505f424dc074f0ab2ae1617246cb.jpg", "img_caption": ["Figure 4: Experiment on Adult dataset: risk convergence, unfairness convergence and comparison with ADW. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Running time. Additional details about training time for Communities and Crime, Law School and Adult datasets are presented in Table 1. ", "page_idx": 28}, {"type": "text", "text": "Additional experiments on a synthetic dataset. We conduct an additional experiment to demonstrate the results of Algorithm 1 in the case of multiple sensitive attributes. We generate a synthetic dataset $\\mathcal{D}_{n}=(X_{i},S_{i},\\mathbf{\\bar{{y}}}_{i})_{i=1}^{n}$ of $n=2000$ points, where $(\\mathbf{X}_{i})_{i}^{n}=(X_{i1},X_{i2},\\bar{X_{i2}})_{i}^{n}\\sim\\mathcal{N}(\\bar{0_{,}}1)$ . We choose $S_{i}=0$ if $X_{i1}\\leqslant-0.7$ , $S_{i}=1$ if $X_{i1}<0$ , $S_{i}=2$ if $X_{i1}<0.7$ and $S_{i}=4$ if $X_{i1}\\geqslant-0.7$ . For $i\\in[n]$ , we generate $\\begin{array}{r}{y_{i}=4\\sum_{j=1}^{3}X_{i j}+X_{i1}+\\xi_{i}}\\end{array}$ , where $\\pmb{\\xi}=(\\xi_{i})_{i}^{n}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{1})$ . We split $\\mathcal{D}_{n}$ into train, unlabeled and test datasets with proportions of $0.4\\times0.4\\times0.2$ . We use $(X_{t r a i n},y_{t r a i n})$ to train the base estimator, $(X_{t r a i n},S_{t r a i n})$ to train the classifier and $X_{u n l a b}$ to train the fair regression model. We evaluate our model on test dataset. In Figure 5 we illustrate the distributions of the predictions (scaled to $[-1,1])$ ) of the fair and base models. ", "page_idx": 28}, {"type": "text", "text": "This experiment is for visual representation of the Algorithm 1 in the case of multiple sensitive attributes, thus we do not collect further statistics. ", "page_idx": 28}, {"type": "image", "img_path": "UtbjD5LGnC/tmp/aa97c7392f060d2ec59f81d247565645e7f04180236ef9651a21cda80a7018e0.jpg", "img_caption": ["Figure 5: The distributions of the (scaled) predictions of the fair and base models. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We translate the fairness problem to a smooth and convex problem as stated in Lemma 3.1. Theorem 5.1 gives the main theoretical control on fairness and risk and Section 6 illustrates the performance of the method and a comparison to a state-of-the-art method. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: A limitation section is included in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We extensively developed our methodology in the main body and decided to postpone all the proofs to the Appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the experiments are reproducible. We provide the link to the source code in Section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All the datasets that are used are benchmark datasets. Moreover, as previously mentioned, we point to a GitHub link in Section 6 to make our experiments reproducible. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Section 6 gives all necessary information on the datasets splits and on the tuning parameters. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All our experiments include and illustrate the standard deviations reported over 10 repetitions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We reported all this information in a footnote in Section 6. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We only used open source data and codes. Therefore, we believe that there are no issues form the ethical point of view. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The contribution falls within the scope of fairness and the goal is to mitigate bias in decision making. The paper focus on the general notion of approximate fairness \u2014 $\\varepsilon$ -fairness. While approximate fairness is desired in general, this setting allows for a control of the amount of unfairness that we allow or accept. From this point of view, providing the control on the fairness to some user may generate ethical issues. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the datasets are open source and widely used by the fairness community. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provided references for the two datasets we are using. In addition, we also referred the papers that we considered for a numerical comparison. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The main contribution is a novel approach for imposing demographic parity fairness in the unawareness case. This is challenging problem for which we spend the whole core of the paper to explain the method and its new considerations. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper considered public data that does not involve human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]