[{"figure_path": "lTUXlmjcva/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of how the non-linearity of a given neural network is measured. (Top) The non-linearity signature of a DNN is a collection of affinity scores calculated for each activation function spread across its hidden layers. (Bottom) The affinity score is calculated based on 3 main steps. First, given an input (grey) and an output (red) of an activation function (left), we estimate the best affine OT fit Taff (X) (green) transporting the input to the output (middle-left). Second, we measure the mismatch between the two by summing the transportation costs (middle-right) to obtain the Wasserstein distance W2(Taff X, Y). Finally, this distance is normalized with the magnitudes of variance (arrows in the rightmost plot) of the output data based on its covariance matrix.", "description": "This figure illustrates the process of measuring the non-linearity of a deep neural network (DNN).  The top part shows the overall architecture, highlighting how the non-linearity signature is composed of affinity scores for each activation function across the network's layers. The bottom part details the calculation of a single affinity score, using three steps:\n\n1. Finding the best affine Optimal Transport (OT) fit between the input and output of an activation function.\n2. Measuring the mismatch using the Wasserstein distance between the true output and the OT fit.\n3. Normalizing this distance by the output's variance.  This normalized distance represents the affinity score, quantifying the activation function's non-linearity.", "section": "Non-linearity signature of deep neural networks"}, {"figure_path": "lTUXlmjcva/figures/figures_4_1.jpg", "caption": "Figure 2: (A) Non-linearity of ReLU depends on the range of input values (red); (B) ReLU, Tanh, and Sigmoid exhibit different degrees of non-linearity for the same input; (C) Affinity score captures the increasing non-linearity of polynomials of different degrees.", "description": "This figure demonstrates how the affinity score, a measure of non-linearity, varies across different activation functions and input ranges. Panel (A) shows that ReLU's non-linearity depends on the range of its input values. Panel (B) illustrates that ReLU, Tanh, and Sigmoid have different non-linearity degrees for the same input range.  Panel (C) displays how the affinity score increases as the degree of a polynomial activation function increases, indicating a correlation between higher polynomial orders and higher non-linearity.", "section": "Non-linearity signature"}, {"figure_path": "lTUXlmjcva/figures/figures_6_1.jpg", "caption": "Figure 3: Median, minimum, and maximum values of non-linearity signatures of the different architectures spanning a decade (2012\u20132022) of computer vision research. We observe a clear trend toward the increase of the spread and the maximum values of the linearity in neural networks lasting until the arrival of transformers in 2020. ViTs have a distinct pattern of maximizing the non-linearity of their activation functions. Swin transformers and Convnext models retain this property from them while remaining close to the pure convolutional networks.", "description": "This figure shows the median, minimum, and maximum values of non-linearity signatures for various computer vision models developed between 2012 and 2022.  It illustrates how the non-linearity of these models changed over time, highlighting a trend toward increased non-linearity culminating with the introduction of Vision Transformers (ViTs) in 2020.  ViTs demonstrate a distinct pattern of high non-linearity, which is somewhat maintained in their successors, Swin Transformers and ConvNeXt models.", "section": "Experimental evaluations"}, {"figure_path": "lTUXlmjcva/figures/figures_6_2.jpg", "caption": "Figure 4: Best found dependency between the different statistics extracted from the non-linearity signatures of the DNN families and their respective Imagenet-1K accuracy. The results are compared in terms of the R2 score against the most precise of the other common DNN characteristics such as depth, size, and the GFLOPS.", "description": "This figure displays the correlation between different metrics derived from the non-linearity signatures of various Deep Neural Networks (DNNs) and their ImageNet-1K accuracy.  It compares the R-squared (R\u00b2) values of these correlations against those obtained using more traditional DNN characteristics like depth, number of parameters, and GFLOPS. The goal is to determine which metric extracted from the non-linearity signature best predicts accuracy and how this compares to the predictive power of traditional metrics.", "section": "4 Experimental evaluations"}, {"figure_path": "lTUXlmjcva/figures/figures_7_1.jpg", "caption": "Figure 5: Comparing the different families of the neural architectures based on their non-linearity signatures. (A) Hierarchical clustering of all DNNs considered in our study revealing meaningful clusters with close architectural characteristics; (B) 9 representative architectures from all studied families and the similarities between them. Note how the similarities between early convnets and other models is decreasing with time until computer vision priors are introduced into Swin transformers in 2021; (C) Distributions of affinity scores in each network. Most models expand the non-linearity ranges of their activation functions compared to early convnets. ViTs are dominated by highly non-linear activation functions, Resnets have a bimodal distribution, Densenets, and EfficientNets have a diametrically skewed distribution compared to ViTs. (D) Comparing the same convnet with 20 layers when trained with (Residual Resnet20) and without (Plain Resnet20) residual connections (top row). Residual connections introduce a clear trend toward a bimodal distribution of affinity scores; the same effect is observed for Resnet18 and Resnet34 (bottom row).", "description": "This figure compares different families of neural network architectures based on their non-linearity signatures. It uses hierarchical clustering to group similar architectures, shows the evolution of similarities over time, presents distributions of affinity scores, and illustrates the effect of residual connections on non-linearity distributions.", "section": "Closer look at accuracy/non-linearity trade-off"}, {"figure_path": "lTUXlmjcva/figures/figures_15_1.jpg", "caption": "Figure 6: Median affinity scores of Sigmoid, ReLU, GELU, ReLU6, LeakyReLU with a default value of slope, Tanh, HardTanh, SiLU, and HardSwish obtained across random draws from Gaussian distribution with a sliding mean and varying stds used as their input. Whiskers of boxplots show the whole range of values obtained for each mean across all stds. The baseline value is the affinity score obtained for a sample covering the whole interval. The ranges and extreme values of each activation function over its subdomain are indicative of its non-linearity limits.", "description": "This figure shows the median affinity scores for various activation functions (sigmoid, ReLU, GELU, ReLU6, LeakyReLU, Tanh, Hardtanh, SiLU, HardSwish).  The affinity score measures the non-linearity of the activation function.  The boxplots show the distribution of affinity scores obtained using different input data with varying standard deviations. Whiskers show the full range of scores. The baseline represents the affinity score obtained over the entire input range. The figure illustrates how the range and extreme values of each activation function's scores reflect its non-linearity characteristics.  It highlights how the non-linearity of an activation function depends on the input range and function shape.", "section": "3.2 Non-linearity signature"}, {"figure_path": "lTUXlmjcva/figures/figures_17_1.jpg", "caption": "Figure 7: (Top left) Affinity score is robust to the dimensionality reduction both when using averaging and summation over the spatial dimensions; (Top right) When d > n, sample covariance matrix estimation leads to a lack of robustness in the estimation of the affinity score; (Bottom) Shrinkage of the covariance matrix leads to constant values of the affinity scores with increasing d.", "description": "This figure demonstrates the robustness of the affinity score calculation to dimensionality reduction techniques. The top-left panel shows that averaging or summing spatial dimensions yields consistent results.  The top-right panel highlights a loss of robustness when the dimensionality (d) exceeds the number of samples (n) in covariance matrix estimation. Finally, the bottom panel illustrates that applying shrinkage to the covariance matrix ensures stable affinity score values across varying dimensions.", "section": "E Implementation details"}, {"figure_path": "lTUXlmjcva/figures/figures_18_1.jpg", "caption": "Figure 8: Non-linearity signature of VGG16 on CIFAR10 with a varying batch size (left) and when retrained from 9 different random seeds (right).", "description": "This figure shows the robustness of the non-linearity signature of VGG16 trained on CIFAR10 against variations in the batch size used during training (left panel) and when training the model from nine different random seeds (right panel).  The x-axis represents the depth of the network. The y-axis represents the affinity score, a measure of non-linearity. The plots demonstrate that the non-linearity signature is relatively stable across different batch sizes and random seeds, indicating that the proposed method is robust to these variations.", "section": "Experimental evaluations"}, {"figure_path": "lTUXlmjcva/figures/figures_18_2.jpg", "caption": "Figure 9: Non-linearity signatures of VGG16 on CIFAR10 in the beginning and end of training on Imagenet.", "description": "This figure shows the non-linearity signatures of a VGG16 model at the beginning and end of training on the Imagenet dataset. The non-linearity signature at initialization is nearly linear in the last layers, but training introduces non-linearity in a non-monotonic way.  The figure highlights that the non-linearity signature captures information from the training process.", "section": "Impact of training"}, {"figure_path": "lTUXlmjcva/figures/figures_19_1.jpg", "caption": "Figure 10: Raw non-linearity signatures of popular DNN architectures, plotted as affinity scores over the depth throughout the network.", "description": "The figure shows the raw non-linearity signatures for various popular deep neural network architectures. The affinity score, a measure of non-linearity, is plotted against the depth of the network for each architecture.  Different activation functions are represented by different colors, and the standard deviation of affinity scores is indicated in the title of each subplot. The figure provides a visual representation of how non-linearity varies across different architectures and depths.", "section": "3.2 Non-linearity signature"}, {"figure_path": "lTUXlmjcva/figures/figures_19_2.jpg", "caption": "Figure 11: ViTs: Large ViT with 16x16 and 32x32 patch sizes and Huge ViT.", "description": "This figure displays the raw non-linearity signatures of three Vision Transformer (ViT) models: Large ViT with 16x16 patches, Large ViT with 32x32 patches, and Huge ViT.  The y-axis represents the affinity score (a measure of non-linearity), and the x-axis represents the depth of the network.  The plots show how the non-linearity changes across different depths within each model.  The purpose is to demonstrate the varying non-linearity patterns in different ViT models, potentially related to their architecture and performance.", "section": "F Raw signatures"}, {"figure_path": "lTUXlmjcva/figures/figures_20_1.jpg", "caption": "Figure 12: Impact of depth on the non-linearity signature of VGGs.", "description": "Impact of depth on the non-linearity signature of VGG networks (VGG11, VGG13, VGG16, and VGG19).  The figure shows that although the networks' depth varies significantly, the non-linearity signature remains largely consistent across different depths.", "section": "3.2 Non-linearity signature"}, {"figure_path": "lTUXlmjcva/figures/figures_20_2.jpg", "caption": "Figure 13: Impact of depth on the non-linearity signature of Resnets.", "description": "This figure shows the impact of depth on the non-linearity signature of ResNet architectures.  It plots the affinity score (a measure of non-linearity) against depth for five different ResNet models (ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152).  The plots show that the non-linearity in ResNets changes with the increase in depth, with some showing increases, decreases, or remaining relatively consistent. The differences in non-linearity patterns across various ResNets highlight the impact of architectural choices and depth on the non-linearity of the models.", "section": "3.2 Non-linearity signature"}, {"figure_path": "lTUXlmjcva/figures/figures_22_1.jpg", "caption": "Figure 5: Comparing the different families of the neural architectures based on their non-linearity signatures. (A) Hierarchical clustering of all DNNs considered in our study revealing meaningful clusters with close architectural characteristics; (B) 9 representative architectures from all studied families and the similarities between them. Note how the similarities between early convnets and other models is decreasing with time until computer vision priors are introduced into Swin transformers in 2021; (C) Distributions of affinity scores in each network. Most models expand the non-linearity ranges of their activation functions compared to early convnets. ViTs are dominated by highly non-linear activation functions, Resnets have a bimodal distribution, Densenets, and EfficientNets have a diametrically skewed distribution compared to ViTs. (D) Comparing the same convnet with 20 layers when trained with (Residual Resnet20) and without (Plain Resnet20) residual connections (top row). Residual connections introduce a clear trend toward a bimodal distribution of affinity scores; the same effect is observed for Resnet18 and Resnet34 (bottom row).", "description": "This figure compares different families of neural network architectures based on their non-linearity signatures.  Panel A shows a hierarchical clustering of the networks, revealing groups with similar characteristics. Panel B illustrates the similarities between nine representative architectures, highlighting how the relationships between early convolutional networks and other models change over time, particularly with the introduction of Swin transformers. Panel C displays the distributions of affinity scores for each network, showing how the range of non-linearity increases for most models compared to earlier convnets, with Vision Transformers (ViTs) exhibiting the highest non-linearity. Panel D compares a 20-layer convolutional neural network trained with and without residual connections, demonstrating the effect of these connections on the distribution of affinity scores.", "section": "Comparing the different families of the neural architectures based on their non-linearity signatures"}, {"figure_path": "lTUXlmjcva/figures/figures_22_2.jpg", "caption": "Figure 3: Median, minimum, and maximum values of non-linearity signatures of the different architectures spanning a decade (2012\u20132022) of computer vision research. We observe a clear trend toward the increase of the spread and the maximum values of the linearity in neural networks lasting until the arrival of transformers in 2020. ViTs have a distinct pattern of maximizing the non-linearity of their activation functions. Swin transformers and Convnext models retain this property from them while remaining close to the pure convolutional networks.", "description": "This figure shows the median, minimum, and maximum values of the non-linearity signatures of various deep neural network architectures used in computer vision research over the past decade (2012-2022).  It illustrates the evolution of the non-linearity in these models, highlighting a general trend towards increased spread and maximum non-linearity values until the introduction of transformers in 2020.  Vision Transformers (ViTs) show a unique pattern of maximizing non-linearity, while Swin Transformers and ConvNext models maintain this characteristic but remain closer in behavior to traditional convolutional networks.", "section": "Experimental evaluations"}, {"figure_path": "lTUXlmjcva/figures/figures_23_1.jpg", "caption": "Figure 16: Hierarchical clustering of supervised and self-supervised pre-trained Resnet50 using the DTW distances between their non-linearity signatures.", "description": "This figure shows the hierarchical clustering of different versions of Resnet50 models, trained with various self-supervised and supervised methods. The dendrogram is generated using the dynamic time warping (DTW) distances between the non-linearity signatures of the models.  The proximity of the models in the dendrogram illustrates the similarity of their non-linearity characteristics.  This helps understand how different training methods affect the non-linearity patterns within a consistent model architecture.", "section": "I Results for self-supervised methods"}, {"figure_path": "lTUXlmjcva/figures/figures_24_1.jpg", "caption": "Figure 1: Illustration of how the non-linearity of a given neural network is measured. (Top) The non-linearity signature of a DNN is a collection of affinity scores calculated for each activation function spread across its hidden layers. (Bottom) The affinity score is calculated based on 3 main steps. First, given an input (grey) and an output (red) of an activation function (left), we estimate the best affine OT fit Taff (X) (green) transporting the input to the output (middle-left). Second, we measure the mismatch between the two by summing the transportation costs (middle-right) to obtain the Wasserstein distance W2(Taff X, Y). Finally, this distance is normalized with the magnitudes of variance (arrows in the rightmost plot) of the output data based on its covariance matrix.", "description": "This figure illustrates the method used in the paper to measure the non-linearity of a deep neural network.  The top part shows the overall process of calculating the \"non-linearity signature\", which involves calculating affinity scores for each activation function across the network's layers. The bottom part details the three steps involved in calculating a single affinity score: 1) finding the optimal transport (OT) map between the input and output of an activation function, 2) measuring the Wasserstein distance between the output and the best affine approximation of the output using the OT map, and 3) normalizing this distance using the output's covariance matrix. ", "section": "Non-linearity signature of deep neural networks"}]