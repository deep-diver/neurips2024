[{"type": "text", "text": "From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural Networks with Affine Optimal Transport ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In the last decade, we have witnessed the introduction of several novel deep   \n2 neural network (DNN) architectures exhibiting ever-increasing performance across   \n3 diverse tasks. Explaining the upward trend of their performance, however, remains   \n4 difficult as different DNN architectures of comparable depth and width \u2013 common   \n5 factors associated with their expressive power \u2013 may exhibit a drastically different   \n6 performance even when trained on the same dataset. In this paper, we introduce   \n7 the concept of the non-linearity signature of DNN, the first theoretically sound   \n8 solution for approximately measuring the non-linearity of deep neural networks.   \n9 Built upon a score derived from closed-form optimal transport mappings, this   \n0 signature provides a better understanding of the inner workings of a wide range   \n11 of DNN architectures and learning paradigms, with a particular emphasis on the   \n2 computer vision task. We provide extensive experimental results that highlight the   \n13 practical usefulness of the proposed non-linearity signature and its potential for   \n14 long-reaching implications. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Deep neural networks (DNNs) are undoubtedly the most powerful AI models currently available   \n17 [1, 2, 3, 4, 5]. Their performance on many tasks, including natural language processing (NLP) [6]   \n18 and computer vision [7], is already on par or exceeds that of a human being. One of the reasons   \n19 explaining such progress is of course the increasing computational resources [8, 9]. Another one is   \n20 the endeavour for finding ever more efficient neural architectures pursued by researchers over the   \n21 last decade. As of today, the transformer architecture [10] has firmly imposed itself as a number   \n22 one choice for most, if not all, of the recent breakthroughs [11, 12, 13] in the machine learning and   \n23 artificial intelligence fields.   \n25 Limitations But why transformers are more capable than other architectures? Answering this   \n26 question requires finding a meaningful measure to compare the different famous models over   \n27 time gauging the trend of their intrinsic capacity. For such a comparison to be informative, it is   \n28 particularly appropriate to consider the computer vision field that produced many of the landmark   \n29 neural architectures improving upon each other over the years. Indeed, the decade-long revival of   \n30 deep learning started with Alexnet\u2019s [14] architecture, the winner of the ImageNet Large Scale Visual   \n31 Recognition Challenge [15] in 2012. By achieving a significant improvement over the traditional   \n32 approaches, Alexnet was the first truly deep neural network to be trained on a dataset of such   \n33 scale, suggesting that deeper models were likely to bring even more gains. In the following years,   \n34 researchers proposed novel ways to train deeper models with hundreds of layers [16, 17, 18, 19]   \n35 pushing the performance frontier even further. The AI research landscape then reached a turning   \n36 point with the proposal of transformers [10], starting their unprecedented dominance first in NLP and   \n37 then in computer vision [20]. Surprisingly, transformers are not particularly deep, and the size of   \n38 their landmark vision architecture is comparable to that of Alexnet, and this despite a significant   \n39 performance gap between the two. Ultimately, this gap should be explained by the differences in the   \n40 expressive power [21] of the two models: a term used to denote the ability of a DNN to approximate   \n41 functions of a certain complexity. Unfortunately, the existing theoretical results related to this either   \n42 associate higher expressive power with depth [22, 23, 24] or width [25, 26, 27, 28] falling short in   \n43 comparing different families of architectures. This, in turn, limits our ability to understand what   \n44 underpins the achieved progress and what challenges and limitations still exist in the field, guiding   \n45 future research efforts.   \n47 Contributions We argue that quantifying the non-linearity of a DNN may be what we were missing   \n48 so far to understand the evolution of the deep learning models at a more fine-grained level. To verify   \n49 this hypothesis in practice, we put forward the following contributions:   \n50 1. We propose a first theoretically sound measure, called the affinity score, that estimates the   \n51 non-linearity of a given (activation) function using optimal transport (OT) theory. We use   \n52 the proposed affinity score to introduce the concept of the non-linearity signature of DNNs   \n53 defined as a set of affinity scores of all its activation functions.   \n54 2. We compare non-linearity signatures of a wide range of popular DNNs used in computer   \n55 vision: from Alexnet to vision transformers (ViT) and their more recent variations. Through   \n56 this, we clearly illustrate the disruptive patterns in the evolution of the deep learning field.   \n57 3. We demonstrate that non-linearity signature can be predictive of DNNs performance and   \n58 used to meaningfully identify the family of approaches to which a given DNN belongs. We   \n59 further show that the non-linearity signature is unique as it doesn\u2019t correlate strongly with   \n60 other potential candidates used for this task.   \n61 The rest of the paper is organized as follows. We start by presenting the relevant background   \n62 knowledge on OT in Section 2. Then, we introduce the affinity score together with its different   \n63 theoretical properties in Section 3. Section 4 presents experimental evaluations on a wide range of   \n64 popular convolutional neural networks. Finally, we conclude in Section 5. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "65 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "66 Optimal Transport Let $(X,d)$ be a metric space equipped with a lower semi-continuous cost   \n67 function $c:X\\times X\\to\\mathbb{R}_{\\geq0}$ , e.g the Euclidean distance $c(\\bar{x},y)=\\|x-y\\|$ . Then, the Kantorovich   \n68 formulation of the OT problem between two probability measures $\\mu,\\nu\\in{\\mathcal{P}}(X)$ is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{OT}_{c}(\\mu,\\nu)=\\operatorname*{min}_{\\gamma\\in\\mathrm{ADM}(\\mu,\\nu)}\\mathbb{E}_{\\gamma}[c],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "69 where $\\mathrm{ADM}(\\mu,\\nu)$ is the set of joint probabilities with marginals $\\mu$ and $\\nu$ , and $\\mathbb{E}_{\\nu}[f]$ denotes the   \n70 expected value of $f$ under $\\nu$ . The optimal $\\gamma$ minimizing equation 1 is called the OT plan. Denote by   \n71 ${\\mathcal{L}}(X)$ the law of a random variable $X$ . Then, the OT problem extends to random variables $X,Y$ and   \n72 we write $\\mathrm{OT}_{c}(X,Y)$ meaning $\\mathrm{OT}_{c}(\\mathcal{L}(X),\\mathcal{L}(Y))$ .   \n73 Assuming that either of the considered measures is absolutely continuous, then the Kantorovich   \n74 problem is equivalent to the Monge problem ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathrm{OT}}_{c}(\\mu,\\nu)=\\operatorname*{min}_{T:T_{\\#}\\mu=\\nu}\\mathbb{E}_{X\\sim\\mu}[c(X,T(X))],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "75 where the unique minimizing $T$ is called the OT map, and $T_{\\#}\\mu$ denotes the push-forward measure,   \n76 which is equivalent to the law of $T(X)$ , where $X\\sim\\mu$ .   \n77 Wasserstein distance Let $X$ be a random variable over $\\mathbb{R}^{d}$ satisfying $\\mathbb{E}[\\|X-x_{0}\\|^{2}]<\\infty$ for some   \n78 $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{d}$ , and thus for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . We denote this class of random variables by $\\mathcal{P}_{2}(\\mathbb{R}^{d})$ . Then, the   \n79 2-Wasserstein distance $W_{2}$ between $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ is defined as ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nW_{2}(X,Y)=\\mathrm{OT}_{||x-y||^{2}}(X,Y)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "80 We now proceed to the presentation of our main contribution. ", "page_idx": 1}, {"type": "text", "text": "81 3 Non-linearity signature of deep neural networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "82 Among all non-linear operations introduced into DNNs in the last several decades, activation functions   \n83 remain the only structural piece that they all inevitably share. Without non-linear activation functions,   \n84 most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.   \n85 Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow   \n86 network capable of approximating any function, however complex it may be, to arbitrary precision.   \n87 We thus build our study on the following intuition: if activation functions play in important role   \n88 in making DNNs non-linear, then measuring their degree of non-linearity can provide us with an   \n89 approximation of the DNN\u2019s non-linearity itself. To implement this intuition in practice, however, we   \n90 first need to find a way to measure the non-linearity of an activation function. Surprisingly, there is   \n91 no widely accepted measure for this, neither in the field of mathematics nor in the field of computer   \n92 science. To fill this gap, we will use the OT theory to develop a so-called affinity score below. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "93 3.1 Affinity score ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 Identifiability We consider the pre-activation signal $X$ of an activation function within a neural   \n95 network, and the post-activation signal $\\sigma(X)$ denoted by $Y$ as input and output random variables.   \n96 Our first step to build the affinity score then is to ensure that we can identify when $\\sigma$ is linear with   \n97 respect to (wrt) $X$ (for instance, when an otherwise non-linear activation is locally linear at the   \n98 support of $X$ ). To show that such an identifiability condition can be satisfied with OT, we first recall   \n99 the following classic result from the literature characterizing the OT maps.   \n100 Theorem 3.1 ([33]). Let $X\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ , $T(x)=\\nabla\\phi(x)$ for a convex function $\\phi$ with $T(X)\\in{\\mathcal{P}}_{2}(\\mathbb{R}^{d})$ .   \n101 Then, $T$ is the unique optimal OT map between $\\mu$ and $T_{\\#}\\mu$ .   \n102 Using this theorem about the uniqueness of OT maps expressed as gradients of convex functions, we   \n103 can prove the following result (all proofs can be found in the Appendix C):   \n104 Corollary 3.2. Without loss of generality, let $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be centered, and let $Y=\\sigma(X)=T X$ ,   \n105 where $T$ is a positive definite linear transformation. Then, $T$ is the OT map from $X$ to $Y$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "106 Whenever the activation function $\\sigma$ is linear, the solution to the OT problem $T$ exactly reproduces it. ", "page_idx": 2}, {"type": "text", "text": "107 Characterization We now seek to understand whether $T$ can be characterized more explicitly. For   \n108 this, we prove the following theorem stating that $T$ can be computed in closed-form using the normal   \n109 approximations of $X$ and $Y$ .   \n110 Theorem 3.3. Let $X,Y\\in{\\mathcal{P}}_{2}(\\mathbb{R}^{d})$ be centered and $Y=T X$ for a positive definite matrix $T$ . Let   \n111 ${\\cal N}_{X}\\sim\\mathcal{N}(\\mu(X),\\Sigma(X))$ and $\\dot{N_{Y}}\\sim\\mathcal{N}(\\mu(Y),\\Sigma(Y))$ be their normal approximations where $\\mu$ and   \n112 $\\Sigma$ denote mean and covariance, respectively. Then, $\\dot{W_{2}}(N_{X},N_{Y})=W_{2}(X,Y)$ and $T=T_{\\mathrm{aff}}$ , where   \n113 $T_{\\mathrm{aff}}$ is the OT map between $N_{X}$ and $N_{Y}$ and can be calculated in closed-form ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{\\mathrm{aff}}(x)=A x+b,\\,}&{A=\\Sigma(Y)^{\\frac{1}{2}}\\left(\\Sigma(Y)^{\\frac{1}{2}}\\Sigma(X)\\Sigma(Y)^{\\frac{1}{2}}\\right)^{-\\frac{1}{2}}\\Sigma(Y)^{\\frac{1}{2}},}\\\\ &{b=\\mu(Y)-A\\mu(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 Upper bound When the activation $\\sigma$ is non-linear wrt $X$ , the affine OT mapping $T_{\\mathrm{aff}}(X)$ will   \n115 deviate from the true activation outputs $Y$ . One important step toward quantifying this deviation is   \n116 given by the famous Gelbrich bound, formalized by means of the following theorem:   \n117 Theorem 3.4 (Gelbrich bound [34]). Let $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and let $N_{X},N_{Y}$ be their normal approxi  \n118 mations. Then, $W_{2}(N_{X},N_{Y})\\leq W_{2}(X,Y)$ .   \n119 This upper bound provides a first intuition of why OT can be a great tool for measuring non-linearity:   \n120 the cost of the affine map solving the OT problem on the left-hand side increases when the map   \n121 becomes non-linear. We now upper bound the difference between $W_{2}(N_{X},N_{Y})$ and $W_{2}(X,Y)$ , two   \n122 quantities that coincide only when $\\sigma$ is linear. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "123 Proposition 3.5. Let $X,Y\\in{\\mathcal{P}}_{2}(\\mathbb{R}^{d})$ and $N_{X},N_{Y}$ be their normal approximations. Then, ", "page_idx": 2}, {"type": "text", "text": "124 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.~\\left|W_{2}(N_{X},N_{Y})-W_{2}(X,Y)\\right|\\leq\\frac{2\\operatorname{Tr}\\left[(\\Sigma(X)\\Sigma(Y))^{\\frac{1}{2}}\\right]}{\\sqrt{\\operatorname{Tr}[\\Sigma(X)]+\\operatorname{Tr}[\\Sigma(Y)]}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "lTUXlmjcva/tmp/9304c263d5de0674c0c2095c6181e9d3f268f6454e0b66b9cbb772781db511ad.jpg", "img_caption": ["Figure 1: Illustration of how the non-linearity of a given neural network is measured. (Top) The non-linearity signature of a DNN is a collection of affinity scores calculated for each activation function spread across its hidden layers. (Bottom) The affinity score is calculated based on 3 main steps. First, given an input (grey) and an output (red) of an activation function (left), we estimate the best affine OT fit $T_{\\mathrm{aff}}(X)$ (green) transporting the input to the output (middle-left). Second, we measure the mismatch between the two by summing the transportation costs (middle-right) to obtain the Wasserstein distance $W_{2}(T_{\\mathrm{aff}}X,Y)$ . Finally, this distance is normalized with the magnitudes of variance (arrows in the rightmost plot) of the output data based on its covariance matrix. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "125 ", "page_idx": 3}, {"type": "text", "text": "126 To have a more informative non-linearity measure, we now need to normalize the non-negative Wasser  \n127 stein distance $W_{2}(T_{\\mathrm{aff}}X,Y)$ to an interpretable interval of $[0,1]$ . The bound given in Proposition 3.5   \n128 lets us define the following affinity score ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{aff}}(X,\\sigma(X))=1-\\frac{W_{2}(T_{\\mathrm{aff}}X,\\sigma(X))}{\\sqrt{2\\,\\mathrm{Tr}[\\Sigma(\\sigma(X))]}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 The proposed affinity score quantifies how far a given activation $\\sigma$ is from an affine transformation.   \n130 It is equal to 1 for any input for which the activation function is linear, and 0 when it is maximally   \n131 non-linear, i.e., when $T_{\\mathrm{aff}}X$ and $\\sigma(X)$ are independent random variables.   \n132 Remark 3.6. One may wonder whether a simpler alternative to the affinity score can be to use,   \n133 instead of $T_{\\mathrm{aff}}$ , a mapping $T_{W}(x)\\,=\\,W x$ defined as a solution of a linear regression problem   \n134 minW $||Y-W X||_{F}^{2}$ . Then, one can use the coefficient of determination $R^{2}$ score) to measure how   \n135 well $T_{W}$ fits the observed data. This approach, however, has two drawbacks. First, following the   \n136 famous Gauss-Markov theorem, $T_{W}$ is an optimal linear (linear in $Y$ ) estimator. On the contrary, $T_{\\mathrm{aff}}$   \n137 is a globally optimal non-linear mapping aligning $X$ and $Y$ . Second, $R^{2}$ compares the fti of $T_{W}$ with   \n138 that of a mapping outputting $\\mu(Y)$ for any value of $X$ . This is contrary to $\\rho_{\\mathrm{aff}}$ that compares how   \n139 well $T_{\\mathrm{aff}}$ ftis the data wrt to the worst possible cost incurred by $T_{\\mathrm{aff}}$ as quantified in Proposition 3.5.   \n140 This gives us a bounded score, i.e. $\\rho_{\\mathrm{aff}}\\in[0,1]$ , whereas $R^{2}$ is not lower bounded, i.e. $R^{\\hat{2}}\\in[-\\infty,1]$ .   \n141 We confirm experimentally in Section $^{4}$ that the two coefficients do not correlate consistently across   \n142 the studied DNNs suggesting that $R^{2}$ is a poor proxy to $\\rho_{\\mathrm{aff}}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "lTUXlmjcva/tmp/1e9602b70bd1e65f04c63eb23014fbd8ca56b83bb9a239f30e1dbb063c65153a.jpg", "img_caption": ["Figure 2: (A) Non-linearity of ReLU depends on the range of input values $(r e d)$ ; (B) ReLU, Tanh, and Sigmoid exhibit different degrees of non-linearity for the same input; (C) Affinity score captures the increasing non-linearity of polynomials of different degrees. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "143 3.2 Non-linearity signature ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "144 We now turn our attention to the definition of a non-linearity signature of deep neural networks. We   \n145 define a neural network $\\mathbf{N}$ as a composition of layers $F_{i}$ where each layer $F_{i}$ is a function taking   \n146 as input a tensor $\\mathbf{X}_{i}\\,\\in\\,\\mathbb{R}^{h_{i}\\times w_{i}\\times c_{i}}$ (for instance, an image of size $224\\times224\\times3$ for $i=1$ ) and   \n147 outputting a tensor $\\mathrm{Y}_{i}\\in\\mathbb{R}^{h_{i+1}\\times w_{i+1}\\times c_{i+1}}$ used as an input of the following layer $F_{i+1}$ . This defines   \n148 $\\mathbf{N}=F_{L}\\odot...\\odot F_{i}\\dots\\odot F_{1}=\\odot_{k=1,...,L}F_{k}$ where $\\odot$ stands for a composition.   \n149 We now present the definition of a non-linearity signature of a network $\\mathbf{N}$ . Below, we abuse the   \n150 compositional structure of $F_{i}$ and see it as an ordered sequence of functions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. Let $N=\\odot_{k=1,...,L}F_{k}$ be a neural network. Define by $\\boldsymbol{\\mathcal{A}}$ a finite set of common activation functions such that $\\begin{array}{r}{\\mathcal{A}:=\\{\\sigma|\\sigma:\\mathbb{R}^{h\\times w\\times c}\\rightarrow\\mathbb{R}^{h\\times w\\times c}\\}}\\end{array}$ . Let r be a pooling operation such that $r:\\mathbb{R}^{h\\times w\\times c}\\rightarrow\\mathbb{R}^{c}$ . Then, the non-linearity signature of N given an input $\\mathrm{X}$ is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{aff}}(N;\\mathrm{X})=\\{\\rho_{\\mathrm{aff}}(r(\\mathrm{X}_{i}),\\sigma(r(\\mathrm{X}_{i}))),\\quad\\forall\\sigma\\in F_{i}\\cap\\mathcal{A},\\quad i=\\{1,\\dots,L\\}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "151 Non-linearity signature, illustrated in Figure 1, associates to each network $\\mathbf{N}$ a vector of affinity   \n152 scores calculated over the inputs and outputs of all activation functions encountered across its layers.   \n153   \n154 What makes an activation function non-linear? We now want to understand the mechanism   \n155 behind achieving a lower or higher non-linearity with a given (activation) function. This will   \n156 explain what the different values of the affinity scores stand for when defining the non-linearity   \n157 signature of a DNN. In Figure 2(A), we show how the ReLU function [35], defined element-wise as   \n158 $\\operatorname{ReLU}(x)=\\operatorname*{max}(0,x)$ , achieves its varying degree of non-linearity. Interestingly, this degree depends   \n159 only on the range of the input values. Second, in Figure 2(B) we also show how the shape of activation   \n160 functions impacts their non-linearity for a fixed input: surprisingly, piece-wise linear ReLU function   \n161 is more non-linear than $\\mathrm{Sigmoid}(\\dot{x})\\,=\\,1/(e^{-x}\\:\\dot{+}\\:1)$ [36] or $\\bar{\\mathrm{Tanh}(x)}\\,=\\,(e^{-x}\\,-\\,e^{x})/(e^{-x}\\,+\\,e^{x})$ .   \n162 Similar observations also apply to compare polynomials of varying degrees (Figure 2(C)). We refer   \n163 the reader to Appendix $\\mathrm{D}$ for more visualizations of the affinity score of popular activation functions. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "164 3.3 Related work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 Layer-wise similarity analysis of DNNs A line of work that can be distantly related to our main   \n166 proposal is that of quantifying the similarity of the hidden layers of the DNNs as proposed [37] and   \n167 [38] (see [39] for a complete survey of the subsequent works). [37] extracts activation patterns of   \n168 the hidden layers in the DNNs and use CCA on the singular vectors extracted from them to measure   \n169 how similar the two layers are. Their analysis brings many interesting insights regarding the learning   \n170 dynamics of the different convnets, although they do not discuss the non-linearity propagation in the   \n171 convnets, nor do they propose a way to measure it. [38] proposed to use a normalized Frobenius   \n172 inner product between kernel matrices calculated on the extracted activations of the hidden layers   \n173 and argued that such a similarity measure is more meaningful than that proposed by [37].   \n174 Impact of activation functions [40] provides the most comprehensive survey on the activation   \n175 functions used in DNNs. Their work briefly discusses the non-linearity of the different activation   \n176 functions suggesting that piecewise linear activation functions with more linear components are more   \n177 non-linear (e.g., ReLU vs. ReLU6). [41] show theoretically that smooth versions of ReLU allow   \n178 for more efficient information propagation in DNNs with a positive impact on their performance.   \n179 Our work provides a first extensive comparison of all popular activation functions; we also show that   \n180 smooth version of ReLU exhibit wider regions of high non-linearity (see Appendix D).   \n181 Non-linearity measure The only work similar to ours in spirit is the paper by [42] proposing the   \n182 non-linearity coefficient in order to predict the train and test error of DNNs. Their coefficient is   \n183 defined as a square root of the Jacobian of the neural network calculated wrt its input, multiplied by   \n184 the covariance matrix of the Jacobian, and normalized by the covariance matrix of the input. The   \n185 presence of the Jacobian in it calls for the differentiability assumption making its application to   \n186 most of the neural networks with ReLU non-linearity impossible as is. The authors didn\u2019t provide   \n187 any implementation of their coefficient and we were not able to find any other study reporting the   \n188 reproduced results from this work. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "189 4 Experimental evaluations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "190 We consider computer vision models trained and evaluated on the same Imagenet dataset with 1,000   \n191 output categories (Imagenet-1K) publicly available at [43]. The non-linearity signatures of different   \n192 studied models presented in the paper is calculated by passing batches of size 512 through the   \n193 pre-trained models for the entirety of the Imagenet-1K validation set (see Appendix H for more   \n194 datasets) with a total of 50,000 images. We include the following landmark architectures in our study:   \n195 Alexnet [14], four VGG models [16], Googlenet [44], Inception v3 [17], five Resnet models [18],   \n196 four Densenet models [19], four MNASNet models [45], four EfficientNet models [46], five ViT   \n197 models, three Swin transformer [47] and four Convnext models [48]. We include MNASNet and   \n198 EfficientNet models as prominent representatives of the neural architecture search approach [49].   \n199 Such models are expected to explicitly maximize the accuracy for a given computational budget.   \n200 Swin transformer and Convnext models are introduced as ViTs with traditional computer vision   \n201 priors. Their presence will be useful to better grasp how such priors impact ViTs. We refer the reader   \n202 to Appendix E for more practical details.   \n203 History of deep vision models at a glance We give a general outlook of the developments in   \n204 computer vision over the last decade when seen through the lens of their non-linearity. In Figure 3   \n205 we present the minimum, median, and maximum values of the affinity scores calculated for the   \n206 considered neural networks (see Appendix F for raw non-linearity signatures). We immediately   \n207 see that until the arrival of transformers, the trend of the landmark models was to decrease their   \n208 non-linearity, rather than to increase it. On a more fine-grained level, we note that pure convolution   \n209 architectures such as Alexnet (2012) and VGGs (2014) exhibit a very low spread of the affinity   \n210 score values. This trend changes with the arrival of the inception module first used in Googlenet   \n211 (2014): the latter includes activation functions that extend the range of the non-linearity on both   \n212 ends of the spectrum. Importantly, we can see that the trend toward increasing the maximum and   \n213 average non-linearity of the neural networks has continued for almost the whole decade. Even more   \n214 surprisingly, EfficientNet models (2019), trained through neural architecture search, have strong   \n215 negative skewness toward higher linearity, although they were state-of-the-art in their time. The   \n216 second surprising finding comes with the arrival of ViTs (2020): they break the trend and leverage   \n217 the non-linearity of their hidden activation functions becoming more or more non-linear with the   \n218 varying size of the patches (see Appendix F for a more detailed comparison with raw signatures).   \n219 This trend remains valid also for Swin transformers (2021), although introducing the computer vision   \n220 priors into them makes their non-linearity signature look more similar to pure convolutional networks   \n221 from the early 2010s, such as Alexnet and VGGs. Finally, we observe that the non-linearity signature   \n222 of a modern Convnext architecture (2022), designed as a convnet for 2020s using the best practices   \n223 of Swin transformers, further confirms this observation.   \n224 Closer look at accuracy/non-linearity trade-off Different families of vision models leverage differ  \n225 ent characteristics of their internal non-linearity to achieve better performance. To better understand   \n226 this phenomenon, we now turn our attention to a more detailed analysis of the accuracy/non-linearity   \n227 trade-off by looking for a statistic extracted from their non-linearity signatures that is the most predic  \n228 tive of their accuracy as measured by the $R^{2}$ score. Additionally, we also want to understand whether   \n229 the non-linearity of DNNs can explain their performance better than the traditional characteristics   \n230 such as the number of parameters, the number of giga floating point operations per second (GFLOPS),   \n231 and the depth. From the results presented in Figure 4, we observe the following. First, the information   \n232 extracted from the non-linearity signatures often correlates more with the final accuracy, than the   \n233 usual DNN characteristics. This is the case for Residual networks (ResNets and DenseNets), ViTs,   \n234 and vision models influenced by transformers (Post-ViT). Unsurprisingly, for models based on neural   \n235 architecture search (NAS-based, i.e. EfficientNets and MNASNets) the number of parameters is   \n236 the most informative metric as they are specifically designed to reach the highest accuracy with the   \n237 increasing model size and compute. For Pre-residual pure convolutional models (Alexnet, VGGs,   \n238 Googlenet, and Inception), the spread of the non-linearity explains the accuracy increase similarly to   \n239 depth. Second, we observe that all models preceding ViTs were implicitly optimizing the spread of   \n240 their affinity score values to achieve better performance. After the arrival of the transformers, the   \n241 observed trend is to increase either the median or the minimum values of the non-linearity. This   \n242 suggests a fundamental shift in the implicit bias that the transformers carry.   \n243 Distinct signature for every architecture Non-linearity signature correctly identifies the different   \n244 families of neural architectures. To show this, we perform hierarchical clustering using pairwise   \n245 dynamic time warping (DTW) distances [50] between the non-linearity signatures of the models from   \n246 Figure 3. The results in Figure 5 (A), as well as the pairwise distance matrix between a representative   \n247 of each studied family in Figure 5 (B) (see Appendix G for the full matrix), show that we correctly   \n248 cluster all similar models together, both within their respective families (such as the different   \n249 variations of the same architecture) and across them (such as the cluster of Swin and pure convolution   \n250 models). Additionally, we highlight the individual affinity scores\u2019 distributions of representative   \n251 models in Figure 5 (C). Finally, we highlight the exact effect of residual connections proposed in   \n252 2016 and used ever since by every benchmark model in Figure 5 (D). It reveals vividly that residual   \n253 connections make the distribution of the affinity scores bimodal with one such mode centered around   \n254 highly linear activation functions. This confirms in a principled way that residual connections indeed   \n255 tend to enable the learning of the identity function just as suggested in the seminal work that proposed   \n256 them [18]. Non-linearity signatures can also be applied to meaningfully identify training methods,   \n257 such as popular nowadays self-supervised approaches, for a fixed architecture (see Appendix I).   \n259 Uniqueness of the affinity score No other metric extracted from the activation functions of the   \n260 considered networks exhibits a strong consistent correlation with the non-linearity signature. To   \n261 validate this claim, we compare in Table 1 the Pearson correlation between the non-linearity signature   \n262 and several other metrics comparing the inputs and the outputs of the activation functions. We can see   \n263 that for different models the non-linearity correlates with different metrics suggesting that it captures   \n264 the information that other metrics fail to capture consistently across all architectures. This becomes   \n265 even more apparent when analyzing the individual correlation values (in Appendix G). Overall, the   \n266 proposed affinity score and the non-linearity signatures derived from it offer a unique perspective on   \n267 the developments in the ML field. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "lTUXlmjcva/tmp/c97a36e966defec50c6259ad8555f94da3f3ea7ba069a0bbafd966e1b7cebbda.jpg", "img_caption": ["Figure 3: Median, minimum, and maximum values of non-linearity signatures of the different architectures spanning a decade (2012-2022) of computer vision research. We observe a clear trend toward the increase of the spread and the maximum values of the linearity in neural networks lasting until the arrival of transformers in 2020. ViTs have a distinct pattern of maximizing the non-linearity of their activation functions. Swin transformers and Convnext models retain this property from them while remaining close to the pure convolutional networks. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "lTUXlmjcva/tmp/a9269f756fc173ea15284df5400d8c2f4e53bdf3b8c6c2d33127fe19755ea59a.jpg", "img_caption": ["Figure 4: Best found dependency between the different statistics extracted from the non-linearity signatures of the DNN families and their respective Imagenet-1K accuracy. The results are compared in terms of the $R^{2}$ score against the most precise of the other common DNN characteristics such as depth, size, and the GFLOPS. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "lTUXlmjcva/tmp/cb668d14f706b4318c81767ad2193c7bd6299edccf60bc30b0248c0132ff075d.jpg", "img_caption": ["Figure 5: Comparing the different families of the neural architectures based on their non-linearity signatures. (A) Hierarchical clustering of all DNNs considered in our study revealing meaningful clusters with close architectural characteristics; (B) 9 representative architectures from all studied families and the similarities between them. Note how the similarities between early convnets and other models is decreasing with time until computer vision priors are introduced into Swin transformers in 2021; (C) Distributions of affinity scores in each network. Most models expand the non-linearity ranges of their activation functions compared to early convnets. ViTs are dominated by highly non-linear activation functions, Resnets have a bimodal distribution, Densenets, and EfficientNets have a diametrically skewed distribution compared to ViTs. (D) Comparing the same convnet with 20 layers when trained with (Residual Resnet20) and without (Plain Resnet20) residual connections (top row). Residual connections introduce a clear trend toward a bimodal distribution of affinity scores; the same effect is observed for Resnet18 and Resnet34 (bottom row). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "lTUXlmjcva/tmp/d019114c2decbbd8e6d396be96302cd8b91056876f7b762ac3de5edfce1164c2.jpg", "table_caption": ["Table 1: Pearson correlations between the non-linearity signature and other metrics, for all the architectures evaluated in this study. The highest absolute value in each group is reported in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "268 5 Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "269 We proposed the first sound approach to measure non-linearity of activation functions in neural   \n270 networks and defined their non-linearity signature based on it. We further used non-linearity signatures   \n271 to provide a meaningful overview of the evolution of neural architectures proposed over the last   \n272 decade with clear interpretable patterns. We showed that until the arrival of transformers, the trend in   \n273 DNNs was to decrease their non-linearity, rather than to increase it. Vision transformers changed   \n274 this pattern drastically. We also showcased that our measure is unique, as no other metric correlates   \n275 strongly with it across all architectures.   \n276 In the future, our work can be applied to study the non-linearity of the LLM models to better under  \n277 stand the effect of different architectural choices in them. On a higher level, our approach can also be   \n278 used to identify new disruptive neural architectures by identifying those of them that leverage different   \n279 internal non-linearity characteristics to obtain better performance. This capacity of identifying novel   \n280 technologies is even more crucial in the age of very large models where experimenting with the   \n281 building blocks of the optimized backbone comes at a very high cost. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "282 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "283 [1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444,   \n284 2015.   \n285 [2] J\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks,   \n286 61:85\u2013117, 2015.   \n287 [3] Michael I Jordan and Tom M Mitchell. Machine learning: Trends, perspectives, and prospects.   \n288 Science, 349(6245):255\u2013260, 2015.   \n289 [4] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Adaptive computation and machine   \n290 learning. MIT Press, 2016.   \n291 [5] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud A.A. Setio, Francesco Ciompi,   \n292 Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, and Clara I. S\u00e1nchez.   \n293 A survey on deep learning in medical image analysis. Medical image analysis, 42:60\u201388, 2017.   \n294 [6] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Wei Chen. Deberta: Decoding-enhanced   \n295 bert with disentangled attention. In Proceedings of the International Conference on Learning   \n296 Representations, 2021.   \n297 [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:   \n298 Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE   \n299 International Conference on Computer Vision, page 1026\u20131034, 2015.   \n300 [8] OpenAI. Ai and compute. 2018. Accessed: March 13, 2024.   \n301 [9] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for   \n302 deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.   \n303 [10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n304 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa  \n305 tion Processing Systems, pages 5998\u20136008, 2017.   \n306 [11] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,   \n307 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n308 few-shot learners. arXiv preprint arXiv:2005.14165, 2020.   \n309 [12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n310 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,   \n311 Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation   \n312 language models, 2023.   \n313 [13] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n314 [14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep   \n315 convolutional neural networks. Advances in neural information processing systems, 25:1097\u2013   \n316 1105, 2012.   \n317 [15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng   \n318 Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.   \n319 ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision,   \n320 115(3):211\u2013252, 2015.   \n321 [16] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale   \n322 image recognition. In International Conference on Learning Representations, 2015.   \n323 [17] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Arman Alemi. Rethinking the   \n324 inception architecture for computer vision. arXiv preprint arXiv:1512.00567, 2016.   \n325 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n326 recognition. arXiv preprint arXiv:1512.03385, 2016.   \n327 [19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected   \n328 convolutional networks. arXiv preprint arXiv:1608.06993, 2017.   \n329 [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,   \n330 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,   \n331 Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image   \n332 recognition at scale. In ICLR, 2021.   \n333 [21] Ingo G\u00fchring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks.   \n334 arXiv:2007.04759, 2020.   \n335 [22] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th   \n336 Annual Conference on Learning Theory, pages 907\u2013940, 2016.   \n337 [23] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with   \n338 neural networks. In Proceedings of the 34th International Conference on Machine Learning,   \n339 pages 2979\u20132987, 2017.   \n340 [24] Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc  \n341 dimension and pseudodimension bounds for piecewise linear neural networks. Journal of   \n342 Machine Learning Research, 20(63):1\u201317, 2019.   \n343 [25] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the   \n344 expressive power of deep neural networks. In Proceedings of the International Conference on   \n345 Machine Learning, pages 2847\u20132854, 2017.   \n346 [26] Guido Mont\u00fafar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of   \n347 linear regions of deep neural networks. In NeurIPS, pages 2924\u20132932, 2014.   \n348 [27] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power   \n349 of neural networks: a view from the width. In Advances in Neural Information Processing   \n350 Systems, page 6232\u20136240, 2017.   \n351 [28] Gal Vardi, Gilad Yehudai, and Ohad Shamir. On the optimal memorization power of relu neural   \n352 networks. In The Tenth International Conference on Learning Representations, ICLR, 2022.   \n353 [29] Kurt Hornik. Multilayer feedforward networks are universal approximators. Neural Networks,   \n354 2(5):359\u2013366, 1989.   \n355 [30] Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Mach.   \n356 Learn., 14(1):115\u2013133, 1994.   \n357 [31] Kurt and Hornik. Approximation capabilities of multilayer feedforward networks. Neural   \n358 Networks, 4(2):251\u2013257, 1991.   \n359 [32] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,   \n360 Signals, and Systems (MCSS), 2(4):303\u2013314, 1989.   \n361 [33] Cyril S Smith and Martin Knott. Note on the optimal transportation of distributions. Journal of   \n362 Optimization Theory and Applications, 52(2):323\u2013329, 1987.   \n363 [34] Matthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean   \n364 and hilbert spaces. Mathematische Nachrichten, 147(1):185\u2013203, 1990.   \n365 [35] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann   \n366 machines. In Proceedings of the International Conference on Machine Learning, pages 807\u2013   \n367 814, 2010.   \n368 [36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating   \n369 errors. Nature, 323(6088):533\u2013536, 1986.   \n370 [37] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular   \n371 vector canonical correlation analysis for deep learning dynamics and interpretability. In NIPS\u201917,   \n372 page 6078\u20136087, 2017.   \n373 [38] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural   \n374 network representations revisited. In ICML, volume 97, pages 3519\u20133529. PMLR, 09\u201315 Jun   \n375 2019.   \n376 [39] MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and Eugene   \n377 Belilovsky. Reliability of CKA as a similarity measure in deep learning. In ICLR, 2023.   \n378 [40] Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in   \n379 deep learning: A comprehensive survey and benchmark. Neurocomput., 503(C):92\u2013108, 2022.   \n380 [41] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function   \n381 on deep neural networks training. In Proceedings of the 36th International Conference on   \n382 Machine Learning, pages 2672\u20132680, 2019.   \n383 [42] George Philipp. The nonlinearity coefficient - A practical guide to neural architecture design.   \n384 CoRR, abs/2105.12210, 2021.   \n385 [43] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library.   \n386 GitHub repository, 2016.   \n387 [44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,   \n388 Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.   \n389 arXiv preprint arXiv:1409.4842, 2014.   \n390 [45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and   \n391 Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of   \n392 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.   \n393 [46] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural   \n394 networks. In Proceedings of the International Conference on Machine Learning, pages 6105\u2013   \n395 6114, 2019.   \n396 [47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining   \n397 Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings   \n398 of the IEEE/CVF International Conference on Computer Vision, 2021.   \n399 [48] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining   \n400 Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision   \n401 and Pattern Recognition, 2022.   \n402 [49] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.   \n403 Journal of Machine Learning Research, 20(55):1\u201321, 2019.   \n404 [50] Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word   \n405 recognition. IEEE transactions on acoustics, speech, and signal processing, 26(1):43\u201349, 1978.   \n406 [51] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In   \n407 Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk, editors, Proceedings of the Fourteenth   \n408 International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings   \n409 of Machine Learning Research, pages 315\u2013323, Fort Lauderdale, FL, USA, 11\u201313 Apr 2011.   \n410 PMLR.   \n411 [52] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint   \n412 arXiv:1606.08415, 2016.   \n413 [53] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei Wang, Wenhan Weng,   \n414 Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for   \n415 mobile vision applications. In Proceedings of the 2017 IEEE Conference on Computer Vision   \n416 and Pattern Recognition, pages 4200\u20134210. IEEE, 2017.   \n417 [54] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural   \n418 network acoustic models. In Proceedings of the ICML Workshop on Deep Learning for Audio,   \n419 Speech and Language Processing, 2013.   \n420 [55] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network   \n421 function approximation in reinforcement learning. Neural networks, 107:3\u201311, 2018.   \n422 [56] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,   \n423 Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3.   \n424 In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314\u20131324,   \n425 2019.   \n426 [57] Olivier Ledoit and Michael Wolf. Honey, i shrunk the sample covariance matrix. Journal of   \n427 Portfolio Management, 30(4):110\u2013119, 2004.   \n428 [58] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.   \n429 Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural   \n430 information processing systems, 33:9912\u20139924, 2020.   \n431 [59] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,   \n432 and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings   \n433 of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n434 [60] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for   \n435 unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on   \n436 computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n437 [61] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British   \n438 Machine Vision Conference 2016. British Machine Vision Association, 2016.   \n439 [62] Mert B\u00fclent Sar\u0131y\u0131ld\u0131z, Yannis Kalantidis, Karteek Alahari, and Diane Larlus. No reason for   \n440 no supervision: Improved generalization in supervised models. In The Eleventh International   \n441 Conference on Learning Representations, 2023.   \n442 [63] Julien Denize, Jaonary Rabarisoa, Astrid Orcesi, Romain H\u00e9rault, and St\u00e9phane Canu. Similarity   \n443 contrastive estimation for self-supervised soft contrastive learning. In Proceedings of the   \n444 IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2706\u20132716, 2023.   \n445 [64] Guangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving   \n446 inefficiency of self-supervised representation learning. In Proceedings of the IEEE/CVF   \n447 International Conference on Computer Vision, pages 9505\u20139515, 2021.   \n448 [65] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and   \n449 Chang Xu. Ressl: Relational self-supervised learning with weak augmentation. Advances in   \n450 Neural Information Processing Systems, 34:2543\u20132555, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "451 A Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "452 This paper presents work whose goal is to advance the field of Machine Learning and better understand   \n453 the underlying behavior of Deep Neural Networks architectures. There are many potential societal   \n454 consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 13}, {"type": "text", "text": "455 B Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "456 An important assumption of Theorem 3.3, is that the activation function that we want to analyze   \n457 through $\\rho_{\\mathrm{aff}}$ needs to be a positive definite transformation of the inputs. Fortunately, this is the case for   \n458 activation functions, that we consider in this paper. Finally, we note that despite the strong correlation   \n459 between the statistics extracted from the non-linearity signatures for certain DNNs\u2019 architectures,   \n460 we are yet to show that explicitly optimizing affinity scores through backpropagation can have an   \n461 actionable impact on DNNs performance or its other properties, such as robustness or transferability. ", "page_idx": 13}, {"type": "text", "text": "462 C Proofs of main theoretical results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "463 In this section, we provide proofs of the main theoretical results from the paper. ", "page_idx": 13}, {"type": "text", "text": "464 Corollary 3.2. Without loss of generality, let $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be centered, and such that $Y=T X$ ,   \n465 where $T$ is a positive semi-definite linear transformation. Then, $T$ is the OT map from $X$ to $Y$ .   \n466 Proof. We first proof that we can consider centered distributions without loss of generality. To this   \n467 end, we note that ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{2}^{2}(X,Y)=W_{2}^{2}(X-\\mathbb{E}[X],Y-\\mathbb{E}[Y])+\\|\\mathbb{E}[X]-\\mathbb{E}[Y]\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "468 implying that splitting the 2-Wasserstein distance into two independent terms concerning the $L^{2}$   \n469 distance between the means and the 2-Wasserstein distance between the centered measures. ", "page_idx": 13}, {"type": "text", "text": "470 Furthermore, if we have an OT map $T^{\\prime}$ between $X-\\mathbb{E}[X]$ and $Y-\\mathbb{E}[Y]$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\nT(x)=T^{\\prime}(x-\\mathbb{E}[X])+\\mathbb{E}[Y],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "471 is the OT map between $X$ and $Y$ ", "page_idx": 13}, {"type": "text", "text": "472 To prove the statement of the Corollary, we now need to apply Theorem 3.1 to the convex $\\phi(x)=$   \n473 $x^{T}\\dot{T}x$ , where $T$ is positive semi-definite. \u53e3   \n474 Theorem 3.3. Let $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ be centered and $Y=T X$ for a positive definite matrix $T$ . Let   \n475 ${\\cal N}_{X}\\sim\\mathcal{N}(\\mu(X),\\Sigma(X))$ and $\\dot{N_{Y}}\\sim\\mathcal{N}(\\mu(Y),\\Sigma(Y))$ be their normal approximations where $\\mu$ and $\\Sigma$   \n476 denote mean and covariance, respectively. Then, $\\bar{W_{2}}(N_{X},N_{Y})=\\bar{W_{2}}(X,Y)$ and $T=T_{\\mathrm{aff}}$ , where   \n477 $T_{\\mathrm{aff}}$ is the OT map between $N_{X}$ and $N_{Y}$ and can be calculated in closed-form ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{\\mathrm{aff}}(x)=A x+b,\\,}&{A=\\Sigma(Y)^{\\frac{1}{2}}\\left(\\Sigma(Y)^{\\frac{1}{2}}\\Sigma(X)\\Sigma(Y)^{\\frac{1}{2}}\\right)^{-\\frac{1}{2}}\\Sigma(Y)^{\\frac{1}{2}},}\\\\ &{b=\\mu(Y)-A\\mu(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "478 Proof. Corollary 3.2 states that $T$ is an OT map, and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Sigma(T N_{X})=T\\Sigma(X)T=\\Sigma(Y).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "479 Therefore, $T N_{X}=N_{Y}$ , and by Theorem 3.1, $T$ is the OT map between $N_{X}$ and $N_{Y}$ . Finally, we   \n480 compute ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(N_{X},N_{Y})=\\mathrm{Tr}[\\Sigma(X)]+\\mathrm{Tr}[T\\Sigma(X)T]-2\\,\\mathrm{Tr}[T^{\\frac{1}{2}}\\Sigma(X)T^{\\frac{1}{2}}]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{T:T(X)=Y}{\\mathrm{arg\\,min}}~\\mathbb{E}_{X}[\\|X-T(X)\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad=W_{2}^{2}(X,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "481 ", "page_idx": 13}, {"type": "text", "text": "482 Proposition 3.5. Let $X,Y\\in\\mathcal{P}_{2}(\\mathbb{R}^{d})$ and $N_{X},N_{Y}$ be their normal approximations. Then, ", "page_idx": 14}, {"type": "text", "text": "483 ", "page_idx": 14}, {"type": "text", "text": "484 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{l.~}\\left\\vert W_{2}(N_{X},N_{Y})-W_{2}(X,Y)\\right\\vert\\leq\\frac{2\\,\\mathrm{Tr}\\big[(\\Sigma(X)\\Sigma(Y))^{\\frac{1}{2}}\\big]}{\\sqrt{\\mathrm{Tr}[\\Sigma(X)]+\\mathrm{Tr}[\\Sigma(Y)]}}.}\\\\ {\\mathrm{2.~For}\\,T_{\\mathrm{aff~as~in~}(4),\\,}W_{2}(T_{\\mathrm{aff}}X,Y)\\leq\\sqrt{2}\\,\\mathrm{Tr}\\left[\\Sigma(Y)\\right]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "485 Proof. By Theorem 3.4, we have $W_{2}(N_{X},N_{Y})\\leq W_{2}(X,Y)$ . On the other hand, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(X,Y)=\\underset{\\gamma\\in\\mathrm{ADM}(X,Y)}{\\operatorname*{min}}\\int_{\\mathbb R^{d}\\times\\mathbb R^{d}}\\|x-y\\|^{2}d\\gamma(x,y)}\\\\ &{\\qquad\\qquad\\leq\\int_{\\mathbb R^{d}\\times\\mathbb R^{d}}\\left(\\|x\\|^{2}+\\|y\\|^{2}\\right)d\\gamma(x,y)}\\\\ &{\\qquad\\qquad=\\mathrm{Tr}[\\Sigma(X)]+\\mathrm{Tr}[\\Sigma(Y)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "486 Combining the above inequalities, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n|W_{2}(N_{X},N_{Y})-W_{2}(X,Y)|\\leq\\left|\\sqrt{\\mathrm{Tr}[\\Sigma(X)]+\\mathrm{Tr}[\\Sigma(Y)]}-W_{2}(N_{X},N_{Y})\\right|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "487 Let $a=\\mathrm{Tr}[\\Sigma(X)]+\\mathrm{Tr}[\\Sigma(Y)]$ , and so $W_{2}^{2}(N_{X},N_{Y})=a-b$ , where $b=2\\operatorname{Tr}\\left[(\\Sigma(X)\\Sigma(Y))^{\\frac{1}{2}}\\right]$ .   \n488 Then the RHS of can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\Big|}{\\sqrt{a}}-{\\sqrt{a-b}}{\\Big|}={\\frac{|a-(a-b)|}{\\sqrt{a}+{\\sqrt{a-b}}}}\\leq{\\frac{b}{\\sqrt{a}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "489 where the inequality follows from positivity of $W_{2}(N_{X},N_{Y})={\\sqrt{a-b}}$ . Letting $X=T_{\\mathrm{aff}}X$ in the   \n490 obtained bound gives 2). \u53e3 ", "page_idx": 14}, {"type": "image", "img_path": "lTUXlmjcva/tmp/7d72545e545be49dd340c8bd75b3cb8cef80a75333078ac24809878a234105d0.jpg", "img_caption": ["Figure 6: Median affinity scores of Sigmoid, ReLU, GELU, ReLU6, LeakyReLU with a default value of slope, Tanh, HardTanh, SiLU, and HardSwish obtained across random draws from Gaussian distribution with a sliding mean and varying stds used as their input. Whiskers of boxplots show the whole range of values obtained for each mean across all stds. The baseline value is the affinity score obtained for a sample covering the whole interval. The ranges and extreme values of each activation function over its subdomain are indicative of its non-linearity limits. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "491 D Affinity scores of other popular activation functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "492 Many works aimed to improve the way how the non-linearity \u2013 represented by activation functions \u2013   \n493 can be defined in DNNs. As an example, a recent survey on the commonly used activation functions in   \n494 deep neural networks [40] identifies over 40 activation functions with first references to sigmoid dating   \n495 back to the seminal paper [36] published in late 80s. The fashion for activation functions used in deep   \n496 neural networks evolved over the years in a substantial way, just as the neural architectures themselves.   \n497 Saturating activations, such as sigmoid and hyperbolic tan, inspired by computational neuroscience   \n498 were a number one choice up until the arrival of rectifier linear unit (ReLU) in 2010. After being the   \n499 workhorse of many famous models over the years, the arrival of transformers popularized Gaussian   \n500 Error Linear Unit (GELU) which is now commonly used in many large language models including   \n501 GPTs.   \n502 We illustrate in Figure 6 the affinity scores obtained after a single pass of the data through the   \n503 following activation functions: Sigmoid, ReLU [51], GELU [52], ReLU6 [53], LeakyReLU [54]   \n504 with a default value of the slope, Tanh, HardTanh, SiLU [55], and HardSwish [56]. As the non  \n505 linearity of activation functions depends on the domain of their input, we fix 20 points in their   \n506 domain equally spread in $[-20,20]$ interval. We use these points as means $\\{m_{i}\\}_{i=1}^{20}$ of Gaussian   \n507 distributions from which we sample 1000 points in $\\mathbb{R}^{300}$ with standard deviation (std) $\\sigma$ taking values   \n508 in $[2,1,0.5,0.25,0.1,0.01]$ . Each sample denoted by $X_{m_{i}}^{\\sigma_{j}}$ is then passed through the activation   \n509 function act \u2208 {sigmoid, ReLU, GELU} to obtain \u03c1amffi, $\\stackrel{\\cdot}{\\rho_{\\mathrm{aff}}^{m_{i},\\sigma_{j}}}\\ :=\\ \\rho_{\\mathrm{aff}}\\dot{\\left(X_{m_{i}}^{\\sigma_{j}},\\operatorname{act}(X_{m_{i}}^{\\bar{\\sigma}_{j}})\\right)}$ . Larger std   \n510 values make it more likely to draw samples that are closer to the region where the studied activation   \n511 functions become non-linear. We present the obtained results in Figure S2 where each of 20 boxplots   \n512 showcases median $(\\rho_{\\mathrm{aff}}^{m_{i},\\sigma.})$ values with $50\\%$ confidence intervals and whiskers covering the whole   \n513 range of obtained values across all $\\sigma_{j}$ .   \n514 This plot allows us to derive several important conclusions. We observe that each activation function   \n515 can be characterized by 1) the lowest values of its non-linearity obtained for some subdomain of the   \n516 considered interval and 2) the width of the interval in which it maintains its non-linearity. We note   \n517 that in terms of 1) both GELU and ReLU may attain affinity scores that are close to 0, which is not   \n518 the case for Sigmoid. For 2), we observe that the non-linearity of Sigmoid and GELU is maintained   \n519 in a wide range, while for ReLU it is rather narrow. We can also see a distinct pattern of more   \n520 modern activation functions, such as SiLU and HardSwish having a stronger non-linearity pattern in   \n521 large subdomains. We also note that despite having a shape similar to Sigmoid, Tanh may allow for   \n522 much lower affinity scores. Finally, the variations of ReLU seem to have a very similar shape with   \n523 LeakyReLU being on average more linear than ReLU and ReLU6. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "lTUXlmjcva/tmp/d6f80bd10a13921d1fcbce67f09bdbbbe4520f0b4c65f0b3d0c5cfd3171c0de1.jpg", "img_caption": ["Figure 7: (Top left) Affinity score is robust to the dimensionality reduction both when using averaging and summation over the spatial dimensions; (Top right) When $d>n$ , sample covariance matrix estimation leads to a lack of robustness in the estimation of the affinity score; (Bottom) Shrinkage of the covariance matrix leads to constant values of the affinity scores with increasing $d$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "524 E Implementation details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "525 Dimensionality reduction Manipulating 4-order tensors is computationally prohibitive and thus   \n526 we need to find an appropriate lossless function $r$ to facilitate this task. One possible choice for $r$   \n527 may be a vectorization operator that flattens each tensor into a vector. In practice, however, such   \n528 flattening still leads to very high-dimensional data representations. In our work, we propose to use   \n529 averaging over the spatial dimensions to get a suitable representation of the manipulated tensors. In   \n530 Figure 7 (left), we show that the affinity score is robust wrt such an averaging scheme and maintains   \n531 the same values as its flattened counterpart.   \n532 Computational considerations The non-linearity signature requires calculating the affinity score   \n533 over \u201cwide\u201d matrices. Indeed, after the reduction step is applied to a batch of $n$ tensors of size   \n534 $h\\times w\\times c$ , we end up with matrices of size $n\\times c$ where $n$ may be much smaller than $c$ . This is also   \n535 the case when input tensors are 2D when the batch size is smaller than the dimensionality of the   \n536 embedding space. To obtain a well-defined estimate of the covariance matrix in this case, we use a   \n537 known tool from the statistics literature called Ledoit-Wolfe shrinkage [57]. In Figure 7 (right), we   \n538 show that shrinkage allows us to obtain a stable estimate of the affinity scores that remain constant in   \n539 all regimes.   \n540 Robustness to batch size and different seeds In this section, we highlight the robustness of the   \n541 non-linearity signature with respect to the batch size and the random seed used for training. To this   \n542 end, we concentrate on VGG16 architecture and CIFAR10 dataset to avoid costly Imagenet retraining.   \n543 In Figure 8, we present the obtained result where the batch size was varied between 128 and 1024   \n544 with an increment of 128 (left plot) and when VGG16 model was retrained with seeds varying from   \n545 1 to 9 (right plot). The obtained results show that the affinity score is robust to these parameters   \n546 suggesting that the obtained results are not subject to a strong stochasticity.   \n547 Impact of training Finally, we also show how a non-linearity signature of a VGG16 model looks   \n548 like at the beginning and in the end of training on Imagenet. We extract its non-linearity signature   \n549 at initialization when making a feedforward pass over the whole CIFAR10 dataset and compare it   \n550 to the non-linearity signature obtained in the end. In Figure 9, we can see that at initialization the   \n551 network\u2019s non-linearity signature is increasing, reaching almost a perfectly linear pattern in the last   \n552 layers. Training the network enhances the non-linearity in a non-monotone way. Importantly, it also   \n553 highlights that the non-linearity signature is capturing information from the training process. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "lTUXlmjcva/tmp/707ef7e9a5cba28a759e4bded6a9141824f1fc14a9290c2749e7edd00b1f98eb.jpg", "img_caption": ["Figure 8: Non-linearity signature of VGG16 on CIFAR10 with a varying batch size (left) and when retrained from 9 different random seeds (right). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "lTUXlmjcva/tmp/0cc629084a3314678741b70cfd86a71e4fccec995b5b7fd90b77efabca778bc6.jpg", "img_caption": ["Figure 9: Non-linearity signatures of VGG16 on CIFAR10 in the beginning and end of training on Imagenet. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "lTUXlmjcva/tmp/16921c6dce88f981543fa6eebcd6fe352b13817c7ebde654fd08dd50acc39bfc.jpg", "img_caption": ["Figure 10: Raw non-linearity signatures of popular DNN architectures, plotted as affinity scores over the depth throughout the network. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "lTUXlmjcva/tmp/b8976338672fb593807a9365f71ca863dbf5c089494e9a88a53c5c0c15e1deea.jpg", "img_caption": ["Figure 11: ViTs: Large ViT with 16x16 and $32\\mathtt{x}32$ patch sizes and Huge ViT. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "554 F Raw signatures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "555 In Figure 10, we portray the raw non-linearity signatures of several representative networks studied   \n556 in the main paper. We use different color codes for distinct activation functions appearing repeatedly   \n557 in the considered architecture (for instance, every first ReLU in a residual block of a Resnet). We   \n558 also indicate the mean standard deviation of the affinity scores over batches in the title.   \n559 We see that the non-linearities across ReLU activations in all of Alexnet\u2019s 8 layers remain stable. Its   \n560 successor, VGG network, reveals tiny, yet observable, variations in the non-linearity propagation with   \n561 increasing depth and, slightly lower overall non-linearity values. We attribute this to the decreased   \n562 size of the convolutional filters (3x3 vs. 7x7). The Googlenet architecture was the first model   \n563 to consider learning features at different scales in parallel within the so-called inception modules.   \n564 This add more variability as affinity scores of activation in Googlenet vary between 0.6 and 0.9.   \n565 Despite being almost 20 times smaller than VGG16, the accuracy of Googlenet on Imagenet remains   \n566 comparable, suggesting that increasing and varying the linearity is a way to have high accuracy with   \n567 a limited computational complexity compared to predecessors. This finding is further confirmed with   \n568 Inception v3 that pushed the spread of the affinity score toward being more linear in some hidden   \n569 layers. When comparing this behavior with Alexnet, we note just how far we are from it. Resnets   \n570 achieve the same spread of values of the non-linearity but in a different, and arguably, simpler way.   \n571 Indeed, the activation after the skip connection exhibits affinity scores close to 1, while the activations   \n572 in the hidden layers remain much lower. Densenet, that connect each layer to all previous layers and   \n573 not just to the one that precedes it, is slightly more non-linear than Resnet152, although the two bear   \n574 a striking similarity: they both have an activation function that maintains the non-linearity low with   \n575 increasing depth. Additionally, transition layers in Densenet act as linearizers and allow it to reset the   \n576 non-linearity propagation in the network by reducing the feature map size. ViTs (Large with $16\\mathrm{x}16$   \n577 and $32\\mathtt{x}32$ patch sizes, and Huge with 14x14 patches) are all highly non-linear models to the degree   \n578 yet unseen. Interestingly, as seen in Figure 11 the patch size affects the non-linearity propagation   \n579 in a non-trivial way: for 16x16 size a model is more non-linear in the early layers, while gradually   \n580 becoming more and more linear later, while $32\\mathrm{x}32$ patch size leads to a plateau in the hidden layers   \n581 of MLP blocks, with a steep change toward linearity only in the final layer. We hypothesize that   \n582 attention modules in ViT act as a focusing lens and output the embeddings in the domain where the   \n583 activation function is the most non-linear.   \n584 Finally, we explore the role of increasing depth for VGG and Resnet architectures. We consider   \n585 VGG11, VGG13, VGG16 and VGG19 models in the first case, and Resnet18, Resnet34, Resnet50,   \n586 Resnet101 and Resnet152. The results are presented in Figure 12 and Figure 13 for VGGs and   \n587 Resnets, respectively. Interestingly, VGGs do not change their non-linearity signature with increasing   \n588 depth. In the case of Resnets, we can see that the separation between more linear post-residual   \n589 activations becomes more distinct and approaches 1 for deeper networks. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "lTUXlmjcva/tmp/a673b553f863c59936c1b318400aa1cbef5ab0f45982b68342dd966c91be402e.jpg", "img_caption": ["Figure 12: Impact of depth on the non-linearity signature of VGGs. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "lTUXlmjcva/tmp/38580187f3d278952614a208a7d38fa5027408797ce36343d7c0f6f4f9b3fc31.jpg", "img_caption": ["Figure 13: Impact of depth on the non-linearity signature of Resnets. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "lTUXlmjcva/tmp/dbd70f5faea43adfbada98c7c37c0d3514361633ec97be4a57f66e8ddb415040.jpg", "table_caption": ["Table 2: Pearson correlations between the affinity score and other metrics, for all the architectures evaluated in this study. We see that no other metric can reliably provide the same information as the proposed non-linearity signature across different neural architectures. "], "table_footnote": ["Average -0.31 \u00b1 0.43 -0.29 \u00b1 0.39 0.13 \u00b1 0.50 "], "page_idx": 21}, {"type": "text", "text": "590 G Detailed comparisons between architectures ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "591 We consider the following metrics as 1) the linear CKA [38] commonly used to assess the similarity   \n592 of neural representations, the average change in 2) SPARSITY and 3) ENTROPY before and after the   \n593 application of the activation function as well as the 4) Frobenius NORM between the input and output   \n594 of the activation functions, and the 5) $R^{2}$ score between the linear model fitted on the input and the   \n595 output of the activation function. We present in Table 2, the detailed values of Pearson correlations   \n596 obtained for each architecture and all the metrics considered in this study. In Figure 14, we show the   \n597 full matrix of pairwise DTW distances [50] obtained between architectures, then used to obtain the   \n598 clustering presented in the main text. ", "page_idx": 21}, {"type": "image", "img_path": "lTUXlmjcva/tmp/4a09e58eaf3c6432594f5d76aa33ad509ae9c93d51dd6d2b10f9dc683b8218fb.jpg", "img_caption": ["Figure 14: Full matrix of DTW distances between non-linearity signatures. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "lTUXlmjcva/tmp/b0679e4ea79dc1ebce620632279ba511fe556bafe456e9ee99e3fd163947f921.jpg", "img_caption": ["Figure 15: Deviation in terms of the Euclidean distance of the non-linearity signature obtained on CIFAR10, CIFAR100, and Random datasets from the non-linearity signature of the Imagenet dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "599 H Results on more datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "600 Below, we compare the results obtained on CIFAR10, CIFAR100 datasets as well as when the random   \n601 data tensors are passed through the network. As the number of plots for all chosen 33 models on   \n602 these datasets will not allow for a meaningful visual analysis, we rather plot the differences \u2013 in terms   \n603 of the DTW distance \u2013 between the non-linearity signature of the model on Imagenet dataset with   \n604 respect to three other datasets. We present the obtained results in Figure 15.   \n605 We can see that the overall deviation for CIFAR10 and CIFAR100 remains lower than for Random   \n606 dataset suggesting that these datasets are semantically closer to Imagenet. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "lTUXlmjcva/tmp/e37ee2daf4560b1449b041d8e5b2117b9ecb3e610900930e827daa84649fcbed.jpg", "img_caption": ["Figure 16: Hierarchical clustering of supervised and self-supervised pre-trained Resnet50 using the DTW distances between their non-linearity signatures. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 3: Robustness of the different criteria when considering the same architectures pre-trained for different tasks. Affinity score achieves the lowest standard deviation suggesting that it is capable of correctly identifying the architecture even when it was trained differently. ", "page_idx": 23}, {"type": "table", "img_path": "lTUXlmjcva/tmp/2832994ca371194acf69c8f9c5ed3370d4c2123a1e141ec4f77c1b5fa16a1d94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "607 I Results for self-supervised methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "608 In this section, we show that the non-linearity signature of a network remains almost unchanged   \n609 when considering other pertaining methodologies such as for instance, self-supervised ones. To this   \n610 end, we use 17 Resnet50 architecture pre-trained on Imagenet within the next 3 families of learning   \n611 approaches:   \n612   \n613   \n614   \n615   \n616   \n617 ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "1. SwAV [58], DINO [59], and MoCo [60] that belong to the family of contrastive learning methods with prototypes;   \n2. Resnet50 [18], Wide Resnet50 [61], TRex, and TRex\\* [62] that are supervised learning approaches;   \n3. SCE [63], Truncated Triplet [64], and ReSSL [65] that perform contrastive learning using relational information. ", "page_idx": 23}, {"type": "text", "text": "618 From the dendrogram presented in Figure 16, we can observe that the DTW distances between the   \n619 non-linearity signatures of all the learning methodologies described above allow us to correctly cluster   \n620 them into meaningful groups. This is rather striking as the DTW distances between the different   \n621 instances of the Resnet50 model are rather small in magnitude suggesting that the affinity scores still   \n622 retain the fact that it is the same model being trained in many different ways.   \n623 While providing a fine-grained clustering of different pre-trained models for a given fixed architecture,   \n624 the average affinity scores over batches remain surprisingly concentrated as shown in Table 3. This   \n625 hints at the fact that the non-linearity signature is characteristic of architecture but can also be subtly   \n626 multi-faceted when it comes to its different variations. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "lTUXlmjcva/tmp/7c333f7a99117502850429bbce523860952c3482b4a666f7ab9c587b7f0caafc.jpg", "img_caption": ["Figure 17: DTW distances associated with the clustering presented in Figure 16. We can see distinct clusters as revealed by the dendrogram. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "627 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "628 1. Claims   \n629 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n630 paper\u2019s contributions and scope?   \n631 Answer: [Yes]   \n632 Justification: Proposition of affinity score and non-linearity signature in Section 3. Experi  \n633 ments showing non-linearity signatures of DNNs, prediction of performance, clustering and   \n634 uniqueness in Section 4.   \n635 Guidelines:   \n636 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n637 made in the paper.   \n638 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n639 contributions made in the paper and important assumptions and limitations. A No or   \n640 NA answer to this question will not be perceived well by the reviewers.   \n641 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n642 much the results can be expected to generalize to other settings.   \n643 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n644 are not attained by the paper.   \n645 2. Limitations   \n646 Question: Does the paper discuss the limitations of the work performed by the authors?   \n647 Answer: [Yes]   \n648 Justification: We discuss limitations in Appendix B.   \n649 Guidelines:   \n650 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n651 the paper has limitations, but those are not discussed in the paper.   \n652 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n653 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n654 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n655 model well-specification, asymptotic approximations only holding locally). The authors   \n656 should reflect on how these assumptions might be violated in practice and what the   \n657 implications would be.   \n658 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n659 only tested on a few datasets or with a few runs. In general, empirical results often   \n660 depend on implicit assumptions, which should be articulated.   \n661 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n662 For example, a facial recognition algorithm may perform poorly when image resolution   \n663 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n664 used reliably to provide closed captions for online lectures because it fails to handle   \n665 technical jargon.   \n666 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n667 and how they scale with dataset size.   \n668 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n669 address problems of privacy and fairness.   \n670 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n671 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n672 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n673 judgment and recognize that individual actions in favor of transparency play an impor  \n674 tant role in developing norms that preserve the integrity of the community. Reviewers   \n675 will be specifically instructed to not penalize honesty concerning limitations.   \n676 3. Theory Assumptions and Proofs   \n677 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n678 a complete (and correct) proof?   \n680 Justification: Full proofs in Appendix C.   \n681 Guidelines:   \n82 \u2022 The answer NA means that the paper does not include theoretical results.   \n83 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n84 referenced.   \n85 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n86 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n87 they appear in the supplemental material, the authors are encouraged to provide a short   \n88 proof sketch to provide intuition.   \n89 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n90 by formal proofs provided in appendix or supplemental material.   \n91 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "692 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "693 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n694 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n695 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Justification: All models are pretrained checkpoints from torchvision. Experiments are conducted on Imagenet, publicly available. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "731 5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "732 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n733 tions to faithfully reproduce the main experimental results, as described in supplemental   \n734 material?   \n735 Answer: [Yes]   \n736 Justification: Anonymized code to reproduce experiments is available as a zip file, with a   \n737 README file to explain how to run it.   \n738 Guidelines:   \n739 \u2022 The answer NA means that paper does not include experiments requiring code.   \n740 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n741 public/guides/CodeSubmissionPolicy) for more details.   \n742 \u2022 While we encourage the release of code and data, we understand that this might not be   \n743 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n744 including code, unless this is central to the contribution (e.g., for a new open-source   \n745 benchmark).   \n746 \u2022 The instructions should contain the exact command and environment needed to run to   \n747 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n748 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n749 \u2022 The authors should provide instructions on data access and preparation, including how   \n750 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n751 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n752 proposed method and baselines. If only a subset of experiments are reproducible, they   \n753 should state which ones are omitted from the script and why.   \n754 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n755 versions (if applicable).   \n756 \u2022 Providing as much information as possible in supplemental material (appended to the   \n757 paper) is recommended, but including URLs to data and code is permitted.   \n758 6. Experimental Setting/Details   \n759 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n760 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n761 results?   \n762 Answer: [Yes]   \n763 Justification: Experimental details are described in Section 4 and Appendix E.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not include experiments.   \n766 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n767 that is necessary to appreciate the results and make sense of them.   \n768 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n769 material.   \n770 7. Experiment Statistical Significance   \n771 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n772 information about the statistical significance of the experiments?   \n773 Answer: [Yes]   \n774 Justification: Standard deviations across multiple batch of data are reported.   \n775 Guidelines:   \n776 \u2022 The answer NA means that the paper does not include experiments.   \n777 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n778 dence intervals, or statistical significance tests, at least for the experiments that support   \n779 the main claims of the paper.   \n780 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n781 example, train/test split, initialization, random drawing of some parameter, or overall   \n782 run with given experimental conditions).   \n83 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n84 call to a library function, bootstrap, etc.)   \n85 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n86 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n87 of the mean.   \n88 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n89 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n90 of Normality of errors is not verified.   \n91 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n92 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n93 error rates).   \n94 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n95 they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "796 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "797 Question: For each experiment, does the paper provide sufficient information on the com  \n798 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n799 the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "801 Justification: All experiments are carried out on a single A100 GPU.   \n802 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "812 Question: Does the research conducted in the paper conform, in every respect, with the   \n813 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n15 Justification: Standard and public datasets used, no experiments on human subjects.   \n16 Guidelines:   \n817 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n818 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n819 deviation from the Code of Ethics.   \n820 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n821 eration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "822 10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "823 Question: Does the paper discuss both potential positive societal impacts and negative   \n824 societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "826 Justification: We discuss broader impacts in Appendix A.   \n827 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 28}, {"type": "text", "text": "835 \u2022 The conference expects that many papers will be foundational research and not tied   \n836 to particular applications, let alone deployments. However, if there is a direct path to   \n837 any negative applications, the authors should point it out. For example, it is legitimate   \n838 to point out that an improvement in the quality of generative models could be used to   \n839 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n840 that a generic algorithm for optimizing neural networks could enable people to train   \n841 models that generate Deepfakes faster.   \n842 \u2022 The authors should consider possible harms that could arise when the technology is   \n843 being used as intended and functioning correctly, harms that could arise when the   \n844 technology is being used as intended but gives incorrect results, and harms following   \n845 from (intentional or unintentional) misuse of the technology.   \n846 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n847 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n848 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n849 feedback over time, improving the efficiency and accessibility of ML).   \n850 11. Safeguards   \n851 Question: Does the paper describe safeguards that have been put in place for responsible   \n852 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n853 image generators, or scraped datasets)?   \n854 Answer: [NA]   \n855 Justification: No such risks, no checkpoints released.   \n856 Guidelines:   \n857 \u2022 The answer NA means that the paper poses no such risks.   \n858 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n859 necessary safeguards to allow for controlled use of the model, for example by requiring   \n860 that users adhere to usage guidelines or restrictions to access the model or implementing   \n861 safety filters.   \n862 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n863 should describe how they avoided releasing unsafe images.   \n864 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n865 not require this, but we encourage authors to take this into account and make a best   \n866 faith effort.   \n867 12. Licenses for existing assets   \n868 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n869 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n870 properly respected?   \n871 Answer: [Yes]   \n872 Justification: Torchvision contributors credited for checkpoints, and datasets as well, in   \n873 Section 4.   \n874 Guidelines:   \n875 \u2022 The answer NA means that the paper does not use existing assets.   \n876 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n877 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n878 URL.   \n879 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n880 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n881 service of that source should be provided.   \n882 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n883 package should be provided. For popular datasets, paperswithcode.com/datasets   \n884 has curated licenses for some datasets. Their licensing guide can help determine the   \n885 license of a dataset.   \n886 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n887 the derived asset (if it has changed) should be provided.   \n888 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n889 the asset\u2019s creators.   \n890 13. New Assets   \n891 Question: Are new assets introduced in the paper well documented and is the documentation   \n892 provided alongside the assets?   \n893 Answer: [Yes]   \n894 Justification: Anonymized code to reproduce experiments is available as a zip file, with a   \n895 README file to explain how to run it.   \n896 Guidelines:   \n897 \u2022 The answer NA means that the paper does not release new assets.   \n898 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n899 submissions via structured templates. This includes details about training, license,   \n900 limitations, etc.   \n901 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n902 asset is used.   \n903 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n904 create an anonymized URL or include an anonymized zip file.   \n905 14. Crowdsourcing and Research with Human Subjects   \n906 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n907 include the full text of instructions given to participants and screenshots, if applicable, as   \n908 well as details about compensation (if any)?   \n909 Answer: [NA]   \n910 Justification: No experiments on human subjects.   \n911 Guidelines:   \n912 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n913 human subjects.   \n914 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n915 tion of the paper involves human subjects, then as much detail as possible should be   \n916 included in the main paper.   \n917 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n918 or other labor should be paid at least the minimum wage in the country of the data   \n919 collector.   \n920 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n921 Subjects   \n922 Question: Does the paper describe potential risks incurred by study participants, whether   \n923 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n924 approvals (or an equivalent approval/review based on the requirements of your country or   \n925 institution) were obtained?   \n926 Answer: [NA]   \n927 Justification: No experiments on or with human subjects.   \n928 Guidelines:   \n929 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n930 human subjects.   \n931 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n932 may be required for any human subjects research. If you obtained IRB approval, you   \n933 should clearly state this in the paper.   \n934 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n935 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n936 guidelines for their institution.   \n937 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n938 applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]