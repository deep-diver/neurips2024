[{"type": "text", "text": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yibin Wang \u2217\u20201 Haizhou Shi \u2217\u20201 Ligong Han 12 Dimitris Metaxas 1 Hao Wang \u20201 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches\u2019 performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data. Code is available at https://github.com/Wang-ML-Lab/bayesian-peft. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the recent advancements in Large Language Models (LLMs) [9, 106, 105, 66, 15, 4, 91, 92, 77, 12, 1, 71], addressing the challenges of reliability and responsibility remains imperative [42, 100, 99]. LLMs often produce overconfident responses detached from factual grounding, posing potential harm to users [3, 107, 44, 43, 87, 50, 7, 118, 111, 120, 33, 68, 115, 45]. Therefore, accurately estimating response confidence (or uncertainty) is crucial to preemptively intervene before harm occurs. Current research predominantly focuses on eliciting the internal capability of uncertainty estimation of LLMs. For example, studies suggest that verbalized uncertainty yields better-calibrated results compared to conditional probability [87, 45]. ", "page_idx": 0}, {"type": "text", "text": "While effective, the aforementioned methods do not offer a universal solution for expressing LLM uncertainty across all scenarios, especially when adapted [113] to domain-specific corpora, human preferences, or downstream tasks [44]. Even a well-calibrated LLM may struggle to estimate uncertainty during fine-tuning due to catastrophic forgetting of general knowledge [84]. Moreover, when applied to limited-scale downstream tasks, excessively over-parameterized LLMs can rapidly overfit, leading to overconfidence. Thus, enabling accurate uncertainty estimation of LLMs is vital for their reliable and responsible deployment. ", "page_idx": 0}, {"type": "text", "text": "Bayesian methods emerge as a natural solution for learning uncertainty estimation abilities among their counterparts [88, 11, 101, 98, 29, 46, 51, 61, 97, 58, 102, 20, 110]. These methods model predictive uncertainty $P(y|x,\\mathcal{D})$ by marginalizing the posterior parameter distribution $P(\\pmb\\theta|\\mathcal D)$ after observing the dataset $\\mathcal{D}$ : ", "page_idx": 0}, {"type": "equation", "text": "$$\nP(\\pmb{y}|\\pmb{x},\\mathcal{D})=\\int P(\\pmb{y}|\\pmb{x},\\pmb{\\theta})P(\\pmb{\\theta}|\\mathcal{D})d\\pmb{\\theta}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "However, adapting the Bayesian framework to LLMs poses significant challenges. LLM architectures typically incorporate complex components, including non-linear activation functions, rendering exact ", "page_idx": 0}, {"type": "text", "text": "Bayesian inference of parameter posteriors intractable, i.e., unable to compute the integral precisely. Consequently, finding an accurate approximation algorithm for the true posterior distribution becomes a primary challenge. Additionally, modeling parameter posterior distributions demands extra memory space, imposing a prohibitive burden on systems due to the massive scale of LLMs. ", "page_idx": 1}, {"type": "text", "text": "Contemporary methods leverage Parameter-Efficient Fine-Tuning (PEFT) to reduce the number of tunable parameters, thus alleviating computational and storage resource burdens [23, 41, 26, 121, 56, 53]. Built on this, recent research explores Bayesianizing only the PEFT module during fine-tuning to calibrate LLMs [8, 103, 116, 69], somewhat relieving the burden of introducing more parameters for posterior approximation. However, initial investigations suggest that straightforward combinations of PEFT and basic Bayesian techniques like Monte-Carlo Dropout (MCD, [29]) or Deep Ensemble (ENS, [51, 8, 103]) yield only marginal improvements in generalization and uncertainty estimation. The most promising results to date involve Kronecker factorized Laplace approximation, applied after maximum a posteriori (MAP) estimation provided by any optimization algorithm [116]. Nevertheless, we argue that such post-training procedures bifurcate posterior approximation into two stages, inevitably leading to suboptimal estimation. ", "page_idx": 1}, {"type": "text", "text": "To address this challenge, we propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), a Bayesian Deep Learning framework for fine-tuning LLMs with LoRA. BLoB jointly estimates the low-rank variational distributions\u2019 mean and covariance throughout the entire fine-tuning stage via backpropagation. Unlike methods relying on post-training approximation, BLoB enables simultaneous estimation of both the parameter mode (i.e., the mean if one assumes Gaussian distributions) and the parameter variance. Random sampling of model parameters based on variance estimation can enhance mode estimation. It thereby improves model performance in terms of accuracy and uncertainty estimation on both in-distribution and out-of-distribution datasets, as verified by our extensive experiments across multiple datasets. In summary, our contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a principled Bayesianization framework for Low-Rank Adaptation (LoRA) in Large Language Models (LLMs) by assuming that full weights\u2019 approximate posterior distribution has a low-rank structure containing a linear combination of independent Gaussian distributions.   \n\u2022 We show that, under mild conditions, optimization of the full-weight variational distribution can be done efficiently in the low-rank space of the weight update matrices.   \n\u2022 We introduce BLoB, a variational Bayesian low-rank adaptation framework for LLMs that jointly learns the mean and covariance of the variational distribution during fine-tuning.   \n\u2022 Extensive evaluations demonstrate the superiority of BLoB in terms of generalization and uncertainty estimation across different scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we describe the notation as well as some preliminaries. ", "page_idx": 1}, {"type": "text", "text": "Notation. In this paper, scalars are denoted by lowercase letters, vectors by lowercase boldface letters, and matrices by uppercase boldface letters. Probability, expectation, and the dataset are denoted by $P,\\mathbb{E}$ , and $\\mathcal{D}$ , respectively. We use $[m]=\\{1,2,\\cdots\\,,m\\}$ to denote the set of consecutive integer numbers starting from 1 and ending at $m$ . For a matrix $\\pmb{X}\\,=\\,[\\pmb{x}_{1},\\cdot\\cdot\\cdot\\cdot\\,,\\pmb{x}_{n}]\\,\\in\\,\\mathbb{R}^{m\\times n}$ , we use $\\mathrm{vec}(X)=[\\pmb{x}_{1}^{\\top},\\pmb{x}_{2}^{\\top},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{x}_{n}^{\\top}]^{\\top}\\,\\in\\,\\mathbb{R}^{(m n)\\times1}$ to denote the vectorization operation; we use $\\begin{array}{r}{\\|\\pmb{X}\\|_{p}=\\left[\\sum_{i j}|X_{i j}|^{p}\\right]^{1/p}}\\end{array}$ to define the $p$ -norm of a matrix. We use $\\otimes$ and $\\circ$ to denote the Kronecker product and the element-wise product, respectively. ", "page_idx": 1}, {"type": "text", "text": "2.1 Low-Rank Adaptation (LoRA) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inspired by the pioneering work on identifying and leveraging the low intrinsic rank of overparameterized models during fine-tuning [55, 2], Low-Rank Adaptation (LoRA) assumes a low rank for the network\u2019s weight updates [41]. Typically in a single linear layer, LoRA decomposes each update matrix $\\Delta W=B A$ into the product of two low-rank matrices, where $B\\in\\mathbb{R}^{m\\times\\bar{r}}$ and $A\\in\\mathbb{R}^{r\\times n}$ . Here, $m,n,$ , and $r$ denote the number of input neurons, output neurons, and the rank of the decomposition, respectively [41]. The forward pass of the linear layer with LoRA is formulated ", "page_idx": 1}, {"type": "equation", "text": "$$\nz=W_{0}h+\\Delta W h=W_{0}h+B A h,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $^h$ and $_{\\textit{z}}$ denote the input and output of the layer. Since the rank $r\\ll\\operatorname*{min}\\{m,n\\}$ is significantly smaller than the numbers of input and output neurons (e.g., $r=8\\ll m=n=4096$ in the attention layer [41]), LoRA can drastically reduce the number of trainable parameters by approximately three orders of magnitude compared to full-parameter fine-tuning, while achieving comparable performance to the full-rank fine-tuning. This also leads to a similar reduction in memory consumption for storing optimizer states, thereby reducing the hardware requirements for fine-tuning LLMs to a great extent. ", "page_idx": 2}, {"type": "text", "text": "2.2 Variational Bayesian Networks (VBNs) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Bayesian Neural Networks (BNNs) estimate the posterior distributions of network parameters rather than relying on single-point estimates [10, 102]. Due to the intractability of exact inference of the true posterior, Variational Bayesian Networks (VBNs) approximate the true posterior using a variational distribution; this is done by minimizing its KL divergence from the true posterior distribution [39, 30, 11]. Specifically, if the weights $W$ \u2019s variational distribution $q(W|\\theta)$ is parameterized by $\\pmb{\\theta}$ , minimizing the divergence $\\mathrm{KL}[q(W|\\pmb\\theta)||P(W|\\mathcal D)]$ is equivalent to minimizing the following variational free energy with respect to $\\pmb{\\theta}$ [67, 117, 28]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mathcal{D},\\pmb{\\theta})\\triangleq-\\mathbb{E}_{q(W|\\pmb{\\theta})}[\\log P(\\mathcal{D}|W)]+\\mathrm{KL}[q(W|\\pmb{\\theta})\\parallel P(W)].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The final formulation of the objective function in Eqn. 3 offers another interpretation beyond minimizing the KL divergence between the variational and true posterior distributions [11]. Specifically, the first term maximizes the likelihood of the data, while the second term regularizes the variational distribution $q(W|\\theta)$ . We refer to the first term as the likelihood cost and the second term as the complexity cost. Optimizing these two terms involves balancing the expressiveness of the approximate posterior distribution and its simplicity. ", "page_idx": 2}, {"type": "text", "text": "Optimizing the first term of Eqn. 3 requires integrating out the parameterized variational distribution, necessitating Monte Carlo gradient estimation [52, 81]. Using this approach, we can incorporate the re-parameterization trick to enable backpropagation of the gradient to the underlying parameter $\\pmb{\\theta}$ [72, 47, 78]. In Bayes By Backprop (BBB) [11], the variational distribution is further simplified as a diagonal Gaussian $\\mathcal{N}(\\pmb{\\mu},\\pmb{\\sigma}^{2})$ , where $\\pmb{\\sigma}=\\log(1+\\exp(\\pmb{\\rho}))$ ensures the standard deviation is positive. Then we have the Monte-Carlo estimation of Eqn. 3 that can pass the gradient to $\\pmb{\\theta}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}(\\mathcal{D},\\theta)\\approx-\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\log P(\\mathcal{D}|W_{k})+\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}[\\log q(W_{k}|\\theta)-\\log P(W_{k})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W_{k}=\\pmb{\\mu}+\\log(1+\\exp(\\pmb{\\rho}))\\odot\\pmb{\\epsilon}_{k}$ is the $k$ -th sample of the weights yielded by parameterization and $\\mathbf{\\epsilon}_{{\\epsilon}_{k}}\\sim\\mathcal{N}(\\mathbf{0},I)$ . In BBB, the authors assume the prior distribution $\\dot{P(W)}=\\bar{\\pi}\\dot{\\tilde{N}}({\\bf0},\\pmb{\\sigma}_{1}^{2})+(1-$ $\\pi)\\mathcal{N}(\\mathbf{0},\\pmb{\\sigma}_{2}^{2})$ to be a mixture of Gaussians. Consequently, they optimize the second term based on weight sampling. In different scenarios, a simpler form of the prior, which allows for a closed-form solution, can also be considered. Although our proposed method is largely based on the existing framework of BBB, trivially combining BBB with LoRA does not yield satisfactory results. It is important to note that our specific designs are necessary to encourage the fast convergence of the variational distribution, which will be introduced later in Sec. 3. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally introduce our proposed method, Bayesian Low-Rank Adaptation by Backpropagation (BLoB). We begin by discussing the design choices for Bayesianizing LoRA parameters in Sec. 3.1, highlighting the assumptions BLoB makes about the approximate posterior in the full-weight space. Next, in Sec. 3.2, we explore the low-rank structure of the prior distribution in the full-weight space, which in turn motivates our choice of prior distributions in the low-rank parameter space. In Sec. 3.3, we introduce our parameterization method for the variational distributions. In Sec. 3.4, we integrate Flipout [108] into LoRA for improved sampling efficiency and faster convergence. Finally, we present the complete algorithmic description of BLoB in Sec. 3.5. Proof of the theorems and claims in this section can be found in Appendix A. ", "page_idx": 2}, {"type": "image", "img_path": "MaDykgj4Ru/tmp/1a80de3b760ea34b1972e2ea5d0823a83ae985fcab9772ef50fe2f165128fa63.jpg", "img_caption": ["Figure 1: Overview of our Bayesian Low-Rank Adaptation by Backpropagation, i.e., BLoB (right) as well as comparison with existing methods such as LoRA (left) and Laplace LoRA (middle). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Low-Rank Variational Approximate Posterior Distribution: LoRA Bayesianization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Asymmetric LoRA Bayesianization. In LoRA [41], the weights are treated asymmetrically. $\\pmb{A}$ is randomly initialized, usually from the standard normal distribution or using Kaiming initialization, while $\\boldsymbol B=\\textbf{0}$ is initialized as a zero matrix to ensure that the model fully retains the capabilities of the pre-trained weights at the start of fine-tuning. The trivial solution of estimating the variational approximate posterior for the entire set of LoRA parameters can significantly hinder training convergence. For example, consider the Gaussian posteriors $q(\\pmb{A}|\\pmb{\\theta})=\\mathcal{N}\\!\\left(\\pmb{A}|\\pmb{M}_{4},\\dot{\\Omega}_{A}^{2}\\right)$ and $q(B|\\theta)=\\mathcal{N}(\\bar{B}|\\mathbf{0},\\boldsymbol{\\Omega}_{B}^{2})$ , where $\\Omega_{A}$ and $\\Omega_{B}$ are variance estimates added to $\\pmb{A}$ and $_B$ , respectively. Although the expectation $\\mathbb{E}_{A,B}[(W_{0}+B A)x]\\,=\\,W_{0}x+\\mathbb{E}_{A,B}[B A x]\\,=\\,W_{0}x$ preserves the functionality of the pre-trained model, accurate estimation requires an impractically large number of weight samples. Such variational distributions lead to significant fluctuations during the early stages of fine-tuning, unless the initial variance of $_B$ , $\\Omega_{B}\\to{\\bf0}^{+}$ , is intentionally minimized towards zero. Therefore, we take an asymmetric approach to initialize $\\mathbf{\\Omega}\\Omega_{B}=\\mathbf{0}$ and keep it fixed throughout the fine-tuning process. This, in effect, gives up Bayesian modeling of the $_B$ component and focuses only on the posterior of $\\pmb{A}$ in LoRA, as shown in Fig. 1. ", "page_idx": 3}, {"type": "text", "text": "Additional Advantages. In addition to reducing sampling noise and improving convergence speed, our Bayesianization design has two further advantages. First, compared to modeling the variational distributions of both $\\pmb{A}$ and $_B$ , our approach significantly reduces additional memory cost by approximately $50\\%$ per layer. Second, our design is equivalent to finding a posterior estimate for the full-weight matrix with a low-rank structure. For instance, by assuming a deterministic $_B$ and Bayesianizing $P(A|\\pmb\\theta)=\\mathcal{N}(\\pmb A|M,\\Omega^{2})$ , each element of the full weight matrix $W_{i j}$ is calculated as ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{i j}=W_{0,i j}+\\sum_{k=1}^{r}B_{i k}A_{k j},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A_{k j}\\,\\sim\\,\\mathcal{N}(M_{k j},\\Omega_{k j}^{2})$ is drawn independently $\\forall k\\,\\in\\,[r]$ . It is noteworthy that due to the low-rank structure defined in Eqn. 5, the full-weight parameters of $W$ are no longer independent from each other. The correlation among them can be reflected by the following theorem: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Variational Distribution of the Full-Weight Matrix in BLoB). With the pre-trained weight matrix $W_{0}\\,\\in\\,\\mathbb{R}^{m\\times n}$ and the low-rank weight update matrix $\\boldsymbol{B}\\,\\in\\,\\mathbb{R}^{m\\times r}$ , suppose that the variational distribution of the other low-rank update matrix $\\boldsymbol{A}\\,\\in\\,\\mathbb{R}^{r\\times n}$ is Gaussian with $\\begin{array}{r}{q(A|\\theta=\\{M,\\Omega\\})=\\prod_{i j}\\dot{\\mathcal{N}}(A_{i j}|M_{i j},\\Omega_{i j}^{2}),}\\end{array}$ , where $\\b{M}=[M_{i j}]\\in\\mathbb{R}^{r\\times n}$ and $\\Omega=[\\Omega_{i j}]\\in\\mathbb{R}^{r\\times n}$ are its mean and standard deviation, respectively. The equivalent variational distribution defined on the full weight matrix $W$ as in Eqn. 3 is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathrm{vec}(W)|B,\\theta)=\\mathcal{N}(\\mathrm{vec}(W)|\\mu_{q},\\Sigma_{q}),}\\\\ &{\\quad\\quad w h e r e\\quad\\mu_{q}=\\mathrm{vec}(W_{0}+B M),}\\\\ &{\\quad\\quad\\quad\\quad\\Sigma_{q}=[I_{n}\\otimes B]\\cdot[\\mathrm{diag}(\\mathrm{vec}(\\Omega)^{2})]\\cdot[I_{n}\\otimes B^{\\top}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 shows that our asymmetric LoRA Bayesianization is equivalent to using a Gaussian variational distribution for the full weight $W$ (i.e., Eqn. 6), with a flexible covariance matrix (i.e., Eqn. 8), to approximate the postetior distribution of the full weight $W$ . ", "page_idx": 3}, {"type": "text", "text": "Remark. The covariance matrix $\\Sigma_{q}$ is strictly singular, which consequently inspires us to design a prior $P(W)$ with such low-rank structure in Sec. 3.2. Previous work on low-rank Gaussians typically considers covariance with a similar structure $D^{2}+\\Sigma_{q}$ , where $_{D}$ is diagonal [89, 70, 86, 90, 73]. However, sampling from a Gaussian with this structure requires sampling noise of the same shape as the full-weight matrix, which is not parameter-efficient; we therefore do not adopt this in our work. ", "page_idx": 4}, {"type": "text", "text": "3.2 Low-Rank Prior Distribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Eqn. 3, optimizing the KL divergence between the variational and prior distributions in the space of full weights can be burdensome. Therefore, we assume the prior distribution of the full weights to be a low-rank Gaussian, with its mean centered at the pre-trained weights $\\mathrm{vec}(W_{0})$ and its covariance matrix parameterized by a rank-r\u2032 matrixR  \u2208R(mn)\u00d7r: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\mathrm{vec}(W))=\\mathcal{N}(\\mathrm{vec}(W)|\\mu_{p},\\Sigma_{p}),}\\\\ &{\\mathrm{where}\\quad\\mu_{p}=\\mathrm{vec}(W_{0}),}\\\\ &{\\Sigma_{p}=\\widetilde{R}\\widetilde{R}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assuming a low-rank prior distribution and designing an appropriateR  allows us to optimize the KL divergence in the decomposed low-rank weight space, as suggested  by the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Efficient Computation of Full-Weight KL Divergence). Suppose the pre-trained weights $W_{0}$ , update matrix $_B$ , and the variational distribution $q(A|\\pmb\\theta)$ are defined as in Theorem 3.1, and the prior distribution of the full-weight matrix $P(\\mathrm{vec}(W))$ is defined as Eqn. 9. Consider the Gaussian prior distribution $\\begin{array}{r}{P(\\dot{\\bf A})=\\prod_{i j}\\mathcal{N}(A_{i j}|0,\\sigma_{p}^{2}),}\\end{array}$ ; we then have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{KL}[q(\\mathrm{vec}(W)|B,\\theta)\\|P(\\mathrm{vec}(W))]=\\mathrm{KL}[q(A|\\theta)\\|P(A)],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 shows that with a properR, one can compute the KL divergence for the high-dimensional full weight $\\mathrm{vec}(W)$ simply by computing the KL divergence for $\\pmb{A}$ , which is much lower-dimension, more parameter-efficient, more memory-efficient, and faster. Note that the Gaussian distributions we define for both the prior and the posterior are degenerate. However, they are valid for probabilistic inference [83], as (i) their probability density is well-defined, and (ii) their KL divergence is computable under the assumptions of Theorem 3.2. See Appendix A.1 for a detailed discussion. ", "page_idx": 4}, {"type": "text", "text": "Concretely, we assume that the prior distribution in BLoB follows the low-rank structure described in Theorem 3.2 and minimize the KL divergence term for the low-rank component $\\pmb{A}$ using its analytical solution in Eqn. 3: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}[q(A|\\theta=\\{M,\\Omega\\})\\|P(A)]=\\frac{1}{2\\sigma_{p}^{2}}(\\|M\\|_{2}^{2}+\\|\\Omega\\|_{2}^{2})-\\displaystyle\\sum_{i j}\\log\\Omega_{i j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Parameterization of the Low-Rank Variational Distribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The parameterization of the Gaussian variational distribution $q(A|\\pmb\\theta)$ significantly affects the convergence speed of the KL term in Eqn. 11. The mean matrix $_M$ of $q(A|\\pmb\\theta)$ has no additional constraints, we therefore parameterize it directly as the output of a neural network. Each entry of $q(A|\\pmb\\theta)$ \u2019s diagonal covariance matrix $\\Omega$ (i.e., standard deviation) is non-negative; we therefore use element-wise parameterization $\\Omega_{i j}=G_{i j}^{2}$ , where $G=[G_{i j}]\\in\\mathbb{R}^{r\\times n}$ is the real parameter matrix that determines the standard deviation $\\pmb{\\Omega}$ . Since $\\pmb{\\Omega}$ is usually initialized with small positive values close to zero, our parameterization method provides large gradients initially, contributing to the rapid decrease of the KL term. We further show, both theoretically and empirically, that our parameterization method, unlike BBB\u2019s softplus function $\\log(1+\\exp(\\cdot))$ , is crucial for the fast convergence of $\\pmb{\\Omega}$ when $q(A|\\pmb\\theta)$ is close to the prior distribution $P(A)$ (see more analysis in Appendix A.2). ", "page_idx": 4}, {"type": "text", "text": "3.4 On Improving the Sample Efficiency of BLoB ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Improving Sample Efficiency with Flipout. One main challenge in estimating the variational distribution (i.e., the approximate posterior) during fine-tuning lies in the sample efficiency of the ", "page_idx": 4}, {"type": "text", "text": "Require: dataset $\\mathcal{D}$ , pre-trained weight $W_{0}$ , low-rank component $_B$ , $\\pmb{\\theta}=\\{\\boldsymbol{M},\\boldsymbol{G}\\}$ for parameterizing the mean and variance of $\\pmb{A}$ ;   \nRequire: prior standard deviation $\\sigma_{p}$ , initialization hyperparameter $\\epsilon$ , number of input features $n$ ;   \nRequire: number of samples during training $K$ , number of iterations $T$ , learning rate $\\eta$ ; $:G\\sim\\mathcal{U}(\\frac{\\epsilon}{\\sqrt{2}},\\epsilon),M\\sim\\mathcal{U}\\left(-\\sqrt{\\frac{6}{n}},\\sqrt{\\frac{6}{n}}\\right)$ $\\triangleright$ Initialization of $\\pmb{A}$ \u2019s parameters.   \n2: $B\\leftarrow0$ \u25b7Initialization of $_B$ .   \n3: for $t=1,\\cdot\\cdot\\cdot,T$ do   \n4: Sample a mini-batch of data $\\mathcal{D}_{t}\\sim\\mathcal{D}$ containing $b$ samples.   \n5: for $k=1,\\cdots\\,,K$ do   \n6: Sample batched noise $E_{k}\\sim\\mathcal{N}\\left(\\mathbf{0},{I}\\right)$ . \u25b7Sample the noise.   \n7: Let $\\{\\widetilde{E}_{k j}\\}_{j=1}^{b}\\leftarrow\\mathrm{BLoBFlipout}(E_{k})$ . \u25b7Eqn. 12   \n8: Let $\\{A_{k j}=M+G^{2}\\circ\\widetilde{E}_{k j}\\}_{j=1}^{b}$ .   \n190:: eLnetd $\\begin{array}{r}{\\widetilde{\\widehat{\\mathcal{F}}}_{t}^{\\cdot}=-\\frac{1}{K b}\\sum_{k=1}^{K}\\sum_{j=1}^{b}\\log P(\\mathcal{D}_{t}|A_{k j},B)+\\frac{1}{2\\sigma_{p}^{2}}(\\|M\\|_{2}^{2}+\\|G\\|_{2}^{4})-2\\sum_{i j}\\log G_{i j}.}\\end{array}$   \n11: \u25b7Eqn. 13 and 11.   \n12: Calculate the gradient w.r.t. the parameters: $\\Delta_{M}=\\partial\\tilde{\\hat{\\mathcal{F}}}_{t}/\\partial M,\\Delta_{G}=\\partial\\hat{\\mathcal{F}}_{t}\\rvert_{\\partial G},\\Delta_{B}=\\partial\\hat{\\mathcal{F}}_{t}\\rvert_{\\partial B}$ .   \n13: Update the parameters: $\\begin{array}{r l}&{M\\leftarrow\\dot{M}-\\eta\\Delta_{M}}\\\\ &{G\\leftarrow G-\\eta\\Delta_{G};}\\\\ &{B\\leftarrow B-\\eta\\Delta_{B}\\;.}\\end{array}$ ;   \n14: end for ", "page_idx": 5}, {"type": "text", "text": "weights [109, 25, 58]. During mini-batch stochastic gradient descent, a batch of examples typically share the same weights drawn from the variational distribution. This can lead to slow convergence of the likelihood cost in Eqn. 3. Drawing inspiration from [108], we introduce the technique of flipout to speed up the sampling procedure of our low-rank variational distributions $q(A|\\theta)$ . ", "page_idx": 5}, {"type": "text", "text": "LoRA Flipout. Unlike the original approach, which applies rank-1 random flipping to the full weights, we apply filpout exclusively to the low-rank component $\\pmb{A}$ . Specifically, suppose we have a mini-batch of input vectors $\\b{H}\\in\\mathbb{R}^{\\bar{n}\\times\\b{b}}$ , where $b$ represents the batch size. We randomly sample two low-rank flipping matrices $S\\in\\{-1,+1\\}^{n\\times b}$ and $\\pmb{T}\\in\\{-1,+1\\}^{b\\times r}$ . Denoting as $\\pmb{E}\\in\\mathbb{R}^{r\\times n}$ the weight noise sampled for this mini-batch, the batched output $Z$ after applying flipout is then ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ=W_{0}H+B(M H+[(E\\circ\\Omega)(H\\circ S)]\\circ T),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is crucial that the independent noises added to the low-rank weight noise $\\Delta A\\triangleq E\\circ\\Omega$ ensure sampling independence across examples within a mini-batch, thereby enhancing the sampling efficiency of the algorithm. This is done without violating the assumptions outlined in Theorem 3.1 and 3.2. As illustrated in algorithm 1, we use $\\tilde{E}_{k j}$ to represent the equivalent noise applied to parameter $\\pmb{A}$ for the $j$ -th example in the $k$ -th batc h after BLoBFlipout. Due to the low-rank structure of our Bayesianization method, the computational overhead of employing flipout in BLoB is also minimal. ", "page_idx": 5}, {"type": "text", "text": "3.5 BLoB: Final Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are now ready to present our full BLoB algorithm. ", "page_idx": 5}, {"type": "text", "text": "During training, under the assumptions outlined in Theorem 3.1 and 3.2, optimizing the evidence lower bound on the full weight $W$ can be efficiently done in the low-rank space, using the following final objective function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}(\\mathcal{D},B,\\pmb{\\theta})=-\\mathbb{E}_{q(\\pmb{W}|\\pmb{B},\\pmb{\\theta})}[\\log P(\\mathcal{D}|\\pmb{W})]+\\mathrm{KL}[q(\\pmb{W}|\\pmb{B},\\pmb{\\theta})\\parallel P(\\pmb{W})]}\\\\ &{\\quad\\quad\\quad\\quad=-\\mathbb{E}_{q(\\pmb{A}|\\pmb{\\theta})}[\\log P(\\mathcal{D}|\\pmb{A},\\pmb{B})]+\\mathrm{KL}[q(\\pmb{A}|\\pmb{\\theta})\\parallel P(\\pmb{A})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\theta}=\\{\\boldsymbol{M},\\boldsymbol{\\Omega}\\}$ denotes the set of the parameters underlying the variational distribution of the low-rank matrix $\\pmb{A}$ . Additionally, to trade off between data fitting and posterior approximation, ", "page_idx": 5}, {"type": "text", "text": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use $N$ to represent the number of samples during inference in BLoB. \u201c\u2191\u201d and \u201c\u2193\u201d indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively. ", "page_idx": 6}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/1dccc19c5839cce55710404b110bfd01b8bff12d686a2b699a21e46773d0c9d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "we employ a KL re-weighting scheme, which is detailed in Appendix B.1. The full algorithmic description of BLoB training is shown in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "During inference, for an input $\\textbf{\\em x}$ , we approximate the expected output distribution $P(y|x)$ of BLoB by drawing $N$ samples from the variational distribution $\\bar{\\b{q}}(\\b{W}|\\b{\\theta})$ . Empirically, $N=10$ provides a good balance between estimation quality and computational efficiency: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q(\\pmb{W}|\\theta)}[P(\\pmb{y}|\\pmb{x},\\pmb{W})]\\approx\\frac{1}{N}\\sum_{n=1}^{N}P(\\pmb{y}|\\pmb{x},\\pmb{W}_{n}),\\quad\\pmb{W}_{n}\\sim q(\\pmb{W}|\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare our BLoB with existing methods on real-world datasets. Sec. 4.1 introduces the experimental settings, including baselines, fine-tuning, and evaluation protocols. We then evaluate BLoB\u2019s generalization and uncertainty estimation abilities in both in-distribution (Sec. 4.2) and out-of-distribution scenarios (Sec. 4.3). ", "page_idx": 6}, {"type": "text", "text": "4.1 Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fine-tuning and Evaluation. We implement BLoB in the PEFT library [63] and fine-tune the LlaMA2-7B [92] model on common-sense reasoning tasks. Following Laplace-LoRA [116], we apply LoRA to the output layer as well as the queries and values of all the attention layers. For hyperparameters, we strictly adhere to the default settings in the PEFT library and the original LoRA paper [63, 41] to ensure maximal reproducibility. This includes the number of training steps, learning rate, and LoRA rank $r$ (see Appendix B.1 for details). For common-sense reasoning tasks, we select the next token logits corresponding to possible answers from each dataset and fine-tune the LLM to maximize the likelihood of the correct token. For evaluation, in addition to Accuracy (ACC), we use Expected Calibration Error (ECE [31]) and Negative Log-Likelihood (NLL) to assess the models\u2019 uncertainty estimation ability (see Appendix B.2 for details). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Baselines and Implementation Details. We compare BLoB with state-of-the-art uncertainty estimation methods applied to the LoRA adapters of LLMs, including Monte-Carlo Dropout (MCD) [29], Bayes By Backprop (BBB) [11], Deep Ensemble (ENS) [51, 8, 103], and the latest LaplaceLoRA (LAP) [116]. We also report the performance of two standard PEFT baseline methods for reference: Maximum Likelihood Estimation (MLE) [41] and Maximum A Posteriori (MAP). ", "page_idx": 7}, {"type": "text", "text": "For MLE, we use the LoRA implementation. For MAP, we use a weight decay rate of $1e-5$ . For MCD, we use an ensemble of 10 LoRAs with a dropout rate of $p=0.1$ . For ENS, we independently fine-tune 3 LoRAs and average their logits during evaluation. For BBB, we adopt the default settings from the Bayesian-Torch library [48] and only Bayesianize the $\\pmb{A}$ matrix, similar to BLoB. We sample $N=10$ times for BBB during test. We re-implement LAP and apply it to the MAP checkpoints. We keep all BLoB-specific hyperparameters consistent across all datasets. Typically, we set the number of samples $K=1$ during training for all our BLoB experiments, which highlights BLoB\u2019s sampling efficiency. As shown in Table 1, we also report BLoB\u2019s performance with different numbers of samples during Bayesian inference, where $N=0$ indicates directly using the mean of the weight distribution for prediction. ", "page_idx": 7}, {"type": "text", "text": "4.2 Results on In-distribution Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We fine-tune Llama2-7B on six common-sense reasoning tasks: Winogrande-small (WG-S), Winogrande-medium (WG-M) [82], ARC-Challenge (ARC-C) [18], ARC-Easy (ARC-E) [18], OpenBookQA (OBQA) [65], and BoolQ [17]. For all baseline methods, using the same pre-trained LLM backbone, we maintain consistent hyperparameters across all datasets and do not use additional validation sets to achieve higher performance (See Appendix B.3 for detailed settings). ", "page_idx": 7}, {"type": "text", "text": "Table 1 shows the performance of BLoB compared to the baselines, including ACC, ECE, and NLL, on the in-distribution test set with the pre-trained Llama2-7B model. The high ECE and NLL for MLE indicate overconfidence in LLMs during conventional fine-tuning, except for BoolQ due to its large dataset size. Simple but popular baselines like MAP, MCD, and ENS show mixed results in terms of NLL and/or ECE, highlighting the challenge of uncertainty estimation during LLM fine-tuning. LAP, the most competitive post-training baseline for uncertainty estimation, significantly reduces NLL and ECE on some datasets but lacks consistent performance, as indicated by its failures on ARC-C and ARC-E. BBB mitigates the overconfidence issue in LLMs across almost all datasets, showcasing the advantage of jointly optimizing the mean and covariance of the variational weight distributions during fine-tuning. However, there remains considerable room for improvement. ", "page_idx": 7}, {"type": "text", "text": "BLoB consistently achieves better or comparable performance across all datasets. With the number of samples during inference set to $N=10$ , the same as MCD, BLoB provides the best uncertainty estimation performance, significantly reducing NLL and ECE, and greatly mitigating overconfidence while maintaining comparable or better ACC than MLE. Even with half the number of samples, $N\\,=\\,5$ , BLoB still delivers performance comparable to that of $N\\,=\\,10$ and outperforms other baselines on most datasets. By abandoning the modeling of the posterior distribution, prediction using the mean of the weight distribution, i.e., BLoB $(\\mathrm{N}{=}0)$ sacrifices some degree of calibration in exchange for improved accuracy. Appendix C.5 presents the trade-off between accuracy and calibration, which is controlled by the standard deviation of the prior Gaussian distribution). ", "page_idx": 7}, {"type": "text", "text": "Besides Llama2-7B, we also include additional results for RoBERTa-base [60] on text classification tasks in Appendix C.1. Our method consistently achieved either the best or runner-up performance across nearly all datasets, demonstrating its versatility across different architectures. ", "page_idx": 7}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/6a4d4e6a27ed625dcf8bd9e755215e46c3dc4c2f8991d2ef118d940b54afcdc7.jpg", "table_caption": ["Table 2: Performance on in-distribution and out-of-distribution datasets. All the uncertainty estimation methods are applied to the LoRA adapter added upon the pre-trained Llama2-7B weights. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Results on Out-of-Distribution Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We use models fine-tuned on OBQA [65] to evaluate the generalization ability of different methods under distributional shifts. OBQA consists of multiple-choice elementary-level science questions. We categorize the distributional shifts into two types: smaller and larger shifts. The ARC [18] dataset, which also consists of multiple-choice science questions, represents a smaller distributional shift. The college-level chemistry and physics subsets of MMLU [38] represent larger distributional shifts. ", "page_idx": 8}, {"type": "text", "text": "The results in Table 2 highlight BLoB\u2019s superior OOD generalization ability compared to other methods on both smaller and larger distribution shifts. BLoB achieves the highest accuracy when solely utilizing the mean of the weight distribution in smaller distribution shifts. For larger distribution shifts, incorporating uncertainty through sampling improves model accuracy. Regarding uncertainty estimation, BLoB demonstrates the best or second-best performance in smaller distribution shifts. Although there is a slight performance drop with larger distribution shifts, BLoB remains comparable to baselines such as ENS and LAP. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT) for LLMs. Due to the prohibitively large size of LLMs, parameter-efficient fine-tuning has become a trending topic. Computational paradigms in this area include adapter-based fine-tuning [40, 36, 80, 75, 62], prompt-based fine-tuning [34, 54, 59, 57, 94, 6], and partial fine-tuning [119, 124, 5, 112, 32]. Among these, LoRA [41] has gained significant attention due to its simplicity and effectiveness. Building on LoRA, numerous studies have aimed to further optimize parameter efficiency when fine-tuning large models [26, 35, 22, 21]. For instance, KronA models weight updates as the Kronecker product of two smaller matrices without decreasing the update rank [26], and SVDiff performs Singular Value Decomposition (SVD) on the original weight matrices, fine-tuning only the singular values [35]. However, in this paper, we focus solely on ", "page_idx": 8}, {"type": "text", "text": "Bayesianizing LoRA due to its widespread application in existing works. We also note that BLoB can be naturally adapted to handle different LoRA variants. ", "page_idx": 9}, {"type": "text", "text": "Uncertainty Estimation in Large Language Models. Large-scale pre-trained models are wellcalibrated during pre-training [44], but fail to accurately express predictive uncertainty during inference [3, 107, 44, 43, 87], especially after fine-tuning [8, 103, 116, 69]. This indicates that measures effective during pre-training [93, 114, 16, 122, 14] may lose their power of uncertainty estimation after fine-tuning for domain-specific knowledge. To address this issue, [27, 123] define priors and approximate posteriors on the full attention weights during fine-tuning, achieving better uncertainty estimation but at a significant cost in time and space. Consequently, recent work integrates Bayesian methods and PEFT for efficient uncertainty estimation. For instance, [8, 103] train and store multiple copies of different LoRAs, ensembling their outputs during inference to achieve somewhat better results. [116] applies Kronecker factorized Laplace approximation on fine-tuned LoRA. However, such post-training procedures bifurcate posterior approximation into two stages, leading to suboptimal estimation. In contrast, our BLoB enables simultaneous estimation of both the mean and covariance of LLM parameters in a single fine-tuning stage, substantially improving performance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a principled Bayesianization framework for parameter-efficiently fine-tuning LLMs. Our theoretical analysis shows that a full-weight variational distribution can be efficiently optimized by approximately using a low-rank space of the weight update matrices. Our empirical evaluations corroborate this theoretical insight, demonstrating superior generalization and uncertainty estimation capabilities across diverse scenarios compared to various baseline methods. Building on LoRA, our approach seamlessly integrates with existing LLM architectures while imposing minimal additional memory overhead and training time. Our method highlights that jointly learning the mean and covariance of the variational distribution during fine-tuning can mutually improve both, underscoring the powerful potential of Bayesian methods in enhancing the reliability and generalization of LLMs. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main limitations of our proposed BLoB method are: (i) BLoB is confined to fine-tuning scenarios and is not applicable to training-free tasks, such as direct uncertainty estimation during inference [64]. (ii) As a typical mean-field variational inference method, BLoB requires multiple sampling iterations during inference, which challenges stable and efficient deployment. (iii) While BLoB\u2019s effectiveness has been empirically demonstrated for downstream classification tasks, its application to generation tasks requires further investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank the reviewers/AC for the constructive comments to improve the paper. We thank Sanket Jantre for identifying improvements to our proof and for other valuable discussions. HS and HW are partially supported by Microsoft Research AI & Society Fellowship, NSF Grant IIS-2127918, NSF CAREER Award IIS-2340125, NIH Grant 1R01CA297832, and the Amazon Faculty Research Award. This research is also supported by NSF National Artificial Intelligence Research Resource (NAIRR) Pilot. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 9}, {"type": "text", "text": "[2] A. Aghajanyan, S. Gupta, and L. Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319\u20137328, 2021.   \n[3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9. Concrete problems in ai safety, 2016.   \n[4] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[5] A. Ansell, E. M. Ponti, A. Korhonen, and I. Vuli\u00b4c. Composable sparse fine-tuning for crosslingual transfer. arXiv preprint arXiv:2110.07560, 2021.   \n[6] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6655\u20136672, 2022.   \n[7] A. Azaria and T. Mitchell. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734, 2023.   \n[8] O. Balabanov and H. Linander. Uncertainty quantification in fine-tuned llms using lora ensembles. arXiv preprint arXiv:2402.12264, 2024.   \n[9] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[10] C. M. Bishop. Pattern recognition and machine learning. Springer google schola, 2:1122\u20131128, 2006.   \n[11] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural network. In International conference on machine learning, pages 1613\u20131622. PMLR, 2015.   \n[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[13] T. H. Chan, K. W. Lau, J. Shen, G. Yin, and L. Yu. Adaptive uncertainty estimation via high-dimensional testing on latent representations. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 39975\u201339993. Curran Associates, Inc., 2023.   \n[14] W. Chen and Y. Li. Calibrating transformers via sparse gaussian processes. arXiv preprint arXiv:2303.02444, 2023.   \n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.   \n[16] T. Cinquin, A. Immer, M. Horn, and V. Fortuin. Pathologies in priors and inference for bayesian transformers. arXiv preprint arXiv:2110.04020, 2021.   \n[17] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.   \n[19] I. Dagan, O. Glickman, and B. Magnini. The pascal recognising textual entailment challenge. In J. Qui\u00f1onero-Candela, I. Dagan, B. Magnini, and F. d\u2019Alch\u00e9 Buc, editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177\u2013190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.   \n[20] E. Daxberger, A. Kristiadi, A. Immer, R. Eschenhagen, M. Bauer, and P. Hennig. Laplace redux - effortless bayesian deep learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20089\u201320103. Curran Associates, Inc., 2021.   \n[21] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \n[22] N. Ding, X. Lv, Q. Wang, Y. Chen, B. Zhou, Z. Liu, and M. Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint arXiv:2311.11696, 2023.   \n[23] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235, 2023.   \n[24] W. B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.   \n[25] M. Dusenberry, G. Jerfel, Y. Wen, Y. Ma, J. Snoek, K. Heller, B. Lakshminarayanan, and D. Tran. Efficient and scalable Bayesian neural nets with rank-1 factors. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2782\u20132792. PMLR, 13\u201318 Jul 2020.   \n[26] A. Edalati, M. Tahaei, I. Kobyzev, V. P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.   \n[27] X. Fan, S. Zhang, B. Chen, and M. Zhou. Bayesian attention modules. Advances in Neural Information Processing Systems, 33:16362\u201316376, 2020.   \n[28] K. Friston, J. Mattout, N. Trujillo-Barreto, J. Ashburner, and W. Penny. Variational free energy and the laplace approximation. Neuroimage, 34(1):220\u2013234, 2007.   \n[29] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u2013 1059. PMLR, 2016.   \n[30] A. Graves. Practical variational inference for neural networks. Advances in neural information processing systems, 24, 2011.   \n[31] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[32] D. Guo, A. M. Rush, and Y. Kim. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884\u20134896, 2021.   \n[33] N. Gupta, H. Narasimhan, W. Jitkrittum, A. S. Rawat, A. K. Menon, and S. Kumar. Language model cascades: Token-level uncertainty and beyond. arXiv preprint arXiv:2404.10136, 2024.   \n[34] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4921\u20134933, 2021.   \n[35] L. Han, Y. Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7323\u20137334, 2023.   \n[36] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2021.   \n[37] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[38] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[39] G. E. Hinton and D. Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5\u201313, 1993.   \n[40] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n[41] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[42] C. Huang, R. Wang, K. Xie, T. Yu, and L. Yao. Learn when (not) to trust language models: A privacy-centric adaptive model-aware approach, 2024.   \n[43] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023.   \n[44] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. HatfieldDodds, N. DasSarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan. Language models (mostly) know what they know, 2022.   \n[45] S. Kapoor, N. Gruver, M. Roberts, K. Collins, A. Pal, U. Bhatt, A. Weller, S. Dooley, M. Goldblum, and A. G. Wilson. Large language models must be taught to know what they don\u2019t know. arXiv preprint arXiv:2406.08391, 2024.   \n[46] A. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[47] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.   \n[48] R. Krishnan, P. Esposito, and M. Subedar. Bayesian-torch: Bayesian neural network layers for uncertainty estimation. https://github.com/IntelLabs/bayesian-torch, Jan. 2022.   \n[49] R. Krishnan, M. Subedar, and O. Tickoo. Specifying weight priors in bayesian deep neural networks with empirical bayes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4477\u20134484, 2020.   \n[50] L. Kuhn, Y. Gal, and S. Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.   \n[51] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.   \n[52] Y. LeCun. Une procedure d\u2019apprentissage ponr reseau a seuil asymetrique. Proceedings of Cognitiva 85, pages 599\u2013604, 1985.   \n[53] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[54] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, 2021.   \n[55] C. Li, H. Farkhoor, R. Liu, and J. Yosinski. Measuring the intrinsic dimension of objective landscapes. arXiv preprint arXiv:1804.08838, 2018.   \n[56] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[57] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.   \n[58] J. Liu, Z. Lin, S. Padhy, D. Tran, T. Bedrax Weiss, and B. Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in neural information processing systems, 33:7498\u20137512, 2020.   \n[59] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands, too. AI Open, 2023.   \n[60] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[61] W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, and A. G. Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32, 2019.   \n[62] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565\u2013576, 2021.   \n[63] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, and B. Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft, 2022.   \n[64] L. Mi, H. Wang, Y. Tian, and N. Shavit. Training-free uncertainty estimation for neural networks. In AAAI, 2022.   \n[65] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.   \n[66] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.   \n[67] R. M. Neal and G. E. Hinton. A view of the em algorithm that justifies incremental, sparse, and other variants. In Learning in graphical models, pages 355\u2013368. Springer, 1998.   \n[68] A. Nikitin, J. Kossen, Y. Gal, and P. Marttinen. Kernel language entropy: Fine-grained uncertainty quantification for llms from semantic similarities. arXiv preprint arXiv:2405.20003, 2024.   \n[69] E. Onal, K. Fl\u00f6ge, E. Caldwell, A. Sheverdin, and V. Fortuin. Gaussian stochastic weight averaging for bayesian low-rank adaptation of large language models, 2024.   \n[70] V. M.-H. Ong, D. J. Nott, and M. S. Smith. Gaussian variational approximation with a factor covariance structure. Journal of Computational and Graphical Statistics, 27(3):465\u2013478, 2018.   \n[71] OpenAI. Introducing chatgpt. [online]. available: https://openai.com/blog/chatgpt. 2022.   \n[72] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural computation, 21(3):786\u2013792, 2009.   \n[73] Y. Park and D. Blei. Density uncertainty layers for reliable uncertainty estimation. In International Conference on Artificial Intelligence and Statistics, pages 163\u2013171. PMLR, 2024.   \n[74] E. Parzen. On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33(3):1065 \u2013 1076, 1962.   \n[75] J. Pfeiffer, A. Kamath, A. R\u00fcckl\u00e9, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487\u2013503, 2021.   \n[76] M. T. Pilehvar and J. Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[77] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[78] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[79] M. Rosenblatt. Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics, 27(3):832 \u2013 837, 1956.   \n[80] A. R\u00fcckl\u00e9, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapterdrop: On the efficiency of adapters in transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7930\u20137946, 2021.   \n[81] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by backpropagating errors. nature, 323(6088):533\u2013536, 1986.   \n[82] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99\u2013106, aug 2021.   \n[83] J. C. Schoeman, C. E. van Daalen, and J. A. du Preez. Degenerate gaussian factors for probabilistic inference. International Journal of Approximate Reasoning, 143:159\u2013191, 2022.   \n[84] H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, Z. Wang, S. Ebrahimi, and H. Wang. Continual learning of large language models: A comprehensive survey. arXiv preprint arXiv:2404.16789, 2024.   \n[85] E. Stengel-Eskin and B. Van Durme. Calibrated interpretation: Confidence estimation in semantic parsing. Transactions of the Association for Computational Linguistics, 11:1213\u2013 1231, 2023.   \n[86] L. S. Tan and D. J. Nott. Gaussian variational approximation with sparse precision matrices. Statistics and Computing, 28:259\u2013275, 2018.   \n[87] K. Tian, E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn, and C. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5433\u20135442, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[88] L. Tierney and J. B. Kadane. Accurate approximations for posterior moments and marginal densities. Journal of the american statistical association, 81(393):82\u201386, 1986.   \n[89] M. Titsias and M. L\u00e1zaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1971\u2013 1979, Bejing, China, 22\u201324 Jun 2014. PMLR.   \n[90] M. Tomczak, S. Swaroop, and R. Turner. Efficient low rank gaussian variational inference for neural networks. Advances in Neural Information Processing Systems, 33:4610\u20134622, 2020. [91] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [92] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[93] D. Tran, M. Dusenberry, M. Van Der Wilk, and D. Hafner. Bayesian layers: A module for neural network uncertainty. Advances in neural information processing systems, 32, 2019.   \n[94] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5039\u20135059, 2022. [95] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. SuperGLUE: a stickier benchmark for general-purpose language understanding systems. Curran Associates Inc., Red Hook, NY, USA, 2019. [96] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In T. Linzen, G. Chrupa\u0142a, and A. Alishahi, editors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, Nov. 2018. Association for Computational Linguistics.   \n[97] H. Wang, C. Mao, H. He, M. Zhao, T. S. Jaakkola, and D. Katabi. Bidirectional inference networks: A class of deep bayesian networks for health profliing. In AAAI, volume 33, pages 766\u2013773, 2019.   \n[98] H. Wang, X. Shi, and D.-Y. Yeung. Natural-parameter networks: A class of probabilistic neural networks. Advances in neural information processing systems, 29, 2016.   \n[99] H. Wang, S. Tan, Z. Hong, D. Zhang, and H. Wang. Variational language concepts for interpreting foundation language models. In EMNLP, 2024.   \n[100] H. Wang, S. Tan, and H. Wang. Probabilistic conceptual explainers: Towards trustworthy conceptual explanations for vision foundation models. In ICML, 2024.   \n[101] H. Wang and D.-Y. Yeung. Towards bayesian deep learning: A framework and some existing methods. TDKE, 28(12):3395\u20133408, 2016.   \n[102] H. Wang and D.-Y. Yeung. A survey on bayesian deep learning. ACM computing surveys (csur), 53(5):1\u201337, 2020.   \n[103] X. Wang, L. Aitchison, and M. Rudolph. Lora ensembles for large language model fine-tuning, 2023.   \n[104] A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.   \n[105] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.   \n[106] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[107] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm from language models, 2021.   \n[108] Y. Wen, P. Vicol, J. Ba, D. Tran, and R. Grosse. Flipout: Efficient pseudo-independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018.   \n[109] J. G. Wiese, L. Wimmer, T. Papamarkou, B. Bischl, S. G\u00fcnnemann, and D. R\u00fcgamer. Towards efficient mcmc sampling in bayesian neural networks by exploiting symmetry. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 459\u2013474. Springer, 2023.   \n[110] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generalization, 2022.   \n[111] M. Xiong, Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.   \n[112] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and F. Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9514\u20139528, 2021.   \n[113] Z. Xu, G.-H. Lee, Y. Wang, H. Wang, et al. Graph-relational domain adaptation. In ICLR, 2022.   \n[114] B. Xue, J. Yu, J. Xu, S. Liu, S. Hu, Z. Ye, M. Geng, X. Liu, and H. Meng. Bayesian transformer language models for speech recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7378\u20137382. IEEE, 2021.   \n[115] Y. A. Yadkori, I. Kuzborskij, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. To believe or not to believe your llm. arXiv preprint arXiv:2406.02543, 2024.   \n[116] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models. arXiv preprint arXiv:2308.13111, 2023.   \n[117] J. S. Yedidia, W. Freeman, and Y. Weiss. Generalized belief propagation. Advances in neural information processing systems, 13, 2000.   \n[118] Z. Yin, Q. Sun, Q. Guo, J. Wu, X. Qiu, and X. Huang. Do large language models know what they don\u2019t know? arXiv preprint arXiv:2305.18153, 2023.   \n[119] E. B. Zaken, Y. Goldberg, and S. Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1\u20139, 2022.   \n[120] H. Zhang, S. Diao, Y. Lin, Y. R. Fung, Q. Lian, X. Wang, Y. Chen, H. Ji, and T. Zhang. R-tuning: Teaching large language models to refuse unknown questions. arXiv preprint arXiv:2311.09677, 2023.   \n[121] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, and J. Malik. Side-tuning: a baseline for network adaptation via additive side networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 698\u2013714. Springer, 2020.   \n[122] R. Zhang, C. Li, J. Zhang, C. Chen, and A. G. Wilson. Cyclical stochastic gradient mcmc for bayesian deep learning. arXiv preprint arXiv:1902.03932, 2019.   \n[123] S. Zhang, X. Fan, B. Chen, and M. Zhou. Bayesian attention belief networks. In International Conference on Machine Learning, pages 12413\u201312426. PMLR, 2021.   \n[124] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch\u00fctze. Masking as an efficient alternative to finetuning for pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2226\u20132241, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Appendix A, we present the proofs for the theorems in the main body of our paper. In Appendix B, we introduce the experimental settings, including evaluation metrics and training schemes. Finally, in Appendix C, we present supplementary empirical results including the experiments on another language model and analysis of the space and time cost of our algorithm. ", "page_idx": 18}, {"type": "text", "text": "A Proof of Theorems and Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we first present the proof of the two main theorems (Theorem 3.1 and Theorem 3.2) in Appendix A.1. Next, we show how a analysis on our design of parameterization in Appendix A.2. Finally, we provide a detailed derivation of the LoRA Flipout in Appendix A.3. ", "page_idx": 18}, {"type": "text", "text": "A.1 Proof of Main Theorems ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 3.1 (Variational Distribution of the Full-Weight Matrix in BLoB). With the pre-trained weight matrix $W_{0}\\,\\in\\,\\mathbb{R}^{m\\times n}$ and the low-rank weight update matrix $\\boldsymbol{B}\\,\\in\\,\\mathbb{R}^{m\\times r}$ , suppose that the variational distribution of the other low-rank update matrix $\\boldsymbol{A}\\,\\in\\,\\mathbb{R}^{r\\times n}$ is Gaussian with $\\begin{array}{r}{q(A|\\theta=\\{M,\\Omega\\})=\\prod_{i j}\\mathcal{N}(A_{i j}|M_{i j},\\Omega_{i j}^{2})}\\end{array}$ , where $\\b{M}=[M_{i j}]\\in\\mathbb{R}^{r\\times n}$ and $\\Omega=[\\Omega_{i j}]\\in\\mathbb{R}^{r\\times n}$ are its mean and standard deviation, respectively. The equivalent variational distribution defined on the full weight matrix $W$ as in Eqn. 3 is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathrm{vec}(W)|B,\\theta)=\\mathcal{N}(\\mathrm{vec}(W)|\\mu_{q},\\Sigma_{q}),}\\\\ &{\\quad\\quad w h e r e\\quad\\mu_{q}=\\mathrm{vec}(W_{0}+B M),}\\\\ &{\\quad\\quad\\quad\\quad\\Sigma_{q}=[I_{n}\\otimes B]\\cdot[\\mathrm{diag}(\\mathrm{vec}(\\Omega)^{2})]\\cdot[I_{n}\\otimes B^{\\top}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We begin by calculating the mean value of $q$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{q}=\\mathrm{vec}(\\mathbb{E}[W_{0}+B A])}\\\\ {=\\mathrm{vec}(\\pmb{W}_{0}+\\pmb{B}\\mathbb{E}[A])}\\\\ {=\\mathrm{vec}(\\pmb{W}_{0}+\\pmb{B}M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose the deterministic matrix $\\textbf{\\emph{B}}=~\\left[b_{1},b_{2},\\cdot\\cdot\\cdot~,b_{r}\\right]~~\\in~~\\mathbb{R}^{m\\times r}$ , random matrix $\\textrm{\\textbf{A}}=$ $[\\bar{\\pmb{a}_{1}},\\bar{\\pmb{a}_{2}},\\cdot,\\pmb{a}_{r}]^{\\top}\\,\\in\\,\\mathbb{R}^{r\\times n}$ , with its underlying parameters of mean and standard deviation defined likewise $M\\in\\mathbb{R}^{r\\times n}$ and $\\Omega\\in\\mathbb{R}^{r\\times n}$ . We have $\\begin{array}{r}{\\mathbf{\\dot{W}}=B A=\\sum_{i=1}^{r}\\pmb{b}_{i}\\cdot\\pmb{a}_{i}^{\\top}}\\end{array}$ . We then rewrite $\\mathrm{vec}(W)$ in the form of Kronecker product $\\otimes$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{vec}(W)=\\operatorname{vec}(\\sum_{i=1}^{r}b_{i}\\cdot a_{i}^{\\top})=\\sum_{i=1}^{r}(a_{i}\\otimes b_{i})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We then calculate the covariance matrix $\\Sigma_{q}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{q}=\\mathrm{cor~[vec(W),vec(W)]}=\\mathrm{cor~}[\\displaystyle\\sum_{i=1}^{r}(a_{i}\\otimes b_{i}),\\displaystyle\\sum_{i=1}^{r}(a_{i}\\otimes b_{i})]}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\mathrm{cor}\\big[a_{i}\\otimes b_{i},a_{i}\\otimes b_{i}\\big]+\\sum_{i\\neq j}\\mathrm{cor}\\big[a_{i}\\otimes b_{i},a_{j}\\otimes b_{j}\\big]}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\Big\\{\\mathbb{E}_{a_{i}}\\big[(a_{i}\\otimes b_{i})(a_{i}\\otimes b_{i})^{\\top}\\big]-\\mathbb{E}_{a_{i}}\\big[(a_{i}\\otimes b_{i})\\big]\\mathbb{E}_{a_{i}}\\big[(a_{i}\\otimes b_{i})^{\\top}\\big]\\Big\\}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\Big\\{\\mathbb{E}_{a_{i}}\\big[(a_{i}a_{i}^{\\top})\\big]\\otimes(b_{i}b_{i}^{\\top})-(\\mathbb{E}_{a_{i}}\\big[a_{i}|\\mathbb{E}_{a_{i}}\\big[a_{i}\\big]^{\\top})\\otimes(b_{i}b_{i}^{\\top})\\Big\\}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\mathrm{diag}(\\sigma_{i}^{2})\\otimes(b_{i}b_{i}^{\\top})}\\\\ &{\\quad=\\displaystyle[I_{\\mathrm{\\Huge~\\otimes~B}}]\\big\\{\\mathrm{diag}(\\vee(\\mathbf{e}(\\mathbf{e}(\\mathbf{e}\\mathbf{E})))\\big]\\cdot[I_{\\mathrm{\\Hat{e}}}\\otimes B_{1}^{\\top}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "completing the proof. ", "page_idx": 18}, {"type": "text", "text": "It is crucial to note here, the final covariance matrix of $q\\bigl(\\mathrm{vec}(W)\\bigr)$ follows a block-diagonal structure, which will be further utilized for the proof of Theorem 3.2. Defining $\\begin{array}{r}{\\Sigma_{i}=\\mathrm{diag}(\\Omega_{i\\cdot}^{2})}\\end{array}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\pmb{\\Sigma}}_{q}=\\left[\\begin{array}{c c c c}{{\\pmb{B}\\Sigma_{1}\\pmb{B}^{\\top}}}&{{}}&{{}}&{}\\\\ {{}}&{{\\cdot\\cdot}}&{{}}&{{}}\\\\ {{}}&{{}}&{{\\pmb{B}\\Sigma_{n}\\pmb{B}^{\\top}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Another important fact about $\\Sigma_{q}$ is its singularity. It can be seen directly as we consider the rank of any one of the block matrix $B\\bar{\\Sigma_{i}}B^{\\top}\\in\\mathbb{R}^{m\\times m},\\forall i\\in[n]$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{r}(B\\Sigma B^{\\top})\\leq\\operatorname*{min}\\{\\mathrm{r}(B),\\mathrm{r}(\\Sigma_{i}),\\mathrm{r}(B^{\\top})\\}\\leq r<m,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $r$ is the rank of LoRA, strictly smaller than the output dimension of $m$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 3.2 (Efficient Computation of Full-Weight KL Divergence). Suppose the pre-trained weights $W_{0}$ , update matrix $_B$ , and the variational distribution $q(A|\\pmb\\theta)$ are defined as in Theorem 3.1, and the prior distribution of the full-weight matrix $P\\big(\\mathrm{vec}(W)\\big)$ is defined as Eqn. 9. Consider the Gaussian prior distribution $\\begin{array}{r}{P(\\dot{\\bf A})=\\prod_{i j}\\mathcal{N}(A_{i j}|0,\\sigma_{p}^{2}),}\\end{array}$ ; we then have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}[q(\\mathrm{vec}(W)|B,\\theta)\\|P(\\mathrm{vec}(W))]=\\mathrm{KL}[q(A|\\theta)\\|P(A)],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "if $\\widetilde{\\pmb{R}}=[\\sigma_{p}\\pmb{I}_{n}\\otimes\\pmb{R}]$ , where $\\boldsymbol{R}$ satisfies $R R^{\\top}=B B^{\\top}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. We start by assuming the low-rank structure of the prior $P(\\mathrm{vec}(W))$ , and then reveal the conditions reaching to our final conclusion step by step. ", "page_idx": 19}, {"type": "text", "text": "Typically, for two Gaussian distributions $q$ and $p$ whose covariance matrices $\\pmb{\\Sigma}_{q}\\ \\in\\ \\mathbb{R}^{d\\times d}$ and $\\pmb{\\Sigma}_{p}\\,\\in\\,\\mathbb{R}^{d\\times d}$ are both full-rank, and their means as $\\pmb{\\mu}_{q}\\,\\in\\,\\mathbb{R}^{d}$ and $\\pmb{\\mu_{p}}\\,\\in\\,\\mathbb{R}^{d}$ , we have their KLdivergence as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}[q\\|p]=\\frac{1}{2}\\left[\\log\\frac{|\\Sigma_{p}|}{|\\Sigma_{q}|}-d+\\mathrm{tr}(\\Sigma_{p}^{-1}\\Sigma_{q})+(\\mu_{q}-\\mu_{p})^{\\top}\\Sigma_{p}^{-1}(\\mu_{q}-\\mu_{p})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The singularity of the covariance matrices of $P(\\mathrm{vec}(W))$ and $q\\bigl(\\mathrm{vec}(W)\\bigr)$ , i.e., $|\\Sigma_{q}|=|\\Sigma_{p}|=0$ , can cause issues when computing the KL-divergence as it includes the log-determinant term. Therefore in this proof, we consider the alternative of the covariance matrices, where an extremely small diagonal elements are added. ", "page_idx": 19}, {"type": "text", "text": "For the prior distribution, following the alternative form of a degenerate Gaussian [83], as suggested in Eqn. 9, we assume ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{P(\\mathrm{vec}(W))=\\mathcal{N}(W_{0},\\Sigma_{p}),}&{{}}&{}\\\\ {\\mathrm{where}\\quad}&{{}\\Sigma_{p}=\\lambda I+\\widetilde{R}\\widetilde{R}^{\\top},\\quad(\\lambda\\to0^{+}).}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By default, we assume that the low-rank tall matrix $\\tilde{R}\\,\\in\\,\\mathbb{R}^{(m n)\\times r^{\\prime}}$ has the full column rank $r^{\\prime}$ Otherwise if $\\mathrm{r}(\\widetilde{R})=r^{\\prime\\prime}<r^{\\prime}$ , then we can in effect consider a new matrix component $\\widetilde{R}^{\\prime}\\in\\mathbb{R}^{(m n)\\times r^{\\prime\\prime}}$ that has the sa me rank as $r^{\\prime\\prime}$ , which satisfies our assumption of full column rank. Th erefore, we have the SVD decomposition ofR is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{R}}=\\pmb{U}_{R}\\pmb{D}_{R}\\pmb{V}_{R}^{\\top},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $U_{R}\\in\\mathbb{R}^{(m n)\\times(m n)}$ and $V_{R}\\in\\mathbb{R}^{r^{\\prime}\\times r^{\\prime}}$ are orthonormal, i.e., $U_{R}\\pmb{U}_{R}^{\\top}=\\pmb{U}_{R}^{\\top}\\pmb{U}_{R}=\\pmb{I}_{(m n)}$ and $V_{R}V_{R}^{\\top}=V_{R}^{\\top}V_{R}=I_{r^{\\prime}}$ . $D_{R}$ is a tall matrix where its upper part is diagonal and the lower part is a zero matrix, denoted as $D_{R}=[D_{R}^{*},O]^{\\top}=[\\mathrm{diag}([d_{R_{1}}>0,d_{R_{2}}>0,\\cdots,d_{R_{r^{\\prime}}}>0]),O]^{\\top}$ . ", "page_idx": 19}, {"type": "text", "text": "For the approximate posterior $q(\\mathrm{vec}(W)|B,\\theta)$ , we consider ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathrm{vec}(W)|B,\\theta)=\\mathcal{N}(W_{0}+B M,\\Sigma_{q}),}\\\\ &{\\quad\\mathrm{where}\\qquad\\Sigma_{q}=\\lambda I+\\widetilde{B}\\Sigma\\widetilde{B}^{\\top},\\quad(\\lambda\\to0^{+}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we simplify the notation for the covariance matrix $\\Sigma_{q}$ by defining $\\widetilde{B}=[I_{n}\\!\\otimes\\!B]\\in\\mathbb{R}^{(m n)\\times(m r)}$ and $\\Sigma=\\mathrm{diag}\\bigl(\\mathrm{vec}(\\Omega)^{2}\\bigr)$ . Likewise, we have the SVD-decomposed matrices for $\\tilde{B}$ where they are defined in the same way as Eqn. 29: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{B}=U_{B}D_{B}V_{B}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $U_{B}$ and $V_{B}$ are orthogonal matrices, and $D_{B}\\;=\\;[D_{B}^{*},O]^{\\top}\\;=\\;[\\mathrm{diag}([d_{B_{1}}\\;>\\;0,d_{B_{2}}\\;>\\;$ $0,\\cdot\\cdot\\cdot\\:,d_{B_{m r}}>0]),O]^{\\top}$ . ", "page_idx": 20}, {"type": "text", "text": "First, we calculate the log-determinant part of the KL-divergence. For the log-determinant of the covariance matrix of the prior distribution $\\Sigma_{p}$ , by applying SVD decomposition in Eqn. 29, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\log|\\Sigma_{p}|=\\log|\\lambda I+\\widetilde{R}\\widetilde{R}^{\\top}|=\\log|\\lambda I+U_{R}D_{R}V_{R}^{\\top}V_{R}D_{R}^{\\top}U_{R}^{\\top}|}}\\\\ &{=\\log|U_{R}(\\lambda I+D_{R}D_{R}^{\\top})U_{R}^{\\top}|}\\\\ &{=\\log\\left|U_{R}\\left[\\binom{D_{R}^{\\ast}}{{\\cal O}}^{2}+\\lambda I_{r^{\\prime}}\\qquad O}\\\\ &{\\qquad=\\log|(D_{R}^{\\ast})^{2}+\\lambda I_{r^{\\prime}}|+\\log|\\lambda I_{m n-r^{\\prime}}|\\right.}\\\\ &{\\quad\\left.=(m n-r^{\\prime})\\log\\lambda+\\displaystyle\\sum_{i=1}^{r^{\\prime}}\\log(d_{R_{i}}^{2}+\\lambda).}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Following (almost) the same idea, we now have the log-determinant of the variational distribution\u2019s covariance $\\Sigma_{q}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log|\\Sigma_{q}|=\\log|\\lambda I+\\widetilde{B}\\widetilde{B}^{\\top}|=\\log\\left|\\boldsymbol{D}_{B}^{*}\\boldsymbol{V}_{B}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{V}_{B}\\boldsymbol{D}_{B}^{*}+\\lambda I_{m r}\\qquad\\ O}\\\\ &{\\qquad\\qquad=(m n-m r)\\log\\lambda+2\\log|\\boldsymbol{D}_{B}^{*}|+\\log|\\boldsymbol{V}_{B}^{\\top}\\boldsymbol{\\Sigma}\\boldsymbol{V}_{B}+\\lambda(\\boldsymbol{D}_{B}^{*})^{-2}|}\\\\ &{\\qquad=(m n-m r)\\log\\lambda+2\\displaystyle\\sum_{i=1}^{m r}\\log d_{B_{i}}+\\log|\\boldsymbol{\\Sigma}|+\\log|I+\\lambda\\boldsymbol{V}_{B}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{V}_{B}(\\boldsymbol{D}_{B}^{*})^{-2}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We make two observations when $\\lambda\\to0^{+}$ : (i) compare the terms that contain $\\log\\lambda$ on both sides, to make sure the log-determinant in the divergence term bounded, we have to set $r^{\\prime}=m r$ ; (ii) the last term in Eqn. 39, $\\begin{array}{r}{\\mathrm{\\check{log}}\\left|I+\\lambda V_{B}^{\\top}\\Sigma^{-1}V_{B}(\\check{\\boldsymbol D_{B}^{*}})^{-2}\\right|=\\mathrm{log}\\left|I\\right|=0}\\end{array}$ . Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log\\frac{|\\Sigma_{p}|}{|\\Sigma_{q}|}=\\sum_{i=1}^{m r}\\log\\frac{d_{R_{i}}^{2}+\\lambda}{d_{B_{i}}^{2}}-\\log|\\Sigma|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next we calculate $\\mathrm{tr}(\\Sigma_{p}^{-1}\\Sigma_{q})$ in Eqn. 27. Following the same assumptions and notations above, we have the inverse of the covariance of the prior distribution as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\Sigma}_{p}^{-1}=\\pmb{U}_{R}\\left[\\left[(\\pmb{D}_{R}^{*})^{2}+\\lambda\\pmb{I}\\right]^{-1}\\begin{array}{c}{\\pmb{O}}\\\\ {\\lambda^{-1}\\pmb{I}}\\end{array}\\right]\\pmb{U}_{R}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\Sigma_{p}^{-1}\\Sigma_{q})=\\mathrm{tr}(U_{R}\\left[\\overset{\\displaystyle[(D_{R}^{*})^{2}+\\lambda I]^{-1}}{O}\\right.\\left.\\lambda^{-1}I\\right]U_{R}^{\\top}U_{B}\\left[\\overset{D_{B}^{*}V_{B}^{\\top}\\Sigma V_{B}D_{B}^{*}+\\lambda I}{O}\\right.\\left.\\lambda I\\right]U_{B}^{\\top}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By using the condition $R R^{\\top}=B B^{\\top}$ and $\\widetilde{\\pmb{R}}=[\\sigma_{p}\\pmb{I}_{n}\\otimes\\pmb{R}]$ defined in Theorem 3.2, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{R}\\tilde{R}^{\\top}=\\sigma_{p}^{2}\\tilde{B}\\tilde{B}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and there exists an orthogonal matrix $\\pmb{P}\\in\\mathbb{R}^{(m r)\\times(m r)}$ , such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{R}=\\sigma_{p}\\widetilde{B}P.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The SVD decomposition on $\\tilde{R}$ can then be formulated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\pmb{R}}=\\sigma_{p}\\pmb{U}_{B}\\pmb{D}_{B}\\pmb{V}_{B}^{\\top}\\pmb{P}}\\\\ &{\\quad=(\\pmb{U}_{R}=\\pmb{U}_{B})(\\pmb{D}_{R}=\\sigma_{p}\\pmb{D}_{B})(\\pmb{V}_{R}=\\pmb{V}_{B}^{\\top}\\pmb{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting ${\\cal U}_{R},D_{R},V_{R}$ back to Eqn. 42 and applying $\\lambda\\to0^{+}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}(\\Sigma_{p}^{-1}\\Sigma_{q})=\\mathrm{tr}(I_{m n-n r})+\\mathrm{tr}([(\\sigma_{p}D^{*})^{2}+\\lambda I]^{-1}[D_{B}^{*}V_{B}^{\\top}\\Sigma V_{B}D_{B}^{*}])}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =(m n-n r)+\\sigma_{p}^{-2}\\operatorname{tr}(V_{B}^{\\top}\\Sigma V_{B})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =(m n-n r)+\\sigma_{p}^{-2}\\operatorname{tr}(\\Sigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the quadratic term in Eqn. 27, the pre-trained weights $W_{0}$ cancel out, and we can calculate it as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{vec}(B M)^{\\top}\\Sigma_{p}^{-1}\\mathrm{vec}(B M)}\\\\ &{=[M_{1}^{\\top}B^{\\top},\\cdots,M_{1}^{\\top}B^{\\top}]\\left[\\begin{array}{l l l}{[B\\Sigma_{1}B^{\\top})^{-1}}&&\\\\ &{\\ddots}&\\\\ &&{(B\\Sigma_{n}B^{\\top})^{-1}}\\end{array}\\right]\\left[\\begin{array}{l}{B M_{1}\\cdots}\\\\ {\\vdots}\\\\ {B M_{n}}\\\\ {\\vdots}\\\\ {B M_{n}}\\end{array}\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}M_{1}^{\\top}B^{\\top}(B\\Sigma_{i}B^{\\top})^{-1}B M_{i}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}M_{1}^{\\top}(V_{p}^{\\top}\\frac{d_{1}}{(\\partial_{1}\\cdots)^{\\top}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\ddots}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{d_{2}}{\\partial_{p}^{-1}d_{10\\alpha_{n}}-\\lambda})V_{p}^{\\top}\\Bigg\\}M_{1}}\\\\ &{=\\displaystyle\\frac{1}{\\sigma_{p}^{2}}\\sum_{i=1}^{n}M_{1}^{\\top}M_{2}}\\\\ &{=\\displaystyle\\frac{1}{\\sigma_{p}^{2}}[M]_{2}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, proof is completed by combining Eqn. 40, Eqn. 49, and Eqn. 55. ", "page_idx": 21}, {"type": "text", "text": "A.2 Analysis on BLoB Parameterization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "General Analysis on Parameterization. Consider a path of parameterization for a single variable: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\rho\\to\\sigma=f(\\rho)\\to{\\mathcal{L}}=l(\\sigma),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\rho$ is the real parameter we perform update on, $f$ is our parameterization choice for the variable $\\sigma$ , and $l$ represents the loss function we aim to minimize. When comparing two different parameterization methods, we consider the same initial conditions of $\\sigma=\\sigma_{0}$ , and we assume the same step size $\\eta$ on the real parameter $\\rho$ . To show the influence of the choice of parameterization, we calculate the decrease of the loss value by performing one step of gradient descent. First, by the chain rule, the gradient w.r.t. $\\rho_{0}$ is calculated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left.\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\rho}\\right|_{\\rho_{0}}=\\left.\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\sigma}\\right|_{\\sigma_{0}}\\cdot\\left.\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}\\rho}\\right|_{\\rho_{0}}=l^{\\prime}(\\sigma_{0})\\cdot f^{\\prime}(\\rho_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "After one step of the gradient descent, we have $\\rho_{1}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\rho_{1}=\\rho_{0}-\\eta\\cdot l^{\\prime}(\\sigma_{0})\\cdot f^{\\prime}(\\rho_{0}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the loss value decreased at $\\rho_{1}$ can be approximated by the first-order Taylor expansion, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{L}=l(f(\\rho_{0}))-l(f(\\rho_{1}))}\\\\ &{\\quad=l(f(\\rho_{0}))-[l(f(\\rho_{0}-\\eta\\cdot l^{\\prime}(\\sigma_{0})\\cdot f^{\\prime}(\\rho_{0})))]}\\\\ &{\\quad\\approx l(f(\\rho_{0}))-[l(f(\\rho_{0})-\\eta\\cdot l^{\\prime}(\\sigma_{0})\\cdot(f^{\\prime}(\\rho_{0}))^{2})]}\\\\ &{\\quad\\approx\\eta\\cdot(l^{\\prime}(\\sigma_{0}))^{2}\\cdot(f^{\\prime}(\\rho_{0}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since the initialization of the different parameterization variable $\\rho_{0}$ is set to ensure the same initial condition of $\\sigma_{0}$ for different $f\\mathrm{s}$ , we can see that the amount the loss decreases by after one step of update is proportional to the squared gradient $\\Delta\\mathcal{L}\\propto(l^{\\prime}(\\sigma_{0}))^{2}\\cdot(f^{\\prime}(\\rho))^{2}=(\\mathrm{d}\\mathcal{L}/\\dot{\\mathrm{d}}\\rho)^{2}$ . ", "page_idx": 21}, {"type": "text", "text": "Parameterization with $\\log(1+\\exp(\\cdot))$ or $(\\cdot)^{2}\\mathfrak{P}$ Previous VBNs [11, 49] typically use a softplus function $\\sigma_{q}=\\log(1+\\exp(\\dot{\\rho}))$ to parameterize the standard deviation. For a single element $\\rho$ , the derivative of the closed-form solution of the KL divergence in Eqn. 11 is calculated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}\\,\\mathrm{KL}}{\\mathrm{d}\\rho}=-\\frac{e^{\\rho}}{(1+e^{\\rho})\\log(1+e^{\\rho})}+\\frac{e^{\\rho}\\log(1+e^{\\rho})}{\\sigma_{p}^{2}(1+e^{\\rho})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Due to the fact that $\\sigma_{q}$ is typically initialized to a small value close to 0 to ensure stable optimization of the likelihood cost term (e.g., $1e-3)$ , and in order to ensure that the model obtains good uncertainty, ", "page_idx": 21}, {"type": "text", "text": "$\\sigma_{p}$ is usually set to a larger value close to 1 (e.g., $1e-1)$ . In this case, the derivative of $\\rho$ in Eqn. 63 is almost always a constant $-1$ , which, based on our previous analysis, leads to slow convergence for large $\\sigma_{p}$ values when $\\sigma_{q}$ is small. ", "page_idx": 22}, {"type": "text", "text": "Therefore, we parameterize $\\sigma_{q}$ with quadratic function: $\\sigma_{q}=\\rho^{2}$ . In this case, the derivative of the KL divergence with respect to $\\rho$ in Eqn. 11 becomes: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}\\,\\mathrm{KL}}{\\mathrm{d}\\rho}=-\\frac{2}{\\rho}+\\frac{2\\rho^{3}}{\\sigma_{p}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Under the same initialization conditions, the derivative in Eqn. 64 is approximately of the order of $\\rho^{-1}$ , leading to rapid convergence towards larger $\\sigma_{p}$ values when $\\sigma_{q}$ is small. Building on this, we use SGD without momentum to optimize the complexity loss term, thereby achieving the natural convergence of $\\sigma_{q}$ . ", "page_idx": 22}, {"type": "text", "text": "Visualization. To visually demonstrate the differences in the convergence of KL divergence during training with these two parameterizations, we set $\\sigma_{p}=1$ and employ gradient descent to optimize the KL divergence. As introduced in B.1, in practical mini-batch gradient descent, the KL divergence is weighted by 1/#mini-batches. Therefore, assuming there are 100 mini-batches, the learning rate is set to 0.01, which translates to an actual learning rate of $1e\\mathrm{~-~}4$ for $\\rho$ . We initialize $\\sigma_{q}\\,=\\,0.01$ for both parameterizations. The growth of $\\sigma_{q}$ during the KL training, for $\\sigma_{q}=\\log(1+e^{\\rho})$ and $\\sigma_{q}=\\rho^{2}$ is shown in Fig. 2. In the same setting, the softplus parameterization takes nearly 100,000 gradient steps to converge, while the square parameterization takes only about 5,000 gradient steps. This modification makes it suitable for our fine-tuning setup, where the number of gradient steps is relatively small. ", "page_idx": 22}, {"type": "image", "img_path": "MaDykgj4Ru/tmp/5bb3b3e6eaa2b654da67deb42d52f9a5e2e62e988198bb4e58f6a41d5c740e24.jpg", "img_caption": ["Figure 2: The growth curve of $\\sigma_{q}\\,=\\,\\log(1+e^{\\rho})$ and $\\sigma_{q}\\,=\\,\\rho^{2}$ during the optimization of KL divergence (without data likelihood). The number of gradient steps (5000) is marked with the red line. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A.3 Deriving Flipout for BLoB ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For the $i$ -th input vector $h_{i}$ in a mini-batch, we randomly sample two filpping vector $s\\in\\{-1,+1\\}^{n}$ and $t\\in\\{-1,+1\\}^{r}$ . Denoting $\\pmb{A}$ as the weight matrix sampled from posterior distribution, and $\\Delta{A}$ as the batched noise for sampling $\\pmb{A}$ , the output vector $z_{i}$ after applying flipout is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{z_{i}=W h_{i}}}\\\\ &{=W_{0}h_{i}+B A h_{i}}\\\\ &{=W_{0}h_{i}+B(M+\\Delta A)h_{i}}\\\\ &{=W_{0}h_{i}+B M h+B(\\widehat{A}\\circ t_{i}s_{i}^{\\top})h_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, for a mini-batch input matrix $\\b{H}\\in\\mathbb{R}^{n\\times b}$ with batch size $b$ , we randomly sample two low-rank filpping matrices $S\\overset{\\cdot}{\\in}\\{-1,+1\\}^{n\\times b}$ and $\\pmb{T}\\in\\{-1,+1\\}^{b\\times r}$ . The batched output matrix $_{z}$ after applying flipout is then: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{Z}=W_{0}\\boldsymbol{H}+\\boldsymbol{B}(\\boldsymbol{M}\\boldsymbol{H}+\\left[\\widehat{\\boldsymbol{A}}(\\boldsymbol{H}\\circ\\boldsymbol{S})\\right]\\circ\\boldsymbol{T})}\\\\ &{\\quad=W_{0}\\boldsymbol{H}+\\boldsymbol{B}(\\boldsymbol{M}\\boldsymbol{H}+[(\\boldsymbol{E}\\circ\\Omega)(\\boldsymbol{H}\\circ\\boldsymbol{S})]\\circ\\boldsymbol{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/de4ecf8e79780552d248f9afbb151f8dd6715af4b513067af243a72a25bef95f.jpg", "table_caption": ["Table 3: Hyperparameters of LoRA "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/8e88fbfbeec0445f1caee954d594005b9991e5c16362285a109538ff8f9835e5.jpg", "table_caption": ["Table 4: BLoB-Specific Hyperparameters "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we first introduce the implementation details of BLoB in Appendix B.1, including the KL Re-weighting scheme, initialization of the parameters, and learning scheduling, etc. Next, we introduce the two evaluation metrics for uncertainty estimation in Appendix B.2. Finally, we present some statistics of the adopted datasets in Appendix B.3. ", "page_idx": 23}, {"type": "text", "text": "B.1 Implementation of BLoB ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "KL Re-weighting. In mini-batch SGD, the training data $\\mathcal{D}$ is randomly divided into $M$ equally sized subsets: $\\mathcal{D}_{1},\\mathcal{D}_{2},\\ldots,\\mathcal{D}_{M}$ . For mini-batch $i=1,2,\\dots,M$ , the cost function is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mathcal{D}_{i},\\theta)=-\\mathbb{E}_{q(W|\\theta)}[\\log P(\\mathcal{D}_{i}|W)]+\\lambda_{i}\\,\\mathrm{KL}[q(W|\\theta)\\parallel P(W)],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\lambda_{i}\\in[0,1]$ and $\\textstyle\\sum_{i=1}^{M}\\lambda_{i}=1$ . There are various approaches for controlling the weight of $\\mathrm{KL}$ divergence. [30] utili zes $\\lambda_{i}={^1\\mathord{\\left/{\\vphantom{^1\\mathrm{M}}}\\right.\\kern-\\nulldelimiterspace}M}$ , while [11] adopts $\\lambda_{i}\\,=\\,^{2^{M-i}\\!}/2^{M}\\!-\\!1$ . In fine-tuning tasks, we found that using a scheduler with $\\lambda_{i}=2^{i}/2^{\\bar{M}}\\!-\\!1$ performs well. This allows the model to find good fits to the data points within the early stages and then optimize the complexity cost in later stages. ", "page_idx": 23}, {"type": "text", "text": "In multiple epochs of mini-batch SGD, larger datasets require more iterations to complete one epoch, resulting in delayed convergence of the complexity cost. To enhance the stability of BLoB\u2019s performance across datasets with varying sizes, we pseudo-rescaled the size of the training dataset to make smaller datasets slightly larger and larger datasets slightly smaller. For the portions of the dataset that required expansion, we incorporated additional mini-batches from subsequent epochs. Conversely, for the datasets needing reduction, we deferred the excess mini-batches to subsequent epochs. We denote the size of original dataset as $L_{0}$ , The rescaled dataset size is: ", "page_idx": 23}, {"type": "equation", "text": "$$\nL^{*}=100\\cdot L_{0}^{\\frac{\\pi}{\\gamma}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\gamma$ is a coefficient used to control the scaling magnitude, and we set it to 8 in all experiments. ", "page_idx": 23}, {"type": "text", "text": "The pseudo-rescaling does not affect the likelihood cost in practical mini-batch gradient descent. In fact, it only changes the warm-up period in KL reweighting from $M$ to $L^{*}/1$ batch size, thereby facilitating more consistent optimization of the complexity cost across datasets of different sizes. ", "page_idx": 23}, {"type": "text", "text": "Additional Details. We initialize standard deviation parameterization matrix $\\boldsymbol{G}$ by element-wise sampling from a uniform distribution with a range of $[\\frac{\\epsilon}{\\sqrt{2}},\\epsilon]$ , while keeping the remaining initialization settings consistent with LoRA. To maintain consistency, we use the same learning rate scheduler and warmup ratio for the optimizer of the KL term as we do for the likelihood term. We sample only once during the training process. During inference, we sample $N$ times, then take the average of the logits obtained after passing through the softmax function. Detailed hyperparameter settings are provided in the Table 4. Table 3 provides the hyperparameters for fine-tuning with LoRA shared with other baselines. Our experiments on Llama2-7B were conducted using 2 NVIDIA RTX A5000 GPUs for parallel training, while experiments on RoBERTa-base were conducted using 4 NVIDIA RTX A5000 GPUs for parallel training. ", "page_idx": 23}, {"type": "text", "text": "B.2 Evaluation Metrics for Uncertainty Estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Negative Log-Likelihood (NLL) and Expected Calibration Error (ECE [31]) are two prevalent metrics for assessing uncertainty estimation. NLL calculates the sum of the negative expected log probability of predicting the actual label. Suppose this predicted probability is given by the model $P_{\\theta}$ , and we have a test dataset $\\{{\\pmb x}_{n},y_{n}\\}_{n=1}^{N}$ of size $N$ . Then the NLL measured on this dataset is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{NLL}=\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}-\\log P_{\\pmb{\\theta}}(y_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This metric prefers models that assign higher probabilities to correct labels. If the model exhibits overconfident in an incorrect prediction, the probability assigned to the correct label will be diminished, thereby increasing the NLL. ", "page_idx": 24}, {"type": "text", "text": "On the other hand, ECE measures how well the model\u2019s confidence matches its accuracy. This is done by binning the predictions based on their confidence levels and then computing a weighted average of the absolute difference between accuracy and confidence within each bin: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{ECE}=\\sum_{m=1}^{M}\\frac{|B_{m}|}{n}\\left|\\mathrm{acc}(B_{m})-\\mathrm{conf}(B_{m})\\right|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\operatorname{acc}(B_{m})$ and conf $\\left(B_{m}\\right)$ denote the average accuracy and confidence within bin $B_{m}$ , respectively. These are given by: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\textstyle\\operatorname{acc}(B_{m})={\\frac{1}{|B_{m}|}}\\sum_{i\\in B_{m}}\\mathbf{1}({\\widehat{y}}_{i}=y_{i}),\\quad\\operatorname{conf}(B_{m})={\\frac{1}{|B_{m}|}}\\sum_{i\\in B_{m}}P({\\widehat{y}}_{i}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $|B_{m}|$ is the number of samples in bin $m$ . We set $|B_{m}|=15$ across all experiments. ", "page_idx": 24}, {"type": "text", "text": "B.3 Dataset Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 5 summarizes the size of the training set and the number of labels for each dataset. Table 6 summarizes the prompt templates used for common sense reasoning tasks. ", "page_idx": 24}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/a7febf8e97067fcf2d6f6ff82c9b4a1bc7b0b862a574cc7a6ab86fabdcb83ef7.jpg", "table_caption": ["Table 5: Size of the training set and number of labels for each dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/b9eaaf2526eaab70cb9342131968dc3ce58eb81c49c730c46724ec0e867df1af.jpg", "table_caption": ["Table 6: Prompt templates for common sense reasoning tasks. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C Additional Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section provides additional experimental results omitted from the main body of the paper due to space limitations. First, we present the results of BLoB when applied to RoBERTa, another pre-trained language model, in Appendix C.1. Next, in Appendix C.2, we conduct the ablation ", "page_idx": 24}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/d81122554f51e0817d48129805743a40ec45dc2e2bffb23a1e659b9f8798930f.jpg", "table_caption": ["Table 7: Performance of different methods applied to LoRA on RoBERTa-base pre-trained weights. The evaluation is undertaken on five GLUE [96] and SuperGLUE [95] tasks, with a shared hyper-parameter setting without using individual validation dataset. \u201c\u2191\u201d and \u201c\u2193\u201d represent that higher and lower values are preferred, respectively. The boldface and underline are used to denote the best and runner-up performance, respectively. The asterisk \u201c\u2217\u201d denotes training failure. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "study on our proposed refinement in BLoB. Then we analyze the memory and training time costs in Appendix C.3. Finally, we provide visualization illustrating our BLoB\u2019s advantage on embedding uncertainty in Appendix C.6. ", "page_idx": 25}, {"type": "text", "text": "C.1 Performance of RoBERTa on In-distribution Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We also evaluate different methods on RoBERTa-base, which has approximately $^1\\!/\\!50$ the parameter count of Llama2-7B. Table 7 shows the results. Compared to MLE, MAP shows minor improvements in NLL and ECE, while MCD, ENS, and LAP enjoy significant improvements. The convergence difficulty observed with the BBB algorithm is further exacerbated on the smaller model, resulting in significant decreases in ACC across all datasets, and even training failures on RTE and WiC. In contrast, our method demonstrates the best or runner-up performance in uncertainty estimation on almost all datasets. Only a slight decrease in ACC is observed on BoolQ and CoLA. We suspect that such decrease is caused by RoBERTa-base\u2019s small model size compared to the large size of these datasets BoolQ and CoLA (i.e., underfitting). Using a larger pretrained model, e.g., Llama2-7B, would potentially address this issue. ", "page_idx": 25}, {"type": "text", "text": "C.2 Ablation Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We perform an ablation study on the Llama2-7B model to showcase the effects of a range of techniques we designed: KL Re-Weighting (RW, Appendix B.1), Re-Parameterization (RP, Sec. 3.3), and Asymmetric Bayesianization (AB, Sec. 3.1). In the scenarios w/o AB, we Bayesianize both matrices, $\\pmb{A}$ and $_B$ . In practice, using identical initialization and prior for the standard deviation matrix $\\pmb{G}$ of the variational distribution on both $\\pmb{A}$ and $_B$ leads to training failures caused by \u201cNaN\u201d loss across all datasets; this is consistent with the findings in Sec. 3.1. As a solution, we introduce a scaled standard deviation matrix $G/100$ on $_B$ to alleviate early-stage fluctuations. Nevertheless, it is important to note that not using AB incurs double the additional memory cost and training time, as described in Appendix C.3. ", "page_idx": 25}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/1e9210f7e7905e351bd848eb0e9df0e8e5bc7f62ca55bc55089b66c6995de95b.jpg", "table_caption": ["Table 8: Ablation study of BLoB, applied to LoRA on Llama2-7B pre-trained weights, where RW, RP, and AB represent our designed techniques of KL Re-Weighting (Appendix B.1), ReParameterization (Sec. 3.3), and Asymmetric Bayesianization (Sec. 3.1), respectively. The evaluation is done following Table 1. We set the number of samples during training $K=1$ and the number of samples during inference $N=10$ across the variants (denoted by \u201c $\\mathbf{\\ddot{\\rho}}$ ) of the BBB [11] and BLoB for fair comparison. We denote by \u201c\u2217\u201d experiments with the scaled standard deviation matrix. The hyphen \u201c\u2212\u201d in the table denotes training failure caused by \u201cNaN\u201d loss. \u201c\u2191\u201d and $\\downarrow^{\\,,}$ indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "As demonstrated in Table 8, BBB w/o AB fails to converge due to the unbounded NaN loss, which cannot be solved by using scaled standard deviation. By introducing KL Re-Weighting, Re-Parameterization, and scaled standard deviation, BLoB w/o AB achieves the runner-up performance and improves accuracy on small datasets. However, BLoB with all techniques achieves the best ECE and NLL with minimal additional computational cost. ", "page_idx": 26}, {"type": "text", "text": "C.3 Additional Results on Memory and Time Efficiency ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "By introducing an additional standard deviation matrix $\\pmb{\\Omega}$ of the same size as the LoRA $\\pmb{A}$ matrix, the number of trainable parameters in BLoB increases by half compared to LoRA. In the case of BLoB w/o Asymmetric Bayesianization (AB), the number of trainable parameters are twice as many as those in LoRA. The calculation of KL divergence and the inclusion of the additional standard deviation matrix in the likelihood loss computation result in additional forward and backward propagation time. We conduct parallel training using two NVIDIA RTX A5000 GPUs to observe the differences in GPU memory cost and training time between BLoB and standard LoRA fine-tuning on the Llama2-7B model. The results are shown in Table 9. BLoB increases memory cost by only about $3\\%$ to $13\\%$ compared to LoRA, with training time increased by about $15\\%$ . ", "page_idx": 26}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/4af2e361a41acf8849a41a728e3f628fd6ba791714dae5c77af5ac4626a687ce.jpg", "table_caption": ["Table 9: A comparison of time and maximum memory cost between standard LoRA and BLoB, during training. The evaluation is based on fine-tuning for 5,000 steps on the Llama2-7B model. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "MaDykgj4Ru/tmp/273ecb5cdcf243aebf768244b85c4f60b3d5e7915fedfb8a4317f9a092123bc8.jpg", "table_caption": ["Table 10: A comparison of time and max memory cost between Standard LoRA, LAP, and BLoB ${\\bf(N=10)}$ ) during inference. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "We further evaluate the inference time and maximum memory usage for standard LoRA, LaplaceLoRA (LAP), and our BLoB, as shown in Table 10. The experiments are conducted on two NVIDIA A100 GPUs. These results show that compared to LAP, our BLoB can achieve comparable or better ECE and NLL with less inference time and less memory usage. Notably, our BLoB\u2019s memory overhead compared to standard LoRA is minimal, while LAP introduces significant memory overhead. ", "page_idx": 27}, {"type": "text", "text": "C.4 Impact of Sample Size on Inference Performance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here we provide a more detailed empirical study on the sample size of BLoB during inference. Specifically, we report the results for different number of samples from $N=1$ to $N=160$ on the WG-S dataset, demonstrating improved uncertainty estimation with increased number of samples, as shown in Fig. 3. ", "page_idx": 27}, {"type": "image", "img_path": "MaDykgj4Ru/tmp/e94193c524ab95a3d1ad95cc90f21e7a85318637b7ca16407fb5e1e5976aa973.jpg", "img_caption": ["Figure 3: Performance of BLoB with Varying Sample Sizes $N$ during Inference. We fine-tune the Llama2-7B model on the WG-S dataset for 5,000 steps, evaluating the model\u2019s performance with different sample sizes, specifically when $N$ is 1, 2, 3, 4, 5, 10, 20, 40, 80, and 160. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C.5 Trade-Off between Accuracy and Calibration Controlled by Gaussian Prior ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The empirical trade-off between accuracy and calibration caused by different model architectures is observed in [85]. By controlling the standard deviation of the prior Gaussian distribution, we observed a similar trade-off between accuracy and calibration. Specifically, we report results for different prior Gaussian standard deviations, ranging from 0.05 to 0.25, while proportionally scaling the learning rate of KL divergence from its original value of 0.01 to values between 0.0025 and 0.0125. This highlights the trade-off between accuracy and calibration, as shown in Fig. 4. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "image", "img_path": "MaDykgj4Ru/tmp/3ca4e47d3b269d62d7444a98d0aab39e63ed3502f7f1def0ea2dbea28a17bd79.jpg", "img_caption": ["Figure 4: Performance of BLoB $\\mathbf{(N=10)}$ ) with Varying Prior Gaussian Standard Deviations $\\sigma_{p}$ . We fine-tune the Llama2-7B model on the WG-S dataset for 5,000 gradient steps, evaluating the model\u2019s performance with different prior Gaussian standard deviations and learning rates of KL divergence. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "C.6 Embedding Uncertainty of BLoB: A Preliminary Visual Study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Estimating the uncertainty of LLMs in the embedding space has recently garnered significant attention in the community [13]. Expressing models\u2019 uncertainty via their generated embeddings can benefit both discriminative (the focus of this paper) and generative models. In this section, we present a preliminary study on uncertainty estimation in the embedding spaces of different models, as illustrated in Fig. 5. We compare BLoB with two baseline models, BBB and MCD, which can generate embedding samples and effectively estimate uncertainty. We exclude LAP from this section due to its excessive memory consumption, which consistently results in Out-Of-Memory (OOM) errors during inference. The experiment is conducted on the OBQA dataset [65], which consists of four categories. ", "page_idx": 28}, {"type": "text", "text": "For each input sequence $\\pmb{s}$ , we use the last token\u2019s embedding generated by the final transformer block in Llama2-7B as the final embedding. Given the weights $W$ , we denote the embedding as $\\phi(s;W)$ . Generally, three types of embeddings can be generated using the Bayesian approach: ", "page_idx": 28}, {"type": "text", "text": "(a) Embeddings generated by the mean of the weights (these embeddings are shown as $\\omega\\bullet\\bullet$ in Fig. 5): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\phi(s;\\mathbb{E}_{W\\sim q(\\cdot|\\theta)}[W])=\\phi(s;W_{0}+B M).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(b) Embedding samples generated by sampling different weights from the approximate posterior, whose distribution is plotted by the solid line $(-)$ . ", "page_idx": 28}, {"type": "text", "text": "(c) The expectation of the embedding, which is approximated by averaging the sampled embeddings: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{W\\sim q(\\cdot|\\theta)}[\\phi(s;W)]\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\phi(s;W_{0}+B(M+E_{n}\\circ\\Omega)),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $N$ denotes the number of samples during inference, and $E_{n}$ denotes the $n$ -th sampled noise for the weight matrix. We show this expectation as $\\bullet\\bullet$ in Fig. 5. ", "page_idx": 28}, {"type": "text", "text": "To visually demonstrate the confidence calibration effect of the Bayesian treatment, we adopt the following pipeline of visualization, which we believe can be further applied in visualizing other frameworks\u2019 embedding uncertainty quality. ", "page_idx": 28}, {"type": "text", "text": "(1) Acquire high-dimensional embeddings produced by the weight mean for the given test dataset, as decribed in (a) and Eqn. 76 above. ", "page_idx": 28}, {"type": "image", "img_path": "MaDykgj4Ru/tmp/10a48560125017a7e2708f6b2eb644599206931980977964a3183a7f5018de81.jpg", "img_caption": ["Figure 5: Visualization of embedding uncertainty quality for different methods. The model is fine-tuned for 5,000 steps on the Llama2-7B. We fine-tune the Llama2-7B model on the OBQA dataset for 5000 steps. The two contour lines represent the probability mass of 0.5 and 0.75, respectively. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "(2) Use Linear Discriminant Analysis (LDA) [10] to project these high-dimensional embeddings into a low-dimensional 2D space.   \n(3) In the 2D space, fit a logistic regression model to mimic the decision regions and color them based on the true labels.   \n(4) Sample weights 10 times from the approximate posterior, generate the embeddings, and project them into the same 2D space using the previously learned LDA. Use Kernel Density Estimation (KDE) [79, 74] to show their distributions, as described in (b) above.   \n(5) Average the sampled embeddings for each example and visualize them in the 2D space, as described in (c) and Eqn. 77 above. ", "page_idx": 29}, {"type": "text", "text": "In Fig. 5, we show 4 correct and incorrect predictions made by each model. Ideally, a model with better uncertainty estimation should produce lower level of uncertainty (smaller embedding variance, i.e., smaller contours, and further away from the decision boundary) for correct predictions, and higher level of uncertainty (larger embedding variance, i.e., larger contours, and closer to the decision boundary). From the figure, we have the following observations: ", "page_idx": 29}, {"type": "text", "text": "\u2022 All three Bayesian approaches produce higher embedding variance for incorrect predictions and lower embedding variance for correct predictions. However, BLoB achieves significantly larger embedding variance compared to the baselines, consistent with the quantitative evaluation shown in Table 1. BLoB\u2019s produced variance is higher for the incorrect predictions, demonstrating its accurate uncertainty estimation even in the embedding space. \u2022 In BLoB, the mean embedding produced by sampling weights from the approximate posterior is closer to the decision boundary than the embedding generated by the mean of weights $(\\star\\rightarrow\\blacktriangledown)$ . This effect is most apparent when the prediction is incorrect, consistent with the quantitative results yielded from the final softmax layer of the model. Again, this demonstrates BLoB\u2019s Bayesian inference can bring the final prediction closer to the ground truth. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The abstract and introduction contains summarized claims about this paper that accurately reflect our contributions. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Our main theorems (Theorem 3.1 and 3.2) state the hidden assumptions of the low-rank structure of the approximate posterior we propose for LLMs, and we have discussed the similar topics we do not consider in the Remark. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the main statement of the theorems and their proof, we clearly include the assumptions we make underlying them. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided the full settings of our experiments in Sec. 4 and Appendix B. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we have submitted the anonymized code to Openreview, with the dependency specification of the packages and instructions for running the code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided the full settings of our experiments in Sec. 4 and Appendix B. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our experiments are repeated for 3 runs with different random seeds, and we have reported the standard deviation of all the methods in all the tables. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix C.3. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have read and fully considered the Code Of Ethics of NeurIPS, and we confirm we follow it strictly without any violation. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have some discussions on how our research can achieve a reliable LLM deployment with reduced harm to people by estimating the uncertainty of the prediction in Sec. 1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not pose any risks since there is no release of models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: we perform all the experiments under the license, and have cited work properly in the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: There is no new assets introduced in this paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing experiments are involved in this paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No IRB is involved in this paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]