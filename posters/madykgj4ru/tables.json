[{"figure_path": "MaDykgj4Ru/tables/tables_6_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table presents the performance comparison of different uncertainty estimation methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB) applied to the LoRA on Llama2-7B pre-trained weights.  The evaluation metrics include accuracy (ACC), expected calibration error (ECE), and negative log-likelihood (NLL) across six common-sense reasoning datasets. The number of samples (N) used during inference for BLOB is also varied (N=0, 5, 10).  The results show BLOB's superior performance in terms of accuracy and uncertainty estimation.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_8_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table presents the performance comparison of different uncertainty estimation methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB) applied to LoRA on Llama2-7B pre-trained weights across six common-sense reasoning datasets.  The metrics used for comparison are accuracy (ACC), expected calibration error (ECE), and negative log-likelihood (NLL).  The table highlights the impact of the number of samples (N) during inference for the BLOB method.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_23_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of various Bayesian methods (BLOB, BBB, MCD, ENS, LAP) against standard Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) methods.  The comparison is done across six different common sense reasoning tasks using the same hyperparameters and after 5000 gradient steps.  The metrics used for evaluation are Accuracy (ACC), Expected Calibration Error (ECE), and Negative Log-Likelihood (NLL). The number of samples used during inference (N) for BLOB is varied to show the impact of sampling on performance.  Bold values indicate the best performing method for each metric, and underlined values indicate the second-best.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_23_2.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of different methods for applying Bayesian Low-Rank Adaptation (LoRA) to a Llama2-7B pre-trained language model across six common sense reasoning tasks.  It shows accuracy (ACC), expected calibration error (ECE), and negative log-likelihood (NLL) for various methods, including Maximum Likelihood Estimation (MLE), Maximum A Posteriori (MAP), Monte Carlo Dropout (MCD), Deep Ensemble (ENS), Bayes by Backprop (BBB), Laplace LoRA (LAP), and the proposed BLOB method with varying numbers of samples (N) during inference.  Higher ACC and lower ECE/NLL values indicate better performance.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_24_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of various uncertainty estimation methods applied to the LoRA adapter on Llama2-7B pre-trained weights across six common-sense reasoning datasets.  Metrics include Accuracy (ACC), Expected Calibration Error (ECE), and Negative Log-Likelihood (NLL).  The number of inference samples (N) for BLOB is varied to show its impact.  Higher ACC is better; lower ECE and NLL are better.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_24_2.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of different methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB with different sample sizes N) on six common-sense reasoning tasks using Llama2-7B pre-trained weights.  The metrics used are Accuracy (ACC), Expected Calibration Error (ECE), and Negative Log-Likelihood (NLL).  It shows how well each method calibrates its confidence and its overall accuracy and uncertainty estimation.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_25_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table presents the performance comparison of several methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB with different sample numbers) for fine-tuning LLMs on six common-sense reasoning tasks using LoRA. The metrics used for evaluation include Accuracy (ACC), Expected Calibration Error (ECE), and Negative Log-Likelihood (NLL), reflecting both performance and uncertainty estimation.  The table highlights the superior performance of the proposed BLOB method, particularly when considering uncertainty estimation.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_26_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of various Bayesian methods (including the proposed BLOB) and maximum likelihood/maximum a posteriori estimation baselines on six common sense reasoning datasets.  Performance is measured by accuracy, expected calibration error, and negative log-likelihood, reflecting both predictive accuracy and uncertainty calibration.  The impact of varying the number of samples used during inference with BLOB is also explored.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_27_1.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \u201c\u2191\u201d and \u201c\u2193\u201d indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of various methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB) for adapting a Llama2-7B language model using LoRA to six common-sense reasoning tasks.  The metrics used are accuracy (ACC) and expected calibration error (ECE), both shown as percentages.  The table highlights the impact of the number of samples (N) used during inference for the BLOB method.", "section": "4.2 Results on In-distribution Datasets"}, {"figure_path": "MaDykgj4Ru/tables/tables_27_2.jpg", "caption": "Table 1: Performance of different methods applied to LoRA on Llama2-7B pre-trained weights, where Accuracy (ACC) and Expected Calibration Error (ECE) are reported in percentages. The evaluation is done across six common-sense reasoning tasks with a shared hyper-parameter setting after 5,000 gradient steps. We use N to represent the number of samples during inference in BLOB. \"\u2191\" and \"\u2193\" indicate that higher and lower values are preferred, respectively. Boldface and underlining denote the best and the second-best performance, respectively.", "description": "This table compares the performance of several methods (MLE, MAP, MCD, ENS, BBB, LAP, and BLOB) for adapting Llama2-7B language models using LoRA on six common sense reasoning tasks.  The metrics used are accuracy (ACC), expected calibration error (ECE), and negative log-likelihood (NLL).  Different numbers of samples (N) during inference are tested for BLOB to show the impact on performance.  The best and second-best performances are highlighted.", "section": "4.2 Results on In-distribution Datasets"}]