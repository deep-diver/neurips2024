[{"type": "text", "text": "General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhou Fang Yong-Lu Li\u2217 Lixin Yang Cewu Lu\u2217 Shanghai Jiao Tong University {joefang, yonglu_li, siriusyang, lucewu}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks. Recently, several image editing methods based on diffusion models have been proposed to manipulate articulated objects according to text prompts. However, these methods often generate weird artifacts or even fail in real images. To this end, we introduce the Part-Aware Diffusion Model to approach the manipulation of articulated objects in real images. First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently and arbitrarily. Then we propose dynamic feature maps to transfer the appearance of objects from input images to edited ones, meanwhile generating novel views or novel-appearing parts reasonably. Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works. Additionally, we verify our method on 3D articulated object understanding for embodied robot scenarios and the promising results prove that our method supports this task strongly. The project page is at https://mvig-rhos.com/pa_diffusion. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image editing is a long-standing popular computer vision task. Specifically, manipulating articulated objects has garnered significant attention owing to its application in various fields, such as image augmentation for downstream tasks [43], building goal conditions to train reinforcement learning models for robotic manipulation [41, 55], creating videos with extra supervision information [33], detecting human-object interactions [21, 22, 25], reasoning object affordance [24, 23], etc. Thanks to the large-scale training data and immense computing power, diffusion-based [40] generative models have achieved surprising results in the field of image and video generation. ", "page_idx": 0}, {"type": "text", "text": "Inspired by these successes, several recent works have adopted diffusion models as the backbone and implemented text-guided object manipulation [19, 15, 7, 51]. We can properly divide these studies into a couple of groups. The first one is to directly edit 2D images by transferring the feature/attention maps from original images to edited ones such as [12, 31, 7]. However, weird artifacts are prone to appear when the objects are rotated and deformed, or novel views appear. Consequently, these methods are restricted to structure-preserving image editing. Another group relies on reconstructing 3D object models. As the most related work to ours, [51] reconstructed 3D object models for manipulation and projected them back to images later. Nevertheless, this approach depends on the quality of reconstructed 3D models heavily. And the reconstruction model has to be fine-tuned when dealing with new categories. Moreover, manipulation has to be done manually which is laborious and impractical to support editing large quantities of images. ", "page_idx": 0}, {"type": "text", "text": "To address these problems, we propose the Part-Aware Diffusion Model (PA-Diffusion model) for articulated object manipulation in real images, as illustrated in Fig. 1. Firstly, we introduce the concept of Abstract 3D Model and build a Primitive Prototype Library to represent articulated objects in 3D space, so that our method can not only cover many common objects but also handle novel categories without extra training data or fine-tuning processes. Besides, arbitrary manipulation can be done efficiently. Second, we proposed dynamic feature maps to assist generation models in accurately transferring object appearances to accurate locations in edited images. As a result, weird artifacts are eliminated, and meanwhile, novel views or novel-appearing parts are generated more reasonably. Finally, owing to the simple manipulation and editing process, the procedure is brief and the model can strongly support other tasks by editing a large volume of images. ", "page_idx": 0}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/78f7682004f23b704dd4248df12c89b5128737d2c6f7551ea1d28c49faf1e6db.jpg", "img_caption": ["Figure 1: We propose the Part-Aware Diffusion Model: Abstract 3D model of the articulated object is constructed referring to the input 2D real image. Arbitrary manipulation can be done in 3D space based on the text instruction or human interaction, the generation model then creates the edited image according to the manipulation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "(1) We introduce the concept of the Abstract 3D Model which accurately and robustly represents various articulated object categories with primitive prototypes. Meanwhile, novel categories can also be incorporated quickly. In addition, the articulated objects can be efficiently manipulated with text instructions or human interactions in 3D space. ", "page_idx": 1}, {"type": "text", "text": "(2) We propose dynamic feature maps that let the diffusion model comprehend the object structure. Consequently, the diffusion model can generate novel views or novel-appearing parts of objects reasonably and preserve the appearance of the seen parts simultaneously. ", "page_idx": 1}, {"type": "text", "text": "(3) We present comprehensive experiments to highlight the advantages of our PA-Diffusion model including comparing with state-of-the-art editing methods both qualitatively and quantitatively, choosing a 3D articulated object understanding experiment to demonstrate how our method supports the tasks in embodied robot scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Diffusion Model for Image Generation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In recent years, diffusion models [39, 38, 10] have achieved great success in the fields of image/video generation [6, 16], segmentation [4, 50], and many downstream computer vision tasks. To make the generation results controllable, [34] first proposed to extract and incorporate text features into the denoising process. Following this concept, [40, 11, 42, 14, 8] improved the performance of text-guided diffusion models with more effective text embedding methods. ", "page_idx": 1}, {"type": "text", "text": "However, as an implicit instruction, text guidance is still not strong enough to finish fine-grained image control such as determining the image layouts, objects\u2019 shape and texture, and so on. To make up this gap, [46] provided structural guidance by enhancing the similarity between the features of other conditions and the text guidance. [13, 5] proposed to modify the cross-attention maps and then guide the denoising process. To handle more complex scenarios and achieve more precise control, [52] and [32] proposed adding an extra module to the diffusion model. Then extra condition information can be imported to guide the denoising process. ", "page_idx": 1}, {"type": "text", "text": "2.2 Diffusion Based Image Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Considering the remarkable capability of understanding images, several recent works have also reported editing real and synthetic images with using diffusion models as the backbone. These methods can generally be summarized into two groups: Inversion-Based and Feature-Sharing Based. ", "page_idx": 2}, {"type": "text", "text": "The first group is primarily based on adding extra control to the inverted noise maps of images, then re-generating the image such as [19]. However, because the deterministic DDIM sampling process cannot be reversed perfectly, these methods struggle to preserve the appearance of original objects and backgrounds precisely. The second group attempts to maintain the appearance of objects by transferring the feature/attention/activation maps between guidance and generation branches or by adding extra loss items during the denoising process, as seen in [7, 31, 12]. Recent approaches like DragGAN [35] and DragDiffusion [31] propose to utilize a point-to-point dragging scheme, which can achieve refined content dragging. Nonetheless, these approaches often perform poorly on articulated object manipulation in real images, resulting in weird and blurry artifacts in edited images. ", "page_idx": 2}, {"type": "text", "text": "2D-3D-2D is another promising way of image editing, the recent work [51] introduced reconstructing 3D models from 2D images and projecting them back after manipulation. However, this approach highly relies on the quality of 3D reconstructed models, and reconstructing 3D models from a single 2D image is still a challenging task. ", "page_idx": 2}, {"type": "text", "text": "In contrast to the aforementioned approaches, our method demonstrates advantages when manipulating articulated objects in real images - high fidelity edited images, easy and arbitrary manipulation, covering multiple categories, and incorporating novel categories quickly. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this session, we go through the proposed PA-Diffusion model in detail. The overall architecture is demonstrated in Fig. 2. Initially, we reconstruct abstract 3D models for articulated objects with the Primitive Prototype Library. Then arbitrary manipulation can be done according to text instructions or human interactions. Next, leveraging DDIM Inversion [44, 30], initial inverted noise maps are created and manipulated following the previous actions. During the generation stage, we introduce dynamic feature maps, including manipulated inverted noise maps and compositional activation maps. These ensure that the appearance of seen parts of objects can be preserved accurately and that novel-appearing parts are generated reasonably. Besides, Texture and Style Consistency Score Loss are introduced to alleviate the blurry and style mismatch problems. ", "page_idx": 2}, {"type": "text", "text": "3.2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models aim to convert random Gaussian noise into high-resolution images through a sequential denoising and sampling process [12]. Given the conditioning $y$ , we start from the initial Gaussian noise map $z_{t}$ , and then iteratively estimate the reduced noise $\\hat{\\epsilon_{t}}$ at each time step $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\hat{\\epsilon_{t}}=\\epsilon_{\\theta}(z_{t};t,y),}\\\\ {z_{t-1}=u p d a t e(z_{t},\\hat{\\epsilon_{t}},t,t-1,\\epsilon_{t-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The update function could be DDPM [17], DDIM [44], or other sampling methods. Nevertheless, conventional sampling from conditional diffusion models often fails to produce high-quality images that align well with the condition $y$ . To enhance the effect of the desired condition, extra class loss guidance is added to the reduced noise during the sampling process such as Classifier or Classifier-free guidance [45, 18]. ", "page_idx": 2}, {"type": "text", "text": "Classifier guidance is introduced to generate conditional samples from an unconditional model by combining the unconditional score $\\epsilon_{t}$ with a classifier $p(y|z_{t})$ , where $p(y|z_{t})$ is the probability distribution of condition $y$ based on the noise at time step $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\epsilon_{t}}=\\epsilon_{\\theta}(z_{t};t,y)+\\beta\\,\\bigtriangledown_{z_{t}}\\,p(y|z_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Classifier-free guidance eliminates the need for a separate classifier by incorporating the class information directly into the generative model as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\epsilon}_{t}=(1+\\alpha)\\epsilon_{\\theta}(z_{t};t,y)-\\alpha\\epsilon_{\\theta}(z_{t};t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/ae15359e1c0b160e04ddb799490b1e78d1611eaffc22314ef3b130832d8b9f3c.jpg", "img_caption": ["Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, arbitrary manipulation can be implemented in the 3D space based on text guidance or human interaction. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Following these concepts, custom energy functions can also be utilized to guide the denoising process, instead of the probability function. In [12] [29] [54], various energy functions $g$ are incorporated alongside classifier-free guidance to obtain high-fidelity samples as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\epsilon}_{t}=(1+\\alpha)\\epsilon_{\\theta}(z_{t};t,y)-\\alpha\\epsilon_{\\theta}(z_{t};t)+\\beta\\underbrace{\\gamma_{z_{t}}}g(z_{t};t,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our proposed PA-Diffusion model is built on the diffusion model with classifier-free guidance. Extra energy functions are employed during the image editing process. ", "page_idx": 3}, {"type": "text", "text": "3.3 Arbitrary Manipulation in 3D Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a promising workaround to the methods of dealing with images directly, the 2D-3D-2D pipeline has successfully handled many articulated objects with precise 3D models. Unfortunately, creating 3D models for various categories from a single image remains challenging, particularly for novel categories or instances. In this work, we introduce the concept of Abstract 3D Model that reconstructs accurate 3D models, supports efficient object manipulation, and incorporates novel objects easily. ", "page_idx": 3}, {"type": "text", "text": "Abstract 3D Model. Unlike previous methods, there is no need for precise 3D models of our method, the conditional information we have to provide to the diffusion model is coarse sketch maps and part-level masks. Therefore, we introduce the use of an abstract 3D model to represent the articulated object. As an abstract 3D model, the object is represented by combining several basic prototypes. As depicted at the bottom of Fig. 2, the laptop can be represented by two planes, storage furnitures and microwaves can be represented by a plane and a box. Primitive Prototype Library, which includes basic 3D prototypes such as cuboids, cubes, and boxes, supports common articulated object categories involving both rotation and translation joint types. ", "page_idx": 3}, {"type": "text", "text": "Camera Alignment. Next, we compute the camera pose in 3D space and align the 2D real image view with the 3D space camera view. The pose computation problem is to calculate the intrinsic and extrinsic matrices for the camera that minimize the reprojection error from 3D-2D point correspondences [3]. Thus, in this work, we first employ Large-scale Segmentation Models to obtain the initial part-level segmentation masks of articulated objects $M^{I n i t}$ and then detect the extreme corner points $A,B,C,D\\:(p t s_{1})$ with simple corner detection functions. These 2D extreme points are aligned with their 3D counterparts $A^{'},B^{'},C^{'},D^{'}\\;(p t s_{2}$ , pre-defined in Primitive Prototype Library) as shown in Fig. 3. Finally, based on Perspective $\\mathbf{n}$ -Points 2D-3D method [49], the camera matrices can be extracted, and 2D-3D views are aligned. ", "page_idx": 3}, {"type": "text", "text": "Manipulation. By representing objects with primitive prototypes, multiple types of manipulations can be implemented in 3D space efficiently with the assistance of 3D computer graphics software. As shown in Fig. 2, manipulating objects through text instructions is a concise approach. For example, the instruction opening the laptop $120^{\\circ}$ is converted to a script, then the manipulation will be done by running the script in 3D software. Our PA-Diffusion model also supports human interaction, which could be even more efficient. Additional manipulation guidance is provided in the Appendix. Contrary to the tedious manipulation experience of previous SOTA works, our proposed PA-Diffusion model offers a more flexible approach to editing articulated objects. ", "page_idx": 3}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/94f226e89d4e463800b7205e943f1ed202b7daf0fded88c41df040a54288f0e5.jpg", "img_caption": ["Figure 3: Algorithm pipeline of the PA-Diffusion model. Symbols and procedures in the figure are the same as those in the content. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Structure Disentangle. Some object parts could still be seen after manipulation, and some unseen parts would appear. As shown in the top right part in Fig. 3, the laptop shell can be seen in the input image. After opening the laptop, the shell can still be seen, however, the keyboard and screen are newly revealed. Therefore, to distinguish them and implement part-aware diffusion, we disentangle articulated objects into seen parts and novel-appearing parts. The appearance of seen parts should be consistent between input and edited images, and the style of novel-appearing parts should be consistent with the objects\u2019 overall appearance. In this work, we regard all the initial part-level masks $M^{I n i t}$ as seen. Then after manipulation, we obtain manipulated seen parts mask $M_{s}^{G e n}$ and manipulated novel-appearing parts mask $M_{n}^{G e n}$ , both of them are exported from 3D software automatically. We use $\\boldsymbol{M}^{\\dot{G}e n}$ to present the union of $M_{s}^{G e n}$ and $M_{n}^{G e n}$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Dynamic Feature Maps ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To maintain the object\u2019s appearance including color and texture, previous editing methods introduced a guidance branch to invert and re-generate the input image, and a generation branch to create the edited image, the attention/feature/activation maps are transferred from the guidance to the generation branch directly [7]. However, when changing the object location or shape during manipulation, directly sharing these maps would transfer the feature from the input image to undesired locations in the edited image. Furthermore, these methods cannot reasonably generate novel views or novelappearing parts. ", "page_idx": 4}, {"type": "text", "text": "To overcome these problems, we propose dynamic feature maps including manipulated inverted noise maps and compositional activation maps. To keep appearance accurate, manipulated inverted noise maps transfer the feature of seen parts in input images to the manipulated seen parts in edited images. Simultaneously, to make novel-appearing parts reasonably, compositional activation maps let the diffusion model create these parts from random noise. The following content describes how to manipulate the noise maps, how to construct compositional activation maps, and how they work. For clarity, the process is presented in Fig. 3, and an additional simpler explanation is provided in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "Manipulated inverted noise map. As shown at the top of Fig. 3, we firstly reverse the input image to the initial inverted noise map zITn with DDIM inversion. After 3D manipulation, we calculate the transform function $T$ based on the initial $M^{I n i t}$ and manipulated seen part-level masks $M_{s}^{G e n}$ and then compute the transformed inverted noise map zTT r $z_{T}^{T r a n}$ with $T$ . Finally, the manipulated inverted ", "page_idx": 4}, {"type": "text", "text": "noise map is created as the following equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{T}^{M a n}=z_{T}^{T r a n}\\times M_{s}^{G e n}+z_{T}^{T r a n}\\times M_{s}^{M a k e}+z_{T}^{I n i t}\\times M_{b g},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $M_{s}^{M a k e}$ is the mask generated by $X O R(M^{I n i t},M^{I n i t}\\cap M_{s}^{G e n})$ . $\\mathit{M}_{b g}$ is the background mask created by $1-M^{I n i t}\\cup M_{s}^{G e n}$ . Besides, we also create a random noise map TRan. The three noise maps $z_{T}^{I n i t}$ , $z_{T}^{R a n}$ , and $z_{T}^{M a n}$ will be sent to the denoising UNet as a batch in the next step. ", "page_idx": 5}, {"type": "text", "text": "Compositional activation map. As shown at the bottom of Fig.3, the diffusion model runs a pipeline denoising process with the three noise maps as a batch and generates three output activation maps: $A_{t}^{G u i}$ generated from initial inverted noise map within guidance branch, $A_{t}^{R a n}$ generated from random noise map within random branch, and $A_{t}^{G e n}$ generated from manipulated inverted noise map within generation branch. At last, $A_{t}^{R a n}$ and $A_{t}^{G e n}$ are merged according to previously defined masks. ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{t}^{G e n}=A_{t}^{R a n}\\times M_{n}^{G e n}+A_{t}^{G e n}\\times M_{s}^{G e n},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Owing to the above steps, we transfer the feature of seen parts in input images to the accurate location in edited images. At the same time, all the other contents and the background in input images can be preserved, as the highlighted yellow part in Fig. 3. Besides, as the highlighted blue part in Fig. 3, an extra image is synthesized with random noise map $z_{T}^{R a n}$ , the novel-appearing parts are cropped and pasted to edited images from the extra image, which makes these parts more reasonable and consistent with the original inputs. ", "page_idx": 5}, {"type": "text", "text": "3.5 Score Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Texture Consistency Score Loss. However, simply manipulating the inverted noise map will lead to a serious blurry problem. This is due to the denoising process includes several convolution steps. As the initial inverted noise map zITnitis not rotation invariant, manipulating zITnitwill disturb the original distribution and make the denoising process fail. To alleviate this limitation, we construct Texture Consistency Score Loss (TCSL) [31] as an extra supervision that lets the specific region in the generation branch match with the one in the guidance branch, ", "page_idx": 5}, {"type": "equation", "text": "$$\nL o s s_{t}=\\frac{\\varphi_{f g}}{c o s(A_{t}^{G u i}[M^{I n i t}],A_{t}^{G e n}[M_{s}^{G e n}])}+\\frac{\\varphi_{b g}}{c o s(A_{t}^{G u i}[1-M^{I n i t}],A_{t}^{G e n}[1-M^{G e n}])},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\varphi_{f g}$ and $\\varphi_{b g}$ are hyper-parameters. We add this loss item as an extra loss in classifier guidance in each denoising iteration step to calibrate the appearance of objects. ", "page_idx": 5}, {"type": "text", "text": "Style Consistency Score Loss. For novel-appearing parts, the diffusion model is prone to randomly select a style to generate them with text guidance or sketch maps. As a result, the texture and style are usually different from the objects in input images. Therefore, we introduce Style Consistency Score Loss (SCSL) to calibrate the style of seen parts and the novel views and novel-appearing parts. ", "page_idx": 5}, {"type": "text", "text": "Different from Texture Consistency Score Loss, there is no need to match every pixel in input images and edited images. Thus we calculate L1 loss between the activation maps of the guidance and the generation branch [12]. The loss function is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL o s s_{s}=\\left|s u m(A_{t}^{G u i}[M^{I n i t}])-s u m(A_{t}^{G e n}[M_{n}^{G e n}])\\right|_{1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This loss item is also added as an extra classifier guidance. The final reduced noise in each denoising iteration is as follows, where $\\gamma_{1}$ are $\\gamma_{2}$ are the hyper-parameter weights of TCSL and SCSL, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\epsilon}_{t}=(1+\\alpha)\\epsilon_{\\theta}(z_{t};t,y)-\\alpha\\epsilon_{\\theta}(z_{t};t)+\\gamma_{1}\\;\\bigtriangledown_{z_{t}}L o s s_{t}+\\gamma_{2}\\;\\bigtriangledown_{z_{t}}L o s s_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide two kinds of experiments to prove the advantages of our proposed PADiffusion model. First, various image editing tasks are conducted to showcase the model\u2019s image editing capabilities. To highlight the superiority of our model compared with state-of-the-art methods, we collect a testbench and evaluate all the methods both qualitatively and quantitatively. Second, we create a synthetic training set to support the challenging 3D articulated object understanding task in the robotic scenarios. ", "page_idx": 5}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/f919d977dd971f9b5f67e07b7dabf1a7a3c9e63fcc3966594ea05c06d68d4bc6.jpg", "img_caption": ["Figure 4: Results of basic manipulations: move, scale/shear. rotate, and manipulate. Blank regions caused by the manipulation are in-painted automatically. The novel views and novel-appearing parts match with the style of the seen parts (left). Articulated objects are opened from $0^{\\circ}$ to $120^{\\circ}$ . The appearance of the increasing novel-appearing parts keeps being consistent throughout the whole process (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this work, we select Grounded Segment Anything [20, 28] to obtain the initial part-level object segmentation masks. T2I Adapter [32] is chosen as the conditional generation model, and the condition we used is the sketch map. The fundamental diffusion model is Stable Diffusion V1-5. All experiments run on a single NVIDIA A100 GPU. Notably, NO models need to be trained or fine-tuned in the image editing process. ", "page_idx": 6}, {"type": "text", "text": "Primitive Prototype Library is built within Blender [9]. 3D cuboids, cubes, boxes, and other 3D primitive shapes are created and combined to represent different objects. In this work, 6 primitive shapes are created to represent 6 categories of articulated objects. The ease of creating prototypes allows for the quick incorporation of novel categories or instances. Rotation, view change, and other manipulations are all implemented in Blender. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fig. 4 demonstrates the editing results of some basic manipulations and a sequential manipulation process. As shown in the left part, our PA-Diffusion model naturally moves, scales/shears, rotates, and opens articulated objects with rotation or translation joint types. The edited objects blend seamlessly with other contents and backgrounds in the original images. When we move or rotate the objects, the blank regions in the background are in-painted semantically according to the surroundings. Moreover, in-painting and editing are completed in a single denoising process by the PA-Diffusion model, no extra in-paint model or process is required. Last but not least, novel-appearing parts of objects are generated reasonably. For example, the fruits are in the refrigerator, the storage furniture and the drawer are empty after opening. ", "page_idx": 6}, {"type": "text", "text": "The right part of Fig. 4 presents a complete operation process of opening articulated objects, from the initial closed state to progressively open states as $30^{\\circ}$ , $60^{\\circ}$ , $90^{\\circ}$ , $120^{\\circ}$ . The appearance of objects\u2019 seen parts is transferred from the original input to different states accurately. The point that needs to be mentioned is that along with the operation progress, more novel-appearing parts of objects appear, our proposed PA-Diffusion model keeps the style and texture of these novel-appearing parts throughout the process. This capability allows our method to generate a complete manipulation video from a single input image, maintaining consistent object appearance and style even as novel views or parts increase. ", "page_idx": 6}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/5ff95485318b29cae0619e3b55c2b05240aa8da4277141411e310dedd40416ac.jpg", "img_caption": ["Figure 5: Manipulate non-rigid objects, non-uniform shapes, and objects with weird or multiple joint types. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Generally, articulated objects\u2019 parts are rigid and connected with one of the typical joint types - rotation and translation. However, we are surprised to notice that our PA-Diffusion model can also handle non-rigid objects with non-uniform shapes and weird joints and manipulations fabulously. As illustrated in Fig. 5, we first select toys as examples of non-rigid with non-uniform shapes, the tail of the shark is moved up together with other close parts as deformable objects. Then, we broke the cup in a real image. Third, the kitchen pot and storage furniture are opened to illustrate the case of weird and multiple joint types within one object. No matter whether the shape of object parts has changed after manipulation or joint types are unconventional, the PA-Diffusion model can edit all the objects successfully. Meanwhile, the background is preserved or inpainted very well. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As mentioned in Section 3, TCSL is added to the denoising loss function to release the serious blurry problem. As shown in the left of Fig. 6, we move the storage furniture to the right. It can be seen that without TCSL, the objects are prone to be blurry. More seriously, the edited image could be blurry, as in the storage furniture example. On the other hand, with TCSL, the texture of objects can be transferred to the desired location, meanwhile, the blank region caused by the movement is in-painted well. SCSL is another loss to keep the style consistent between seen parts and novel-appearing parts. Its effectiveness is shown in the left bottom of Fig. 6 (with SCSL). We notice that when opening the storage furniture, the style of the novel-appeared inner body part is more likely to be consistent with the door and outer body part with SCSL, which makes the edited image natural. More quantitative ablation studies about the two score losses are provided in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further evaluate our PA-Diffusion model, we compare it with four state-of-the-art image editing approaches that are based on diffusion models: Imagic, DragDiffusion, MasaCtrl (with T2I Adapter), and Image Sculpting. In this experiment, we require these models to manipulate different categories of articulated objects, including both rotation and translation joint types. The results are shown in the right part of Fig. 6. It is hard for Imagic to finish the tasks as articulated objects cannot be opened at all or the wrong part is manipulated. This is because the text instruction is too weak and the fundamental generation model cannot understand the structure of objects. Similarly, DragDiffusion cannot finish the tasks even though human interaction is applied. ", "page_idx": 7}, {"type": "text", "text": "MasaCtrl performs better than Imagic and DragDiffusion. The manipulation can be finished, while the edited images are either unrealistic or unreasonable. Take the laptop as an example (second column in the right part of Fig. 6), the object has been moved down and opened, while the region highlighted with the red bounding box in the edited image remains unchanged, which does not make sense. This issue is prevalent across other categories as well. The reason is that MasaCtrl simply shares the whole feature/attention maps between the input and edited image, features of seen parts cannot be transferred to the desired new location when objects move or the shape changes. Finally, Image Sculpting works well on storage furniture. However it is prone to fail to reconstruct precise 3D models of the laptop, trashcan, and drawer, consequently, the edited images are undesirable. ", "page_idx": 7}, {"type": "text", "text": "In contrast, our PA-Diffusion model consistently produces high-fidelity and reasonable edits. The appearance of seen parts is kept accurate no matter whether objects move or the shapes of parts have changed. The novel parts are reasonable and semantically consistent with the original. ", "page_idx": 7}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/ce6fa53a612e4e07c723e9bf91b63f8a9555edc040df0845a55b51e5397d62ef.jpg", "img_caption": ["Figure 6: TCSL and SCSL are employed to release the blurry and style mismatch problem. The model is required to move and open the object (left). Comparison of Imagic, DragDiffusion, MasaCtrl (with T2I adapter), Image Sculpting, and our PA-Diffusion model. The target state is \u2019a photo of an opened object\u2019 (right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Quantitative Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To quantitatively evaluate our method, we built an articulated object manipulation testbench. The testbench comprises 6 object categories including storage furniture, laptop, trashcan, microwave, drawer, and refrigerator, which covers both rotation and translation joint types. In total, 660 real images are collected from the website. Considering articulated objects are typically rigid with uniform shapes, this testbench can represent the characteristics of common articulated object categories. ", "page_idx": 8}, {"type": "text", "text": "The comparison methods we select are Imagic and MasaCtrl (with T2I Adapter). DragDiffusion is excluded as it cannot complete the manipulation tasks. Due to the long processing time and frequent failures in generating 3D models, Image Sculpting is also excluded here. To assess the realism of the edited images, the evaluation metric used is the Frechet Inception Distance (FID) score. The quantitative evaluation results are summarized in Tab. 1. ", "page_idx": 8}, {"type": "text", "text": "Since Imagic relies solely on text instructions, the edited images often do not align well with the original inputs, resulting in poor scores. Due to previously discussed reasons, the edited images of MasaCtrl are confusing and lack coherence. Sequentially, the FID score is not satisfying. In comparison, the PA-Diffusion model outperforms other methods with an obvious improvement. ", "page_idx": 8}, {"type": "text", "text": "4.6 Articulated Object Understanding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this session, we demonstrate how our proposed method supports the task of 3D articulated object understanding. As one of the fundamental steps to understanding 3D articulated objects, estimating the axes and surface normal is still challenging because of the lack of data. Annotating the axes and surface normal in 2D images is expensive and unreliable. On the contrary, leveraging abstract 3D models, our method can easily generate accurate 3D annotations of objects. ", "page_idx": 8}, {"type": "text", "text": "To release the data limitation, we create a synthetic dataset with the PA-Diffusion model. The dataset includes 660 sequential samples, each sample includes the sequence of opening objects from $0^{\\circ}$ to $120^{\\circ}$ with step $30^{\\circ}$ , 3,300 images in total. [37] introduced a 3-step training process to develop the ", "page_idx": 8}, {"type": "table", "img_path": "WRd9LCbvxN/tmp/ad60c51516b70aede0045bc902110174953300edef2b0a0b1b87f3b7b5f866e8.jpg", "table_caption": [], "table_footnote": ["Table 1: FID Score of edited images with Imagic, MasaCtrl (with T2I adapter), and ours. "], "page_idx": 9}, {"type": "table", "img_path": "WRd9LCbvxN/tmp/9cbf6160aa83e5c2e057a1d0028a5a49cf789af7b5850b73eee7c1ac2e96c513.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "WRd9LCbvxN/tmp/2d018fed067963cb73e371baeaf5d46e4605a61fee0c9152505287631e7bcd56.jpg", "table_caption": ["Table 2: Prediction accuracy of the model developed with half (left) and full (right) training set separately. "], "table_footnote": ["Table 3: Mix the edited images with the training set of the InternetVideo dataset, then evaluate the fine-tuned model on the testing set of the InternetVideo dataset. "], "page_idx": 9}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/2a7d388b0a505e6c5dfd32195b577fbb484040491821db55123a32dcd154d23d.jpg", "img_caption": ["Figure 7: Detection results on the sequential samples, including rotation and translation joint types. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "object understanding model including BBox detection, axis prediction, and plane normal estimation.   \nFollowing this schedule, we evaluate the feasibility of edited images by two kinds of experiments. ", "page_idx": 9}, {"type": "text", "text": "First, the generated sequential samples are divided into training/testing sets (612/48). Specifically, we follow the 3-step to train the model with half and full samples separately, and then evaluate the model with three matrices, BBox IoU, Axes EA-score, and surface normal error smaller than $30^{\\circ}$ [37]. Fig. 7 demonstrates the prediction results, the model can understand the structures of articulated objects after training with edited images, including moving plane, joint types, axis and surface normal. Quantitative evaluation results in Tab. 2 indicate that prediction accuracy improves significantly with more training samples, illustrating that the edited images are comparable to real ones. ", "page_idx": 9}, {"type": "text", "text": "Second, to further evaluate the edited images, we merge them with the original training set of Internet Video dataset [37] and fine-tune the pre-trained model. The fine-tuned model is evaluated on the testing set (6,231 real images) of the InternetVideo Dataset. Baseline refers to the model trained on the InternetVideo dataset only. Here, surface normal accuracy is multiplied with BBox and axis, other evaluation metrics are the same as above. The evaluation results are summarized in Tab. 3. Compared with the baseline, the overall performance has been improved by enlarging the training set with edited images. The above two experiments illustrate how our PA-Diffusion model can benefit robotic vision tasks. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Even though our method can handle common articulated objects, there are still some limitations. First, as edited images are generated from inverted noise maps, the quality of the original input images significantly affects the editing outcomes. Blurry or low-resolution inputs will degrade the edited images. Second, when the object undergoes substantial deformation, this editing method is likely to fail. Besides, manipulating deformable objects and fluids remains challenging with this approach. Further explanation is provided in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces the PA-Diffusion model, a novel articulated object manipulation method that covers common object categories and supports arbitrary manipulation. Both the qualitative and quantitative experiments have proven the feasibility and effectiveness of our method. Besides, the 3D articulated object understanding experiment illustrates that the PA-Diffusion model has positive impacts on helping build robots that interact with the real world smartly. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China under Grants 62306175, the National Key R&D Program of China (No.2021ZD0110704), CCF-Tencent Rhino-Bird Open Research Fund, the National Key Research, Development Project of China (No.2022ZD0160102), the National Key Research and Development Project of China (No.2021ZD0110704), and Shanghai Artificial Intelligence Laboratory, XPLORER PRIZE grants. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Opencv find contour. https://docs.opencv.org/3.4/d4/d73/tutorial_py_ contours_begin.html. Accessed: 2010-09-30.   \n[2] Opencv find corner. https://pyimagesearch.com/2016/04/11/ finding-extreme-points-in-contours-with-opencv/. Accessed: 2010-09-30.   \n[3] Opencv solve pnp. https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html. Accessed: 2010-09-30.   \n[4] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021.   \n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediff:i Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \n[6] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In European conference on computer vision, pages 707\u2013723. Springer, 2022.   \n[7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023.   \n[8] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. arXiv preprint arXiv:2304.03373, 2023.   \n[9] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.   \n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[11] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822\u201319835, 2021.   \n[12] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986, 2023.   \n[13] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022.   \n[14] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages 89\u2013106. Springer, 2022.   \n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \n[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017, 2023.   \n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.   \n[21] Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, and Cewu Lu. Hake: A knowledge engine foundation for human activity understanding. TPAMI, 2022.   \n[22] Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, and Cewu Lu. Pastanet: Toward human activity knowledge engine. In CVPR, 2020.   \n[23] Yong-Lu Li, Yue Xu, Xiaohan Mao, and Cewu Lu. Symmetry and group in attribute-object compositions. In CVPR, 2020.   \n[24] Yong-Lu Li, Yue Xu, Xinyu Xu, Xiaohan Mao, Yuan Yao, Siqi Liu, and Cewu Lu. Beyond object recognition: A new benchmark towards object concept learning. In ICCV, 2023.   \n[25] Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yanfeng Wang, and Cewu Lu. Transferable interactiveness knowledge for human-object interaction detection. In CVPR, 2019.   \n[26] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and Jan Kautz. Planercnn: 3d plane detection and reconstruction from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4450\u20134459, 2019.   \n[27] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9298\u20139309, 2023.   \n[28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.   \n[30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[31] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023.   \n[32] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.   \n[33] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18444\u201318455, 2023.   \n[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[35] Xingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[36] Shengyi Qian and David F Fouhey. Understanding 3d object interaction from a single image. arXiv preprint arXiv:2305.09664, 2023.   \n[37] Shengyi Qian, Linyi Jin, Chris Rockwell, Siyi Chen, and David F Fouhey. Understanding 3d object articulation in internet videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1599\u20131609, 2022.   \n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.   \n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[41] Javier Romero, Dimitris Tzionas, and Michael J Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, 36(6), 2017.   \n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[43] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1\u201348, 2019.   \n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[46] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[47] Wikipedia contributors. 3d projection, 2004. [Online; accessed 22-July-2004].   \n[48] Wikipedia contributors. Affine transformation, 2004. [Online; accessed 22-July-2004].   \n[49] Wikipedia contributors. Perspective-n-point, 2004. [Online; accessed 22-July-2004].   \n[50] Julia Wolleb, Robin Sandk\u00fchler, Florentin Bieder, Philippe Valmaggia, and Philippe C Cattin. Diffusion models for implicit image segmentation ensembles. In International Conference on Medical Imaging with Deep Learning, pages 1336\u20131348. PMLR, 2022.   \n[51] Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. Image sculpting: Precise object editing with 3d geometry control. arXiv preprint arXiv:2401.01702, 2024.   \n[52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[53] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-Ming Cheng. Deep hough transform for semantic line detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):4793\u20134806, 2021.   \n[54] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems, 35:3609\u20133623, 2022.   \n[55] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix session, we first go through the pipeline of the PA-Diffusion model, and then provide more experiment results and detailed explanations, the arrangement is as follows: ", "page_idx": 14}, {"type": "text", "text": "Sec. A: Additional algorithm pipeline of the PA-Diffusion model.   \nSec. B: Additional articulated object manipulation results with the PA-Diffusion model.   \nSec. C: Additional ablation study and analysis.   \nSec. D: Additional explanation of 3D articulated object understanding experiment.   \nSec. E: Limitations and future research.   \nSec. F: Societal impacts and potential risks. ", "page_idx": 14}, {"type": "text", "text": "A Additional algorithm pipeline of the PA-Diffusion model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To facilitate the understanding of our proposed PA-Diffusion model, we present the entire algorithm pipeline in Algorithm 1. eq.1, eq.2, and eq.5 refer to the equations in the main paper. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 PA-Diffusion Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Require: Manipulate the articulated objects in RGB images Input: RGB image $x$ , Primitive Prototype Library Output: Edited RGB image $x_{e d i t}$ ", "page_idx": 14}, {"type": "text", "text": "Pre-Process: 1: Generate initial inverted noise map zITn itwith DDIMInvertion 2: Generate initial part-level masks $M^{I n i t}$ with GroundedSAM 3: Create 3D Abstract model and calibrate 2D image - 3D camera view ", "page_idx": 14}, {"type": "text", "text": "Manipulation: 4: Manipulate articulated objects in 3D space with text instructions or human interaction 5: Export manipulated part-level masks $\\bar{M}_{s}^{G e n}$ of seen part and $M_{n}^{G e n}$ of novel-appearing part ", "page_idx": 14}, {"type": "text", "text": "Feature Process: 6: Calculated manipulated inverted noise map zTM anas eq.1. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "7: Send initial , random $z_{T}^{R a n}$ , and manipulated $z_{T}^{M a n}$ inverted noise map to diffusion model   \n8: for $t=T,...,\\bar{1}$ do Construct compositional activation map $A_{t}^{G e n}$ as eq.2 Add extra loss items TCSL $L o s s_{t}$ and SCSL $L o s s_{s}$ as eq.5 end ", "page_idx": 14}, {"type": "text", "text": "9: Output: Edited image $x_{e d i t}=D e c o d e r(z_{0})$ ", "page_idx": 14}, {"type": "text", "text": "Dynamic Feature Maps: To make it clear, we simplify the $64\\times64\\times4$ inverted noise maps and activation maps to pure color block maps, as shown in Fig. 8. As the following equations show, we calculate the transform function Transform based on $p t s_{1}$ and $p t s_{3}$ , where $p t s_{1}$ and $p t s_{3}$ are corner points of the input image masks $A,B,C,D\\left(p t s_{1}\\right)$ and corner points of manipulated masks $\\boldsymbol{A}^{\"},\\boldsymbol{B}^{\"},\\boldsymbol{C}^{\"},\\boldsymbol{D}^{\"}\\left(p t s_{3}\\right)$ as introduced in the main paper. The corner points are automatically detected with a simple corner detection function. For simple actions such as moving and scaling, affine transform [48] is selected, while for rotation, manipulation, and other complex actions, perspective transform [47] is required. And then we can get the transformed inverted noise map T ranby transforming the initial inverted noise map $z_{T}^{I n i t}$ as following equations. Finally, the manipulated inverted noise map is calculated by adding zTT r anand zITn ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=T r a n s f o r m(p t s_{1},p t s_{3}),}}\\\\ {{z_{T}^{T r a n}=T(z_{T}^{I n i t}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, the compositional activation maps are generated by adding $A_{t}^{R a n}$ generated with random noise map and $A_{t}^{G^{\\bar{e}n}}$ generated with manipulated inverted noise map, as shown in the right part of Fig. 8. ", "page_idx": 14}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/ad4a381f346b179825046d1307cda47ff3798ffad8096da84e42a5bc42e433a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: Manipulating noise maps and constructing compositional activation maps. The manipulation is implemented with affine or perspective projection. The compositional activation map is constructed by merging two activation maps from different initial noise maps. ", "page_idx": 15}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/ef1cb5a1aa0599cd8737121e7f1353eb8864da3f8901ebccb3b2b42bddf65b7c.jpg", "img_caption": ["Figure 9: Manipulate 3D objects in Blender with text instructions. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Articulated Object Manipulation in 3D space: Manipulating the articulated objects in 3D space is straightforward in this work. As noted in the main paper, different manipulations can be done with text instructions or human interaction within Blender. ", "page_idx": 15}, {"type": "text", "text": "For the text-based method, considering the objects can be manipulated with Python scripts in Blender, we first construct a table mapping the text instructions to actions. Then these actions can be implemented by running Python scripts. As shown in Fig. 9, we require the laptop to rotate 30 degree, this text instruction is converted to Python script where the object matrix is multiplied with a rotation matrix and then set the new matrix to the object. Finally, running this script can finish the rotation action. For the second type, users can directly manipulate any parts of articulated objects in Blender, which is more flexible and convenient. ", "page_idx": 15}, {"type": "text", "text": "B Additional Manipulation Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Fig. 10 and Fig. 11, more articulated object manipulation results synthesized by our proposed PADiffusion model are demonstrated. Novel categories including door, toilet, and book are experimented with here. For various categories, joint types, and backgrounds, our proposed method can manipulate the objects and preserve other contents in the input images simultaneously. ", "page_idx": 15}, {"type": "text", "text": "C Additional Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To analyze and explain our proposed PA-Diffusion model in detail, we provide more ablation studies in this session. ", "page_idx": 15}, {"type": "text", "text": "Additional Loss Items: In the main paper, we qualitatively demonstrate the effect of TCSL and SCSL in the experiment part. Here, we evaluate them quantitatively. Following the main paper, the evaluation metric is the FID score. The results are summarized in Tab. 4. It is obvious that without TCSL, the performance degrades significantly since the feature cannot be transferred to the edited images correctly leading to inconsistent appearance compared to the input images. On the other hand, without SCSL, the style of novel-appearing parts is inconsistent with the original, therefore the outcome is still unsatisfactory. When including the two losses, the edited images are close to the input real images in the aspects of color, texture, and style. ", "page_idx": 15}, {"type": "text", "text": "Primitive Prototype Library: In the main paper, we create 6 simple primitive prototypes to represent 6 different kinds of articulated objects in the testbench. Since our method does not require precise 3D CAD models of objects, Primitive Prototype Library can cover a wide range of articulated objects with a small number of primitive prototypes. As shown in Fig. 10 and Fig. 11, books can be represented as laptops, doors can be represented with simple planes, and toilets can be represented with a plane and a box. No extra prototypes are required when creating abstract 3D models for novel categories. Besides, the edited images are still high-fidelity and high-quality. Furthermore, the Primitive Prototype Library is easy to expand, more primitive prototypes can be created rapidly when dealing with novel articulated objects. ", "page_idx": 15}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/a1e29e2006ac02bbf37b17f370d029b25a3699238239047ce1c62dfafe156eaf.jpg", "img_caption": ["Figure 10: Additional demonstration of the images edited with our proposed PA-Diffusion model, including storage furniture, laptop, microwave, trashcan, door, drawer, refrigerator, and toilet. ", "Table 4: FID score of edited images with different additional losses: no SCSL and TCSL, with SCSL only, with TCSL only, and with all losses. The performance improves more than $57\\%$ with the assistance of the two losses. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "WRd9LCbvxN/tmp/9d2a261282ae26f5cc4c775bc9d2f4ab7359a6d85e8f163f17b8ab2823c8102b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/c5513e7358c960d429287338f091418daa22bd42a60ae9df7a7564a5eba7a000.jpg", "img_caption": ["Figure 11: Additional demonstration of editing images with manipulation process based on our proposed PA-Diffusion model. The last three columns are novel articulated object categories. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "2D-3D Models analysis: Besides the advantage of convenience, we also compare the quality of reconstructed 3D models with state-of-the-art 2D-3D methods and abstract 3D models. ", "page_idx": 17}, {"type": "text", "text": "Following the method introduced in Image Sculpting [51], we use ClipDrop to remove the background of input images, and then reconstruct 3D models with Zero123 [27]. The tested images are the same as those in the main paper. The reconstructed 3D object mesh models are shown in Fig. 12 including the front view, side view, and the edited images after manipulation. We notice that the result of storage furniture is acceptable, the shape closely matches the original, and the texture is stored in UV maps correctly. However, for other categories, the reconstructed models are poor which is the main reason for low-quality edited images. ", "page_idx": 17}, {"type": "text", "text": "In the main paper, we mentioned that manipulating the reconstructed 3D models created by 2D-3D methods is tedious and inaccurate. As shown in Fig. 12, the reconstructed models are not part-level (only one mesh object), tremendous human effort is required to cut the mesh into parts before manipulation. For example, when trying to split the door from the body of storage furniture, we need to cut the furniture into several parts first (top, bottom, four sides of the body), and then merge others except the door. This process, even with advanced 3D graphics software, is complex and time-consuming. As a result, summarizing reconstruction, manipulation, and generation time, Image Sculpting [51] requires over 10 mins to manipulate one image, making it unsuitable for large-scale image editing tasks. ", "page_idx": 17}, {"type": "text", "text": "In summary, using abstract 3D models to present articulated objects offers several advantages: (1) State-of-the-art 2D-3D methods are still not robust enough to create precise 3D models such as laptops and trashcans. In comparison, it is easy to achieve abstract 3D models with primitive prototypes, as shown in Fig. 12. (2) Manipulating primitive prototypes in 3D space is easier and more accurate than manipulating a single 3D object mesh. (3) Seen parts and novel-appearing parts can be defined and extracted easily. (4) Novel categories and instances can be handled with our method efficiently. (5) Our method is time-efficient and thus can support various downstream tasks. ", "page_idx": 17}, {"type": "text", "text": "D 3D Articulated Objects Understanding ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Overview. In the main paper, we introduce the 3D articulated object understanding experiment and demonstrate how our proposed PA-Diffusion model supports other research fields. Here, we discuss the experiment setup and implementation in more detail. ", "page_idx": 17}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/494b7d10ca22ce6eb3d0e5e59e4ad1c1c0c4fe07e8216cdb54bee366b2cca1db.jpg", "img_caption": ["Figure 12: Reconstructed 3D object models with Image Sculpting, and our abstract 3D models created with Primitive Prototype Library. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Annotation Generation. In previous works [37, 36, 26], human labeling is required for annotating the bounding boxes, rotation/translation axes, and surface normal, which is labor-intensive and inaccurate, especially for 3D scenario. On the contrary, by representing objects with abstract 3D models, our method can achieve these annotations automatically. ", "page_idx": 18}, {"type": "text", "text": "As shown in Fig. 13, part1 and part2 are the masks of articulated objects\u2019 parts that are exported from Grounded SAM or Blender software automatically. The corner points can be calculated with [1, 2]. Then, the bounding box of each object part can be calculated with these corner points. The rotation and translation axis annotations are represented as $[x_{0},y_{0},x_{1},y_{1}]$ , where $x_{0},y_{0},x_{1},y_{1}$ are the coordinates of corner points in part-level masks, different object category uses different corner points. For the 3D surface normal annotations, as shown in Fig. 14, we first export the object\u2019s part world transform matrix $M_{o b j}$ and camera world transform matrix $M_{c a m e r a}$ . Then the two matrices are normalized and calibrated to create the aligned transform matrix $M_{a l i g n e d}$ . Consequently, we simply select the normal of the outer plane to represent the orientation of the object part, and the surface normal $V_{s u r f}$ is equal to the multiplication of the aligned matrix and local plane vector $V_{p l a n e}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M_{a l i g n e d}=A l i g n(N o r m(M_{c a m e r a}),N o r m(M_{o b j})),}\\\\ {V_{s u r f}=M_{a l i g n e d}\\times V_{p l a n e}.\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Evaluation Matrix. For quantitative evaluation, we follow [37] to calculate the average precision of the bounding box, axis, and surface normal. The bounding box is the traditional horizontal type, the threshold of IoU is set as 0.5. The predicted axes are measured with EA score as [53]. To demonstrate the results clearly, we calculate the surface normal error and measure the accuracy that the error is smaller than the threshold $30^{\\circ}$ . ", "page_idx": 18}, {"type": "text", "text": "E Limitations and Future Research ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The limitations of our proposed PA-Diffusion models have been discussed in the main paper. Due to the inaccuracy of DDIM inversion, the inverted noise map might be poor if the input image quality is low. Unfortunately, the poor noise map will lead to mismatch error accumulation and propagation during the iterative denoising process. As in the left of Fig. 15, when the original input image is of low resolution (it is normalized to $512\\times512$ before manipulation), the PA-Diffusion model cannot re-generate the original image with the inverted noise map. Simple actions like moving and scaling also cannot be completed. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Manipulating the initial inverted noise maps is critical to preserve the appearance of seen parts. However, as discussed in the main paper, this step disturbs the original data distribution. The problem will be too serious to be fixed when the object shape deformation is large. As shown in the right of Fig. 15, when reshaping the laptop to a slim non-uniform diamond shape, the object\u2019s appearance cannot be preserved. ", "page_idx": 19}, {"type": "text", "text": "Considering this situation, one promising solution is to add stronger and more precise supervision loss in each denoising step. This is beyond the scope of this work, we plan to implement this later. ", "page_idx": 19}, {"type": "text", "text": "In the future, more categories of articulated objects will be covered and the edited image dataset will be expanded to millions-scale for supporting various computer vision and robotic manipulation tasks. Next, we will extend this method to handle deformable objects and fluids. ", "page_idx": 19}, {"type": "text", "text": "F Societal impacts and potential risks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The articulated object manipulation method presented in this work has profound positive societal implications. This method can serve as a fundamental tool to benefti other computer vision or robot vision tasks. Consequently, the artificial intelligent algorithm can understand and interact with the real world better. Humans will have stronger AI assistants including smart offices, intelligent home or medical robots, and so on. ", "page_idx": 19}, {"type": "text", "text": "All the models and data used in this work are collected from the Website. No personal information is used. The code and data created in this work have a low risk of misuse. ", "page_idx": 19}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/a5bcb3b36f370bcb75e59dedbcc2c9690c6a9f2267884c62d0bda62980628ee2.jpg", "img_caption": ["Figure 13: Demonstration of extracting bounding box annotations and rotation/translation axis annotations from part-level masks. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/e4e5e2e6ac356a76a4d95a960ccffbab6078adec520fbc97f14982a7b57301d8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 14: The camera poses and orientations of 6 planes in Blender, (1)-(6) refers to abstract 3D models of cabinet, laptop, microwave, trashcan, drawer, and refrigerator separately. Red arrows are the surface normal directions. ", "page_idx": 20}, {"type": "image", "img_path": "WRd9LCbvxN/tmp/19afeb99be58c1a4de2625811cf91b58472f3b4944309f91cd91b1292955a5b6.jpg", "img_caption": ["Figure 15: Limitation of the PA-Diffusion model: dealing with low-quality images or large deformation. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The contribution: PA-Diffusion model, a novel articulated object manipulation method in real images has been claimed in the abstract. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The limitation is discussed in both the main paper and the Appendix ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper introduces a novel method, there is no theoretical result. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The completed algorithm pipeline is provided in the Appendix. Each step is also introduced in both the main paper and the Appendix. The context explanation and figures are aligned to explain the proposed method. Besides, to make the algorithm easy to understand, we provide a simpler explanation in the Appendix as well. All the experimental results can be reproduced. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Coda and data will be publicly available later. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the training and test details are described in the session of the experiment in the main paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper reports the experiment results on certain datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar and then state that they have a $96\\%$ CI if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of computing workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The required computing resources are claimed in the session of the experiment in the main paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute worker CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more computing than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research conforms with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g. if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: It is discussed in the Appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for the responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: It is described in the Appendix. The code and data of this work have a quite low risk of misuse. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make the best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited, and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All the models and data used in this work are published, and all the citations have been included in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No new assets are introduced in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no crowdsourcing experiments and research in this work ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing or research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work has no crowdsourcing experiments and research. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing or research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]