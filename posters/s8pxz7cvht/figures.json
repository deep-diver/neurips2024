[{"figure_path": "s8Pxz7cvHT/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the proposed Adversarial Attacks in Diffusion (AdvAD) that models the attack as a non-parametric diffusing process. At each step, Attacked Model Guidance (AMG) module adopts the non-Markovian process for approximating xadv using to craft adversarial guidance and injects it into the initialized diffusion noise, then Pixel-level Constraint (PC) module imposes restriction to produce the noise for the next step and serves to control the whole process precisely.", "description": "This figure illustrates the AdvAD framework.  It shows how the attack process is modeled as a non-parametric diffusion process, starting from an initial noise and gradually evolving towards the adversarial example. At each step, the AMG (Attacked Model Guidance) module uses the attacked model to craft subtle adversarial guidance, which is then refined by the PC (Pixel-level Constraint) module. This ensures that the diffusion process remains close to the original image, maintaining imperceptibility while still achieving a successful attack. The figure visually depicts the process using a hot air balloon as the input image.", "section": "3 Proposed Adversarial Attacks in Diffusion"}, {"figure_path": "s8Pxz7cvHT/figures/figures_7_1.jpg", "caption": "Figure 2: Visualizations of adversarial examples and corresponding perturbations crafted by nine imperceptible attacks. Perturbations are amplified as marked in top-right for the convenience of observation. Please zoom in to observe the details of the images with original resolution of 224 \u00d7 224.", "description": "This figure visualizes the adversarial examples and their corresponding perturbations generated by nine different imperceptible attack methods. The perturbations are amplified for better visibility.  The figure aims to demonstrate the differences in the visual quality and imperceptibility of the attacks, showing how some attacks result in more noticeable changes than others. It is important to zoom in to fully appreciate the details at the original image resolution.", "section": "4.2 Comparison with State-of-the-art Methods"}, {"figure_path": "s8Pxz7cvHT/figures/figures_8_1.jpg", "caption": "Figure 3: Rubostness on JPEG compression and Bit-depth reduction with different factors.", "description": "This figure shows the robustness of different imperceptible attacks against two common image processing defenses: JPEG compression and bit-depth reduction. The x-axis represents the level of the defense (e.g., different compression ratios or bit depths). The y-axis shows the attack success rate (ASR) achieved by each attack method. The figure demonstrates the robustness of the proposed AdvAD method, which maintains high ASR even under strong defenses compared to other attack methods.", "section": "4.3 Robustness"}, {"figure_path": "s8Pxz7cvHT/figures/figures_8_2.jpg", "caption": "Figure 1: Overview of the proposed Adversarial Attacks in Diffusion (AdvAD) that models the attack as a non-parametric diffusing process. At each step, Attacked Model Guidance (AMG) module adopts the non-Markovian process for approximating xadv using to craft adversarial guidance and injects it into the initialized diffusion noise, then Pixel-level Constraint (PC) module imposes restriction to produce the noise for the next step and serves to control the whole process precisely.", "description": "This figure illustrates the AdvAD framework, which models adversarial attacks as a non-parametric diffusion process.  It starts with an initial Gaussian noise and iteratively refines it through a series of steps. At each step, two modules, Attacked Model Guidance (AMG) and Pixel-level Constraint (PC), work together. AMG uses the attacked model to craft subtle yet effective adversarial guidance, injected into the noise. PC then constrains the noise to ensure the modified trajectory stays close to the original, maintaining imperceptibility.  The process continues until the final adversarial example (xadv) is generated.", "section": "3 Proposed Adversarial Attacks in Diffusion"}, {"figure_path": "s8Pxz7cvHT/figures/figures_9_1.jpg", "caption": "Figure 5: Values of \u03bbt (left) and ||\u03b4t||\u221e (right) of Eq. (12) throughout the diffusion process.", "description": "This figure visualizes the values of \u03bbt and ||\u03b4t||\u221e, which are key components in the mathematical formulation of the AdvAD attack process.  The left panel shows the values of \u03bbt, which represents the coefficient that scales the magnitude of the adversarial guidance injected at each step of the diffusion process.  The right panel displays the infinity norm of \u03b4t (||\u03b4t||\u221e), which signifies the maximum change in the noise at each step.  Both graphs show a decrease over time (t), indicating that the strength of the adversarial perturbation reduces progressively as the attack progresses, thus contributing to the imperceptibility of the adversarial examples.", "section": "4.5 Analysis"}, {"figure_path": "s8Pxz7cvHT/figures/figures_22_1.jpg", "caption": "Figure 1: Overview of the proposed Adversarial Attacks in Diffusion (AdvAD) that models the attack as a non-parametric diffusing process. At each step, Attacked Model Guidance (AMG) module adopts the non-Markovian process for approximating xadv using to craft adversarial guidance and injects it into the initialized diffusion noise, then Pixel-level Constraint (PC) module imposes restriction to produce the noise for the next step and serves to control the whole process precisely.", "description": "This figure provides a visual representation of the AdvAD model, illustrating its non-parametric diffusion process for generating imperceptible adversarial examples. It details the steps involved, starting from an initial noise and progressively incorporating adversarial guidance via the AMG and PC modules, to reach a final adversarially conditioned distribution. The process is achieved without any additional networks, using only the attacked model.", "section": "3 Proposed Adversarial Attacks in Diffusion"}]