[{"figure_path": "z4FaPUslma/tables/tables_7_1.jpg", "caption": "Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.", "description": "This table presents the top-1 training accuracy results for different neural network models trained on four benchmark datasets (CIFAR-10, CIFAR-100, STL-10, and ImageNet).  Three training methods are compared: standard training, fixed simplex ETF, and the proposed implicit ETF method. The results are reported at epochs 50 and 200, showcasing the convergence speed and performance differences between methods.  The median accuracy and the range of values across five runs with different random seeds are presented to demonstrate the stability of the method.", "section": "4 Experiments"}, {"figure_path": "z4FaPUslma/tables/tables_7_2.jpg", "caption": "Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.", "description": "This table presents the top-1 training accuracy results for different neural network models trained on four benchmark datasets (CIFAR-10, CIFAR-100, STL10, and ImageNet).  The results are shown for two different epochs (50 and 200) and for three training methods: Standard, Fixed ETF, and Implicit ETF.  The median accuracy and the range of accuracy across five different runs are presented.  Bold values indicate the best result for each dataset and epoch.", "section": "4 Experiments"}, {"figure_path": "z4FaPUslma/tables/tables_8_1.jpg", "caption": "Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.", "description": "This table shows the training top-1 accuracy results for different neural network models (ResNet18, VGG13, ResNet50) trained on various datasets (CIFAR10, CIFAR100, STL10, ImageNet).  The results are presented as median values across five runs with different random seeds, along with the range of values.  The table compares three training methods: standard training, fixed ETF, and implicit ETF.  The best results for each dataset and network architecture are highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "z4FaPUslma/tables/tables_9_1.jpg", "caption": "Table 3: GPU memory (in Gigabytes) during training.", "description": "This table shows the GPU memory usage in gigabytes for different models and training methods.  It compares the memory consumption of the standard training method, the fixed ETF method, and the implicit ETF method (with and without the DDN backward pass). The models include various UFM sizes (UFM-10, UFM-100, UFM-200, UFM-1000) and several real-world models trained on CIFAR10 and ImageNet datasets using ResNet18 and ResNet50 architectures.", "section": "Experiments"}]