[{"figure_path": "Wxc6KvQgLq/figures/figures_1_1.jpg", "caption": "Figure 1: The GOOSE framework for learning heuristic functions for numeric planning. Cyan colours indicate components that are influenced by the training phase. (a) A numeric planning state and goal condition is encoded into a graph G via the vILG representation defined in Defn. 3.1. (b) Graphs are either embedded into vectors x in Euclidean space with the CCWL kernel from Sec. 3 or transformed into a graph G' with a real-valued matrix representing node features as inputs into GNNs described in Sec. 4. (c) Features x are fed into a linear model, whereas transformed graphs G' are fed into GNNs. (d) Linear models are either trained by the ranking formulation in Eq. 1 or by Support Vector Regression (SVR) with a linear kernel. GNN models are either trained by the ranking formulation in Eq. 2 or by backpropagation minimising the mean squared error (MSE) loss.", "description": "This figure illustrates the GOOSE framework, which is a method for learning heuristic functions for numeric planning.  It shows how a numeric planning state and goal are encoded into a graph, which is then processed using either a classical machine learning approach (CCWL kernel and linear model) or a deep learning approach (graph neural network).  The resulting heuristic function can then be used in a search algorithm to solve numeric planning problems. The cyan coloring highlights the parts of the system affected by the training phase.", "section": "1 Introduction"}, {"figure_path": "Wxc6KvQgLq/figures/figures_4_1.jpg", "caption": "Figure 2: An example ccBlocksworld task where each base has capacity 3 (left), a subgraph of its ILG representation (middle), and the matrix representation of the node features of the ILG (right).", "description": "This figure illustrates an example of Capacity Constrained Blocksworld (ccBlocksworld) planning task, showing its representation in three different ways: (left) a visual representation of the initial state and goal condition; (middle) a subgraph of its Numeric Instance Learning Graph (vILG) representation showing the nodes (objects, variables, and goals) and edges representing relationships between them; and (right) a matrix representation of the node features in the vILG, showing both categorical and continuous features used to encode the planning problem.", "section": "3 Relational features for numeric planning"}, {"figure_path": "Wxc6KvQgLq/figures/figures_4_2.jpg", "caption": "Figure 1: The GOOSE framework for learning heuristic functions for numeric planning. Cyan colours indicate components that are influenced by the training phase. (a) A numeric planning state and goal condition is encoded into a graph G via the vILG representation defined in Defn. 3.1. (b) Graphs are either embedded into vectors x in Euclidean space with the CCWL kernel from Sec. 3 or transformed into a graph G' with a real-valued matrix representing node features as inputs into GNNs described in Sec. 4. (c) Linear models are either trained by the ranking formulation in Eq. 1 or by Support Vector Regression (SVR) with a linear kernel. GNN models are either trained by the ranking formulation in Eq. 2 or by backpropagation minimising the mean squared error (MSE) loss.", "description": "This figure illustrates the GOOSE framework, a machine learning approach for solving numeric planning problems.  It shows how a numeric planning state and goal are encoded as a graph (a).  This graph is then processed using either a classical method (CCWL kernel, embedding to vector) or a deep learning method (GNN, transformation to graph G').  Finally, the resulting features are used to train linear models or GNNs using ranking formulations or MSE loss (d). The cyan color highlights the components modified during training.", "section": "1 Introduction"}, {"figure_path": "Wxc6KvQgLq/figures/figures_4_3.jpg", "caption": "Figure 3: CCWL with one iteration, POOL = \u2211, and C = [4].", "description": "This figure illustrates one iteration of the CCWL algorithm.  It shows how categorical and continuous node features are processed.  Categorical features (colors) are updated using a hash function that incorporates both the node's current color and the colors of its neighbors.  Continuous features are pooled (summed in this example) for each color group. The resulting combined categorical and continuous features are used to generate the final feature vector for the graph.", "section": "3 Relational features for numeric planning"}, {"figure_path": "Wxc6KvQgLq/figures/figures_6_1.jpg", "caption": "Figure 4: Examples of heuristic functions that achieve 0 loss when optimising cost-to-go (left) and ranking (right) on an optimal plan. Coloured nodes indicate states on the optimal plan with the goal state indicated by a double circle. Edges indicate successors of a node. A cost-to-go heuristic can achieve 0 loss on the plan trace but may not generalise correctly to state successors. A ranking heuristic does not need represent correct cost-to-go values and only need to satisfy ranking constraints. GBFS will return a plan in linear time for the ranking heuristic here but not for the cost-to-go heuristic.", "description": "This figure shows examples of heuristic functions that achieve zero loss when optimizing either for cost-to-go or ranking.  The left side shows a cost-to-go heuristic, which can achieve zero loss on the optimal plan path, but may not generalize well to other states.  The right side demonstrates a ranking heuristic, which doesn't need perfect cost-to-go values but only needs to maintain correct rankings between states.  The figure highlights that the Greedy Best First Search (GBFS) algorithm is significantly more efficient for ranking heuristics than cost-to-go heuristics.", "section": "Optimisation formulations for learning heuristic functions"}, {"figure_path": "Wxc6KvQgLq/figures/figures_7_1.jpg", "caption": "Figure 5: Number of objects in training and testing problems (left) and distributions of training data generation time with number of training problems (right) per domain. Note the log scales.", "description": "This figure presents a comparative analysis of the number of objects across different domains used in the training and testing phases of the experiments.  The left panel displays bar charts showing the number of objects (on a logarithmic scale) in the training and testing datasets for each of eight domains: Blocksworld, Childsnack, Ferry, Miconic, Rovers, Satellite, Spanner, and Transport.  The right panel uses box plots to illustrate the distribution of training data generation times (also on a logarithmic scale) for each domain, providing insight into the computational effort required for data preparation in each case. This helps to understand the resources needed and potential differences in training times for each domain.", "section": "6 Experiments"}, {"figure_path": "Wxc6KvQgLq/figures/figures_9_1.jpg", "caption": "Figure 6: Plot comparisons of expanded nodes and plan length of selected pairs of models with L = 1. A point (x, y) represents the metric of the models indicated on the x and y axis on the domain. Points on the top left (resp. bottom right) triangle favour the model on the x-axis (resp. y-axis).", "description": "This figure compares the performance of different models in terms of the number of nodes expanded and the plan length generated.  Each plot shows a comparison between two model types across eight domains. The points in the plot represent the performance metrics (x,y) for each domain; points in the top-left quadrant indicate better performance for the model on the x-axis, while those in the bottom-right quadrant suggest better performance for the model on the y-axis. The plots help to visually assess the relative strengths and weaknesses of the different models.", "section": "Experiments"}, {"figure_path": "Wxc6KvQgLq/figures/figures_15_1.jpg", "caption": "Figure 2: An example ccBlocksworld task where each base has capacity 3 (left), a subgraph of its ILG representation (middle), and the matrix representation of the node features of the ILG (right).", "description": "This figure shows an example of a Capacity Constrained Blocksworld planning task.  The left side depicts the initial state of the problem: blocks stacked on three bases, each with a capacity of 3 blocks. The middle shows a subgraph of the Numeric Instance Learning Graph (vILG) representation of this task.  The nodes in the graph represent objects, propositional variables, numeric variables, and goal conditions, while edges connect these nodes based on their relationships. The right side displays the node feature matrix of the vILG. This matrix encodes the categorical and continuous node features (such as object type, propositional/numeric variable values, etc.)  as input for later machine learning methods.", "section": "3 Relational features for numeric planning"}, {"figure_path": "Wxc6KvQgLq/figures/figures_19_1.jpg", "caption": "Figure 8: Distributions of heuristic evaluation time for GNN and CCWL models with L = 1 on problems where both were able to solve in the given timeout. Blue box plots correspond to GNN models and red box plots correspond to CCWL models.", "description": "This figure shows the distribution of heuristic evaluation times for Graph Neural Networks (GNNs) and the Color-Coded Weisfeiler-Lehman (CCWL) algorithm.  The box plots display the median, quartiles, and outliers for each model across various planning domains.  Blue boxes represent GNNs, while red boxes represent CCWL.  The results illustrate the relative computational efficiency of each method for heuristic computation within the context of numeric planning.", "section": "6 Experiments"}]