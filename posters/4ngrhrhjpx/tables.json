[{"figure_path": "4NGrHrhJPx/tables/tables_20_1.jpg", "caption": "Table 1: Baseline value factorization algorithms", "description": "This table lists four popular value factorization algorithms used as baselines in the paper's experiments.  It includes QMIX, which learns a mixer of individual utilities with monotonic constraints; QPLEX, which learns a mixer of advantage functions and state value functions; DMIX, which integrates distributional reinforcement learning with QMIX; and RMIX, which integrates risk-sensitive reinforcement learning with QMIX.  These algorithms are compared against the proposed ReBorn method to demonstrate its effectiveness in improving performance.", "section": "D.1 Experimental Setup"}, {"figure_path": "4NGrHrhJPx/tables/tables_20_2.jpg", "caption": "Table 2: Hyperparameter of different value factorization algorithms", "description": "This table lists the hyperparameters used for four different value factorization algorithms (QMIX, QPLEX, DMIX, and RMIX) in the experiments.  These hyperparameters control various aspects of the training process, including the exploration strategy (epsilon greedy), batch size, buffer size, learning rate, optimizer, runner type, mixing network dimensions, target update interval, discount factor, dormant neuron threshold, over-active neuron threshold, and the execution interval.  The values provided represent the settings used in the experiments reported in the paper.", "section": "6.1 Environmental Setup"}, {"figure_path": "4NGrHrhJPx/tables/tables_21_1.jpg", "caption": "Table 3: Methods in Experimental Section", "description": "This table summarizes the different methods used in the experimental section. It shows the application of ReBorn, ReDo, and ReSet to different parts of the algorithms (Mixing Network vs. Whole Network) and whether the Knowledge Invariant principle is satisfied.", "section": "D.1 Experimental Setup"}, {"figure_path": "4NGrHrhJPx/tables/tables_21_2.jpg", "caption": "Table 4: Comparison of Predator-prey Configurations", "description": "This table shows different configurations used in the Predator-prey environment. Each configuration varies in the number of predators, the number of preys, the map size, and the reward given for capturing a prey.  This allows researchers to test the algorithms in different levels of complexity.", "section": "D.2.1 Predator-prey"}, {"figure_path": "4NGrHrhJPx/tables/tables_22_1.jpg", "caption": "Table 5: Overview of SMAC scenarios used in the experiment.", "description": "This table lists five scenarios from the StarCraft Multi-Agent Challenge (SMAC) used in the paper's experiments.  For each scenario, it provides the difficulty level, the types and number of allied units controlled by the MARL agents, and the types and number of enemy units controlled by the game's built-in AI.", "section": "D.2.2 StarCraft II Multi-Agent Challenges (SMAC)"}, {"figure_path": "4NGrHrhJPx/tables/tables_22_2.jpg", "caption": "Table 6: Overview of SMACv2 scenarios used in the experiment.", "description": "This table lists the configurations of three different scenarios from the SMACv2 environment used in the paper's experiments. Each scenario involves 10 allied units and 11 enemy units.  The \"Unit Types\" column indicates the types of units included in each scenario: Zerglings, Hydralisks, and Banelings for the Zerg scenario; Marines, Marauders, and Medivacs for the Terran scenario; and Stalkers, Zealots, and Colossi for the Protoss scenario. These variations are intended to test the robustness and generalizability of the proposed ReBorn method.", "section": "D.2.3 SMACv2"}]