[{"figure_path": "4NGrHrhJPx/figures/figures_3_1.jpg", "caption": "Figure 1: The existence of Dormant Neuron Phenomenon in Value Function Factorization Methods.", "description": "This figure shows the percentage of dormant neurons in both agent networks and mixing networks across three different SMAC scenarios (3s_vs_5z, MMM2, and 27m_vs_30m) while training with QMIX and QPLEX algorithms.  It visually demonstrates the increase in dormant neurons over training steps, especially within the mixing networks, highlighting the dormant neuron phenomenon in MARL value factorization. The x-axis represents the number of environmental steps (training iterations), and the y-axis shows the percentage of dormant neurons.", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_3_2.jpg", "caption": "Figure 2: (a) The MSE Loss for fitting a simple Mixing Network increases with an increasing number of Dormant Neurons. It indicates that dormant neurons hurt mixing network expressivity. (b) The percentage of dormant neurons in QMIX mixing network with different target network update intervals. (c) The Normalized Activation Score (NAS) percentage ranking for top-25 over-active neurons in the QMIX mixing network.", "description": "This figure demonstrates three key findings related to dormant neurons in multi-agent reinforcement learning. (a) shows that as the number of dormant neurons in a simple mixing network increases, the mean squared error (MSE) loss also increases, indicating a negative impact on network expressivity. (b) illustrates how the percentage of dormant neurons in a QMIX mixing network changes with varying target network update intervals; smaller intervals (more non-stationarity) lead to higher dormant neuron percentages. Finally, (c) shows the distribution of normalized activation scores (NAS) for the top 25 most active neurons, highlighting the existence of over-active neurons with significantly larger scores than the rest.", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_4_1.jpg", "caption": "Figure 3: Over-active neurons in QMIX mixing networks: (a) The percentage contribution of the number of\ndormant neurons (depicted as Dormant), the number of over-active neurons (depicted as Overactive-Number),\nthe sum of NAS (depicted as Overactive-Sum) for over-active neurons over time. (b) Overlap coefficients for\nDormant/Over-active neurons between the current iteration and previous iterations. (c) Percentage of dormant\nneurons that re-enter dormancy after ReDo within different time steps.", "description": "This figure analyzes over-active neurons in the QMIX mixing network. Subfigure (a) shows the percentage contribution of dormant neurons, over-active neurons, and the sum of normalized activation scores (NAS) of over-active neurons over time. Subfigure (b) illustrates the overlap between dormant and over-active neurons across different training iterations. Subfigure (c) shows the percentage of dormant neurons that become dormant again after applying the ReDo method.", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_5_1.jpg", "caption": "Figure 4: The procedure of ReBorn neurons. The weights of over-active neurons are distributed to M randomly picked dormant neurons.", "description": "The figure illustrates the ReBorn mechanism.  It shows an over-active neuron (orange) whose weights (win, wout, bx) are redistributed to M randomly selected dormant neurons (grey).  The dormant neurons become \"reborn neurons\" (yellow). The weights of the reborn neurons are adjusted using \u03b1i and \u03b2i, which are randomly sampled from specified ranges to introduce variation.  This weight transfer aims to alleviate the dormant neuron phenomenon by balancing neuron activity.", "section": "5 The ReBorn Method"}, {"figure_path": "4NGrHrhJPx/figures/figures_7_1.jpg", "caption": "Figure 5: ReBorn can improve the performance of various value factorization algorithms: (a-b) the test win rate for the 3s5z_vs_3s6z and the MMM2 environments, (c) the return for predator-prey small environment, (d-f) the dormant percent for the the 3s5z_vs_3s6z, the MMM2, and the predator-prey small environment.", "description": "This figure shows the results of applying the ReBorn method to various value factorization algorithms (QMIX, QPLEX, RMIX) in different environments.  The plots show the test win rate (a, b) and return (c) for three different environments: 3s5z_vs_3s6z, MMM2, and predator-prey small. Additionally, the plots demonstrate the percentage of dormant neurons over time for each algorithm in the same three environments (d, e, f).  The results illustrate that ReBorn improves the performance and reduces the number of dormant neurons across various algorithms and environments.", "section": "6.2 ReBorn can improve the performance of various value factorization algorithms"}, {"figure_path": "4NGrHrhJPx/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison with other methods that satisfy the KI principle.", "description": "The figure compares the performance of ReBorn against other parameter perturbation methods (ReDo, ReSet, SR, MARR) that satisfy the Knowledge Invariant (KI) principle across various MARL value factorization algorithms (QMIX, QPLEX, RMIX) and environments (MMM2, 27m_vs_30m, predator-prey large). The results demonstrate that ReBorn consistently outperforms other methods in terms of win rate, dormant neuron ratio, and over-active neuron sum ratio, highlighting its effectiveness in addressing the dormant neuron phenomenon while preserving learned cooperation knowledge.", "section": "6.4.3 Sensitivity analyses of hyper-parameters"}, {"figure_path": "4NGrHrhJPx/figures/figures_8_2.jpg", "caption": "Figure 7: Importance of satisfying the KI Principle for (a) QMIX, (b) QPLEX, and (C) RMIX. A variant of ReBorn without satisfying the KI Principle is depicted as Reborn w/o KI.", "description": "The figure demonstrates the importance of adhering to the Knowledge Invariance (KI) principle in the ReBorn method.  It shows the test win rate for three different MARL algorithms (QMIX, QPLEX, RMIX) in the MMM2 scenario of the StarCraft Multi-Agent Challenge (SMAC) environment.  Each algorithm is tested with and without the KI principle being satisfied. Results indicate that adhering to the KI principle significantly improves performance, while violating it leads to performance drops.  This highlights the importance of the KI principle for effective MARL.", "section": "6.4.1 Satisfying the KI Principle is of great importance"}, {"figure_path": "4NGrHrhJPx/figures/figures_9_1.jpg", "caption": "Figure 8: Comparison with other methods that satisfy the KI principle.", "description": "This figure compares the performance of ReBorn against other methods that also satisfy the Knowledge Invariant principle on three different MARL value factorization algorithms (QMIX, QPLEX, RMIX).  Each subplot shows the test win rate over environmental steps.  The comparison demonstrates the effectiveness of ReBorn, which outperforms alternative KI-satisfying methods in improving the performance of these algorithms.  The different variants of ReBorn (ReDo, ReSet, Reverse ReDo, Pruning) are compared to the baseline ReBorn method to highlight the impact of different weight-sharing strategies.", "section": "6.4.2 ReBorn is better than other methods that satisfy the KI principle"}, {"figure_path": "4NGrHrhJPx/figures/figures_9_2.jpg", "caption": "Figure 9: Ablation of different hyperparameters in ReBorn. (a) the dormant threshold \u03b1. (b) the over-active threshold \u03b2. (c) the ReBorn interval T.", "description": "This figure presents an ablation study on the hyperparameters of the ReBorn algorithm. It shows the impact of varying the dormant neuron threshold (\u03b1), the over-active neuron threshold (\u03b2), and the ReBorn interval (T) on the algorithm's performance.  The results illustrate the sensitivity of the ReBorn algorithm to these hyperparameters and suggest that appropriate settings are essential for optimal performance.  Specifically, it demonstrates how finding the right balance in identifying dormant and over-active neurons and their weight redistribution frequency affects the overall performance and the reduction of dormant neurons.", "section": "6.4.3 Sensitivity analyses of hyper-parameters"}, {"figure_path": "4NGrHrhJPx/figures/figures_15_1.jpg", "caption": "Figure 1: An example to show that ReDo does not satisfy KI principle.", "description": "This figure illustrates an example demonstrating that the ReDo method fails to satisfy the Knowledge Invariant (KI) principle.  It shows a three-layer mixing network before and after applying the ReDo parameter perturbation method. The network processes a joint state-action history (\u03c4) and joint actions (u), producing a joint state-action value function Q(\u03c4, u).  The weights of the network are shown, and the left side depicts the network before ReDo, while the right side displays it after ReDo.  Importantly, ReDo modifies the weights of dormant neurons (represented in blue), and the optimal joint action changes after the application of ReDo, violating the KI principle which mandates that learned action preferences remain unchanged after parameter perturbation.  The figure uses a simplified example to clearly showcase this violation.", "section": "B Principle and Theorem"}, {"figure_path": "4NGrHrhJPx/figures/figures_16_1.jpg", "caption": "Figure 2: An example to show that ReSet does not satisfy KI principle.", "description": "This figure illustrates how the ReSet method, a parameter perturbation technique, fails to satisfy the Knowledge Invariant (KI) principle in the context of multi-agent reinforcement learning (MARL).  The left side shows the initial state of a simple mixing network with weights assigned to neurons, while the right side depicts the network after ReSet's application.  The transformation highlights changes in optimal actions due to the weight re-initialization, specifically that the re-initialization process in ReSet does not preserve previously learned knowledge.", "section": "B Principle and Theorem"}, {"figure_path": "4NGrHrhJPx/figures/figures_23_1.jpg", "caption": "Figure 1: The existence of Dormant Neuron Phenomenon in Value Function Factorization Methods.", "description": "This figure shows the percentage of dormant neurons in the agent and mixing networks of QMIX and QPLEX across three different SMAC scenarios: 3s_vs_5z, MMM2, and 27m_vs_30m.  The x-axis represents environmental steps, and the y-axis represents the percentage of dormant neurons.  The figure demonstrates that the dormant neuron phenomenon, where an increasing number of neurons become inactive during training, is more prominent in the mixing network than in the agent network, and that this phenomenon is more severe in scenarios with a larger number of agents.", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_24_1.jpg", "caption": "Figure 4: ReBorn can improve the performance of various value factorization algorithms in SMACv2.", "description": "This figure displays the results of applying the ReBorn algorithm to various value factorization algorithms within the SMACv2 environment. It showcases the impact of ReBorn on the test win rate and dormant neuron percentage across three different SMACv2 scenarios: 10gen_zerg, 10gen_terran, and 10gen_protoss.  Each subfigure presents the performance of QMIX and QPLEX, both with and without ReBorn, demonstrating how the algorithm affects the test win rate and reduces the number of dormant neurons.  The results illustrate that ReBorn improves performance in various scenarios by addressing the dormant neuron phenomenon in multi-agent reinforcement learning value factorization.", "section": "D.4.1 ReBorn can improve the performance of various value factorization algorithms"}, {"figure_path": "4NGrHrhJPx/figures/figures_24_2.jpg", "caption": "Figure 3: Over-active neurons in QMIX mixing networks: (a) The percentage contribution of the number of dormant neurons (depicted as Dormant), the number of over-active neurons (depicted as Overactive-Number), the sum of NAS (depicted as Overactive-Sum) for over-active neurons over time. (b) Overlap coefficients for Dormant/Over-active neurons between the current iteration and previous iterations. (c) Percentage of dormant neurons that re-enter dormancy after ReDo within different time steps.", "description": "This figure analyzes over-active neurons in the QMIX mixing network.  Subfigure (a) shows the contribution of dormant and over-active neurons to the total activation over time.  Subfigure (b) illustrates the overlap between dormant and over-active neurons across different training iterations, showing persistence. Subfigure (c) shows the percentage of dormant neurons that remain dormant after applying the ReDo method (a parameter perturbation technique), highlighting the persistence of dormant neurons.", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_24_3.jpg", "caption": "Figure 5: ReBorn can improve the performance of various value factorization algorithms: (a-b) the test win rate for the 3s5z_vs_3s6z and the MMM2 environments, (c) the return for predator-prey small environment, (d-f) the dormant percent for the the 3s5z_vs_3s6z, the MMM2, and the predator-prey small environment.", "description": "This figure displays the results of the ReBorn algorithm on three different environments: 3s5z_vs_3s6z, MMM2, and predator-prey small.  For each environment, it shows the test win rate (a-c) and the percentage of dormant neurons over time (d-f) for four different value factorization algorithms (QMIX, QPLEX, RMIX) with and without the ReBorn method. The results demonstrate that ReBorn improves the performance of various value factorization algorithms by reducing the percentage of dormant neurons.", "section": "6.2 ReBorn can improve the performance of various value factorization algorithms"}, {"figure_path": "4NGrHrhJPx/figures/figures_25_1.jpg", "caption": "Figure 1: The existence of Dormant Neuron Phenomenon in Value Function Factorization Methods.", "description": "The figure displays the percentage of dormant neurons in agent networks and mixing networks for different MARL algorithms (QMIX, QPLEX) across three SMAC scenarios (3s vs 5z, MMM2, 27m vs 30m).  It visually demonstrates the increase in dormant neurons over environmental steps, particularly within the mixing network, and how this increase correlates with the number of agents involved in the scenario.  The graphs highlight that the dormant neuron phenomenon is more pronounced in the mixing network compared to the agent networks across different scenarios and algorithms. ", "section": "4 The Dormant Neuron Phenomenon in MARL"}, {"figure_path": "4NGrHrhJPx/figures/figures_25_2.jpg", "caption": "Figure 5: ReBorn can improve the performance of various value factorization algorithms: (a-b) the test win rate for the 3s5z_vs_3s6z and the MMM2 environments, (c) the return for predator-prey small environment, (d-f) the dormant percent for the the 3s5z_vs_3s6z, the MMM2, and the predator-prey small environment.", "description": "This figure demonstrates that ReBorn enhances the performance of multiple value factorization algorithms (QMIX, QPLEX, RMIX) across different environments (3s5z_vs_3s6z, MMM2, predator-prey small).  Specifically, it shows the test win rate and return improvements in (a), (b), and (c) respectively, and illustrates the reduction in dormant neurons' percentage for each environment in (d), (e), and (f).", "section": "6.2 ReBorn can improve the performance of various value factorization algorithms"}]