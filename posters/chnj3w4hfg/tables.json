[{"figure_path": "ChnJ3W4HFG/tables/tables_3_1.jpg", "caption": "Table 1: CelebA WGA (std. dev.) using embeddings from a noisy base model. We see that KNN label correction still offers a significant increase in performance over vanilla two-stage methods.", "description": "This table shows the worst-group accuracy (WGA) and standard deviation for different methods on the CelebA dataset when using embeddings from a noisy base model.  It compares the performance of standard two-stage methods (RAD and SELF), their improved versions using KNN label correction (KNN-RAD and KNN-SELF), and a state-of-the-art method robust to label noise (END).  The results are presented for different levels of symmetric label noise (0%, 10%, 20%, 30%). The table demonstrates that the KNN label correction significantly improves the performance of two-stage methods, especially at higher noise levels.", "section": "4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_7_1.jpg", "caption": "Table 1: CelebA WGA (std. dev.) using embeddings from a noisy base model. We see that KNN label correction still offers a significant increase in performance over vanilla two-stage methods.", "description": "This table presents the worst-group accuracy (WGA) results for the CelebA dataset using embeddings from a noisy base model.  It compares the performance of standard two-stage methods (RAD and SELF) with and without the proposed kNN-based label correction preprocessing step. The table shows that kNN label correction significantly improves the WGA, especially at higher noise levels (10%, 20%, and 30%). The results demonstrate that kNN preprocessing enhances the robustness of the two-stage methods.", "section": "4.4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_7_2.jpg", "caption": "Table 2: Waterbirds WGA (std. dev.) using embeddings from a noisy base model. We see that KNN strongly outperforms vanilla two-stage methods at every noise level.", "description": "This table presents the worst-group accuracy (WGA) of different methods on the Waterbirds dataset when using embeddings from a noisy base model.  It compares the performance of standard two-stage methods (RAD and SELF) with their kNN-enhanced versions (kNN-RAD and kNN-SELF).  The results show that incorporating kNN significantly improves WGA across various levels of label noise (0%, 10%, 20%, 30%).", "section": "4.4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_8_1.jpg", "caption": "Table 3: CMNIST WGA (std. dev.): GUW* and GDS* denote the worst-group accuracies of upweighting and downsampling, respectively, achieved with oracle access to clean domain labels which aren't available to the two-stage methods. We list both the best domain annotation-free method for each noise level and the method within one standard deviation of the best in bold. We see that CMNIST is a relatively easy dataset in general, so label noise does not cause dramatic performance loss. Yet, kNN provides some additional robustness for both RAD and SELF. CMNIST is not considered in [16] and thus, results for END are not reported.", "description": "This table shows the worst-group accuracy (WGA) for different methods on the CMNIST dataset under various levels of symmetric label noise.  It compares methods with and without access to clean domain labels, highlighting the robustness of kNN-enhanced approaches. Note that the END method is not included because it was not tested on the CMNIST dataset in the original paper.", "section": "4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_8_2.jpg", "caption": "Table 4: CelebA WGA (std. dev.): We see that RAD and SELF achieve strong performance at 0% noise, but are not robust at larger noise levels. Here KNN-RAD and KNN-SELF maintain strong performance relative to their vanilla counterparts up to 30% noise and KNN-RAD strongly outperforms END at all noise levels.", "description": "This table compares the worst-group accuracy (WGA) of different methods for correcting model bias on the CelebA dataset under various levels of symmetric label noise.  The methods compared include standard two-stage techniques (RAD, SELF), a robust full retraining method (END), and the proposed kNN-enhanced versions of RAD and SELF (kNN-RAD, kNN-SELF).  The results demonstrate the superior robustness of the kNN-enhanced methods to label noise, particularly kNN-RAD,  outperforming other methods even with high noise levels (up to 30%).", "section": "4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_9_1.jpg", "caption": "Table 4: CelebA WGA (std. dev.): We see that RAD and SELF achieve strong performance at 0% noise, but are not robust at larger noise levels. Here KNN-RAD and KNN-SELF maintain strong performance relative to their vanilla counterparts up to 30% noise and KNN-RAD strongly outperforms END at all noise levels.", "description": "This table compares the worst-group accuracy (WGA) of different fairness-enhancing methods on the CelebA dataset under varying levels of symmetric label noise.  It shows the performance of standard methods (GUW*, GDS*, RAD, SELF, END) and the proposed methods (KNN-RAD, KNN-SELF). The results demonstrate that the proposed kNN label spreading pre-processing step significantly improves the robustness of both RAD and SELF to label noise, achieving state-of-the-art performance at higher noise levels.", "section": "4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_9_2.jpg", "caption": "Table 6: Civil Comments WGA (std. dev.): SELF struggles on this highly class-imbalanced dataset, but kNN-RAD is competitive with domain-aware methods even for large noise. Oh et al. [16] do not consider this dataset and thus, results for END are not reported.", "description": "This table shows the worst-group accuracy (WGA) for different methods on the Civil Comments dataset with varying levels of label noise.  It compares the performance of kNN-RAD and kNN-SELF against other state-of-the-art methods, including those that require domain annotations.  The table highlights that KNN-RAD is robust to noisy labels and achieves comparable results to methods that use domain information, unlike SELF which struggles with this imbalanced dataset.", "section": "4.4 Results"}, {"figure_path": "ChnJ3W4HFG/tables/tables_13_1.jpg", "caption": "Table 7: RAD Hyperparameters", "description": "This table shows the hyperparameters used for the RAD algorithm across four different datasets: CelebA, Waterbirds, CMNIST, and CivilComments.  For each dataset, it lists the penalty factor for the identification model (c (id)), the penalty factor for the retraining model (c (retraining)), the learning rate for the identification model (LR (id)), the number of epochs used for training the identification model (epochs (id)), and the range of upweight factors explored during hyperparameter tuning (upweight factor range). These hyperparameters were tuned to optimize the performance of the RAD algorithm on each dataset.", "section": "4.1 Experimental Details"}, {"figure_path": "ChnJ3W4HFG/tables/tables_13_2.jpg", "caption": "Table 8: KNN - RAD Hyperparameters", "description": "This table lists the hyperparameters used for the KNN-RAD algorithm, including the penalty factors for identification and retraining models, learning rate, number of epochs, range of nearest neighbors, and range of upweight factors for four different datasets: CelebA, Waterbirds, CMNIST, and Civilcomments.  The hyperparameter ranges were used in the hyperparameter selection phase of the experiments.", "section": "4.1 Experimental Details"}, {"figure_path": "ChnJ3W4HFG/tables/tables_13_3.jpg", "caption": "Table 9: SELF Hyperparameters", "description": "This table presents the hyperparameters used for the SELF algorithm across four different datasets: CelebA, Waterbirds, CMNIST, and CivilComments. For each dataset, it shows the number of fine-tuning steps, the range of learning rates explored, and the range of numbers of points considered for class balancing during the retraining process.", "section": "4.1 Experimental Details"}, {"figure_path": "ChnJ3W4HFG/tables/tables_13_4.jpg", "caption": "Table 10: KNN - SELF Hyperparameters", "description": "This table shows the hyperparameters used for the KNN-SELF algorithm on each dataset.  Specifically, it lists the number of fine-tuning steps, the learning rate range, the number of points used for reweighting, and the range of values tested for the number of nearest neighbors used in the KNN label-spreading preprocessing step.", "section": "A.5 KNN-SELF"}, {"figure_path": "ChnJ3W4HFG/tables/tables_14_1.jpg", "caption": "Table 11: a-RAD", "description": "This table shows the worst-group accuracy (WGA) of the a-RAD model on CelebA and CMNIST datasets under different levels of symmetric label noise (SLN). The a-RAD model uses the alpha-loss function, which is designed to be robust to label noise. The table shows that the a-RAD model's performance degrades significantly as the noise level increases, indicating that using a robust loss function alone is not enough to make two-stage methods robust to label noise.", "section": "C RAD, Why Robust Losses are Not Enough"}]