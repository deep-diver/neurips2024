{"importance": "This paper is crucial because it tackles the significant challenge of **selective forgetting in large black-box pre-trained models (PTMs)**, a critical limitation in deploying such models for real-world applications where processing all object classes is unnecessary.  The proposed approach provides a practical and effective solution for selective forgetting without needing model internals, opening new avenues for research into PTM optimization and responsible AI.", "summary": "Black-Box Forgetting achieves selective forgetting in large pre-trained models by optimizing input prompts, not model parameters, thus enabling targeted class removal without requiring internal model information.", "takeaways": ["Selective forgetting is addressed in black-box pre-trained models using derivative-free optimization.", "Latent Context Sharing (LCS) technique enhances efficiency in high-dimensional optimization.", "The proposed method effectively reduces accuracy for specified classes without significantly impacting overall model accuracy."], "tldr": "Large pre-trained models (PTMs) offer powerful zero-shot classification but often include unnecessary classes that degrade overall accuracy.  Existing solutions assume 'white-box' access, requiring model parameters and gradients; this is impractical for commercially sensitive or socially responsible PTMs.  The 'Black-Box Forgetting' problem thus emerges, as selective forgetting without model internal information is needed. \nThis paper introduces a novel approach to solve the black-box forgetting problem by tuning the input text prompt.  To handle the high dimensionality of prompt optimization, a latent context sharing (LCS) method is proposed, which introduces common low-dimensional latent components among multiple tokens for the prompt. Using derivative-free optimization, the method efficiently enhances forgetting performance on benchmark datasets. The proposed solution offers a significant step toward practical and ethical use of large-scale PTMs.", "affiliation": "Tokyo University of Science", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "lpFDhC91Oj/podcast.wav"}