[{"figure_path": "6SRPizFuaE/tables/tables_7_1.jpg", "caption": "Table 1: Test accuracy on Digit-5. Avg means average results among all clients. Details in Sec. 5.1.", "description": "This table presents the test accuracy results for different federated learning methods on the Digit-5 dataset.  The methods compared include FedAvg, FedProx, FedProto, FedPCL, FedFA, FPL, and the proposed FedPLVM method.  The accuracy is reported for each of the five domains within Digit-5 (MNIST, SVHN, USPS, Synth, MNIST-M) and as an average across all domains.  The \u0394 column shows the improvement of each method compared to the baseline FedAvg method.", "section": "5.1 Performance Comparison"}, {"figure_path": "6SRPizFuaE/tables/tables_7_2.jpg", "caption": "Table 2: Test accuracy on Office-10. Details in Sec. 5.1.", "description": "This table presents the test accuracy results of different federated learning methods on the Office-10 dataset.  The results are broken down by individual dataset (Amazon, Caltech, DSLR, Webcam) and provide the average accuracy across all datasets.  The \u0394 column shows the improvement in average accuracy compared to the FedAvg baseline.", "section": "5.1 Performance Comparison"}, {"figure_path": "6SRPizFuaE/tables/tables_7_3.jpg", "caption": "Table 3: Comparison on prototype generation methods. Variance means the average distance from the normalized feature vector of one sample to its corresponding class feature center (i.e. the averaged prototype). Results are then used for visualization in Fig. 3. Details in Sec. 5.2.1.", "description": "This table compares three different prototype generation methods: averaging local and global prototypes, averaging local prototypes and clustering global prototypes, and clustering local and global prototypes.  The results show that clustering at both the local and global levels yields the best performance, as indicated by the highest average accuracy and lowest variance.  This highlights the importance of capturing variance information in prototype generation for improved performance in federated learning with heterogeneous data domains.", "section": "5.2.1 Impact of Dual-Level Prototype Generation"}, {"figure_path": "6SRPizFuaE/tables/tables_8_1.jpg", "caption": "Table 4: Comparison between w/o and w/ global clustering. w/o means the server distributes all local clustered prototypes to the clients for local training. Avg # of prototypes is the average number of prototypes each client receives from the server during each global round. Details in Sec. 5.2.1.", "description": "This table compares the performance of FedPLVM with and without global clustering.  The 'w/o' row shows results where the server sends all local clustered prototypes to each client.  The 'w/' row shows results where global clustering is used to reduce the number of prototypes sent. The table shows that using global clustering improves performance while significantly reducing communication costs and improving privacy.", "section": "5.2.1 Impact of Dual-Level Prototype Generation"}, {"figure_path": "6SRPizFuaE/tables/tables_8_2.jpg", "caption": "Table 5: Comparison on components of \u03b1-sparsity prototype loss. Contrast and Correction stand for the contrastive and corrective loss term in the total \u03b1-sparsity loss respectively. Avg is the average accuracy result for all clients. Details in Sec. 5.2.2.", "description": "This table presents the ablation study results on the impact of different components of the \u03b1-sparsity prototype loss. It compares the performance when using only the contrastive term, only the correction term, neither term, and both terms.  The results show the average accuracy across all clients for each configuration and the improvement achieved by adding each component.", "section": "5.2.2 Impact of \u03b1-Sparsity Prototype Loss"}, {"figure_path": "6SRPizFuaE/tables/tables_14_1.jpg", "caption": "Table 6: Test accuracy on DomainNet. Details in Sec. 5.1.", "description": "This table presents the test accuracy results on the DomainNet dataset for different federated learning methods.  It shows the average accuracy across all clients for each of the six domains within DomainNet (Clipart, Infograph, Painting, Quickdraw, Real, Sketch) and the overall average accuracy. The delta (\u0394) column indicates the improvement in average accuracy compared to the FedAvg baseline.", "section": "5.1 Performance Comparison"}, {"figure_path": "6SRPizFuaE/tables/tables_15_1.jpg", "caption": "Table 7: Test accuracy on Digit-5 under label non-i.i.d. setting. Avg means average results among all clients. We apply the Dirichlet method (\u03b1 = 0.5) to obtain the data distribution and create the non-i.i.d. dataset for each client. Details in Sec. D.", "description": "This table presents the test accuracy results for different federated learning methods on the Digit-5 dataset under a non-i.i.d. label setting, which is a scenario where data is not independently and identically distributed across clients.  The Dirichlet distribution with parameter \u03b1=0.5 is used to generate the non-i.i.d. data.  The table shows the average test accuracy across all clients for each method and also shows the difference in average accuracy compared to the baseline FedAvg method (\u0394 column).  The results are broken down by individual dataset (MNIST, SVHN, USPS, Synth, MNIST-M) within Digit-5.", "section": "D Label Non-I.I.D. Setting"}, {"figure_path": "6SRPizFuaE/tables/tables_15_2.jpg", "caption": "Table 2: Test accuracy on Office-10. Details in Sec. 5.1.", "description": "This table presents the test accuracy results for the Office-10 dataset.  The results are shown for different methods, including FedAvg, FedProx, FedProto, FedPCL, FedFA, FPL, and the proposed method (Ours). The accuracy is reported for each of the four domains in Office-10 (Amazon, Caltech, DSLR, and Webcam), as well as the average accuracy across all domains.  The \u0394 column shows the improvement of each method compared to FedAvg.  Section 5.1 provides further details on the experimental setup.", "section": "5.1 Performance Comparison"}, {"figure_path": "6SRPizFuaE/tables/tables_15_3.jpg", "caption": "Table 9: Comparison with K-Means Algorithm. Adaptive K means we use the number of clustering centers from FINCH as K. Details in Sec. E.", "description": "This table compares the performance of the proposed FedPLVM method using FINCH clustering with the performance using K-Means clustering.  The adaptive K-means approach uses the number of clusters determined by FINCH as the K value for K-Means.  The table shows that while carefully tuned K-means can achieve similar results, poorly tuned K-means performs worse than FINCH, highlighting the advantage of the parameter-free FINCH algorithm used in the paper.", "section": "E Different Clustering Algorithms"}, {"figure_path": "6SRPizFuaE/tables/tables_16_1.jpg", "caption": "Table 10: Comparison on unbalanced clients distribution. Test accuracy on each dataset domain is the average result among all clients that own the corresponding dataset. Avg means average results among all clients. Details in Sec. F.", "description": "This table presents the results of the experiment conducted with unbalanced client data distribution.  The goal is to showcase FedPLVM's performance in a more realistic scenario where some domains have many more clients than others.  The table shows the average accuracy across all clients for each domain (MNIST, SVHN, USPS, Synth, MNIST-M).  The \"Avg\" column provides the overall average accuracy across all domains and clients.  This is compared to FPL to demonstrate FedPLVM's improved fairness and robustness in dealing with uneven client representation across different data domains.", "section": "5. Experiments"}, {"figure_path": "6SRPizFuaE/tables/tables_16_2.jpg", "caption": "Table 11: Impact of differential privacy. Avg means average results among all clients. w/ and w/o represents we incorporate the local model or the local clustered prototypes with the privacy protection technologies or not. Details in Sec. G.", "description": "This table presents the results of an ablation study evaluating the impact of differential privacy on the model's performance. Four scenarios are compared: 1) both the model and prototypes use differential privacy, 2) only the model uses differential privacy, 3) only the prototypes use differential privacy, and 4) neither use differential privacy.  The results show the average test accuracy across five datasets (MNIST, SVHN, USPS, Synth, MNIST-M) for each scenario, illustrating the trade-off between privacy and accuracy.", "section": "G Privacy Protection"}, {"figure_path": "6SRPizFuaE/tables/tables_17_1.jpg", "caption": "Table 1: Test accuracy on Digit-5. Avg means average results among all clients. Details in Sec. 5.1.", "description": "This table presents the test accuracy results for five different digit recognition datasets (MNIST, SVHN, USPS, Synth, MNIST-M) using various federated learning methods.  The \"Avg\" column represents the average test accuracy across all datasets.  The \"\u0394\" column shows the improvement in average accuracy compared to the FedAvg baseline.  Section 5.1 provides more detailed explanations of the experimental setup and results.", "section": "5.1 Performance Comparison"}]