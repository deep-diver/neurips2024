[{"figure_path": "6SRPizFuaE/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of federated learning with heterogeneous data domains. The Vanilla column depicts the local feature distribution of the standard FedPL approach, obtaining average local and global prototypes directly. Proposed method showcased in the adjacent column yields a larger inter-class distance and a reduced intra-class distance. Note that without capturing variance information, even for hard domains, local averaged prototypes for each class can be well distinguished while the feature vectors are still mixed up. Both methods illustrate noticeable variations in domain characteristics across datasets, as detailed in Fig. 4.", "description": "This figure compares the Vanilla and Proposed approaches of Federated Prototype Learning (FedPL) in heterogeneous data domains.  Vanilla FedPL directly averages local feature distributions to create prototypes, while the proposed approach uses a dual-level clustering method (local and global) to improve prototype quality by capturing data variance. The visualization shows how the proposed method better separates different classes (larger inter-class distance, smaller intra-class distance), especially in the \u2018harder\u2019 domains where data is less cleanly separated.", "section": "1 Introduction"}, {"figure_path": "6SRPizFuaE/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of our proposed FedPLVM framework. Once the sample embedding is generated by the feature extractor, the client conducts the first-level local clustering, following Eq. 3. Subsequently, the server gathers all local clustered prototypes and local models (comprising feature extractors and classifiers), initiates the second-level global clustering based on Eq. 4, and averages the local models to form a global model. Finally, clients utilize the received global clustered prototypes to update the local model, employing loss functions La from Eq. 5 and LCE from Eq. 9.", "description": "This figure illustrates the FedPLVM framework's workflow.  It begins with clients generating sample embeddings using feature extractors.  These embeddings undergo local clustering to create local clustered prototypes. The server then collects these prototypes, performs global clustering to generate global clustered prototypes, and averages client models to create a global model. Finally, the global model and clustered prototypes are sent back to the clients for local model training using the \u03b1-sparsity prototype loss (La) and cross-entropy loss (LCE).", "section": "3 Preliminary"}, {"figure_path": "6SRPizFuaE/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of different prototype generation methods. The first row averages feature vectors locally and averages local prototypes globally. The second row averages feature vectors locally and clusters local prototypes globally. The last row (ours) clusters feature vectors locally and clusters local clustered prototypes globally. The last column Total is the visualization of mixing the feature vectors from all datasets. Details in Sec. 5.2.1.", "description": "This figure compares three different prototype generation methods using t-SNE visualization.  The first method averages features locally and then globally. The second method averages features locally but clusters prototypes globally. The third method (the proposed method) clusters features locally and globally. The \"Total\" column shows a visualization of all the data combined, highlighting the differences in feature distribution across methods.", "section": "5.2.1 Impact of Dual-Level Prototype Generation"}, {"figure_path": "6SRPizFuaE/figures/figures_9_1.jpg", "caption": "Figure 4: Impact of a sparsity and \u03bb prototype loss weight. The left figure shows the accuracy of two selected datasets and the average accuracy among all clients with different \u03b1. The right figure shows the effects of different \u03bb for both FPL and our proposed approach. Details in Sec. 5.2.2.", "description": "This figure shows the impact of hyperparameters \u03b1 and \u03bb on the performance of the proposed FedPLVM model. The left subplot illustrates how different values of \u03b1 (a sparsity parameter in the \u03b1-sparsity prototype loss) affect the accuracy on two selected datasets (Synth and MNIST-M) and the average accuracy across all clients.  The right subplot compares the performance of the proposed method and FPL model (a baseline method) across various values of \u03bb (the weight balancing between the \u03b1-sparsity loss and the cross-entropy loss).", "section": "5.2 Impact of a-Sparsity Prototype Loss"}, {"figure_path": "6SRPizFuaE/figures/figures_9_2.jpg", "caption": "Figure 3: Visualization of different prototype generation methods. The first row averages feature vectors locally and averages local prototypes globally. The second row averages feature vectors locally and clusters local prototypes globally. The last row (ours) clusters feature vectors locally and clusters local clustered prototypes globally. The last column Total is the visualization of mixing the feature vectors from all datasets. Details in Sec. 5.2.1.", "description": "This figure visualizes the different prototype generation methods used in the paper.  It compares three methods: averaging feature vectors locally and globally; averaging locally and clustering globally; and clustering locally and globally. The visualization shows how different prototype generation approaches affect the distribution of feature vectors, particularly highlighting the improvement of the proposed dual-level clustering approach in separating features from different classes and domains.", "section": "5.2.1 Impact of Dual-Level Prototype Generation"}, {"figure_path": "6SRPizFuaE/figures/figures_14_1.jpg", "caption": "Figure 4: Impact of a sparsity and \u03bb prototype loss weight. The left figure shows the accuracy of two selected datasets and the average accuracy among all clients with different \u03b1. The right figure shows the effects of different \u03bb for both FPL and our proposed approach. Details in Sec. 5.2.2.", "description": "This figure demonstrates the impact of two hyperparameters, \u03b1 (alpha) and \u03bb (lambda), on the performance of the proposed FedPLVM model.  The left subplot shows how the accuracy varies across different \u03b1 values for three different datasets (Clipart, Quickdraw, and the average across all datasets). The right subplot compares the performance of FedPLVM and another model (FPL) at different values of \u03bb, demonstrating the sensitivity of model accuracy to this parameter.", "section": "5.2 Impact of a-Sparsity Prototype Loss"}, {"figure_path": "6SRPizFuaE/figures/figures_14_2.jpg", "caption": "Figure 3: Visualization of different prototype generation methods. The first row averages feature vectors locally and averages local prototypes globally. The second row averages feature vectors locally and clusters local prototypes globally. The last row (ours) clusters feature vectors locally and clusters local clustered prototypes globally. The last column Total is the visualization of mixing the feature vectors from all datasets. Details in Sec. 5.2.1.", "description": "This figure compares three different prototype generation methods in federated prototype learning with heterogeneous data domains.  The methods differ in how they handle local and global prototypes. The first method averages feature vectors locally and then averages the resulting local prototypes to get global prototypes. The second method averages feature vectors locally, then clusters the local prototypes to obtain global prototypes. The third method, proposed by the authors, first clusters feature vectors locally and then clusters the local clustered prototypes to get the global prototypes. The visualization shows that the authors' method leads to better separation of features in each domain, likely improving model performance.", "section": "5.2.1 Impact of Dual-Level Prototype Generation"}, {"figure_path": "6SRPizFuaE/figures/figures_17_1.jpg", "caption": "Figure 9: Tendency of average number of local clustered prototypes for different classes in different domains. Details in Sec. I.", "description": "This figure shows how the average number of local clustered prototypes changes over global rounds for different classes (MNIST, SVHN, and the average across all classes) in different domains.  It demonstrates that the number of prototypes tends to decrease as the global round progresses, but the rate of decrease varies across classes and datasets.  The easy dataset MNIST has consistently fewer local prototypes than the hard dataset SVHN, suggesting that more prototypes are needed to capture the complexity of the hard dataset. This is relevant to section I which discusses the impact of Local Prototype Clustering, showing that the number of local prototypes is dynamically adjusted based on the data distribution and domain complexity.", "section": "I Local Prototypes Clustering"}]