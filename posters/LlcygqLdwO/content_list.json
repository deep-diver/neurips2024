[{"type": "text", "text": "Visual-TCAV: Explainability of Image Classification through Concept-based Saliency Maps ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Convolutional Neural Networks (CNNs) have seen significant performance im  \n2 provements in recent years. However, due to their size and complexity, their   \n3 decision-making process remains a black-box, leading to opacity and trust issues.   \n4 State-of-the-art saliency methods can generate local explanations that highlight the   \n5 area in the input image where a class is identified but do not explain how different   \n6 features contribute to the prediction. On the other hand, concept-based methods,   \n7 such as TCAV (Testing with Concept Activation Vectors), provide global explain  \n8 ability, but cannot compute the attribution of a concept in a specific prediction nor   \n9 show the locations where the network detects these concepts. This paper introduces   \n0 a novel explainability framework, Visual-TCAV, which aims to bridge the gap   \n11 between these methods. Visual-TCAV uses Concept Activation Vectors (CAVs) to   \n2 generate saliency maps that show where concepts are recognized by the network.   \n3 Moreover, it can estimate the attribution of these concepts to the output of any   \n14 class using a generalization of Integrated Gradients. Visual-TCAV can provide   \n5 both local and global explanations for any CNN-based image classification model   \n16 without requiring any modifications. This framework is evaluated on widely used   \n17 CNNs and its validity is further confirmed through experiments where a ground   \n18 truth for explanations is known. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Recent advancements in Deep Neural Networks (DNNs) have revolutionized the field of Artificial   \n21 Intelligence, and Convolutional Neural Networks (CNNs) have emerged as the state-of-the-art for   \n22 image classification due to their ability to learn complex patterns and features within images. However,   \n23 as the performance of these models has grown significantly over recent years, their complexity has   \n24 also increased. Consequently, it became a challenge to understand how these models produce their   \n25 classifications. This led to the widespread use of the term black-box to describe these models, as only   \n26 their inputs and outputs are known, while their internal mechanisms remain too complex for humans   \n27 to comprehend. The black-box problem results in a lack of transparency [29], which can undermine   \n28 trust in AI-based systems [12]. Indeed, blindly trusting AI poses serious ethical dilemmas, especially   \n29 in critical fields such as healthcare or autonomous driving in which image classification systems are   \n30 becoming increasingly employed [28, 3]. Additionally, debugging black-box models and identifying   \n31 biases becomes difficult without comprehending the process they use to make predictions. To this   \n32 end, the field of Explainable Artificial Intelligence (XAI) has made significant progress in developing   \n33 techniques for producing explanations of AI decisions. However, comprehending the specific features   \n34 or patterns that networks identify in an image and their precise impact on the prediction remains a   \n35 challenge. State-of-the-art approaches for local explainability (i.e., for individual predictions) use   \n36 saliency maps to locate where a class is identified in an input image, but they can\u2019t explain which   \n37 features led the model to its prediction. For instance, when analyzing an image of a golf ball, these   \n38 saliency methods cannot determine whether the golf ball was recognized by the spherical shape, the   \n39 dimples, or some other feature. Striving to cover this need, Kim et al. [11] introduced TCAV (Testing   \n40 with Concept Activation Vectors), a concept-based method that can discern whether a user-defined   \n41 concept (e.g., dimples, spherical) correlates positively with the output of a selected class. However,   \n42 TCAV is designed exclusively for global explainability (i.e., for explaining the general behavior of a   \n43 model) and therefore cannot measure the influence of a concept in a specific prediction or show the   \n44 locations within the input images where the networks recognize these concepts.   \n45 In this article, we introduce a novel explainability framework, Visual-TCAV, which integrates the core   \n46 principles of both saliency methods and concept-based approaches while aiming to overcome their   \n47 respective limitations. Visual-TCAV can be applied to any layer of a CNN model whose output is a   \n48 set of feature maps. Its main contributions are: (a) it provides visual explanations that show where   \n49 the network identifies user-defined concepts; (b) it can estimate the importance of these concepts to   \n50 the output of a selected class; (c) it can be used for both local and global explainability. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "51 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "52 In recent years, there has been a significant increase in the body of work exploring the explainability   \n53 of black-box models. For CNN-based image classification, state-of-the-art methods primarily focus   \n54 on providing explanations via saliency maps. These heatmaps highlight the most important regions   \n55 of the input image and therefore can be used to gain insights into how a model makes its decisions.   \n56 One approach for generating such visualizations involves studying the input-output relationship   \n57 of the model by creating a set of perturbed versions of the input and analyzing how the output   \n58 changes with each perturbation. Notable contributions to this approach include Local Interpretable   \n59 Model-Agnostic Explanations (LIME) [17], which uses random perturbations, and SHapley Additive   \n60 exPlanations (SHAP) [14], which estimates the importance of each pixel using Shapley values. A   \n61 different approach that instead tries to access the internal workings of the model was originally   \n62 proposed by Simonyan et al. [22] and consists of generating saliency maps based on the gradients   \n63 of the model output w.r.t. the input images. This idea led many researchers [24, 23] to investigate   \n64 how to exploit gradients to produce more accurate saliency maps. Selvaraju et al. [20] proposed a   \n65 method named Gradient-weighted Class Activation Mapping (Grad-CAM) that extracts the gradients   \n66 of the logits (i.e., raw pre-softmax predictions) w.r.t. the feature maps. It then uses a Global Average   \n67 Pooling (GAP) operation to transform these gradients into class-specific weights for each feature   \n68 map and performs a weighted sum of these feature maps to produce a class localization map, a   \n69 saliency map that highlights where a class is identified. Grad-CAM has gained considerable attention   \n70 and is extensively used for explaining convolutional networks. However, Sundararajan et al. [25]   \n71 demonstrated that gradients can saturate, leading to an inaccurate assessment of feature importance.   \n72 To address this issue, they introduced Integrated Gradients (IG), a method that calculates feature   \n73 attribution by integrating the gradients along a path from a baseline (e.g., a black image) to the   \n74 actual input image. Notable contributions of IG and its variants [10, 16, 30] include the ability to   \n75 provide fine-grained saliency maps (i.e., each pixel has its attribution) and adherence to the axiom of   \n76 completeness (i.e., the sum of the attributions of all pixels equals the logit value).   \n77 While saliency methods are effective and intuitive, they might not always provide a complete picture   \n78 of why a model made a certain decision. This is because these methods perform class localization,   \n79 but cannot explain which features led the model to recognize the highlighted class. Furthermore,   \n80 these techniques rely on per-pixel importance which can\u2019t be generalized across multiple instances, as   \n81 the position of these pixels is only meaningful for a specific input image. Consequently, they can only   \n82 explain one image at a time, preventing them from providing global explanations. To overcome these   \n83 limitations, Kim et al. [11] proposed Testing with Concept Activation Vectors (TCAV), a method that   \n84 investigates the correlations between user-defined concepts and the network\u2019s predictions using a set   \n85 of example images representing a concept. For instance, images of stripes can be used to determine   \n86 whether the network is sensitive to the \u201cstriped\u201d concept for predicting the \u201czebra\u201d class. This is   \n87 accomplished by calculating a Concept Activation Vector (CAV), which is a vector orthogonal to   \n88 the decision boundary of a linear classifier, typically Support Vector Machines (SVMs), trained to   \n89 differentiate between the feature maps of concept examples and random images. From this, a TCAV   \n90 score for any concept and model\u2019s layer can be computed using the signs of the dot products between   \n91 the CAV and the gradients of the loss w.r.t. the feature maps produced by images of a selected class.   \n92 TCAV is effective in detecting specific biases in neural networks (e.g., ethnicity-related) and can be   \n93 considered complementary to saliency methods. Indeed, while saliency methods apply exclusively   \n94 to individual predictions, TCAV can only provide global explanations. However, TCAV does not   \n95 provide any information about the locations where concepts are identified within the input images.   \n96 This makes it challenging to assess whether a high score can truly be attributed to the intended   \n97 concept and not to a related one. Moreover, TCAV computes the network\u2019s sensitivity to a concept,   \n98 but not the magnitude of its importance in the prediction as the score only depends on the signs of the   \n99 directional derivatives. For instance, \u201cwhite\u201d and \u201cdimples\u201d concepts might have identical TCAV   \n100 scores for the \u201cgolf ball\u201d class, even if one contributes substantially more to the prediction.   \n101 TCAV has received attention within the XAI community, leading to various extensions [5, 8] and   \n102 applications [13, 2]. While our study focuses on user-defined concepts, unsupervised approaches   \n103 have also been proposed. Ghorbani et al. [7] introduced Automatic Concept Extraction (ACE), a   \n104 method that automatically extracts concepts from images for applying TCAV. This is accomplished   \n105 by segmenting input images and subsequently clustering their activations. Building upon ACE, Zhang   \n106 et al. [31] proposed Invertible Concept-based Explanations (ICE). This extension uses non-negative   \n107 CAVs derived from non-negative matrix factorization and can also be used to explain locally by   \n108 associating extracted concepts with a relevant area in the input image. Later, Bianchi et al. [1]   \n109 proposed an unsupervised method for visualizing the entire feature extraction process of CNNs. They   \n110 perform layer-wise clustering of similar feature maps to extract a set of concepts for each layer to   \n111 which they assign a descriptive label through crowdsourcing. This approach provides local and global   \n112 explanations, but the reliance on crowdsourcing can pose a practical challenge. Furthermore, these   \n113 unsupervised approaches may provide opaque explanations. This is because, when the extracted   \n114 image regions contain overlapping concepts (e.g., dimples, spherical, and white in a golf ball), it   \n115 remains unclear which concepts the network has learned to recognize or considers more important. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "LlcygqLdwO/tmp/68d505e539de84abd463fd96a7951f99b50d9608e052113d2907e3f20828c967.jpg", "img_caption": ["Figure 1: The Visual-TCAV process for generating local explanations. A Pooled-CAV is computed using the feature maps of user-defined concept examples and random images. A concept map is then produced through a weighted sum of the Pooled-CAV and the image\u2019s feature maps. Finally, a concept attribution is obtained by extracting the IG attributions of the neurons that the concept activates using the Pooled-CAV and the concept map, which is used as a spatial mask. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "116 3 Visual-TCAV ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 This section presents the methodology of our framework, Visual-TCAV, which is designed to explain   \n118 the outputs of image classification CNNs using user-defined concepts. Local explanations can be   \n119 generated considering any layer and consist of two key components. The first is the Concept Map, a   \n120 saliency map that serves as a visual representation of the areas where the network has recognized   \n121 the selected concept in the input image. The second is the Concept Attribution, a numerical value   \n122 that estimates the importance of the concept for the output of a selected class. Figure 1 illustrates the   \n123 pipeline for generating a local explanation. For global explanations, the process is replicated across   \n124 multiple input images. The concept attributions for each image are then averaged to quantify how the   \n125 concept influences the network\u2019s decisions across a wide range of inputs. ", "page_idx": 2}, {"type": "image", "img_path": "LlcygqLdwO/tmp/8ed00687130605f0c236392f564484a19db145660a5cd40d1a746623afbad34c.jpg", "img_caption": ["Figure 2: Examples of class-independent concept maps for various input images and concepts. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "126 3.1 CAV Generation and Spatial Pooling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Similarly to the TCAV framework, the initial step of our method consists of computing a Concept   \n128 Activation Vector (CAV) from a set of example images representing a user-defined concept, and a set   \n129 of negative examples (e.g., random images). Specifically, we use the Difference of Means method,   \n130 proposed by Martin and Weller [15], to compute the CAV. They demonstrated that this approach   \n131 produces CAVs that are more resilient to perturbation and consistent than logistic classifiers or SVMs.   \n132 As the name suggests, this method uses the arithmetic mean to determine the centroids of both the   \n133 concept\u2019s activations and the activations of random images. Subsequently, it directly computes the   \n134 CAV as the difference between these centroids.   \n135 Since we are interested in identifying which feature maps are activated by the concept, irrespective of   \n136 its location within the example images, we apply a Global Average Pooling (GAP) operation on the   \n137 obtained CAV. The result is a vector of scalar values whose length is equal to the number of feature   \n138 maps of the layer under consideration. Each vector element is associated with a feature map, and its   \n139 raw value approximates the degree of correlation between that feature map and the concept. Moving   \n140 forward, we will refer to this vector as the Pooled-CAV. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "141 3.2 Concept Map ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 From the Pooled-CAV, we can construct a concept map that locates a concept $(c)$ within any input   \n143 image to be explained. This is achieved by performing a weighted sum of the feature maps $(f m a p s_{k})$   \n144 of the input image, with the weights being the Pooled-CAV values $(p_{k}^{c})$ . Equation (1) shows how   \n145 to compute a raw concept map $(M_{r a w}^{c})$ . We also apply a ReLU function after the weighted sum   \n146 because we are only interested in the image regions that positively correlate with the concept. The   \n147 computation is similar to Grad-CAM\u2019s equation, with the difference that we use the elements of the   \n148 Pooled-CAV as weights instead of the global-average-pooled gradients. ", "page_idx": 3}, {"type": "equation", "text": "$$\nM^{c,r a w}=R e L U\\Big(\\sum_{k}p_{k}^{c}\\cdot f m a p s_{k}\\Big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "149 We refer to this concept map as raw due to the absence of a scale factor (i.e., a maximum value) that   \n150 would allow us to compare the degree of activation of the concept map across different concepts,   \n151 input images, and model layers. To this end, we derive a concept map\u2019s scale factor from the   \n152 example images the user provided, which represent an ideal concept. Formally, we use Equation (2)   \n153 to calculate the scale factor ${(s_{c})}$ as the maximum value of a hypothetical concept map, computed   \n154 using the centroid $(C^{c})$ , derived from the mean of the feature maps of the example images for a   \n155 concept $(c)$ . Subsequently, we normalize the raw concept map by dividing it by the scale factor ${\\bf\\Pi}(s_{c})$   \n156 and limiting the values to a unitary maximum, as shown in Equation (3). An epsilon $(\\varepsilon)$ is added to   \n157 the denominator to prevent division by zero. ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{c}=\\operatorname*{max}\\left(R e L U\\left(\\sum_{k}p_{k}^{c}\\cdot C_{k}^{c}\\right)\\right)\\quad\\mathrm{~(2)~}\\quad\\qquad\\quad M_{i j}^{c}=\\operatorname*{min}\\left(1,\\ {\\frac{M_{i j}^{c,r a w}}{s_{c}+\\varepsilon}}\\right)\\quad\\forall i,j\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "159 By overlaying the normalized concept map $(M^{c})$ on the input image, we can generate a class  \n160 independent visualization (examples are shown in Figure 2) that highlights the region of the image   \n161 where the network recognized the concept. This allows us to know, for any input image, the   \n162 concept\u2019s location and its degree of activation w.r.t an ideal concept defined by the user. Additionally,   \n163 the concept map can provide a direct validation for the learned CAV, without requiring activation   \n164 maximization techniques or sorting images based on their similarity to the CAV. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "165 3.3 Concept Attribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 Once we acquire a set of concepts, we can gain insights into the network\u2019s decision-making process   \n167 by measuring the attribution of these user-defined concepts towards the raw predictions, also known   \n168 as the logits. For instance, if the \u201cchurch\u201d class is predicted with a certain logit, we aim to quantify   \n169 how much of this value is attributable to the \u201cpews\u201d concept, the \u201cfresco\u201d concept, and so on. More   \n170 specifically, given an input image and a layer, we compute the attributions of the activations (i.e.,   \n171 the values of the feature maps) to the logit of a specific target class. Subsequently, we utilize the   \n172 Pooled-CAV to approximate which activations are attributable to a certain concept, and then we   \n173 extract and sum these attributions. The attributions of a layer\u2019s activations can be computed through a   \n174 generalized variant of the IG approach which computes the integrated gradients of a target class\u2019s   \n175 logit w.r.t. the feature maps, instead of the input image. Specifically, we calculate the gradients along   \n176 a straight-line path from zero-filled matrices to the actual feature maps and then approximate the   \n177 integral using the Riemann trapezoidal rule. In our experiments, we consistently used 300 steps,   \n178 which are sufficient to approximate the integral within a $5\\%$ error margin, as shown by Sundararajan   \n179 et al. [25]. We then calculate the raw attributions by multiplying the integrated gradients with the   \n180 feature maps, as shown in Figure 1. Since IG respects the completeness axiom regardless of which   \n181 layer is considered as input, the attributions add up to the logit value of the target class, within the   \n182 approximation error. A ReLU is then applied to extract positive attributions. These attributions   \n183 are on the same scale as the raw logits, which can make their interpretation difficult. To obtain a   \n184 comprehensible unitary scale, we normalize the attributions so that their sum equals a normalized   \n185 logit, not the raw one. These normalized logits are obtained by applying a ReLU, followed by   \n186 [0,1] rescaling to retain their relative ratios.   \n187 To estimate the attribution of a concept $(c)$ , we can utilize the Pooled-CAV to perform a weighted sum   \n188 of the normalized attributions $(A^{t,n o r m})$ . Before this summation, we apply a ReLU and [0,1] rescaling   \n189 to the Pooled-CAV $(p^{c})$ so that we extract gradually less attribution for feature maps that are less   \n190 correlated with the concept. The rationale behind using the ReLU is to discard the attribution of   \n191 feature maps that show a negative correlation with the concept. In other words, if a certain feature   \n192 map is activated by other non-correlated features, we discard its attribution. Finally, as shown in   \n193 Equation (4), we obtain the ConceptAttribution for a concept (c) and a target class $(t)$ by summing   \n194 all values of an element-wise multiplication of the weighted attributions and the concept map $(M^{c})$ ,   \n195 which is used as a spatial mask. This enables us to discard the attributions of activations related to the   \n196 regions within the input image where the concept is not present or was not recognized. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nC o n c e p t A t t r i b u t i o n_{c,t}=\\sum_{i,j}M_{i j}^{c}\\cdot\\Big(\\sum_{k}R e L U(p_{k}^{c,n o r m})\\cdot A_{k}^{t,n o r m}\\Big)_{i j}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "197 The concept attribution is a per-concept metric of importance, meaning that two concepts can have   \n198 significantly different attributions even if they are recognized in the same location of the input   \n199 image, resulting in similar concept maps. For instance, considering the \u201czebra\u201d class, the attribution   \n200 of the \u201cstriped\u201d concept could be significantly different from the attribution of the \u201cfur\u201d concept.   \n201 This distinction is achieved by focusing not on per-pixel attributions but on the attributions of the   \n202 activations produced by the neurons responsible for recognizing these two concepts. Moreover, since   \n203 the attribution of a concept is independent of its location, we can average it across multiple input   \n204 images to provide a quantitative measure of the overall importance of that concept for that particular   \n205 class, thus providing a global explanation. For instance, we can calculate a global attribution of the   \n206 \u201cstriped\u201d concept for the \u201czebra\u201d target class by averaging the attribution of \u201cstriped\u201d across a large   \n207 number (e.g., 200) of images containing zebras. ", "page_idx": 4}, {"type": "text", "text": "208 4 Experiments and Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "209 In this section, we present the results of applying Visual-TCAV to the following convolutional   \n210 networks pre-trained on the ImageNet [6] dataset: GoogLeNet [26], InceptionV3 [27], VGG16 [21],   \n211 and ResNet50V2 [9]. Examples of \u201cstriped\u201d, \u201czigzagged\u201d, \u201cwaffled\u201d, and \u201cchequered\u201d concepts are   \n212 sourced from the Describable Textures Dataset (DTD) [4], while \u201cpews\u201d and \u201cfresco\u201d are generated   \n213 through Stable Diffusion v1.5 [18] (more on this in Appendix E). Other concepts are obtained from   \n214 popular image search engines. Similarly to TCAV, we use a minimum of 30 example images per   \n215 concept and 500 random images as negative examples, as suggested by Martin and Weller [15].   \n216 Our experiments are conducted on an Intel i7 13700k with an Nvidia RTX 4060Ti 16GB, and 32 GB   \n217 of DDR5 RAM. The software runs on TensorFlow 2.15.1, CUDA 12.2, and Python 3.11.5. Local   \n218 explanations, with 300 steps and seven layers, take less than a minute, while global explanations with   \n219 200 class images, 300 steps, and seven layers, can take anywhere from 5 to 20 minutes, depending on   \n220 the model. For global explanations, the computation time remains nearly constant regardless of the   \n221 number of concepts processed simultaneously. The official implementation is available in our GitHub   \n222 repository: removed for anonymity, see supplemental material .zip file. ", "page_idx": 4}, {"type": "image", "img_path": "LlcygqLdwO/tmp/eb1ba65f9c6679beca05f8092eeba95069f2b5a2ab4a85f1b50b89ab7da9a3b7.jpg", "img_caption": ["Figure 3: Examples of layer-wise local explanations for various concepts and networks. We compute the attribution of each concept for the top three predicted classes and the last seven layers. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "223 4.1 Local Explanations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 In Figure 3, we provide local explanations for various concepts. While concept maps are class  \n225 independent, the attribution of each concept depends on the class considered. We examine the top   \n226 three predicted classes in our examples and apply Visual-TCAV to a subset of the CNNs\u2019 layers. On   \n227 one hand, we can observe a substantial increase in attributions in deeper layers, reaching a peak in   \n228 the final layer, which holds the most information about the importance of each concept for a specific   \n229 class, given its proximity to the output. On the other hand, the most accurate concept maps are   \n230 typically found in slightly earlier layers due to their neurons having smaller receptive fields.   \n231 Furthermore, these layer-wise explanations enable us to identify when specific concepts are recognized   \n232 within the network. For instance, the \u201cwaffled\u201d concept does not significantly activate the initial   \n233 layers of InceptionV3, but it is recognized by deeper layers with a considerable attribution in the final   \n234 one. We also observe that the \u201chands\u201d concept is detected mainly by earlier layers and contributes   \n235 only marginally to the score of the top classes for the analyzed image. This observation aligns   \n236 with the common intuition that \u201chands\u201d are not class-discriminative in this particular case for the   \n237 classes \u201cbeer glass\u201d, \u201ccocktail shaker\u201d, and \u201cespresso\u201d. In contrast, the \u201cstriped\u201d and \u201cpews\u201d concepts   \n238 significantly activate the final layer and substantially contribute to the predictions, although with   \n239 different magnitudes of importance. In the case of the \u201czebra\u201d image, for instance, the network\u2019s   \n240 decision is largely influenced by the \u201cstriped\u201d concept, which accounts for more than half the logit   \n241 value of the \u201czebra\u201d class. This concept also has a notable impact on the \u201cprairie chicken\u201d class and a   \n242 marginal one on the \u201cgondola\u201d class, probably since gondoliers usually wear striped t-shirts. More   \n243 examples of local explanations can be found in Appendix C. ", "page_idx": 5}, {"type": "image", "img_path": "LlcygqLdwO/tmp/3b4161ca80175679583a24832f812a2786b1b4d2b723b037e2c67db1c9db71bd.jpg", "img_caption": ["Figure 4: Results of global explanations for a variety of concepts, classes, and networks. Each bar chart reports the attributions of three concepts for a given class, throughout the last seven layers of each network. The attributions of each concept are computed across 200 images of the selected class. Although the theoretical limit of concept attributions is 1.0, the scale in our charts only extends to 0.6. This is based on our empirical observations, which rarely identified concepts with a global attribution exceeding this value. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "244 4.2 Global Explanations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "245 The concept attribution is a per-concept metric of importance, hence we can derive global explanations   \n246 by aggregating this attribution across a wide range of input images of a selected class. In our   \n247 experiments, we utilize 200 images per class for each global explanation. For concepts that are   \n248 inherently part of the class (e.g., \u201cstriped\u201d for \u201czebra\u201d or \u201cdimples\u201d for \u201cgolf ball\u201d), we can directly   \n249 use any image representing that class. On the other hand, for concepts that appear sporadically, we   \n250 only use images where the concept is present. For instance, we only use images of church interiors   \n251 for \u201cpews\u201d and \u201cfresco\u201d concepts, and images of church exteriors for the \u201csteeple\u201d concept. This   \n252 ensures that the explanations are independent of the frequency of the concept\u2019s appearance in the   \n253 class images.   \n254 The results are shown in Figure 4. The attributions match our intuitive expectations, considering, for   \n255 instance, the importance of the \u201cstriped\u201d concept for \u201czebra\u201d or \u201cspotted\u201d for \u201cdalmatian\u201d. Moreover,   \n256 the final layer typically provides the highest attribution, which is expected for class discriminative   \n257 concepts. However, there are instances, such as \u201cchequered\u201d and \u201cnewspaper\u201d for \u201ccrossword puzzle\u201d,   \n258 where concepts recognized in the earlier layers have a greater impact on the network\u2019s prediction. We   \n259 observe a more gradual increase in attribution in VGG16 and GoogleNet, compared to InceptionV3   \n260 and ResNet50V2. This could be attributed to the depth of the latter networks, which means they   \n261 perform more convolution operations that could potentially lead to a more complex feature extraction   \n262 between the analyzed layers. More examples of global explanations are provided in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "LlcygqLdwO/tmp/f4b5ac72b023520508031fa66c093c3dbf4ff00e9c0b9f53b428e4f6d21508f5.jpg", "img_caption": ["Figure 5: The results of the validation experiment. The upper section of the figure shows the test results and the concept attributions for both entities and tags across all models. The lower section provides examples of tagged images and concept maps for the no tags model and $100\\%$ tags model. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "263 4.3 Validation Experiment with Ground Truth ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "264 We conduct a validation experiment to evaluate the effectiveness of Visual-TCAV. In this experiment,   \n265 we train convolutional networks in a controlled setting, where ground truth is known, and assess   \n266 whether the Visual-TCAV attributions match this ground truth. For this purpose, we create a dataset   \n267 of three classes \u2013 cucumber, taxi, and zebra \u2013 which are the same classes used in the TCAV paper.   \n268 We then create multiple versions of this dataset by altering a percentage of the images with a tag,   \n269 represented by a letter enclosed in a randomly sized square and added in a random location of the   \n270 image (examples are shown in Figure 5a). Specifically, zebra images are tagged with a $^{\\bullet\\bullet}\\mathbf{Z}^{\\bullet}$ in a   \n271 purple square, taxi images with a \u201cT\u201d in a magenta square, and cucumber images with a \u201cC\u201d in a   \n272 cyan square. From these tagged images, we create five datasets: one of images without tags, and four   \n273 others with $25\\%$ , $50\\%$ , $75\\%$ , and $100\\%$ of tagged images, respectively. Each dataset is then used   \n274 to train a different model, each including six convolutional layers and a GAP layer. Depending on   \n275 the dataset used for training, each model may learn to recognize either the entities (i.e., cucumbers,   \n276 taxis, and zebras), the tags, or both and will decide which ones to give more importance. To obtain   \n277 an approximated ground truth assessing which concept \u2013 entity or tag \u2013 is more important, we ask   \n278 the models to classify a set of 200 incorrectly tagged test images per class. In this test set, taxis are   \n279 tagged with the \u201cZ\u201d, cucumbers are tagged with the \u201cT\u201d and zebras are tagged with the \u201cC\u201d. If the   \n280 network correctly classifies most of the images, it indicates that the entity is more important than the   \n281 tag, and thus, its attribution should be higher. On the other hand, if the performance deteriorates on   \n282 these wrongly tagged images, it indicates that the tag is more important than the entity, and thus its   \n283 attribution should be higher. We obtain the CAVs for entities using images of each class as concept   \n284 examples and random images as negative examples. For tags, we use random images containing that   \n285 tag as concept examples and images of cucumbers, taxis, and zebras containing the other two tags as   \n286 negative examples. We use the same incorrectly tagged test set to compute the concept attributions   \n287 for both entities and tags across the last convolutional layer of all models.   \n288 The results are shown in Figure 5. As expected, an increase in the percentage of tagged images   \n289 correlates with a decrease in accuracy. In particular, for the \u201ccucumber\u201d class the accuracy declines   \n290 much faster compared to other classes, with the majority of the images being incorrectly classified   \n291 as taxis. This suggests that even the models trained on a small fraction of tagged images tend to   \n292 overfit on the \u201cT\u201d tag. The concept attributions for both the \u201ccucumber\u201d entity and the \u201cT\u201d tag   \n293 closely mirror this ground truth. The \u201czebra\u201d entity and the \u201cC\u201d tag are also consistent with the   \n294 ground truth: the attributions for \u201czebra\u201d show a positive correlation with accuracy, whereas the   \n295 attributions for the \u201cC\u201d tag demonstrate a clear inverse correlation. Notably, the networks did not   \n296 pay much attention to the $^{\\bullet\\bullet}\\mathbf{Z}^{\\bullet}$ tag, focusing instead on the absence of the other two tags to classify   \n297 zebras. Indeed, the model trained with $100\\%$ of images tagged classifies any image without a \u201cC\u201d   \n298 or a \u201cT\u201d tag as \u201czebra\u201d, regardless of whether the \u201cZ\u201d tag is present or not. This is confirmed by   \n299 our method, which assigns an attribution of nearly zero to both the \u201cZ\u201d tag and the \u201ctaxi\u201d entity for   \n300 the aforementioned model. We tested other saliency methods, such as Grad-CAM and IG, to further   \n301 validate these findings. These methods do not highlight the \u201cZ\u201d tag either, but rather the entire image,   \n302 in search of the \u201czebra\u201d class (see Appendix B). For models trained with less than $100\\%$ of tags, the   \n303 accuracy for \u201ctaxi\u201d remains high, implying that these models are indeed capable of recognizing the   \n304 \u201ctaxi\u201d entity. The concept attribution for the \u201ctaxi\u201d entity aligns with this observation. In Figures   \n305 5b and 5c, we provide examples of concept maps for the model trained without tags and the model   \n306 trained with $100\\%$ of tagged images. The former recognizes the entities but not the tags, while the   \n307 latter struggles to recognize the entities but effectively identifies the \u201cT\u201d and \u201cC\u201d tags.   \n308 Comparison with the TCAV Score. The primary difference between our concept attribution and the   \n309 TCAV score is that the former considers not only the direction of gradients but also their magnitude.   \n310 This allows us to measure the concept\u2019s impact on the predictions, beyond just the network\u2019s sensitivity   \n311 to it. To demonstrate this, we compute the TCAV scores for tags and entities across each validation   \n312 model (see Figure 6). On one hand, TCAV scores match the ground truth in showing that the network   \n313 trained without tags exhibits high sensitivity to the entities and no sensitivity to the tags. Furthermore,   \n314 TCAV aligns with the concept attribution in showing that the $100\\%$ tags model is sensitive to the   \n315 \u201cT\u201d and \u201cC\u201d tags but not to the $^{\\bullet\\bullet}\\mathbf{Z}^{\\bullet}$ . On the other hand, TCAV struggles to capture the variations   \n316 in the concept\u2019s importance defined by ground truth. In fact, all models except the $100\\%$ tags show   \n317 very similar TCAV scores for the entity concepts, even though their importance varies significantly   \n318 across these models. This is attributable to most of the networks being sensitive to the entities.   \n319 Indeed, on images without tags, the models\u2019 accuracies are $96.5\\%$ , $96.2\\%$ , $96.2\\%$ , $95.2\\%$ , and $36.2\\%$   \n320 respectively. Similarly, the \u201cC\u201d tag has almost the same TCAV score for the models trained with $25\\%$ ,   \n321 $75\\%$ , and $100\\%$ tags, which is inconsistent with the decline in accuracy for the \u201cC\u201d tagged zebras. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "LlcygqLdwO/tmp/620938c04853f042a6244305238d39a93c1e713d0ff6853eeeabdd5e9f78a1f0.jpg", "img_caption": ["Figure 6: TCAV scores for tags and entities across each validation model. Results marked with an asterisk $(^{\\ast\\ast\\ast\\ast})$ have been excluded due to statistical insignificance $\\mathrm{\\Deltap}$ -value $>0.05$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "322 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "323 In this article, we introduced a novel method, Visual-TCAV, to explain the outputs of image classi  \n324 fication models. This framework is capable of providing both local and global explanations based   \n325 on high-level concepts, by estimating their attribution to the network\u2019s predictions. Additionally,   \n326 Visual-TCAV generates saliency maps to show where concepts are identified by the network, thereby   \n327 assuring the user that the attributions correspond to the intended concepts. The effectiveness of this   \n328 B method was demonstrated across a range of widely used CNNs and through a validation experiment,   \n329 where Visual-TCAV successfully identified the most important concept in each examined model.   \n330 Limitations and Future Work. Visual-TCAV provides a novel approach for concept-based explain  \n331 ability, but it has some limitations. Our current implementation only considers positive attributions   \n332 for classes with positive logit values. However, since a concept may negatively impact the output,   \n333 in future implementations we aim to include negative values, which would improve explanations   \n334 and also extend the applicability of Visual-TCAV beyond classification tasks. Another limitation   \n335 arises from the accumulation of noise along the IG linear path, which may sometimes result in   \n336 slightly underestimated attributions. Future studies could investigate how to mitigate this using   \n337 alternative IG variants to compute the attributions of feature maps. Additionally, future research   \n338 could explore generative approaches such as DreamBooth [19] to generate a large number of concept   \n339 images starting from a small set of examples, leading to more robust CAVs and reducing workload for   \n340 analysts. Finally, future works could study interconnections between concepts to determine how the   \n341 activation of a concept might influence not only the output but also the activation of other concepts. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "342 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "343 [1] Matteo Bianchi, Antonio De Santis, Andrea Tocchetti, and Marco Brambilla. Interpretable   \n344 network visualizations: A human-in-the-loop approach for post-hoc explainability of cnn-based   \n345 image classification, 2024. URL https://doi.org/10.48550/arXiv.2405.03301.   \n346 [2] Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. The effects of example-based explana  \n347 tions in a machine learning interface. In Proceedings of the 24th International Conference   \n348 on Intelligent User Interfaces, page 258\u201326. Association for Computing Machinery, 2019.   \n349 ISBN 9781450362726. doi: 10.1145/3301275.3302289. URL https://doi.org/10.1145/   \n350 3301275.3302289.   \n351 [3] Lei Cai, Jingyang Gao, and Di Zhao. A review of the application of deep learning in medical   \n352 image classification and segmentation. Annals of Translational Medicine, 8(11), 2020. ISSN   \n353 2305-5847. URL https://atm.amegroups.com/article/view/36944.   \n354 [4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild.   \n355 In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.   \n356 [5] Jonathan Crabb\u00e9 and Mihaela van der Schaar. Concept activation regions: A gen  \n357 eralized framework for concept-based explanations. In Sanmi Koyejo, S. Mohamed,   \n358 A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural In  \n359 formation Processing Systems 35: Annual Conference on Neural Information Process  \n360 ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - Decem  \n361 ber 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/   \n362 11a7f429d75f9f8c6e9c630aeb6524b5-Abstract-Conference.html.   \n363 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n364 scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern   \n365 Recognition, pages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.   \n366 [7] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept  \n367 based explanations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and   \n368 R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran   \n369 Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/   \n370 2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf.   \n371 [8] Mara Graziani, Vincent Andrearczyk, and Henning M\u00fcller. Regression concept vectors for   \n372 bidirectional explanations in histopathology. In Danail Stoyanov, Zeike Taylor, Seyed Mostafa   \n373 Kia, Ipek Oguz, Mauricio Reyes, Anne Martel, Lena Maier-Hein, Andre F. Marquand, Edouard   \n374 Duchesnay, Tommy L\u00f6fstedt, Bennett Landman, M. Jorge Cardoso, Carlos A. Silva, Sergio   \n375 Pereira, and Raphael Meier, editors, Understanding and Interpreting Machine Learning in   \n376 Medical Image Computing Applications, pages 124\u2013132, Cham, 2018. Springer International   \n377 Publishing. ISBN 978-3-030-02628-8.   \n378 [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual   \n379 networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer   \n380 Vision \u2013 ECCV 2016, pages 630\u2013645, Cham, 2016. Springer International Publishing. ISBN   \n381 978-3-319-46493-0.   \n382 [10] Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, and   \n383 Tolga Bolukbasi. Guided integrated gradients: an adaptive path method for removing noise.   \n384 In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n385 5048\u20135056, 2021. doi: 10.1109/CVPR46437.2021.00501.   \n386 [11] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas,   \n387 and Rory sayres. Interpretability beyond feature attribution: Quantitative testing with concept   \n388 activation vectors (TCAV). In Jennifer Dy and Andreas Krause, editors, Proceedings of the   \n389 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine   \n390 Learning Research, pages 2668\u20132677. PMLR, 10\u201315 Jul 2018. URL https://proceedings.   \n391 mlr.press/v80/kim18d.html.   \n392 [12] Zachary Lipton. The mythos of model interpretability. Communications of the ACM, 61, 10   \n393 2016. doi: 10.1145/3233231.   \n394 [13] Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Braun, Muhammad Imran Malik, Andreas   \n395 Dengel, and Sheraz Ahmed. On interpretability of deep learning based skin lesion classifiers   \n396 using concept activation vectors. pages 1\u201310, 07 2020. doi: 10.1109/IJCNN48605.2020.   \n397 9206946.   \n398 [14] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In   \n399 I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,   \n400 editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,   \n401 Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/   \n402 8a20a8621978632d76c43dfd28b67767-Paper.pdf.   \n403 [15] Tyler Martin and Adrian Weller. Interpretable Machine Learning. M.Phil. diss., Dept. of   \n404 Engineering, University of Cambridge, August 2019. URL https://www.mlmi.eng.cam.ac.   \n405 uk/files/tam_final_reduced.pdf.   \n406 [16] Deng Pan, Xin Li, and Dongxiao Zhu. Explaining deep neural network models with adversarial   \n407 gradient integration. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International   \n408 Joint Conference on Artificial Intelligence, IJCAI-21, pages 2876\u20132883. International Joint   \n409 Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/396. URL   \n410 https://doi.org/10.24963/ijcai.2021/396. Main Track.   \n411 [17] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cwhy should i trust you?\u201d: Explaining   \n412 the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International   \n413 Conference on Knowledge Discovery and Data Mining, KDD \u201916. ACM, August 2016. doi:   \n414 10.1145/2939672.2939778. URL http://dx.doi.org/10.1145/2939672.2939778.   \n415 [18] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis   \n416 with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern   \n417 Recognition (CVPR), pages 10674\u201310685, Los Alamitos, CA, USA, jun 2022. IEEE Computer   \n418 Society. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.ieeecomputersociety.   \n419 org/10.1109/CVPR52688.2022.01042.   \n420 [19] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning   \n421 text-to-image diffusion models for subject-driven generation. In 2023 IEEE/CVF Conference   \n422 on Computer Vision and Pattern Recognition (CVPR), pages 22500\u201322510, Los Alamitos,   \n423 CA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.02155. URL   \n424 https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.02155.   \n425 [20] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi   \n426 Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based   \n427 localization. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, October   \n428 2017. doi: 10.1109/iccv.2017.74. URL http://dx.doi.org/10.1109/ICCV.2017.74.   \n429 [21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale   \n430 image recognition. In International Conference on Learning Representations, 2015.   \n431 [22] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:   \n432 Visualising image classification models and saliency maps. In Workshop at International   \n433 Conference on Learning Representations, 2014.   \n434 [23] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Vi\u00e9gas, and Martin Wattenberg.   \n435 Smoothgrad: removing noise by adding noise. CoRR, abs/1706.03825, 2017. URL http:   \n436 //arxiv.org/abs/1706.03825.   \n437 [24] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving   \n438 for simplicity: The all convolutional net. CoRR, abs/1412.6806, 2014.   \n439 [25] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In   \n440 Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917,   \n441 page 3319\u20133328. JMLR.org, 2017.   \n442 [26] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,   \n443 Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.   \n444 In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1\u20139,   \n445 2015. doi: 10.1109/CVPR.2015.7298594.   \n446 [27] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re  \n447 thinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer   \n448 Vision and Pattern Recognition (CVPR), pages 2818\u20132826, 2016. doi: 10.1109/CVPR.2016.308.   \n449 [28] Tolga Turay and Tanya Vladimirova. Toward performing image classification and object   \n450 detection with convolutional neural networks in autonomous driving systems: A survey. IEEE   \n451 Access, 10:14076\u201314119, 2022. doi: 10.1109/ACCESS.2022.3147495.   \n452 [29] Warren von Eschenbach. Transparency and the black box problem: Why we do not trust ai.   \n453 Philosophy & Technology, 34, 12 2021. doi: 10.1007/s13347-021-00477-0.   \n454 [30] Chase Walker, Sumit Jha, Kenny Chen, and Rickard Ewetz. Integrated decision gradients:   \n455 Compute your attributions where the model makes its decision. Proceedings of the AAAI   \n456 Conference on Artificial Intelligence, 38:5289\u20135297, 03 2024. doi: 10.1609/aaai.v38i6.28336.   \n457 [31] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A. Ehinger, and Benjamin I. P. Rubinstein.   \n458 Invertible concept-based explanations for cnn models with non-negative concept activation   \n459 vectors. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11682\u201311690,   \n460 May 2021. doi: 10.1609/aaai.v35i13.17389. URL https://ojs.aaai.org/index.php/   \n461 AAAI/article/view/17389. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "462 A Appendix Overview ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "463 In the appendix, we provide: ", "page_idx": 11}, {"type": "text", "text": "464 B. Saliency methods for $100\\%$ tags model   \n465 C. Additional results of Local Explanations   \n466 D. Additional results of Global Explanations   \n467 E. Example images for generated concepts ", "page_idx": 11}, {"type": "text", "text": "468 B Saliency methods for $100\\%$ tags model ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "469 We provide the results obtained by applying IG and Grad-CAM to the $100\\%$ tags model (see Figure 7).   \n470 These methods align with Visual-TCAV in showing that this model does not pay attention to the $^{\\bullet\\bullet}Z^{\\bullet}$ ,   \n471 but rather to the absence of the \u201cT\u201d and the \u201cC\u201d for predicting the \u201czebra\u201d class. ", "page_idx": 11}, {"type": "image", "img_path": "LlcygqLdwO/tmp/5c5e9e526f7c404b6a1c2e9afc49f2848f45a58b4035bd150dd6692df9198778.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Figure 7: Integrated Gradients and Grad-CAM for the model with $100\\%$ tags, searching respectively for the classes \u201czebra\u201d, \u201ctaxi\u201d, and \u201ccucumber\u201d. Both methods highlight the \u201cT\u201d for class \u201ctaxi\u201d and the \u201cC\u201d for class \u201ccucumber\u201d, but fail to recognize the $\\bullet\\bullet$ for class \u201czebra\u201d. ", "page_idx": 11}, {"type": "text", "text": "472 C Additional results of Local Explanations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "473 Continuing from the results presented in Section 4.1, we further provide additional local explanations   \n474 for more input images and concepts in Figure 8. ", "page_idx": 11}, {"type": "image", "img_path": "LlcygqLdwO/tmp/8b35d5a0294eb8990440a8df6abbef80454275488b89f12f6e74281c7990ec70.jpg", "img_caption": ["Figure 8: More examples of layer-wise local explanations for various concepts and networks. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "475 D Additional results of Global Explanations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "476 Building upon the results outlined in Section 4.2, we provide additional global explanations for   \n477 various classes and concepts in Figure 9. ", "page_idx": 13}, {"type": "image", "img_path": "LlcygqLdwO/tmp/129545d5543e84c58c2c29fc8b83dde27ba15b6a3732c7142c6239eb73c057af.jpg", "img_caption": ["Figure 9: More examples of global explanations for various classes, concepts, and networks. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "478 E Example images for generated concepts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "479 Some of the concepts used in the paper were automatically generated using Stable Diffusion v1.5 [18]   \n480 with default parameters. In particular, we generated the following concepts: \u201cpews\u201d, \u201cfresco\u201d,   \n481 \u201carches\u201d, \u201csky\u201d, \u201cpipes\u201d, and \u201cbrass\u201d. We used just the concept name as a prompt and generated 200   \n482 images per concept. A subsequent manual revision was still necessary to eliminate errors and strange   \n483 artifacts. In Figure 10, we provide three example images for each generated concept. ", "page_idx": 14}, {"type": "image", "img_path": "LlcygqLdwO/tmp/f26acaa15ca4a9794c6325d5b92f7426f89604583844f92c01b505d341e9b7d1.jpg", "img_caption": ["Figure 10: We provide three example images for each concept generated with Stable Diffusion v1.5. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "484 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "485 1. Claims   \n486 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n487 paper\u2019s contributions and scope?   \n488 Answer: [Yes]   \n489 Justification: We claim that our method can provide visual explanations through saliency   \n490 maps based on user-defined concepts, estimate the attributions of these concepts for a   \n491 selected class, and provide both local and global explanations. These claims are all validated   \n492 through the experimental results performed in the paper.   \n493 2. Limitations   \n494 Question: Does the paper discuss the limitations of the work performed by the authors?   \n495 Answer: [Yes]   \n496 Justification: Our work has some limitations, we discuss them in Section 5.1.   \n497 3. Theory Assumptions and Proofs   \n498 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n499 a complete (and correct) proof?   \n500 Answer: [NA]   \n501 Justification: The paper does not include any new proof or theorem.   \n502 4. Experimental Result Reproducibility   \n503 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n504 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n505 of the paper (regardless of whether the code and data are provided or not)?   \n506 Answer: [Yes]   \n507 Justification: Every experimental result presented in the paper is fully reproducible using   \n508 the provided code and data.   \n509 5. Open access to data and code   \n510 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n511 tions to faithfully reproduce the main experimental results, as described in supplemental   \n512 material?   \n513 Answer: [Yes]   \n514 Justification: We provide the code, data, and instructions needed to reproduce every ex  \n515 periment both to reviewers and to the public through a GitHub repository (in case of   \n516 publication).   \n517 6. Experimental Setting/Details   \n518 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n519 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n520 results?   \n521 Answer: [Yes]   \n522 Justification: The paper describes in detail all the necessary steps to reproduce and un  \n523 derstand the experiments. Furthermore, the code used is also available as supplementary   \n524 material.   \n525 7. Experiment Statistical Significance   \n526 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n527 information about the statistical significance of the experiments?   \n528 Answer: [Yes]   \n529 Justification: In our bar plots we always report 2-sigma error bars.   \n530 8. Experiments Compute Resources   \n531 Question: For each experiment, does the paper provide sufficient information on the com  \n532 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n533 the experiments?   \n534 Answer: [Yes]   \n535 Justification: We describe in detail the characteristics of the machine used to run all the   \n536 experiments and the execution time.   \n537 9. Code Of Ethics   \n538 Question: Does the research conducted in the paper conform, in every respect, with the   \n539 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n540 Answer: [Yes]   \n541 Justification: We have reviewed the NeurIPS Code of Ethics and our research conforms with   \n542 it.   \n543 10. Broader Impacts   \n544 Question: Does the paper discuss both potential positive societal impacts and negative   \n545 societal impacts of the work performed?   \n546 Answer: [Yes]   \n547 Justification: In the introduction, we briefly discuss the problem of transparency in AI   \n548 systems, particularly as Convolutional Neural Networks are being widely utilized in critical   \n549 sectors such as healthcare and autonomous driving. Our work can have a positive societal   \n550 impact by facilitating a trustworthy adoption of these systems. We are not aware of any   \n551 negative impact our work could have.   \n552 11. Safeguards   \n553 Question: Does the paper describe safeguards that have been put in place for responsible   \n554 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n555 image generators, or scraped datasets)?   \n556 Answer: [NA]   \n557 Justification: This paper does not release any data or models that pose such risks.   \n558 12. Licenses for existing assets   \n559 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n560 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n561 properly respected?   \n562 Answer: [Yes]   \n563 Justification: All models and datasets used for the experiments are properly cited in the   \n564 paper.   \n565 13. New Assets   \n566 Question: Are new assets introduced in the paper well documented and is the documentation   \n567 provided alongside the assets?   \n568 Answer: [NA]   \n569 Justification: The paper does not introduce new assets.   \n570 14. Crowdsourcing and Research with Human Subjects   \n571 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n572 include the full text of instructions given to participants and screenshots, if applicable, as   \n573 well as details about compensation (if any)?   \n574 Answer: [NA]   \n575 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n576 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n577 Subjects ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?   \nAnswer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}]