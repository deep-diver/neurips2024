{"references": [{"fullname_first_author": "Been Kim", "paper_title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)", "publication_date": "2018-07-10", "reason": "This paper introduces TCAV, a foundational concept-based method for explaining image classifiers, which is directly built upon and extended by the current work."}, {"fullname_first_author": "Ramprasaath R. Selvaraju", "paper_title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization", "publication_date": "2017-10-00", "reason": "This highly influential paper presents Grad-CAM, a widely used saliency method, which is referenced and compared to in the current work's methodology."}, {"fullname_first_author": "Mukund Sundararajan", "paper_title": "Axiomatic attribution for deep networks", "publication_date": "2017-00-00", "reason": "This paper introduces Integrated Gradients (IG), a method for computing feature attributions that addresses limitations of previous gradient-based approaches; it is extended in the current work for concept-based attribution."}, {"fullname_first_author": "Marco Tulio Ribeiro", "paper_title": "\u201cWhy should I trust you?\u201d: Explaining the predictions of any classifier", "publication_date": "2016-08-00", "reason": "This paper introduces LIME, a widely used model-agnostic explainability technique, which is referenced as a contrasting approach to the current work\u2019s model-specific methodology."}, {"fullname_first_author": "Zachary Lipton", "paper_title": "The mythos of model interpretability", "publication_date": "2016-10-00", "reason": "This paper provides valuable context and background on the challenges and complexities of interpreting deep learning models, which are relevant to the current work's goal of improving explainability."}]}