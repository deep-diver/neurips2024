[{"figure_path": "LlcygqLdwO/figures/figures_2_1.jpg", "caption": "Figure 1: The Visual-TCAV process for generating local explanations. A Pooled-CAV is computed using the feature maps of user-defined concept examples and random images. A concept map is then produced through a weighted sum of the Pooled-CAV and the image's feature maps. Finally, a concept attribution is obtained by extracting the IG attributions of the neurons that the concept activates using the Pooled-CAV and the concept map, which is used as a spatial mask.", "description": "This figure illustrates the Visual-TCAV process for generating local explanations. It starts with concept examples and random images, from which a Pooled-CAV is computed using a difference of means method.  This Pooled-CAV is then combined with the input image's feature maps to create a concept map. Finally, Integrated Gradients are used to compute attributions, focused on the neurons activated by the concept, producing the concept attribution.", "section": "3 Visual-TCAV"}, {"figure_path": "LlcygqLdwO/figures/figures_3_1.jpg", "caption": "Figure 2: Examples of class-independent concept maps for various input images and concepts.", "description": "This figure shows six examples of concept maps generated by Visual-TCAV for different concepts and input images. Each image has a concept map overlaid, which highlights the regions where the network recognizes the specific concept. The concepts are diverse, including \"hands\", \"dimples\", \"arches\", \"sky\", \"car\", and \"chequered\".  These visualizations help demonstrate how Visual-TCAV can pinpoint regions of an image that are relevant to a given concept, regardless of the image's class. The class-independence is crucial as it allows analyzing how a concept is identified in images irrespective of the assigned class label.", "section": "3 Visual-TCAV"}, {"figure_path": "LlcygqLdwO/figures/figures_5_1.jpg", "caption": "Figure 3: Examples of layer-wise local explanations for various concepts and networks. We compute the attribution of each concept for the top three predicted classes and the last seven layers.", "description": "This figure shows several examples of local explanations generated by Visual-TCAV for different concepts across various CNN architectures (VGG16, InceptionV3, ResNet50V2). Each example demonstrates the concept map (visual representation of concept recognition) and concept attribution (numerical importance) in several layers.  The attributions are calculated for the top three predicted classes. The results show how the concept's importance increases as you approach the final layers, indicating that this framework can effectively identify and quantify the contribution of specific concepts to a model's predictions.", "section": "4.1 Local Explanations"}, {"figure_path": "LlcygqLdwO/figures/figures_6_1.jpg", "caption": "Figure 3: Examples of layer-wise local explanations for various concepts and networks. We compute the attribution of each concept for the top three predicted classes and the last seven layers.", "description": "This figure shows local explanations generated by Visual-TCAV for various concepts across different CNN architectures.  For each concept, the attribution (importance) is calculated for the top three predicted classes and displayed across the last seven layers of the network.  Each bar represents the attribution of a concept to a specific class, and the color coding across layers shows how the concept's importance changes as the model processes the information.", "section": "4.1 Local Explanations"}, {"figure_path": "LlcygqLdwO/figures/figures_7_1.jpg", "caption": "Figure 5: The results of the validation experiment. The upper section of the figure shows the test results and the concept attributions for both entities and tags across all models. The lower section provides examples of tagged images and concept maps for the no tags model and 100% tags model.", "description": "This figure presents the results of a validation experiment designed to test the effectiveness of Visual-TCAV.  The top row shows three bar charts. The leftmost chart displays the accuracy of three different models (trained on datasets with varying percentages of tagged images) on a test set of incorrectly tagged images.  The middle and right charts show concept attributions (with error bars) from Visual-TCAV, calculated using either the entities (cucumber, taxi, zebra) or the tags ('C', 'T', 'Z') as concepts.  The bottom row shows example images from the training and test sets, along with corresponding concept maps generated by Visual-TCAV for a model trained without tags and a model trained with 100% tagged images, demonstrating how the model's attention shifts from recognizing the entities to recognizing the tags as the proportion of tagged images increases.", "section": "4.3 Validation Experiment with Ground Truth"}, {"figure_path": "LlcygqLdwO/figures/figures_8_1.jpg", "caption": "Figure 6: TCAV scores for tags and entities across each validation model. Results marked with an asterisk (\"*\") have been excluded due to statistical insignificance (p-value > 0.05).", "description": "This figure shows the TCAV scores for both tags and entities across different models in a validation experiment.  The x-axis represents the percentage of tagged images in the training data (from 0% to 100%). The y-axis represents the TCAV score, indicating the correlation between the concept (tag or entity) and the model's prediction. The bars are grouped by concept (taxi, cucumber, zebra for entities; \"Z\" tag, \"T\" tag, \"C\" tag for tags) and show the average TCAV score for each model.  The asterisk (*) indicates results excluded due to statistical insignificance.", "section": "4.3 Validation Experiment with Ground Truth"}, {"figure_path": "LlcygqLdwO/figures/figures_11_1.jpg", "caption": "Figure 5: The results of the validation experiment. The upper section of the figure shows the test results and the concept attributions for both entities and tags across all models. The lower section provides examples of tagged images and concept maps for the no tags model and 100% tags model.", "description": "This figure displays the results of a validation experiment conducted to evaluate the effectiveness of Visual-TCAV.  The upper part shows bar charts illustrating test results and concept attributions for entities (zebra, taxi, cucumber) and their corresponding tags (Z, T, C) across various models trained with different percentages of tagged images. The lower part showcases examples of images: those without tags and those with 100% of images tagged. Concept maps are also shown, visualizing the regions recognized by the model as corresponding to each concept.", "section": "4.3 Validation Experiment with Ground Truth"}, {"figure_path": "LlcygqLdwO/figures/figures_12_1.jpg", "caption": "Figure 3: Examples of layer-wise local explanations for various concepts and networks. We compute the attribution of each concept for the top three predicted classes and the last seven layers.", "description": "This figure shows several examples of local explanations generated by Visual-TCAV for different concepts, image classification models (VGG16, InceptionV3, ResNet50V2, and GoogleNet), and layers within the models.  For each example, it displays the input image, a concept map (visualizing where the concept is recognized), and a table of concept attributions for the top 3 predicted classes. The attributions show how strongly each concept contributed to the prediction for each class, layer by layer. The purpose is to illustrate how Visual-TCAV localizes concepts and assesses their impact on the model's predictions at different stages of the network's processing.", "section": "4.1 Local Explanations"}, {"figure_path": "LlcygqLdwO/figures/figures_13_1.jpg", "caption": "Figure 1: The Visual-TCAV process for generating local explanations. A Pooled-CAV is computed using the feature maps of user-defined concept examples and random images. A concept map is then produced through a weighted sum of the Pooled-CAV and the image's feature maps. Finally, a concept attribution is obtained by extracting the IG attributions of the neurons that the concept activates using the Pooled-CAV and the concept map, which is used as a spatial mask.", "description": "This figure illustrates the Visual-TCAV process step by step. It starts by generating a Pooled-CAV from concept and random examples. Then, it creates a concept map by applying a weighted sum of the Pooled-CAV and the input image's feature maps, which is used as a spatial mask to compute the concept attribution using Integrated Gradients. The output is a local explanation that visualizes where the concept is identified and quantifies its impact on the prediction.", "section": "3 Visual-TCAV"}, {"figure_path": "LlcygqLdwO/figures/figures_14_1.jpg", "caption": "Figure 1: The Visual-TCAV process for generating local explanations. A Pooled-CAV is computed using the feature maps of user-defined concept examples and random images. A concept map is then produced through a weighted sum of the Pooled-CAV and the image's feature maps. Finally, a concept attribution is obtained by extracting the IG attributions of the neurons that the concept activates using the Pooled-CAV and the concept map, which is used as a spatial mask.", "description": "This figure illustrates the Visual-TCAV process. It shows how a pooled-CAV (Concept Activation Vector) is generated from concept and random image feature maps.  A concept map is created by weighting the image's feature maps with the pooled-CAV. Finally, integrated gradients (IG) are used, along with the concept map as a mask, to calculate the concept's attribution.", "section": "3 Visual-TCAV"}]