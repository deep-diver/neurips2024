[{"figure_path": "RxQoIekEa2/figures/figures_7_1.jpg", "caption": "Figure 1: Concentration of empirical KKLa for d = 10, \u03c3 = 10, p, q Gaussians.", "description": "This figure shows the concentration of the regularized Kernel Kullback-Leibler (KKL) divergence around its population limit as the number of samples increases.  The experiment uses two anisotropic Gaussian distributions (p and q) in 10 dimensions.  The plot displays the empirical KKL (KKL\u0251(p\u0302, q\u0302)) for different values of the regularization parameter \u03b1.  The results are averaged over 50 runs, showing the mean and standard deviation.  The figure demonstrates that the empirical KKL converges to its population value faster as \u03b1 increases.", "section": "Illustrations of skewness and concentration of the KKL"}, {"figure_path": "RxQoIekEa2/figures/figures_8_1.jpg", "caption": "Figure 2: MMD, KALE and KKL flow for 3 rings target.", "description": "This figure compares the performance of three different gradient flows (MMD, KALE, and KKL) in transporting a set of points to a target distribution shaped as three non-overlapping rings.  The images show the evolution of the point cloud at different time steps during the optimization process.  It visualizes how each method's gradient flow affects the distribution of the points over time.  The figure highlights the strengths and weaknesses of each approach in terms of approximating the target distribution.", "section": "Experiments"}, {"figure_path": "RxQoIekEa2/figures/figures_25_1.jpg", "caption": "Figure 1: Concentration of empirical KKLa for d = 10, \u03c3 = 10, p, q Gaussians.", "description": "The figure illustrates the concentration of the regularized Kernel Kullback-Leibler (KKL) divergence for empirical measures around its population limit as the number of samples increases. It shows the results obtained over 50 runs for different values of the regularization parameter \u03b1, with thick lines representing the average values. The distributions p and q are anisotropic Gaussian distributions with different means and variances. The figure demonstrates the convergence behavior of the empirical KKL towards its population counterpart, which is faster for larger values of \u03b1 as predicted by the theoretical results.", "section": "Illustrations of skewness and concentration of the KKL"}, {"figure_path": "RxQoIekEa2/figures/figures_26_1.jpg", "caption": "Figure 9: \u03c3 = 10, h = 5, p is Gaussian and q is a mixture of 2 Gaussians.", "description": "This figure displays the evolution of the 2-Wasserstein distance (W2(p||q)) during the gradient descent in dimension d=10 for various parameters \u03b1.  The initial distribution p is a Gaussian, and the target distribution q is a mixture of two Gaussians.  The plot shows the average W2(p,q) over 10 runs, where the mean of p is randomly initialized for each run.  The figure illustrates how the convergence speed and the optimal Wasserstein distance at the end of the algorithm are affected by the choice of \u03b1.", "section": "Sampling with KKL gradient descent"}, {"figure_path": "RxQoIekEa2/figures/figures_26_2.jpg", "caption": "Figure 2: MMD, KALE and KKL flow for 3 rings target.", "description": "This figure compares the performance of three different methods: MMD, KALE, and KKL, in transporting a set of points (initial distribution) to a target distribution shaped like three rings. The images show the evolution of the point distribution over time for each method.  The results demonstrate that KKL and KALE effectively move the points toward the target distribution, whereas MMD does not adequately capture the support of the target distribution. ", "section": "Experiments"}, {"figure_path": "RxQoIekEa2/figures/figures_27_1.jpg", "caption": "Figure 2: MMD, KALE and KKL flow for 3 rings target.", "description": "This figure compares the performance of three different methods: MMD, KALE, and KKL, in transporting a set of points (the source distribution) towards a target distribution shaped like three non-overlapping rings. The initial source distribution is a Gaussian distribution near the rings. The evolution of the distributions across different timesteps (T=0, T=2, T=30, T=60, T=99) is visualized, showing how the points are moved towards the target distribution over time using Wasserstein gradient descent.", "section": "Experiments"}]