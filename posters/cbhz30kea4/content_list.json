[{"type": "text", "text": "Taming \u201cdata-hungry\u201d reinforcement learning? Stability in continuous state-action spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yaqi Duan Department of Technology, Operations, and Statistics Stern School of Business, New York University New York, NY 10012 yaqi.duan@stern.nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Martin J. Wainwright Laboratory for Information and Decision Systems, Statistics and Data Science Center Department of Electrical Engineering and Computer Science, and Department of Mathematics Massachusetts Institute of Technology Cambridge, MA 02139 wainwrigwork@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes. Our analysis also offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many domains of science and engineering involve making a sequence of decisions over time, with previous decisions influencing the future in uncertain ways [1, 13, 22, 31, 32]. For instance, clinicians managing diabetes [36] or engineers optimizing plasma control in tokamak systems [5] must develop policies that adapt based on evolving conditions and lead to desirable outcomes over a longer period. Markov decision processes (MDPs) and reinforcement learning (RL) provide frameworks and methods for estimating effective policies for such sequential problems. While RL excels in data-rich scenarios such as competitive gaming (e.g., AlphaGo and its extensions [30]), its application in data-scarce areas like healthcare [36] and finance [27] remains challenging due to lack of history, or underlying non-stationarity. With limited data, characterizing and improving the sample complexity of RL methods becomes critical. ", "page_idx": 0}, {"type": "text", "text": "Considerable research effort has been devoted to studying RL sample complexity in many settings. Existing studies for either the generative or the off-line settings (e.g., [21, 37, 35]) give \u221aprocedures that, when applied to a dataset of size $n$ , yield a value gap that decays at the rate $1/\\bar{\\sqrt{n}}$ . In the on-line setting, there are various procedures that yield cumulative regret that grows at the rate $\\sqrt{T}$ (e.g., [18, 20, 19, 6]). In contrast, the main result of this paper is to formalize conditions, suitable for RL in continuous domains, under which much faster rates can be obtained using the same dataset, achieving a value gap decay of $1/n$ and reducing regret growth to $\\log T$ . ", "page_idx": 0}, {"type": "text", "text": "As revealed by our analysis, these accelerated rates depend on certain stability properties, ones that\u2014as we argue\u2014are naturally satisfied in many control problems with continuous state-action spaces. Roughly speaking, these conditions ensure that the evolution of the dynamic system depends in a \u201csmooth\u201d way on the influence of decision policy. Such notions of stability should be expected in various controlled systems with continuous state-action spaces. In robotics, for example, a minor torque or motion perturbation that occurs during a single step should not cause a notable deviation from the intended trajectory. Similarly, in clinical treatment, slight deviations in medication dosage should not significantly compromise effectiveness or safety. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 A simple illustrative example: Mountain Car ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The \u201cMountain Car\u201d problem, a benchmark continuous control task, illustrates the acceleration phenomenon and underlying stability. In this task, as shown in Figure 1(a), a car must reach the top of a hill by adjusting its acceleration within the interval $[-1,1]$ . We employed ftited $Q$ -iteration (FQI) with carefully selected linear basis functions to derive near-optimal policies with off-line data. This learning procedure exhibits a\u221a value sub-optimality decay at a rate of $1/n$ , a significant improvement over the classical rate of $1/\\sqrt{n}$ , as detailed in Figure 1(b). (See Appendix D for further explanation. The experiment ran for 3 days on two laptops, each equipped with an Apple M2 Pro CPU and 16 GB RAM.) In this example, slight perturbations in the driving policy lead to only modest changes in future trajectories, which shows the stability. Our theoretical analysis confirms that fast rates are achievable in this and similar continuous control tasks when such stability properties are present. ", "page_idx": 1}, {"type": "image", "img_path": "CbHz30KeA4/tmp/7f7350ee0303c37a3ce862f7db18eedd5181d61148782ffcd30b3a0cc5de47a6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of the \u201cfast rate\u201d phenomenon using FQI on the Mountain Car problem. Each red point in the plot represents the average value sub-optimality $J(\\pi^{\\star})-J(\\widehat\\pi_{n})$ from $T=80$ Monte Carlo trials, with the shaded area showing twice the standard errors. The  blue dashed line is a least-squares fti to the last 6 data points, yielding a $95\\%$ confidence interval of $\\left(-1.084,-0.905\\right)$ for the slope, significantly faster than the typical $-0.5$ \u201cslow rate\u201d. ", "page_idx": 1}, {"type": "text", "text": "1.2 Contributions of this paper ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "With this high-level perspective in mind, let us summarize the key contributions of this paper. ", "page_idx": 1}, {"type": "text", "text": "Fast rate of convergence: We develop a framework for analyzing RL in continuous state-action spaces, and use it to prove a general result (Theorem 1) under which fast rates can be obtained. The key insight is that stability conditions lead to upper bounds on the value sub-optimality that are proportional to the squared norm of Bellman residuals. In the off-line setting, this quadratic scaling improves con\u221avergence from a rate of $n^{-{\\frac{1}{2}}}$ to $n^{-1}$ , while in on-line learning, it enhances the regret bound from $\\sqrt{T}$ to $\\log T$ . ", "page_idx": 1}, {"type": "text", "text": "Reconsidering pessimism and optimism principles: Our framework provides a novel perspective on the roles of pessimism [21, 4] and optimism [18, 20, 19, 6, 12] in off-line and on-line RL. Our theory reveals that there are settings in which neither pessimism nor optimism are required for effective policy optimization\u2014in particular, they are not required as long as one has a sufficiently accurate pilot estimate policy. Moreover, our analysis shows that some procedures based on certainty equivalence can achieve fast-rate convergence, showing that the benefits gained from incorporating additional pessimism or optimism measures may be limited in this context. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss related work having to do with fast rates in optimization and statistics. ", "page_idx": 2}, {"type": "text", "text": "Fast rates in stochastic optimization and risk minimization: For many statistical estimators (e.g., likelihood methods, empirical risk minimization), it is well-understood that the local geometry around the optimum determines whether fast rates can be obtained. For instance, when the loss function exhibits some form of strong convexity (such as exp-c\u221aoncave loss) or strict saddle properties, it can lead to significant reductions in additive regret from $\\bar{\\mathcal{O}}(\\sqrt{T})$ to just ${\\mathcal{O}}(\\log T)$ in stochastic approximation (e.g., [15]), or a decrease in the error rate from $n^{-{\\frac{1}{2}}}$ to $n^{-1}$ in empirical risk minimization [23, 14]. These fast rate phenomena rely on a form of stability, one which relates the similarity of functions to the closeness of their optima. Our work develops a new framework for analyzing value-based RL methods, focusing on identifying specific stability conditions and inherent curvature properties that promote fast rate convergence in RL, similar to the role of stability analysis in statistical learning. ", "page_idx": 2}, {"type": "text", "text": "Fast rates in reinforcement learning: In the RL literature, there are various lines of work related to fast rates, but the underlying mechanisms are typically different from those considered here. For problems with discrete state-action spaces, there is a line of recent work [17, 16, 33, 25] that performs gap/marginal-dependent analyses of RL algorithms. However, such separation assumptions are not helpful for continuous action spaces. Other work for discrete state-action spaces [28] has shown convergence rates in off-line RL are influenced by data quality, with a nearly-expert dataset enabling faster rate. In contrast, our analysis reveals that for off-line RL in continuous domains, fast convergence can occur whether or not the dataset has good coverage properties. ", "page_idx": 2}, {"type": "text", "text": "An important sub-class of continuous state-action problems are those with linear dynamics and quadratic reward functions (LQR for short). For such problems, it has been shown [24, 29] that value sub-optimality can be connected with the squared error in system identification. Our general theory can also be used to derive guarantees for LQR problems, as we explore in more detail in a follow-up paper [8]. Stability also arises in the analysis of (deterministic) policy optimization and Newton-type algorithms [26, 3], where it is possible to show superlinear convergence in a local neighborhood. This accelerated rate stems from the smoothness of the on-policy transition operator $\\mathcal{P}^{\\pi_{f}}$ with respect to changes in the value function $f$ ; for instance, see condition (10) in Puterman and Brumelle [26]. Our framework exploits related notions of smoothness, but is tailored to the stochastic setting of reinforcement learning, in which understanding the effect of function approximation and finite sample sizes is essential. ", "page_idx": 2}, {"type": "text", "text": "2 Fast rates for value-based reinforcement learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us now set up and state the main result of this paper. We begin in Section 2.1 with background on Markov decision processes (MDPs) and value-based methods, before turning to the statement of our main result in Section 2.2. In Section 2.3, we provide intuition for why stability leads to faster rates, and discuss consequences for both the off-line and on-line settings of RL. ", "page_idx": 2}, {"type": "text", "text": "2.1 Markov decision processes and value-based methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Basic set-up: We consider an episodic Markov decision process (MDP) defined by a quadruple $\\left(\\boldsymbol{S},\\boldsymbol{\\mathcal{A}},\\bar{\\boldsymbol{P}}\\,=\\,\\{\\mathcal{P}_{h}\\}_{h=1}^{H-1},\\{r_{h}\\}_{h=1}^{\\bar{H^{\\prime}}}\\right)$ . We assume that the rewards $r_{h}:S\\times A\\to\\mathbb{R}$ are known; however, this condition can be relaxed. A policy at time is a mapping from any state to a distribution $\\pi_{h}(\\cdot\\ |\\ \\ s)$ over the action space $\\boldsymbol{\\mathcal{A}}$ . If the support of $\\pi_{h}(\\cdot\\mid s)$ is a singleton, we also let $\\pi_{h}(s)\\in{\\dot{A}}$ denote the single action to be chosen at state $s$ . Given an initial distribution $\\xi_{1}$ over the states at time $h=1$ , the expected reward obtained by choosing actions according to a policy sequence ${\\boldsymbol{\\pi}}\\,=\\,(\\pi_{1},\\ldots,\\pi_{H})$ is given by $\\begin{array}{r}{J({\\pi})\\,\\equiv\\,J({\\pi};{\\dot{\\xi}}_{1}){:=}\\mathbb{E}_{\\xi_{1},\\dot{\\pi}}\\bigl[\\sum_{h=1}^{H}r_{h}(S_{h},\\dot{A_{h}})\\bigr]}\\end{array}$ , where $S_{1}\\sim\\xi_{1},\\,S_{h+1}\\sim\\mathcal{P}_{h}(\\cdot\\mid S_{h},A_{h})$ and $A_{h}\\sim\\pi_{h}(\\cdot\\mid S_{h})$ for $h=1,2,\\dots,H$ . Our goal is to estimate an optimal policy $\\pi^{\\star}\\in\\arg\\operatorname*{max}_{\\pmb{\\pi}}J(\\pmb{\\pi})$ . ", "page_idx": 2}, {"type": "text", "text": "Value functions and Bellman operators: Starting from a given state-action pair $(s,a)$ at stage $h$ , the expected return over subsequent stages defines the state-action value function $\\begin{array}{r}{Q_{h}^{\\pi}(s,a):=\\operatorname{\\mathbb{E}}_{\\pi}\\big[\\sum_{h^{\\prime}=h}^{H}\\,r(S_{h^{\\prime}},A_{h^{\\prime}})\\;\\big|\\;S_{h}=s,A_{h}=a\\big]}\\end{array}$ . The sequence of functions $Q^{\\pi}=(Q_{1}^{\\pi},\\ldots,Q_{H}^{\\pi})$ known as the $Q$ -functions  associated with $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "The $Q$ -functions $Q^{\\pi}$ have an important connection with the Bellman evaluation operator for $\\pi$ . For any policy $\\pi$ and stage $h$ , we introduce a linear transition operator $(\\mathcal{P}_{h}^{\\bar{\\pi}}f)(s,a)\\;\\;:=$ $\\begin{array}{r}{\\int_{S\\times A}f(s^{\\prime},a^{\\prime})\\ \\mathcal{P}_{h}(d s^{\\prime}\\ \\mid\\ s,a)\\,\\pi_{h+1}(d a^{\\prime}\\ \\mid\\ s^{\\prime})}\\end{array}$ for any function $f\\,\\in\\,\\mathbb{R}^{S\\times A}$ . With this notation, the Bellman evaluation operator at stage $h$ takes the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n({\\mathcal T}_{h}^{\\pi}f)(s,a):=r_{h}(s,a)+({\\mathcal P}_{h}^{\\pi}f)(s,a).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From classical dynamic programming, the $Q$ -functions $Q^{\\pi}$ must satisfy the Bellman relations $Q_{h}^{\\pi}(s,a)=(T_{h}^{\\pi}\\dot{Q}_{h+1}^{\\pi})(\\bar{s.}a)$ for $h=1,\\ldots,H-1$ . ", "page_idx": 3}, {"type": "text", "text": "Bellman principle for optimal policies: Under mild regularity conditions, there is at least one policy $\\pi^{\\star}$ such that, for any other policy $\\pi$ , we have $Q_{h}^{\\pi^{\\overline{{\\star}}}}(s,a)^{\\overline{{\\star}}}\\geq Q_{h}^{\\pi}(s,a)$ , for any $h\\in[H]$ , and uniformly over all state-action pairs $(s,a)$ . Any optimal policy $\\pi^{\\star}$ must be greedy with respect to the optimal $Q$ -function $Q^{\\star}$ . By classical dynamic programming, the optimal $Q$ -function $Q^{\\star}$ is obtained by setting $Q_{H}^{\\star}=r_{H}$ , and then recursively computing $Q_{h}^{\\star}=T_{h}^{\\star}\\,Q_{h+1}^{\\star}$ for $h=H-1,\\ldots,2,1$ , with the Bellman optimality operator defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n(T_{h}^{\\star}\\,f)(s,a):=r_{h}(s,a)+\\mathbb{E}_{h}\\big[\\operatorname*{max}_{a^{\\prime}\\in{\\cal A}}f(S^{\\prime},a^{\\prime})\\ |\\ s,a\\big]\\qquad\\mathrm{for~}S^{\\prime}\\sim\\mathcal{P}_{h}(\\cdot\\ |\\ s,a).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Value-based RL methods The main result of this paper applies to a broad class of methods for reinforcement learning. They are known as value-based, due to their reliance on the following two step approach for approximating an optimal policy $\\pi^{\\star}$ : (1) Construct an estimate $\\widehat{Q}=(\\widehat{f}_{1},\\ldots,\\widehat{f}_{H})$ of the optimal value function $\\boldsymbol{Q}^{\\star}=(Q_{1}^{\\star},\\ldots,Q_{H}^{\\star})$ . (2) Use $\\hat{Q}$ to compute the gre e dy-op ti mal po l icy $\\widehat{\\pi}_{h}(s)\\in\\arg\\operatorname*{max}_{a}\\widehat{f}_{h}(s,a)$ for $h=1,2,\\dots,H$ . It should be   noted that there is considerable freedom i n the design of a  v alue-based method, since different methods can be used to approximate value functions in Step 1. Rather than applying to a single method, our main result applies to a very broad class of these methods. ", "page_idx": 3}, {"type": "text", "text": "Underlying any value-based method is a class $\\mathcal{F}$ of functions $(s,a)\\mapsto f(s,a)$ used to approximate the state-action value functions.1 We assume that the function class $\\mathcal{F}$ is rich enough\u2014relative to the Bellman evaluation operators\u2014to ensure that for any greedy policy $\\pi$ induced by some $\\pmb{f}=(f_{1},\\dots,f_{H})\\in\\mathcal{F}^{H}$ , we have the inclusion $T_{h}^{\\pi}\\,\\mathcal{F}\\subseteq\\mathcal{F}$ for $h=1,\\ldots,H\\!-\\!1$ . We see that this condition depends on the structure of the transition distributions $\\mathcal{P}_{h}(\\cdot\\mid s,a)$ . In many practical examples, the reward function itself has some number of derivatives, and these transition distributions perform some type of smoothing, so that we expect that the output of the Bellman update, given a suitably differentiable function, will remain suitably differentiable. ", "page_idx": 3}, {"type": "text", "text": "2.2 Stable problems have fast rates ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now turn the central question in understanding the behavior of any value-based method: ", "page_idx": 3}, {"type": "text", "text": "At a high level, existing theory provides guarantees of the following type: if the $Q$ -function estimates are $\\varepsilon$ -accurate for some $\\varepsilon\\in(0,1)$ , then the value gap is bounded by a quantity proportional to $\\varepsilon$ In contrast, our main result shows that when the MDP is stable in a suitable sense, the value gap can be upper bounded by a quantity proportional to $\\varepsilon^{2}$ . This quadratic as opposed to linear scaling encapsulates the \u201cfast rate\u201d phenomenon of this paper. ", "page_idx": 3}, {"type": "text", "text": "Our analysis isolates two key stability properties required for faster rates; both are Lipschitz conditions with respect to a certain norm. Here we define them with respect to the $L^{2}$ -norm induced by the state-action occupation measure induced by the optimal policy\u2014namely ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f\\|_{h}:=\\sqrt{\\mathbb{E}_{\\pi^{\\star}}\\left[f^{2}(S_{h},A_{h})\\right]}\\qquad\\mathrm{for}\\;\\mathrm{any}\\;f\\in\\partial\\mathcal{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and over a neighborhood $\\mathcal{N}$ of the optimal $Q$ -value function $Q^{\\star}$ ", "page_idx": 4}, {"type": "text", "text": "Bellman stability: The first condition measures the stability of the Bellman optimality operator (2): in particular, we require that there is a scalar $\\kappa_{h}^{\\star}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathcal{T}_{h}^{\\star}\\,f_{h+1}-\\mathcal{T}_{h}^{\\star}\\,Q_{h+1}^{\\star}\\right\\|_{h}\\;\\le\\;\\kappa_{h}^{\\star}\\,\\left\\|\\,f_{h+1}-Q_{h+1}^{\\star}\\right\\|_{h+1}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any $\\pmb{f}\\in\\mathcal{N}$ . Moreover, for any pair $(h,h^{\\prime})$ of indices such that $1\\leq h<h^{\\prime}\\leq H-1$ , we define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa_{h,h^{\\prime}}(\\mathcal{T}^{\\star}):=\\kappa_{h}^{\\star}\\,\\kappa_{h+1}^{\\star}\\,.\\,.\\,.\\,\\kappa_{h^{\\prime}-1}^{\\star}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Condition $(\\mathbf{Stb}(\\mathcal{T}))$ is directly linked to the stability of estimating the $Q$ -function $Q^{\\star}$ . In typical estimation procedures, such as approximate dynamic programming, the estimation is carried out iteratively in a backward manner, so that it is important to control the propagation of estimation errors across the iterations. Condition $(\\mathbf{Stb}(\\mathcal{T}))$ captures this property, since it implies that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\big\\|\\,{\\mathcal T}_{h}^{\\star}\\,{\\mathcal T}_{h+1}^{\\star}\\,\\ldots\\,{\\mathcal T}_{h^{\\prime}-1}^{\\star}\\,f_{h^{\\prime}}-{\\mathcal T}_{h}^{\\star}\\,{\\mathcal T}_{h+1}^{\\star}\\,\\ldots\\,{\\mathcal T}_{h^{\\prime}-1}^{\\star}\\,Q_{h^{\\prime}}^{\\star}\\,\\big\\|_{h}\\ \\le\\ \\kappa_{h,h^{\\prime}}({\\mathcal T}^{\\star})\\cdot\\big\\|\\,f_{h^{\\prime}}-Q_{h^{\\prime}}^{\\star}\\,\\big\\|_{h^{\\prime}}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which shows how the estimation error $\\left(f_{h^{\\prime}}-Q_{h^{\\prime}}^{\\star}\\right)$ at step $h^{\\prime}$ can be controlled in terms of estimation error at an earlier time step $h\\le h^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "Occupation measure stability: Our second condition is more subtle, and is key in our argument. Let us begin with some intuition. Consider two sequences of policies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{\\big(\\pi_{1}^{\\star},\\ldots,\\pi_{h-1}^{\\star},\\pi_{h}^{\\star},\\pi_{h+1}^{\\star},\\ldots,\\pi_{h^{\\prime}}^{\\star}\\big)}&&{\\mathrm{{and}}}&&{\\big(\\pi_{1}^{\\star},\\ldots,\\pi_{h-1}^{\\star},\\pi_{h},\\pi_{h+1}^{\\star},\\ldots,\\pi_{h^{\\prime}}^{\\star}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that only differ at the $h$ -th step, where $\\pi_{h}^{\\star}$ has been replaced by $\\pi_{h}$ . These two policy sequences induce Markov chains whose distributions differ from stage $h$ onwards, and our second condition controls this difference in terms of the difference $\\|f_{h}-Q_{h}^{\\star}\\|_{h}$ between the two $Q$ -functions $f_{h}$ and $Q_{h}^{\\star}$ that induce $\\pi_{h}$ and $\\pi_{h}^{\\star}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "We adopt $\\mathcal{P}_{h}^{\\star}$ as a convenient shorthand for the transition operator $\\mathcal{P}_{h}^{\\pi^{\\star}}$ , and define the multi-step transition operator $\\mathcal{P}_{h,h^{\\prime}}^{\\star}:=\\mathcal{P}_{h}^{\\star}\\,\\mathcal{P}_{h+1}^{\\star}\\cdot\\cdot\\cdot\\mathcal{P}_{h^{\\prime}-1}^{\\star}$ . Using this notation, for any $h^{\\prime}\\geq h+1$ , we require that there is a scalar $\\kappa_{h,h^{\\prime}}(\\pi^{\\star})$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{g\\in\\mathcal{B}\\mathcal{F}}\\frac{\\left|\\mathbb{E}_{\\pi^{\\star}}\\!\\left[\\left(\\mathcal{P}_{h,h^{\\prime}}^{\\star}g\\right)\\left(S_{h},\\pi_{h}^{\\star}(S_{h})\\right)-\\left(\\mathcal{P}_{h,h^{\\prime}}^{\\star}g\\right)\\left(S_{h},\\pi_{h}(S_{h})\\right)\\right]\\right|}{\\|g\\|_{h^{\\prime}}}\\le\\kappa_{h,h^{\\prime}}(\\pi^{\\star})\\,\\frac{\\|f_{h}-Q_{h}^{\\star}\\|_{h}}{\\|Q_{h}^{\\star}\\|_{h}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any $\\pmb{f}\\in\\mathcal{N}$ . The renormalization in this definition serves to enforce a natural scale invariance. ", "page_idx": 4}, {"type": "text", "text": "With these notions of stability in hand, we are now equipped to state our main result. Taking as input a value function estimate $\\hat{Q}$ , it relates the induced value gap to the Bellman residuals $T_{h}^{\\star}\\,\\widehat{f_{h+1}}-\\widehat{f_{h}}$ . Note that these residuals a r e a way of quantifying proximity to the optimal value function $Q^{\\star}$ , whic h has Bellman residual zero by definition. We assume that $\\hat{Q}$ has Bellman residuals bounded as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\,\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}\\,\\right\\|_{h}\\ \\leq\\ \\varepsilon_{h}\\qquad\\mathrm{for}\\;h=1,2,\\ldots,H-1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some sequence $\\pmb{\\varepsilon}=(\\varepsilon_{1},\\dots,\\varepsilon_{H-1},\\varepsilon_{H}=0)$ ) that satisfies the constraint ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varepsilon_{h}\\geq{\\frac{1}{H-h}}\\sum_{h^{\\prime}=h+1}^{H}\\varepsilon_{h^{\\prime}}\\qquad{\\mathrm{for~}}h=1,2,\\ldots,H-1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This last condition means that the Bellman residual $\\varepsilon_{h}$ is larger than or equal to the average of the bounds established after step $h+1$ . It is natural because estimating at step $h$ is at least as challenging as a stage $h^{\\prime}>h$ ; indeed, any such state $h^{\\prime}$ occurs earlier in the dynamic programming backward iteration process. As a special case, the bound (4b) holds when $\\varepsilon_{h}=\\varepsilon$ for all stages. ", "page_idx": 4}, {"type": "text", "text": "With this set-up, we have the following guarantee in terms of the stability coefficients $\\kappa_{h,h^{\\prime}}(\\pi^{\\star})$ and $\\kappa_{h,h^{\\prime}}(\\mathcal{T}^{\\star})$ from conditions $\\mathbf{\\beta}(\\mathbf{Stb}(\\xi))$ and $(\\mathbf{Stb}(\\mathcal{T}))$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. There is a neighborhood of $Q^{\\star}$ such that for any value function estimate $\\hat{\\pmb f}$ with $\\varepsilon$ -bounded Bellman residuals (4a), the induced greedy policy $\\widehat{\\pi}$ has value gap bounded as ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\;\\le\\;2\\;\\sum_{h=1}^{H-1}\\;\\frac{1}{\\|Q_{h}^{\\star}\\|_{h}}\\,\\bigg\\{\\sum_{h^{\\prime}=h}^{H-1}\\,\\kappa_{h,h^{\\prime}}(\\pi^{\\star})\\;\\varepsilon_{h^{\\prime}}\\bigg\\}\\bigg\\{\\sum_{h^{\\prime}=h}^{H-1}\\kappa_{h,h^{\\prime}}(T^{\\star})\\;\\varepsilon_{h^{\\prime}}\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See Appendix A for the proof. ", "page_idx": 5}, {"type": "text", "text": "Treating dependence on the stability coefficients as constant, the main take-away is that value suboptimality is bounded above by a quantity proportional to the squared norm of the Bellman residuals. Concretely, if the Bellman residuals are uniformly upper bounded by some $\\varepsilon$ , then equation (5) leads to an upper bound of the form ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\leq c\\,H^{3}\\,\\varepsilon^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c$ is a universal constant. Due to the quad ratic scaling in the Bellman residual error $\\varepsilon$ , this bound is substantially tighter than the linear in $\\varepsilon$ rates afforded by a conventional analysis. ", "page_idx": 5}, {"type": "text", "text": "2.3 Intuition for fast rates: Smoothness and cancelling terms in the telescope bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Why does \u201cfast rate\u201d phenomenon formalized in Theorem 1 arise? The fast rates proved in this paper are established by a novel argument, starting from a known telescope bound, which we begin by stating. Given a $Q$ -function estimate $\\widehat{\\pmb f}\\,=\\,\\overline{{{\\left(\\widehat{f}_{1},...,\\widehat{f}_{H}\\right)}}}$ , let $\\widehat{\\pi}$ denote the induced greedy policy. Then the value gap of $\\widehat{\\pi}$ with respect t o  an arbi trary c o mparat or  policy $\\pi$ is bounded as ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\widehat\\pi)\\;\\leq\\;\\sum_{h=1}^{H-1}\\big(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat\\pi}\\big)\\big[\\big(\\mathcal{T}_{h}^{\\star}\\,\\widehat f_{h+1}-\\widehat f_{h}\\big)\\big(S_{h},A_{h}\\big)\\big]\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result follows by a \u201ctelescope\u201d relation induced by the structure of the Bellman updates.3 For completeness, we provide a proof of the telescope bound in Appendix E.2. ", "page_idx": 5}, {"type": "text", "text": "A key feature of inequality (6) is the difference of two expectations $\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat{\\pi}}$ , corresponding to the occupation measures under $\\pi$ versus $\\widehat{\\pi}$ . In standard uses of this inequality, an  initial argument is used to guarantee that one of these expect a tions is negative, and so can be dropped [20, 21]. ", "page_idx": 5}, {"type": "text", "text": "In contrast, the proof of our Theorem 1 exploits a more refined approach, one that handles the difference of expectations directly. Doing so can be beneficial\u2014and lead to \u201cfast rates\u201d\u2014 because various terms in this difference can cancel each other out. Specifically, under the smoothness conditions that underlie Theorem 1, when applying the telescope inequality (6) with comparator $\\pi=\\pi^{\\star}$ , we show that the discrepancy between the occupation measures associated with $\\pi^{\\star}$ and $\\widehat{\\pi}$ is of the same order as the Bellman residual associated with ${\\widehat{\\pmb f}}.$ . Note that the Bellman residuals o f $\\widehat{\\pmb f}$ already appear on the right-hand side of inequality (6), so th a t this fortuitous cancellation can be ex p loited\u2014along with a number of auxiliary results laid out in the proof\u2014so as to upper bound the value gap by a quantity proportional to the squared Bellman residual $\\varepsilon^{2}$ . ", "page_idx": 5}, {"type": "text", "text": "It is worthwhile making an explicit comparison of our cancellation approach with the more standard uses of the telescope relation, which typically consider only one portion of the Bellman residuals (e.g., [18, 20, 21, 19, 6, 12, 35]). We do so in the following two subsections. ", "page_idx": 5}, {"type": "text", "text": "2.3.1 Pessimism for off-line RL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the off-line instantiation of RL, the goal is to learn a \u201cgood\u201d policy based on a pre-collected dataset $\\mathcal{D}$ . Note that no further interaction with the environment is permitted, hence the notion of the learning being off-line. More precisely, an off-line dataset $\\mathcal{D}$ of size $n$ consists of quadruples ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}=\\left\\{\\left(s_{h,\\,i},a_{h,\\,i},s_{h,\\,i}^{\\prime},r_{h,\\,i}\\right)\\right\\}_{i=1}^{n},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $s_{h,\\,i}$ and $a_{h,\\,i}$ represent the $i$ -th state and action at the $h$ -th step in the MDP; $s_{h,\\,i}^{\\prime}$ is the successive state; and $r_{h,\\,i}=r_{h}(s_{h,\\,i},a_{h,\\,i})$ denotes the scalar reward. Note that while the successive states are defined by transition dynamics, and the rewards by the reward function, there are no restrictions on how the state-action pairs $(s_{h,\\,i},a_{h,\\,i})$ are collected. That is, they need not have been generated by any fixed policy, but may have collected from some ensemble of behavioral policies, or even adaptively by human experts. The goal of off-line reinforcement learning is to use the $n$ -sample dataset $\\mathcal{D}$ so as to estimate a policy ${\\widehat{\\pmb{\\pi}}}\\,\\equiv\\,{\\widehat{\\pmb{\\pi}}}_{n}$ that (approximately) maximizes the expected return $J(\\widehat{\\pmb{\\pi}}_{n})$ . We expect that\u2014at least for   a sens ible method for estimating ${\\widehat{\\pmb{\\pi}}}_{n}$ \u2014the value gap $J(\\pmb{\\pi}^{\\star})~-~J(\\widehat{\\pmb{\\pi}}_{n})$ should decay to zero as $n$ increases to infinity, and we are inte r ested in understanding this rate o f decay. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The use of pessimism is standard in off-line RL algorithms. Its purpose is to mitigate risks associated with \u201cpoor coverage\u201d of the off-line dataset. For instance, the naive approach of simply maximizing $Q$ -function estimates based on an off-line dataset can behave poorly when certain portions of the state-action space are not well covered by the given dataset. The pessimism principle suggests to form a conservative estimate of the value function\u2014say with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{f}_{h}(s,a)\\leq T_{h}^{\\star}\\,\\widehat{f}_{h+1}(s,a)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with high probability over state-action pairs $(s,a)$ . Thus, the estimated value $\\widehat{f}_{h}(s,a)$ is an underestimate of the Bellman update, a form of conservatism that protects against   unrealistically high estimates due to poor coverage. Doing so in the appropriate way ensures that ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\mathbb{E}_{\\widehat{\\pi}}\\left[\\big(\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}\\big)(S_{h},A_{h})\\right]\\le0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Applying this upper bound to the inequality (6) yields the sub-optimality bound ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\widehat\\pi)\\;\\leq\\;\\sum_{h=1}^{H-1}\\mathbb{E}_{\\pi}\\big[\\big(\\mathcal{T}_{h}^{\\star}\\,\\widehat f_{h+1}-\\widehat f_{h}\\big)(S_{h},A_{h})\\big]\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Upper bounds derived in this manner only contain one portion of the Bellman residual. When the value functions are approximated in a parametric way (e.g., tabular problems, linear\u221a function approximation), this line of analysis leads to value sub-optimality decaying at a \u201cslow\u201d $1/\\sqrt{n}$ rate in terms of the sample size $n$ (e.g., [21]). In contrast, an application of Theorem 1 can lead to value gaps bounded by $1/n$ . ", "page_idx": 6}, {"type": "text", "text": "2.3.2 Optimism in on-line RL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the setting of on-line RL, a learning agent interacts with the environment in a sequential manner, receiving feedback in the form of rewards based on its actions. At the beginning, the learner possesses no prior knowledge of the system\u2019s dynamics. In the $t$ -th episode, the agent learns an optimal policy\u03c0(t) using existing observations, implements the policy and collects data  s(ht ), a(ht ), r(ht)  hH=1 fro m  the new episode. In each round, the system starts at an initial state s(1t)independently drawn from a fixed distribution $\\xi_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "In this on-line setting, it is common to measure the performance of an algorithm by comparing it, over the $T$ rounds of learning, with an oracle that knows and implements an optimal policy. At each round $t$ , we incur the instantaneous regret $J(\\pmb{\\pi}^{\\star})-J(\\widehat{\\pmb{\\pi}}^{(t)})$ , where $\\pi^{\\star}$ is any optimal policy. Over $T$ rounds, we measure performance in terms of the cumu lative regret ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}\\bigl(\\bigl\\{\\widehat{\\pi}^{(t)}\\bigr\\}_{t=1}^{T}\\bigr):=\\operatorname*{max}_{\\mathrm{policy}}\\;\\sum_{t=1}^{T}\\left\\{J(\\pi)-J(\\widehat{\\pi}^{(t)})\\right\\}\\;=\\;\\sum_{t=1}^{T}\\underbrace{\\Bigl\\{J(\\pi^{\\star})-J(\\widehat{\\pi}^{(t)})\\Bigr\\}}_{\\mathrm{Regret~at~round}\\;t}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In a realistic problem, the cumulative regret of any procedure grows with $T$ , and our goal is to obtain algorithms whose regret grows as slowly as possible. ", "page_idx": 6}, {"type": "text", "text": "In contrast to off-line RL, the on-line setting allows for exploring state-action pairs that have been rarely encountered; doing so makes sense since they might be associated with high rewards. Principled exploration of this type can be effected via the optimism principle: one constructs function estimates such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{f}_{h}(s,a)\\geq T_{h}^{\\star}\\,\\widehat{f}_{h+1}(s,a)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with high probability over state-action pairs.4 Note that $\\widehat{f}_{h}(s,a)$ is optimistic in the sense that it is an over-estimate of the Bellman update $\\bar{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}(s,a)$ . In  t his way, we can ensure that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi}\\big[\\big(\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}\\big)\\big(S_{h},A_{h}\\big)\\big]\\le0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Combining this inequality with the telescope bound (6) allows one to upper bound the regret as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{R e g r e t}\\big(\\{\\widehat{\\pi}^{(t)}\\}_{t=1}^{T}\\big)\\;=\\;\\sum_{t=1}^{T}\\big\\{J(\\pi^{\\star})-J(\\widehat{\\pi}^{(t)})\\big\\}\\;\\leq\\;\\sum_{t=1}^{T}\\sum_{h=1}^{H-1}\\mathbb{E}_{\\widehat{\\pi}^{(t)}}\\big[\\big(\\widehat{f}_{h}-\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}\\big)(S_{h},A_{h})\\big]\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which only includes a single portion of the Bellman residua\u221al. In the case of tabular or linear representations of the $Q$ -functions, it results in a regret rate of $\\sqrt{T}$ (e.g., see the papers [18, 20]). In contrast, an appropriate use of Theorem 1 leads to regret growing only as $\\log(T)$ , which corresponds to a much better guarantee. ", "page_idx": 7}, {"type": "text", "text": "In summary, then, the fast rates obtained in this paper are based on a different approach than the standard pessimism or optimism principles. Since we deal directly with the difference of expectations in the bound (6), there is no need to nullify either of them through the use of these principles. However, it should be noted that we are assuming smoothness conditions that allow us to control this difference. As we discuss in the sequel, such smoothness conditions rule out certain \u201chard instances\u201d used in past work on lower bounds (e.g. [18, 20, 21, 37]). ", "page_idx": 7}, {"type": "text", "text": "3 Consequences for linear function approximation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we explore some consequences of our general theory when applied to value-based methods using (finite-dimensional) linear function approximation. ", "page_idx": 7}, {"type": "text", "text": "Let $\\phi:S\\times A\\rightarrow\\mathbb{R}^{d}$ be a given feature map on the state-action space, and consider linear expansions of the form $\\begin{array}{r}{f_{\\pmb{w}}(s,a)=\\langle\\phi(s,a),\\,\\pmb{w}\\rangle\\equiv\\sum_{j=1}^{d}w_{j}\\phi_{j}(s,a)}\\end{array}$ where $\\pmb{w}\\in\\mathbb{R}^{d}$ is a weight vector. We adopt the conventional assumption that $\\|\\phi(s,a)\\|_{2}\\leq1$ and $r_{h}(s,a)\\in[0,1]$ for all state-action pairs. Defining the linear function class $\\mathcal{F}\\!:=\\!\\left\\{f_{\\pmb{w}}\\mid\\pmb{w}\\in\\mathbb{R}^{d}\\right\\}$ , we note that the Minkowski difference class $\\partial\\mathcal{F}$ is equal to $\\mathcal{F}$ . ", "page_idx": 7}, {"type": "text", "text": "In our analysis of linear approximation, we make use of the norm $\\|f\\|_{h}:=\\sqrt{\\mathbb{E}_{\\pmb{\\pi}^{\\star}}[f^{2}(S_{h},A_{h})]}$ , corresponding to $L^{2}$ -norm under the occupation measure induced by the optimal policy $\\pi^{\\star}$ . ", "page_idx": 7}, {"type": "text", "text": "3.1 Consequences for off-line RL ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now turn to some implications of Theorem 1 for off-line reinforcement learning. Let us recall the off-line setting: for each $h=1,\\ldots,H-1$ , we are given a dataset $\\mathcal{D}_{h}=\\{(s_{h,i},\\bar{a_{h,i}},s_{h,i}^{\\prime},r_{h,i})\\}_{i=1}^{n}$ of quadruples, from which we can compute estimates $\\widehat{\\pmb{f}}=(\\widehat{f}_{h})_{h=1}^{H}$ with certain Bellman residuals $\\{\\varepsilon_{h}\\}_{h=1}^{H-1}$ , which then appear in the bound (5). The  r emain i ng factors on the right-hand side of inequality (5) do not depend on the dataset itself (but rather on structural properties of the MDP). Consequently, in terms of statistical understanding, the main challenge is to establish high-probability bounds on the Bellman residuals {\u03b5h}hH=\u221211 for a particular estimator. ", "page_idx": 7}, {"type": "text", "text": "3.1.1 Fitted $Q$ -iteration (FQI) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As an illustration, let us analyze the use of fitted $Q$ -iteration (FQI) for computing estimates of the $Q$ -function. At a given stage $h\\,=\\,1,\\ldots,H\\,-\\,1$ , we can use the associated data $\\mathcal{D}_{h}$ to define a regularized objective function ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{h}\\big(f,g\\big):=\\frac{1}{|\\mathcal{D}_{h}|}\\bigg[\\sum_{(s_{h,i},a_{h,i},s_{h,i}^{\\prime},r_{h,i})\\in\\mathcal{D}_{h}}\\left\\{f\\big(s_{h,i},\\,a_{h,i}\\big)-\\big(r_{h,i}+\\operatorname*{max}_{a\\in\\mathcal{A}}g\\big(s_{h,i}^{\\prime},a\\big)\\big)\\right\\}^{2}\\bigg]+\\Lambda_{h}^{2}(f)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here $g$ represents the target function from stage $h+1$ , and it defines the targeted responses $\\begin{array}{r}{y_{h,i}(g):=r_{h,i}+\\operatorname*{max}_{a\\in\\bar{\\mathcal{A}}}\\bar{g}(s_{h,i}^{\\prime},a)}\\end{array}$ . For a given target $g$ , we obtain a $Q$ -function estimate for stage $h$ by minimizing the functional $f\\mapsto\\mathcal{L}_{h}(f,g)$ . Given that our objective is defined with a quadratic cost, doing so can be understood as a regression method for estimating the conditional expectation that underlies the Bellman update\u2014viz. $T_{h}^{\\star}\\,g(s,a)=\\mathbb{E}[\\,y_{h,\\,i}(g)\\mid(s_{h,\\,i}^{\\bullet},\\,a_{h,\\,i})=(s,a)\\,]$ . Here $\\Lambda_{h}^{2}(f)\\,=\\,\\lambda_{h}\\,\\|{\\pmb w}\\|_{2}^{2}$ for $f=\\langle\\phi(\\cdot),\\,w\\rangle$ is a regularizer, with $\\lambda_{h}~\\geq~0$ being the regularization weight. Given this set-up, we can generate a $Q$ -function estimate $\\widehat{\\pmb f}~=~(\\widehat{f}_{1},\\ldots,\\widehat{f}_{H})$ by first initializing $\\widehat{f}_{H}\\,=\\,r_{H}$ , and then recursively computing $\\begin{array}{r}{\\widehat{f}_{h}\\,=\\,\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathcal{L}_{h}\\big(f,\\,\\widehat{f}_{h+1}\\big)}\\end{array}$ , for $h=H-1,H-2,\\ldots,2,1.$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.1.2 Fast rates for FQI-based estimates ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the analysis here, we assume that the dataset consists of i.i.d. tuples (but this can be relaxed as needed). We now state a corollary of Theorem 1, applicable to value function estimates based on FQI with ridge regression. ", "page_idx": 8}, {"type": "text", "text": "Corollary 1 (Fast rates for ridge-based FQI). For FQI based on ridge regression, with a sufficiently large sample size n and with suitable choices of the regularization parameters $\\{\\lambda_{h}\\}_{h=1}^{H-1}$ , the bound (5) from Theorem 1 holds with ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\varepsilon_{h}=c\\,{\\sqrt{\\{d(H-h)/n\\}\\,\\log(d H/\\delta)}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "We omit the proof of Corollary 1, as it follows from standard ridge regression analysis. ", "page_idx": 8}, {"type": "text", "text": "Fast rates and comparisons to past work: So as to be able to compare with results from past work, let us consider some consequences \u221aof the bound (10) under the following assumptions: (i) $\\kappa_{h,h^{\\prime}}(\\mathcal{T}^{\\star})=\\mathcal{O}(1)$ ; (ii) $\\kappa_{h,h^{\\prime}}(\\pi^{\\star})=\\mathcal{O}(\\sqrt{d})$ ; (iii) $\\|Q_{h}^{\\star}\\|_{h}\\asymp H-h+1$ . Then it can be shown that the bound from Corollary 1 takes the form ", "page_idx": 8}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\;\\leq\\;c\\,d^{3/2}\\,H^{3}\\,n^{-1}\\,\\log(d H/\\delta),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and is valid for a sample size $n\\,\\geq\\,c d^{2}H^{3}$ . Alternatively stated, Corollary 1 guarantees that for FQI using ridge regression with $d$ -dimensional function approximation, the number of samples $n(\\epsilon)$ required to obtain $\\epsilon$ -optimal policy is at most ", "page_idx": 8}, {"type": "equation", "text": "$$\nn_{\\mathrm{fast}}(\\epsilon)\\asymp d^{\\frac{3}{2}}H^{3}/\\epsilon+d^{2}H^{3},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where we use $\\asymp$ to denote a scaling that ignores constants and logarithmic factors. ", "page_idx": 8}, {"type": "text", "text": "Let us compare this guarantee to related work by Zanette et al. [37], who analyzed the use of pessimistic actor-critic methods for linear function classes. When translated into the notation of our paper, their analysis established5 a sample complexity of the order $n_{\\mathrm{Zan}}(\\epsilon)\\asymp d^{2}H^{3}/\\epsilon^{2}$ . Consequently, we see that once the target error $\\epsilon$ is relatively small\u2014 $\\epsilon\\in(0,1)$ \u2014then stable MDPs can exhibit a much smaller $(1/\\epsilon)$ sample complexity. ", "page_idx": 8}, {"type": "text", "text": "It should be noted that past work (e.g., [21, 37]) has established $(1/\\epsilon^{2})$ -lower bounds on the sample complexity of estimating $\\epsilon$ policies in the off-line setting. However, these lower bounds do not contradict our fast rate guarantee (11b), because the \u201chard instances\u201d used in these lower bound proofs violate the stability condition $\\mathbf{\\beta}(\\mathbf{Stb}(\\xi))$ . In particular, even infinitessimally small perturbations in policy lead to occupation measures that are significantly different. ", "page_idx": 8}, {"type": "text", "text": "When is pessimism necessary? An interesting aspect of the guarantee from Corollary 1 is that it provides guarantees for off-policy RL (and with fast rates) using a method that does not incorporate any form of pessimism. This is a sharp contrast with many other methods for off-policy RL, such as pessimistic forms of $Q$ -learning and actor-critic methods (e.g., [21, 37]). ", "page_idx": 8}, {"type": "text", "text": "To be clear, as noted following the bound (11a), the guarantee from Corollary 1 requires the sample size to be lower bounded as $n\\bar{\\geq}c d^{2}H^{3}$ . In contrast, pessimistic schemes only require a sample size sufficiently large to ensure validity of the Bellman residual upper bounds that underlie Corollary 1\u2014 meaning that $n\\gtrsim d$ up to logarithmic factors. Thus, the pessimism principle can be useful for problems with smaller sample sizes. ", "page_idx": 8}, {"type": "text", "text": "3.2 Consequences for on-line RL ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we explore some consequences of Theorem 1 for on-line reinforcement learning. We begin by describing a two-stage procedure6 that allows us to convert the risk bounds for FQI from off-line RL into regret in on-line RL: ", "page_idx": 9}, {"type": "text", "text": "Phase 1 (Exploration) In the initial $T_{0}\\mathbf{f}$ episodes, the focus is purely on exploration, resulting in an estimate of $Q$ -function denoted as $\\widehat{\\pmb f}^{(T_{0})}$ . ", "page_idx": 9}, {"type": "text", "text": "Phase 2 (Fine-tuning) For $k=0,1,\\ldots,K-1$ with $K\\!:=\\!\\lceil\\log_{2}(T/T_{0})\\rceil$ , repeat: \u2022 In the $t$ -th episode, for each $t=T_{0}\\,2^{k}+1,\\dots,T_{0}\\,2^{k+1}$ , execute the greedy policy induced by functionf (T0 2 ). \u2022 Update the $Q$ -functi o n estimate $\\widehat{f}^{(T_{0}\\;2^{k+1})}$ using FQI based on observations collected from episodes $T_{0}\\,2^{k}+1,T_{0}\\,2^{k}+2,\\ldots,T_{0}\\,2^{k+1}$ . ", "page_idx": 9}, {"type": "text", "text": "We assume the burn-in time $T_{0}$ is large enough so as to ensure the pilot $Q$ -function estimate $\\widehat{\\pmb f}^{(T_{0})}$ obtained in Phase 1 falls within a certain \u201cabsorbing\u201d region $\\mathcal{N}(\\pmb{\\rho})$ around $Q^{\\star}$ . Under these condi ti ons, we have the following bound on the regret. ", "page_idx": 9}, {"type": "text", "text": "Corollary 2. For FQI based on ridge regression with rewards in [0, 1], with a sufficiently large burn-in time $T_{0}$ and with suitable choices of the regularization parameters $\\{\\lambda_{h}\\}_{h=1}^{H-1}$ , the two-phase scheme achieves regret bounded as ", "page_idx": 9}, {"type": "equation", "text": "$$\nR e g r e t(T)\\ \\leq\\ c\\left\\{T_{0}\\cdot H\\ +\\ d\\sqrt{d}\\ H^{4}\\,\\log T\\,\\cdot\\,\\log(d H K/\\delta)\\right\\}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "with probability at least $1-\\delta$ .   \nSee Appendix C.1 for the proof. ", "page_idx": 9}, {"type": "text", "text": "Sharper bound on regret: The leadi\u221ang term (as $T$ grows) in the regret bound\u221a grows as $\\log T$ , which is much smaller than the typical $\\sqrt{T}$ -rate found in past work [18, 20]. The $\\sqrt{T}$ rate has been shown to be unimprovable in general, but the worst-case instances[18, 20] that lead to $\\sqrt{T}$ -regret violate the stability conditions used in our analysis. ", "page_idx": 9}, {"type": "text", "text": "When is optimism needed? The use of optimism\u2014by adding bonuses to the current value function estimates so as to encourage exploration\u2014underlies many schemes in on-line RL. An interesting take-away from Corollary 2 is that under the stability conditions highlighted by our theory, it is possible to achieve excellent regret bounds without the use of optimism. In our two-phase scheme, the only exploration occurs in Phase 1. All other data is simply collected using the greedy policy induced by the current $Q$ -function estimate. A well-designed exploration scheme\u2014one that might incorporate the optimism principle\u2014is necessary only during the burn-in Phase 1. ", "page_idx": 9}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces a novel approach for the analysis of value-based RL methods for continuous state-action spaces. Our analysis highlights two key stability properties of MDPs under which much sharper bounds on value sub-optimality can be guaranteed. Our analysis offers fresh perspectives on the commonly used pessimism and optimism principles, in off-line and on-line settings respectively. ", "page_idx": 9}, {"type": "text", "text": "Our study leaves open various questions for future work. First, our main result (Theorem 1) has consequences for linear quadratic control, to be described in an upcoming paper [8]. It provides insight into the role of covariate shift in linear quadratic control, as well as efficient exploration in the on-line setting. Second, our current statistical analysis focused on i.i.d. data with linear function approximation. It is interesting to consider the extensions to dependent data and non-parametric function approximation (e.g. kernels, boosting, and neural networks). Third, while this paper has provided upper bounds, it remains to address the complementary question of lower bounds for policy optimization over the classes of stable MDPs isolated here. Last, to better align our framework with real-world scenarios, we intend to go beyond the idealized completeness condition used in this paper, and treat the role of model mis-specification. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by NSF grant CCF-1955450, ONR grant N00014-21-1-2842, and NSF DMS-2311072 to MJW. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Altamimi, C. Lagoa, J. G. Borges, M. E. McDill, C. Andriotis, and K. Papakonstantinou. Large-scale wildfire mitigation through deep reinforcement learning. Frontiers in Forests and Global Change, 5:734330, 2022. [2] H. Bastani, M. Bayati, and K. Khosravi. Mostly exploration-free algorithms for contextual bandits. Management Science, 67(3):1329\u20131349, 2021. [3] D. Bertsekas. Lessons from AlphaZero for optimal, model predictive, and adaptive control. Athena Scientific, 2022. [4] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic regulator. Foundations of Computational Mathematics, 20(4):633\u2013679, 2020. [5] J. Degrave, F. Felici, and J. B. et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602:414\u2013419, 2022. [6] S. Du, S. Kakade, J. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in RL. In International Conference on Machine Learning, pages 2826\u20132836. PMLR, 2021.   \n[7] Y. Duan, C. Jin, and Z. Li. Risk bounds and Rademacher complexity in batch reinforcement learning. In International Conference on Machine Learning, pages 2892\u20132902. PMLR, 2021.   \n[8] Y. Duan and M. J. Wainwright. Covariate shift in linear quadratic control. Manuscript. [9] Y. Duan and M. J. Wainwright. Policy evaluation from a single path: Multi-step methods, mixing and mis-specification. arXiv preprint arXiv:2211.03899, 2022.   \n[10] Y. Duan and M. Wang. Minimax-optimal off-policy evaluation with linear function approximation. In International Conference on Machine Learning, pages 2701\u20132709. PMLR, 2020.   \n[11] Y. Duan, M. Wang, and M. J. Wainwright. Optimal policy evaluation using kernel-based temporal difference methods. arXiv preprint arXiv:2109.12002, 2021.   \n[12] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[13] J. Gijsbrechts, R. N. Boute, J. A. Van Mieghem, and D. J. Zhang. Can deep reinforcement learning improve inventory management? Performance on lost sales, dual-sourcing, and multiechelon problems. Manufacturing & Service Operations Management, 24(3):1349\u20131368, 2022.   \n[14] A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. In Conference on Learning Theory, pages 1043\u20131063. PMLR, 2017.   \n[15] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69:169\u2013192, 2007.   \n[16] J. He, D. Zhou, and Q. Gu. Logarithmic regret for reinforcement learning with linear function approximation. In International Conference on Machine Learning, pages 4171\u20134180. PMLR, 2021.   \n[17] Y. Hu, N. Kallus, and M. Uehara. Fast rates for the regret of offline reinforcement learning. Conference on Learning Theory, 134:2462\u20132462, 2021.   \n[18] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan. Is Q-learning provably efficient? Advances in neural information processing systems, 31, 2018.   \n[19] C. Jin, Q. Liu, and S. Miryoosef.i Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. Advances in neural information processing systems, 34:13406\u2013 13418, 2021.   \n[20] C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u20132143. PMLR, 2020.   \n[21] Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? International Conference on Machine Learning, pages 5084\u20135096, 2021.   \n[22] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[23] T. Koren and K. Levy. Fast rates for exp-concave empirical risk minimization. Advances in Neural Information Processing Systems, 28, 2015.   \n[24] H. Mania, S. Tu, and B. Recht. Certainty equivalence is efficient for linear quadratic control. Advances in Neural Information Processing Systems, 32, 2019.   \n[25] T. Nguyen-Tang, M. Yin, S. Gupta, S. Venkatesh, and R. Arora. On instance-dependent bounds for offline reinforcement learning with linear function approximation. Association for the Advancement of Artificial Intelligence, 2023.   \n[26] M. L. Puterman and S. L. Brumelle. On the convergence of policy iteration in stationary dynamic programming. Mathematics of Operations Research, 4(1):60\u201369, 1979.   \n[27] A. Rao and T. Jelvis. Foundations of Reinforcement Learning with Applications to Finance. CRC Press, Boca Raton, FL, 2022.   \n[28] P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021.   \n[29] B. Recht. A tour of reinforcement learning: The view from continuous control. Annual Review of Control, Robotics, and Autonomous Systems, 2:253\u2013279, 2019.   \n[30] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017.   \n[31] S. Spielberg, A. Tulsyan, N. P. Lawrence, P. D. Loewen, and R. B. Gopaluni. Toward self-driving processes: A deep reinforcement learning approach to control. Amer. Inst. Chem. Eng. Journal, 65:e16689, 2022.   \n[32] L. Tai, G. Paolo, and M. Liu. Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 31\u201336. IEEE, 2017.   \n[33] X. Wang, Q. Cui, and S. S. Du. On gap-dependent bounds for offline reinforcement learning. Advances in Neural Information Processing Systems, 35:14865\u201314877, 2022.   \n[34] T. Xie and N. Jiang. $Q^{*}$ approximation schemes for batch reinforcement learning: A theoretical comparison. In Conference on Uncertainty in Artificial Intelligence, pages 550\u2013559. PMLR, 2020.   \n[35] M. Yin, Y. Duan, M. Wang, and Y.-X. Wang. Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. arXiv preprint arXiv:2203.05804, 2022.   \n[36] C. Yu, J. Liu, S. Nemati, and G. Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1\u201336, 2021.   \n[37] A. Zanette, M. J. Wainwright, and E. Brunskill. Provable benefits of actor-critic methods for offilne reinforcement learning. Advances in neural information processing systems, 34:13626\u2013 13640, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section is devoted to the proof of Theorem 1, which consists of three main steps. These steps rely on two auxiliary lemmas whose proofs are fairly technical, so that they are deferred to in Appendices B.1 and B.2. ", "page_idx": 12}, {"type": "text", "text": "High-level outline: Let us outline the three steps of the proof. In Step 1, we use a one-step expansion of the difference in the occupation measures to reformulate the standard telescope inequality (6). Doing so results in a relation with structure similar to that of the left-hand side of inequality $\\mathbf{\\beta}(\\mathbf{Stb}(\\xi))$ . In Step 2, we develop a constraint on the function estimation error $d_{h}(\\widehat{f_{h}},Q_{h}^{\\star})$ that ensures the occupation measure produced by policy $\\widehat{\\pi}$ remains stable and does not de viate too much from the occupation measure associated with the   optimal policy $\\pi^{\\star}$ . In Step 3, we use Bellman stability $(\\mathbf{Stb}(\\mathcal{T}))$ to connect the $Q$ -function error $\\overrightharpoon{f_{h}}-Q_{h}^{\\star}$ with Bellman residuals. With this high-level view in place, we now work through the three  s teps. ", "page_idx": 12}, {"type": "text", "text": "A.1 Step 1: Reformulation of the telescope inequality. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Recall the standard telescope inequality (6). Our proof makes use of an alternative form, which involves the functions ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Delta_{h}(\\pi;\\,s,a\\,)\\,=\\,\\sum_{h^{\\prime}=h}^{H-1}\\,\\mathcal{P}_{h,h^{\\prime}}^{\\pi}\\bigl(\\mathcal{T}_{h^{\\prime}}^{\\star}\\,\\widehat f_{h^{\\prime}+1}-\\widehat f_{h^{\\prime}}\\bigr)(s,a).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 1. Given a $Q$ -function estimate $\\widehat{\\pmb f}=\\left(\\widehat{f}_{1},\\ldots,\\widehat{f}_{H-1},\\widehat{f}_{H}=r_{H}\\right)$ and the associated greedy policy $\\widehat{\\pi}$ , we have the bound ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\widehat\\pi)\\;\\leq\\;\\sum_{h=1}^{H-1}\\mathbb{E}_{\\widehat\\pi}\\left[\\Delta_{h}(\\pi;\\,s_{h},\\pi_{h}(s_{h}))-\\Delta_{h}(\\pi;\\,s_{h},\\widehat\\pi_{h}(s_{h}))\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "valid for any policy $\\pi$ . ", "page_idx": 12}, {"type": "text", "text": "See Appendix B.1 for the proof. ", "page_idx": 12}, {"type": "text", "text": "We apply the bound (13) with $\\pi=\\pi^{\\star}$ . Following some algebra, we find that ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\;\\leq\\;\\sum_{h=1}^{H-1}\\sum_{h^{\\prime}=h}^{H-1}\\;\\widehat{\\beta}(h,\\,h^{\\prime})\\:\\cdot\\:\\varepsilon_{h^{\\prime}}\\;,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\varepsilon_{h^{\\prime}}$ is an upper bound on the Bellman residual $\\left\\|T_{h^{\\prime}}^{\\star}\\,\\widehat{f}_{h^{\\prime}+1}-\\widehat{f}_{h^{\\prime}}\\right\\|_{h^{\\prime}}$ as given in equation (4a). The term $\\widehat{\\beta}(h,\\,h^{\\prime})$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{\\beta}(h,h^{\\prime}):=\\operatorname*{sup}_{f\\in\\partial\\mathcal{F}:\\|f\\|_{h^{\\prime}}>0}\\,\\left\\{\\frac{1}{\\|f\\|_{h^{\\prime}}}\\Big|\\mathrm{E}_{\\widehat{\\pi}}\\Big[\\big(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big)\\big(s_{h},\\pi_{h}^{\\star}(s_{h})\\big)-\\big(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big)\\big(s_{h},\\widehat{\\pi}_{h}(s_{h})\\big)\\Big]\\Big|\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We note that the left-hand side of inequality $\\mathbf{\\beta}(\\mathbf{Stb}(\\xi))$ has a similar form to the term $\\widehat{\\beta}(h,\\,h^{\\prime})$ , differing only in that the expectation is taken over the occupation measure of running the  o ptimal policy $\\pi^{\\star}$ , rather than the estimated policy $\\widehat{\\pi}$ . ", "page_idx": 12}, {"type": "text", "text": "A.2 Step 2: Constraint to ensure stability ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our next step is to establish an upper bound on the coefficient $\\widehat{\\beta}(h,h^{\\prime})$ defined by the estimated policy $\\widehat{\\pi}$ in terms of the analogous quantity defined by the optimal  p olicy $\\pi^{\\star}$ \u2014namely, the coefficient ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\beta(h,h^{\\prime}):=\\operatorname*{sup}_{\\substack{f\\in\\partial\\mathcal{F}\\colon\\|f\\|_{h^{\\prime}}>0}}\\,\\left\\{\\frac{1}{\\|f\\|_{h^{\\prime}}}\\Big|\\mathbb{E}_{\\pi^{\\star}}\\Big[\\big(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big)\\big(s_{h},\\pi_{h}^{\\star}(s_{h})\\big)-\\big(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big)\\big(s_{h},\\widehat{\\pi}_{h}(s_{h})\\big)\\Big]\\Big|\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In order to do so, we demonstrate that a sufficiently small function estimation error $d_{h}(\\widehat{f}_{h},Q_{h}^{\\star})$ ensures the inequality ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H-1}\\sum_{h^{\\prime}=h}^{H-1}\\;\\widehat\\beta(h,\\,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}\\;\\leq\\;2\\;\\sum_{h=1}^{H-1}\\sum_{h^{\\prime}=h}^{H-1}\\;\\beta(h,\\,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Once we have established this bound, we can replace the term $\\beta(h,h^{\\prime})$ with $\\kappa_{h,h^{\\prime}}(\\pmb{\\pi}^{\\star})\\cdot\\|\\widehat{f}_{h}\\!-\\!Q_{h}^{\\star}\\|_{h}/\\|Q_{h}^{\\star}\\|_{h}$ , using the inequality $(\\mathbf{Stb}(\\xi))$ . ", "page_idx": 13}, {"type": "text", "text": "We summarize the result in the following auxiliary lemma: ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Suppose that the function estimation errors satisfy $\\begin{array}{r}{d_{h}\\big(Q_{h},\\,Q_{h}^{\\star}\\big)\\leq\\frac{1}{2\\,b_{\\mathcal{F}}}\\,(H-h+1)^{-1}}\\end{array}$ for $h=2,3,\\ldots,H-1$ and the sequence $\\pmb{\\varepsilon}\\;=\\;(\\varepsilon_{1},\\dots,\\varepsilon_{H-1},\\varepsilon_{H}\\;=\\;0)$ ) satisfies the regularity condition (4b). Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\;\\leq\\;2\\,\\sum_{h=1}^{H-1}\\frac{\\|\\widehat f_{h}-Q_{h}^{\\star}\\|_{h}}{\\|Q_{h}^{\\star}\\|_{h}}\\,\\Big\\{\\sum_{h^{\\prime}=h}^{H-1}\\kappa_{h,h^{\\prime}}(\\pi^{\\star})\\;\\varepsilon_{h^{\\prime}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "See Appendix B.2 for the proof. ", "page_idx": 13}, {"type": "text", "text": "A.3 Step 3: Connecting $Q$ -function error and Bellman residuals ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The remaining piece of the proof is to connect the function difference $\\widehat{f_{h}}-Q_{h}^{\\star}$ with Bellman residuals $T_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}$ , using the stability condition $(\\mathbf{Stb}(\\mathcal{T}))$ on the Bellman   operator $\\tau^{\\star}$ . This is relatively stra i ghtforwa rd: indeed, we claim that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\big\\|\\widehat{f}_{h}-Q_{h}^{\\star}\\big\\|_{h}\\ \\leq\\ \\sum_{h^{\\prime}=h}^{H-1}\\,\\kappa_{h,h^{\\prime}}({\\mathcal T}^{\\star})\\cdot\\big\\|{\\mathcal T}_{h^{\\prime}}^{\\star}\\,\\widehat{f}_{h^{\\prime}+1}-\\widehat{f}_{h^{\\prime}}\\big\\|_{h^{\\prime}}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that $Q_{h}^{\\star}=T_{h}^{\\star}\\,Q_{h+1}^{\\star}$ for $h=1,2,\\dots,H-1$ . Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widehat{f}_{h}-Q_{h}^{\\star}=\\left(\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\mathcal{T}_{h}^{\\star}\\,Q_{h+1}^{\\star}\\right)-\\left(\\mathcal{T}_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By employing the triangle inequality and the Bellman stability given in equation $(\\mathbf{Stb}(\\mathcal{T}))$ , we derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{f}_{h}-Q_{h}^{\\star}\\right\\|_{h}\\leq\\left\\|\\mathcal{T}_{h}^{\\star}\\widehat{f}_{h+1}-\\widehat{f}_{h}\\right\\|_{h}+\\left\\|\\mathcal{T}_{h}^{\\star}\\widehat{f}_{h+1}-\\mathcal{T}_{h}^{\\star}Q_{h+1}^{\\star}\\right\\|_{h}}\\\\ {\\leq\\left\\|\\mathcal{T}_{h}^{\\star}\\widehat{f}_{h+1}-\\widehat{f}_{h}\\right\\|_{h}+\\kappa_{h}^{\\star}\\left\\|\\widehat{f}_{h+1}-Q_{h+1}^{\\star}\\right\\|_{h+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Applying this inequality recursively yields the claim (17). ", "page_idx": 13}, {"type": "text", "text": "With this piece in place, we can complete the proof of Theorem 1. Indeed, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{J(\\pi^{\\star})-J(\\widehat{\\pi})\\overset{(a)}{\\leq}\\;2\\sum_{h=1}^{H-1}\\frac{\\Vert\\widehat{f}_{h}-Q_{h}^{\\star}\\Vert_{h}}{\\Vert Q_{h}^{\\star}\\Vert_{h}}\\left\\{\\sum_{h^{\\prime}=h}^{H-1}\\kappa_{h,h^{\\prime}}(\\pi^{\\star})\\;\\varepsilon_{h^{\\prime}}\\right\\}}}\\\\ &{\\overset{(b)}{\\leq}\\;2\\sum_{h=1}^{H-1}\\frac{1}{\\Vert Q_{h}^{\\star}\\Vert_{h}}\\left\\{\\sum_{h^{\\prime}=h}^{H-1}\\;\\kappa_{h,h^{\\prime}}(\\mathcal{T}^{\\star})\\;\\varepsilon_{h^{\\prime}}\\right\\}\\Big\\{\\sum_{h^{\\prime}=h}^{H-1}\\kappa_{h,h^{\\prime}}(\\pi^{\\star})\\;\\varepsilon_{h^{\\prime}}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here step (a) is a restatement of the bound (16) from Lemma 2, whereas step (b) follows from inequality (17). Thus, we have established the claim given in Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "B Proof of auxiliary lemmas for Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We now turn to proofs of the two auxiliary results used to establish our main theorem, with Lemmas 1 and 2 treated in Appendices B.1 and B.2, respectively. ", "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For any integrable vector function $\\pmb{g}=(g_{1},\\dots,g_{H})\\in\\mathbb{R}^{S\\times A\\times H}$ , we define ", "page_idx": 14}, {"type": "equation", "text": "$$\nD(g)\\;=\\;\\sum_{h=1}^{H}\\left(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat{\\pi}}\\right)\\left[g_{h}(S_{h},A_{h})\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We claim that this functional satisfies the recursive relation ", "page_idx": 14}, {"type": "equation", "text": "$$\nD(g)=\\sum_{h=1}^{H}\\mathbb{E}_{\\widehat\\pi}\\big[g_{h}(S_{h},\\pi_{h}(S_{h}))-g_{h}(S_{h},\\widehat\\pi_{h}(S_{h}))\\big]+D(\\mathcal{P}^{\\pi}g),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have introduced the shorthand $\\pmb{\\mathscr{P}}^{\\pi}g:=\\left(\\mathcal{P}_{1}^{\\pi}\\,g_{2},\\ldots,\\mathcal{P}_{H-1}^{\\pi}\\,g_{H},0\\right)\\in\\mathbb{R}^{S\\times A\\times H}.$ ", "page_idx": 14}, {"type": "text", "text": "Taking this claim as given for the moment, let us prove the bound (13) from Lemma 1. First, we set $\\textit{\\textbf{g}}:$ $\\mathbf{\\Psi}=(\\mathcal{\\bar{P}}^{\\mathbf{\\bar{\\pi}}})^{h}\\,g=\\left(\\mathcal{P}_{1,1+h}^{\\pi^{-}}\\,g_{1+h},\\ldots,\\mathcal{P}_{H-h,H}^{\\pi}\\,g_{H},\\mathit{\\dot{0}},\\ldots,0\\right)$ in equation (18b) for $h=0,1,\\ldots,H\\!-\\!1$ , which yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\big((\\mathcal{P}^{\\pi})^{h}\\,g\\big)=\\displaystyle\\sum_{1\\leq h^{\\prime}\\leq j\\leq H,\\atop j-h^{\\prime}=h}\\mathbb{E}_{\\widehat{\\pi}}\\big[\\{\\mathcal{P}_{h^{\\prime},j}^{\\pi}\\,g_{j}\\}\\big(S_{h^{\\prime}},\\pi_{h^{\\prime}}(S_{h^{\\prime}})\\big)-\\{\\mathcal{P}_{h^{\\prime},j}^{\\pi}\\,g_{j}\\}\\big(S_{h^{\\prime}},\\widehat{\\pi}_{h^{\\prime}}(S_{h^{\\prime}})\\big)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\operatorname{D}\\big((\\mathcal{P}^{\\pi})^{h+1}\\,g\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $(\\pmb{\\mathscr{P}}^{\\pi})^{H}\\,\\pmb{g}=0$ , which implies $D\\bigl((\\mathcal{P}^{\\pi})^{H}\\,\\pmb{g}\\bigr)=0$ . We then sum the resulting bounds so as to obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\nD(g)=\\sum_{1\\leq h\\leq h^{\\prime}\\leq H}\\mathbb{E}_{\\widehat{\\pi}}\\big[\\{\\mathcal{P}_{h,h^{\\prime}}^{\\pi}g_{h^{\\prime}}\\}(S_{h},\\pi_{h}(S_{h}))-\\{\\mathcal{P}_{h,h^{\\prime}}^{\\pi}g_{h^{\\prime}}\\}(S_{h},\\widehat{\\pi}_{h}(S_{h}))\\big].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting $\\begin{array}{r}{g=\\mathcal{T}^{\\star}\\widehat{f}-\\widehat{f}}\\end{array}$ , or equivalently $g_{h}=T_{h}^{\\star}\\,\\widehat{f}_{h+1}-\\widehat{f}_{h}$ , in equation (19), we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\nD\\big(\\mathcal{T}^{\\star}\\widehat{f}-\\widehat{f}\\big)=\\sum_{h=1}^{H-1}\\mathbb{E}_{\\widehat\\pi}\\big[\\Delta_{h}(\\pi;S_{h},\\pi_{h}(S_{h}))-\\Delta_{h}(\\pi;S_{h},\\widehat\\pi_{h}(S_{h}))\\big]\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have used the fact (12) that $\\begin{array}{r}{\\Delta_{h}(\\pmb{\\pi};\\,\\cdot)=\\sum_{h^{\\prime}=h}^{H}\\mathcal{P}_{h,h^{\\prime}}^{\\pmb{\\pi}}\\big(\\mathcal{T}_{h^{\\prime}}^{\\star}\\,\\widehat f_{h^{\\prime}+1}-\\widehat f_{h^{\\prime}}\\big)}\\end{array}$ . Thus, we have established the bound (13) stated in Lemma 1. ", "page_idx": 14}, {"type": "text", "text": "It remains to establish the auxiliary claim (18b). Note that the functional $D$ can be decomposed as $D(\\pmb{g})=D_{1}+D_{2}$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{1}:=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\widehat{\\pi}}\\left[g_{h}(S_{h},\\pi_{h}(S_{h}))-g_{h}(S_{h},\\widehat{\\pi}_{h}(S_{h}))\\right]\\qquad\\mathrm{and}}\\\\ &{D_{2}:=\\displaystyle\\sum_{h=1}^{H}\\left(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat{\\pi}}\\right)\\left[g_{h}(S_{h},\\pi_{h}(S_{h}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the tower property of conditional expectation, we find that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle D_{2}\\ =\\ \\sum_{h=1}^{H-1}\\left(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat\\pi}\\right)\\left[\\mathbb{E}[g_{h+1}(S_{h+1},\\pi_{h+1}(S_{h+1}))\\mid S_{h},A_{h}]\\right]}}\\\\ {{\\displaystyle=\\ \\sum_{h=1}^{H-1}\\left(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat\\pi}\\right)\\left[(\\mathcal{P}_{h}^{\\pi}\\,g_{h+1})(S_{h},A_{h})\\right]=D\\big(\\mathcal{P}^{\\pi}g\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the expressions for $D_{1}$ and $D_{2}$ above yields the claim (18b). ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The key step in proving Lemma 2 is establishing that inequality (15) holds when the function estimation error $d_{h}(\\widehat{f_{h}},Q_{h}^{\\star})$ is sufficiently small. In order to do so, we need to establish upper bounds on the term $\\widehat{\\beta}(h,h^{\\prime})$ by using $\\beta(h,h^{\\prime})$ . In particular, we will show that for any $1\\leq h\\leq h^{\\prime}\\leq H-1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\beta}(h,\\,h^{\\prime})\\ \\leq\\ \\beta(h,\\,h^{\\prime})\\ +\\ \\sum_{j=1}^{h-1}\\,\\widehat{\\beta}(j,\\,h-1)\\,\\cdot\\,b_{\\mathcal{F}}\\,\\cdot\\,d_{h}\\big(\\widehat{f}_{h},\\,Q_{h}^{\\star}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inequality (20) is derived based on the definitions of metric $d_{h}$ and parameter $b_{\\mathcal{F}}=1$ . After a close examination of the right-hand side of this inequality, it becomes evident that as long as the function estimation error $d_{h}\\hat{(f_{h},Q_{h}^{\\star})}$ is sufficiently small, the terms associated with $d_{h}(\\widehat{f_{h}},Q_{h}^{\\star})$ are negligible and are dominated  by $\\beta(h,\\,h^{\\prime})$ . Consequently, inequality (15) within the arg uments in Appendix A.2 is likely to hold true. ", "page_idx": 15}, {"type": "text", "text": "With claim (20) assumed to be valid at this point, we now establish a proper upper bound on the estimation error $d_{h}(\\widehat{f_{h}},Q_{h}^{\\star})$ under which inequality (15) is satisfied. By taking linear combinations of inequality (20) usi ng weights $\\pmb{\\varepsilon}=(\\varepsilon_{1},\\dots,\\varepsilon_{H-1},\\varepsilon_{H}=0)$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{h=1}^{H-1}\\sum_{h^{\\prime}=h}^{H-1}\\widehat{\\beta}(h,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}\\ \\le\\ \\displaystyle\\sum_{h=1}^{H-1}\\sum_{h^{\\prime}=h}^{H-1}\\beta(h,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}}&\\\\ &{\\displaystyle\\ \\ \\ +\\sum_{h=2}^{H-1}\\sum_{j=1}^{h-1}\\widehat{\\beta}(j,\\,h-1)\\,\\cdot\\,b\\cdot\\,\\cdot\\,d_{h}\\left(\\widehat{f}_{h},\\,Q_{h}^{\\star}\\right)\\ \\sum_{h^{\\prime}=h}^{H-1}\\varepsilon_{h^{\\prime}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When the sequence $\\pmb{\\varepsilon}=(\\varepsilon_{1},\\dots,\\varepsilon_{H-1},\\varepsilon_{H}=0)$ ) is regular in the sense that inequality (4b) holds, the bound (21) reduces to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{1\\leq h\\leq h^{\\prime}\\leq H}\\widehat{\\beta}(h,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}\\ \\leq\\ \\displaystyle\\sum_{1\\leq h\\leq h^{\\prime}\\leq H}\\beta(h,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}}&{}\\\\ {\\displaystyle+\\sum_{1\\leq h\\leq h^{\\prime}\\leq H-2}\\widehat{\\beta}(h,h^{\\prime})\\cdot\\varepsilon_{h^{\\prime}}\\ \\cdot\\ b_{\\mathcal{F}}\\left(H-h^{\\prime}\\right)\\ \\cdot\\ d_{h^{\\prime}+1}\\left(\\widehat{f}_{h^{\\prime}+1},\\,Q_{h^{\\prime}+1}^{\\star}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under the condition $\\begin{array}{r}{d_{h}\\big(\\widehat f_{h},\\,Q_{h}^{\\star}\\big)\\,\\le\\,\\frac{1}{2\\,b\\mathcal{F}}(H-h+1)^{-1}}\\end{array}$ for $2\\leq h\\leq H-1$ , the inequality above implies bound (15), which  further establishes the bound (16), as stated in Lemma 2. ", "page_idx": 15}, {"type": "text", "text": "It remains to prove the relation between $\\widehat{\\beta}(h,\\,h^{\\prime})$ and $\\beta(h,\\,h^{\\prime})$ , as shown in inequality (20). ", "page_idx": 15}, {"type": "text", "text": "Proof of bound (20): It is evident that inequality (20) holds for $h=1$ , therefore, we focus on its validation for indices $2\\leq h\\leq H-1$ . Recall the definitions of functions $\\widehat{\\beta}(h,h^{\\prime})$ and $\\beta(h,h^{\\prime})$ , as given by equations (14a) and (14b). We apply the triangle inequality and d e rive that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\bigl|\\widehat{\\beta}(h,h^{\\prime})-\\beta(h,h^{\\prime})\\bigr|}}\\\\ &{\\leq\\underset{f\\in\\partial\\mathcal{F}:\\|f\\|_{h^{\\prime}}>0}{\\operatorname*{sup}}\\,\\biggl\\{\\frac{1}{\\|f\\|_{h^{\\prime}}}\\Bigl|\\bigl(\\mathbb{E}_{\\widehat{\\pi}}-\\mathbb{E}_{\\pi^{\\star}}\\bigr)\\Bigl[\\bigl(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\bigr)(S_{h},\\pi_{h}^{\\star}(S_{h}))-\\bigl(\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\bigr)(S_{h},\\widehat{\\pi}_{h}(S_{h}))\\bigr]\\Bigr|\\biggr\\}}\\\\ &{=\\underset{f\\in\\partial\\mathcal{F}:\\|f\\|_{h^{\\prime}}>0}{\\operatorname*{sup}}\\biggl\\{\\frac{1}{\\|f\\|_{h^{\\prime}}}\\Bigl|\\bigl(\\mathbb{E}_{\\widehat{\\pi}}-\\mathbb{E}_{\\pi^{\\star}}\\bigr)\\Bigl[\\bigl\\{\\bigl(\\mathcal{P}_{h-1}^{\\star}-\\mathcal{P}_{h-1}^{\\widehat{\\pi}}\\bigr)\\,\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\bigr\\}\\bigl(S_{h-1},A_{h-1}\\bigr)\\Bigr]\\Bigr|\\biggr\\}}\\\\ &{=:\\Delta\\beta(h,h^{\\prime})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The term $\\Delta\\beta(h,\\,h^{\\prime})$ involves differences from two sources: (i) the difference in transition kernels $\\mathcal{P}_{h-1}^{\\star}-\\mathcal{P}_{h-1}^{\\widehat{\\pi}}$ that captures the divergence between policies $\\pi_{h}^{\\star}$ and $\\widehat{\\pi}_{h}$ ; (ii) the discrepancy of occupation measures at the $(h-1)$ -th step reflected by the difference  i n expectations $\\mathbb{E}_{\\pi^{\\star}}-\\mathbb{E}_{\\widehat{\\pi}}$ , which is determined by the policies $(\\pi_{1}^{\\star},\\ldots,\\pi_{h-1}^{\\star})$ and $(\\widehat{\\pi}_{1},\\ldots,\\widehat{\\pi}_{h-1})$ until the $(h-1)$ -th step. W e treat them separately and write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta\\beta(h,\\,h^{\\prime})\\;\\leq\\;\\nu_{1}(h-1,\\,h^{\\prime})\\,\\cdot\\,\\nu_{2}(h-1)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the functionals $\\nu_{2}$ and $\\nu_{1}$ are defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{1}(h-1,\\,h^{\\prime})\\,:=\\,\\displaystyle\\operatorname*{sup}_{f\\in\\partial\\mathcal{F}:\\,\\|f\\|_{h^{\\prime}}>0}\\,\\left\\lbrace\\frac{1}{\\|f\\|_{h^{\\prime}}}\\big\\|\\big(\\mathcal{P}_{h-1}^{\\star}-\\mathcal{P}_{h-1}^{\\star}\\big)\\,\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\,\\big\\|_{h-1}\\right\\rbrace,}\\\\ &{\\quad\\nu_{2}(h-1)\\,:=\\,\\displaystyle\\operatorname*{sup}_{f\\in\\partial\\mathcal{F}:\\,\\|f\\|_{h-1}>0}\\,\\left\\lbrace\\frac{1}{\\|f\\|_{h-1}}\\big|\\big(\\mathbb{E}_{\\widehat{\\pi}}-\\mathbb{E}_{\\pi^{\\star}}\\big)\\,\\big[f\\big(S_{h-1},A_{h-1}\\big)\\big]\\,\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We first consider the term $\\nu_{1}$ . According to the definitions of metric $d_{h}$ and parameter $b_{\\mathcal{F}}$ we find that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|\\big(\\mathcal{P}_{h-1}^{\\star}-\\mathcal{P}_{h-1}^{\\widehat{\\pi}}\\big)\\,\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big\\|_{h-1}\\leq d_{h}\\big(\\widehat{f}_{h},\\,Q_{h}^{\\star}\\big)\\cdot\\big\\|\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\big\\|_{h}\\overset{(*)}{\\leq}d_{h}\\big(\\widehat{f}_{h},\\,Q_{h}^{\\star}\\big)\\cdot b_{\\mathcal{F}}\\,\\|f\\|_{h^{\\prime}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which in turn implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu_{1}(h-1,\\,h^{\\prime})\\;\\leq\\;b_{\\mathcal{F}}\\,\\cdot\\,d_{h}\\big(\\widehat{f}_{h},\\,Q_{h}^{\\star}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof of inequality $(*)$ for $b_{\\mathcal{F}}=1$ , as mentioned above, can be found in Appendix E.1. ", "page_idx": 16}, {"type": "text", "text": "As for term $\\nu_{2}$ , we claim that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nu_{2}(h-1)\\,\\leq\\,\\sum_{j=1}^{h-1}\\,\\widehat{\\beta}(j,\\,h-1)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the bound $\\widehat{\\beta}(h,\\,h^{\\prime})\\le\\beta(h,\\,h^{\\prime})+\\Delta\\beta(h,\\,h^{\\prime})$ with inequalities (22), (23a) and (23b), we establish the bound (2 0 ), as claimed. It remains to prove the claim (23b). ", "page_idx": 16}, {"type": "text", "text": "Proof of inequality (23b): This proof is analogous to that of Lemma 1. We begin by introducing an analogue of the functional $D(g)$ from equation (18a); in particular, for any index $h\\in[H-1]$ and function $g\\in\\partial{\\mathcal{F}}$ , define ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{h}^{\\star}(g):=\\left(\\mathbb{E}_{\\pmb{\\pi}^{\\star}}-\\mathbb{E}_{\\widehat{\\pmb{\\pi}}}\\right)\\left[g(S_{h},A_{h})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the notation of $D_{h}^{\\star}$ , we can rewrite the left-hand side of inequality (23b) as $\\nu_{2}(h-1)\\,=$ $\\begin{array}{r l}{\\operatorname*{sup}_{f\\in\\partial\\mathcal{F}:\\,\\|f\\|_{h-1}>0}\\left\\{|D_{h-1}^{\\star}(f)|/\\|f\\|_{h-1}\\right\\}}&{{}}\\end{array}$ . Following the same arguments as in the proof of inequality (18b), we can show that ", "page_idx": 16}, {"type": "text", "text": "$D_{h}^{\\star}(g)=\\mathbb{E}_{\\widehat{\\pi}}\\big[g(S_{h},\\pi_{h}^{\\star}(S_{h}))-g(S_{h},\\widehat{\\pi}_{h}(S_{h}))\\big]+D_{h-1}^{\\star}(\\mathcal{P}_{h-1}^{\\star}\\,g)\\qquad\\mathrm{for}\\;h=1,2,\\ldots,H,$ (24) where we set $D_{0}^{\\star}\\equiv0$ . ", "page_idx": 16}, {"type": "text", "text": "We consider function $g:=\\mathcal{P}_{j,h-1}^{\\star}\\,f$ for $1\\leq j<h\\leq H-1$ . It follows from equation (24) that ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{j}^{\\star}(\\mathcal{P}_{j,h-1}^{\\star}\\,f)=\\mathrm{E}_{\\widehat{\\pi}}\\big[\\big(\\mathcal{P}_{j,h-1}^{\\star}f\\big)(S_{j},\\pi_{j}^{\\star}(S_{j}))\\!-\\!\\big(\\mathcal{P}_{j,h-1}^{\\star}f\\big)(S_{j},\\widehat{\\pi}_{j}(S_{j}))\\big]+D_{j-1}^{\\star}\\big(\\mathcal{P}_{j-1,h-1}^{\\star}\\,f\\big)\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have used the relation $\\mathcal{P}_{j-1}^{\\star}\\mathcal{P}_{j,h-1}^{\\star}=\\mathcal{P}_{j-1,h-1}^{\\star}$ Pj\u22c6\u22121,h\u22121. Recalling the definition of\u03b2(j, h \u22121) in equation (14a), applying the triangle inequality yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|D_{j}^{\\star}\\big(\\mathcal{P}_{j,h-1}^{\\star}\\,f\\big)\\right|\\ \\leq\\ \\widehat{\\beta}(j,\\,h-1)\\cdot\\|f\\|_{h-1}\\,+\\,\\left|D_{j-1}^{\\star}\\big(\\mathcal{P}_{j-1,h-1}^{\\star}\\,f\\big)\\right|\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing this equation over indices $j=1,2,3,\\dots,h-1$ yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n|D_{h-1}^{\\star}(f)|\\,\\le\\,\\sum_{j=1}^{h-1}\\,\\widehat{\\beta}(j,\\,h-1)\\cdot\\|f\\|_{h-1}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which establishes inequality (23b). ", "page_idx": 16}, {"type": "text", "text": "C Proof of corollaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section cotains proofs of several corollaries. ", "page_idx": 17}, {"type": "text", "text": "C.1 Proof of Corollary 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now turn to proving Corollary 2 regarding ridge-based FQI in on-line settings. ", "page_idx": 17}, {"type": "text", "text": "In Phase 1 of pure exploration, the cumulative regret is always bounded from above by $T_{0}\\cdot H$ . During Phase 2 of fine-tuning, we let $\\widehat{\\pi}^{k}$ be the policy employed in the rounds $T_{0}2^{k}{+}1$ , $T_{0}2^{k}{+}2,\\ldots$ , $T_{0}2^{k+1}$ , which is determined by the   estimate $\\widehat{\\pmb f}^{(T_{0}2^{k})}$ calculated at the end of the $\\left(T_{0}2^{k}\\right)$ -th round. To estimate the regret, we consider the decom po sition ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=T_{0}+1}^{T}\\left\\{J(\\pi^{\\star})-J(\\widehat\\pi^{(t)})\\right\\}\\leq\\sum_{k=0}^{K-1}\\sum_{t=T_{0}2^{k}}^{T_{0}2^{k+1}}\\left\\{J(\\pi^{\\star})-J(\\widehat\\pi^{(t)})\\right\\}=\\sum_{k=0}^{K-1}T_{0}\\,2^{k}\\big\\{J(\\pi^{\\star})-J(\\widehat\\pi^{k})\\big\\}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We leverage our bound (11a) for off-line RL in Section 3.1.2 to control the value sub-optimality $J(\\pmb{\\pi}^{\\star})-\\bar{J}(\\widehat{\\pmb{\\pi}}^{k})$ . Recall that the policy $\\widehat{\\pi}^{k}$ is derived from i.i.d. trajectories collected from the rounds $T_{0}\\,2^{k-1}+1,T_{0}\\,2^{k-1}+2,\\ldots,T_{0}\\,2^{k}$ .  W e divide those $T_{0}\\,2^{k-1}$ trajectories into $H-1$ equal shares and use each share to conduct estimation in one iteration of the FQI procedure. This subsampling technique ensures the independence of samples used in different iterations. It is primarily adopted for the sake of convenience (to keep the explanations concise) and is not essential in general. It follows from inequality (11a) that the bound ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi^{k})\\;\\leq\\;c\\,\\frac{d\\sqrt{d}\\;H^{4}}{T_{0}\\,2^{k}}\\,\\log(d H K/\\delta)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds uniformly for indices $k=0,1,\\ldots,K-1$ with a probability exceeding $1-\\delta$ . ", "page_idx": 17}, {"type": "text", "text": "Putting together the pieces, we arrive at ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{Regret}(T)\\leq T_{0}\\cdot H+c\\,d{\\sqrt{d}}\\,H^{4}\\,K\\,\\log(d H K/\\delta)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We then derive the regret bound in Corollary 2 by noticing that $K=\\mathcal{O}(\\log T)$ . ", "page_idx": 17}, {"type": "text", "text": "C.2 Comparing to known off-line bounds ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we derive the sample complexity $\\begin{array}{r}{n_{Z\\mathrm{an}}(\\epsilon)\\asymp\\frac{d^{2}H^{3}}{\\epsilon^{2}}}\\end{array}$ d2\u03f52H3 in Section 3.1.2 based on the results of Zanette et al. [37]; it gives the conventional $1/\\sqrt{n}$ slow rate to which we compare. Zanette et al. [37] proved upper bounds on a pessimistic actor-critic scheme based on $d$ -dimensional linear function approximation. Using our notation, Theorem 1 in their paper [37] can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat\\pi)\\;\\le\\;c\\,\\bigg\\{\\frac{1}{H}\\sum_{h=1}^{H-1}\\,\\sqrt{\\overline{{\\phi}}_{h}^{\\,\\top}(\\widehat\\Sigma_{h,\\mathcal{D}}+\\lambda_{h}I)^{-1}\\,\\overline{{\\phi}}_{h}^{\\,}}\\bigg\\}\\,\\sqrt{\\frac{d H^{4}}{n}}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the vector $\\overline{{\\phi}}_{h}$ is given by $\\overline{{\\phi}}_{h}:=\\mathbb{E}_{\\pmb{\\pi}^{\\star}}\\left[\\phi(S_{h},A_{h})\\right]$ , the covariance matrix ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\Sigma}_{h,\\mathcal{D}}:=\\frac{1}{|\\mathcal{D}_{h}|}\\sum_{(s_{h,i},a_{h,i},s_{h,i}^{\\prime},r_{h,i})\\in\\mathcal{D}_{h}}\\!\\!\\!\\phi(s_{h,\\,i},\\,a_{h,\\,i})\\phi(s_{h,\\,i},\\,a_{h,\\,i})^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now consider the explicit dependence of this upper bound on dimension $d$ , horizon $H$ and sample size $n$ . The divergence term $\\overline{{\\phi}}_{h}^{\\,\\,\\,\\,\\top}(\\widehat{\\Sigma}_{h,{\\cal D}}+\\lambda_{h}{\\cal I})^{-1}\\overline{{\\phi}}_{h}$ measures the conditioning of the regularized covariance matrix $(\\widehat{\\pmb{\\Sigma}}_{h,\\mathcal{D}}+\\lambda_{h}\\pmb{I})$ along a specific direction of $\\overline{{\\phi}}_{h}$ . When the feature mapping $\\phi$ operates within a $d$ -d imensional space, it is reasonable to assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overline{{\\phi}}_{h}\\sp{\\,\\top}(\\widehat{\\Sigma}_{h,\\mathcal{D}}+\\lambda_{h}I)\\sp{-1}\\,\\overline{{\\phi}}_{h}\\;\\leq\\;c^{\\prime}\\;d\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The bound (25) then reduces to $J(\\pi^{\\star})\\,-\\,J(\\widehat\\pi)\\ \\leq\\ c\\,d H^{2}/\\sqrt{n}$ . Regarding the dependence on horizon $H$ , we conjecture that by incorporating  the law of total vari\u221aance in a more refined manner, it may be possible to further reduce the dependence by a factor of $\\sqrt{H}$ . Under these conditions, the bound takes the form $J(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\,c\\,d\\sqrt{H^{3}/n}$ . ", "page_idx": 17}, {"type": "text", "text": "D Details of the mountain car experiment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this experiment, a car is situated in a valley between two hills. The car\u2019s objective is to overcome the gravitational pull and reach the top of the right hill by efficiently controlling its acceleration. ", "page_idx": 18}, {"type": "text", "text": "D.1 Structure of the Markov decision process ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Markov decision process underlying the mountain car problem has a state space $S\\subset\\mathbb{R}^{2}$ and an action space ${\\mathcal{A}}\\subset\\mathbb{R}$ . The state $s=(p,v)$ consists of the current position $p$ and velocity $v$ , whereas the scalar action $a=f$ corresponds to the applied input force. The state variables $(p,v)$ and action $f$ are restricted as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\in[p_{\\operatorname*{min}},p_{\\operatorname*{max}}]=[-1.2,0.6],}\\\\ &{v\\in[v_{\\operatorname*{min}},v_{\\operatorname*{max}}]=[-0.07,0.07]\\quad\\mathrm{and}}\\\\ &{f\\in[f_{\\operatorname*{min}},f_{\\operatorname*{max}}]=[-1,1]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The mountain is described by the function ", "page_idx": 18}, {"type": "equation", "text": "$$\nm(p)=\\frac{1}{3}\\sin(3p)+\\frac{0.025}{(p_{\\mathrm{max}}-p)(p-p_{\\mathrm{min}})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "over the interval $p\\in[p_{\\mathrm{min}},p_{\\mathrm{max}}]$ . ", "page_idx": 18}, {"type": "text", "text": "Let $m^{\\prime}$ be the derivative of the mountain shape function $m$ , which represents the instantaneous slope, and let $(\\sigma_{v},\\sigma_{p})=(0.01,0.0025)$ be a pair of standard deviations that dictate the amount of randomness in the updates. For an interval $[a,b]$ , we define the truncation function ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi_{[a,b]}(u):={\\left\\{\\begin{array}{l l}{u}&{{\\mathrm{if~}}u\\in[a,b],}\\\\ {b}&{{\\mathrm{if~}}u>b,}\\\\ {a}&{{\\mathrm{if~}}u<a.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With this notation, at each discrete time step $h=0,1,2,\\ldots$ , the position and velocity of the car evolve as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{h+1}=\\Psi_{\\left[v_{\\operatorname*{min}},v_{\\operatorname*{max}}\\right]}\\big(v_{h}+0.0015\\,f_{h}-0.0025\\,m^{\\prime}(p_{h})+\\sigma_{v}Z_{h}\\big)}\\\\ &{p_{h+1}=\\Psi_{\\left[p_{\\operatorname*{min}},p_{\\operatorname*{max}}\\right]}\\big(p_{h}+v_{h+1}+\\sigma_{p}Z_{h}^{\\prime}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(Z_{h},Z_{h}^{\\prime})$ are a pair of independent standard normal variables. Note that the system dynamics are non-linear due to both the presence of the derivative $m^{\\prime}$ and the truncation function $\\Psi$ . ", "page_idx": 18}, {"type": "text", "text": "The objective of the car is to reach the peak of the mountain, designated by the position $p_{\\mathrm{goal}}=0.45$ . The reward at state-action pair $(s,a)$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r(s,a):=-\\frac{1}{10}f^{2}+100\\big[\\,\\mathrm{max}\\{0,\\,p-p_{\\mathrm{goal}}\\}\\big]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any policy $\\pi$ , we define the $\\gamma$ -discounted value function ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ(\\pi):=\\mathbb{E}_{\\pi}\\Big[\\sum_{h=0}^{\\infty}\\,\\gamma^{h}\\,r(S_{h},A_{h})\\Big],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "using $\\gamma=0.97$ . The initial state $s_{0}=(p_{0},v_{0})$ is generated with $p_{0}$ following a uniform distribution over the interval $[-0.6,-0.4]$ , and we initialize with velocity $v_{0}=0$ . ", "page_idx": 18}, {"type": "text", "text": "D.2 Fitted Q-iteration (FQI) with linear function approximation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we describe the use of fitted Q-iteration (FQI) with linear function approximation to estimate the optimal $Q$ -function, along with the corresponding greedy policy $\\widehat{\\pi}$ . ", "page_idx": 18}, {"type": "text", "text": "Linear function approximation We approximate the the optimal $Q$ -function $(s,a)\\mapsto Q^{\\star}(s,a)$ using a $d$ -dimensional linear function class with $d=3000$ features. We begin by defining the base ", "page_idx": 18}, {"type": "text", "text": "feature maps $\\phi_{p}:[\\,p_{\\mathrm{min}},p_{\\mathrm{max}}\\,]\\to\\mathbb{R}^{50}$ for position, and $\\phi_{v}:[\\,v_{\\operatorname*{min}},v_{\\operatorname*{max}}\\,]\\to\\mathbb{R}^{15}$ for velocity, with components given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\phi_{p,2j+1}(p):=\\cos(j p),\\enspace\\mathrm{for}\\,j=0,1,\\dots,24,\\enspace\\mathrm{~and}\\right.}\\\\ &{\\left.\\phi_{p,2j}(p):=\\sin(j p),\\enspace\\enspace\\mathrm{~for}\\,j=1,2,\\dots,25\\,;}\\\\ &{\\left\\{\\phi_{v,2j+1}(v):=\\cos(j v),\\enspace\\mathrm{for}\\,j=0,1,\\dots,7,\\enspace\\mathrm{~and}\\right.}\\\\ &{\\left.\\phi_{v,2j}(v):=\\sin(j v),\\enspace\\enspace\\mathrm{~for}\\,j=1,2,\\dots,7.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To represent the action $a\\equiv f$ , we define the base action feature map ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{f}(f):={\\big(}1,f,f^{2},f^{3}{\\big)}\\in\\mathbb{R}^{4}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The overall feature map $\\phi:S\\times A\\to\\mathbb{R}^{3000}$ is constructed by taking the outer product of the three base feature maps $\\phi_{p},\\phi_{v}$ , and $\\phi_{f}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi(s,a):=\\mathrm{vec}\\{\\phi_{p}(p)\\otimes\\phi_{v}(v)\\otimes\\phi_{f}(f)\\}\\in\\mathbb{R}^{3000}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking all possible triples of the three base features in the outer product leads to the overall dimension $d\\,=\\,3000\\,=\\,50\\,\\times\\,15\\,\\times\\,4$ . Given a weight vector $\\pmb{w}\\in\\dot{\\mathrm{~\\mathbb{R}~}}^{3000}$ , we define the function $f_{\\pmb{w}}(s,a):=\\langle\\pmb{w},\\,\\phi(s,a)\\rangle$ , and we approximate the optimal $Q$ -function using the function class $\\mathcal{F}:=\\left\\{f_{w}\\ |\\ w\\in\\mathbb{R}^{3000}\\right\\}$ . ", "page_idx": 19}, {"type": "text", "text": "Fitted Q-iteration (FQI) We employed ftited Q-iteration with the linear feature $\\phi:S\\!\\times\\!A\\!\\rightarrow\\!\\mathbb{R}^{3000}$ to estimate an optimal policy $\\widehat{\\pi}$ . The FQI process begins by initializing the weight vector as $w_{0}:=$ $\\mathbf{0}\\in\\mathbb{R}^{3000}$ . In each iteration,  w e first use the dataset $\\bar{\\mathcal{D}}=\\left\\{(s_{i},a_{i},r_{i},\\bar{s_{i}^{\\prime}})\\right\\}_{i=1}^{n}\\bar{\\subset}\\mathcal{S}\\times\\mathcal{A}\\times\\mathbb{R}\\times\\mathcal{S}$ to construct the pseudo-responses ", "page_idx": 19}, {"type": "equation", "text": "$$\ny_{i}:=r_{i}+\\gamma\\operatorname*{max}_{a\\in\\cal{A}}\\underbrace{\\langle w_{t},\\phi(s_{i}^{\\prime},a)\\rangle}_{f_{w_{t}}(s_{i}^{\\prime},a)}\\qquad\\mathrm{for}\\;i=1,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "corresponding to a stochastic estimate of the Bellman update applied to our current $Q$ -function estimate $f_{w_{t}}$ . The polynomial form of the force feature $\\phi_{f}$ allows for a closed-form solution to the maximum operation required in equation (27). Given these pseudo-responses, we then update the weight vector $\\pmb{w}_{t}\\rightarrow\\pmb{w}_{t+1}$ via the ridge regression ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pmb{w}_{t+1}:=\\arg\\operatorname*{min}_{\\pmb{w}\\in\\mathbb{R}^{3000}}\\Big\\{\\frac{1}{n}\\sum_{i=1}^{n}\\big\\{y_{i}-\\langle\\pmb{w},\\,\\phi(s_{i},a_{i})\\rangle\\big\\}^{2}+\\lambda_{n}\\|\\pmb{w}\\|_{2}^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\textstyle\\lambda_{n}={\\frac{0.01}{n}}$ in all experiments reported here. ", "page_idx": 19}, {"type": "text", "text": "We terminate the procedure after at most 500 iterations, or when there have been 5 consecutive iteration\u221as with insignificant improvements in weights, where insignificant means that $\\lVert\\pmb{w}_{t+1}-$ $w_{t}\\vert\\vert_{2}\\,/\\sqrt{3000}\\ <\\ 0.005$ . Letting $\\widehat{\\pmb w}$ represent the weight vector obtained from this procedure, the resulting policy $\\widehat{\\pi}$ is given by  s electing the greedy action based on the $Q$ -function estimate $\\widehat{f}(s,a):=\\langle\\widehat{\\pmb{w}},\\,\\phi(s,a)\\rangle$ . ", "page_idx": 19}, {"type": "text", "text": "D.3 Experimental configurations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our experiments were based on an off-line dataset consisting of $n$ i.i.d. tuples ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{D}=\\left\\{(s_{i},a_{i},r_{i},s_{i}^{\\prime})\\right\\}_{i=1}^{n}\\subset\\mathcal{S}\\times\\mathcal{A}\\times\\mathbb{R}\\times\\mathcal{S},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the state-action pairs $\\left\\{(s_{i},a_{i})=(p_{i},v_{i},f_{i})\\right\\}_{i=1}^{n}$ were generated from a uniform distribution over the cube $\\left[p_{\\operatorname*{min}},p_{\\operatorname*{max}}\\right]\\times\\left[v_{\\operatorname*{min}},v_{\\operatorname*{max}}\\right]\\times\\left[f_{\\operatorname*{min}},f_{\\operatorname*{max}}\\right]$ . We performed independent experiments with the sample size $n$ varying over the range ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{n\\in\\left\\{\\lfloor e^{k}\\rfloor\\mid k=10.5,10.75,11,\\ldots,13\\right\\}}\\\\ &{\\quad=\\left\\{36315,46630,59874,76879,98715,126753,162754,208981,268337,344551,442413\\right\\}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In each experiment, we generated a dataset $\\mathcal{D}$ , estimated an optimal policy $\\widehat{\\pi}$ based on the data, and evaluated the return $J(\\widehat{\\pi})$ . For each sample size, we conducted 80 indepen d ent trials. ", "page_idx": 19}, {"type": "text", "text": "In order to evaluate the return $J(\\widehat{\\pi})$ , for each initial position $p_{0}\\,=\\,-0.5+0.2\\,j/1000$ with $j\\,=$ $-500,-499,-498,\\ldots,499$ , we si mulated 30 independent 1000-step trajectories by executing the estimated policy $\\widehat{\\pi}$ . The average return over the $30\\times1000$ trajectories is used as the estimate of $\\bar{J}(\\widehat{\\pi})$ . ", "page_idx": 20}, {"type": "text", "text": "In order to appro  ximate the policy7 $\\pi^{\\dagger}$ that represents \u201cground truth\u201d, we conducted a single expe riment with sample size $n=6.4\\times10^{6}$ to obtain $\\pi^{\\dagger}$ . We simulated 1000 trajectories for each initial position $p_{0}$ and calculated the average return, which serves as the reference value $J(\\pi^{\\dagger})$ . The value sub-optimality is then computed as the difference $J(\\pi^{\\dagger})-J(\\widehat{\\pi})$ . ", "page_idx": 20}, {"type": "text", "text": "E Verification of auxiliary claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this appendix, we collect the verification of various auxiliary claims made in the main text. ", "page_idx": 20}, {"type": "text", "text": "E.1 Properties of occupation measures ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this appendix, we prove a useful inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathcal{P}_{h,h^{\\prime}}^{\\star}\\,f\\right\\|_{h}\\leq\\|f\\|_{h^{\\prime}}\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which holds for the state-action occupation measures (3). This bound is used in the proof of bound (20) in Appendix B.2. ", "page_idx": 20}, {"type": "text", "text": "By definition, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{P}_{h}^{\\star}f\\|_{h}^{2}=\\mathbb{E}_{\\pi^{\\star}}\\left[(\\mathcal{P}_{h}^{\\star}f)^{2}(S_{h},A_{h})\\right]=\\mathbb{E}_{\\pi^{\\star}}\\left[\\mathbb{E}_{h}\\left[f(S_{h+1},\\pi_{h+1}^{\\star}(S_{h+1}))\\mid S_{h},A_{h}\\right]^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to the property of variance, we can deduce ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi^{\\star}}\\left[\\mathbb{E}_{h}\\left[f\\big(S_{h+1},\\pi_{h+1}^{\\star}(S_{h+1})\\big)\\mid S_{h},A_{h}\\right]^{2}\\right]\\leq\\mathbb{E}_{\\pi^{\\star}}\\left[f^{2}\\big(S_{h+1},\\pi_{h+1}^{\\star}(S_{h+1})\\big)\\right]=\\|f\\|_{h+1}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a consequence, we find that $\\|\\mathcal{P}_{h}^{\\star}f\\|_{h}\\leq\\|f\\|_{h+1}$ . Applying this inequality recursively leads to the conclusion that for any indices $1\\leq h\\leq h^{\\prime}\\leq H$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{P}_{h,h^{\\prime}}^{\\star}f\\right\\|_{h}=\\left\\|\\mathcal{P}_{h}^{\\star}\\mathcal{P}_{h+1,h^{\\prime}}^{\\star}f\\right\\|_{h}\\leq\\left\\|\\mathcal{P}_{h+1,h^{\\prime}}^{\\star}f\\right\\|_{h+1}\\leq\\left\\|\\mathcal{P}_{h+2,h^{\\prime}}^{\\star}f\\right\\|_{h+2}\\leq\\cdots\\leq\\left\\|f\\right\\|_{h^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This establishes the bound (29). ", "page_idx": 20}, {"type": "text", "text": "E.2 Proof of the telescope inequality (6) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For completeness of this paper,8 let us prove the telescope relation (6) stated in Section 2.3. For any policy ${\\boldsymbol{\\pi}}=(\\pi_{1},\\ldots,\\pi_{H})$ and sequence of functions $\\pmb{f}=(f_{1},\\dots,f_{H})$ with $f_{H}=r_{H}$ , we have the \u201ctelescope\u201d relation ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{1}^{\\pi}(s)=f_{1}(s,\\pi_{1}(s))+\\sum_{h=1}^{H-1}\\mathbb{E}_{\\pi}\\big[\\big(\\mathcal{T}_{h}^{\\pi}f_{h+1}-f_{h}\\big)(S_{h},A_{h})\\;\\big|\\;S_{1}=s\\big]\\quad\\mathrm{for~any~state~}s\\in\\mathcal{S}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here the value function $V_{1}^{\\pi}$ is given by $V_{1}^{\\pmb{\\pi}}(s):\\,=Q_{1}^{\\pmb{\\pi}}(s,\\pi_{1}(s))$ . Taking $\\pmb{f}=\\hat{\\pmb{f}}$ in equation (30) yields ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{1}^{\\pi}(s)=\\widehat{f}_{1}(s,\\pi_{1}(s))+\\sum_{h=1}^{H-1}\\mathbb{E}_{\\pi}\\big[\\big(\\mathcal{T}_{h}^{\\pi}\\widehat{f}_{h+1}-\\widehat{f}_{h}\\big)(S_{h},A_{h})\\bigm|S_{1}=s\\big]\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Letting $\\pi=\\widehat{\\pi}$ in equation (31a) yields ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{1}^{\\widehat{\\pi}}(s)=\\widehat{f}_{1}(s,\\widehat{\\pi}_{1}(s))+\\sum_{h=1}^{H-1}\\mathbb{E}_{\\widehat{\\pi}}\\left[\\left(\\mathcal{T}_{h}^{\\widehat{\\pi}}\\widehat{f}_{h+1}-\\widehat{f}_{h}\\right)(S_{h},A_{h})\\;\\big|\\;S_{1}=s\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\widehat{\\pi}$ is a greedy policy with respect to function ${\\widehat{\\pmb f}}.$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}_{1}(s,\\widehat{\\pi}_{1}(s))\\geq\\widehat{f}_{1}(s,\\pi_{1}(s)),\\quad\\mathrm{and}\\quad\\mathcal{T}_{h}^{\\widehat{\\pi}}\\widehat{f}_{h+1}=\\mathcal{T}_{h}^{\\star}\\widehat{f}_{h+1}\\geq\\mathcal{T}_{h}^{\\pi}\\widehat{f}_{h+1}\\quad\\mathrm{~for~any~policy~}\\pi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using this   fact and subtracting equations (31a ) and (31b),  we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{1}^{\\pi}(s)-V_{1}^{\\widehat{\\pi}}(s)\\leq\\sum_{h=1}^{H-1}\\,\\left(\\mathbb{E}_{\\pi}-\\mathbb{E}_{\\widehat\\pi}\\right)\\big[\\big({\\mathcal T}_{h}^{\\star}\\,\\widehat f_{h+1}-\\widehat f_{h}\\big)\\big(S_{h},A_{h}\\big)\\bigm|S_{1}=s\\big]\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, taking the expectation over the initial distribution $\\xi_{1}$ yields the claimed inequality (6). ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper thoroughly discusses each point made in the abstract, including fast-rate convergence of RL in continuous state-action spaces, key stability properties, and the pessimism and optimism principles. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Section 4 discusses the work\u2019s limitations and future directions, such as adapting the framework to linear quadratic control, extending beyond i.i.d. data and finitedimensional linear function spaces, proving a lower bound to demonstrate sharpness, and examining model mis-specification. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The theoretical results in this paper are mathematically rigorous, with intuitions in the main body and detailed justifications and proofs in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Appendix D explains how to reproduce the synthetic Mountain Car experiment from Section 1.1, including MDP structure, linear function space construction, FQI implementation, and experimental setups. No data is required. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 23}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The submission provides a supplementary code document to reproduce the Mountain Car experiment. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Appendix D explains how to reproduce the synthetic Mountain Car experiment from Section 1.1, including MDP structure, linear function space construction, FQI implementation, and experimental setups. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Figure 1(b) in the paper includes error bars for the standard errors, and the estimated slope in Figure 1(b) uses a bootstrap confidence interval to confirm observation validity. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 1.1 details the computer setup: \u201cThe experiment ran for 3 days on two laptops, each equipped with an Apple M2 Pro CPU and 16 GB RAM.\u201d ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the research in the paper fully conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper focuses on purely theoretical aspects of reinforcement learning and does not discuss direct societal impacts, given its theoretical nature. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not use existing assets. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]