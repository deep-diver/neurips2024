[{"figure_path": "ZxVrkm7Bjl/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot downstream performance and perplexity on various language modeling datasets. MoEUT marginally outperforms standard Transformers in most tasks, confirming that MoEUT is indeed a capable language model.", "description": "This table presents the zero-shot performance of MoEUT and baseline Transformer models on six downstream tasks (LAMBADA, BLIMP, Children's Book Test, HellaSwag, PIQA, and ARC-E) across different datasets (C4, peS2o, SlimPajama) and model sizes.  The results show that MoEUT achieves comparable or slightly better performance than the baseline Transformers on these tasks, highlighting its effectiveness even without specific training for the downstream tasks.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/tables/tables_14_1.jpg", "caption": "Table 1: Zero-shot downstream performance and perplexity on various language modeling datasets. MoEUT marginally outperforms standard Transformers in most tasks, confirming that MoEUT is indeed a capable language model.", "description": "This table presents the results of zero-shot evaluations on six downstream tasks (LAMBADA, BLIMP, CBT, HellaSwag, PIQA, and ARC-E) for different language models (Baseline, MoEUT, SUT, and SUT w.o. ACT) trained on various datasets (C4, peS2o, and SlimPajama). It demonstrates that MoEUT achieves competitive or slightly better performance than the baseline on multiple tasks, highlighting its capabilities as a general-purpose language model.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/tables/tables_15_1.jpg", "caption": "Table 3: Hyperparameters of different models used in our main experiments.", "description": "This table lists the hyperparameters used for various models in the main experiments of the paper.  It includes different model sizes (indicated by #params), the number of layers (nlayers), group size (G), model dimension (dmodel), feed-forward dimension (dff), number of heads (H), number of attention experts (NA), expert head dimension (dhead), number of feedforward experts (NE), number of active experts (K), number of warmup steps (Nwarmup), and the coefficient for the entropy regularization (\u03ba). The table shows hyperparameters for baseline Transformer models and for MoEUT models with different configurations.  It provides a detailed breakdown of the settings used for comparative analysis in the paper.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/tables/tables_15_2.jpg", "caption": "Table 4: Hyperparameters of SUT models used in our experiments.", "description": "This table shows the hyperparameters used for the Sparse Universal Transformer (SUT) models in the paper's experiments.  It includes the number of parameters, number of layers, model dimension, expert dimension, number of heads, number of attention experts, dimension of attention experts, dimension of head, number of experts in the feedforward layer, number of active experts, coefficient for MLP loss, coefficient for attention loss, number of warmup steps and the kappa value. Note that the meanings of the parameters are not directly analogous to those used for MoEUT models in the paper.", "section": "Main Experimental Results"}, {"figure_path": "ZxVrkm7Bjl/tables/tables_17_1.jpg", "caption": "Table 5: Wall-Clock Time for the forward-backward pass and the total memory usage of our training loop with different 244M parameter models on 8 V100 GPUs, which a batch size of 64x1024 tokens. MoEUT is 1.7x slower with our suboptimal MoE kernel implementation than the standard transformer, but it is much faster than the other UT variants. It also uses much less memory, allowing training on larger scales.", "description": "This table presents the results of a controlled experiment to compare the real-world resource usage of different models. It shows the wall-clock time per batch and memory usage on 8 V100 32GB GPUs for four different models, each with 244M parameters. The models compared are a non-shared transformer, a naive universal transformer (UT), the proposed MoEUT, and the SUT.  The experiment measures training iteration time and memory usage, demonstrating the improved efficiency of the MoEUT model in terms of training speed and memory usage compared to the other UT variants.", "section": "A.7 Wall Clock Time and Memory Comparison"}, {"figure_path": "ZxVrkm7Bjl/tables/tables_18_1.jpg", "caption": "Table 6: Training hardware information for the experiments reported in the paper", "description": "This table details the computational resources used for each experiment conducted in the study.  It lists the model, its parameter count, dataset used, layer grouping size (G), GPU type, number of GPUs, CPUs, RAM used, and the duration of the training process.  This information aids in understanding the scalability and resource requirements of the MoEUT model compared to other models. Note that some values are marked as \"?\" indicating missing information.", "section": "A.8 Compute Requirements"}]