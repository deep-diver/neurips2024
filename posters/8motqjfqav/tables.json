[{"figure_path": "8moTQjfqAV/tables/tables_19_1.jpg", "caption": "Table 1: Test accuracy of each layer of 4-layer AD networks trained on MNIST and CIFAR-10. Each layer has the same number of hidden activations; for example, the first row refers to an AD network with layers of 500, 500, 500, and 500 activations. Since each layer makes its own prediction, we can easily see how performance increases per layer.", "description": "This table presents the test accuracy achieved by each layer in a 4-layer Artificial Dopamine (AD) network trained on the MNIST and CIFAR-10 datasets.  Each layer of the network has an identical number of hidden activations. The table demonstrates how the model's performance improves layer by layer, as each layer produces its own prediction.", "section": "J Experiments on MNIST and CIFAR-10"}, {"figure_path": "8moTQjfqAV/tables/tables_20_1.jpg", "caption": "Table 2: Existing assets used.", "description": "This table lists the versions, licenses, and URLs of the software packages and libraries used in the paper's implementation and experiments.  It includes information about JAX, MinAtar, DeepMind Control Suite (DMC), and Gymnasium.", "section": "L Usage of Existing Assets"}]