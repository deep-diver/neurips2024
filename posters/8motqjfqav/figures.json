[{"figure_path": "8moTQjfqAV/figures/figures_1_1.jpg", "caption": "Figure 1: Simplified illustration of dopamine distribution in the NAc. Dopamine is synthesized in the VTA and transported along axons to the NAc, where it is picked up by receptors in medium spiny neurons. Dopamine concentrations (error signals) are locally homogenous, but can vary across regions. Connections between NAc neurons not shown.", "description": "This figure illustrates the pathway of dopamine from the ventral tegmental area (VTA) to the nucleus accumbens (NAc). Dopamine neurons in the VTA synthesize dopamine and transport it along their axons to the NAc. In the NAc, dopamine is released and picked up by receptors on medium spiny neurons. The concentration of dopamine is relatively homogeneous within local regions of the NAc, but can vary across different regions.  The figure simplifies the complex neural connections in the NAc for clarity.", "section": "2 Background"}, {"figure_path": "8moTQjfqAV/figures/figures_2_1.jpg", "caption": "Figure 2: Network architecture of a 3-layer AD network.  ht represents the activations of layer l at time t, and st the input state. The blocks are AD cells, as shown in Figure 3. Similar to how dopamine neurons compute and distribute error used by a local region, each cell computes its own local TD error used by its updates; errors do not propagate across layers. To relay information, upper layers send activations to lower layers in the next timestep. For example, red shows all active connections at t = 1.", "description": "This figure illustrates the architecture of a three-layer Artificial Dopamine (AD) network. Each layer contains an AD cell which computes its own local temporal difference (TD) error and updates its weights independently of other layers.  There's no backpropagation of error signals between layers. Information is passed forward in time from upper layers to lower layers using activations, not errors.  The diagram shows how the activations (ht) are passed to AD cells across timesteps (t).", "section": "3 Artificial Dopamine"}, {"figure_path": "8moTQjfqAV/figures/figures_4_1.jpg", "caption": "Figure 3: Inner workings of our proposed AD cell (i.e., hidden layer).  ht is the activations of the cell l at time t, and Q is a vector of Q-value predictions given the current state and each action. We compute the cell's activations h using a ReLU weight layer, then use an attention-like mechanism to compute Q. Specifically, we obtain Q by having the cell's tanh weight layers, one for each action, compute attention weights that are then applied to h. Each cell computes its own error.", "description": "This figure details the internal structure and computations of a single Artificial Dopamine (AD) cell, a core component of the proposed deep Q-learning algorithm. The AD cell takes as input the hidden layer activations from the previous timestep from the layer above (h[l+1]t-1) and the current timestep from the layer below (h[l-1]t). It uses these inputs with multiple tanh and ReLU layers, and an attention mechanism to compute its Q-value predictions (Q[l]t) for each action.  Importantly, each cell independently computes its own error, mirroring the local nature of dopamine signals in the brain's reward system.", "section": "3.1 AD Cell Internals"}, {"figure_path": "8moTQjfqAV/figures/figures_7_1.jpg", "caption": "Figure 4: Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2 and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure presents the results of the Artificial Dopamine (AD) algorithm on a range of reinforcement learning tasks, comparing its performance to existing state-of-the-art algorithms.  The tasks come from the MinAtar (discrete) and DeepMind Control (DMC) suites (continuous). For each task, the average episodic return is plotted over the course of training for AD, DQN (Deep Q-Network), TD-MPC2 (Temporal Difference Model Predictive Control), and SAC (Soft Actor-Critic).  Error bars representing three standard errors are included to show the variability in performance across multiple runs.", "section": "Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_7_2.jpg", "caption": "Figure 5: Ablation study comparing the performance of AD against AD without the forward-in-time connections, and a single-layer AD cell. In Seaquest and Asterix, AD achieves qualitatively stronger performance. In Seaquest the line for AD single layer is overlapped by the line for AD no forward.", "description": "This figure presents an ablation study to evaluate the impact of the forward-in-time connections and multiple layers in the AD architecture. It compares the performance of three different versions of the AD algorithm on five MinAtar environments: Freeway, Breakout, Seaquest, Asterix, and Space Invaders.  The three versions are:\n\n1. **AD:** The full AD architecture with forward connections.\n2. **AD no forward:** The AD architecture without forward connections.\n3. **AD single layer:** A version of AD with only a single layer. \n\nThe results indicate that both forward connections and multiple layers contribute significantly to the performance.  In Seaquest and Asterix, AD performed much better than the other two versions.", "section": "Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_8_1.jpg", "caption": "Figure 6: Episodic returns of different-sized single-layer AD, compared to the standard 3-layer AD. Single 128 is a single-layer with 128 hidden activations. Overall, increasing the layer size of the single layer does not result in clear increases in performance. Lines show the mean return over 8 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure shows an ablation study comparing the performance of the proposed 3-layer AD network against single-layer AD networks with different numbers of hidden units (128, 256, and 512).  The results show that increasing the size of the single layer does not significantly improve performance, indicating the importance of multiple layers in the AD architecture for effective learning. The plot displays the mean episodic return for each network configuration over eight random seeds.  Error bars representing 3 standard errors are also provided to show the variability in performance.", "section": "Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_8_2.jpg", "caption": "Figure 4: Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2 and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure presents a comparison of the performance of the ARTIFICIAL DOPAMINE (AD) algorithm against three other deep reinforcement learning algorithms (DQN, TD-MPC2, and SAC) across a range of tasks from the MinAtar and DeepMind Control Suite benchmark environments.  The x-axis represents the number of environment steps, and the y-axis represents the average episodic return.  The lines show the mean return achieved across 10 different random seeds for each algorithm and each environment, while the shaded areas represent the standard error of the mean.  The results show that the AD algorithm achieves performance comparable to the other algorithms across most of the tested environments, suggesting that synchronously distributed per-layer temporal difference errors may be sufficient for learning.", "section": "4 Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_16_1.jpg", "caption": "Figure 4: Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2 and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure compares the performance of the proposed Artificial Dopamine (AD) algorithm against three established deep reinforcement learning (RL) algorithms: Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Temporal Difference Model Predictive Control (TD-MPC2).  The comparison is conducted across a set of 14 tasks, which includes discrete tasks from the MinAtar environment (miniaturized Atari games) and continuous control tasks from the DeepMind Control Suite (DMC). The plot shows the mean episodic reward (average return per episode) obtained by each algorithm over 10 random seeds, with error bars indicating 3 standard errors. This visualization helps assess the relative performance of AD compared to the baselines across various task complexities.", "section": "Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_17_1.jpg", "caption": "Figure 4: Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2 and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure presents the performance comparison of the proposed Artificial Dopamine (AD) algorithm against three established deep reinforcement learning algorithms: Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Temporal Difference Model Predictive Control (TD-MPC2).  The results are shown across a set of benchmark tasks from the MinAtar and DeepMind Control Suite (DMC) environments. For each environment, the mean episodic reward and its associated standard error across 10 independent trials are shown, allowing for a clear comparison of the algorithms' performance. The x-axis represents the total number of environment steps taken, and the y-axis indicates the average episodic reward achieved.", "section": "4 Experiments"}, {"figure_path": "8moTQjfqAV/figures/figures_17_2.jpg", "caption": "Figure 4: Episodic returns of AD in MinAtar and DMC environments, compared to DQN, TD-MPC2 and SAC. Lines show the mean return over 10 seeds and the shaded area conforms to 3 standard errors. The axes are return and environmental steps.", "description": "This figure compares the performance of the proposed ARTIFICIAL DOPAMINE (AD) algorithm to other state-of-the-art reinforcement learning algorithms (DQN, TD-MPC2, and SAC) across a set of 14 tasks.  The tasks are split between MinAtar (discrete) and DeepMind Control Suite (DMC, continuous) environments.  The x-axis represents the number of environment steps and the y-axis represents the mean episodic return.  Error bars (shaded regions) represent three standard errors, indicating the variability in performance across multiple runs (10 seeds). The plot visualizes how well the AD algorithm performs compared to baselines, demonstrating comparable performance despite not using backpropagation.", "section": "Experiments"}]