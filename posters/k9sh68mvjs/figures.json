[{"figure_path": "k9SH68MvJs/figures/figures_2_1.jpg", "caption": "Figure 1: Denoising diffusion probabilistic model. Latent variables x1,..., XN are produced from the data point xo via a forward diffusion process, i.e., gradually adding noises to the latent variables. A diffusion model & learns to reverse the diffusion process by denoising the noisy data to reconstruct the original data point x0.", "description": "This figure illustrates the denoising diffusion probabilistic model. It shows how the forward diffusion process adds Gaussian noise to the data points over time steps, transforming them into an isotropic Gaussian distribution.  The reverse diffusion process, trained by a diffusion model, learns to predict and remove the added noise, step-by-step, eventually reconstructing the original data point from the noisy version. This model is used as a key component in the proposed method DRAIL for generating improved reward functions.", "section": "3.2 Diffusion models"}, {"figure_path": "k9SH68MvJs/figures/figures_3_1.jpg", "caption": "Figure 2: Diffusion-Reward Adversarial Imitation Learning. Our proposed framework DRAIL incorporates a diffusion model into GAIL. (a) Our proposed diffusion discriminative classifier D learns to distinguish expert data (SE, AE) ~ TE from agent data (s\u03c0, a\u03c0) ~ T\u03b8 using a diffusion model. D\u03c6 is trained to predict a value closer to 1 when the input state-action pairs are sampled from expert demonstration and predict a value closer to 0 otherwise. (b) The policy \u03c0\u03b8 learns to maximize the diffusion reward r computed based on the output of D that takes the state-action pairs from the policy as input. The closer the policy resembles expert behaviors, the higher the rewards it can obtain.", "description": "This figure illustrates the architecture of the Diffusion-Reward Adversarial Imitation Learning (DRAIL) framework.  Panel (a) shows how a diffusion discriminative classifier (D\u03c6) is trained to distinguish between expert and agent state-action pairs using a diffusion model (\u03c6).  The classifier learns to output a value close to 1 for expert data and 0 for agent data.  Panel (b) shows how a policy (\u03c0\u03b8) is trained to maximize the diffusion reward (r), which is calculated based on the output of the diffusion discriminative classifier.  The reward incentivizes the policy to produce state-action pairs similar to those of the expert.", "section": "4 Approach"}, {"figure_path": "k9SH68MvJs/figures/figures_6_1.jpg", "caption": "Figure 3: Environments & tasks. (a) MAZE: A point-mass agent (green) within a 2D maze is trained to move from its initial position to reach the goal (red). (b) FETCHPUSH: The manipulation task is implemented with a 7-DoF Fetch robotics arm. FETCHPUSH requires picking up or pushing an object to a target location (red). (c) HANDROTATE: For this dexterous manipulation task, a Shadow Dexterous Hand is employed to in-hand rotate a block to achieve a target orientation. (d) ANTREACH: This task trains a quadruped ant to reach a goal randomly positioned along the perimeter of a half-circle with a radius of 5 m. (e) WALKER: This locomotion task requires training a bipedal walker policy to achieve the highest possible walking speed while maintaining balance. (f) CARRACING This image-based racing game task requires driving a car to navigate a track as quickly as possible.", "description": "This figure shows the six different environments used in the paper's experiments.  Each subfigure illustrates a different task: navigation in a maze, robotic arm manipulation (pushing and picking), dexterous hand manipulation (in-hand rotation), quadruped locomotion (ant reaching), bipedal locomotion (walker), and image-based car racing. The tasks vary in complexity and dimensionality.", "section": "5.1 Experimental setup"}, {"figure_path": "k9SH68MvJs/figures/figures_7_1.jpg", "caption": "Figure 4: Learning efficiency. We report success rates (MAZE, FETCHPUSH, HANDROTATE, ANTREACH) and return (WALKER, CARRACING), evaluated over five random seeds. Our method DRAIL learns more stably, faster, and achieves higher or competitive performance compared to the best-performing baseline in all the tasks.", "description": "This figure shows the success rate and return of DRAIL and several baseline algorithms across six different continuous control tasks. The x-axis represents the number of environment steps, and the y-axis represents either the success rate (for MAZE, FETCHPUSH, HANDROTATE, and ANTREACH) or the return (for WALKER and CARRACING). DRAIL consistently outperforms or is competitive with the best baseline method across all tasks.", "section": "5 Experiments"}, {"figure_path": "k9SH68MvJs/figures/figures_8_1.jpg", "caption": "Figure 5: Generalization experiments in FETCHPUSH. We present the performance of our proposed DRAIL and baselines in the FETCHPUSH task, under varying levels of noise in initial states and goal locations. The evaluation spans three random seeds, and the training curve illustrates the success rate dynamics.", "description": "This figure shows the success rate of different imitation learning methods in the FETCHPUSH task under various levels of noise added to the initial states and goal locations. The x-axis represents the number of steps, and the y-axis represents the success rate. The different lines represent different methods: BC, Diffusion Policy, GAIL, GAIL-GP, WAIL, DiffAIL, DRAIL, and Expert. The shaded area around each line represents the standard deviation across three random seeds. The results show that DRAIL outperforms other methods in terms of success rate, especially under high noise levels.", "section": "5.4 Generalizability"}, {"figure_path": "k9SH68MvJs/figures/figures_8_2.jpg", "caption": "Figure 4: Learning efficiency. We report success rates (MAZE, FETCHPUSH, HANDROTATE, ANTREACH) and return (WALKER, CARRACING), evaluated over five random seeds. Our method DRAIL learns more stably, faster, and achieves higher or competitive performance compared to the best-performing baseline in all the tasks.", "description": "This figure shows the learning curves of DRAIL and various baseline methods across six different continuous control tasks.  The y-axis represents either success rate (for the navigation, manipulation, and reaching tasks) or cumulative return (for the locomotion and racing tasks). The x-axis represents the number of environment steps.  DRAIL consistently outperforms or matches the best-performing baseline method across all tasks, highlighting its improved learning efficiency and stability.", "section": "5.3 Experimental results"}, {"figure_path": "k9SH68MvJs/figures/figures_9_1.jpg", "caption": "Figure 7: Reward function visualization. We present visualizations of the learned reward values by the discriminative classifier of GAIL and the diffusion discriminative classifier of our DRAIL. The target expert demonstration for imitation is depicted in (a), which is a discontinuous sine function. The reward distributions of GAIL and our DRAIL are illustrated in (b) and (c), respectively.", "description": "This figure visualizes the learned reward functions of GAIL and DRAIL in a simple sine wave environment.  Panel (a) shows the expert's discontinuous sine wave function. Panel (b) shows the reward function learned by GAIL, demonstrating overfitting and a narrow reward distribution.  Panel (c) shows the reward function learned by DRAIL; it's smoother and has a broader distribution, indicating better generalization.", "section": "5.6 Reward function visualization"}, {"figure_path": "k9SH68MvJs/figures/figures_18_1.jpg", "caption": "Figure 8: Extended results of generalization experiments. MAZE is evaluated with different coverages of state and goal locations in the expert demonstrations, while FETCHPICK, FETCHPUSH, and HANDROTATE environments are evaluated in environments of different noise levels. The number indicates the amount of additional noise in agent learning compared to that in the expert demonstrations, with more noise requiring harder generalization. The noise level rises from left to right.", "description": "This figure displays the success rates of different imitation learning algorithms across six tasks, under varying conditions.  The top row shows results from the MAZE task with different proportions of training data used. The following rows show results from FETCHPICK and FETCHPUSH, which involve pushing and picking tasks with varying levels of noise in initial conditions and target positions. The general trend is that DRAIL (the authors' method) consistently outperforms other algorithms, showcasing its robustness and ability to generalize well to unseen situations.", "section": "C Extended results of generalization experiments"}, {"figure_path": "k9SH68MvJs/figures/figures_19_1.jpg", "caption": "Figure 9: Hyperparameter Sensitivity of DRAIL in the MAZE Environment. The results show the performance of DRAIL under varying learning rates for the discriminator (a) and policy (b). Different scaling factors (5x, 2x, 1x, 0.5x, 0.2x) of the baseline learning rate are tested. The results demonstrate that DRAIL remains robust across these variations, maintaining stable performance in the MAZE environment.", "description": "This figure displays the results of a hyperparameter sensitivity experiment conducted on the DRAIL model within the MAZE environment.  Two subfigures show the impact of varying the discriminator learning rate (a) and the policy learning rate (b) on the model's success rate.  Multiple lines represent different scaling factors (0.2x, 0.5x, 1x, 2x, 5x) applied to the baseline learning rate. The consistent success rate across the varying learning rates demonstrates the robustness of DRAIL to different hyperparameter choices.", "section": "D Hyperparameter Sensitivity Experiment"}]