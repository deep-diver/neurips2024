[{"figure_path": "EiIelh2t7S/tables/tables_5_1.jpg", "caption": "Table 1: Context length and its corresponding lower bound of RoPE's base.", "description": "This table shows the relationship between the context length and the lower bound of the RoPE base value.  For a given context length (e.g., 1k, 2k, 4k, etc.), the table provides the minimum base value required to achieve that context length capability.  The values are expressed in scientific notation (e.g., 4.3e3 means 4.3 x 10^3 or 4300).  This table is a key finding of the paper, demonstrating a fundamental constraint of RoPE in extending the context window of large language models.", "section": "4.3 Base of ROPE Bounds the Context Length"}, {"figure_path": "EiIelh2t7S/tables/tables_5_2.jpg", "caption": "Table 1: Context length and its corresponding lower bound of RoPE's base.", "description": "This table shows the relationship between the context length and the lower bound of the RoPE base value.  For a given context length, there is a minimum base value required for the model to effectively utilize that context length. The table lists these lower bound values for various context lengths, ranging from 1,000 to 1 million tokens.", "section": "4.3 Base of ROPE Bounds the Context Length"}, {"figure_path": "EiIelh2t7S/tables/tables_8_1.jpg", "caption": "Table 3: The comparison of \"Method 1\" and \"Method 2\". These methods are designed carefully. They both are no OOD, but they are very different under our theory.", "description": "This table compares two methods, Method 1 and Method 2, designed to avoid out-of-distribution (OOD) issues in the context of extending the context length of Language Models.  Despite both avoiding OOD, their performance on the Long-eval task differs significantly, highlighting that avoiding OOD alone doesn't guarantee good long-context capabilities.  Method 2, while technically avoiding OOD, shows substantially worse performance in the Long-eval metric (0.00 vs 0.27 at 30k context length).  The number of m values (relative distances) where Bm,\u03b8 (a function representing the ability to attend to similar tokens over random ones) is less than or equal to 0 is much higher for Method 2, indicating a failure to maintain the desired attention properties at longer distances.", "section": "5.5 OOD theory is insufficient to reveal long context capability"}, {"figure_path": "EiIelh2t7S/tables/tables_13_1.jpg", "caption": "Table 4: Training hyper-parameters in our experiments", "description": "This table lists the hyperparameters used in the experiments described in the paper.  It shows the training length, training tokens, batch size, base learning rate, learning rate decay method, and weight decay for three different models: Llama2-7B-Base, Baichuan2-7B-Base, and a 2B model trained from scratch by the authors.  The differences in hyperparameters reflect the different training approaches used for each model.", "section": "5.1 Experiments Setup"}, {"figure_path": "EiIelh2t7S/tables/tables_17_1.jpg", "caption": "Table 5: Evaluation results on RULER. We finetune Llama2-7b to 32k context length (the low bound base is 6e5) using different RoPE's bases. NS is short for NIAH-single and NM is short for NIAH-Multikey.", "description": "This table presents the evaluation results of the RULER benchmark.  Llama2-7B is fine-tuned to a context length of 32k using various RoPE base values (the lowest bound is 6e5). The results are broken down by subtasks (NS-1 through NS-3, NM-1 through NM-3, NIAH_Multivalue, NIAH_Multiquery, VT, CWE, FWE, QA1, QA2), showing the performance with different RoPE bases for each subtask and context length.", "section": "5 Experiment"}]