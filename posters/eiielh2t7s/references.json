{"references": [{"fullname_first_author": "Xin Men", "paper_title": "Base of ROPE Bounds Context Length", "publication_date": "2024-05-14", "reason": "This is the current paper, and all other references are cited within it."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduces the Transformer architecture, the foundation of many LLMs including those discussed in the current paper."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-00-00", "reason": "This paper introduces RoPE (Rotary Position Embedding), a key component of the LLMs examined in the current work."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "This paper introduces the Llama language model family, a central subject of the current paper's experiments and analysis."}, {"fullname_first_author": "Aiyuan Yang", "paper_title": "Baichuan 2: Open large-scale language models", "publication_date": "2023-09-10", "reason": "This paper introduces the Baichuan language model family, another central subject of the current paper's experiments and analysis."}]}