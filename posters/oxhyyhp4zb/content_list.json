[{"type": "text", "text": "SparseLLM: Towards Global Pruning of Pre-trained Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guangji Bai1 Yijiang Li2 Chen Ling1 Kibaek Kim2 Liang Zhao1,\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Emory University, Atlanta, GA, USA 2Argonne National Laboratory, Lemont, IL, USA \u2217Corresponding Author {guangji.bai,chen.ling,liang.zhao}@emory.edu {yijiang.li,kimk}@anl.gov ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM\u2019s approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes, surpassing current state-of-the-art methods. Our source code is publicly available at https://github.com/BaiTheBest/SparseLLM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) [1, 2] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks [3, 4, 5]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs [6, 7], including pruning, quantization, knowledge distillation, and low-rank factorization. Pruning, by introducing sparsity, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy. ", "page_idx": 0}, {"type": "text", "text": "Model pruning has a long history [8] and has proven effective in applications related to vision and smaller language models [9]. However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU [10, 11], become impractical for today\u2019s LLMs due to their vast size. Recently, several local pruning methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by \u201cstitching together\u201d the individually compressed layers. SparseGPT [12], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to $60\\%$ with minimal performance loss. Another approach, Wanda [13], introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations. Despite its efficiency gains, local pruning only aims to minimize the local error for each specific layer under sparsity constraints, resulting in a suboptimal solution for the overall model. ", "page_idx": 0}, {"type": "text", "text": "This is because local pruning over-aligns the intermediate layers\u2019 activations, leading to suboptimal performance, especially in high-sparsity regimes [11, 14]. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges and achieve global pruning with low memory consumption, we propose SparseLLM that decomposes the global pruning objective into multiple subproblems, each of which can be solved with low resources and coordinate to achieve the global pruning objective. More specifically, we first formulate LLMs as a composite function where the output of one module is the input of the next. Based on this formulation, we reformulate the global pruning goal into an equivalent form with auxiliary variables that facilitate its decomposition and coordination of the subproblems. Then we propose an alternating optimization algorithm to efficiently solve the subproblems, achieving computational resource efficiency and global optimality, due to the close-form solution of each subproblem. Empirically, we find that SparseLLM can consistently improve the performance of local pruning methods, particularly in high sparsity regimes $(>60\\%)$ , where the perplexity can be significantly decreased by up to around $80\\%$ as compared to the state-of-the-art methods. ", "page_idx": 1}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/f89440f7319d78deecbb7dbf3491c620b334f87b90e5944a6d246b6e41e5d173.jpg", "img_caption": ["LLM too large to fit Only consider local Global pruning with in a single machine! error for pruning! low memory cost! ", "Figure 1: SparseLLM decomposes the global pruning of LLMs into manageable subproblems by leveraging the chain of modules and auxiliary variables while maintaining dependencies. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Furthermore, our SparseLLM framework can be readily applicable to enhance the performance of most existing local pruning solvers, such as SparseGPT and Wanda, with marginal additional computational overhead. This adaptability ensures that our framework can be seamlessly integrated into a wide range of LLMs and pruning methods, making it a versatile tool and useful baseline for future research exploiting the sparsity of LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Pruning, a pivotal concept in machine learning that introduces sparsity into neural networks, dates back to the 1980s [8]. It gained renewed attention in the late 2010s, especially for deep neural networks, under the banner of reducing inference costs [15]. LLM pruning techniques can broadly be categorized into structured and unstructured prunings. ", "page_idx": 1}, {"type": "text", "text": "Unstructured pruning [16, 17] looks at simplifying the complexity of LLMs by removing certain parameters regardless of the model\u2019s inherent structure. This approach typically involves setting a threshold to nullify parameters below it, leading to a model with a non-uniform sparse structure. SparseGPT [12], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves up to $60\\%$ parameter reduction with minimal performance loss. A novel pruning criterion is introduced in Wanda [13], which evaluates weights by considering both magnitude and related input activations. This approach is beneficial in linear layers of LLMs, helping to identify and remove less significant weights. Tuli and Jha [18] proposed DynaTran, a dynamic inference scheme for pruning activations at runtime, supported by a specially designed ASIC architecture, AccelTran, to enhance transformer inference throughput. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, structured pruning involves the selective removal of groups of weights, where \u201cgroup\u201d might mean blocks of weights, filters, attention heads, or other structures conducive to hardware acceleration. Ma et al. [19] introduced the LLM-Pruner, a framework designed for structured pruning of LLMs, which utilizes a combination of first-order data and Hessian information for effective importance estimation. This aids in identifying crucial groups for pruning. Li et al. [20] proposed LoSparse, a novel approach combining low-rank and sparse matrix approximations to balance pruning and expressive power. Tao et al. [21] extended this concept to pruning hidden dimensions in LLMs, including embedding layers and attention heads. ZipLM [22], a structured pruning method for LLMs, is proposed to optimize for compression and accuracy while considering specific hardware constraints. More recently, Xia et al introduced LLM-shearing [23], a structured pruning method that scales down LLaMA models by selectively pruning layers, heads, and dimensions. ", "page_idx": 1}, {"type": "text", "text": "This approach, combined with dynamic data batching, reduces pre-training compute costs while maintaining competitive performance, outperforming similar open-source models on key tasks. ", "page_idx": 2}, {"type": "text", "text": "Our work falls in the category of unstructured pruning of LLMs, where existing methods such as SparseGPT and Wanda only consider an entirely local pruning algorithm and suffer from suboptimal performance. We discuss the limitations and challenges of entirely local pruning in Sec. 3. ", "page_idx": 2}, {"type": "text", "text": "3 Background and notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Global pruning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a pre-trained neural network $f$ with parameter W and inputs $\\mathbf{X}$ , global pruning aims to find a global sparsity mask $\\mathbf{M}$ and possibly updated weightsW to minimize the global loss $\\mathcal{L}$ between the final outputs of the uncompressed and compressed model: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{M},\\widehat{\\mathbf{W}}}\\;\\;\\mathcal{L}\\big(f(\\mathbf{X};\\mathbf{M}\\odot\\widehat{\\mathbf{W}}),f(\\mathbf{X};\\mathbf{W})\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\odot$ denotes the element-wise multiplication. In addition to NP-hardness [24], however, a critical challenge in solving Eq. 1 is the huge memory cost, as one needs to store the entire model in a single GPU, rendering this method impractical for modern billion-scale LLMs. ", "page_idx": 2}, {"type": "text", "text": "3.2 Local pruning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Local pruning circumvents the memory issue mentioned above by dividing the full model compression into subproblems for each layer and constructing a local loss to measure the $\\ell_{2}$ -error between the outputs of the uncompressed and compressed layers. Hence, the local pruning can be formulated by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{M}_{\\ell},\\widehat{\\mathbf{W}}_{\\ell}}\\|\\mathbf{W}_{\\ell}\\cdot\\mathbf{X}_{\\ell}-\\left(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell}\\right)\\cdot\\mathbf{X}_{\\ell}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Although smaller than the global pruning, the local pruning still needs to optimize both the mask ${\\bf M}_{\\ell}$ and the remaining weights $\\widehat{\\mathbf{W}}_{\\ell}$ and thus remains NP-hard. Therefore, exactly solving it for larger layers is unrealistic, leading all existing methods to resort to approximations. ", "page_idx": 2}, {"type": "text", "text": "Mask selection $\\pmb{\\&}$ weight reconstruction. A particularly popular approach is to separate the problem into mask selection and weight reconstruction [25, 26]. Concretely, this means first choosing a pruning mask M according to some salient criterion, like the weight magnitude [27], and then optimizing the remaining unpruned weights while keeping the mask unchanged. Importantly, once the mask is fixed, Eq. 2 turns into a linear regression problem that can be easily optimized. ", "page_idx": 2}, {"type": "text", "text": "Existing solvers. Early work [28] applied iterated linear regression to small networks. Recently, the AdaPrune approach [25] has shown good results for this problem on modern models via magnitudebased weight selection, followed by applying SGD steps to reconstruct the remaining weights. Followup works demonstrate that pruning accuracy can be further improved by removing the strict separation between mask selection and weight reconstruction. More recently, [12] developed SparseGPT, an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieving up to $60\\%$ parameter reduction with minimal performance loss. [13] introduced a novel pruning criterion in Wanda, which evaluates weights by considering both magnitude and related input activations. ", "page_idx": 2}, {"type": "text", "text": "3.3 What is wrong with local pruning? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Eq. 2, local pruning focuses on minimizing the error for each specific layer $\\ell$ subject to sparsity constraints. This results in a suboptimal solution with respect to the global pruning problem. While the primary goal of pruning is to ensure that the input and output of the pruned model align closely with those of the original models, the local pruning overly constrains the activations of all the intermediate layers between the two models, leading to performance degradation. ", "page_idx": 2}, {"type": "text", "text": "4 SparseLLM: Towards global pruning for LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present our proposed method SparseLLM that can address the drawbacks of existing pruning methods by achieving a global pruning with low memory consumption. SparseLLM decomposes the global pruning objective into many subproblems, each of which can be solved using low resources and can coordinate each other toward the global pruning objective. An overview of SparseLLM on the OPT and LlaMA configurations are shown in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "4.1 Motivation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The development of SparseLLM is motivated by the observation: LLMs can be formulated as a composite function such that the output of one module is the input of the next. This allows us to reformulate the global pruning goal into its equivalent form with auxiliary variables that enable the decomposition into multiple subproblems, as detailed in Sec. 4.2. Then we develop a resource-efficient algorithm that achieves the alternating optimization of the subproblems with global optimality, thanks to the close-form solution of each subproblem, as illustrated in Sec. 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.2 A unified formulation of pruning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the reformulation of the global pruning problem into an equivalent one by introducing auxiliary variables. This reformulation provides a more flexible form and enables the decomposition of the problem into many manageable subproblems. ", "page_idx": 3}, {"type": "text", "text": "The key idea behind our formulation is to decouple the densely parametric parts (linear layers) from non-parametric parts (activation function, self-attention, layer norm, etc) using a splitting technique. Rather than feeding the output of the dense linear layer $\\mathbf{W}_{\\ell}$ directly into the non-parametric and potentially nonlinear layer $\\phi_{\\ell}$ , we store the output of layer $\\ell$ in a new variable $\\mathbf{z}_{\\ell}=\\mathbf{\\bar{W}}_{\\ell}\\mathbf{a}_{\\ell-1}\\mathbf{\\Lambda}^{1}$ . We also represent the output of the non-parametric layer as a vector of activations $\\begin{array}{r}{\\mathbf{a}_{\\ell}=\\phi_{\\ell}(\\mathbf{z}_{\\ell})}\\end{array}$ . We then solve the following problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}_{\\{\\widehat{\\mathbf{W}}_{\\ell}\\},\\{\\mathbf{M}_{\\ell}\\},\\{\\mathbf{a}_{\\ell}\\}}\\qquad\\mathcal{L}(\\mathbf{z}_{L},\\mathbf{y}),}\\\\ {\\mathrm{s.t.}\\ \\ \\mathbf{z}_{\\ell}=(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1},\\ \\forall\\ \\ell\\in[L],}\\\\ &{\\mathbf{a}_{\\ell}=\\phi_{\\ell}(\\mathbf{z}_{\\ell}),\\ \\forall\\ \\ell\\in\\Omega,}\\\\ &{\\mathbf{a}_{\\ell},\\mathbf{z}_{\\ell}=\\mathbf{a}_{\\ell}^{p r e},\\mathbf{z}_{\\ell}^{p r e},\\ \\forall\\ \\ell\\in[L-1]\\backslash\\Omega,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L$ represents the total number of dense (linear) layers and $[L]=\\{1,2,\\cdots\\,,L\\}$ . $[L-1]\\backslash\\Omega$ denotes the complement set of $\\Omega$ . We use ${\\pmb a}_{\\ell}^{p r e},\\;{\\bf z}_{\\ell}^{p r e}$ to denote the corresponding intermediate variables\u2019 values of the original dense (i.e., without pruning) pre-trained model. $y$ denotes the ground-truth final output of the dense pre-trained model. ", "page_idx": 3}, {"type": "text", "text": "In our proposed formulation above, its unified nature lies in the interpretation and application of the set $\\Omega$ , which denotes the indices of layers subject to the pruning process. Intuitively, $\\Omega$ measures how \u201cglobal\u201d the pruning is. The bigger the set of $\\Omega$ is, the more layers are connected via the second constraint, and the pruning is more towards the global extreme, and vice versa. The generality and versatility of our formulation is illustrated in the following remark: ", "page_idx": 3}, {"type": "text", "text": "Remark 4.1 (Generality and flexibility of Eq. 3). Given an LLM formulated as a composite function with dense layers $l\\in\\{1,2,\\dots,L-\\mathrm{i}\\}$ , where $L$ is the total number of dense layers and $\\Omega$ denotes the set of layers subject to the pruning process. Our formulation can seamlessly treat both global and local pruning as special cases under certain conditions. Specifically: ", "page_idx": 3}, {"type": "text", "text": "\u2022 When $\\Omega=\\{1,2,\\ldots,L-1\\}$ , solving our pruning formulation is equivalent to global pruning, accounting for inter-layer dependencies across the entire network. \u2022 When $\\Omega=\\emptyset$ , the formulation simplifies to local pruning, considering each layer independently (the last constraint dominates and \u201ccuts\u201d all layer dependencies with pre-trained values.) ", "page_idx": 3}, {"type": "text", "text": "The ability to shift between these two extremes, and potentially any intermediate configurations, demonstrates the flexibility and comprehensiveness of our formulation. By adjusting $\\Omega$ , one can seamlessly transition from a global perspective to a local perspective. This flexibility not only caters to a wide range of pruning strategies but also provides a unified framework to compare and contrast the effectiveness of different pruning methods under a consistent mathematical lens. ", "page_idx": 3}, {"type": "text", "text": "4.3 Algorithm design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the algorithm design of SparseLLM, which alternatively optimizes the subproblems associated with the corresponding variables. This approach is resource-efficient and achieves global optimality, attributed to the closed-form solutions that each subproblem yields. ", "page_idx": 3}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/8c21120780d65f035f731b3377da7dc4ebe08b2f7ee2e0cb2d61ac388b570681.jpg", "img_caption": ["Figure 2: Illustration of SparseLLM on OPT and LlaMA. The auxiliary variables and soft constraints (i.e., $\\approx$ ) allow SparseLLM to decompose the global pruning into manageable subproblems while maintaining the dependencies. Subproblems are analytically solvable and enjoy fast convergence. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The key idea of our algorithm lies behind the flexibility of $\\Omega$ in our Eq. 3, as we want to find a better trade-off between completely global (memory bottleneck) and completely local (suboptimal performance) pruning. Naively applying SparseLLM to prune all layers globally is impractical. On the other hand, recent work shows that the feed-forward network (FFN) module in each decoder layer accounts for more than two-thirds of the total parameters in an LLM [29]. Therefore, our SparseLLM prioritizes the global pruning of the FFN module, while still adhering to a local pruning strategy for the multi-head attention (MHA) module (see Figure 2). This strategy strikes a balance between the computational feasibility of pruning large-scale models and the effectiveness of the pruning process, adhering to the limitations and practices of state-of-the-art LLM pruning frameworks. ", "page_idx": 4}, {"type": "text", "text": "Formally speaking, rather than trying to solve Eq. 3 directly, we first relax the constraints by adding an $\\ell_{2}$ -penalty function to the objective and attack the unconstrained problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{z}_{L},\\mathbf{y})+\\alpha\\sum_{\\ell\\in[L]}\\|\\mathbf{z}_{\\ell}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1}\\|_{2}^{2}+\\beta\\sum_{\\ell\\in\\Omega_{\\mathrm{FW}}}\\|\\mathbf{a}_{\\ell}-\\phi_{\\ell}(\\mathbf{z}_{\\ell})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha,\\beta$ are hyperparameters for controlling the weight of each constraint. $\\Omega_{\\mathrm{FFN}}$ denotes the set of indexes for the linear layers in the FFN module of each decoder layer, i.e., linear layers from the same FFN module are pruned globally. For simplicity, the superscript \u201cpre\u201d of $\\mathbf{\\delta}_{a_{\\ell}}$ and $\\mathbf{z}_{\\ell}$ in the third constraint in Eq. 3 is omitted here, i.e., for $\\ell\\notin\\Omega_{\\mathrm{FFN}}$ the $\\mathbf{\\delta}_{a_{\\ell}}$ and $\\mathbf{z}_{\\ell}$ are fixed and equal to the pre-trained model\u2019s intermediate value in the second term of Eq. 4. In the following subsections, we illustrate how we approach the pruning of FFN and MHA modules, respectively. ", "page_idx": 4}, {"type": "text", "text": "4.3.1 SparseLLM on OPT models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each decoder layer in a pre-trained LLM, our Eq. 4 instantly simplifies to globally pruning the corresponding FFN module within that decoder layer as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\|\\mathbf{z}_{\\ell+1}^{p r e}-(\\mathbf{M}_{\\ell+1}\\odot\\widehat{\\mathbf{W}}_{\\ell+1})\\alpha_{\\ell}\\|_{2}^{2}+\\beta\\|\\mathbf{a}_{\\ell}-\\phi_{\\ell}(\\mathbf{z}_{\\ell})\\|_{2}^{2}+\\alpha\\|\\mathbf{z}_{\\ell}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1}^{p r e}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where layers $\\ell$ and $\\ell+1$ correspond to the up-projection and down-projection linear layers. ", "page_idx": 4}, {"type": "text", "text": "In this work, we consider the alternating method to optimize our Eq. 5, i.e., optimize each variable while keeping the rest fixed. The careful and elaborate design of our Eq. 5 allows us to derive a closed-form solution to every subproblem as shown below. ", "page_idx": 4}, {"type": "text", "text": "Pruning weight. First consider optimizing Eq. 5 with respect to ${\\bf M}_{\\ell}$ and $\\widehat{\\mathbf{W}}_{\\ell}$ . For each linear layer $\\ell$ in a FFN module, the optimal solution minimizes ${\\|\\mathbf{z}_{\\ell}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell}})\\mathbf{a}_{\\ell-1}{\\|}_{2}^{2}$ . To solve it, the first step is to decompose $\\mathbf{z}_{\\ell}$ to $\\mathbf{W}_{\\ell}\\pmb{a}_{\\ell-1}$ , where $\\mathbf{W}_{\\ell}\\,=\\,\\mathbf{z}_{\\ell}\\mathbf{a}_{\\ell-1}^{\\dagger}$ ( $\\mathrm{\\ddot{~}}$ denotes the pseudo-inverse.) Plug decomposed $\\mathbf{z}_{\\ell}$ back in original loss and we get ${\\|\\mathbf{W}_{\\ell}\\mathbf{a}_{\\ell-1}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell}})\\mathbf{a}_{\\ell-1}}\\|_{2}^{2}$ , which aligns with the pruning objective of Eq. 2 and can be analytically solved by existing pruning solver e.g., SparseGPT. The superscript of \u201cpre\u201d for $\\mathbf{\\nabla}a_{\\ell-1}$ is omitted in this section for simpler notation. ", "page_idx": 4}, {"type": "text", "text": "Updating activation. Minimization for $\\mathbf{\\delta}_{a_{\\ell}}$ is a simple least-squares problem similar to weight pruning. However, in this case, the matrix $\\mathbf{\\nabla}a_{\\ell-1}$ appears in two penalty terms in Eq. 5, so we must minimize $\\begin{array}{r}{\\alpha\\|\\mathbf{z}_{\\ell+1}^{p r e}-(\\mathbf{M}_{\\ell+1}\\odot\\widehat{\\mathbf{W}}_{\\ell+1})\\pmb{a}_{\\ell}\\|_{2}^{2}+\\beta\\|\\mathbf{a}_{\\ell}-\\phi_{\\ell}(\\mathbf{z}_{\\ell})\\|_{2}^{2}}\\end{array}$ for $\\mathbf{\\delta}_{a_{\\ell}}$ , holding all other variables fixed. By following a very similar idea to Ridge regression, the new value of $\\mathbf{\\delta}_{a_{\\ell}}$ is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\alpha\\mathbf{W}_{\\ell+1}^{\\mathsf{T}}\\mathbf{W}_{\\ell+1}+\\beta\\mathbf{I})^{-1}(\\alpha\\mathbf{W}_{\\ell+1}^{\\mathsf{T}}\\mathbf{z}_{\\ell+1}^{p r e}+\\beta\\cdot\\mathrm{ReLU}(\\mathbf{z}_{\\ell})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{W}_{\\ell}$ denotes the updated weight matrix after pruning, i.e., $\\mathbf{W}_{\\ell}:=\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell}$ . ", "page_idx": 5}, {"type": "text", "text": "Updating output. The update for $\\mathbf{z}_{\\ell}$ requires minimizing the following loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\|\\boldsymbol{a}_{\\ell}-\\mathrm{ReLU}(\\mathbf{z}_{\\ell})\\|_{2}^{2}+\\alpha\\|\\mathbf{z}_{\\ell}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1}^{p r e}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This problem is non-convex and non-quadratic (because of the non-linear function ReLU). Fortunately, because the ReLU function works entry-wise on its argument, the entries in $\\mathbf{z}_{\\ell}$ are de-coupled. Solving Eq. 7 is particularly easy for the case of ReLU, as it can be solved in closed form followed by a simple if-then logic. Specifically, one only needs to compute two solutions of a quadratic equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{z}_{\\ell}^{(1)}=(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1}^{p r e},\\quad\\mathbf{z}_{\\ell}^{(2)}=(\\alpha+\\beta)^{-1}\\cdot\\bigl(\\beta\\mathbf{a}_{\\ell}+\\alpha\\mathbf{z}_{\\ell}^{(1)}\\bigr),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the first solution corresponds to those entries of $\\mathbf{z}_{\\ell}$ that are negative (reduced to zero by ReLU), and the second solution corresponds to those entries of $\\mathbf{z}_{\\ell}$ that are non-negative. ", "page_idx": 5}, {"type": "text", "text": "4.3.2 SparseLLM on LlaMA models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce how SparseLLM decomposes global pruning into subproblems and solves them iteratively on LlaMA model families. The model architecture of LlaMA can be found in Figure 2. Overall, SparseLLM operates similarly on both LlaMA and OPT models, with the main difference being that LlaMA includes an additional dense linear layer, known as the gate projection layer, and uses the SiLU activation function instead of ReLU. ", "page_idx": 5}, {"type": "text", "text": "Pruning weight. In this part, SparseLLM functions almost identically to its operation on OPTs. ", "page_idx": 5}, {"type": "text", "text": "Updating activation ${\\bf a}_{\\ell}$ . Similarly, for updating $\\mathbf{\\delta}_{a_{\\ell}}$ , SparseLLM works nearly the same as on OPT. The minimization for $\\mathbf{\\delta}_{a_{\\ell}}$ is a simple least-squares problem, akin to weight pruning. However, in this case, the matrix $\\mathbf{a}_{\\ell-1}$ appears in two penalty terms in Eq. 5, necessitating the minimization of: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\|\\mathbf{z}_{\\ell+1}^{p r e}-\\big(\\mathbf{M}_{\\ell+1}\\odot\\widehat{\\mathbf{W}}_{\\ell+1}\\big)\\pmb{a}_{\\ell}\\|_{2}^{2}+\\beta\\|\\pmb{a}_{\\ell}-\\mathrm{SiLU}(\\mathbf{s}_{\\ell})\\odot\\mathbf{z}_{\\ell}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $\\mathbf{\\delta}_{a_{\\ell}}$ , with all other variables held fixed. Following a concept similar to Ridge regression, the updated value of $\\mathbf{\\delta}_{a_{\\ell}}$ is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\big(\\alpha\\mathbf{W}_{\\ell+1}^{\\intercal}\\mathbf{W}_{\\ell+1}+\\beta\\mathbf{I}\\big)^{-1}\\big(\\alpha\\mathbf{W}_{\\ell+1}^{\\intercal}\\mathbf{z}_{\\ell+1}^{p r e}+\\beta\\cdot\\mathrm{SiLU}(\\mathbf{s}_{\\ell})\\odot\\mathbf{z}_{\\ell}\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{W}_{\\ell}$ denotes the updated weight matrix after pruning, i.e., $\\mathbf{W}_{\\ell}:=\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell}.$ ", "page_idx": 5}, {"type": "text", "text": "Updating output $\\mathbf{z}_{\\ell}$ . Updating $\\mathbf{z}_{\\ell}$ is somewhat simpler in LlaMA since the activation function applies over the gate projection layer. The update requires minimizing the loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\|\\pmb{a}_{\\ell}-\\mathrm{SiLU}(\\mathbf{s}_{\\ell})\\odot\\mathbf{z}_{\\ell}\\|_{2}^{2}+\\alpha\\|\\mathbf{z}_{\\ell}-(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\pmb{a}_{\\ell-1}^{p r e}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This problem is quadratic when solving for $\\mathbf{z}_{\\ell}$ with other variables fixed. Through mathematical manipulations, the analytical solution for $\\mathbf{z}_{\\ell}$ is found by solving a quadratic equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{z}_{\\ell}^{*}=\\frac{(\\mathbf{M}_{\\ell}\\odot\\widehat{\\mathbf{W}}_{\\ell})\\mathbf{a}_{\\ell-1}^{p r e}+\\mathrm{SiLU}(\\mathbf{s}_{\\ell})\\odot\\mathbf{a}_{\\ell}}{\\mathrm{SiLU}(\\mathbf{s}_{\\ell})^{2}+\\mathbf{1}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the division is element-wise and 1 denotes the all-one matrix. ", "page_idx": 5}, {"type": "text", "text": "Updating gate projection output $\\mathbf{s}_{\\ell}$ . Updating $\\mathbf{s}_{\\ell}$ involves minimizing: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\|\\boldsymbol{a}_{\\ell}-\\mathrm{SiLU}(\\mathbf{s}_{\\ell})\\odot\\mathbf{z}_{\\ell}\\|_{2}^{2}+\\alpha\\|\\mathbf{s}_{\\ell}-\\big(\\mathbf{M}_{s}\\odot\\widehat{\\mathbf{W}}_{s}\\big)\\boldsymbol{a}_{\\ell-1}^{p r e}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{M}_{s}$ and $\\widehat{\\mathbf{W}}_{s}$ denote the mask and layer weights for the gate projection layer. This problem is non-convex and non-quadratic due to the non-linear SiLU function. However, since SiLU operates entry-wise, the entries in $\\mathbf{s}_{\\ell}$ are decoupled. Despite LlaMA lacking a simple closed-form solution as in OPT (which uses ReLU), the problem can still be solved quickly and analytically using a lookup table of pre-computed solutions, since each element in $\\mathbf{s}_{\\ell}$ depends on only three variables. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.2 (Global convergence of SparseLLM). Consider the objective function given by Eq. 5, under the condition that the activation function $\\phi$ is ReLU. Notice that (1) the objective function is convex with respect to each variable when all others are fixed, and (2) given that closed-form solutions exist for the subproblems in the alternating optimization scheme, the proposed algorithm resembles multiblock ADMM which has been shown to converge to in many applications. ", "page_idx": 6}, {"type": "text", "text": "4.3.3 Pruning of MHAs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "SparseLLM also prunes other linear layers besides those in FFNs. By following Eq. 4, for each linear layer out of FFN modules, the pruning objective simplifies to $\\alpha\\|\\mathbf{z}_{\\ell+1}^{p r e}-(\\mathbf{M}_{\\ell+1}\\odot\\widehat{\\mathbf{W}}_{\\ell+1})\\mathbf{a}_{\\ell}^{p r e}\\|_{2}^{2}$ , which is equivalent (with some simple math) to that of completely local pruning as shown in Eq. 2. Existing LLM pruning solvers such as SparseGPT and Wanda are applicable here. ", "page_idx": 6}, {"type": "text", "text": "4.4 Time complexity analyses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proposed SparseLLM consists of three main steps, with the overall time complexity being the sum of the complexities of these steps. In the weights pruning step, the complexity is dominated by the pseudo-inverse computation of matrix $\\mathbf{\\delta}_{a_{\\ell}}$ (dimensions $n\\times h)$ , which is $O\\dot{(}n h^{2})$ . Using SparseGPT as the solver, the exact pruning step has a complexity of $O(h^{3})$ . The second step, updating activations, involves matrix inversion of the weight matrix $\\mathbf{W}_{\\ell}$ (size $h\\times h)$ with a complexity of $O(\\bar{h^{3}})$ . The third step, updating outputs, has a lower complexity. Thus, the overall algorithm complexity is bounded by $\\bar{O(h^{3})}$ , therefore making our method\u2019s per-epoch time complexity comparable to SparseGPT. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experiments setup. We implemented SparseLLM in PyTorch [30] and use the HuggingFace Transformers library [31] for handling models and datasets. All pruning experiments are conducted on NVIDIA A100 GPUs. For calibration data, we follow [12] and use 128 2048-token segments, randomly chosen from the first shard of the C4 [32] dataset. This represents generic text data crawled from the internet and ensures our experiments are zero-shot as no task-specific data is seen during pruning. We followed existing work [12, 13] and pruned all linear layers (in FFN and MHA) to the target sparsity. ", "page_idx": 6}, {"type": "text", "text": "Models, datasets & evaluation. We consider the OPT model family [33] and LlaMA-2 model family [1] in our experiments as well as the most recent LlaMA-3 model. We show results on different sizes of models to provide a broader picture for the performances of SparseLLM. In terms of metrics, we mainly focus on perplexity, which is known to be a challenging and stable metric that is well-suited for evaluating the accuracy of compression methods [34, 35]. We consider the test sets of raw-WikiText2 [36] (WT2) and PTB [37] as well as a subset of the C4 validation data, all popular benchmarks in LLM compression literature [34, 38, 12, 13]. For additional interpretability, we also provide zero-shot accuracy results following the same setup of [13], which is based on the popular EleutherAI-eval harness [39]. ", "page_idx": 6}, {"type": "text", "text": "Comparison methods. We compare against three baselines, magnitude pruning [27] applied locally, and two other state-of-the-art local pruning methods, SparseGPT [12] and Wanda [13]. ", "page_idx": 6}, {"type": "text", "text": "5.1 Results and analyses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Pruning vs. model sizes. We begin by exploring the pruning capabilities of SparseLLM across various model sizes in comparison to baseline methods. For each model, we consider unstructured sparsity ranging from $70\\%$ to $90\\%$ with a $10\\%$ increment, as well as a 3:4 semi-structured sparsity. The 3:4 semi-structured sparsity is inspired by our preliminary results that suggest good performance SparseLLM at high sparsity regimes. However, note that two of our baselines, Magnitude and Wanda, are unable to be configured to this sparsity out-of-box. We conduct a sensitivity study on the calibration sample sizes (see Appendix A.3) and use calibration sample sizes between 32 and 64 for all experiments. Moreover, we prune the first $50\\%$ of the Transformer decoder layers in each model to achieve a balance between the computation resources and the performances. Detailed results can be found in Table 1 and Table 2 as well as Table 8 in Appendix A.5. Note that in Table 2 for LlaMA-3 model, we only compare SparseGPT to the proposed SparseLLM. The perplexity results of the dense models are reported next to the names of the models. ", "page_idx": 6}, {"type": "text", "text": "From the tables, it shows a general trend of increasing perplexity with increasing sparsity. Moreover, we observe a trend of decreasing perplexity for SparseGPT and SparseLLM at the same sparsity with increasing model sizes. However, such a trend is not obvious for Magnitude and Wanda. We also observe that SparseGPT and SparseLLM consistently outperform Magnitude and Wanda by a significant margin. For smaller sparsity, SparseLLM achieves comparable perplexity to SparseGPT. As we increase the sparsity, SparseLLM starts to demonstrate noticeable improvements over SparseGPT. In numerous instances for the OPT model family, SparseLLM achieves perplexity reductions of more than $50\\%$ compared to SparseGPT. We also see that performance improvements from SparseLLM over SparseGPT are more significant for the OPT model family than the LlaMA-2 model family. ", "page_idx": 6}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/c2f6f33a305e88772c85dfa80b8fd5b2a880210f968d0b60b80f64311454a7fa.jpg", "table_caption": ["Table 1: Perplexity of OPT models for sparsity $\\ge70\\%$ ; the lower the perplexity, the better. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/d12bec1e6c488e7ff1c1845c853aa8300d42e7c9c2083da8b3b6447e8e04ecc1.jpg", "table_caption": ["Table 2: Perplexity of LlaMA models for sparsity $\\ge70\\%$ ; the lower the perplexity, the better. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/496035e229c315a5363407c3782494805448857ea70f520e9208ec64b4fcf009.jpg", "table_caption": ["Table 3: Perplexity of 2:4 sparsity; the lower the perplexity, the better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We provide additional set of perplexity results for a 2:4 semi-structured sparsity for a few OPT models in Table 3. We see that SparseLLM and SparseGPT generally outperform Magnitude and Wanda while SparseLLM has comparable if not slightly better performances compared to SparseGPT with the 2:4 semi-structured sparsity. Note that a 2:4 semi-structure sparsity is considered to be in low sparsity regime. ", "page_idx": 8}, {"type": "text", "text": "Zero-shot experiments. To further conclude the evaluations and discussions, we show results for several zeroshot tasks in Table 4 and Table 5 as well as Table 9 in Appendix A.5, comparing SparseGPT and SparseLLM. These evaluations are known to be relatively noisy [40], but more interpretable. We also report the results for zero-shot tasks from the dense models in the \u201cDense\" row. We see that the accuracy of both methods decreases with increasing sparsity, which is expected, as more parameters are pruned. A similar trend of increasing accuracy ", "page_idx": 8}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/eaf95f20703c8e8213709f2e52308d28c28d2cd4f47f24cc3891564fef4fe22a.jpg", "img_caption": ["Figure 3: Fast convergence of SparseLLM. Training loss per epoch for pruning layer 3 of OPT- $125\\mathrm{m}$ at $80\\%$ sparsity (Left) and layer 6 of LlaMA-2 13b at $70\\%$ sparsity (Right). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "with increasing model size is observed too. Across all the tasks, OBQA and ARC-c remain the most challenging ones as the accuracy for both methods is $30\\%$ or below $30\\%$ while both methods perform well for BoolQ, RTE, WinoGrande, and ARC-e. In general, SparseLLM is able to achieve higher accuracy in the majority of tasks across the models of different sizes in both OPT and LlaMA-2 model families. ", "page_idx": 8}, {"type": "text", "text": "Training loss vs. epochs in SparseLLM. Figure 3 illustrates the change in training loss over epochs for SparseLLM, with the training loss plotted on a scale of $10^{3}$ for clarity. We observe that the training loss decreases rapidly during the initial epochs, highlighting the efficiency of SparseLLM in achieving effective global pruning within a short period. This rapid convergence is largely due to the closed-form solutions employed by SparseLLM for various subproblems, which streamline the pruning process and ensure optimal layer-wise pruning without extensive iterative computations. These analytical solutions enable SparseLLM to perform precise pruning operations quickly, making it a powerful tool for optimizing large-scale models like LlaMA, significantly reducing model size while maintaining high accuracy. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work presents SparseLLM, a cutting-edge framework poised to redefine the compression of LLMs through sparsity. By adeptly circumventing the scalability issues of global pruning and optimizing the local suboptimality of existing methods, SparseLLM stands as a significant advancement in the field. Our empirical results affirm its efficacy, particularly in high-sparsity environments. It achieves a notable reduction in perplexity, thereby setting a new precedent for model compression. The versatility and minimal computational overhead of SparseLLM complement its integration with current pruning technologies, underscoring its potential as a universal tool for enhancing the performance and accessibility of LLMs. ", "page_idx": 8}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/b49c0ca83d75215277a9e13b16348c27edbbc4e8bfa342c3f647071aff6a5780.jpg", "table_caption": ["Table 4: Accuracy $(\\%)$ of zero-shot tasks for OPT models; the higher the accuracy, the better. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/520d199654b4b9d3e8c333660a77a6746f25f9c84c1a251eae553d5ccbd00cbb.jpg", "table_caption": ["Table 5: Accuracy $(\\%)$ of zero-shot tasks for LlaMA models; the higher the accuracy, the better. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Science Foundation (NSF) Grant No.1755850, No.1841520, No.1942594, No.2403312, No.2007716, No.2007976, No.1907805. This work was supported by the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research, under Contract DE-AC02-06CH11357. This research used resources of the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[2] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.   \n[3] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[4] Michael Bommarito II and Daniel Martin Katz. Gpt takes the bar exam. arXiv preprint arXiv:2212.14402, 2022.   \n[5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[6] Canwen Xu and Julian McAuley. A survey on model compression and acceleration for pretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10566\u201310575, 2023.   \n[7] Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. Beyond efficiency: A systematic survey of resource-efficient large language models. arXiv preprint arXiv:2401.00625, 2024.   \n[8] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \n[9] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. The Journal of Machine Learning Research, 22(1):10882\u201311005, 2021.   \n[10] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018.   \n[11] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. Advances in Neural Information Processing Systems, 33:18098\u201318109, 2020.   \n[12] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.   \n[13] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.   \n[14] Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal. Ecoflap: Efficient coarse-to-fine layer-wise pruning for vision-language models. arXiv preprint arXiv:2310.02998, 2023.   \n[15] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.   \n[16] Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, and Haifeng Chen. Pruning as a domain-specific llm extractor. arXiv preprint arXiv:2405.06275, 2024.   \n[17] Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, and Kibaek Kim. Fedspallm: Federated pruning of large language models. arXiv preprint arXiv:2410.14852, 2024.   \n[18] Shikhar Tuli and Niraj K Jha. Acceltran: A sparsity-aware accelerator for dynamic inference with transformers. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023.   \n[19] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627, 2023.   \n[20] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language models based on low-rank and sparse approximation. arXiv preprint arXiv:2306.11222, 2023.   \n[21] Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Structured pruning for efficient generative pre-trained language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10880\u201310895, 2023.   \n[22] Eldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Hardware-aware structured pruning of language models. arXiv preprint arXiv:2302.04089, 2023.   \n[23] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.   \n[24] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations. Journal of Fourier analysis and Applications, 14:629\u2013654, 2008.   \n[25] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks. Advances in neural information processing systems, 34:21099\u201321111, 2021.   \n[26] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. Advances in Neural Information Processing Systems, 35:24101\u201324116, 2022.   \n[27] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.   \n[28] Jason Kingdon and Jason Kingdon. Hypothesising neural nets. Intelligent Systems and Financial Forecasting, pages 81\u2013106, 1997.   \n[29] Zirui Liu, Qingquan Song, Qiang Charles Xiao, Sathiya Keerthi Selvaraj, Rahul Mazumder, Aman Gupta, and Xia Hu. Ffsplit: Split feed-forward network for optimizing accuracy-efficiency trade-off in language model inference. arXiv preprint arXiv:2401.04044, 2024.   \n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[31] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n[33] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[34] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168\u201327183, 2022.   \n[35] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pages 7750\u20137774. PMLR, 2023.   \n[36] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[37] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.   \n[38] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.   \n[39] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.   \n[40] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section includes supplemental materials (pseudo-code, additional experiments, and plots). ", "page_idx": 13}, {"type": "text", "text": "A.1 Pseudo-code of SparseLLM ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/788d3da18464a244b47c416b6ba2b9d9c7abf2bb9a3b7ee56b809085338ad288.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "The SparseLLM algorithm presented in Algorithm 1 demonstrates how SparseLLM works on an OPT decoder layer. The key inputs to the algorithm include the pre-trained weight matrices for both the up-scaling and down-scaling linear layers of the FFN, along with a set of hyperparameters and a sparsity constraint. The goal of SparseLLM is to achieve a targeted level of sparsity in the linear layers without significantly compromising the model\u2019s performance. ", "page_idx": 13}, {"type": "text", "text": "Initiating with the pre-trained weights, SparseLLM employs a series of pruning and activation update steps across $K$ iterations. In each iteration, it solves optimization problems to prune the current and subsequent layer weights, followed by updating the activation variables. The utilization of SparseGPT solvers for pruning and the strategic update of activations ensures that the pruned network approximates the original network\u2019s behavior as closely as possible. The final output of the algorithm is a pair of pruned weight matrices for the consecutive layers, which are expected to deliver comparable or improved performance with a reduced number of parameters. ", "page_idx": 13}, {"type": "text", "text": "A.2 Two-layer Demo on the Details behind our Global Pruning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 4 illustrates the SparseLLM pruning method compared to conventional global pruning and local pruning, using a two-layer neural network as an abstraction for simplicity. The figure is divided into three main parts: ", "page_idx": 13}, {"type": "text", "text": "On the left, conventional global pruning is depicted. This method applies a global mask to the entire network, resulting in significant memory costs due to poor scalability. Both functions $f_{1}$ and $f_{2}$ are pruned using the same mask across all layers, leading to high memory usage. ", "page_idx": 13}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/d88bd2e015c44ba563df992787840c9d23ef5eab0c4f6dcb3fba0f19ce8619a6.jpg", "img_caption": ["Figure 4: Illustration of SparseLLM pruning method compared to conventional global pruning and local pruning. We consider a two-layer neural network as an abstraction for simplicity. Global pruning (left) is memory prohibitive due to poor scalability. Local pruning (mid) considers pruning each layer independently, while inevitably sacrificing performance due to the ignorance of global supervision. Our adaptive global pruning (right) achieves global pruning with low memory cost by leveraging auxiliary variables and soft constraints. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In the middle, local pruning is shown, where each layer is pruned independently. This approach reduces memory costs by applying separate masks to each layer. However, it inevitably sacrifices performance because it ignores global supervision, which can lead to suboptimal pruning decisions that do not consider the network as a whole. ", "page_idx": 14}, {"type": "text", "text": "On the right, the adaptive global pruning method of SparseLLM is presented. This method achieves global pruning with low memory cost by leveraging auxiliary variables and soft constraints. It combines the benefits of global pruning\u2014considering the entire network structure\u2014with efficient memory usage. The introduction of auxiliary variables allows for flexible and adaptive pruning, ensuring that the overall performance of the network is maintained while keeping memory costs low. ", "page_idx": 14}, {"type": "text", "text": "Thus, the figure highlights the trade-offs between different pruning strategies. Conventional global pruning incurs high memory costs, local pruning reduces memory usage at the expense of performance, and the adaptive global pruning of SparseLLM strikes a balance by maintaining performance with lower memory requirements through the use of auxiliary variables and soft constraints. ", "page_idx": 14}, {"type": "text", "text": "A.3 Calibration Samples ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 5 and Figure 6 present how perplexity changes with the calibration sample sizes on the datasets PTB and C4 for OPT-2.7b and LlaMA-2 7b, respectively. In both figures, as the number of calibration samples increases, the perplexity decreases for both SparseGPT and SparseLLM. This indicates that having more calibration samples can be beneficial in the pruning process. Perplexity decreases more rapidly from 8 samples to 32 samples. Beyond 32 samples, the rate at which perplexity decreases starts to slow down. In addition, increasing the number of calibration samples requires more computational resources, e.g., memory and computation time, in the overall pruning process. This suggests that the calibration sample sizes should be between 32 and 64 to ensure good performance while maintaining computational efficiency. Lastly, the figures show that SparseLLM achieves better perplexity than SparseGPT does with 32 or larger sample sizes for both OPT and LlaMA-2 models. ", "page_idx": 14}, {"type": "text", "text": "A.4 Computation Time vs. Model Sizes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We study how the computation time per layer of SparseGPT and SparseLLM varies with different model sizes, as illustrated in Table 6 and Table 7 for OPT models and LlaMA-2 models. The rate at which the time taken increases is comparable for SparseGPT and SparseLLM as the model size increases. Additionally, computation time for SparseLLM are reported for a configuration of 4 to 10 epochs. As we have reported in Section 5, SparseLLM can reduce the training loss in as few as 2 to 3 epochs. This suggests that the proposed SparseLLM remains computationally efficient. ", "page_idx": 14}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/324fa80f1f1f16fb9fcb90caa59f4c3b733bb59e758008454fe5bd4bf690cbcf.jpg", "img_caption": ["Figure 5: Sensitivity of OPT-2.7b on the calibration sample sizes for datasets PTB and C4. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "oXHyYHp4Zb/tmp/9621a7e86709d71d0ae4c0510b128c9b0c8c1d5b98504c183013654ce50c5c98.jpg", "img_caption": ["Figure 6: Sensitivity of LlaMA-2 7b models on the calibration sample sizes for datasets PTB and C4. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/c2fcb034d626cefe6373ea712622a5034106083ac1507cd9cabb57c9b0e0ae7b.jpg", "table_caption": ["Table 6: Computation time in seconds of OPT models. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/563dfc9b3c418db9130d4d89aefd1eb42dd01b148cd2f96eeb15ba0b453503dd.jpg", "table_caption": ["Table 7: Computation time in seconds of LlaMA-2 models. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Experiment Results for Additional Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Detailed results on perplexity and zero-shot task accuracy for additional models are reported in Table 8 and Table 9. Similar to other models, we report the perplexity results for the dense model next to the name of the model in the table. In particular, we see that SparseGPT and SparseLLM outperform Magnitude and Wanda with a significant margin across different sparsity. SparseLLM shares similar perplexity with SparseGPT for smaller sparsity but demonstrates much better perplexity for larger sparsity. Similar perplexity trends are observed across all three datasets, although, PTB, having the highest perplexity for each sparsity and method, is likely the most challenging dataset among the three. For the zero-shot taks accuracy, we see that SparseLLM achieves comparable results to SparseGPT for smaller perplexity and the performance improvements are more obvious and significant with higher sparsity. ", "page_idx": 15}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/96e82ec37a21f7654e98ab7fa2ae1225b54319aa4852b097d713b68a75500c62.jpg", "table_caption": ["Table 8: Perplexity in high sparsity regimes $(\\ge70\\%)$ ; the lower the perplexity, the better. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/54693a38b8bace301051db896faf09e745aebb4a03544dbd7cfc7ee252f01b0a.jpg", "table_caption": ["Table 9: Accuracy $(\\%)$ of zero-shot tasks; the higher the accuracy, the better. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.6 Hyperparameter $\\alpha$ and $\\beta$ Selection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Hyperparameters $\\alpha$ and $\\beta$ are used in Eq. 5. We select $\\alpha$ and $\\beta$ from the set $\\{0.01,0.1,1,5,10,100\\}$ and perform a study on models to understand the impact of the hyperparameters. Results for OPT-1.3b with $70\\%$ sparsity is shown in Table 10. ", "page_idx": 16}, {"type": "table", "img_path": "oXHyYHp4Zb/tmp/568913f937e3038c60028e968ce70d9d788cb6e1faae84d8a079f8c303c33ce6.jpg", "table_caption": ["Table 10: Ablations of the hyperparameters $\\alpha$ and $\\beta$ on OPT-1.3b with $70\\%$ sparsity (in perplexity) "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.7 Limitations and Future Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "While SparseLLM marks a significant step forward in the efficient pruning of large language models, it is important to acknowledge the inherent trade-offs associated with any model compression technique. Firstly, while our method reduces the complexity of LLMs and enhances computational efficiency, there is an inevitable balance between sparsity and performance that requires careful calibration. Additionally, in this work, we still assume homogeneous sparsity, i.e., the pruning sparsity for each layer is the same and equal to the global sparsity. How to achieve heterogeneous sparsity under our framework and fully fulfill the potential of global pruning is of great interest. Lastly, the effectiveness of SparseLLM, like any pruning method, may vary across different models and tasks, and its generalizability to all scenarios remains an area for further exploration. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our paper\u2019s contributions and scope. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We have included a section to discuss our paper\u2019s limitation in Sec. A.7. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provided all assumptions and proofs for theories in our paper. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provided comprehensive information as well as the source code of our method. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provided open-access data and code to reproduce our experiments. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provided the necessary details to run our experiments. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We are closely following existing literature in reporting the experimental results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provided necessary details on the compute resources we used in our experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper has no societal impact. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We properly credited and cited licenses of existing assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provided proper documents for our released code and data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our research does not involve these. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]