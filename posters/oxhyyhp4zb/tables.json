[{"figure_path": "oXHyYHp4Zb/tables/tables_7_1.jpg", "caption": "Table 1: Perplexity of OPT models for sparsity \u2265 70%; the lower the perplexity, the better.", "description": "This table presents the perplexity scores achieved by different model pruning methods (Magnitude, Wanda, SparseGPT, and SparseLLM) on various OPT model sizes (1.3B, 2.7B, 13B, and 30B parameters) across three datasets (WikiText2, Penn Treebank, and C4).  The results are broken down by sparsity levels (70%, 80%, 90%, and 3:4 semi-structured). Lower perplexity indicates better performance.  The table allows for a comparison of the effectiveness of different pruning techniques under various sparsity constraints and model sizes.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_7_2.jpg", "caption": "Table 2: Perplexity of LlaMA models for sparsity \u2265 70%; the lower the perplexity, the better.", "description": "This table presents the perplexity scores achieved by different LLM pruning methods (Magnitude, Wanda, SparseGPT, and SparseLLM) on various LlaMA model sizes (7B, 13B) across different sparsity levels (70%, 80%, 90%, and 3:4).  The perplexity is a metric used to evaluate the performance of language models, and lower scores indicate better performance.  The table allows for a comparison of the different methods' effectiveness in reducing model size while maintaining performance, particularly at high sparsity levels.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_8_1.jpg", "caption": "Table 3: Perplexity of 2:4 sparsity; the lower the perplexity, the better.", "description": "This table presents the perplexity scores for various OPT and LLAMA models at a 2:4 sparsity level.  Lower perplexity indicates better performance.  The results are compared across different datasets (WT2, PTB, C4) and methods (Magnitude, Wanda, SparseGPT, SparseLLM).  It shows how SparseLLM compares to other state-of-the-art local pruning methods at a lower sparsity level.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_9_1.jpg", "caption": "Table 1: Perplexity of OPT models for sparsity \u2265 70%; the lower the perplexity, the better.", "description": "This table presents the perplexity scores achieved by different model pruning methods (Magnitude, Wanda, SparseGPT, and SparseLLM) on various OPT models (OPT-1.3B, OPT-2.7B, OPT-13B, OPT-30B, OPT-66B) with different sparsity levels (70%, 80%, 90%, and 3:4 semi-structured sparsity).  The perplexity is a measure of how well the model predicts the next word in a sequence, with lower scores indicating better performance. The results are shown for three datasets: WikiText2 (WT2), Penn Treebank (PTB), and C4.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_9_2.jpg", "caption": "Table 2: Perplexity of LlaMA models for sparsity \u2265 70%; the lower the perplexity, the better.", "description": "This table presents the perplexity scores achieved by different model pruning methods (Magnitude, Wanda, SparseGPT, and SparseLLM) on various LlaMA models (7B, 13B) at different sparsity levels (70%, 80%, 90%, and 3:4).  The perplexity is a metric used to evaluate the performance of language models. Lower perplexity indicates better performance. The table allows for a comparison of the effectiveness of different pruning techniques in maintaining model performance while reducing model size.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_15_1.jpg", "caption": "Table 6: Computation time in seconds of OPT models.", "description": "This table presents the computation time, in seconds, required for different OPT models using SparseGPT and SparseLLM methods.  It shows how the computation time increases with model size for both methods.", "section": "5 Experiments"}, {"figure_path": "oXHyYHp4Zb/tables/tables_15_2.jpg", "caption": "Table 7: Computation time in seconds of LlaMA-2 models.", "description": "This table shows the computation time, in seconds, for SparseGPT and SparseLLM methods applied to Llama-2 models with 7 billion and 13 billion parameters.  It provides a comparison of the computational efficiency of the two methods across different model sizes.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_16_1.jpg", "caption": "Table 1: Perplexity of OPT models for sparsity \u2265 70%; the lower the perplexity, the better.", "description": "This table presents the perplexity scores achieved by different model pruning methods (Magnitude, Wanda, SparseGPT, and SparseLLM) on various OPT model sizes (1.3B, 2.7B, 13B, 30B, and 66B parameters) at different sparsity levels (70%, 80%, 90%, and 3:4).  The perplexity is a metric measuring how well the model predicts the next word in a sequence, with lower scores indicating better performance. The table allows comparison of the effectiveness of these methods in reducing model size while preserving performance.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_16_2.jpg", "caption": "Table 4: Accuracy (%) of zero-shot tasks for OPT models; the higher the accuracy, the better.", "description": "This table presents the zero-shot accuracy results for various OPT models with different sparsity levels (70%, 80%, 90%, and 3:4).  The accuracy is evaluated across multiple tasks including BoolQ, RTE, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA.  The \"Dense\" row shows the performance of the original, unpruned model. The table allows comparison of the SparseLLM method's performance against other methods like SparseGPT for various sparsity levels.  Lower perplexity indicates better performance.", "section": "5.1 Results and analyses"}, {"figure_path": "oXHyYHp4Zb/tables/tables_16_3.jpg", "caption": "Table 10: Ablations of the hyperparameters \u03b1 and \u03b2 on OPT-1.3b with 70% sparsity (in perplexity)", "description": "This table presents the results of an ablation study on the hyperparameters \u03b1 and \u03b2 used in the SparseLLM model.  The study was conducted on the OPT-1.3b model with 70% sparsity.  Different combinations of \u03b1 and \u03b2 values were tested (0.01, 0.1, 1, 5, 10, 100) to assess their impact on model performance, measured by perplexity. The lowest perplexity value achieved is highlighted in bold, indicating the optimal combination of hyperparameters for this specific model and sparsity level.", "section": "A.6 Hyperparameter \u03b1 and \u03b2 Selection"}]