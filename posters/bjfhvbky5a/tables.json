[{"figure_path": "bjFhVbky5A/tables/tables_6_1.jpg", "caption": "Table 1: Models for evaluation, where \"-\" indicates that the values are different across layers.", "description": "This table presents the configurations of the five different MoE models used in the paper's experiments.  For each model, it lists the number of layers, the dimension of the model (dmodel), the feed-forward network dimension (dffn), the number of experts, the number of parameters in the MoE layer, and the total number of parameters in the model.  The table helps readers understand the scale and complexity of the models used in the evaluations.", "section": "4 Experiment"}, {"figure_path": "bjFhVbky5A/tables/tables_8_1.jpg", "caption": "Table 3: Results of fine-tuning Swin-MoE on the ImageNet-1K dataset.", "description": "This table presents the results of fine-tuning the Swin-MoE model on the ImageNet-1K dataset.  It compares the performance of the original Swin-MoE model against the proposed LSH-MoE method. The metrics include Top-1 accuracy, Top-5 accuracy, compression rate, and samples per second. The compression rate shows the efficiency of LSH-MoE in reducing communication overhead during training. The samples/second metric indicates a speedup achieved by LSH-MoE.", "section": "4.4 Overall Performance"}, {"figure_path": "bjFhVbky5A/tables/tables_14_1.jpg", "caption": "Table 1: Models for evaluation, where \"-\" indicates that the values are different across layers.", "description": "This table presents the specifications of the four different MoE models used in the paper's experiments.  For each model, it lists the number of layers, the dimensionality of the model (dmodel), the dimensionality of the feed-forward network (dffn), the number of experts used, and the total number of parameters in the model. The model sizes vary significantly, ranging from hundreds of millions to tens of billions of parameters, showcasing the scale of models used in the experiments.", "section": "4.2 Benchmarks and Datasets"}]