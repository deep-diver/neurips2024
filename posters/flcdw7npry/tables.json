[{"figure_path": "FlcdW7NPRY/tables/tables_2_1.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the data distribution across five forecasting platforms used in the study and the baseline performance of pre-trained language models on a test set.  Table 2(a) shows the number of training, validation, and test questions from each platform. Table 2(b) displays the Brier scores (a measure of forecasting accuracy) for several pre-trained language models and provides a comparison to a random baseline and the performance of a crowd of human forecasters. The lower the Brier score, the better the performance.  The human crowd serves as the benchmark.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_3_1.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the distribution of the training, validation, and test datasets across five different forecasting platforms.  The second part shows the baseline performance (Brier score and standard error) of several pre-trained language models on the test set, comparing them to a random baseline and the average performance of human forecasters.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_4_1.jpg", "caption": "Table 3: System performance on the test set. \"All Questions\": Brier score on full test set. Other rows: selective evaluation when criteria are met. \"Crowd Uncertain\": crowd predictions 0.3\u20130.7. \"Early Retrieval\": first 3 retrieval dates. \u201c5+ Articles\u201d: \u22655 relevant articles. \u201cAll Criteria\u201d: all 3 criteria met. System-crowd aggregate performs best in all settings. Subscripts: 1 standard error. Bold: outperforms crowd aggregate. Underlined: best in category.", "description": "This table presents the performance of the proposed forecasting system on a test set of questions, broken down by different criteria.  It shows Brier scores (lower is better), accuracy (higher is better), and the percentage of data retained for each criterion.  The criteria include considering only questions where crowd predictions show uncertainty, focusing on results from early retrieval attempts, and looking only at questions with at least 5 relevant articles.  It also shows a final combined analysis for when all criteria are met. The results are compared to the performance of the human crowd.  The table also indicates where the system outperformed the crowd.", "section": "System Performance on the test set"}, {"figure_path": "FlcdW7NPRY/tables/tables_8_1.jpg", "caption": "Table 4: Ablation results: Fine-tuning GPT-3.5 has similar performance to fine-tuning GPT-4-0613 (rows 2-3). Our system degrades without fine-tuning (row 4) or retrieval (row 5), as expected. \"Aggregate\" is the weighted average with crowd prediction. Subscripts are standard errors; bold entries beat the human crowd.", "description": "This table presents the ablation study results, showing the impact of different components of the proposed forecasting system on its performance.  It compares the full system's performance against versions where either fine-tuning or the retrieval system were removed. The results highlight the importance of both fine-tuning and retrieval for achieving near human-level forecasting accuracy.  The Brier score and accuracy are reported, with statistical significance indicated using standard errors.  The \"Aggregate\" column shows the weighted average performance combining the system's predictions and the crowd's.", "section": "3.3 Models are not naturally good at forecasting"}, {"figure_path": "FlcdW7NPRY/tables/tables_12_1.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the data distribution of the training, validation, and test sets across five different forecasting platforms.  It also shows the baseline performance of several pre-trained language models on the test set, measured by Brier score, with standard errors indicated.  A random baseline and the human crowd performance are included for comparison.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_15_1.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the distribution of the training, validation, and test datasets across five different forecasting platforms. It then shows the baseline performance of various pre-trained language models on the test set, measured by the Brier score and accuracy, with standard errors included.  A random baseline and the human crowd's performance are also provided for comparison.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_16_1.jpg", "caption": "Table 7: Comparison of knowledge accuracy across categories and models on the train and validation sets. We list the knowledge accuracy of all base models with respect to all categories in the train and validation set.", "description": "This table shows the knowledge accuracy of 14 different Language Models (LMs) across 11 different categories.  The knowledge accuracy is measured by the percentage of correctly answered questions from the training and validation datasets.  The results highlight the varying performance of different LMs in different knowledge domains.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_17_1.jpg", "caption": "Table 6: Zero-shot and scratchpad Brier scores on the test set. All models fall significantly far from human aggregate. Subscript numbers denote 2 standard errors. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the Brier scores achieved by fourteen different language models on a test set, categorized by two prompting methods: zero-shot and scratchpad.  The results highlight the significant underperformance of these models compared to the human crowd's aggregated performance, even with the more sophisticated scratchpad prompting.", "section": "B.2 Baseline Evaluation Results"}, {"figure_path": "FlcdW7NPRY/tables/tables_17_2.jpg", "caption": "Table 9: Correlation between knowledge accuracy and zero-shot prompt Brier score by category. Categories with an absolute correlation of 0.3 or greater, shown in bold, indicate a high correlation between accuracy on the training and validation set and forecasting performance on the test set. This highlights that in certain domains model\u2019s forecasting capabilities are correlated with its pre-training knowledge.", "description": "This table shows the correlation between the model's knowledge accuracy (based on its performance on training and validation sets) and its Brier score (a measure of forecast accuracy) on the test set, broken down by category.  Strong negative correlations (shown in bold) indicate that better knowledge in a particular category leads to better forecasting performance in that category.  This suggests the potential benefit of domain-adaptive training, where models are specifically trained for certain categories to improve accuracy.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_20_1.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the distribution of training, validation, and test datasets across five different forecasting platforms (Metaculus, GJOpen, INFER, Polymarket, and Manifold).  It also shows the baseline performance (Brier score and accuracy) of several pre-trained language models on the test set.  The subscript numbers indicate one standard error. The table provides a baseline to compare against, showing the performance of pre-trained models before any further optimization or fine-tuning.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_20_2.jpg", "caption": "Table 11: Raw dataset statistics across platforms. The Brier scores are calculated by averaging over all time points where the platforms provide crowd aggregates.", "description": "This table shows the number of questions and predictions from five different forecasting platforms.  It breaks down the data into all questions and predictions, and then specifically for binary questions and predictions. The Brier score, a metric for evaluating probabilistic forecasts, is provided for the binary predictions.  It reflects the overall size and characteristics of the dataset used for training and evaluating the forecasting model.", "section": "3 Preliminaries: Data, Models and Baseline"}, {"figure_path": "FlcdW7NPRY/tables/tables_21_1.jpg", "caption": "Table 3: System performance on the test set. \"All Questions\": Brier score on full test set. Other rows: selective evaluation when criteria are met. \"Crowd Uncertain\": crowd predictions 0.3-0.7. \"Early Retrieval\": first 3 retrieval dates. \u201c5+ Articles\u201d: \u22655 relevant articles. \u201cAll Criteria\u201d: all 3 criteria met. System-crowd aggregate performs best in all settings. Subscripts: 1 standard error. Bold: outperforms crowd aggregate. Underlined: best in category.", "description": "This table presents the performance of the proposed forecasting system on a test set of questions, comparing its Brier score and accuracy to the crowd aggregate.  It further breaks down the results based on different criteria to show performance under various conditions such as when crowd predictions are uncertain, when early retrieval data is used, when many relevant articles are available, and when all three of these conditions hold.  The table highlights situations where the system either matches or outperforms the crowd.  Statistical significance is also shown through standard error values.", "section": "System performance on the test set"}, {"figure_path": "FlcdW7NPRY/tables/tables_27_1.jpg", "caption": "Table 13: Brier scores across different ensembling methods on the validation set. \u201cBaseline\u201d refers to the average Brier score of the base predictions (i.e., the inputs to ensembling).", "description": "This table presents a comparison of different ensemble methods used to combine multiple forecasts generated by the system on a validation set.  The Brier score, a metric measuring the accuracy of probabilistic forecasts, is used to evaluate the effectiveness of each method. A lower Brier score indicates better performance. The table also includes the baseline Brier score obtained from individual forecasts (without ensembling) and the Brier score achieved by the human crowd, which serves as a benchmark for the system\u2019s performance.", "section": "F.2 Further Details and Results on Hyperparameter Sweep"}, {"figure_path": "FlcdW7NPRY/tables/tables_29_1.jpg", "caption": "Table 3: System performance on the test set. \"All Questions\": Brier score on full test set. Other rows: selective evaluation when criteria are met. \"Crowd Uncertain\": crowd predictions 0.3-0.7. \"Early Retrieval\": first 3 retrieval dates. \u201c5+ Articles\u201d: \u22655 relevant articles. \u201cAll Criteria\": all 3 criteria met. System-crowd aggregate performs best in all settings. Subscripts: 1 standard error. Bold: outperforms crowd aggregate. Underlined: best in category.", "description": "This table presents the performance of the proposed forecasting system on a test set of questions, comparing it against human forecasters' aggregate performance (crowd). It shows Brier scores and accuracy, broken down into several scenarios based on different conditions (crowd uncertainty, early retrieval dates, number of relevant articles retrieved, and a combination of these conditions).  The results highlight the system's overall performance and its performance under specific circumstances, illustrating its relative strengths and weaknesses compared to human forecasters.", "section": "System Performance on the Test Set"}, {"figure_path": "FlcdW7NPRY/tables/tables_29_2.jpg", "caption": "Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.", "description": "This table presents the distribution of the training, validation, and test datasets across five forecasting platforms.  The dataset was curated to avoid information leakage by only including questions in the test set that were published after the knowledge cutoff date of the models.  The second part of the table shows the baseline performance of pre-trained language models on the test set, measured by Brier score and accuracy, with standard errors included.  A random baseline and the average performance of the human crowd are also provided for comparison.", "section": "3 Preliminaries: Data, Models and Baseline"}]