[{"figure_path": "FYLcH4HAZr/tables/tables_2_1.jpg", "caption": "Table 1: Quantitative results on Sintel and KITTI. The average End-Point Error (EPE) is reported as the evaluation metric if not specified. * refers to the warm-start strategy [45] that use the previous flow for initialization. Bold and underlined metrics denote the method that ranks 1st and 2nd, respectively.", "description": "This table presents a quantitative comparison of StreamFlow against various state-of-the-art optical flow estimation methods on the Sintel and KITTI datasets.  It shows the end-point error (EPE) for both clean and final passes of the Sintel dataset, along with the all, clean and final evaluations for KITTI.  The table also highlights the performance of methods using a warm-start strategy, indicating the use of prior flow information.", "section": "4 Experiments"}, {"figure_path": "FYLcH4HAZr/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative results on Sintel and KITTI. The average End-Point Error (EPE) is reported as the evaluation metric if not specified. * refers to the warm-start strategy [45] that use the previous flow for initialization. Bold and underlined metrics denote the method that ranks 1st and 2nd, respectively.", "description": "This table presents a quantitative comparison of StreamFlow against other state-of-the-art methods on the Sintel and KITTI datasets.  The results are broken down by dataset (Sintel and KITTI), and further by test set (Clean and Final for Sintel).  The main metric used is End-Point Error (EPE), representing the average error in predicted optical flow.  The table also indicates whether a warm-start strategy was used (using the previous flow for initialization).  The best and second-best performing methods in each category are highlighted.", "section": "4.1 Quantitative results"}, {"figure_path": "FYLcH4HAZr/tables/tables_7_1.jpg", "caption": "Table 2: Quantitative results on Spring test benchmark. Measures are from the official Spring website, including the total score, EPE, FI, WAUC, and detailed metrics such as 1px outlier rate, etc. Important metrics are highlighted. \u2020 denotes 0-shot test using the checkpoint from \u201cC+T+S+H+K\u201d.", "description": "This table presents a quantitative comparison of various optical flow methods on the Spring benchmark dataset.  It includes the total score (a summary metric), EPE (End-Point Error), FI (F-measure), and WAUC (weighted area under the curve).  Detailed metrics are also provided, such as the percentage of outliers with 1-pixel error, along with breakdowns for low- and high-detection difficulty, matched and unmatched regions, rigid and non-rigid motions, and sky and non-sky regions.  The results are shown for different thresholds for flow errors. Note that the results marked with \u2020 are obtained by performing a 0-shot test (evaluating the model without fine-tuning on the Spring dataset itself).", "section": "4 Experiments"}, {"figure_path": "FYLcH4HAZr/tables/tables_7_2.jpg", "caption": "Table 1: Quantitative results on Sintel and KITTI. The average End-Point Error (EPE) is reported as the evaluation metric if not specified. * refers to the warm-start strategy [45] that use the previous flow for initialization. Bold and underlined metrics denote the method that ranks 1st and 2nd, respectively.", "description": "This table presents a comparison of the StreamFlow model's performance against other state-of-the-art optical flow estimation methods on the Sintel and KITTI datasets.  The performance is measured using the average End-Point Error (EPE), a common metric for optical flow accuracy.  The table also shows results for different test sets (clean and final) within each dataset and highlights the top two performing methods for each metric.", "section": "4.1 Quantitative results"}, {"figure_path": "FYLcH4HAZr/tables/tables_8_1.jpg", "caption": "Table 4: Ablations on our proposed design. All models are trained using the \"C+T\" schedule. The number of refinements is 12 for all methods. The settings used in our final model are underlined.", "description": "This table presents the ablation study of the proposed StreamFlow model. It systematically evaluates the impact of different components of the model, namely, the SIM pipeline, temporal modules, extra parameters, GTR module, and ISC module, on the model's performance.  The results are presented in terms of End-Point Error (EPE) metrics on both Sintel and KITTI datasets, along with the number of parameters (M) and latency (ms). The underlined results indicate the final model configuration.", "section": "4 Experiments"}, {"figure_path": "FYLcH4HAZr/tables/tables_14_1.jpg", "caption": "Table 5: Comparison of latency using memory bank.", "description": "This table compares the latency of VideoFlow-BOF and StreamFlow when using a memory bank.  The comparison highlights the efficiency gains achieved by StreamFlow, showing a significant reduction in latency despite using a memory bank.  It emphasizes the efficiency improvements of StreamFlow are not solely attributed to memory bank usage but also to optimizations in the decoder.", "section": "A.1 Comparison of latency using memory bank"}, {"figure_path": "FYLcH4HAZr/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative results on Sintel and KITTI. The average End-Point Error (EPE) is reported as the evaluation metric if not specified. * refers to the warm-start strategy [45] that use the previous flow for initialization. Bold and underlined metrics denote the method that ranks 1st and 2nd, respectively.", "description": "This table presents a comparison of the proposed StreamFlow model against other state-of-the-art methods for optical flow estimation on the Sintel and KITTI datasets.  It shows the performance (End-Point Error or EPE) of various methods under different training data conditions.  The results are broken down by dataset (Sintel clean, Sintel final, KITTI15), and further separated into overall performance, and performance on occluded regions.  The table highlights the top-performing methods.", "section": "4.1 Quantitative results"}, {"figure_path": "FYLcH4HAZr/tables/tables_15_2.jpg", "caption": "Table 7: Impact of frame distance. \u2020 denotes using nearer frames.", "description": "This table shows the impact of using different numbers of frames (3 and 4) and different frame distances on the performance of the StreamFlow model. The results are evaluated on the Sintel dataset using two metrics: clean and final. The \"+\" symbol indicates the use of nearer frames, suggesting that using frames closer together might improve accuracy in some cases.", "section": "4.3 Ablations"}, {"figure_path": "FYLcH4HAZr/tables/tables_16_1.jpg", "caption": "Table 8: Results on Sintel test set. Unm. and Mat. denote performance on unmatched and matched areas, respectively. \"Baseline\" denotes our baseline method Twins-SKFlow.", "description": "This table presents a detailed comparison of the performance of the proposed StreamFlow model and its baseline (Twins-SKFlow) on the Sintel test dataset.  It breaks down the results (End-Point Error or EPE) for various motion characteristics: unmatched areas (regions where motion is only visible in one of two frames), matched areas (motion visible in both frames), and across different ranges of motion speed. The results highlight StreamFlow's improved accuracy, particularly in challenging unmatched areas.", "section": "4.2 Occlusion analysis"}, {"figure_path": "FYLcH4HAZr/tables/tables_16_2.jpg", "caption": "Table 9: Comparison of different ways of initialization. All models are trained under the FlyingThings.", "description": "This table compares the performance of different initialization methods for the GTR (Global Temporal Regressor) module in the StreamFlow model.  The models were trained using the FlyingThings dataset, and the results are evaluated on the Sintel and KITTI datasets using End-Point Error (EPE) and the overall flow error metric (Fl-all).  The comparison highlights the impact of different initialization strategies on model performance.", "section": "A.7 Initialization of GTR"}]