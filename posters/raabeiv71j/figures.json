[{"figure_path": "raABeiV71j/figures/figures_1_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys across various large language models.  It demonstrates that a surprisingly small number of principal components (around 80) explain 90% of the variance in the key vectors, despite the key vectors having a much larger dimensionality (e.g., 128). This observation motivates the Loki method. The right plot provides a visual overview of the Loki algorithm: during offline calibration, PCA is used to obtain a low-dimensional representation of the key vectors, which is then used during inference to efficiently select the most relevant tokens based on approximate attention scores.  Finally, the full-dimensional keys are used to compute the final attention scores for the selected tokens.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_3_1.jpg", "caption": "Figure 2: Rank at which 90% of the variance is explained for pre-rotary and post-rotary keys produced by each layer averaged across all heads (Rank\u0131@90) for different models. We observe that all models exhibit significantly low rank (full dimensionality is 128 or 96 represented by the black dashed line) consistently across all datasets.", "description": "This figure shows the results of a principal component analysis (PCA) on the attention keys from several different large language models (LLMs).  For each model, the graph displays how many principal components are needed to capture 90% of the variance in the key vectors, broken down by layer.  The x-axis represents the layer number and the y-axis the rank at 90% explained variance.  The results indicate that the key vectors across various LLMs consistently occupy a significantly lower-dimensional space than the full attention head dimension, supporting the claim that attention keys lie in a low-dimensional space and that this property is consistent across various datasets and LLMs.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_6_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "This figure shows two plots. The left plot is a bar chart showing the rank at which 90% of the variance in attention keys is explained for various large language models (LLMs). The x-axis represents the different LLMs considered, and the y-axis represents the rank. A black dashed line indicates the full rank of the keys. The right plot is a schematic diagram illustrating the Loki algorithm proposed in the paper, which uses low-dimensional key vector projections for sparse attention.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_7_1.jpg", "caption": "Figure 3: Evaluation of Loki on perplexity (left plot) and short-context tasks (right plot) for different models. Task accuracy is an average across all short-context tasks mentioned in 5.", "description": "This figure shows the results of evaluating Loki's performance against a full attention model for various LLMs, focusing on perplexity and short-context downstream tasks.  The left plot displays perplexity scores, while the right plot shows average task accuracy across multiple downstream tasks.  Different configurations of Loki (using pre-rotary and post-rotary PCA transforms with varying values of k and d) are compared against the full attention model to demonstrate Loki's efficacy and minimal performance degradation.", "section": "6 Results"}, {"figure_path": "raABeiV71j/figures/figures_7_2.jpg", "caption": "Figure 3: Evaluation of Loki on perplexity (left plot) and short-context tasks (right plot) for different models. Task accuracy is an average across all short-context tasks mentioned in 5.", "description": "This figure displays the results of evaluating Loki's performance on perplexity and several downstream tasks, comparing it against the performance of a full attention model.  The left plot shows perplexity scores across various models (Llama2-7B, Llama2-13B, Llama3-8B, Llama3-70B, Mistral-7B, Mixtral-8x7B)  for different configurations of Loki's hyperparameters (kf and df). The right plot illustrates average task accuracy across the same models and  hyperparameter settings.  The results indicate Loki's effectiveness in maintaining model quality and achieving comparable performance to the full attention model, despite using fewer resources. Note that pre- and post-rotary key versions of the models are also compared in the left graph.", "section": "Comparison with Full Attention"}, {"figure_path": "raABeiV71j/figures/figures_8_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the rank (dimensionality) at which 90% of the variance in attention keys is explained for different language models.  It demonstrates that a significantly lower dimensionality than the full key vector size captures most of the variance.  The right plot provides a schematic overview of the Loki algorithm, illustrating how it uses PCA to reduce the dimensionality of key vectors before computing sparse attention scores and selecting top-k tokens.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_9_1.jpg", "caption": "Figure 7: Time per layer for vanilla attention (V) and Loki (L-A: kf = 0.25, df = 0.25; L-B: kf = 0.125, df = 0.25) for Llama2-13B using huggingface transformers (left two plots). Long-Bench average accuracy for different Loki configurations, alongside micro-benchmark attention times (right plot, all layers, prompt length = 3500 & generation length = 512). We choose the prompt length to match LongBench's configuration for this model and generation length to match the maximum in any LongBench task. For both figures, we use a batch size of 16 and report the average time over 10 trials (std. dev. in measured times was less than 0.05 percent of the mean).", "description": "This figure presents a comparison of the computation time per layer for vanilla attention and Loki on Llama2-13B model using HuggingFace transformers. It showcases the breakdown of time spent on various operations within the attention mechanism for both methods at different prompt and generated token lengths.  The right plot shows LongBench average accuracy across various Loki configurations, demonstrating the trade-off between accuracy and speed of attention computation. It highlights the impact of different parameter settings (kf and df) on both accuracy and time.", "section": "6.4 Computational Efficiency"}, {"figure_path": "raABeiV71j/figures/figures_13_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The figure shows two plots. The left plot is a bar chart showing the rank at which 90% of the variance in the attention keys is explained for various large language models (LLMs). The x-axis represents the different LLMs, and the y-axis represents the rank. The right plot is a diagram showing the overall process of the Loki model, including its offline calibration phase and its online token selection process during inference.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_14_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys across different large language models.  It demonstrates that a surprisingly small number of principal components (around 80) capture 90% of the variance in the key vectors, even though the full dimensionality is much higher (128). This observation motivates the Loki method. The right plot is a schematic overview of the Loki algorithm:  during offline calibration, PCA is applied to the keys to obtain low-dimensional representations;  during inference, top-k tokens are selected based on approximate attention scores calculated in this low-dimensional space; the final attention scores are then computed using these top-k tokens in full dimensionality.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_15_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys for various LLMs.  It demonstrates that a surprisingly small number of principal components (around 80) can explain 90% of the variance in the key vectors, despite the much higher dimensionality of the full attention heads. This observation forms the basis for the Loki algorithm. The right plot provides a visual overview of Loki's operational steps: It begins by generating approximate attention scores for all tokens using a lower-dimensional representation (learned offline).  Then, it selects top-k most relevant tokens based on these scores and only computes the final attention scores for those selected tokens.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_15_2.jpg", "caption": "Figure 2: Rank at which 90% of the variance is explained for pre-rotary and post-rotary keys produced by each layer averaged across all heads (Rank\u0131@90) for different models. We observe that all models exhibit significantly low rank (full dimensionality is 128 or 96 represented by the black dashed line) consistently across all datasets.", "description": "This figure displays the rank at which 90% of the variance is explained for pre-rotary and post-rotary keys across different layers for four different language models.  Each model's data is shown across three datasets: WikiText, C4, and BookCorpus. The results demonstrate that the key vectors consistently occupy a significantly lower dimensional space (around 80) than the full attention head dimension (128 or 96), across various models and datasets. This observation is consistent regardless of whether rotary embeddings are applied to the keys or not.  The black dashed line represents the full dimensionality of the keys.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_16_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys across different large language models.  The key observation is that a relatively small number of principal components (around 80) capture most of the variance (90%) in the key vectors, despite the larger dimensionality of the attention heads (128). This suggests that key vectors effectively live in a lower-dimensional subspace. The right plot provides a schematic overview of the Loki method. Loki uses PCA on a calibration dataset to identify the most relevant tokens (based on low-dimensional attention scores) and only computes the exact attention scores for the selected tokens. This strategy is expected to improve efficiency and reduce computation costs.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_16_2.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys across different large language models.  It demonstrates that a surprisingly small number of principal components (around 80) explain 90% of the variance in the key vectors, even though the full dimensionality is much higher (128). This observation is key to the Loki method. The right plot provides a schematic overview of the Loki algorithm: using a calibration dataset, PCA is performed on the key tensors offline to generate approximate attention scores, and these scores are used to select only the most relevant tokens from the KV-cache, speeding up inference without substantial accuracy loss. For these selected tokens, the full dimensional key vectors are used to compute the final attention scores.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_17_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the rank (dimensionality) at which 90% of the variance in key vectors is explained, averaged across all layers and heads for various LLMs.  The full rank (dimensionality of the key vector) is shown as a black dashed line for reference.  The plot shows that a significantly lower-rank representation captures almost all the information in the key vectors, demonstrating their intrinsic low-dimensionality. The right plot provides a schematic overview of the Loki method. Loki leverages this low dimensionality to achieve computational efficiency.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_20_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "This figure shows the low dimensionality of attention keys across various large language models.  The left panel displays a bar chart, with error bars, illustrating the rank at which 90% of the variance in attention keys is explained for each model.  The results demonstrate that a relatively low rank captures most of the variance, indicating a low-dimensional structure.  The right panel provides a schematic overview of the Loki algorithm, summarizing the steps involved in dimensionality reduction, token selection, and final attention computation.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_21_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The left plot shows the dimensionality of attention keys across different large language models.  It demonstrates that a surprisingly small number of principal components (around 80) are sufficient to capture 90% of the variance in the key vectors, even though the full dimensionality of the keys is much larger (128).  This observation motivates the Loki method. The right plot provides a schematic overview of the Loki algorithm, highlighting the key steps: offline PCA on keys from a calibration dataset, online selection of top-k tokens using low-rank attention scores, and the final full-rank attention computation on the selected tokens.", "section": "3 Dimensionality Analysis of Attention Keys"}, {"figure_path": "raABeiV71j/figures/figures_23_1.jpg", "caption": "Figure 1: Rank at which 90% of the variance is explained, averaged across all layers and heads for different models. Full rank is represented by the black dashed line (left). Overview of Loki (right).", "description": "The figure on the left shows the dimensionality of attention keys across different LLMs.  It plots the rank at which 90% of the variance in the key vectors is explained by principal component analysis (PCA), averaged across all layers and heads within each model.  The black dashed line indicates the full dimensionality of the key vectors, showing that a significantly lower dimensionality captures most of the variance. The figure on the right shows an overview of the Loki algorithm, illustrating the process of offline calibration using PCA, online token selection based on low-dimensional attention scores, and the computation of final attention scores using the full dimensionality for the selected tokens.", "section": "3 Dimensionality Analysis of Attention Keys"}]