{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-03", "reason": "This paper introduced the transformer architecture, the foundation of many large language models (LLMs), including the self-attention mechanism that is the focus of the current paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper introduced FlashAttention, a highly optimized method for efficient attention computation, which is directly relevant to the goal of improving the efficiency of sparse attention in the current paper."}, {"fullname_first_author": "Krzysztof Choromanski", "paper_title": "Rethinking attention with performers", "publication_date": "2022-01-01", "reason": "This paper proposed the Performer model, an alternative to the standard transformer that uses low-rank approximations to speed up attention calculations, which the current paper builds upon."}, {"fullname_first_author": "Zhenyu Zhang", "paper_title": "H2O: Heavy-hitter oracle for efficient generative inference of large language models", "publication_date": "2023-06-01", "reason": "This paper introduced H2O, a sparse attention method using token eviction, which is compared to the proposed method in the current paper."}, {"fullname_first_author": "Luka Ribar", "paper_title": "SparQ Attention: Bandwidth-efficient LLM inference", "publication_date": "2023-01-01", "reason": "This paper introduced SparQ Attention, another sparse attention method, whose optimized kernels are used and further improved upon in the current work. "}]}