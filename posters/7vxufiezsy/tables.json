[{"figure_path": "7vXufiEzSy/tables/tables_3_1.jpg", "caption": "Table 1: Feature variance (\u03c3F) and similarity variance (\u03c3s).", "description": "This table presents a quantitative comparison of feature variance (\u03c3F) and similarity variance (\u03c3s) for different models: MAE encoder, MAE decoder, MoCo [23], and ViT [15].  Higher values of \u03c3F indicate more diverse patch embeddings in the feature space, while higher \u03c3s indicates stronger patch clustering.  The results demonstrate that MAE encoder and decoder exhibit significantly higher variance compared to MoCo and ViT, indicating superior patch clustering capability.", "section": "3.2 What is Learned by MAE?"}, {"figure_path": "7vXufiEzSy/tables/tables_8_1.jpg", "caption": "Table 2: Performance on downstream tasks. LP and FT stand for Linear probing and Fine-tuning, respectively. Det. indicates the Object Detection task.", "description": "This table presents the performance comparison of the proposed method against baseline methods (MAE and AMT) across three downstream tasks: image classification, object detection, and semantic segmentation.  For image classification, performance is measured using linear probing (LP) and fine-tuning (FT) on four datasets (ImageNet-1K, iNaturalist 2019, CIFAR, and CUB). Object detection performance is evaluated using average precision for bounding boxes (APbox) and segmentation masks (Apmask) on the COCO dataset. Finally, semantic segmentation performance is measured using mean Intersection over Union (mIoU) on the ADE20K dataset.", "section": "5.2 Performance on Downstream Tasks"}, {"figure_path": "7vXufiEzSy/tables/tables_8_2.jpg", "caption": "Table 3: Ablation studies. The default is highlighted in gray. Detailed analysis can be found in Appendix D.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different factors on the performance of the proposed self-guided masked autoencoder.  The studies investigated the choice of layer for embedding extraction, hint strategy, and masking ratio.  The results show the linear probing performance for image classification on the ImageNet-1K dataset for each configuration.  The default configuration is highlighted in gray, and a more detailed analysis is available in Appendix D.", "section": "5.3 Ablation Studies"}, {"figure_path": "7vXufiEzSy/tables/tables_14_1.jpg", "caption": "Table 1: Feature variance (\u03c3F) and similarity variance (\u03c3s).", "description": "This table presents a quantitative analysis comparing the feature variance (\u03c3F) and similarity variance (\u03c3s) for different models: MAE encoder, MAE decoder, MoCo, and ViT.  Feature variance measures the spread of patch embeddings in the feature space, while similarity variance indicates the strength of patch clustering. Higher values for both indicate more diverse and clearly clustered embeddings. The table shows that MAE (both encoder and decoder) exhibits significantly higher values in both metrics compared to MoCo and ViT, suggesting more effective patch clustering based on visual patterns.", "section": "3.2 What is Learned by MAE?"}, {"figure_path": "7vXufiEzSy/tables/tables_17_1.jpg", "caption": "Table 3: Ablation studies. The default is highlighted in gray. Detailed analysis can be found in Appendix D.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different factors on the linear probing performance of the proposed self-guided masked autoencoder method for image classification.  The studies systematically varied the layer used for embedding extraction, the strategy for generating hint tokens (random or based on similarity score), and the masking ratio. The results show the importance of choosing the appropriate layer, the contribution of hint tokens, and the effect of the masking ratio on the model's performance.", "section": "5.3 Ablation Studies"}, {"figure_path": "7vXufiEzSy/tables/tables_18_1.jpg", "caption": "Table III: Reconstruction loss (MSE) with 400 pre-training epochs according to each training method.", "description": "This table presents a comparison of the reconstruction loss (measured as Mean Squared Error or MSE) achieved by three different models after 400 epochs of pre-training.  The models compared are the original Masked Autoencoder (MAE), the proposed method without hint tokens, and the proposed method with hint tokens.  The table shows that the inclusion of hint tokens in the proposed method significantly reduces reconstruction loss compared to the no hint version, and provides better results than the MAE.", "section": "5.3 Ablation Studies"}, {"figure_path": "7vXufiEzSy/tables/tables_19_1.jpg", "caption": "Table 2: Performance on downstream tasks. LP and FT stand for Linear probing and Fine-tuning, respectively. Det. indicates the Object Detection task.", "description": "This table presents the performance comparison of different models on three downstream tasks: image classification, object detection, and semantic segmentation.  The results are given for both linear probing (LP) and fine-tuning (FT) methods. For image classification, multiple datasets (ImageNet-1K, iNat2019, CIFAR, CUB) were used. Object detection results are shown for the COCO dataset and semantic segmentation results for ADE20K. The metrics reported vary based on the task (e.g., accuracy for image classification, APbox and Apmask for object detection, mIoU for semantic segmentation).", "section": "5.2 Performance on Downstream Tasks"}, {"figure_path": "7vXufiEzSy/tables/tables_19_2.jpg", "caption": "Table 2: Performance on downstream tasks. LP and FT stand for Linear probing and Fine-tuning, respectively. Det. indicates the Object Detection task.", "description": "This table presents the performance comparison of different models on various downstream tasks, namely image classification, object detection, and semantic segmentation.  It shows the results using linear probing (LP) and fine-tuning (FT) methods across several datasets.  The metrics used include linear probing accuracy and fine-tuning accuracy for image classification; average precision (AP) for bounding boxes (Apbox) and segmentation masks (Apmask) for object detection; and mean Intersection over Union (mIoU) for semantic segmentation.  This allows for a comprehensive evaluation of the model's ability to transfer learned representations to diverse vision tasks.", "section": "5.2 Performance on Downstream Tasks"}]