[{"figure_path": "Nv0Vvz588D/figures/figures_6_1.jpg", "caption": "Figure 2: SB-GFlowNet accurately learns the posterior over the utility\u2019s parameters in a streaming setting. Each plot compares the marginal distribution learned by SB-GFlowNet (horizontal axis) and the targeted posterior distribution (vertical axis) at increasingly advanced stages of the streaming process, i.e., from \u03c0\u2081(\u00b7|D1) (left-most) to \u03c0\u2088(\u00b7|D1:\u2088) (right-most).", "description": "This figure shows the results of applying SB-GFlowNets to a linear preference learning problem with integer-valued features.  Each subplot shows a comparison of the marginal distribution learned by the SB-GFlowNet and the true posterior distribution at a given stage in the streaming process. The leftmost subplot shows the comparison after the first update, and subsequent subplots show the comparisons after additional updates. The plots demonstrate how well the SB-GFlowNet's learned distributions match the true posterior distributions as more data are processed during the streaming updates.", "section": "5.2 Linear preference learning with integer-valued features"}, {"figure_path": "Nv0Vvz588D/figures/figures_7_1.jpg", "caption": "Figure 2: SB-GFlowNet accurately learns the posterior over the utility's parameters in a streaming setting. Each plot compares the marginal distribution learned by SB-GFlowNet (horizontal axis) and the targeted posterior distribution (vertical axis) at increasingly advanced stages of the streaming process, i.e., from \u03c0\u2081(\u00b7|D1) (left-most) to \u03c08(\u00b7|D1:8) (right-most).", "description": "This figure shows the accuracy of SB-GFlowNet in learning the posterior distribution over a parameter in a streaming setting.  Each subplot displays a comparison between the marginal distribution learned by the model and the true posterior at different stages of the streaming process. The plots show that the learned distribution closely matches the true posterior as more data is processed, demonstrating the model's accuracy in learning the posterior distribution dynamically.", "section": "5.2 Linear preference learning with integer-valued features"}, {"figure_path": "Nv0Vvz588D/figures/figures_7_2.jpg", "caption": "Figure 3: Predictive performance of SB-GFlowNets in terms of pred. NLL and avg. MSE. SB-GFlowNets behaves similarly to the ground-truth, wrt how the NLL evolves as a function of data chunks.", "description": "This figure shows the predictive performance of SB-GFlowNets in terms of predictive negative log-likelihood (NLL) and mean squared error (MSE).  It demonstrates that as more data chunks are processed (indicating an increase in the amount of streaming data used for training), the predictive NLL decreases for both SB-GFlowNets and the ground truth. This shows the model's ability to learn effectively from streaming data and maintain accuracy over time, and the similarity between the SB-GFlowNet's performance and the ground truth.", "section": "5.2 Linear preference learning with integer-valued features"}, {"figure_path": "Nv0Vvz588D/figures/figures_7_3.jpg", "caption": "Figure 4: SB-GFlowNet's accurate fit to the true posterior in terms of the probability of the true phylogeny (left) and of the learned model's accuracy (right).", "description": "This figure shows two plots. The left plot shows the negative logarithm of the probability of the true phylogeny, which is a measure of how well the model's learned distribution fits the true posterior distribution.  The y-axis is the negative log probability and x-axis is the number of streaming updates.  The right plot shows the expected L1 distance between the true and learned posterior distributions over the set of trees, which is another measure of how well the model is learning. The y-axis is the expected L1 distance and x-axis is the number of streaming updates. Both plots illustrate the performance of SB-GFlowNets over multiple streaming updates.", "section": "5.3 Online Bayesian phylogenetic inference"}, {"figure_path": "Nv0Vvz588D/figures/figures_8_1.jpg", "caption": "Figure 5: SB-GFlowNets accurately learns a distribution over DAGs for causal discovery in each time step. At each update, an additional dataset of 200 points was sampled from the true model. For this problem, we implemented a DAG-GFlowNet on 5-variable data sets, similarly to [11, Figure 3].", "description": "This figure shows the accuracy of SB-GFlowNets in learning the distribution over directed acyclic graphs (DAGs) for causal discovery.  Each plot represents a different time step (t=1 to t=6), showing the learned distribution against the true posterior. As more data is added (each update adds 200 more data points), the accuracy of the learned distribution improves.", "section": "5.4 Bayesian structure learning"}, {"figure_path": "Nv0Vvz588D/figures/figures_8_2.jpg", "caption": "Figure 6: The probability mass on the true DAG increases as more samples are added to SB-GFlowNet.", "description": "This figure shows how the probability mass assigned by the SB-GFlowNet to the true DAG that is responsible for generating the data increases with the number of streaming updates.  It demonstrates the model's ability to learn the true data-generating process more accurately as it receives more data.", "section": "5.4 Bayesian structure learning"}, {"figure_path": "Nv0Vvz588D/figures/figures_16_1.jpg", "caption": "Figure 7: Illustration of the task of generating sets of size |S| = 2 with elements in {1,2,3}. On each streaming update, a novel reward function R\u221e) is observed; a small value of a entails a more sparse and harder-to-sample-from distribution. Terminal states X are illustrated in green and non-terminal states are depicted in blue. At the tth iteration, we learn a generative model p(+) sampling S \u2208 X proportionally to \u03a01\u2264i\u2264t R(i)(S).", "description": "This figure illustrates the set generation task used in the paper to demonstrate the effectiveness of SB-GFlowNets.  Each panel shows a different stage (t=1,2,3,4) of a streaming update where a novel reward function is introduced at each step.  The goal is to learn a probability distribution over sets of size 2 from elements {1,2,3}. The nodes represent states in a generative model, with terminal states (sets) in green and non-terminal states in blue. The edges show transitions between states, and the associated reward functions (R(i)(S)) influence the learned probability distribution at each time step.", "section": "5.1 Set generation"}, {"figure_path": "Nv0Vvz588D/figures/figures_17_1.jpg", "caption": "Figure 8: Permutation invariance of SB-GFlowNets for phylogenetics (a) and set generation (b). When the first GFlowNet is not adequately trained, the learned distribution after two streaming updates depends on the ordering of the observed datasets (left (a), left (b)). In contrast, when both the first and second GFlowNets are accurate, the resulting distribution is approximately invariant to the data permutation (right (a), right (b)).", "description": "This figure demonstrates the permutation invariance property of SB-GFlowNets.  The left-hand side plots show that if the initial GFlowNet is poorly trained, the final distribution after two updates depends on the order in which data arrives.  The right-hand side plots show that when both the initial and subsequent GFlowNets are accurately trained, the final distribution is largely invariant to data order.", "section": "D On the permutation invariance of SB-GFlowNets"}]