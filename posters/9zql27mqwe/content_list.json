[{"type": "text", "text": "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhenfeng Tu Santiago Aranguri Arthur Jacot Courant Institute Courant Institute Courant Institute New York University New York University New York University New York, NY 10012 New York, NY 10012 New York, NY 10012 zt2255@nyu.edu aranguri@nyu.edu arthur.jacot@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unifying formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the \u2018best of both worlds\u2019: it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Whether in linear networks or nonlinear ones, there has been a lot of interest in the distinction between the lazy regime [27] and the active regime [16, 43, 15, 52, 13] as the number of neurons grows towards infinity. In the lazy regime the training dynamics become linear, so that they can be easily described in terms of the Neural Tangent Kernel (NTK) [27, 9, 51, 36], while the active regime exhibits complex nonlinear dynamics. While our understanding of the active regime remains much more limited, it appears to be characterized by the emergence of feature learning[22, 52], and of a form of sparsity [3, 11, 2, 1] (the type of sparsity observed depends on the network type [11, 19, 25, 24], but we will focus on fully-connected linear networks which exhibit a rank sparsity in the learned linear map [7, 35, 28, 47]) which are both absent in the lazy regime . ", "page_idx": 0}, {"type": "text", "text": "Note that even though it is common to talk of the \u2018the\u2019 active regime, we do not know yet whether there is only one or multiple active regimes. Indeed the term active regime is usually used to describe any regime that differs from the lazy regime and exhibit some form of feature learning. Though we do not have an complete understanding of where the lazy regimes ends and the active regime(s) start, we know that the lazy regime requires extreme overparametrization (a large number of neurons in comparison to the number of datapoints) [5, 20], a \u2018large\u2019 initialization of the weights [15], a small learning rate, and early stopping when using a cross-entropy loss or weight decay . Indeed, active regimes have been observed by breaking either of these requirements: taking limits with mild or no overparametrization [10], taking smaller or even vanishingly small initializations [35, 28], using large learning rates [33] or SGD [42, 47], or studying the late training dynamics with the cross-entropy loss [30, 17] or weight decay [34, 39, 29, 26]. Though each of these can lead to active regimes with significantly different dynamics, they often lead to similar types of feature learning and sparsity. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we study this transition in the context of linear networks and focus mainly on the effects of the width $w$ and the variance of the weights at initialization $\\sigma^{2}$ , and give a precise and almost complete phase diagram, showing the transitions between lazy and active regimes. In this setting, we will show that there typically is only \u2018one\u2019 active regime, which is the same (up to approximation) as the already well-studied balanced regime [44, 7, 8]. ", "page_idx": 1}, {"type": "text", "text": "But our result also paint a more subtle picture than the lazy/active dichotomy. We propose a more granular approach, where at a certain time some part of the network can be in the lazy regime, while others are in the active or balanced regime. More precisely the network is lazy along the singular values of the matrix represented by the network that are smaller than $\\sigma^{2}w$ , and in the active regime along the singular values larger than $\\sigma^{2}w$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the training dynamics of shallow linear networks $A_{\\theta}=W_{2}W_{1}$ and show that for large enough width $w$ (the inner dimension), and a iid ${\\mathcal{N}}(0,\\sigma^{2})$ initialization of all weights, the dynamics of $A_{\\theta(t)}$ as a result of training the parameters $\\theta=(W_{1},W_{2})$ with GD/GF on the loss ${\\mathcal{L}}(\\theta){\\overset{}{=}}\\ C(A_{\\theta})$ for a general matrix cost $C$ with learning rate $\\eta$ is approximately given by the self-consistent dynamics ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}A_{\\theta(t)}\\approx-\\eta\\sqrt{A_{\\theta}A_{\\theta}^{T}+\\sigma^{4}w^{2}I}\\nabla C(A_{\\theta})-\\eta\\nabla C(A_{\\theta})\\sqrt{A_{\\theta}^{T}A_{\\theta}+\\sigma^{4}w^{2}I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "These dynamics contain as special cases both the lazy dynamics ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial_{t}A_{\\theta(t)}\\approx-2\\eta\\sigma^{2}w\\nabla C(A_{\\theta})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "when $\\sigma^{2}w\\gg\\lambda_{m a x}(A_{\\theta})$ and the balanced dynamics ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial_{t}A_{\\theta(t)}=-\\eta\\sqrt{A_{\\theta}A_{\\theta}^{T}}\\nabla C(A_{\\theta})-\\eta\\nabla C(A_{\\theta})\\sqrt{A_{\\theta}^{T}A_{\\theta}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "when $\\sigma^{2}w\\ll\\lambda_{m i n}(A_{\\theta})$ . But it also reveals the whole spectrum of mixed dynamics in between, where some singular values of $A_{\\theta}$ are below the $\\sigma^{2}w$ threshold and some are above it. ", "page_idx": 1}, {"type": "text", "text": "This suggests that the lazy/active transition is best understood at a more granular level, where at each time $t$ every singular value of $A_{\\theta}$ can either be lazy or active/balanced. The mixed regime is the best of both worlds: on one hand, since $\\sqrt{A_{\\theta}A_{\\theta}^{T}+\\sigma^{4}w^{2}I}$ is always positive definite, the network can never get stuck at a saddle/local minimum as can happen in the balanced regime, on the other hand there is a momentum effect where the dynamics along large singular values is much faster than along the small ones, leading to an incremental learning behavior and a low-rank bias, which is absent in lazy learning. By choosing the threshold $\\sigma^{2}w$ adequately, one can best take advantage of these two phenomenon. ", "page_idx": 1}, {"type": "text", "text": "Finally, we focus on the task of recovering a low-rank $d\\times d$ matrix $A^{*}$ from noisy observations $A^{*}+E$ , training on the MSE error $\\begin{array}{r}{\\frac{1}{d^{2}}\\left\\|A_{\\theta}-(A^{*}+E)\\right\\|_{F}^{2}}\\end{array}$ in the limit as the dimension $d$ , width $w$ and variance $\\sigma^{2}$ scale together with scaling laws $w\\,=\\,d^{\\gamma_{w}}$ and $\\sigma^{2}\\,=\\,d^{\\gamma_{\\sigma^{2}}}$ . We describe the training dynamics for almost all reasonable scalings $\\gamma_{w},\\gamma_{\\sigma^{2}}$ leading to a phase diagram with two main regimes: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lazy $(1<\\gamma_{\\sigma^{2}}+\\gamma_{w})$ where all singular values remain below the threshold $\\sigma^{2}w$ throughout training, and where the network fails to recover $A^{*}$ due to the absence of low-rank bias. \u2022 Active $(1>\\gamma_{\\sigma^{2}}+\\gamma_{w})$ where $K=\\mathrm{Rank}A^{*}$ singular values pass the threshold and fit $A^{*}$ before the other singular values have time to fit the noise $E$ , leading to the recovery of $A^{*}$ . ", "page_idx": 1}, {"type": "text", "text": "There are two other degenerate regimes that we avoid: the underparametrized regime when $w<d$ (or $\\gamma_{w}\\ll1;$ where the rank is constrained by the network architecture rather than the training dynamics, and the noisy regime $2\\gamma_{\\sigma^{2}}+\\gamma_{w}+1>0$ where the variance of the entries of $A_{\\theta(0)}$ at initialization is infinite. ", "page_idx": 1}, {"type": "image", "img_path": "9zQl27mqWE/tmp/d05d61a75e6df189edcb2823c848a1e2dcb6b67808bef5ea8688af151e1a9219.jpg", "img_caption": ["Figure 1: For both plots, we train either using gradient descent or the self-consistent dynamics from equation (1), with the scaling $\\gamma_{\\sigma^{2}}=-1.85$ , $\\gamma_{w}=2.25$ which lies in the active regime. (Left panel): We plot train and test error for both dynamics. We observe that the train/test error for gradient descent is very close to the train/test error for the self-consistent dynamics. (Right panel): We plot with a solid line the singular values of $A_{\\theta(t)}$ when running the self-consistent dynamics, and use a dashed line for the singular values from running gradient descent. In this experiment, $\\mathrm{Rank}A^{\\star}=5$ . We use different colors for the 5 largest singular values and the same color for the remaining singular values. We can see how the 5 largest singular values \u2018speed up\u2019 as they cross the $\\sigma^{2}w$ threshold, allowing them to converge earlier than the rest. The minimal test error is achieved in the short period where the large singular values have converged but not the rest. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "1.2 Previous Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Linear networks have been used as a testing ground, a stepping stone on the way to understand nonlinear networks. Linear networks and their training dynamics are in many ways much simpler than nonlinear ones, but in spite of a long research history, our understanding remains limited. ", "page_idx": 2}, {"type": "text", "text": "The setting that is best understood is that of diagonal linear networks where the dynamics decouple along the diagonal entries leading to an incremental learning behavior and a sparsity bias [44, 3, 45, 23, 41], some of this analysis has been extended to include effects of initialization scale [48] and SGD [42]. While the same decoupling happens in general linear with diagonal initializations and diagonal task, it remains an extremely strong assumption. ", "page_idx": 2}, {"type": "text", "text": "Some work has been done to prove similar incremental learning dynamics outside the diagonal case [35, 28, 31] where the incremental aspect can be understood as the parameters going from saddle to saddle. For shallow linear networks, the training dynamics with MSE can be explicitly solved [21] but remain very complex so that one needs to assume some form of alignment to guarantee convergence [14]. For deeper networks there exists explicit formulas in the mean-field limit where the number of neurons grows to infinity [18], these results can of course be applied to the special case of shallow nets, our paper goes further by giving self-consistent dynamics for the full matrix, revealing the lazy/active transition, and also extends the analysis to finite widths. ", "page_idx": 2}, {"type": "text", "text": "A very powerful tool in the analysis of a linear network is its training invariants, and the balancedness condition which greatly simplifies the dynamics [6, 7]. Balanced networks exhibit a momentum effect, where the training dynamics along a singular value $s_{i}$ have \u2018speed\u2019 proportional to $s_{i}$ itself (or $s_{i}$ to some power), while this momentum effect seems to be key to understand the low-rank bias of linear networks [8], it also means that one needs to guarantee that the dynamics never approach zero, which is one the main hurdle towards proving convergence in balanced networks. To solve this issue, recent work has focused on initialization that slightly imbalanced [49, 37, 46, 38, 50]. This suggests that it is key to find the right balance between balancedness and imbalancedness to obtain both fast convergence and low-rank bias. ", "page_idx": 2}, {"type": "text", "text": "In a concurrent work [32] a similar transition between lazy and active regimes is observed, and the same mixed dynamics are derived for a specific initialization. In contrast, we prove that these dynamics are approximately true with high probability for random Gaussian initializations, which is the standard initialization scheme for neural networks. ", "page_idx": 2}, {"type": "text", "text": "1.3 Setup", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will study shallow linear networks (or matrix factorization) where a $d_{o u t}\\times d_{i n}$ matrix $A_{\\theta}$ is represented as the product of two matrices $A_{\\theta}=W_{2}W_{1}$ , where the weight matrices $W_{1}$ and $W_{2}$ are respectively $w\\times d_{i n}$ and $d_{o u t}\\times w$ dimensional, for some width $w$ . The parameters $\\theta$ of the network are the concatenation of the entries of both submatrices $\\theta=(W_{1},W_{2})$ . ", "page_idx": 3}, {"type": "text", "text": "The parameters $\\theta$ are learned in the following manner: they are initialized as i.i.d. Gaussian ${\\mathcal{N}}(0,\\sigma^{2})$ , and then optimized with gradient descent to minimize a loss ${\\mathcal{L}}(\\theta)=C(A_{\\theta})$ . Though most of our analysis works for general convex costs $C:\\mathbb{R}^{d_{o u t}\\times d_{i n}}\\rightarrow\\mathbb{R}$ on matrices, we will in the second part focus on the task of recovering a low-rank matrix $A^{*}$ from noisy observations $A^{*}+E$ , by training a linear network $A_{\\theta}$ on the MSE loss ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\frac{1}{d^{2}}\\left\\|A_{\\theta}-(A^{*}+E)\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The width $w$ allows us to control the over parametrzation, indeed the set of matrices that can be represented by a network of width $w$ is the set $\\mathcal{M}_{\\leq w}$ of matrices of rank $w$ or less. The overparametrized regime is when $w\\geq\\operatorname*{min}\\{d_{i n},d_{o u t}\\}$ because all matrices can be represented in this case. ", "page_idx": 3}, {"type": "text", "text": "1.4 Lazy Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The evolution of the weight matrices during gradient descent with learning rate $\\eta$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{1}(t+1)=W_{1}(t)-\\eta W_{2}^{T}(t)\\nabla C(A_{\\theta(t)})}\\\\ {W_{2}(t+1)=W_{2}(t)-\\eta\\nabla C(A_{\\theta(t)})W_{1}^{T}(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we view the gradient $\\nabla C(A_{\\theta(t)})$ of the cost $C$ as a $d_{o u t}\\times d_{i n}$ matrix, which for the MSE cost equals $\\nabla C(A_{\\theta(t)})=2d^{-2}(A_{\\theta(t)}-\\langle A^{*}+E))$ . ", "page_idx": 3}, {"type": "text", "text": "But we care more about the evolution of the complete matrix $A_{\\theta(t)}=W_{2}(t)W_{1}(t)$ induced by the evolution of $W_{1}(t),W_{2}(t)$ , which can be approximated by ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{\\theta(t+1)}=A_{\\theta(t)}-\\eta W_{2}(t)W_{2}^{T}(t)\\nabla C(A_{\\theta(t)})-\\eta\\nabla C(A_{\\theta(t)})W_{1}^{T}(t)W_{1}(t)+O(\\eta^{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus we see that if we can describe the matrices $C_{1}=W_{1}^{T}W_{1}$ and $C_{2}=W_{2}W_{2}^{T}$ throughout training, then we can describe the evolution of A\u03b8(t). ", "page_idx": 3}, {"type": "text", "text": "When $w$ is very large, we end up in the lazy regime where the parameters move enough up to a time $t$ to change $A_{\\theta(t)}$ , but not enough to change $C_{1},C_{2}^{\\phantom{\\dagger}}{}^{1}$ , allowing us to make the approximation $C_{i}(t)\\approx C_{i}(0)$ . Furthermore at initialization these matrices concentrate as $w\\to\\infty$ around their expectations $\\dot{\\mathbb{E}}\\left[C_{1}\\right]=\\sigma^{2}w I_{d_{i n}}$ , $\\mathbb{E}\\left[C_{2}\\right]=\\sigma^{2}w I_{d_{o u t}}$ . The GD dynamics can then be approximated by the much simpler dynamics: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{\\theta(t+1)}=A_{\\theta(t)}-2\\eta\\sigma^{2}w\\nabla C(A_{\\theta(t)}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which are equivalent to doing GD on the cost $C$ directly with a learning rate of $2\\eta\\sigma^{2}w$ . ", "page_idx": 3}, {"type": "text", "text": "One can then easily prove exponential convergence for any convex cost $C$ following the convergence analysis of traditional linear models. But we can see the absence of feature learning from the fact that the covariance $C_{1}$ of the \u2018feature map\u2019 $W_{1}$ is (approximately) constant. More problematic in the context of low-rank matrix recovery is the absence of low-rank bias, indeed one can easily solve the dynamics to obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{\\theta(t)}=(A^{*}+E)+(1-4d^{-2}\\eta\\sigma^{2}w)^{t}(A_{\\theta(0)}-(A^{*}+E)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and since $\\mathbb{E}A_{\\theta(0)}=0$ we obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[A_{\\theta(t)}\\right]=\\left(1-(1-4d^{-2}\\eta\\sigma^{2}w)^{t}\\right)(A^{*}+E).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The expected test error $\\mathbb{E}\\left\\lVert A_{\\theta(t)}-A^{*}\\right\\rVert^{2}$ is therefore lower bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbb{E}A_{\\theta(t)}-A^{*}\\right\\|^{2}=\\left\\|(1-4d^{-2}\\eta\\sigma^{2}w)^{t}A^{*}+(1-(1-4d^{-2}\\eta\\sigma^{2}w)^{t})E\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which never approaches zero. ", "page_idx": 4}, {"type": "text", "text": "In linear networks, there is no advantage to being in the lazy regime, as we simply recover a simple linear model at an additional cost of more parameters and thus more compute. But we will see that a short period of lazy regime at the beginning of training plays a crucial role in making sure that the subsequent active regime starts from an \u2018aligned\u2019 state. ", "page_idx": 4}, {"type": "text", "text": "1.5 Balanced Dynamics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There has been much more focus on so-called balanced linear networks, which are networks that satisfy the balanced condition $W_{1}W_{1}^{T}=W_{2}^{T}W_{2}$ . If the network is balanced at initialization, it remains so throughout training, because, the difference $W_{1}W_{1}^{T}-W_{2}^{T}W_{2}$ is an invariant of GF (and an approximate invariant of GD with small enough learning rate). ", "page_idx": 4}, {"type": "text", "text": "First observe that the balanced condition implies the following shared eigendecomposition $W_{1}W_{1}^{T}=$ $W_{2}^{T}W_{2}\\;=\\;U S U^{T}$ . This implies the following shared SVD decompositions $W_{1}\\ =\\ U\\sqrt{S}U_{i n}^{T}$ , $W_{2}=U_{o u t}\\sqrt{S}U^{T}$ and $A_{\\theta}=U_{o u t}S U_{i n}^{T}$ . Furthermore, we have $C_{1}=U_{i n}S U_{i n}^{T}=\\sqrt{A_{\\theta}^{T}A_{\\theta}}$ and $C_{2}=U_{o u t}S U_{o u t}^{T}=\\sqrt{A_{\\theta}A_{\\theta}^{T}}$ , which leads to self-consistent dynamics for $A_{\\theta(t)}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{\\theta(t+1)}=A_{\\theta(t)}-\\eta\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}}\\nabla C(A_{\\theta(t)})-\\eta\\nabla C(A_{\\theta(t)})\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}}+O(\\eta^{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now these dynamics are quite complex in general, and it remains difficult to prove convergence. Indeed one can easily find initializations $A_{\\theta(0)}$ that will not converge, for example if $A_{\\theta(0)}=0$ then GD will remain stuck there. A lot of work has been dedicated to finding conditions that guarantee the convergence of the above dynamics [6, 14], but these assumptions are often quite strong. ", "page_idx": 4}, {"type": "text", "text": "The simplest initialization that guarantees convergence (and the one that will be most relevant to our analysis) is the positively aligned initialization. If at initialization $A_{\\theta(0)}$ and $A^{*}+E$ are \u2018aligned\u2019, i.e. shares the same singular vectors $A_{\\theta(0)}=U_{o u t}S U_{i n}^{T}$ and $A^{*}+E=U_{o u t}S^{*}U_{i n}^{T}$ , then they will remain aligned throughout training $A_{\\theta(t)}=U_{o u t}S(t)U_{i n}^{T}$ and the dynamics decouple along each singular value ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{i}(t+1)=s_{i}(t)+2\\eta\\left|s_{i}(t)\\right|(s_{i}^{*}-s_{i}(t))+O(\\eta^{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since we always have $s_{i}^{*}\\geq0$ , then for small enough learning rates $\\eta$ , we see that if $s_{i}(0)\\in(0,s_{i}^{*}]$ it will grow monotonically and converge to $s_{i}^{*}$ ; if $s_{i}(0)>s_{i}^{*}$ it will decrease monotonically to $s_{i}^{*}$ , and if $s_{i}(0)\\leq0$ it will increase and converge to 0. Thus one can guarantee convergence if we further assume positive alignment $s_{i}(0)>0$ . ", "page_idx": 4}, {"type": "text", "text": "The advantage is that there is a momentum effect in the form of the prefactor $\\vert s_{i}(t)\\vert$ , which implies that the dynamics along large singular values are faster than along small ones. As a result, if all singular values are initialized with the same small value, then they will at first grow very slowly until they reach a critical size where the momentum effect will make them converge very fast. The singular values aligned with the top singular values of $A^{*}+E$ will reach this threshold much faster, and they will therefore converge to approximately their final value $s_{i}=s_{i}^{*}$ at a time when the other singular values are still basically zero. If we stop training at this time then the linear network will have essentially learned only the top $K$ singular values of $A^{*}+E$ , which is a good approximation for $A^{*}$ , leading to a small test error (see [23] for details). ", "page_idx": 4}, {"type": "text", "text": "But this analysis relies on the very strong assumption of positive alignment at initialization. If we do not assume a positive alignment and assume that the $s_{i}$ are random (i.i.d. w.r.t. a symmetric distribution), then each $s_{i}$ has probability $1/2$ of starting with a negative alignment and getting stuck at zero, which means that with high probability training will fail to recover $A^{*}$ and will recover only a random subset of the singular values of $A^{*}$ . The presence of these attractive saddles shows the complexity of the balanced dynamics. ", "page_idx": 4}, {"type": "text", "text": "A limitation of this approach is that it requires a quadratic cost and a very specific initialization, and in the case of positive alignment, an initialization that requires knowledge of the (SVD of the) ", "page_idx": 4}, {"type": "text", "text": "true function $A^{*}$ . Nevertheless, the positively aligned and balanced dynamics seem to capture some qualitative phenomenon that has been observed empirically outside of this restricted setting. This is the phenomenon of incremental learning, where if the singular values are initialized as very small, they first grow very slowly, but the multiplicative momentum will lead to come up one by one in a very abrupt manner, and this leads to a low rank bias where the network first only fits the largest singular value, then two largest, and so on. More generally, this can be interpreted as the network performing a greedy low-rank algorithm [35]. ", "page_idx": 5}, {"type": "text", "text": "Our analysis will confirm the fact that positive alignment happens naturally as a result of a short period of lazy training, allowing us to prove similar decoupling and incremental learning for a general random initialization. ", "page_idx": 5}, {"type": "text", "text": "Remark. We can define the time dependent map $\\Theta(G;t)\\,=\\,C_{2}(t)G+G C_{1}(t)$ , so that the GD dynamics can be rewritten as $A_{\\theta(t+1)}=A_{\\theta(t)}-\\eta\\Theta(\\nabla C(A_{\\theta(t)}),t)+O(\\eta^{2})$ . The map $\\Theta$ is none other than the NTK for shallow linear networks, but it has also been called the preconditioning matrix in previous work [7]. The lazy regime is then characterized by the NTK $\\Theta$ being approximately equal to the time-independent NTK $\\Theta^{\\mathrm{lazy}}(G)=2\\sigma^{2}w G$ , whereas the balanced regime is characterized by the time-dependent $\\Theta^{\\mathrm{bal}}(G;t)=\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}}G+G\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}},$ , with the distinction that the time dependence is only through $A_{\\theta(t)}$ . ", "page_idx": 5}, {"type": "text", "text": "2 Mixed Lazy/Balanced Dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Both lazy and balanced dynamics have the surprising but very useful property that the evolution of the network matrix $A_{\\theta}$ is approximately self-consistent: the evolution of $A_{\\theta}$ can be expressed in terms of itself. The lazy approximation becomes correct for a sufficiently large initialization, while the balanced one is correct for a balanced initialization. However, for most initializations, neither of these approximations are correct. ", "page_idx": 5}, {"type": "text", "text": "We flil this gap by providing a self-consistent evolution of $A_{\\theta}$ that applies for any initialization scale: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}A_{\\theta(t+1)}\\approx-\\eta\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}+\\sigma^{4}w^{2}I}\\nabla C(A_{t})-\\eta\\nabla C(A_{t})\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}+\\sigma^{4}w^{2}I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This approximation is formalized in the following theorem, denoting $\\hat{C}_{1}(t)=\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}+\\sigma^{4}w^{2}I}$ and $\\hat{C}_{2}(t)=\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}+\\sigma^{4}w^{2}I}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For a linear net $A_{\\theta}=W_{2}W_{1}$ with width $w$ , initialized with i.i.d. ${\\mathcal{N}}(0,\\sigma^{2})$ weights and trained with Gradient Flow, we have with high probability that for all time $t$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\Vert C_{1}(t)-\\hat{C}_{1}(t)\\right\\Vert_{o p},\\left\\Vert C_{2}(t)-\\hat{C}_{2}(t)\\right\\Vert_{o p}\\leq m i n\\left\\{O(\\sigma^{2}w),O\\left(\\sqrt{\\frac{d}{w}}\\left\\Vert C_{1}(t)\\right\\Vert_{o p}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. (sketch) The quantity $W_{1}W_{1}^{T}-W_{2}^{T}W_{2}$ is invariant under GF (and approximately so under GD) and it is approximately equal to $\\sigma^{2}w(P_{1}\\mathrm{~-~}P_{2})$ for two orthogonal projections $P_{1},P_{2}$ (at initialization and for all subsequent times because of the invariance). We therefore have ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{1}^{T}(W_{1}W_{1}^{T}-W_{2}^{T}W_{2})^{2}W_{1}\\approx\\sigma^{4}w^{2}W_{1}^{T}(P_{1}+P_{2})W_{1}\\approx\\sigma^{4}w^{2}C_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus the pairs $C_{1},C_{2}$ approximately satisfy the following equations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\approx C_{1}^{3}-A_{\\theta}^{T}A_{\\theta}C_{1}-C_{1}A_{\\theta}^{T}A_{\\theta}-\\sigma^{4}w^{2}C_{1}+A_{\\theta}^{T}C_{2}A_{\\theta}}\\\\ &{0\\approx C_{2}^{3}-A_{\\theta}A_{\\theta}^{T}C_{2}-C_{2}A_{\\theta}A_{\\theta}^{T}-\\sigma^{4}w^{2}C_{2}+A_{\\theta}C_{1}A_{\\theta}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The pair $\\hat{C}_{1},\\hat{C}_{2}$ is a solution of the above, and one can show that $C_{1},C_{2}$ must approach them and not any of the other solutions. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "The takeaway from theorem 1 is the following. ", "page_idx": 5}, {"type": "text", "text": "It is true that the error does not vanish. However, for our purpose it suffices to show that $\\|\\hat{C}_{1}-C_{1}\\|_{o p}$ is infinitely smaller than $C_{1}$ for all times, regardless of the magnitude of $\\|C_{1}(t)\\|_{o p}$ . ", "page_idx": 6}, {"type": "text", "text": "We see how both the lazy and balanced dynamics appear as special cases depending on how large the variance at initialization $\\sigma^{2}$ is in comparison to the singular values of the matrix $A_{\\theta(t)}$ : ", "page_idx": 6}, {"type": "text", "text": "\u2022 Lazy: When $\\sigma^{2}w\\gg s_{m a x}(A_{\\theta(t)})$ , then $\\hat{C}_{1}\\approx\\sigma^{2}w I_{d_{o u t}}$ and $\\hat{C}_{2}\\approx\\sigma^{2}w I_{d_{i n}}$ , recovering the lazy dynamics. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Balanced: When $\\sigma^{2}w\\ll s_{m i n}(A_{\\theta(t)})$ , then $\\hat{C}_{1}\\approx\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}}$ and $\\hat{C}_{2}\\approx\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}},$ recovering the balanced dynamics. ", "page_idx": 6}, {"type": "text", "text": "But clearly there can be times when neither conditions are satisfied, when some singular values of $A_{\\theta(t)}$ are larger than the threshold $\\sigma^{2}w$ while others are smaller, in such cases we are in a mixed regime, where the network is lazy along the small singular values of $A_{\\theta(t)}$ $\\langle s_{i}\\ll\\sigma^{2}w\\rangle$ and active/balanced along the large ones $(s_{i}\\gg\\sigma^{2}w)$ . ", "page_idx": 6}, {"type": "text", "text": "At initialization, the singular values are of size $\\sigma^{2}{\\sqrt{w d}}$ . This implies that with overparametrization $(w\\gg d)$ , all singular values start in the lazy regime and follow the simple lazy dynamics, which may (or may not) lead to some singular growing and crossing the $\\sigma^{2}w$ threshold, at which point they will switch to balanced dynamics (after a short transition period when the singular value is around the threshold $s_{i}\\approx\\sigma^{2}w_{j}$ ). Once a singular value is far past the threshold ${{s}_{i}}\\gg\\sigma^{2}w$ , training along this singular value will be much faster than along the lazy singular values (this speed up can be seen in Figure 1). This allows the newly active singular values to converge while the lazy singular values remain almost constant. Once the active singular values have converged, the slow training of the remaining lazy singular values continues until some of these singular values reaches the threshold, or until GD converges. ", "page_idx": 6}, {"type": "text", "text": "This type of behavior is illustrated by the following formula, which describes the derivative in time of the $i$ -th singular value $s_{i,t}$ of $A_{t}$ , with singular vectors $u_{i,t},v_{i,t}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{i,t+1}-s_{i,t}\\approx\\eta_{t}u_{i,t}^{T}\\partial_{t}A_{\\theta(t)}v_{i,t}\\approx-2\\eta_{t}\\sqrt{s_{i,t}^{2}+\\sigma^{4}w^{2}}u_{i,t}^{T}\\nabla C(A_{\\theta(t)})v_{i,t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the prefactor $2\\eta_{t}\\sqrt{s_{i,t}^{2}+\\sigma^{4}w^{2}}$ describes the effective learning rate along the $i$ -th singular value, which depends on the $i$ -th singular value $s_{i,t}$ itself. ", "page_idx": 6}, {"type": "text", "text": "This suggests that it is more natural to distinguish between the lazy and active regime at a much more granular level: at every time $t$ a singular value can be either active or lazy (or very close to the transition but this typically only happens for a very short time). In contrast, the traditional definition of the lazy regime was defined for a whole network and over the whole training time. To avoid confusion, we call this the pure lazy regime, where all singular values remain lazy throughout training. This begs the question of whether a pure balanced regime also exists, but all singular values will always be lazy for at least a short time period (assuming $w>d_{\\phantom{},}$ ), and as we will see this short lazy period plays a crucial role in aligning the network so that the subsequent balanced regime can learn successfully. A pure balanced regime can only be obtained in the underparametrized regime, or by taking a balanced initialization instead of the traditional i.i.d. random initialization. ", "page_idx": 6}, {"type": "text", "text": "While this challenges the traditional lazy/active dichotomy, it also reinforces it, as it shows that there is no fundamentally different third regime, only lazy, active, and some mix of the two. Theorem 1 thus allows us to revisit previous descriptions of lazy and balanced dynamics and \u2018glue them together\u2019 to extend them to the general case. This simple strategy will allow to almost fully \u2018fill in the phase diagram\u2019, i.e. describe the dynamics, convergence and generalization properties of DLNs for almost all reasonable initialization scales $\\sigma^{2}$ and widths $w$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. The transition of a singular value $s_{i}$ from lazy to active can be understood as a form of alignment happening in the hidden layer: the two vectors $W_{1}v_{i}$ and $W_{2}^{T}u_{i}$ for $u_{i},v_{i}$ the left and right singular vectors of $s_{i}$ are orthogonal in the lazy regime and become perpendicular in the balanced where $A_{\\theta(t)}$ comes from GD and $\\hat{A}_{\\theta(t)}$ from the self-consistent dynamics. We observe that this distance is not only small for the region where our theoretical results apply but also almost everywhere outside this region. ", "page_idx": 6}, {"type": "image", "img_path": "9zQl27mqWE/tmp/08428e1d47a59b6de6627961be86e4cc0eadad5651dc386b9869cffe337e6563.jpg", "img_caption": ["Figure 2: As a function of $\\gamma_{\\sigma^{2}},\\gamma_{w}$ , we run GD and plot different quantities. Our theoretical results only apply to the top left region for $\\gamma_{w}>1$ and below the red line, although these plots suggest that some results may extend to smaller $\\gamma_{w}\\mathbf{s}$ . (Top left panel): We plot the smallest test error $\\begin{array}{r}{\\frac{1}{d^{2}}\\|A_{\\theta(t)}\\,-\\,A^{*}\\|_{F}^{2}}\\end{array}$ in the whole run. The active region (below the black line) has a small error while the lazy region does not. (Top right panel): We plot the stable rank of $A_{\\theta(t)}$ (defined as $\\|A_{\\theta(t)}\\|_{F}^{2}/\\|A_{\\theta(t)}\\|_{\\mathrm{op}}^{2})$ at the time of minimal test error. In this experiment, we took Rank $A^{*}=5$ . We see that the active region has approximately the correct rank while the lazy region overestimates it. (Bottom left panel): We plot the number of iterations until minimal test error, illustrating the trade-off between test error and training time. (Bottom right panel): We compute $\\begin{array}{r}{\\ln\\left(\\frac{1}{d^{2}}\\|A_{\\theta(t)}-\\hat{A}_{\\theta(t)}\\|_{F}^{2}\\right)}\\end{array}$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "regime. Indeed the normalized scalar product of these two vectors satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{u_{i}^{T}W_{2}W_{1}v_{i}}{\\left\\|W_{2}^{T}u_{i}\\right\\|\\|W_{1}v_{i}\\|}=\\frac{s_{i}}{\\sqrt{u_{i}^{T}C_{2}u_{i}}\\sqrt{u_{i}^{T}C_{1}u_{i}}}\\approx\\frac{s_{i}}{\\sqrt{s_{i}^{2}+\\sigma^{4}w^{2}}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is close to zero for lazy singular values $s_{i}\\ll\\sigma^{2}w$ and close to one for active ones $s_{i}\\gg\\sigma^{2}w$ . ", "page_idx": 7}, {"type": "text", "text": "2.1 Phase Diagram for MSE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate the power of Theorem 1 we provide a phase diagram of the behavior of large shallow networks on a MSE task, for almost all (reasonable) choices of width $w$ and variance $\\sigma^{2}$ scalings. ", "page_idx": 7}, {"type": "text", "text": "We want to recover a rank $K$ and $d\\times d$ -dimensional matrix $A^{*}$ with $s_{i}(A^{*})\\;=\\;d a_{i}$ for some $a_{1}\\geq a_{2}\\geq\\dots\\geq a_{K}$ independent of the dimension $d$ . We however only observe a noisy version $A^{*}+E$ for some $E$ such that $\\|E\\|_{o p}\\leq c_{0}d^{\\delta}$ . One could imagine $E$ to have iid random Gaussian entries ${\\mathcal{N}}(0,1)$ in which case $\\|E\\|_{o p}\\leq c_{0}\\sqrt{d}$ with high probability. ", "page_idx": 7}, {"type": "text", "text": "As the dimension $d$ grows, the size of the network needs to scale too, as well as the initialization variance, but it is unclear what is the optimal way to choose $w$ and $\\sigma^{2}$ . We will therefore consider general scalings $w=d^{\\gamma_{w}}$ and $\\sigma^{2}=d^{\\gamma}\\dot{\\sigma}^{2}$ . We will now describe the $(\\gamma_{w},\\gamma_{\\sigma^{2}})$ -phase diagram which features 4 regimes: underparametrized, infinite-noise, lazy and mixed/active. ", "page_idx": 8}, {"type": "text", "text": "We can identify a region of \u2018reasonable\u2019 pairs $(\\gamma_{\\sigma^{2}},\\gamma_{w})$ by ruling out degenerate behavior. First, the width $w$ needs to be larger than the dimension $d$ , since a network of width $w$ can only represent matrices of rank $w$ or less, this means that we need $\\gamma_{w}\\,\\geq\\,1$ . Another constraint comes from the variance of $A_{\\theta}$ at initialization: the entries $A_{\\theta(0),i j}$ at initialization have variance $\\sigma^{4}w$ . We want this variance to go to zero as $d$ grows which implies that we need $2\\gamma_{\\sigma^{2}}+\\gamma_{w}<0$ . ", "page_idx": 8}, {"type": "text", "text": "Now within this reasonable region we observe two regimes, the pure lazy regime for $1<\\gamma_{\\sigma^{2}}+\\gamma_{w}$ where the network simply ftis $A^{*}+E$ thus failing to learn $A^{*}$ and the mixed regime for $1>\\gamma_{\\sigma^{2}}+\\gamma_{w}$ where the dynamics are lazy for a short amount of time until $K$ singular values grow large enough to switch to the balanced dynamics and fit the true matrix $A^{*}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. For pairs $\\gamma_{w},\\gamma_{\\sigma^{2}}$ such that $\\gamma_{w}>1$ and $2\\gamma_{\\sigma^{2}}+\\gamma_{w}<0,$ , we have two regimes: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Lazy $(1\\,\\,<\\,\\gamma_{\\sigma^{2}}\\,+\\,\\gamma_{w})$ : with a learning rate $\\begin{array}{r}{\\eta\\ \\ll\\ \\frac{d^{2}}{\\sigma^{2}w}}\\end{array}$ we have that for all time $t$ $\\begin{array}{r}{\\frac{1}{d^{2}}\\left\\|A_{\\theta(t)}-A^{*}\\right\\|_{F}^{2}\\geq c.}\\end{array}$   \n\u2022 Active $(1>\\gamma_{\\sigma^{2}}+\\gamma_{w})$ : with a learning rate $\\begin{array}{r}{\\eta\\ll\\frac{d^{2}}{s_{1}(A^{*})}\\sim d,}\\end{array}$ , and at time $t=\\frac{1}{\\eta}\\left(\\frac{\\Delta}{a_{K}}+\\frac{2m a x(1,2\\Delta)}{c(a_{1},\\dots,a_{K})}+\\frac{m a x(1,2\\Delta)}{2a_{K}}\\right)d\\log d+\\eta^{-1}O(d\\log\\log d),$ for $\\Delta=1-\\gamma_{\\sigma^{2}}-\\gamma_{w}>0,$ , we have that $\\begin{array}{c}{{\\displaystyle\\frac1{d^{2}}\\left\\|A_{\\theta(t)}-A^{*}\\right\\|_{F}^{2}\\le{\\cal O}(\\sigma^{4}w+\\frac{\\sigma^{4}w^{2}\\log^{2}d}{d^{2}}+d^{-\\frac12}+\\frac{\\sigma^{2}w}{d}+\\eta^{2}\\frac{\\log^{2}d}{d^{2}}),}}\\\\ {{r\\,c(a_{1},\\dots,a_{K})=\\frac{m i n_{k,j:a_{k}\\neq a_{j}}|a_{k}-a_{j}|a_{K}^{2}}{m a x_{k,j:a_{k}\\neq a_{j}}|a_{k}^{2}-a_{j}^{2}|}.}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Note that all the terms inside the final $O(\\dots)$ term vanish: $\\sigma^{4}w\\;\\rightarrow\\;0$ because $\\gamma_{\\sigma^{2}}+\\gamma_{w}\\,<\\,0$ , $\\begin{array}{r}{\\frac{\\sigma^{4}w^{2}\\log^{2}d}{d^{2}}+\\frac{\\sigma^{2}w}{d}\\rightarrow0}\\end{array}$ since $1>\\gamma_{\\sigma^{2}}+\\gamma_{w}$ , and $\\eta^{2}{\\frac{\\log^{2}d}{d^{2}}}\\to0$ since we assumed $\\eta\\ll d$ . ", "page_idx": 8}, {"type": "text", "text": "This shows that the lazy regime only appears for very large widths $\\gamma_{w}>2$ (or at least the lazy regime with finite variance at initialization). Indeed the choice $\\gamma_{w}=2,\\gamma_{\\sigma^{2}}=-1$ is at the boundary of the lazy regime with the smallest $\\gamma_{w}$ . This could explain why it is rare to observe the lazy regime in practice. ", "page_idx": 8}, {"type": "text", "text": "Our theoretical results applies to the overparametrized regime $w\\gg d$ , but actually we only want to fti $A^{*}$ which has a much smaller rank $r$ , and so we might only need $w\\gg r$ . Figure 2, top left panel, confirms this, since we see a good generalization even for small widths $w<d$ , and in particular when $w\\approx\\mathrm{Rank}A^{*}$ . But to leverage this underparametrized regime, one would need to know the rank of the true matrix $A^{*}$ in advance, which is typically not the case in practice. Nevertheless, the interesting behavior we observe in the (mildly) underparametrized regime warrants further analysis, and the fact that our self-consistent dynamics remain a good approximation in this regime (Figure 2, bottom right panel), suggests that the analysis we present here could be extended to this regime too. ", "page_idx": 8}, {"type": "text", "text": "Finally, we observe a trade-off between generalization error and training time: on one hand the test error has terms that scale negatively with $1-\\gamma_{\\sigma^{2}}-\\gamma_{w}$ , which is the distance to the lazy/active transition, on the other hand, the time it takes to reach the minimal loss point scales positively with the same term. This can be seen from Figure 2, bottom left panel, which plots the number of steps required to reach minimal test error, which increases as one goes further into the active regime. ", "page_idx": 8}, {"type": "text", "text": "Remark. In general when trying to fit a matrix $B$ (instead of the special case $B=A^{*}+E)$ , the transition between lazy and mixed regime is when $\\sigma^{2}w\\approx\\|B\\|_{o p}$ . Thus the exact location of the transition is task-dependent, so that the same variance $\\sigma^{2}$ and width $w$ can lead to NTK or mixed regimes depending on the task. For example, let us assume that $A^{*}$ is full-rank instead of finite rank, then we expect $\\|A^{*}\\|_{o p}\\sim{\\sqrt{d}}$ instead of $\\|A^{*}\\|_{o p}\\sim d$ , thus the transition would be at $\\textstyle{\\frac{1}{2}}=\\gamma_{\\sigma^{2}}+\\gamma_{w}$ instead of $1=\\gamma_{\\sigma^{2}}+\\gamma_{w}$ . This suggests that linear networks are able to adapt themselves to the task: ", "page_idx": 8}, {"type": "text", "text": "leveraging active dynamics when the true data is low-rank to get better generalization, or remaining in the lazy dynamics in the absence of low-rank structure, to take advantage of the faster convergence. Note also that in the absence of sparsity, the lazy regime can be attained with a smaller width $(\\gamma_{w}>1$ instead of $\\gamma_{w}\\,>\\,2,$ ), since the choice $\\gamma_{w}\\,=\\,\\mathrm{j},\\gamma_{\\sigma^{2}}\\,=\\,-\\,\\frac{1}{2}$ is already on the boundary of the lazy regime. ", "page_idx": 9}, {"type": "text", "text": "3 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We prove a surprisingly simple self-consistent dynamic for the evolution of the matrix represented by a shallow linear network under gradient descent. This description not only unifies the already known lazy and balanced dynamics, but reveals the existence of a spectrum of mixed dynamics where some of the singular values are lazy while others are balanced. ", "page_idx": 9}, {"type": "text", "text": "Thanks to this description we are able to give an almost complete phase diagram of training dynamics as a function of the scaling of the width and variance at initialization w.r.t. the dimension. ", "page_idx": 9}, {"type": "text", "text": "A natural question that comes out of these results is whether nonlinear network also feature similar mixed regimes, and whether they could be the key to understand the convergence of general DNNs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.   \n[2] Emmanuel Abbe, Enric Boix-Adser\u00e0, Matthew Stewart Brennan, Guy Bresler, and Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[3] Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural networks, 2017.   \n[4] Bloemendal Alex, L\u00e1szl\u00f3 Erdos, Antti Knowles, Horng-Tzer Yau, and Jun Yin. Isotropic local laws for sample covariance and generalized wigner matrices. 2014.   \n[5] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. pages 242\u2013252, 2019.   \n[6] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019.   \n[7] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 244\u2013253. PMLR, 10\u201315 Jul 2018.   \n[8] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.   \n[9] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32, 2019.   \n[10] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[11] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine Learning Research, 18(1):629\u2013681, 2017. ", "page_idx": 9}, {"type": "text", "text": "[12] Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013. ", "page_idx": 10}, {"type": "text", "text": "[13] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. Advances in Neural Information Processing Systems, 35:32240\u201332256, 2022.   \n[14] Lukas Braun, Cl\u00e9mentine Domin\u00e9, James Fitzgerald, and Andrew Saxe. Exact learning dynamics of deep linear networks with prior knowledge. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 6615\u20136629. Curran Associates, Inc., 2022.   \n[15] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming. arXiv preprint arXiv:1812.07956, 2018.   \n[16] L\u00e9na\u00efc Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Overparameterized Models using Optimal Transport. In Advances in Neural Information Processing Systems 31, pages 3040\u20133050. Curran Associates, Inc., 2018.   \n[17] L\u00e9na\u00efc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 1305\u20131338. PMLR, 09\u201312 Jul 2020.   \n[18] L\u00e9na\u00efc Chizat, Maria Colombo, Xavier Fern\u00e1ndez-Real, and Alessio Figalli. Infinite-width limit of deep linear neural networks. Communications on Pure and Applied Mathematics, 2022.   \n[19] Zhen Dai, Mina Karzand, and Nathan Srebro. Representation costs of linear neural networks: Analysis and design. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[20] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2019.   \n[21] Kenji Fukumizu. Effect of batch learning in multilayer neural networks. In International Conference on Neural Information Processing, 1998.   \n[22] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020(11):113301, 2020.   \n[23] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[24] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 23607\u201323629. Curran Associates, Inc., 2023.   \n[25] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. ICLR, 2023.   \n[26] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In The Eleventh International Conference on Learning Representations, 2023.   \n[27] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31, pages 8580\u20138589. Curran Associates, Inc., 2018.   \n[28] Arthur Jacot, Fran\u00e7ois Ged, Berfin \u00b8Sims\u00b8ek, Cl\u00e9ment Hongler, and Franck Gabriel. Saddle-tosaddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.   \n[29] Arthur Jacot, Eugene Golikov, Cl\u00e9ment Hongler, and Franck Gabriel. Feature learning in $l_{2}$ -regularized dnns: Attraction/repulsion and sparsity. In Advances in Neural Information Processing Systems, volume 36, 2022.   \n[30] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. CoRR, abs/1810.02032, 2018.   \n[31] Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon Shaolei Du, and Jason D. Lee. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 15200\u201315238. PMLR, 23\u201329 Jul 2023.   \n[32] Daniel Kunin, Allan Ravent\u00f3s, Cl\u00e9mentine Domin\u00e9, Feng Chen, David Klindt, Andrew Saxe, and Surya Ganguli. Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning, 2024.   \n[33] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.   \n[34] Aitor Lewkowycz and Guy Gur-Ari. On the training dynamics of deep networks with l_2 regularization. Advances in Neural Information Processing Systems, 33:4790\u20134799, 2020.   \n[35] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In International Conference on Learning Representations, 2020.   \n[36] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for overparameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint arXiv:2003.00307, 2020.   \n[37] Hancheng Min, Salma Tarmoun, Ren\u00e9 Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. In International Conference on Machine Learning, pages 7760\u20137768. PMLR, 2021.   \n[38] Hancheng Min, Rene Vidal, and Enrique Mallada. On the convergence of gradient flow on multi-layer linear models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 24850\u201324887. PMLR, 23\u201329 Jul 2023.   \n[39] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width relu nets: The multivariate case. In International Conference on Learning Representations, 2020.   \n[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. 2019.   \n[41] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7475\u20137505. Curran Associates, Inc., 2023.   \n[42] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefti of stochasticity. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 29218\u201329230. Curran Associates, Inc., 2021.   \n[43] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks. In Advances in Neural Information Processing Systems 31, pages 7146\u20137155. Curran Associates, Inc., 2018.   \n[44] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014.   \n[45] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537\u201311546, 2019.   \n[46] Salma Tarmoun, Guilherme Franca, Benjamin D Haeffele, and Rene Vidal. Understanding the dynamics of gradient flow in overparameterized linear models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10153\u201310161. PMLR, 18\u201324 Jul 2021.   \n[47] Zihan Wang and Arthur Jacot. Implicit bias of SGD in $l_{2}$ -regularized linear DNNs: Oneway jumps from high to low rank. In The Twelfth International Conference on Learning Representations, 2024.   \n[48] Blake Woodworth, Suriya Gunasekar, Pedro Savarese, Edward Moroshko, Itay Golan, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models, 2020.   \n[49] Nuoya Xiong, Lijun Ding, and Simon Shaolei Du. How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization. In OPT 2023: Optimization for Machine Learning, 2023.   \n[50] Ziqing Xu, Hancheng Min, Salma Tarmoun, Enrique Mallada, and Rene Vidal. Linear convergence of gradient descent for finite width over-parametrized linear networks with general initialization. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 2262\u20132284. PMLR, 25\u201327 Apr 2023.   \n[51] Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548, 2020.   \n[52] Greg Yang and Edward J. Hu. Feature learning in infinite-width neural networks, 2020.   \n[53] Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis\u2013kahan theorem for statisticians. Biometrika, 102(2):315\u2013323, 2015.   \n[54] Difan Zou, Philip M Long, and Quanquan Gu. On the global convergence of training deep linear resnets. arXiv preprint arXiv:2003.01094, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "The appendix is structured as follows. ", "page_idx": 13}, {"type": "text", "text": "\u2022 In section A, we introduce the notation, and establish several results about how perturbing a matrix would impact its singular vectors.   \n\u2022 In section C, we study the gradient flow dynamics of $A_{t}$ in the active regime and prove that $A_{t}$ will be approximately aligned with $A^{*}$ throughout the Saddle-to-Saddle regime.   \n\u2022 In section B, we prove theorem 1 for general cost.   \n\u2022 In section D, we study the gradient flow dynamics for $A_{\\theta(t)}$ in the lazy regime.   \n\u2022 In section E, we show that $A_{\\theta(t)}$ is also approximately aligned with $A^{*}$ throughout the Saddle-to-Saddle regime, using results in section C and section B. In subsection E.3, we summarize the approximate dynamics of $A_{\\theta(t)}$ throughout training. In section E.4, we bound the final test error.   \n\u2022 In section F, we bound the error from gradient descent and prove theorem 2.   \n\u2022 In section G, we describe the experimental setup. ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Convention and Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Constants. $d$ is the dimension of the input and the output layer, $K$ is the rank of matrix $A^{*}$ , and $w$ is the dimension of hidden layer. $c$ and $C$ will usually denote constants that are independent of $d$ , and depending on the context, the value of $c$ and $C$ might be different. If $x$ is a scalar that depends on $d$ and $y$ is a scalar, then $x=O(y)$ means there exists a constant $c$ independent of $d$ , such that for $d$ sufficiently large we have $x\\leq c y$ . If $A$ is a matrix, then $A=O(y)$ means there exists a constant $c$ independent of $d$ such that for $d$ sufficiently large, $\\|A\\|_{o p}\\leq c y$ . $O(y)$ can be either a matrix or a scalar, and its meaning will be always clear from context. ", "page_idx": 13}, {"type": "text", "text": "Matrix. We use $\\|\\cdot\\|_{o p},\\|\\cdot\\|_{F},\\|\\cdot\\|$ to denote the operator norm of a matrix, the Frobenius norm of a matrix, and the $L^{2}$ norm for vectors. For every matrix $A$ , we define max $A$ as the $L^{\\infty}$ norm, $\\operatorname*{max}_{i,j}|A_{i j}|$ . We use $I$ to denote identity matrix, the dimension of which is determined by context. We shall assume that the signal singular values of $A^{*}$ are $s_{1}^{*},\\ldots,s_{K}^{*}$ . $\\forall i=1,2,\\ldots,K$ , and $s_{i}^{*}=a_{i}d$ where $a_{1}\\geq a_{2}\\geq...\\geq a_{K}$ are constants independent of $d$ . By selecting proper basis in the input and output space, we assume that $A^{*}=S^{*}$ , where $S^{*}$ is the diagonal matrix consisting of singular values of $A^{*}$ , ordered from largest to smallest. The $p,q$ -th element of a matrix $A$ is denoted $A_{p q}$ . We reserve the notation $A(i,j)$ for the $i,j$ -th block matrix of $A$ , which we shall define below. ", "page_idx": 13}, {"type": "text", "text": "Submatrix. Assume that $n_{0}\\,=\\,0$ , and $a_{1}=.\\,.\\,.\\,=\\,a_{n_{1}}>a_{n_{1}+1}=.\\,.\\,.\\,a_{n_{2}}>.\\,.\\,.\\,=a_{n_{m}}=a_{K},$ and let $n_{m+1}\\,=\\,d$ . For a matrix $U$ , we define the $k,j$ -th sub-block $U(k,j)$ of a matrix $U$ as $U_{n_{k-1}+1:n_{k+1},n_{j-1}+1:n_{j}}$ , with both sides included. Notice that $U^{T}(k,j)=U(\\dot{j},\\dot{k})^{T}$ . In this notation, we can write the singular value decomposition of a matrix $A$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\nA(i,j)=\\sum_{k:\\mathrm{signal}}U(i,k)S(k,k)V(j,k)+U(i,m+1)S(m+1,m+1)V^{T}(m+1,j).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We call an index $k$ (of sub-block) \"signal\", if $k\\leq m$ . Index $m+1$ is called \"noise\". Let $S(k,k)^{*}$ be block matrix $A^{*}(n_{k}~:~n_{k+1},n_{k}~:~n_{k+1})$ . Then $A^{*}\\;=\\;\\mathrm{diag}(S(1,1)^{*},\\ldots,S(m,m)^{*})$ and $S(k,k)^{*}=s_{n_{k}}I$ is the $k$ -th sub-block of $A^{*}$ . Each matrix has only finitely many sub-blocks. ", "page_idx": 13}, {"type": "text", "text": "Indexing Conventions. Entries of matrices will usually be indexed by $p,q,r$ and sub-blocks of matrices will usually be indexed by $i,j,k,\\ell$ . Usually $k$ ranges from 1 to $m$ , and $j$ usually ranges from 1 to $m+1$ . ", "page_idx": 13}, {"type": "text", "text": "Element-wise Product.We use $\\odot$ to represent element-wise product of two matrix of the same shape. ", "page_idx": 13}, {"type": "text", "text": "Important Assumptions. Throughout the paper, we shall always assume that ", "page_idx": 13}, {"type": "text", "text": "A.2 Matrix Inequalities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the proof of main theorems, we will work extensively with inequalities of matrix norms and inequalities that involves element-wise product. The element-wise product appears naturally in the derivative of singular vectors of a matrix. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Assume that $A,\\,B$ and $R$ are square matrices. Let $R_{m a x}=m a x_{i,j}|R_{i j}|$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\nt r[A(R\\odot B)]=t r[B R^{T}\\odot A],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n|t r[A(R\\odot B)]|\\leq R_{m a x}\\sqrt{t r(A^{T}A)}\\sqrt{t r(B^{T}B)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, $i f\\forall p,q,R_{p q}\\geq R_{m i n}>0$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\nt r[A^{T}(R\\odot A)]\\geq R_{m i n}t r(A^{T}A)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. All are simple computations. ", "page_idx": 14}, {"type": "equation", "text": "$$\nt r[A R\\odot B]=\\sum_{p,q}A_{p q}R_{q p}B_{q p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nt r[B R^{T}\\odot A]=\\sum_{p,q}B_{p q}R_{p q}A_{q p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The two equations above prove the first claim. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|t r[A(R\\odot B)]\\right|\\le\\!\\sqrt{t r(A^{T}A)}\\sqrt{t r((R\\odot B)^{T}R\\odot B)}}\\\\ &{\\qquad\\qquad\\qquad\\le\\!R_{\\operatorname*{max}}\\sqrt{t r(A^{T}A)}\\sqrt{t r(B^{T}B)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes the second claim. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle t r[A^{T}(R\\odot A)]=\\sum_{i,j}A_{j i}R_{j i}A_{j i}}&{{}}\\\\ {\\geq R_{\\mathrm{min}}t r[A^{T}A]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes the third claim. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. $\\sigma_{m i n}(A)\\|B\\|_{F}\\leq\\|A B\\|_{F}\\leq\\sigma_{m a x}(A)\\|B\\|_{F}.$ ", "page_idx": 14}, {"type": "text", "text": "Proof. This is lemma B.3 of [54]. ", "page_idx": 14}, {"type": "text", "text": "A.3 Perturbation of Singular Values and Singular Vectors ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We will often use the following variant of the Davis-Kahan $\\sin\\theta$ theorem. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.3 (DK- $\\sin\\theta$ Theorem). . Let $\\Sigma,\\hat{\\Sigma}\\,\\in\\,\\mathbb{R}^{p\\times p}$ be symmetric, with eigenvalues $\\lambda_{1}~\\geq$ $\\dots\\geq\\lambda_{p}$ and $\\hat{\\lambda}_{1}\\geq...\\geq\\hat{\\lambda}_{p}$ . Fix $1\\leq r\\leq s\\leq p,$ , let $d=r-s+1$ and let $V=\\left(v_{r},\\ldots,v_{s}\\right)$ and $\\hat{V}=\\big(\\hat{v}_{r},\\dots,\\hat{v}_{s}\\big)$ have orthonormal columns satisfying $\\Sigma v_{j}=\\lambda_{j}v_{j}$ and $\\Sigma\\hat{v}_{j}=\\hat{\\lambda}_{j}v_{j}$ . Let $\\sigma_{1},\\ldots,\\sigma_{d}$ be the singular values of $\\hat{V}^{T}V$ . Let $\\Theta(V,{\\hat{V}})$ be the diagonal matrix with cos $\\Theta(V,\\hat{V})_{j j}=\\sigma_{j}$ and $\\sin\\Theta(V,\\hat{V})$ be defined entry-wise. Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\sin\\Theta(V,\\hat{V})\\|_{F}\\leq\\frac{2m i n(\\sqrt{d}\\|\\hat{\\Sigma}-\\Sigma\\|_{o p},\\|\\hat{\\Sigma}-\\Sigma\\|_{F})}{m i n(\\lambda_{r-1}-\\lambda_{r},\\lambda_{s}-\\lambda_{s+1})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. This is theorem 2 in [53]. ", "page_idx": 14}, {"type": "text", "text": "The implication of the theorem is that if two matrices are sufficiently close, then their singular vectors are also close to each other. In the case where $r=s$ and $\\lambda_{r}$ is of multiplicity 1, the theorem reduces to saying that the sine value of the angle between $v_{r}$ and $\\hat{v}_{r}$ is very small. ", "page_idx": 15}, {"type": "text", "text": "The term $\\|\\sin\\Theta(V,\\hat{V})\\|_{F}$ is complicated to take derivative. In this paper we will use the following characterization of alignment, which is easier to take derivatives. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4. Let $\\hat{\\Sigma}$ be a $d\\times d$ diagonal matrix, and let $s_{1},\\ldots,s_{K},\\ldots,s_{d}$ be its diagonal entries. Assume that $s_{1}=\\ldots=s_{n_{1}}>s_{n_{1}+1}=\\ldots=s_{n_{2}}\\geq\\ldots s_{n_{m}}=s_{K}>s_{K+1}=\\ldots=s_{d}.$ . Then $\\hat{\\Sigma}$ has $m+1$ blocks in total. Assume that $\\|X-{\\hat{\\Sigma}}\\|_{o p}=d^{\\alpha}$ . Let $X=U S V^{T}$ , and define ", "page_idx": 15}, {"type": "equation", "text": "$$\nx=4K-\\sum_{k:s i g n a l}\\mathrm{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k)+2U(k,k)V(k,k)^{T}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 15}, {"type": "equation", "text": "$$\nx\\leq K\\frac{\\|\\hat{\\Sigma}-X\\|_{\\sigma p}}{s_{K}}+\\frac{m s_{1}}{s_{K}}\\left(\\frac{2m i n(\\sqrt{K}(\\|X\\|_{\\sigma p}+\\|\\hat{\\Sigma}\\|_{\\sigma p}),\\|X\\|_{F}+\\|\\hat{\\Sigma}\\|_{F})}{m i n_{k}(s_{n_{k}}^{2}-s_{n_{k+1}}^{2})}\\right)^{2}\\|X-\\hat{\\Sigma}\\|_{\\sigma p}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Remark. To better understand $x$ , consider the special case where each signal singular value is of multiplicity $^{\\,l}$ . Since matrix $X$ is \"close\" to diagonal matrix $\\hat{\\Sigma},$ , matrix $U$ and matrix $V$ should also be close to identity along signal directions: $|U_{k k}|\\approx1$ , $|V_{k k}|\\approx1$ , $U_{k k}V_{k k}\\approx1$ for $1\\le k\\le K$ . Quantity $x$ captures how much $|U_{k k}|,|V_{k k}|,U_{k k}V_{k k}$ deviate from $^{\\,l}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $X~=~U S V^{T}$ . Then $X^{T}X~=~V S^{2}V^{T}$ is a symmetric matrix. Let $V(:,k)\\;\\;=\\;\\;$ $(v_{n_{k-1}+1},\\ldots,v_{n_{k}})$ be the singular vectors of $X$ corresponding to $s_{n_{k-1}+1},\\ldots,s_{n_{k}}$ . There is some freedom to choose $\\hat{V}$ , but for simplicity we pick $\\hat{V}(:,k)=(e_{n_{k-1}+1},\\dots,e_{n_{k}})$ where $e_{i}$ is the $i$ -th coordinate vector. As a result, $\\hat{V}(:,k)^{T}V(:,k)=V(k,k)$ . Apply $\\operatorname{DK-sin}\\theta$ theorem to $V$ and $\\hat{V}$ , we see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\sin\\Theta(V(:,k),\\hat{V}(:,k))\\|_{F}\\le\\frac{2\\operatorname*{min}(\\sqrt{n_{k}-n_{k-1}}\\|X^{T}X-\\hat{\\Sigma}^{2}\\|_{o p},\\|X^{T}X-\\hat{\\Sigma}^{2}\\|_{F})}{\\operatorname*{min}(s_{n_{k-1}}^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k}+1}^{2})}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\sigma_{p}$ be the singular values of $V(k,k)$ , for $1\\leq p\\leq n_{k}-n_{k-1}$ . Here we are using the notation for sub-block of a big matrix. Then $\\sin\\Theta(V(:,k),\\hat{V}(:,k))$ is the diagonal matrix, whose diagonal entries are given by $\\sqrt{1-\\sigma_{p}^{2}}$ . So $\\begin{array}{r}{\\|\\sin\\Theta(V,\\hat{V})\\|_{F}^{2}=\\sum_{p}(1-\\sigma_{p}^{2})\\,=\\,t r[I-V(k,k)^{T}V(k,k)]}\\end{array}$ . Now observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|X^{T}X-\\hat{\\Sigma}^{2}\\|_{o p}\\leq\\!(\\|X\\|_{o p}+\\|\\hat{\\Sigma}\\|_{o p})\\|X-\\hat{\\Sigma}\\|_{o p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|X^{T}X-\\hat{\\Sigma}^{2}\\|_{F}\\leq\\!(\\|X\\|_{F}+\\|\\hat{\\Sigma}\\|_{F})\\|X-\\hat{\\Sigma}\\|_{o p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r[I-V(k,k)^{T}V(k,k)]\\leq\\left(\\frac{2\\operatorname*{min}(\\sqrt{n_{k}-n_{k-1}}(\\|X\\|_{\\sigma p}+\\|\\hat{\\Sigma}\\|_{\\sigma p}),\\|X\\|_{F}+\\|\\hat{\\Sigma}\\|_{F})}{\\operatorname*{min}(s_{n_{k-1}}^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k+1}}^{2})}\\|X-\\hat{\\Sigma}\\|_{\\sigma p}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similar conclusion is true for $U(k,k)$ . Next we bound $t r(I-U(k,k)V(k,k)^{T})$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\hat{\\Sigma}(k,k)-X(k,k)\\|_{F}^{2}=\\sum_{q_{1},q_{2}=n_{k-1}+1}^{n_{k}}(\\hat{\\Sigma}_{q_{1}q_{2}}-X_{q_{1}q_{2}})^{2}\\leq(n_{k}-n_{k-1})\\|\\hat{\\Sigma}-X\\|_{o p}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\widehat{\\Sigma}(k,k)-X(k,k)\\vert_{F}^{2}=\\vert\\widehat{\\Sigma}(k,k)-\\sum_{j}U(k,j)S(j,j)V^{T}(k,k)\\vert_{F}^{2}\\vert}&{}\\\\ &{\\quad-\\vert\\widehat{\\Sigma}(k,k)-\\sum_{j\\neq k}^{\\infty}\\vert\\nu(k,j)S(j,j)V(k,k)\\vert_{F}^{2}-U(k,k)S(k,k)V(k,k)^{T}\\vert_{F}^{2}}\\\\ &{\\quad-\\vert\\widehat{\\Sigma}(k,k)-U(k,k)\\vert_{\\widehat{\\Sigma}(k,k)}V(k,k)^{T}\\vert_{F}^{2}}\\\\ &{\\quad-\\left\\Vert\\sum_{j\\leq N}U(k,j)S(j,j)V(k,j)^{T}\\vert_{F}^{2}\\right.}\\\\ &{\\quad-\\left\\Vert U(k,k)\\vert_{G}(k,k)-\\sum_{\\widehat{\\Sigma}(k,k)}\\vert\\nu(k,k)^{T}\\vert_{F}^{2}\\right\\Vert_{F}^{2}}\\\\ &{\\quad\\left.\\geq z_{m}^{2}\\vert\\vert I-U(k,k)\\vert_{F}^{2}\\vert\\leq\\vert\\delta(j,k)\\vert_{F}^{2}\\vert\\right\\Vert_{F}^{2}}\\\\ &{\\quad-\\sum_{j\\neq N}\\vert\\nu(k,j)\\vert_{F}^{2}\\vert S(j,j)\\vert V(k,j)\\vert_{F}^{2}}\\\\ &{\\quad-\\left\\Vert X-\\sum_{\\widehat{\\Sigma}(k)}^{2}\\right\\Vert_{F}^{2}}\\\\ &{\\quad\\geq z_{m}^{2}\\vert\\vert I-U(k,k)\\vert_{F}^{2}\\vert\\vert_{F}^{2}}\\\\ &{\\quad-\\vert\\widehat{\\Pi}(k,k)\\vert_{G}^{2}\\vert\\vert_{F}(k,k)\\vert_{F}^{2}\\vert}\\\\ &{\\quad-\\vert\\widehat{\\Pi}(k,k)\\vert_{G}^{2}\\vert\\vert_{F}(k,k)\\vert_{F}(I-V(k,k)^{T}\\vert_{F}(k,k))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For every $n_{k}~-~n_{k-1}~\\times~n_{k}~-~n_{k-1}$ matrix $M$ , we have $\\begin{array}{r l r}{t r(A)}&{{}\\leq}&{\\sum_{p}|\\lambda_{p}|\\quad\\leq}\\end{array}$ $\\begin{array}{r}{\\sqrt{n_{k}-n_{k-1}}\\left(\\sum\\left|\\lambda_{p}\\right|^{2}\\right)^{\\frac{1}{2}}=\\sqrt{n_{k}-n_{k-1}}\\|M\\|_{F}}\\end{array}$ . Using this inequality, we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(I-U(k,k)V(k,k)^{T})\\le\\!(n_{k}-n_{k-1})\\frac{\\|\\hat{\\Sigma}-X\\|_{o p}}{s_{n_{k}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\frac{s_{1}}{s_{n_{k}}}\\left(\\frac{2\\operatorname*{min}\\left(\\sqrt{n_{k}-n_{k-1}}(\\|X\\|_{o p}+\\|\\hat{\\Sigma}\\|_{o p}),\\|X\\|_{F}+\\|\\hat{\\Sigma}\\|_{F}\\right)}{\\operatorname*{min}\\left(s_{n_{k-1}}^{2}-s_{n_{k-1}+1}^{2},s_{n_{k}}^{2}-s_{n_{k+1}}^{2}\\right)}\\|X-\\hat{\\Sigma}\\|_{H^{s}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing on $k$ , we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ensuremath{\\boldsymbol{x}}\\le K\\frac{\\|\\hat{\\ensuremath{\\Sigma}}-\\ensuremath{\\boldsymbol{X}}\\|_{o p}}{s_{K}}+\\frac{m s_{1}}{s_{K}}\\left(\\frac{2\\operatorname*{min}(\\sqrt{K}(\\|\\ensuremath{\\boldsymbol{X}}\\|_{o p}+\\|\\hat{\\ensuremath{\\Sigma}}\\|_{o p}),\\|\\ensuremath{\\boldsymbol{X}}\\|_{F}+\\|\\hat{\\ensuremath{\\Sigma}}\\|_{F})}{\\operatorname*{min}_{k}(s_{n_{k}}^{2}-s_{n_{k+1}}^{2})}\\right)^{2}\\|\\ensuremath{\\boldsymbol{X}}-\\hat{\\ensuremath{\\boldsymbol{\\Sigma}}}\\|_{o p}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the case where $d\\rightarrow\\infty$ , $m$ is a constant, $\\alpha<1$ , $s_{1}=O(s_{K})$ , $s_{K}\\,>\\,c s_{K+1}$ , $K$ is a constant, \u2225X \u2212\u03a3\u02c6\u2225op = o(\u2225\u03a3\u02c6\u2225op), and mink(s2nk \u2212 $\\mathrm{min}_{k}(s_{n_{k}}^{2}\\,-\\,s_{n_{k+1}}^{2})\\ \\geq\\ c s_{K}^{2}$ for some constant $c$ , we have $x\\ \\leq$ $O\\big(\\frac{\\|X-\\hat{\\Sigma}\\|_{o p}}{\\|\\hat{\\Sigma}\\|_{o p}}\\big)$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma A.5. Let $X=U S V^{T}$ , and define ", "page_idx": 16}, {"type": "equation", "text": "$$\nx=4K-\\sum_{k:s i g n a l}\\mathrm{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k)+2U(k,k)V(k,k)^{T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\Sigma$ be a $d\\times d$ diagonal matrix whose diagonal entries are given by $b_{1},\\ldots,b_{K},0,\\ldots,0,$ ,where $b_{1}=.\\,.\\,.\\,b_{n_{1}}>b_{n_{1}+1}=.\\,.\\,.\\,=b_{n_{2}}\\geq.\\,.\\,.\\,=b_{n_{m}}=b_{K}$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Sigma-U^{T}\\Sigma V\\|_{o p}\\leq\\|\\Sigma\\|_{o p}((m+1)^{3}\\sqrt{x}+2(m+1)^{3}x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. First observe that $\\begin{array}{r}{\\|\\Sigma-U^{T}\\Sigma V\\|_{o p}\\,\\leq\\,\\sum_{i,j=1}^{m+1}\\|\\Sigma(i,j)-U^{T}\\Sigma V(i,j)\\|_{o p}}\\end{array}$ . If $i\\neq j$ , then $\\Sigma(i,j)=0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|U^{T}\\Sigma V(i,j)\\|_{o p}=\\|\\displaystyle\\sum_{\\ell=1}^{m+1}U(\\ell,i)^{T}\\Sigma(\\ell,\\ell)V(\\ell,j)\\|_{o p}}}\\\\ &{\\qquad\\qquad\\leq\\boldsymbol{b}_{1}\\displaystyle\\sum_{\\ell=1}^{m+1}\\|U(\\ell,i)\\|_{o p}\\|V(\\ell,j)\\|_{o p}}\\\\ &{\\leq\\boldsymbol{b}_{1}(m+1)\\sqrt{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $i=j$ , then $\\Sigma(i,i)=b_{n_{i}}I$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert U^{T}\\Sigma V(i,i)-b_{n_{i}}I\\Vert_{o p}\\leq\\Vert b_{n_{i}}U(i,i)^{T}V(i,i)-b_{n_{i}}I\\Vert_{o p}+b_{1}\\displaystyle\\sum_{\\ell\\neq i}\\Vert U(\\ell,i)\\Vert_{o p}\\Vert V(\\ell,i)\\Vert_{o p}}\\\\ {\\leq b_{1}(m+1)x+b_{1}(m+1)x.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Sigma-U^{T}\\Sigma V\\|_{o p}\\leq\\|\\Sigma\\|_{o p}((m+1)^{3}\\sqrt{x}+2(m+1)^{3}x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This bound is not optimal in $m$ , but throughout the paper, $m$ is of constant order and it is fine to miss a constant factor when estimating error. ", "page_idx": 17}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Weak bound ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We prove the following weak bound. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.1. For every $\\varepsilon>0$ and every $t<T$ , we have with high probability, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|W_{1}^{T}W_{1}-\\sqrt{A^{T}A+\\sigma^{4}w^{2}I}\\|_{\\mathrm{op}}\\leq(1+\\varepsilon)\\sigma^{2}w\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Analogous results holds for $W_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Our main tool is the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. For every cost $C$ and every time $t,$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{2}^{T}W_{2}(t)-W_{1}W_{1}^{T}(t)=W_{2}^{T}(0)W_{2}(0)-W_{1}(0)W_{1}^{T}(0)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $L=\\|A^{*}-A\\|^{2}$ be the loss function. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}W_{1}=2W_{2}^{T}\\nabla C}\\\\ {\\partial_{t}W_{2}=2\\nabla C W_{1}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We see that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}(W_{1}W_{1}^{T})=2W_{2}^{T}\\nabla C W_{1}^{T}+W_{1}\\nabla C^{T}W_{2}=\\partial_{t}(W_{2}^{T}W_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we show that at initialization, $W_{1}W_{1}^{T}$ and $W_{2}^{T}W_{2}$ are approximately orthogonal projections, up to a factor. Stating precisely, we have the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.3. There exists two projections $P_{1}$ and $P_{2}:\\mathbb{R}^{w}\\to\\mathbb{R}^{w}$ such that the following are true. ", "page_idx": 17}, {"type": "text", "text": "1. The image of $P_{1}$ and $P_{2}$ are orthogonal to each other; ", "page_idx": 17}, {"type": "text", "text": "2. With high probability, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{1}W_{1}^{T}(0)-\\sigma^{2}w P_{1}\\|_{\\mathrm{op}}=O(\\sigma^{2}\\sqrt{w d}\\log d)}\\\\ {\\|W_{2}^{T}W_{2}(0)-\\sigma^{2}w P_{2}\\|_{\\mathrm{op}}=O(\\sigma^{2}\\sqrt{w d}\\log d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma B.3 . : At initialization, the rank of wis $w$ with probability 1. Therefore $W_{1}W_{1}^{T}$ has $w-d$ eigenvalues that are 0, and the $w$ non-zero eigenvalues equals the eigenvalues of $d\\times d$ matrix $W_{1}^{T}W_{1}$ . ", "page_idx": 18}, {"type": "text", "text": "$W_{1}^{T}W_{1}(0)$ is a (scaled) Wishart ensemble, whose limiting distribution is given by the MarchenkoPastur law. The Marchenko-Pastur law, as stated in [4], proves the following. Let $X$ be an $M\\times N$ matrix with complex-valued independent entries $X_{i\\mu}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2.~\\mathbb{E}[\\left|X_{i\\mu}\\right|^{2}]=\\frac{1}{\\sqrt{M N}};}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. for every $p\\in\\mathbb N$ , there exists a constant $C_{p}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|(N M)^{\\frac{1}{4}}X_{i\\mu}\\right|^{p}\\right]\\leq C_{p}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $M$ satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n0<C^{-1}\\leq{\\frac{\\log M}{\\log N}}\\leq C<\\infty\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some constant $C$ independent of $M$ and $N$ . Let $\\begin{array}{r}{\\phi=\\frac{M}{N}}\\end{array}$ , which may or may not depend on $N$ . Then the eigenvalues of $N\\times N$ matrix $X^{*}X$ has the same asymptotics as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho_{\\phi}(d x):=\\frac{\\sqrt{\\phi}}{2\\pi}\\sqrt{\\frac{[(x-\\gamma_{-})(\\gamma_{+}-x)]_{+}}{x^{2}}}d x+(1-\\phi)_{+}\\delta(d x)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{\\pm}:=\\sqrt{\\phi}+\\frac{1}{\\sqrt{\\phi}}\\pm2\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our situation, $W_{1}$ is a $w\\times d$ matrix with independent and identically distributed Gaussian entries whose variance is $\\sigma^{2}$ . Let $M=w$ , $N=d$ and therefore $W_{1}^{T}W_{1}(0)$ has the same distribution as $\\sigma^{2}\\sqrt{w d}X^{*}X$ . Notice that for this choice of $M$ and $N$ , the asymptotic distribution eigenvalues of $X^{*}X$ is $\\rho_{\\frac{w}{d}}(d x)$ . Notice that $\\rho_{\\frac{w}{d}}$ is supported on interval $\\begin{array}{r}{\\left[\\sqrt{\\frac{w}{d}}+\\sqrt{\\frac{d}{w}}-2,\\sqrt{\\frac{w}{d}}+\\sqrt{\\frac{d}{w}}+2\\right];}\\end{array}$ from which we conclude that in the limit, all eigenvalues of $W_{1}^{T}W_{1}(0)$ is approximately $\\sqrt{\\frac{d}{w}}$ . ", "page_idx": 18}, {"type": "text", "text": "By Theorem 2.10 of [4], we have eigenvalue rigidity results for $X^{*}X$ . Let $\\lambda_{k}^{\\prime}$ be the $k$ -th largest eigenvalue for $X^{*}X.\\;\\forall k\\in\\{1,2,\\ldots,w\\}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\lambda_{k}^{\\prime}-\\gamma_{k}|<d^{-\\frac{2}{3}+\\varepsilon}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with high probability. Here $\\gamma_{\\alpha}$ is defined through ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\gamma_{k}}^{\\infty}\\rho_{\\phi}(d x)=\\frac{k}{d}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\lambda_{k}$ be the $k$ -th largest eigenvalue of $W_{1}^{T}W_{1}(0)$ . By the relationship between $W_{1}^{T}W_{1}(0)$ and $X^{*}X$ , we know $\\lambda_{k}$ has the same law as $\\sigma^{2}\\sqrt{w d}\\lambda_{k}^{\\prime}$ . Then for every $k$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\lambda_{k}-\\sigma^{2}w\\right|\\le\\sigma^{2}\\sqrt{w d}\\left(\\left|\\lambda_{k}^{\\prime}-\\gamma_{k}\\right|+\\left|\\gamma_{k}-\\sqrt{\\frac{w}{d}}\\right|\\right)}\\\\ &{\\qquad\\qquad\\qquad\\le O(\\sigma^{2}\\sqrt{w d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with high probability. We conclude that the first $w$ eigenvalues of $W_{1}^{T}W_{1}(0)$ is at most $O(\\sigma^{2}{\\sqrt{w d}})$ - away from 1 and all other eigenvalues are 0. Therefore there exists a projection $P_{1}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|W_{1}W_{1}^{T}(0)-\\sigma^{2}w P_{1}\\|_{\\mathrm{op}}=O(\\sigma^{2}\\sqrt{w d})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, there exists a projection $\\tilde{P}_{2}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|W_{2}^{T}W_{2}(0)-\\sigma^{2}w\\tilde{P}_{2}\\|_{\\mathrm{op}}=O(\\sigma^{2}\\sqrt{w d})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that $\\tilde{P}_{2}$ is not exactly orthogonal to $P_{1}$ , and it remains to find a projection $P_{2}$ that is orthogonal to $P_{1}$ and is close to $W_{2}^{T}W_{2}(0)$ . Assume that the column vectors of $W_{1}(0)$ are $u_{1},\\dotsc,u_{d}\\in\\mathbb{R}^{w}$ and column vectors of $W_{2}^{T}\\bar{(0)}$ are $v_{1},\\ldots,v_{d}\\in\\mathbb{R}^{w}$ . For $k=1,2,\\ldots,d$ , we define vector $v_{k}^{\\prime}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{k}^{\\prime}=v_{k}-P_{1}v_{k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We claim that $P_{1}v_{k}$ is very small. By law of large numbers, $\\|v_{k}\\|\\leq\\sigma{\\sqrt{w}}\\log d$ with high probability. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\|P_{1}v_{k}\\|\\leq\\|\\frac{1}{\\sigma^{2}w}W_{1}W_{1}^{T}(0)v_{k}\\|+{\\cal O}(\\sqrt{\\frac{d}{w}}\\|v_{1}\\|)}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{\\sigma^{2}w}\\|\\langle u_{1},v_{k}\\rangle u_{1}+...+\\langle u_{w},v_{k}\\rangle u_{w}\\|+{\\cal O}(\\sigma\\sqrt{d}\\log d)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that $\\langle u_{j},v_{k}\\rangle u_{j},\\forall j$ is a family of independent and identically distributed random vectors. For each of these random vectors, all entries have zero mean. The variance of any one of the entries is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\langle u_{i},v_{k}\\rangle^{2}\\langle u_{j},e_{\\ell}\\rangle^{2}\\right]=O(\\sigma^{6}w)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We conclude that for each $\\ell$ we have, by CLT, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\sigma^{3}w^{\\frac{1}{2}}}\\frac{\\langle u_{1},v_{k}\\rangle\\langle u_{1},e_{\\ell}\\rangle+\\ldots+\\langle u_{w},v_{k}\\rangle\\langle u_{w},e_{\\ell}\\rangle}{\\sqrt{d}}\\overset{(d)}{\\longrightarrow}N(0,1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Im\\{\\operatorname*{max}_{\\ell}\\displaystyle\\sum_{i=1}^{w}|\\langle u_{i},v_{k}\\rangle\\langle u_{k},e_{\\ell}\\rangle|>100\\sigma^{3}\\sqrt{d}w^{\\frac{1}{2}}\\log d\\}\\le w\\mathbb{P}\\{\\displaystyle\\sum_{i=1}^{w}|\\langle u_{i},v_{k}\\rangle\\langle u_{k},e_{1}\\rangle|>100\\sigma^{3}\\sqrt{d}w^{\\frac{1}{2}}\\log d\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2w\\mathbb{P}\\{N(0,1)>100\\log d\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le O(d^{-50})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore with high probability, max\u2113 $\\begin{array}{r}{\\sum_{i=1}^{w}\\vert\\langle u_{i},v_{k}\\rangle\\langle u_{k},e_{\\ell}\\rangle\\vert\\leq100\\sigma^{3}\\sqrt{d}w^{\\frac{1}{2}}\\log d}\\end{array}$ . This implies that with high probability, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\langle u_{1},v_{k}\\rangle u_{1}+...+\\langle u_{w},v_{k}\\rangle u_{w}\\|\\leq\\sqrt{w}\\sigma^{3}\\sqrt{d}w^{\\frac{1}{2}}\\log d}\\\\ {\\|P_{1}v_{k}\\|\\leq\\sigma\\sqrt{d}\\log d~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now let $W_{2}^{\\prime}$ be the matrix with column vector $v_{1}^{\\prime},\\ldots,v_{w}^{\\prime}$ \u221aand let $P_{2}$ be the projection to the column space \u221aof ${W_{2}^{\\prime}}^{T}$ . By construction $\\|W_{2}-W_{2}^{\\prime}\\|_{\\mathrm{op}}\\leq O(\\sigma\\sqrt{d}\\log d)$ , so $\\lVert W_{2}^{T}W_{2}-W_{2}^{\\prime}{}^{T}W_{2}^{\\prime}\\rVert_{\\mathrm{op}}\\leq$ $O(\\sigma^{2}{\\sqrt{w d}}\\log d)$ . Since the nonzero eigenvalues of $W_{2}^{T}W_{2}$ is at m\u221aost $O(\\sigma^{2}{\\sqrt{w d}})$ from 1, we know that the nonzero singular va\u221alues of $W_{2}^{\\prime}{}^{T}W_{2}^{\\prime}$ is also at most $O(\\sigma^{2}{\\sqrt{w d}})$ from 1. We conclude that $\\|P_{2}-W_{2}^{T}W_{2}\\|_{\\mathrm{op}}\\leq O(\\sigma^{2}\\sqrt{w d}\\log d)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition B.1. : We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}^{T}W_{1}W_{1}^{T}W_{1}(t)=W_{1}^{T}W_{2}^{T}W_{2}W_{1}+W_{1}^{T}(W_{1}W_{1}^{T}(t)-W_{2}^{T}W_{2}(t))W_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=A^{T}A+W_{1}^{T}(W_{1}W_{1}^{T}(0)-W_{2}^{T}W_{2}(0))W_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From lemma B.3 we see that as positive semi-definite matrix, for every constant $\\varepsilon>0$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n0\\le W_{1}W_{1}^{T}(0)\\le(1+\\varepsilon)\\sigma^{2}w I\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with high probability. Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-(1+\\varepsilon)\\sigma^{2}w W_{1}^{T}W_{1}\\leq W_{1}^{T}(P_{1}(0)-P_{2}(0))W_{1}\\leq(1+\\varepsilon)\\sigma^{2}w W_{1}^{T}W_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By moving terms around and corollary, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(W_{1}^{T}W_{1})^{2}-(1+\\varepsilon)\\sigma^{2}w W_{1}^{T}W_{1}+(1+\\varepsilon)^{2}\\frac{\\sigma^{4}w^{2}}{4}I}\\\\ &{\\leq\\!A^{T}A+\\frac{(1+\\varepsilon)^{2}}{4}\\sigma^{4}w^{2}I}\\\\ &{\\leq\\!(W_{1}^{T}W_{1})^{2}+(1+\\varepsilon)W_{1}^{T}W_{1}+(1+\\varepsilon)^{2}\\frac{\\sigma^{4}w^{2}}{4}I}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem V.1.\u221a9 of [12\u221a] states that the square-root function is operator monotone, which implies that if $A\\geq B$ then ${\\sqrt{A}}\\geq{\\sqrt{B}}$ . Taking square-root, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}^{T}W_{1}-(1+\\varepsilon)\\frac{\\sigma^{2}w}{2}I d\\leq\\sqrt{A^{T}A+(1+\\varepsilon)^{2}\\frac{\\sigma^{4}w^{2}}{4}}\\leq W_{1}^{T}W_{1}+(1+\\varepsilon)\\frac{\\sigma^{2}w}{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.2 Strong Bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The weak bound does not provide useful information if $\\|W_{1}^{T}W_{1}\\|_{o p}<<\\sigma^{2}w$ . For this reason we prove strong bound, which provide useful information if $\\|W_{1}^{T}W_{1}\\|_{o p}<<\\sigma^{2}w^{1+\\square}$ for some constant $\\boxed{\\begin{array}{r l}\\end{array}}>0$ . Recall that the evolution of weight matrix in gradient descent is given by the following. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d}{d t}W_{1}(t)=\\eta W_{2}^{T}\\nabla C}\\\\ {\\frac{d}{d t}W_{2}(t)=\\eta\\nabla C W_{1}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The goal of this section is to prove that ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}^{T}W_{1}\\approx\\sqrt{A^{T}A+\\sigma^{4}w^{2}I}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For simplicity of notations, we shall assume that $\\begin{array}{r l r}{C_{1}}&{{}=}&{W_{1}^{T}W_{1}}\\end{array}$ , $\\begin{array}{r l r}{C_{2}}&{{}=}&{W_{2}W_{2}^{T}}\\end{array}$ , $\\begin{array}{r l}{\\hat{C}_{1}}&{{}=}\\end{array}$ $\\sqrt{A^{T}A+\\sigma^{4}w^{2}I}$ and $C_{2}\\;=\\;\\sqrt{A A^{T}+\\sigma^{4}w^{2}I}$ . It is easy of see that ${\\hat{C}}_{1}$ and ${\\hat{C}}_{2}$ are invertible. Our main result for this section is the following proposition. ", "page_idx": 20}, {"type": "text", "text": "Proposition B.4. For every cost $C$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|W_{1}^{T}W_{1}-\\sqrt{A^{T}A+\\sigma^{4}w^{2}I}\\|_{\\mathrm{op}}\\leq m i n\\{O(\\sigma^{2}w),O\\left(\\sqrt{\\frac{d}{w}}\\|W_{1}^{T}W_{1}\\|_{o p}\\right)\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.4 . : We start from the equations: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}^{T}\\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\\sigma^{4}w^{2}I\\right]W_{1}=C_{1}^{3}-A^{T}A C_{1}-C_{1}A^{T}A-\\sigma^{4}w^{2}C_{1}+A^{T}C_{2}A}\\\\ &{W_{2}\\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\\sigma^{4}w^{2}I\\right]W_{2}^{T}=C_{2}^{3}-A A^{T}C_{2}-C_{2}A A^{T}-\\sigma^{4}w^{2}C_{2}+A C_{1}A^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Our goal is to show that $C_{1},C_{2}$ are close to the solution $\\begin{array}{r l r}{\\hat{C}_{1}}&{{}=}&{\\sqrt{A^{T}A+\\sigma^{4}w^{2}I},\\hat{C}_{2}}\\end{array}=$ $\\sqrt{A A^{T}+\\sigma^{4}w^{2}I}$ with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\hat{C}_{1}^{3}-A^{T}A\\hat{C}_{1}-\\hat{C}_{1}A^{T}A-\\sigma^{4}w^{2}\\hat{C}_{1}+A^{T}\\hat{C}_{2}A}\\\\ &{0=\\hat{C}_{2}^{3}-A A^{T}\\hat{C}_{2}-\\hat{C}_{2}A A^{T}-\\sigma^{4}w^{2}\\hat{C}_{2}+A\\hat{C}_{1}A^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Apriori, the cubic equation for $C_{1}$ and $C_{2}$ might have multiple solutions. We give an intuitive argument explaining why $\\hat{C}_{1}$ and ${\\hat{C}}_{2}$ are the correct solutions. By selecting a proper basis, we assume $A=d i a g(a_{1},\\ldots,a_{d})$ is diagonal. Assume that $(W_{2}^{T}W_{2}-W_{1}\\dot{W}_{1}^{T})^{2}=\\stackrel{\\bullet}{\\sigma}^{2}\\dot{w}I$ . Also assume that $C_{1}$ and $C_{2}$ both commute with $A$ . In this case the equations for $C_{1}$ and $C_{2}$ reduces to cubic equation for scalars. Let $\\lambda_{1},\\ldots,\\lambda_{d}$ be the eigenvalues of $C_{1}$ . Solving the equations for scalars, we have $\\lambda_{i}=0$ $\\mathfrak{r}\\pm\\sqrt{a_{i}^{2}+\\sigma^{4}w^{2}}$ . By lemma B.2 and lemma B.3, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}W_{1}^{T}(t)-W_{2}^{T}W_{2}(t)=W_{1}W_{1}^{T}(0)-W_{2}^{T}W_{2}(0)=\\sigma^{2}w P_{1}-\\sigma^{2}w P_{2}+o(\\sigma^{2}w).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $W_{1}W_{1}^{T}$ is positive semi-definite, all its eigenvalues are non-negative, and thus ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}W_{1}^{T}(t)\\geq(\\sigma^{2}w+o(\\sigma^{2}w))P_{1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the top $w$ eigenvalues of $W_{1}^{T}W_{1}$ is the same as the top $w$ eigenvalues of $W_{1}W_{1}^{T}$ , we conclude that $\\lambda_{i}\\geq\\sigma^{2}w(1+o(1))$ . This forces $\\lambda_{i}=\\sqrt{a_{i}^{2}+\\sigma^{4}w^{2}}$ . ", "page_idx": 20}, {"type": "text", "text": "Since $C_{1},C_{2}$ are not assumed to be aligned with $A$ , we cannot reduce the matrix cubic equations into scalar cubic equations. The high level idea for proving $d C_{i}:=C_{i}-\\hat{C}_{i}$ is small is the inverse function theorem. ", "page_idx": 20}, {"type": "text", "text": "1. Step 1: show that LHS of the equations are small. ", "page_idx": 21}, {"type": "text", "text": "2. Step 2: reduce the RHS of the equations to a linear function of $d C_{1}$ and $d C_{2}$ . The system of equations is thus reduced to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left({\\mathrm{small}}\\atop{\\mathrm{small}}\\right)=\\left(*\\quad*\\atop*\\quad*\\right)\\left(v_{i}^{T}d C_{1}\\atop u_{i}^{T}d C_{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The $^*$ matrix is now the \"Jacobian\" matrix, and $u_{i},v_{i}$ are left and right singular vectors of $A$ . ", "page_idx": 21}, {"type": "text", "text": "3. Step 3: prove that the \"Jacobian\" matrix $\\left(\\!\\!{\\begin{array}{r l}{*}&{{}*}\\\\ {*}&{*}\\end{array}}\\!\\!{\\Bigg)}$ is strictly positive definite, thus proving that $v_{i}^{T}d C_{1}$ and $u_{i}^{T}d C_{2}$ have small magnitude for all $i$ . ", "page_idx": 21}, {"type": "text", "text": "For gradient flow, $W_{2}^{T}W_{2}-W_{1}W_{1}^{T}$ is preser\u221aved and there exists projections $P_{1}$ and $P_{2}$ such that $W_{2}^{T}W_{2}-W_{1}W_{1}^{T}=\\sigma^{2}w(P_{2}-P_{1})+{\\cal O}(\\sigma^{2}\\sqrt{w d})$ . Therefore for every unit vector $v$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|v^{T}W_{1}^{T}\\left[(W_{2}^{T}W_{2}-W_{1}W_{1}^{T})^{2}-\\sigma^{4}w^{2}I\\right]W_{1}\\|\\leq\\|C_{1}\\|_{o p}\\sigma^{4}d^{\\frac{1}{2}}w^{\\frac{3}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Substracting the second pair of equations from the first pair and denoting $d C_{i}=C_{i}-\\hat{C}$ , we obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|C_{1}\\|_{o p}O(\\sigma^{4}w^{2}\\sqrt{\\frac{d}{w}})=C_{1}^{3}-\\hat{C}_{1}^{3}-A^{T}A d C_{1}-d C_{1}A^{T}A-\\sigma^{4}w^{2}d C_{1}+A^{T}d C_{2}A}\\\\ &{\\|C_{2}\\|_{o p}O(\\sigma^{4}w^{2}\\sqrt{\\frac{d}{w}})=C_{2}^{3}-\\hat{C}_{2}^{3}-A A^{T}d C_{2}-d C_{2}A A^{T}-\\sigma^{4}w^{2}d C_{2}+A d C_{1}A^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now since ", "page_idx": 21}, {"type": "equation", "text": "$$\nC_{1}^{3}-\\hat{C}_{1}^{3}=\\hat{C}_{1}^{2}d C_{1}+\\hat{C}_{1}d C_{1}C_{1}+d C_{1}C_{1}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we substitute the above relation to equation 31 and obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|C_{1}||_{o p}O(\\sigma^{4}w^{2}\\sqrt{\\frac{d}{w}})=\\left(\\hat{C}_{1}^{2}-A^{T}A\\right)d C_{1}+\\hat{C}_{1}d C_{1}C_{1}+d C_{1}\\left(C_{1}^{2}-A^{T}A\\right)-d C_{1}+A^{T}d C_{2}A}\\\\ &{}&{(33)}\\\\ &{}&{=\\hat{C}_{1}d C_{1}C_{1}+d C_{1}\\left(C_{1}^{2}-A^{T}A\\right)+A^{T}d C_{2}A,\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(34)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and similarly for equation 32. For any singular value $s_{i}$ of $A$ , with left and right singular vectors $u_{i},v_{i}$ , we multiply equation 34 to the left by $v_{i}^{T}$ , and divide both sides by $\\sigma^{2}w$ , to obtain an equation for $v_{i}^{T}d C_{1}$ . Similarly, we obtain an equation for $u_{i}^{T}d C_{2}$ : for $v_{i}^{T}d C_{1}$ and $u_{i}^{T}d C_{2}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|C_{1}||_{o p}O(\\sigma^{2}\\sqrt{w d})=v_{i}^{T}d C_{1}\\left(\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}C_{1}+\\frac{1}{\\sigma^{2}w}(C_{1}^{2}-A^{T}A)\\right)+\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)u_{i}^{T}d C_{2}A}\\\\ &{|C_{2}||_{o p}O(\\sigma^{2}\\sqrt{w d})=u_{i}^{T}d C_{2,i}\\left(\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}C_{2}+\\frac{1}{\\sigma^{2}w}(C_{2}^{2}-A A^{T})\\right)+\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)v_{i}^{T}d C_{1}A^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that $C_{1}^{2}-A^{T}A=W_{1}^{T}(W_{1}W_{1}^{T}-W_{2}^{T}W_{2})W_{1}=\\sigma^{2}w W_{1}^{T}(P_{1}-P_{2})W_{1}+\\lVert C_{1}\\rVert O(\\sigma^{2}\\sqrt{w d})$ . In the two equations above, by replacing $C_{1}^{2}-\\dot{A}^{T}A$ with $\\sigma^{2}w W_{1}^{T}(P_{1}-P_{2})W_{1}$ , we are making an error of at most $\\begin{array}{r}{\\|C_{1}\\|_{o p}\\|d C_{1}\\|_{o p}O(\\sqrt{\\frac{d}{w}})}\\end{array}$ . From weak bound we know that $\\|d C_{1}\\|_{o p}\\leq O(\\sigma^{2}w)$ . Therefore the error we made by making the approximation on the right hand side can be absorbed into left hand side. ", "page_idx": 21}, {"type": "text", "text": "To show that $\\|v_{i}^{T}d C_{1}\\|$ and $\\|u_{i}^{T}d C_{2}\\|$ are small, it suffices to show that the $(d_{i n}+d_{o u t})\\times(d_{i n}+d_{o u t})$ block matrix ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}C_{1}+W_{1}^{T}(P_{1}-P_{2})W_{1}}&{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)A}\\\\ {\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)A^{T}}&{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}C_{2}+W_{2}(P_{2}-P_{1})W_{2}^{T}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is strictly positive definite. This matrix can be further simplified to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{W_{1}^{T}}&{0}\\\\ {0}&{W_{2}}\\end{array}\\right)\\left(\\begin{array}{c c}{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}I+P_{1}-P_{2}}&{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)I}\\\\ {\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)I}&{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}I+P_{2}-P_{1}}\\end{array}\\right)\\left(\\begin{array}{c c}{W_{1}}&{0}\\\\ {0}&{W_{2}^{T}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The inner matrix can then be rewritten as $R R^{T}$ where $R$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\nR=\\left(\\begin{array}{l}{\\sqrt{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}+1}P_{1}+\\sqrt{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}-1}P_{2}}\\\\ {\\sqrt{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}-1}P_{1}+\\sqrt{\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}+1}P_{2}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $Q=\\left(\\begin{array}{c c}{{W_{1}^{T}}}&{{0}}\\\\ {{0}}&{{W_{2}}}\\end{array}\\right)$ .The \"Jacobian\" matrix is then $Q R(Q R)^{T}$ . As described in the strategy, we need to show that the singular values are strictly positive. The smallest nonzero singular value of $Q R(Q R)^{T}$ is the same as the smallest nonzero singular value of $(Q R)^{T}Q R$ . We expand $(Q R)^{T}Q R$ as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\partial_{t}\\alpha)^{*}(\\beta,\\hbar\\alpha)}\\\\ &{=\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}+\\coth^{2}\\gamma\\alpha,\\hbar\\alpha^{\\prime}\\beta^{*}+\\left(\\frac{\\partial_{t}}{\\partial\\alpha}\\right)^{2}\\gamma\\alpha,\\hbar\\alpha^{\\prime}\\beta^{*}}\\\\ &{\\quad+\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}\\hbar\\alpha^{\\prime}\\beta^{*}+\\cfrac{\\partial_{t}\\alpha^{\\prime}\\beta^{*}}{\\partial\\alpha^{\\prime}\\beta^{*}}+\\,-1\\geq\\partial_{t}\\alpha\\sqrt{2}\\gamma\\alpha,}\\\\ &{\\quad+\\left(\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}+\\cfrac{\\partial_{t}\\alpha^{\\prime}\\beta^{*}}{\\partial\\alpha^{\\prime}\\beta^{*}}+\\,-2\\right)\\beta^{*}\\alpha\\sqrt{2}\\beta^{*}\\beta^{*}}\\\\ &{\\quad+\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}\\hbar\\alpha^{\\prime}\\beta^{*}\\sin^{2}{\\beta\\alpha}\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}-\\cfrac{\\partial_{t}\\alpha^{\\prime}\\beta^{*}}{\\partial\\alpha^{\\prime}\\beta^{*}}}\\\\ &{\\quad+\\left(\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}+\\cfrac{\\partial_{t}\\alpha^{\\prime}\\beta^{*}}{\\partial\\alpha^{\\prime}\\beta^{*}}+\\,-1\\right)\\beta^{*}\\boldsymbol{W}_{i}^{\\prime}\\beta^{*},}\\\\ &{\\quad-\\left(\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}+\\cfrac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)\\beta^{*}\\alpha^{\\prime}}\\\\ &{\\quad+2\\sqrt{\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}+1}-\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)\\beta^{*}\\alpha^{\\prime}\\beta^{*}+\\hbar\\alpha^{\\prime}\\boldsymbol{W}_{i}^{\\prime}\\beta^{*})}\\\\ &{\\quad+\\left(\\frac{\\partial_{t}\\alpha}{\\partial\\alpha}\\right)^{2}\\left(\\hbar\\alpha^{\\prime}\\beta^{*}+\\cfrac{\\\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the fact that ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{1}W_{1}^{T}=P_{1}+P_{1}W_{2}^{T}W_{2}P_{1}+P_{1}W_{1}W_{1}^{T}P_{2}+P_{2}W_{1}W_{1}^{T}P_{1}+P_{2}W_{1}W_{1}^{T}P_{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The $(d_{i n}+d_{o u t})$ -th eigenvalue of the above is lower bounded by $\\begin{array}{r}{\\sigma^{2}w\\sqrt{\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)^{2}+1}+\\sigma^{2}w\\mathrm{~-~}}\\end{array}$ $\\begin{array}{r}{\\sigma^{2}w\\left(\\frac{s_{i}}{\\sigma^{2}w}\\right)\\geq\\sigma^{2}w}\\end{array}$ . We conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|d C_{1}\\|_{o p}+\\|d C_{2}\\|_{o p}\\leq O(\\sqrt{\\frac{d}{w}})\\|C_{1}\\|_{o p},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As suggested by an anonymous referee, it is possible to obtain the same approximated dynamics of $A_{\\theta(t)}$ by imposing a non-homogeneous balance condition (in a different setup). Assume that $W_{1}W_{1}^{T}-W_{2}^{T}W_{2}=2\\sigma^{2}w I.$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{1}^{2}+2\\sigma^{2}w C_{1}-A^{T}A=0;}\\\\ {C_{2}^{2}-2\\sigma^{2}w C_{2}-A A^{T}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore $C_{1}\\,=\\,-\\sigma^{2}w+\\sqrt{A^{T}A+\\sigma^{4}w^{2}}$ and $C_{2}\\,=\\,\\sigma^{2}w+\\sqrt{A A^{T}+\\sigma^{4}w^{2}}$ . Substituting into Gradient Flow equation, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d A}{d t}=-\\eta(\\sqrt{A A^{T}+\\sigma^{4}w^{2}}\\nabla C+\\nabla C\\sqrt{A^{T}A+\\sigma^{4}w^{2}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The advantage of the setup is that it significantly simplifies the proof. The The limitation of the setup is that $W_{1}\\check{W_{1}}^{T}-W_{2}^{T}{W_{2}}^{\\star}\\not=\\sigma^{4}w^{2}I$ if the initial variance of entries of $W_{1}$ and $W_{2}$ are comparable. In this case, $\\bar{W}_{1}W_{1}^{T}-W_{2}^{T}W_{2}$ will have $w$ positive singular values and $w$ negative singular values, and the absolute value of positive and negative singular values are comparable. In the setup of our problem, the variance of entries of $W_{1}$ and $W_{2}$ equal. ", "page_idx": 23}, {"type": "text", "text": "C Gradient Flow Dynamics of $A_{t}$ in Active Regime ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Saddle to Saddle Regime ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let $A_{t}$ have the following dynamics: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d}{d t}}A_{t}=(A^{*}-A_{t})\\sqrt{A_{t}^{T}A_{t}+\\sigma^{4}w^{2}I}+\\sqrt{A_{t}A_{t}^{T}+\\sigma^{4}w^{2}I}(A^{*}-A_{t}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The goal of this section is to prove that the singular vectors of $A_{t}$ is well-aligned with the singular vectors of $A^{*}$ , throughout the Saddle-to-Saddle regime. In the rest of the section, we will assume the dependence of $A_{t}$ on $t$ and use $A$ to represent $A_{t}$ . If at initialization, $A_{t}$ commutes with $A^{*}$ , then throughout the training, $A_{t}$ will always commute with $A^{*}$ . In this section, we use a delicate stability argument to show that if $A_{t}$ almost commute with $A^{*}$ at the beginning of the Saddle to Saddle regime, then it will continue to be almost commutative with $A^{*}$ throughout the training process. ", "page_idx": 23}, {"type": "text", "text": "Definition C.1. Define $P_{1}$ be the family of $d\\times d$ matrices $A$ that satisfies the following conditions. ", "page_idx": 23}, {"type": "text", "text": "\u2022 $s_{K}\\geq C\\sigma^{2}w$ , $s_{K+1}\\leq C^{\\prime}\\sigma^{2}w$ and $\\begin{array}{r}{\\frac{s_{K+1}}{s_{K}}\\leq c<\\frac{1}{2}}\\end{array}$ for some $d$ -independent constants c, $C$ and $C^{\\prime}$ . \u2022 $I f\\,a_{k}>a_{k+1}$ , then $s_{k}-s_{k+1}\\geq c s_{k}$ for some $d$ -independent constant c. ", "page_idx": 23}, {"type": "text", "text": "Define P 1\u2032 to be the family of w \u00d7 w matrix A such that sk \u2212sk+1 \u22652csk if ak+1 < ak, ssKK+1 $\\begin{array}{r}{\\frac{s_{K+1}}{s_{K}}\\leq\\frac{3}{4}}\\end{array}$ and $s_{K}\\,\\geq\\,c\\sigma^{2}w$ . Let $\\gamma\\,>\\,0$ be constant. Define $P_{2}(C,\\gamma)$ be the family of $w\\,\\times\\,w$ matrices $A$ satisfying the following conditions. ", "page_idx": 23}, {"type": "text", "text": "\u2022 (alignment of signals). Let $A=U S V^{T}$ . Define ", "page_idx": 23}, {"type": "equation", "text": "$$\nx=4K-\\sum_{k:s i g n a l}\\mathrm{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k)+2U(k,k)V(k,k)^{T}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$P_{2}(C,\\gamma)$ is the family of matrix $A$ such that $x\\leq C d^{-\\gamma}$ ", "page_idx": 23}, {"type": "text", "text": "Theorem C.2. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d A}{d t}}=(A^{*}-A)\\sqrt{A^{T}A+\\sigma^{4}w^{2}I}+\\sqrt{A A^{T}+\\sigma^{4}w^{2}I}(A^{*}-A)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nA(0)\\in P_{1}\\cap P_{2}(C_{4},\\gamma)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $T=O(d\\log d)$ . Then $\\forall t\\in[0,T]$ , we have $A_{t}\\in P_{1}\\cap P_{2}(C,m i n(1,\\gamma))$ . ", "page_idx": 23}, {"type": "text", "text": "We will use the following induction lemma to show that the singular vectors of $A_{t}$ are roughly aligned with $A^{*}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma C.3. Assume that $P_{1}$ and $P_{2}$ be families of increasing sets, and let $P_{1}^{\\prime}\\supset P_{1}$ . Assume that we have a family of matrices $A_{t}$ , $0\\leq t\\leq T$ for some fixed number $T$ . T does not depend on the family of matrices. Assume that $A_{0}\\in P_{1}\\cap P_{2}$ . Let $A_{[t_{1},t_{2}]}=\\{A_{t}:t_{1}\\leq t\\leq t_{2}\\}$ . Assume the following are true. ", "page_idx": 24}, {"type": "text", "text": "1. If $A_{[0,t]}\\in P_{1}$ then there exists a constant $\\varepsilon>0$ independent of $A_{t}$ such that $A_{[0,t+\\varepsilon]}\\subset P_{1}^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "2. Let $t_{1}<t_{2}$ . If $A_{[0,t_{2}]}\\subset P_{1}^{\\prime}$ and $A_{[0,t_{1}]}\\in P_{2}$ , then $A_{[0,t_{2}]}\\subset P_{2}$ . ", "page_idx": 24}, {"type": "text", "text": "Then $A_{t}\\in P_{1}$ and $A_{t}\\in P_{2}$ , $\\forall0\\leq t\\leq T$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Since $A_{0}$ satisfies $P_{1}$ , use condition 1 we have $A_{[0,\\varepsilon]}\\subset P_{1}^{\\prime}$ . Using condition 2, we know that $A_{[0,\\varepsilon]}\\subset P_{2}$ . By condition 3 we know that $A_{[0,\\varepsilon}]\\subset P_{1}$ and by condition 2, $A_{[0,\\varepsilon]}\\subset P_{2}$ . Iterate the argument. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Lemma C.4. If $A_{t}\\in P_{1}$ , then there exists $\\tau=\\sigma^{2}\\sqrt{d w}d^{-1}$ such that $A_{[t,t+\\tau]}\\subset P_{1}^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $0\\leq s\\leq\\tau$ . Using the approximation in time $[t,s+t]$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nd^{2}\\frac{d S}{d t}={\\cal I}\\odot\\Big({\\cal U}^{T}A^{*}V\\sqrt{S^{2}+\\sigma^{4}w^{2}}+\\sqrt{S^{2}+\\sigma^{4}w^{2}}{\\cal U}^{T}A^{*}V-2S\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\Big)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $j=1,2,\\dots,K+1$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nd^{2}\\bigg|\\frac{d s_{j}}{d t}\\bigg|\\le C d(s_{j}+\\sigma^{2}w)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $C$ independent of $w$ and $A_{t}$ . The conclusion follows from Gr\u00f6nwall. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.5. Assume that ", "page_idx": 24}, {"type": "equation", "text": "$$\nd^{2}\\frac{d A}{d t}=\\!(A^{*}-A+E)\\sqrt{A^{T}A+\\sigma^{2}w I}+\\sqrt{A A^{T}+\\sigma^{4}w^{2}I}(A^{*}-A+E).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then for signal $k$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nd^{2}\\frac{d}{d t}t r[U^{T}(k,k)U(k,k)]=t r[U^{T}(k,k)\\sum_{j\\neq k}U(k,j)D(j,k)]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here for $j\\neq k$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nD(j,k)=R(k,j)\\odot C(j,k)-R(j,k)\\odot C(k,j)^{T}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\begin{array}{r}{C\\;=\\;U^{T}(A^{*}\\,+\\,E)V,\\ R_{p q}\\;=\\;\\frac{s_{p}\\tilde{s}_{p}+s_{p}\\tilde{s}_{q}}{s_{p}^{2}-s_{q}^{2}}}\\end{array}$ for $1\\;\\leq\\;p,q\\;\\leq\\;w,\\;p\\;\\neq\\;q,$ , and $R(k,j)\\;=\\;$ $R_{n_{k}:n_{k+1},n_{j}:n_{j+1}}$ being the $k,j$ -th block of $R$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Use SVD dervative. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{d U}{d t}=U\\left(F\\odot\\left[U^{T}\\frac{d A}{d t}V S+S V^{T}\\frac{d A^{T}}{d t}U\\right]\\right)}}\\\\ &{}&{=U\\left(F\\odot\\left[(U^{T}(A^{*}+E)V-S)\\sqrt{S^{2}+\\sigma^{4}w^{2}}S+\\sqrt{S^{2}+\\sigma^{4}w^{2}}(U^{T}(A+E)^{*}V-S)S\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n+\\left.U\\left(F\\odot\\left[\\sqrt{S^{2}+\\sigma^{4}w^{2}}S(V^{T}(A^{*}+E)^{T}U-S)+S(V^{T}(A^{*}+E)^{T}U-S)\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\right]\\right)\\right\\}~.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=U\\left(F\\odot\\Big[U^{T}(A^{*}+E)V\\sqrt{S^{2}+\\sigma^{4}w^{2}}S+\\sqrt{S^{2}+\\sigma^{4}w^{2}}U^{T}(A^{*}+E)V S\\Big]\\right)}\\\\ &{\\quad+U\\left(F\\odot\\Big[\\sqrt{S^{2}+\\sigma^{4}w^{2}}S V^{T}(A^{*}+E)^{T}U+S V^{T}(A^{*}+E)^{T}U\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\Big]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\begin{array}{r}{D\\;=\\;F\\odot\\left[U^{T}\\frac{d A}{d t}V S+S V^{T}\\frac{d A^{T}}{d t}U\\right]}\\end{array}$ . Since $F$ is anti-symmetric and the ter\u221am in square bracket is symmetric, we know $D$ is anti-symmetric. Then $\\begin{array}{r}{\\frac{d U}{d t}\\,=\\,U D}\\end{array}$ . Let $\\tilde{S}\\,=\\,\\sqrt{S^{2}+\\sigma^{4}w^{2}}$ , $C=U^{T}(A^{*}+E)V$ . As a result, $\\forall1\\leq p,q\\leq w$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{d U_{p q}}{d t}=\\sum_{r:r\\ne q}U_{p r}\\frac{1}{s_{q}^{2}-s_{r}^{2}}\\left[(s_{q}\\tilde{s}_{q}+\\tilde{s_{r}}s_{q})C_{r q}+(\\tilde{s_{r}}s_{r}+s_{r}\\tilde{s}_{k})C_{r q}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let Rpq $\\begin{array}{r}{R_{p q}=\\frac{s_{p}\\tilde{s}_{p}+s_{p}\\tilde{s}_{q}}{s_{p}^{2}-s_{q}^{2}}}\\end{array}$ if $p\\neq q$ and $R_{p p}=0$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{d U}{d t}(k,k)=\\sum_{j:j\\neq k}U(k,j)\\left(R(k,j)\\odot C(j,k)-R(j,k)\\odot C(k,j)^{T}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\nD(j,k)=R(k,j)\\odot C(j,k)-R(j,k)\\odot C(k,j)^{T}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As a result, for nk \u2264p, q < nk+1, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\frac{d}{d t}t r[U^{T}(k,k)U(k,k)]=\\sum_{\\substack{p,q\\in[n_{k},n_{k+1})}}U_{p q}\\sum_{r}U_{p r}D_{r q}}\\\\ {\\displaystyle=\\sum_{\\substack{p,q,r\\in[n_{k},n_{k+1})}}U_{p q}U_{p r}D_{r q}+\\sum_{\\substack{p,q\\in[n_{k},n_{k+1})}}U_{p q}\\sum_{r\\notin[n_{k},n_{k+1})}U_{p r}D_{r q}}\\\\ {\\displaystyle=\\sum_{\\substack{p,q\\in[n_{k},n_{k+1})}}U_{p q}\\sum_{r\\notin[n_{k},n_{k+1})}U_{p r}D_{r q}}\\\\ {\\displaystyle=t r[U^{T}(k,k)\\sum_{j\\neq k}U(k,j)D(j,k)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma C.6. If $A_{[t_{1},t_{2}]}\\subset P_{1}$ and $A_{t_{1}}\\in P_{2}(C_{4},\\gamma)$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d x}{d t}}\\leq-c d x+O({\\sqrt{d x}}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{\\forall c=\\operatorname*{inf}_{t\\in[t_{1},t_{2}]}m i n\\big(\\frac{(s_{k}^{*}-s_{j}^{*})(s_{k}+s_{j})(\\tilde{s}_{k}+\\tilde{s}_{j})}{s_{k}^{2}-s_{j}^{2}}}\\end{array}$ , $\\frac{(s_{k}\\!-\\!s_{j})(s_{k}^{*}\\!+\\!s_{j}^{*})(\\tilde{s}_{k}\\!+\\!\\tilde{s}_{j})}{s_{k}^{2}\\!-\\!s_{j}^{2}}\\Big)$ . In particular, $A_{[t_{1},t_{2}]}\\subset$ $P_{2}(C,m i n(\\gamma,1))$ for some constant $C$ . If $\\gamma<1$ then we can take $C=C_{4}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Use previous lemma. Let ", "page_idx": 25}, {"type": "equation", "text": "$$\nx=4K-\\sum_{k:\\mathrm{signal}}\\mathrm{tr}(U^{T}(k,k)U(k,k)+V^{T}(k,k)V(k,k)+2U(k,k)V(k,k)^{T}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$x$ measures the alignment of singular vectors of $A$ with singular vectors of $A^{*}$ . From $U U^{T}=I$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\ell}U(k,\\ell)U(k,\\ell)^{T}=I\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and for every $j\\neq k$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nU(k,j)U(k,j)^{T}\\le I-U(k,k)U(k,k)^{T}\\le O(x)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular, $\\|U(k,j)\\|_{o p}\\leq O(\\sqrt{x})$ . We first estimate $C$ . For all $j=1,2,\\ldots,w,a_{j}\\neq a_{k}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U^{T}A^{*}V)(j,k)=U^{T}(j,j)S^{*}(j,j)V(j,k)+U^{T}(j,k)S^{*}(k,k)V(k,k)+\\displaystyle\\sum_{\\ell\\neq j,k}U^{T}(j,\\ell)S_{\\ell}^{*}V(\\ell,k)}\\\\ &{\\qquad\\qquad\\qquad=U^{T}(j,j)S^{*}(j,j)V(j,k)+U(k,j)^{T}S^{*}(k,k)V(k,k)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+O(d x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(U^{T}E V)(j,k)=\\displaystyle\\sum_{\\ell_{2}\\neq k}U^{T}(j,j)E(j,\\ell_{2})V(\\ell_{2},k)+\\displaystyle\\sum_{\\ell_{1}\\neq j}U^{T}(j,\\ell_{1})E(\\ell_{1},k)V(k,k)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle U^{T}(j,j)E(j,k)V(k,k)+\\displaystyle\\sum_{\\ell_{1}\\neq j,\\ell_{2}\\neq k}U^{T}(j,\\ell_{1})E(\\ell_{1},\\ell_{2})V(\\ell_{2},k)}\\\\ &{\\qquad\\qquad\\qquad=O(\\sqrt{d}\\sqrt{x})+O(\\sqrt{d})+O(\\sqrt{d}x)}\\\\ &{\\qquad\\qquad\\qquad=O(\\sqrt{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(j,k)=\\!R(k,j)\\odot C(j,k)-R(j,k)\\odot C(k,j)^{T}}\\\\ &{\\qquad\\quad=\\!R(k,j)\\odot(U(j,j)^{T}S^{*}(j,j)V(j,k))+R(k,j)\\odot U(k,j)^{T}S^{*}(k,k)V(k,k)}\\\\ &{\\qquad\\quad\\quad-R(j,k)\\odot(V(k,j)^{T}S^{*}(k,k)U(k,k)+V(j,j)^{T}S^{*}(j,j)U(j,k))}\\\\ &{\\qquad\\quad\\quad+O(d x+\\sqrt{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By $U U^{T}=I$ we know that $\\begin{array}{r}{\\sum_{\\ell}U(j,\\ell)U^{T}(\\ell,k)=0}\\end{array}$ . Therefore ", "page_idx": 26}, {"type": "equation", "text": "$$\nU(j,j)U(k,j)^{T}+U(j,k)U(k,k)^{T}+O(x)=0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\nU(j,k)=-O(x)-U(j,j)U(k,j)^{T}U(k,k).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly we have for $V$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nV(k,j)=-O(x)-V(k,k)V(j,k)^{T}V(j,j).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can rewrite $D(j,k)$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{2}(j,k)=\\!R(k,j)\\odot(U(j,j)^{T}S^{*}(j,j)V(j,k)+U(k,j)^{T}S^{*}(k,k)V(k,k))}\\\\ &{\\qquad+R(j,k)\\odot(V(j,j)^{T}V(j,k)V(k,k)^{T}S^{*}(k,k)U(k,k)+V(j,j)^{T}S^{*}(j,j)U(j,j)U(k,j)^{T}U}\\\\ &{\\qquad+O(d x+\\sqrt{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\beta\\frac{d}{d\\alpha}\\pi[\\langle\\ell(k,k^{\\prime})\\rangle^{T}\\langle\\ell(k,k)\\rangle}\\\\ &{=2\\pi\\bigg[\\langle\\ell(k,k^{\\prime})\\rangle^{T}\\sum_{\\ell}\\langle\\ell(k,j)D_{\\ell}(k,k)\\rangle}\\\\ &{-2\\sum_{\\ell}\\pi\\langle\\ell(k,k^{\\prime})D_{\\ell}(\\ell(k,j)\\rangle\\otimes{\\ell(\\ell(\\ell,j))^{T}\\otimes(j,k)\\nu(\\ell,k)\\nu(\\ell(k,j)\\rangle^{T}\\otimes(k,k)\\nu(\\ell(k,k))})}\\\\ &{+\\;2\\sum_{\\ell}\\pi\\langle\\ell(k,k)^{T}\\ell(\\ell(k,j)\\rangle\\otimes{\\ell(k,j)^{T}\\otimes(\\ell(\\ell,j))^{T}\\otimes(k,k)\\nu(\\ell(k,k))^{T}\\otimes(k,k)\\nu(\\ell(k,k))}\\rangle}\\\\ &{+\\;2\\sum_{\\ell}\\pi\\langle\\ell(k,k)^{T}\\ell(\\ell(k,j)\\rangle\\otimes\\langle\\ell(k,j)\\rangle^{T}\\mathfrak{S}\\langle\\ell(k,j)\\rangle\\ell(\\ell(k,j)\\rangle\\mathcal{F}\\langle\\ell(k,k)\\rangle)]}\\\\ &{+\\;2\\delta\\alpha[\\langle\\ell(k,k)\\rangle\\otimes\\langle\\ell(k,k)\\rangle\\otimes\\langle\\ell(k,j)\\rangle^{T}\\mathfrak{S}\\langle\\ell(k,j)\\rangle\\ell(\\ell(k,j)\\rangle\\mathcal{F}\\langle\\ell(k,k)\\rangle)}\\\\ &{=2\\sum_{\\ell}\\;\\langle\\ell^{\\prime}(k,j)D_{\\ell}(k,j)\\rangle+\\mathcal{S}\\langle\\ell_{\\ell},k\\rangle\\beta\\langle\\ell_{\\ell}|\\rangle\\sqrt{\\ell(k^{\\prime})\\gamma^{T}\\langle\\ell(k,j)\\rangle}\\sqrt{\\ell(k^{\\prime})\\gamma^{T}\\langle\\ell_{\\ell},k\\rangle}}\\\\ &{+2\\sum_{\\ell}\\;\\delta\\langle\\ell_{\\ell}|\\rangle\\mathfrak{S}\\langle\\ell(k,j)\\rangle\\mathcal{F}\\langle\\ell(k,j)\\rangle\\otimes{\\ell(\\ell(k,j)\\rangle^{T}\\langle\\ell(k,k)\\rangle}}\\\\ &{+2\\sum_{\\ell}\\;\\delta\\langle\\ell_{\\ell}|\\rangle\\mathfrak{S}\\langle\\ell(k,k)^{T}\\ell(k,j)\\rangle\\mathcal{F}\\langle\\ell(k,j)\\rangle\\mathcal{F}\\langle\\ell(k,k)\\rangle}\\\\ &{+\\;2\\sum_{\\ell}\\;\\delta\\langle\\ell_{\\ell}|\\rangle+\\delta\\langle\\ell_{\\ell}|\\rangle}\\\\ &{+\\;\\partial\\ell_{\\ell}\\rangle\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We know that ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(S^{*}(k,k)R(k,j)+S^{*}(j,j)R(j,k))-|S^{*}(j,j)R(k,j)+S^{*}(k,k)R(j,k)|}\\\\ &{=\\!\\!\\operatorname*{min}(\\frac{(s_{k}^{*}-s_{j}^{*})\\left(s_{k}+s_{j}\\right)\\left(\\tilde{s}_{k}+\\tilde{s}_{j}\\right)}{s_{k}^{2}-s_{j}^{2}},\\frac{\\left(s_{k}-s_{j}\\right)\\left(s_{k}^{*}+s_{j}^{*}\\right)\\left(\\tilde{s}_{k}+\\tilde{s}_{j}\\right)}{s_{k}^{2}-s_{j}^{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some positive constant $c$ independent of $w$ . We conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\nd^{2}\\frac{d}{d t}t r[U(k,k)^{T}U(k,k)]\\geq c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The same trick applies to $V$ . We similarly have ", "page_idx": 27}, {"type": "equation", "text": "$$\nd^{2}\\frac{d}{d t}t r[V(k,k)^{T}V(k,k)]\\geq c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It remains to show that $\\begin{array}{r}{\\frac{d}{d t}t r[V(k,k)^{T}U(k,k)]}\\end{array}$ bounded below. ", "page_idx": 27}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d}{d t}}\\mathrm{tr}[U(k,k)^{T}V(k,k)]=t r[U(k,k)^{T}{\\frac{d V}{d t}}(k,k)]+t r[V(k,k)^{T}{\\frac{d U}{d t}}(k,k)]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t r[V(k,k)^{T}\\cfrac{d}{d t}U(k,k)]=t r[U(k,k)^{T}\\cfrac{d}{d t}U(k,k)]+t r[(V(k,k)^{T}-U(k,k)^{T})\\cfrac{d}{d t}U(k,k)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq t r[U(k,k)^{T}\\cfrac{d}{d t}U(k,k)]-\\|V(k,k)-U(k,k)\\|_{F}\\|\\cfrac{d}{d t}U(k,k)\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\geq O(d x^{\\frac{3}{2}})+c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x})}\\\\ &{\\qquad\\qquad\\qquad\\geq c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, ", "page_idx": 27}, {"type": "equation", "text": "$$\nt r[U(k,k)^{T}{\\frac{d}{d t}}V(k,k)]\\geq c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This proves that ", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d x}{d t}}\\leq-c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Observe that if $x\\geq d^{-1+\\varepsilon}$ for some $\\varepsilon>0$ , then $\\textstyle{\\frac{d x}{d t}}\\leq-{\\frac{c}{2}}d x$ an\u221ad therefore $x$ decrease exponentially. On the other hand, if $x\\leq d^{-1-\\varepsilon}$ for some $\\varepsilon>0$ , then $\\begin{array}{r}{\\overline{{d x}}\\overline{{\\geq O(\\sqrt{d x})}}}\\end{array}$ and therefore $x$ might increase. We therefore conclude that $A_{[t_{1},t_{2}]}\\subset P_{2}(C,\\operatorname*{min}(\\gamma,1))$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma C.7. Assume that $A_{[0,t_{2}]}\\subset P_{2}(C_{4},\\gamma)$ and $A_{[0,t_{1}]}\\subset P_{1}$ . Then $A_{[0,t_{2}]}\\subset P_{1}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. By assumption we know that $t_{2}=O(d\\log d)$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}\\frac{d S}{d t}=I\\odot\\Big(U^{T}(A^{*}+E)V\\sqrt{S^{2}+\\sigma^{4}w^{2}}+\\sqrt{S^{2}+\\sigma^{4}w^{2}}U^{T}(A^{*}+E)V-2S\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\Big)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{K+1}}{d t}=2\\left(O(\\sqrt{d})+O(d^{1-\\gamma})-s_{K+1}\\right)\\sqrt{s_{K+1}^{2}+\\sigma^{4}w^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n|s_{K+1}(t_{2})-s_{K+1}(t_{1})|\\leq O(\\sigma^{2}w(d^{-\\frac{1}{2}}+d^{-\\gamma})\\log d)<<\\sigma^{2}w\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It remains to verify the gap between different families. Let $T$ be the first time when $\\begin{array}{r}{\\|A\\|_{o p}=\\frac{1}{2}s_{K}^{*}}\\end{array}$ and assume that $t_{2}\\leq T$ . Assume that $a_{k}>a_{j}$ , then there exists some constant $\\varepsilon$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{k}}{d t}\\geq(s_{k}^{*}(1-\\varepsilon)-s_{k})s_{k}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{j}}{d t}\\leq(s_{j}^{*}(1+\\varepsilon)-s_{j})(s_{j}+\\sigma^{2}w)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "An application of Gr\u00f6nwall inequality on $s_{k}$ and $s_{j}$ implies that there exists some constant $\\delta>0$ that depends only on aj, ak such that ssjk((TT  )) \u2264O(d\u2212\u03b4). Next we deal with the case when t2 > T. One important observation is that the $\\begin{array}{r}{\\operatorname*{inf}\\{t>T:s_{1}(t)\\geq\\frac{a_{k}d+a_{j}d}{2}\\}-T=O(d).}\\end{array}$ , as a simple result of Gr\u00f6nwall. Moreover, $s_{2}(T+O(d))\\leq{\\textstyle\\frac{1}{4}}s_{K}^{*}$ . This implies that $s_{1}-s_{k}>c s_{1}$ for some constant $c$ Repeat this argument for all remaining signal singular values completes the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "C.2 First NTK Regime ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "At initialization, the singular values of $A$ are of order $\\sigma^{2}{\\sqrt{w d}}$ by using Mechenko-Pastur law on $A^{T}A$ , which is infinitely smaller than $\\sigma^{2}w$ . As a result, there is a very short period when the dynamics is very close to NTK. This section is again a delicate stability argument to show that at the end of the first NTK regime, $A_{t}$ is almost commutative with $A^{*}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma C.8. Let $t_{1}$ be the first when the first singular value of $A$ hits $\\sigma^{2}w d^{-\\frac{\\delta}{2}}$ for $\\delta=\\frac{\\gamma_{w}-1}{2}$ . Then $A_{t_{1}}\\in P_{2}(C,m i n(\\frac{1}{2},\\frac{\\delta}{2}))$ for some constant $C$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We approximate the dynamics with NTK dynamics. Assume that ", "page_idx": 28}, {"type": "equation", "text": "$$\nd^{2}\\frac{d B}{d t}=2(A^{*}+E-B)\\sigma^{2}w\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with $B(0)=A(0)$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d^{\\underline{{{2}}}}\\frac{d(A-B)}{d t}=\\!2(B-A)\\sigma^{2}w(A^{*}+E-A)}}\\\\ {{\\qquad\\qquad\\qquad+\\,(\\sqrt{A^{T}A+\\sigma^{4}w^{2}}-\\sigma^{2}w)+(\\sqrt{A A^{T}+\\sigma^{4}w^{2}}-\\sigma^{2}w)(A^{*}+E-A).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "A simple application of Gr\u00f6nwall inequality implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A-B\\|_{o p}(t)\\le\\!\\!\\frac{1}{d^{2}}e^{-2\\sigma^{2}w t}\\displaystyle\\int_{0}^{t}e^{2\\sigma^{2}w\\tau}\\frac{2s_{1}(\\tau)^{2}}{\\sigma^{2}w}\\|A^{*}+E-A\\|_{o p}(\\tau)d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\le\\!3\\frac{\\|A_{t}\\|_{o p}^{2}}{\\sigma^{2}w}s_{1}^{*}\\frac{t}{d^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We first check $P_{2}$ . Clearly, $t_{1}$ is of order $d^{1-\\frac{\\delta}{2}}$ . Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|A_{t_{1}}-\\displaystyle\\frac{t_{1}}{d^{2}}\\sigma^{2}w A^{*}\\|_{o p}\\leq2d^{-\\delta}\\|A_{t_{1}}\\|_{o p}+\\|\\displaystyle\\frac{t_{1}}{d^{2}}\\sigma^{2}w E\\|_{o p}+O(\\sigma^{2}\\sqrt{w d})}}\\\\ {{=(O(d^{-\\delta})+O(d^{-\\frac{1}{2}})+O(d^{-\\frac{\\delta}{2}}))\\|A_{t_{1}}\\|_{o p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By lemma C.4 we have $x\\leq C d^{-\\operatorname*{min}\\left(\\frac{1}{2},\\frac{\\delta}{2}\\right)}$ for some constant $C$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma C.9. Let $t_{2}=c d_{\\mathrm{{}}}$ , where $c$ is a constant to be chosen. Then ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t_{2}}\\in P_{1}\\cap P_{2}(C,m i n(\\frac{\\gamma_{w}-1}{4},\\frac{1}{2})).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, there exists constants $b_{1}=\\dots b_{n_{1}}>b_{n_{1}+1}=\\dots=b_{n_{2}}\\geq\\dots=b_{n_{m}}=b_{K}$ , such that for a $d\\times d$ diagonal matrix $\\Sigma$ whose diagonal entries are given by $b_{1},\\ldots,b_{K},0,\\ldots,0$ , we have \u2225At2 \u2212\u03a3\u2225op \u2264O(\u03c32w)d\u2212min( \u03b3w8\u22121, 14 ). ", "page_idx": 29}, {"type": "text", "text": "Proof. At time $t_{2}$ , $\\|B(t_{2})\\|_{o p}=2a_{1}c\\sigma^{2}w(1+o(1))$ . From previous lemma we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|A-B\\|_{o p}(t)\\leq3\\frac{\\|A_{t}\\|_{o p}^{2}}{\\sigma^{2}w}s_{1}^{*}\\frac{t}{d^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let t = t2 we see that |\u2225A(t2)\u2225op \u2212\u2225B(t2)\u2225op| \u2264\u2225A \u2212B\u2225op(t2) \u22642\u2225A\u03c3t22w\u22252op lon $c$ sufficiently small, we can guarantee that $\\begin{array}{r}{\\|A-B\\|_{o p}(t)\\leq\\frac{\\operatorname*{min}_{1\\leq k\\leq K}\\left(a_{k-1}-a_{k}\\right)}{100a_{1}}\\|B\\|_{o p}(t)}\\end{array}$ This guarantees that the singular values of $A$ grows linearly in $[t_{1},t_{2}]$ , that $s_{k}\\,-\\,s_{k+1}\\,\\geq\\,c s_{k}$ if $a_{k}>a_{k+1}$ for some constant $c$ , that $\\begin{array}{r}{s_{K+1}<\\frac{1}{2}\\bar{s}_{K}}\\end{array}$ and that $A_{t_{2}}\\in P_{1}$ . It remains to check that $A_{t_{2}}$ satisfies $P_{2}(C,\\frac{\\gamma_{w}-1}{2})$ . We are ready to use similar techniques as in lemma C.6. All computation are similar, and the only difference is that $\\begin{array}{r}{|R(k,j)|=O(\\frac{\\sigma^{2}w}{s_{K}})}\\end{array}$ . We have ", "page_idx": 29}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d}{d t}}\\mathrm{tr}[U(k,k)^{T}U(k,k)]\\geq c{\\frac{\\sigma^{2}w}{s_{K}}}d x+O(d x^{{\\frac{3}{2}}}{\\frac{\\sigma^{2}w}{s_{K}}})+O(\\sqrt{d x}{\\frac{\\sigma^{2}w}{s_{K}}})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\nd^{2}\\frac{d x}{d t}\\leq2C\\frac{\\sigma^{2}w}{a_{K}(\\sigma^{2}w w t+\\sigma^{2}w d^{-\\frac{\\delta}{2}})}(-c d x+d x^{\\frac{3}{2}}+\\sqrt{d x})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore $\\begin{array}{r}{A_{[t_{1},t_{2}]}\\subset P_{2}(C,\\operatorname*{min}(\\frac{\\gamma_{w}-1}{4},\\frac{1}{2}))}\\end{array}$ . Let $\\begin{array}{r}{\\gamma=\\operatorname*{min}(\\frac{\\gamma_{w}-1}{4},\\frac{1}{2})}\\end{array}$ . From lemma C.8 we see that if $n_{k1}+1\\le p_{1},p_{2}\\le n_{k}$ , then $|s_{p_{1}}-s_{p_{2}}|\\leq O(d^{-\\gamma})s_{K}$ . Moreover, the dynamics of $s_{p_{i}},i=1,2$ are both given by ", "page_idx": 29}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{p_{i}}}{d t}=2(O(\\sqrt{d})+O(d^{1-\\gamma})+a_{k}d-s_{p_{i}})\\sqrt{s_{p_{i}}^{2}+\\sigma^{4}w^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{d|s_{p_{1}}-s_{p_{2}}|}{d t}\\leq O(d^{-1})|s_{p_{1}}-s_{p_{2}}|\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n|s_{p_{1}}-s_{p_{2}}|(t_{2})\\leq O(1)|s_{p_{1}}-s_{p_{2}}|(t_{1})=\\sigma^{2}w O(d^{-\\gamma-\\frac{\\gamma_{w}-1}{4}}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $b_{n_{i}}=s_{i}$ . By lemma A.5, there exists a $d\\times d$ diagonal matrix $\\Sigma$ , whose diagonal entries are given by $b_{1}=\\ldots=b_{n_{1}}>\\ldots=b_{n_{2}}\\geq\\ldots=b_{K}$ , such that $\\|A_{t_{2}}-\\Sigma\\|_{o p}\\leq O(\\sigma^{2}w)d^{-\\frac{\\gamma}{2}}$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "D The Gradient Flow Dynamics of $A_{\\theta(t)}$ in Lazy Regime ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we use a stability argument to show that if $\\sigma^{2}w$ is infinitely larger than $d$ , then the algorithm cannot converge. ", "page_idx": 29}, {"type": "text", "text": "Proposition D.1. With high probability, for all time $t$ , $\\begin{array}{r}{\\|A_{\\theta(t)}-A^{*}\\|_{F}^{2}\\geq\\frac{1}{3}m i n(\\|A^{*}\\|_{F}^{2},\\|E\\|_{F}^{2}).}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. In gradient flow dynamics, $\\|A_{\\theta(t)}-A^{*}-E\\|_{F}$ is decreasing with time and therefore for all time $t$ , $\\|A_{\\theta(t)}\\|_{F}^{2}\\,\\leq\\,9(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})$ . In particular, $\\|A_{\\theta(t)}\\|_{o p}\\,\\leq\\,9(\\|A^{*}\\|_{F}+\\|E\\|_{F})$ . By ", "page_idx": 29}, {"type": "text", "text": "assumption, $\\begin{array}{r}{\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(\\sqrt{\\frac{d}{w}})\\|C_{1}\\|_{o p}=O(d\\sqrt{\\frac{d}{w}})}\\end{array}$ , and the dynamics of $A_{\\theta(t)}$ can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\nd^{2}\\frac{d A_{\\theta(t)}}{d t}=2(A^{*}+E-A)\\sigma^{2}w+(A^{*}+E-A)O(d\\sqrt{\\frac{d}{w}})+O(d\\sqrt{\\frac{d}{w}})(A^{*}+E-A)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Assume that $B(0)=A_{\\theta(0)}$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d B}{d t}}=2(A^{*}+E-B)\\sigma^{2}w.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}d^{2}\\cfrac{d}{d t}\\|A_{\\theta(t)}-B\\|_{F}^{2}=-\\,2\\sigma^{2}w\\|A_{\\theta(t)}-B\\|_{F}^{2}+t r\\big((A-B)^{T}(A^{*}+E-A)O(d\\sqrt{\\frac{w}{d}})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,t r\\big((A-B)^{T}O(d\\sqrt{\\frac{w}{d}})(A^{*}+E-A)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq-\\,2\\sigma^{2}w\\|A_{\\theta(t)}-B\\|_{F}^{2}+\\|A_{\\theta(t)}-B\\|_{F}O(d^{2}\\sqrt{\\frac{d}{w}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This implies that for all time, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|A_{\\theta(t)}-B\\|_{F}\\leq d O(\\frac{d}{\\sigma^{2}w}\\sqrt{\\frac{d}{w}}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The dynamics of $B$ is linear, and we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{B_{t}=(A^{*}+E)(1-e^{-2\\sigma^{2}w t/d^{2}})+B_{0}e^{-2\\sigma^{2}w t/d^{2}}.}}\\\\ {{.}}\\\\ {{.}}\\\\ {{.}=\\Vert e^{-2\\sigma^{2}t/d^{2}}A^{*}+(1-e^{-2\\sigma^{2}w t/d^{2}})E+B_{0}e^{-2\\sigma^{2}w t/d^{2}}\\Vert_{F}^{2}\\geq0.99\\Vert e^{-2\\sigma^{2}t/d^{2}}A^{*}+(1-e^{-2\\sigma^{2}w t/d^{2}})E\\Vert_{F}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $P$ be the projection to the image of $A^{*}$ . Then with high probability, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{t}-A^{*}\\|_{F}^{2}=\\|e^{-2\\sigma^{2}w t/d^{2}}A^{*}+(1-e^{-2\\sigma^{2}w t/d^{2}})E+B_{0}e^{-2\\sigma^{2}w t/d^{2}}\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\geq0.99\\|e^{-2\\sigma^{2}t/d^{2}}A^{*}+(1-e^{-2\\sigma^{2}w t/d^{2}})E\\|_{F}^{2}}\\\\ &{\\qquad\\qquad=0.99\\|e^{-2\\sigma^{2}w t/d^{2}}A^{*}+(1-e^{-2\\sigma^{2}w t/d^{2}})P E\\|_{F}^{2}+0.99\\|(1-e^{-2\\sigma^{2}w t/d^{2}})(I-P)E\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{2}\\operatorname*{min}(\\|A^{*}\\|_{F}^{2},\\|E\\|_{F}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\|A_{\\theta(t)}-B\\|_{F}^{2}=o(d^{2})$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|A_{\\theta(t)}-A^{*}\\|_{F}^{2}\\geq\\frac{1}{3}\\mathrm{min}(\\|A^{*}\\|_{F}^{2},\\|E\\|_{F}^{2})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E The Gradient Flow Dynamics of $A_{\\theta(t)}$ in Active Regime ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In section C, we proved stability for gradient flow dynamics for $A_{t}$ . In this section, we prove similar stability statements for $A_{\\theta(t)}$ , using the approximation results from section B. We also summarize the behavior of $A_{\\theta(t)}$ in section E.3. ", "page_idx": 30}, {"type": "text", "text": "E.1 Saddle-to-Saddle Regime ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The goal of this section is to show that at the end of the first NTK regime, the singular vectors of $A_{\\theta(t)}$ are roughly aligned with $A^{*}$ . Moreover, the alignment remains to be good throughout the mixed regime. The following generalization of lemma C.6 and lemma C.7 will be useful. ", "page_idx": 30}, {"type": "text", "text": "Lemma E.1. Assume that $A_{[t_{1},t_{2}]}^{\\prime}\\subset P_{1}$ and $A_{t_{1}}\\in P_{2}(C_{4},\\gamma)$ . Let $s_{1}^{\\prime},\\ldots,s_{K}^{\\prime}$ be the first $K$ singular values of $A^{\\prime}$ . Assume that the dynamics of $A^{\\prime}$ is the following. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\ell^{2}\\frac{d A^{\\prime}}{d t}=(A^{*}+E-A^{\\prime})\\sqrt{A^{\\prime T}A^{\\prime}+\\sigma^{4}w^{2}I}+\\sqrt{A^{\\prime}A^{\\prime T}+\\sigma^{4}w^{2}}(A^{*}+E-A^{\\prime})}\\ ~}}\\\\ {{{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+O(d^{-\\lambda}s_{K}^{\\prime})(A^{*}+E-A^{\\prime})+(A^{*}+E-A^{\\prime})O(d^{-\\lambda}s)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, $O(d^{-\\lambda}s_{K}^{\\prime})$ is a matrix whose operator norm is bounded by $O(d^{-\\lambda}s_{K}^{\\prime})$ . Then $A_{[t_{1},t_{2}]}~\\subset$ $P_{2}(C_{4},m i n(\\gamma,1,2\\lambda))$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $A^{\\prime}=U S V^{T}$ . Computing the SVD derivative for $A^{\\prime}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d^{2}\\displaystyle\\frac{d U}{d t}=U\\left(F\\odot\\left[U^{T}A^{*}V\\sqrt{S^{2}+\\sigma^{4}w^{2}}S+\\sqrt{S^{2}+\\sigma^{4}w^{2}}U^{T}A^{*}V S\\right]\\right)}}\\\\ {{\\qquad\\qquad+U\\left(F\\odot\\left[\\sqrt{S^{2}+\\sigma^{4}w^{2}}S V^{T}A^{*}U+V^{T}A^{*}U S\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\right]\\right)}}\\\\ {{\\qquad\\qquad+U\\left(F\\odot\\left[O(s_{K}d^{-\\lambda})(U^{T}A^{*}V S+S V^{T}A^{*}U-2S^{2})\\right]\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assume that $a_{k}\\neq a_{\\ell}$ . Notice that ", "page_idx": 31}, {"type": "equation", "text": "$$\nF_{k\\ell}({\\cal O}(s_{k}d^{-\\lambda})U^{T}A^{*}V S)_{k\\ell}={\\cal O}(d^{1-\\lambda})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the same technique as C.6 we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\;\\;\\frac{1}{2}d^{2}\\frac{d}{d t}t r[U(k,k)^{T}U(k,k)]}\\\\ &{\\geq\\!c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{d x})}\\\\ &{\\;\\;\\;+t r[U(k,k)^{T}\\displaystyle\\sum_{j\\neq k}U(k,j)(F\\odot\\big[O(s_{K}d^{-\\lambda})(U^{T}A^{*}V S+S V^{T}A^{*}U-2S^{2})\\big])]}\\\\ &{\\geq\\!c d x+O(d x^{\\frac{3}{2}})+O(d^{1-\\lambda}\\sqrt{x})+O(\\sqrt{d x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similar conclusion also holds for $t r[U(k,k)^{T}V(k,k)]$ and $t r[V(k,k)^{T}V(k,k)]$ . We conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d x}{d t}}\\leq-c d x+O(d x^{\\frac{3}{2}})+O(d^{1-\\lambda}{\\sqrt{x}})+O({\\sqrt{d x}}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and $A_{[t_{1},t_{2}]}^{\\prime}\\subset P_{2}(C_{4},\\operatorname*{min}(\\gamma,1,2\\lambda))$ ). ", "page_idx": 31}, {"type": "text", "text": "Lemma E.2. Assume that $A_{[t_{1},t_{2}]}^{\\prime}\\subset P_{2}(C_{4},\\gamma)$ and $A_{t_{1}}\\in P_{1}$ . Let $s_{1}^{\\prime},\\ldots,s_{K}^{\\prime}$ be the first $K$ singular values of $A^{\\prime}$ . Assume that the dynamics of $A^{\\prime}$ is the following. ", "page_idx": 31}, {"type": "equation", "text": "$$\nl^{2}\\frac{d A^{\\prime}}{d t}=(A^{*}-A^{\\prime})\\sqrt{A^{\\prime T}A^{\\prime}+\\sigma^{4}w^{2}I}+\\sqrt{A^{\\prime}A^{\\prime T}+\\sigma^{4}w^{2}}(A^{*}-A^{\\prime})+O(d^{-\\lambda}s_{K}^{\\prime})(A^{*}-A)+(A^{*}-A)C^{2}(A^{*}-A^{\\prime}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, $O(d^{-\\lambda}s_{K}^{\\prime})$ is a matrix whose operator norm is bounded by $O(d^{-\\lambda}s_{K}^{\\prime})$ . Then $A_{[t_{1},t_{2}]}\\subset P_{1}$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Proceeding as in lemma C.7, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{p}}{d t}=2(O(\\sqrt{d})+O(d^{1-\\gamma})+a_{p}d-s_{p})(\\sqrt{s_{p}^{2}+\\sigma^{4}w^{2}}+O(d^{-\\lambda})s_{K})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "if $p$ is a signal, and ", "page_idx": 31}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{K+1}}{d t}=2(O(\\sqrt{d})+O(d^{1-\\gamma})-s_{K+1})(\\sqrt{s_{K+1}^{2}+\\sigma^{4}w^{2}}+O(d^{-\\gamma})s_{K}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now the conclusion follows from Gr\u00f6nwall. ", "page_idx": 31}, {"type": "text", "text": "Theorem E.3. Assume that $A_{\\theta(t)}\\,\\subset\\,P_{1}\\cap P_{2}(C,\\gamma)$ . Then $A_{[\\theta([t,T])}\\,\\subset\\,P_{1}\\cap P_{2}(C,\\gamma^{\\prime})$ for some constant $\\gamma^{\\prime}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. We use induction lemma C.3 to prove the theorem. For the first requirement, notice that for every $i=1,2,\\dots,w$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\nd^{2}\\bigg|\\frac{d s_{i}}{d t}\\bigg|\\le4\\|A^{*}\\|_{o p}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If A\u03b8(t) \u2208P1, then A\u03b8([t,t+\u03c4]) \u2282P 1\u2032 for \u03c4 = c d2\u03c3\u22252A\u2217w\u2225d2op . Here $c$ is some small constant that depends only on $P_{1}$ . This proves the first requirement. For the second requirement, we observe that $A_{\\theta(t)}$ satisfies the dynamics described in lemma E.1 at each stage, as long as $\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(s_{K}d^{-\\dot{\\lambda}}$ for some $\\lambda>0$ . It suffices to show that there exists some positive $\\lambda$ independent of $w$ such that $\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(d^{-\\lambda}s_{K})$ in $[0,T]$ . Let $T_{1}$ be the first time when $\\|A\\|_{o p}=\\sigma^{2}w$ . In $[0,T_{1}]$ , all singular values are of the same order. Therefore ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(\\|C_{1}\\|\\sqrt{\\frac{d}{w}})=O(s_{K}d^{-\\frac{\\gamma_{w}-1}{2}}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now let $T_{2}$ be the first time when $s_{1}=\\sigma^{2}w d^{\\frac{\\gamma w-1}{4}}$ . In $[T_{1},T_{2}]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(\\|C_{1}\\|\\sqrt{\\frac{d}{w}})\\leq O(s_{K}d^{-\\frac{\\gamma_{w}-1}{4}})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore we can pick $\\lambda=\\frac{\\gamma_{w}}{4}$ . During $[T_{1},T_{2}]$ , we have $s_{K}(0)\\geq c\\sigma^{2}w$ for some constant $c$ , and ", "page_idx": 32}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d s_{K}}{d t}}\\geq s_{K}\\big({\\frac{1}{2}}a_{K}d-s_{K}\\big)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover, since $\\begin{array}{r}{d^{2}\\frac{d s_{1}}{d t}\\leq(2a_{1}-s_{1})(s_{1}+\\sigma^{2}w)}\\end{array}$ , we have $T_{2}-T_{1}\\geq c\\frac{\\log w}{w}$ for some $c$ . Therefore we have $s_{K}(T_{2})\\;\\geq\\;\\sigma^{2}w d^{-\\nu}$ for some constant . Now in $[T_{2},T]$ , we have weak bound $\\|\\hat{C}_{1}-\\hat{C}\\|_{o p}\\leq O(\\sigma^{2}w)$ . We can therefore pick $\\lambda=\\nu$ for $[T_{2},T]$ . The third requirement is verified in lemma E.2. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "E.2 First NTK Regime ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As in $A_{t}$ , we need to show that at the end of the NTK regime, $A_{\\theta(t)}$ must be in $P_{1}$ and $P_{2}$ . Based on what we already have for $A_{t}$ , this conclusion follows from Gr\u00f6nwall. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.4. Assume that $A_{0}=A_{\\theta(0)}$ . Let $T_{1}^{\\prime}$ be the first time when $\\|A_{t}\\|_{o p}+\\|A_{\\theta(t)}\\|_{o p}$ reaches $\\sigma^{2}w$ . Then ", "page_idx": 32}, {"type": "text", "text": "In particular, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|A_{T_{1}^{\\prime}}-A_{\\theta(T_{1}^{\\prime})}\\|_{o p}\\leq O(\\sigma^{2}\\sqrt{w d}).}}\\\\ {{A_{\\theta(T_{1}^{\\prime})}\\in P_{1}\\cap P_{2}(C,m i n(\\frac{\\gamma_{w}-1}{8},\\frac{1}{4})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. From dynamics of $A_{t}$ we see that $\\begin{array}{r}{T_{1}^{\\prime}\\leq\\frac{c}{w}}\\end{array}$ for some constant $c$ . The dynamics of $A_{t}$ is the following. ", "page_idx": 32}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d A_{t}}{d t}}=\\:(A^{*}-A_{t})\\sqrt{A_{t}^{T}A_{t}+\\sigma^{2}w I}+\\sqrt{A_{t}A_{t}^{T}+\\sigma^{4}w^{2}I}(A^{*}-A_{t}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The dynamics of $A_{\\theta(t)}$ can be written as follows. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}\\frac{d A_{\\theta(t)}}{d t}=\\!(A^{*}-A_{\\theta(t)})\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}+\\sigma^{4}w^{2}I}+\\sqrt{A_{\\theta(t)}A_{\\theta(t)}^{T}+\\sigma^{4}w^{2}I}(A^{*}-A_{\\theta(t)})}\\\\ &{\\qquad\\qquad\\quad+\\ O(\\sigma^{2}\\sqrt{w d})(A^{*}-A_{\\theta(t)})+(A^{*}-A_{\\theta(t)})O(\\sigma^{2}\\sqrt{w d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Observe that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\|A_{\\theta(t)}^{T}A_{\\theta(t)}-A_{t}^{T}A_{t}\\|_{o p}\\leq\\|A_{\\theta(t)}^{T}(A_{\\theta(t)}-A_{t})\\|_{o p}+\\|(A_{\\theta(t)}-A_{t})^{T}A_{t}\\|_{o p}}\\\\ {\\leq\\sigma^{2}w\\|A_{\\theta(t)}-A_{t}\\|_{o p}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies that we have the following inequality on positive definite matrices. ", "page_idx": 32}, {"type": "equation", "text": "$$\nA_{t}^{T}A_{t}-\\sigma^{2}w\\|A_{\\theta(t)}-A_{t}\\|_{o p}I\\leq A_{\\theta(t)}^{T}A_{\\theta(t)}\\leq A_{t}^{T}A_{t}+\\sigma^{2}w\\|A_{\\theta(t)}-A_{t}\\|_{o p}I\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Assume that $\\boldsymbol{A}_{t}=\\boldsymbol{U}\\boldsymbol{S}\\boldsymbol{V}^{T}$ , then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\parallel\\!\\sqrt{A_{\\theta(t)}^{T}A_{\\theta(t)}+\\sigma^{4}w^{2}I}-\\sqrt{A_{t}^{T}A+\\sigma^{4}w^{2}I}\\!\\parallel_{o p}}\\\\ &{\\le\\!\\!\\parallel\\!\\sqrt{\\sigma^{4}w^{2}I+A_{t}^{T}A_{t}+\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}I}-\\sqrt{\\sigma^{4}w^{2}I+A_{t}^{T}A_{t}-\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}I}\\!\\parallel_{o p}}\\\\ &{=\\!\\!\\parallel\\!\\sqrt{\\sigma^{4}w^{2}I+\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}I+S^{2}}-\\sqrt{\\sigma^{4}w^{2}I-\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}I+S^{2}}\\!\\parallel_{o p}}\\\\ &{=\\!\\operatorname*{max}_{i}\\Bigg(\\sqrt{\\sigma^{4}w^{2}+s_{i}^{2}+\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}}-\\sqrt{\\sigma^{4}w^{2}+s_{i}^{2}-\\sigma^{2}w\\!\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}}\\Bigg)}\\\\ &{\\le\\!3\\parallel\\!A_{\\theta(t)}-A_{t}\\Bigr\\Vert_{o p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can control the difference between $A_{t}$ and $A_{\\theta(t)}$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\nd^{2}{\\frac{d}{d t}}\\|A_{t}-A_{\\theta(t)}\\|_{o p}\\leq O(d)\\|A_{t}-A_{\\theta(t)}\\|+O(d)O(\\sigma^{2}{\\sqrt{w d}})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Gr\u00f6nwall inequality implies that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|A_{T_{1}^{\\prime}}-A_{\\theta(T_{1}^{\\prime})}\\|_{o p}\\leq\\!O(\\sigma^{2}\\sqrt{w d})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We check that $A_{\\theta(T_{1}^{\\prime})}$ satisfies the $P_{1}$ . Since $\\|A_{T_{1}^{\\prime}}-A_{\\theta(T_{1}^{\\prime})}\\|_{o p}\\leq O(\\sigma^{2}\\sqrt{w d})$ , $\\|A_{\\theta(T_{1}^{\\prime})}\\|_{o p}\\geq\\textstyle{\\frac{1}{3}}\\sigma^{2}w$ . Let $\\sigma_{j}(A_{T_{1}^{\\prime}})$ and $\\bar{\\sigma_{j}}\\big(A_{\\theta(T_{1}^{\\prime})}\\big)$ be the $j$ -th singular value of $A_{T_{1}^{\\prime}}$ and $A_{\\theta(T_{1}^{\\prime})}$ . If $a_{j}\\neq a_{k},\\,j,k\\leq K$ , then $\\left|\\sigma_{j}(A_{T_{1}^{\\prime}})-\\sigma_{k}(A_{T_{1}^{\\prime}})\\right|\\geq c\\sigma^{2}w$ , and therefore ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sigma_{j}\\big(A_{\\theta(T_{1}^{\\prime})}\\big)-\\sigma_{k}\\big(A_{\\theta(T_{1}^{\\prime})}\\big)\\right|\\geq c\\sigma^{2}w+O(\\sigma^{2}\\sqrt{w d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The noise is clearly of order $O(\\sigma^{2}{\\sqrt{w d}})$ . This completes the verification of $P_{1}$ . By lemma C.9, $\\begin{array}{r}{\\|A_{\\theta(T_{1}^{\\prime})}-\\Sigma\\|_{o p}\\leq\\sigma^{2}w O(\\sigma^{2}w)d^{-\\operatorname*{min}(\\frac{\\gamma_{w}-1}{8},\\frac{1}{4})}}\\end{array}$ . By Lemma C.4 we are done. ", "page_idx": 33}, {"type": "text", "text": "E.3 Summary of Approximate Dynamics of $A_{\\theta(t)}$ at Each Stage ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In previous sections we have proved that $A_{\\theta(t)}$ satisfies $P_{1}$ and $P_{2}(C,\\gamma)$ for some $\\gamma>0$ . The $P_{1}$ and $P_{2}(C,\\gamma)$ property actually implies that the dynamics of $A_{\\theta(t)}$ is such that each group of singular values evolve independently. We state the approximate dynamics for each stage of the dynamics, and show that the alignment will be improved, if the alignment is not already good enough. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Initialization. At initialization, $A_{\\theta(0)}$ is a random matrix. The mean of each entry is 0 and the variance of each entry is $\\sigma^{2}{\\sqrt{w}}$ . The gap between singular values is infinitely smaller than the magnitude of singular values, and the singular vectors are not aligned with $A^{*}$ .   \n\u2022 Initialization to $\\|A_{\\theta(t)}\\|_{o p}=\\sigma^{2}w.$ . Let $T_{1}$ be the first time when $\\|A_{\\theta(t)}\\|_{o p}$ reaches $\\sigma^{2}w$ . $[0,T_{1}]$ corresponds the very short NTK regime for the signals, and the dynamics of $A_{\\theta(t)}$ is approximately linear. The evolution of signal singular values are roughly linear (i.e., bounded above and below by linear functions), and at $T_{1}$ , we have $\\begin{array}{r}{A_{\\theta(T_{1})}\\in P_{2}(C,\\operatorname*{min}(\\frac{\\gamma_{w}-1}{8},\\frac{1}{4}))}\\end{array}$ . Since the singular values grows roughly linearly, $T_{1}=O(d)$ .   \n\u2022 $\\|A_{\\theta(t)}\\|_{o p}=\\sigma^{2}w$ to $\\begin{array}{r}{\\|A_{\\theta(t)}\\|_{o p}=\\sigma^{2}w\\left(\\frac{w}{d}\\right)^{\\frac{1}{4}}}\\end{array}$ . Let $T_{2}$ be the first time when $\\|A_{\\theta(t)}\\|_{o p}=$ $\\sigma^{2}w\\left({\\frac{w}{d}}\\right)^{\\frac{1}{4}}$ . $[T_{1},T_{2}]$ is the early stage of saddle-to-saddle dynamics. The dynamics of $s_{i}$ is given by ", "page_idx": 33}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{i}}{d t}=2(a_{i}d(1+o(1))-s_{i})\\sqrt{s_{i}^{2}+\\sigma^{4}w^{2}}(1+o(1)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By theorem 1, we have $\\begin{array}{r}{\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq\\sigma^{2}w\\left(\\frac{w}{d}\\right)^{-\\frac{1}{4}}}\\end{array}$ . Let $\\begin{array}{r}{d^{-\\lambda}\\sigma^{w}=\\sigma^{2}w\\left(\\frac{w}{d}\\right)^{-\\frac{1}{4}}}\\end{array}$ , we have $\\begin{array}{r}{\\lambda=\\frac{\\gamma_{w}-1}{4}}\\end{array}$ . By lemma 4,1, the dynamics of $x$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\nd^{2}\\frac{d x}{d t}\\leq-c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{x}d^{1-\\frac{\\gamma_{w}-1}{4}})+O(\\sqrt{d x})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with $\\begin{array}{r}{x(T_{1})=\\frac{\\gamma_{w}-1}{2}}\\end{array}$ . We conclude that ", "page_idx": 33}, {"type": "equation", "text": "$$\nx(T_{2})\\leq O\\big(d^{-\\frac{\\gamma_{w}-1}{2}}\\big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\|A_{\\theta(t)}\\|_{o p}\\,=\\,\\sigma^{2}w\\left(\\frac{w}{d}\\right)^{\\frac{1}{4}}}\\end{array}$ to $s_{K}(A_{\\theta(t)})\\;=\\;{\\textstyle\\frac{1}{2}}a_{K}d$ . Let $T_{3}$ be the first time $s_{K}(A_{\\theta(t)})\\;=\\;$ $\\textstyle{\\frac{1}{2}}a_{K}d$ . At time $T_{2}$ , we have $s_{K}\\geq\\sigma^{2}w d^{\\delta}$ for some $\\delta>0$ that depends only on $a_{1},\\dots,a_{K}$ . Then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(s_{K}d^{-\\delta}),}\\\\ {\\displaystyle d^{2}\\frac{d x}{d t}\\leq-c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{x}d^{1-\\delta})+O(\\sqrt{d x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We conclude that $x(T_{3})=O(d^{-2\\delta})$ . The dynamics of $s_{i}$ is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}\\cfrac{d s_{i}}{d t}=2(a_{i}d(1+O(d^{-2\\delta}))-s_{i})\\sqrt{s_{i}^{2}+\\sigma^{4}w^{2}}(1+O(d^{-2\\delta})).}\\\\ &{\\quad s_{i}(a_{i}d-s_{i})\\leq d^{2}\\cfrac{d s_{i}}{d t}\\leq2(a_{i}d+C d^{1-2\\delta}-s_{i})(s_{i}+\\sigma^{2}w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The bounds on $\\frac{d s_{i}}{d t}$ implies that ", "page_idx": 34}, {"type": "equation", "text": "$$\nT_{3}-T_{1}=\\frac{d\\log\\frac{a_{K}d}{\\sigma^{2}w}}{a_{K}}+O(d)=\\frac{1-\\gamma_{\\sigma^{2}}-\\gamma_{w}}{a_{K}}d\\log d+O(d).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u2022 Final Stage. Let $t\\geq T_{3}$ . Since $s_{K}=O(d)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\hat{C}_{1}-C_{1}\\|_{o p}\\leq O(s_{K}\\frac{\\sigma^{2}w}{d})=O(s_{K}d^{\\gamma_{\\sigma^{2}}+\\gamma_{w}-1});\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\nd^{2}\\frac{d x}{d t}\\leq-c d x+O(d x^{\\frac{3}{2}})+O(\\sqrt{x}d^{\\gamma_{\\sigma^{2}}+\\gamma_{w}})+O(\\sqrt{d x}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recall that the constant $c$ in term $-c d x$ must satisfy ", "page_idx": 34}, {"type": "equation", "text": "$$\nc\\leq\\frac{1}{d}\\mathrm{min}_{k,j:a_{k}\\neq a_{j}}\\big(\\frac{(s_{k}^{*}-s_{j}^{*})\\big(s_{k}+s_{j}\\big)\\big(\\tilde{s}_{k}+\\tilde{s}_{j}\\big)}{s_{k}^{2}-s_{j}^{2}},\\frac{(s_{k}-s_{j})\\big(s_{k}^{*}+s_{j}^{*}\\big)\\big(\\tilde{s}_{k}+\\tilde{s}_{j}\\big)}{s_{k}^{2}-s_{j}^{2}}\\big)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As a result, we can take ", "page_idx": 34}, {"type": "equation", "text": "$$\nc=c(a_{1},\\ldots,a_{K})={\\frac{\\operatorname*{min}_{k,j:a_{k}\\neq a_{j}}|a_{k}-a_{j}|a_{K}^{2}}{\\operatorname*{max}_{k,j:a_{k}\\neq a_{j}}\\left|a_{k}^{2}-a_{j}^{2}\\right|}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $c^{\\prime}$ be a large constant such that if $x~>~c^{\\prime}(d^{-1}{\\bf\\sigma}+d^{2\\left(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1\\right)})$ , then $\\begin{array}{r l}{{\\frac{d x}{d t}}}&{{}\\leq}\\end{array}$ $-{\\frac{c(a_{1},...,a_{K})d}{2}}x$ $T_{4}$ $x\\ \\leq\\ c^{\\prime}(d^{-1}\\,+\\,d^{2\\left(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1\\right)})$ e r,a fftoerr $T_{3}$ $\\begin{array}{r}{T_{4}\\,-\\,T_{3}\\,\\,\\le\\,\\,-\\frac{2d}{c(a_{1},...,a_{K})}\\log\\bigl(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)}\\bigr)\\,+\\,{\\cal O}(d)}\\end{array}$ every $t\\ >\\ T_{4}$ , $x$ cannot be larger than $c^{\\prime}(d^{-1}\\,+\\,d^{2\\left(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1\\right)})$ because $\\textstyle{\\frac{d x}{d t}}~<~0$ if $x\\,=\\,c^{\\prime}(d^{-1}+d^{2\\left(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1\\right)})$ . After $T_{4}$ , the dynamics of each singular value is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\nd^{2}\\frac{d s_{i}}{d t}=2((1+O(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)}))a_{i}d-s_{i})s_{i}(1+O(d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)}))\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $T_{5}$ be the first time after $T_{4}$ when $|s_{i}-a_{i}d|\\leq O(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)})\\log d$ . Then ", "page_idx": 34}, {"type": "equation", "text": "$$\nT_{5}-T_{4}\\leq-\\frac{\\operatorname*{max}(-1,2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1))}{2a_{K}}d\\log d+O(d\\log\\log d).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We conclude that at time ", "page_idx": 34}, {"type": "equation", "text": "$$\nT^{*}=\\left(\\frac{1-\\gamma_{\\sigma^{2}}-\\gamma_{w}}{a_{K}}+\\frac{2\\operatorname*{max}(1,2(-\\gamma_{\\sigma^{2}}-\\gamma_{w}+1))}{c(a_{1},\\ldots,a_{K})}+\\frac{\\operatorname*{max}(1,2(-\\gamma_{\\sigma^{2}}-\\gamma_{w}+1))}{2a_{K}}\\right)d\\log d\\sigma\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~x(T^{*})\\le O(d^{\\operatorname*{max}(-1,2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1))}),}\\\\ &{A_{\\theta(T^{*})}\\in P_{2}(C,\\operatorname*{max}\\{1,2(1-\\gamma_{\\sigma^{2}}-\\gamma_{w})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n|s_{i}(T^{*})-a_{i}d|\\leq O(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)})\\log d\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.4 Analysis of Testing Error ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In section E.3 we proved that $A_{\\theta(t)}$ is almost aligned with $A^{*}$ throughout the training. With the alignment in hand, we are ready to give a time to stop training and the testing error at the end of training. ", "page_idx": 35}, {"type": "text", "text": "Theorem E.5. Assume that $A_{\\theta(t)}$ follows the gradient flow dynamics. At time ", "page_idx": 35}, {"type": "equation", "text": "$$\nr^{\\ast}=\\left(\\frac{1-\\gamma_{\\sigma^{2}}-\\gamma_{w}}{a_{K}}+\\frac{2m a x(1,2(-\\gamma_{\\sigma^{2}}-\\gamma_{w}+1))}{c(a_{1},\\ldots,a_{K})}+\\frac{m a x(1,2(-\\gamma_{\\sigma^{2}}-\\gamma_{w}+1))}{2a_{K}}\\right)d\\log d+O(d).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "the testing error ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A_{\\theta(T^{*})}-A^{*}\\|_{F}^{2}\\leq O(\\sigma^{4}w d^{2})+O(\\sigma^{4}w^{2}\\log^{2}d)+O(d^{\\frac{3}{2}})+O(\\sigma^{2}w d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let $P_{K}$ be the projection to the largest $K$ singular values. Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A_{\\theta(t)}-A^{*}\\|_{F}^{2}\\leq\\|P_{K}A_{\\theta(t)}-A^{*}\\|_{F}^{2}+\\|(I-P_{K})A_{\\theta(t)}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $s_{1},\\ldots,s_{d}$ be the singular values of $A_{\\theta(t)}$ . Then $\\begin{array}{r}{\\|(I-P_{K})A_{\\theta(t)}\\|_{F}^{2}\\;=\\;\\sum_{p\\geq K+1}s_{p}^{2}}\\end{array}$ . The derivative of $s_{1},\\ldots,s_{d}$ reads ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{2}\\frac{d S}{d t}=I\\odot\\Big(U^{T}(A^{*}+E)V\\sqrt{S^{2}+\\sigma^{4}w^{2}}+\\sqrt{S^{2}+\\sigma^{4}w^{2}}U^{T}(A^{*}+E)V-2S\\sqrt{S^{2}+\\sigma^{4}w^{2}}\\Big)}\\\\ &{\\qquad\\qquad+I\\odot\\big(U^{T}(A^{*}+E)V O(\\sigma^{2}w_{1})+O(\\sigma^{2}w_{1})U^{T}(A^{*}+E)V-2S O(\\sigma^{2}w_{1})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If $p\\geq K+1$ , then $s_{p}\\leq C^{\\prime}\\sigma^{2}w$ for some constant $C^{\\prime}$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\nd^{2}\\bigg|\\frac{d s_{p}}{d t}\\bigg|\\leq\\big|(U^{T}A^{*}V)_{p p}+(U^{T}E V)_{p p}\\big|O(\\sigma^{2}w).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d^{2}\\displaystyle\\frac{d}{d t}\\sum_{p\\geq K+1}s_{p}^{2}\\leq\\sum_{p}s_{p}\\left(\\sum_{q\\leq K}U_{q p}V_{q p}A^{*}(q q)+(U^{T}A^{*}V)_{p p}\\right)O(\\sigma^{2}w)}\\\\ &{\\leq\\displaystyle\\sum_{p}s_{p}\\left(\\sum_{q\\leq K}|U_{q p}||V_{q p}|O(d)+O(\\sqrt{d})\\right)O(\\sigma^{2}w)}\\\\ &{\\leq\\displaystyle\\sum_{q\\leq K}\\left(\\sum_{p}s_{p}^{2}\\right)^{\\frac{1}{2}}\\left(\\sum_{p}|U_{q p}|^{2}\\right)^{\\frac{1}{2}}\\operatorname*{max}_{p}|V_{q p}|O(\\sigma^{2}w d)+\\left(\\sum_{p}s_{p}^{2}\\right)^{\\frac{1}{2}}O(\\sigma^{2}w d)}\\\\ &{\\leq\\left(\\sum_{p}s_{p}^{2}\\right)^{\\frac{1}{2}}\\left(O(d^{1-\\gamma}\\sigma^{2}w)+O(\\sigma^{2}w d)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\nd^{2}\\frac{d}{d t}\\|(I-P_{K})A_{\\theta(t)}\\|_{F}\\leq O(\\sigma^{2}w d),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\|(I-P_{K})A_{\\theta(t)}\\|_{F}\\leq\\sqrt{d}O(\\sigma^{2}\\sqrt{w d})+O(\\sigma^{2}w d\\frac{\\log d}{d})=O(\\sigma^{2}\\sqrt{w}d+\\sigma^{2}w\\log d)}}\\\\ {\\displaystyle{\\|(I-P_{K})A_{\\theta(t)}\\|_{F}^{2}\\leq O(\\sigma^{4}w d^{2}+\\sigma^{4}w^{2}\\log^{2}d)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next we estimate $\\|P_{K}A_{\\theta(t)}-A^{*}\\|_{F}^{2}$ . Notice that $\\begin{array}{r}{\\|P_{K}A_{\\theta(t)}-A^{*}\\|_{F}^{2}\\,=\\,\\sum_{i,j}\\|P_{K}A_{\\theta(t)}(i,j)\\,-\\,}\\end{array}$ $A^{*}(i,j)\\|_{F}^{2}$ . If $i\\neq j$ , then $A^{*}(i,j)=0$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert P_{K}A_{\\theta(t)}(i,j)\\Vert_{F}^{2}\\leq\\underset{k:\\mathrm{signal},k\\neq i}{\\sum}\\Vert U(i,k)\\Vert_{F}^{2}\\Vert S(k,k)V(j,k)^{T}\\Vert_{F}^{2}}&{}\\\\ {+\\left\\Vert U(i,j)S(j,j)\\right\\Vert_{F}^{2}+\\left\\Vert V(j,k)^{T}\\right\\Vert_{F}^{2}}&{}\\\\ {\\leq\\cal O(x d^{2})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If $i=j=m+1$ , we also have $A^{*}(m+1,m+1)=0$ , and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|P_{K}A_{\\theta(t)}(m+1,m+1)\\|_{F}^{2}\\leq\\sum_{k:\\mathrm{signal}}O(d^{2})\\|U(m+1,k)\\|_{F}^{2}+\\|V(m+1,k)\\|_{F}^{2}\\leq O(d^{2}x)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now assume that $i=j$ are both signals. Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{K}A_{\\theta(t)}(i,i)-A^{*}(i,i)\\|_{F}^{2}\\le\\|U(i,i)(S(i,i)-S^{*}(i,i))V^{T}(i,i)\\|_{F}^{2}+O(d^{2})\\|U(i,i)V(i,i)^{T}-I\\|_{F}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le O(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)})\\log d+O(d^{2}\\sqrt{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since there are only finitely many blocks in total, we conclude that at time $T^{*}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{A_{\\theta(T^{*})}-A^{*}\\|_{F}^{2}\\le O(\\sigma^{4}w d^{2}+\\sigma^{4}w^{2}\\log^{2}d+d^{2}O(d^{\\operatorname*{max}(-\\frac{1}{2},(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1))}))+O(d^{-1}+d^{2(\\gamma_{\\sigma^{2}}+\\gamma_{w}-1)}}\\\\ &{}&{\\|A_{\\theta(T^{*})}-A^{*}\\|_{F}^{2}\\le O(\\sigma^{4}w d^{2})+O(\\sigma^{4}w^{2}\\log^{2}d)+O(d^{\\frac{3}{2}})+O(\\sigma^{2}w d)\\quad\\quad\\quad\\mathrm{(41)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "F Gradient Descent Dynamics and Proof of Theorem 2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section we prove that gradient descent dynamics of $A_{\\theta(t)}$ is well-approximated by gradient flow dynamics of $A_{\\theta(t)}$ . ", "page_idx": 36}, {"type": "text", "text": "F.1 Gradient Descent vs Gradient Flow ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To study the dynamics of $A_{t}$ under gradient flow, we show that if the learning rate is small enough, then the gradient flow dynamics will be close to the gradient descent dynamics. ", "page_idx": 36}, {"type": "text", "text": "Lemma F.1. Assume that $A$ is a matrix (not necessarily square matrix). $F$ is a function: $\\mathbb{R}^{\\dim A}\\to$ $\\mathbb{R}^{\\dim A}$ . The norm $\\Vert\\cdot\\Vert$ satisfies $\\|A B\\|\\leq\\|A\\|\\|B\\|$ for all $A$ and $B$ . In particular, operator norm and Frobenius norm satisfies this property. Assume that $\\operatorname*{sup}_{A}\\|F(A)\\|\\leq C_{0}$ and $\\|\\nabla F\\|\\leq C_{1}$ for some constant. Consider gradient flow dynamics and gradient descent dynamics. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{G r a d i e n t\\:F l o w:\\:\\frac{d A_{f}}{d t}=F(A_{f})}}\\\\ {{G r a d i e n t\\:D e s c e n t{:\\:\\frac{A_{d}\\left((k+1)\\eta\\right)-A_{d}(k\\eta)}{\\eta}=F(A_{d}(k\\eta))}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Assume that $A_{f}(0)=A_{d}(0)$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|A_{f}-A_{d}\\|(k\\eta)\\le((1+\\eta C_{1})^{k-1}-1)\\frac{1}{2}\\eta C_{0}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Notice that we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A_{f}((k+1)\\eta)-A_{f}(k\\eta)=\\displaystyle\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))d t}}\\\\ {{\\displaystyle(A_{f}-A_{d})((k+1)\\eta)-((A_{f}-A_{d})(k\\eta))=\\displaystyle\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))-F(A_{d}(k\\eta))d t}}\\\\ {{\\displaystyle\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))-F(A_{d}(k\\eta))d t=\\displaystyle\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))-F(A_{f}(k\\eta))d t}}\\\\ {{\\displaystyle+\\,\\eta(F(A_{f}(k\\eta))-F(A_{d}(k\\eta)))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\begin{array}{r}{G(t)=\\int_{0}^{t}F(A_{f}(k\\eta+s))d s}\\end{array}$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))-F(A_{f}(k\\eta))d t=G(\\eta)-G(0)-\\eta G^{\\prime}(0)-\\frac{1}{2}\\eta^{2}G^{\\prime\\prime}(\\xi)}}\\\\ &{}&{\\quad=\\frac{1}{2}\\eta^{2}\\frac{d}{d t}|_{t=\\xi}F(A_{f}(k\\eta+t))}\\\\ &{}&{\\quad=\\frac{1}{2}\\eta^{2}\\frac{\\partial F}{\\partial A}(A_{f}(k\\eta+\\xi))\\frac{d}{d t}|_{t=\\xi}A_{f}(k\\eta+t)}\\\\ &{}&{\\quad=\\frac{1}{2}\\eta^{2}\\frac{\\partial F}{\\partial A}(A_{f}(k\\eta+\\xi))F(A_{f}(k\\eta+\\xi))}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\int_{0}^{\\eta}F(A_{f}(k\\eta+t))-F(A_{f}(k\\eta))d t\\|\\leq\\frac{1}{2}\\eta^{2}C_{0}C_{1}}\\\\ {\\eta\\|(F(A_{f}(k\\eta))-F(A_{d}(k\\eta)))\\|\\leq\\eta\\|\\nabla F\\|\\|A_{f}(k\\eta)-A_{d}(k\\eta)\\|\\leq\\eta C_{1}\\|A_{f}(k\\eta)-A_{d}(k\\eta)\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We conclude that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|A_{f}((k+1)\\eta)-A_{d}((k+1)\\eta)\\|\\leq(1+\\eta C_{1})\\|A_{f}(k\\eta)-A_{d}(k\\eta)\\|+\\frac{1}{2}\\eta^{2}C_{0}C_{1}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|A_{f}-A_{d}\\|((k+1)\\eta)+\\frac{1}{2}\\eta C_{0}\\leq(1+\\eta C_{1})(\\|A_{f}-A_{d}\\|(k\\eta)+\\frac{1}{2}\\eta C_{0})}\\\\ {\\leq(1+\\eta C_{1})^{k}\\frac{1}{2}\\eta C_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|A_{f}-A_{d}\\|(k\\eta)\\le((1+\\eta C_{1})^{k-1}-1)\\frac{1}{2}\\eta C_{0}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To apply the lemma above we need to prove that $\\|A_{\\theta(t)}\\|_{F}^{2}\\leq O(d^{2})$ throughout the training. ", "page_idx": 37}, {"type": "text", "text": "Lemma F.2. For both lazy and active regime, we always have $\\|A_{\\theta(t)}\\|_{F}^{2}\\,\\leq\\,10(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})$ throughout the training. ", "page_idx": 37}, {"type": "text", "text": "Proof. The gradient descent dynamics of $A_{\\theta(t)}$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\nA_{\\theta(t+1)}-A_{\\theta(t)}=\\eta d^{-2}(A^{*}+E-A_{\\theta(t)})C_{1}+C_{2}(A^{*}+E-A_{\\theta(t)}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad t r((A^{*}+E-A_{\\theta(t+1)})^{T}(A^{*}+E-A_{\\theta(t+1)})-(A^{*}+E-A_{\\theta(t)})^{T}(A^{*}+E-A_{\\theta(t)}))}\\\\ &{=-2t r((A^{*}+E-A_{\\theta(t)}))^{T}(A_{\\theta(t+1)}-A_{\\theta(t)}))+\\|A_{\\theta(t+1)}-A_{\\theta(t)}\\|_{F}^{2}}\\\\ &{\\leq-2\\eta d^{-2}t r((A^{*}+E-A_{\\theta(t)})^{T}(C_{1}+C_{2})(A^{*}+E-A_{\\theta(t)}))+\\eta^{2}O(d^{-2}\\|C_{1}\\|_{o p}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From theorem 2, we know that $C_{1}+C_{2}\\geq\\textstyle{\\frac{1}{3}}\\sigma^{2}w I$ . Therefore $t r((A^{*}+E-A_{\\theta(t)})^{T}(C_{1}+C_{2})(A^{*}+$ $\\begin{array}{r}{E-A_{\\theta(t)}))\\ge\\|A^{*}+E-A_{\\theta(t)}\\|_{F}^{2}\\frac{1}{3}\\sigma^{2}w\\ge c d^{2}\\sigma^{2}w}\\end{array}$ for some constant $c$ . Therefore for the lazy regime, we always have $\\lVert A^{*}+E-A_{\\theta(t)}\\rVert_{F}^{2}(t+1)\\le\\lVert A^{*}+E-A_{\\theta(t)}\\rVert_{F}^{2}(t)$ if $5(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})\\leq$ $\\|A_{\\theta(t)}\\|_{F}^{2}\\leq7(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})$ , which implies that $\\|A_{\\theta(t)}\\|_{F}^{2}\\leq10(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})$ for all time. For the active regime, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A^{*}+E-A_{\\theta(t)}\\|_{F}^{2}(t+1)-\\|A^{*}+E-A_{\\theta(t)}\\|_{F}^{2}(t)\\leq\\eta^{2}O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since the training has at most $O(\\frac{T^{*}}{\\eta})$ steps, we see that $\\forall t$ , $\\|A^{*}+E-A_{\\theta(t)}\\|_{F}^{2}\\,\\leq\\,2(\\|A^{*}\\|_{F}^{2}\\,+$ $\\|E\\|_{F}^{2})+O(\\eta T^{*})\\leq10(\\|A^{*}\\|_{F}^{2}+\\|E\\|_{F}^{2})$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Proof of main theorem. We first consider the active regime. It suffices to consider the error from considering gradient descent, rather than gradient flow. To apply lemma F.1, it is more convenient to consider the dynamics f\u221aor $W_{1}$ and $W_{2}$ . By lemma G.2, $\\lVert\\dot{W}_{1}^{T}\\dot{W}_{1}\\rVert_{F}^{2}+\\lVert W_{2}W_{2}^{T}\\rVert_{F}^{2}\\leq O(d^{2})$ and $\\|W_{1}\\|_{o p}+\\|W_{2}\\|_{o p}\\leq O(\\sqrt{d})$ . The gradient flow dynamics of $[W_{1},W_{2}^{T}]$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{d}{d t}[W_{1},W_{2}^{T}]=d^{-2}[W_{2}^{T}(A^{*}-W_{2}W_{1}),W_{1}(A^{*}-W_{2}W_{1})^{T}].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $F([W_{1},W_{2}^{T}])=d^{-2}[W_{2}^{T}(A^{*}-W_{2}W_{1}),W_{1}(A^{*}-W_{2}W_{1})^{T}],$ . Then $\\operatorname*{sup}_{t}\\|F(W_{1},W_{2}^{T})\\|_{F}\\leq$ $O(d^{-\\frac{1}{2}})$ . Computing the differential of $F$ , we obtain that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d F([W_{1},W_{2}^{T}])=d^{-2}[\\;-\\;d W_{2}^{T}(A^{*}-W_{2}W_{1})+W_{2}^{T}(-d W_{2}W_{1}-W_{2}d W_{1}),}\\\\ {\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;-\\;d W_{1}(A^{*}-W_{2}W_{1})^{T}+W_{1}(-d W_{1}^{T}W_{2}-W_{1}^{T}d W_{2})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and therefore $\\|\\nabla F\\|_{F}\\;\\leq\\;O(d^{-1})$ . In the active regime, the total number of training steps is $\\eta^{-1}O(d\\log d)$ By lemma F.1, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|W_{1}^{f l o w}-W_{1}^{d e s c e n t}\\|_{F}(T^{*})\\leq O(\\eta d^{-\\frac12})((1+\\eta O(d^{-1}))^{\\frac{O(d\\log d)}{\\eta}}-1)=O(\\eta d^{-\\frac12}\\log d)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the same holds true for $W_{2}$ . We conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\|W_{2}^{f l o w}W_{1}^{f l o w}-W_{2}^{d e s c e n t}W_{1}^{d e s c e n t}\\|_{F}}\\\\ &{{\\leq}(\\|W_{2}^{d e s c e n t}\\|_{o p}+\\|W_{1}^{d e s c e n t}\\|_{o p})O(\\eta d^{-\\frac{1}{2}}\\log d)}\\\\ &{{=}O(\\eta\\log d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|A_{\\theta}^{f l o w}-A_{\\theta}^{d e s c e n t}\\|_{F}^{2}\\leq O(\\eta^{2}\\log^{2}d).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the lazy regime, from strong bound we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nW_{1}^{T}W_{1}=(1+O(\\sqrt{\\frac{d}{w}}))\\sqrt{\\sigma^{4}w^{2}I+A^{T}A}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore\u221a $O(d^{-1}\\sigma\\sqrt{w})$ $\\|W_{1}\\|_{o p}~\\leq~O(\\sigma\\sqrt{w})$ . Similarly, $\\begin{array}{r l}&{2(\\sigma\\sqrt{w}),\\,\\operatorname*{sup}_{t}\\|F([W_{1},W_{2}^{T}])\\|_{F}\\ \\leq\\ d^{-2}(\\|W_{1}\\|_{o p}+\\|W_{2}\\|_{o p})O(d)\\ =}\\\\ &{\\|\\nabla F\\|_{F}\\,\\leq\\ d^{-2}O(\\|A^{*}-W_{2}W_{1}\\|_{F}+\\|W_{2}\\|_{F}\\|W_{1}\\|_{F})\\,=\\ d^{-2}O(\\sigma^{2}w).}\\end{array}$ By lemma F.1, at time $\\frac{100d^{2}\\log d}{\\sigma^{2}w}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\nW_{1}^{f l o w}-W_{1}^{d e s c e n t}\\|_{F}(\\frac{100d^{2}\\log d}{\\sigma^{2}w})\\le O(\\eta d^{-1}\\sigma\\sqrt{w})((1+\\eta O(d^{-2}\\sigma^{2}w))^{\\frac{100d^{2}\\log d}{\\sigma^{2}w\\eta}}-1)=O(\\eta d^{-1}\\sigma\\sqrt{w}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\|W_{2}^{f l o w}W_{1}^{f l o w}-W_{2}^{d e s c e n t}W_{1}^{d e s c e n t}\\|_{F}}\\\\ &{\\ \\ \\leq(\\|W_{2}^{d e s c e n t}\\|_{o p}+\\|W_{1}^{d e s c e n t}\\|_{o p})O(\\eta d^{-1}\\sigma\\sqrt{w}\\log d)}\\\\ &{\\ \\ \\ =O(\\eta d^{-1}\\sigma^{2}w\\log d).}\\\\ &{\\ \\ \\ \\ \\|W_{2}^{f l o w}W_{1}^{f l o w}-W_{2}^{d e s c e n t}W_{1}^{d e s c e n t}\\|_{F}\\leq O(\\eta^{2}d^{-2}\\sigma^{2}w\\log^{2}d).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Recall that in lemma D.1 we proved that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|A_{\\theta(t)}^{f l o w}-B_{t}\\|_{F}^{2}=o(d^{2}),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and at time $\\begin{array}{r}{00\\frac{d^{2}\\log d}{\\sigma^{2}w},\\|B_{t}-A^{*}-E\\|_{F}^{2}\\leq d^{-50}}\\end{array}$ , which implies that $\\|A_{\\theta}^{d e s c e n t}-A^{*}-E\\|_{F}^{2}\\leq o(d^{2})$ Before this time, we have $\\begin{array}{r}{\\|B_{t}-A^{*}\\|_{F}^{2}\\geq\\frac{1}{3}\\mathrm{min}(\\|A^{*}\\|_{F}^{2},\\|E\\|_{F}^{2})}\\end{array}$ . After this time, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t r((A^{*}+E-A_{\\theta(t+1)}^{d e s c e n t})^{T}(A^{*}+E-A_{\\theta(t+1)}^{d e s c e n t})-(A^{*}+E-A_{\\theta(t)}^{d e s c e n t})^{T}(A^{*}+E-A_{\\theta(t)}^{d e s c e n t}))}\\\\ &{=-2t r((A^{*}+E-A_{\\theta(t)}^{d e s c e n t}))^{T}(A_{\\theta(t+1)}^{d e s c e n t}-A_{\\theta(t)}^{d e s c e n t}))+\\|A_{\\theta(t+1)}^{d e s c e n t}-A_{\\theta(t)}^{d e s c e n t}\\|_{F}^{2}}\\\\ &{\\leq-2\\eta d^{-2}t r((A^{*}+E-A_{\\theta(t)}^{d e s c e n t})^{T}(C_{1}+C_{2})(A^{*}+E-A_{\\theta(t)}^{d e s c e n t}))}\\\\ &{\\quad+\\eta^{2}t r((A^{*}+E-A_{\\theta(t)}^{d e s c e n t})^{T}(10\\sigma^{4}w^{2}I)(A^{*}+E-A_{\\theta(t)}^{d e s c e n t}))}\\\\ &{\\leq-(2\\eta d^{-2}\\sigma^{2}-10\\eta^{2}\\sigma^{4}w^{2})\\|A^{*}+E-A_{\\theta(t)}^{d e s c e n t}\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore $\\|A^{*}\\ +\\ E\\ -\\ A_{\\theta(t)}^{d e s c e n t}\\|_{F}^{2}$ is decreasing and therefore $\\begin{array}{r l}{\\|A_{\\theta}^{d e s c e n t}\\ -\\ A^{*}\\|_{F}^{2}}&{{}\\geq}\\end{array}$ $\\textstyle{\\frac{1}{3}}\\mathbf{min}(\\|A^{*}\\|_{F}^{2},\\|E\\|_{F}^{2})$ for all time. ", "page_idx": 38}, {"type": "text", "text": "G Experimental setup ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We now describe the experimental setup for the experiments shown in Figures 1 and 2. For all the experiments, we used the losses ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{train}}(\\theta)={\\frac{1}{d^{2}}}\\left\\|A_{\\theta}-(A^{\\star}+E)\\right\\|_{F}^{2};\\quad{\\mathcal{L}}_{\\mathrm{test}}(\\theta)={\\frac{1}{d^{2}}}\\left\\|A_{\\theta}-A^{\\star}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where E has i.i.d. N(0, 1) entries, A\u22c6 = K\u22121/2  iK= 1 uiviT with ui, vi \u223c N(0, Idd) Gaussian vectors in $\\mathbb{R}^{d}$ . This means that Rank $A^{\\star}=K$ . The factor $K^{-1/2}$ ensures that $\\|A^{\\star}\\|_{F}=\\Theta(d)$ . ", "page_idx": 38}, {"type": "text", "text": "We then either run the self-consistent dynamics (equation (1)) or gradient descent (equation (2)). Following Theorem 2, we take a learning rate $\\begin{array}{r}{\\eta=\\frac{\\dot{d^{2}}}{c w\\sigma^{2}}}\\end{array}$ for $\\gamma_{\\sigma^{2}}+\\gamma_{2}>1$ , and $\\begin{array}{r}{\\eta=\\frac{d^{2}}{c\\|A^{\\star}\\|_{\\mathrm{op}}}}\\end{array}$ otherwise, where $c$ is usually 50 but can be taken to be 2 or 5 for faster convergence at the cost of more unstable training. ", "page_idx": 39}, {"type": "text", "text": "For the experiments in Figure 1, we took $d\\,=\\,500$ and $K\\,=\\,5$ . For the experiments in Figure 2, we took $d=200$ and $K=5$ . For making the contour plot, we took a grid with 35 points for $\\gamma_{\\sigma^{2}}\\in[-3.0,0.0]$ and 35 points for $\\gamma_{w}\\in[0,2.8]$ . For each of the $35^{2}$ pair of values for $(\\gamma_{\\sigma^{2}},\\gamma_{w})$ , we ran gradient descent (and for the lower right plot the self-consistent dynamics too) until the train error converged. For all the runs, we took the same realizations of $A^{\\star}$ and $E$ . ", "page_idx": 39}, {"type": "text", "text": "All the experiments were implemented in PyTorch [40]. Experiments took 12 hours of compute, using two GeForce RTX 2080 Ti (11GB memory) and two TITAN V (12GB memory). ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The contribution section accurately describes our contributions, and all theorems/propositions are proven in the main or the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We discuss limitations of our results and approach after we state them. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: All assumptions are stated in the Theorem statements. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The experimental setup is described in the Appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: We use synthetic data, with a description of how to build this synthetic data. The experiments are only there for visualization purposes, we see no particular need to publish it. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Most details are given in the experimental setup section in the Appendix. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 42}, {"type": "text", "text": "Justification: The numerical experiments are mostly there as a visualization of the theoretical results, our main goal is therefore clarity, which would be hurt by putting error bars everywhere. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: In the experimental setup section of the Appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We have read the Code of Ethics and see no issue. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper is theoretical in nature, so it has no direct societal impact that can be meaningfully discussed. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Not relevant to our paper. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: We only use our own synthetic data. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: We do not release any new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Not relevant to this paper. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Not relevant to this paper. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]