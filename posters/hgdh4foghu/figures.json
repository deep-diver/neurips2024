[{"figure_path": "hgdh4foghu/figures/figures_1_1.jpg", "caption": "Figure 1: Policy-Shaped Prediction in an environment with challenging distractions. (left) Training of an otherwise-unaltered DreamerV3 agent is modified in two ways: 1) A head is added to predict the previous action based on the image encoding, and the gradient of the head is subtracted from the gradient of the image encoder, and 2) the loss is scaled pixelwise by a policy-shaped loss weight. (right) The loss weight uses the gradient of the policy to the input pixels. The image is segmented, and the pixel weights are averaged within each segmented object. Dashed lines signify gradient flow.", "description": "The figure illustrates the architecture of Policy-Shaped Prediction (PSP).  The left panel shows how the standard DreamerV3 model is modified by adding an action prediction head and a policy-shaped loss weighting mechanism. The action prediction head helps mitigate distractions caused by the agent's own actions. The policy-shaped loss weighting focuses learning on important parts of the image. The right panel shows how image segmentation and gradient aggregation refine the loss weighting process, focusing the model's capacity on task-relevant information.", "section": "2 Policy-Shaped Prediction"}, {"figure_path": "hgdh4foghu/figures/figures_4_1.jpg", "caption": "Figure 1: Policy-Shaped Prediction in an environment with challenging distractions. (left) Training of an otherwise-unaltered DreamerV3 agent is modified in two ways: 1) A head is added to predict the previous action based on the image encoding, and the gradient of the head is subtracted from the gradient of the image encoder, and 2) the loss is scaled pixelwise by a policy-shaped loss weight. (right) The loss weight uses the gradient of the policy to the input pixels. The image is segmented, and the pixel weights are averaged within each segmented object. Dashed lines signify gradient flow.", "description": "This figure illustrates the core idea behind Policy-Shaped Prediction (PSP).  The left panel shows how a modified DreamerV3 architecture is trained.  A new action prediction head helps mitigate distractions caused by the agent's own actions, and the reconstruction loss is weighted based on the gradient of the policy w.r.t. the image. The right panel shows how this weighting is computed using image segmentation to focus on task-relevant parts of the image.", "section": "2 Policy-Shaped Prediction"}, {"figure_path": "hgdh4foghu/figures/figures_5_1.jpg", "caption": "Figure 3: Training curve comparisons on Reafferent Deepmind Control. Mean \u00b1 std. err.", "description": "This figure displays the training curves for different model-based reinforcement learning (MBRL) agents on the Reafferent DeepMind Control Suite. The Reafferent environment introduces learnable but irrelevant distractions to challenge the agents' ability to focus on important information. The figure showcases the performance of the proposed method (PSP) in comparison to several baselines, including DreamerV3, Task Informed Abstractions (TIA), Denoised MDP, and DreamerPro. The results demonstrate that PSP significantly outperforms the baselines in handling the distractors while maintaining similar performance in non-distracting environments. The y-axis represents the score achieved by the agents, and the x-axis shows the number of training steps taken.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/figures/figures_5_2.jpg", "caption": "Figure 4: Reconstructed image comparison, PSP vs. DreamerV3 on Reafferent Cheetah Run. This highlights how PSP focuses on modeling the hind leg (white arrow), while DreamerV3 focuses on modeling the background but fails to model the hind leg. From left: True, reconstructed, difference (true - recon.), PSP salience loss weight.", "description": "This figure compares the image reconstruction quality of DreamerV3 and the proposed method, PSP, on the Reafferent Cheetah Run environment.  DreamerV3 accurately reconstructs the background but fails to accurately reconstruct the cheetah's hind leg, demonstrating its susceptibility to distractions. PSP, in contrast, successfully focuses on the relevant parts of the image, resulting in a much more accurate reconstruction of the agent.", "section": "2.2 Object-based aggregation of gradient weights"}, {"figure_path": "hgdh4foghu/figures/figures_5_3.jpg", "caption": "Figure 1: Policy-Shaped Prediction in an environment with challenging distractions. (left) Training of an otherwise-unaltered DreamerV3 agent is modified in two ways: 1) A head is added to predict the previous action based on the image encoding, and the gradient of the head is subtracted from the gradient of the image encoder, and 2) the loss is scaled pixelwise by a policy-shaped loss weight. (right) The loss weight uses the gradient of the policy to the input pixels. The image is segmented, and the pixel weights are averaged within each segmented object. Dashed lines signify gradient flow.", "description": "This figure illustrates the modifications made by the Policy-Shaped Prediction (PSP) method to the DreamerV3 architecture.  The left side shows the training process modifications: a new action prediction head is added to reduce the impact of self-linked distractions, and a policy-shaped loss weight is calculated and applied to the loss. The right side shows how this loss weight is generated using the gradient of the policy and image segmentation to focus the model on task-relevant parts of the image.", "section": "Policy-Shaped Prediction"}, {"figure_path": "hgdh4foghu/figures/figures_6_1.jpg", "caption": "Figure 6: Training curve comparison on Distracting Control. Mean \u00b1 std. err.", "description": "This figure compares the training performance of different model-based reinforcement learning (MBRL) agents on the Distracting Control Suite environment.  The x-axis represents the number of training steps, and the y-axis represents the average score achieved by each agent.  The figure shows that the proposed Policy-Shaped Prediction (PSP) method significantly outperforms other state-of-the-art MBRL methods in terms of robustness to distracting stimuli.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/figures/figures_13_1.jpg", "caption": "Figure 4: Reconstructed image comparison, PSP vs. DreamerV3 on Reafferent Cheetah Run. This highlights how PSP focuses on modeling the hind leg (white arrow), while DreamerV3 focuses on modeling the background but fails to model the hind leg. From left: True, reconstructed, difference (true - recon.), PSP salience loss weight.", "description": "This figure compares the image reconstruction quality of DreamerV3 and PSP on a specific example from the Reafferent Cheetah Run environment.  It shows that DreamerV3 accurately reproduces the distracting background but fails to accurately reconstruct the cheetah's leg, indicating that it misallocates its capacity. In contrast, PSP successfully reconstructs the leg, demonstrating its ability to prioritize task-relevant information over distracting details. The salience map (loss weight) generated by PSP visually highlights the agent's leg.", "section": "2 Policy-Shaped Prediction"}, {"figure_path": "hgdh4foghu/figures/figures_13_2.jpg", "caption": "Figure A2: Denoised MDP reconstructs the background with a high degree of fidelity, but does not clearly render the Cheetah agent.", "description": "This figure compares the true image with the reconstructed image from a Denoised MDP model. The background is reconstructed with high fidelity, but the cheetah agent is not clearly rendered, indicating a failure of the model to focus on the task-relevant aspects of the image.", "section": "3.2 Reafferent Deepmind Control Suite"}, {"figure_path": "hgdh4foghu/figures/figures_13_3.jpg", "caption": "Figure A3: To more thoroughly show that the reafferent environment impacts DreamerV3 because of the learnable time & action mapping to backgrounds (and not purely because of the presence of the backgrounds themselves), we include this training curve from an environment which uses the same backgrounds, but with a random choice of background at each timestep. This demonstrates effective policy learning in spite of the distracting (but unlearnable) background.", "description": "The figure shows a training curve for DreamerV3 agent on Cheetah Run task with a distractor.  The distractor is white noise, which is randomly selected at each timestep, thus unlearnable by the agent.  The goal is to demonstrate that the impact of the reafferent environment stems from the deterministic mapping of time and action to the backgrounds, not simply the presence of the distracting background itself.", "section": "3.2 Reafferent Deepmind Control Suite"}, {"figure_path": "hgdh4foghu/figures/figures_14_1.jpg", "caption": "Figure A4: Improvement in specificity of the salience map across training.", "description": "This figure shows how the salience maps, generated by the policy gradient, change over training. The top row shows the visual input at different training steps (50K, 100K, 500K, 950K). The bottom row shows the corresponding salience maps. As training progresses, the salience maps become increasingly focused on the agent, indicating that the model is learning to prioritize relevant information for policy learning. This shows that the method is learning to ignore the distracting information and focus on the relevant information that helps in making decisions.", "section": "3.2 Reafferent Deepmind Control Suite"}, {"figure_path": "hgdh4foghu/figures/figures_14_2.jpg", "caption": "Figure 3: Training curve comparisons on Reafferent Deepmind Control. Mean \u00b1 std. err.", "description": "This figure compares the training performance of different model-based reinforcement learning (MBRL) methods on the Reafferent DeepMind Control Suite.  The Reafferent environment introduces highly predictable but irrelevant distractions to test the robustness of the algorithms.  The plot shows the average score (mean \u00b1 standard error) of each algorithm over training steps. PSP (Policy-Shaped Prediction), the authors' proposed method, is compared against several baselines designed to handle distractors, including DreamerV3, TIA (Task Informed Abstractions), Denoised MDP, and DreamerPro.  The figure shows the training curves for two tasks: Cheetah Run and Hopper Stand.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/figures/figures_14_3.jpg", "caption": "Figure A6: Interpolation of the gradient based reconstruction loss and a uniform reconstruction loss yields superior performance to a gradient based reconstruction loss without interpolation, which manifests as a faster initial rise in score with PSP. Interpolation allows the agent to construct a useful world model even when the policy is not yet very good (and hence minimizes the impact of the 'chicken-and-egg' problem where the agent has neither a good policy nor a good world model). Mean \u00b1 std. err., n=3 runs.", "description": "This figure shows the training performance curves for the Cheetah Run task in the Reafferent environment.  Two versions of the Policy-Shaped Prediction (PSP) method are compared: one with and one without loss interpolation. The results demonstrate that incorporating loss interpolation leads to significantly improved performance, particularly during the initial stages of training. This highlights the benefit of preventing the world model from getting stuck in poor local minima by maintaining a balance between focusing on crucial information and reconstructing less important details.", "section": "Ablation study"}, {"figure_path": "hgdh4foghu/figures/figures_15_1.jpg", "caption": "Figure A7: In order to test the adaptability of a model trained for one task via PSP to another task for the same agent, we switched from Walker Run to Walker Stand at step 1M, while leaving the Reafferent background unchanged. We see that the learned model can quickly adapt to the new task, even with the Reafferent background. Mean \u00b1 std. err., n=3 runs.", "description": "This figure shows the training performance of the Policy-Shaped Prediction (PSP) method on a task-switching scenario. Initially, the model is trained on the \"Walker Run\" task of the Reafferent DeepMind Control Suite. At step 1 million, the task is switched to \"Walker Stand\", while maintaining the same Reafferent background. The plot shows that the PSP agent quickly adapts to the new task, demonstrating its ability to transfer knowledge and adapt to changes in the task even with complex, predictable distractors. This highlights the robustness and adaptability of the PSP method.", "section": "3.5 Additional segmentation models"}, {"figure_path": "hgdh4foghu/figures/figures_15_2.jpg", "caption": "Figure A8: In order to test the adaptability of a PSP model to a dynamic distraction, we switched the settings of the Reafferent distraction at step 1M. We see that the learned model can quickly adapt to the new distraction. Mean \u00b1 std. err., n=2 runs.", "description": "This figure shows the result of an experiment designed to test the adaptability of the Policy-Shaped Prediction (PSP) model to dynamic distractions. In this experiment, the background distractions in the Reafferent DeepMind Control environment were changed at step 1 million. The plot shows that the PSP model quickly adapts to the new distractions, demonstrating its robustness and adaptability.", "section": "3.4 Ablation study"}, {"figure_path": "hgdh4foghu/figures/figures_15_3.jpg", "caption": "Figure 1: Policy-Shaped Prediction in an environment with challenging distractions. (left) Training of an otherwise-unaltered DreamerV3 agent is modified in two ways: 1) A head is added to predict the previous action based on the image encoding, and the gradient of the head is subtracted from the gradient of the image encoder, and 2) the loss is scaled pixelwise by a policy-shaped loss weight. (right) The loss weight uses the gradient of the policy to the input pixels. The image is segmented, and the pixel weights are averaged within each segmented object. Dashed lines signify gradient flow.", "description": "This figure illustrates the Policy-Shaped Prediction (PSP) method. The left panel shows the modified DreamerV3 architecture, highlighting the addition of an action prediction head and policy-shaped loss weighting. The right panel details the process of calculating policy-shaped loss weights using image segmentation and gradients of the policy.", "section": "2 Policy-Shaped Prediction"}, {"figure_path": "hgdh4foghu/figures/figures_16_1.jpg", "caption": "Figure 3: Training curve comparisons on Reafferent Deepmind Control. Mean \u00b1 std. err.", "description": "This figure compares the training curves of different model-based reinforcement learning (MBRL) agents on the Reafferent DeepMind Control Suite. The Reafferent environment introduces challenging distractors that are highly predictable but irrelevant to learning a good policy. The figure shows that the proposed Policy-Shaped Prediction (PSP) method significantly outperforms existing MBRL methods like DreamerV3, Task Informed Abstractions (TIA), Denoised MDP, and DreamerPro in this challenging environment.  The y-axis represents the cumulative reward achieved during training (score), and the x-axis represents the number of training steps. Error bars show mean \u00b1 standard error.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/figures/figures_16_2.jpg", "caption": "Figure 6: Training curve comparison on Distracting Control. Mean \u00b1 std. err.", "description": "This figure compares the training performance of various model-based reinforcement learning (MBRL) agents on the Distracting Control Suite benchmark.  The agents are trained on two tasks: Cheetah Run and Hopper Stand.  The figure shows that PSP (ours) consistently outperforms other methods, including DreamerV3, TIA, Denoised MDP, and DreamerPro. The shaded areas represent standard error, indicating the variability of the results across multiple runs of each agent.", "section": "3.3 Performance on unaltered DMC and Distracting Control Suite"}]