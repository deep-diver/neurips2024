[{"figure_path": "hgdh4foghu/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison across environments. DrQv2 is model-free, all others are model-based. TIA is task-informed abstraction, dMDP is denoised MDP, mean \u00b1 standard deviation.", "description": "This table compares the performance of different model-based reinforcement learning (MBRL) agents and one model-free agent (DrQv2) across three different environments: Reafferent DeepMind Control, Unmodified DeepMind Control, and Distracting DeepMind Control.  For each environment, the table shows the average scores achieved by each agent on two tasks: Cheetah Run and Hopper Stand.  The scores represent the performance of the agents in each environment and task, showing their mean and standard deviation across multiple runs.  The table highlights the relative performance of PSP against existing MBRL methods, particularly in challenging environments with distractions.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/tables/tables_7_1.jpg", "caption": "Table 2: Performance of ablated versions of PSP. Scores are shown for Cheetah Run in unaltered and reafferent Deepmind Control environments. The unablated PSP achieves good performance on both environments, and while some ablations achieve slightly better scores on either unaltered or reafferent, they trade off performance in the other environment.", "description": "This table presents the results of an ablation study on the Policy-Shaped Prediction (PSP) method.  It shows the performance of different versions of PSP, each lacking one or more components of the full method (gradient weighting, segmentation, and adversarial action head), on two environments: the standard DeepMind Control Suite and a modified version called Reafferent DeepMind Control.  The results highlight the contribution of each component and demonstrate that the full PSP model achieves a good balance of performance across both environments.", "section": "3.4 Ablation study"}, {"figure_path": "hgdh4foghu/tables/tables_7_2.jpg", "caption": "Table 3: Performance comparison across environments. DrQv2 is model-free, all others are model-based. TIA is task-informed abstraction, dMDP is denoised MDP, mean \u00b1 standard deviation.", "description": "This table compares the performance of the proposed PSP method against several baselines (DreamerV3, TIA, Denoised MDP, DreamerPro, and DrQv2) across three different environments: Reafferent Deepmind Control, Unmodified Deepmind Control, and Distracting Deepmind Control.  Each environment is tested on two tasks, Cheetah Run and Hopper Stand. The table shows the mean \u00b1 standard deviation of the scores achieved by each method on each task and environment.  It highlights the superior performance of PSP, especially in the Reafferent Control environment, demonstrating its effectiveness in handling complex, learnable distractions.", "section": "3 Experiments"}, {"figure_path": "hgdh4foghu/tables/tables_17_1.jpg", "caption": "Table A1: Comparison of computational overhead. FPS stands for frames per second.", "description": "This table presents a comparison of the computational overhead of different versions of the PSP algorithm.  The various versions differ in terms of the inclusion of the adversarial action head, the use of the SAM segmentation model, the use of the policy-gradient weighting approach versus the Value-Gradient weighted Model loss (VaGraM), and the inclusion of the linear interpolation technique for loss weighting. The final column indicates the frames per second (FPS) achieved by each version of the algorithm.", "section": "Computational Overhead"}]