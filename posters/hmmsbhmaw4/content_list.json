[{"type": "text", "text": "Periodic agent-state based Q-learning for POMDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Amit Sinha1, Matthieu Geist2, and Aditya Mahajan1 1McGill University, Mila 2Cohere ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The standard approach for Partially Observable Markov Decision Processes (POMDPs) is to convert them to a fully observed belief-state MDP. However, the belief state depends on the system model and is therefore not viable in reinforcement learning (RL) settings. A widely used alternative is to use an agent state, which is a model-free, recursively updateable function of the observation history. Examples include frame stacking and recurrent neural networks. Since the agent state is model-free, it is used to adapt standard RL algorithms to POMDPs. However, standard RL algorithms like Q-learning learn a stationary policy. Our main thesis that we illustrate via examples is that because the agent state does not satisfy the Markov property, non-stationary agent-state based policies can outperform stationary ones. To leverage this feature, we propose PASQL (periodic agent-state based Q-learning), which is a variant of agent-state-based Q-learning that learns periodic policies. By combining ideas from periodic Markov chains and stochastic approximation, we rigorously establish that PASQL converges to a cyclic limit and characterize the approximation error of the converged periodic policy. Finally, we present a numerical experiment to highlight the salient features of PASQL and demonstrate the benefit of learning periodic policies over stationary policies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in reinforcement learning (RL) have successfully integrated algorithms with strong theoretical guarantees and deep learning to achieve significant successes [Mni+13; Sil+16]. However, most RL theory is limited to models with perfect state observations [SB08; BT96]. Despite this, there is substantial empirical evidence showing that RL algorithms perform well in partially observed settings [Wie+07; Wie $+10$ ; Hau00; HS15; Gru $_{+18}$ ; Kap $+19$ ; Haf $\\vdots20$ ; Haf $+21$ ]. Recently, there has been a significant advances in the theoretical understanding of different RL algorithms for POMDPs [Sub $+22$ ; KY22; Sey $+23$ ; DRZ22] but a complete understanding is still lacking. ", "page_idx": 0}, {"type": "text", "text": "Planning in POMDPs. When the system model is known, the standard approach [ \u02daAst65; SS73; CKL94] is to construct an equivalent MDP with the belief state (which is the posterior distribution of the environment state given the history of observations and actions at the agent) as the information state. The belief state is policy independent and has time-homogeneous dynamics, which enables the formulation of a belief-state based dynamic program (DP). There is a rich literature which leverages the structure of the resulting DP to propose efficient algorithms to solve POMDPs [SS73; CKL94; CLZ97; Cha07; Zha09; PGT $^{\\prime}\\!+\\!03$ ; SS04; SV05]. See [KWW22] for a review. However, the belief state depends on the system model, so the belief-state based approach does not work for RL. ", "page_idx": 0}, {"type": "text", "text": "RL in POMDPs. An alternative approach for RL in POMDPs is to consider policies which depend on an agent state $\\{z_{t}\\}_{t\\ge1}$ , where $Z_{t}~\\in~Z$ , which is a recursively updateable compression of the history: the agent starts at an initial state $z_{0}$ and recursively updates the agent state as some function of the current agent-state, next observation, and current action. A simple instance of agent-state is frame stacking, where a window of previous observations is used as state [WS94; Mni+13; KY22]. Another example is to use a recurrent neural network such as LSTM or GRU to compress the history of observations and actions into an agent state [Wie $+07$ ; Wie+10; HS15; Kap $_{1+19}$ ; Haf+20]. In ", "page_idx": 0}, {"type": "text", "text": "2 3 5 6 8 9 10 11 12 13 14 15 16 ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Figure 1: The cells indicate the state of the environment. Cells with the same background color have the same observation. The cells with a thick red boundary correspond to elements of the set $\\mathsf{D}_{0}:=\\{n(n+1)/2+1:$ $n\\in\\mathbb{N}\\}$ , where the action 0 gives a reward of $+1$ and moves the state to the right, while the action 1 gives a reward of $-1$ and resets the state to 1. The cells with a thin black boundary correspond to elements of the set $\\mathsf{D}_{1}=\\mathbb{N}\\setminus\\mathsf{D}_{0}$ , where the action 1 gives the reward of $+1$ and moves the state to the right while the action 0 gives a reward of $-1$ and resets the state to 1. Discount factor $\\gamma=0.9$ . ", "page_idx": 1}, {"type": "text", "text": "fact, as argued in [DVZ22; $\\mathrm{Lu}{+}23$ ] such an agent state is present in most deep RL algorithms for POMDPs. We refer to such a representation as an \u201cagent state\u201d because it captures the agent\u2019s internal state that it uses for decision making. ", "page_idx": 1}, {"type": "text", "text": "When the agent state is an information state, i.e., satisfies the Markov property, i.e., $\\mathbb{P}(z_{t+1}|\\boldsymbol{z}_{1:t},\\boldsymbol{a}_{1:t})\\,=\\,\\mathbb{P}(z_{t+1}|\\boldsymbol{z}_{t},\\boldsymbol{a}_{t})$ and is sufficient for reward prediction, i.e., ${\\mathbb E}[R_{t}|y_{1:t},a_{1:t}]\\,=$ $\\mathbb{E}[R_{t}|z_{t},a_{t}]$ (where $y_{t}$ is the observation, $a_{t}$ is the action, and $R_{t}$ is the per-step reward), the optimal agent-state based policy can be obtained via a dynamic program (DP) $[\\mathrm{Sub}+22]$ . An example of such an agent state is the belief state. But, in general, the agent state is not an information state. For example, frame stacking and RNN do not satisfy the Markov property, in general. It is also possible to have agent-states that satisfy the Markov property but are not sufficient for reward prediction (e.g., when the agent state is always a constant). In all such settings, the best agent-state policy cannot be obtained via a DP. Nonetheless, there has been considerable interest to use RL to find a good agent-state based policy. ", "page_idx": 1}, {"type": "text", "text": "One of the most commonly used RL algorithms is off-policy Q-learning, which we call agent-state Q-learning (ASQL). In ASQL for POMDPs, the Q-learning iteration is applied as if the agent state satisfied the Markov property even though it does not. The agent starts with an initial $Q_{1}(\\bar{z},a)$ , acts according to a behavior policy $\\mu$ , i.e., chooses $a_{t}\\sim\\mu(z_{t})$ , and recursively updates ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ_{t+1}(z,a)=Q_{t}(z,a)+\\alpha_{t}(z,a)\\Big[R_{t}+\\gamma\\operatorname*{max}_{a^{\\prime}\\in\\mathbb{A}}Q_{t}(z_{t+1},a^{\\prime})-Q_{t}(z,a)\\Big]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\gamma~\\in~[0,1)$ is the discount factor and the learning rates $\\{\\alpha_{t}\\}_{t\\ge1}$ are chosen such that $\\alpha_{t}(z,a)=0$ if $(z,a)\\neq(z_{t},a_{t})$ . The convergence of ASQL has been recently presented in [KY22; $\\mathrm{Sey}{+23}]$ which show that under some technical assumptions, ASQL converges to a limit. The policy determined by ASQL is the greedy policy w.r.t. this limit. ", "page_idx": 1}, {"type": "text", "text": "Limitation of Q-learning with agent state. The greedy policy determined by ASQL is stationary (i.e., uses the same control law at every time). In infinite horizon MDPs (and, therefore, also in POMDPs when using the belief state as an agent state), stationary policies perform as well as nonstationary policies. This is because the agent-state satisfies the Markov property. However, in ASQL the agent state generally does not satisfy the Markov property. Therefore, restricting attention to stationary policies may lead to a loss of optimality! ", "page_idx": 1}, {"type": "text", "text": "As an illustration, consider the POMDP shown in Fig. 1, which is described in detail in App. A.2 as Ex. 2. Suppose the system starts in state 1. Since the dynamics are deterministic, the agent can infer the current state from the history of past actions and can take the action to increment the current state and receive a per-step reward of $+1$ . Thus, the performance $J_{\\mathrm{BD}}^{\\star}$ of belief-state based policies is $\\begin{array}{r}{J_{\\mathrm{BD}}^{\\star}=1/(1\\!-\\!\\gamma)}\\end{array}$ . Contrast this with the performance $J_{\\mathrm{SD}}^{\\star}$ of deterministic agent-state base policies with agent state equal to current observation, which is given by $J_{\\mathrm{SD}}^{\\star}=\\bigl(1+\\gamma-\\gamma^{2}\\bigr)/\\bigl(1-\\gamma^{3}\\bigr)^{\\circ}<J_{\\mathrm{BD}}^{\\star}$ . In particular, for $\\gamma=0.9$ , $J_{\\mathrm{BD}}^{\\star}=10\\$ which is larger than $J_{\\mathrm{SD}}^{\\star}=4.022$ . ", "page_idx": 1}, {"type": "text", "text": "We show that the gap between $J_{\\mathrm{SD}}^{\\star}$ and $J_{\\mathrm{BD}}^{\\star}$ can be reduced by considering non-stationary policies. Ex. 2 has deterministic dynamics, so the optimal policy can be implemented in open-loop via a sequence of control actions $\\{a_{t}^{\\star}\\}_{t\\ge1}$ , where $\\bar{a_{t}^{\\star}}=\\bar{\\mathbb{I}^{\\{t\\in\\bar{\\mathbb{D}}_{1}\\}}}$ . This open-loop policy can be implemented via any information structure, including agent-state based policies. Thus, a non-stationary deterministic agent-state based policy performs better than stationary deterministic agent-state based policies. A similar conclusion also holds for models with stochastic dynamics. ", "page_idx": 1}, {"type": "text", "text": "The main idea. Arbitrary non-stationary policies cannot be used in RL because such policies have countably infinite number of parameters. In this paper, we consider a simple class of non-stationary policies with finite number of parameters: periodic policies. An agent-state based policy $\\pi\\ =$ $\\overline{{(\\pi_{1},\\pi_{2},\\ldots)}}$ is said to be periodic with period $L$ if $\\pi_{t}=\\pi_{t^{\\prime}}$ whenever $t\\equiv t^{\\prime}$ (mod $L$ ). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To highlight the salient feature of periodic policies, we perform a brute force search over all deterministic periodic policies of period $L$ , for $L\\,=\\,\\{1,\\dotsc,10\\}$ , in Ex. 2. Let $J_{L}^{\\star}$ denote the optimal performance for policies of period $L$ . The result is shown below (see App. A.2 for details): ", "page_idx": 2}, {"type": "table", "img_path": "HmMSBhMAw4/tmp/a8318d074afe0f372590ffbc7e6ee38d2afa187b3e38ceee4fdb1945e26c4f1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "The above example highlights some salient features of periodic policies: (i) Periodic deterministic agent-state based policies may outperform stationary deterministic agent-state based policies. (ii) $\\bar{\\{J_{L}^{\\star}\\}}\\L L{\\ge}1$ is not a monotonically increasing sequence. This is because $\\Pi_{L}$ , the set of all periodic deterministic agent-state based policies of period $L$ , is not monotonically increasing. (iii) If $L$ divides $M$ , then $J_{L}^{\\star}\\,\\leq\\,J_{M}^{\\star}$ . This is because $\\Pi_{L}\\subseteq\\Pi_{M}$ . In other words, if we take any integer sequence $\\{L_{n}\\}_{n\\ge1}$ that has the property that $L_{n}$ divides $L_{n+1}$ , then the performance of the policies with period ${\\overline{{L}}}_{n}$ is monotonically increasing in $n$ . For example, periodic policies with period $L\\in\\{n!:n\\in\\mathbb{N}\\}$ will have monotonically increasing performance. (iv) In the above example, the set $\\mathsf{D}_{0}$ is chosen such that the optimal sequence of actions1 is not periodic. Therefore, even though periodic policies can achieve a performance that is arbitrarily close to the optimal belief-based policies, they are not necessarily globally optimal (in the class of non-stationary agent-state based policies). Thus, the periodic deterministic policy class is a middle ground between the stationary deterministic and non-stationary policy classes and provides us a simple way of leveraging the benefits of non-stationarity while trading-off computational and memory complexity. ", "page_idx": 2}, {"type": "text", "text": "The main contributions of this paper are as follows. ", "page_idx": 2}, {"type": "text", "text": "1. Motivated by the fact that non-stationary agent-state based policies outperform stationary ones, we propose a variant of agent-state based Q-learning (ASQL) that learns periodic policies. We call this algorithm periodic agent-state based Q-learning (PASQL).   \n2. We rigorously establish that PASQL converges to a cyclic limit. Therefore, the greedy policy w.r.t. the limit is a periodic policy. Due to the non-Markovian nature of the agent-state, the limit (of the Q-function and the greedy policy) depends on the behavioral policy used during learning.   \n3. We quantify the sub-optimality gap of the periodic policy learnt by PASQL.   \n4. We present numerical experiments to illustrate the convergence results, highlight the salient features of PASQL, and show that the periodic policy learned by PASQL indeed performs better than stationary policies learned by ASQL. ", "page_idx": 2}, {"type": "text", "text": "2 Periodic agent-state based Q-learning (PASQL) with agent state ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model for POMDPs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A POMDP is a stochastic dynamical system with state $s_{t}\\,\\in\\,{\\mathsf S}$ , input $a_{t}\\,\\in\\,{\\mathsf{A}}$ , and output $y_{t}\\in\\mathsf{Y}$ , where we assume that all sets are finite. The system operates in discrete time with the dynamics given as follows: The initial state $s_{1}\\sim\\rho$ and for any time $t\\in\\mathbb{N}$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}(s_{t+1},y_{t+1}\\mid s_{1:t},y_{1:t},a_{1:t})=\\mathbb{P}(s_{t+1},y_{t+1}\\mid s_{t},a_{t})=:P(s_{t+1},y_{t+1}\\mid s_{t},a_{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $P$ is a probability transition matrix. In addition, at each time the system yields a reward $R_{t}=r(s_{t},a_{t})$ . We will assume that $R_{t}\\in[0,R_{\\operatorname*{max}}]$ . The discount factor is denoted by $\\gamma\\in[0,1)$ . Let $\\vec{\\pi}\\,=\\,(\\vec{\\pi}_{1},\\vec{\\pi}_{2},\\ldots)$ denote any (history dependent and possibly randomized) policy. Then the action at time $t$ is given by $a_{t}\\sim\\vec{\\pi}_{t}\\big(y_{1:t},a_{1:t-1}\\big)$ . The performance of policy $\\vec{\\pi}$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ^{\\vec{\\pi}}:=\\mathbb{E}_{\\left.(s_{t}\\sim\\vec{\\pi}_{t}(y_{1:t},a_{t-1})\\right.}\\left[\\sum_{t=1}^{\\infty}\\gamma^{t-1}r(s_{t},a_{t})\\;\\right|\\,s_{1}\\sim\\rho\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The objective is to find a (history dependent and possibly randomized) policy $\\vec{\\pi}$ to maximize $J^{\\vec{\\pi}}$ . ", "page_idx": 2}, {"type": "text", "text": "Agent state for POMDPs. An agent-state is model-free recursively updateable function of the history of observations and actions. In particular, let Z denote agent-state space. Then, the agent state process $\\{z_{t}\\}_{t\\ge0}$ , $z_{t}\\,\\in\\,Z$ , starts with an initial value $z_{0}$ , and is then recursively computed as $z_{t+1}=\\phi(z_{t},y_{t+1},\\overline{{a}}_{t})$ for a pre-specified agent-state update function $\\phi$ . ", "page_idx": 3}, {"type": "text", "text": "We use ${\\boldsymbol\\pi}\\,=\\,(\\pi_{1},\\pi_{2},\\ldots)$ to denote an agent-state based policy,2 i.e., a policy where the action at time $t$ is given by $a_{t}\\sim\\bar{\\pi}_{t}(z_{t})$ . An agent-state based policy is said to be stationary if for all $t$ and $t^{\\prime}$ , we have $\\pi_{t}(a|z)\\,=\\,\\pi_{t^{\\prime}}(a|z)$ for all $(z,a)\\in\\mathsf{Z}\\times\\mathsf{A}$ . An agent-state based policy is said to be periodic with period $L$ if for all $t$ and $t^{\\prime}$ such that $t\\equiv t^{\\prime}$ (mod $L$ ), we have $\\pi_{t}(\\bar{a}|z)\\overset{\\cdot}{=}\\pi_{t^{\\prime}}(a|z)$ for all $(z,a)\\in\\mathsf{Z}\\stackrel{}{\\times}\\mathsf{A}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 PASQL: Periodic agent-state based Q-learning algorithm for POMDPs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present a periodic variant of agent-state based Q-learning, which we abbreviate as PASQL. PASQL is an online, off-policy learning algorithm in which the agent acts according to a behavior policy $\\mu=(\\mu_{1},\\mu_{2},\\dots)$ which is a periodic (stochastic) agent-state based policy $\\mu$ with period $L$ . Let $[\\![t]\\!]:=(t{\\bmod{L}})$ and $\\mathsf{L}:=\\{0,1,\\ldots,L-1\\}$ . Let $(z_{1},a_{1},R_{1},z_{2},a_{2},R_{2},\\ldots)$ be a sample path of a g e nt-state, action, and reward observed by the agent. In PASQL, the agent maintains an $L$ -tuple of Q-functions $(Q_{t}^{0},Q_{t}^{1},\\ldots,Q_{t}^{L-1})$ , $t\\geq1$ . The $\\ell$ -th component, $\\ell\\,\\in\\,{\\mathsf L}$ , is updated at time steps when $\\mathbb{[}t]=\\ell$ . In particular, we can write the update as $Q_{t+1}^{\\ell}(z,a)=Q_{t}^{\\ell}(z,a)+\\alpha_{t}^{\\ell}(z,a)\\Big[R_{t}+\\gamma\\operatorname*{max}_{a^{\\prime}\\in\\mathbb{A}}Q_{t}^{\\mathbb{I}\\ell+1]}(z_{t+1},a^{\\prime})-Q_{t}^{\\ell}(z,a)\\Big],\\quad\\forall\\ell\\in\\mathbb{L},\\;(\\ell,\\ell)\\in\\mathbb{R}.$ (PASQL) where the learning rate sequence $\\{(\\alpha_{t}^{0},\\dots,\\alpha_{t}^{L-1})\\}_{t\\geq1}$ is chosen such that $\\alpha_{t}^{\\ell}(z,a)~=~0$ if $(\\ell,z,a)\\neq(\\[t],z_{t},a_{t})$ and satisfies Assm. 1. PASQL differs from ASQL in two aspects: (i) The behavior poli cy $\\mu$ is periodic. (ii) The update of the $\\mathrm{{Q}}.$ -function is periodic. When $L=1$ , PASQL collapses to ASQL. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The standard convergence analysis of Q-learning for MDPs shows that the Q-function convergences to the unique solution of the MDP dynamic program (DP). The key challenge in characterizing the convergence of PASQL is that the agent state $\\{Z_{t}\\}_{t\\ge1}$ does not satisfy the Markov property. Therefore, a DP to find the best agent-state based policy does not exist. So, we cannot use the standard analysis to characterize the convergence of PASQL. In Sec. 2.3, we provide a complete characterization of the convergence of PASQL. ", "page_idx": 3}, {"type": "text", "text": "The quality of the converged solution depends on the expressiveness of the agent state. For example, if the agent state is not expressive (e.g., agent state is always constant), then even if PASQL converges to a limit, the limit will be far from optimal. Therefore, it is important to quantify the degree of sub-optimality of the converged limit. We do so in Sec. 2.4. ", "page_idx": 3}, {"type": "text", "text": "2.3 Establishing the convergence of tabular PASQL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To characterize the convergence of tabular PASQL, we impose two assumptions which are standard for analysis of RL algorithms [JSJ94; BT96]. The first assumption is on the learning rates. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 For all $(\\ell,z,a)$ , the learning rates $\\{\\alpha_{t}^{\\ell}(z,a)\\}_{t\\geq1}$ are measurable with respect to the sigma-algebra generated by $\\left(z_{1:t},a_{1:t}\\right)$ and satisfy $\\alpha_{t}^{\\ell}(z,a)=0$ if $(\\ell,z,a)\\neq(\\mathbb{I}t],z_{t},a_{t})$ . Moreover, $\\begin{array}{r}{\\sum_{t\\geq1}\\alpha_{t}^{\\ell}(z,a)=\\infty}\\end{array}$ and $\\begin{array}{r}{\\sum_{t\\geq1}(\\alpha_{t}^{\\ell}(z,a))^{2}<\\infty}\\end{array}$ , almost surely. ", "page_idx": 3}, {"type": "text", "text": "The second assumption is on the behavior policy $\\mu$ . We first state an immediate property. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 For any behavior policy $\\mu_{;}$ , the process $\\{(S_{t},Z_{t})\\}_{t\\geq1}$ is Markov. Therefore, the processes $\\{(S_{t},Z_{t},A_{t})\\}_{t\\geq1}$ and $\\{(S_{t},Y_{t},Z_{t},A_{t})\\}_{t\\geq1}$ are also Markov. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 The behavior policy $\\mu$ is such that the Markov chain $\\{(S_{t},Y_{t},Z_{t},A_{t})\\}_{t\\geq1}$ is timeperiodic3 with period $L$ and converges to a cyclic limiting distribution $(\\zeta_{\\mu}^{0},\\ldots,\\zeta_{\\mu}^{L-1})$ , where $\\begin{array}{r}{\\sum_{(s,y)}\\zeta_{\\mu}^{\\ell}(s,y,z,a)>0}\\end{array}$ for all $(\\ell,z,a)$ (i.e., all $(\\ell,z,a)$ are visited infinitely often). ", "page_idx": 3}, {"type": "text", "text": "For the ease of notation, we will continue to use $\\zeta_{\\mu}^{\\ell}$ to denote the marginal and conditional distributions w.r.t. $\\zeta_{\\mu}^{\\ell}$ . In particular, for marginals we use $\\zeta_{\\mu}^{\\ell}(y,z,a)$ to denote $\\begin{array}{r}{\\sum_{s\\in\\mathsf{S}}\\zeta_{\\mu}^{\\ell}(s,y,z,a)}\\end{array}$ and so on; for conditionals, we use $\\zeta_{\\mu}^{\\ell}(s|z,a)$ to denote $\\zeta_{\\mu}^{\\bar{\\ell}}(s,z,a)/\\zeta_{\\mu}^{\\ell}(z,a)$ and so on. Note that $\\zeta_{\\mu}^{\\ell}(s,z,y,a)=\\zeta_{\\mu}^{\\ell}(s,z)\\mu(a\\mid z)P(y|\\dot{s},a)$ . Thus, we have that $\\zeta_{\\mu}^{\\ell}(s\\mid z,a)=\\zeta_{\\mu}^{\\ell}(s\\mid z)$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 Under Assms. 1 and 2, the process $\\{(Q_{t}^{0},\\ldots,Q_{t}^{L-1})\\}_{t\\geq1}$ converges to a limit $(Q_{\\mu}^{0},\\ldots,Q_{\\mu}^{L-1})$ a.s., where the limit is the unique fixed point of the $D P$ for a periodic MDP:4 ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{\\mu}^{\\ell}(z,a)=r_{\\mu}^{\\ell}(z,a)+\\gamma\\sum_{z^{\\prime}\\in\\mathsf{Z}}P_{\\mu}^{\\ell}(z^{\\prime}|z,a)\\operatorname*{max}_{a^{\\prime}\\in\\mathsf{A}}Q_{\\mu}^{[\\ell+1]}(z^{\\prime},a^{\\prime}),\\quad\\forall\\ell\\in\\mathsf{L},\\forall(z,a)\\in\\mathsf{Z}\\times\\mathsf{A}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the periodic rewards $(r_{\\mu}^{0},\\ldots,r_{\\mu}^{L-1})$ and dynamics $(P_{\\mu}^{0},\\dots,P_{\\mu}^{L-1})$ are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{\\mu}^{\\ell}(z,a):=\\sum_{s\\in\\mathbb{S}}r(s,a)\\zeta_{\\mu}^{\\ell}(s\\mid z),\\qquad P_{\\mu}^{\\ell}(z^{\\prime}|z,a):=\\sum_{(s,y^{\\prime})\\in\\mathbb{S}\\times\\mathbb{Y}}\\mathbb{I}_{\\{z^{\\prime}=\\phi(z,y^{\\prime},a)\\}}P(y^{\\prime}|s,a)\\zeta_{\\mu}^{\\ell}(s|z).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "See App. E for proof. Some salient features of the result are as follows: ", "page_idx": 4}, {"type": "text", "text": "\u2022 In contrast to Q-learning for MDPs, the limiting value $Q_{\\mu}^{\\ell}$ depends on the behavioral policy $\\mu$ . This dependence arises because the agent state $Z_{t}$ is not an information state and thus is not policy-independent. See [Wit75] for a discussion on policy independence of information states. \u2022 We can recover some existing results in the literature as special cases of Thm. 1. If we take $L=1$ , Thm. 1 recovers the convergence result for ASQL obtained in [Sey $+23$ , Thm. 2]. In addition, if the agent state is a sliding window memory, Thm. 1 recovers the convergence result obtained in [KY22, Thm. 4.1]. Note that the results of Thm. 1 for these special cases is more general because the previous results were derived under a restrictive assumption on the learning rates. ", "page_idx": 4}, {"type": "text", "text": "The policy learned by PASQL is the periodic policy $\\pi_{\\mu}=(\\pi_{\\mu}^{0},...\\,,\\pi_{\\mu}^{L-1})$ given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\mu}^{\\ell}(z)=\\arg\\operatorname*{max}_{a\\in\\mathsf{A}}Q_{\\mu}^{\\ell}(z,a),\\quad\\forall\\ell\\in\\mathsf{L},z\\in\\mathsf{Z}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since PASQL learns a periodic policy, it circumvents the limitation of ASQL described in the introduction. Thm. 1 addresses the main challenge in the convergence analysis of PASQL: the nonMarkovian dynamics of $\\{Z_{t}\\}_{t\\ge1}$ . A natural follow-up question is: How good is the learnt policy (PASQL-policy) compared to the optimal? We address this in the next section. ", "page_idx": 4}, {"type": "text", "text": "2.4 Characterizing the optimality-gap of the converged limit ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "History-dependent policies and their value functions. Let $h_{t}=(y_{1:t},a_{1:t-1})$ denote the history of observations and actions until time $t$ . and let $\\sigma_{t}\\colon h_{t}\\mapsto\\ z_{t}$ denotes the map from histories to agent-states obtained by unrolling the memory update function $\\phi$ , i.e., $\\sigma_{1}(h_{1}\\bar{)}\\;=\\;\\phi(z_{0},y_{1},a_{0})$ , where $z_{0}$ is the initial agent state, $a_{0}$ is a dummy action used to initialize the process, $\\sigma_{2}(h_{2})\\;=\\;$ $\\phi(\\sigma_{1}(h_{1}),y_{2},a_{1})$ , etc. ", "page_idx": 4}, {"type": "text", "text": "For any history dependent policy $\\vec{\\pi}\\;\\;=\\;\\;(\\vec{\\pi}_{1},\\vec{\\pi}_{2},\\cdot\\cdot\\cdot)$ , where $\\vec{\\pi}_{t}\\colon h_{t}\\ \\ \\mapsto\\ a_{t}$ , let $V_{t}^{\\vec{\\pi}}(h_{t})\\;\\;:=$ $\\begin{array}{r}{\\mathbb{E}^{\\vec{\\pi}}\\left[\\sum_{\\tau=t}^{\\infty}\\gamma^{\\tau}R_{\\tau}~\\right|~h_{t}\\right]}\\end{array}$ denote the value function of policy $\\vec{\\pi}$ starting from history $h_{t}$ at time $t$ . Let $\\begin{array}{r}{V_{t}^{\\star}(h_{t})\\,:=\\,\\operatorname*{sup}_{\\vec{\\pi}}V_{t}^{\\vec{\\pi}}(h_{t})}\\end{array}$ denote the optimal value function, where the supermum is over all history dependent policies. In Thm. 1, we have shown that PASQL converges to a limit. Let $\\vec{\\pi}_{\\mu}\\,=\\,(\\vec{\\pi}_{\\mu,1},\\vec{\\pi}_{\\mu,2},\\bar{\\,.\\,.\\,})$ denote the history dependent policy corresponding to the periodic policy $(\\pi_{\\mu}^{0},\\dots,\\pi_{\\mu}^{L-1})$ given by (PASQL-policy), i.e., $\\vec{\\pi}_{\\mu,t}(h_{t}):=\\pi^{[t]}(\\sigma_{t}(h_{t}))$ , In this section, we present a bound on the sub-optimality gap $V_{t}^{\\star}(h_{t})-V_{t}^{\\vec{\\pi}_{\\mu}}(h_{t})$ . ", "page_idx": 4}, {"type": "text", "text": "Integral probability metrics. Let $\\mathfrak{F}$ be a convex and balanced5 subset of (measureable) real-valued functions on S. The integral probability metric (IPM) w.r.t. $\\mathfrak{F}$ , denoted by $d_{\\mathfrak{F}}$ , is defined as follows: any probability measures $\\xi_{1}$ and $\\xi_{2}$ on S, we have $\\begin{array}{r}{d_{\\mathfrak{F}}(\\xi_{1},\\xi_{2}):=\\operatorname*{sup}_{f\\in\\mathfrak{F}}\\!\\left|\\int f d\\xi_{1}-\\int f d\\xi_{2}\\right|}\\end{array}$ . Moreover, for any real-valued function $f$ on S, define $\\rho_{\\mathfrak{F}}:=\\operatorname*{inf}\\{\\rho>0\\colon f/\\rho\\in\\mathfrak{F}\\}$ to be the Minkowski functional w.r.t. $\\mathfrak{F}$ . Note that if for every positive $\\rho$ , $,f/\\rho\\not\\in{\\mathfrak{F}}$ , then $\\rho_{\\mathfrak{F}}(f)=\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Many commonly used metrics on probability spaces are IPMs. For example, (i) Total variation distance for which $\\bar{\\mathfrak{F}}=\\{\\mathrm{span}(f)\\le\\bar{1}\\}$ , where $\\operatorname{span}(f)=\\operatorname*{max}f-\\operatorname*{min}f$ is the span seminorm of $f$ . In this case, $\\rho_{\\mathfrak{F}}(f)\\,=\\,\\operatorname{span}(f)$ . (ii) Wasserstein distance for which $\\mathfrak{F}\\,=\\,\\{\\mathrm{Lip}(f)\\,\\le\\,1\\}$ , where $\\operatorname{Lip}(f)$ is the Lipschitz constant of $f$ . In this case, $\\rho_{\\mathfrak{F}}(f)=\\mathrm{Lip}(f)$ . Other examples include Kantorovich metric, bounded Lipschitz metric, and maximum mean discrepancy. See [Mu\u00a8l97; $\\operatorname{Sub}\\!+\\!22]$ for more details. ", "page_idx": 5}, {"type": "text", "text": "Sub-optimality gap. Let ${\\mathsf{T}}(t,\\ell):=\\{\\tau\\geq t:\\mathbb{I}\\tau\\mathbb{J}=\\ell\\}$ . Furthermore, for any $\\ell\\in{\\mathsf{L}}$ and $t$ , define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{t}^{\\ell}:=\\underset{\\tau\\in\\mathsf{T}(t,\\ell)}{\\operatorname*{sup}}\\ \\underset{h_{\\tau},a_{\\tau}}{\\operatorname*{sup}}\\Big|\\mathbb{E}[R_{\\tau}\\mid h_{\\tau},a_{\\tau}]-\\underset{s\\in\\mathsf{S}}{\\sum}r(s,a_{\\tau})\\zeta_{\\mu}^{\\ell}(s\\mid\\sigma_{\\tau}(h_{\\tau}),a_{\\tau})\\Big|,}\\\\ &{\\delta_{t}^{\\ell}:=\\underset{\\tau\\in\\mathsf{T}(t,\\ell)}{\\operatorname*{sup}}\\ \\underset{h_{\\tau},a_{\\tau}}{\\operatorname*{sup}}d_{\\mathfrak{F}}(\\mathbb{P}(Z_{\\tau+1}=\\cdot\\mid h_{\\tau},a_{\\tau}),P_{\\mu}^{\\ell}(Z_{\\tau+1}=\\cdot\\vert\\sigma_{\\tau}(h_{\\tau}),a_{\\tau})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, we have the following sub-optimality gap for $\\vec{\\pi}_{\\mu}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 Let $V_{\\mu}^{\\ell}(z):=\\operatorname*{max}_{a\\in\\mathsf{A}}Q_{\\mu}^{\\ell}(z,a)$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h_{t}}\\bigl[V_{t}^{\\star}(h_{t})-V_{t}^{\\pi_{\\mu}}(h_{t})\\bigr]\\leq\\frac{2}{(1-\\gamma^{L})}\\sum_{\\ell\\in\\mathsf{L}}\\gamma^{\\ell}\\Bigl[\\varepsilon_{t+\\ell}^{\\left[t+\\ell\\right]}+\\gamma\\delta_{t+\\ell}^{\\left[t+\\ell\\right]}\\rho_{\\mathfrak{F}}(V_{\\mu}^{\\left[t+\\ell+1\\right]})\\Bigr].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "See App. F for proof. The salient features of the sub-optimality gap of Thm. 2 are as follows. ", "page_idx": 5}, {"type": "text", "text": "\u2022 We can recover some existing results as special cases of Thm. 2. When we take $L=1$ , Thm. 2 recovers the sub-optimality gap for ASQL obtained in [Sey $+23$ , Thm. 3]. In addition, when the agent state is a sliding window memory, Thm. 2 is similar to the sub-optimality gap obtained in [KY22, Thm. 4.1]. Note that the results of Thm. 2 for these special cases is more general because the previous results were derived under a restrictive assumption on the learning rates.   \n\u2022 The sub-optimality gap in Thm. 2 is on the sub-optimality w.r.t. the optimal history-dependent policy rather than the optimal non-stationary agent-state policy. Thus, it inherently depends on the quality of the agent state. Consequently, even if $L\\rightarrow\\infty$ , the sub-optimality gap does not go to zero.   \n\u2022 It is not easy to characterize the sensitivity of the bound to the period $L$ . In particular, increasing $L$ means changing behavioral policy $\\mu$ , and therefore changing the converged limit $(\\zeta_{\\mu}^{0},\\ldots,\\zeta_{\\mu}^{L-1})$ , which impacts the right hand side of (3) in a complicated way. So, it is not necessarily the case that increasing $L$ reduces the sub-optimality gap. This is not surprising, as we have seen earlier in Ex. 2 presented in the introduction that even the performance of periodic agent-state based policies is not monotone in $L$ . ", "page_idx": 5}, {"type": "text", "text": "3 Numerical experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present a numerical example to highlight the salient features of our results. We use the following POMDP model. ", "page_idx": 5}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/505542f6e7bf382ebccea9171151103a40795105e465f3cdfa42480cde74cc20.jpg", "img_caption": ["(a) Dynamics under action 0. (b) Dynamics under action 1. Figure 2: The model for Ex. 1, where states which have the same color give the same observation; the green edges give a reward of $+1$ and blue edges give a reward of $+0.5$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Example 1 Consider a POMDP with $\\mathsf{S}\\,=\\,\\{0,1,\\hdots,5\\}$ , $\\mathsf{A}\\,=\\,\\{0,1\\}$ , ${\\textsf{Y}}=$ $\\{0,1\\}$ and $\\gamma=0.9$ . The dynamics are as shown in Fig. 2. The observation is 0 in states $\\{0,1,2\\}$ which are shaded white and is 1 in states $\\{3,4,5\\}$ which ", "page_idx": 5}, {"type": "text", "text": "are shaded gray. The transitions shown in green give a reward of $+1$ ; those in in blue give a reward of $+0.5$ ; others give no reward. ", "page_idx": 5}, {"type": "text", "text": "We consider a family of models, denoted by $\\mathcal{M}(p)$ , $p\\in[0,1]$ , which are similar to Ex. 1 except the controlled state transition matrix is $p I+(1-p)P$ , where $P$ is the controlled state transition matrix of Ex. 1 shown in Fig. 2. In the results reported below, we use $p=0.01$ . The hyperparameters for the experiments are provided in App. H. ", "page_idx": 5}, {"type": "text", "text": "Convergence of PASQL with $L=2$ . We assume that the agent state $Z_{t}=Y_{t}$ and take period $L=$ 2. We consider three behavioral policies: $\\mu_{k}=(\\mu_{k}^{0},\\mu_{k}^{1})$ , $k\\in\\mathsf{K}:=\\{1,2,3\\}$ , where $\\mu_{k}^{\\ell}\\colon\\{0,1\\}\\to$ $\\Delta(\\{0,1\\})$ , $\\ell\\in\\{0,1\\}$ . The policy $\\mu_{k}$ is completely characterized by four numbers which we write in matrix form as: $[\\bar{\\mu}_{k}^{0}(0|0),\\mu_{k}^{1}(\\bar{0}|0);\\mu_{k}^{0}(0|\\bar{1}),\\mu_{k}^{1}(0|1)]$ . With this notation, the three policies are given by $\\mu_{1}:=[0.2,0.8;0.8,0.\\dot{2}]$ , $\\mu_{2}:=[0.5,0.5;0.5,0.5$ ] , $\\mu_{3}:=[0.8,0.2;0.2,0.8]$ . ", "page_idx": 5}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/23e3cb5f454e3c76338e4058e2c36120ed20e1773f6e0935ef3a15d1f023ed87.jpg", "img_caption": ["Figure 3: PASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For each behavioral policy $\\mu_{k}$ , $k\\in\\mathsf{K}$ , run PASQL for 25 random seeds. The median $^+$ interquantile range of the iterates $\\{Q_{t}^{\\bar{\\ell}}(z,a)\\}_{t\\geq1}$ as well as the theoretical limits $Q_{\\mu_{k}}(z,a)$ (computed using Thm. 1) are shown in Fig. 3. The salient features of these results are as follows: ", "page_idx": 6}, {"type": "text", "text": "\u2022 PASQL converges close to the theoretical limit predicted by Thm. 1.   \n\u2022 As highlighted earlier, the limiting value $Q_{\\mu_{k}}^{\\ell}$ depends on the behavioral policy $\\mu_{k}$ .   \n\u2022 When the aperiodic behavior policy $\\mu_{2}$ is used, the Markov chain $\\{(S_{t},Y_{t},Z_{t},A_{t})\\}_{t\\geq1}$ is aperiodic, and therefore the limiting distribution $\\zeta_{\\mu_{2}}^{\\ell}$ and the corresponding Q-functions $Q_{\\mu_{2}}^{\\ell}$ do not depend on $\\ell$ . This highlights the fact that we have to choose a periodic behavioral policy to converge to a non-stationary policy (PASQL-policy). ", "page_idx": 6}, {"type": "text", "text": "Comparison of converged policies. Finally, we compute the periodic greedy policy $\\bar{\\pi_{\\mu_{k}}}=\\bar{(}\\pi_{\\mu_{k}}^{0},\\pi_{\\mu_{k}}^{1})$ g\u03c0iven by (PASQL-policy), , and compute its performance via policy evaluation on the product space $S\\times Z$ (see App. G). We also do a brute force search over all $L=2$ periodic deterministic agent-state policies to compute the optimal performance $J_{2}^{\\star}$ over all such policies. The results, displayed in Table 1, illustrate the following: ", "page_idx": 6}, {"type": "table", "img_path": "HmMSBhMAw4/tmp/ec42b4a3da197f518d4c401cca7edea822c33e27aeb830ec1778cfa5db732bd6.jpg", "table_caption": ["Table 1: Performance of converged periodic policies. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "\u2022 The greedy policy $\\pi_{\\mu_{k}}$ depends on the behavioral policy. This is not surprising given the fact that \u2022 tThhee l ipmoiltiicnyg uaec $Q_{\\mu_{k}}^{\\ell}$ eds etphee nodpst iomn $\\mu_{k}$ .erformance, whereas the policies and do not per$\\pi_{\\mu_{1}}$ $\\pi_{\\mu_{2}}$ $\\pi_{\\mu_{3}}$ form well. This highlights the importance of starting with a good behavioral policy. See Sec. 5 for a discussion on variants such as $\\epsilon$ -greedy. ", "page_idx": 6}, {"type": "text", "text": "Advantage of learning periodic policies. As stated in the introduction, the main motivation of PASQL is that it allows us to learn non-stationary policies. To see why this is useful, we run ASQL (which is effectively PASQL with $L=1$ ). We again consider three behavioral policies: $\\bar{\\mu}_{k}$ , $k\\,\\in\\,\\mathsf{K}\\,:=\\,\\{1,2,3\\}$ , where $\\bar{\\mu}_{k}\\colon\\{0,1\\}\\ \\rightarrow\\ \\Delta(\\{0,\\dot{1}\\})$ , where (using similar notation as for $L=2$ case) $\\bar{\\mu}_{1}:=\\left[0.2;0.8\\right],\\bar{\\mu}_{2}:=\\left[0.5;0.5\\right],\\bar{\\mu}_{3}:=\\left[0.8;0.2\\right]$ . ", "page_idx": 6}, {"type": "table", "img_path": "HmMSBhMAw4/tmp/66fb0e1c4e10b6cb7bfab3edd05cb4dd78c6a9b7157c16918a2a69fe77a0148f.jpg", "table_caption": ["Table 2: Performance of converged stationary policies. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "For each behavioral policy $\\bar{\\mu}_{k}$ , $k\\ \\in\\ \\mathsf{K}$ , run ASQL for 25 random seeds. The results are shown in App. A.1. The performance of the greedy policies $\\pi_{\\bar{\\mu}_{k}}$ and the performance of the best period ", "page_idx": 6}, {"type": "text", "text": "$L=1$ deterministic agent-state-based policy computed via brute force is shown in Table 2. The key implications are as follows: ", "page_idx": 7}, {"type": "text", "text": "\u2022 As was the case for PASQL, the greedy policy $\\pi_{\\bar{\\mu}_{k}}$ depends on the behavioral policy. As mentioned earlier, this is a fundamental consequence of the fact that the agent state is not an information state. Adding (or removing) periodicity does not change this feature. \u2022 The best performance of ASQL is worse than the best performance of PASQL. This highlights the potential benefits of using periodicity. However, at the same time, if a bad behavioral policy is chosen (e.g., policy $\\mu_{3}]$ ), the performance of PASQL can be worse than that of ASQL for a nominal policy (e.g., policy $\\bar{\\mu}_{2}$ ). This highlights that periodicity is not a magic bullet and some care is needed to choose a good behavioral policy. Understanding what makes a good periodic behavioral policy is an unexplored area that needs investigation. ", "page_idx": 7}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Policy search for agent state policies. There is a rich literature on planning with agent state-based policies that build on the policy evaluation formula presented in App. G. See [KWW22] for review. These approaches rely on the system model and cannot be used in the RL setting. ", "page_idx": 7}, {"type": "text", "text": "State abstractions for POMDPs are related to agent-state based policies. Some frameworks for state abstractions in POMDPs include predictive state representations (PSR) [RGT04; BSG11; HFP14; KJS15b; KJS15a; JKS16], approximate bisimulation [CPP09; $\\mathrm{Cas}{+21}$ , and approximate information states (AIS) $[\\mathrm{Sub}+22]$ (which is used in our proof of Thm. 2). Although there are various RL algorithms based on such state abstractions, the key difference is that all these frameworks focus on stationary policies in the infinite horizon setting. Our key insight that non-stationary/periodic policies improve performance is also applicable to these frameworks. ", "page_idx": 7}, {"type": "text", "text": "ASQL for POMDPs. As stated earlier, ASQL may be viewed as the special case of PASQL when $L=1$ . The convergence of the simplest version of ASQL was established in [SJJ94] for $Z_{t}=Y_{t}$ under the assumption that the actions are chosen i.i.d. (and do not depend on $z_{t}$ ). In [PP02] it was established that $Q_{\\mu}^{0}$ is the fixed point of (ASQL), but convergence of $\\{Q_{t}\\}_{t\\ge1}$ to $Q_{\\mu}^{0}$ was not established. The convergence of ASQL when the agent state is a finite window memory was established in [KY22]. These results were generalized to general agent-state models in $[\\mathrm{Sey}+23]$ . The regret of an optimistic variant of ASQL was presented in [DVZ22]. However, all of these papers focus on stationary policies. ", "page_idx": 7}, {"type": "text", "text": "Our analysis is similar to the analysis of [KY22; Sey $^{+23}$ ] with two key differences. First, their convergence results were derived under the assumption that the learning rates are the reciprocal of visitation counts. We relax this assumption to the standard learning rate conditions of Assm. 1 using ideas from stochastic approximation. Second, their analysis is restricted to stationary policies. We generalize the analysis to periodic policies using ideas from time-periodic Markov chains. ", "page_idx": 7}, {"type": "text", "text": "Q-learning for non-Markovian environments. As highlighted earlier, a key challenge in understanding the convergence of PASQL is that the agent-state is not Markovian. The same conceptual difficulty arises in the analysis of Q-learning for non-Markovian environments $\\mathrm{{[MH+18}}$ ; Cha $+24$ ; DY24]. Consequently, our analysis has stylistic similarities with the analysis in $\\mathrm{[MH+18}$ ; ${\\mathrm{Cha}}{+}24$ ; DY24] but the technical assumptions and the modeling details are different. And more importantly, they restrict attention to stationary policies. Given our results, it may be worthwhile to explore if periodic policies can help in non-Markovian environments as well. ", "page_idx": 7}, {"type": "text", "text": "Continual learning and non-stationary MDPs. Non-stationarity is an important consideration in continual learning (see [Abe+24] and references therein). However, in these settings, the environment is non-stationary. Our setting is different: the environment is stationary, but non-stationary policies help because the agent state is not Markov. ", "page_idx": 7}, {"type": "text", "text": "Hierarchical learning. The options framework [Pre00; SPS99; Die00; BHP17] is a hierarchical approach that learns temporal abstractions in MDPs and POMDPs. Due to temporal abstraction, the policy learned by the options framework is non-stationary. The same is true for other hierarchical learning approaches proposed in [WS97; CSL21; Vez+17]. In principle, PASQL could be considered as a form of temporal abstraction where time is split into trajectories of length $L$ and then a policy of length $L$ is learned. However, the theoretical analysis for options is mostly restricted to MDP setting ", "page_idx": 7}, {"type": "text", "text": "and the convergence guarantees for options in POMDPs are weaker [Ste+18; Qia+18; LVC18].   \nNonetheless, the algorithmic tools developed for options might be useful for PASQL as well. ", "page_idx": 8}, {"type": "text", "text": "Double Q-learning. The update equation of PASQL are structurally similar to the update equations used in double Q-learning [Has10; VGS16]. However, the motivation and settings are different: the motivation for Double Q-learning is to reduce overestimation bias in off-policy learning in MDPs, while the motivation for PASQL is to induce non-stationarity while learning in POMDPs. Therefore, the analysis of the two algorithms is very different. More importantly, the end goals differ: double Q-learning learns a stationary policy while PASQL learns a periodic policy. ", "page_idx": 8}, {"type": "text", "text": "Use of non-stationary/periodic policies in MDPs is investigated in [SL12; LS15; Ber13] in the context of approximate dynamic programming (ADP). Their main result was to show that using non-stationary or periodic policies can improve the approximation error in ADP. Although these results use periodic policies, the setting of ADP in MDPs is very different from ours. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Deterministic vs. stochastic policies. In this work, we restricted attention to periodic deterministic policies. In principle, we could have also considered periodic stochastic policies. For stationary policies (i.e., when period is one), stochastic policies can outperform deterministic policies [SJJ94] as illustrated by Ex. 3 in App. A.3. However, we do not consider stochastic policies in this work because we are interested in understanding Q-learning with agent-state and Q-learning results in a deterministic policy. There are two options to obtain stochastic policies: using regularization [GSP19], which changes the objective function; or using policy gradient algorithms [Sut+99; BB01], which are a different class of algorithms than Q-learning. ", "page_idx": 8}, {"type": "text", "text": "However, as illustrated in the motivating Ex. 2 presented in the introduction, non-stationary policies can do better than stationary stochastic policies as well. So, adding non-stationarity via periodicity remains an interesting research direction when learning stochastic policies as well. ", "page_idx": 8}, {"type": "text", "text": "PASQL is a special case of ASQL with state augmentation. In principle, PASQL could be considered as a special case of ASQL with an augmented agent state $\\bar{Z_{t}}^{\\bullet}=(\\bar{Z_{t}},[t])$ . However, the convergence analysis of ASQL in [KY22; Sey $+23$ ] does not imply the converge  nc e of PASQL because the results of [KY22; Sey+23] are derived under the assumption that Markov chain $\\{(S_{t},Y_{t},Z_{t},A_{t})\\}_{t\\geq1}$ is irreducible and aperiodic, while we assume that the Markov chain is periodic. Due to our weaker assumption, we are able to establish convergence of PASQL to time-varying periodic policies. ", "page_idx": 8}, {"type": "text", "text": "Non-stationary policies vs. memory augmentation. Nonstationarity is a fundamentally different concept than memory augmentation. As an illustration, consider the T-shaped grid world (first considered in [Bak01]) shown in Fig. 4, which has a corridor of length $2n$ . In App. A.4, we show that for this example, a stationary policy which uses a sliding window of past $m$ observations and actions as the agent state needs a memory of at least $m\\,>\\,2n$ to reach the goal state. In contrast, a periodic policy with period $L=3$ can reach the goal state for every $n$ . This example shows that periodicity is a different concept from memory augmentation and highlights the fact that mechanisms other than memory augmentation can achieve optimal behavior. ", "page_idx": 8}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/2c4144016f47d15940c13c30f843409787f026d955f2c1e8970460c4a070fcba.jpg", "img_caption": ["Figure 4: A T-shaped grid world. Agent starts at S, where it learns whether the goal state is $\\mathrm{G_{1}}$ or $\\mathrm{G_{2}}$ . It has to go through the corridor $\\{1,\\ldots,2n\\}$ , without knowing where it is, reach T and go up or down to reach the goal state. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The analysis of this paper is applicable to general memory augmented policies, so we do not need to choose between memory augmentation and periodicity. Our main message is that once the agent\u2019s memory is fixed based on practical considerations, adding periodicity could improve performance. ", "page_idx": 8}, {"type": "text", "text": "Choice of the period $L$ . If the agent state $Z_{t}$ is a good approximation to the belief state, then ASQL (or, equivalently, PASQL with $L=1$ ) would converge to an approximately optimal policy. So, using PASQL a period $L>1$ is useful when the agent state is not a good approximation of the belief state. ", "page_idx": 8}, {"type": "text", "text": "As shown by Ex. 2 in the introduction, the performance of the best periodic policy does not increase monotonically with the period $L$ . However, if we consider periods in the set $\\{{\\dot{n}}^{\\parallel}:n\\in\\mathbb{N}\\}$ , then the performance increases monotonically. However, PASQL does not necessarily converge to the best periodic policy. The quality of the converged policy (PASQL-policy) depends on the behavior policy $\\mu$ . The difficulty of finding a good behavioral policy increases with $L$ . In addition, increasing the period increases the memory required to store the tuple $(Q^{0},\\ldots,Q^{L})$ and the number of samples needed to converge (because each component is updated only once every $L$ samples). Therefore, the choice of the period $L$ should be treated as a hyperparameter that needs to be tuned. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Choice of the behavioral policy. The behavioral policy impacts the converged limit of PASQL, and consequently it impacts the periodic greedy policy that is learned. As we pointed out in the discussion after Thm. 1, this dependence is a fundamental consequence of using an agent state that is not Markov and cannot be avoided. Therefore, it is important to understand how to choose behavioral policies that lead to convergence to good policies. ", "page_idx": 9}, {"type": "text", "text": "Generalization to other variants. Our analysis is restricted to tabular off-policy Q-learning where a fixed behavioral policy is followed. Our proof fundamentally depends on the fact that the behavioral policy induces a cyclic limiting distribution on the periodic Markov chain $\\{(S_{t},Y_{t},Z_{t},A_{t})\\}_{t\\geq1}$ . Such a condition is not satisfied in variants such as $\\epsilon$ -greedy Q-learning and SARSA. Generalizing the technical proof to cover these more practical algorithms (including function approximation) is an important future direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of AS and AM was supported in part by a grant from Google\u2019s Institutional Research Program in collaboration with Mila. The numerical experiments were enabled in part by support provided by Calcul Que\u00b4bec and Compute Canada. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[Abe $+24$ ] David Abel, Andre\u00b4 Barreto, Benjamin Van Roy, Doina Precup, Hado P van Hasselt, and Satinder Singh. \u201cA definition of continual reinforcement learning\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[ \u02daAst65] K.J \u02daAstro\u00a8m. \u201cOptimal Control of Markov Processes with Incomplete State Information\u201d. In: Journal of Mathematical Analysis and Applications 10.1 (Feb. 1965), pp. 174\u2013205. ISSN: 0022247X.   \n[BHP17] Pierre-Luc Bacon, Jean Harb, and Doina Precup. \u201cThe option-critic architecture\u201d. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 31. 1. 2017.   \n[Bak01] Bram Bakker. \u201cReinforcement Learning with Long Short-Term Memory\u201d. In: Advances in Neural Information Processing Systems. Ed. by T. Dietterich, S. Becker, and Z. Ghahramani. Vol. 14. MIT Press, 2001. URL: https://proceedings.neurips. cc/paper_files/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5- Paper.pdf.   \n[BB01] Jonathan Baxter and Peter L Bartlett. \u201cInfinite-horizon policy-gradient estimation\u201d. In: Journal of Artificial Intelligence Research 15 (2001), pp. 319\u2013350.   \n[BMP12] Albert Benveniste, Michel Me\u00b4tivier, and Pierre Priouret. Adaptive algorithms and stochastic approximations. Vol. 22. Springer Science & Business Media, 2012.   \n[BT96] Dimitri Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientific, 1996.   \n[Ber13] Dimitri P Bertsekas. Abstract Dynamic Programming. Athena Scientific, 2013.   \n[BP24] Shalabh Bhatnagar and L.A. Prashanth. Personal communication. 2024.   \n[BSG11] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. \u201cClosing the learning-planning loop with predictive state representations\u201d. In: The International Journal of Robotics Research 30.7 (2011), pp. 954\u2013966.   \n[Bor08] Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint. Hindustan Book Agency, 2008.   \n[CLZ97] Anthony Cassandra, Michael L. Littman, and Nevin L. Zhang. \u201cIncremental pruning: A simple, fast, exact method for partially observable Markov decision processes\u201d. In: Uncertainty in Artificial Intelligence. 1997.   \n[CKL94] Anthony R Cassandra, Leslie Pack Kaelbling, and Michael L Littman. \u201cActing optimally in partially observable stochastic domains\u201d. In: AAAI Conference on Artificial Intelligence. Vol. 94. 1994, pp. 1023\u20131028.   \n[Cas98] Anthony Rocco Cassandra. \u201cExact and approximate algorithms for partially observable Markov decision processes\u201d. PhD thesis. Brown University, 1998.   \n$[\\mathrm{Cas}+21]$ Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. \u201cMico: Improved representations via sampling-based state similarity for Markov decision processes\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 30113\u2013 30126.   \n[CPP09] Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. \u201cEquivalence Relations in Fully and Partially Observable Markov Decision Processes\u201d. In: International Joint Conference on Artificial Intelligence. 2009, pp. 1653\u20131658.   \n[Cha+24] Siddharth Chandak, Pratik Shah, Vivek S Borkar, and Parth Dodhia. \u201cReinforcement learning in non-Markovian environments\u201d. In: Systems & Control Letters 185 (2024), p. 105751.   \n[CSL21] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. \u201cGoal-conditioned reinforcement learning with imagined subgoals\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 1430\u20131440.   \n[Cha07] Joseph Chang. Stochastic Processes. Unpublished. Availale at http://www.stat. yale.edu/\\~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf. 2007.   \n[DY24] Ali Devran Kera and Serdar Yu\u00a8ksel. \u201cQ-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments\u201d. In: Transactions on Machine Learning Research (2024).   \n[Die00] Thomas G Dietterich. \u201cHierarchical reinforcement learning with the MAXQ value function decomposition\u201d. In: Journal of Artificial Intelligence Research 13 (2000), pp. 227\u2013303.   \n[DRZ22] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. \u201cSimple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States\u201d. In: Journal of Machine Learning Research 23.255 (2022), pp. 1\u201354.   \n[DVZ22] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. \u201cSimple agent, complex environment: Efficient reinforcement learning with agent states\u201d. In: Journal of Machine Learning Research 23.255 (2022), pp. 1\u201354.   \n[Dur19] Rick Durrett. Probability: Theory and Examples. Cambridge University Press, Apr. 2019. ISBN: 9781108473682. DOI: 10.1017/9781108591034.   \n[GSP19] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. \u201cA theory of regularized markov decision processes\u201d. In: International Conference on Machine Learning. PMLR. 2019, pp. 2160\u20132169.   \n[Gru+18] Audrunas Gruslys, Re\u00b4mi Munos, Ivo Danihelka, Marc G. Bellemare, and Alex Graves. \u201cThe Reactor: A Sample-Efficient Actor-Critic Architecture\u201d. In: Proceedings of the International Conference on Learning Representations (ICLR). 2018. URL: https: //arxiv.org/abs/1704.04651.   \n[Haf+20] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behaviors by Latent Imagination\u201d. In: International Conference on Learning Representations. 2020.   \n[Haf+21] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. \u201cMastering Atari with Discrete World Models\u201d. In: International Conference on Learning Representations. 2021.   \n[HFP14] William Hamilton, Mahdi Milani Fard, and Joelle Pineau. \u201cEfficient learning and planning with compressed predictive states\u201d. In: The Journal of Machine Learning Research 15.1 (2014), pp. 3395\u20133439.   \n[Han98] Eric A. Hansen. \u201cSolving POMDPs by searching in policy space\u201d. In: Uncertainty in Artificial Intelligence. Madison, Wisconsin, 1998, pp. 211\u2013219. ISBN: 155860555X.   \n[Has10] Hado Hasselt. \u201cDouble Q-learning\u201d. In: Advances in neural information processing systems 23 (2010).   \n[HS15] Matthew Hausknecht and Peter Stone. \u201cDeep recurrent Q-learning for partially observable MDPs\u201d. In: AAAI Fall Symposium Series. 2015.   \n[Hau97] Milos Hauskrecht. \u201cPlanning and control in stochastic domains with imperfect information\u201d. PhD thesis. Massachusetts Institute of Technology, 1997.   \n[Hau00] Milos Hauskrecht. \u201cValue-function approximations for partially observable Markov decision processes\u201d. In: Journal of artificial intelligence research 13 (2000), pp. 33\u2013 94.   \n[JSJ94] Tommi Jaakkola, Satinder Singh, and Michael Jordan. \u201cReinforcement Learning Algorithm for Partially Observable Markov Decision Problems\u201d. In: Advances in Neural Information Processing Systems. Vol. 7. MIT Press, 1994, pp. 345\u2013352.   \n[JKS16] Nan Jiang, Alex Kulesza, and Satinder P Singh. \u201cImproving Predictive State Representations via Gradient Descent.\u201d In: AAAI Conference on Artificial Intelligence. 2016, pp. 1709\u20131715.   \n[Kap+19] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. \u201cRecurrent Experience Replay in Distributed Reinforcement Learning\u201d. In: International Conference on Learning Representations. 2019.   \n[KY22] Ali Devran Kara and Serdar Yu\u00a8ksel. \u201cConvergence of Finite Memory Q Learning for POMDPs and Near Optimality of Learned Policies Under Filter Stability\u201d. In: Mathematics of Operations Research (Nov. 2022). ISSN: 1526-5471. DOI: 10.1287/moor. 2022.1331.   \n[KWW22] Mykel J Kochenderfer, Tim A Wheeler, and Kyle H Wray. Algorithms for decision making. MIT press, 2022.   \n[KJS15a] Alex Kulesza, Nan Jiang, and Satinder Singh. \u201cLow-rank spectral learning with weighted loss functions\u201d. In: Artificial Intelligence and Statistics. 2015, pp. 517\u2013525.   \n[KJS15b] Alex Kulesza, Nan Jiang, and Satinder P Singh. \u201cSpectral Learning of Predictive State Representations with Insufficient Statistics.\u201d In: AAAI Conference on Artificial Intelligence. 2015, pp. 2715\u20132721.   \n[KY97] Harold J. Kushner and G. George Yin. Stochastic Approximation Algorithms and Applications. Springer New York, 1997. DOI: 10.1007/978-1-4899-2696-8.   \n[LVC18] Tuyen P Le, Ngo Anh Vien, and TaeChoong Chung. \u201cA deep hierarchical reinforcement learning algorithm in partially observable Markov decision processes\u201d. In: Ieee Access 6 (2018), pp. 49089\u201349102.   \n[LS15] Boris Lesner and Bruno Scherrer. \u201cNon-Stationary Approximate Modified Policy Iteration\u201d. In: International Conference on Machine Learning. Vol. 37. Proceedings of Machine Learning Research. Lille, France: PMLR, July 2015, pp. 1567\u20131575.   \n[Lit96] Michael Lederman Littman. \u201cAlgorithms for sequential decision-making\u201d. PhD thesis. Brown University, 1996.   \n$[\\mathrm{Lu}+23]$ Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen, et al. \u201cReinforcement learning, bit by bit\u201d. In: Foundations and Trends in Machine Learning 16.6 (2023), pp. 733\u2013865.   \n$[\\mathrm{MH}{+}18]$ 1 Sultan Javed Majeed, Marcus Hutter, et al. \u201cOn Q-learning Convergence for NonMarkov Decision Processes.\u201d In: IJCAI. Vol. 18. 2018, pp. 2546\u20132552.   \n$[\\mathrm{Mni}+13]$ Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. \u201cPlaying atari with deep reinforcement learning\u201d. In: arXiv preprint arXiv:1312.5602 (2013).   \n[Mu\u00a8l97] Alfred Mu\u00a8ller. \u201cIntegral probability metrics and their generating classes of functions\u201d. In: Advances in Applied Probability 29.2 (1997), pp. 429\u2013443.   \n[Nor98] James R Norris. Markov chains. Cambridge University Press, 1998.   \n[PP02] Theodore J. Perkins and Mark D. Pendrith. \u201cOn the Existence of Fixed Points for QLearning and Sarsa in Partially Observable Domains\u201d. In: International Conference on Machine Learning. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2002, pp. 490\u2013497.   \n$[\\mathrm{PGT}{+}03]$ ] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. \u201cPoint-based value iteration: An anytime algorithm for POMDPs\u201d. In: International Joint Conference on Artificial Intelligence. Vol. 3. 2003, pp. 1025\u20131032.   \n[Pla77] Loren Kerry Platzman. \u201cFinite Memory Estimation and Control of Finite Probabilistic Systems.\u201d PhD thesis. Massachusetts Institute of Technology, 1977.   \n[PB24] L.A. Prashanth and Shalabh Bhatnagar. Gradient-based algorithms for zeroth-order optimization. Now publishers, 2024. URL: http : / / www . cse . iitm . ac . in / \\~prashla/bookstuff/GBSO_book.pdf.   \n[Pre00] Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts Amherst, 2000.   \n[Qia+18] Zhiqian Qiao, Katharina Muelling, John Dolan, Praveen Palanisamy, and Priyantha Mudalige. \u201cPomdp and hierarchical options mdp with continuous actions for autonomous driving at intersections\u201d. In: 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE. 2018, pp. 2377\u20132382.   \n[Rii65] Jens Ove Riis. \u201cDiscounted Markov Programming in a Periodic Process\u201d. In: Operations Research 13.6 (Dec. 1965), pp. 920\u2013929. ISSN: 1526-5463. DOI: 10.1287/ opre.13.6.920.   \n[RM51] Herbert Robbins and Sutton Monro. \u201cA Stochastic Approximation Method\u201d. In: The Annals of Mathematical Statistics 22.3 (1951), pp. 400\u2013407.   \n[RGT04] Matthew Rosencrantz, Geoff Gordon, and Sebastian Thrun. \u201cLearning low dimensional predictive representations\u201d. In: International Conference on Machine Learning. 2004.   \n[Sch16] Bruno Scherrer. On Periodic Markov Decision Processes. European Workshop on Reinforcement Learning. Dec. 2016. URL: https://ewrl.files.wordpress.com/ 2016/12/scherrer.pdf.   \n[SL12] Bruno Scherrer and Boris Lesner. \u201cOn the use of non-stationary policies for stationary infinite-horizon Markov decision processes\u201d. In: Advances in Neural Information Processing Systems 25 (2012).   \n[Sey+23] Erfan SeyedSalehi, Nima Akbarzadeh, Amit Sinha, and Aditya Mahajan. \u201cApproximate information state based convergence analysis of recurrent Q-learning\u201d. In: European conference on reinforcement learning. 2023. URL: https://arxiv.org/abs/ 2306.05991.   \n[Sil+16] David Silver et al. \u201cMastering the game of Go with deep neural networks and tree search\u201d. In: Nature 529 (2016), pp. 484\u2013489.   \n[SJJ94] Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. \u201cLearning without stateestimation in partially observable Markovian decision processes\u201d. In: Machine Learning. Elsevier, 1994, pp. 284\u2013292.   \n[SS73] Richard D. Smallwood and Edward J. Sondik. \u201cThe Optimal Control of Partially Observable Markov Processes over a Finite Horizon\u201d. In: Operations Research 21.5 (Oct. 1973), pp. 1071\u20131088. DOI: 10.1287/opre.21.5.1071.   \n[SS04] Trey Smith and Reid Simmons. \u201cHeuristic search value iteration for POMDPs\u201d. In: Conference on Uncertainty in Artificial Intelligence. Banff, Canada, 2004, pp. 520\u2013 527.   \n[SV05] Matthijs TJ Spaan and Nikos Vlassis. \u201cPerseus: Randomized point-based value iteration for POMDPs\u201d. In: Journal of Artificial Intelligence Research 24 (2005), pp. 195\u2013 220.   \n[Ste+18] Denis Steckelmacher, Diederik Roijers, Anna Harutyunyan, Peter Vrancx, He\u00b4le\\`ne Plisnier, and Ann Nowe\u00b4. \u201cReinforcement learning in POMDPs with memoryless options and option-observation initiation sets\u201d. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018.   \n$[\\mathrm{Sub}+22]$ Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. \u201cApproximate information state for approximate planning and reinforcement learning in partially observed systems\u201d. In: Journal of Machine Learning Research 23.12 (2022), pp. 1\u201383.   \n$[\\mathrm{Sut}+99]$ Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. \u201cPolicy gradient methods for reinforcement learning with function approximation\u201d. In: Advances in Neural Information Processing Systems. Vol. 12. 1999.   \n[SPS99] Richard S Sutton, Doina Precup, and Satinder Singh. \u201cBetween MDPs and semiMDPs: A framework for temporal abstraction in reinforcement learning\u201d. In: Artificial intelligence 112.1-2 (1999), pp. 181\u2013211.   \n[SB08] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2008.   \n[VGS16] Hado Van Hasselt, Arthur Guez, and David Silver. \u201cDeep reinforcement learning with double $\\mathbf{q}$ -learning\u201d. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 30. 1. 2016.   \n$[\\mathrm{Vez}{+}17]$ Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. \u201cFeudal networks for hierarchical reinforcement learning\u201d. In: International conference on machine learning. PMLR. 2017, pp. 3540\u20133549.   \n[WS94] Chelsea C White III and William T Scherer. \u201cFinite-memory suboptimal design for partially observed Markov decision processes\u201d. In: Operations Research 42.3 (1994), pp. 439\u2013455.   \n[WS97] Marco Wiering and Ju\u00a8rgen Schmidhuber. \u201cHQ-learning\u201d. In: Adaptive behavior 6.2 (1997), pp. 219\u2013246.   \n[Wie+07] Daan Wierstra, Alexander Foerster, Jan Peters, and Juergen Schmidhuber. \u201cSolving deep memory POMDPs with recurrent policy gradients\u201d. In: International Conference on Artificial Neural Networks (ICANN). Springer. 2007, pp. 697\u2013706.   \n$[\\mathrm{Wie}{+}10]$ Daan Wierstra, Alexander Fo\u00a8rster, Jan Peters, and Ju\u00a8rgen Schmidhuber. \u201cRecurrent policy gradients\u201d. In: Logic Journal of the IGPL 18.5 (2010), pp. 620\u2013634.   \n[Wit75] Hans S Witsenhausen. \u201cOn policy independence of conditional expectations\u201d. In: Information and Control 28.1 (1975), pp. 65\u201375.   \n[Zha09] H. Zhang. \u201cPartially Observable Markov Decision Processes: A Geometric Technique and Analysis\u201d. In: Operations Research (2009). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Illustrative examples 16   \nA.1 Ex. 1: Learning curves for ASQL 16   \nA.2 Ex. 2: non-stationary policies can outperform stationary policies 16   \nA.3 Ex. 3: stochastic policies can outperform deterministic policies 17   \nA.4 Ex. 4: conceptual difference between state-augmentation and periodic policies . . 18   \nB Periodic Markov chains 19   \nB.1 Time-homogeneous Markov chains and their properties 19   \nB.2 Time-varying with periodic transition matrix 20   \nB.3 Constructing an equivalent time-homogeneous Markov chain 21   \nB.4 Limiting behavior of periodic Markov chain 22 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Periodic Markov decision processes 24 ", "page_idx": 14}, {"type": "text", "text": "D Stochastic Approximation with Markov noise 24 ", "page_idx": 14}, {"type": "text", "text": "E Thm. 1: Convergence of periodic Q-learning 26 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Step 1: State splitting of the error function 26   \nE.2 Step 2: Convergence of component $X_{t}^{\\ell,0}$ 26   \nE.3 Step 3: Convergence of component Xt\u2113,1 27   \nE.4 Step 4: Convergence of component $X_{t}^{\\ell,2}$ 28   \nE.5 Putting everything together 30 ", "page_idx": 14}, {"type": "text", "text": "F Thm. 2: Sub-optimality gap 30 ", "page_idx": 14}, {"type": "text", "text": "G Policy evaluation of an agent-state based policy 31 ", "page_idx": 14}, {"type": "text", "text": "H Reproducibility information 31 ", "page_idx": 14}, {"type": "text", "text": "A Illustrative examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Ex. 1: Learning curves for ASQL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For each behavioral policy $\\bar{\\mu}_{k}$ , $k\\ \\in\\ \\mathsf{K}$ , we run PASQL for 25 random seeds. The median $^+$ interquantile range of the iterates $\\{Q_{t}(z,a)\\}_{t\\geq1}$ as well as the theoretical limits $Q_{\\bar{\\mu}_{k}}(z,a)$ (computed as per Thm. 1 for $L=1$ ) are shown in Fig. 5. These curves show that the result of Thm. 1 is valid for the stationary case ( $L=1$ ) as well. ", "page_idx": 15}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/fa66489736013edd3e993f1485066d09ac51caa09f96634ad4f06ecaf1750941.jpg", "img_caption": ["Figure 5: ASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Ex. 2: non-stationary policies can outperform stationary policies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Example 2 Consider a POMDP with ${\\sf S}=\\mathbb{Z}_{>0}$ , $\\mathsf{A}=\\{0,1\\}$ , and ${\\tt Y}\\,=\\,\\{0,1\\}$ . The system starts in an initial state $s_{1}=1$ and has deterministic dynamics. To describe the dynamics and the reward function, we define $\\mathsf{D}_{0}:=\\{n(n{+}1)/2{+}1:n\\in\\dot{\\mathbb{Z}}{\\ge}0\\}$ , $\\mathsf{D}_{1}=\\mathbb{N}\\backslash\\mathsf{D}_{0}$ , and ${\\sf D}={\\sf D}_{0}\\!\\times\\!\\{0\\}\\!\\cup\\!{\\sf D}_{1}\\!\\times\\!\\{1\\}\\!\\subset$ ${\\mathsf{S}}\\times{\\mathsf{A}}$ . Then, the dynamics, observations, and rewards are given by ", "page_idx": 15}, {"type": "equation", "text": "$$\ns_{t+1}=\\left\\{\\!\\!\\begin{array}{l l}{s_{t}+1,}&{(s_{t},a_{t})\\in\\mathsf{D},}\\\\ {1,}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\\quad y_{t}=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{s_{t}\\mathrm{~is~odd},}\\\\ {1,}&{s_{t}\\mathrm{~is~even},}\\end{array}\\!\\!\\right.\\quad r(s,a)=\\left\\{\\!\\!\\begin{array}{l l}{+1,}&{(s,a)\\in\\mathsf{D},}\\\\ {-1}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the state is incremented if the agent takes action 0 when the state is in ${\\sf D}_{0}$ and takes action 1 when the state is in $\\mathsf{D}_{1}$ . Taking these actions yield a reward of $+1$ . Not taking such an action results in a reward of $-1$ and resets the state to 1. The agent does not observe the state, but only observes whether the state is odd or even. A graphical representation of the model is shown in Fig. 1. ", "page_idx": 15}, {"type": "text", "text": "5 6 8 9 10 11 12 13 14 15 16 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 1: Graphical representation of Ex. 2. The cells indicate the state of the environment. Cells with the same background color have the same observation. The cells with a thick red boundary correspond to elements of the set $\\bar{\\mathsf{D}}_{0}:=\\{n(n+1)/2+1:n\\in\\mathbb{N}\\}$ , where the action 0 gives a reward of $+1$ and moves the state to the right, while the action 1 gives a reward of $-1$ and resets the state to 1. The cells with a thin black boundary correspond to elements of the set $\\mathsf{D}_{1}=\\mathbb{N}\\setminus\\mathsf{D}_{0}$ , where the action 1 gives the reward of $+1$ and moves the state to the right while the action 0 gives a reward of $-1$ and resets the state to 1. Discount factor $\\gamma=0.9$ . ", "page_idx": 15}, {"type": "text", "text": "For policy class $\\Pi_{\\mathbf{BD}}$ (the class of all belief-based deterministic policies), since the system starts in a known initial state and the dynamics are deterministic, the agent can compute the current state (thus, the belief is a delta function on the current state). Thus, the agent can always choose the correct action depending on whether the state is in ${\\sf D}_{0}$ and $\\mathsf{D}_{1}$ . Hence $\\bar{J_{\\mathrm{BD}}^{\\star}}=1/(1\\stackrel{\\cdot}{-}\\gamma)$ , which is the highest possible reward. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "For policy class $\\Pi_{\\mathbf{SD}}$ (the class of all agent-state based deterministic policies), there are four possible deterministic policies. For odd observations, the agent may take action 0 and 1. Similarly, for even observations, the agent may take action 0 or 1. Note that the system starts in state 1, which is in ${\\sf D}_{0}$ . Therefore, if the agent chooses action 1 when the observation is odd, it receives a reward of $-1$ and stays at state 1. Therefore, the discounted total reward is $-1/(1-\\gamma)$ , which is the least possible value. Therefore, any policy that chooses 1 on odd observations cannot be optimal. Therefore, the optimal (deterministic) action on odd observations is to pick action 0. Thus, there are two policies that we need to evaluate. ", "page_idx": 16}, {"type": "text", "text": "\u2022 If the agent chooses action 0 at both odd and even observations, the state cycles between $1\\rightarrow$ $2\\rightarrow3\\rightarrow1\\rightarrow2\\rightarrow3\\cdot\\cdot\\cdot$ with the reward sequence $(+1,+1,-1,+1,+1,-1,\\dots)$ . Thus, the cumulative total reward of this policy is $(1+\\bar{\\gamma}-\\gamma^{2})/(1-\\gamma^{3})$ .   \n\u2022 If the agent chooses action 0 at odd observations and action 1 at even observations, the state cycles between $1\\rightarrow2\\rightarrow1\\rightarrow2\\cdot\\cdot.$ with the reward sequence $(+1,-1,+1,-1,\\cdot\\cdot\\cdot)$ . Thus, the cumulative total reward of this policy is $1/(1+\\gamma)$ . ", "page_idx": 16}, {"type": "text", "text": "It is easy to verify that for $\\gamma\\in(0,1),1/(1+\\gamma)<(1+\\gamma-\\gamma^{2})/(1-\\gamma^{3}).$ Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{\\mathrm{SD}}^{\\star}=\\frac{1+\\gamma-\\gamma^{2}}{1-\\gamma^{3}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We also consider policy class $\\Pi_{\\mathrm{ss}}$ : the class of all stationary stochastic agent-state based policies. For policy class $\\Pi_{\\mathrm{ss}}$ , the policy is characterized by two numbers $(p_{0},\\stackrel{\\smile}{p_{1}})\\in[0,1]^{2}$ , where $p_{y}$ denotes the probability of choosing action 1 when the observation is $y$ , $y\\in\\{0,1\\}$ . We compute the approximately optimal policy by doing a brute force search over $(p_{0},p_{1})$ by discretizing them two decimal places and for each choice, running a Monte Carlo simulation of length 1, 000 and averaging it over 100 random seeds. We find that there is negligible difference between the performance of stochastic and deterministic policies. ", "page_idx": 16}, {"type": "text", "text": "Finally, we consider policy class $\\Pi_{L}$ , which is the class of periodic deterministic agent-state based policies. A policy $\\pi\\,\\in\\,\\Pi_{L}$ is characterized by two vectors $p_{0},p_{1}\\,\\in\\,\\{0,1\\}^{L}$ , where $p_{y,\\ell}$ denotes the action chosen when $t$ mod $L=\\ell$ and the observation is $y$ . We do an exhaustive search over all deterministic policies of length $L$ $\\mathsf{L},L\\in\\{1,\\ldots,10\\}$ to compute the numbers shown in the main text. ", "page_idx": 16}, {"type": "text", "text": "A.3 Ex. 3: stochastic policies can outperform deterministic policies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When the agent state is not an information state, the optimal stochastic stationary policy will perform better than (or equal to) the optimal deterministic stationary policy as observed in [SJJ94]. Here is an example to illustrate this for a simple toy POMDP. ", "page_idx": 16}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/7514cecea5078908c08ce2212bc19d0faa0dda7fbaee80876b272b5248d0d13f.jpg", "img_caption": ["Figure 6: The dynamics for Ex. 3. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Example 3 Consider a POMDP with ${\\mathsf{S}}=\\{0,1,2\\}$ , $\\mathsf{A}=\\{0,1\\}$ and ${\\sf Y}\\,=\\,\\{0\\}$ . The system starts at an initial state $s_{1}\\,=\\,0$ and the dynamics under the two actions are shown in Fig. 6. The agent does not observe the state, i.e., $Y_{t}\\equiv0$ . The rewards under action 0 are $r(\\cdot,0)=[\\bar{-}1,0,2]$ and the rewards under action 1 are $r(s,1)=-0.5$ , for all $s\\in S$ . ", "page_idx": 16}, {"type": "text", "text": "We consider agent state $Z_{t}~=~Y_{t}$ . Let $\\Pi_{\\mathrm{ss}}$ denote the of all stationary stochastic policies and $\\Pi_{\\mathrm{SD}}$ denote the class of of all stationary deterministic policic A policy $\\pi\\,\\in\\,\\Pi_{\\mathrm{SS}}$ is parameterized by a single parameter $\\textit{p}\\in\\:[0,1]$ , which indicates the probability of choosing action 1. We denote such a policy by $\\pi_{p}$ . Note that $p\\in\\{0,1\\}$ , $\\pi_{p}\\in\\Pi_{\\mathrm{SD}}$ . Let $(P_{a},r_{a})^{\\phantom{a}}$ denote the probability transition matrix and reward func", "page_idx": 17}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/8157d73804e6a79dfa90683e77c9cb89d09776cc4752f3eb6f5b6c4e45ed57d9.jpg", "img_caption": ["Figure 7: Performance of stationary stochastic policies $\\pi_{p}$ for $p\\in[0,1]$ for Ex. 3. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "tion when $a\\in\\mathsf{A}$ is chosen and let $(P_{p},r_{p})=(1-p)(P_{0},r_{0})+p(P_{1},r_{1})$ . Then, the performance of policy $\\pi_{p}$ is given by $J^{\\pi_{p}}=[(1-\\gamma P_{p})^{-1}r_{p}]_{0}$ . The performance for all $p\\in[0,1]$ for $\\gamma=0.9$ is shown in Fig. 7, which shows that the best performance is achieved by the stochastic policy $\\pi_{p}$ with $p\\approx0.39$ . ", "page_idx": 17}, {"type": "text", "text": "Thus, stochastic policies can outperform deterministic policies. ", "page_idx": 17}, {"type": "text", "text": "A.4 Ex. 4: conceptual difference between state-augmentation and periodic policies ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "HmMSBhMAw4/tmp/5b2fbbb67cdfe5700cd934e9762f289117c2ea5f5c2637275f34da4013da81b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: A T-shaped grid world for Ex. 4. In state S, the agent learns about the goal state. In states $\\{1,2,\\ldots,2n\\}$ , the agent simply knows that it is in the gray corridor, but does not know which cell it is in. In state T, it knows that it has reached the end of corridor and must decide whether to go up or down. The agent gets a reward of $+1$ for reaching the correct goal state and a reward of $-1$ for reaching the wrong goal state. ", "page_idx": 17}, {"type": "text", "text": "Example 4 Consider a T-shaped grid world showed in Fig. 4 with state space $\\mathsf{P}\\times\\mathsf{G}$ , where $\\textsf{P}=$ $\\{\\mathrm{s},1,2,\\ldots,2n,\\mathrm{T}\\}$ is the position of the agent and $\\mathsf{G}=\\left\\lbrace\\mathrm{G}_{1},\\mathrm{G}_{2}\\right\\rbrace$ is the location of the goal. The observation space is $\\mathsf{Y}=\\bar{\\{}0,}1,2,3\\}$ . The observation is a deterministic function of the state and is given as follows: ", "page_idx": 17}, {"type": "text", "text": "\u2022 At state $(\\mathrm{s},\\mathrm{G}_{i})$ , $i\\in\\{1,2\\}$ , the observation is $i$ and reveals the location of the goal state to the agent.   \n\u2022 At states $\\{1,\\ldots,2n\\}\\times\\mathsf{G}$ , the observation is 0, so the agent cannot distinguish between these states.   \n\u2022 At states $\\{\\mathrm{T}\\}\\times\\mathsf{G}$ , the observation is 3, so the agent knows when it reaches the T state. ", "page_idx": 17}, {"type": "text", "text": "The action space depends on the current state: actions $\\left\\{\\mathrm{LEFT},\\mathrm{RIGHT},\\mathrm{STAY}\\right\\}$ are available when the agent is at $\\{{\\bar{\\mathrm{s}}},1,\\dotsc,2n\\}$ and actions $\\{\\mathrm{UP},\\mathrm{DOWN}\\}$ are available at position T. ", "page_idx": 17}, {"type": "text", "text": "The agent receives a reward of $+1$ if it reaching the goal state and $-1$ if it reaches the wrong goal state state (i.e., reaches $\\mathrm{G_{2}}$ when the goal state is $\\mathrm{G_{1}}$ ). The discount factor $\\gamma=1$ . ", "page_idx": 17}, {"type": "text", "text": "We consider two classes of policies:   \n(i) $\\Pi_{\\mathrm{SD}}(m)$ : Stationary policies with agent state equal to a sliding window of the last $m$ observations and actions.   \n(ii) $\\Pi_{L}$ : Periodic policies with agent state equal to the last observation and periodic $L$ . ", "page_idx": 17}, {"type": "text", "text": "It is easy to see that as long as the window length $m\\leq2n$ , any policy in $\\Pi_{\\mathrm{SD}}(m)$ yields an average return of 0; for window lengths $m>2n$ , the agent can remember the first observation, and therefore it is possible to construct a policy that yields a return of $+1$ . ", "page_idx": 17}, {"type": "text", "text": "We now consider a deterministic periodic policy with period $L\\ =\\ 3$ given as follows:6 $\\pi\\ =$ $(\\pi^{0},\\pi^{1},\\pi^{2})$ where $\\pi^{\\ell}\\colon\\mathsf{Y}\\,\\to\\,\\mathsf{A}$ . We denote each $\\pi^{\\ell}$ as a column vector, where the $y$ -th component indicates the action $\\pi^{\\ell}(y)$ , where \u2013 means that the choice of the action for that observation is irrelevant for performance. The policy is given by ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi^{0}=\\left[\\!\\!\\begin{array}{c}{{\\mathrm{RIGHT}}}\\\\ {{\\mathrm{RIGHT}}}\\\\ {{\\mathrm{STAY}}}\\\\ {{\\mathrm{STAY}}}\\end{array}\\!\\!\\right],\\quad\\pi^{1}=\\left[\\!\\!\\begin{array}{c}{{\\mathrm{RIGHT}}}\\\\ {{-}}\\\\ {{\\mathrm{RIGHT}}}\\\\ {{\\mathrm{UP}}}\\end{array}\\!\\!\\right],\\quad\\pi^{2}=\\left[\\!\\!\\begin{array}{c}{{\\mathrm{STAY}}}\\\\ {{-}}\\\\ {{-}}\\\\ {{\\mathrm{DOWN}}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is easy to verify if the system starts in state $\\left(0,\\mathrm{G_{1}}\\right)$ , then by following policy $(\\pi^{0},\\pi^{1},\\pi^{2})$ , the agent reaches state $\\mathrm{G_{1}}$ at time $3n+3$ . Moreover, when the system starts in state $(0,\\mathrm{G_{2}})$ , then by following the policy $(\\pi^{0},\\pi^{1},\\pi^{2})$ , the agent reaches $\\mathrm{G_{2}}$ at time $3n+4$ . Thus, in both cases, the policy $(\\bar{\\pi^{0}},\\pi^{1},\\pi^{2})$ yields the maximum reward of $+1$ . ", "page_idx": 18}, {"type": "text", "text": "B Periodic Markov chains ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In most of the standard reference material on Markov chains, it is assumed that the Markov chain is aperiodic and irreducible. In our analysis, we need to work with periodic Markov chains. In this appendix, we review some of the basic properties of Markov chains and then derive some fundamental results for periodic Markov chains. ", "page_idx": 18}, {"type": "text", "text": "Let S be a finite set. A stochastic process $\\{S_{t}\\}_{t\\ge0}$ , $S_{t}\\in\\mathsf{S}$ , is called a Markov chain if it satisfies the Markov property: for any $t\\in\\mathbb{Z}_{\\geq0}$ and $s_{1:t+1}\\,\\bar{\\mathbf{\\Omega}}\\in\\mathsf{S}^{t+1}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(S_{t+1}=s_{t+1}\\mid S_{1:t}=s_{1:t})=\\mathbb{P}(S_{t+1}=s_{t+1}\\mid S_{t}=s_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If is often convenient to assume that ${\\mathsf{S}}=\\{1,\\dotsc,n\\}$ . We can define an $n\\times n$ transition probability matrix $P_{t}$ given by $[P_{t}]_{i j}\\,=\\,\\mathbb{P}(S_{t+1}\\,=\\,\\boldsymbol{\\dot{j}}\\mid S_{t}\\,=\\,\\boldsymbol{\\dot{i}})$ . Then, all the probabilistic properties of the Markov chain is described by the transition matrices $(P_{0},P_{1},\\dots)$ . ", "page_idx": 18}, {"type": "text", "text": "In particular, suppose the Markov chain starts at the initial PMF (probability mass function) $\\xi_{0}$ and let $\\xi_{t}$ denote the PMF at time $t$ . We will view $\\xi_{t}$ as a $n$ -dimensional row vector. Then, Eq. (4) implies $\\xi_{t+1}=\\xi_{t}P_{t}$ and, therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\xi_{t+1}=\\xi_{0}P_{0}P_{1}\\cdot\\cdot\\cdot P_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.1 Time-homogeneous Markov chains and their properties ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A Markov chain is said to be time-homogeneous if the transition matrix $P_{t}$ is the same for all time $t$ .   \nIn this section, we state some standard results for time-homogeneous Markov chains [Nor98]. ", "page_idx": 18}, {"type": "text", "text": "B.1.1 Classification of states ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The states of a time-homogeneous Markov chain can be classified as follows. ", "page_idx": 18}, {"type": "text", "text": "1. We say that a state $j$ is accessible from $i$ (abbreviated as $i\\not\\sim j$ ) if there is exists an $m\\in\\mathbb{Z}_{\\geq0}$ (which may depend on $i$ and $j$ ) such that $[P^{m}]_{i j}\\;>\\;0$ . The fact that $[P^{m}]_{i j}\\,>\\,0$ implies that there exists an ordered sequence of states $(i_{0},\\ldots,i_{m})$ such that $i_{0}\\,=\\,i$ and $i_{m}\\,=\\,j$ such that $P_{i_{k}i_{k+1}}>0$ ; thus, there is a path of positive probability from state $i$ to state $j$ . Accessibility is an transitive relationship, i.e., if $i\\not\\sim j$ and $j\\rightsquigarrow k$ implies that $i\\not\\sim k$ .   \n2. Two distinct states $i$ and $j$ are said to communicate (abbreviated to $i\\not\\leftarrow\\infty j$ ) if $i$ is accessible from $j$ (i.e., $j\\rightsquigarrow i)$ and $j$ is accessible from $i$ $(i\\rightsquigarrow j)$ . Alternatively, we say that $i$ and $j$ communicate if there exist $m,m^{\\prime}\\in\\mathbb{Z}_{\\geq0}$ such that $[P^{m}]_{i j}>0$ and $[P^{m^{\\prime}}]_{j i}>0$ . Communication is an equivalence relationship, i.e., it is reflexive $(i\\rightsquigarrow i)$ , symmetric $(i\\rightsquigarrow j$ if and only if $j\\rightsquigarrow i$ ), and transitive $(i\\rightsquigarrow j$ and $j\\ \\not\\sim k$ implies $i\\not\\leftarrow\\!k$ ).   \n3. The states in a finite-state Markov chain can be partitioned into two sets: recurrent states and transient states. A state is recurrent if it is accessible from all states that are from it (i.e., $i$ is recurrent if $i\\not\\sim j$ implies that $j\\rightsquigarrow i$ ). States that are not recurrent are transient. It can be shown that a state $i$ is recurrent if and only if ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}[P^{t}]_{i i}=\\infty.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "4. States $i$ and $j$ are said to belong to the same communicating class if $i$ and $j$ communicate. Communicating classes form a partition the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class). ", "page_idx": 19}, {"type": "text", "text": "A Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called irreducible. ", "page_idx": 19}, {"type": "text", "text": "5. The period of a state $i$ , denoted by $d(i)$ , is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nd(i)=\\operatorname*{gcd}\\{t\\in\\mathbb{Z}_{\\geq1}:[P^{t}]_{i i}>0\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If the period is 1, the state is aperiodic, and if the period is 2 or more, the state is periodic. It can be shown that all states in the same class have the same period. ", "page_idx": 19}, {"type": "text", "text": "A Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state $i$ such that $P_{i i}>0$ . In general, for a finite and aperiodic Markov chain, there exists a positive integer $T$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n[P^{t}]_{i i}>0,\\quad\\forall t\\geq T,i\\in\\mathsf{S}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.1.2 Limit behavior of Markov chains ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now state some special distributions for a time-homogeneous Markov chain. ", "page_idx": 19}, {"type": "text", "text": "1. A PMF $\\zeta$ on S is called a stationary distribution if $\\zeta=\\zeta P$ . Thus, if a (time-homogeneous) Markov chain starts in a stationary distribution, it stays in a stationary distribution. ", "page_idx": 19}, {"type": "text", "text": "A finite irreducible Markov chain has a unique stationary distribution. Moreover, when the Markov chain is also aperiodic, the stationary distribution is given by $\\zeta(j)~=~1/m_{j}$ , where $m_{j}$ is the expected return time to state $j$ . ", "page_idx": 19}, {"type": "text", "text": "2. A PMF $\\zeta$ on S is called a limiting distribution if ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}[P^{t}]_{i j}=\\zeta(j),\\quad\\forall i,j\\in\\mathsf{S}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A finite irreducible Markov chain has a limiting distribution if and only if it is aperiodic. Therefore, for an aperiodic Markov chain, the limiting distribution is the same as the stationary distribution. ", "page_idx": 19}, {"type": "text", "text": "Theorem 3 (Strong law of large numbers for Markov chains, Theorem 5.6.1 of [Dur19]) Suppose $\\{S_{t}\\}_{t\\ge1}$ is an irreducible Markov chain that starts in state $i\\in{\\mathsf{S}}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}{\\frac{1}{T}}\\sum_{t=0}^{T-1}\\mathbb{1}\\{S_{t}=j\\}={\\frac{1}{m_{j}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for any function $h\\colon\\mathsf{S}\\to\\mathbb{R},$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1}h(S_{t})=\\sum_{j\\in\\mathsf{S}}\\frac{h(j)}{m_{j}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If, in addition, the Markov chain $\\{S_{t}\\}_{t\\ge1}$ is aperiodic, and has a limiting distribution $\\zeta$ , then we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}{\\frac{1}{T}}\\sum_{t=0}^{T-1}h(S_{t})=\\sum_{j\\in\\mathsf{S}}\\zeta(j)h(j).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Time-varying with periodic transition matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we consider time-varying Markov chains where the transition matrices $(P_{0},P_{1},\\dots)$ are periodic with period $L$ . Let $[t]\\,=\\,(t\\bmod L)$ and ${\\mathsf{L}}\\,=\\,\\{0,\\dotsc,L\\mathrm{~-~}1\\}$ . Then, the transition matrix $P_{t}$ is the same as $P_{\\mathbb{[t]}}$ . Th u s,  the system dynamics are completely described by the transition matrices $\\{P_{\\ell}\\}_{\\ell\\in\\mathsf{L}}$ . With a slight abuse of notation, we will call such a Markov chain as $L$ -periodic Markov chain. We will show later that the notion of time-periodicity that we are considering is equivalent to the notion of state-periodicity for time-homogeneous Markov chains defined earlier. ", "page_idx": 19}, {"type": "text", "text": "B.3 Constructing an equivalent time-homogeneous Markov chain ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since the Markov chain is not time-homogeneous, the classification and results of the previous section are not directly applicable. There are two ways to construct a time-homogeneous Markov chain: using state augmentation or viewing the process after every $L$ steps. ", "page_idx": 20}, {"type": "text", "text": "B.3.1 Method 1: State augmentation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The original time-varying Markov chain $\\{S_{t}\\}_{t\\ge0}$ is equivalent to the time-homogeneous Markov chain $\\{(S_{t},[\\![t]\\!]\\}_{t\\geq0}$ defined on $\\mathsf{S}\\times\\mathsf{L}$ with transition matrix $\\bar{P}$ given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{P}((s^{\\prime},\\ell^{\\prime})\\mid(s,\\ell))=P_{\\ell}(s^{\\prime}\\mid s)\\mathbb{1}\\{\\ell^{\\prime}=\\mathbb{[}\\ell+1\\mathbb{]}\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Example 5 Consider a 2-periodic Markov chain with state space ${\\mathsf{S}}=\\{1,2\\}$ and transition matrices ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{0}=\\left[{\\frac{1}{4}}\\quad{\\frac{3}{4}}\\right]\\quad{\\mathrm{and}}\\quad P_{1}=\\left[{\\frac{3}{4}}\\quad{\\frac{1}{4}}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The time-periodic Markov chain of Ex. 5 may be viewed as a time-homogeneous Markov chain with state space $\\{1,2\\}\\times\\{0,1\\}$ and transition matrix ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{P}=\\begin{array}{c c c c}{(1,0)}&{(2,0)}&{(1,1)}&{(2,1)}\\\\ {(1,0)}\\\\ {(2,0)}\\\\ {(1,1)}\\\\ {(2,1)}\\end{array}\\biggr[\\begin{array}{c c c c}{0}&{0}&{\\frac{1}{4}}&{\\frac{3}{4}}\\\\ {0}&{0}&{\\frac{1}{2}}&{\\frac{1}{2}}\\\\ {\\frac{3}{4}}&{\\frac{1}{4}}&{0}&{0}\\\\ {\\frac{1}{4}}&{\\frac{3}{4}}&{0}&{0}\\end{array}\\biggr]=\\left[\\begin{array}{c c}{0}&{I}\\\\ {I}&{0}\\end{array}\\right]\\left[\\begin{array}{c c}{P_{0}}&{0}\\\\ {0}&{P_{1}}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where 0 denotes the all zero matrix and $I$ denotes the identity matrix (both of size $2\\times2$ ). Note that the time-homogeneous Markov chain is periodic. ", "page_idx": 20}, {"type": "text", "text": "Define the following: ", "page_idx": 20}, {"type": "text", "text": "\u2022 $L$ block diagonal matrices $\\Lambda_{0},...\\,,\\Lambda_{L-1}\\in\\mathbb{R}^{n L\\times n L}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Lambda_{0}=\\mathrm{blkdiag}(P_{0},P_{1},\\ldots,P_{L-1}),\\quad\\Lambda_{1}=\\mathrm{blkdiag}(P_{L-1},P_{0},\\ldots,P_{L-2}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 A permutation matrix $\\Pi\\in\\{0,1\\}^{n L\\times n L}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Pi=\\left[\\!\\!{\\begin{array}{c c c c}{0}&{I}&{\\cdot\\cdot\\cdot}&{0}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\cdot\\cdot}&{\\vdots}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{I}\\\\ {I}&{0}&{\\cdot\\cdot\\cdot}&{0}\\end{array}}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where each block is $n\\times n$ . ", "page_idx": 20}, {"type": "text", "text": "The permutation matrix $\\Pi$ satisfies the following properties (which can be verified by direct algebra): ", "page_idx": 20}, {"type": "text", "text": "(P1) $\\Pi\\Pi^{\\mathsf{T}}=I$ and therefore $\\Pi^{-1}=\\Pi^{\\top}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Lambda_{\\ell}\\Pi=\\Pi\\Lambda_{\\mathbb{I}\\ell+1\\mathbb{I}},\\ell\\in\\mathsf{L}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In general, the transition matrix of the Markov chain $\\{(S_{t},[\\![t]\\!]\\}_{t\\geq0}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{P}=\\left[\\begin{array}{c c c c}{\\mathrm{0}}&{P_{0}}&{\\cdots}&{\\mathrm{0}}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\vdots}\\\\ {\\mathrm{0}}&{\\mathrm{0}}&{\\cdots}&{P_{L-2}}\\\\ {P_{L-1}}&{\\mathrm{0}}&{\\cdots}&{\\mathrm{0}}\\end{array}\\right]_{n L\\times n L}=\\Lambda_{0}\\Pi.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.3.2 Method 2: Viewing the process every $L$ steps ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The original Markov chain viewed every $L$ -steps, i.e., the process $\\{S_{k L+\\ell}\\}_{k\\geq0}$ , $\\ell\\,\\in\\,{\\mathsf L}$ , is a timehomogeneous Markov chain with transition probability matrix $\\mathcal{P}_{\\ell}$ given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\ell}=P_{\\mathbb{[}\\ell\\mathbb{]}}P_{\\mathbb{[}\\ell+1\\mathbb{]}}\\cdot\\cdot\\cdot P_{\\mathbb{[}\\ell+L-1\\mathbb{]}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "that is, ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathcal P}_{0}=P_{0}P_{1}\\cdot\\cdot\\cdot P_{L-2}P_{L-1},\\quad{\\mathcal P}_{1}=P_{1}P_{2}\\cdot\\cdot\\cdot P_{L-1}P_{0},\\quad\\mathrm{e}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.3.3 Relationship between the two constructions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The two constructions are related as follows. ", "page_idx": 21}, {"type": "text", "text": "Proposition 1 We have that $\\bar{P}^{L}=\\mathrm{blkdiag}(\\mathcal{P}_{0},\\ldots,\\mathcal{P}_{L}).$ . ", "page_idx": 21}, {"type": "text", "text": "PROOF From (P3), we get that $\\bar{P}=\\Pi\\Lambda_{1}$ . Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{P}^{2}=\\Lambda_{0}\\Pi\\Lambda_{0}\\Pi=\\Lambda_{0}\\Lambda_{1}\\Pi^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{P}^{3}=\\Lambda_{0}\\Pi\\bar{P}^{2}=\\Lambda_{0}\\Pi\\Lambda_{0}\\Lambda_{1}\\Pi^{2}=\\Lambda_{0}\\Lambda_{1}\\Pi\\Lambda_{1}\\Pi^{2}=\\Lambda_{0}\\Lambda_{1}\\Lambda_{2}\\Pi^{3}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Continuing this way, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{P}^{L}=\\Lambda_{0}\\Lambda_{1}\\dots\\Lambda_{L-1}\\Pi^{L}=\\Lambda_{0}\\Lambda_{1}\\dots\\Lambda_{L-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last equality follows from (P2). The result then follows from the definitions of $\\Lambda_{\\ell}$ and $\\mathcal{P}_{\\ell}$ , $\\ell\\in{\\mathsf{L}}$ . 2 ", "page_idx": 21}, {"type": "text", "text": "B.4 Limiting behavior of periodic Markov chain ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the subsequent discussion, we consider the following assumptions. ", "page_idx": 21}, {"type": "text", "text": "Assumption 3 Every $\\{\\mathcal P_{\\ell}\\},\\ell\\in\\mathsf{L}$ , is irreducible and aperiodic ", "page_idx": 21}, {"type": "text", "text": "Suppose Assm. 3 holds. Define $\\zeta^{\\ell}$ to be the unique stationary distribution for Markov chain $\\mathcal{P}_{\\ell}$ , $\\ell\\in{\\mathsf{L}}$ , i.e., $\\zeta^{\\ell}$ is the unique PMF that satisfies $\\zeta^{\\ell}=\\dot{\\zeta}^{\\ell}\\mathcal{P}_{\\ell}$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 2 The PMFs $\\{\\zeta^{\\ell}\\}_{\\ell\\in{\\mathsf{L}}}$ satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\zeta^{\\ell}P_{\\ell}=\\zeta^{\\left[\\ell+1\\right]},\\quad\\ell\\in\\mathsf{L}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "PROOF We prove the result for $\\ell=0$ . The analysis is the same for general $\\ell$ . By assumption, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\zeta^{0}=\\zeta^{0}\\mathcal{P}_{0}=\\zeta^{0}P_{0}P_{1}\\cdot\\cdot\\cdot P_{L-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\bar{\\zeta}^{1}:=\\zeta^{0}P_{0}$ . Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{\\zeta}^{1}=\\zeta^{0}P_{0}=\\zeta^{0}P_{0}P_{1}\\cdots P_{L-1}P_{0}=\\bar{\\zeta}^{1}P_{1}\\cdots P_{L-1}P_{0}=\\bar{\\zeta}^{1}\\mathcal{P}_{1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus $\\bar{\\zeta}^{1}$ is a stationary distribution. Since $\\mathcal{P}_{1}$ is irreducible, the stationary distribution is unique, hence $\\bar{\\zeta}^{1}$ must equal $\\dot{\\zeta}^{1}$ . 2 ", "page_idx": 21}, {"type": "text", "text": "We can verify this result for $\\mathrm{Ex}$ . 5. For this model, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{0}=P_{0}P_{1}=\\left[\\frac{3}{8}\\ \\ \\ \\frac{5}{8}\\right]\\quad\\mathrm{and}\\quad\\mathcal{P}_{1}=P_{1}P_{0}=\\left[\\frac{5}{16}\\ \\ \\ \\frac{11}{16}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\zeta^{0}=\\left[{\\frac{4}{9}}\\right.\\quad{\\frac{5}{9}}\\right]\\quad{\\mathrm{and}}\\quad\\zeta^{1}=\\left[{\\frac{7}{18}}\\quad{\\frac{11}{18}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And we can verify that $\\zeta^{0}P_{0}=\\zeta^{1}$ and $\\zeta^{1}P_{1}=\\zeta^{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 3 Under Assm. 3, the limiting distribution of the Markov chain $\\{S_{t}\\}_{t\\ge0}$ is cyclic. In particular, for any initial distribution $\\xi_{0}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\rightarrow\\infty}\\xi_{k L+\\ell}=\\zeta^{\\ell}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{K\\to\\infty}\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{1}\\{S_{k L+\\ell}=i\\}=[\\zeta^{\\ell}]_{i},\\quad\\forall i\\in{\\sf S},\\ell\\in{\\sf S}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, for any function $h\\colon\\mathsf{S}\\to\\mathbb{R},$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{K\\to\\infty}\\operatorname*{l}_{K}\\sum_{k=0}^{K-1}h(S_{k L+\\ell})=\\sum_{s\\in\\mathsf{S}}h(s)[\\zeta^{\\ell}]_{s},\\quad\\ell\\in\\mathsf{S}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "PROOF The results follow from standard results for the time-homogeneous Markov chain $\\{S_{k L+\\ell}\\}_{k\\geq0}$ . 2 ", "page_idx": 22}, {"type": "text", "text": "PROOF (ALTERNATIVE) We present an alternative proof that uses the state augmented Markov chain $\\bar{P}$ . We first prove that under Assm. 3, the chain $\\bar{P}$ is irreducible periodic with period $L$ . ", "page_idx": 22}, {"type": "text", "text": "The proof of irreducibility relies on two observations. ", "page_idx": 22}, {"type": "text", "text": "1. Fix an $\\ell\\in{\\mathsf{L}}$ and consider $i,j\\in\\mathsf{S}$ . Since $\\mathcal{P}_{\\ell}$ is irreducible, we have that there exists a positive integer $m$ (depending on $i,\\;j$ , and $\\boldsymbol{\\ell}$ ) such that $[{\\mathcal P}_{\\ell}^{m}]_{i j}\\ >\\ 0$ . Note that Prop. 1 implies that $[\\bar{P}^{m L}]_{(i,\\ell),(j,\\ell)}=[\\mathcal{P}_{\\ell}]_{i j}>0$ . Therefore, in the Markov chain $\\bar{P}$ , states $(i,\\ell)\\,\\sim(j,\\ell)$ . Since $i$ and $j$ were arbitrary, all states ${\\mathsf{S}}\\times\\{\\ell\\}$ belong to the same communicating class. ", "page_idx": 22}, {"type": "text", "text": "2. Now consider two $\\ell,\\ell^{\\prime}\\in{\\mathsf{L}}$ . Suppose we start at some state $(i,\\ell)\\in\\mathsf{S}\\times\\{\\ell\\}$ , then in $[\\ell^{\\prime}-\\ell]$ steps, we will reach some state $(j,\\ell^{\\prime})\\in\\mathsf{S}\\times\\{\\ell^{\\prime}\\}$ . Thus, $(j,\\ell^{\\prime})$ is accessible from $(i,\\ell)$ . But, we have already argued that all states in ${\\mathsf{S}}\\times\\{\\ell\\}$ belong to the same communicating class, therefore all states in ${\\bar{\\mathsf{S}}}\\times\\{\\ell^{\\prime}\\}$ are accessible from all states in ${\\mathsf{S}}\\times\\{\\ell\\}$ . By interchanging the roles of $\\ell$ and $\\ell^{\\prime}$ , we have that all states in ${\\mathsf{S}}\\times\\{\\ell\\}$ are accessible from all starts in ${\\mathsf{S}}\\times\\{\\ell^{\\prime}\\}$ . Therefore, the states ${\\mathsf{S}}\\times\\{\\ell\\}$ and ${\\mathsf{S}}\\times\\{\\ell^{\\prime}\\}$ belong to the same communicating class. Since $\\ell$ and $\\ell^{\\prime}$ were arbitrary, we have that all states of $\\bar{P}$ belong to the same communicating class. Hence, $\\bar{P}$ is irreducible. ", "page_idx": 22}, {"type": "text", "text": "We now show that $\\bar{P}$ is periodic. First observe that the Markov chain starting in the set ${\\mathsf{S}}\\times\\{\\ell\\}$ does not return to the same set for the first $L-1$ steps. Thus, $[\\bar{P}^{t}]_{(i,\\ell),(i,\\ell)}=0$ for $t\\in\\{1,2,\\ldots,L-1\\}$ . Therefore, the only possible values of $t$ for which $[\\bar{P}^{t}]_{(i,\\ell),(i,\\ell)}>0$ are those that are multiples of $L$ . Hence, for any $(i,\\ell)\\in\\sf S\\times\\sf L$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nd(i,\\ell)=\\operatorname*{gcd}\\{t\\in\\mathbb{Z}_{\\geq1}:[\\bar{P}^{t}]_{(i,\\ell),(i,\\ell)}>0\\}=L\\operatorname*{gcd}\\{k\\in\\mathbb{Z}_{\\geq1}:[\\mathcal{P}_{\\ell}^{k}]_{i i}>0\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, since $\\mathcal{P}_{\\ell}$ is aperiodic, $\\operatorname*{gcd}\\{k\\in\\mathbb{Z}_{\\geq1}:[\\mathcal{P}_{\\ell}^{k}]_{i i}>0\\}=1$ . Substituting in (9), we get that $d(i,\\ell)=L$ for all $(i,\\ell)$ . Thus, all states have a period of $L$ . ", "page_idx": 22}, {"type": "text", "text": "Now, from Prop. 1, we know that $\\Bar{P}^{L}=\\mathrm{blkdiag}(\\mathcal{P}_{0},...\\,,\\mathcal{P}_{L-1})$ . Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}[\\bar{P}^{k L}]_{(i,\\ell),(j,\\ell)}=[\\zeta^{\\ell}]_{j},\\quad(i,\\ell)\\in\\mathsf{S}\\times\\mathsf{L}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, if we start with an initial distribution $\\bar{\\xi}_{0}$ such that $\\bar{\\xi}_{0}(\\mathsf{S}\\times\\{0\\})=1$ , then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\rightarrow\\infty}\\bar{\\xi}_{k L}=\\mathrm{vec}(\\zeta_{0},0,\\ldots,0)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the 0 vectors are of size $n$ . Consequently, Prop. 2 implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\bar{\\xi}_{k L+\\ell}=\\mathrm{vec}(0,\\ldots,0,\\zeta_{\\ell},0,\\ldots,0),\\quad\\forall\\ell\\in\\mathsf{L}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\zeta^{\\ell}$ is the $\\ell$ -th place. This completes the proof of (7). ", "page_idx": 22}, {"type": "text", "text": "Now consider the function $\\bar{h}\\colon\\mathsf{S}\\times\\mathsf{L}\\to\\mathbb{R}$ defined as $\\bar{h}(s,\\ell^{\\prime})\\,=\\,h(s)\\mathbb{1}\\{\\ell^{\\prime}=\\ell\\}$ . Then, by taking $T=K L$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{K\\rightarrow\\infty}{\\frac{1}{K}}\\sum_{t=0}^{K-1}h(S_{k L+\\ell})=\\operatorname*{lim}_{T\\rightarrow\\infty}{\\frac{L}{T}}\\sum_{t=0}^{T-1}{\\bar{h}}(S_{t},\\|t\\|)=L\\sum_{s\\in{\\mathsf{S}}}{\\frac{h(s)}{m_{(s,\\ell)}}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equation uses (5) from Thm. 3. Now, (8) follows from observing that mean return time to state $(s,\\ell)$ in Markov chain $\\bar{P}$ is $L$ times the mean-return time to state $s$ in Markov chain $\\mathcal{P}_{\\ell}$ , which equals $\\mathrm{1}/[\\zeta^{\\ell}]_{s}$ since $\\mathcal{P}_{\\ell}$ is irreducible and aperiodic. 2 ", "page_idx": 22}, {"type": "text", "text": "C Periodic Markov decision processes ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Periodic MDPs are a special class time non-stationary MDPs where the dynamics and rewards are periodic. In particular, let $\\mathcal{M}$ be a time-varying MDP with state space S, action space $\\mathsf{A}$ , and dynamics and reward at time $t$ given by $P_{t}\\colon\\mathsf{S}\\times\\mathsf{A}\\to\\Delta(\\mathsf{S})$ and $r_{t}\\colon\\mathsf{S}\\times\\mathsf{A}\\to\\mathbb{R}$ . ", "page_idx": 23}, {"type": "text", "text": "As before, we use $[\\![t]\\!]$ to denote $t$ mod $L$ and $\\mathsf{L}$ to denote $\\{0,\\ldots,L-1\\}$ . The MDP $\\mathcal{M}$ is periodic with period $L$ if th e re  exist $(P^{\\ell},r^{\\ell}),\\ell\\in\\mathsf{L}$ such that for all $t$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{t}(S_{t+1}\\mid S_{t},A_{t})=P^{[t]}(S_{t+1}\\mid S_{t},A_{t})\\quad{\\mathrm{and}}\\quad r_{t}(S_{t},A_{t})=r^{[t]}(S_{t},A_{t}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Periodic MDPs were first considered in [Rii65]. Periodic MDPs may be viewed as stationary MDPs by considering the augmented state $\\left(S_{t},\\mathbb{[}t\\mathbb{]}\\right)$ . By this equivalence, it can be shown that there is no loss of optimality in restricting attenti o n  to periodic policies. In particular, let $(V^{0},\\ldots,V^{L-1})$ denote the fixed point of the following system of equations ", "page_idx": 23}, {"type": "equation", "text": "$$\nV^{\\ell}(s)=\\operatorname*{max}_{a\\in\\mathbb{A}}\\Bigl\\{r^{\\ell}(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathbb{S}}P^{\\ell}(s^{\\prime}|s,a)V^{\\mathbb{I}\\ell+1}\\mathbb{I}(s^{\\prime})\\Bigr\\},\\quad\\forall(\\ell,s,a)\\in\\mathbb{L}\\times\\mathbb{S}\\times\\mathbb{A}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define $\\pi_{\\star}^{\\ell}(s)$ to be the arg-max or the right hand side of (10). Then the time-varying policy $\\pi=$ $(\\pi_{1},\\pi_{2},\\ldots)$ given by \u03c0t = \u03c0 \u22c6t is optimal. ", "page_idx": 23}, {"type": "text", "text": "See [Sch16] for a discussion of how to modify standard MDP algorithms to solve periodic dynamic program (10). ", "page_idx": 23}, {"type": "text", "text": "D Stochastic Approximation with Markov noise ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now state a generalization of Thm. 3 to stochastic approximation style iterations. ", "page_idx": 23}, {"type": "text", "text": "Theorem 4 Let $\\{S_{t}\\}_{t\\ge1}$ , S, be an irreducible and aperiodic finite Markov chain with unique limiting distribution $\\zeta$ . Let $\\mathcal{F}_{t}$ denote the natural filtration w.r.t. $\\{S_{t}\\}_{t\\ge1}$ and $\\{\\alpha_{t}\\}_{t\\ge1}$ be a non-negative real-valued process adapted to $\\{\\mathcal{F}_{t}\\}$ that satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t\\geq1}\\alpha_{t}=\\infty\\quad a n d\\quad\\sum_{t\\geq1}\\alpha_{t}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\{M_{t+1}\\}_{t\\ge1}$ be a square-integrable margingale difference sequence w.r.t. $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ such that $\\mathbb{E}[M_{t+1}^{2}\\,\\mid\\,\\mathcal{F}_{t}\\overline{{\\]}}\\,\\le\\,K(1\\,\\mathbf{\\bar{\\Omega}}+\\,\\|X_{t}\\|^{2})$ for some constant $K$ . Consider the iterative process $\\{X_{t}\\}_{t\\ge1}$ , where $X_{1}$ is arbitrary and for $t\\geq1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{t+1}=(1-\\alpha_{t})X_{t}+\\alpha_{t}\\big[h(S_{t})+M_{t+1}\\big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the sequence $\\{X_{t}\\}_{t\\ge1}$ converges almost surely to limit. In particular, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\rightarrow\\infty}X_{T}=\\sum_{s\\in{\\mathsf{S}}}h(s)\\zeta(s),\\quad a.s.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Eq. (12) is similar to standard stochastic approximation iteration [RM51; KY97; Bor08], which the \u201cnoise sequence\u201d $h(S_{t})$ is assumed to be a martingale difference sequence. The setting considered above is sometimes referred to as stochastic approximation with Markov noise. In fact, more general version of this result where the noise sequence is allowed to depend on the state $X_{t}$ are typically established in the literature [BMP12; Bor08; KY97; PB24]. For the sake of completeness, we will show that Thm. 4 is a special case of these more-general results. ", "page_idx": 23}, {"type": "text", "text": "Before presenting the proof, we point out that Thm. 4 is a generalization of Thm. 3, Eq. (6). In particular, suppose the learning rates are $\\alpha_{t}=1/(1+t)$ . Then, simple algebra shows that ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{T}=\\frac{1}{T}\\sum_{t=1}^{T}h(S_{t}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, (6) of Thm. 3 implies that the limit is given by the right had side of (13). Therefore, Thm. 4 is a generalization of Thm. 3 to general learning rates which satisfy (11). ", "page_idx": 23}, {"type": "text", "text": "PROOF To establish the result, we will show that the iteration $\\{X_{t}\\}_{t\\ge1}$ satisfies the assumptions for the convergence of stochastic approximation with (state dependent) Markov noise and stochastic recursive inclusions given in [PB24, Theorem 2.7]. The proof is due to [BP24]. In particular, we can rewrite (12) as ", "page_idx": 24}, {"type": "equation", "text": "$$\nX_{t+1}=X_{t}+\\alpha_{t}g(X_{t},S_{t})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $g(x,s)=-x+h(s)$ . Moreover, for ease of notation, define $\\begin{array}{r}{\\bar{h}=\\sum_{s\\in\\mathsf{S}}h(s)\\zeta(s)}\\end{array}$ . Then, we have ", "page_idx": 24}, {"type": "text", "text": "\u2022 $g(x,s)$ is Lipschtiz continuous in the first argument, so A2.14 of [PB24] holds.   \n\u2022 From (6), the ergodic occupation measure of $\\{h(S_{t})\\}_{t\\geq1}$ is $\\{\\bar{h}\\}$ , which is compact and convex. So, A2.15 of [PB24] is satisfied.   \n\u2022 The conditions on the martingale noise sequence $\\{M_{t}\\}_{t\\ge1}$ imply that A2.16 of [PB24] holds.   \n\u2022 Eq. (11) is equivalent to A2.17 of [PB24]. ", "page_idx": 24}, {"type": "text", "text": "\u2022 To check A2.18 of [PB24], for any measure $\\nu$ on S, define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{h}(x,\\nu)=\\int g(x,s)\\nu(d s)=-x+\\bar{h}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Also define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{h}_{c}(x,\\nu)=\\frac{\\tilde{h}(c x,c\\nu)}{c}=-x+\\frac{\\bar{h}}{c}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tilde{h}_{\\infty}(x,\\nu)=\\operatorname*{lim}_{c\\to\\infty}h_{c}(x,\\nu)=x}\\end{array}$ . Thus, the differential inclusion in A2.18(ii) is actually an ODE ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\dot{x}}=-x\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which has origin as the unique global asymptotically stable equilibrium point. Thus, A2.18 of [PB24] is satisfied. ", "page_idx": 24}, {"type": "text", "text": "Therefore, all assumptions of Theorem 2.7 of [PB24] are satisfied. Therefore, by that result, the iterates $\\{X_{t}\\}_{t\\ge1}$ converge to solution of the ODE (note that the differential inclusion in Theorem 2.7 of [PB24] is an ODE in our setting) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\dot{x}=-x+\\bar{h}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $x=\\bar{h}$ is the unique asymptotically stable attractor of the ODE (14). Therefore, Theorem 2.7 of [PB24] implies (13). 2 ", "page_idx": 24}, {"type": "text", "text": "Thm. 4 also implies the following generalization of Prop. 3. ", "page_idx": 24}, {"type": "text", "text": "Proposition 4 Suppose $\\{S_{t}\\}_{t\\ge1}$ is a time-periodic Markov chain with period $L$ that satisfies Assm. 3 with the unique limiting distribution $\\{\\zeta^{\\ell}\\}_{\\ell\\in{\\mathsf{L}}}$ . Let $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ denote the natural filtration w.r.t. $\\{S_{t}\\}_{t\\ge1}$ and $\\{\\alpha_{t}^{\\ell}\\}_{t\\ge1}$ , $\\ell\\in{\\mathsf{L}}$ , be non-negative real-valued processes adapted to $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ such that $\\alpha_{t}^{\\ell}=0$ when $\\ell\\neq\\left\\|t\\right\\|$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\geq1}\\alpha_{t}^{\\ell}=\\infty\\quad a n d\\quad\\sum_{t\\geq1}(\\alpha_{t}^{\\ell})^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\{M_{t+1}\\}_{t\\ge1}$ be a square-integrable margingale difference sequence w.r.t. $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ such that $\\mathbb{E}[M_{t+1}^{2}\\mid\\dot{\\mathcal{F}}_{t}\\big]\\leq K(1+\\|X_{t}\\|^{2})$ for some constant $K$ . Fix any $\\ell\\in{\\mathsf{L}}$ , Consider the iterative process $\\{X_{k}^{\\ell}\\}_{k\\ge1}$ , where $X_{1}$ is arbitrary and for $k\\geq1$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nX_{t+1}^{\\ell}=(1-\\alpha_{t}^{\\ell})X_{t}^{\\ell}+\\alpha_{t}^{\\ell}\\big[h(S_{t})+M_{t+1}\\big].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, the sequence $\\{X_{t}^{\\ell}\\}_{t\\ge1}$ converges almost surely to the following limit ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}X_{t}^{\\ell}=\\sum_{s\\in\\mathsf{S}}h(s)\\zeta^{\\ell}(s),\\quad a.s.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "PROOF Note that the learning rates used here can be viewed as the learning rates of $L$ separated stochastic iterations on a common timescale $t$ . Each separate stochastic iteration $\\ell\\in{\\mathsf{L}}$ is actually only updated once every $L$ steps on the timescale $t$ . Because of the condition $\\alpha_{t}^{\\ell}=0$ when $\\ell\\neq\\left\\|t\\right\\|$ , each update is followed by $L-1$ \u201cpseudo\u201d-updates where the learning rate is 0. Therefore, each $X^{\\ell}$ is updated only once every $L$ steps on timescale $t$ . ", "page_idx": 24}, {"type": "text", "text": "The result then follows immediately from Thm. 4 by considering the process $\\{S_{t}\\}_{t\\ge1}$ every $L$ steps for each $\\ell\\in{\\mathsf{L}}$ . 2 ", "page_idx": 24}, {"type": "text", "text": "E Thm. 1: Convergence of periodic Q-learning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The high-level idea of the proof is similar to [KY22] for ASQL when the agent state is a finite window of past observations and action. The key observation of [KY22] is the following: Consider an iterative process $X_{t+1}\\,=\\,(1-\\alpha_{t})X_{t}+\\alpha_{t}{\\dot{U}}_{t}$ with the learning rates $\\alpha_{t}\\,=\\,1/(1+t)$ . Then, $\\begin{array}{r}{X_{t+1}=(X_{0}+\\sum_{\\tau=1}^{t}U_{t})/(1+t)}\\end{array}$ . Then, if the process $\\{U_{t}\\}_{t\\ge1}$ has an ergodic limit (e.g., when $\\{U_{t}\\}_{t\\ge1}$ is a fun ction of a Markov chain, see Thm. 3), the process $\\{X_{t}\\}_{t\\ge1}$ converges to the ergodic limit of $\\{U_{t}\\}_{t\\ge1}$ . We follow a similar idea but with the following changes: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Instead of assuming \u201caveraging\u201d learning rates (i.e., reciprocal of the number of visits), we allow for general learning rates of Assm. 1.   \n\u2022 We account for the fact that that the \u201cnoise\u201d is periodic. ", "page_idx": 25}, {"type": "text", "text": "The rest of the analysis then follows along the standard argument of convergence of Qlearning [JSJ94; KY22; DY24]. ", "page_idx": 25}, {"type": "text", "text": "Define the error function $\\Delta_{t+1}^{\\ell}:=Q_{t+1}^{\\ell}-Q_{\\mu}^{\\ell}$ , for all $\\ell\\in{\\mathsf{L}}$ . To prove Thm. 1, it suffices to prove that $\\|\\Delta_{t}^{\\ell}\\|\\to0$ for all $\\ell\\in{\\mathsf{L}}$ , where $\\left\\Vert\\cdot\\right\\Vert$ is the supremum-norm. The proof proceeds in three steps. ", "page_idx": 25}, {"type": "text", "text": "E.1 Step 1: State splitting of the error function ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Define $V_{t}^{\\ell}(z):=\\textstyle\\operatorname*{max}_{a\\in\\mathsf{A}}Q_{t}^{\\ell}(z,a)$ and $V_{\\mu}^{\\ell}(z):=\\operatorname*{max}_{a\\in\\mathsf{A}}Q_{\\mu}^{\\ell}(z,a)$ , for all $\\ell\\in{\\mathsf{L}}$ , $z\\in{Z}$ . We can combine (PASQL), (1), and (2) as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta_{t+1}^{\\ell}(z,a)=(1-\\alpha_{t}^{\\ell}(z,a))\\Delta_{t}^{\\ell}(z,a)+\\alpha_{t}^{\\ell}(z,a)\\big[U_{t}^{\\ell,0}(z,a)+U_{t}^{\\ell,1}(z,a)+U_{t}^{\\ell,2}(z,a)\\big]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{t}^{\\ell,0}(z,a):=\\left[r(S_{t},A_{t})-r_{\\mu}^{\\ell}(z,a)\\right]\\mathbb{1}_{\\{Z_{t}=z,A_{t}=a\\}},}\\\\ &{U_{t}^{\\ell,1}(z,a):=\\left[\\gamma V_{\\mu}^{\\mathbb{I}\\ell+1}(Z_{t+1})-\\gamma\\sum_{z^{\\prime}\\in\\mathbb{Z}}P_{\\mu}^{\\ell}(z^{\\prime}|z,a)V_{\\mu}^{\\mathbb{I}\\ell+1\\mathbb{I}}(z^{\\prime})\\right]\\mathbb{1}_{\\left\\{Z_{t}=z,A_{t}=a\\right\\}},}\\\\ &{U_{t}^{\\ell,2}(z,a):=\\gamma V_{t}^{\\mathbb{I}\\ell+1\\mathbb{I}}(Z_{t+1})-\\gamma V_{\\mu}^{\\mathbb{I}\\ell+1\\mathbb{I}}(Z_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that we have added extra indicator functions in the $U_{t}^{\\ell,i}(z,a)$ terms, $i\\in\\{0,1\\}$ . This does not change the value of $\\alpha_{t}^{\\ell}(z,a)U_{t}^{\\ell,i}(z,a)$ because the learning rates have the property that $\\alpha_{t}^{\\ell}(z,a)=0$ if $(\\ell,\\ \\stackrel{\\cdot}{z},a)\\neq(\\mathbb{I}t],z_{t},\\stackrel{\\cdot}{a_{t}})$ (see Assm. 1). ", "page_idx": 25}, {"type": "text", "text": "For each $\\ell\\in{\\mathsf{L}}$ , Eq. (16) may be viewed as a linear system with state $\\Delta_{t+1}^{\\ell}$ and three inputs $U_{t}^{\\ell,0},U_{t}^{\\ell,1}$ and $U_{t}^{\\ell,2}$ . We exploit the linearity of the system and split the state into three components: $\\Delta_{t+1}^{\\ell}=$ $X_{t+1}^{\\ell,0}+X_{t+1}^{\\ell,1}+X_{t+1}^{\\ell,2}$ , where the three components evolve as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t+1}^{\\ell,i}(z,a)=(1-\\alpha_{t}^{\\ell}(z,a))X_{t}^{\\ell,i}(z,a)+\\alpha_{t}^{\\ell}(z,a)U_{t}^{\\ell,i}(z,a),\\quad i\\in\\{0,1,2\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Linearity implies that (16) is equivalent to (17). We will now separately show that $\\|X_{t}^{\\ell,0}\\|\\,\\to\\,0$ , $\\vert\\vert X_{t}^{\\ell,1}\\vert\\vert\\rightarrow0$ and $\\|X_{t}^{\\ell,2}\\|\\to0$ . ", "page_idx": 25}, {"type": "text", "text": "E.2 Step 2: Convergence of component $X_{t}^{\\ell,0}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Fix $(\\ell,z_{\\circ},a_{\\circ})\\in\\mathsf{L}\\times\\mathsf{Z}\\times\\mathsf{A}$ and define ", "page_idx": 25}, {"type": "equation", "text": "$$\nh_{r}(S_{t},Z_{t},A_{t};\\ell,z_{0},a_{0})=\\left[r(S_{t},A_{t})-r_{\\mu}^{\\ell}(z_{0},a_{0})\\right]\\mathbb{1}_{\\left\\{Z_{t}=z_{0},A_{t}=a_{0}\\right\\}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the process $\\{X_{t}^{\\ell,0}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ is given by the stochastic iteration ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t+1}^{\\ell,0}(z_{\\circ},a_{\\circ})=(1-\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ}))X_{t}^{\\ell,0}(z_{\\circ},a_{\\circ})+\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ})h_{r}(S_{t},Z_{t},A_{t};\\ell,z_{\\circ},a_{\\circ}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is of the form (15). The process $\\{(S_{t},Z_{t},A_{t})\\}_{t\\geq1}$ is a periodic Markov chain and the learning rates $\\{\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ satisfy the conditions of Prop. 4 due to Assm. 1. Therefore, Prop. 4 implies ", "page_idx": 25}, {"type": "text", "text": "that $\\{X_{t}^{\\ell,0}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ converges a.s. to the following limit ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t\\rightarrow\\infty}{\\mathrm{lim}}X_{t}^{\\ell,0}(z_{0},a_{0})=\\underset{s,z,u\\in\\mathbb{S}\\times\\times\\Sigma\\times\\Lambda}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(s,z,a)h_{r}(s,z,a;\\ell,z_{0},a_{0})}\\\\ &{=\\underset{s,z,u\\in\\mathbb{S}\\times\\Sigma\\times\\Lambda}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(s,z,a)\\mathbb{I}_{\\{s=z_{0},a=a_{0}\\}}\\Big[r(s,a)-r_{\\mu}^{\\ell}(z_{0},a_{0})\\Big]}\\\\ &{=\\left[\\underset{s\\in\\mathbb{S}}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(s,z_{0},a_{0})r(s,a_{0})\\right]-\\int_{\\mu}^{\\ell}(z_{0},a_{0})r_{\\mu}^{\\ell}(z_{0},a_{0})}\\\\ &{=\\left[\\underset{s\\in\\mathbb{S}}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(s,z_{0},a_{0})r(s,a_{0})\\right]-\\left[\\underset{s\\in\\mathbb{S}}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(z_{0},a_{0})\\zeta_{\\mu}^{\\ell}(s|z_{0})r(s,a_{0})\\right]}\\\\ &{=\\left[\\underset{s\\in\\mathbb{S}}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(s,z_{0})\\mu(a_{0}|z_{0})r(s,a_{0})\\right]-\\left[\\underset{s\\in\\mathbb{S}}{\\sum_{\\substack{\\mathscr{G}}}}\\zeta_{\\mu}^{\\ell}(z_{0})\\mu(a_{0}|z_{0})\\zeta_{\\mu}^{\\ell}(s|z_{0})r(s,a_{0})\\right]}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, for all $(\\ell,z_{\\circ},a_{\\circ})$ , the process $\\{X_{t}^{\\ell,0}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ converges to zero almost surely. ", "page_idx": 26}, {"type": "text", "text": "E.3 Step 3: Convergence of component $X_{t}^{\\ell,1}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let $W_{t}$ denote the tuple $(S_{t},Z_{t},A_{t},S_{t+1},Z_{t+1},A_{t+1})$ . Note that $\\{W_{t}\\}_{t\\ge1}$ is also a periodic Markov chain and converges to a cyclic limiting distribution $\\bar{\\zeta}_{\\mu}^{\\ell}$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\zeta}_{\\mu}^{\\ell}(s,z,a,s^{\\prime},z^{\\prime},a^{\\prime})=\\zeta_{\\mu}^{\\ell}(s,z,a)\\sum_{y^{\\prime}\\in\\mathbb{Y}}P(s^{\\prime},y^{\\prime}|s,a)\\mathbb{1}_{\\left\\{z^{\\prime}=\\phi(z,y^{\\prime},a)\\right\\}}\\mu(a^{\\prime}|z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We use $\\bar{\\zeta_{\\mu}}^{\\ell}(s,z,a,\\mathsf{S},\\mathcal{Z},\\mathcal{A})$ to denote the marginalization over the \u201cfuture states\u201d and a similar notation for other marginalizations. Note that $\\bar{\\zeta_{\\mu}}^{\\ell}(s,z,a,{\\mathsf S},{\\mathcal Z},{\\cal A})=\\zeta_{\\mu}^{\\ell}(s,z,a)$ . ", "page_idx": 26}, {"type": "text", "text": "Fix $(\\ell,z_{\\circ},a_{\\circ})\\in\\mathsf{L}\\times\\mathsf{Z}\\times\\mathsf{A}$ and define ", "page_idx": 26}, {"type": "equation", "text": "$$\nh_{P}(W_{t};\\ell,z_{\\circ},a_{\\circ})=\\left[\\gamma V_{\\mu}^{[\\ell+1]}(Z_{t+1})-\\gamma\\sum_{\\bar{z}\\in\\mathbb{Z}}P_{\\mu}^{\\ell}(\\bar{z}|z_{\\circ},a_{\\circ})V_{\\mu}^{[\\ell+1]}(\\bar{z})\\right]\\mathbb{1}_{\\left\\{Z_{t}=z_{\\circ},A_{t}=a_{\\circ}\\right\\}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then the process $\\{X_{t}^{\\ell,1}(z,a)\\}_{t\\geq1}$ is given by the stochastic iteration ", "page_idx": 26}, {"type": "equation", "text": "$$\nX_{t+1}^{\\ell,1}(z_{\\circ},a_{\\circ})=(1-\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ}))X_{t}^{\\ell,1}(z_{\\circ},a_{\\circ})+\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ})h_{P}(W_{t};\\ell,z_{\\circ},a_{\\circ}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is of the form (15). As argued earlier, the process $\\{W_{t}\\}_{t\\ge1}$ is a periodic Markov chain. Due to Assm. 1, the learning rate $\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ})$ is measurable with respect to the sigma-algebra generated by $(Z_{1:t},A_{1:t})$ and is therefore also measurable with respect to the sigma-algebra generated by $W_{1:t}$ . Combining this with Prop. 4 implies that the learning rates $\\{\\alpha_{t}^{\\ell}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ satisfy the conditions of Prop. 4. Therefore, Prop. 4 implies that $\\{X_{t}^{\\ell,1}(z_{\\circ},a_{\\circ})\\}_{t\\geq1}$ converges a.s. to the following limit ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{lim}_{t\\to\\infty}X_{t}^{\\ell,1}(z_{0},a_{0})}}\\\\ &{=\\sum_{\\scriptstyle s,z,a\\in\\mathbb{S}\\times\\mathbb{Z}\\times\\mathbb{A}}\\ \\bar{\\zeta}_{\\mu}^{\\ell}(s,z,a,s^{\\prime},z^{\\prime},a^{\\prime})h_{P}(s,z,a,s^{\\prime},z^{\\prime},a^{\\prime};\\ell,z_{0},a_{0})}\\\\ &{\\quad\\times_{\\scriptstyle s^{\\prime},z^{\\prime},a^{\\prime}\\in\\mathbb{S}\\times\\mathbb{Z}\\times\\mathbb{A}}^{s,z,a\\in\\mathbb{S}\\times\\mathbb{Z}\\times\\mathbb{A}}}\\\\ &{=\\quad\\sum_{\\scriptstyle s,z,a\\in\\mathbb{S}\\times\\mathbb{Z}\\times\\mathbb{A}}\\bar{\\zeta}_{\\mu}^{\\ell}(s,z,a,s^{\\prime},z^{\\prime},a^{\\prime})\\left[\\gamma V_{\\mu}^{\\mathbb{I}(+1)}(z^{\\prime})-\\gamma\\displaystyle\\sum_{\\bar{z}\\in\\mathbb{Z}}P_{\\mu}^{\\ell}(\\bar{z}|z_{0},a_{0})V_{\\mu}^{\\mathbb{I}(+1)}(\\bar{z})\\right]\\mathbb{1}_{\\{z=z_{0},a=a}}}\\\\ &{\\quad\\times_{\\scriptstyle s,z^{\\prime},a^{\\prime}\\in\\mathbb{S}\\times\\mathbb{Z}\\times\\mathbb{A}}}\\\\ &{=\\gamma\\displaystyle\\sum_{z^{\\prime},z^{\\prime}\\in\\mathbb{Z}}\\bar{\\zeta}_{\\mu}^{\\ell}(\\mathbb{S},z_{0},a_{0},\\mathbb{S},z^{\\prime},\\mathbb{A})V_{\\mu}^{\\mathbb{I}(+1)}(z^{\\prime})\\right]-\\left[\\gamma\\bar{\\zeta}_{\\mu}^{\\ell}(\\mathbb{S},z_{0},a_{0},\\mathbb{S},\\mathbb{Z},A)\\displaystyle\\sum_{\\bar{z}\\in\\mathbb{Z}}P_{\\mu}^{\\ell}(\\bar{z}|z_{0},a_{0})V_{\\mu}^{\\mathbb{I}(+1)}(\\bar{z})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last step follows from the fact that $\\begin{array}{r l r}{\\bar{\\zeta}_{\\mu}^{\\ell}(\\mathsf{S},z_{\\mathsf{o}},a_{\\mathsf{o}},\\mathsf{S},\\mathcal{Z},\\mathcal{A})}&{=}&{\\zeta_{\\mu}^{\\ell}(z_{\\mathsf{o}},a_{\\mathsf{o}})}\\end{array}$ and $\\bar{\\zeta}_{\\mu}^{\\ell}(\\mathsf{S},z_{\\circ},a_{\\circ},\\mathsf{S},z^{\\prime},\\mathsf{A})=\\zeta_{\\mu}^{\\ell}(z_{\\circ},a_{\\circ})P_{\\mu}^{\\ell}(z^{\\prime}|z_{\\circ},a_{\\circ})$ . ", "page_idx": 26}, {"type": "text", "text": "E.4 Step 4: Convergence of component $X_{t}^{\\ell,2}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The remaining analysis is similar to corresponding step in the standard convergence proof of Qlearning and its variations [JSJ94; KY22; DY24]. In this section, we use $\\lVert\\cdot\\rVert$ to denote the supremum norm, i.e., $\\|\\cdot\\|_{\\infty}$ . ", "page_idx": 27}, {"type": "text", "text": "In the previous step, we have shown that $\\lVert X_{t}^{\\ell,i}\\rVert\\,\\to\\,0$ a.s., for $i\\,\\in\\,\\{0,1\\}$ . Thus, we have that $\\|X_{t}^{\\ell,0}+X_{t}^{\\ell,1}\\|\\to0$ a.s. Arbitrarily fix an $\\epsilon>0$ . Therefore, there exists a set $\\Omega^{1}$ of measure one and a constant $T(\\omega,\\epsilon)$ such that for $\\omega\\in\\Omega^{1}$ , all $t>T(\\omega,\\epsilon)$ , and $(\\ell,z,a)\\in\\mathsf{L}\\times\\mathsf{Z}\\times\\mathsf{A}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nX_{t}^{\\ell,0}(z,a)+X_{t}^{\\ell,1}(z,a)<\\epsilon.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now pick a constant $C$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa:=\\gamma\\left(1+\\frac{1}{C}\\right)<1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose for some $t>T(\\omega,\\epsilon)$ , $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert>C\\epsilon$ . Then, for $(z,a)\\in\\mathsf{Z}\\times\\mathsf{A}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{t}^{\\ell,2}(z,a)=\\gamma V_{t}^{[\\ell+1]}(Z_{t+1})-\\gamma V_{\\mu}^{[\\ell+1]}(Z_{t+1})}\\\\ &{\\qquad\\qquad=\\gamma\\operatorname*{max}_{a\\in\\mathbb{A}}Q_{t}^{[\\ell+1]}(Z_{t+1},a)-\\operatorname*{max}_{a^{\\prime}\\in\\mathbb{A}}Q_{\\mu}^{[\\ell+1]}(Z_{t+1},a^{\\prime})}\\\\ &{\\qquad\\qquad\\leq\\gamma\\operatorname*{max}_{a\\in\\mathbb{A}}\\left\\{Q_{t}^{[\\ell+1]}(Z_{t+1},a)-\\gamma Q_{\\mu}^{[\\ell+1]}(Z_{t+1},a)\\right\\}}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}\\gamma\\|Q_{t}^{[\\ell+1]}-Q_{\\mu}^{[\\ell+1]}\\|=\\gamma\\|\\Delta_{t}^{[\\ell+1]}\\|}\\\\ &{\\qquad\\qquad\\leq\\gamma\\|X_{t}^{[\\ell+1],0}+X_{t}^{[\\ell+1],1}\\|+\\gamma\\|X_{t}^{[\\ell+1],2}\\|\\overset{(b)}{\\leq}\\gamma\\epsilon+\\gamma\\|X_{t}^{[\\ell+1],2}\\|}\\\\ &{\\qquad\\qquad\\leq\\gamma\\left(1+\\frac{1}{C}\\right)\\operatorname*{max}_{\\ell\\in\\mathbb{A}}\\|X_{t}^{\\ell,2}\\|\\overset{(d)}{=}\\kappa_{\\ell\\in\\mathbb{A}}\\|X_{t}^{\\ell,2}\\|\\overset{(d)}{<}\\underset{\\ell\\in\\mathbb{C}}{\\operatorname*{max}}\\|X_{t}^{\\ell,2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $(a)$ follows from the fact that an upper bound is obtained by maximizing over all realizations of $Z_{t+1}$ , $(b)$ follows from (18), $(c)$ follows from the fact that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\|X_{t}^{\\ell,2}\\|>C\\epsilon,\\,($ $(d)$ follows from (19). Thus, for any $t>T(\\omega,\\epsilon)$ and $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\|X_{t}^{\\ell,2}\\|>C\\epsilon$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{X_{t+1}^{\\ell,2}(z,a)=(1-\\alpha_{t}^{\\ell}(z,a))X_{t}^{\\ell,2}(z,a)+\\alpha_{t}^{\\ell}(z,a)U_{t}^{\\ell,2}(z,a)<\\operatorname*{max}_{\\ell\\in{\\bf L}}\\lVert X_{t}^{\\ell,2}\\rVert}\\\\ &{}&{\\implies\\underset{\\ell\\in{\\bf L}}{\\operatorname*{max}}\\lVert X_{t+1}^{\\ell,2}\\rVert<\\underset{\\ell\\in{\\bf L}}{\\operatorname*{max}}\\lVert X_{t}^{\\ell,2}\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, when $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert\\,>\\,C\\epsilon$ , it decreases monotonically with time. Hence, there are two possibilities: either (i) $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ always remains above $C\\epsilon$ ; or (ii) it goes below $C\\epsilon$ at some stage. We consider these two possibilities separately. ", "page_idx": 27}, {"type": "text", "text": "E.4.1 Possibility (i): $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ always remains above $C\\epsilon$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We will show that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ cannot remain above $C\\epsilon$ forever. We first start with a basic result for random iterations. This is a self-contained result, so we reuse some of the variables used in the rest of the paper. ", "page_idx": 27}, {"type": "text", "text": "Lemma 2 Let $\\{X_{t}\\}_{t\\ge1}$ , $\\{Y_{t}\\}_{t\\ge1}$ , and $\\{\\alpha_{t}\\}_{t\\ge1}$ be non-negative sequences adapted to a filtration $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ that satisfy the following: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{X_{t+1}\\leq(1-\\alpha_{t})X_{t},}}\\\\ {{Y_{t+1}\\leq(1-\\alpha_{t})Y_{t}+\\alpha_{t}c,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where c is a constant. Suppose ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}\\alpha_{t}=\\infty\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, the sequence $\\{X_{t}\\}_{t\\ge1}$ converges to zero almost surely and the sequence $\\{{Y}_{t}\\}_{t\\ge1}$ converges to c almost surely. ", "page_idx": 27}, {"type": "text", "text": "PROOF The iteration (21a) implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{t+1}\\leq\\Big[(1-\\alpha_{1})\\cdot\\cdot\\cdot(1-\\alpha_{t})\\Big]X_{1}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Condition (22) implies that the term in the square brackets converges to zero. Therefore, $X_{t}\\to0$ . Observe that the iteration (21b) can be rewritten as ", "page_idx": 28}, {"type": "equation", "text": "$$\nY_{t+1}-c\\leq(1-\\alpha_{t})(Y_{t}-c)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is of the form (21a). Therefore, $Y_{t}-c\\rightarrow0$ . ", "page_idx": 28}, {"type": "text", "text": "We will now prove that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ cannot remain above $C\\epsilon$ forever. The proof is by contradiction. Suppose $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ remains above $C\\epsilon$ forever. As argued earlier, this implies that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ , $t\\,\\geq\\,T(\\omega,\\epsilon)$ , is a strictly decreasing sequence, so it must be bounded from above. Let $B^{(0)}$ be such that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert\\,\\le\\,B^{(0)}$ for all $t\\ge T(\\omega,\\epsilon)$ . Eq. (20b) implies that $\\lVert U_{t}^{\\ell,2}\\rVert<\\kappa B^{(0)}$ . Then, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\ell\\in\\mathbb{L}}{\\operatorname*{max}}\\,X_{t+1}^{\\ell,2}(z,a)\\leq(1-\\alpha_{t}^{\\ell}(z,a))\\underset{\\ell\\in\\mathbb{L}}{\\operatorname*{max}}\\|X_{t}^{\\ell,2}\\|+\\alpha_{t}^{\\ell}(z,a)\\underset{\\ell\\in\\mathbb{L}}{\\operatorname*{max}}\\|U_{t}^{\\ell,2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq(1-\\alpha_{t}^{\\ell}(z,a))\\underset{\\ell\\in\\mathbb{L}}{\\operatorname*{max}}\\|X_{t}^{\\ell,2}\\|+\\alpha_{t}^{\\ell}(z,a)\\kappa\\underset{\\ell\\in\\mathbb{L}}{\\operatorname*{max}}\\|X_{t}^{\\ell,2}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies that $\\mathrm{max}_{\\ell\\in\\mathsf{L}}\\|X_{t}^{\\ell,2}\\|\\leq\\|M_{t}^{\\ell,(0)}\\|$ , where $\\{M_{t}^{\\ell,(0)}\\}_{t\\ge T(\\omega,\\epsilon)}$ is a sequence given by ", "page_idx": 28}, {"type": "equation", "text": "$$\nM_{t+1}^{\\ell,(0)}(z,a)\\le(1-\\alpha_{t}^{\\ell}(z,a))M_{t}^{\\ell,(0)}(z,a)+\\alpha_{t}^{\\ell}(z,a)\\kappa B^{(0)},\\quad\\forall(z,a)\\in\\mathsf{Z}\\times\\mathsf{A}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lem. 2 implies that $M_{t}^{\\ell,(0)}(z,a)\\rightarrow\\kappa B^{(0)}$ and hence $\\lVert M_{t}^{\\ell,(0)}\\rVert\\to\\kappa B^{(0)}$ . Now pick an arbitrary $\\bar{\\epsilon}\\;\\in\\;(0,(1\\,-\\,\\kappa)C\\epsilon)$ . Thus, there exists a time $T^{(1)}\\,=\\,T^{(1)}(\\omega,\\epsilon,\\bar{\\epsilon})$ such that for all $t\\,>\\,T^{(1)}$ , $\\lVert M_{t}^{\\ell,(0)}\\rVert\\,\\le\\,B^{(1)}\\,:=\\,\\kappa B^{(0)}+\\bar{\\epsilon}.$ . Since $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ is bounded by $\\Vert M_{t}^{\\ell,(0)}\\Vert$ , this implies that for all $t>T^{(1)}$ , $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\|X_{t}^{\\ell,2}\\|\\leq B^{(1)}$ and, by (20b), $\\lVert U_{t}^{\\ell,2}\\rVert\\,\\le\\,\\kappa B^{(1)}$ . By repeating the above argument, there exists a time $T^{(2)}$ such that for all $t\\geq T^{(2)}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert\\le B^{(2)}:=\\kappa B^{(1)}+\\bar{\\epsilon}=\\kappa^{2}B^{(0)}+\\kappa\\bar{\\epsilon}+\\bar{\\epsilon},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and so on. By (19), $\\kappa<1$ and $\\bar{\\epsilon}$ is chosen to be less than $C\\epsilon$ . So eventually, $B^{(m)}:=\\kappa^{m}B^{(0)}+$ $\\kappa^{m-1}\\bar{\\epsilon}+\\cdot\\cdot\\cdot+\\bar{\\epsilon}$ must get below $C\\epsilon$ for some $m$ , contradicting the assumption that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ remains above $C\\epsilon$ forever. ", "page_idx": 28}, {"type": "text", "text": "E.4.2 Possibility (ii): $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert$ goes below $C\\epsilon$ at some stage ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Suppose that there is some $t>T(\\omega,\\epsilon)$ such that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert<C\\epsilon$ . Then (20a) implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|U_{t}^{\\ell,2}\\|\\leq\\gamma\\|X_{t}^{\\[\\ell+1],0}+X_{t}^{[\\ell+1],1}\\|+\\gamma\\|X_{t}^{[\\ell+1],2}\\|\\leq\\gamma\\epsilon+\\gamma C\\epsilon<C\\epsilon\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality uses (19). Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\ell\\in\\mathbb{L}}X_{t+1}^{\\ell,2}(z,a)\\leq(1-\\alpha_{t}^{\\ell}(z,a))\\operatorname*{max}_{\\ell\\in\\mathbb{L}}\\|X_{t}^{\\ell,2}\\|+\\alpha_{t}^{\\ell}(z,a)\\operatorname*{max}_{\\ell\\in\\mathbb{L}}\\|U_{t}^{\\ell,2}\\|<C\\epsilon\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality uses the fact that both $\\|U_{t}^{\\ell,2}\\|$ and $\\mathrm{max}_{\\ell\\in\\mathsf{L}}\\|X_{t+1}^{\\ell,2}\\|$ are both below $C\\epsilon$ Thus, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\ell\\in\\mathsf{L}}X_{t+1}^{\\ell,2}(z,a)<C\\epsilon.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, once $\\mathrm{max}_{\\ell\\in\\mathsf{L}}\\|X_{t+1}^{\\ell,2}\\|$ goes below $C\\epsilon$ , it stays there. ", "page_idx": 28}, {"type": "text", "text": "E.4.3 Implication ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We have show that for sufficiently large $t>T(\\omega,\\epsilon)$ , $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}X_{t}^{\\ell,2}(z,a)<C\\epsilon$ . Since $\\epsilon$ is arbitrary, this means that for all realizations $\\omega\\in\\Omega^{1}$ , $\\mathrm{max}_{\\ell\\in\\mathsf{L}}\\|X_{t}^{\\ell,2}\\|\\to0$ . Thus, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert=0,\\quad a.s.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E.5 Putting everything together ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Recall that we defined $\\Delta_{t}^{\\ell}=Q_{t}^{\\ell}-Q_{\\mu}$ and in Step 1, we split $\\Delta_{t}^{\\ell}=X_{t}^{\\ell,0}+X_{t}^{\\ell,1}+X_{t}^{\\ell,2}$ . Steps 2 and 3 together show that $\\|X_{t}^{\\ell,0}+X_{t}^{\\ell,1}\\|\\to0$ , a.s. and Step 3 (23) shows us that $\\operatorname*{max}_{\\ell\\in\\mathsf{L}}\\lVert X_{t}^{\\ell,2}\\rVert\\to0.$ , a.s. Thus, by the triangle inequality, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\|\\Delta_{t}^{\\ell}\\|\\leq\\operatorname*{lim}_{t\\to\\infty}\\|X_{t}^{\\ell,0}+X_{t}^{\\ell,1}\\|+\\operatorname*{lim}_{t\\to\\infty}\\|X_{t}^{\\ell,2}\\|=0,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which establishes that $Q_{t}^{\\ell}\\to Q_{\\mu}$ , a.s. ", "page_idx": 29}, {"type": "text", "text": "F Thm. 2: Sub-optimality gap ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The high-level idea of proving Thm. 2 is as follows. Thm. 1 shows that PASQL converges to a cyclic limit, which is the solution to a periodic MDP. Thus, the question of characterizing the suboptimality gap is equivalent to the following. Given a PODMP $\\mathcal{P}$ , let $\\mathcal{M}$ be a periodic agent-state based model that approximates the reward and the dynamics of $\\mathcal{P}$ (in the sense of an approximate information state, as defined in [Sub $+22]$ ). Let $\\hat{\\pi}^{\\star}$ be the optimal policy of model $\\mathcal{M}$ . What is the sub-optimality gap when $\\hat{\\pi}^{\\star}$ is used in the original POMDP $\\mathcal{P}$ ? ", "page_idx": 29}, {"type": "text", "text": "To answer such questions, a general framework of approximate information states was developed in $[\\mathrm{Sub}+22]$ for both finite and infinite horizon models. However, we cannot directly used the results of $[\\mathrm{Sub}+22]$ because the infinite horizon results there were restricted to stationary policies, while we are interested in the sub-optimality gap of periodic policies. ", "page_idx": 29}, {"type": "text", "text": "Nonetheless, Thm. 2 can be proved by building on the existing results of $[\\mathrm{Sub}+22]$ . In particular, we start by looking at finite horizon model rather than infinite horizon model. Then, as per $[\\mathrm{Sub}+22$ , Definition 7], the agent state process may be viewed as an approximate information state with approximation errors $\\{(\\bar{\\varepsilon}_{t},\\delta_{t})\\}_{t\\geq1}$ , where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{t}=\\underset{h_{t},a_{t}}{\\operatorname*{sup}}\\Big|\\mathbb{E}[R_{t}\\mid h_{t},a_{t}]-\\underset{s\\in\\mathbb{S}}{\\sum}r(s,a)\\zeta_{\\mu}^{\\mathbb{I}t}](s\\mid z,a)\\Big|,}\\\\ &{\\delta_{t}=\\underset{h_{t},a_{t}}{\\operatorname*{sup}}d_{\\mathfrak{F}}(\\mathbb{P}(Z_{t+1}=\\cdot\\mid h_{t},a_{t}),P_{\\mu}^{\\mathbb{I}t}](Z_{t+1}=\\cdot\\vert\\sigma_{t}(h_{t}),a_{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\begin{array}{r}{V_{t,T}^{\\vec{\\pi}}(h_{t})=\\mathbb{E}^{\\vec{\\pi}}\\left[\\sum_{\\tau=t}^{T}\\gamma^{\\tau-1}R_{\\tau}\\ |\\ h_{t}\\right]}\\end{array}$ denote the value function of policy $\\vec{\\pi}$ for the finite horizon model starting at history $h_{t}$ at time $t$ . Let $V_{t,T}^{\\star}(h_{t})\\;:=\\;\\operatorname*{sup}_{\\vec{\\pi}}V_{t,T}^{\\vec{\\pi}}(h_{t})$ denote the optimal value function, where the optimization is over all history dependent policies. Moreover, let $\\hat{V}_{t,T}(z_{t})$ denote the optimal value function for the periodic MDP model constructed in Thm. 1. Let $\\vec{\\pi}_{\\mu}$ denote the history-based policy defined in Sec. 2.4. ", "page_idx": 29}, {"type": "text", "text": "Then, from $[\\mathrm{Sub}+22$ , Theorem 9] we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h_{t}}\\bigl[V_{t,T}^{\\star}(h_{t})-V_{t,T}^{\\vec{\\pi}_{\\mu}}(h_{t})\\bigr]\\leq2\\sum_{\\tau=t}^{T}\\gamma^{\\tau-t}\\bigl[\\varepsilon_{\\tau}+\\gamma\\delta_{\\tau}\\rho_{\\tilde{\\mathfrak{F}}}(\\hat{V}_{\\tau+1,T})\\bigr]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we set $\\hat{V}_{T+1,T}(z)\\equiv0$ for convenience. ", "page_idx": 29}, {"type": "text", "text": "The following hold when we let $T\\to\\infty$ . ", "page_idx": 29}, {"type": "text", "text": "\u2022 Since $R_{t}$ is uniformly bounded, $V_{t,T}^{\\star}(h_{t})\\rightarrow V_{t}^{\\star}(h_{t})$ as $T\\rightarrow\\infty$ .   \n\u2022 By the same argument, $V_{t,T}^{\\vec{\\pi}_{\\mu}}(h_{t})\\rightarrow V_{t}^{\\vec{\\pi}_{\\mu}}(h_{t})$ as $T\\rightarrow\\infty$ .   \n\u2022 By standard results for periodic MDPs (see App. C), $\\hat{V}_{t,T}\\to V_{\\mu}^{\\[t]}$ as $T\\rightarrow\\infty$ .   \n\u2022 By definition, \u03b5t \u2264\u03b5t t and \u03b4t \u2264\u03b4t t  . ", "page_idx": 29}, {"type": "text", "text": "Therefore, by taking $T\\to\\infty$ in (24), we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h_{t}}\\bigl[V_{t}^{\\star}(h_{t})-V_{t}^{\\vec{\\pi}_{\\mu}}(h_{t})\\bigr]\\leq2\\sum_{\\tau=t}^{\\infty}\\gamma^{\\tau-t}\\bigl[\\varepsilon_{\\tau}^{[\\tau]}+\\gamma\\delta_{\\tau}^{[\\tau]}\\rho_{\\mathfrak{F}}(\\hat{V}^{[\\tau+1]}\\|)\\bigr].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The result then follows from observing that for $\\tau\\in\\mathsf{T}(t,\\ell)$ , $\\epsilon_{t}^{\\ell}$ and $\\delta_{t}^{\\ell}$ are non-decreasing sequences. ", "page_idx": 29}, {"type": "text", "text": "G Policy evaluation of an agent-state based policy ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The performance of any agent-state based policy can be evaluated via a slight generalization of \u201ccross-product MDP\u201d method originally presented in [Pla77]. This method has been rediscovered in slightly different forms multiple times [Lit96; Cas98; Hau97; Han98]. ", "page_idx": 30}, {"type": "text", "text": "The key intuition is Lem. 1. Thus, for any agent-state based policy, $\\{(S_{t},Z_{t})\\}_{t\\geq1}$ is a Markov chain. The only difference in our setting is that the Markov chain is time-periodic. Thus, for any periodic agent-state based policy $(\\pi^{0},\\cdot\\cdot\\cdot,\\pi^{L-1})$ , we can identify the periodic rewards $(\\bar{r}_{,}^{0}\\ldots,\\bar{\\bar{r}}^{L-1})$ and periodic dynamics $(\\bar{P}^{0},\\ldots,\\bar{P}^{L-1})$ (which depend on $\\pi$ but we are not carrying that dependence in our notation) as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{r}^{\\ell}(s,z)=\\displaystyle\\sum_{a\\in\\mathsf{A}}\\pi^{\\ell}(a|z)r(s,a),\\quad\\quad\\quad\\quad}\\\\ {\\bar{P}^{\\ell}(s^{\\prime},z^{\\prime}|s,z)=\\displaystyle\\sum_{(y,a)\\in\\mathsf{Y}\\times\\mathsf{A}}\\pi^{\\ell}(a|z)P(s^{\\prime},y^{\\prime}|s,a)\\mathbb{1}_{\\{z^{\\prime}=\\phi(z,y^{\\prime},a)\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can then evaluate the performance of this time-periodic Markov chain via performance evaluation formulas for periodic MDPs (App. C). In particular, define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tilde{r}=\\bar{r}^{0}+\\gamma\\bar{P}^{0}\\bar{r}^{1}+\\cdot\\cdot\\cdot+\\gamma^{L-1}\\bar{P}^{0}\\bar{P}^{1}\\cdot\\cdot\\cdot\\bar{P}^{L-2}\\bar{r}^{L-1},}}\\\\ {{\\tilde{P}=\\bar{P}^{0}\\bar{P}^{1}\\cdot\\cdot\\cdot\\bar{P}^{L-1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "to be the $L$ -step cumulative rewards and dynamics for the time-periodic Markov chain. Then define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{V}=(1-\\gamma^{L}\\tilde{P})^{-1}\\tilde{r}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, $\\tilde{V}(s,z)$ gives the performance of periodic policy $\\pi$ when starting at initial state $(s,z)$ . If the initial state is stochastic, we can average over the initial distribution. ", "page_idx": 30}, {"type": "text", "text": "H Reproducibility information ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The hyperparameters for the numerical experiments presented in Sec. 3 are shown in Table 3. The experiments were run on a computer cluster by running jobs that requested 2-CPU nodes with $<$ 8GB memory. Each seed typically took less than 10 minutes to execute. ", "page_idx": 30}, {"type": "table", "img_path": "HmMSBhMAw4/tmp/a15e1a5e5a9cdbfd89fd54d364cec4919aa338eb8a49e7f4a782c6660ebaa281.jpg", "table_caption": ["Table 3: Hyperparameters used in Ex. 1 "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claims made have been proved theoretically in detail in the appendix.   \nIllustrative examples have also been shown to provide more clarity on the ideas involved. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper includes the limitations in the discussion section (Sec. 5). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the theoretical results in an organized manner with numbered and crossreferenced equations, theorems, lemmas, assumptions etc. We have considered the proofs in rigorous detail and explain all the details in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The information needed to reproduce the main experimental results of the paper are disclosed in further detail in App. G and App. H. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We intend to make the code open access after the review process is complete. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The main details are provided in the paper. Additional details are provided in the appendix (app. H). ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The example mentioned in the main text is run with 25 random seeds and the median and interquantile range are plotted. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The relevant information about compute resources is provided in the appendix (App. H). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with every aspect of the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in this paper is foundational research that focuses on developing theoretical results and as such, does not directly have a societal impact. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Since the paper is based on foundational research, it does not pose any direct risks that require safeguarding. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]