[{"figure_path": "IxEhb4NCvy/figures/figures_0_1.jpg", "caption": "Figure 1: SSDM. Comparison to other methods", "description": "This figure compares the performance of the proposed SSDM model with other state-of-the-art methods (LTU-AS, SALMONN, ChatGPT) on a speech pronunciation task.  The comparison is based on the clarity and understandability of the pronunciation, as assessed by human evaluation.  The results demonstrate that SSDM outperforms the other methods in terms of identifying and describing specific dysfluencies in speech, such as stutters, blocks and phonetic errors.", "section": "Introduction"}, {"figure_path": "IxEhb4NCvy/figures/figures_1_1.jpg", "caption": "Figure 2: SSDM architecture", "description": "This figure shows the architecture of the Scalable Speech Dysfluency Modeling (SSDM) system.  The input is dysfluent or normal speech, and the reference text is provided.  The system uses an acoustic encoder, adaptor, and articulatory gesture-based representation to achieve dysfluency alignment through a Connectionist Subsequence Aligner (CSA). The model employs a multimodal tokenizer and LLaMA (Large Language Model Meta AI) with LoRA (Low-Rank Adaptation) for end-to-end learning.  Self-distillation is also utilized in the process to refine the model's performance. The output is the identified dysfluencies with their corresponding timestamps.", "section": "2 Articulatory Gesture is Scalable Forced Aligner"}, {"figure_path": "IxEhb4NCvy/figures/figures_5_1.jpg", "caption": "Figure 3: LSA(LCS) delivers dysfluent alignment that is more semantically aligned.", "description": "This figure provides a visual comparison of Local Subsequence Alignment (LSA) and Global Sequence Alignment (GSA) methods for dysfluency alignment.  It uses a specific example of the word \"references\" pronounced with dysfluencies. The left side shows a graphical representation of the dysfluent phonetic alignment (red dots representing LSA and blue dots representing GSA) against the stress-free phonetic transcription of the word \"references\".  The right side provides a more detailed analysis of the differences in cost functions and alignment methods between LSA and GSA, highlighting how LSA focuses on semantically meaningful alignments, whereas GSA considers all pairs of phonemes for calculating cost and generating alignment.  This illustrative example demonstrates LSA's superior ability to accurately detect and align dysfluencies compared to GSA. ", "section": "3 Connectionist Subsequence Aligner (CSA) for Dysfluency Modeling"}, {"figure_path": "IxEhb4NCvy/figures/figures_5_2.jpg", "caption": "Figure 4: CSA", "description": "This figure illustrates the Connectionist Subsequence Aligner (CSA) architecture.  It shows how CSA approximates Local Subsequence Alignment (LSA) by incorporating the constraints of LCS (Longest Common Subsequence) into a modified forward-backward algorithm similar to CTC (Connectionist Temporal Classification). The figure highlights the concepts of \"Transition Skip\" and \"Emission Copy\" to handle non-monotonic alignments effectively while still maintaining differentiability.  It also demonstrates how dysfluencies are implicitly modeled through a discounted factor applied to previous tokens.", "section": "3 Connectionist Subsequence Aligner (CSA) for Dysfluency Modeling"}, {"figure_path": "IxEhb4NCvy/figures/figures_8_1.jpg", "caption": "Figure 5: Gestural Dysfluency Visualization", "description": "The figure visualizes dysfluency using GradCAM applied to gestural scores.  The left panel shows articulatory movements, while the right panel displays the gradient of gestural scores for the vowel 'i' (e). Negative gradient in the center indicates incorrect tongue movement direction during articulation, offering insight into the dysfluency.", "section": "6.6 Dysfluency Visualization"}, {"figure_path": "IxEhb4NCvy/figures/figures_19_1.jpg", "caption": "Figure 6: Gestures, Gestural Scores, Raw Data Visualization", "description": "This figure illustrates the process of decomposing raw articulatory data into gestures and gestural scores.  Raw articulatory data, represented as a matrix X (12 x t), contains the x and y coordinates of six articulators (upper lip, lower lip, lower incisor, tongue tip, tongue blade, and tongue dorsum) over time (t).  K-means clustering is used on 200ms segments of the articulatory data to identify a set of gestures (G, shown as a 12 x T matrix for each gesture). The number of gestures used in the paper is 40.  The resulting gestural scores (H, a 3 x t matrix) represent the duration and intensity of each gesture over time. The figure provides visual representations of the raw data, the extracted gestures, and the resulting gestural scores.", "section": "2 Articulatory Gesture is Scalable Forced Aligner"}, {"figure_path": "IxEhb4NCvy/figures/figures_20_1.jpg", "caption": "Figure 7: Illustration of Convolutive Matrix factorization", "description": "This figure illustrates the process of convolutive matrix factorization (CMF).  It shows how raw articulatory data (X) is decomposed into gestures (G) and gestural scores (H). Gestures represent the articulatory movements, and gestural scores represent the duration and intensity of these movements.  The figure visually depicts the convolution operation between gestures and gestural scores to reconstruct the raw articulatory data.  The figure is used to explain how the model decomposes complex articulatory movements into simpler, interpretable components.", "section": "2.2 Neural Variational Gestural modeling"}, {"figure_path": "IxEhb4NCvy/figures/figures_21_1.jpg", "caption": "Figure 8: Duration and Intensity Modeling", "description": "This figure illustrates the process of implicit duration and intensity modeling in the neural variational gestural modeling framework.  The input is raw articulatory data X. A latent encoder processes this data and generates latent representations Z. These representations are used by the intensity and duration encoders to predict intensity (Ik,i) and duration (Dk,i) posteriors for each gesture. The intensity posteriors are passed through a sigmoid function to ensure positivity. The duration posteriors are obtained using Gumbel Softmax to account for the non-differentiability of the sampling process. These predicted intensity and duration are combined with a Hann window to create the final gestural scores H.  The process highlights the use of a latent encoder for generating latent representations and shows how these are used in predicting the intensity and duration of gestures, ultimately creating the gestural scores.", "section": "2.2 Neural Variational Gestural modeling"}, {"figure_path": "IxEhb4NCvy/figures/figures_22_1.jpg", "caption": "Figure 9: Sparse Sampling", "description": "This figure illustrates the sparse sampling process applied to the gestural scores.  It starts with a matrix of raw gestural scores, each element (k,i) having a rank score calculated from its intensity and duration. Then, a mask is applied based on the top mrow scores, resulting in a sparse matrix where only the highest-ranked elements are retained. This process is essential for creating more efficient representations for downstream tasks.", "section": "2.2.2 Gestural Variational Autoencoders"}, {"figure_path": "IxEhb4NCvy/figures/figures_22_2.jpg", "caption": "Figure 10: Multi-Scale Gestural Decoder", "description": "This figure illustrates the multi-scale gestural decoder architecture. The gestural scores, initially in a dense matrix form, undergo sparse sampling to reduce the number of patches that contribute to gestural scores. These sparse gestural scores then pass through downsampling modules (1/2 and 1/4), followed by a transformation network and upsampling modules, to reconstruct the articulatory data, X. The final representation is downsampled to ensure consistency with acoustic features from WavLM.", "section": "2.2 Neural Variational Gestural modeling"}, {"figure_path": "IxEhb4NCvy/figures/figures_23_1.jpg", "caption": "Figure 11: Self-Distillation Paradigm", "description": "This figure illustrates the self-distillation process used in the SSDM model.  The acoustic encoder processes the input speech, and the adaptor generates acoustic embeddings.  The downsampled gestural scores (\u0124) are processed through a flow model.  This flow model produces the posterior distribution q\u03b8(\u0124[i]|A[i]) for each time step i.  Meanwhile, a text encoder processes the reference text, resulting in the prior distribution p\u03b8(\u0124[i]|C) for each time step. The KL divergence between these posterior and prior distributions is used as a loss function to guide the learning process. This method helps to improve the model's ability to align acoustic and text information, and this process is called self-distillation.", "section": "2.2.3 Gestural Scores as Phonetic Representations"}, {"figure_path": "IxEhb4NCvy/figures/figures_26_1.jpg", "caption": "Figure 2: SSDM architecture", "description": "This figure presents the overall architecture of the Scalable Speech Dysfluency Modeling (SSDM) system.  It shows the various components and their interactions, including the acoustic encoder, adaptor, text encoder, connectionist subsequence aligner (CSA), multimodal tokenizer, and LLaMA language model.  The diagram illustrates the flow of information from the dysfluent speech input to the final dysfluency alignment and response.  Key components such as the gestural encoder, multimodal tokenizer, and LLaMA are highlighted, showcasing the multi-modal approach used in the SSDM framework.", "section": "2 Articulatory Gesture is Scalable Forced Aligner"}, {"figure_path": "IxEhb4NCvy/figures/figures_27_1.jpg", "caption": "Figure 13: Simulation Pipeline", "description": "This figure illustrates the pipeline used for simulating dysfluent speech using LibriTTS. The process starts by converting the reference text into IPA sequences. Then, dysfluencies (repetition, missing, block, replacement, prolongation) are added based on pre-defined rules. The dysfluent IPA sequences are then fed into StyleTTS2 to generate the dysfluent speech. Finally, the dysfluent speech is annotated with the type and time of dysfluencies.", "section": "A.9 Simulation Pipeline"}, {"figure_path": "IxEhb4NCvy/figures/figures_28_1.jpg", "caption": "Figure 14: Existing Simulated dysfluency datasets", "description": "This figure compares the scale and number of dysfluency types in three simulated dysfluency datasets: Libri-Dys, VCTK++, and LibriStutter.  The x-axis represents the number of dysfluency types in each dataset, and the y-axis shows the total number of hours of audio in each dataset using a logarithmic scale.  It visually demonstrates that Libri-Dys is significantly larger and has more diverse dysfluency types than the other two datasets, highlighting its value as a training resource.", "section": "A.9.3 Datasets Statistics"}, {"figure_path": "IxEhb4NCvy/figures/figures_29_1.jpg", "caption": "Figure 3: LSA(LCS) delivers dysfluent alignment that is more semantically aligned.", "description": "The figure uses an example to compare the performance of LSA (LCS) and GSA (DTW) in aligning dysfluent speech with its reference text.  LSA focuses on matching only semantically relevant portions, resulting in a more accurate and meaningful alignment for dysfluent speech, unlike GSA, which considers all pairings. This difference is particularly evident when dealing with missing, repeated, or inserted phonemes in the dysfluent speech. The image uses a specific example of the word \"references\" with various dysfluencies to highlight LSA's superior performance.", "section": "3 Connectionist Subsequence Aligner (CSA) for Dysfluency Modeling"}]