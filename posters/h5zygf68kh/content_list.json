[{"type": "text", "text": "PaGoDA : Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "h5zYGF68KH/tmp/59d3604ba762923317da50f65f91c1e29f8b898f0c5924c9f9c77bdd6e19f80f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The diffusion model performs remarkable in generating high-dimensional content but is computationally intensive, especially during training. We propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline that reduces the training costs through three stages: training diffusion on downsampled data, distilling the pretrained diffusion, and progressive super-resolution. With the proposed pipeline, PaGoDA achieves a $64\\times$ reduced cost in training its diffusion model on $8\\times$ downsampled data; while at the inference, with the single-step, it performs state-of-the-art on ImageNet across all resolutions from $64\\times64$ to $512\\times512$ , and text-to-image. PaGoDA\u2019s pipeline can be applied directly in the latent space, adding compression alongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable Diffusion). The code is available at https://github.com/sony/pagoda. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion Models (DM) [1, 2], which generate content through gradual denoising, have recently achieved high fidelity in high-dimensional generation [3, 4]. While slow sampling has been improved by distilling trained DMs into single-step generators [5\u20137], DMs remain computationally intensive, especially at high resolutions, requiring substantial data and GPU resources, thereby limiting largescale training to a few organizations [8, 9]. This highlights the need for a more efficient pipeline to reduce both training and inference costs while maintaining the quality. ", "page_idx": 0}, {"type": "text", "text": "To address these challenges, we present Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline that significantly reduces costs while achieving competitive quality with one-step sampling. PaGoDA is built on a simple yet effective idea: while diffusion distillation [6] is typically treated as a final stage of the whole pipeline, we explore to have one more stage for the superresolution after diffusion distillation. This approach led us to design PaGoDA with three distinct stages as below. ", "page_idx": 0}, {"type": "image", "img_path": "h5zYGF68KH/tmp/db01a7a81d11b4a5de1965b9e28b57e46dcaa7de7e8a0b398a86d5fda39d6098.jpg", "img_caption": ["Figure 1: Pipeline overview. PaGoDA deterministically encodes with downsampling followed by DDIM inversion, and constructs its decoder in a progressively growing manner. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "PaGoDA\u2019s Proposed Training Pipeline ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stage 1. (Pretraining) Train a DM on downsampled data.   \nStage 2. (Distillation) Distill the trained DM with DDIM inversion to a one-step generator.   \nStage 3. (Super-Resolution) Progressively expand the generator for resolution upsampling. ", "page_idx": 1}, {"type": "text", "text": "By adding Stage 3 for the super-resolution after the distillation phase, our approach gains a key advantage: training DM on a low-dimensional, downsampled space rather than directly in the desired high-dimensional space. This dimensional reduction substantially lowers the computational demands of diffusion pretraining by orders of magnitude. For example, an $8\\times8$ downsampling rate reduces the training computation by a factor of $64\\times$ . Moreover, the computational costs for the distillation and super-resolution stages are relatively minimal compared to the initial diffusion pretraining, making our pipeline highly efficient in terms of overall computation. ", "page_idx": 1}, {"type": "text", "text": "Figure 1 provides an overview of our pipeline. We begin with DM trained at base resolution, and generate a dataset of base-resolution data-latent pairs $(\\mathbf{x},\\mathbf{z})$ , where $\\mathbf{x}$ is real data and $\\mathbf{z}$ is the latent representation of $\\mathbf{x}$ , obtained by DDIM inversion [10]. In Stage 2, we train a decoder to map $\\mathbf{z}$ back to $\\mathbf{x}$ , completing the diffusion distillation [6]. In Stage 3, we add ResNet blocks [11] to enhance sample resolution and progressively train these newly added upscaling networks, as visualized in Figure 2. The novel use of DDIM inversion in the distillation process, first introduced in PaGoDA, enables the decoder to be trained with the high-frequency signal from the real data at Stage 3. This integration of DDIM inversion establishes strong connections across stages, creating a cohesive and unified framework. ", "page_idx": 1}, {"type": "image", "img_path": "h5zYGF68KH/tmp/f82ad5152c136cad5857b69d33bbc34f0ecc5601654b120d3591ae873c44ed83.jpg", "img_caption": ["Figure 2: (Top) At Stage 2, PaGoDA learns the one-step generator at a base resolution. (Down) At Stage 3, PaGoDA progressively learns for super-resolution by adding additional network blocks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In our experiments, we employed the progressively growing generator to upsample from the pretrained diffusion model\u2019s $64\\times64$ resolution to generate samples at $512\\times512$ resolution. Notably, PaGoDA achieved state-of-the-art (SOTA) Fr\u00e9chet Inception Distances (FID) [12] on ImageNet across all resolutions from $64\\times64$ to $512\\times512$ . Additionally, we demonstrated PaGoDA\u2019s effectiveness in addressing inverse problems and facilitating controllable generation. However, PaGoDA\u2019s potential extends beyond its current application. As PaGoDA being a dimensional reduction technique that operates independently of Latent Diffusion Models (LDM) [3], PaGoDA could be directly applied into the latent space as-is, offering the possibility of further gain on training computes. We leave this exploration as a promising avenue for future research. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DM [1] samples from the data distribution $p_{\\mathrm{data}}$ through an iterative denoising process, beginning from a Gaussian prior distribution $p_{\\mathrm{prior}}$ . This denoising proce\u221ass attempts to reverse [2] a forward diffusion process. If the forward process is defined by $\\mathrm{d}\\mathbf{x}_{t}\\,=\\,\\sqrt{2t}\\,\\mathrm{d}\\mathbf{w}_{t}$ [13], the deterministic counterpart of the denoising (generation) process, known as the probability flow ordinary differential equation (PF-ODE) [2], or DDIM [10], is expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-t\\nabla\\log p_{t}(\\mathbf{x}_{t})\\approx-t s_{\\phi_{0}}(\\mathbf{x}_{t},t),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s_{\\phi_{0}}(\\mathbf{x}_{t},t)$ is a neural approximation of $\\nabla\\log p_{t}(\\mathbf{x}_{t})$ . Consequently, (deterministic) sample generation from DM is equivalent to solving the PF-ODE (or DDIM) along the trajectory, formally, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}^{\\mathrm{DDIM}}(\\mathbf{x}_{T})=\\mathbf{x}_{T}-\\int_{T}^{0}t s_{\\phi_{0}}(\\mathbf{x}_{t},t)\\,\\mathrm{d}t,\\quad\\mathbf{x}_{T}\\sim p_{\\mathrm{prior}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Modern solvers of the PF-ODE [10, 14] have significantly accelerated sampling speed, reducing the required network evaluations from hundreds to tens. To further speed up sampling, DMs are distilled with a student model [6] $G_{\\theta}:\\mathbb R^{d}\\rightarrow\\mathbb R^{d}$ to map from $\\mathbf{x}_{T}$ to $\\mathbf{x}_{0}^{\\mathrm{DDM}}(\\mathbf{x}_{T})$ by minimizing ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{dstl}}(G_{\\theta})=\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{x}_{T})}\\left[\\left|\\left|\\mathbf{x}_{0}^{\\mathrm{DDM}}(\\mathbf{x}_{T})-G_{\\theta}(\\mathbf{x}_{T})\\right|\\right|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We call this DDIM-based approach as the noise-to-data distillation. ", "page_idx": 2}, {"type": "text", "text": "3 Progressive Growing of Diffusion Autoencoder ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Stage 1: Diffusion Models Trained on Downsampled Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Training DMs for high-dimensional data generation is primarily feasible for a limited number of well-resourced organizations, largely due to two factors: access to large-scale datasets and substantial computational resources. This centralization of model development underscores the urgent need to democratize access by significantly reducing resource demands during diffusion training. While several strategies [15, 3] have been proposed, our approach, PaGoDA, introduces a paradigm shift by training the DM at a downsampled resolution in Stage 1, rather than at the original full resolution. For instance, training on a $d$ -dimensional downsampled resolution requires approximately $4^{n}$ times less computational budget compared to training in the full $4^{n}d$ -dimensional space. In practical terms, when $n=3$ , this translates to training in an $8\\!\\times\\!8$ downsampled space, effectively reducing training costs by a factor of $64\\times$ , thus making large-scale diffusion training more accessible to a broader range of researchers. ", "page_idx": 2}, {"type": "text", "text": "Although this paper does not extend PaGoDA\u2019s application to the LDM such as SD, training on a (say) $4\\!\\times\\!4$ downsampled latent space could theoretically reduce the computational cost by $16\\times$ compared to full-resolution latent training, further emphasizing PaGoDA\u2019s potential for widespread adoption. In the case of generating $1024\\!\\times\\!1024$ images, PaGoDA requires training the diffusion model at only $32\\!\\times\\!32$ resolution, with Stage 3 subsequently upscaling it to the full $128\\!\\times\\!128$ latent space of conventional approaches [8, 9]. This progressive approach illustrates PaGoDA\u2019s effectiveness in maintaining model quality while lowering the barriers to high-resolution diffusion training. ", "page_idx": 2}, {"type": "text", "text": "3.2 Stage 2: Diffusion Distillation on Downsampled Data with DDIM Inversion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "After pretraining DM on the downsampled space, PaGoDA distills DM to a one-step generator. For distillation, PaGoDA introduces a new loss specifically designed for later usage in super-resolution at Stage 3. In particular, we propose the reconstruction loss (compare it with $\\mathcal{L}_{\\mathrm{dstl}}$ in Eq. 1 of Section 2) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rec}}(G_{\\theta}):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{0})}\\Big[\\big\\|\\mathbf{x}_{0}-G_{\\theta}\\big(\\mathbf{x}_{T}^{\\mathrm{DDIM^{-1}}}(\\mathbf{x}_{0})\\big)\\big\\|_{2}^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{T}^{\\mathrm{DDIM}^{-1}}(\\mathbf{x}_{0})$ is now the latent representation of $\\mathbf{x}_{\\mathrm{0}}$ , obtained from DDIM inversion, not from DDIM, i.e., the solution at time $T$ of the PF-ODE starting from $\\mathbf{x}_{\\mathrm{0}}$ in time forward, defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{T}^{\\mathrm{DDIM}^{-1}}(\\mathbf{x}_{0}):=\\mathbf{x}_{0}-\\int_{0}^{T}t s_{\\phi_{0}}(\\mathbf{x}_{t},t)\\,\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "h5zYGF68KH/tmp/1cc9433bab10df0159d7fafe09bacadb23c684548958da5cfe8236bcee4b0a35.jpg", "img_caption": ["Figure 3: Effect of the reconstruction loss in Stage 3. Without the reconstruction loss, the object moves at each resolution jump. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Distillation using ${\\mathcal{L}}_{\\mathrm{rec}}$ maps latent representations to real data, following a data-to-noise distillation approach. While this method has the potential to improve real data alignment compared to the traditional noise-to-data approach in Eq. 1, we observe a decline in generation quality over iterations. This decline stems from the prior hole problem [16], where the generator\u2019s input, $\\mathbf{x}_{T}^{\\mathrm{{DDIM}^{-1}}}$ , derived from limited real data, fails to cover the entire prior manifold, leaving certain regions unexplored. ", "page_idx": 3}, {"type": "text", "text": "A straightforward strategy like early stopping could alleviate this issue, but it restricts the use of Exponential Moving Average (EMA) in Stage 2. To fundamentally resolve this problem, we propose a solution that maintains generation quality even during prolonged training. In Section 3.4, we provide optimal and stability analysis of Stage 2, guaranteeing that our proposal is stable across training iterations. The key challenge is effectively covering the prior manifold, which we address by introducing an auxiliary adversarial loss, as defined below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{adv}}(G_{\\theta},D_{\\psi}):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\big[\\log D_{\\psi}(\\mathbf{x})\\big]+\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\Big[\\log\\Big(1-D_{\\psi}\\big(G_{\\theta}(\\mathbf{z})\\big)\\Big)\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $D_{\\psi}$ is a discriminator that classifies the real and fake samples by maximizing the adversarial loss, and $p_{\\mathrm{prior}}(\\mathbf{z})$ is the prior distribution. ", "page_idx": 3}, {"type": "text", "text": "The second term in $\\mathcal{L}_{\\mathrm{adv}}$ , which involves $G_{\\theta}(\\mathbf{z})$ with $\\mathbf{z}$ sampled from the prior, ensures that the decoder is exposed to the entire support of the prior distribution during training. Overall, we train PaGoDA with the mini-max optimization of the following combined objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{G_{\\theta}}\\operatorname*{max}_{D_{\\psi}}\\mathcal{L}_{\\mathrm{PaGoDA}}\\big(G_{\\theta},D_{\\psi}\\big):=\\operatorname*{min}_{G_{\\theta}}\\Big[\\mathcal{L}_{\\mathrm{rec}}\\big(G_{\\theta}\\big)+\\lambda\\operatorname*{max}_{D_{\\psi}}\\mathcal{L}_{\\mathrm{adv}}\\big(G_{\\theta},D_{\\psi}\\big)\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While PaGoDA incorporates the adversarial loss, the reconstruction loss simultaneously guides the decoder to accurately reconstruct the entire training data. This combination allows the adversarial loss to address underrepresented regions in the prior distribution effectively without compromising sample diversity. In our ImageNet experiments, we found that updating the reconstruction loss with as little as $1\\%$ of the data-latent pairs did not affect sample quality and diversity. Exploring the impact of varying the number of data-latent pairs is left as a future work. ", "page_idx": 3}, {"type": "text", "text": "3.3 Stage 3: Progressively Growing Decoder for Super-Resolution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Stage 3 trains the super-resolution to generate higher-dimensional data from the downsampled resolution learned in the previous stages. As illustrated in Figure 2, the resolution jump from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{4^{n}d}$ is achieved by freezing most parameters of the distilled model from Stage 2 and training only the final layers, which is augmented with an additional upscaler network (of ResNet blocks [17]). In other words, within the base-resolution U-Net [18], we freeze its input, middle, and output blocks except for the last few layers (previously highest resolution block) during Stage 3 training. Consequently, the unfrozen latter part of the network is trained for super-resolution. We suggest to progressively increasing the resolution by a factor of $2\\times$ , though larger jumps by factors of $4\\times$ or $8\\times$ yield comparable performance. ", "page_idx": 3}, {"type": "text", "text": "Additionally, the last layer typically converts multi-channel (usually 128 or 256 channels) features to 3-channel RGB output. However, to minimize information loss, we retain these features and pass them directly to the next output block without converting them to 3 channels. This architectural choice, along with progressive training, is heavily inspired by Progressive Growing GAN [19]. ", "page_idx": 3}, {"type": "text", "text": "In Stage 3, the reconstruction loss from Stage 2 is adapted as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{\\mathrm{high}})}\\left[\\left|\\left|\\mathbf{x}_{\\mathrm{high}}-G_{\\theta}(\\mathbf{x}_{T}^{\\mathrm{DDIM}^{-1}}(\\mathbf{x}_{0}))\\right|\\right|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}_{0}\\in\\mathbb{R}^{d}$ is the downsampled counterpart of $\\mathbf{x}_{\\mathrm{high}}\\in\\mathbb{R}^{4^{n}d}$ . The adversarial loss in this stage is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{\\mathrm{high}})}\\left[\\log D_{\\psi}(\\mathbf{x}_{\\mathrm{high}})\\right]+\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\Big[\\log\\Big(1-D_{\\psi}\\big(G_{\\pmb{\\theta}}(\\mathbf{z})\\big)\\Big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Overall, both the reconstruction and adversarial losses are combined to guide training. ", "page_idx": 4}, {"type": "text", "text": "Stage 3 employs two key mechanisms to effectively capture high-frequency details while maintaining training stability. First, the reconstruction loss is applied directly to highdimensional real data, which was not feasible with earlier noiseto-data distillation methods with Eq. 1. As illustrated in Figure 3, ${\\mathcal{L}}_{\\mathrm{rec}}$ stabilizes the upscaling process by preventing objects from shifting across resolutions, allowing the added neural network to focus solely on upsampling. Second, the adversarial loss operates directly in high-dimensional space, enabled by the one-step generator trained in Stage 2. This generator is critical; without it, adversarial training in Stage 3 would be infeasible. As shown in Figure 4 tested on ImageNet, the adversarial loss is pivotal for achieving effective upscaling performance. ", "page_idx": 4}, {"type": "image", "img_path": "h5zYGF68KH/tmp/e71d918057b14c3ae93d8256b5cd0df7454bde0e18829df59d6f91c93cf539b3.jpg", "img_caption": ["Figure 4: The adversarial loss makes PaGoDA competitive with GAN-based super-resolution models in Stage 3. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.4 Optimality Guarantee and Training Stability of PaGoDA Pipeline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When using the conventional $\\mathcal{L}_{\\mathrm{dstl}}$ for distillation, the optimal student becomes $G_{\\pmb{\\theta}^{*}}(\\mathbf{x}_{T})\\,=\\,\\mathbf{x}_{0}^{\\mathrm{{DDIM}}}(\\mathbf{x}_{T})$ , meaning that the student\u2019s samples replicate those of DDIM. As a result, the student\u2019s performance is heavily dependent on the teacher\u2019s performance. Consequently, the student\u2019s generative distribution may diverge from the real data distribution, even when $\\mathcal{L}_{\\mathrm{dstl}}$ is combined with adversarial loss. In contrast, by using the DDIM inversion-based reconstruction loss proposed in Stage 2, we mathematically prove in Theorem 3.1 that the optimal student\u2019s generative distribution aligns with the real data distribution. As visualized in Figure 5, our PaGoDA Stage 2 (red) achieves robust performance even with a weaker teacher, unlike traditional noise-to-data distillation loss $\\mathcal{L}_{\\mathrm{dst}}$ of Eq. 1, which struggles despite the use of adversarial loss. ", "page_idx": 4}, {"type": "image", "img_path": "h5zYGF68KH/tmp/70dc45b6caabee9eb3709c5deafb1813cbaef752293557885133219c60992ac4.jpg", "img_caption": ["Figure 5: Comparison of $\\mathcal{L}_{\\mathrm{dstl}}$ and ${\\mathcal{L}}_{\\mathrm{rec}}$ , both combined with $\\mathscr{L}_{\\mathrm{adv}}$ , using identical hyperparameters. ${\\mathcal L}_{\\mathrm{rec}}$ shows the robust performance, also supported by Theorem 3.1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Let $\\lambda>0$ . Suppose $D^{*}(G)\\in\\arg\\operatorname*{max}_{D}\\mathcal{L}_{a d\\nu}(G,D)$ . If both PaGoDA\u2019s reconstruction loss and adversarial loss share a common minimizer $G^{*}$ , then $p_{G^{*}}(\\mathbf{x})=p_{d a t a}(\\mathbf{x})$ . Here, $p_{G^{*}}$ is the generative distribution learned by optimizing Eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Additionally, Theorem 3.2 shows that PaGoDA\u2019s training is stable with the help of reconstruction loss, even with adversarial training. We empirically observe that PaGoDA can be trained effectively without many of the techniques typically used to stabilize GANs [20, 21]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. [Informal] Let $E$ be a fixed deterministic encoder. Suppose that at the generator\u2019s equilibria $G^{*}$ of Eq. (4), $p_{G^{*}}(\\mathbf{x})=p_{d a t a}(\\mathbf{x})$ , and $\\mathbf{x}=G^{*}(E(\\mathbf{x}))$ . Then, under conditions similar to those found in the stability literature for improving GAN [22, 21], training with Eq. (4) is stable (gradient descent locally converges to its equilibria). ", "page_idx": 4}, {"type": "text", "text": "We refer to Theorems B.4 and B.9 for rigorous and extended versions of Theorems 3.1 and 3.2, respectively. All proofs can be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "4 PaGoDA with Classifier-Free Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we integrate Classifier-Free Guidance (CFG) [23, 4] into PaGoDA for Text-toAny generation, with a focus on Text-2-Image. Incorporating CFG alters the sample distribution, necessitating adjustments to the loss functions for Stages 2 and 3. Since previous GAN literature [24\u2013 27] has not addressed CFG integration, we introduce the classifier-free guided adversarial loss to accommodate this adaptation. ", "page_idx": 4}, {"type": "text", "text": "CFG guides the denoising process by adjusting the conditional score gradient $\\nabla\\log p_{t}\\big(\\mathbf{x}_{t}|\\mathbf{c}\\big)$ into a guided score $\\nabla\\log p_{t}(\\mathbf{x}_{t}|\\mathbf{\\bar{c}})+(\\omega-\\mathbf{\\bar{\\kappa}}1)\\bar{\\nabla}\\log p(\\mathbf{\\bar{c}}|\\mathbf{x}_{t})$ . This adjustment leads our distillation learning target from $p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c})$ to $p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)$ , defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)\\propto p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c})^{\\omega}p_{\\mathrm{data}}(\\mathbf{x})^{1-\\omega},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "reflecting the influence of guidance strength $\\omega$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Classifier-Free Guided Adversarial Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To describe the classifier-free adversarial loss, we first consider the loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{adv}}^{\\mathbf{c},\\omega}(G_{\\theta},D_{\\psi}):=\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x}|\\mathbf{c},\\omega)}\\Big[\\log D_{\\psi}(\\mathbf{x},\\mathbf{c},\\omega)\\Big]+\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x}|\\mathbf{c},\\omega)}\\Big[\\log\\Big(1-D_{\\psi}(\\mathbf{x},\\mathbf{c},\\omega)\\Big)\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where now both generator and discriminator incorporates $\\omega$ as an additional condition [28], see Eq. (3) for the comparison. From the standard GAN argument [29], this GAN loss guarantees the optimal generator to match to the data distribution, i.e., $p_{G^{*}}(\\mathbf{x}|\\mathbf{c},\\omega)\\,=\\,p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)$ . Hence, the classifier-free adversarial loss could be defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{adv}}^{\\mathrm{CFG}}(G_{\\theta},D_{\\psi}):=\\mathbb{E}_{p_{\\mathrm{das}}(\\mathbf{c})\\pi(\\boldsymbol{\\omega})}\\left[\\mathcal{L}_{\\mathrm{adv}}^{\\mathrm{c},\\omega}(G_{\\theta},D_{\\psi})\\right]}\\\\ &{\\quad=\\mathbb{E}_{p_{\\mathrm{das}}(\\mathbf{c})\\pi(\\boldsymbol{\\omega})p_{\\mathrm{das}}(\\mathbf{x}|\\mathbf{c},\\omega)}\\Big[\\log D_{\\psi}(\\mathbf{x},\\mathbf{c},\\omega)\\Big]+\\mathbb{E}_{p_{\\mathrm{das}}(\\mathbf{c})\\pi(\\omega)p_{G_{\\theta}}(\\mathbf{x}|\\mathbf{c},\\omega)}\\Big[\\log\\big(1-D_{\\psi}\\big(\\mathbf{x},\\mathbf{c},\\omega\\big)\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A key challenge with $\\mathcal{L}_{\\mathrm{adv}}^{\\mathbf{c},\\omega}$ is that sampling from $p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)$ is generally infeasible, making it difficult to compute the first term of $\\mathcal{L}_{\\mathrm{adv}}^{\\mathrm{CFG}}$ . To address this issue, we leverage the Bayes formula ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathrm{data}}(\\mathbf{c})\\pi(\\omega)p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)=p_{\\mathrm{data}}(\\mathbf{x},\\mathbf{c})p(\\omega|\\mathbf{x},\\mathbf{c}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where both representations are two different ways to decompose the joint distribution over $(\\mathbf{x},\\mathbf{c},\\omega)$ , with $\\pi(\\omega)$ being the prior distribution of the CFG scale $\\omega$ . From this formula, if we could predict the guidance weight $\\omega$ by observing $\\mathbf{x}$ and $\\mathbf{c}$ , i.e., if we know $p(\\omega|\\mathbf{x},\\mathbf{c})$ , then sampling $(\\mathbf{x},\\mathbf{c},\\omega)$ from $p_{\\mathrm{data}}(\\mathbf{c})\\pi(\\omega)p_{\\mathrm{data}}(\\mathbf{x}|\\mathbf{c},\\omega)$ can be alternatively achieved by: 1) sampling $\\left(\\mathbf{x},\\mathbf{c}\\right)$ from $p_{\\mathrm{data}}(\\mathbf{x},\\mathbf{c})$ , and 2) predicting most likely $\\omega$ using $p(\\omega|\\mathbf{x},\\mathbf{c})$ . ", "page_idx": 5}, {"type": "text", "text": "We approximate $p(\\omega|\\mathbf{x},\\mathbf{c})$ with a U-Net encoder network with 1-dimensional output, called $C F G$ weight estimator $\\omega_{\\phi}$ . The input of $\\omega_{\\phi}$ network is a single-channel matrix with $(i,j)$ -th value as the multiplication of the $i/j$ -th values of $\\mathbf{x}/\\mathbf{c}$ CLIP embeddings, respectively. As this matrix is high-dimensional, we input the downsampled $64\\times64\\times1$ matrix to the U-Net encoder. These CLIP embeddings are also used to condition the network. With DM pretrained at Stage 1, which is supposed to be sufficiently close to the data distribution, we train the CFG weight estimator by minimizing $\\begin{array}{r}{\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})p_{\\mathrm{data}}(\\mathbf{c})\\pi(\\omega)}[\\|\\omega-\\omega_{\\phi}(\\hat{\\mathbf{x}}(\\mathbf{z},\\mathbf{c},\\omega),\\mathbf{c})\\|_{2}^{2}]}\\end{array}$ , where $\\hat{\\mathbf{x}}(\\mathbf{z},\\mathbf{c},\\omega)$ is a clean base-resolution sample drawn the teacher diffusion. Then, $\\omega_{\\phi}(\\mathbf{x},\\mathbf{c})$ -value becomes the point estimation of $p(\\omega|\\mathbf{x},\\mathbf{c})$ ", "page_idx": 5}, {"type": "text", "text": "4.2 PaGoDA Pipeline with Classifier-Free Guidance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We replace the adversarial loss in Stages 2 and 3 with the proposed classifier-free guided adversarial loss. In Stage 3, we shift the focus from $\\mathbf{x}\\in\\mathbb{R}^{d}$ to $\\mathbf{x}_{\\mathrm{high}}\\in\\bar{\\mathbb{R}}^{4^{\\bar{n}}d}$ to effectively capture high-frequency details. Additionally, in both Stages 2 and 3, we replace the input of the generator in the reconstruction loss to be classifier-free guided DDIM inversion noise. To enhance text-sample alignment, we further regularize training with CLIP [30] similarity. For training, we use the ViT-L/14 [31] CLIP model pretrained on YFCC100M [32], while for evaluation, we use the ViT- $\\mathrm{\\textg}/14$ CLIP model pretrained on LAION-2B [33], minimizing the risk of overfitting. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 PaGoDA Tested on ImageNet without CFG ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on ImageNet using PaGoDA without CFG to validate the core pipeline described in Section 3, utilizing the discrete time diffusion scheduling proposed by EDM [13]. Before training, we collect DDIM inversion latent representations for all ImageNet data using the Heun method [13] with 40 timesteps (79 NFE). Throughout the experiments, we maintain the batch size ", "page_idx": 5}, {"type": "image", "img_path": "h5zYGF68KH/tmp/527e5b1128d1fe9e35ff5c6405bbda5b43ac98780aafe829c91af4af975e769f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: Uncurated samples generated by PaGoDA at resolution $512\\times512$ without CFG. Left: class 31 (tree frog); Right: class 33 (loggerhead turtle). ", "page_idx": 6}, {"type": "text", "text": "to be 256 for both ${\\mathcal L}_{\\mathrm{rec}}$ and $\\mathcal{L}_{\\mathrm{adv}}$ in Stages 2 and 3. We initialize our base resolution generator with the pre-trained diffusion U-Net. Following CTM [7], we implement adaptive weighting [34] with $\\bar{\\lambda}=0.2\\frac{||\\nabla_{\\theta^{l}}\\mathcal{L}_{\\mathrm{rec}}||_{2}^{2}}{||\\nabla_{\\theta^{l}}\\mathcal{L}_{\\mathrm{adv}}||_{2}^{2}}$ , where $\\theta^{l}$ represents the last layer of the generator. ", "page_idx": 6}, {"type": "text", "text": "For higher resolution generation, we double the previous resolution by adding two auxiliary ResNet blocks followed by one upsampler ResNet block. The previously trained generator remains frozen, except for the highest-resolution blocks, which are unfrozen. We then train these newly added blocks along with the unfrozen parts, using a fixed GAN weight of $\\lambda\\:=\\:1.0$ . Appendix A.1 provides additional details. By freezing part of the trained generator, we achieve greater stability in superresolution training without adaptive weighting. See Figure 6 for uncurated $512\\times512$ random samples of ImageNet without CFG. ", "page_idx": 6}, {"type": "text", "text": "5.1.1 Quantitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents the performance of PaGoDA. Our model consistently outperforms all existing models across all resolutions, achieving SOTA FIDs without the need of CFG and any other stabilization tricks for GAN. Remarkably, PaGoDA\u2019s Inception Score (IS) [35] is on par with other diffusion and GAN models that employed CFG, which implies that PaGoDA samples are as distinctive as CFG samples. Also, PaGoDA generates samples as diverse as the real data distribution, evidenced by diversity recall metric [36], where the PaGoDA reports 0.63 for $64\\times64$ resolution (data\u2019s recall is 0.67). In contrast, StyleGAN-XL is far behind of PaGoDA in terms of the diversity metric, reporting 0.52 for $64\\times64$ resolution. Note that we used StyleGAN-XL\u2019s discriminator in PaGoDA training, implying that the reconstruction loss significantly improves the sample diversity. ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Discussion on Base Resolution ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "When applying PaGoDA pipeline, the choice of downsampled base resolution in Stage 1 will be primarily determined by available computational resources. Thus, we investigate the impact of the base resolution at this section. To understand the impact, we conducted experiments at $32\\times32$ and $64\\times64$ resolutions, as summarized in Table 2. Starting at resolutions below $32\\times32$ imposes excessive complicacy on the upscaling network, while higher resolutions significantly increase the computational costs at the Stage 1. Therefore, our analysis focuses on these two resolutions, balancing between computational efficiency and upscaling feasibility. ", "page_idx": 6}, {"type": "text", "text": "We utilized only 1 H100 node with 8 GPUs for diffusion training on $32\\times32$ with 4096 batch size. Also, for $64\\times64$ diffusion, we borrow a pretrained checkpoint [5], which used $\\geq32^{3}$ A100 GPUs to train with 4096 batch size. Results in Table 2 demonstrate that the diffusion model trained in Stage 1 maintains robust performance across both resolutions. Interestingly, the one-step generator distilled in Stage 2 consistently outperforms the teacher model, likely benefiting from the effectiveness of StyleGAN-XL [40], combined with the reconstrcution loss. In Stage 3, the degree of upscaling from the base resolution emerges as the most influential factor for the quality, with upscaling up to $8\\mathbf{x}$ showing minimal performance degradation across both tested resolutions. ", "page_idx": 6}, {"type": "table", "img_path": "h5zYGF68KH/tmp/910642c75db0fdb9efa9b1d4f487555aa554ea19a8259edf8371af131b95edd3.jpg", "table_caption": ["Table 1: Experimental results of PaGoDA on ImageNet. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "h5zYGF68KH/tmp/24afbb58f32dec1aa67cd9f4c9c5911e5d6ecf838ca3a352977ac97f8972c890.jpg", "table_caption": ["Table 2: Ablation of base resolution. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The upscaler in PaGoDA refines coarse samples generated at lower resolutions, making the pipeline inherently aligned with the scaling laws of smaller resolutions. This design is advantageous, as scaling laws typically worsen with increasing resolution [43], while PaGoDA leverages the more favorable scaling dynamics at lower resolutions to maintain efficiency. Furthermore, the lightweight upscaling module introduces minimal additional latency, keeping inference times nearly identical to those at the base resolution. This practical efficiency makes PaGoDA a promising solution for scalable diffusion model training across various computational settings. ", "page_idx": 7}, {"type": "text", "text": "5.1.3 Discussion on Upscaling Capability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Stage 3, we train the super-resolution module using a combination of reconstruction and adversarial losses. As shown in Figures 3 and 4, we compare PaGoDA\u2019s performance to that of StyleGAN-XL. The comparison reveals key insights: 1) PaGoDA maintains consistent object alignment across resolution jumps, largely due to the reconstruction loss, and 2) its performance is strongly influenced by the GAN component, which plays a crucial role in capturing high-frequency details. ", "page_idx": 7}, {"type": "text", "text": "Other upsampling methods, such as SD and Cascaded Diffusion Models (CDM) [44] also target high-quality upscaling. While PaGoDA, CDM, and SD share the same goal, they adopt different approaches, making them complementary rather than competing solutions. In fact, their strengths can be combined to enhance overall compression and upscaling performance. For instance, CDM or PaGoDA can be applied to the latent space of SD, integrating their techniques for better results. Despite their compatibility, it is still essential to assess how these methods compare in terms of their upscaling effectiveness. In the following analysis, we break down the upscaling capabilities of PaGoDA, CDM, and SD to understand their respective strengths and potential limitations. ", "page_idx": 7}, {"type": "table", "img_path": "h5zYGF68KH/tmp/65bc4dc021cee3c4a65fe985540ba1499596a0f910c2d1dba90996e1a25edf85.jpg", "table_caption": ["Table 3: Comparison on upsampling. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "h5zYGF68KH/tmp/3295a77b3e7e8af63ae1bbf1780464516c3086e8dc3f6641c3a74c6121107d63.jpg", "img_caption": ["Figure 8: Controllable generation of PaGoDA with various tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Since PaGoDA is experimented based on EDM [13], we adapted the experimental results from EDM2 [42] to facilitate a direct comparison with PaGoDA in the upscaling performance. EDM2 presents results for both pixel DM and latent DM. In latent diffusion, a $512\\times512\\times3$ image is compressed into a $64\\times64\\times4$ latent space for training DM, while pixel diffusion operates directly on $64\\times64\\times3$ images, sharing the identical network architecture used in its latent DM. As reported in Table 3, EDM2 shows a minor performance decline from 1.33 to 1.96. ", "page_idx": 8}, {"type": "text", "text": "Similarly, PaGoDA exhibits a comparable performance drop from 1.21 to 1.80 when upscaling from $64\\times64$ to $512~\\times$ 512. This similarity suggests that PaGoDA\u2019s upscaling capacity aligns closely with that of the LDM framework, indicating minimal performance differences even when handling highresolution data. ", "page_idx": 8}, {"type": "text", "text": "Lastly, when comparing PaGoDA to CDM, we observe in Figure 7 that CDM encounters significant performance drops beyond certain dimensional thresholds $(128\\,\\times\\,128)$ , while PaGoDA maintains consistent performance across varying resolutions. This robustness makes PaGoDA a reliable option for high-resolution generation, with its performance remaining steady even as resolution increases. ", "page_idx": 8}, {"type": "image", "img_path": "h5zYGF68KH/tmp/acc9aa69dbf8e009f3362918ee46b6c23d2b73bbfddb38025a2311cd91b686ae.jpg", "img_caption": ["Figure 7: Comparison between PaGoDA and CDM. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Discussion on Controllability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Once we have a trained PaGoDA generator $G_{\\theta_{0}}$ , we can utilize it for solving inverse problems [45] and for controllable generation [46] in a training-free manner [47]. ", "page_idx": 8}, {"type": "text", "text": "Latent Optimization We consider the inverse problem: $\\mathbf{y}={\\mathcal{A}}(\\mathbf{x})+{\\boldsymbol{\\eta}}$ , where $\\mathbf{y}$ represents the observation, and $\\mathcal{A}:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{m}$ with $d\\,\\geq\\,m$ is a known operator. The restored data $\\mathbf{x}$ can be reconstructed by optimizing the latent. Specifically, if $\\begin{array}{r}{\\mathbf{z}^{\\ast}\\in\\bar{\\arg\\operatorname*{min}}_{\\mathbf{z}}\\,\\|\\mathbf{y}-\\mathcal{A}(G_{\\theta_{0}}(\\mathbf{z},\\mathbf{c}))\\|_{2}^{2}}\\end{array}$ , then $G_{\\theta_{0}}(\\mathbf{z}^{*},\\mathbf{c})$ is the best possible estimate of the solution for the inverse problem. Figure 8-(a) displays the outcomes of an inpainting task where latent optimization is employed with Adam optimizer [48]. ", "page_idx": 8}, {"type": "text", "text": "DDIM Inversion Specific tasks, such as super-resolution illustrated in Figure 8-(b) and class transfer depicted in Figure 8-(c), can be effectively addressed without relying on latent optimization. For these tasks, we apply DDIM inversion to the downsampled observations, then map the DDIM latent back to RGB pixel by feeding the latent into the decoder. Generally, using DDIM inversion yields superior outcomes compared to latent optimization for these types of tasks. ", "page_idx": 8}, {"type": "text", "text": "Latent Interpolation Building on techniques from GAN research, we also explored latent interpolation for style mixing. Despite our model\u2019s latent dimension being larger than the typical 512-dimensional style vector used in GAN, our observations indicate that latent mixing by slerp operation [49, 20] achieves effective results, as demonstrated in Figure 8-(d). ", "page_idx": 8}, {"type": "text", "text": "Table 4: Experimental results on T2I. FID-5K is measured on MSCOCO-2017 [54] validation data. CLIP score is measured by the ViT-g/14 backbone. Our model uses DeepFloy-IF as the pre-trained diffusion. ", "page_idx": 9}, {"type": "table", "img_path": "h5zYGF68KH/tmp/39e8389e3029cbd23d5d723177209aeb8863eaf9f610de323c06b87308e23097.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5: Experimental results on T2I. FID30K is based on MSCOCO-2014 [54] validation data. Speed is measured on A100. ", "page_idx": 9}, {"type": "table", "img_path": "h5zYGF68KH/tmp/56e1105916f7751d85aeb9695559a1ebbce75c6bd9ac6cc7300f5e0a98c05323.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Text-to-Image Generation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We collect the data-latent pairs on the CC12M dataset [50] through DDIM inversion and utilize the flitered COYO-700M [51] dataset for adversarial training. The flitering criteria include only data with CLIP score (measured by ViT-B/32 [52]) higher than 32, and aesthetic score-v2 [33] higher than 5.0. Due to concerns regarding sensitive content in the open-sourced LAION dataset [33], we were unable to conduct large-scale diffusion training for Stage 1. This constraint led us to focus primarily on stages 2 and 3, leveraging pretrained open-source checkpoints. For the pretrained teacher diffusion, we used the DeepFloyd-IF model [53], trained on $64\\times64$ pixel space. For further experimental details, see Appendix A.2. ", "page_idx": 9}, {"type": "text", "text": "Table 4 compares our PaGoDA mainly with the distilled models from SD v1.5 on $512\\times512$ . One notable observation from the table is that, even though the latent distilled model generates the latent representation in a single step, additional time is required for decoding this latent into image. In contrast, PaGoDA (on pixel teacher) eliminates such decoding step, thereby overcoming the time constraints associated with distilling SD models. For a more detailed breakdown of the time taken by each component, see Figure 9. ", "page_idx": 9}, {"type": "text", "text": "Returning to the performance results in the table, PaGoDA achieves performance comparable to that of the teacher model. ", "page_idx": 9}, {"type": "text", "text": "This superior performance is also observed on a different test set as shown in Table 5, further demonstrating PaGoDA\u2019s scalability on text-to-image tasks. ", "page_idx": 9}, {"type": "image", "img_path": "h5zYGF68KH/tmp/6ab78edb1e09750b9a4b6c138b58e9cbaaccb8e9973bbc3b800f980cd19587ed.jpg", "img_caption": ["Figure 9: PaGoDA offers faster inference than the one-step LCM. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "PaGoDA introduces a training pipeline that can democratize the diffusion training by cutting training budget with orders of magnitudes. The pipeline is consisted of three stages: 1) we pretrain the diffusion models on the downsampled data, 2) we distill the teacher diffusion into a one-step generator on the downsampled data, and 3) we train an upsampler module until we reach to the desired resolution. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project was supported by Sony, ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and the CZ Biohub. Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used. We extend our special thanks to our colleagues Takashi Shibuya from Sony AI and Yutong He from Carnegie Mellon University for their invaluable feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[5] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[6] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021.   \n[7] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In International Conference on Learning Representations, 2024. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.   \n[9] Black-Forest. Flux. https://blackforestlabs.ai/announcing-black-forest-labs/, 2024.   \n[10] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[13] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[14] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[15] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. W\u00fcrstchen: An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth International Conference on Learning Representations, 2023.   \n[16] Jyoti Aneja, Alex Schwing, Jan Kautz, and Arash Vahdat. A contrastive learning approach for training variational autoencoder priors. Advances in neural information processing systems, 34:480\u2013493, 2021.   \n[17] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.   \n[18] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[19] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[21] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International conference on machine learning, pages 3481\u20133490. PMLR, 2018.   \n[22] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. Advances in neural information processing systems, 30, 2017.   \n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.   \n[24] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10124\u201310134, 2023.   \n[25] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International conference on machine learning, pages 30105\u201330118. PMLR, 2023.   \n[26] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.   \n[27] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024.   \n[28] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297\u201314306, 2023.   \n[29] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373, 2016.   \n[33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[34] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[35] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.   \n[36] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019.   \n[37] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022.   \n[38] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 13213\u2013 13232. PMLR, 2023.   \n[39] Diederik P Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022.   \n[41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[42] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. arXiv preprint arXiv:2312.02696, 2023.   \n[43] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.   \n[44] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):2249\u20132281, 2022.   \n[45] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.   \n[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[47] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, et al. Manifold preserving guided diffusion. In International Conference on Learning Representations, 2023.   \n[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[49] Ken Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pages 245\u2013254, 1985.   \n[50] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.   \n[51] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo- $.700\\mathrm{m}$ : Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022.   \n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[53] DeepFloyd Lab. If by deepfloyd lab at stabilityai. https://github.com/deep-floyd/IF, 2023.   \n[54] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[55] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.   \n[56] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[57] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023.   \n[58] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. arXiv preprint arXiv:2311.09257, 2023.   \n[59] Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zhengjun Zha, and Haonan Lu. Scott: Accelerating diffusion models with stochastic consistency distillation. arXiv preprint arXiv:2403.01505, 2024.   \n[60] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \n[61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[62] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.   \n[63] Tianwei Yin, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023.   \n[64] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17907\u201317917, 2022.   \n[65] Bosco Yung. Open-nsfw 2. https://github.com/bhky/opennsfw2, 2021.   \n[66] Gant Laborde. https://github.com/GantMan/nsfw_model.   \n[67] Roman Inflianskas. https://github.com/rominf/profanity-filter/blob/master/ profanity_filter/data/en_profane_words.txt.   \n[68] Jaclyn Brockschmidt. https://github.com/snguyenthanh/better_profanity/blob/ master/better_profanity/profanity_wordlist.txt.   \n[69] Jamie Dubs and Ryan Lewis. https://gist.github.com/ryanlewis/ a37739d710ccdb4b406d.   \n[70] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[71] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[72] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. Advances in neural information processing systems, 33:7559\u20137570, 2020.   \n[73] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019.   \n[74] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[75] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.   \n[76] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[77] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[78] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021.   \n[79] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review. Statistics surveys, 8:45, 2014.   \n[80] Wenpin Tang and Hanyang Zhao. Contractive diffusion probabilistic models. arXiv preprint arXiv:2401.13115, 2024.   \n[81] Junlong Lyu, Zhitang Chen, and Shoubo Feng. Sampling is as easy as keeping the consistency: convergence guarantee for consistency models. 2023.   \n[82] Xuefeng Gao, Hoang M Nguyen, and Lingjiong Zhu. Wasserstein convergence guarantees for a general class of score-based generative models. arXiv preprint arXiv:2311.11003, 2023.   \n[83] Bernard A Asner, Jr. On the total nonnegativity of the hurwitz matrix. SIAM Journal on Applied Mathematics, 18(2):407\u2013414, 1970.   \n[84] Nam Parshad Bhatia and Giorgio P Szeg\u00f6. Stability theory of dynamical systems. Springer Science & Business Media, 2002.   \n[85] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. Advances in neural information processing systems, 30, 2017.   \n[86] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pages 354\u2013363. PMLR, 2018.   \n[87] Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational inequalities. arXiv preprint arXiv:1808.01531, 2018.   \n[88] Chuang Wang, Hong Hu, and Yue Lu. A solvable high-dimensional model of gan. Advances in Neural Information Processing Systems, 32, 2019.   \n[89] Chongli Qin, Yan Wu, Jost Tobias Springenberg, Andy Brock, Jeff Donahue, Timothy Lillicrap, and Pushmeet Kohli. Training generative adversarial networks by solving ordinary differential equations. Advances in Neural Information Processing Systems, 33:5599\u20135609, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have made our abstract and introduction to accurately reflect the core contribution of the paper. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We have created a separate \u201cLimitations and Broader Impacts\u201d section in the appendix to enumerate potential limitations of our methodology, including the algorithmic, theoretical, and experimental limitations. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Although we omit some of assumptions in the main paper mainly due to page limit, we provide full details of assumptions and complete proof in the appendix. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We disclose all experimental details in the main paper and the appendix, including the hyperaparameters used and the datasets used with their flietering methodologies. For further reproducibility, we plan to release our code upon acceptance. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In the reviewing process, we release our code to the reviewers to regenerate our experimental results. After the acceptance, we plan to release the code to the public. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We faithfully release our hyperparameters and experimental details in the appendix and the main paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have not reported error bars mainly due to the lack of computational resources. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We explain how much compute resources we used for experiments in the appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We faithfully follow the code of ethics, suggested by the link above. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the broader impacts as a separate section in the \u201cLimitations and Broader Impacts\u201d of the Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: For the T2I checkpoint release, we plan to use HuggingFace to enroll every users to the system so to control the downloaded user list. Additionally, we prohibited using the LAION dataset [33], which includes NSFW contents. Instead, we used the COYO700M [51] dataset, a large-scale text-to-image dataset that removes NSFW images by NSFW image detectors [65, 66] and texts that contain NSFW words [67\u201369]. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have properly credited the original owners of assets by citing them. In the code release, we comply the license and terms of the assets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In the supplementary, we include the the details of the dataset/code/model via structured templates. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1 Introduction ", "page_idx": 22}, {"type": "text", "text": "2 Preliminary 3 ", "page_idx": 22}, {"type": "text", "text": "3 Progressive Growing of Diffusion Autoencoder 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "3.1 Stage 1: Diffusion Models Trained on Downsampled Data 3   \n3.2 Stage 2: Diffusion Distillation on Downsampled Data with DDIM Inversion 3   \n3.3 Stage 3: Progressively Growing Decoder for Super-Resolution 4   \n3.4 Optimality Guarantee and Training Stability of PaGoDA Pipeline 5 ", "page_idx": 22}, {"type": "text", "text": "4 PaGoDA with Classifier-Free Guidance 5 ", "page_idx": 22}, {"type": "text", "text": "4.1 Classifier-Free Guided Adversarial Loss 6   \n4.2 PaGoDA Pipeline with Classifier-Free Guidance . 6 ", "page_idx": 22}, {"type": "text", "text": "5 Experiments 6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "5.1 PaGoDA Tested on ImageNet without CFG 6   \n5.1.1 Quantitative Results 7   \n5.1.2 Discussion on Base Resolution 7   \n5.1.3 Discussion on Upscaling Capability 8   \n5.2 Discussion on Controllability 9   \n5.3 Text-to-Image Generation 10 ", "page_idx": 22}, {"type": "text", "text": "6 Conclusion 10 ", "page_idx": 22}, {"type": "text", "text": "A Experimental Details 24 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A.1 Conditional Generation with ImageNet 24   \nA.2 Text-to-Image Generation 25 ", "page_idx": 22}, {"type": "text", "text": "B Theoretical Analysis 28 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 Convergence with PaGoDA\u2019s Reconstruction Loss 28   \nB.1.1 Preliminaries of Convergence Analysis 28   \nB.1.2 $W_{2}$ Bound with PaGoDA\u2019s Reconstruction Loss . 29   \nB.1.3 $W_{1}$ Bound with PaGoDA\u2019s Reconstruction Loss . . 33   \nB.2 Optimality analysis 34   \nB.3 Stability Analysis 35   \nB.3.1 Preliminaries of Dynamical System 35   \nB.3.2 Preliminaries for Analysis of PaGoDA Training 36   \nB.3.3 PaGoDA\u2019s Training is Stable . . 37   \nB.3.4 Literature on Stability Analysis of Adversarial Training . 40 ", "page_idx": 22}, {"type": "text", "text": "C Limitations and Broader Impacts 42 ", "page_idx": 22}, {"type": "image", "img_path": "h5zYGF68KH/tmp/c841c76c98312878076f76b7ebe1dadd069df1204d4e1641c5c2e8c80033227a.jpg", "img_caption": ["Figure 10: Text-to-image samples from PaGoDA. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A.1 Conditional Generation with ImageNet ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Throughout the experiments, we omit the class condition c otherwise mentioned for notational simplicity. ", "page_idx": 23}, {"type": "text", "text": "Dataset Construction. We loaded ImageNe $2014^{4}$ dataset using center cropping and downsampling using the bicubic algorithm from the PIL python package. To augment the data, we applied a horizontal random flip, and obtained each of latent representations by solving the EDM\u2019s 2nd-order ODE sampler (Heun\u2019s method) [13] with their suggested diffusion time scheduling and timestep selection. Consequently, in total, we processed approximately 2.5 million data instances forward in time using the PF-ODE to prepare for training. This computational cost of constructing the training dataset is comparable to sampling an equivalent volume of sample from a pre-trained diffusion model. ", "page_idx": 23}, {"type": "text", "text": "GAN Details. We adopted the discriminator architecture from StyleGAN-XL. Initially, We loaded DeiT-base [70] and EfficientNet-lite [71] as feature extractors, in line with StyleGAN-XL\u2019s setup. When processing real or fake data through the discriminator, we first applied differentiable augmentation (DiffAugment) [72], incorporating three transformations: Translation, Cutout, and Color. Interestingly, we observed no performance differences between the unconditional and conditional discriminators. We hypothesize that this lack of disparity arises because the discriminator primarily updates the generator to refine high-frequency details, while preserving the low-frequency global semantics due to the reconstruction power. Additionally, we opted not to use additional techniques to tame the GAN training, such as R1 regularization [21] or path length regularization [20] in our GAN training. PaGoDA\u2019s training generally remains stable due to its reconstruction loss, which is consistent with our theoretical expectation (Theorem B.9). ", "page_idx": 23}, {"type": "text", "text": "We conducted tests on GANs under two distinct scenarios. Initially, following the approach used in Stable Diffusion\u2019s VAE training, we introduced both the real data $\\mathbf{x}$ and the reconstructed sample $\\tilde{\\mathbf{x}}\\,=\\,G_{\\pmb\\theta}(E(\\mathbf{x}))$ to the discriminator, training it to differentiate between the two while updating the generator to maximize $\\log D_{\\psi}(\\tilde{\\mathbf{x}})$ . In this setup, as the reconstruction only utilizes the latent representation $E({\\bf x})$ , the generation quality is not improved. ", "page_idx": 23}, {"type": "text", "text": "In the second scenario, adhering to the traditional GAN framework, we trained the discriminator using randomly sampled real data alongside randomly generated fake data $\\tilde{\\mathbf{x}}=G_{\\theta}(\\mathbf{z})$ from $\\mathbf{z}\\sim p_{\\mathrm{prior}}(\\mathbf{z})$ . Then, the endeavor of maximizing $\\log D_{\\psi}(\\tilde{\\mathbf{x}})$ now significantly improves the generation quality. Overall, we observed no performance degradation when both types of GAN training were applied to ", "page_idx": 23}, {"type": "text", "text": "Table 6: Comparison on ImageNet $64\\times64$ . We evaluate scores, including Fr\u00e9chet distance on DINOv2 features [74], based on the statistics released by EDM2. The validation scores are measured by comparing 50k samples and 50k ImageNet validation data. ", "page_idx": 24}, {"type": "table", "img_path": "h5zYGF68KH/tmp/be284e5b9566e07afe4f644626b3128e1678a4c3d60385f528c2817c8fc8274d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "h5zYGF68KH/tmp/a5955d1269688267ef6f1c1ed75fd103a8db024af813e5fc14b53a5127222657.jpg", "table_caption": ["Table 7: Comparison on ImageNet $512\\times512$ . From this result, it would be interesting to experiment PaGoDA on EDM2 architecture for better performance. \\* the results of ImageNet $256\\times256$ . ", "Figure 11: Input prompt of LLaVA to recaption the text-image paired data. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. You should follow the instructions carefully and explain your answers in detail. Given the caption of this image \"{text prompt}\", describe this image in a very detailed manner the generator. However, given our limited budget and the goal to develop a generative model rather than a compression model, we opted to proceed solely with the second type of GAN setup. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Reconstruction Details. For the reconstruction loss, we train the generator $G_{\\theta}$ by comparing the original data $\\mathbf{x}\\sim p_{\\mathrm{data}}(\\mathbf{x})$ and its reconstructed counterpart $G_{\\pmb\\theta}(E(\\mathbf x,\\mathbf c),\\mathbf c)$ at the data\u2019s resolution, where $E(\\mathbf{x},\\mathbf{c})$ is the solution of the DDIM inversion. Since our training occurs in pixel space, we conduct this comparison in the feature space using the Learned Perceptual Image Patch Similarity (LPIPS) metric, and there is no need to develop a new feature extractor in latent space. We experimented with features extracted from DeiT-base [70] and EfficientNet-lite [71]; however, we observed no notable improvement from using LPIPS. ", "page_idx": 24}, {"type": "text", "text": "For the training, we use the RAdam [73] with learning rate of 8e-6 for the decoder and 2e-3 for the discriminator, and without weight decay. We use the EMA of 0.999, and all reported FIDs are based on the EMA checkpoint. Until $256^{2}$ resolution, we use only 1 H100 node (with 80Gb memory) to train, and we use 8 A100 nodes (with 40Gb memory, in total $8\\times8=64$ GPUs) to train the $\\mathrm{\\dot{5}12^{2}}$ model. Throughout the experiments, we use the batch size of 256. ", "page_idx": 24}, {"type": "text", "text": "For the concerns on the overfitting, we provide additional results in Tables 6 and 7. ", "page_idx": 24}, {"type": "text", "text": "A.2 Text-to-Image Generation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Dataset Construction. Due to the presence of inappropriate contents (CSAM) in the LAION dataset [33], we have decided to discontinue its use. Instead, we are now training our model using the CC12M [50] and a filtered version of COYO-700M [51] datasets. For COYO-700M, we apply fliters to select only those text-image pairs that meet specific criteria: a CLIP score (measured by ViTB/32 [52]) above 32.0 and an aesthetic score-v2 [33] higher than 5.0. Additionally, we are enhancing the dataset quality by recaptioning the original text prompts from CC12M, adopting practices similar to those used in DallE-3 [75] and PixArt- $\\alpha$ [62]. Specifically, we employ LLaVA-7B [76], a language ", "page_idx": 24}, {"type": "image", "img_path": "h5zYGF68KH/tmp/6d4d2355ddfd5d2d265c5c12e9773453bd005f6e6b6d55c498c3b7d0e48c0bbf.jpg", "img_caption": ["Figure 12: Example of recaptioned image-text pair. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "h5zYGF68KH/tmp/46a27e29ca6f30514e995dbc0bf2b00be7738b7b8ef2b29684cbc42ae3f47021.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 13: Caption vs. Recaption. From left to right, CFG scale increases. The caption and its corresponding recaption are given by the exemplary case in Figure 12. ", "page_idx": 25}, {"type": "text", "text": "model with vision assistance, to generate descriptions of the images based on the text prompts, thereby ensuring more relevant and accurate text-image pairings. ", "page_idx": 25}, {"type": "text", "text": "The input prompt of LLaVA is depicted in Figure 11, where we put text prompt to $\\{{\\tt t e x t\\ p r o m p t}\\}$ . The output from this recaptioning process adheres to a consistent format, typically beginning with phrases like \u201cThis image features ...\u201d or \u201cThis image shows ...\u201d. To provide clear demonstration, Figure 13 displays several examples of original captions alongside their recaptioned counterparts. ", "page_idx": 25}, {"type": "text", "text": "Interestingly, the recaptioned samples generally outperform the original caption samples. Notably, the recaptioned samples exhibit sufficient quality, particularly when the CFG scale is small, as shown in Figure 14. Therefore, to ensure balanced generation performance across varying CFG scales, we generate samples from the original captions with the CFG scale uniformly sampled from the range [2, 10]. For the recaptioned text, we use a CFG scale that follows a truncated Gaussian distribution on the range [1, 10], centered at 2 with a scale of 3. Overall, incorporating these recaptioned texts into the PaGoDA training results in only a marginal improvement in performance metrics such as FID and CLIP. However, it significantly enhances the actual quality of generation, particularly at smaller CFG scales, because the recaptioning provides better-aligned training data. ", "page_idx": 25}, {"type": "image", "img_path": "h5zYGF68KH/tmp/9ebb65a1f27095bf955301d67c03313e602d08836a672d366270df6e901830ed.jpg", "img_caption": ["Figure 14: Effect of recaptioning. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Using LLaVA, we recaption $\\tilde{\\mathbf{c}}(\\mathbf{x},\\mathbf{c})$ and obtain the DDIM latent representation, $E(\\mathbf{x},\\tilde{\\mathbf{c}}(\\mathbf{x},\\mathbf{c}))$ , on the entire CC12M dataset. Then, for the original text c, we have a triplet of (image, text, latent) of $({\\bf x},{\\bf c},E({\\bf x},{\\bf c}))$ for one set, and another triplet $({\\bf x},\\tilde{{\\bf c}}({\\bf x},{\\bf c}),E({\\bf x},\\tilde{{\\bf c}}({\\bf x},{\\bf c}))$ for recaptioned dataset. When computing ${\\mathcal L}_{\\mathrm{rec}}$ , we mix these triplets and randomly sample from this mixed dataset. ", "page_idx": 25}, {"type": "text", "text": "GAN Details. Similar to the ImageNet case, we have adopted the discriminator architecture from StyleGAN-T. In line with StyleGAN-T, we utilize the DINO ViT-S/16 [77] as the feature extractor and apply DiffAugment [72], incorporating Translation, Cutout, and Color transformations. Building upon this architecture, we integrate a $\\omega$ condition into each discriminator head, as illustrated in Figure 15. The inputs for each discriminator head include the DINO feature, text CLIP embedding, and the CFG scale $\\omega$ , which is scaled by a factor of 100. We handle the CFG scale similarly to the time variable in traditional diffusion U-Net models, incorporating the output CFG embedding into the existing components of the StyleGAN-T discriminator head. We assume both image $\\mathbf{x}$ and text c are related with the CFG scale, thus we designed the discriminator to incorporate $\\omega$ information into both modules, enhancing the relevance and contextuality of the discrimination process. ", "page_idx": 25}, {"type": "image", "img_path": "h5zYGF68KH/tmp/96145120d6c389a1b2b2f346e41c53736ca73f78f19eb40c58e2bdae18324db2.jpg", "img_caption": ["Figure 15: Discriminator architecture. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Reconstruction Details. In our text-to-image training, we largely adhere to the protocols established for ImageNet training. However, a notable modification involves the decoder network, which now incorporates a $\\omega$ condition as an auxiliary input. Crucially, this $\\omega$ condition is processed in decoder in the same way as the time condition in diffusion models. We achieve this by scaling $\\omega$ by a factor of 100, thus aligning it with the existing time ranges. This method ensures a consistent treatment of the $\\omega$ parameter, integrating it smoothly into the established model architecture. ", "page_idx": 26}, {"type": "text", "text": "CLIP Details. Neither the reconstruction loss nor the GAN loss directly models or maximizes the text-image correlation. To address this, we introduce an additional text-image alignment metric to train our model. Specifically, we employ ViT-L/14 [52] to assess the CLIP value. This regularization significantly enhances PaGoDA\u2019s performance, as evidenced in Figure 16 by improving both FID and CLIP scores. These enhancements suggest that not only is the sample quality improving, but also the alignment between text and images is becoming more accurate. ", "page_idx": 26}, {"type": "text", "text": "For the training, we use the AdamW8bit optimizer [78] to minimize the required memory with learning rate of 1e-5 for both decoder and discriminator. Similar to the ImageNet experiment, we do not apply the weight decay. In this text-to-image experiment, we do not use EMA, following previous works [61]. In the base resolution, we use the adaptive weighting with \u03bb = 4\u2225\u2225\u2207\u2207\u03b8lLLrec\u2225\u222522. Overall, we use the DeepFloyd-IF-I model with 0.9B number of parameters. ", "page_idx": 26}, {"type": "text", "text": "Figure 17 compares PaGoDA with the existing baselines. ", "page_idx": 26}, {"type": "image", "img_path": "h5zYGF68KH/tmp/8b0aff1f918ca88bdd607fa20594abcfb30472c73f08e515f35339aaa1386e7a.jpg", "img_caption": ["Figure 16: Effect of CLIP regularization. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "h5zYGF68KH/tmp/13ae241e1b35cd321fb8f9d26ddeb2986cf3c460316603ebfb15c9e04e840887.jpg", "img_caption": ["Figure 17: Human evaluation result on T2I with CFG set to be 7 across models. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "B Theoretical Analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we present rigorous statements and proofs of all theorems. The theorems are shown for the unconditional generation case (i.e., without the condition c), but the analysis can be extended to the conditional scenario. ", "page_idx": 27}, {"type": "text", "text": "B.1 Convergence with PaGoDA\u2019s Reconstruction Loss ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Section B.1.1, we introduce the necessary notations and preliminaries. In Sections B.1.2 and B.1.3, we demonstrate that the Wasserstein-1 and Wasserstein-2 discrepancies of the learned density (with PaGoDA\u2019s reconstruction loss) from $p_{\\mathrm{data}}$ are upper bounded by PaGoDA\u2019s reconstruction loss and the pre-trained DM\u2019s training error. All results are proved for unconditional generation (i.e., without c as an input), but they can be easily generalized to the conditional case. ", "page_idx": 27}, {"type": "text", "text": "B.1.1 Preliminaries of Convergence Analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Consider OU process for $t\\in[0,T]$ , where $T>0$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-f(t)\\mathbf{x}_{t}\\,\\mathrm{d}t+g(t)\\,\\mathrm{d}\\mathbf{w}_{t}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Its associated PF-ODE is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\big[-f(t)\\mathbf{x}_{t}-\\frac{1}{2}g^{2}(t)\\nabla\\log p_{t}(\\mathbf{x}_{t})\\big]\\,\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We consider $f(t)\\equiv1$ and $g(t)\\equiv\\sqrt{2}$ for simplicity. That is, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\mathbf{x}_{t}\\,\\mathrm{d}t+\\sqrt{2}\\,\\mathrm{d}\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We recall that PaGoDA\u2019s reconstruction loss (unconditional case) is defined as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rec}}(\\pmb{\\theta};\\phi_{0}):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\Big[\\big\\|\\mathbf{x}-G_{\\pmb{\\theta}}^{T\\rightarrow0}(\\mathbf{z})\\big\\|_{2}^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, we use $p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})$ to denote the density obtained by solving the pre-trained teacher DM\u2019s empirical PF-ODE forward in time from $t=0$ to $t=T$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\big[-f(t)\\mathbf{x}_{t}-\\frac{1}{2}g^{2}(t)\\mathbf{s}_{\\phi_{0}}(\\mathbf{x}_{t},t)\\big]\\,\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathbf{s}_{\\phi}(\\mathbf{x}_{t},t)$ indicates the pre-trained DM. We remark that $p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})$ defines a deterministic process. ", "page_idx": 27}, {"type": "text", "text": "dWiset triabkue $p_{\\mathrm{prior}}:=\\mathcal{N}\\big(\\mathbf{0},(1-e^{-2T})\\mathbf{I}\\big)$ taesa cthhee rp-rdieotre rdimstirniebdu tieomn,p iarnicda dl ePfFin-e $p_{T,\\phi_{0}}:=G_{\\phi_{0}}^{0\\to T}\\sharp p_{\\mathrm{data}}$ Laes tt hues consider the density obtained by sampling from PaGoDA (trained without GAN) $p_{0,\\pmb{\\theta}}:=G_{\\pmb{\\theta}}^{T\\rightarrow0}\\sharp p_{\\mathrm{prior}}$ We also let $G^{T\\rightarrow0}$ denote the ground truth transition map from $T$ to 0, defined by the PF-ODE. ", "page_idx": 27}, {"type": "text", "text": "Conceptually, Theorems B.1 and B.3 demonstrate that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{p}(p_{0,\\pmb\\theta},p_{\\mathrm{data}})\\lesssim\\mathcal{L}_{\\mathrm{rec}}(\\pmb\\theta;\\phi_{0})+\\epsilon_{\\mathrm{DM}},\\quad p=1,2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that training with PaGoDA\u2019s reconstruction loss ensures the learned density $p_{0,\\pmb\\theta}\\,=$ $G_{\\theta}^{T\\rightarrow0}\\sharp p_{\\mathrm{prior}}$ is close to $p_{\\mathrm{data}}$ in Wasserstein distance sense. Moreover, improving the teacher DM to reduce the error $\\epsilon_{\\mathrm{DM}}$ is a way to further decrease the discrepancy between $p_{0,\\pmb\\theta}$ and $p_{\\mathrm{data}}$ . ", "page_idx": 28}, {"type": "text", "text": "We remark that the differences between the two theorems primarily lie in the distinct smoothness assumptions on pdata. ", "page_idx": 28}, {"type": "text", "text": "B.1.2 $W_{2}$ Bound with PaGoDA\u2019s Reconstruction Loss ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Assumption I-1. ", "page_idx": 28}, {"type": "text", "text": "(i) $m^{2}:=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left\\|\\mathbf{x}\\right\\|_{2}^{2}<\\infty;$ ", "page_idx": 28}, {"type": "text", "text": "(ii) There is a $\\epsilon_{\\mathrm{DSM}}>0$ so that $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x},t}\\|\\mathbf{s}_{\\phi_{0}}(\\mathbf{x},t)-\\nabla\\log p_{t}(\\mathbf{x})\\|_{2}^{2}\\leq\\epsilon_{\\mathrm{DM}}^{2};}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "(iii) $G_{\\pmb{\\theta}}^{T\\rightarrow0}$ is Lipschitz in $\\mathbf{x}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Lambda:=\\operatorname*{sup}_{\\mathbf{x}\\neq\\mathbf{y}}\\frac{\\left\\|G_{\\pmb{\\theta}}^{T\\rightarrow0}(\\mathbf{x})-G_{\\pmb{\\theta}}^{T\\rightarrow0}(\\mathbf{y})\\right\\|_{2}}{\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|_{2}}<\\infty,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $\\pmb{\\theta}$ and $T$ . ", "page_idx": 28}, {"type": "text", "text": "(iv) $\\log{p_{\\mathrm{data}}}$ is $\\gamma.$ -strongly concave with $\\gamma>3/2$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{x}-\\mathbf{y},\\nabla\\log p_{\\mathrm{data}}(\\mathbf{x})-\\nabla\\log p_{\\mathrm{data}}(\\mathbf{y})\\rangle\\leq-\\gamma\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $\\mathbf{x}$ and y. ", "page_idx": 28}, {"type": "text", "text": "Theorem B.1. Given that Assumption I-1 holds, suppose $\\delta$ is a positive constant such that $\\delta<$ $\\frac{e^{-2T}}{3-e^{-2T}}$ , and let $\\begin{array}{r}{h(\\gamma,T):=\\frac{\\gamma}{e^{-2T}+\\gamma(1-e^{-2T})}-(1+\\delta),}\\end{array}$ , where it is noted that $h(\\gamma,T)$ is also positive. Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(p_{0,\\theta},p_{d a t a})\\leq\\mathcal{L}_{r e c}(\\pmb{\\theta};\\phi_{0})+\\Big[\\mathbb{E}_{p_{d a t a}(\\pmb{\\mathbf{x}})p_{\\phi_{0}}(\\pmb{\\mathbf{z}}|\\pmb{\\mathbf{x}})}\\big\\|\\pmb{\\mathbf{x}}-G^{T\\rightarrow0}(\\pmb{\\mathbf{z}})\\big\\|_{2}^{2}\\Big]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\big(\\Lambda+e^{-\\frac{1}{2}h(\\gamma,T)T}\\big)W_{2}\\big(p_{T},p_{T,\\phi_{0}}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{\\epsilon_{D M}}{\\sqrt{2\\delta h(\\gamma,T)}}\\big(1-e^{-h(\\gamma,T)T}\\big)^{\\frac{1}{2}}+e^{-\\frac{T}{2}}m\\Lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In particular, $i f$ we assume Assumption I-1 (iii) holds also for $G^{T\\rightarrow0}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nW_{2}(p_{0,\\pmb\\theta},p_{d a t a})\\lesssim\\mathcal{L}_{r e c}(\\pmb\\theta;\\phi_{0})+\\epsilon_{D M}+e^{-\\frac{T}{2}}m\\Lambda.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, we use $\\lesssim$ to absorb the dependence on the constants $T$ and $\\gamma$ into the inequality. ", "page_idx": 28}, {"type": "text", "text": "We present an inequality which is essential for the proof of Theorem B.1. ", "page_idx": 28}, {"type": "text", "text": "Lemma B.2 (Proposition 3.5. in [79]). Let $P$ and $Q$ be two distributions on $\\mathbb{R}^{D}$ . Suppose that log $P$ is $\\gamma_{P}$ -concave and $\\log Q$ is $\\gamma_{Q}$ -concave. Then the convolution of $\\log{P}*Q$ is $a$ $(1/\\bar{\\gamma}_{P}+1/\\gamma_{Q})^{-1}$ - concave distribution. ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem B.1. The proof of the theorem is inspired by [80, 81]. Define $p_{T,\\phi_{0}}~:=$ $G_{\\phi_{0}}^{0\\to T}\\sharp p_{\\mathrm{data}}$ , and $p_{0,\\phi_{0}}:=G^{T\\rightarrow0}\\sharp p_{T,\\phi_{0}}$ . From the triangle inequality, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{2}(p_{0,\\theta},p_{\\mathrm{data}})\\le\\underbrace{W_{2}(p_{0,\\theta},p_{0,\\phi_{0}})}_{(A)}+\\underbrace{W_{2}(p_{0,\\phi_{0}},p_{\\mathrm{data}})}_{(B)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For (A), let $\\pi(\\mathbf{y},\\mathbf{z})\\in\\Pi\\big(p_{\\mathrm{prior}},p_{T,\\phi_{0}}\\big)$ be a coupling of $p_{\\mathrm{prior}}$ and $p_{T,\\phi_{0}}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A)=W_{2}\\big(G_{\\theta}^{T\\rightarrow0}{\\mathfrak{j}}p_{\\mathfrak{p r i o r}},G^{T\\rightarrow0}{\\mathfrak{j}}p_{T,\\phi_{0}}\\big)}\\\\ &{\\quad\\quad\\leq\\Big({\\mathbb{E}}_{(\\mathbf{y},\\mathbf{z})\\sim\\pi}\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{y})-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\leq\\underbrace{\\Big({\\mathbb{E}}_{(\\mathbf{y},\\mathbf{z})\\sim\\pi}\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{y})-G_{\\theta}^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}_{(A~1)}+\\underbrace{\\Big({\\mathbb{E}}_{(\\mathbf{y},\\mathbf{z})\\sim\\pi}\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{z})-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}_{(A~9)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For (A.1), we can yield ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A.1)\\leq\\Lambda\\underset{\\pi\\in\\Pi\\big(p_{\\mathrm{piar}},p_{T},\\phi_{0}\\big)}{\\operatorname*{min}}\\Big(\\mathbb{E}_{(\\mathbf{y},\\mathbf{z})\\sim\\pi}\\left\\|\\mathbf{y}-\\mathbf{z}\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{\\qquad=\\Lambda W_{2}\\big(p_{\\mathrm{pior}},p_{T},\\phi_{0}\\big)}\\\\ &{\\qquad\\leq\\Lambda W_{2}\\big(p_{\\mathrm{pior}},p_{T}\\big)+\\Lambda W_{2}\\big(p_{T},p_{T,\\phi_{0}}\\big)}\\\\ &{\\qquad\\leq e^{-\\frac{T}{2}}\\big(\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x}_{0})}\\left\\|\\mathbf{x}_{0}\\right\\|_{2}^{2}\\big)^{\\frac{1}{2}}\\Lambda+\\Lambda W_{2}\\big(p_{T},p_{T,\\phi_{0}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, the last inequality is a consequence of the following bound ", "page_idx": 29}, {"type": "equation", "text": "$$\nW_{2}(p_{\\mathrm{prior}},p_{T})\\leq e^{-\\frac{T}{2}}\\bigl(\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{0})}\\left\\|\\mathbf{x}_{0}\\right\\|_{2}^{2}\\bigr)^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which holds because $p_{\\mathrm{prior}}$ is taken as $\\mathcal{N}\\big(\\mathbf{0},(1-e^{-2T})\\mathbf{I}\\big)$ , and $\\mathbf{x}_{T}\\sim p_{T}$ governed by Eq. (5) admits the expression ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\bf x}_{T}=e^{-T}{\\bf x}_{0}+\\int_{0}^{T}e^{-(T-s)}\\sqrt{2}\\,{\\mathrm{d}}{\\bf w}_{s}=e^{-T}{\\bf x}_{0}+{\\bf z},\\quad{\\bf z}\\sim{\\mathcal N}\\big({\\bf0},(1-e^{-2T}){\\bf I}\\big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For (A.2), since $\\begin{array}{r}{p_{T,\\phi_{0}}(\\mathbf{z})=\\int p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})p_{\\mathrm{data}}(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}}\\end{array}$ , by applying Minkowski inequality we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A.2)=\\Big(\\mathbb{E}_{(\\mathbf{y},\\mathbf{z})\\sim\\pi}\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{z})-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{\\quad\\quad=\\Big(\\mathbb{E}_{\\mathbf{z}\\sim p_{T},\\phi_{0}}(\\mathbf{z})\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{z})-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\leq\\Big(\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\left\\|G_{\\theta}^{T\\rightarrow0}(\\mathbf{z})-\\mathbf{x}\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}+\\Big(\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\left\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big)^{\\frac{1}{2}}}\\\\ &{\\quad\\quad=\\mathcal{L}_{\\mathrm{rec}}(\\theta;\\phi_{0})+\\Big[\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\left\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\Big]^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The proof for (B) is motivated by [80]. Consider the following two reverse time PF-ODEs on the interval $[0,T]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\hat{\\mathbf{z}}_{t,\\phi_{0}}}{\\mathrm{d}t}=\\hat{\\mathbf{z}}_{t,\\phi_{0}}+\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{t,\\phi_{0}},T-t),\\quad\\hat{\\mathbf{z}}_{0,\\phi_{0}}\\sim p_{T,\\phi_{0}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\hat{\\mathbf{z}}_{t}}{\\mathrm{d}t}=\\hat{\\mathbf{z}}_{t}+\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t}),\\quad\\hat{\\mathbf{z}}_{0}\\sim p_{T},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with a coupling of $\\hat{\\mathbf{z}}_{0,\\phi_{0}}~\\sim~p_{T,\\phi_{0}}$ and $\\hat{\\mathbf{z}}_{0}\\ \\sim\\ p_{T}$ so that $W_{2}^{2}(p_{T,\\phi_{0}},p_{T})\\ =\\ \\mathbb{E}\\left\\lVert\\hat{{\\mathbf{z}}}_{0,\\phi_{0}}-\\hat{{\\mathbf{z}}}_{0}\\right\\rVert_{2}^{2}$ . We notice that $W_{2}^{2}(p_{0,\\phi_{0}},p_{\\mathrm{data}})\\,\\leq\\,\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}\\right\\|_{2}^{2}$ . Thus, we need to obtain a upper bound of $\\mathbb{E}\\left\\lVert\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}\\right\\rVert_{2}^{2}$ . Let $u(t):=\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t}\\right\\|_{2}^{2}$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}u(t)=\\,2\\mathbb{E}\\langle\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t},\\frac{\\mathrm{d}}{\\mathrm{d}t}\\big(\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t}\\big)\\rangle}\\\\ &{\\quad\\quad\\quad=\\,2u(t)+2\\mathbb{E}\\Big[\\langle\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t},\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{t,\\phi_{0}},T-t)-\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t})\\rangle\\Big]}\\\\ &{\\quad\\quad\\quad=\\,2u(t)+2\\underbrace{\\mathbb{E}\\Big[\\langle\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t},\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{t,\\phi_{0}},T-t)-\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t,\\phi_{0}})\\rangle\\Big]}_{(B.1)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,2\\mathbb{E}\\Big[\\langle\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t},\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t,\\phi_{0}})-\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t})\\rangle\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\delta>0$ , by applying Yang\u2019s inequality $\\begin{array}{c c c}{{a b=\\left(\\sqrt{2\\delta}a\\right)\\left(\\frac{b}{\\sqrt{2\\delta}}\\right)\\le\\delta a^{2}+\\frac{b^{2}}{4\\delta}}}\\end{array}$ to (B.1) for nonnegative $a$ and $b$ , and the Assumption I-1, it becomes ", "page_idx": 29}, {"type": "equation", "text": "$$\n(B.1)\\leq\\ \\delta u(t)+\\frac{\\epsilon_{\\mathrm{DM}}^{2}}{4\\delta}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We turn our attention to (B.2). Naively, (B.2) may be naively bounded above by $\\mathrm{Lip}\\big(\\nabla\\log p_{t}(\\cdot)\\big)u(t)$ , where $\\mathrm{Lip}\\big(\\nabla\\log p_{t}(\\cdot)\\big)$ is the Lipschitz constant of $\\nabla\\log{p_{t}(\\cdot)}$ in $\\mathbf{x}$ . However, we will now derive a sharper bound by incorporating assumptions on the data distribution. ", "page_idx": 30}, {"type": "text", "text": "We notice that $\\begin{array}{r}{p_{t}(\\mathbf{x}_{t})\\ =\\ \\int p_{t|0}(\\mathbf{x}_{t}|\\mathbf{x}_{0})p_{\\mathrm{data}}(\\mathbf{x}_{0})\\,\\mathrm{d}\\mathbf{x}_{0}}\\end{array}$ , where $p_{t|0}(\\mathbf{x}_{t}|\\mathbf{x}_{0})\\ =\\ N\\big(\\mathbf{x}_{t};e^{-t}\\mathbf{x}_{0},(1\\mathrm{~-~}$ $e^{-2t})\\mathbf{I})$ is a transition kernel from 0 to $t$ determined by the forward SDE. Therefore, expressing $p_{t}$ in convolution form, under Assumption I-1, and leveraging Lemma B.2, we deduce that $\\log{p_{T-t}}$ is a $\\gamma/\\Bigl(e^{-2(T-t)}+\\gamma\\bigl(1-e^{-2(T-t)}\\bigr)\\Bigr)$ -strongly concave distribution (see [82]). Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n(B.2)\\leq-\\frac{\\gamma}{e^{-2(T-t)}+\\gamma(1-e^{-2(T-t)})}u(t).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "With the inequalities (9) and (10), we deduce from Eq. (8) that ", "page_idx": 30}, {"type": "equation", "text": "$$\nu^{\\prime}(t)\\leq a(t)u(t)+\\frac{\\epsilon_{\\mathrm{DM}}^{2}}{2\\delta},\\quad\\mathrm{where}\\quad a(t):=\\Big(2+2\\delta-\\frac{2\\gamma}{e^{-2(T-t)}+\\gamma(1-e^{-2(T-t)})}\\Big).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By applying Gr\u00f6nwall\u2019s inequality, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}\\right\\|_{2}^{2}\\leq e^{A(T)}\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{0,\\phi_{0}}-\\hat{\\mathbf{z}}_{0}\\right\\|_{2}^{2}+\\displaystyle\\frac{\\epsilon_{\\mathrm{DM}}^{2}}{2\\delta}\\int_{0}^{T}e^{A(T)-A(t)}\\,\\mathrm{d}t,}\\\\ &{\\qquad\\qquad\\qquad=e^{A(T)}W_{2}^{2}(p_{T,\\phi_{0}},p_{T})+\\displaystyle\\frac{\\epsilon_{\\mathrm{DM}}^{2}}{2\\delta}\\int_{0}^{T}e^{A(T)-A(t)}\\,\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\begin{array}{r}{A(t):=\\int_{0}^{t}a(s)\\,\\mathrm{d}s}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "We aim to find an upper bound for inequality (11) that decays exponentially with respect to $T$ . In $a(t)$ , $\\begin{array}{r}{b(t):=\\frac{\\gamma}{e^{-2(T-t)}+\\gamma\\left(1-e^{-2(T-t)}\\right)}}\\end{array}$ as a function of $t$ has the derivative as $\\begin{array}{r}{\\frac{2\\gamma(\\gamma-1)e^{-2(T-t)}}{\\left(e^{-2(T-t)}+\\gamma(1-e^{-2(T-t)})\\right)^{2}}}\\end{array}$ This implies when $\\gamma\\,\\geq\\,1$ , $b$ \u2019s minimum occurs at \u03b3+e\u22122\u03b3T (1\u2212\u03b3), which implies a(t) \u2264 $2\\big(1+\\delta-b(0)\\big)$ for all $t\\,\\in\\,[0,T]$ . Setting $\\begin{array}{r}{\\delta\\,<\\,\\frac{e^{-2T}}{3-e^{-2T}}}\\end{array}$ , which implies $\\textstyle{\\frac{1}{2}}~>~\\frac{\\delta}{(1+\\delta)e^{-2T}-\\delta}$ , then $\\begin{array}{r}{\\gamma>\\frac{3}{2}=1+\\frac{1}{2}>1+\\frac{\\delta}{(1+\\delta)e^{-2T}-\\delta}}\\end{array}$ (notice that $(1+\\delta)e^{-2T}-\\delta>2\\delta)$ , we can deduce that ", "page_idx": 30}, {"type": "equation", "text": "$$\na(t)\\le1+\\delta-\\frac{\\gamma}{e^{-2T}+\\gamma(1-e^{-2T})}<0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\begin{array}{r}{h(\\gamma,T):=\\frac{\\gamma}{e^{-2T}+\\gamma\\left(1-e^{-2T}\\right)}-(1+\\delta)>0.}\\end{array}$ . Then we establish that $a(t)\\leq-h(\\gamma,T),A(T)\\leq$ $-h(\\gamma,T)T$ , and $A(T)-A(t)\\leq-h(\\gamma,T)t$ which implies $\\begin{array}{r}{\\int_{0}^{T}e^{A(T)-A(t)}\\,\\mathrm{d}t\\leq1-e^{-h(\\gamma,T)T}}\\end{array}$ . By applying the above bounds and inequality (11), (B) becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(B)\\leq\\big(\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}\\right\\|_{2}^{2}\\big)^{\\frac{1}{2}}}\\\\ &{\\qquad\\leq\\bigg(e^{-h(\\gamma,T)T}W_{2}^{2}(p_{T,\\phi_{0}},p_{T})+\\frac{\\epsilon_{\\mathrm{DM}}^{2}}{2\\delta h(\\gamma,T)}\\big(1-e^{-h(\\gamma,T)T}\\big)\\bigg)^{\\frac{1}{2}}}\\\\ &{\\qquad\\leq e^{-\\frac{1}{2}h(\\gamma,T)T}W_{2}(p_{T,\\phi_{0}},p_{T})+\\frac{\\epsilon_{\\mathrm{DM}}}{\\sqrt{2\\delta h(\\gamma,T)}}\\big(1-e^{-h(\\gamma,T)T}\\big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, the last inequality is from a simple inequality ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for nonnegative $a$ and $b$ . By combining inequalities (6), (7), and (12), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(p_{0,\\theta},p_{\\mathrm{data}})\\leq e^{-\\frac{T}{2}}\\big(\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{0})}\\left\\|\\mathbf{x}_{0}\\right\\|_{2}^{2}\\big)^{\\frac{1}{2}}\\Lambda+\\Lambda W_{2}\\big(p_{T},p_{T,\\phi_{0}}\\big)}\\\\ &{\\phantom{\\quad}+\\mathcal{L}_{\\mathtt{p_{a G o D A}}}(\\theta;\\phi_{0})+\\Big[\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\big\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\big\\|_{2}^{2}\\Big]^{\\frac{1}{2}}}\\\\ &{\\phantom{\\quad}+e^{-\\frac{1}{2}h(\\gamma,T)T}W_{2}\\big(p_{T,\\phi_{0}},p_{T}\\big)+\\frac{\\epsilon_{\\mathrm{DM}}}{\\sqrt{2\\delta h(\\gamma,T)}}\\big(1-e^{-h(\\gamma,T)T}\\big)^{\\frac{1}{2}}}\\\\ &{\\phantom{\\quad}=\\mathcal{L}_{\\mathrm{rec}}(\\theta;\\phi_{0})+\\Big[\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\big\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\big\\|_{2}^{2}\\Big]^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\left(\\Lambda+e^{-\\frac{1}{2}h(\\gamma,T)T}\\right)W_{2}\\big(p_{T},p_{T,\\phi_{0}}\\big)}}\\\\ {{\\displaystyle+\\,\\frac{\\epsilon_{\\mathrm{DM}}}{\\sqrt{2\\delta h(\\gamma,T)}}\\big(1-e^{-h(\\gamma,T)T}\\big)^{\\frac{1}{2}}+e^{-\\frac{T}{2}}m\\Lambda.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This shows the first inequality in Theorem B.1. ", "page_idx": 31}, {"type": "text", "text": "Now, we show the second inequality in the statement of Theorem B.1. First, we establish an upper bound for $\\begin{array}{r}{\\left[\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\left|\\left|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\right|\\right|_{2}^{2}\\right]^{\\frac{1}{2}}}\\end{array}$ in terms of $\\epsilon_{\\mathrm{DM}}$ . Let $G_{\\phi_{0}}^{0\\to T}$ denote the transition map defined by the empirical PF-ODE defined by the teacher $p_{\\phi_{0}}(\\mathbf{x}|\\mathbf{z})$ , and $G^{0\\to T}$ denote the ground truth transition map defined by the PF-ODE from 0 to $T$ . Then we have $\\mathbf{x}=G^{T\\rightarrow0}(G^{0\\rightarrow T}(\\mathbf{x}))$ for all $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ , and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\cdot\\underline{{\\mathrm{i}}}_{p_{\\mathrm{fata}}(\\mathbf{x})}p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})\\left\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}^{2}\\right]^{1/2}=\\left[\\mathbb{E}_{p_{\\mathrm{fata}}(\\mathbf{x})}\\left\\|G^{T\\rightarrow0}(G^{0\\rightarrow T}(\\mathbf{x}))-G^{T\\rightarrow0}(G_{\\phi_{0}}^{0\\rightarrow T}(\\mathbf{x}))\\right\\|_{2}^{2}\\right]^{1/2}}\\\\ &{}&{\\leq\\Lambda\\left[\\mathbb{E}_{p_{\\mathrm{fata}}(\\mathbf{x})}\\big\\|G^{0\\rightarrow T}(\\mathbf{x})-G_{\\phi_{0}}^{0\\rightarrow T}(\\mathbf{x})\\big\\|_{2}^{2}\\right]^{1/2}.\\qquad\\qquad(13)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, we utilize the assumption that Assumption I-1 (iii) also holds for $G^{T\\rightarrow0}$ . ", "page_idx": 31}, {"type": "text", "text": "Consider the following two forward-time PF-ODEs on the interval $[0,T]$ , both starting from $\\mathbf{x}_{0}\\sim$ pdata: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-\\mathbf{x}_{t}-\\nabla\\log p_{t}(\\mathbf{x}_{t}),\\quad\\frac{\\mathrm{d}\\mathbf{x}_{t,\\phi_{0}}}{\\mathrm{d}t}=-\\mathbf{x}_{t,\\phi_{0}}-\\mathbf{s}_{\\phi_{0}}(\\mathbf{x}_{t,\\phi_{0}},t).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By subtracting them and integrating from 0 to $t$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t,\\phi_{0}}\\|_{2}\\leq\\underbrace{\\mathbf{x}_{0}-\\mathbf{x}_{0},\\phi_{0}}_{0}+\\int_{0}^{t}\\big\\|\\big(\\mathbf{x}_{\\tau}-\\mathbf{x}_{\\tau,\\phi_{0}}\\big)+\\big(\\nabla\\log p_{\\tau}(\\mathbf{x}_{\\tau})-\\mathbf{s}_{\\phi_{0}}(\\mathbf{x}_{\\tau,\\phi_{0}},\\tau)\\big)\\big\\|_{2}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{t}\\|\\mathbf{x}_{\\tau}-\\mathbf{x}_{\\tau,\\phi_{0}}\\|_{2}\\,\\mathrm{d}\\tau+\\epsilon_{\\mathrm{DM}}T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By applying Gr\u00f6nwall\u2019s inequality, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{x}_{t}-\\mathbf{x}_{t,\\phi_{0}}\\|_{2}\\leq T e^{T}\\epsilon_{\\mathrm{DM}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the above inequality with inequality (13), it implies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})\\left|\\left|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\right|\\right|_{2}^{2}\\right]^{1/2}\\leq\\Lambda T e^{T}\\epsilon_{\\mathrm{DM}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Next, we derive an upper bound for $W_{2}(p_{T,\\phi_{0}},p_{T})$ related to $\\epsilon_{\\mathrm{DM}}$ . Let $\\pi(\\hat{\\mathbf{z}},\\mathbf{z})$ be a coupling between $\\hat{\\mathbf{z}}\\sim p_{T,\\phi_{0}}=G_{\\phi_{0}}^{0\\to T}\\sharp p_{\\mathrm{data}}$ and $\\mathbf{z}\\sim p_{T}=G^{0\\to T}\\sharp p_{\\mathrm{data}}$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\nW_{2}^{2}(p_{T,\\phi_{0}},p_{T})=W_{2}^{2}\\big(G_{\\phi_{0}}^{0\\to T}\\sharp p_{\\mathrm{data}},G^{0\\to T}\\sharp p_{\\mathrm{data}}\\big)\\leq\\mathbb{E}_{\\pi(\\hat{\\bf z},{\\bf z})}\\,\\|\\hat{{\\bf z}}-{\\bf z}\\|_{2}^{2}\\leq\\big(T e^{T}\\epsilon_{\\mathrm{DM}}\\big)^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality is derived from the inequality (14). ", "page_idx": 31}, {"type": "text", "text": "Therefore, with the first conclusion of Theorem B.1 and inequalities (15) and (16), we derive ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{2}(p_{0,\\pmb\\theta},p_{\\mathrm{data}})\\lesssim\\mathcal{L}_{\\mathrm{rec}}(\\pmb\\theta;\\phi_{0})+\\epsilon_{\\mathrm{DM}}+e^{-\\frac{T}{2}}m\\Lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The proof can be easily extended in two directions: (1) a more general (VP)-SDE: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-f(t)\\mathbf{x}_{t}\\,\\mathrm{d}t+g(t)\\,\\mathrm{d}\\mathbf{w}_{t}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with $\\|f\\|_{L^{\\infty}(t;[0,T])},\\|g\\|_{L^{\\infty}(t;[0,T])}<\\infty$ , and (2) truncation at the least time $t=\\delta$ (instead of $t=0$ ), with an additional argument based on [81] ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(p_{\\delta},p_{\\mathrm{data}})\\leq\\left(\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{0})}\\mathbb{E}_{p_{\\mathrm{prior}}(\\xi)}\\left\\|\\left(1-e^{-\\delta}\\right)\\mathbf{x}_{0}+\\sqrt{1-e^{-2\\delta}}\\pmb{\\xi}\\right\\|_{2}^{2}\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leq\\left((1-e^{-\\delta})^{2}m^{2}+(1-e^{-2\\delta})D\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\lesssim(\\sqrt{D}\\vee m)\\sqrt{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $p_{\\delta}=G^{T\\rightarrow\\delta}\\sharp p_{\\mathrm{prior}}$ . ", "page_idx": 31}, {"type": "text", "text": "B.1.3 $W_{1}$ Bound with PaGoDA\u2019s Reconstruction Loss ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Assumption II-1. (i) $m:=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left\\|\\mathbf{x}\\right\\|_{2}<\\infty$ ; ", "page_idx": 32}, {"type": "text", "text": "(ii) There is a $\\epsilon_{\\mathrm{DSM}}>0$ so that $\\begin{array}{r}{\\operatorname*{sup}_{\\mathbf{x},t}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)-\\nabla\\log p_{t}(\\mathbf{x})\\|_{2}^{2}\\leq\\epsilon_{\\mathrm{DM}}^{2};}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "(iii) $G_{\\pmb{\\theta}}^{T\\rightarrow0}$ is Lipschitz in $\\mathbf{x}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Lambda:=\\operatorname*{sup}_{\\mathbf{x}\\neq\\mathbf{y}}\\frac{\\left\\|G_{\\pmb{\\theta}}^{T\\rightarrow0}(\\mathbf{x})-G_{\\pmb{\\theta}}^{T\\rightarrow0}(\\mathbf{y})\\right\\|_{2}}{\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|_{2}}<\\infty,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $\\pmb{\\theta}$ and $T$ . ", "page_idx": 32}, {"type": "text", "text": "(iv) $\\nabla\\log p_{t}(\\cdot)$ is Lipschitz in $\\mathbf{x}$ with integrable Lipschitz constant: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Lambda_{s}(t):=\\operatorname*{sup}_{\\mathbf{x}\\neq\\mathbf{y}}\\frac{\\|\\nabla\\log p_{t}(\\mathbf{x})-\\nabla\\log p_{t}(\\mathbf{y})\\|_{2}}{\\|\\mathbf{x}-\\mathbf{y}\\|_{2}}<\\infty,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $\\Lambda_{s}$ is an $L^{1}$ -integrable function on $(0,\\infty)$ . ", "page_idx": 32}, {"type": "text", "text": "In the following proposition, we prove a variant of Theorem B.1 which does not assume log-concavity of the data density (i.e., Assumption I-1 (iv)). ", "page_idx": 32}, {"type": "text", "text": "Theorem B.3 (Variant of Theorem B.1). Assume that Assumption II-1 holds. Let \u03bd be either the oracle data distribution pdata or an empirical distribution $\\begin{array}{r}{\\hat{p}_{d a t a,N}:=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{\\mathbf{x}_{i}}}\\end{array}$ , where $\\mathbf{x}_{i}\\sim p_{d a t a}$ for $i=1,\\cdot\\cdot\\cdot,N$ . Let the PaGoDA\u2019s reconstruction loss starting from $\\nu$ be defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r e c}(\\pmb{\\theta}_{\\nu};\\phi_{0}):=\\mathbb{E}_{\\nu(\\mathbf{x})p_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[\\left\\|\\mathbf{x}-G_{\\pmb{\\theta}_{\\nu}}^{T\\rightarrow0}(\\mathbf{z})\\right\\|_{2}\\big].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(p_{0,\\theta},\\nu)\\,\\le\\,\\mathcal{L}_{r e c}(\\theta_{\\nu};\\phi_{0})+\\mathbb{E}_{\\nu(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\Big[\\big\\|\\mathbf{x}-G^{T\\to0}(\\mathbf{z})\\big\\|_{2}\\Big]+C_{T}T\\epsilon_{D M}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\big(C_{T}+\\Lambda\\big)W_{1}(p_{T,\\phi_{0}},p_{T})+e^{-T}\\big(\\mathbb{E}_{p_{d a t a}(\\mathbf{x}_{0})}\\,\\|\\mathbf{x}_{0}\\|_{2}\\big)\\Lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular, if we assume Assumption I-1 (iii) holds also for $G^{T\\rightarrow0}$ , then for $\\begin{array}{r}{T=\\mathcal{O}\\Big(\\log\\big(\\frac{m\\Lambda}{\\epsilon_{D M}}\\big)^{2}\\Big)}\\end{array}$ is sufficiently large, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nW_{1}(p_{0,\\pmb\\theta},p_{d a t a})\\lesssim\\mathcal{L}_{r e c}(\\pmb\\theta;\\pmb\\phi_{0})+\\epsilon_{D M}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here, we use $\\lesssim$ to absorb the dependence on the constants $T$ and $\\gamma$ into the inequality. ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem B.3. Define $p_{T,\\phi_{0}}:=G_{\\phi_{0}}^{0\\to T}\\sharp\\nu$ , and $p_{0,\\phi_{0}}:=G^{T\\rightarrow0}\\sharp p_{T,\\phi_{0}}.$ . From the triangle inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nW_{1}(p_{0,\\pmb\\theta},\\nu)\\leq\\underbrace{W_{1}(p_{0,\\pmb\\theta},p_{0,\\phi_{0}})}_{(A)}+\\underbrace{W_{1}(p_{0,\\phi_{0}},\\nu)}_{(B)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For (A), by following the similar argument as in Theorem B.1, we can obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A\\Big)\\leq e^{-T}\\big(\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x}_{0})}\\,\\|\\mathbf{x}_{0}\\|_{2}\\big)\\Lambda+\\Lambda W_{1}\\big(p_{T},p_{T,\\phi_{0}}\\big)+\\mathcal{L}_{\\mathrm{PaGoDA}}\\big(\\theta_{\\nu};\\phi_{0}\\big)+\\mathbb{E}_{\\nu(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\Big[\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\|_{2}\\Big]\\Big)\\leq}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For (B), by subtracting the following equations and integrating over $t$ from 0 to $T$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{\\mathrm{d}\\hat{\\mathbf{z}}_{t,\\phi_{0}}}{\\mathrm{d}t}}&{=\\hat{\\mathbf{z}}_{t,\\phi_{0}}+\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{t,\\phi_{0}},T-t),\\quad\\hat{\\mathbf{z}}_{0,\\phi_{0}}\\sim p_{T,\\phi_{0}}}\\\\ {\\frac{\\mathrm{d}\\hat{\\mathbf{z}}_{t}}{\\mathrm{d}t}}&{=\\hat{\\mathbf{z}}_{t}+\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t}),\\quad\\hat{\\mathbf{z}}_{0}\\sim p_{T},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we will obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}=\\left(\\hat{\\mathbf{z}}_{0,\\phi_{0}}-\\hat{\\mathbf{z}}_{0}\\right)+\\int_{0}^{T}\\left(\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{t,\\phi_{0}},T-t)-\\nabla\\log p_{T-t}(\\hat{\\mathbf{z}}_{t})\\right)\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now let $u(t):=\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{t,\\phi_{0}}-\\hat{\\mathbf{z}}_{t}\\right\\|_{2}$ . Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{u(t)\\le\\,u(0)+\\mathbb{E}\\int_{0}^{t}\\|\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{\\tau,\\phi_{0}},T-\\tau)-\\nabla\\log p_{T-\\tau}(\\hat{\\mathbf{z}}_{\\tau})\\|_{2}\\,\\mathrm{d}\\tau}}\\\\ &{}&{\\le\\,u(0)+\\int_{0}^{T}\\mathbb{E}\\left\\|\\mathbf{s}_{\\phi_{0}}(\\hat{\\mathbf{z}}_{\\tau,\\phi_{0}},T-\\tau)-\\nabla\\log p_{T-\\tau}(\\hat{\\mathbf{z}}_{\\tau,\\phi_{0}})\\right\\|_{2}\\mathrm{d}\\tau}\\\\ &{}&{\\ \\ \\ +\\int_{0}^{t}\\mathbb{E}\\left\\|\\nabla\\log p_{T-\\tau}(\\hat{\\mathbf{z}}_{\\tau,\\phi_{0}})-\\nabla\\log p_{T-\\tau}(\\hat{\\mathbf{z}}_{\\tau})\\right\\|_{2}\\mathrm{d}\\tau}\\\\ &{}&{\\le\\,u(0)+T\\epsilon_{\\mathrm{DM}}+\\int_{0}^{t}\\Lambda_{s}(\\tau)u(\\tau)\\,\\mathrm{d}\\tau,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\Lambda_{s}(t)$ is the Lipschitz constant of $\\nabla\\log p_{t}(\\cdot)$ in $\\mathbf{x}$ . By applying integral form of Gr\u00f6nwall\u2019s inequality, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(B)\\leq\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{T,\\phi_{0}}-\\hat{\\mathbf{z}}_{T}\\right\\|_{2}\\leq C_{T}\\mathbb{E}\\left\\|\\hat{\\mathbf{z}}_{0,\\phi_{0}}-\\hat{\\mathbf{z}}_{0}\\right\\|_{2}+C_{T}T\\epsilon_{\\mathrm{DM}}=C_{T}W_{1}(p_{T,\\phi_{0}},p_{T})+C_{T}T\\epsilon_{\\mathrm{DM}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{C_{T}:=\\exp\\big(\\int_{0}^{T}\\Lambda_{s}(t)\\,\\mathrm{d}t\\big)}\\end{array}$ and the last equality follows from choosing a coupling of $\\hat{\\mathbf{z}}_{0,\\phi_{0}}\\sim$ $p_{T,\\phi_{0}}$ and $\\hat{\\mathbf{z}}_{0}\\sim p_{T}$ so that $W_{1}(\\overset{\\cdot}{p}_{T,\\phi_{0}},p_{T})=\\mathbb{E}\\left\\lVert\\hat{{\\mathbf{z}}}_{0,\\phi_{0}}-\\hat{{\\mathbf{z}}}_{0}\\right\\rVert_{2}$ . ", "page_idx": 33}, {"type": "text", "text": "By combining inequalities (17) and (18), we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{1}(p_{0,\\theta},\\nu)\\,\\leq\\mathcal{L}_{\\mathrm{rec}}(\\pmb{\\theta}_{\\nu};\\phi_{0})+\\mathbb{E}_{\\nu(\\mathbf{x})p_{\\phi_{0}}(\\mathbf{z}|\\mathbf{x})}\\Big[\\big\\|\\mathbf{x}-G^{T\\rightarrow0}(\\mathbf{z})\\big\\|_{2}\\Big]+C_{T}T\\epsilon_{\\mathrm{DM}}}\\\\ {+\\left(C_{T}+\\Lambda\\right)W_{1}(p_{T,\\phi_{0}},p_{T})+e^{-T}\\big(\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x}_{0})}\\,\\|\\mathbf{x}_{0}\\|_{2}\\big)\\Lambda.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "A similar argument to Theorem B.1 can be applied to obtain the second inequality in the statement of Theorem B.3. \u25a0 ", "page_idx": 33}, {"type": "text", "text": "B.2 Optimality analysis ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we compare the optimality of the learned distributions resulting from PaGoDA\u2019s training and distillation-based training loss, incorporating GAN [7, 6]. ", "page_idx": 33}, {"type": "text", "text": "PaGoDA\u2019s Loss We recall PaGoDA\u2019s training objective $\\mathcal{L}_{\\mathrm{PaGoDA}}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PaGoDA}}(G_{\\pmb\\theta},D_{\\psi})=\\mathcal{L}_{\\mathrm{rec}}(G_{\\pmb\\theta})+\\lambda\\mathcal{L}_{\\mathrm{adv}}(G_{\\pmb\\theta},D_{\\psi})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "leverages the reconstruction loss ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{rec}}(G_{\\theta})=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\Big[\\big\\|\\mathbf{x}-G_{\\theta}\\big(E(\\mathbf{x})\\big)\\big\\|_{2}^{2}\\Big],\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and adversarial loss ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{adv}}(G_{\\theta},D_{\\psi})=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\big[\\log D_{\\psi}(\\mathbf{x})\\big]+\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\Big[\\log\\Big(1-D_{\\psi}\\big(G_{\\theta}(\\mathbf{z})\\big)\\Big)\\Big].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Knowledge Distillation Loss In the realm of knowledge distillation (KD) methods for DMs, approaches like local consistency [5], global consistency [6], or soft consistency [7] are utilized to learn the noise-to-data trajectory of the teacher DM. Let us consider the global consistency loss as a case study (similar arguments can apply to other distillation objectives), where the teacher\u2019s trajectory is obtained by solving its empirical PF-ODE from $T$ to 0. The long jump along the trajectory is represented as $G_{\\mathrm{teacher}}^{T\\rightarrow0}(\\mathbf{z})$ , where $\\mathbf{z}$ denotes the initial point (noise), $T$ signifies the initial time, and 0 denotes the final time. The output of $G_{\\mathrm{teacher}}^{T\\rightarrow0}$ corresponds to the estimation of clean data, starting from $\\mathbf{z}$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{KD}}(G_{\\theta}):=\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\Big[\\|G_{\\mathrm{teacher}}^{T\\rightarrow0}(\\mathbf{z})-G_{\\theta}(\\mathbf{z})\\|_{2}^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In this context, we abuse the notation $G_{\\theta}(\\mathbf{z})$ to denote the generator for KD. ", "page_idx": 33}, {"type": "text", "text": "The training of KD can also incorporate adversarial loss for enhanced performance [7, 27]. We represent the combined loss as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{KD+GAN}}(G_{\\pmb{\\theta}},D_{\\psi}):=\\mathcal{L}_{\\mathrm{KD}}(G_{\\pmb{\\theta}})+\\mathcal{L}_{\\mathrm{adv}}(G_{\\pmb{\\theta}},D_{\\psi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Theorem B.4. Let $p_{\\phi_{0}}$ be the density determined the teacher DM. Suppose that GAN admits an optimal discriminator $D^{*}$ . ", "page_idx": 34}, {"type": "text", "text": "\u2022 In PaGoDA, assume that the network parametrized generator class $\\{G_{\\theta}\\}$ is expressive enough so that it can simultaneously optimize both $\\mathcal{L}_{r e c}(G_{\\pmb{\\theta}})$ and $\\mathcal{L}_{a d\\nu}(G_{\\theta};D^{*})$ with a same minimizer. Namely, a $\\mathrm{trg\\,min}_{\\theta}\\{\\mathcal{L}_{r e c}\\dot{(G_{\\theta})}\\}\\cap$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{\\theta}}\\{\\mathcal{L}_{a d\\nu}(G\\pmb{\\theta};D^{*})\\}\\neq\\emptyset.}\\end{array}$ . Then ", "page_idx": 34}, {"type": "equation", "text": "$$\np_{\\theta^{*},P a G o D A}:=G_{\\theta^{*},P a G o D A}\\sharp p_{p r i o r}=p_{d a t a}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u2022 In contrast, suppose that $p_{\\phi_{0}}\\neq p_{d a t a},$ , then under similar conditions for $K D\\!+\\!G A N$ where arg $\\operatorname*{min}_{\\pmb{\\theta}}\\{\\mathcal{L}_{K D}\\bar{(G_{\\pmb{\\theta}})}\\}\\cap$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\{\\mathcal{L}_{a d\\nu}(G_{\\theta};D^{*})\\}\\neq\\emptyset,}\\end{array}$ , there is no minimizer $\\theta^{*}$ so that $p_{\\theta^{*},K D+G A N}:=G_{\\theta^{*},K D+G A N}\\sharp p_{p r i o r}=p_{d a t a}$ . ", "page_idx": 34}, {"type": "text", "text": "The first part of the proof of the theorem follows from the following Lemma. ", "page_idx": 34}, {"type": "text", "text": "Lemma B.5. $I\\!f$ $f\\ \\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\}\\cap\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{g(\\pmb{\\theta})\\}\\ \\neq\\ \\emptyset$ , then $\\begin{array}{r l}{\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\ +\\ g(\\pmb{\\theta})\\}}&{=}\\end{array}$ arg $\\operatorname*{min}_{\\theta}\\{f(\\theta)\\}\\cap$ arg $\\operatorname*{min}_{\\pmb{\\theta}}\\{g(\\pmb{\\theta})\\}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. First, we prove the relationship $\\begin{array}{r l r}{\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})~+~g(\\pmb{\\theta})\\}}&{{}\\supseteq}&{\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\}}\\end{array}$ \u2229 $\\arg\\operatorname*{inin}_{\\pmb\\theta}\\{g(\\pmb\\theta)\\}$ . Indeed, it holds without additional assumption. Suppose that $\\pmb{\\theta}^{*}\\qquad\\in$ arg $;\\mathrm{min}_{\\pmb\\theta}\\{f(\\pmb\\theta)\\}\\cap\\mathrm{arg}\\,\\mathrm{min}_{\\pmb\\theta}\\{g(\\pmb\\theta)\\}$ . Then for any $\\pmb{\\theta}$ , we have $f(\\pmb\\theta)\\geq f(\\pmb\\theta^{*})$ and $g(\\pmb\\theta)\\,\\geq\\,g(\\pmb\\theta^{*})$ , which implies $f(\\pmb\\theta)+g(\\pmb\\theta)\\geq f(\\pmb\\theta^{*})+g(\\pmb\\theta^{*})$ . That is, $\\pmb{\\theta}^{*}\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})+g(\\pmb{\\theta})\\}$ . ", "page_idx": 34}, {"type": "text", "text": "On the other hand, suppose that $\\pmb{\\theta}^{*}\\ \\in\\ \\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\ +\\ g(\\pmb{\\theta})\\}$ . We want to prove that $\\pmb{\\theta}^{*}~\\in$ a $\\mathrm{rg\\,min}_{\\pmb\\theta}\\{f(\\pmb\\theta)\\}\\cap\\mathrm{arg\\,min}_{\\pmb\\theta}\\{g(\\pmb\\theta)\\}$ . Let $\\pmb{\\theta}_{\\cap}^{*}\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\}\\cap\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{g(\\pmb{\\theta})\\}$ , where we notice that the existence of $\\theta_{\\cap}^{*}$ is guaranteed by the assumption. In particular, we have $\\tilde{f}(\\pmb{\\theta}^{*})\\geq f(\\pmb{\\theta}_{\\cap}^{*})$ and $g(\\pmb{\\theta}^{*})\\geq g(\\pmb{\\theta}_{\\cap}^{*})$ . Then for any $\\pmb{\\theta}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\{f(\\theta)+g(\\theta)\\}=f(\\theta^{*})+g(\\theta^{*})\\geq f(\\theta_{\\cap}^{*})+g(\\theta_{\\cap}^{*})\\geq\\operatorname*{min}_{\\theta}\\{f(\\theta)+g(\\theta)\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\{f(\\theta)+g(\\theta)\\}=f(\\theta^{*})+g(\\theta^{*})=f(\\theta_{\\cap}^{*})+g(\\theta_{\\cap}^{*})}\\end{array}$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left[f(\\pmb\\theta^{*})-f(\\pmb\\theta_{\\cap}^{*})\\right]+\\left[g(\\pmb\\theta^{*})-g(\\pmb\\theta_{\\cap}^{*})\\right]=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This implies $f(\\pmb\\theta^{*})=f(\\pmb\\theta_{\\cap}^{*})=\\operatorname*{min}_{\\pmb\\theta}\\{f(\\pmb\\theta)\\}$ and $g(\\pmb{\\theta}^{*})=g(\\pmb{\\theta}_{\\cap}^{*})=\\operatorname*{min}_{\\pmb{\\theta}}\\{g(\\pmb{\\theta})\\}$ , as the individual terms are nonnegative. Therefore, $\\pmb{\\theta}^{*}\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{f(\\pmb{\\theta})\\}\\cap\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\{g(\\pmb{\\theta})\\}$ , which concludes the proof. ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem B.4. With the lemma above, let $\\pmb{\\theta}^{*}~\\in$ arg min\u03b8 ${\\mathcal{L}}_{\\mathrm{PaGoDA}}(G_{\\theta},D^{*})$ . Consequently, $\\theta^{*}$ should also simultaneously minimize both ${\\mathcal{L}}_{\\mathrm{rec}}$ and ${\\mathcal{L}}_{\\mathrm{adv}}$ . Minimizing ${\\mathcal{L}}_{\\mathrm{rec}}$ implies that $p_{\\theta^{*}\\,,\\mathrm{PaGoDA}}=G_{\\theta^{*}\\,,\\mathrm{PaGoDA}}\\sharp p_{T,\\phi_{0}},$ , where $p_{T,\\phi_{0}}$ represents the density derived from solving the teacher\u2019s empirical PF-ODE forward, starting from $p_{\\mathrm{data}}$ . On the other hand, optimizing ${\\mathcal{L}}_{\\mathrm{adv}}$ implies that $p\\theta^{*},\\mathtt{P a G o D A}=p_{\\mathtt{d a t a}}$ by applying Theorem 1 in [29]. This establishes the first part of the theorem. ", "page_idx": 34}, {"type": "text", "text": "In the second part, suppose on the contrary that there is a minimizer $\\theta^{*}$ of $\\mathcal{L}_{\\mathrm{KD+GAN}}$ such that $p_{\\theta^{*},\\mathrm{KD+GAN}}=p_{\\mathrm{data}}$ . Again, by applying the above lemma, we infer that $\\theta^{*}$ should also minimize $\\mathcal{L}_{\\mathrm{KD}}$ (and $\\mathcal{L}_{\\mathrm{adv}}$ ). This implies that $p_{\\theta^{*},\\mathrm{KD+GAN}}=p_{\\phi_{0}}$ . However, this contradicts our assumption that $p_{\\mathrm{data}}\\neq p_{\\phi_{0}}$ . Thus, such a minizer does not exist and the second part of the theorem is proven. ", "page_idx": 34}, {"type": "text", "text": "We remark that (1) optimality of ${\\mathcal{L}}_{\\mathrm{GAN}}(\\theta)$ may not be unique in $\\pmb{\\theta}$ , and that (2) the first part of the theorem can be directly extended to scenarios involving downsampling in the encoder . ", "page_idx": 34}, {"type": "text", "text": "B.3 Stability Analysis ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "B.3.1 Preliminaries of Dynamical System ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "To study its convergence and stability, we first introduce the prerequisites for Lyapunov stability [83, 84] in a general setup. Let $\\mathcal{F}\\colon\\Xi\\rightarrow\\Xi$ be a continuously differentiable operator (that is, $\\mathcal{C}^{1}$ operator), where $\\bar{\\Omega^{\\subset}}\\,\\mathbb{R}^{N}$ . We consider the discrete iteration dynamical system defined by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\xi_{k+1}=\\mathcal{F}(\\xi_{k})\\quad\\mathrm{with}\\;\\xi_{0}\\in\\Omega.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Namely, $\\begin{array}{r}{\\pmb{\\xi}_{k+1}=\\mathcal{F}^{(k)}(\\pmb{\\xi}_{0}):=\\underbrace{\\mathcal{F}\\circ\\dots\\circ\\mathcal{F}(\\pmb{\\xi}_{0})}_{\\mathrm{~:~}\\mathrm{~conise}}(\\pmb{\\xi}_{0})}\\end{array}$ . A point $\\xi^{\\ast}\\in\\Omega$ is called a fixed point or equilibrium k-c o pies   \n(we use the terms interchangeably) of $\\mathcal{F}$ if $\\xi^{*}=\\mathcal{F}(\\xi^{*})$ . The stability and convergence analysis   \nfocuses on how the dynamical system $\\mathcal{F}^{(k)}(\\pmb{\\xi}_{0})$ approaches a fixed point as iterations $k$ are sufficiently   \nlarge. ", "page_idx": 35}, {"type": "text", "text": "Definition B.1. (Stability [84]) Let $\\xi^{\\ast}$ be an equilibrium of the $\\mathcal{C}^{1}$ operator $\\mathcal{F}\\colon\\Omega\\,\\rightarrow\\,\\Omega$ . The equilibrium $\\xi^{*}$ is said to be ", "page_idx": 35}, {"type": "text", "text": "\u2022 stable if for every $\\epsilon\\,>\\,0$ there is a $\\delta\\,>\\,0$ so that whenever $\\|\\pmb{\\xi}-\\pmb{\\xi}^{*}\\|_{2}\\,<\\,\\delta$ , we have $\\left\\|{\\mathcal{F}}^{(k)}({\\pmb{\\xi}})-{\\pmb{\\xi}}^{*}\\right\\|_{2}<\\epsilon$ for all $k\\in\\mathbb{N}\\cup\\{0\\}$ .   \n\u2022 asymptotically stable if $\\xi^{*}$ is stable, and there is a $\\delta>0$ so that whenever $\\|\\pmb{\\xi}-\\pmb{\\xi}^{*}\\|_{2}<\\delta$ ,   \nwe have $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\left\\|\\mathcal{F}^{(k)}(\\pmb{\\xi})-\\pmb{\\xi}^{*}\\right\\|_{2}=0.}\\end{array}$ .   \n\u2022 exponentially stable if $\\xi^{*}$ is asymptotically stable, and there is a $\\delta\\:>\\:0$ and $\\alpha,\\beta\\,>\\,0$   \nso that whenever $\\|\\pmb{\\xi}-\\pmb{\\xi}^{*}\\|_{2}<\\delta$ , we have $\\|{\\mathcal{F}}^{(k)}({\\pmb{\\xi}})-{\\pmb{\\xi}}^{*}\\|_{2}\\leq\\alpha\\,\\|{\\pmb{\\xi}}-{\\pmb{\\xi}}^{*}\\|_{2}\\,e^{-\\beta k}$ for all $k\\,\\in\\,\\mathbb{N}\\cup\\{0\\}$ . The largest $\\beta\\,>\\,0$ that satisfies the inequality for exponential stability is referred to as the rate of convergence. ", "page_idx": 35}, {"type": "text", "text": "Let $\\Gamma$ be a subset of the set of all equilibria. We say the dynamical system $\\mathcal{F}^{(k)}$ locally converges on $\\Gamma$ if $\\mathcal{F}^{(k)}$ is exponentially stable at any point in $\\Gamma$ . ", "page_idx": 35}, {"type": "text", "text": "The intuitions of the above stability notions are ", "page_idx": 35}, {"type": "text", "text": "\u2022 A stable equilibrium indicates that if an initialization is within some $\\delta$ -neighborhood of the equilibrium, the iterations starting from that initialization will always remain within an $\\epsilon$ -neighborhood of the equilibrium, for any arbitrarily chosen $\\epsilon$ .   \n\u2022 An asymptotically stable equilibrium indicates that iterations starting near the equilibrium not only remain close but ultimately converge to the equilibrium.   \n\u2022 An asymptotically stable equilibrium indicates that the iterations not only converge but do so at a rate no slower than the rate $e^{-\\beta k}$ with respective to iteration step $k$ . ", "page_idx": 35}, {"type": "text", "text": "Analyzing the eigenvalues of the Jacobian $\\nabla_{\\xi}\\mathcal{F}(\\pmb{\\xi}^{*})$ of the operator $\\mathcal{F}$ at an equilibrium $\\xi^{\\ast}$ is a crucial tool for studying stability. In principle [83, 84], if we can ensure that the Jacobian of $\\mathcal{F}$ at some equilibrium has only eigenvalues with strictly negative real parts, then the dynamical system $\\mathcal{F}^{(k)}$ is asymptotically stable at that equilibrium. In particular, we refer to a matrix as a Hurwitz matrix if all its eigenvalues have strictly negative real parts. ", "page_idx": 35}, {"type": "text", "text": "In the following lemma, we present a necessary condition to ensure that a special class of matrices will be Hurwitz. ", "page_idx": 35}, {"type": "text", "text": "Lemma B.6. (Necessary condition for a Hurwitz matrix $I2I J$ ) Consider the following matrix $\\mathcal{I}\\in$ $\\mathbb{R}^{\\left(N+M\\right)\\times\\left(N+M\\right)}$ with $\\dot{P}\\in\\mathbb{R}^{N\\times N}$ , $\\stackrel{\\triangledown}{Q}\\in\\mathbb{R}^{M\\times M}$ , and $B\\in\\mathbb{R}^{\\dot{M}\\times N}$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{I}=\\left[\\!\\!\\begin{array}{c c}{P}&{-B^{T}}\\\\ {B}&{Q}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Suppose that $B$ is full rank. Then all eigenvalues of $\\mathcal{I}$ have negative real part, if either $(I)\\:P$ is negative definite and $Q$ is negative semi-definite, or (2) $P$ is negative semi-definite and $Q$ is negative definite. ", "page_idx": 35}, {"type": "text", "text": "B.3.2 Preliminaries for Analysis of PaGoDA Training ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We consider PaGoDA\u2019s training, integrating reconstruction and adversarial losses with a weight $\\eta>0$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\pmb{\\theta},\\pmb{\\psi}):=\\mathbb{E}_{p_{\\operatorname*{dat}}(\\mathbf{x})}\\left[\\eta\\left\\|\\mathbf{x}-G_{\\pmb{\\theta}}(E(\\mathbf{x}))\\right\\|_{2}^{2}+f(D_{\\psi}(\\mathbf{x}))\\right]+\\mathbb{E}_{p_{G_{\\pmb{\\theta}}}(\\mathbf{x})}\\left[f(-D_{\\psi}(\\mathbf{x}))\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{p_{\\operatorname*{dat}}(\\mathbf{x})}\\left[\\eta\\left\\|\\mathbf{x}-G_{\\pmb{\\theta}}(E(\\mathbf{x}))\\right\\|_{2}^{2}+f(D_{\\psi}(\\mathbf{x}))\\right]+\\mathbb{E}_{p_{\\operatorname*{prior}}(\\mathbf{z})}\\left[f(-D_{\\psi}(G_{\\pmb{\\theta}}(\\mathbf{z})))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here, $f\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ is a continuous differentiable function. In the vanilla GAN [29], the $f$ -function is taken as $f(u)\\;:=\\;-\\log\\left(1+\\exp(-u)\\right)$ , where $f^{\\prime}(u)\\,=\\,\\exp(-u)/\\bigl(1+\\bigl\\exp(-u)\\bigr)\\,>\\,0$ and ", "page_idx": 35}, {"type": "text", "text": "$f^{\\prime\\prime}(u)=-\\exp(-u)/\\bigl(1+\\exp(-u)\\bigr)<0$ for all $u\\in\\mathbb R$ . We maintain the generality of $f$ and will prove the training stability of PaGoDA across a wide class of $f$ . ", "page_idx": 36}, {"type": "text", "text": "The velocity field ${\\bf v}(\\pmb\\theta,\\psi)$ corresponding to the gradient descent update is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbf{v}(\\theta,\\psi):=\\left[\\begin{array}{l}{-\\nabla_{\\theta}\\mathcal{L}(\\theta,\\psi)}\\\\ {\\nabla_{\\psi}\\mathcal{L}(\\theta,\\psi)}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Gradient descent is a special case of fixed-point iteration. Now, we specify the operator $\\mathcal{F}$ as an alternative gradient descent operator. That is, we consider $\\mathcal{F}_{h}:=\\mathcal{F}_{D,h}\\circ\\mathcal{F}_{G,h}$ with a learning rate $h>0$ . Here, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{F}_{G,h}(\\pmb{\\theta},\\psi):=\\left[\\begin{array}{c}{\\pmb{\\theta}-h\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta},\\psi)}\\\\ {\\psi}\\end{array}\\right]\\quad\\mathrm{and}\\quad\\mathcal{F}_{D,h}(\\pmb{\\theta},\\psi):=\\left[\\psi+h\\nabla_{\\psi}\\mathcal{L}(\\pmb{\\theta},\\psi)\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "A point $(\\theta^{*},\\psi^{*})$ is called an equilibrium of the system defined by $\\mathbf{v}$ if $\\mathbf{v}(\\pmb{\\theta}^{*},\\pmb{\\psi}^{*})=0$ (equivalently, $\\mathcal{F}_{h}(\\theta^{*},\\dot{\\psi}^{*})=0)$ . We can analyze the learning dynamic via the Jacobian matrix of ${\\bf v}(\\pmb\\theta,\\psi)$ which is defined as the following: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\pmb{\\theta},\\psi):=\\left[\\stackrel{-\\nabla_{\\theta}^{2}\\mathcal{L}(\\pmb{\\theta},\\psi)}{\\nabla_{\\theta,\\psi}^{2}\\mathcal{L}(\\pmb{\\theta},\\psi)}\\quad\\quad\\nabla_{\\psi}^{2}\\mathcal{L}(\\pmb{\\theta},\\psi)\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The following proposition relates Lemma B.8 to the stability of the gradient descent operator $\\mathcal{F}_{h}$ , serving as the main tool to prove the training stability of PaGoDA in Theorem B.9. ", "page_idx": 36}, {"type": "text", "text": "Lemma B.7. (Locally stable on manifold \u2013 modification of [21]) Suppose that the gradient descent operator $\\mathcal{F}_{h}\\,=\\,\\mathcal{F}_{h}(v,\\omega)$ is a $\\mathcal{C}^{1}$ mapping. Let $(\\boldsymbol{v}^{*},\\omega^{*})$ be an equilibrium (fixed point) of ${\\mathcal{F}}_{h}$ . Assume that there is a neighborhood $\\Omega$ of $\\omega^{*}$ so that $\\mathcal{F}_{h}$ admits equilibrium on $\\{\\pmb{v}^{*}\\}\\times\\Omega$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{h}(\\boldsymbol{v}^{*},\\omega)=(\\boldsymbol{v}^{*},\\omega)\\quad{f o r}\\,a l l\\,\\omega\\in\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If all the eigenvalues of $\\mathcal{I}:=\\nabla_{v}\\mathcal{F}_{h}({\\boldsymbol{v}}^{*},\\omega^{*})$ have negative real parts, then for a sufficiently small learning rate $h$ , the gradient descent iteration defined by $\\mathcal{F}_{h}$ locally converges on $\\Gamma:=\\{(\\boldsymbol{v}^{*},\\boldsymbol{\\omega})|\\boldsymbol{\\omega}\\in$ $\\Omega\\}$ with a rate of convergence $\\lvert\\lambda_{m a x}\\rvert$ . Here, $\\lambda_{m a x}$ denotes the eigenvalue of $\\mathcal{I}$ with the largest absolute value. ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma B.7. This proposition is followed by Lemma A.5. and Theorem A.3. of [21]. \u25a0 ", "page_idx": 36}, {"type": "text", "text": "B.3.3 PaGoDA\u2019s Training is Stable ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proving PaGoDA\u2019s stability involves two steps: First, derive the components of First, deriving the components of $\\mathcal{I}(\\pmb{\\theta}^{*},\\pmb{\\psi}^{*})$ . Second, verify that these components satisfy Lemma B.6. After these, we can apply Lemma B.7 to conclude PaGoDA\u2019s training stability whenever the learning rate $h>0$ is sufficiently small. ", "page_idx": 36}, {"type": "text", "text": "Assumption III-1. (i) $E$ is not an identity map. ", "page_idx": 36}, {"type": "text", "text": "(ii) At $\\theta^{*}$ , $p_{\\theta^{*}}=p_{\\mathrm{data}}$ , and $\\mathbf{x}=G_{\\pmb{\\theta}^{*}}(E(\\mathbf{x}))\\ {\\mathrm{for~a.e.}}\\ \\mathbf{x}\\in{\\mathrm{supp}}\\!\\left(p_{\\mathrm{data}}\\right).$ . ", "page_idx": 36}, {"type": "text", "text": "(iii) At $\\psi^{*}$ , $D_{\\psi^{*}}(\\mathbf{x})=0$ and $\\nabla_{\\mathbf x}D_{\\psi^{*}}(\\mathbf x)=0$ for $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma B.8. Suppose that Assumption III-1 holds for an equilibrium $(\\theta^{*},\\psi^{*})$ . Then the Jacobian at the equilibrium can be computed as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\pmb{\\theta}^{*},\\pmb{\\psi}^{*})=\\left[K_{G G}\\quad-K_{D G}^{T}\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Here, and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{G G}=-\\ 2\\eta\\mathbb{E}_{p_{d a t}(\\mathbf{x})}\\left[\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))^{T}\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))\\right]}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x}+f^{\\prime}(0)\\mathbb{E}_{p_{p r i o r}(\\mathbf{z})}\\left[\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})^{T}\\cdot\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(G_{\\theta^{*}}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})\\right]\\cdot}\\\\ &{K_{D G}=-f^{\\prime}(0)\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\left[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\right]\\Big|_{\\theta=\\theta^{*}}}\\\\ &{K_{D D}=2f^{\\prime\\prime}(0)\\mathbb{E}_{p_{d a t a}(\\mathbf{x})}\\left[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma B.8. We first compute the gradients of $\\mathcal{L}$ in terms of $\\pmb{\\theta}$ and $\\psi$ , where we utilize the formulations Eqs. (19) and (20), respectively. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{L}(\\theta,\\psi)=-2\\eta\\mathbb{E}_{p_{\\mathrm{daa}}(\\mathbf{x})}\\big[\\langle\\mathbf{x}-G_{\\theta}(E(\\mathbf{x})),\\nabla_{\\theta}G_{\\theta}(E(\\mathbf{x}))\\rangle\\big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\mathbb{E}_{p_{\\mathrm{priot}}(\\mathbf{z})}\\big[f^{\\prime}(-D_{\\psi}(G_{\\theta}(\\mathbf{z})))\\cdot\\nabla_{\\mathbf{x}}D_{\\psi}(G_{\\theta}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta}(\\mathbf{z})\\big].}\\\\ &{\\nabla_{\\psi}\\mathcal{L}(\\theta,\\psi)=\\mathbb{E}_{p_{\\mathrm{daa}}(\\mathbf{x})}\\big[f^{\\prime}(D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}D_{\\psi}(\\mathbf{x}))\\big]-\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\big[f^{\\prime}(-D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}D_{\\psi}(\\mathbf{x}))\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\theta}^{2}\\mathcal{L}(\\theta,\\psi)=2\\eta\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})}\\big[\\langle\\nabla_{\\theta}G_{\\theta}(E(\\mathbf{x})),\\nabla_{\\theta}G_{\\theta}(E(\\mathbf{x}))\\rangle\\big]-2\\eta\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})}\\big[\\langle\\mathbf{x}-G_{\\theta}(E(\\mathbf{x})),\\nabla_{\\theta}^{2}G_{\\theta}(E(\\mathbf{x}))\\rangle\\big]}\\\\ &{\\quad\\quad\\quad\\quad+\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\big[f^{\\prime\\prime}(-D_{\\psi}(G_{\\theta}(\\mathbf{z})))\\cdot\\nabla_{\\mathbf{x}}D_{\\psi}(G_{\\theta}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta}(\\mathbf{z})\\cdot\\nabla_{\\mathbf{x}}D_{\\psi}(G_{\\theta}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta}(\\mathbf{z})}\\\\ &{\\quad\\quad\\quad\\quad-\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\big[f^{\\prime}(-D_{\\psi}(G_{\\theta}(\\mathbf{z})))\\cdot\\nabla_{\\theta}G_{\\theta}(\\mathbf{z})^{T}\\cdot\\nabla_{\\mathbf{x}}^{2}D_{\\psi}(G_{\\theta}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta}(\\mathbf{z})\\big]}\\\\ &{\\quad\\quad\\quad\\quad-\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\big[f^{\\prime}(-D_{\\psi}(G_{\\theta}(\\mathbf{z})))\\cdot\\nabla_{\\mathbf{x}}D_{\\psi}(G_{\\theta}(\\mathbf{z}))\\cdot\\nabla_{\\theta}^{2}G_{\\theta}(\\mathbf{z})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "According to Assumption III-1 (ii) and (iii), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{*},\\psi^{*})=2\\eta\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left[\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))^{T}\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))\\right]}\\\\ &{\\qquad\\qquad\\qquad-\\ f^{\\prime}(0)\\mathbb{E}_{p_{\\mathrm{piar}}(\\mathbf{z})}\\left[\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})^{T}\\cdot\\nabla_{\\mathbf{x}}^{2}D_{\\psi}(G_{\\theta^{*}}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\partial_{GG}}=\\mathrm{\\partial}-\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{*},\\psi^{*})}\\\\ &{\\quad\\quad=\\mathrm{\\partial}-2\\eta\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})}\\big[\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))^{T}\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))\\big]+f^{\\prime}(0)\\mathbb{E}_{p_{\\mathrm{ptw}}(\\mathbf{z})}\\big[\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})^{T}\\cdot\\nabla_{\\mathbf{x}}^{2}D_{\\psi}(G_{\\theta^{*}}(\\mathbf{z}))\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To compute $K_{D G}$ , we first derive $\\nabla_{\\theta}\\mathcal{L}$ from Eq. (19) as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}(\\theta,\\psi)=\\,-\\,2\\eta\\mathbb{E}_{p_{\\mathrm{tata}}(\\mathbf{x})}\\big[\\langle\\mathbf{x}-G_{\\theta}(E(\\mathbf{x})),\\nabla_{\\theta}G_{\\theta}(E(\\mathbf{x}))\\rangle\\big]+\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\big[f(-D_{\\psi}(\\mathbf{x}))\\big].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, we can compute ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\theta,\\psi}^{2}\\mathcal{L}(\\theta,\\psi)=-\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\big[f^{\\prime}(-D_{\\psi}(\\mathbf{x}))\\cdot\\nabla_{\\psi}D_{\\psi}(\\mathbf{x})\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and hence, ", "page_idx": 37}, {"type": "equation", "text": "$$\nK_{D G}=\\nabla_{\\theta,\\psi}^{2}\\mathcal{L}(\\theta^{*},\\psi^{*})=-f^{\\prime}(0)\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\big[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\big]\\Big|_{\\theta=\\theta^{*}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To compute $K_{D D}$ , we can obtain from Eq. (20) that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi}^{2}\\mathcal{L}(\\theta,\\psi)=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left[f^{\\prime\\prime}(D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}D_{\\psi}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi}(\\mathbf{x})^{T}\\right]}\\\\ &{\\phantom{\\quad}+\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\left[f^{\\prime\\prime}(-D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}D_{\\psi}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi}(\\mathbf{x})^{T}\\right]}\\\\ &{\\phantom{\\quad}+\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left[f^{\\prime}(D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}^{2}D_{\\psi}(\\mathbf{x})\\right]-\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\left[f^{\\prime}(-D_{\\psi}(\\mathbf{x}))\\nabla_{\\psi}^{2}D_{\\psi}(\\mathbf{x}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, by using Assumption III-1 (ii) and (iii), we get ", "page_idx": 37}, {"type": "equation", "text": "$$\nK_{D D}=\\nabla_{\\psi}^{2}\\mathcal{L}(\\theta^{*},\\psi^{*})=2f^{\\prime\\prime}(0)\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\bigl[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\bigr].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We consider the following two sets ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{G}:=\\left\\{\\theta\\middle|p_{\\theta}=p_{\\mathrm{data}},\\,\\mathbf{x}=G_{\\theta}(E(\\mathbf{x}))\\mathrm{~for~a.e.~}\\mathbf{x}\\in\\mathrm{supp}\\middle(p_{\\mathrm{data}}\\right)\\right\\}}\\\\ &{\\mathcal{M}_{D}:=\\left\\{\\psi\\middle|S(\\psi)=0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $S(\\psi):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\left[|D_{\\psi}(\\mathbf{x})|^{2}+\\|\\nabla_{\\mathbf{x}}D_{\\psi}(\\mathbf{x})\\|_{2}^{2}\\right]$ . Also, we let $\\tau_{\\psi^{*}}\\mathcal{M}_{D}$ denote the tangent space of $\\mathcal{M}_{D}$ at $\\psi^{\\bar{*}}$ . ", "page_idx": 37}, {"type": "text", "text": "Assumption III-2. (i) The second continuously differentiable function $f\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ satisfies: $f^{\\prime}(0)>0$ and $f^{\\prime\\prime}(0)<0$ . ", "page_idx": 37}, {"type": "text", "text": "(ii) There is a $\\delta>0$ so that $\\mathcal{M}_{G}\\cap\\mathbb{B}_{\\delta}(\\pmb{\\theta}^{*})$ and $\\mathcal{M}_{D}\\cap\\mathbb{B}_{\\delta}(\\psi^{*})$ are $\\mathcal{C}^{1}$ manifolds. ", "page_idx": 37}, {"type": "text", "text": "(iii) $\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))^{T}\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(E(\\mathbf{x}))$ is positive definite, for all $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ ", "page_idx": 38}, {"type": "text", "text": "(iv) $\\partial_{\\mathbf{w}}h(\\psi^{*})\\neq0$ $\\neq0\\;\\mathrm{for}\\;\\mathrm{any}\\;{\\mathbf w}\\notin T_{\\psi^{*}}\\mathcal{M}_{D},\\;\\mathrm{where}\\;h(\\psi):=\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}({\\mathbf x})}\\big[D_{\\psi}({\\mathbf x})\\big]\\,\\Big|_{\\theta=\\theta^{*}}.$   \n(v) $\\mathbf{w}^{T}\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(\\mathbf{x})\\mathbf{w}\\geq0$ , for all w $\\notin\\pi_{\\theta^{*}}\\mathcal{M}_{G}$ and $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ . Remark. Two special cases are either (v-1) $\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(\\mathbf{x})=0$ for $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ , or (v-2) $\\mathbf{w}^{T}\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(\\mathbf{x})\\mathbf{w}>0$ , for all $w\\notin\\tau_{\\theta^{*}}\\mathbf{\\mathcal{M}}_{G}$ and $\\mathbf{x}\\in\\mathsf{s u p p}(p_{\\mathrm{data}})$ . ", "page_idx": 38}, {"type": "text", "text": "Theorem B.9. Suppose that Assumptions III-1 and III-2 hold for an equilibrium $(\\theta^{*},\\psi^{*})$ and $\\eta>0$ is sufficiently large. Then the alternative gradient descent iteration $\\mathcal{F}_{h}$ described in Section B.3.2 is locally convergent on $\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ for a sufficiently small learning rate $h>0$ . ", "page_idx": 38}, {"type": "text", "text": "Proof of Theorem B.9. The argument is motivated by [21]. We notice that $\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ is a subset of all equilibria of the operators $\\mathcal{F}_{h}$ (or ${\\bf v}(\\pmb\\theta,\\psi))$ . This is because that for any $(\\theta,\\psi)\\in\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ , we have $p_{\\theta}=p_{\\mathrm{data}},\\,\\mathbf{x}=G_{\\theta}(E(\\mathbf{x})),\\,D_{\\psi}(\\mathbf{x})=0$ , and $\\nabla_{\\mathbf x}D_{\\psi}(\\mathbf x)=\\mathbf0$ for $\\mathbf{x}\\in\\,\\mathrm{supp}(p_{\\mathrm{data}})$ . From Eqs. (21) and (22), we then can obtain $\\nabla_{\\theta}\\mathcal{L}(\\theta,\\psi)\\;=\\;\\nabla_{\\psi}\\mathcal{L}(\\theta,\\psi)\\;=\\;0$ , meaning $(\\theta,\\psi)$ is an equilibrium. ", "page_idx": 38}, {"type": "text", "text": "Now, we show that the alternating gradient descent converges locally on $\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ by verifying Lemma B.8 is fulfliled, and hence, Lemma B.7 can be applied. Let $(\\theta^{*},\\psi^{*})\\in\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ . There is a $\\mathcal{C}^{1}$ -diffeomorphism $\\Psi$ that transforms a neighborhood of $(\\theta^{*},\\psi^{*})$ onto an open set in $\\mathbb{R}^{(N+M)}$ due to Assumption III-2 (ii). More precisely, we can compute the relation of ${\\mathcal{F}}_{h}$ and $\\mathbf{v}$ after the $\\Psi$ -reparametrization. Let $\\zeta:=\\Psi(\\theta,\\psi)$ , and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{h}^{\\Psi}(\\zeta):=\\Psi\\circ\\mathcal{F}_{h}\\circ\\Psi^{-1}(\\zeta)}\\\\ &{\\mathbf{v}^{\\Psi}(\\zeta):=\\Psi^{\\prime}(\\pmb{\\theta},\\psi)\\cdot\\big(\\mathbf{v}\\circ\\Psi^{-1}(\\zeta)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\zeta}\\mathcal{F}_{h}^{\\Psi}(\\zeta^{*})=\\nabla_{\\theta,\\psi}\\Psi(\\theta^{*},\\psi^{*})\\cdot\\nabla_{\\theta,\\psi}\\mathcal{F}_{h}(\\theta^{*},\\psi^{*})\\cdot\\nabla_{\\theta,\\psi}\\Psi(\\theta^{*},\\psi^{*})^{-1}}\\\\ &{\\nabla_{\\zeta}\\mathbf{v}^{\\Psi}(\\zeta^{*})=\\nabla_{\\theta,\\psi}\\Psi(\\theta^{*},\\psi^{*})\\cdot\\nabla_{\\theta,\\psi}\\mathbf{v}(\\theta^{*},\\psi^{*})\\cdot\\nabla_{\\theta,\\psi}\\Psi(\\theta^{*},\\psi^{*})^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We remark that similar matrices have identical ranks and spectrum. Therefore, without loss of the generality, we can assume that $(\\pmb{\\theta}^{*},\\pmb{\\psi}^{*})=(\\mathbf{0}_{N},\\mathbf{0}_{M})\\in\\mathbb{R}^{N}\\times\\mathbb{R}^{M}$ , and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{G}=\\mathcal{T}_{\\pmb{\\theta}^{*}}\\mathcal{M}_{G}=\\{0\\}^{N_{G}}\\times\\mathbb{R}^{N-N_{G}}}\\\\ &{\\mathcal{M}_{D}=\\mathcal{T}_{\\pmb{\\psi}^{*}}\\mathcal{M}_{D}=\\{0\\}^{M_{D}}\\times\\mathbb{R}^{M-M_{D}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We write the new parameterizations as $\\pmb{\\theta}:=(\\pmb{v}_{G},\\pmb{\\omega}_{G})\\in\\mathbb{R}^{N_{G}}\\times\\mathbb{R}^{N-N_{G}}$ and $\\pmb{\\psi}:=(\\pmb{v}_{D},\\pmb{\\omega}_{D})\\in$ $\\mathbb{R}^{M_{D}}\\times\\mathbb{R}^{M-M_{D}}$ . For simplicity, we write $\\mathbf{v}(\\pmb{\\theta},\\pmb{\\psi}):=\\mathbf{v}(\\pmb{v}_{G},\\pmb{\\omega}_{G},\\pmb{v}_{D},\\pmb{\\omega}_{D})$ . To apply Lemma B.7, we now aim to show that $\\nabla_{(v_{G},v_{D})}\\mathbf{v}(\\theta^{*},\\psi^{*})$ only admits eigenvalues with negative real parts. From Lemma B.8, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\nabla_{(v_{G},v_{D})}\\mathbf{v}(\\pmb{\\theta}^{*},\\psi^{*})=\\left[\\hat{K}_{G G}\\quad-\\hat{K}_{D G}^{T}\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here, $\\hat{K}_{G G},\\,\\hat{K}_{D G}$ , and $\\hat{K}_{D D}$ represent submatrices of $K_{G G},K_{D G}$ , and $K_{D D}$ , respectively, with coordinates $\\left(v_{G},v_{D}\\right)$ , indicating the Jacobian of $\\mathbf{v}$ with derivatives taken along the $v_{G}$ and $v_{D}$ directions. ", "page_idx": 38}, {"type": "text", "text": "First of all, we show that $K_{D D}$ is generally negative semi-definite. Let $\\pmb{\\xi}\\in\\mathbb{R}^{(N+M)}$ be any vector. Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi^{T}K_{D D}\\xi=\\,2f^{\\prime\\prime}(0)\\mathbb{E}_{p_{\\mathrm{daa}}(\\mathbf{x})}\\big[\\xi^{T}\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\xi\\big]}\\\\ &{\\qquad\\qquad=\\,2f^{\\prime\\prime}(0)\\mathbb{E}_{p_{\\mathrm{daa}}(\\mathbf{x})}\\big[\\big(\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\xi\\big)^{T}\\cdot\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\xi\\big]\\le0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "because $f^{\\prime\\prime}(0)\\,<\\,0$ from Assumption III-2 (i). Thus, for any $\\hat{\\pmb{\\xi}}_{G}\\,\\in\\,\\mathbb{R}^{N_{G}}$ and $\\hat{\\pmb{\\xi}}_{D}\\,\\in\\,\\mathbb{R}^{M_{D}}$ if we consider $\\hat{\\pmb\\xi}:=(\\hat{\\xi}_{G},\\hat{\\xi}_{D})$ in $(v_{G},v_{D})$ -coordinate, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\hat{\\pmb\\xi}^{T}\\hat{K}_{D D}\\hat{\\pmb\\xi}=\\pmb\\xi^{T}K_{D D}\\pmb\\xi\\leq0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\pmb{\\xi}:=(\\hat{\\xi}_{G},\\mathbf{0}_{N-N_{G}},\\hat{\\xi}_{D},\\mathbf{0}_{M-M_{D}})\\in\\mathbb{R}^{(N+M)}.$ ", "page_idx": 39}, {"type": "text", "text": "Next, we demonstrate that $\\hat{K}_{D G}$ is full rank. We observe that $\\hat{\\xi}_{D}\\neq0$ if and only if $\\xi\\notin\\mathcal{T}_{\\psi^{*}}\\mathcal{M}_{D}$ . Then, according to Assumption III-2 (iv), we deduce that if $\\hat{\\xi}_{D}\\neq0$ ", "page_idx": 39}, {"type": "equation", "text": "$$\nK_{D G}\\xi=-f^{\\prime}(0)\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta^{*}}}(\\mathbf{x})}\\big[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\cdot\\boldsymbol{\\xi}\\big]\\Big|_{\\theta=\\theta^{*}}=-f^{\\prime}(0)\\partial_{\\xi}h(\\psi^{*})\\neq0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The elements of $K_{D G}\\pmb{\\xi}$ corresponding to the $v_{D}$ -coordinates are represented by $\\hat{K}_{D G}\\hat{\\pmb{\\xi}}_{D}$ , while those corresponding to the $\\omega_{D}$ -coordinates are 0. Therefore, we conclude that $\\hat{K}_{D G}\\hat{\\xi}_{D}\\neq0$ . Consequently, by the rank\u2013nullity theorem, $\\hat{K}_{D G}$ is full-rank. ", "page_idx": 39}, {"type": "text", "text": "Finally, by using similar arguments by selecting $(v_{G},v_{D})$ -coordinate, without loss of generality, we only need to show $K_{G G}$ is negative definite. By applying Assumption III-2 (i) and (v), the following lemma concludes that if $\\eta>0$ is sufficiently large, we can conclude the negative definiteness of $\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{*},\\psi^{*})$ under Assumption III-2 (v-2). ", "page_idx": 39}, {"type": "text", "text": "Lemma B.10. Let A be positive definite, and $\\mathbf{B}$ be positive semi-definite. Then there is a $\\eta_{m i n}>0$ so that $-\\eta\\mathbf{A}+\\mathbf{B}$ is negative definite for all $\\eta>\\eta_{m i n}$ . ", "page_idx": 39}, {"type": "text", "text": "The lemma holds because, for positive (semi-) definite matrix $\\mathbf{X}$ , we generally have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}(\\mathbf{X})\\left\\|\\mathbf{w}\\right\\|^{2}\\geq\\mathbf{w}^{T}\\mathbf{X}\\mathbf{w}\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{X})\\left\\|\\mathbf{w}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "tfiovre lalyl. Th. uHs eirf es, $\\lambda_{\\operatorname*{max}}(\\mathbf{X})$ $\\begin{array}{r}{\\eta>\\frac{\\lambda_{\\mathrm{max}}(\\mathbf{B})}{\\lambda_{\\mathrm{min}}(\\mathbf{A})}}\\end{array}$ $\\lambda_{\\operatorname*{min}}(\\mathbf{X})$ dfoern oatney t $\\mathbf{w}\\neq\\mathbf{0}$ ,i mwue mh aavned minimum eigenvalues of $\\mathbf{X}$ , respec", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbf{w}^{T}(-\\eta\\mathbf{A}+\\mathbf{B})\\mathbf{w}=-\\eta\\mathbf{w}^{T}\\mathbf{A}\\mathbf{w}+\\mathbf{w}^{T}\\mathbf{B}\\mathbf{w}\\leq\\left(\\mathbf{\\eta}-\\eta\\lambda_{\\operatorname*{min}}(\\mathbf{A})+\\lambda_{\\operatorname*{max}}(\\mathbf{B})\\right)\\left\\|\\mathbf{w}\\right\\|_{2}^{2}<0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By applying Lemma B.6, we know that $\\nabla_{(v_{G},v_{D})}\\mathbf{v}(\\theta^{*},\\psi^{*})$ only has eigenvalues with negative real parts. Therefore, with a sufficiently small learning rate $h>0$ , Lemma B.7 guarantees the locally convergence of ${\\mathcal{F}}_{h}$ on $\\mathcal{M}_{G}\\times\\mathcal{M}_{D}$ . ", "page_idx": 39}, {"type": "text", "text": "B.3.4 Literature on Stability Analysis of Adversarial Training ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Studying the stability of GAN training from a dynamical systems perspective has been a popular approach [85, 22, 21, 86\u201389]. Generally, proving or disproving whether adversarial training is stable is challenging. However, [21] provides an example (Dirac-GAN) showing that, in general, GANs are not stable unless additional conditions are imposed. ", "page_idx": 39}, {"type": "text", "text": "As a result, researchers have explored additional conditions to stabilize GAN training. Essentially, the goal is to impose extra regularizations on the GAN loss $\\mathcal{L}_{\\mathrm{GAN}}(\\pmb{\\theta},\\pmb{\\psi}):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\big[\\bar{f}(D_{\\psi}(\\mathbf{x}))\\big]\\dot{+}$ $\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\big[f(-D_{\\psi}(\\mathbf{x}))\\big]$ , or its velocity field $\\mathbf{v}_{\\mathrm{GAN}}(\\pmb{\\theta},\\pmb{\\psi}):=\\left[\\frac{-\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{GAN}}(\\pmb{\\theta},\\pmb{\\psi})}{\\nabla_{\\psi}\\mathcal{L}_{\\mathrm{GAN}}(\\pmb{\\theta},\\pmb{\\psi})}\\right]$ to ensure that the resulting Jacobian is Hurwitz. To elaborate further, we revisit the Jacobian $\\mathcal{I}_{\\mathrm{GAN}}$ of the vanilla GAN, given by $\\mathbf{v}_{\\mathrm{GAN}}(\\theta,\\psi)$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\mathrm{GAN}}(\\theta,\\psi):=\\left[\\!\\!\\begin{array}{c c}{-\\nabla_{\\theta}^{2}\\mathcal{L}_{\\mathrm{GAN}}(\\theta,\\psi)}&{-\\nabla_{\\theta,\\psi}^{2}\\mathcal{L}_{\\mathrm{GAN}}(\\theta,\\psi)}\\\\ {\\nabla_{\\theta,\\psi}^{2}\\mathcal{L}_{\\mathrm{GAN}}(\\theta,\\psi)}&{\\nabla_{\\psi}^{2}\\mathcal{L}_{\\mathrm{GAN}}(\\theta,\\psi)}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c c}{K_{G G}}&{-K_{D G}^{T}}\\\\ {K_{D G}}&{K_{D D}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Here, we slightly abuse the notation from Section B.3.3 by using $K_{i j}$ , $i,j\\in\\{D,G\\}$ , to denote the corresponding components in $\\mathcal{I}_{\\mathrm{GAN}}$ . By similar argument of Lemma B.8, we can obtain (indeed, $\\eta=0$ in Lemma B.8) that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{G G}=f^{\\prime}(0)\\mathbb{E}_{p_{\\mathrm{prior}}(\\mathbf{z})}\\left[\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})^{T}\\cdot\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(G_{\\theta^{*}}(\\mathbf{z}))\\cdot\\nabla_{\\theta}G_{\\theta^{*}}(\\mathbf{z})\\right].}\\\\ &{K_{D G}=-f^{\\prime}(0)\\nabla_{\\theta}\\mathbb{E}_{p_{G_{\\theta}}(\\mathbf{x})}\\left[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\right]\\Big|_{\\theta=\\theta^{*}}}\\\\ &{K_{D D}=2f^{\\prime\\prime}(0)\\mathbb{E}_{p_{\\mathrm{dat}}(\\mathbf{x})}\\left[\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})\\cdot\\nabla_{\\psi}D_{\\psi^{*}}(\\mathbf{x})^{T}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Conceptually [83, 84], if we can ensure that the Jacobian at some equilibrium has only eigenvalues with strictly negative real parts, then the gradient descent iteration of $\\mathcal{L}_{\\mathrm{GAN}}$ is asymptotically stable at that equilibrium. Therefore, the objective of many studies [85, 22, 21, 89] is to find conditions to verify Lemma B.6. We focus on discussing the conditions for $K_{G G}$ and $K_{D D}$ to be negative (semi-)definite, as this distinguishes PaGoDA\u2019s Theorem B.9 from the existing literature. ", "page_idx": 39}, {"type": "table", "img_path": "h5zYGF68KH/tmp/37de8497ea88b3f3c5ed3840ac47b3ff9e394e36c25942c4aa95c2536016a608.jpg", "table_caption": ["Table 8: Comparison of various assumptions on stability analysis. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "Under Assumption III-2 (i) that $f^{\\prime\\prime}(0)<0$ , it is worth noting that $K_{D D}$ is generally negative semidefinite without additional conditions. Hence, studies [85, 22, 21] attempted to impose additional regularizers on $\\mathcal{I}_{\\mathrm{GAN}}$ or $\\mathbf{v}_{\\mathrm{GAN}}$ to ensure that either $K_{D D}$ is negative definite (as in [22, 21]) or $K_{G G}$ is negative definite (as in [21]). In Table 8, we provide a comparison of the various assumptions, at a high-level, drawn from the literature. ", "page_idx": 40}, {"type": "text", "text": "We emphasize that PaGoDA does not require $\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(\\mathbf{x})$ to be strictly positive definite, thanks to PaGoDA\u2019s reconstruction loss. Specifically, it accommodates the scenario where $\\nabla_{\\mathbf{x}}^{2}D_{\\psi^{*}}(\\mathbf{x})=0$ on supp $\\left(p_{\\mathrm{data}}\\right)$ . It\u2019s noteworthy that this capability enables PaGoDA to address cases where the instability of GAN is demonstrated, as exemplified by examples provided by [21]. ", "page_idx": 40}, {"type": "text", "text": "C Limitations and Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Limitations. Algorithmically, the reconstruction loss is incompatible with the classifier-free guidance, which requires us to adopt the original distillation loss. However, as reconstruction loss directly uses the real data, it provides additional merit to decoder training, resulting in better performance as evidenced in the experiments. Theoretically, some theoretical assumptions of PaGoDA are challenging to verify in practice. For example, Theorems B.1 and B.3 require certain Lipschitz continuity of the score functions. This assumption is difficult to maintain at $t=0$ due to the potential concentration of the data manifold in a lower-dimensional space, causing singularity. However, by truncating the PF-ODE solving at $t=\\delta$ (for some $\\delta>0$ ), which is common in practice, this singularity is avoided, making the Lipschitz continuity assumption more feasible. In addition, Theorem B.4\u2019s assumption of the existence of a common minimizer can be difficult to verify empirically. However, with proper neural network parametrization and effective optimization, this assumption becomes more feasible. At last, verifying Assumptions III-1 and III-2 concerning the optimal properties of the generator and discriminator $\\left(G_{\\theta},D_{\\psi}\\right)$ is challenging in practice. These assumptions, essential for general (Lyapunov) stability analysis, are difficult to validate empirically. However, they appear reasonable based on our experimental observations. Last, empirically, PaGoDA\u2019s T2I generation capability relies heavily on the scale and quality of the training dataset. ", "page_idx": 41}, {"type": "text", "text": "Broader Impacts. PaGoDA, as a general media generative model, carries the risk of producing harmful or inappropriate content, such as deepfake images, graphic violence, or offensive material. To mitigate these risks, we avoid using the LAION dataset [33] in our model training, but robust content filtering and moderation mechanisms are essential to additionally prevent the generation of unethical or harmful media. ", "page_idx": 41}]