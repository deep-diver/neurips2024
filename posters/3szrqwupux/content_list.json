[{"type": "text", "text": "Theoretical Foundations of Deep Selective State-Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nicola Muca Cirone Antonio Orvieto Benjamin Walker Department of Mathematics MPI for Intelligent Systems, Mathematical Institute mperial College London T\u00fcbingen AI Center University of Oxford ELLIS Institute T\u00fcbingen ", "page_idx": 0}, {"type": "text", "text": "Cristopher Salvi Department of Mathematics Imperial College London ", "page_idx": 0}, {"type": "text", "text": "Terry Lyons Mathematical Institute University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Structured state-space models (SSMs) are gaining popularity as effective foundational architectures for sequential data, demonstrating outstanding performance across a diverse set of domains alongside desirable scalability properties. Recent developments show that if the linear recurrence powering SSMs allows for a selectivity mechanism leveraging multiplicative interactions between inputs and hidden states (e.g. Mamba, GLA, Hawk/Griffin, HGRN2), then the resulting architecture can surpass attention-powered foundation models trained on text in both accuracy and efficiency, at scales of billion parameters. In this paper, we give theoretical grounding to the selectivity mechanism, often linked to in-context learning, using tools from Rough Path Theory. We provide a framework for the theoretical analysis of generalized selective SSMs, fully characterizing their expressive power and identifying the gating mechanism as the crucial architectural choice. Our analysis provides a closed-form description of the expressive powers of modern SSMs, such as Mamba, quantifying theoretically the drastic improvement in performance from the previous generation of models, such as S4. Our theory not only motivates the success of modern selective state-space models, but also provides a solid framework to understand the expressive power of future SSM variants. In particular, it suggests cross-channel interactions could play a vital role in future improvements. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequence-to-sequence blocks are fundamental components of modern deep learning models for language, images, video, audio, time series, and genomics. For the last five years,attention [Vaswani et al., 2017, Dosovitskiy et al., 2020] has been the dominant mechanism powering these architectures. However, competitive results have recently been achieved without attention, by using state-space models (SSMs): GPU-efficient linear recurrent sequence-to-sequence blocks stemming from S4 [Gu et al., 2021]. SSMs achieve state-of-the-art results on long-range-reasoning benchmarks [Tay et al., 2020] and show outstanding performance in various domain including vision [Nguyen et al., 2022], audio [Goel et al., 2022], biological signals [Gu et al., 2021], reinforcement learning [Lu et al., 2023] and online learning [Zucchet et al., 2023]. SSMs recently have gained significant interest in the community since their computational complexity scales linearly in sequence length, while attention scales quadratically; moreover, unlike other recurrent mechanisms such as LSTMs [Hochreiter and Schmidhuber, 1997] and GRUs [Cho et al., 2014], they can be efficiently parallelized on GPUs during training using parallel scans [Martin and Cundy, 2017, Smith et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "While standard SSMs were shown to be particularly powerful on signal processing tasks, their computation power is limited: the core sequential mechanism of S4 is equivalent to a convolution (flitering) [Li et al., 2022a]. This represents a drawback in challenging domains such as text and genetics, where the ability to select data efficiently in an input-dependent manner \u2013 i.e., perform content-based reasoning \u2013 is crucial (see [Wang et al., 2022, Fu et al., 2022, Arora et al., 2023]). Towards reaching this goal with recurrent models, various adaptations of S4 have been proposed in the last few months. Notably, Mamba [Gu and Dao, 2023] implements simple and efficient gating mechanisms on the S4 recurrence, unlocking input selectivity in the memory update. Mamba achieved state-of-the-art performance in various language modeling tasks while greatly improving the inference throughput. Similar ideas can be found in recent developments inspired by attention, such as RWKV [Peng et al., 2023], RetNet [Sun et al., 2023], Gateloop [Katsch, 2023], Gated Linear Attention (GLA) [Yang et al., 2023], and HGRN2 [Qin et al., 2024]. Very recently, De et al. [2024] surpassed the performance of Mamba with a gated RNN architecture \u2013 Griffin \u2013 based on an improved version of the LRU [Orvieto et al., 2023a], and [Feng et al., 2024] introduced minimal versions of GRU and LSTM as gated SSMs. ", "page_idx": 1}, {"type": "text", "text": "Contributions At the core of the models discussed above is a time-varying dynamical system, where reasoning is performed through an efficient and parallelizable update linear in the hidden state. In this paper, we generalize the structure of such models, drawing a direct link to controlled differential equations $'C D E s)$ [Young, 1936, Lyons, 1994, Kidger et al., 2020, Morrill et al., 2021, Fermanian et al., 2021, Salvi et al., 2022, Hoglund et al., 2023, Walker et al., 2024] and use tools from rough path theory [Lyons et al., 2007] to study expressivity. ", "page_idx": 1}, {"type": "text", "text": "1. In Sec. 3.1 we provide a framework for the analysis of (input-controlled) linear (in the hidden state) recurrences such as S4 and Mamba. This framework allows the use of powerful tools and results in the Rough Path Theory literature by casting a large family of SSMs as Linear CDEs driven by the two possibly nonlinear embeddings $X\\mapsto\\omega^{X}$ and $X\\stackrel{*}{\\mapsto}\\xi^{X}$ , defining gates. Appendices A and E provide a largely self-contained exposition of the key theoretical tools now available to us.   \n2. In Sec. 4 we fully characterize the closure (i.e. the class of functions which can be arbitrarily well approximated) of our generalized models. This provides a generalization of the results by Li et al. [2022b], Orvieto et al. [2023b], Wang and Xue [2023], who only consider the case of S4. The Mamba setting is more rich, complex, and relevant given the rising interest in selective SSMs.   \n3. We show (Thm. 4.2) that full expressivity can be obtained by training only a linear layer on a Linear CDE with random parameters, providing a direct link to kernel methods and reservoirs.   \n4. We point out (Thm. 4.3) that if the recurrence is diagonal, as the case for Mamba, the closure is strictly smaller than in the general dense case. Interestingly though, the closure is a peculiar set of fliters that unlock some specific context-dependent processing. Full expressive power is recovered by stacking multiple SSMs without MLPs in between (Prop. 4.5). ", "page_idx": 1}, {"type": "text", "text": "Our framework not only provides significant theoretical insight regarding some recently proposed SSM architectures, but we also envision it to be a useful tool in analysing, and perhaps developing, future architectural advances. ", "page_idx": 1}, {"type": "text", "text": "2 State-space Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We describe here the structure of the main SSMs-based strategies for processing length- $L$ input sequences of $d$ dimensional tokens: $\\boldsymbol{x}\\in\\mathbb{R}^{d\\times L}$ . We denote by $x_{\\ell}$ the $\\ell_{}$ -th column of $x$ (the $\\ell\\cdot$ -th token) and by $x^{i}$ the $i$ -th row of $x$ (time series for the $i$ -th channel). We will write $A\\cdot v$ for matrix-vector multiplication when this enhances comprehension, and use bold letters for \u201ctensors\u201d of order greater than 2 (such as $\\mathbf{z}\\in\\times_{i}\\mathbb{R}^{N_{i}\\times L}$ introduced below). ", "page_idx": 1}, {"type": "text", "text": "2.1 Review of Modern SSMs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We start with a quick simplified recap of S4 [Gu et al., 2021], the first SSM proposed in the literature, and then describe recent improved variants such as Mamba (in particular, the S6 block) [Gu and Dao, 2023]. We restrict our focus to the recurrent mechanism and invite the reader to refer to the original papers for a description of the token-wise operations following and preceding each block. ", "page_idx": 1}, {"type": "text", "text": "SSM basics and S4. Most1 SSMs [Gu et al., 2021, 2022] operate independently on input channels. Each time series $x^{i}\\in\\mathbb{R}^{L}$ is seen as the result of sampling a latent continuous-time signal $X^{i}$ : $[0,1]\\rightarrow\\mathbb{R}$ at multiples of a channel-dependent stepsize $\\Delta_{i}^{\\bar{}}>0$ : $X_{\\Delta_{i}\\ell}^{i}:=X^{i}(\\Delta_{i}\\ell)=\\bar{x}_{\\ell}^{i}$ . In S4, each path $X^{i}$ produces a complex-valued hidden state signal $\\mathbf{Z}_{i}:[0,1]\\to\\mathbb{C}^{N_{i}}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{Z}_{i;t}=A_{i}\\cdot\\mathbf{Z}_{i;t}\\;d t+B\\;X_{t}^{i}d t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A_{i}=\\mathrm{diag}(a_{i,1},a_{i,2},.~.~.~a_{i,N})$ is channel-specific diagonal $N_{i}\\times N_{i}$ complex valued matrix and $B\\in\\mathbb{C}^{N_{i}}$ is an input projection shared across input components $i\\in[d]$ . SSMs are based on a stable discretization of the continuous system above: each input sequence channel $\\boldsymbol{x}^{i}\\in\\mathbb{R}^{L}$ produces a sequence of hidden states $\\mathbf{z}_{i}=[\\mathbf{z}_{i;1}|\\dot{\\mathbf{z}}_{i;2}|\\dots|\\mathbf{z}_{i;L}]\\in\\mathbb{C}^{N_{i}\\times L}$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{z}_{i;\\ell}=\\bar{A}_{i}\\cdot\\mathbf{z}_{i;\\ell-1}+\\bar{B}_{i}x_{\\ell}^{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{A_{i}}$ and ${\\bar{B}}_{i}$ are determined by the discretization technique and the channel-dependent stepsize $\\Delta_{i}$ . Under the commonly used Zero-Order Hold discretization2, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{A}_{i}=\\exp(\\Delta_{i}A_{i}),\\quad\\bar{B}_{i}=(\\Delta_{i}A_{i})^{-1}(\\exp(\\Delta_{i}A_{i})-I)\\Delta_{i}B\\approx\\Delta_{i}B.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note from (2) that SSMs at inference time are equivalent to linear recurrent neural networks (RNNs). Yet, learning with gradient descent is performed on the continuous-time variables, unlocking stable signal propagation and alleviating vanishing gradients [Orvieto et al., 2023a, Zucchet and Orvieto, 2024]. Finally, at each channel $i$ , the sequence of hidden states is mapped back to real numbers, and linear projections $C_{i}:\\mathbb{C}^{N_{i}}\\rightarrow\\mathbb{R}$ are performed to produce an output a sequence of tokens $\\boldsymbol{y}\\in\\mathbb{R}^{d\\times L}$ with the same dimensions as $x$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{\\ell}^{i}:=C_{i}\\cdot{\\mathbf{z}}_{i;\\ell}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To conclude, we point out that the transition matrices $A_{i}$ are often structured, i.e. initialized deterministically through HiPPO theory $\\lceil\\mathrm{Gu}$ et al., 2020] in diagonal form. Common choices [Gu et al., 2022] are $\\textstyle a._{,n}={\\overline{{-{\\frac{1}{2}}+\\mathrm{i}\\pi n}}}$ (S4D-Lin3) and $a.,n=-{\\frac{1}{2}}$ (S4D-Real). ", "page_idx": 2}, {"type": "text", "text": "Mamba. As done in practice, let us consider all channels\u2019 hidden dimensions $N_{i}$ equal to $N$ . The Selective SSM (S6) powering the Mamba architecture [Gu and Dao, 2023] augments S4 with input-controlled matrices: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{i;\\ell}=\\bar{A}_{i}(x_{\\ell})\\cdot\\mathbf{z}_{i;\\ell-1}+\\bar{B}_{i}(x_{\\ell})x_{\\ell}^{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the most crucial component (see in-context learning argument by $\\mathrm{Gu}$ and Dao [2023]) is the dependency of the diagonal matrix $\\bar{A}_{i}(x_{\\ell})\\;\\in\\;\\mathbb{R}^{N\\times N}$ at timestamp $\\ell$ on all input channels at timestamp $\\ell$ . This makes the operation $\\bar{A}_{i}(x_{\\ell})\\cdot\\mathbf{z}_{i;\\ell-1}$ effectively a gate. The dependency of $\\bar{A}_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{N\\times N}$ on the input is achieved efficiently by letting $\\Delta_{i}$ in (3) be computed, at step $\\ell$ , as $\\Delta_{i}(x_{\\ell})$ where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\\quad\\Delta_{i}(x)=\\mathrm{softplus}(\\alpha_{i}\\cdot x+\\beta_{i})\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\cdot$ is the scalar product and 4 $\\cdot~\\alpha_{i}\\,\\in\\,\\mathbb{R}^{d}$ , $\\beta_{i}\\,\\in\\,\\mathbb{R}$ . Further, $\\bar{B}_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{N}$ is computed via a, shared between channels, linear map $B\\in\\mathbb{R}^{N\\times d}$ via $\\bar{B}_{i}(x_{l})=(B\\cdot x_{\\ell})\\,\\Delta_{i}(x_{\\ell})\\in\\mathbb{R}^{N}$ . Finally, each $\\mathbf{z}_{i;\\ell}\\,\\in\\,\\mathbb{R}^{N}$ is projected to $y_{\\ell}^{i}\\in\\dot{\\mathbb{R}}$ via a matrix $C_{i}$ . This step can also be done by means of output gating $C_{i}$ function of the input), but we avoid this complication here as it can be seen as an architectural component outside the recurrence. ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1. While each channel evolves separately, the laws of evolution are pointwise determined by all input features: $A_{i}$ and $B_{i}$ can be functions of $x_{\\ell}$ , and not just of $\\boldsymbol{x}_{\\ell}^{i}$ . We will discuss this in Sec. 4.3 after presenting our general results. ", "page_idx": 2}, {"type": "text", "text": "The RG-LRU [De et al., 2024] works similarly, yet processing all input channels at once with a diagonal recurrence. Gateloop [Katsch, 2023], GLA [Yang et al., 2023], and HGRN2 [Qin et al., 2024] leverage similar ideas, though they differ in parametrization and gating strategies. ", "page_idx": 2}, {"type": "text", "text": "2.2 Known properties of (non-linear) recurrences ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The expressiveness of standard nonlinear RNNs of the form $z_{\\ell}=A\\sigma(z_{\\ell-1})+B x_{\\ell}$ , where $\\sigma$ is a nonlinearity, has been extensively studied since the seminal work of Siegelmann and Sontag [1992], with recent contributions such as Korsky and Berwick [2019] and Hanson and Raginsky [2020]. In particular, Hanson and Raginsky [2020] proved that wide enough non-linear RNNs can approximate up to vanishing precision non-linear time-homogeneous systems of differential equations driven by input paths. The argument used here is based on the celebrated Barron\u2019s theorem [Barron, 1993] for approximation of continuous functions with neural networks with one hidden layer. Indeed, note that non-linear RNNs are recurrent perceptrons with one hidden layer, acting both on the state and the input [Tallec and Ollivier, 2018]. Instead, (selective) SSMs such as S4 and Mamba have transition map which is linear in the state \u2013 unlocking parallelization [Smith et al., 2023, Gu and Dao, 2023]. In the context of linear RNNs and non-selective SSMs, many results (classic and new) exist that characterize expressivity. Li et al. [2022b] showed that linear RNNs (i.e. S4-like recurrences) can approximate arbitrary convolution filters in the width limit. Further, Hanson and Raginsky [2019] proved that stacking exponentially (in the sequence length) many temporal convolution fliters, chained together with ReLU activations, leads to approximation of arbitrary non-linear fliters. Recent works [Orvieto et al., 2023b, Wang and Xue, 2023] prove the universality of linear recurrences (one layer) when equipped with a fixed (timestamp independent) point-wise MLP acting across the recurrence output, with intriguing connections to Volterra series [Boyd and Chua, 1985]. ", "page_idx": 3}, {"type": "text", "text": "Mamba (alongside with gated linear attention variants e.g. Yang et al. [2023]) falls neither in the linear RNN nor the nonlinear RNN setting: its recurrence is linear on the hidden state (can be parallelized) but unlike S4, it is not linear time-invariant as the input controls the recurrence eigenvalues. In this paper, we are interested in this hybrid setting. It is worth noting that some work exploring Mamba\u2019s expressiveness has already been performed to study some interesting toy tasks [Jelassi et al., 2024] and to understand its limitation using the framework of formal language theory [Merrill et al., 2024]. Compared to these works, which outline interesting failure cases, this paper studies a more general class of models, allowing to identify how architectural choices impact expressivity. ", "page_idx": 3}, {"type": "text", "text": "3 SSMs as Linear CDEs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The crucial component that unlocks in-context learning and selectivity in modern SSMs is the inputdependent state-to-state transition matrix [Gu and Dao, 2023], gating the hidden state and thus allowing the system to fliter out unnecessary context and remember relevant information indefinitely. ", "page_idx": 3}, {"type": "text", "text": "At the core of most modern SSMs is a a recurrence which is linear in the hidden state, but potentially non-linear in the input. This class includes many recent SSM-based or inspired models. Crucially, it does not contain classical RNNs, LSTMs, and GRUs \u2013 for which results are known and rely on the non-linear dependence of the hidden state in the update rule (Sec. 2.2). As we will shortly see, structure and features of (selective) SSMs can be studied within a unified, convenient, continuous-time framework (Linear Controlled Differential Equations). This allows to answer the following question: ", "page_idx": 3}, {"type": "text", "text": "\u201cWhat is the most one can achieve using a recurrence which is linear in the hidden state and potentially non-linear in the input?\u201d ", "page_idx": 3}, {"type": "text", "text": "3.1 Linear CDEs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "According to their continuous-time formulation, SSMs process input data sampled from a continuous path $X:[0,1]\\rightarrow\\mathbb{R}^{d}$ , where $d$ is the number of channels. By $\\mathbf{\\bar{\\boldsymbol{X}}}_{t}^{i}$ we denote the input channel $i$ , evaluated at time $t\\in[0,1]$ . More formally, we consider input trajectories in the separable Banach space $\\mathbb{X}=C_{1,0}([0,1];\\mathbb{R}^{d})$ of absolutely continuous $\\mathbb{R}^{d}$ dimensional paths. In this space, one can write $\\begin{array}{r}{X=\\int_{0}^{t}\\dot{X}_{s}}\\end{array}$ ds since $X\\in L^{1}([0,1];\\mathbb{R}^{d})$ ; and the norm is $\\begin{array}{r}{\\|X\\|_{1;[0,1]}:=\\int_{0}^{1}|\\dot{X}_{s}|\\;d s}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "To model gates (see Mamba in (4)), we introduce two maps transforming the path $X$ , which output trajectories $\\omega^{\\mathrm{X}},\\xi^{\\mathrm{X}}$ living in potentially higher dimensions: $d_{\\omega}$ and $d_{\\xi}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega:\\mathbb{X}\\to C_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}}),\\quad\\xi:\\mathbb{X}\\to C_{1,0}([0,1];\\mathbb{R}^{d_{\\xi}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we used the shorthand notation $\\omega^{\\mathtt{X}}:=\\omega(X),\\xi^{\\mathtt{X}}=\\xi(X)$ . Akin the notation for the $i$ -the channel of $X$ (i.e. $X^{i}$ ), we denote by $\\omega^{\\mathrm{X},i}$ the $i$ -th channel in $\\omega^{\\mathrm{X}}$ . ", "page_idx": 3}, {"type": "text", "text": "The role of the $\\xi,\\omega$ functions \u2013 which we denote gating functions \u2013 will be clear very soon. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Linear CDE). Fix $N\\in\\mathbb{N}$ (hidden-state dimension), matrices $A_{1},...,A_{d_{\\omega}}\\in\\mathbb{R}^{N\\times N}$ and $B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ . The hidden state $Z^{\\mathrm{X}}:=Z(X)$ is computed by the Linear CDE through a linear (in the hidden state) differential equation driven by $\\omega,\\xi$ \u2013 functions of the input path: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd Z_{t}^{\\mathrm{X}}=\\sum_{i=1}^{d_{\\omega}}A_{i}Z_{t}^{\\mathrm{X}}d\\omega_{t}^{\\mathrm{X},i}+B d\\xi_{t}^{\\mathrm{X}},\\quad Z_{0}^{\\mathrm{X}}=Z_{0}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We show that both S4 and Mamba can be written in continuous-time as systems of parallel Linear CDEs. The key ingredient setting the two models apart is the choice of drivers $\\omega$ and $\\xi$ . As in the preceding section, we will use the bold tensor notation to stress the parallel nature of these CDEs. ", "page_idx": 4}, {"type": "text", "text": "\u2022 S4 is a Linear CDE: It is sufficient to consider the setting $d=1$ , $N=N_{i}$ . The S4 model [Gu et al., 2021] in this case is $d Z_{t}=A\\cdot Z_{t}\\;d t+B\\;(X_{t}d t)$ , where $A\\in\\mathbb{R}^{N\\times N}$ is diagonal. This can be written as a Linear CDE with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{t}^{\\mathrm{x}}=t\\in\\mathbb{R},\\qquad\\xi_{t}^{\\mathrm{x}}=\\int_{0}^{t}X_{s}d s\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "since $d\\omega_{t}^{\\mathrm{x}}=d t$ and $d\\xi_{t}^{\\mathrm{X}}=X_{t}d t$ . As the reader can promptly notice, here $\\omega^{\\mathrm{X}}$ is not a function of $X$ \u2013 this will have a crucial role in expressivity (see Sec. 4.2). ", "page_idx": 4}, {"type": "text", "text": "\u2022 Mamba is a Linear CDE: Recall from (4) that the recurrence inside Mamba (i.e. S6), can be written as $\\begin{array}{r}{\\mathbf{z}_{i;\\ell}=\\bar{A}_{i}(x_{\\ell})\\cdot\\mathbf{z}_{i;\\ell-1}+\\bar{B}_{i}(x_{\\ell})x_{\\ell}^{i}}\\end{array}$ where for generic timestamp features $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have $\\bar{A}_{i}(x)=\\exp(\\Delta_{i}(x)A_{i})$ , $\\bar{B}_{i}(x)\\approx(B{\\cdot}x)\\Delta_{i}(x)$ and $\\Delta_{i}(x)=\\operatorname{softplus}(\\alpha_{i}\\cdot x\\mathbf{+}\\beta_{i})$ . Let us introduce a parameter $\\delta>0$ , and consider $\\tilde{\\alpha}_{i}=\\alpha_{i}/\\delta,\\tilde{\\beta}_{i}=\\beta_{i}/\\delta$ . Let us further approximate the softplus function with a ReLU $(\\sigma(x)=\\operatorname{ReLU}(x)\\simeq\\log(1+e^{x}))$ to obtain $\\Delta_{i}(x)=\\sigma(\\delta\\tilde{\\alpha}_{i}\\cdot x+\\delta\\tilde{\\beta}_{i})=$ $\\sigma(\\tilde{\\alpha}_{i}\\cdot x+\\tilde{\\beta}_{i})\\delta$ . Therefore, as $\\delta\\ \\rightarrow\\ 0$ , one has $\\bar{A}_{i}(x)\\;=\\;\\exp(\\Delta_{i}(x)A_{i})\\;=\\;\\exp(\\sigma(\\tilde{\\alpha}_{i}\\,\\cdot\\,x\\,+\\,$ $\\tilde{\\beta}_{i})\\delta A_{i})\\stackrel{\\delta\\to0}{\\rightarrow}1+\\sigma(\\tilde{\\alpha}_{i}\\cdot x+\\tilde{\\beta}_{i})\\delta A_{i}$ , leading to the recurrence ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{i;\\ell}=\\mathbf{z}_{i;\\ell-1}+A_{i}\\cdot\\mathbf{z}_{i;\\ell-1}\\,\\sigma(\\tilde{\\alpha}_{i}\\cdot x_{\\ell}+\\tilde{\\beta}_{i})\\delta+(B\\cdot x_{\\ell})\\,x_{\\ell}^{i}\\sigma(\\tilde{\\alpha}_{i}\\cdot x_{\\ell}+\\tilde{\\beta}_{i})\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As we show formally in Appendix F, $\\delta$ plays the role of the differential $d t$ . The equation above is the Euler discretization of the differential equation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{R}^{N}\\ni d\\mathbf{Z}_{i;t}^{\\times}=A_{i}\\cdot\\mathbf{Z}_{i;t}^{\\times}\\,\\sigma(\\tilde{\\alpha}_{i}\\cdot X_{t}+\\tilde{\\beta}_{i})d t+(B\\cdot X_{t})\\,X_{t}^{i}\\sigma(\\tilde{\\alpha}_{i}\\cdot X_{t}+\\tilde{\\beta}_{i})d t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where for each $i$ , $\\mathbf{Z}_{i}^{\\mathrm{x}}:[0,1]\\to\\mathbb{R}^{N}$ . These are Linear CDEs with carefully chosen $\\omega$ and $\\xi$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{i;t}^{\\mathrm{x}}=\\int_{0}^{t}\\sigma(\\tilde{\\alpha}_{i}\\cdot X_{s}+\\tilde{\\beta}_{i})d s\\in\\mathbb{R},\\quad\\xi_{i;t}^{\\mathrm{x}}=\\int_{0}^{t}X_{s}\\;X_{s}^{i}\\sigma(\\tilde{\\alpha}_{i}\\cdot X_{s}+\\tilde{\\beta}_{i})d s\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that here the $\\boldsymbol{\\xi}_{i}$ depend on higher powers of $X$ \u2019s dimensions, an intriguing feature connected to input gating [Hochreiter and Schmidhuber, 1997, Cho et al., 2014]. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2 (Are hidden state components recurrently mixed?). In the Linear CDE defined by $\\begin{array}{r}{d Z_{t}^{\\mathrm{x}}=[\\sum_{j=1}^{d_{\\omega}}A_{j}d\\omega_{t}^{\\mathrm{x},j}]\\cdot Z_{t}^{\\mathrm{x}}+B\\cdot d\\xi_{t}^{\\mathrm{x}}}\\end{array}$ channels of the transformed input path $\\omega^{\\mathrm{X}}$ are mixed in a linear way in the recurrent step and stored in a shared hidden state $Z^{\\mathrm{X}}$ . This allows our framework to be more general compared to S4 and Mamba, where each channel of the input is processed individually, and hidden states are later combined. We know already from Merrill et al. [2024] that this distinction between hidden state mixing strategies is crucial for expressivity. Our discussion in Sec. 4.3 provides an in-depth look at the effects of separate channel processing, achieved in our framework by choosing the $A_{i}\\mathrm{s}$ to be diagonal and non-zero only around a channel-specific portion. ", "page_idx": 4}, {"type": "text", "text": "4 Expressivity of Linear CDEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Having established the connection between SSMs and Linear CDEs, we now provide an explicit characterization of the uniform closure of Linear CDEs, i.e. a description of all the functions from compact subsets of $\\mathbb{X}$ to $\\mathbb{R}$ that can be uniformly approximated at an arbitrary precision by a Linear CDE of the form given in (5). ", "page_idx": 4}, {"type": "text", "text": "4.1 Characterization of the closure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The proof of the main theorem we present here (Thm. 4.1) is involved and requires tools from Rough Path theory; we provide a full derivation in Appendix B with tools reviewed in the self contained Appendix E, as well as an introduction to the Signature approach in Section 4.5 and Appendix A. While familiarity with these concepts is not necessary to grasp the main results as stated, it is essential for a thorough comprehension. ", "page_idx": 5}, {"type": "text", "text": "In this subsection, Linear CDEs are analyzed in full generality \u2013 i.e., in the dense setting. While for efficiency reasons non-diagonal recurrences are rarely used in SSMs, our results allow to precisely characterize the Linear CDE hypothesis class (i.e. the closure), thus to the answer the question \u201cWhat is the most we can achieve from recurrences which are linear in the hidden state?\u201d. We show that Linear CDEs can model, at fixed time $t$ , arbitrary continuous functions of the whole seen inputs. Of course, understanding the diagonal setting with separate channel processing is of utmost importance in the current research landscape. The tools and results in this subsection allow us to directly discuss this case and compare it to the dense setting \u2013 this is presented in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "In this section, for any path $\\gamma\\in C_{1,0}([0,1],\\mathbb{R}^{d_{\\gamma}})$ and any sub-interval $[s,t]\\subset[0,1]$ , we denote by $\\gamma_{[s,t]}\\in C_{1,0}([0,1],\\mathbb{R}^{d_{\\gamma}})$ the path $\\gamma_{[s,t]}(u)=\\gamma_{t\\wedge u}-\\gamma_{s\\wedge u}$ , where $a\\wedge b=\\operatorname*{min}(a,b)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let $\\mathbb{X}\\subset C_{1,0}([0,1],\\mathbb{R}^{d})$ be compact and choose continuous gating functions $\\omega,\\xi$ such that 5 $\\omega_{t}^{x,1}\\,=\\,t$ and $\\omega_{t}^{X,2}\\,=\\,t^{2}$ . Consider the Linear CDE model (5). Let $\\Psi,\\ \\Phi$ be generic continuous functions from paths to real vectors: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi:C_{1,0}([0,1],\\mathbb{R}^{d_{\\omega}})\\to\\mathbb{R},\\quad\\Phi:C_{1,0}([0,1],\\mathbb{R}^{d_{\\omega}})\\to\\mathbb{R}^{d_{\\xi}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "There exist dense matrices $A_{1},...,A_{d_{\\omega}},B$ such that, after a fixed final linear projection $C\\in\\mathbb{R}^{1\\times N}$ , the output $Y_{t}^{X}=C Z_{t}^{X}$ is arbitrarily close, uniformly on $\\mathbb{X}\\times[0,1]$ , to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi(\\omega_{[0,t]}^{X})+\\int_{0}^{t}\\Phi(\\omega_{[s,t]}^{X})\\cdot d\\xi_{s}^{X}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where \u00b7 is the scalar product. Moreover $Y_{t}^{X}=C Z_{t}^{X}$ is itself of form (7). ", "page_idx": 5}, {"type": "text", "text": "In Theorem 4.1, the $A_{i}$ are dense matrices constructed ad hoc for the proof. We show next that, with high probability, random Glorot-initialized matrices [LeCun et al., 2012] provide enough expressivity \u2013 one only has to choose the appropriate matrix $C$ . This is a similar mechanism to the paradigm advocated in reservoir computing [Lukoveviius and Jaeger, 2009, Cuchiero et al., 2021a, Compagnoni et al., 2023]. The next result, however, is novel and of independent interest in Rough Path Theory. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Under the hypothesis of Theorem 4.1, pick $\\begin{array}{r}{[A_{j}]_{n,n^{\\prime}}\\overset{i i d}{\\sim}\\mathcal{N}(0,\\frac{1}{N})}\\end{array}$ and $[Z_{0}]_{n},[B]_{n,j}\\stackrel{i i d}{\\sim}$ $\\mathcal{N}(0,1)$ . For any functional $F:\\mathbb{X}\\times[0,1]\\rightarrow\\mathbb{R}$ of the form (7) and $\\epsilon>0$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}\\mathbb{P}\\bigg[\\Big\\{\\exists C\\in\\mathbb{R}^{1\\times N}\\;s u c h\\;t h a t\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-C Z_{t}^{x}|\\leq\\epsilon\\Big\\}\\Big]=1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Intuition on the closure result: role of gates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Roughly speaking, our result shows that dense Linear CDEs have drastically superior expressive power compared to dense linear RNNs. This contrast is to be attributed completely to the gate $\\omega$ . ", "page_idx": 5}, {"type": "text", "text": "Warmup \u2013 Linear RNNs. $\\operatorname{Thm}4.1$ can be seen as a generalization of the Universal Approximation for Linear RNNs presented by Li et al. [2022b][Thm. 7] for generic gates $\\omega,\\xi$ . In fact, their setting is restricted to $\\omega_{t}^{\\mathrm{{x}}}=t$ and $\\begin{array}{r}{\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t}X_{s}d s}\\end{array}$ . This is also the case for S4, S5 and the LRU \u2013 which are linear RNNs at test time. Then the only information contained in $\\omega_{[s,t]}$ is the increment $t-s$ so that family (7) reduces to $\\displaystyle\\left\\{(X,t)\\mapsto\\psi(t)+\\int_{0}^{t}\\phi(t-s)\\cdot X_{s}d s\\right\\}$ . This is, fundamentally, the set of linear filters on the input. As shown in Gu and Dao [2023], such processing is unable to adapt information flow in-context. ", "page_idx": 5}, {"type": "text", "text": "How does a Linear CDE process inputs? Take without loss in generality6 $\\omega_{t}^{\\mathrm{X}}=X_{t}$ . The first term in the function class $\\begin{array}{r}{\\{\\Psi(X_{[0,t]})+\\int_{0}^{t}\\Phi(X_{[s,t]})\\cdot d\\xi_{s}^{\\mathrm{x}}\\}}\\end{array}$ is already enough to establish that the output $C Z_{t}^{\\mathrm{X}}$ is a nonlinear function of all previously seen inputs $X_{[0,t]}-\\xi$ could be set to zero. The term $\\Psi(X_{[0,t]})$ is however non-trivial only in the $Z_{0}\\neq0$ case (cf. Appendix B): picking $\\xi_{t}^{\\mathrm{X}}=0\\in\\mathbb{R}^{N}$ for all $t$ indeed leads to $\\begin{array}{r}{d Z_{t}^{\\mathrm{x}}=\\left[\\sum_{i=1}^{d_{\\omega}}A_{i}d\\omega_{t}^{\\mathrm{X},i}\\right]Z_{t}^{\\mathrm{x}}}\\end{array}$ , which is evolving only if $Z_{0}\\neq0$ . While this setting is interesting and suggests a clear direction for future research, a case more similar to Mamba (see Sec. 4.3) is $Z_{0}=0$ and $\\begin{array}{r}{\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t}X_{s}d s}\\end{array}$ . Here, we can approximate arbitrarily well outputs of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{(X,t)\\mapsto\\int_{0}^{t}\\Phi(X_{[s,t]})\\cdot X_{s}d s\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Phi$ is any continuous function of the input path, restricted to the portion $[s,t]$ . This clearly shows that dense Linear CDEs are capable of context-dependent flitering: the output is again a linear combination of previously seen inputs, but weights are not predetermined as in Linear RNNs (S4) \u2013 they are a function of the context. A similar yet less powerful processing is happening in the diagonal setting, which we explore next. ", "page_idx": 6}, {"type": "text", "text": "4.3 The Diagonal Case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The choice of diagonal weights considerably restricts the family of learnable functionals. Intuitively the diagonal choice corresponds to running $N$ independent 1-dimensional systems, this absence of mixing between the different hidden dimensions is the main culprit for the loss of expressivity. The full power can however be recovered by chaining the diagonal schemes (cf. Prop. 4.5). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (Diagonal Case). If the matrices $A_{1},...,A_{d_{\\omega}}$ are constrained to be diagonal, the requirements $\\omega_{t}^{x,1}=t$ , $\\omega_{t}^{X,2}=t^{2}$ can be dropped and the closure reduces to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{(X,t)\\mapsto\\psi(\\omega_{t}^{X})+\\int_{0}^{t}\\phi(\\omega_{t}^{X}-\\omega_{s}^{X})\\cdot d\\xi_{s}^{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for continuous $\\psi:\\mathbb{R}^{d_{\\omega}}\\rightarrow\\mathbb{R}$ and $\\phi:\\mathbb{R}^{d_{\\omega}}\\rightarrow\\mathbb{R}^{d_{\\xi}}$ . ", "page_idx": 6}, {"type": "text", "text": "Compared to the dense setting, the effect of diagonality is pretty clear. Let us again assume $\\omega_{t}^{\\mathrm{X}}=X_{t}$ and $\\begin{array}{r}{\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t}X_{s}}\\end{array}$ ds for simplicity. While $\\begin{array}{r}{\\int_{0}^{t}\\Phi(X_{[s,t]})\\cdot X_{s}}\\end{array}$ ds unlocks filtering based on the entire trajectory $X_{[s,t]}$ , the diagonal case term $\\textstyle\\int_{0}^{t}\\phi(X_{t}-X_{s})\\cdot X_{s}\\;d s$ indicates that filtering coefficients can only be chosen by comparing two elements of the (potentially transformed through $\\omega$ ) input sequence. While this precise and tight result reveals a pitfall of diagonal recurrence, it also brings about an interesting connection to attention [Vaswani et al., 2017], where only a finite number of tokens are compared at each layer. While a smart choice of gating functions $\\omega,\\xi$ can improve on the learned nonlinear filtering strategy (e.g. based on filtered input versions as in Mamba), our theory reveals the fundamental processing discrepancy compared to the dense setting, a property which was also explored in recent literature [Merrill et al., 2024] using different tools. ", "page_idx": 6}, {"type": "text", "text": "As already noted, on top of diagonality, recent SSMs also mix inputs as linear combinations of independently run channel-dependent systems. This slightly modifies the function class as: ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.4 (Mamba Case). In the Mamba setting, the closure reduces to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{(X,t)\\mapsto\\sum_{i=1}^{d_{\\omega}}\\psi_{i}(\\omega_{t}^{X,i})+\\sum_{i=1}^{d_{\\omega}}\\int_{0}^{t}\\phi_{i}(\\omega_{t}^{X,i}-\\omega_{s}^{X,i})\\ d\\xi_{s}^{X,i}\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for continuous $\\psi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $\\phi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ ", "page_idx": 6}, {"type": "text", "text": "4.4 Chaining Diagonal CDEs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fortunately it is possible to re-gain expressivity without sacrificing the computational advantages of diagonal schemes through chaining. This means driving a new Linear CDE by the solution of a previous Linear CDE, and repeating this procedure $K$ times (cf. Appendix C). ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.5. Assume a compact $\\mathbb{X}\\subset C_{1,0}([0,1];\\mathbb{R}^{d})$ . For any functional $F:\\mathbb{X}\\times[0,1]\\rightarrow\\mathbb{R}$ of the form (7) and $\\epsilon>0$ there exists a sequence of linear maps $W_{k}\\in\\mathbb{R}^{M_{k}\\times N_{k}}$ , diagonal weights $A_{i}^{(k)}$ and $B^{(k)}$ for the following family of chained diagonal Linear CDEs ", "page_idx": 7}, {"type": "equation", "text": "$$\nZ_{t}^{0,x}\\equiv0,\\quad d Z_{t}^{k+1,x}=\\sum_{i=1}^{d+M_{k}}A_{i}^{(k+1)}Z_{t}^{k+1,x}d\\left[{W_{k}}Z^{k,X}\\right]_{t}^{i}+B^{(k+1)}d X_{t}\\in\\mathbb{R}^{N_{k+1}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "such that eventually, as $k\\rightarrow\\infty,$ , there exists $C_{k}\\in\\mathbb{R}^{1\\times N_{k}}$ with sup $|F(X,t)-C_{k}Z_{t}^{k,X}|\\leq\\epsilon.$ $(X,t)\\!\\in\\!\\mathbb{X}\\!\\times$ [0,1] ", "page_idx": 7}, {"type": "text", "text": "Intuitively, with chaining one recovers the mixing between the input dimensions which was so important for the expressiveness of dense Linear CDEs. This does not happen immediately but crucially depends on the length of the chain, the result in fact tells us that the recovery holds for long enough chains (and big enough hidden states). In Appendix C.2.1 we argue that the same conclusions hold also in the Mamba setting with non-linear gates. ", "page_idx": 7}, {"type": "text", "text": "4.5 Proof Idea - Signature expansion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we introduce the primary tools, objects, and techniques from Rough Path theory used in our proofs. Given their technical nature, we have chosen to present the main results of this paper without explicit reference to them, allowing readers to understand the work without needing specialized knowledge. For those interested in the finer details, here we provide a brief overview and refer to Appendix A for additional details and references. ", "page_idx": 7}, {"type": "text", "text": "To study the expressivity of Linear CDEs it is convenient to introduce the so-called signature transform [Lyons et al., 2007, Kidger et al., 2019, Fermanian et al., 2023], a classical path-transform from stochastic analysis. The main reason for doing so is that, as a simple consequence of the Stone-Weirestrass theorem, linear functionals on the signature provide the essential building blocks (analogous to monomials on Euclidean spaces) to approximate continuous functions on path space. ", "page_idx": 7}, {"type": "text", "text": "Consider a path $\\gamma\\in C_{1,0}([0,1];\\mathbb{R}^{d_{\\gamma}})$ and define as $\\mathbb{W}_{d_{\\gamma}}$ the set of words (i.e. ordered sequences) in $\\{1,\\dots,d_{\\gamma}\\}^{\\ \\gamma}$ . The signature transform is the following infinite collection of scalar iterated integrals ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\gamma)_{s,t}:=\\left(\\mathrm{Sig}(\\gamma)_{s,t}^{(I)}\\right)_{I\\in\\mathbb{W}_{d_{\\gamma}}},\\quad\\mathrm{Sig}(\\gamma)_{s,t}^{(I)}:=\\int_{s<u_{1}<\\ldots<u_{n}<t}\\dot{\\gamma}_{u_{1}}^{(i_{1})}\\ldots\\dot{\\gamma}_{u_{n}}^{(i_{n})}d u_{1}\\ldots d u_{n}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "A classical result from rough path theory states that a Linear CDE can be expanded explicitly as an (infinite) linear combination of terms in the signature of the driving path. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.6. For any choice of matrices $A_{1},...,A_{d_{\\omega}}$ and $B$ , the unique solution Linear CDE (5) is, using the notation $A_{I}:=A_{i_{n}}...A_{i_{1}}$ , given by ", "page_idx": 7}, {"type": "equation", "text": "$$\nZ_{t}^{X}=\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}Z_{0}\\;S i g(\\omega^{X})_{0,t}^{(I)}+\\sum_{i=1}^{d_{\\xi}}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}B_{i}\\;\\int_{0}^{t}S i g(\\omega^{X})_{s,t}^{(I)}d\\xi_{s}^{X,i}\\in\\mathbb{R}^{N}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice that the previous result does not rely on any assumptions on the nature of $Z_{0},\\,A_{i}$ , and $B$ ; for any such choice the result is a time-independent linear map on a feature vector $T(X)_{0,t}:=$ $\\begin{array}{r}{(\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{0,t}^{(I)},\\int_{0}^{t}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{(I)}d\\xi_{s}^{\\mathrm{x},i})_{(I,i)}}\\end{array}$ , where the index $(I,i)$ runs over $\\mathbb{W}_{d_{\\omega}}\\times\\{1,...,d_{\\xi}\\}$ . ", "page_idx": 7}, {"type": "text", "text": "The main takeaway is that any linear projection $Y_{t}^{\\mathrm{x}}\\,:=\\,C Z_{t}^{\\mathrm{x}}$ is written as an (infinite) linear combination of the terms in $T(X)_{0,t}$ . This means that the expressive power of such schemes is almost completely determined by the gate $\\omega$ , which is the only path of which high order information is taken into consideration through its full signature. It is evident then that the classical choice of input-independent $\\omega$ (i.e. $\\omega_{t}^{\\mathrm{x}}=t$ ) then precludes the use of higher order statistics of $X$ . ", "page_idx": 7}, {"type": "text", "text": "5 Path-to-Path Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 4, we showed that Linear CDEs (and chained Mamba) can model, at fixed time $t$ , arbitrary continuous functions of the whole seen inputs. Assume wanting to learn a functional of type $(X,t)\\mapsto\\Psi_{t}(\\omega_{[0,t]}^{\\mathrm{X}})$ where the map $\\Psi_{t}$ is changing with time as well, this is an example of a general continuous path-to-path model. Note in particular how this is a more general family than the one of Theorem 4.1, where the maps $\\Psi$ and $\\Phi$ are fixed once and for all. Here, we discuss how passing the hidden state through a multi-layer perceptron (MLP), instead of just a linear readout (matrix $C$ ), allows us to efficiently approximate this richer class, interpolating between the $\\Psi_{t}\\mathbf{s}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As shown in Orvieto et al. [2023b], classical SSMs followed by an MLP are universal on sequences. Given their nature of input-independent convolutions, their construction defers all reasoning to the MLP acting on the output: in S4, the SSM is simply providing an input compression \u2013 with no added reasoning. In our setting, we instead characterized the processing power of input-controlled (dense or diagonal) SSMs precisely, showing how it greatly surpasses linear filtering. For this reason, the computational burden for an MLP action on a general Linear CDE would be greatly diminished and its actual function reduced to an interpolation of the maps $\\Psi_{t}$ . We defer the proof to Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.1. Fix a compact set $\\mathbb{K}\\subseteq\\mathbb{X}$ and continuous $\\omega,\\xi$ with $\\omega_{t}^{x,1}\\equiv t$ . Then for all $\\epsilon>0$ and all causal 8 continuous mapping $G:C_{1,0}([0,1],\\mathbb{R}^{d_{\\omega}})\\times[0,1]\\to\\mathbb{R}$ there exist an integer $N\\geq0$ , some MLP $F:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}$ , and parameters $Z_{0}\\in\\mathbb{R}^{N},A_{i}\\in\\mathbb{R}^{N\\times N},B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{K}\\times[0,1]}|F(Z_{t}^{X})-G(\\omega^{X},t)|<\\epsilon.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "6 Empirical Validation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Code to reproduce all of our experiments can be found at: ", "page_idx": 8}, {"type": "text", "text": "https://github.com/Benjamin-Walker/selective-ssms-and-linear-cdes ", "page_idx": 8}, {"type": "text", "text": "Datasets. The first task is based on a dataset from Walker et al. [2024] where the aim is to predict terms in the anti-symmetric part of the input path\u2019s signature. The dataset\u2019s objective aligns with the proofs of Theorem 4.1 and 4.2, which characterise the closure using the path\u2019s signature. We created two datasets with dimensions 2 and 3 respectively. The increment in each channel at each step is an integer-rounded sample from a standard Normal distribution. The 2D dataset\u2019s target is an area integral $\\begin{array}{r}{\\int_{0}^{1}\\int_{0}^{v}d X_{u}^{1}d X_{v}^{2}}\\end{array}$ , and the 3D dataset\u2019s target is a volume integral $\\begin{array}{r}{\\int_{0}^{1}\\int_{0}^{w}\\int_{0}^{v}d X_{u}^{1}d X_{v}^{2}d X_{w}^{3}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "The second task is the $A_{5}$ benchmark from Merrill et al. [2024]. It tests models on state-tracking, a crucial ability for tasks involving permutation composition, such as chess. The dataset comprises sequences from the group of even permutations on five elements, $A_{5}$ , where the target is the cumulative composition of all preceding permutations. Datasets vary by sequence length, ranging from 3 to 20. ", "page_idx": 8}, {"type": "text", "text": "Models. On the anti-symmetric signature task, we considered seven models: (i-ii) S4 or Mamba recurrence with linear readout, (iii-iv) two stacked S4 or Mamba recurrences with a linear mixing layer in-between and a linear readout, (v-vi) two stacked S4 or Mamba recurrences with a linear mixing layer $^+$ ReLU in-between, and a linear readout, (vii) a linear CDE with gates $\\omega_{t}^{\\mathrm{X}}=\\xi_{t}^{\\mathrm{X}}=(t,X_{t})$ and a linear readout. All state space models have trainable matrices in their recurrences, whereas the linear CDE is using fixed matrices. All models use a hidden dimension of 256, with the state space models using a state dimension of 256. The state space models are trained using gradient descent with a batch size of 32 and Adam with a learning rate of $10^{-4}$ . The output from the linear CDE\u2019s recurrence is obtained using the Tsit5 adaptive ODE solver, with an absolute and relative tolerance of $10^{-2}$ . The linear CDEs linear readout is optimised via ordinary least squares. ", "page_idx": 8}, {"type": "text", "text": "On the $A_{5}$ benchmark, we consider five models: a linear CDE, a RNN, a transformer, and S4 and Mamba recurrences. All models use an embedding layer followed by a series of blocks that combine the sequence-to-sequence model, linear mixing with a non-linear activation function, layer normalization, and a residual connection. Furthermore, all models have trainable matrices in their recurrences and are trained using batch gradient descent with a batch size of 32 and AdamW with a weight decay of 0.01. The RNN, transformer, S4, and Mamba use a hidden dimension of 1024, with the state space models using a state dimension of 64 and the transformer using 64 heads. Due to memory constraints, the linear CDE uses a hidden dimension of 256. Note that the IDS4 recurrence introduced by Merrill et al. [2024] corresponds directly to a linear CDE with dense transition matrices, hence we did not include the model as a baseline. ", "page_idx": 8}, {"type": "text", "text": "Results. Results on the anti-symmetric signature prediction task (Fig. 1) empirically demonstrate a number of the theoretical results presented in this paper. Firstly, as discussed in Sec. 4.2, recurrences which are linear in the input, such as S4, require a non-linearity inbetween the layers to perform well. Furthermore, as stated in Thm. 4.3 and Prop. 4.5, even if the recurrence is non-linear in the input, such as Mamba, the expressivity of models with diagonal matrices is improved by stacking. Additionally, the inclusion of the non-linearity inbetween the Mamba layers does not improve performance, as the recurrences themselves are expressive enough. Finally, as stated in Thm. 4.2, dense matrices can achieve strong expressivity with random initialization, no stacking, and only a trainable linear readout. ", "page_idx": 9}, {"type": "text", "text": "Fig. 2 is a plot of the results on the $A_{5}$ benchmark. The figure shows that the number of blocks S4, Mamba, and the transformer require to achieve greater than $90\\%$ validation accuracy grows with the sequence length. In fact, given our model architecture, none of these models could achieve greater than $90\\%$ validation accuracy for sequences of length twenty9. On the other hand, the RNN and Linear CDE are able to achieve greater than $90\\%$ validation accuracy for all lengths considered using only one block. These empirical results further validate Thm. 4.3 and Prop. 4.5: there exists a gap in expressivity between diagonal and dense transition matrices, and stacking is required to recover the expressivity. Furthermore, they provide empirical evidence that even for simple state-tracking problems, the number of blocks required can grow quickly with sequence length. ", "page_idx": 9}, {"type": "image", "img_path": "3SzrqwupUx/tmp/fb7b2a9f0bb3b2c6f96a06d1fe3d9a81026cac6461a2371c806565ece0c6bbee.jpg", "img_caption": ["Figure 1: Comparison of the Linear CDE, Mamba, and S5 on the anti-symmetric signature prediction tasks. For each model, we plotted the mean and range of the validation accuracy over 5 independent runs. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "3SzrqwupUx/tmp/bb2cb8e794b98769cfd08cb78e72d1b42d4b326b6ef6b670c4543a324ba825d1.jpg", "img_caption": ["Figure 2: For each sequence length, the plot shows the minimum number of blocks required to achieve at least $90\\%$ validation accuracy, with each grey band corresponding to a number of blocks. Missing points mean the model did not achieve at least $90\\%$ validation accuracy with 4 blocks or less. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper explores Linear CDEs, a model family that extends both classical and modern SSM architectures, including recent gated RNNs. Using Rough Paths theory, we have characterized their uniform closure, generalizing the results of Li et al. [2022b] for linear RNNs. We precisely identified the advantages of input-controlled transition dynamics, which allow you to capture highorder statistics of the input as opposed to just the linear ones extracted by convolutions. While dense models reach full expressiveness, imposing diagonality (e.g. Mamba) weakens the model capabilities. This is in direct contrast to S4, where dense and diagonal settings share the same closure. Our analysis lays the theoretical foundation for analyzing the expressive power of future SSM variants and hints at non-diagonality as a potential source of improvements. We believe that light and efficient channel mixing in the recurrent block might already unlock the modeling of higher-order statistics. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The current framework examines expressivity from a continuous-time perspective with real-valued inputs. The RNN literature suggests that finite precision often plays a significant role in practice, making it an interesting direction for future exploration. Additionally, although dense transition matrices are shown to be theoretically and empirically more expressive than diagonal ones, their increased computational cost makes them impractical for large-scale models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Antonio Orvieto acknowledges the financial support of the Hector Foundation. Terry Lyons was funded in part by the EPSRC [EP/S026347/1], in part by The Alan Turing Institute [EP/N510129/1], in part by the Defence and Security Programme, in part by the Office for National Statistics, and in part by the Hong Kong Innovation and Technology Commission (InnoHK Project CIMDA). Benjamin Walker was funded by the Hong Kong Innovation and Technology Commission (InnoHK Project CIMDA). The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work. http://dx.doi.org/10.5281/zenodo.22558 ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. ", "page_idx": 10}, {"type": "text", "text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Neil Houlsby, Sylvain Gelly, Xiaohua Zhang, and Jakob Uszkoreit. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. ", "page_idx": 10}, {"type": "text", "text": "Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. ", "page_idx": 10}, {"type": "text", "text": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020. ", "page_idx": 10}, {"type": "text", "text": "Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A. Baccus, and Christopher R\u00e9. S4nd: Modeling images and videos as multidimensional signals using state spaces. Advances in Neural Information Processing Systems, 2022. ", "page_idx": 10}, {"type": "text", "text": "Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It\u2019s raw! audio generation with state-space models. International Conference on Machine Learning, 2022. ", "page_idx": 10}, {"type": "text", "text": "Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 2023. ", "page_idx": 10}, {"type": "text", "text": "Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and Jo\u00e3o Sacramento. Online learning of long-range dependencies, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sepp Hochreiter and J\"urgen Schmidhuber. Long short-term memory. Neural computation, 1997. ", "page_idx": 10}, {"type": "text", "text": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. ", "page_idx": 10}, {"type": "text", "text": "Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. ", "page_idx": 10}, {"type": "text", "text": "Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023. ", "page_idx": 10}, {"type": "text", "text": "Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022a. ", "page_idx": 10}, {"type": "text", "text": "Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. ", "page_idx": 10}, {"type": "text", "text": "Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023. ", "page_idx": 11}, {"type": "text", "text": "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.   \nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.   \nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.   \nTobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023.   \nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.   \nZhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024.   \nSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.   \nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023a.   \nLeo Feng, Frederick Tung, Mohamed Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh. Were rnns all we needed?, 2024. URL https://arxiv.org/abs/2410.01201.   \nLaurence C Young. An inequality of the h\u00f6lder type, connected with stieltjes integration. 1936.   \nTerry Lyons. Differential equations driven by rough signals (i): An extension of an inequality of lc young. Mathematical Research Letters, 1(4):451\u2013464, 1994.   \nPatrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020.   \nJames Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829\u20137838. PMLR, 2021.   \nAdeline Fermanian, Pierre Marion, Jean-Philippe Vert, and G\u00e9rard Biau. Framing rnn as a kernel method: A neural ode approach. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 3121\u20133134. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/p aper_files/paper/2021/file/18a9042b3fc5b02fe3d57fea87d6992f-Paper.pdf.   \nCristopher Salvi, Maud Lemercier, and Andris Gerasimovics. Neural stochastic pdes: Resolutioninvariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35:1333\u20131344, 2022.   \nMelker Hoglund, Emilio Ferrucci, Camilo Hernandez, Aitor Muguruza Gonzalez, Cristopher Salvi, Leandro Sanchez-Betancourt, and Yufei Zhang. A neural rde approach for continuous-time non-markovian stochastic control problems. arXiv preprint arXiv:2306.14258, 2023.   \nBenjamin Walker, Andrew D. McLeod, Tiexin Qin, Yichuan Cheng, Haoliang Li, and Terry Lyons. Log neural controlled differential equations: The lie brackets make a difference. International Conference on Machine Learning, 2024.   \nTerry J Lyons, Michael Caruana, and Thierry L\u00e9vy. Differential equations driven by rough paths. Springer, 2007.   \nZhong Li, Jiequn Han, E Weinan, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. J. Mach. Learn. Res., 23:42\u20131, 2022b.   \nAntonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023b.   \nShida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. arXiv preprint arXiv:2309.13414, 2023.   \nAlbert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022.   \nNicolas Zucchet and Antonio Orvieto. Recurrent neural networks: vanishing and exploding gradients are not the end of the story. arXiv preprint arXiv:2405.21064, 2024.   \nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: 1474\u20131487, 2020.   \nHava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. In Proceedings of the ffith annual workshop on Computational learning theory, pages 440\u2013449, 1992.   \nSamuel A Korsky and Robert C Berwick. On the computational power of rnns. arXiv preprint arXiv:1906.06349, 2019.   \nJoshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent neural nets. In Learning for Dynamics and Control, pages 384\u2013392. PMLR, 2020.   \nAndrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930\u2013945, 1993.   \nCorentin Tallec and Yann Ollivier. Can recurrent neural networks warp time?, 2018.   \nJoshua Hanson and Maxim Raginsky. Universal approximation of input-output maps by temporal convolutional nets. Advances in Neural Information Processing Systems, 32, 2019.   \nStephen Boyd and Leon Chua. Fading memory and the problem of approximating nonlinear operators with volterra series. IEEE Transactions on circuits and systems, 32(11):1150\u20131161, 1985.   \nSamy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.   \nWilliam Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024.   \nBen Hambly and Terry Lyons. Uniqueness for the signature of a path of bounded variation and the reduced path group. Annals of Mathematics, pages 109\u2013167, 2010.   \nYann A. LeCun, L\u00e9on Bottou, Genevieve B. Orr, and Klaus-Robert M\u00fcller. Efficient BackProp, pages 9\u201348. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3. URL https://doi.org/10.1007/978-3-642-35289-8_3.   \nMantas Lukoveviius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. Comput. Sci. Rev., 3:127\u2013149, 2009. URL https://api.semanticscholar.org/Co rpusID:554006.   \nChrista Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega, and Josef Teichmann. Discrete-time signatures and randomness in reservoir computing. IEEE Transactions on Neural Networks and Learning Systems, Forthcoming, 04 2021a. doi: 10.1109/TNNLS.2021.3076777.   \nEnea Monzio Compagnoni, Anna Scampicchio, Luca Biggio, Antonio Orvieto, Thomas Hofmann, and Josef Teichmann. On the effectiveness of randomized signatures as reservoir for learning rough dynamics. In 2023 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2023.   \nPatrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, and Terry Lyons. Deep signature transforms. Advances in Neural Information Processing Systems, 32, 2019.   \nAdeline Fermanian, Terry Lyons, James Morrill, and Cristopher Salvi. New directions in the applications of rough path theory. IEEE BITS the Information Theory Magazine, 2023.   \nThomas Cass and Cristopher Salvi. Lecture notes on rough paths and applications to machine learning. arXiv preprint arXiv:2404.06583, 2024.   \nPeter K. Friz and Nicolas B. Victoir. Multidimensional Stochastic Processes as Rough Paths: Theory and Applications. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2010. doi: 10.1017/CBO9780511845079.   \nAdeline Fermanian. Embedding and learning with signatures, 2020.   \nKuo-Tsai Chen. Integration of paths\u2013a faithful representation of paths by noncommutative formal power series. Transactions of the American Mathematical Society, 89(2):395\u2013407, 1958. ISSN 00029947. URL http://www.jstor.org/stable/1993193.   \nCristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. The signature kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873\u2013899, 2021a. doi: 10.1137/20M1366794. URL https://doi.org/10.1137/20M1366794.   \nA. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer US, 2011. ISBN 9781441990969. URL https://books.google.co.uk/books?id= bX3TBwAAQBAJ.   \nMaud Lemercier, Cristopher Salvi, Theodoros Damoulas, Edwin V. Bonilla, and Terry Lyons. Distribution regression for sequential data, 2021.   \nCristopher Salvi, Maud Lemercier, Chong Liu, Blanka Horvath, Theodoros Damoulas, and Terry Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes. Advances in Neural Information Processing Systems, 34:16635\u201316647, 2021b.   \nThomas Cochrane, Peter Foster, Varun Chhabra, Maud Lemercier, Terry Lyons, and Cristopher Salvi. Sk-tree: a systematic malware detection algorithm on streaming trees via the signature kernel. In 2021 IEEE international conference on cyber security and resilience (CSR), pages 35\u201340. IEEE, 2021.   \nCristopher Salvi, Maud Lemercier, Thomas Cass, Edwin V Bonilla, Theodoros Damoulas, and Terry J Lyons. Siggpde: Scaling sparse gaussian processes on sequential data. In International Conference on Machine Learning, pages 6233\u20136242. PMLR, 2021c.   \nNicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as infinitewidth-depth-limits of controlled resnets, 2023.   \nZacharia Issa, Blanka Horvath, Maud Lemercier, and Cristopher Salvi. Non-adversarial training of neural sdes with signature kernel scores. Advances in Neural Information Processing Systems, 2023.   \nAlexandre Pannier and Cristopher Salvi. A path-dependent pde solver based on signature kernels. arXiv preprint arXiv:2403.11738, 2024.   \nGeorg Manten, Cecilia Casolo, Emilio Ferrucci, S\u00f8ren Wengel Mogensen, Cristopher Salvi, and Niki Kilbertus. Signature kernel conditional independence tests in causal discovery for stochastic processes. arXiv preprint arXiv:2402.18477, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Patrick Kidger. On neural differential equations, 2022. ", "page_idx": 13}, {"type": "text", "text": "Guillaume Dubach and Yuval Peled. On words of non-Hermitian random matrices. The Annals of Probability, 49(4):1886 \u2013 1916, 2021. doi: 10.1214/20-AOP1496. URL https://doi.org/10 .1214/20-AOP1496.   \nSanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22, 2003. URL https://api.semanticscholar.org/Corp usID:10327785.   \nChrista Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega, and Josef Teichmann. Expressive power of randomized signature. In The Symbiosis of Deep Learning and Differential Equations, 2021b. ", "page_idx": 14}, {"type": "text", "text": "A Introduction to Signatures ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This initial section of the Appendix is devoted to a brief introduction to the topic of Signature Transform. For a more in-depth account we refer the interested reader to Cass and Salvi [2024]. ", "page_idx": 15}, {"type": "text", "text": "A.1 Intuition - Controlled Differential Equations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the simplest setting of smooth paths a CDE is a differential equation of form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d Z_{t}}{d t}=F\\Big(\\frac{d X_{t}}{d t},Z_{t}\\Big),\\quad Z_{0}\\in\\mathbb{R}^{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $X:[0,1]\\rightarrow\\mathbb{R}^{d}$ is a known smooth path to which we refer as control, $Z_{0}$ the known initial condition and $\\bar{Z}:[0,1]\\to\\mathbb{R}^{n}$ the unknown solution. ", "page_idx": 15}, {"type": "text", "text": "The natural generalization is the following: assume to have two spaces $\\mathbb{R}^{d_{x}}$ and $\\mathbb{R}^{d_{z}}$ , $X\\ \\in$ $C_{1}([0,1];\\mathbb{R}^{d_{x}})$ , $Z\\ \\in\\ C_{1}([0,1];\\mathbb{R}^{d_{z}})$ , $F\\,:\\,\\overline{{\\mathbb{R}^{d_{z}}}}\\ \\to\\ \\mathcal{L}(\\mathbb{R}^{d_{x}},\\mathbb{R}^{d_{z}})$ and $Z_{0}~\\in~\\mathbb{R}^{d_{z}}$ . We say that $(Z,X,F,Z_{0})$ satisfy the CDE ", "page_idx": 15}, {"type": "equation", "text": "$$\nd Z_{t}=F(Z_{t})d X_{t},\\quad Z_{0}\\in\\mathbb{R}^{d_{z}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whenever ", "page_idx": 15}, {"type": "equation", "text": "$$\nZ_{t}=Z_{0}+\\int_{0}^{t}F(Z_{s})d X_{s}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The theory of Rough Paths has its origins in the study of such types of differential equations and provides a theoretical framework to define and work in rough settings i.e. when $X$ is not kust BV but even $\\alpha$ -H\u00f6lder for $\\alpha\\in(0,1)$ cf. Friz and Victoir [2010]. ", "page_idx": 15}, {"type": "text", "text": "Assume thus to have the CDE ", "page_idx": 15}, {"type": "equation", "text": "$$\nd Z_{t}=\\sum_{i=1}^{d_{x}}V_{i}(Z_{t})d X_{t}^{i},\\quad Z_{0}\\in\\mathbb{R}^{d_{z}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for sufficiently regular vector fields $V_{i}$ and $X\\in C_{1}([0,1];\\mathbb{R}^{d_{x}})$ . Given a smooth $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , by the change of variable formula (i.e. fundamental theorem of calculus) we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(Z_{t})=f(Z_{0})+\\sum_{i=1}^{d_{x}}\\int_{0}^{t}V_{i}f(Z_{s})d X_{s}^{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V_{i}f(z):=d f_{y}[V_{i}(z)]$ . Iterating this procedure on the $V_{i}f$ s, i.e. substituting in the previous equation the analogously obtained equality ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{i}f(Z_{s})=V_{i}f(Z_{0})+\\sum_{j=1}^{d_{x}}\\int_{0}^{s}V_{j}(V_{i}f)(Z_{u})d X_{u}^{j},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(Z_{t})=f(Z_{0})+\\sum_{i=1}^{d}V_{i}f(Z_{0})\\int_{0}^{t}d X_{s}^{i}+\\sum_{i,j=1}^{d}\\int_{0}^{t}\\int_{0}^{s}V_{j}V_{i}f(Z_{u})d X_{u}^{j}d X_{s}^{i}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Keeping with this procedure for $N$ steps we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(Z_{t})=f(Z_{0})+\\sum_{k=1}^{N}\\sum_{|I|=k}V_{I}f(Z_{0})\\int.\\cdot\\cdot\\int_{-u_{k}<t}d X_{u_{1}}^{i_{1}}\\cdot\\cdot\\cdot d X_{u_{k}}^{i_{k}}+R_{N}(t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $I=(i_{1},\\ldots,i_{k})$ runs through the multi-indices, $V_{I}f:=V_{i_{1}}V_{i_{2}}\\ldots V_{i_{k}}f$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{N}(t):=\\sum_{|J|=k+1}\\int\\d x\\cdot\\cdot\\int\\d t{\\,\\psi{\\,f(Z_{u_{1}})d X_{u_{1}}^{j_{1}}\\cdot\\cdot\\cdot d X_{u_{k+1}}^{j_{k+1}}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As one can imagine, under reasonable regularity assumptions, the remainder goes to 0 as $N\\rightarrow\\infty$ and at the limit ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(Z_{t})=f(Z_{0})+\\sum_{k=1}^{\\infty}\\sum_{|I|=k}V_{I}f(Z_{0})\\bigcup_{s<u_{1}<\\cdots<u_{k}<t}d X_{u_{1}}^{i_{1}}\\cdot\\cdot\\cdot d X_{u_{k}}^{i_{k}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is a remarkable result: to know the solution $Z_{t}$ to the original CDE it suffices to know the quantities $V_{I}f$ for all multi-indices and $f$ in the coordinate maps, together with the iterated integrals ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(X)_{s,t}^{I}:=\\int_{\\stackrel{s<u_{1}<\\cdots<u_{k}<t}{s<u_{1}<\\cdots<u_{k}<t}}d X_{u_{1}}^{i_{1}}\\cdot\\cdot\\cdot d X_{u_{k}}^{i_{k}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This observation is at the core of Rough Path Analysis, the theory can in a sense be considered an extreme development of it. The collection of iterated integrals, the Signature, will be the main for our analysis. ", "page_idx": 16}, {"type": "text", "text": "In Appendix $\\boldsymbol{\\mathrm E}$ we expand and make rigorous the arguments of this section in the case of affine vector fields. ", "page_idx": 16}, {"type": "text", "text": "A.2 Basic Definitions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Denote by $(\\mathbb{R}^{d})^{\\otimes n}:=\\mathbb{R}^{d}\\otimes\\cdots\\otimes\\mathbb{R}^{d}$ the tensor product of $n$ copies $\\mathbb{R}^{d}$ , set $(\\mathbb{R}^{d})^{\\otimes0}:=\\mathbb{R}$ . Let $T(\\mathbb{R}^{d}):=\\bigoplus_{k=0}^{\\infty}(\\mathbb{R}^{d})^{\\otimes k}$ be the tensor algebra equipped with sum and tensor product. ", "page_idx": 16}, {"type": "text", "text": "Definition A.1. Let $\\{e_{1},\\ldots,e_{d}\\}$ be the canonical basis of $\\mathbb{R}^{d}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\{e_{i_{1}}\\otimes\\cdot\\cdot\\cdot\\otimes e_{i_{k}}:(i_{1},\\ldots,i_{k})\\in[d]^{k}\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is a basis of $(\\mathbb{R}^{d})^{\\otimes k}$ . We equip $(\\mathbb{R}^{d})^{\\otimes k}$ with the inner product $\\langle\\cdot,\\cdot\\rangle_{(\\mathbb{R}^{d})\\otimes k}$ defined on basis elements as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle e_{i_{1}}\\otimes\\cdot\\cdot\\cdot\\otimes e_{i_{k}},e_{j_{1}}\\otimes\\cdot\\cdot\\cdot\\otimes e_{j_{k}}\\rangle_{(\\mathbb{R}^{d})^{\\otimes k}}=\\delta_{(i_{1},\\dots,i_{k})}^{(j_{1},\\dots,j_{k})}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We extend this product to $T(\\mathbb{R}^{d})$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle A,B\\rangle_{T(\\mathbb{R}^{d})}:=\\sum_{k=0}^{\\infty}\\langle a_{k},b_{k}\\rangle_{(\\mathbb{R}^{d})^{\\otimes k}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $A=(a_{0},a_{1},\\dots)$ and $\\boldsymbol{B}=(b_{0},b_{1},\\dots)$ ", "page_idx": 16}, {"type": "text", "text": "Note how for any $A,B\\in T(\\mathbb{R}^{d})$ we have $\\langle A\\otimes e_{i},B\\otimes e_{j}\\rangle_{T(\\mathbb{R}^{d})}=\\langle A,B\\rangle_{T(\\mathbb{R}^{d})}\\langle e_{i},e_{j}\\rangle_{\\mathbb{R}^{d}}$ , we refer to this as to the coproduct property. ", "page_idx": 16}, {"type": "text", "text": "Definition A.2 (Infinite Tensor Algebra). The infinite tensor algebra is defined as the space $\\begin{array}{r}{T((\\mathbb{R}^{d}))\\,:=\\,\\prod_{k=0}^{\\infty}(\\mathbb{R}^{d})^{\\otimes k}}\\end{array}$ equipped with the operations $^+$ and $\\otimes$ which act in the natural algebraic way; its elements are called tensor series. ", "page_idx": 16}, {"type": "text", "text": "It is easily seen that $(T((\\mathbb{R}^{d})),+,\\otimes)$ is an algebra with unit $\\mathbf{1}=(1,0,0,\\cdots)$ and we can endow it with a natural product which inherits the coproduct property. ", "page_idx": 16}, {"type": "text", "text": "Another point of view could be taken on the definitions of these spaces, one that we will prefer later on. If we define $\\mathbb{W}_{d}$ to be the set of words in $d$ letters then $T((\\mathbb{R}^{\\hat{d}}))\\sim\\mathbb{R}^{\\mathbb{W}_{d}}$ , $T(\\mathbb{R}^{d})$ is the subset of such functions with finite support and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle A,B\\rangle_{T(\\mathbb{R}^{d})}=\\sum_{I\\in\\mathbb{W}_{d}}A_{I}B_{I}=\\sum_{k=0}^{\\infty}\\sum_{|I|=k}A_{I}B_{I}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $|I|$ is the length of the word $I$ . The empty word, the only one with length 0, is denoted by () and corresponds to the basis element of $(\\mathbb{R}^{d})^{\\otimes0}$ . In this view the tensor product coincides with concatenation of words accordingly distributed and the closure of $T(\\mathbb{R}^{d})$ with respect to its product is just the $l^{2}$ space $l^{2}(\\mathbb{W}_{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Definition A.3 (Signature). Given $\\gamma\\,\\in\\,C_{1,0}([0,1];\\mathbb{R}^{d})$ and $s,t\\in[0,1]$ s.t. $s\\leq t$ , the signature $S i g(\\gamma)_{s,t}\\in T((\\mathbb{R}^{d}))$ of the path $\\gamma$ over $[s,t]$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\gamma)_{s,t}:=(1,\\int_{s<u_{1}<t}d\\gamma_{u_{1}},\\ \\cdots,\\int_{s<u_{1}<\\cdots<u_{k}<t}d\\gamma_{u_{1}}\\otimes\\cdots\\otimes d\\gamma_{u_{k}},\\cdot\\cdot\\cdot)^{10}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Equivalently $\\mathrm{Sig}(\\gamma)_{s,t}$ is that element of $l^{2}(\\mathbb{W}_{d})$ defined recursively on words as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\gamma)_{s,t}^{()}=1,\\quad\\mathrm{Sig}(\\gamma)_{s,t}^{I j}=\\int_{s}^{t}\\mathrm{Sig}(\\gamma)_{s,r}^{I}d\\gamma_{r}^{j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 Notable Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we present some notable results of which we will make use through the paper. We omit the proofs if they can be easily found in the suggested references. ", "page_idx": 17}, {"type": "text", "text": "The first result is about bounding the norm of Signature entries: ", "page_idx": 17}, {"type": "text", "text": "Proposition A.4 (Factorial Decay Rate). Given $\\gamma\\in C_{1,0}([0,1];\\mathbb{R}^{d})$ , for all $k\\geq1$ and $s,t\\in[0,1]$ s.t. $s\\leq t$ one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{s<u_{1}<\\cdots<u_{k}<t}d\\gamma_{u_{1}}\\otimes\\cdots\\otimes d\\gamma_{u_{k}}\\right\\|_{(\\mathbb{R}^{d})^{\\otimes k}}\\leq\\frac{\\|\\gamma\\|_{1-v a r,[s,t]}^{k}}{k!}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The most important fact about Signature is that it acts as the basis for a Taylor expansion in path space. In fact just as finite linear combinations of monomials are dense in the continuous functions with a compact input set, finite linear combinations of Signature entries are dense in continuous functions from compact path-spaces: ", "page_idx": 17}, {"type": "text", "text": "Theorem A.5 (Universal Approximation Fermanian [2020]). Fix $\\cdot K\\subset C_{1,0}([0,1];\\mathbb{R}^{d+1})$ compact such that for any $\\gamma\\,\\in\\,K$ it holds $\\gamma_{t}^{1}=t$ . For any $F\\,\\in\\,C^{0}(K;\\mathbb{R})$ and $\\epsilon>0$ there is an integer $N\\geq0$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\gamma\\in K}|F(\\gamma)-\\sum_{|I|\\leq N}\\alpha_{I}S i g(\\gamma)_{0,1}^{I}|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some finite sequence $(\\alpha_{I})_{|I|\\leq N}$ of real numbers. ", "page_idx": 17}, {"type": "text", "text": "Remark A.6. There is no magic in this result, it is just an application of Stone-Weiestrass enabled by the rich algebraic structure of iterated integrals, studied originally in Chen [1958]. ", "page_idx": 17}, {"type": "text", "text": "We will need to restrict some paths to sub-intervals of $[0,1]$ in such a way to still be able to consider them meaningfully as elements of $C_{1,0}([0,1];\\mathbb{R}^{d})$ , this is done in the following way: ", "page_idx": 17}, {"type": "text", "text": "Definition A.7. Given any path $\\gamma\\,\\in\\,C_{1,0}([0,1];\\mathbb{R}^{d})$ we define its restriction on a sub-interval $[s,t]\\subseteq[0,1]$ as the path $\\gamma_{[s,t]}\\in C_{1,0}([0,1];\\mathbb{R}^{d})$ with values ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma_{[s,t]}(r):=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{~if~}r<s\\smallskip}\\\\ {\\gamma_{r}-\\gamma_{s}}&{\\mathrm{~if~}s\\leq r\\leq t}\\\\ {\\gamma_{t}-\\gamma_{s}}&{\\mathrm{~if~}r>t}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This definition is such that the following important equation holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\gamma)_{s,t}=\\mathrm{Sig}(\\gamma_{[s,t]})_{0,1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With the right augmentation of the paths one can see that the Signature distinguishes between different sections of paths, this will be crucial for some of the original results presented in this work. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.8. Assume $\\omega,\\gamma\\in C_{1,0}([0,1];\\mathbb{R}^{d+2})$ with $\\omega_{t}^{1}=\\gamma_{t}^{1}\\equiv t$ and $\\omega_{t}^{2}=\\gamma_{t}^{2}\\equiv t^{2}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\nS i g(\\omega)_{s,t}=S i g(\\gamma)_{s^{\\prime},t^{\\prime}}\\iff\\omega_{[s,t]}=\\gamma_{[s^{\\prime},t^{\\prime}]}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The $i f$ part is follows from ${\\mathrm{Sig}}(\\gamma)_{s,t}={\\mathrm{Sig}}(\\gamma_{[s,t]})_{0,1}$ . For the only $i f$ part, If $s=s^{\\prime}$ and $t=t^{\\prime}$ the statement holds; this is because if the signatures over the time interval $[s,t]$ of two time-augmented paths are equal, then the two paths must be equal on $[s,t]$ . We now show that augmenting the path with $t^{2}$ and imposing equality of signatures, implies $s=s^{\\prime}$ and $t=t^{\\prime}$ , which will in turn allow us to conclude the proof by the previous remark. Assume $\\mathrm{Sig}(\\omega)_{s,t}=\\mathrm{Sig}(\\gamma)_{s^{\\prime},t^{\\prime}}$ , in particular we must have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\int_{s}^{t}d(r^{2})=t^{2}-s^{2}=(t^{\\prime})^{2}-(s^{\\prime})^{2}=\\int_{s^{\\prime}}^{t^{\\prime}}d(r^{2})}}\\\\ {{\\displaystyle\\int_{s}^{t}d(r)=t-s=t^{\\prime}-s^{\\prime}=\\int_{s^{\\prime}}^{t^{\\prime}}d(r)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which reduces to the system ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{t^{2}-s^{2}=(t^{\\prime})^{2}-(s^{\\prime})^{2}\\right.}&{{}\\left.\\left\\{t+s=t^{\\prime}+s^{\\prime}\\right.\\right.}\\\\ {\\left.t-s=t^{\\prime}-s^{\\prime}\\right.}&{{}\\left.\\left\\{t-s=t^{\\prime}-s^{\\prime}\\right.\\right.}\\end{array}\\right.\\left\\{2t=2t^{\\prime}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence it must be true that $t=t^{\\prime}$ and $s=s^{\\prime}$ . ", "page_idx": 18}, {"type": "text", "text": "B Expressivity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Model Recap ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the body of the paper we have presented the main results with the simplified assumption of $Z_{0}^{\\mathrm{X}}=0$ or at best $\\dot{Z}_{0}^{\\mathrm{X}}=Z_{0}^{\\mathrm{~\\!~\\!~}}$ i.e. with an initial value independent from the input. In this appendix we will carry on the proofs in a more general setting in which $Z_{\\mathrm{0}}^{\\mathrm{X}}$ is allowed to be input-dependent, as previously discussed the choice of initial value is, in contrast to the classical setting, meaningful inasmuch it allows to approximate linear maps on the signature of $\\omega_{[0,1]}^{\\mathrm{X}}$ . In order to do so we have to introduce a new gate, the initial value gate, in the form of a map ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{(\\cdot)_{0}:\\mathbb{X}\\to\\mathbb{R}^{d_{0}}}\\\\ {X\\mapsto X_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Despite the notation, there is no reason why $(X)_{0}$ should be the initial value of the path $X$ , one should think of this map as the one summarizing the data which still matters for the task but which does not have a time-series nature. ", "page_idx": 19}, {"type": "text", "text": "To recapitulate, the general setting of our models is the following: a topological input space space $\\mathbb{X}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\cdot)_{0}:\\mathbb{X}\\to\\mathbb{R}^{d_{0}},}\\\\ &{\\omega:\\mathbb{X}\\to C_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}}),}\\\\ &{\\xi:\\mathbb{X}\\to C_{1,0}([0,1];\\mathbb{R}^{d_{\\xi}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{((\\cdot)_{0}\\mathrm{-gate})}\\\\ {(\\omega\\mathrm{-gate})}\\\\ {(\\xi\\mathrm{-gate})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where all the gates are continuous functions on $\\mathbb{X}$ . The space $\\mathbb{X}$ does not have to be a space of paths, a topological structure suffices, as long as the gates $(\\cdot)_{0},\\omega,\\xi$ are well defined and continuous. Remark B.1. Typical examples for the choice of gates are $\\mathbb{X}$ space of paths and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{(X)_{0}=0}}&{{\\omega_{t}^{X}=t}}&{{\\xi_{t}^{X}=\\displaystyle\\int_{0}^{t}X_{s}d s}}\\\\ {{(X)_{0}=0}}&{{\\omega_{t}^{X}=\\displaystyle\\int_{0}^{t}s o f t p l u s(\\alpha X_{s}+\\beta)d s}}&{{\\xi_{t}^{X}=\\displaystyle\\int_{0}^{t}s o f t p l u s(\\alpha X_{s}+\\beta)X_{s}d s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then the main object of study, \"gated\" Linear CDEs, are defined as: ", "page_idx": 19}, {"type": "text", "text": "Definition B.2. Fix gates $(\\cdot)_{0},\\omega,\\xi$ as above, $N\\;\\in\\;\\mathbb{N}$ , matrices $\\{A_{i}\\}_{i=1,\\ldots,d_{\\omega}}$ $(A_{i}\\ \\in\\ \\mathbb{R}^{N\\times N})$ , $B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ , $C\\in\\mathbb{R}^{N\\times d_{0}}$ . The corresponding Linear CDE is the functional ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ:\\mathbb{X}\\to C_{1}([0,1];\\mathbb{R}^{N})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{0}^{\\mathrm{x}}=C X_{0},\\quad Z_{t}^{\\mathrm{x}}=\\sum_{i=1}^{d_{\\omega}}A_{i}Z_{t}^{\\mathrm{x}}d\\omega_{t}^{\\mathrm{x},i}+B d\\xi_{t}^{\\mathrm{x}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Main Result - Statement and Strategy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we present the unified expressivity result in its most general form: ", "page_idx": 19}, {"type": "text", "text": "Theorem B.3. For any compact set $\\mathbb{K}\\subseteq\\mathbb{X}$ and continuous gates $(\\cdot)_{0},\\omega,\\xi$ with $\\omega_{t}^{x,1}\\,\\equiv\\,t$ and $\\omega_{t}^{X,2}\\equiv t^{2}$ . For any $\\epsilon>0$ and any ", "page_idx": 19}, {"type": "equation", "text": "$$\nF\\in\\left\\{(X,t)\\mapsto\\Psi(\\omega_{[0,t]}^{X})\\cdot X_{0}+\\int_{0}^{t}\\Phi(\\omega_{[s,t]}^{X})\\cdot d\\xi_{s}^{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Psi\\in C^{0}(C_{1,0};\\mathbb{R}^{d_{0}})$ and $\\Phi\\in C^{0}(C_{1,0};\\mathbb{R}^{d_{\\xi}}),$ , there exist a choice of hidden dimension $N\\geq1$ and parameters $v\\in\\mathbb{R}^{N},A_{i}\\in\\mathbb{R}^{N\\times N},B\\in\\mathbb{R}^{N\\times d_{\\xi}},C\\in\\mathbb{R}^{N\\times d_{0}}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{K}\\times[0,1]}|F(X,t)-\\langle v,Z_{t}^{X}\\rangle|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover generic parameters suffice with high probability in the sense that under LeCun initialization ", "page_idx": 19}, {"type": "equation", "text": "$$\n[A_{i}]_{n,j}\\overset{i i d}{\\sim}\\mathcal{N}(0,\\frac{1}{N})\\quad C_{n,j},B_{n,j}\\overset{i i d}{\\sim}\\mathcal{N}(0,1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the following holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\rightarrow\\infty}\\mathbb{P}\\big[\\exists v\\in\\mathbb{R}^{N}:\\,(27)\\,h o l d s\\big]=1\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If the $A_{i}s$ are constrained to be diagonal, as often is the case in practice, the requirements $\\omega_{t}^{x,1}\\equiv t$ , $\\omega_{t}^{X,2}\\equiv t^{2}$ can be dropped and the existence result only holds with ", "page_idx": 20}, {"type": "equation", "text": "$$\nF\\in\\left\\{(X,t)\\mapsto\\psi(\\omega_{t}^{X})\\cdot X_{0}+\\int_{0}^{t}\\phi(\\omega_{t}^{X}-\\omega_{s}^{X})\\cdot d\\xi_{s}^{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $\\psi\\in C^{0}(\\mathbb{R}^{d_{\\omega}};\\mathbb{R}^{d_{0}})$ and $\\phi\\in C^{0}(\\mathbb{R}^{d_{\\omega}};\\mathbb{R}^{d_{\\xi}})$ . ", "page_idx": 20}, {"type": "text", "text": "Moreover in both the dense and diagonal cases the \"reverse\" also holds in the sense that, given any choice of matrices $A_{i},B,C$ there is an \u03f5-close map $F$ in the corresponding family. ", "page_idx": 20}, {"type": "text", "text": "As one can see the theorem is composed of different sub-results, which we believe are better understood separately from each other. The proof will thus be split in the following steps: ", "page_idx": 20}, {"type": "text", "text": "1. Using the theory developed in Appendix $\\boldsymbol{\\mathrm E}$ we see how linear functions on the $Z_{t}\\mathbf{s}$ can be seen as linear functions on certain terms of the Signature Transform.   \n2. Such terms define a feature map $T(X)_{0,t}$ which generates a Reproducing Kernel Hilbert Space $\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}$ . This abstract space acts as an upper bound on expressivity: linear functions on $Z_{t}$ always belong to its closure (in uniform norm), independently of dimension and weights chosen, hence they cannot reach what functions in t(\u00b7)0,\u03c9,\u03b7can\u2019t approximate.   \n3. The full expressive range of $\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}$ is shown to be captured by generic $Z_{t}\\mathbf{s}$ .   \n4. Diagonal systems are shown to be restricted to a subset of the $\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}$ of which they capture the full expressive range. ", "page_idx": 20}, {"type": "text", "text": "B.3 Main Result - Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.3.1 An expansion for $Z_{t}^{\\mathbf{x}}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition B.4. For any choice of $A_{i}\\in\\mathbb{R}^{N\\times N},B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ and $C\\in\\mathbb{R}^{N\\times d_{0}}$ , the unique solution to ", "page_idx": 20}, {"type": "equation", "text": "$$\nd Z_{t}^{X}=\\sum_{i=1}^{d_{\\omega}}A_{i}Z_{t}^{X}d\\omega_{t}^{X,i}+B d\\xi_{t}^{X}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nZ_{0}^{X}=C X_{0}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is given, using the notation $A_{I j}:=A_{j}A_{I}$ , by ", "page_idx": 20}, {"type": "equation", "text": "$$\nZ_{t}^{x}=\\sum_{i=1}^{d_{0}}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}C_{i}\\;X_{0}^{i}S i g(\\omega^{X})_{0,t}^{I}+\\sum_{j=1}^{d_{\\xi}}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}B_{j}\\int_{0}^{t}S i g(\\omega^{X})_{s,t}^{I}d\\xi_{s}^{X,j}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice here $A_{I}C_{i},A_{I}B_{j}\\in\\mathbb{R}^{N}$ and $\\begin{array}{r}{X_{0}^{i}S i g(\\omega^{X})_{0,t}^{I},\\int_{0}^{t}S i g(\\omega^{X})_{s,t}^{I}d\\xi_{s}^{X,j}\\in\\mathbb{R}.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Just apply Theorems (E.2) and (E.6) of Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 20}, {"type": "text", "text": "Remark B.5. A property highlighted by the previous result is the interpretability of these models. After training the CDEs one can compute the matrix multiplications and observe which entries of the signature the model chooses to take into consideration, to attend. ", "page_idx": 20}, {"type": "text", "text": "B.3.2 The feature map $T$ and its RKHS ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The expression in (30) is a linear map on a feature vector given, at time $t$ , by ", "page_idx": 20}, {"type": "equation", "text": "$$\nT(X)_{0,t}:=\\left(X_{0}^{i}\\mathbf{S}\\mathbf{ig}(\\omega^{\\mathrm{x}})_{0,t}^{I},\\int_{0}^{t}\\mathbf{S}\\mathbf{ig}(\\omega^{\\mathrm{x}})_{s,t}^{I}d\\xi_{s}^{\\mathrm{x},j}:i\\in[d_{0}],I\\in\\mathbb{W}_{d_{\\omega}},j\\in[d_{\\xi}]\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This feature vector can be understood as a tensor in the following way: ", "page_idx": 20}, {"type": "text", "text": "Definition B.6. Let $\\mathbb{W}_{d_{0},d_{\\omega},d_{\\xi}}$ be the set of words in the alphabet ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{d_{0},d_{\\omega},d_{\\xi}}:=\\{e_{i}\\}_{i=1,\\dots,d_{0}}\\cup\\{\\epsilon_{j}^{\\xi}\\}_{j=1,\\dots,d_{\\xi}}\\cup\\{\\epsilon_{k}^{\\omega}\\}_{k=1,\\dots,d_{\\omega}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Fixed the gates $(\\cdot)_{0},\\omega,\\xi$ we define $T(X):[0,1]\\to l^{2}(\\mathbb{W}_{d_{0},d_{\\omega},d_{\\xi}})\\subseteq T((A_{d_{0},d_{\\omega},d_{\\xi}}))$ as the unique solution to: ", "page_idx": 21}, {"type": "equation", "text": "$$\nT(X)_{0,t}=\\sum_{i=1}^{d}X_{0}^{i}e_{i}+\\sum_{j=1}^{d_{\\xi}}\\xi_{t}^{\\tt X,j}\\epsilon_{j}^{\\xi}+\\sum_{k=1}^{d_{\\omega}}\\int_{0}^{t}T(X)_{0,s}\\;d\\omega_{s}^{\\tt X,k}\\otimes\\epsilon_{k}^{\\omega}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In fact one readily sees that the only non-zero terms of $T(X)_{0,t}$ defined as above are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle T(X)_{0,t},e_{i}\\rangle=X_{0}^{i}}\\\\ &{\\langle T(X)_{0,t},e_{i}\\otimes\\epsilon_{I k}^{\\omega}\\rangle=\\displaystyle\\int_{0}^{t}\\langle T(X)_{0,s},e_{i}\\otimes\\epsilon_{I}^{\\omega}\\rangle d\\omega_{s}^{X,k}=X_{0}^{i}\\operatorname{Sig}(\\omega^{X})_{0,t}^{I k}}\\\\ &{\\langle T(X)_{0,t},\\epsilon_{j}^{\\xi}\\rangle=\\xi_{t}^{X,j}=\\displaystyle\\int_{0}^{t}d\\xi_{s}^{X,j}}\\\\ &{\\langle T(X)_{0,t},\\epsilon_{j}^{\\xi}\\otimes\\epsilon_{I k}^{\\omega}\\rangle=\\displaystyle\\int_{0}^{t}\\langle T(X)_{0,s},\\epsilon_{j}^{\\xi}\\otimes\\epsilon_{I}^{\\omega}\\rangle d\\omega_{s}^{X,k}=\\displaystyle\\int_{s=0}^{t}\\int_{r=0}^{s}\\operatorname{Sig}(\\omega^{X})_{r,s}^{I}d\\xi_{r}^{X,j}d\\omega_{s}^{X,k}}\\\\ &{\\qquad=\\displaystyle\\int_{r=0}^{t}\\int_{s=r}^{t}\\operatorname{Sig}(\\omega^{X})_{r,s}^{I}d\\omega_{s}^{X,k}d\\xi_{r}^{X,j}=\\displaystyle\\int_{0}^{t}\\operatorname{Sig}(\\omega^{X})_{r,t}^{I}d\\xi_{r}^{X,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is similar to the tensor-valued CDE defining the signature as a tensor i.e. Salvi et al. [2021a] ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\omega)_{0,t}=()+\\int_{0}^{t}\\mathrm{Sig}(\\omega)_{0,s}\\otimes d\\omega_{s}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with the addition of two terms to track $X_{0}$ and $\\xi^{\\mathrm{X}}$ . One could also understand $T(X)_{s,t}$ as a sub-tensor of ", "page_idx": 21}, {"type": "equation", "text": "$$\nX_{0}\\otimes\\mathrm{Sig}((\\omega^{\\mathrm{X}},\\xi^{\\mathrm{X}}))_{s,t}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "but in doing this one would have to explicitly ignore most of the terms of this vector; the CDE (32) does exactly this, but implicitly. In any case the subtensor view shows that $T:\\mathbb{X}\\!\\times\\![0,1]^{2}\\to l^{2}(\\mathbb{W}_{d_{0},d_{\\omega},d_{\\xi}})$ is well defined and continuous. ", "page_idx": 21}, {"type": "text", "text": "To the feature map $T(\\cdot)_{0,t}$ with values in the Hilbert space $l^{2}(\\mathbb{W}_{d_{0},d_{\\omega},d_{\\xi}})$ is then associated a Reproducing Kernel Hilbert Space Berlinet and Thomas-Agnan [2011], where the Kernel is the one induced by the $l^{2}$ product, which we denote by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}\\subseteq C^{0}(\\mathbb{X};\\mathbb{R})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Classical RKHS theory tells us that we can characterize its elements as: ", "page_idx": 21}, {"type": "text", "text": "Proposition B.7. A map $F(\\cdot)_{t}:\\mathbb{X}\\to\\mathbb{R}$ is an element of $\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}$ if and only if it is of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\nF(x)_{t}=\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},S i g(\\omega^{X})_{0,t}\\rangle+\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},S i g(\\omega^{X})_{s,t}\\rangle d\\xi_{s}^{X,j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $\\alpha_{i},\\beta_{j}\\in l^{2}(\\mathbb{W}_{d_{\\omega}})$ . Moreover $\\big\\lVert F(\\cdot)_{t}\\big\\rVert_{\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}}^{2}$ is equal to the minimal value of ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d_{0}}\\big\\|\\alpha_{i}\\big\\|_{l^{2}(\\mathbb{W}_{d_{\\omega}})}^{2}+\\sum_{j=1}^{d_{\\xi}}\\big\\|\\beta_{j}\\big\\|_{l^{2}(\\mathbb{W}_{d_{\\omega}})}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "taken over those $\\gamma,\\beta$ for which the above equality holds. ", "page_idx": 21}, {"type": "text", "text": "Signature kernels Salvi et al. [2021a] are a class of universal kernels on sequential data which have received attention in recent years thanks to their efficiency in handling path-dependent problems Lemercier et al. [2021], Salvi et al. [2021b], Cochrane et al. [2021], Salvi et al. [2021c], Cirone et al. [2023], Issa et al. [2023], Pannier and Salvi [2024], Manten et al. [2024]. ", "page_idx": 21}, {"type": "text", "text": "Just as signature kernels, the kernel associated to $T(X)_{0,t}$ can be explicitly written as the solution of a two-parameter CDE: ", "page_idx": 21}, {"type": "text", "text": "Lemma B.8. Let $\\mathcal{K}^{X,Y}(s,t):=\\langle T(X)_{0,s},T(Y)_{0,t}\\rangle_{l^{2}}$ then ", "page_idx": 22}, {"type": "equation", "text": "$$\nK^{X,Y}(s,t)=\\langle X_{0},Y_{0}\\rangle+\\langle\\xi_{s}^{X},\\xi_{t}^{Y}\\rangle+\\int_{\\eta=0}^{s}\\int_{\\tau=0}^{t}K^{X,Y}(\\eta,\\tau)\\langle d\\omega_{\\eta}^{X},d\\omega_{\\tau}^{Y}\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or, directly in terms of Signature, also ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{K}^{x,Y}(s,t)=\\langle X_{0},Y_{0}\\rangle\\langle S i g(\\omega^{x})_{0,s},S i g(\\omega^{Y})_{0,t}\\rangle+\\int_{\\eta=0}^{s}\\int_{\\tau=0}^{t}\\langle S i g(\\omega^{x})_{\\eta,s},S i g(\\omega^{Y})_{\\tau,t}\\rangle\\langle d\\xi_{\\eta}^{x},d\\xi_{\\tau}^{Y}\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The first expression follows immediately from (32) the second one by summing the products of $T(X)_{0,s}$ \u2019s and $\\bar{T}(Y)_{0,t}$ \u2019s entries given above. ", "page_idx": 22}, {"type": "text", "text": "Definition B.9. Define the space H[(0\u00b7),01],\u03c9,\u03b7\u2286C0(X \u00d7 [0, 1]; R) as the space of functions of form ", "page_idx": 22}, {"type": "equation", "text": "$$\n(X,t)\\mapsto\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},\\operatorname{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle+\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},\\operatorname{Sig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{\\mathrm{X},j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\alpha_{i},\\beta_{j}\\in l^{2}(\\mathbb{W}_{d_{\\omega}})$ . Thus for all $t\\in[0,1]$ and $F\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ it holds $F(\\cdot,t)\\in\\mathcal{H}_{t}^{(\\cdot)_{0},\\omega,\\eta}$ . ", "page_idx": 22}, {"type": "text", "text": "B.3.3 Linear maps on $Z_{t}^{\\mathbf{x}}$ are close to the RKHS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The following proposition will show how linear maps on $Z_{t}^{\\mathrm{x}}$ cannot be more expressive than elements of the RKHS H[(0\u00b7),01],\u03c9,\u03b7since their closure is in the closure of H[( 0\u00b7),01],\u03c9,\u03b7. In this precise sense these spaces act like upper bounds to expressiveness. ", "page_idx": 22}, {"type": "text", "text": "Proposition B.10. Assume $\\mathbb{X}$ compact. Consider fixed the gates and $A_{i}\\,\\in\\,\\mathbb{R}^{N\\times N},B\\,\\in\\,\\mathbb{R}^{N\\times d_{\\xi}}$ and $C\\,\\in\\,\\mathbb{R}^{N\\times d_{0}}$ . Consider a linear readout $\\boldsymbol{v}\\,\\in\\,\\mathbb{R}^{N}$ . For any $\\epsilon\\mathrm{~>~0~}$ there exist choices of $\\alpha_{i},\\beta_{j}\\in l^{2}(\\mathbb{W}_{d_{\\omega}})$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|\\langle v,Z_{t}^{X}\\rangle-F(X,t)|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $F\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ . In other words, linear maps on the $Z_{t}^{X}$ are in the uniform closure of $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Using (30) we see that $\\langle v,Z_{t}^{\\mathrm{X}}\\rangle$ is a linear map on $T(X)_{0,t}$ with coefficients ", "page_idx": 22}, {"type": "equation", "text": "$$\nv^{\\top}A_{I}B_{j}\\quad v^{\\top}A_{I}C_{i}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "using Cauchy-Schwartz it\u2019s moreover easy to see the existence of a constant $\\lambda\\geq0$ such that for all $I,i,j$ one has ", "page_idx": 22}, {"type": "equation", "text": "$$\n|v^{\\top}A_{I}B_{j}|\\leq\\lambda^{|I|}\\quad|v^{\\top}A_{I}C_{i}|\\leq\\lambda^{|I|}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{|\\mathrm{Sig}(\\omega)_{s,t}^{I}|\\leq\\frac{1}{|I|!}\\,\\|\\omega\\|_{1-v a r,[s,t]}^{|I|}}\\end{array}$ we have that, given an integer $M\\geq0$ , the bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{M}(t):=\\left|\\sum_{i=1}^{d_{0}}\\sum_{|I|\\geq M}v^{\\top}A_{I}C_{i}\\;X_{0}^{i}\\mathtt{S i g}(\\omega^{\\mathrm{x}})_{0,t}^{I}+\\sum_{j=1}^{d_{\\xi}}\\sum_{|I|\\geq M}v^{\\top}A_{I}B_{j}\\int_{0}^{t}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{I}d\\xi_{s}^{\\mathrm{x},j}\\right|}}\\\\ {{\\displaystyle\\leq\\left(\\|X_{0}\\|_{1}+\\|\\xi_{t}^{\\mathrm{x}}\\|_{1}\\right)\\sum_{m=M}^{\\infty}\\frac{\\lambda^{m}d_{\\omega}^{m}\\left\\|\\omega^{\\mathrm{x}}\\right\\|_{1-v a r,[0,t]}^{m}}{m!}}}\\\\ {{\\displaystyle\\leq\\left(\\|X_{0}\\|_{1}+\\|\\xi_{t}^{\\mathrm{x}}\\|_{1}\\right)\\sum_{m=M}^{\\infty}\\frac{\\lambda^{m}d_{\\omega}^{m}\\left\\|\\omega^{\\mathrm{x}}\\right\\|_{1-v a r,[0,1]}^{m}}{m!}\\leq K\\sum_{m=M}^{\\infty}\\frac{\\left(\\lambda d_{\\omega}K\\right)^{m}}{m!}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $K\\geq0$ is a constant which must exist by compactness of $\\mathbb{X}$ and continuity of the gates. Since $K\\sum_{m=M}^{\\infty}\\frac{(\\lambda d_{\\omega}K)^{m}}{m!}$ is just the tail of the taylor expansion of $K e^{\\lambda d_{\\omega}K}$ there must be an $M$ such that $\\operatorname*{sup}_{t\\in[0,1]}R_{M}(t)\\leq\\epsilon$ . But then the choice ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha_{i}^{I}:=v^{\\top}A_{I}C_{i}\\;\\mathbb{I}(|I|<M)\\quad\\beta_{j}^{I}:=v^{\\top}A_{I}B_{j}\\;\\mathbb{I}(|I|<M)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "suffices for the required bound. ", "page_idx": 22}, {"type": "text", "text": "B.3.4 Uniform closure of the RKHS ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Now that we have established the theoretical interest of the $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ we proceed to characterize which maps $(X,t)\\rightarrow\\mathbb{R}$ can be uniformly approximated through them. ", "page_idx": 23}, {"type": "text", "text": "Proposition B.11. Fix a compact input set $\\mathbb{X}$ and continuous gates $(\\cdot)_{0},\\omega,\\xi$ with $\\omega_{t}^{x,1}\\equiv t$ and $\\omega_{t}^{X,2}\\equiv t^{2}$ . For any $\\epsilon>0$ and any ", "page_idx": 23}, {"type": "equation", "text": "$$\nF\\in\\left\\{(X,t)\\mapsto\\Psi(\\omega_{[0,t]}^{X})\\cdot X_{0}+\\int_{0}^{t}\\Phi(\\omega_{[s,t]}^{X})\\cdot d\\xi_{s}^{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\Psi\\in C^{0}(C_{1,0};\\mathbb{R}^{d_{0}})$ and $\\Phi\\in C^{0}(C_{1,0};\\mathbb{R}^{d_{\\xi}})$ , there exist a $G\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-G(X,t)|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Note first that the map ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{X}\\times[0,1]\\times[0,1]\\to C_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}})\\quad(X,s,t)\\mapsto\\omega_{[s,t]}^{\\mathrm{X}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is a continuous map from a compact space, thus the image must be compact too. Moreover by Prop. A.8 the Signature separates the points in this image. Since any $G$ as above has form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{G(X,t)=\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle+\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{\\mathrm{X},j}\\ d s}}\\\\ &{}&{=\\displaystyle\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},\\mathrm{Sig}(\\omega_{[0,t]}^{\\mathrm{X}})_{0,1}\\rangle+\\displaystyle\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},\\mathrm{Sig}(\\omega_{[s,t]}^{\\mathrm{X}})_{0,1}\\rangle d\\xi_{s}^{\\mathrm{X},j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the proof follows from the uniform density on compact sets of linear functionals on the (truncated) Signature (Thm. A.5), by also uniformly bounding thanks to compactness and continuity the norms of $X_{0}$ and $\\xi_{1}^{\\mathrm{X}}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Remark B.12. The specific restriction of $\\omega$ to subsets of $[0,1]$ is a crucial part of the result. The family of approximable maps does not include all path-to-path causal11 functions $t\\mapsto Y_{t}^{\\mathrm{X}}$ but a subset of them, of type $t\\mapsto Y_{t}^{\\mathrm{X}}:=\\Psi(\\omega_{[0,t]}^{\\mathrm{X}})$ , satisfying the specific time-homogeneity specified by the form of the restriction, akin to that in Li et al. [2022b]. ", "page_idx": 23}, {"type": "text", "text": "B.3.5 Generic Weights are fully expressive ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We have seen how linear maps on $Z_{t}^{\\mathrm{x}}$ are in the uniform closure of $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ , and we have explicitly characterized this closure. It is then natural to ask \"how much\" of this closure the are able to \"explore\". The present section not only shows that the $Z_{t}^{\\mathrm{x}}$ \"explore\" all the closure, but also that a generic choice of weights is enough to eventually do this with high probability. ", "page_idx": 23}, {"type": "text", "text": "The fact that these maps are \"universal\" in the above sense is not surprising, since it is well known that Linear CDEs are universal for path-to-point tasks cf. Kidger [2022], what is surprising is that this universality can be achieved probabilistically with one of the standard parametrizations used in ML practice (LeCun) 12. ", "page_idx": 23}, {"type": "text", "text": "Theorem B.13. Fix X compact and $\\epsilon>0$ . For all $F\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0,\\omega,\\eta}}$ H[(0\u00b7),01],\u03c9,\u03b7there exist a choice of hidden dimension $N\\geq1$ and parameters $v\\in\\mathbb{R}^{N},A_{i}\\in\\mathbb{R}^{N\\times N},B\\,\\in\\mathbb{R}^{N\\times d_{\\xi}},C\\in\\mathbb{R}^{N\\times d_{0}}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\langle v,Z_{t}^{X}\\rangle|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover generic weight choices suffice with high probability, in the sense that under LeCun initialization ", "page_idx": 23}, {"type": "equation", "text": "$$\n[A_{j}]_{n,n^{\\prime}}\\stackrel{i i d}{\\sim}\\mathcal{N}(0,\\frac{1}{N})\\quad[C]_{n,i},[B]_{n,j}\\stackrel{i i d}{\\sim}\\mathcal{N}(0,1)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the following holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\rightarrow\\infty}\\mathbb{P}\\big[\\exists v\\in\\mathbb{R}^{N}:\\,(4l)\\,h o l d s\\,\\big]=1\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We propose two proofs, the first one of a deterministic character concerns the first claim in the theorem, the second one is probabilistic and concerns the whole result. The deterministic proof follows the same arguments employed by Kidger [2022] and is included to highlight the main idea of the probabilistic result, which reduces to a \"spin\" on the central argument of this proof. ", "page_idx": 24}, {"type": "text", "text": "Deterministic Proof. Any $F\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ has form ", "page_idx": 24}, {"type": "equation", "text": "$$\nF(X,t)=\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},\\mathbf{S}\\mathbf{ig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle+\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},\\mathbf{S}\\mathbf{ig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{\\mathrm{X},j}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for fixed $\\alpha_{i},\\beta_{j}\\in l^{2}(\\mathbb{W}_{d_{\\omega}})$ . Consider an integer $M\\geq0$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(x,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\pi_{M}\\alpha_{i},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle-\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\pi_{M}\\beta_{j},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{X,j}|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\pi_{M}$ is the truncation at length $M$ . ", "page_idx": 24}, {"type": "text", "text": "Fix $d=d_{0}+d_{\\omega}+d_{\\xi}$ . Consider $\\mu(M,d)\\in\\mathbb{N}$ such that $\\mathbb{R}^{\\mu(M,d)}\\simeq T^{M}(\\mathbb{R}^{d})$ . We are going to write $e_{I}\\in\\mathbb{R}^{\\mu(M,d)}$ to mean the image of $e_{I}\\in T^{M}(\\mathbb R^{d})$ through this identification. Note that $\\left(\\cdot\\right)\\otimes_{M}e_{k}:$ $T^{M}(\\mathbb{R}^{d})\\to T^{M}(\\mathbb{R}^{d})$ is a linear map, it does then correspond to a matrix $\\Lambda_{k}\\,\\in\\,\\mathbb{R}^{\\mu(M,d)\\,\\times\\,\\mu(M,d)}$ Write $\\varepsilon_{j}^{\\xi}:=e_{d_{0}+j}$ for $j=1,\\dots,d_{\\xi}$ and $\\varepsilon_{k}^{\\omega}:=e_{d_{0}+d_{\\xi}+k}$ for $k=1,\\ldots,d_{\\omega}$ . Then the solution to ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathbb{R}}^{\\mu(M,d)}\\ni\\tilde{Z}_{t}=\\sum_{i=1}^{d_{0}}X_{0}^{i}e_{i}+\\sum_{j=1}^{d_{\\xi}}\\xi_{t}^{\\tt X,j}\\varepsilon_{j}^{\\tt B}+\\sum_{k=1}^{d_{\\omega}}\\int_{0}^{t}\\Lambda_{d_{0}+d_{\\xi}+k}\\tilde{Z}_{s}d\\omega_{s}^{\\tt X,k}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is the object in $\\mathbb{R}^{\\mu(M,d)}$ corresponding to the truncated tensor $\\pi_{M}(T(X)_{0,t})$ . ", "page_idx": 24}, {"type": "text", "text": "This ${\\tilde{Z}}_{t}$ is of the form $Z_{t}^{\\mathrm{x}}$ with $N\\,=\\,\\mu(M,d)$ , $A_{k}\\;=\\;\\Lambda_{d_{0}+d_{\\xi}+k}$ , $B\\,=\\,\\left[\\varepsilon_{1}^{\\xi}|\\cdot\\cdot\\cdot|\\varepsilon_{d_{\\xi}}^{\\xi}\\right]$ and ${\\cal C}=$ $\\left[e_{1}\\right|\\cdots\\left|e_{d_{0}}\\right]$ . ", "page_idx": 24}, {"type": "text", "text": "In particular note how these matrices are such that ", "page_idx": 24}, {"type": "equation", "text": "$$\ne_{J}^{T}A_{I}C_{i}=\\mathbb{I}(e_{J}=e_{i}\\otimes\\varepsilon_{I}^{\\omega}),\\quad e_{J}^{T}A_{I}B_{j}=\\mathbb{I}(e_{J}=\\varepsilon_{j}^{\\xi}\\otimes\\varepsilon_{I}^{\\omega}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\nA_{I}C_{i}=e_{i}\\otimes\\varepsilon_{I}^{\\omega},\\quad A_{I}B_{j}=\\varepsilon_{j}^{\\xi}\\otimes\\varepsilon_{I}^{\\omega},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and for all $|I|>M$ one has necessarily $A_{I}=0$ . ", "page_idx": 24}, {"type": "text", "text": "Our strategy is that of using these equaities to create a vector $\\boldsymbol{v}\\in\\mathbb{R}^{N}$ corresponding to the $\\pi_{M}\\alpha_{i}$ and $\\pi_{M}\\beta_{j}$ . Define the vector ", "page_idx": 24}, {"type": "equation", "text": "$$\nv:=\\sum_{i=1}^{d_{0}}\\sum_{|I|\\leq M}\\alpha_{i}^{I}\\;e_{i}\\otimes\\varepsilon_{I}^{\\omega}+\\sum_{j=1}^{d_{\\omega}}\\sum_{|I|\\leq M}\\beta_{j}^{I}\\;\\varepsilon_{j}^{\\xi}\\otimes\\varepsilon_{I}^{\\omega}\\in\\mathbb{R}^{\\mu(M,d)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then expanding $Z_{t}^{\\mathrm{x}}$ as in (30) and using the equalities above one has ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\langle\\boldsymbol{v},\\boldsymbol{Z}_{t}^{\\boldsymbol{X}}\\rangle=\\sum_{i=1}^{d_{0}}\\sum_{I}\\boldsymbol{v}^{\\top}A_{I}C_{i}\\;X_{0}^{i}\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{0,t}+\\displaystyle\\sum_{j=1}^{d_{\\omega}}\\sum_{I}\\boldsymbol{v}^{\\top}A_{I}B_{j}\\;\\int_{0}^{t}\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{s,t}d\\xi_{s}^{\\boldsymbol{X},j}}\\\\ &{\\displaystyle\\qquad=\\sum_{i=1}^{d_{0}}\\sum_{|I|\\leq M}\\alpha_{i}^{I}X_{0}^{i}\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{0,t}+\\displaystyle\\sum_{j=1}^{d_{\\xi}}\\sum_{|I|\\leq M}\\beta_{j}^{I}\\int_{0}^{t}\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{s,t}d\\xi_{s}^{\\boldsymbol{X},j}}\\\\ &{\\displaystyle\\qquad=\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\pi_{M}\\alpha_{i},\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{0,t}\\rangle+\\displaystyle\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\pi_{M}\\beta_{j},\\mathrm{Sig}(\\omega^{\\boldsymbol{X}})_{s,t}\\rangle d\\xi_{s}^{\\boldsymbol{X},j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "proving that for such $v$ and $Z^{\\mathrm{X}}$ it holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(x,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\langle v,Z_{t}^{\\mathrm{X}}\\rangle|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The crucial ingredient for the success of this proof is the possibility to recreate the space $T^{M}(\\mathbb{R}^{d_{0}+d_{\\omega}+d_{\\xi}^{\\smile}})$ as an euclidean space. To do this one needs $\\mu(\\bar{M_{}},d_{0}\\!+\\!d_{\\omega}\\!+\\!d_{\\xi})\\sim(d_{0}\\!+\\!d_{\\omega}\\!+\\!\\bar{d}_{\\xi})^{M}$ orthogonal vectors and a way to express them using the matrices $A_{i},B$ and $C$ , the essential equations which capture this are given by (45). ", "page_idx": 25}, {"type": "text", "text": "The core idea of the following probabilistic proof of this same result is that of allowing for some error in (45), so the idea is that of exhibiting only approximately orthogonal vectors. At the cost of losing exactness, one can leverage results of the Johnson-Lindenstrauss Dasgupta and Gupta [2003] type to find on the order of $\\sim e^{\\varepsilon^{2}N}$ vectors in $\\mathbb{R}^{N}$ orthogonal up to an $\\varepsilon$ error, using random projections. This idea in the context of Signature goes back to Cuchiero et al. [2021b], and allows for much smaller hidden dimensions. ", "page_idx": 25}, {"type": "text", "text": "Proof. (Probabilistic Proof) Any $F\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ has form ", "page_idx": 25}, {"type": "equation", "text": "$$\nF(X,t)=\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\alpha_{i},\\operatorname{Sig}(\\omega^{\\mathrm{x}})_{0,t}\\rangle+\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\beta_{j},\\operatorname{Sig}(\\omega^{\\mathrm{x}})_{s,t}\\rangle d\\xi_{s}^{\\mathrm{X},j}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for fixed $\\alpha_{i},\\beta_{j}\\in l^{2}(\\mathbb{W}_{d_{\\omega}})$ . Consider an integer $M\\geq0$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(x,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\pi_{M}\\alpha_{i},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle-\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\pi_{M}\\beta_{j},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{X,j}|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\pi_{M}$ is the truncation at length $M$ ", "page_idx": 25}, {"type": "text", "text": "From Cirone et al. [2023][Appendix C] we know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\left\\|\\frac{1}{N}C_{i}^{\\top}A_{I}^{\\top}A_{J}C_{j}-\\delta_{i I}^{j J}\\right\\|_{L^{2}}=\\mathcal{O}(\\frac{1}{\\sqrt{N}})2^{\\frac{|I|+|J|}{2}}(|I|+|J|)!!}}\\\\ {{\\displaystyle\\left\\|\\frac{1}{N}C_{i}^{\\top}A_{I}^{\\top}A_{J}B_{j}\\right\\|_{L^{2}}=\\mathcal{O}(\\frac{1}{\\sqrt{N}})2^{\\frac{|I|+|J|}{2}}(|I|+|J|)!!}}\\\\ {{\\displaystyle\\left\\|\\frac{1}{N}B_{i}^{\\top}A_{I}^{\\top}A_{J}B_{j}-\\delta_{i I}^{j J}\\right\\|_{L^{2}}=\\mathcal{O}(\\frac{1}{\\sqrt{N}})2^{\\frac{|I|+|J|}{2}}(|I|+|J|)!!}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Our strategy is that of using these bounds to create a vector $\\boldsymbol{v}\\in\\mathbb{R}^{N}$ \"acting\" like the $\\pi_{M}\\alpha_{i}$ and $\\pi_{M}\\beta_{j}$ . Define, noting that the $A_{i},C_{i},B_{j}$ depend on $N$ , the vector ", "page_idx": 25}, {"type": "equation", "text": "$$\nv^{\\mathrm{N}}:=\\frac{1}{N}\\left(\\sum_{i=1}^{d_{0}}\\sum_{|I|\\leq M}\\alpha_{i}^{I}\\;A_{I}C_{i}+\\sum_{j=1}^{d_{\\omega}}\\sum_{|I|\\leq M}\\beta_{j}^{I}\\;A_{I}B_{j}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then expanding $Z_{t}^{\\mathrm{x}}$ as in (30) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle R_{M}:=\\left\\|\\operatorname*{sup}_{j\\in\\mathcal{N}\\times\\{s,t\\}}\\left|\\langle v^{\\mathrm{N}},Z_{t}^{\\mathrm{N}}\\rangle-\\sum_{i=1}^{d}X_{0}^{i}\\langle\\pi_{M}\\alpha_{i},\\mathrm{Sig}(\\omega^{\\mathrm{N}})_{0,t}\\rangle-\\sum_{j=1}^{d_{\\mathrm{c}}}\\int_{0}^{t}\\langle\\pi_{M}\\beta_{j},\\mathrm{Sig}(\\omega^{\\mathrm{N}})_{s,t}|d\\xi_{s}^{\\mathrm{N}}\\rangle\\right|}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{d}\\sum_{\\substack{\\|i\\geq M}}\\left\\|(v^{\\mathrm{N}})^{\\top}A_{I}C_{i}-\\alpha_{i}^{t}\\right\\|_{L^{2}(\\Omega_{i}^{\\mathrm{N}})_{\\Omega}}\\operatorname*{sup}_{\\lambda\\in\\mathcal{N}\\times\\{0,1\\}}|X_{0}^{i}\\mathrm{Sig}(\\omega^{\\mathrm{N}})_{0,t}|}\\\\ &{\\displaystyle~~~~+\\sum_{i=1}^{d}\\sum_{\\substack{\\|i\\geq M}}\\|(v^{\\mathrm{N}})^{\\top}A_{I}C_{i}\\|_{L^{2}(\\Omega_{i}^{\\mathrm{N}})_{\\Omega}}\\operatorname*{sup}_{(x,v)\\in\\mathcal{N}\\times\\{0,1\\}}|X_{0}^{i}\\mathrm{Sig}(\\omega^{\\mathrm{N}})_{0,t}|}\\\\ &{+\\displaystyle\\sum_{j=1}^{d}\\sum_{\\substack{\\|i\\geq M}}\\|(v^{\\mathrm{N}})^{\\top}A_{I}B_{j}-\\beta_{j}^{t}\\|_{L^{2}(\\Omega_{i}^{\\mathrm{N}})_{\\Omega}}\\operatorname*{sup}_{(x,v)\\in\\mathcal{N}\\times\\{0,1\\}}\\big|\\int_{0}^{t}\\hat{S}i g(\\omega^{\\mathrm{N}})_{s,t}^{I}d\\xi_{s}^{\\mathrm{N}}|}\\\\ &{+\\displaystyle\\sum_{j=1}^{d_{\\mathrm{c}}}\\sum_{\\substack{\\|i\\geq M}}\\|(v^{\\mathrm{N}})^{\\top}A_{I}B_{j}\\|_{L^{2}(\\Omega_{i}^{\\mathrm{N}})_{\\Omega}}\\operatorname*{sup}_{(x,v)\\in\\mathcal{N}\\times\\{0,1\\}}\\big|\\int_{0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note how for $|I|\\leq M$ one has ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|(v^{\\mathrm{N}})^{\\top}A_{I}C_{i}-\\alpha_{i}^{I}\\right\\|_{L^{2}}\\leq\\mathcal{O}_{M}(\\frac{1}{\\sqrt{N}})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and that similarly for $|I|>M$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|(v^{\\mathrm{N}})^{\\top}A_{I}C_{i}\\right\\|_{L^{2}}\\leq\\mathcal{O}_{M}(\\frac{1}{\\sqrt{N}})2^{\\frac{|I|}{2}}(M+|I|)!!\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Which leads, thanks to the same bounds of Cirone et al. [2023][Appendix C], to ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{M}=\\frac{1}{\\sqrt{N}}\\mathcal{O}_{M,\\mathbb{X}}(1)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "But then by Markov\u2019s inequality it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n>\\left[\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}\\left\\vert\\langle v^{\\mathrm{N}},Z_{t}^{\\mathrm{X}}\\rangle-\\sum_{i=1}^{d_{0}}X_{0}^{i}\\langle\\pi_{M}\\alpha_{i},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle-\\sum_{j=1}^{d_{\\xi}}\\int_{0}^{t}\\langle\\pi_{M}\\beta_{j},\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}\\rangle d\\xi_{s}^{\\mathrm{X},j}\\right\\vert\\leq\\epsilon\\right]\\rightarrow0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and thus there must be a choice of $N,\\{A_{i}\\},B,C$ such that the inequality holds, and we thus obtain using (50) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\langle v^{\\mathrm{N}},Z_{t}^{\\mathrm{x}}\\rangle|\\leq2\\epsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and we conclude by arbitrariness of $\\epsilon$ . ", "page_idx": 26}, {"type": "text", "text": "B.4 The Diagonal Case ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we study the particular, but empirically important, case where the matrices $A_{i}$ are taken to be diagonal13. ", "page_idx": 26}, {"type": "text", "text": "What we\u2019ll discover is that the $Z_{t}^{\\mathrm{x}}$ cannot differentiate between $\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{I}$ and other $\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{\\sigma(I)}$ for any permutation $\\sigma$ of the letters in the word $I$ . ", "page_idx": 26}, {"type": "text", "text": "B.4.1 Diagonal Expansion for $Z_{t}^{\\mathbf{x}}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proposition B.14. For any choice of $V\\in\\mathbb{R}^{N\\times d_{\\omega}},B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ and $C\\in\\mathbb{R}^{N\\times d_{0}}$ , writing $A_{i}:=$ $\\operatorname{diag}(V_{i})$ , the unique solution to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d Z_{t}^{X}=\\displaystyle\\sum_{i=1}^{d_{\\omega}}A_{i}Z_{t}^{X}d\\omega_{t}^{X,i}+B d\\xi_{t}^{X}}\\\\ &{Z_{0}^{X}=C X_{0}\\in\\mathbb{R}^{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nZ_{t}^{X}=e^{\\mathrm{diag}(V\\omega_{t}^{X})}C X_{0}+\\int_{0}^{t}e^{\\mathrm{diag}(V(\\omega_{t}^{X}-\\omega_{s}^{X}))}B d\\xi_{s}^{X}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which can be expanded as ", "page_idx": 27}, {"type": "equation", "text": "$$\nZ_{t}^{X}=\\sum_{i=1}^{d_{0}}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}^{s y m}C_{i}\\;X_{0}^{i}S i g(\\omega^{X})_{0,t}^{s y m,I}+\\sum_{j=1}^{d_{\\xi}}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}^{s y m}B_{j}\\int_{0}^{t}S i g(\\omega^{X})_{s,t}^{s y m,I}d\\xi_{s}^{X,j}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\nA_{I}^{s y m}:=\\frac{1}{|I|!}\\sum_{\\sigma\\in S_{k}}A_{\\sigma(I)}=A_{I}\\quad S i g(\\omega^{x})_{s,t}^{s y m,I}:=\\frac{1}{|I|!}\\sum_{\\sigma\\in S_{k}}S i g(\\omega^{x})_{s,t}^{\\sigma(I)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By Theorem E.1 and Theorem E.6 we know that the solution of ", "page_idx": 27}, {"type": "equation", "text": "$$\nZ_{t}^{\\mathrm{x}}=Z_{0}^{\\mathrm{x}}+\\sum_{i=1}^{d_{\\omega}}\\int_{0}^{t}A_{i}Z_{t}^{\\mathrm{x}}d\\omega_{t}^{\\mathrm{x},i}+\\int_{0}^{t}B d\\xi_{t}^{\\mathrm{x}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is explicitly given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nZ_{t}^{\\mathrm{X}}=W_{0,t}^{\\mathrm{X}}Z_{0}^{\\mathrm{X}}+\\int_{0}^{t}W_{s,t}^{\\mathrm{X}}B d\\xi_{s}^{\\mathrm{X}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $W_{s,t}$ is the unique solution to ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{s,t}^{\\mathrm{X}}=I d+\\sum_{i=1}^{d_{\\omega}}\\int_{s}^{t}A_{i}W_{s,r}^{\\mathrm{X}}d\\omega_{r}^{\\mathrm{X},i}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In case the $A_{i}\\mathrm{s}$ are commuting matrices one can explicitly write the solution as ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{s,t}^{\\mathrm{x}}=\\exp\\left(\\sum_{i=1}^{d_{\\omega}}\\int_{s}^{t}A_{i}d\\omega_{r}^{\\mathrm{x},i}\\right)=\\exp\\left(\\mathrm{diag}(V(\\omega_{t}^{\\mathrm{x}}-\\omega_{s}^{\\mathrm{x}}))\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "since for fixed $s$ one has, using commutativity, that ", "page_idx": 27}, {"type": "equation", "text": "$$\nd W_{s,t}^{\\mathrm{X}}=W_{s,t}^{\\mathrm{X}}\\left(\\sum_{i=1}^{d_{\\omega}}A_{i}d\\omega_{t}^{\\mathrm{X},i}\\right)=\\sum_{i=1}^{d_{\\omega}}A_{i}W_{s,t}^{\\mathrm{X}}d\\omega_{t}^{\\mathrm{X},i}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand we know, Theorem E.2, that ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{s,t}^{\\mathrm{X}}=\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{I}=\\sum_{k=0}^{\\infty}\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}A_{I}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{I}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The two views are reconciled by noticing that the symmetric group $S_{k}$ acts on $\\mathbb{W}_{d_{\\omega}}^{k}$ , the space of words of lenth $k$ , by permuting the letters and, by commutativity, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall\\sigma\\in S_{k}.\\forall I\\in\\mathbb{W}_{d_{\\omega}}^{k}.\\ A_{I}=A_{\\sigma(I)}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}A_{I}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{I}=\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}\\frac{1}{k!}\\sum_{\\sigma\\in S_{k}}A_{\\sigma(I)}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{\\sigma(I)}=\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}\\frac{A_{I}}{k!}\\sum_{\\sigma\\in S_{k}}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{\\sigma(I)}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "recalling then how $e_{I_{1}}$ \u00b7 \u00b7 \u00b7 $\\begin{array}{r}{e_{I_{k}}=\\sum_{\\sigma\\in S_{k}}e_{\\sigma(I)}}\\end{array}$ we get to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}A_{I}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{I}}\\\\ &{=\\displaystyle\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}\\frac{A_{I}}{k!}\\sum_{\\sigma\\in S_{k}}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{\\sigma(I)}=\\displaystyle\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}\\frac{A_{I}}{k!}\\prod_{i=1}^{k}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{I_{i}}}\\\\ &{=\\displaystyle\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}^{k}}\\frac{1}{k!}\\prod_{i=1}^{k}A_{I}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{I_{i}}=\\displaystyle\\frac{1}{k!}\\left(\\sum_{i=1}^{d_{\\omega}}A_{i}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{i}\\right)^{k}=\\displaystyle\\frac{1}{k!}\\left(\\sum_{i=1}^{d_{\\omega}}\\int_{s}^{t}A_{i}d\\omega_{r}^{\\mathrm{x},i}\\right)^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In particular we see how in the commuting case ", "page_idx": 28}, {"type": "equation", "text": "$$\nW_{s,t}^{\\mathrm{X}}=\\sum_{I\\in\\mathbb{W}_{d_{\\omega}}}A_{I}^{s y m}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{s y m,I}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\nA_{I}^{s y m}:=\\frac{1}{|I|!}\\sum_{\\sigma\\in S_{k}}A_{\\sigma(I)}=A_{I}\\quad\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{s y m,I}:=\\frac{1}{|I|!}\\sum_{\\sigma\\in S_{k}}\\mathrm{Sig}(\\omega^{\\mathrm{X}})_{s,t}^{\\sigma(I)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "B.4.2 Diagonal Expressiveness ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem B.15. Fix a compact input set X and continuous gates $(\\cdot)_{0},\\omega,\\xi$ . For any $\\epsilon>0$ and any ", "page_idx": 28}, {"type": "equation", "text": "$$\nF\\in\\left\\{(X,t)\\mapsto\\psi(\\omega_{t}^{X})\\cdot X_{0}+\\int_{0}^{t}\\phi(\\omega_{t}^{X}-\\omega_{s}^{X})\\cdot d\\xi_{s}^{X}\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for $\\psi\\in C^{0}(\\mathbb{R}^{d_{\\omega}};\\mathbb{R}^{d_{0}})$ and $\\phi\\in C^{0}(\\mathbb{R}^{d_{\\omega}};\\mathbb{R}^{d_{\\xi}}),$ , there exist a choice of hidden dimension $N\\geq1$ and parameters $v\\in\\mathbb{R}^{N},B\\in\\mathbb{R}^{N\\times d_{\\xi}},C\\in\\mathbb{R}^{N\\times d_{0}}$ and diagonal $A_{i}\\in\\mathbb{R}^{N\\times N}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|F(X,t)-\\langle v,Z_{t}^{X}\\rangle|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover the \"reverse\" also holds i.e. given any choice of matrices $A_{i},B,C$ there is an \u03f5-close map $F$ in the family. ", "page_idx": 28}, {"type": "text", "text": "Proof. This is just a repetition of the arguments used for the dense case with little more care to get the uniformity in time. ", "page_idx": 28}, {"type": "text", "text": "One defines the subset Sym(H[(0\u00b7),01],\u03c9,\u03b7) \u2282H [(0\u00b7),01],\u03c9,\u03b7of those F of type (37) defined by \u03b1i, \u03b2j \u2208 $l^{2}(\\mathbb{W}_{d_{\\omega}})$ such that for any word and any permutation $\\sigma(I)$ of it ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\alpha_{i}^{I}=\\alpha_{i}^{\\sigma(I)}\\quad\\beta_{j}^{I}=\\beta_{j}^{\\sigma(I)}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The same argument of Proposition B.10 shows that the uniform closure of the space of linear maps tohni st hlaet $Z_{t}^{\\mathrm{x}}$ cilso csournet aiisn tehde i sna tmhee  uasn itfhoart mo fc litoss usrueb soeft $S y m(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta})$ ,o asne ohwa vtihnagt $F\\in S y m(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta})$ entries eventually equal to 0. ", "page_idx": 28}, {"type": "text", "text": "Since ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{s y m,I}:=\\frac{1}{|I|!}\\sum_{\\sigma\\in S_{k}}\\mathrm{Sig}(\\omega^{\\mathrm{x}})_{s,t}^{\\sigma(I)}=\\frac{1}{|I|!}\\prod_{i=1}^{|I|}(\\omega_{t}^{\\mathrm{x},I_{i}}-\\omega_{s}^{\\mathrm{x},I_{i}}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "such maps can be expressed exactly in the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nP(\\omega_{t}^{\\mathrm{X}})\\cdot X_{0}+\\int_{0}^{t}Q(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}})\\cdot d\\xi_{s}^{\\mathrm{X}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for polynomial maps $P,Q$ fixed in time. The usual compactness and continuity argument, together with an application of Stone-Weiestrass, thus proves that the uniform closure of $S y m(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\overline{{\\eta}}})$ has the form needed. ", "page_idx": 29}, {"type": "text", "text": "aTnhoet hfeinra cl oinnsgereqduieennct ei so ft hSet odneen-siWtyei eosf ttrhase ss apsa csee eonf  flrionema rP rmoapposs itoino nt hBe. $Z_{t}^{\\mathrm{x}}$ in $S y m(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta})$ ; this is ", "page_idx": 29}, {"type": "text", "text": "Remark B.16. Notice how here there is no need to augment the paths in creative ways in order to ensure separability of the points. The map $(\\omega,s,t)\\,\\stackrel{=}{\\mapsto}\\omega_{[s,t]}\\,\\in\\,\\dot{C}_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}})$ is replaced by $(\\omega,s,t)\\mapsto\\omega_{t}-\\omega_{s}\\in\\mathbb{R}^{d_{\\omega}}$ and the space of polynomials always separates points in $\\mathbb{R}^{d_{\\omega}}$ . ", "page_idx": 29}, {"type": "text", "text": "Remark B.17. It is not necessary to pass through $S y m(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta})$ to prove the previous result, since it directly follows from Proposition B.18. This choice of presentation has been motivated by the conviction of the usefulness of drawing parallels and comparisons. ", "page_idx": 29}, {"type": "text", "text": "Proposition B.18. Fix a compact set $\\mathbb{K}\\subset\\mathbb{R}^{d}$ and a $d$ -dimensional convex cone $C$ containing the origin. The space ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{E}:=S p a n\\left(\\mathbb{K}\\ni x\\mapsto e^{\\langle\\alpha,x\\rangle_{\\mathbb{R}^{d}}}\\in\\mathbb{R}:\\alpha\\in C\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is uniformly dense in $C^{0}(\\mathbb{K};\\mathbb{R})$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. This is an application of Stone-Weiestrass: $\\mathcal{E}$ is a sub-algebra since ", "page_idx": 29}, {"type": "equation", "text": "$$\ne^{\\langle\\alpha,x\\rangle}e^{\\langle\\beta,x\\rangle_{\\mathbb{R}^{d}}}=e^{\\langle\\alpha+\\beta,x\\rangle_{\\mathbb{R}^{d}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\alpha,\\beta\\in C\\implies\\alpha+\\beta\\in C$ by convexity of the cone; $\\mathcal{E}$ contains the constant function $e^{\\langle0,x\\rangle}=1$ and is clearly point separating since the cone, being $d$ -dimensional, it contains a basis of the whole space. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Remark B.19. The usefulness of stating the previous result in such a general setting is the following: with this formalism we can, for example, restrict to $\\alpha\\leq0$ , in this way we would have a method to control the stability (cf. Appendix C.1) of the Linear CDEs by choosing the gate with a.s. $\\dot{\\omega}^{\\mathrm{X}}\\geq0$ . ", "page_idx": 29}, {"type": "text", "text": "Corollary B.20 (Mamba Case). In the Mamba setting, the closure reduces to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\{(X,t)\\mapsto\\sum_{i=1}^{d_{\\omega}}\\psi_{i}(\\omega_{t}^{X,i})+\\sum_{i=1}^{d_{\\omega}}\\int_{0}^{t}\\phi_{i}(\\omega_{t}^{X,i}-\\omega_{s}^{X,i})\\ d\\xi_{s}^{X,i}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for continuous $\\psi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $\\phi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ ", "page_idx": 29}, {"type": "text", "text": "Proof. In this setting one runs in parallel $d_{\\omega}$ diagonal systems and then takes a linear combination of the stacked hidden state. The maps in the closure of the whole system are then just the sums of maps in the closure of the subsystems. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "C Stability and Chaining of Diagonal Systems ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For this section consider, unless otherwise stated, a fixed $N\\geq0$ , compact $\\mathbb{X}$ and gates $(\\cdot)_{0},\\omega,\\xi$ . ", "page_idx": 30}, {"type": "text", "text": "We will study the stability and chaining of diagonal systems defined by the choice of a matrix $V\\in\\mathbb{R}^{N\\times d_{\\omega}}$ such that $A_{i}:=d i a g(V_{i})$ , where $V=\\left[V_{1}\\right|\\cdot\\cdot\\cdot\\left|V_{d_{\\omega}}\\right]$ . ", "page_idx": 30}, {"type": "text", "text": "Note that the present discussion holds even for non-diagonal but commuting matrices, since these can be simultaneously diagonalized (at the cost of considering the complex plane). ", "page_idx": 30}, {"type": "text", "text": "C.1 Stability ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Here we explore the stability of the dynamical system $Z^{\\mathrm{X}}$ , thus we need to study the eigenvalues of the $W_{s,t}^{\\mathrm{X}}$ . Recall how in this setting ", "page_idx": 30}, {"type": "equation", "text": "$$\nW_{s,t}^{X}=\\exp\\left(\\sum_{i=1}^{d_{\\omega}}\\int_{s}^{t}A_{i}d\\omega_{r}^{x,i}\\right)=\\exp\\left(\\mathrm{diag}(\\int_{s}^{t}V d\\omega_{r}^{x})\\right)=\\exp\\left(\\mathrm{diag}\\left(V(\\omega_{t}^{x}-\\omega_{s}^{x})\\right)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that because $\\omega^{\\mathrm{X}}$ is continuous and of bounded variation, it can be reparameterised to be Lipschitz continuous, hence absolutely continuous. Thus we can assume that $\\omega^{\\mathrm{X}}$ is almost everywhere differentiable and its derivative $\\dot{\\omega}\\in\\dot{L}^{1}$ . ", "page_idx": 30}, {"type": "text", "text": "sTihneg usltaarb ivleitcyt oorfs  tohf $V$ . yInf $V\\dot{\\omega}_{t}^{\\mathrm{X}}\\leq\\dot{0}$ s fteorm a ltlh teinm dese,p ewnhdesr eo tnh et hien eaqliuganlitmye inst  cboeotrwdeiennat $\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}$ eann $W_{s,t}^{\\mathrm{X}}$ has eigenvalues all in $[0,1]$ thus the system is stable making training easier Orvieto et al. [2023a]. ", "page_idx": 30}, {"type": "text", "text": "Consider the singular value decomposition (SVD) of the matrix $V$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nV=\\sum_{k=1}^{K}\\sigma_{k}\\;v_{k}u_{k}^{\\top}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, a sufficient condition for stability is that for any $k=1,...,K$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n0>\\sigma_{k}\\in\\mathbb{R},\\quad0\\leq v_{k}\\in\\mathbb{R}^{N},\\quad\\mathrm{and}\\quad\\langle u_{k},\\dot{\\omega}_{t}^{\\mathrm{x}}\\rangle\\geq0\\,\\mathrm{~for~any~}t\\in[0,T].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.1.1 The case of Mamba ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the case of Mamba Gu and Dao [2023] the matrices are diagonal and ", "page_idx": 30}, {"type": "equation", "text": "$$\nd\\omega_{t}^{\\mathrm{X}}=s o f t p l u s(W x_{t}+\\lambda)d t,\\quad d\\xi_{t}^{\\mathrm{X}}=x_{t}\\odot d\\omega_{t}^{\\mathrm{X}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "moreover the proposed choices of $V$ are all of type ", "page_idx": 30}, {"type": "equation", "text": "$$\nV=-\\mathbf{v}\\otimes\\mathbf{1}_{d_{\\omega}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some choice of $0\\,\\le\\,{\\bf v}\\,\\in\\,\\mathbb{R}^{N}$ . Note that softmax is just a smooth approximation of $R e L U$ and that $I m(R e L U)\\,\\subseteq\\,\\{w\\,\\in\\,\\mathbb{R}^{d_{\\omega}}\\,:\\,\\langle\\mathbf{1}_{d_{\\omega}},w\\rangle\\,\\ge\\,0\\}$ hence mamba is implicitly ensuring that the dynamical system is approximately always well-conditioned. ", "page_idx": 30}, {"type": "text", "text": "C.2 Chaining ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The diagonal case differs from the general one not only in the fact that the class of approximable functions is much weaker but also in the necessity for the presence of $\\xi^{\\mathrm{X}}$ in order to obtain any path-dependence. The term ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\phi(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}})\\cdot d\\xi_{s}^{\\mathrm{X}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "becomes then a crucial component. At first sight one might think that such a term allows to recover at least level two components of the Signature of $\\left(\\omega^{\\mathrm{X}},\\xi^{\\mathrm{X}}\\right)$ , unfortunately things are not as easy as they may seem. Notice how inside of the integral time is \"going backwards\" from the perspective of $\\omega^{\\bar{X}}$ , thus we can in general approximate terms of type ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\int_{s}^{t}d\\omega_{r}^{x,i}d\\xi_{s}^{x,j}=\\int_{1-t}^{1}\\int_{1-t}^{r}d\\overleftarrow{\\omega}_{r}^{x,i}d\\overleftarrow{\\xi}_{s}^{\\ x,j}=\\mathrm{Sig}((\\overleftarrow{\\omega}^{x},\\overleftarrow{\\xi}^{x}))_{1-t,1}^{i_{\\omega}j_{\\xi}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which are indeed terms of the Signature, but of the reverse paths $\\smash{\\overleftarrow{\\boldsymbol{\\omega}}_{r}^{\\mathrm{X}}=\\omega_{1-r}^{\\mathrm{X}}}$ and $\\overleftarrow{\\xi}_{s}=\\xi_{1-s}^{\\mathrm{X}}!$ ", "page_idx": 30}, {"type": "text", "text": "Proposition C.1. Fix a compact input set $\\mathbb{X}$ , continuous gates $(\\cdot)_{0},\\omega,\\xi$ and $X_{0}^{1}\\ =\\ 1$ . If the components of $\\xi^{X}$ are linear combinations of those of $\\omega^{X}$ , with time-independent weights, then linear functionals on $Z_{t}^{X}$ can, uniformly in $\\mathbb{X}\\times[0,1]$ , approximate arbitrarily well the following level 2 terms of $S i g((\\omega^{X},\\xi^{X}))_{0,t}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\int_{0}^{s}d\\omega_{r}^{X,i}d\\xi_{s}^{X,j}=S i g((\\omega^{X},\\xi^{X}))_{0,t}^{i_{\\omega}j_{\\xi}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Under these hypotheses we know that linear functionals on $Z_{t}^{\\mathrm{x}}$ are uniformly dense, for continuous $\\psi,\\phi$ , in ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\{(X,t)\\mapsto\\psi(\\omega_{t}^{\\mathrm{x}})\\cdot X_{0}+\\int_{0}^{t}\\phi(\\omega_{t}^{\\mathrm{x}}-\\omega_{s}^{\\mathrm{x}})\\cdot d\\xi_{s}^{\\mathrm{x}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Assume $\\xi_{s}^{\\mathrm{X},j}=\\langle\\alpha_{j},\\omega_{t}^{\\mathrm{X}}\\rangle$ and consider the choices ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\psi(x)=(x^{i}\\langle\\alpha_{j},x\\rangle,0,\\cdots\\,,0)^{\\top},\\quad\\phi(x)=-(0,\\cdots\\,,0,x^{i},0,\\cdots\\,,0).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "so that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\omega_{t}^{\\mathrm{x}})\\cdot X_{0}=\\omega_{t}^{\\mathrm{x},i}\\xi_{t}^{\\mathrm{x},j}\\quad\\phi(\\omega_{t}^{\\mathrm{x}}-\\omega_{s}^{\\mathrm{x}})\\cdot d\\xi_{s}^{\\mathrm{x}}=-(\\omega_{t}^{\\mathrm{x},i}-\\omega_{s}^{\\mathrm{x},i})d\\xi_{s}^{\\mathrm{x},j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To conclude note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\omega_{t}^{\\mathrm{x},i}\\xi_{t}^{\\mathrm{x},j}=\\displaystyle\\int_{s=0}^{t}\\int_{r=0}^{t}d\\omega_{r}^{\\mathrm{x},i}d\\xi_{s}^{\\mathrm{x},j}=\\displaystyle\\int_{s=0}^{t}\\int_{r=0}^{s}d\\omega_{r}^{\\mathrm{x},i}d\\xi_{s}^{\\mathrm{x},j}+\\int_{s=0}^{t}\\int_{r=s}^{t}d\\omega_{r}^{\\mathrm{x},i}d\\xi_{s}^{\\mathrm{x},j}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\int_{0}^{t}\\int_{0}^{s}d\\omega_{r}^{\\mathrm{x},i}d\\xi_{s}^{\\mathrm{x},j}+\\int_{s=0}^{t}(\\omega_{t}^{\\mathrm{x},i}-\\omega_{s}^{\\mathrm{x},i})d\\xi_{s}^{\\mathrm{x},j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "hence ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\int_{0}^{s}d\\omega_{r}^{x,i}d\\xi_{s}^{x,j}=\\omega_{t}^{x,i}\\xi_{t}^{x,j}-\\int_{s=0}^{t}(\\omega_{t}^{x,i}-\\omega_{s}^{x,i})d\\xi_{s}^{x,j}=\\psi(\\omega_{t}^{x})\\cdot X_{0}+\\int_{0}^{t}\\phi(\\omega_{t}^{x}-\\omega_{s}^{x})\\cdot d\\xi_{s}^{x,j}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "If $X\\in C_{1,0}([0,1];\\mathbb{R}^{d})$ we can use the previous result to compute its Signature entries by chaining diagonal Linear CDEs. ", "page_idx": 31}, {"type": "text", "text": "Theorem C.2. Assume a compact input set $\\mathbb{X}\\subset C_{1,0}([0,1];\\mathbb{R}^{d})$ . For any $I\\in\\mathbb{W}_{d}$ with $|I|\\ge2$ and $\\epsilon>0$ there is a sequence of linear maps $W_{k}\\in\\mathbb{R}^{N_{k}\\times1}$ and weights for the following family of chained Linear CDEs ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle d Z_{t}^{1,x}=\\sum_{i=1}^{d}A_{i}^{(1)}Z_{t}^{1,x}d X_{t}^{i}+B^{(1)}d X_{t}\\in\\mathbb{R}^{N_{1}},\\quad\\displaystyle Z_{0}^{1,x}=Z_{0}^{1},}\\\\ {d Z_{t}^{k+1,x}=\\displaystyle\\sum_{i=1}^{d+1}A_{i}^{(k+1)}Z_{t}^{k+1,x}d\\left[\\!\\!\\begin{array}{c}{\\!\\!\\!W_{k}Z^{k,x}\\!\\!\\right]_{t}^{i}+B^{(k+1)}d X_{t}\\in\\mathbb{R}^{N_{k+1}},\\quad\\displaystyle Z_{0}^{k+1,x}=Z_{0}^{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "such that for some $v\\in\\mathbb{R}^{N_{|I|-1}}$ one has ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|S i g(X)_{0,t}^{I}-\\langle v,Z_{t}^{|I|-1,X}\\rangle|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. For $|I|\\,=\\,2$ we can apply Prop. C.1. Assume the theorem holds for $|I|\\ \\leq\\ k$ and let $M:=\\operatorname*{sup}_{X\\in\\mathbb{X}}\\|X\\|_{1-v a r}$ . Fix $|I j|=k+1$ and $W_{k-1}\\in\\mathbb{R}^{N_{k-1}\\times1}$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}|S i g(X)_{0,t}^{I}-W_{k-1}Z_{t}^{k-1,x}|\\leq\\frac{\\epsilon}{M}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Again by Prop. C.1 there are a $N_{k}$ and $\\boldsymbol{v}\\in\\mathbb{R}^{N_{k}}$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(X,t)\\in\\mathbb{X}\\times[0,1]}\\Big|\\int_{0}^{t}W_{k-1}Z_{s}^{k-1,\\mathrm{x}}d X_{s}^{j}-\\langle v,Z_{t}^{k,\\mathrm{x}}\\rangle|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Sig}(X)_{0,t}^{I_{j}}-\\langle v,Z_{t}^{k,X}\\rangle|\\leq|\\displaystyle\\int_{0}^{t}\\mathrm{Sig}(X)_{0,s}^{I}d X_{s}^{j}-\\displaystyle\\int_{0}^{t}W_{k-1}Z_{s}^{k-1,X}d X_{s}^{j}|+|\\displaystyle\\int_{0}^{t}W_{k-1}Z_{s}^{k-1,x}d X_{s}^{j}-\\langle X\\rangle|_{\\displaystyle\\operatorname*{max}\\{s,X\\}}^{I}d X_{s}^{j}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{t}|\\mathrm{Sig}(X)_{0,s}^{I}-W_{k-1}Z_{s}^{k-1,X}||d X_{s}^{j}|+\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{t}\\frac{\\epsilon}{M}|d X_{s}^{j}|+\\epsilon\\leq2\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "thus concluding the proof. ", "page_idx": 32}, {"type": "text", "text": "Then Proposition 4.5 follows as a corollary by running in parallel the systems above to recover simultaneously multiple Signature entries. ", "page_idx": 32}, {"type": "text", "text": "C.2.1 ReLU activation choice ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Models like Mamba do not only use diagonal matrices but also consider controls of a specific kind: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\omega_{t}^{X}=\\int_{0}^{t}R e L U(W X_{s}+b)d s\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The choice of $R e L U$ enforces $\\dot{\\omega}_{t}~\\geq~0$ for all times as seen above, but could, a priori, destroy information about $X$ which allows for the recovery, after chaining, of its Signature. ", "page_idx": 32}, {"type": "text", "text": "Does this choice keep some expressivity? Fortunately almost all of it: since ", "page_idx": 32}, {"type": "equation", "text": "$$\nR e L U(x)-R e L U(-x)=x\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "one can choose a linear map $W$ which allows to linearly recover ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\tilde{\\omega}_{t}^{\\mathrm{X}}=\\int_{0}^{t}X_{s}d s\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "from $\\omega_{t}^{\\mathrm{x}}$ . By correspondingly modifying the form of $\\psi$ and $\\phi$ in (74) such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\psi(\\omega_{t}^{\\mathrm{x}})\\cdot X_{0}=\\tilde{\\omega}_{t}^{\\mathrm{x},i}\\xi_{t}^{\\mathrm{x},j}\\quad\\phi(\\omega_{t}^{\\mathrm{x}}-\\omega_{s}^{\\mathrm{x}})\\cdot d\\xi_{s}^{\\mathrm{x}}=-(\\tilde{\\omega}_{t}^{\\mathrm{x},i}-\\tilde{\\omega}_{s}^{\\mathrm{x},i})d\\xi_{s}^{\\mathrm{x},j}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "one is able, through a similar chaining procedure, to recover arbitrarily deep entries of the Signature of $\\begin{array}{r}{\\tilde{\\omega}_{t}^{\\mathrm{X}}=\\int_{0}^{t}X_{s}d s}\\end{array}$ . ", "page_idx": 32}, {"type": "text", "text": "D Path-to-Path ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Definition D.1. A map $G\\in C^{0}(C_{1,0}([0,1];\\mathbb{R}^{d})\\times[0,1];\\mathbb{R})$ is causal iff for all $t\\in[0,1]$ and paths $\\omega,\\tilde{\\omega}\\in C_{1,0}([0,1];\\mathbb{R}^{d})$ one has ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\omega|_{[0,t]}=\\tilde{\\omega}|_{[0,t]}\\implies G(\\omega,t)=G(\\tilde{\\omega},t)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "i.e. G is causal if it does not look in the future. ", "page_idx": 33}, {"type": "text", "text": "Proposition D.2. Assume a compact input set ${\\mathbb X},$ continuous $(\\cdot)_{0},\\omega,\\xi,\\,X_{0}^{1}\\equiv1$ and $\\omega_{t}^{x,1}\\equiv t$ . Then for all $\\epsilon>0$ and all causal $G\\in C^{0}(C_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}})\\times[0,1];\\mathbb{R})$ there exist an integer $N\\geq0,$ , some Feed Forward neural network $F:\\mathbb{R}^{N}\\rightarrow\\mathbb{R},$ and parameters $C\\in\\mathbb{R}^{N\\times d_{0}},A_{i}\\in\\mathbb{R}^{N\\times N},B\\in\\mathbb{R}^{N\\times d_{\\xi}}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{X}}\\operatorname*{sup}_{t\\in[0,1]}|F(Z_{t}^{X})-G(\\omega^{X},t)|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Fix $\\epsilon>0$ . By B.7 the space $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ contains all functionals of form ", "page_idx": 33}, {"type": "equation", "text": "$$\n(X,t)\\mapsto\\langle\\alpha,\\operatorname{Sig}(\\omega^{\\mathrm{X}})_{0,t}\\rangle\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "thus, by the properties of the signature and by compactness of $\\mathbb{X}$ , for any fixed $s_{0}\\in[0,1]$ there is some f \u2208H [(0\u00b7),01],\u03c9,\u03b7such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{X}}|f(X,s_{0})-G(\\omega^{\\mathrm{X}},s_{0})|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the fact that $G\\,\\in\\,C^{0}([0,1];C^{0}(C_{1,0}([0,1];\\mathbb{R}^{d_{\\omega}});\\mathbb{R}))$ and compactness of $[0,1]$ , we find a finite set $\\{0\\leq s_{0}\\leq\\cdot\\cdot\\leq s_{M}\\leq1\\}$ of points and $f_{0},\\ldots,f_{M}\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{(X,s)\\in\\mathbb{X}\\times[s_{i-1},s_{i+1}]}|G(\\omega^{\\mathrm{x}},s)-G(\\omega^{\\mathrm{x}},s_{i})|<\\epsilon}\\\\ &{\\qquad\\qquad\\mathrm{sup~}|f_{i}(X,s_{i})-G(\\omega^{\\mathrm{x}},s_{i})|<\\epsilon}\\\\ &{\\qquad\\qquad\\mathrm{xup~}}\\\\ &{(X,s)\\mathsf{in}\\mathbb{X}\\times[s_{i-1},s_{i+1}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for $i=0,\\dots,M-1$ . Notice then how for all $X\\in\\mathbb{X}$ and $s\\in[s_{i-1},s_{i+1}]$ ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{i}(X,s)-G(\\omega^{x},s)|\\leq|f_{i}(X,s)-f_{i}(X,s_{i})|+|f_{i}(X,s_{i})-G(\\omega^{x},s_{i})|+|G(\\omega^{x},s_{i})-G(\\omega^{x},s)|\\leq1\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It follows that the map $F\\in C^{0}([0,1]\\times\\mathbb{X};\\mathbb{R})$ linearly interpolating the $f_{i}$ in time satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{X}}\\operatorname*{sup}_{t\\in[0,1]}|F(X)_{t}-G(\\omega^{X},t)|<6\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To conclude note that $\\mathbb{X}$ being compact, the $f_{i}$ take values in a common compact set $K\\subseteq\\mathbb{R}$ . There exist then a neural network $\\bar{\\Psi}:[0,\\dot{1}]\\times K^{M}\\!\\to\\mathbb{R}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{i\\in0,\\ldots,M-1}\\operatorname*{sup}_{s\\in[s_{i},s_{i+1}]}|\\Psi(t,z)-\\left(\\frac{s_{i+1}-s}{s_{i+1}-s_{i}}z_{i}+\\frac{s-s_{i}}{s_{i+1}-s_{i}}z_{i+1}\\right)|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which means that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{X}}\\operatorname*{sup}_{t\\in[0,1]}|\\Psi(t,f_{0}(X,t),\\ldots,f_{M}(X,t))-F(X,t)|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recalling that $\\omega_{t}^{{\\mathrm{x}},1}=t$ we get that $X\\mapsto\\{t\\mapsto t\\}\\in\\mathcal{H}_{[0,1]}^{(\\cdot)_{0},\\omega,\\eta}$ H[(0\u00b7),01],\u03c9,\u03b7so that, given density of linear maps on in the space, $\\Psi(t,f_{0}(X,t),\\ldots,f_{M}(X,t))$ can be uniformly approximated. Triangular inequality gives finally ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{X}}\\operatorname*{sup}_{t\\in[0,1]}|\\Psi(t,f_{0}(X,t),\\ldots,f_{M}(X,t))-G(\\omega^{\\mathrm{x}},t)|<7\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which, by arbitrariness of $\\epsilon$ , gives the thesis. ", "page_idx": 33}, {"type": "text", "text": "The non-linearity is crucial for the path-to-path result. A map of type $(\\omega,t)\\mapsto\\langle t\\alpha,\\mathrm{Sig}(\\omega)_{0,t}\\rangle$ cannot be approximated arbitrarily well by $(\\omega,t)\\dot{\\mapsto}\\left\\langle\\beta,\\mathrm{Sig}(\\omega)_{0,t}\\right\\rangle$ .   \nIn any case, note that in the proof the role of the neural network is only that of interpolating the RKHS elements in the right order and at the right time. All the non-linear complexity of learning the particular $G$ is offloaded and taken care of by the RKHS elements.   \nRemark D.3. In the proof we have only considered the part of $T(X)$ concerning $\\omega^{\\mathrm{X}}$ , but $T(X)_{t}$ depends linearly on $X_{0}$ and $\\xi^{\\mathrm{X}}$ suggesting that neural networks on H[(0\u00b7),01],\u03c9,\u03b7have stronger generalization properties. In fact one can prove that it is possible to approximate all continuous $G(X_{0},\\omega^{\\mathrm{X}},\\xi^{\\mathrm{X}},t)$ , this is done by reconstructing $X_{0}$ and $\\xi_{[0,1]}^{\\mathrm{X}}$ as in the classical SSM case cf. Orvieto et al. [2023b]. ", "page_idx": 34}, {"type": "text", "text": "E Wronskian Matrix Theory ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section we obtain a unified theory studying the solutions to general Linear CDEs. The results presented here are not new and can be found in different terms in the literature Friz and Victoir [2010], despite this we have decided to reproduce them from scratch for completeness, notational reasons and to present a self-contained theory. ", "page_idx": 35}, {"type": "text", "text": "Theorem E.1. For any choice $\\{A^{1},\\ldots,A^{d}\\}\\subseteq C^{0}([0,1];\\mathbb{R}^{N\\times N})$ and $\\omega\\,\\in\\,C_{1}([0,1];\\mathbb{R}^{d})$ there exist a unique map $W\\in C^{0}([0,\\Bar{1}]\\times[0,1];\\mathbb{R}^{N\\times N})$ solving the following $C D E$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nW_{s,t}=I d_{N}+\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}W_{s,\\tau}d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. We will use Banach fixed point theorem leveraging the completeness of the space $\\Omega\\,:=$ $C^{0}([0,1]\\times[0,1];\\mathbb{R}^{N\\times N})$ with the uniform norm ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|X\\|_{\\infty}:=\\operatorname*{sup}_{s,t\\in[0,1]}\\|X_{s,t}\\|_{o p}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Define the map $\\Gamma:\\Omega\\to\\Omega$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Gamma(X)_{s,t}=I d_{N}+\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}X_{s,\\tau}d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "One has, for $X,Y\\in\\Omega$ and $k\\in\\mathbb{N}$ setting $\\Gamma^{0}=I d_{\\Omega}$ , that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Gamma^{k+1}(X)_{s,t}-\\Gamma^{k+1}(Y)_{s,t}=\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}\\left(\\Gamma^{k}(X)_{s,\\tau}-\\Gamma^{k}(Y)_{s,\\tau}\\right)d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which iterated gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Gamma^{k+1}(X)_{s,t}-\\Gamma^{k+1}(Y)_{s,t}=\\sum_{\\stackrel{I\\in\\mathcal{W}_{d}}{|I|=k+1}}\\int_{\\tau_{k+1}=s}^{t}...\\int_{\\tau_{1}=s}^{\\tau_{2}}\\left(\\prod_{j=k+1}^{1}A_{\\tau_{j}}^{I_{j}}\\right)(X_{s,\\tau_{1}}-Z_{s,\\tau_{1}})\\prod_{j=1}^{k+1}d\\omega_{\\tau_{j}}^{I_{j}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $w_{d}$ is the set of words in the alphabet $\\{1,\\ldots,d\\}$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\Delta_{[s,t]}^{k}:=\\{(\\tau_{1},\\dots,\\tau_{k})\\in[0,1]^{k}:\\forall j\\in1,\\dots,k-1.\\ \\tau_{j}\\leq\\tau_{j+1}\\}}\\\\ {\\displaystyle A_{\\tau}^{I}:=\\prod_{j=k+1}^{1}A_{\\tau_{j}}^{I_{j}}\\quad d\\omega_{\\tau}^{I}:=\\prod_{j=1}^{k+1}d\\omega_{\\tau_{j}}^{I_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By defining $M=\\operatorname*{max}\\{\\left\\|A^{i}\\right\\|_{\\infty}:i\\in\\{1,\\ldots,d\\}\\}$ then one clearly has ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\Gamma^{k}(X)-\\Gamma^{k}(Y)\\right\\|_{\\infty}\\leq\\frac{(d M\\left\\|\\omega\\right\\|_{1-v a r})^{k}}{k!}\\left\\|X-Y\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "thus definitely $(\\sin{k})$ ) the map $\\Gamma^{k}$ is a contraction. By Banach fixed point there exist a unique fixed point $W\\in\\Omega$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Theorem E.2. Under the assumptions of the previous theorem one can write $W_{s,t}$ explicitly as ", "page_idx": 35}, {"type": "equation", "text": "$$\nW_{s,t}=\\sum_{I\\in\\mathcal{W}_{d}}\\int_{\\tau\\in\\Delta_{[s,t]}^{|I|}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "moreover if for all i the matrix-valued maps are constant on all $[0,1]$ i.e. $A_{t}^{i}\\equiv A_{i}$ then ", "page_idx": 35}, {"type": "equation", "text": "$$\nW_{s,t}=\\sum_{I\\in\\mathcal{W}_{d}}A_{I}S i g(\\omega)_{s,t}^{I}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $S i g(\\omega)_{s,t}^{I}$ is the Signature of the path $\\omega$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. The second assertion follows from the first by definition of the Signature of a path. ", "page_idx": 36}, {"type": "text", "text": "Regarding the first notice how the series is absolutely convergent in $\\mathbb{R}^{N\\times N}$ , uniformly in $s,t$ since ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{I\\in\\mathcal{W}_{d}}\\left\\|\\int_{\\tau\\in\\Delta_{[s,t]}^{[I]}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}\\right\\|_{o p}\\leq\\displaystyle\\sum_{k=0}^{\\infty}d^{k}M^{k}\\frac{\\|\\omega\\|_{1-v a r,[s,t]}^{k}}{k!}}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=e^{d M\\|\\omega\\|_{1-v a r,[s,t]}}\\leq e^{d M\\|\\omega\\|_{1-v a r,[0,1]}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "thus for any $s,t\\in[0,1]$ the series defines an element of $\\tilde{W}_{s,t}\\in\\mathbb{R}^{N\\times N}$ ", "page_idx": 36}, {"type": "text", "text": "Using the uniformity of this bound and the fact that for all $I\\in\\mathcal{W}_{d}$ one has ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{W}_{s,t}^{I}:=\\int_{\\tau\\in\\Delta_{[s,t]}^{|I|}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}\\in\\Omega\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as a function of $(s,t)$ , which moreover is uniformly continuous ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\tilde{W}_{s_{1},t_{1}}^{I}-\\tilde{W}_{s_{2},t_{2}}^{I}\\right\\|_{o p}=\\left\\|\\int_{\\tau\\in\\Delta_{[s_{1}\\wedge s_{2},t_{1}\\vee t_{2}]}^{|I|}}(\\delta_{\\tau\\in\\Delta_{[s_{1},t_{1}]}^{|I|}}-\\delta_{\\tau\\in\\Delta_{[s_{2},t_{2}]}^{|I|}})A_{\\tau}^{I}d\\omega_{\\tau}^{I}\\right\\|_{o p}}\\\\ &{\\qquad\\qquad\\qquad\\leq M^{|I|}\\int_{\\tau\\in\\Delta_{[s_{1}\\wedge s_{2},t_{1}\\vee t_{2}]}^{|I|}}\\left|\\delta_{\\tau\\in\\Delta_{[s_{1},t_{1}]}^{|I|}}-\\delta_{\\tau\\in\\Delta_{[s_{2},t_{2}]}^{|I|}}\\right||d\\omega_{\\tau}^{I}|}\\\\ &{\\qquad\\qquad\\qquad\\leq M^{|I|}\\|\\omega\\|_{[s_{1}\\wedge s_{2},s_{1}\\vee s_{2}]}^{|I|}\\|t_{1}\\wedge t_{2},t_{1}\\vee t_{2}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "one concludes that $\\tilde{W}_{s,t}\\in\\Omega$ . Finally notice that $\\tilde{W}_{s,t}$ is a fixed point of $\\Gamma$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma(\\Tilde{W}_{s,t})=I d_{N}+\\displaystyle\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}\\left(\\displaystyle\\sum_{I\\in\\mathcal{W}_{d}}\\int_{\\tau\\in\\Delta_{[0,1]}^{[I]}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}\\right)d\\omega_{\\tau}^{i}}\\\\ &{\\quad\\quad\\quad=I d_{N}+\\displaystyle\\sum_{I\\in\\mathcal{W}_{d}}\\int_{\\tau\\in\\Delta_{[0,1]}^{[I]}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}d\\omega_{\\tau}^{i}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{I\\in\\mathcal{W}_{d}}\\int_{\\tau\\in\\Delta_{[0,1]}^{[I]}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}=\\Tilde{W}_{s,t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and conclude by uniqueness. ", "page_idx": 36}, {"type": "text", "text": "Proposition E.3. Under the previous conditions, the unique solution of the $N$ -dimensional CDE ", "page_idx": 36}, {"type": "equation", "text": "$$\nd X_{t}=X_{0}+\\sum_{i=1}^{d}\\int_{\\tau=0}^{t}A_{\\tau}^{i}X_{\\tau}d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\nX_{t}=W_{0,t}X_{0}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. The solutions are unique by standard results Friz and Victoir [2010][Thm. 3.7], moreover ", "page_idx": 36}, {"type": "equation", "text": "$$\nW_{0,t}X_{0}=\\left(I d_{N}+\\sum_{i=1}^{d}\\int_{\\tau=0}^{t}A_{\\tau}^{i}W_{0,\\tau}d\\omega_{\\tau}^{i}\\right)X_{0}=X_{0}+\\sum_{i=1}^{d}\\int_{\\tau=0}^{t}A_{\\tau}^{i}(W_{0,\\tau}X_{0})d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proposition E.4. The Wronskian matrix has the following properties: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l.\\ \\forall r,s,t\\in[0,1].\\quad W_{r,t}=W_{s,t}W_{r,s}}\\\\ &{2.\\ \\forall s,t\\in[0,1].\\quad W_{s,t}^{-1}=W_{t,s}}\\\\ &{3.\\ \\forall s,t\\in[0,1].\\quad W_{s,t}=I d_{N}+\\sum_{i=1}^{d}\\int_{\\sigma=s}^{t}W_{\\sigma,t}A_{\\sigma}^{i}d\\omega_{\\sigma}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Regarding the first statement notice that for all $X_{0}\\in\\mathbb{R}^{N}$ one has ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{X}_{t}:=\\!W_{s,t}W_{r,s}X_{0}=\\left(I d_{N}+\\displaystyle\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}W_{s,\\tau}d\\omega_{\\tau}^{i}\\right)W_{r,s}X_{0}}\\\\ &{\\quad\\quad=\\!W_{r,s}X_{0}+\\displaystyle\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}(W_{s,\\tau}W_{r,s}X_{0})d\\omega_{\\tau}^{i}}\\\\ &{\\quad\\quad=\\!W_{r,s}X_{0}+\\displaystyle\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}\\tilde{X}_{\\tau}d\\omega_{\\tau}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and by the previous proposition also ", "page_idx": 37}, {"type": "equation", "text": "$$\nX_{t}:=W_{r,t}X_{0}=W_{r,t}X_{0}+\\sum_{i=1}^{d}\\int_{\\tau=r}^{t}A_{\\tau}^{i}X_{\\tau}d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "thus $X_{t}$ and $\\tilde{X}_{t}$ solve the same $C D E$ and coincide at time $t=s$ . This means, by uniqueness, that $X_{t}$ and $\\tilde{X}_{t}$ coincide for all times; hence $W_{s,t}W_{r,s}$ and $W_{r,t}$ coincide for all times too since for any choice of $X_{0}$ one has $W_{s,t}W_{r,s}X_{0}=W_{r,t}X_{0}$ . ", "page_idx": 37}, {"type": "text", "text": "The second statement follows from the previous one setting first $r=t$ and subsequently exchanging $s$ and $t$ . ", "page_idx": 37}, {"type": "text", "text": "To prove the third equality note that ", "page_idx": 37}, {"type": "equation", "text": "$$\n0=d_{s}(W_{s,t}W_{t,s})=(d_{s}W_{s,t})W_{t,s}+W_{s,t}(d_{s}W_{t,s})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "hence ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{s}W_{s,t}=-W_{s,t}(d_{s}W_{t,s})W_{t,s}^{-1}=-W_{s,t}(\\sum_{i=1}^{d}A_{s}^{i}W_{t,s}d\\omega_{s}^{i})W_{t,s}^{-1}=-\\sum_{i=1}^{d}W_{s,t}A_{s}^{i}d\\omega_{s}^{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proposition E.5 (Liouville\u2019s Formula). Under the assumptions of the previous theorems, if $\\omega\\in$ $C^{1}([0,1];\\mathbb{R}^{d})$ then ", "page_idx": 37}, {"type": "equation", "text": "$$\nd e t(W_{s,t})=1+\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}t r(A_{\\tau}^{i})d e t(W_{s,t})d\\omega_{\\tau}^{i}=\\exp\\left(\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}t r(A_{\\tau}^{i})d\\omega_{\\tau}^{i}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. This just follows from the classical case since we can write ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\int_{\\tau=s}^{t}A_{\\tau}^{i}W_{s,\\tau}d\\omega_{\\tau}^{i}=\\int_{\\tau=s}^{t}\\left(\\sum_{i=1}^{d}A_{\\tau}^{i}\\dot{\\omega}_{\\tau}^{i}\\right)W_{s,\\tau}d\\tau\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can now state the main result of the section: ", "page_idx": 37}, {"type": "text", "text": "Theorem E.6. Under the assumptions of the previous theorems, given continuous functions $\\{B^{1},\\ldots,B^{t}\\}\\in(\\mathbb{R}^{d})^{[0,1]}$ the unique solution of the $N$ -dimensional CDE ", "page_idx": 37}, {"type": "equation", "text": "$$\nX_{t}=X_{0}+\\sum_{i=1}^{d}\\int_{\\tau=0}^{t}\\left(A_{\\tau}^{i}X_{\\tau}+B_{\\tau}^{i}\\right)d\\omega_{\\tau}^{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "is given explicitly by ", "page_idx": 37}, {"type": "equation", "text": "$$\nX_{t}=W_{0,t}X_{0}+\\sum_{i=1}^{d}\\int_{0}^{t}W_{s,t}B_{s}^{i}d\\omega_{s}^{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $W_{s,t}\\in C^{0}([0,1]\\times[0,1];\\mathbb{R}^{N\\times N})$ is the Wronskian matrix defined by ", "page_idx": 37}, {"type": "equation", "text": "$$\nW_{s,t}=\\sum_{I\\in\\mathcal{W}_{d}}\\int_{\\tau\\in\\Delta_{[s,t]}^{|I|}}A_{\\tau}^{I}d\\omega_{\\tau}^{I}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Given the unique solution $X_{t}$ one has ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{s}(W_{s,t}X_{s})\\mathop{=}\\mathop{d_{s}}(W_{s,t})X_{s}+W_{s,t}d_{s}(X_{s})}\\\\ &{\\qquad\\qquad\\qquad\\mathop{=}_{i=1}^{d}\\left(-W_{s,t}A_{s}^{i}X_{s}+W_{s,t}A_{s}^{i}X_{s}+W_{s,t}B_{s}^{i}\\right)d\\omega_{s}^{i}}\\\\ &{\\qquad\\qquad\\qquad\\mathop{=}_{i=1}^{d}W_{s,t}B_{s}^{i}d\\omega_{s}^{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "hence ", "page_idx": 38}, {"type": "equation", "text": "$$\nX_{t}-W_{0,t}X_{0}=W_{t,t}X_{t}-W_{0,t}X_{0}=\\sum_{i=1}^{d}\\int_{s=0}^{t}W_{s,t}B_{s}^{i}d\\omega_{s}^{i}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "F ZOH and Exact Solutions ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Consider a Linear CDE as the one of (25) ", "page_idx": 39}, {"type": "equation", "text": "$$\nd Z_{t}=\\sum_{i=1}^{d_{\\omega}}A_{i}Z_{t}d\\omega_{t}^{i}+B d\\xi_{t}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and recall how the solution can be explicitly written, for times $s<t$ , as ", "page_idx": 39}, {"type": "equation", "text": "$$\nZ_{t}=W_{s,t}Z_{s}+\\int_{s}^{t}W_{r,t}B d\\xi_{r}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Assume moreover that in the interval $[s,t]$ both drivers have constant derivative i.e. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\omega_{r}=\\omega_{s}+\\pmb{w}(r-s)\\quad\\xi_{r}=\\xi_{s}+\\pmb{v}(r-s)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then if $\\begin{array}{r}{\\mathbb{A}_{\\pmb{w}}:=\\sum_{i=1}^{d_{\\omega}}A_{i}\\pmb{w}^{i}}\\end{array}$ we get that $W_{r,t}=e^{\\mathbb{A}_{w}(t-r)}$ thus ", "page_idx": 39}, {"type": "equation", "text": "$$\nZ_{t}=e^{\\mathbb{A}_{w}(t-s)}Z_{s}+\\int_{s}^{t}e^{\\mathbb{A}_{w}(t-r)}B v d r=e^{\\mathbb{A}_{w}(t-s)}Z_{s}+\\left(\\int_{s}^{t}e^{\\mathbb{A}_{w}(t-r)}d r\\right)B v\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "But the integral can be explicitly solved as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\int_{s}^{t}e^{\\mathbb{A}_{w}\\left(t-r\\right)}d r=\\left(-\\mathbb{A}_{w}^{-1}e^{\\mathbb{A}_{w}\\left(t-r\\right)}\\Big|_{r=s}^{t}=\\mathbb{A}_{w}^{-1}\\left(e^{\\mathbb{A}_{w}\\left(t-s\\right)}-\\mathbb{I}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "leaving us with ", "page_idx": 39}, {"type": "equation", "text": "$$\nZ_{t}=e^{\\mathbb{A}_{w}(t-s)}Z_{s}+\\mathbb{A}_{w}^{-1}\\left(e^{\\mathbb{A}_{w}(t-s)}-\\mathbb{I}\\right)B v\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which, setting $\\Delta=t-s$ , can be rewritten as ", "page_idx": 39}, {"type": "equation", "text": "$$\nZ_{t}=e^{\\mathbb{A}_{w}\\Delta}Z_{s}+(\\mathbb{A}_{w}\\Delta)^{-1}\\left(e^{\\mathbb{A}_{w}\\Delta}-\\mathbb{I}\\right)(B\\Delta)v\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "i.e. exactly the ZOH scheme. ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 40}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 40}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 40}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 40}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 40}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 40}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: In the Introduction we list our contributions and refer, point by point, to the relevant sections of our work. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have discussed some limitations in the conclusion section. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: All the proofs are found in the Appendices, the specific sections of interest are referred after the statements. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 41}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: Code to reproduce experiments included in supplementary material. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The code to generate the data used and to run the experiments are included in the supplementary material alongside a README explaining how to run the experiments. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 42}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Details can be found in the section titled empirical validation, with additional details for reproducing the results included in the supplementary material. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Details included in the empirical validation section. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 43}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: This is a theoretical work. We provide information sufficient for the reproduction of empirical results. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 45}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 45}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This is a theoretical work, not involving crowdsourcing nor research with human subjects. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: This is a theoretical work, not involving crowdsourcing nor research with human subjects. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]