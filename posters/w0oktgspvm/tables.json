[{"figure_path": "W0okTgsPvM/tables/tables_6_1.jpg", "caption": "Table 1: Per Image Embedding Token Length and Total Context Length for Models", "description": "This table shows the number of tokens used to embed a single image and the total context length supported by four different large multimodal models: VILA-1.5-8B, Idefics2-8B, QwenVL, and MANTIS-LLaMA3-8B.  The token length per image embedding varies significantly across the models, ranging from 64 tokens to 256 tokens. All models have a total context length of 8192 tokens.", "section": "4.2 Models"}, {"figure_path": "W0okTgsPvM/tables/tables_7_1.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the results of the Multimodal Task Vector (MTV) method and compares it with several baseline models on two types of tasks: Visual Question Answering (VQA) and object classification. The left half shows the performance on three different VQA datasets (VizWiz, OK-VQA), while the right half shows the results for object classification on two datasets (Flowers, CUB).  The table compares the performance of MTV with different numbers of shots in in-context learning (ICL) and a zero-shot setting. The baselines are shown in gray for easy comparison.  Each row represents a different model, while the columns indicate the datasets and the performance metrics.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_8_1.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the performance comparison of the proposed Multimodal Task Vectors (MTV) method against several baseline models on two different types of tasks: Visual Question Answering (VQA) and object classification.  The left half shows the results for three VQA datasets (VizWiz, OK-VQA), comparing the performance of different models (Flamingo, BLIP, QwenVL, Idefics, ViLA) with and without the MTV method and varying numbers of shots in in-context learning (ICL). The right half shows results for object classification datasets (Flowers, CUB), again comparing MTV against several models with and without ICL, demonstrating MTV's ability to scale to many shots and generalize to unseen tasks.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_8_2.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the performance comparison of Multimodal Task Vectors (MTV) against various baselines on two different types of tasks: Visual Question Answering (VQA) and object classification.  The left side shows the results for three different VQA datasets (VizWiz, OK-VQA), comparing MTV's performance against several models including  4-shot, 8-shot ICL and zero-shot. The right side presents the results for object classification datasets (Flowers, CUB),  again showing a comparison against different models using MTV and few-shot ICL methods. The baselines are highlighted in gray for easier comparison.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_9_1.jpg", "caption": "Table 4: Efficiency: We show that even though MTV encodes 400 multimodal ICL examples in the mean activations, it still requires less runtime and memory than 8-shot and 16-shot multimodal ICL.", "description": "This table presents a comparison of the maximum GPU memory usage and runtime per 100 iterations for different methods: 0-shot, 4-shot, 8-shot, 16-shot ICL, and the proposed MTV method with 400 shots.  It demonstrates that MTV, despite encoding significantly more examples (400), achieves lower memory usage and runtime than the other ICL methods.", "section": "5.6 Compute and runtime efficiency"}, {"figure_path": "W0okTgsPvM/tables/tables_16_1.jpg", "caption": "Table 5: ICL Degradation, Shot-Quality Impact, and Stability (Left) Degradation of ICL with increasing number of shots. (Right) Impact of shot quality on MTV and stability of ICL vs MTV with noisy examples.", "description": "This table presents a comparison of different experimental setups to highlight the impact of shot quality and the robustness of MTV to noisy examples.  It shows how accuracy changes with the number of in-context learning (ICL) shots used, demonstrating the diminishing returns of simply increasing the number of shots.  It also shows that using higher quality shots improves the effectiveness of MTV, and demonstrates the relative stability of the MTV approach compared to standard ICL when noisy data is included.", "section": "A Additional Experiment Results"}, {"figure_path": "W0okTgsPvM/tables/tables_16_2.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the results of the Multimodal Task Vector (MTV) method compared to baselines (shown in gray) on various vision and language tasks. The left side shows results on Visual Question Answering (VQA) datasets (VizWiz and OK-VQA), while the right side shows results on object classification datasets (Flowers and CUB). For each dataset and model, multiple baselines and MTV methods are compared, demonstrating the effectiveness of the MTV approach in improving accuracy across different tasks.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_17_1.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the results of the Multimodal Task Vector (MTV) method and several baselines on various vision-and-language tasks.  The left side shows results for Visual Question Answering (VQA) on three different datasets (VizWiz, OK-VQA), comparing MTV's performance against baseline methods (few-shot ICL) and other models like Flamingo. The right side shows results for object classification on the Flowers and CUB datasets, again comparing MTV to baselines. Gray shading indicates baseline performance for comparison.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_18_1.jpg", "caption": "Table 2: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object classification datasets. The baselines are shown in gray.", "description": "This table presents the results of the Multimodal Task Vector (MTV) method and compares it to various baselines (few-shot ICL methods) on two different types of tasks: Visual Question Answering (VQA) and Object Classification. The left side shows the performance of MTV on three different VQA datasets (VizWiz, OK-VQA), while the right side shows the performance on two object classification datasets (Flowers, CUB).  The results are presented for different models and different numbers of shots used in the baseline few-shot ICL methods. The grayed-out rows represent the baseline results without MTV.", "section": "5 Results"}, {"figure_path": "W0okTgsPvM/tables/tables_18_2.jpg", "caption": "Table 8: Performance on Language and Document Tasks (Left) Evaluation on English-Spanish and Antonym Generation tasks. (Right) MTV performance across different shot settings on document tasks.", "description": "This table presents the results of applying the Multimodal Task Vector (MTV) method and standard few-shot in-context learning (ICL) to language-only tasks. The left side shows the performance on English-Spanish translation and antonym generation tasks, while the right side shows the performance on document tasks using different numbers of shots (0-shot, 4-shot, 8-shot, and MTV 4-100).  The results demonstrate the effectiveness of MTV in improving performance on these tasks compared to traditional few-shot ICL.", "section": "5.3 MTV for language-only tasks"}, {"figure_path": "W0okTgsPvM/tables/tables_18_3.jpg", "caption": "Table 8: MTV Document Task Performance", "description": "This table presents the performance of Multimodal Task Vectors (MTV) on document-related tasks, comparing it against various few-shot settings (0-shot, 4-shot, 8-shot) for two specific tasks: ChartQA and TextVQA.  The results highlight MTV's ability to improve performance compared to standard few-shot methods by encoding many-shot examples implicitly in the model's activations.", "section": "5.3 MTV for language-only tasks"}]