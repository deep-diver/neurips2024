{"references": [{"fullname_first_author": "M. Arjovsky", "paper_title": "Invariant risk minimization", "publication_date": "2019-07-02", "reason": "This paper introduces a fundamental concept for domain generalization, focusing on minimizing risk across different domains to improve generalization."}, {"fullname_first_author": "K. Muandet", "paper_title": "Domain generalization via invariant feature representation", "publication_date": "2013-01-01", "reason": "This paper lays groundwork for domain generalization by proposing to learn invariant features that generalize well across different domains."}, {"fullname_first_author": "K. He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-01-01", "reason": "The deep residual network architecture introduced in this paper is foundational and highly relevant to many modern deep learning models, including those used in domain generalization."}, {"fullname_first_author": "J. Kirkpatrick", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2016-12-01", "reason": "This paper addresses the catastrophic forgetting problem in continual learning, relevant to the evolving domain generalization setting, where learning new domains should not cause forgetting of previously learned domains."}, {"fullname_first_author": "F. Zenke", "paper_title": "Continual learning through synaptic intelligence", "publication_date": "2017-01-01", "reason": "This paper proposes a method for continual learning based on synaptic intelligence, which offers insights for designing models that adapt gracefully to evolving data distributions in the domain generalization context."}]}