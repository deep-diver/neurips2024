[{"type": "text", "text": "Distributed Lion for Communication Efficient Distributed Training ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "wDirCeTIoz/tmp/bd169c1c227678acbc8f3a7ffe759ac907c8bf3fdf1e1ae157d08d165f6ff259.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or Adam W optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that Distributed Lion presents a more favorable performancebandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The pursuit of modern artificial intelligence hinges on the training of large-scale models like large language models[28] and large vision models (LVM)[20]. As the stakes - in terms of time, cost, and environmental impact - grow ever higher for training expansive AI systems, the hunt for efficient optimizers becomes critical. ", "page_idx": 0}, {"type": "text", "text": "Recently, a new optimization named Lion (evolved sign momentum) [11] has been discovered with an evolutionary program. It was shown that it exhibits performance on par with the current state-ofthe-art AdamW [26] across a wide range of tasks, while reducing the memory cost and training time. ", "page_idx": 0}, {"type": "image", "img_path": "wDirCeTIoz/tmp/dc9d8c57419a80a9dc1faeb1aef41a6be9ceac5648e4b212a0293bef4b3825c9.jpg", "img_caption": ["Figure 1: Illustration of Distributed-Lion. Each worker keeps its own optimizer state and applies the Lion optimizer individually to a binary update $\\delta_{i,t}\\,=\\,\\mathtt{L i o n}(x,\\mathscr{D}_{i})$ (without the weight decay), then the server aggregates all $\\delta_{i,t}$ to produce a binary $\\Delta_{t}$ by majority vote (or an integer $\\Delta_{t}$ by averaging) and send it back to all workers. The workers then apply $\\Delta_{t}$ and weight decay to update their model parameters (Algorithm 1). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Consider optimizing a loss function $f_{\\cal D}(x)$ on $\\mathbb{R}^{d}$ with a dataset $\\mathcal{D}$ , the update rule of Lion is: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{t+1}=\\beta_{2}m_{t}+(1-\\beta_{2})\\nabla f_{\\mathcal{D}}(x_{t}),}\\\\ &{\\delta_{t}=\\mathtt{L i o n}(x_{t},\\mathcal{D})\\overset{d e f}{=}\\mathrm{sign}(\\beta_{1}m_{t}+(1-\\beta_{1})\\nabla f_{\\mathcal{D}}(x_{t})),}\\\\ &{x_{t+1}=x_{t}-\\epsilon\\big(\\delta_{t}+\\lambda x_{t}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $m_{t}$ plays the role of the momentum, $\\epsilon$ is the learning rate, $\\beta_{1},\\beta_{2}\\in[0,1]^{2}$ are two momentum related coefficients, and $\\lambda\\geq0$ is the weight decay coefficient. Comparing Lion against AdamW, one observes that Lion only requires the storage of the first-order momentum term, which results in a more relaxed memory requirement. ", "page_idx": 1}, {"type": "text", "text": "In this study, we tailor the Lion optimizer for distributed training. The Lion optimizer is particularly suitable for this context due to two main attributes: (1) its simple update mechanism that relies solely on first-order momentum, and (2) its use of the $\\mathrm{{sign}(\\cdot)}$ function. We showcase the effective employmentofthe $\\mathrm{sign}(\\cdot)$ function to streamline communication processes, leading to the development of a novel distributed training framework named Distributed Lion. Within the Distributed Lion framework, each participating worker independently adjusts the model parameters using a distinct instance of the Lion optimizer, thereby maintaining separate optimizer states. A distinctive feature of this framework is the mode of communication between workers and the central server, which is restricted to binary orlow-precisionvectors. ", "page_idx": 1}, {"type": "text", "text": "Crucially, in this setup, workers convey updates rather than raw gradients to the central server. The server, in turn, aggregates these updates through either a straightforward averaging process (Distributed Lion-Avg) or a majority voting mechanism (Distributed Lion-MaVo). In the case of Distributed Lion-MaVo, the consolidated update is maintained as a binary vector, whereas for Distributed Lion-Avg, given the presence of $n$ workers, each element of the update vector is encoded using $\\log(n)$ bits. This approach markedly reduces the bandwidth requirements compared to traditional distributed training methods, which typically rely on high-precision foating-point vectors for communication. The bandwidth efficiencies achieved by our method are detailed in Table 1. Our contributions are: 1) We introduce the Distributed Lion algorithm, a simple yet effective approach to extend Lion to distributed training, where all communications between workers and the server are done through binary or low-precision vectors (Section 2); 2) We provide theoretical analysis to ensure the convergence of Distributed Lion (Section 3); 3) Empirically, we demonstrate that on both vision and language modeling tasks, Distributed Lion achieves comparable performance against applying Lion and Adam with the synchronized gradients from all workers, while being significantly more communication efficient. In addition, we show that Distributed Lion achieves a better trade-off than existing effcient distributed training methods like deep gradient compression [24] and ternary gradients [36] (Section 5). ", "page_idx": 1}, {"type": "table", "img_path": "wDirCeTIoz/tmp/2302c6886eac2a229c314cfcf588cdf3fb0bb51b4d17792dd6e044eb97dc3c93.jpg", "table_caption": [], "table_footnote": ["Table 1: Minimum bandwidth requirements of different methods for a model with $d$ parameters and $n$ workers. For Deep Gradient Compression (DGC), $\\eta$ denotes the compression rate (default: $\\eta=0.96)$ "], "page_idx": 2}, {"type": "text", "text": "2  The Distributed Lion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce the distributed learning problem and then our Distributed Lion framework. ", "page_idx": 2}, {"type": "text", "text": "2.1  Distributed Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In distributed training, we aim to minimize the following learning objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}F(x)=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\bigg[f(x;\\xi_{i})\\bigg].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $N$ denotes the number of workers, $\\{\\mathcal{D}_{i}\\}$ are $N$ datasets,3 and $x$ is the model parameter (e.g.. the weights of a neural network). In the distributed learning setting, each worker $i\\in[n]$ will get its own dataset $\\mathcal{D}_{i}$ , and we assume there is a centralized server that all workers can communicate with. The simplest distributed training technique is to perform distributed gradient aggregation: ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{\\mathrm{server}}=\\frac{1}{N}\\sum_{i=1}^{N}g_{i},\\ \\ \\mathrm{where}\\ \\ g_{i}=\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\big[\\nabla_{x}f(x;\\xi_{i})\\big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, each local gradient $g_{i}$ is an unbiased estimation of the true gradient $\\nabla_{x}F(x)$ when $\\mathcal{D}_{i}$ are i.i.d. drawn from the same underlying distribution. The server aggregates all local gradients into $g_{\\mathrm{t}}$ sever, and then applies an optimizer like Adam [19] on top of $g_{\\mathrm{server}}$ . However, the aggregation step requires communicating the full gradient vectors $g_{i}$ , which can be expensive for large models. ", "page_idx": 2}, {"type": "text", "text": "Notation.  Given a function $f(x;\\xi)$ , the gradient $\\nabla f(x;\\xi)$ is taken with respect to variable $x$ .We use $\\|\\cdot\\|,\\|\\cdot\\|_{1}$ , and $\\|\\cdot\\|_{\\infty}$ to denote the $\\ell_{2},\\ell_{1}$ , and $\\ell_{\\infty}$ norm, respectively. $\\xi_{i,t}$ is the sampled data at time $t$ for the $i$ -th worker and $g_{i,t}=\\nabla f(x_{t};,\\xi_{i,t})$ . We similarly denote $z_{i,t}$ as any variable $z$ at time $t$ from worker $i$ ", "page_idx": 2}, {"type": "text", "text": "2.2  Distributed Lion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The main idea of Distributed Lion is to leverage the binary nature of the Lion's update for efficient communication. To enable that, we want the workers to only send the binary updates to the server. As a result, we let each worker keep tracks of its own optimizer state, i.e., the momentum $m_{i,t}$ .Then at each step, each worker $i$ firstcomputes: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{i,t+1}=\\beta_{2}m_{i,t}+(1-\\beta_{2})g_{i,t},}\\\\ {\\delta_{i,t}=\\mathrm{sign}(\\beta_{1}m_{i,t}+(1-\\beta_{1})g_{i,t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then all workerssend the $\\delta_{i,t}$ back to the server. The server receives the binary \u201cupdates\" from all workers and then aggregates them. Here, we propose two simple ways for aggregation. Denote $\\begin{array}{r}{S_{t}=\\sum_{i=1}^{N}\\delta_{i,t}}\\end{array}$ which is a vector of integers in $\\{0,\\ldots N\\}$ .Define the aggregation as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta_{t}=\\operatorname{aggregate}(S_{t})={\\binom{\\frac{1}{N}S_{t}}{\\mathrm{sign}(S_{t})}}\\quad{\\mathrm{(Averaging)}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Distributed Lion Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inputs: Initial parameters $x_{0}\\,\\in\\,\\mathbb{R}^{d}$ , datasets $\\{D_{1},...,D_{N}\\}$ , loss function $f$ , learning rate $\\epsilon$ hyper-parameters $\\beta_{1},\\beta_{2}\\in[0,1]$ (default to $0.9,0.99)^{4}$ , and the weight decay $\\lambda$ ", "page_idx": 3}, {"type": "text", "text": "Initialization: $t=0,\\ \\forall i,m_{i,0}=\\mathbf{0}$ , and $x_{i,0}=x_{0}$   \nwhile not convergent do Worker-side: Each worker $i$ samples a batch $\\xi_{i,t}\\in D_{i}$ , computes the following, and sends $\\delta_{i,t}$ to the server: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{if}\\,t>0,\\,\\,\\,x_{i,t}\\leftarrow x_{i,t-1}-\\epsilon\\big(\\Delta_{t-1}+\\lambda x_{i,t-1}\\big)}\\\\ &{\\qquad\\qquad\\quad\\delta_{i,t}\\leftarrow\\mathrm{sign}\\big(\\beta_{1}m_{i,t}+(1-\\beta_{1})\\nabla_{x}f(x_{i,t};\\xi_{i,t})\\big)}\\\\ &{\\qquad\\quad m_{i,t+1}\\leftarrow\\beta_{2}m_{i,t}+(1-\\beta_{2})\\nabla_{x}f(x_{i,t};\\xi_{i,t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Server-side: The server computes the aggregated update $\\Delta_{t}$ and broadcast it to all workers: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{t}=\\left\\{\\!\\!\\begin{array}{l l}{\\frac{1}{N}\\!\\left(\\sum_{i=1}^{N}\\delta_{i,t}\\right)\\!\\!\\!\\!}&{\\!\\!\\!(\\mathrm{Averaging})}\\\\ {\\mathrm{sign}\\!\\left(\\sum_{i=1}^{N}\\delta_{i,t}\\right)\\!\\!\\!\\!}&{\\!\\!\\!(\\mathrm{Majority\\;Vote})}\\end{array}\\right.\\;\\mathrm{and}\\;\\;\\;t\\gets t+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "end while ", "page_idx": 3}, {"type": "text", "text": "So we simply average or take the majority vote from all $\\{\\delta_{i,t}\\}$ . Here, we denote binary vectors in magenta and low precision vectors in cyan. In the end, the server broadcasts $\\Delta_{t}$ back to each worker $i$ , and each worker performs $x_{i,t+1}=x_{i,t}-\\epsilon(\\Delta_{t}+\\lambda x_{i,t})$ , where $\\epsilon$ is the step size and $\\lambda$ is the weight decay coefficient. ", "page_idx": 3}, {"type": "text", "text": "Communication Cost  In both variants of Distributed Lion, the $N$ workers only need to send the binary vectors $\\delta_{i,t}$ to the server. The server then sends the aggregated update $\\Delta_{t}$ back to the workers, which is binary when using the majority vote aggregation, and an integer in $\\{0,\\ldots,N\\}$ when using the averaging aggregation. Note that an integer in $\\{0,\\ldots,N\\}$ can be represented by at most $\\log(N)$ bits. In practice, usually $N\\ll2^{32}$ hence $\\log(N)<32$ and we still save the communication bandwidth even with the average aggregation, comparing against communicating with foating point numbers (Check Table 1). The full Distributed Lion algorithm is summarized in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "3 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We provide our theoretical analysis of the Distributed Lion algorithm, both with the averaging and the majority vote aggregation methods. In the following, we first describe that the distributed training problem can be viewed as a constrained optimization problem when Distributed Lion is used. We provide convergence results for Distributed Lion with both aggregation methods. ", "page_idx": 3}, {"type": "text", "text": "3.1 Lion as Constrained Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Chen et al. [10] showed that the (global) Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a box-constrained optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}f(x)\\quad s.t.\\quad\\|\\lambda x\\|_{\\infty}\\leq1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the constraint is introduced due to the use of the weight decay coefficient $\\lambda$ .Moreover, Chen et al. [10] showed that the Lion dynamics consists of two phases: ", "page_idx": 3}, {"type": "text", "text": "1) [Phase 1] When the constraint is not satisfied, that is, $x\\ \\notin\\ {\\mathcal{F}}$ \uff0cwhere $\\mathcal{F}$ is the feasible set $\\mathcal{F}\\overset{d e f}{=}\\{x\\colon\\left\\|\\lambda x\\right\\|_{\\infty}\\leq1\\}$ $\\mathcal{F}$ $\\alpha\\in(0,1)$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{dist}(x_{t+n},\\mathcal{F})\\leq\\alpha^{n}\\operatorname{dist}(x_{t},\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $n\\geq0$ . Hence, $x_{t}$ converges to $\\mathcal{F}$ rapidly and stays within $\\mathcal{F}$ once it reaches it. ", "page_idx": 3}, {"type": "text", "text": "2) [Phase 2] After $\\lambda x_{t}$ enters $\\mathcal{F}$ , the dynamics minimizes the objective $f(x)$ while being confined within the set $\\mathcal{F}$ . This step is proved in [10] by constructing a Lyapunov function when $\\mathrm{{sign}(\\cdot)}$ is treated as the sub-gradient of a convex function. ", "page_idx": 3}, {"type": "text", "text": "3.2  Convergence Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we analyze the convergence of distributed Lion algorithms. Similar to the case of global Lion, we show that distributed Lion also solves the box constrained optimization (6). Its dynamics also unfolds into two phases aligning with Lion's dynamics: Phase I shows rapid convergence to a feasible set $\\mathcal{F}$ , while Phase I seeks to minize the objective $f(x)$ within the feasible set $\\mathcal{F}$ . Different from the Lyapunov approach used in Chen et al. [10], the proof of our Phase II result is made by introducing a surrogate metric $S(x)$ of constrained optimality, and providing upper bound of ${\\cal S}(x_{t})$ following the algorithm. Our analysis makes the following assumptions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 (Variance bound). $\\mathcal{D}_{i}$ is i.i.d.drawn from a common distribution $\\pi_{*}$ ,andthe stochastic sample $\\xi^{i}\\,\\sim\\,\\mathcal{D}_{i}$ is i.i.d. and upon receiving. query $x\\,\\in\\,\\mathbb{R}^{d}$ ,the stochastic gradient oracle gives us an independent unbiased estimate $\\nabla f(x;\\xi^{i})$ from the $i$ -th worker that has coordinate bounded variance: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}[\\nabla f(x;\\xi^{i})]=\\nabla f(x),\\quad\\mathbb{E}_{\\xi}\\left[\\|\\nabla f(x;\\xi^{i})-\\nabla f(x)\\|^{2}\\right]\\le\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 (Smooth and Differentiable $f$ .Function $f(\\cdot)$ is differentiable and $L$ smooth. ", "page_idx": 4}, {"type": "text", "text": "Assumption 33 (Bias Correction). Consider the sequence $\\{m_{t}^{i}\\}_{t>0,i\\in[N]}$ generated by Algorithm $^{\\,l}$ $\\mathbb{E}[\\tilde{m}_{t}^{i}]/\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t}^{i})]\\ge0.$ ", "page_idx": 4}, {"type": "text", "text": "Note that assumption 3.1 and 3.2 are standard in the analysis of stochastic optimization algorithms [8, 341. When Assumption 3.1 holds, $\\begin{array}{r}{\\mathbb{E}\\|\\frac{1}{N}\\sum_{i=1}^{N}\\nabla f(x;\\xi_{i})-\\nabla f(x)\\|^{2}\\leq\\sigma^{2}/N}\\end{array}$ Indistributedtraining setting, $m_{1,t},m_{2,t},\\cdot\\cdot\\cdot,m_{N,t}$ are i.i.d., so $\\mathbb{E}[\\beta_{1}m_{i,t}+(1-\\beta_{1})g_{i,t}]$ and $\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]$ don't depend on $i$ . Assumption 3.3 evaluates the discrepancy between the expected value and the expected sign of a measure, positing that the expected values of $\\tilde{m}_{t}^{i}$ and $\\mathrm{sign}(m_{t}^{i})$ ought to share the same sign. ", "page_idx": 4}, {"type": "text", "text": "We now present our results. Similar to the case of global Lion, the dynamics of distributed lion can also be divided into two phases depending on if the constraint $x\\in{\\mathcal{F}}$ is satisfied. ", "page_idx": 4}, {"type": "text", "text": "Phase I $(x\\not\\in{\\mathcal{F}})$ In line with the behavior observed in the global Lion, when the constraint is not satisfied, both variants of distributed Lion decrease the distance to the feasible set exponentially fast. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4 (Phase I). Assume $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ .s $L$ -smooth, $\\beta_{1},\\beta_{2}\\in(0,1)$ and $\\beta_{2}>\\beta_{1}$ and $\\epsilon,\\lambda>0$ Let $(x_{t})_{t\\geq0}$ be generated by Algorithm $^{\\,l}$ .Define $\\mathcal{F}\\,=\\,\\left\\{x\\colon\\,\\|\\lambda x\\|_{\\infty}\\,\\leq\\,1\\right\\}$ ,and $\\mathrm{dist}(x_{t},{\\mathcal F})\\,=$ $\\operatorname*{inf}_{z\\in{\\mathcal{F}}}\\|\\bar{z}-x_{t}\\|$ w.rt. any norm $\\lVert\\cdot\\rVert$ For any two non-negative integers $s\\leq t,$ then $\\forall s\\leq t$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{dist}(x_{t},\\mathcal{F})\\leq(1-\\epsilon\\lambda)^{t-s}\\mathrm{dist}(x_{s},\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence, $x_{t}$ converges to $\\mathcal{F}$ rapidly and stays within $\\mathcal{F}$ once it arrived. ", "page_idx": 4}, {"type": "text", "text": "Phase II $(x\\in{\\mathcal{F}})$ Now, we present the main result of the analysis for Phase II in Theorems 3.6, 3.7, and 3.8. We start with introducing a surrogate metric that quantifies the optimality of the solution within Phase II: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(x):=\\langle\\nabla f(x),\\mathrm{sign}(\\nabla f(x))+\\lambda x\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let's delve into the implications of ${\\cal S}(x)=0$ ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.5. Assume $f$ is continuously differentiable, $\\lambda>0,$ and $\\|\\lambda x\\|_{\\infty}\\leq1$ Then ${\\cal S}(x)=0$ implies a KKT stationary condition of $\\operatorname*{min}_{x}f(x)$ s.t. $\\|\\lambda x\\|_{\\infty}\\leq1$ ", "page_idx": 4}, {"type": "text", "text": "This KKT score (7)is tailored to encompass the stationary solutions of the box constrained problem as described in (6). Building on this, we then proceed to analyze the convergence for the majority vote, averaging, and global LION strategies throughout this section. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.6 (Majority Vote). Assumptions 3.1, 3.2, and 3.3 hold, consider the Majority vote scheme inAlgorithm $^{\\,l}$ \uff0c $\\beta_{1},\\beta_{2}\\,\\in\\,(0,1)$ ,and $\\beta_{2}\\,>\\,\\beta_{1}$ and $\\sigma\\,\\leq\\,2{\\sqrt{d}}\\beta_{1}\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|,1\\,\\leq\\,t\\,\\leq\\,T$ ,and $\\epsilon,\\lambda>0.$ Let $(x_{t})_{t\\geq0}$ be generated by Majority Vote, and it is in Phase $I I$ $\\|\\lambda x_{t}\\|_{\\infty}\\leq1$ for all $t$ ", "page_idx": 4}, {"type": "text", "text": "We have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[S(x_{t})]\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2D\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+2\\rho}{\\sqrt{N}}+2L\\epsilon d.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{t}[k]=\\left\\{0\\atop\\mathbb{E}[\\Tilde{m}_{t+1}^{i}[k]]/\\mathbb{E}[\\mathrm{sign}(\\Tilde{m}_{t+1}^{i}[k])]\\right.\\right.\\ \\left.o t\\ \\mathbb{E}[\\mathrm{sign}(\\Tilde{m}_{t+1}^{i}[k])]=0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": ",and $\\rho=\\operatorname*{max}_{1\\leq t\\leq T}\\left\\|\\rho_{t}\\right\\|$ ", "page_idx": 5}, {"type": "text", "text": "The result above shows that $\\textstyle{\\frac{1}{T}}\\sum_{t=1}^{T}\\mathbb{E}[S(x_{t})]$ decays witharate of $\\begin{array}{r}{\\mathcal{O}(\\frac{1}{T\\epsilon}+\\frac{1}{T(1-\\beta_{2})}+\\epsilon+\\frac{1}{\\sqrt{N}})}\\end{array}$ This rate is in fact on par with global Lion as we show in the following result. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.7 (Global). Assumptions 3.1 and 3.2 hold, Consider the scheme in Algorithm (16), with the same settings inTheorem3.6,wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[S(x_{t})]\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2(1-\\beta_{1})\\sqrt{d}\\sigma}{\\sqrt{N}}+2L\\epsilon d.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.8 (Averaging). Assumptions 3.1 and 3.2 hold, consider the Averaging scheme in Algorithm $^{\\,l}$ ,with the same settings in Theorem 3.6,we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[S(x_{t})]\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\beta_{1}\\sqrt{d}\\sigma}{\\sqrt{1+\\beta_{2}}}+2(1-\\beta_{1})\\sqrt{d}\\sigma+2L\\epsilon d.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The Averaging method's convergence bound doesn't improve with more workers since   \n$\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\bar{\\delta}_{i,t})}\\end{array}$ docs'taproximater $\\mathrm{sign}(\\sum_{i=1}^{N}\\delta_{i,t})$   \nproach $\\begin{array}{r}{\\mathrm{sign}(\\sum_{i=1}^{N}\\mathrm{sign}(\\delta_{i,t}))}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a summary of optimizers that use the sign function and existing literature on bandwidth-friendly distributed training. ", "page_idx": 5}, {"type": "text", "text": "Sign Operation in Optimization The sign operation is integral to optimization for several reasons. Primarily, it acts as a normalization mechanism by disregarding the magnitude of gradients, thereby equilibrating updates across different dimensions and potentially facilitating the avoidance of saddle points. Additionally, the binary nature of the sign function's output significantly reduces the memory footprint required for storing gradient updates. The concept of sign-based optimization dates back to RProp [30] and has seen renewed interest with the advent of SignSGD and its momentum-enhanced variant, Signum [4]. A more recent advancement is the generalized SignSGD algorithm introduced by [14], which incorporates a preconditioner, making it a superset of SignSGD and akin to Adam in certain aspects. A noteworthy addition to sign-based optimizers is the Lion optimizer, which emerged from evolutionary program search, achieving performance comparable to Adam [19] and AdamW [26] for the first time. Lion distinguishes itself from Signum by employing a different convex combination for outputting local updates, a technique referred to as the double- $\\cdot\\beta$ scheme, reminiscent of Nesterov's momentum update, and encapsulates Signum as a particular case. On the theoretical front, SignSGD and Signum have been shown to exhibit convergence rates comparable to traditional SGD [4]. Recent work by [34] has extended the theoretical understanding by providing a convergence theory that relaxes the requirements for bounded stochastic gradients and enlarged batch sizes. Additionally, Lion has demonstrated its capability in performing constrained optimization under the $\\ell_{\\infty}$ -norm constraint [10]. ", "page_idx": 5}, {"type": "text", "text": "Distributed Training  In addressing the communication constraints of distributed training, the research community has devised several innovative strategies, prominently featuring asynchronous Stochastic Gradient Descent (SGD), gradient quantization, and sparsification techniques. Asynchronous SGD offers a solution by enabling parameter updates immediately after back-propagation, bypassing the need for gradient synchronization, thereby expediting the training process [9, 40, 25]. Li et al. [21] utilizes sketch-based algorithms for lossless data compression [23], achieving an asymptotically optimal compression ratio [22]. However, its applicability is limited to highly sparse gradients, making it orthogonal to our research. In the realm of gradient quantization, methods such as 1-bit SGD [33], QSGD [2], and TernGrad [36] are pivotal. These approaches compact the gradient data, substantially reducing the required communication bandwidth, with 1-bit SGD demonstrating a tenfold acceleration in speech applications and both QsGD and TernGrad confirming the feasibility of quantized training in maintaining convergence. Moreover, gradient sparsification further mitigates the communication load by transmitting only the most substantial gradients. Techniques like threshold quantization and Gradient Dropping [1] exemplify this, with Gradient Dropping notably achieving a 99 reduction in gradient exchange with minimal impact on performance metrics, such as a mere 0.3 loss in BLEU score for machine translation tasks. The recent Deep Gradient Compression (DGC) strategy [24] also contributes to this field by incorporating momentum correction and local gradient clipping among other methods to maintain accuracy while significantly reducing communication demands, albeit at the cost of increased computational overhead. Compared to gradient quantization methods, Distributed Lion uniquely leverages the binary nature of Lion's update and can be viewed as performing quantization on updates rather than the gradient. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we perform a thorough evaluation of the Distributed Lion algorithm, employing both the averaging and majority vote aggregation methods. The design of our experiments is aimed at addressing the following questions to ascertain the algorithm's efficacy and performance: ", "page_idx": 6}, {"type": "text", "text": "(Q1) How does Mavolion perform in comparison to traditional global distributed training methods, which aggregate gradients from local workers to apply an optimizer to the collective gradient? ", "page_idx": 6}, {"type": "text", "text": "(Q2) How does Mavolion measure up against established methodologies known for their communication efficiency in distributed training? ", "page_idx": 6}, {"type": "text", "text": "(Q3) How does Distributed Lion scale on large vision or language problems? ", "page_idx": 6}, {"type": "text", "text": "5.1 Comparing Distributed Lion Against Established Methods on CIFAR-10 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To address Q1 and Q2, we compare Distributed Lion with both the averaging and the majority vote methods, against established low-bandwidth distributed training techniques and the global distributed training methods. We consider the following baseline methods: 1) Global AdamW (G-AdamW), where we apply Adam W with the averaged gradients from all workers. 2) Global Lion (G-Lion), where we apply Lion with the averaged gradients from all workers. Note that Global Adam W and Global Lion serve as the performance and communication upper bounds. 3) Distributed Lion with Averaged Updates (D-Lion (Avg)), In contrast to the majority vote mechanism used in Distributed Lion, this variant averages the binary update vectors from all workers. While D-Lion (Avg) might offer improved performance in principle, it comes at the cost of non-binary communication from the server to the workers. 4) TernGrad [36]. The main idea is to tenarize the gradient into a vectorof $\\{-1,0,1\\}$ , which is similar to what Lion does. But this process is done on the gradient level instead of on the update level 5) Gradient Dropping (GradDrop) [1]. The main idea is to drop insignificant gradient entries and only transmit sparse gradient signals. 6) Deep Gradient Compression (DGC) [24]. DGC is built on top of the GradDrop, but additionally applies momentum correction, local gradient clipping, momentum factor masking, and warm-up training. ", "page_idx": 6}, {"type": "text", "text": "Experiment Setup For GradDrop, DGC, and TernGrad, we choose the compression rate of 0.04 (note that $1/32=0.03125)$ to match the bandwidth of the D-Lion (MaVo). We conduct experiments on the CIFAR-10 dataset using a vision transformer (ViT) with 6 layers, 8 heads, and a hidden dimension of 512. This is because ViT has arguably become the most widely used architecture in computer vision, and we empirically found no additional gain in performance when using a larger ViT on CIFAR-10. In addition, to validate how Distributed Lion performs with different numbers of workers,weconsider $k\\in\\{4,8,16,32\\}$ , each worker at each step samples an i.i.d batch of size 32. ", "page_idx": 6}, {"type": "text", "text": "We list the optimal hyperparameters selected for each method from Figure 2 in Table 4. The learning rates are selected from $\\{0.00005,0.001,0.005,0.01\\}$ and the weight decays are selected from $\\{0.0005,0.001,0.005\\}$ . For each experiment, we use a cosine learning rate scheduler and run for 200 epochs, and we ensure that in each epoch, each local worker sees the entire dataset once. ", "page_idx": 6}, {"type": "image", "img_path": "wDirCeTIoz/tmp/4ecae86b6e0e2a8a78e16ba3c108df7dc5ada0aee8621114686317f34e02d394.jpg", "img_caption": ["Figure 2: Performance of Distributed Lion v.s. baseline distributed optimizers on CIFAR-10 with 4, 8, 16, and 32 workers, each worker at each step runs on a local batch with size 32. All results are averaged over three seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Each experiments are conducted with three random seeds $\\{42,52,62\\}$ , which results in a total of $4\\times7\\times3=84$ experiments. ", "page_idx": 7}, {"type": "image", "img_path": "wDirCeTIoz/tmp/f9c629934cc8e8744b5555fc8e04bdcefd0490cefb9f4a831e14614ec11ff0eb.jpg", "img_caption": ["Figure 3: Performance of G-Lion, G-AdamW, GradDrop, DGC, TernGrad, and D-Lion (Avg/MaVo) v.s. the number of workers $k$ "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "wDirCeTIoz/tmp/c66ed5c4a39634338614283acc7dc666254c97f3444a6d02cb4ec1a13df24ce7.jpg", "img_caption": ["Figure 4: Test Error v.s. Communication Bits per Iteration (closer to the lower-left is better). Note that we set G-Lion and G-AdamW are both 64, because they require 32 bits per parameter, and there are both workerto-server and server-to-worker communications. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Observation We plot the testing accuracy (Test Acc.) over epochs for different methods in Figure 2, the best testing accuracy of different methods over the number of workers in Figure 3, and the performance versus per-iteration bandwidth in Figure 4 when using $k=4$ workers.From the above plots, we make the following observations. ", "page_idx": 7}, {"type": "text", "text": "\u00b7 Compared to global methods, D-Lion (MaVo) performs on par with G-Lion. D-Lion (Avg) performs slightly worse than G-Lion but is on par with G-Adamw (Figure 2). \u00b7 Compared to established communication efficient methods, both D-Lion $\\mathrm{(MaVo)}$ and D-Lion (Avg) outperform GradDrop, DGC and TernGrad by a large margin (Figure 2). \u00b7 We observe that both D-Lion $\\mathrm{(MaVo)}$ and D-Lion $\\left(\\mathrm{Avg}\\right)$ exhibit strong performance while being $30\\mathrm{x}$ more communication efficient than global distributed training methods like G-Adam W. To broaden our comparison, we introduced two additional baseline methods: DSIGNUM (Avg) and D-SIGNUM (MaVo). These baselines apply our proposed techniques to the SIGNUM framework instead of Lion. We set $\\beta=0.99$ for D-SIGNUM. According to our results, depicted in Figure 4, these SIGNUM-based methods do not perform as well as their Lion-based counterparts. \u00b7 We notice that the overall performance of the same optimizer is worse as $k$ is larger, this is consistent with the observation made in DGC [24]. We hypothesize that this may be due to the larger effective batch size resulting in smaller stochasticity, which is consistent with why D-Lion (MaVo) performs a bit better than G-Lion on CIFAR-10 (Figure 3). ", "page_idx": 7}, {"type": "text", "text": "5.2 Scale to Larger Models on Larger Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer Q3, we validate Distributed Lion on several large-scale setups including both vision and natural language processing tasks. Under this setting, we compare D-Lion (MaVo) and D-Lion (Avg) against G-AdamW and G-Lion. For the vision task, we tested ViT-S/16 [16] and ViT-B/16 on the ImageNet-1K [31] classification benchmark. For the natural language processing task, we perform both language pretraining and finetuning tasks. This is because Lion has shown good results on language modeling. For the language model pretraining task, we pretrain $\\mathrm{GPT}2++$ [29] (the GPT-2 model with modern training techniques adopted from the LLaMA model [35]) on the OpenWebText [17] benchmark, for both 350M and 760M size models. For the language model finetuning task, we conduct few-shot finetuning of the LLaMA 7B model [35] and evaluate the models' downstream performance on standard downstream evaluation benchmarks [13, 37, 12, 27, 7, 32]. ", "page_idx": 8}, {"type": "text", "text": "Experiment Setup  For the ImageNet-1K benchmark, we train all methods for 300 epochs, using a global batch size of 4096 and data augmentations MixUp [39] of 0.5 and AutoAug [15]. When training ViT-S/16, we use a learning rate of $3e^{-3}$ for G-AdamW, with betas of (0.9, 0.999) and a weight decay of 0.1. For G-Lion, D-Lion (MaVo), and D-Lion (Avg), we use a learning rate of $3e^{-4}$ betas of $(0.9,0.99)$ , and a weight decay of 1.0. As for ViT-B/16, we use a learning rate of $1e^{-3}$ for G-AdamW, with betas of (0.9, 0.999) and a weight decay of 1.0, while for all Lion variants, we use a learning rate of $1e^{-4}$ , betas of (0.9, 0.99), and a weight decay of 10.0. For pretraining language models on the Open WebText dataset, we build $\\mathrm{GPT}2++$ models using the original GPT2 model, but with modern training techniques from the LLaMA model, including using the Gated Linear Unit activation for the multilayer layer perceptron layers (MLPs) and the RMSNorm [38] instead of the LayerNorm [3]. Following the Chinchilla scaling law [18], we trained the 350M model for 14,000 iterations and the 760M model for 30,000 iterations, both with 1,024 tokens. For G-AdamW, we use a learning rate of $3e^{-4}$ , betas of (0.95, 0.99), and a weight decay of 0.1. For all Lion variants, we use a learning rate of $9e^{-5}$ , betas of (0.9, 0.99), and a weight decay of 1.0. All the models are trained under a global batch size of 480. For the instruction finetuning task, we instruct finetune a LLaMA 7B model for 3 epochs with batch size 32. We use $2e^{-5}$ learning rate, betas of (0.9, 0.999), 0 weight decay for G-AdamW and $6e^{-6}$ , (0.9, 0.99) betas, 0.01 weight decay for all Lion variants. For all pretraining experiments, we use 4nodes $\\times\\,\\mathrm{8gpus}=32$ workers. For instruction finetuning experiments, we use 4 workers per experiment. ", "page_idx": 8}, {"type": "table", "img_path": "wDirCeTIoz/tmp/e9689bde032e6f657464989d5d80b16fa94c788da44089af11261cfc5ae1ead2.jpg", "table_caption": ["Table 2: Results on ImageNet classification and Open WebText language modeling. For ImageNet experiments, we report the Top-1 accuracy. For language modeling experiments, we report the validation perplexity. The best performance is marked with bold text, and the second best with an underline. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wDirCeTIoz/tmp/0743fb0957a327474a82bb18cb4d0b7470cb36d31eb15842937de561a301646e.jpg", "table_caption": ["Table 3: 3-Shot instruction finetuning downstream evaluation results on various datasets. We mark the best performance with bold text and the second one with an underline. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Observation We summarize the results in Table 2 (ImageNet 1K and Open WebText Language Model Pretraining) and Table 3 (Instruction Finetuning). Both D-Lion (Avg) and D-Lion (MaVo) ", "page_idx": 8}, {"type": "text", "text": "can maintain a performance similar to, or even better than, that of G-Adam W and G-Lion, on both large-scale vision and language tasks. We observe that D-Lion (Avg) outperforms D-Lion (MaVo) on ImageNet, and observe the opposite on language modeling and instruction finetuning. We hypothesize that these differences are due to the impact of global batch size. As a result, we recommend using D-Lion (Avg) / (MaVo) when the global batch size is large / small. ", "page_idx": 9}, {"type": "text", "text": "6  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced Distributed Lion, a communication-efficient distributed training strategy that builds upon the Lion optimizer's binary update mechanism. Distributed Lion is designed to minimize communication overhead by allowing workers to independently manage their optimizer states and exchange only binary or low-precision update vectors with the server. We proposed two aggregation techniques within the Distributed Lion framework: average-based (Distributed Lion Avg) and majority vote-based (Distributed Lion MaVo) algorithms. We provide both theoretical and empirical results to demonstrate Distributed Lion's effectiveness, scalability, and efficiency. Notably, we show that Distributed Lion performs significantly better than existing communication-friendly methods. In the meantime, Distributed Lion demonstrates performance on par with strong global distributed training baselines, while being $32\\mathbf{x}$ more communication efficient. As our method is orthogonal to existing communication-efficient methods, an interesting future direction is to combine both techniques for further improvement. As a limitation, currently Distributed Lion $\\left(\\mathrm{Avg}\\,/\\,\\mathrm{MaVo}\\right)$ performs inconsistently across different datasets and benchmarks, it will be an interesting future research direction to understand when and why one performs better than the other. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research is conducted in Statistics & AI group at UT Austin, which receives supports in part from NSF CAREER1846421, SenSE2037267, Office of Navy Research, and NSF AI Institute for Foundations of Machine Learning (IFML). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021, 2017.   \n[2] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. Advances in neural information processing systems, 30, 2017.   \n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[4] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.  signSGD: Compressed Optimisation for Non-Convex Problems, August 2018. arXiv:1802.04434 [cs, math].   \n[5] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pages 560-569. PMLR, 2018.   \n[6] Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadeneshel, and Anima Anandkumar. signsgd with majority vote is communication effcient and fault tolerant. arXiv preprint arXiv: 1810.05291, 2018.   \n[7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAl conference on artificial intelligence, volume 34, pages 7432-7439, 2020.   \n[8]  Lon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223-311, 2018.   \n[9] Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed synchronous sgd. arXiv preprint arXiv: 1604.00981, 2016.   \n[10] Lizhang Chen, Bo Liu, Kaizhao Liang, and Qiang Liu. Lion secretly solves constrained optimization: As lyapunov predicts. arXiv preprint arXiv:2310.05898, 2023.   \n[11] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023.   \n[12] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019.   \n[13]  Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.   \n[14] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. arXiv preprint arXiv:2208.11195, 2022.   \n[15] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.   \n[16]  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[17]  Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http: //Skylion007 . github . i0/ OpenWebTextCorpus, 2019.   \n[18] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[19]  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whithead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[21] Haoyu Li, Qizhi Chen, Yixin Zhang, Tong Yang, and Bin Cui. Stingy sketch: a sketch framework for accurate and fast frequency estimation. Proceedings of the VLDB Endowment, 15(7):1426-1438, 2022.   \n[22] Haoyu Li, Liuhui Wang, Qizhi Chen, Jianan Ji, Yuhan Wu, Yikai Zhao, Tong Yang, and Aditya Akella. Chainedfilter: Combining membership filters by chain rule. Proceedings of the ACM on Management of Data, 1(4):1-27, 2023.   \n[23] Haoyu Li, Yuchen Xu, Jiayi Chen, Rohit Dwivedula, Wenfei Wu, Keqiang He, Aditya Akella,. and Daehyeok Kim. Accelerating distributed deep learning using lossless homomorphic compression, 2024.   \n[24] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017.   \n[25] Bo Liu, Rachita Chhaparia, Arthur Douillard, Satyen Kale, Andrei A Rusu, Jiajun Shen, Arthur Szlam, and Marc'Aurelio Ranzato. Asynchronous local-sgd training for language modeling. arXiv preprint arXiv:2401.09135, 2024.   \n[26] lya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[27]  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Conference on Empirical Methods in Natural Language Processing, 2018.   \n[28]  OpenA1. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[30]  Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The rprop algorithm. In IEEE international conference on neural networks, pages 586-591.IEEE,1993.   \n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.   \n[32] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv: 1904.09728, 2019.   \n[33] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dns. In Fifteenth annual conference of the international speech communication association, 2014.   \n[34]  Tao Sun, Qingsong Wang, Dongsheng Li, and Bao Wang. Momentum ensures convergence of signsgd under weaker assumptions. In International Conference on Machine Learning, pages 33077-33099. PMLR, 2023.   \n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[36] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. Advances in neural information processing systems, 30, 2017.   \n[37] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[38]  Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32,2019.   \n[39] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \n[40] Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous stochastic gradient descent with delay compensation. In International Conference on Machine Learning, pages 4120-4129. PMLR, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Additional Experiment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide additional experiment details. ", "page_idx": 13}, {"type": "text", "text": "CIFAR Experiments We list the optimal hyperparameters selected for each method from Figure 2 in Table 4. The learning rates are selected from $\\{0.00005,0.001,0.005,0.01\\}$ and theweight decays are selected from $\\left\\lbrace0.0005,0.001,0.005\\right\\rbrace$ . For each experiment, we use a cosine learning rate scheduler and run for 200 epochs, and we ensure that in each epoch, each local worker sees the entire dataset once. ", "page_idx": 13}, {"type": "table", "img_path": "wDirCeTIoz/tmp/f0a61cc354699c7e0f215f0504c44db02efbdbb82d17b4bf2ba0116d3c6bcdf3.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameters for each method in Figure 2. Where LR represents learning rate and wD represents weight decay. "], "page_idx": 13}, {"type": "text", "text": "B Theory ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section is focusing on the proof of Lion dynamics, and will be organized into these folders: ", "page_idx": 13}, {"type": "text", "text": "\u00b7 Phase I: Constraint enforcing: Discrete time ", "page_idx": 13}, {"type": "text", "text": "\u00b7 Phase II: ", "page_idx": 13}, {"type": "text", "text": "- Majority Voting convergence - Avg update convergence - Global LION convergence ", "page_idx": 13}, {"type": "text", "text": "In line with the behavior observed in the global Lion approach, Lion under a distributed setting also exhibits the two phases. In Section B.1, we show that converging to box can be exponentially fast using our Algorithm 1. We start with introducing a notion of KKT score function that quantifies a stationary solution to the box constrained optimization problem (6) in Section B.2. Building on this, we then proceed to analyze the convergence in terms of the KKT score function for the majority vote (Section B.2.1), averaging (Section B.2.2), and global LION strategies (Section B.2.3). ", "page_idx": 13}, {"type": "text", "text": "B.1  Phase I: Constraint Enforcing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We study phase I in this section. We show that when the constraint is not satisfied, both variants of distributed Lion decrease the distance to the feasible set exponentially fast. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1 (Phase I). Assume $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ $L$ -smooth, $\\beta_{1},\\beta_{2}\\in(0,1)$ and $\\beta_{2}>\\beta_{1}$ and $\\epsilon,\\lambda>0$ and $1-\\epsilon\\lambda\\in(0,1)$ Let $(x_{t})_{t\\geq0}$ be generated by Algorithm 1. Define $\\mathcal{F}=\\{x\\colon\\left\\|\\lambda x\\right\\|_{\\infty}\\leq1\\}$ , and $\\mathrm{dist}(x_{t},\\mathcal{F})=\\operatorname*{inf}_{z\\in\\mathcal{F}}\\|z-x_{t}\\|\\ w.$ rt. any norm $\\left\\Vert\\cdot\\right\\Vert$ ", "page_idx": 13}, {"type": "text", "text": "For any two non-negative integers $s\\leq t$ then $\\forall s\\leq t$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{dist}(x_{t},\\mathcal{F})\\leq(1-\\epsilon\\lambda)^{t-s}\\mathrm{dist}(x_{s},\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Recall Algorithm 1: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~~\\delta_{i,t}\\leftarrow\\mathrm{sign}\\big(\\beta_{1}m_{i,t}+(1-\\beta_{1})\\nabla_{x}f(x_{t};\\xi_{i,t})\\big)}\\\\ &{m_{i,t+1}\\leftarrow\\beta_{2}m_{i,t}+(1-\\beta_{2})\\nabla_{x}f(x_{t};\\xi_{i,t})}\\\\ &{~~~~\\Delta_{t}=\\left\\{\\frac{1}{N}\\big(\\sum_{i=1}^{N}\\delta_{i,t}\\big)\\ \\ \\ \\ \\ (\\mathrm{Averaging})\\right.}\\\\ &{~~~\\left.\\!\\!\\!x_{t+1}=x_{t}-\\epsilon(\\Delta_{t}+\\lambda x_{t})\\ \\ \\ \\ \\mathrm{(Majority\\Vote)}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rewrite the update into the following form: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}=(1-\\epsilon\\lambda)x_{t}-\\epsilon\\Delta_{t},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Define $w_{s\\rightarrow t}=(1-\\epsilon\\lambda)^{t-s}$ . Unrolling this update yields, ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t}=(1-w_{s\\rightarrow t})z_{s\\rightarrow t}+w_{s\\rightarrow t}x_{s},\\qquad\\;\\,z_{s\\rightarrow t}=\\frac{\\sum_{k=s}^{t-1}w_{k\\rightarrow t}(-\\Delta_{t}/\\lambda)}{\\sum_{k=s}^{t-1}w_{k\\rightarrow t}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have $z_{s\\rightarrow t}\\ \\in\\ {\\mathcal{F}}$ since $-\\Delta_{t}/\\lambda\\,\\in\\,\\mathcal{F}$ For any $\\epsilon\\mathrm{~>~0~}$ , let $\\hat{x}_{s}~\\in~\\mathcal{F}$ be the point satisfying $\\|\\hat{\\boldsymbol{x}}_{s}-\\boldsymbol{x}_{s}\\|\\le\\mathrm{dist}(\\boldsymbol{x}_{s},\\mathcal{F})+\\eta$ .Hence, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{dist}(x_{t},\\,\\mathcal{F})=\\underset{z\\in\\mathcal{F}}{\\operatorname*{inf}}\\,\\|x_{t}-z\\|}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\|x_{t}-(1-w_{s\\rightarrow t})z_{s\\rightarrow t}-w_{s\\rightarrow t}\\hat{x}_{s})\\|}\\\\ &{\\quad\\quad\\quad=w_{s\\rightarrow t}\\,\\|x_{s}-\\hat{x}_{s}\\|}\\\\ &{\\quad\\quad\\quad\\leq(1-\\epsilon\\lambda)^{t-s}(\\mathrm{dist}(x_{s},\\mathcal{F})+\\eta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As $\\eta\\rightarrow0$ , we achieve the desired result. ", "page_idx": 14}, {"type": "text", "text": "B.2 Phase II ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We study the convergence of Phase II in this section. We begin by defining a KKT score function to quantify stationary solutions for the box-constrained optimization problem discussed in Section B.2. Following this, we analyze convergence through the KKT score across majority vote (Section B.2.1), averaging (Section B.2.2), and global Lion strategies (Section B.2.3). ", "page_idx": 14}, {"type": "text", "text": "First, we list the following assumptions used in our proof. ", "page_idx": 14}, {"type": "text", "text": "Assumption B.2 (Smooth and Differentiable $f$ ).Function $f(\\cdot)$ is differentiable and $L$ -smooth. ", "page_idx": 14}, {"type": "text", "text": "Assumption B.3 (Variance bound). $\\mathcal{D}_{i}$ is i.i.d. drawn from a common distribtion $\\pi_{*}$ \uff0candthe stochastic sample $\\xi^{i}\\,\\sim\\,\\mathcal{D}_{i}$ is i.i.d. and upon receiving. query $x\\,\\in\\,\\mathbb{R}^{d}$ ,the stochastic gradient oracle gives us an independent unbiased estimate $\\nabla f(x;\\xi^{i})$ from the $i$ -th worker that has coordinate bounded variance: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}[\\nabla f(x;\\xi^{i})]=\\nabla f(x),\\qquad\\mathbb{E}_{\\xi}\\left[\\|\\nabla f(x;\\xi^{i})-\\nabla f(x)\\|^{2}\\right]\\le\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "AssumptionB4 Bias Corection). Consider the sequence $\\{m_{t}^{i}\\}_{t>0,i\\in[N]}$ generated by Algorithm $^{\\,l}$ $\\mathbb{E}[\\tilde{m}_{t}^{i}]/\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t}^{i})]\\ge0.$ ", "page_idx": 14}, {"type": "text", "text": "Here we define the a KKT score function for box constrained problem (6): ", "page_idx": 14}, {"type": "equation", "text": "$$\nS(x):=\\langle\\nabla f(x),\\mathrm{sign}(\\nabla f(x))+\\lambda x\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proposition B.5. Assume $f$ is continuously differentiable, $\\lambda>0$ and $\\|\\lambda x\\|_{\\infty}\\leq1$ Then ${\\cal S}(x)=0$ implies aKKTstationaryconditionof $\\operatorname*{min}_{x}f(x)$ s.t. $\\|\\lambda x\\|_{\\infty}\\leq1$ ", "page_idx": 14}, {"type": "text", "text": "Proof. We will verify that ${\\cal S}(x)\\;=\\;0$ coincides with the first order KKT conditions of the box constrained optimization problem (6). ", "page_idx": 14}, {"type": "text", "text": "Recall the box constrained problem in (6), we can rewrite it into the following formulation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}f(x)\\quad s.t.\\quad\\lambda x_{i}-1\\leq0,\\quad-\\,\\lambda x_{i}-1\\leq0,\\ \\ \\forall\\,i\\in[d].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\boldsymbol{\\mu}=(\\mu_{1},\\mu_{2},\\cdots,\\mu_{d})^{\\top}$ and $\\tilde{\\mu}=(\\tilde{\\mu}_{1},\\tilde{\\mu}_{2},\\cdot\\cdot\\cdot,\\tilde{\\mu}_{d})^{\\top}$ , then its first order KKT stationary condition can be written as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{x_{i}}f(x)+\\mu_{i}\\lambda-\\tilde{\\mu}_{i}\\lambda=0}\\\\ &{\\mu_{i}(\\lambda x_{i}-1)=0,\\;\\;\\;\\tilde{\\mu}_{i}(-\\lambda x_{i}-1)=0}\\\\ &{\\mu_{i}\\geq0,\\;\\;\\;\\tilde{\\mu}_{i}\\geq0}\\\\ &{\\lambda x_{i}-1\\leq0,\\;\\;\\;-\\lambda x_{i}-1\\leq0}\\\\ &{\\forall\\,i\\in\\{1,2,\\cdots,d\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Expressing $S(x)$ element-wisely, we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\nS(x)=\\sum_{k=1}^{d}S_{k}(x),\\qquad\\mathrm{~with~}\\qquad\\:\\:S_{k}(x)=\\partial_{x_{k}}f(x)\\cdot(\\mathrm{sign}(\\partial_{x_{k}}f(x))+\\lambda x_{k})\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{k}$ denotes the $k$ -th element of vector $x$ . Since $\\|\\lambda x\\|_{\\infty}\\leq1$ , we have $S_{k}(x)\\geq0$ , because ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{k}(x)=\\partial_{x_{k}}f(x)\\cdot\\left(\\mathrm{sign}(\\partial_{x_{k}}f(x))+\\lambda x_{k}\\right)}\\\\ &{\\quad\\quad=|\\partial_{x_{k}}f(x)|+\\lambda\\partial_{x_{k}}f(x)\\cdot x_{k}}\\\\ &{\\quad\\quad\\geq|\\partial_{x_{k}}f(x)|-|\\partial_{x_{k}}f(x)|\\cdot|\\lambda x_{k}|}\\\\ &{\\quad\\quad=|\\partial_{x_{k}}f(x)|(1-|\\lambda x_{k}|)}\\\\ &{\\quad\\quad\\geq0\\qquad/|\\mathrm{since}\\,\\,\\|\\lambda x\\|_{\\infty}\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, if ${\\cal S}(x)\\,=\\,0$ , we have $S_{k}(x)\\,=\\,0$ for each component $k$ . It means that we have either $\\mathrm{sign}(\\partial_{x_{k}}f(x))+\\lambda x_{k}=0$ or $\\partial_{x_{k}}f(x)=0$ for each coordinate $k$ ", "page_idx": 15}, {"type": "text", "text": "There are two primary cases to consider for each $k$ ", "page_idx": 15}, {"type": "text", "text": "\u00b7 Case I: $\\partial_{x_{k}}f(x)\\,=\\,0$ .This suggests that we reach a stationary condition of $f(x)$ w.r.t. coordinate $x_{k}$ , and the KKT condition is satisfied in this case with $\\mu_{k}=\\tilde{\\mu}_{k}=0$   \n\u00b7 Case II: $\\mathrm{sign}(\\partial_{x_{k}}f(x))+\\lambda x_{k}=0$ it follows that $\\begin{array}{r}{x_{k}=-\\frac{1}{\\lambda}\\mathrm{sign}(\\partial_{x_{k}}f(x))}\\end{array}$ -if $\\mathrm{sign}(\\partial_{x_{k}}f(x)\\,=\\,1$ , then $\\partial_{x_{k}}f(x)\\,\\geq\\,0$ , and the KKT condition is satisfied with $\\mu_{k}=0$ and $\\tilde{\\mu}_{k}=\\partial_{x_{k}}f(x)/\\lambda$ - if $\\mathrm{sign}(\\partial_{x_{k}}f(x))=-1$ , then $\\partial_{x_{k}}f(x)\\leq0$ , and the KKT condition is satisfied with $\\tilde{\\mu}_{k}=0$ and $\\mu_{k}=\\partial_{x_{k}}f(x)/\\lambda$ ", "page_idx": 15}, {"type": "text", "text": "It turns out the two cases above exactly covers the KKT stationary solution pair $(x,\\mu,\\tilde{\\mu})$ of thebox constrained problem in (6). ", "page_idx": 15}, {"type": "text", "text": "In conclusion, ${\\cal S}(x)=0$ signifies reaching a stationary point of the bound-constrained optimization problem, as formulated in (6), providing critical insights into the convergence behavior of the algorithm under consideration. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B.2.1 Majority Vote ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Assume $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $L$ smooth, and $N$ is the number of workers, on the $i$ -th worker, consider the following scheme based on the majority vote: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{g_{t}^{i}}:=\\nabla f\\big(x_{t};\\boldsymbol{\\xi_{t}^{i}}\\big)}\\\\ &{m_{t+1}^{i}=\\beta_{2}m_{t}^{i}+(1-\\beta_{2})\\boldsymbol{g_{t}^{i}}}\\\\ &{\\tilde{m}_{t+1}^{i}=\\beta_{1}m_{t}^{i}+(1-\\beta_{1})\\boldsymbol{g_{t}^{i}}}\\\\ &{x_{t+1}=x_{t}-\\epsilon\\left(\\mathrm{sign}\\left(\\displaystyle\\sum_{i=1}^{N}\\mathrm{sign}\\big(\\tilde{m}_{t+1}^{i}\\big)\\right)+\\lambda x_{t}\\right).\\qquad/\\mathrm{/Majority~Voting}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem B.6 (Convergence in Phase I). Assumption B.2 B.3 B.4 hold, consider the scheme in Algorithm $_{l l}$ and $\\beta_{1},\\bar{\\beta}_{2}\\in(0,1)$ and $\\beta_{2}>\\beta_{1}$ and $\\epsilon,\\lambda>0$ $\\|\\lambda x_{0}\\|_{\\infty}\\leq1$ ", "page_idx": 16}, {"type": "text", "text": "We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2D\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+2\\rho}{\\sqrt{N}}+2L\\epsilon d}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\rho_{t}[k]=\\left\\{0\\atop\\mathbb{E}[\\tilde{m}_{t+1}^{i}[k]]/\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])]\\right.\\right.\\left.e l s\\mathrm{j}e\\mathrm{n}(\\tilde{m}_{t+1}^{i}[k])\\right]=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Following Theorem B.1 from phase 1, once we have $\\|\\lambda x_{0}\\|_{\\infty}\\,\\leq\\,1$ , we stay within the constraint set with $\\|\\lambda x_{t}\\|\\leq1$ for all subsequent time $t\\geq0$ ", "page_idx": 16}, {"type": "text", "text": "For notation, write $\\begin{array}{r}{\\tilde{M}_{t+1}=\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})}\\end{array}$ This yields $x_{t+1}=x_{t}-\\mathrm{{\\epsilon}}\\mathrm{{\\epsilon}}\\mathrm{{sign}}(\\tilde{M}_{t+1})-\\epsilon\\lambda x_{t}$ We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{t+1})-f(x_{t})\\leq\\langle\\nabla f(x_{t}),x_{t+1}-x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}\\qquad\\lceil/L\\mathrm{-smoothness~of~}f\\rceil}\\\\ &{\\qquad\\qquad=-\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\tilde{M}_{t+1})+\\lambda x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=-\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))+\\lambda x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle}\\\\ &{\\qquad\\leq-\\epsilon S(x_{t})+2L\\epsilon^{2}d+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $\\left\\Vert x_{t+1}-x_{t}\\right\\Vert^{2}=\\epsilon^{2}\\left\\Vert\\mathrm{sign}(\\tilde{M}_{t+1})+\\lambda x_{t}\\right\\Vert^{2}\\leq4\\epsilon^{2}d,$ because $\\|\\lambda x_{t}\\|_{\\infty}\\leq1$ ", "page_idx": 16}, {"type": "text", "text": "By Assumption B.3, $\\tilde{m}_{t+1}^{1},\\tilde{m}_{t+1}^{2},\\cdots,\\tilde{m}_{t+1}^{N}$ ar i.dl.so $\\mathbb{E}[\\tilde{m}_{t+1}^{i}]$ and $\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]$ don't depend on $i$ . Hence we can define $R_{t+1}=\\mathbb{E}[\\tilde{m}_{t+1}^{i}]/\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]$ , where the division operation is element wise, so $R_{t+1}\\in\\mathbb{R}^{d}$ ", "page_idx": 16}, {"type": "text", "text": "By Assumption 3.3, $R_{t}$ is non-negative, one special case for the ratio $R_{t}$ is when $\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t}^{i}[k])]=0$ yet $\\mathbb{E}[\\tilde{m}_{t}^{i}[\\tilde{k}]]\\neq0$ , leading to $R_{t}[k]\\,=\\,+\\infty$ for $k\\,\\in\\,[d]$ . In such instance, $P(\\tilde{m}_{t}^{i}[k]\\,>\\,0)\\,=\\,1/2$ derived from the equation $\\mathbb{E}[\\mathrm{sign}\\bar{(m}_{t}^{i}[k])]=2P(\\tilde{m}_{t}^{i}[k]>0)-1=0$ for $k\\in[d]$ ", "page_idx": 16}, {"type": "text", "text": "First, recognizing that $\\mathbb{E}[\\mathrm{sign}(\\tilde{M}_{t}[k])]\\mathrm{~\\=~\\}0$ is straightforward as we model it as a binomial distribution with success probability $\\textit{p}\\,=\\,1/2$ for $t~~>~~0$ This leads to the result $\\mathbb{E}\\nabla f(x_{t})[k]\\left(\\mathrm{sign}(\\nabla f(x_{t})[k])-\\mathrm{sign}(\\tilde{M}_{t}[k])\\right)=\\mathbb{E}\\,|\\nabla f(x_{t})[k]|.$ ", "page_idx": 16}, {"type": "text", "text": "Given that $\\begin{array}{r}{\\mathbb{E}[X]\\,=\\,\\arg\\operatorname*{min}_{z}\\mathbb{E}\\left\\|X-z\\right\\|_{2}}\\end{array}$ defines the expectation of a random variable $X$ as the value $z$ minimizes the expected euclidean distance to $X$ , and the median $X=\\arg\\operatorname*{min}_{z}\\mathbb{E}\\left\\|X-z\\right\\|_{1}$ defines the median as the value $z$ minimizing the expected absolute distance to $X$ , for a R.V. $X$ in $\\mathbb{R}$ recall our case where $P(\\tilde{m}_{t}^{i}[k]>0)=1/2$ , which is equivalent to that the median is 0. From this, it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi\\left|\\nabla f(x_{t})[k]\\right|\\leq\\mathbb{E}[\\mathbb{E}_{\\xi}[\\left|\\nabla f(x_{t};\\xi_{t}^{i})[k]-\\nabla f(x_{t})[k]\\right|_{1}]]\\leq\\mathbb{E}\\sqrt{\\mathbb{E}_{\\xi}\\left\\|\\nabla f(x_{t};\\xi_{t}^{i})[k]-\\nabla f(x_{t})[k]\\right\\|_{2}^{2}}\\leq\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To bound the last term in (12) $\\langle\\nabla f(x_{t}),\\operatorname{sign}(\\nabla f(x_{t}))\\,-\\,\\operatorname{sign}(\\tilde{M}_{t+1})\\rangle$ , we follow a structured approach. Here's an outline for bounding this term: ", "page_idx": 16}, {"type": "text", "text": "To bound the last term in Equation (12), $\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle$ , we follow a structured approach: ", "page_idx": 16}, {"type": "text", "text": "1. Transform Inner Product into Norm of Difference: Using Lemma B.8 to convert the inner product $\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle$ into the norm of a difference. ", "page_idx": 16}, {"type": "text", "text": "2. Introduce $R_{t}$ as a De-bias Ratio: $R_{t}$ is defined to adjust or correct for any bias in the expected value of $\\tilde{m}_{t}^{i}$ and the expected sign of $\\tilde{m}_{t}^{i}$ as in Assumption B.4. ", "page_idx": 17}, {"type": "text", "text": "3. Handle Cases of $R_{t}$ Separately: Given the possibility of $R_{t}[k]=+\\infty$ , it's essential to treat the scenarios of $R_{t}[k]<+\\infty$ and $R_{t}[k]=+\\infty$ with separate proofs. ", "page_idx": 17}, {"type": "text", "text": "\u00b7For $R_{t}[k]<+\\infty$ standard bounding techniques can be applied, potentially leveraging properties of $R_{t}$ to establish a finite upper bound.   \n\u00b7 For $R_{t}[k]\\,=\\,+\\infty$ , it's actually bounding $\\left\\|\\nabla f(x_{t})\\right\\|$ . This can be bounded by the variance of the stochastic gradient $g_{t}^{i}$ ", "page_idx": 17}, {"type": "text", "text": "4. Merge Cases with Finite $\\rho_{t}$ Replacing $R_{t}$ : After separately proving bounds for each case of $R_{t}$ , the results are unified by substituting $R_{t}$ with a finite $\\rho_{t}$ , where $\\rho_{t}$ serves a similar purpose but ensures a manageable, finite adjustment. ", "page_idx": 17}, {"type": "text", "text": "Case I (Finite $R_{t+1}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The first step is to expand this inner product, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\bar{M}_{t+1})\\rangle}\\\\ &{=\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\frac{1}{N}\\overbar{M}_{t+1})\\rangle}\\\\ &{=\\displaystyle\\mathbb{E}\\sum_{k=1}^{d}\\nabla f(x_{t})[k]\\left(\\mathrm{sign}(\\nabla f(x_{t})[k])-\\mathrm{sign}(\\frac{1}{N}\\overbar{M}_{t+1}[k])\\right)}\\\\ &{=2\\mathbb{E}\\displaystyle\\sum_{k=1}^{d}R_{t+1}[k]\\left\\vert\\nabla f(x_{t})[k]/R_{t+1}[k]-\\frac{1}{N}\\overbar{M}_{t+1}[k]\\right\\vert}\\\\ &{=2\\mathbb{E}\\displaystyle\\sum_{k=1}^{d}R_{t+1}[k]\\left\\vert\\nabla f(x_{t})[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\overbar{m}_{t+1}^{i}[k])\\right\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition of $R_{t}$ , it is a debiasing ratio between $\\mathbb{E}[\\tilde{m}_{t+1}^{i}]$ and $\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]$ , so we construct adiference betwen $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])}\\end{array}$ and $\\textstyle{\\frac{1}{N}}\\sum_{i=1}^{N}{\\tilde{m}}_{t+1}^{i}[k]$ by decoupling te diference between $\\nabla f(x_{t})[k]/R_{t+1}[k]$ and $\\begin{array}{r}{\\frac{1}{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\xi R_{t+1}[k]\\left|\\nabla f(x_{t})[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\bar{m}_{t+1}^{i}[k])\\right|}\\\\ {\\displaystyle=\\mathbb{E}R_{t+1}[k]\\left|\\nabla f(x_{t})[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]/R_{t+1}[k]+\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]/\\slash R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]\\right|}\\\\ {\\displaystyle=\\mathbb{E}R_{t+1}[k]\\left|\\nabla f(x_{t})[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]/R_{t+1}[k]\\right|+R_{t+1}[k]\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]/R_{t+1}[k]\\right|\\,.}\\\\ {\\displaystyle=\\mathbb{E}\\left|\\nabla f(x_{t})[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]\\right|+R_{t+1}[k]\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\bar{m}_{t+1}^{i}[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\bar{m}_{t+1}^{i}[k])\\right|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term $\\begin{array}{r}{\\mathbb{E}\\left|\\nabla f(x_{t})[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]\\right|}\\end{array}$ doesn't depend on $R_{t+1}$ , we can bound this term across $d$ coordinates using Lemma B.10: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname{\\xi}\\sum_{k=1}^{d}\\left|\\nabla f(x_{t})[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]\\right|\\leq\\sqrt{d}\\mathbb{E}\\left\\|\\nabla f(x_{t})-\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}\\right\\|}}\\\\ &{\\leq\\sqrt{d}\\mathbb{E}\\left\\|\\nabla f(x_{t})-\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\beta_{1}m_{t}^{i}+(1-\\beta_{1})g_{t}^{i}\\right)\\right\\|}\\\\ &{\\leq\\sqrt{d}\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\beta_{1}\\left(\\nabla f(x_{t})-m_{t}^{i}\\right)\\right\\|+\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}(1-\\beta_{1})\\left(\\nabla f(x_{t})-m_{t}^{i}\\right)\\right\\|}\\\\ &{\\leq\\sqrt{d}\\beta_{1}\\left(\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\frac{2L\\epsilon\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}\\right)+\\frac{\\sqrt{d}\\sigma(\\sqrt{d})^{2}}{\\forall}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second term $\\begin{array}{r}{\\mathbb{E}R_{t+1}[k]\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])\\right|}\\end{array}$ can be decoupled into the variance of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])}\\end{array}$ and the variance of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota\\sum_{k=1}^{d}R_{t+1}|k|\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\hat{m}_{t+1}^{i}|k|\\mathcal{H}_{t+1}[k]-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sym}_{(\\hat{m}_{t+1}^{i}|k|)}\\right|}\\\\ &{=\\displaystyle\\sum_{k=1}^{d}R_{t+1}|k|\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\hat{m}_{t+1}^{i}|k|\\mathcal{H}_{t+1}[k]-\\mathbb{E}\\hat{m}_{t+1}^{i}|k|\\mathcal{H}_{t+1}[k]+\\mathbb{E}\\hat{m}_{t+1}^{i}[k]/N_{t+1}[k]-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{cos}_{t}^{i}\\mathrm{\\Delta}\\hat{m}_{t}^{i}\\right|}\\\\ &{=\\displaystyle\\sum_{k=1}^{d}R_{t+1}|k|\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\hat{m}_{t+1}^{i}[k]/R_{t+1}[k]-\\mathbb{E}\\hat{m}_{t+1}^{i}[k]/R_{t+1}[k]+\\mathbb{E}\\mathrm{sign}(\\hat{m}_{t+1}^{i}|k|)-\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\hat{m}_{t+1}^{i}|k|)\\right|}\\\\ &{=\\displaystyle\\sum_{k=1}^{d}R_{t+1}|k|\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\hat{m}_{t+1}^{i}[k]/R_{t+1}[k]-\\mathbb{E}\\hat{m}_{t+1}^{i}[k]/R_{t+1}[k]\\right|+R_{t+1}[k]\\left|\\displaystyle\\sum_{l\\leq i,n}\\mathrm{fosp}(\\hat{m}_{t+1}^{i}|k|)-\\displaystyle\\frac{1}{N}\\right|}\\\\ &{=\\displaystyle\\mathbb{E}\\displaystyle\\sum_{k=1}^{d}\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\hat{m}_{t+1}^{i}[k]-\\mathbb{E}\\hat{m}_{t+1}^{i}[k]\\right|+R_{t+1}[k]\\left|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sym}_{(\\hat{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we haegt the ariance of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])}\\end{array}$ an the ariance of $\\textstyle{\\frac{1}{N}}\\sum_{i=1}^{N}{\\tilde{m}}_{t+1}^{i}[k]$ Jlet us bound them one by one: ", "page_idx": 18}, {"type": "text", "text": "The variance of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}[k]}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{d}\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}-\\mathbb{E}\\tilde{m}_{t+1}^{i}\\right\\|\\leq\\sqrt{d}\\sqrt{\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}-\\mathbb{E}\\tilde{m}_{t+1}^{i}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{d}\\sqrt{\\displaystyle\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\|\\tilde{m}_{t+1}^{i}-\\mathbb{E}\\tilde{m}_{t+1}^{i}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\displaystyle\\frac{C d\\sigma^{2}}{N}},\\qquad/[\\mathrm{Lemma~B.11}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\beta_{1}^{2}(1-\\beta_{2})\\frac{1}{1+\\beta_{2}}+(1-\\beta_{1})^{2}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "The variance of $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R_{t+1}\\|\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})-\\mathbb{E}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})\\right\\|\\leq\\sqrt{\\mathbb{E}\\left\\|\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})/N-\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]\\right\\|^{2}}}\\quad}&{}\\\\ &{=\\|R_{t+1}\\|\\,\\sqrt{\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathbb{E}\\left\\|\\mathrm{sign}(\\tilde{m}_{t+1}^{i})-\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i})]\\right\\|^{2}}}\\\\ &{\\leq\\|R_{t+1}\\|\\,\\sqrt{\\frac{1}{N}}.\\quad\\quad/[\\mathrm{Lemma~}\\mathbf{B}.9]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In above, we have the bound of the last term in (12) $\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle;$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\Tilde{M}_{t+1})\\rangle}\\\\ &{\\le2\\mathbb{E}\\displaystyle\\sum_{k=1}^{d}\\Bigg|\\nabla f(x_{t})[k]-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\Tilde{m}_{t+1}^{i}[k]\\Bigg|+2\\mathbb{E}\\displaystyle\\sum_{k=1}^{d}R_{t+1}[k]\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\Tilde{m}_{t+1}^{i}[k]/R_{t+1}[k]-\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\Tilde{m}_{t+1})\\right|}\\\\ &{\\le2\\sqrt{d}\\mathbb{E}\\left\\|\\nabla f(x_{t})-\\frac{1}{N}\\displaystyle\\sum_{i=1}^{N}\\Tilde{m}_{t+1}^{i}\\right\\|+2\\mathbb{E}\\sqrt{d}\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\Tilde{m}_{t+1}^{i}-\\mathbb{E}\\Tilde{m}_{t+1}^{i}\\right\\|+2\\left\\|R_{t+1}\\right\\|\\left\\|\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\Tilde{m}_{t})\\right\\|}\\\\ &{\\le2\\sqrt{d}\\beta_{1}\\left(\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\frac{2L\\epsilon\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}\\right)+2\\frac{\\sqrt{d}\\sigma(1-\\beta_{1})}{\\sqrt{N}}+2\\sqrt{\\frac{C d\\sigma^{2}}{N}}+2\\left\\|R_{t+1}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case II (Infinite $R$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From our discussion above, we know that $P(\\tilde{m}_{t}^{i}[k]>0)=1/2$ since $\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t}^{i}[k])]=2P(\\tilde{m}_{t}^{i}[k]>$ $0)-1=0$ ,where $k\\,\\in[d]$ . For notion, write $\\mathcal{D}=\\{j\\in[d]\\,\\,|\\,\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[j])]=0\\}$ . In this case, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\sum_{j\\in\\mathcal{D}}\\nabla f(x_{t})[j]\\left(\\mathrm{sign}(\\nabla f(x_{t})[j])-\\mathrm{sign}(\\tilde{M}_{t}[j])\\right)=\\mathbb{E}\\sum_{j\\in\\mathcal{D}}|\\nabla f(x_{t})[j]|}}\\\\ &{\\leq\\mathbb{E}\\left[\\mathbb{E}_{\\xi}\\sum_{j\\in\\mathcal{D}}\\left|\\nabla f(x_{t};\\xi_{t}^{i})[j]-\\nabla f(x_{t})[j]\\right|\\right]}\\\\ &{\\leq\\mathbb{E}\\sqrt{\\mathbb{E}_{\\xi}\\sum_{j\\in\\mathcal{D}}\\left\\|\\nabla f(x_{t};\\xi_{t}^{i})[j]-\\nabla f(x_{t})[j]\\right\\|_{2}^{2}}}\\\\ &{\\leq\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, the inner product $\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle$ is still bounded. Hence we can merge both cases into a unified bound by simply replacing $R_{t}$ by $\\rho_{t}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho_{t}[k]=\\left\\{0\\atop\\mathbb{E}[\\tilde{m}_{t+1}^{i}[k]]/\\mathbb{E}[\\mathrm{sign}(\\tilde{m}_{t+1}^{i}[k])]\\right.\\right.\\left.\\mathrm{else}.\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Adding one constant $D\\geq1$ to make the bound in finite case adpative to infinite case: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma\\leq2D\\sqrt{d}\\beta_{1}\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|,\\forall t,1\\leq t\\leq T.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\displaystyle\\sum_{j\\in D}\\nabla f(x_{t})[j]\\left(\\mathrm{sign}(\\nabla f(x_{t})[j])-\\mathrm{sign}(\\tilde{M}_{t}[j])\\right)}\\\\ &{\\leq2D\\sqrt{d}\\beta_{1}\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\displaystyle\\frac{4L d\\beta_{1}\\epsilon}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+2\\,\\|\\rho_{t+1}\\|}{\\sqrt{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, we have the bound for both cases: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle}\\\\ &{\\leq2\\sqrt{d}\\beta_{1}\\left(\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\frac{2L\\epsilon\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}\\right)+2\\frac{\\sqrt{d}\\sigma(1-\\beta_{1})}{\\sqrt{N}}+2\\sqrt{\\frac{C d\\sigma^{2}}{N}}+2\\left\\|\\rho_{t+1}\\right\\|\\sqrt{\\frac{d}{N}}}\\\\ &{\\leq2D\\sqrt{d}\\beta_{1}\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\frac{4L d\\beta_{1}\\epsilon}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+2\\left\\|\\rho_{t+1}\\right\\|}{\\sqrt{N}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\ast}(x_{t+1})-f(x_{t})\\leq-\\epsilon\\mathcal{S}(x_{t})+2L\\epsilon^{2}d+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\tilde{M}_{t+1})\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\epsilon\\mathcal{S}(x_{t})+2L\\epsilon^{2}d+\\epsilon\\left(2D\\sqrt{d}\\beta_{1}\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\displaystyle\\frac{4L d\\beta_{1}\\epsilon}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+\\epsilon}{\\sqrt{N}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, a telescope yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2D\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma(1+\\sqrt{C})+2\\rho}{\\sqrt{N}}+2L\\epsilon d}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\rho=\\operatorname*{max}_{1\\leq t\\leq T}\\left\\|\\rho_{t}\\right\\|$ ", "page_idx": 20}, {"type": "text", "text": "Lemma B.7. Let $(X,Y)$ is a joint random variable on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ For any constant $a\\in(0,+\\infty)$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle X,\\mathrm{sign}(X)-\\mathrm{sign}(Y)\\rangle]\\leq2a\\sqrt{d}\\mathbb{E}\\|X/a-Y\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Without loss of generality, set $a=1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\langle X,\\mathrm{sign}(X)-\\mathrm{sign}(Y)\\rangle]=\\mathbb{E}[\\|X\\|_{1}-\\langle X,\\mathrm{sign}(Y)\\rangle]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\mathbb{E}[\\|X-Y\\|_{1}]\\mathrm{\\quad~}//[\\mathrm{Lemma~}\\mathbf{B.8}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{d}\\mathbb{E}[\\|X-Y\\|]\\mathrm{\\quad~}//\\mathrm{by~Cauchy-Schwarz},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\left\\|\\cdot\\right\\|_{1}$ is the $\\ell_{1}$ norm and $\\lVert\\cdot\\rVert$ denotes the Euclidean norm. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.8. For any $x,y\\in\\mathbb{R}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|x|-x\\mathrm{sign}(y)\\leq2\\left|x-y\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. If $\\mathrm{sign}(y)=\\mathrm{sign}(x)$ , we have $|x|-x\\mathrm{sign}(y)=0\\leq2\\left|x-y\\right|$ ", "page_idx": 20}, {"type": "text", "text": "If $\\mathrm{sign}(y)=-\\mathrm{sign}(x)$ , we have $|x|-x\\mathrm{sign}(y)=2\\left|x\\right|\\leq2\\left|x\\right|+2\\left|y\\right|=2\\left|x-y\\right|\\!.$ f $\\mathrm{sign}(y)=0$ we have $|x|-x\\mathrm{sign}(y)=|x|=|x-y|\\leq2\\left|x-y\\right|$ ", "page_idx": 20}, {"type": "text", "text": "Lemma B.9. Let $X$ be a random variable in $\\mathbb{R}$ wehave $\\mathbb{E}\\left\\|\\operatorname{sign}(X)-\\mathbb{E}[\\operatorname{sign}(X)]\\right\\|^{2}<1.$ ", "page_idx": 20}, {"type": "text", "text": "Proof. The result is a direct derivation from Bernoulli distribution's variance, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}\\|\\operatorname{sign}(X)-\\operatorname{\\mathbb{E}}[\\operatorname{sign}(X)]\\|^{2}=\\operatorname{\\mathbb{E}}[\\operatorname{sign}(X)^{2}]-\\operatorname{\\mathbb{E}}[\\operatorname{sign}(X)]^{2}<1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.10. Following the same setting in Theorem B.6, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\frac{1}{N}\\sum_{i=1}^{N}m_{t}^{i}-\\nabla f(x_{t})\\|\\le\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\frac{2L\\varepsilon\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We use the notions: $g_{t}^{i}\\;:=\\;\\nabla f(x_{t};\\xi_{t}^{i})$ $\\begin{array}{r}{M_{t}\\;=\\;\\frac{1}{N}\\sum_{i=1}^{N}m_{t}^{i}}\\end{array}$ $\\varepsilon_{t}\\;:=\\;M_{t}\\,-\\,\\nabla f(x_{t})$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}g_{t}^{i},\\delta_{t}:=\\overline{{g_{t}}}-\\nabla f(x_{t})}\\end{array}$ and $s_{t}=\\nabla f(x_{t-1})-\\nabla f(x_{t})$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{t}=M_{t}-\\nabla f(x_{t})}\\\\ &{\\quad=\\beta_{2}M_{t-1}+(1-\\beta_{2})\\overline{{g_{t}}}-\\nabla f(x_{t})}\\\\ &{\\quad=\\beta_{2}\\bigl(M_{t-1}-\\nabla f(x_{t-1})\\bigr)+(1-\\beta_{2})(\\overline{{g_{t}}}-\\nabla f(x_{t}))+\\beta_{2}\\bigl(\\nabla f(x_{t-1})-\\nabla f(x_{t})}\\\\ &{\\quad=\\beta_{2}\\varepsilon_{t-1}+(1-\\beta_{2})\\delta_{t}+\\beta_{2}s_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon_{t}=\\beta_{2}\\varepsilon_{t-1}+(1-\\beta_{2})\\delta_{t}+\\beta_{2}s_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Under the $L$ -smoothness assumption B.2: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|s_{t}\\|=\\|\\nabla f(x_{t-1})-\\nabla f(x_{t})\\|\\leq L\\|x_{t-1}-x_{t}\\|\\leq2L\\sqrt{d}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\varepsilon$ is the step size. Using mathematical induction, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon_{t}=\\beta_{2}^{t}\\varepsilon_{0}+\\sum_{i=1}^{t}\\beta_{2}^{t-i+1}s_{i}+(1-\\beta_{2})\\sum_{i=1}^{t}\\beta_{2}^{t-i}\\delta_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By taking the norms of both sides of the above equation and using the strong bound 13 we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\varepsilon_{t}\\|\\leq\\beta_{2}^{t}\\|\\varepsilon_{0}\\|+2L\\sqrt{d}\\epsilon\\sum_{i=1}^{t}\\beta_{2}^{t-i+1}+(1-\\beta_{2})\\|\\sum_{i=1}^{t}\\beta_{2}^{t-i}\\delta_{t}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking expectations on both sides, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\varepsilon_{t}\\|\\leq\\beta_{2}^{t}\\|\\varepsilon_{0}\\|+\\frac{2L\\sqrt{d}\\varepsilon}{1-\\beta_{2}}+(1-\\beta_{2})\\|\\sum_{i=1}^{t}\\beta_{2}^{t-i}\\delta_{t}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that r.v.s $(\\delta_{i})_{1\\leq i\\leq t}$ are mean zero, using B.11, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\sum_{i=1}^{t}\\beta_{2}^{t-i}\\delta_{i}\\right\\|=\\sqrt{\\mathbb{E}\\sum_{i=1}^{t}\\beta_{2}^{2t-2i}\\frac{\\sigma^{2}}{N}}\\leq\\frac{\\sigma}{\\sqrt{N(1-\\beta_{2}^{2})}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\varepsilon_{t}\\|\\leq\\beta_{2}^{t}\\|\\varepsilon_{0}\\|+\\frac{2L\\sqrt{d}\\varepsilon}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $M_{0}=0$ under our setting, so $\\varepsilon_{0}=-\\nabla f(x_{0})$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\varepsilon_{t}\\Vert\\leq\\beta_{2}^{t}\\Vert\\nabla f(x_{0})\\Vert+\\frac{2L\\sqrt{d}\\varepsilon}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{N(1+\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.11 (Cumulative error of stochastic gradient [4]). Assume the same settings as in Theorem B.6. Define $\\begin{array}{r}{Y_{k}:=\\sum_{l=1}^{k}\\alpha_{\\ell}\\delta_{l}}\\end{array}$ where $\\delta_{t}:=\\overline{{g_{t}}}-\\nabla f(x_{t})$ with $\\begin{array}{r}{\\overline{{g_{t}}}=\\sum_{i=1}^{N}g_{t}^{i}}\\end{array}$ and $g_{t}^{i}:=\\nabla f(x_{t};\\xi_{t}^{i})$ followingtheupdatein(11),and $\\{\\alpha_{\\ell}\\colon\\ell=0,1,\\ldots\\}$ isadeterministicsequence.Then $Y_{k}$ is a martingale, and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left[\\sum_{l=1}^{k}\\alpha_{l}\\delta_{l}\\right]^{2}\\right]=\\frac{1}{N}\\sum_{l=1}^{k}\\alpha_{l}^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We simply check the definition of martingales. First, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[|{\\cal Y}_{k}|]=\\mathbb{E}\\left[\\displaystyle\\left|\\sum_{l=1}^{k}\\alpha_{l}\\delta_{l}\\right|\\right]}\\\\ &{\\phantom{=}\\leq\\displaystyle\\sum_{l}|\\alpha_{l}|\\mathbb{E}[|\\delta_{l}|]\\quad\\quad\\quad|\\pi\\mathrm{triangle~inequality}}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{l}|\\alpha_{l}|\\mathbb{E}[|\\delta_{l}||s_{l}]\\quad\\quad\\quad|\\mathrm{//law~of~total~probability}}\\\\ &{\\phantom{=}\\leq\\displaystyle\\sum_{l}|\\alpha_{l}|\\mathbb{E}[\\sqrt{\\mathbb{E}[\\delta_{l}^{2}|x_{l}]}]\\quad\\quad\\quad|\\boldsymbol{J}|\\mathrm{fensen`s~inequality}}\\\\ &{\\phantom{=}\\leq\\displaystyle\\sum_{l}|\\alpha_{l}|\\sigma<\\infty\\quad\\quad\\quad|\\boldsymbol{/}\\Lambda\\mathrm{sumption~}\\mathbb{B}\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Second, again using the law of total probability, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[Y_{k+1}|Y_{1},...,Y_{k}]=\\mathbb{E}\\left[\\underset{l=1}{\\overset{k+1}{\\sum}}\\alpha_{l}\\delta_{l}\\Bigg|\\alpha_{1}\\delta_{1},...,\\alpha_{k}\\delta_{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad=Y_{k}+\\alpha_{k+1}\\mathbb{E}\\left[\\delta_{k+1}|\\alpha_{1}\\delta_{1},...,\\alpha_{k}\\delta_{k}\\right]}\\\\ &{\\qquad\\qquad=Y_{k}+\\alpha_{k+1}\\mathbb{E}\\left[\\mathbb{E}\\left[\\delta_{k+1}|x_{k+1},\\alpha_{1}\\delta_{1},...,\\alpha_{k}\\delta_{k}\\right]|\\alpha_{1}\\delta_{1},...,\\alpha_{k}\\delta_{k}\\right]}\\\\ &{\\qquad\\qquad=Y_{k}+\\alpha_{k+1}\\mathbb{E}\\left[\\mathbb{E}\\left[\\delta_{k+1}|x_{k+1}\\right]|\\alpha_{1}\\delta_{1},...,\\alpha_{k}\\delta_{k}\\right]}\\\\ &{\\qquad\\qquad=Y_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This completes the proof that it is a martingale. We now make use of the properties of martingale difference sequences to establish a variance bound on the martingale. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\big[\\sum_{l=1}^{k}\\alpha_{l}\\delta_{l}|^{2}\\big]=\\sum_{l=1}^{k}\\mathbb{E}[\\alpha_{l}^{2}\\delta_{l}^{2}]+2\\sum_{l<j}\\mathbb{E}[\\alpha_{l}\\alpha_{j}\\delta_{l}\\delta_{j}]}\\\\ {\\displaystyle}&{=\\sum_{l=1}^{k}\\alpha_{l}^{2}\\mathbb{E}[\\mathbb{E}[\\delta_{l}^{2}|\\delta_{1},...,\\delta_{l-1}]]+2\\sum_{l<j}\\alpha_{l}\\alpha_{j}\\mathbb{E}\\Big[\\delta_{l}\\mathbb{E}\\big[\\mathbb{E}[\\delta_{j}|\\delta_{1},...,\\delta_{j-1}]\\big|\\delta_{l}\\big]\\Big]}\\\\ {\\displaystyle}&{=\\sum_{l=1}^{k}\\alpha_{l}^{2}\\mathbb{E}[\\mathbb{E}[\\mathbb{E}[\\delta_{l}^{2}|\\alpha_{l},\\delta_{1},...,\\delta_{l-1}]|\\delta_{1},...,\\delta_{l-1}]]+0}\\\\ {\\displaystyle}&{=\\frac{1}{N}\\sum_{l=1}^{k}\\alpha_{l}^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a direct result of Lemma B.11, we have the following. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.12. Under the same settings as in Theorem 3.6, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert\\tilde{m}_{t+1}^{i}-\\mathbb{E}[\\tilde{m}_{t+1}^{i}]\\right\\Vert^{2}\\leq\\left(\\beta_{1}^{2}(1-\\beta_{2})\\frac{1}{1+\\beta_{2}}+(1-\\beta_{1})^{2}\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{m}_{t+1}^{i}=\\beta_{1}m_{t}^{i}+(1-\\beta_{1})g_{t}^{i}}\\\\ &{\\qquad=\\beta_{1}(1-\\beta_{2})\\left(g_{t-1}^{i}+\\beta_{2}g_{t-2}^{i}+\\cdot\\cdot\\cdot+\\beta_{2}^{t-1}g_{0}^{i}\\right)+(1-\\beta_{1})g_{t}^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta_{1}^{2}(1-\\beta_{2})^{2}\\left(1+\\beta_{2}^{2}+\\cdot\\cdot\\cdot+\\beta_{2}^{2(t-1)}\\right)+(1-\\beta_{1})^{2}=\\beta_{1}^{2}(1-\\beta_{2})^{2}\\frac{1-\\beta_{2}^{2t}}{1-\\beta_{2}^{2}}+(1-\\beta_{1})^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By using lemma B.11, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert\\tilde{m}_{t+1}^{i}-\\mathbb{E}[\\tilde{m}_{t+1}^{i}]\\right\\Vert^{2}\\leq\\left(\\beta_{1}^{2}(1-\\beta_{2})\\frac{1}{1+\\beta_{2}}+(1-\\beta_{1})^{2}\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.2.2 Averaging Update Convergence ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Assume $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ is $L$ -smooth, $N$ is the number of workers, on the $i$ -th worker, consider the following scheme based on the averaging: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g_{t}^{i}:=\\nabla f(x_{t};\\xi_{t}^{i}),\\quad\\forall i=1,\\ldots,N}}\\\\ {{m_{t+1}^{i}=\\beta_{2}m_{t}^{i}+(1-\\beta_{2})g_{t}^{i},\\quad\\forall i=1,\\ldots,N}}\\\\ {{\\tilde{m}_{t+1}^{i}=\\beta_{1}m_{t}^{i}+(1-\\beta_{1})g_{t}^{i},\\quad\\forall i=1,\\ldots,N}}\\\\ {{x_{t+1}=x_{t}-\\epsilon\\left(\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})+\\lambda x_{t}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Theorem B.13 (Convergence in Phase II). Under Assumption B.2 B.3, consider the scheme in (15), and $\\beta_{1},\\beta_{2}\\in(0,1)$ and $\\beta_{2}>\\beta_{1}$ andE, $\\lambda>0$ $\\|\\lambda x_{0}\\|_{\\infty}\\leq1$ .Wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\beta_{1}\\sigma}{\\sqrt{1+\\beta_{2}}}+2(1-\\beta_{1})\\sigma+2L\\epsilon d L.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For notation, write $\\begin{array}{r}{\\tilde{M}_{t+1}=\\sum_{i=1}^{N}\\mathrm{sign}(\\tilde{m}_{t+1}^{i})}\\end{array}$ This yields $x_{t+1}=x_{t}-\\epsilon\\tilde{M}_{t+1}-\\epsilon\\lambda x_{t}$ ", "page_idx": 23}, {"type": "text", "text": "Following Theorem B.1 from phase 1, once we have $\\|\\lambda x_{0}\\|_{\\infty}\\leq1$ , we stay within the constraint set with $\\|\\lambda x_{t}\\|\\leq1$ for all subsequent time $t\\geq0$ ", "page_idx": 23}, {"type": "text", "text": "Following a similar procedure in B.6, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{t+1})-f(x_{t})\\leq\\langle\\nabla f(x_{t}),x_{t+1}-x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq-\\epsilon\\langle\\nabla f(x_{t}),\\tilde{M}_{t+1}+\\lambda x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq-\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))+\\lambda x_{t}\\rangle+\\displaystyle\\frac{L}{2}\\|x_{t+1}-x_{t}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle}\\\\ &{\\qquad\\qquad\\leq-\\epsilon S(x_{t})+2L\\epsilon^{2}d+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let us bound the last term $\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle.$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle}\\\\ &{=\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\cfrac{1}{N}\\sum_{i=1}^{N}\\mathrm{sign}(\\bar{m}_{t+1}^{i})\\rangle}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\frac{1}{N}\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\bar{m}_{t+1}^{i})\\rangle}\\\\ &{=\\mathbb{E}\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\mathrm{sign}(\\bar{m}_{t+1}^{i})\\rangle\\qquad\\mathcal{V}\\{\\bar{m}_{t+1}^{i}\\}_{1\\leq i\\leq N}\\arg\\mathrm{sign}(\\ker}\\\\ &{\\leq2\\sqrt{d}\\mathbb{E}\\left\\|\\nabla f(x_{t})-\\bar{m}_{t+1}^{i}\\right\\|\\qquad\\mathcal{V}\\{\\mathrm{Armm}_{{\\mathbf{B}}},\\mathrm{Z}\\}}\\\\ &{\\leq2\\sqrt{d}\\mathbb{E}\\left[\\beta_{1}\\left\\|\\nabla f(x_{t})-m_{t}^{i}\\right\\|+(1-\\beta_{1})\\left\\|\\nabla f(x_{t})-g_{t}^{i}\\right\\|\\right]\\qquad\\mathcal{I}\\mathrm{ridangle~inequality}}\\\\ &{\\leq2\\sqrt{d}\\left(\\beta_{1}\\left(\\beta_{2}^{i}\\right\\|\\nabla f(x_{0})\\right\\|+\\frac{2L e\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{1+\\beta_{2}}}\\right)+(1-\\beta_{1})\\sigma\\right).\\quad\\mathrm{~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{^{\\ast}(x_{t+1})-f(x_{t})\\leq-\\epsilon S(x_{t})+2L\\epsilon^{2}d+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle}\\\\ {\\leq-\\epsilon S(x_{t})+2L\\epsilon^{2}d+2\\epsilon\\sqrt{d}\\left(\\beta_{1}\\left(\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\displaystyle\\frac{2L\\epsilon\\sqrt{d}}{1-\\beta_{2}}+\\frac{\\sigma}{\\sqrt{1+\\beta_{2}}}\\right)+(1-\\epsilon)^{2}\\epsilon\\|\\nabla f(x_{t})\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, a telescope yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\beta_{1}\\sigma\\sqrt{d}}{\\sqrt{1+\\beta_{2}}}+2(1-\\beta_{1})\\sqrt{d}\\sigma+2L\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.2.3  Global Lion Convergence ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Assume $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ is $L$ -smooth, $N$ is the number of workers, on the $i$ -th worker, consider the following scheme based on the global Lion: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle g_{t}^{i}:=\\nabla f({\\boldsymbol x}_{t};\\xi_{t}^{i})}}\\\\ {{\\displaystyle m_{t+1}^{i}=\\beta_{2}m_{t}^{i}+(1-\\beta_{2})g_{t}^{i}}}\\\\ {{\\displaystyle\\tilde{m}_{t+1}^{i}=\\beta_{1}m_{t}^{i}+(1-\\beta_{1})g_{t}^{i}}}\\\\ {{\\displaystyle x_{t+1}=x_{t}-\\epsilon\\left(\\mathrm{sign}(\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i})+\\lambda x_{t}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Theorem B.14 (Convergence in Phase I). Under Assumption $B.2$ and B.3, consider the scheme in (16), and $\\beta_{1},\\beta_{2}\\in(0,1\\bar{)}$ and $\\beta_{2}>\\beta_{1}$ and E, $\\lambda>0$ $\\|\\lambda x_{0}\\|_{\\infty}\\leq1$ Wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2\\sqrt{d}\\sigma}{\\sqrt{N}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For notation, write $\\begin{array}{r}{\\tilde{G}_{t+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\tilde{m}_{t+1}^{i}}\\end{array}$ Thisyields $x_{t+1}=x_{t}-\\epsilon\\mathrm{sign}(\\tilde{G}_{t+1})-\\epsilon\\lambda x_{t}.$ ", "page_idx": 24}, {"type": "text", "text": "Following Theorem B.1 from phase 1, once we have $\\|\\lambda x_{0}\\|_{\\infty}\\leq1$ , we stay within the constraint set with $\\|\\lambda x_{t}\\|\\leq1$ for all subsequent time $t\\geq0$ ", "page_idx": 24}, {"type": "text", "text": "Following the same procedure in B.6, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(x_{i+1})-f(x_{i})\\leq\\eta(f(x_{i}),x_{i+1},x_{j})+\\frac{L}{2}\\|x_{i+1}-x_{i}\\|^{2}}&{}\\\\ {\\leq-\\alpha\\cdot(\\eta(f(x_{i}),x_{j}),\\psi(x_{i+1})+\\lambda_{1})+\\frac{L}{2}\\|x_{i+1}-x_{i}\\|_{2}^{2}}\\\\ {\\leq-(\\eta(f(x_{i}),x_{j}),\\psi(y(\\tau(x_{i}))+x_{i})+\\frac{L}{2}\\|x_{i+1}-x_{i}\\|_{2}^{2}}\\\\ {+\\psi(f^{\\prime}(x_{i}),x_{j})\\#(f(x_{i}),\\cdot-\\psi(x(\\tau(f(x_{i})),\\cdot-\\psi(x(\\tau(x_{i})))}\\\\ {\\cdot\\psi(x_{i})\\log(\\tau(x_{i}),\\cdot\\log(\\tau(x_{i})),\\cdot\\log(\\tau(x_{i+1}))}\\\\ {\\vdots\\psi(x_{i})\\log(\\tau(x_{i}),\\cdot\\log(\\tau(x_{i}))),\\cdot\\log(\\tau(x_{i+1}))}\\\\ {\\mathbb{L}\\exp(\\tau(x_{i}),\\cdot\\log(\\tau(x_{i}))-\\psi(x(\\tau(x_{i})),\\cdot\\log(\\tau(x_{i+1})),\\;\\log(\\tau(x_{i}))-\\psi(x(\\tau(\\hat{x}_{i+1}))}\\\\ {\\mathbb{L}\\exp(\\tau(x_{i}),\\cdot\\log(\\tau(x_{i})))}&{-\\psi(x(\\tau(x_{i})),\\cdot\\log(\\tau(x_{i+1}))}\\\\ {=\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}}&{}\\\\ {\\leq\\eta(\\sum_{k}\\log(\\tau(x_{i})-\\frac{1}{N}\\frac{L}{N}\\psi(x_{i+1}))}&{\\mathbb{L}\\exp(\\tau(x_{i})-1)}\\\\ {\\leq2\\nu\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\left\\|\\nabla f(x_{i})-\\frac{1}{N}\\frac{L}{N}\\psi_{x}\\right\\|}&{\\mathbb{H}\\mathrm{EqG}=\\mathbb{E}\\mathbb{E}\\mathbb{E}\\mathbb{E}\\Bigg\\|\\exp\\left(-1,\\frac{1}{N}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{t+1})-f(x_{t})\\le-\\epsilon S(x_{t})+2L\\epsilon^{2}d+\\epsilon\\langle\\nabla f(x_{t}),\\mathrm{sign}(\\nabla f(x_{t}))-\\tilde{M}_{t+1}\\rangle}\\\\ &{\\phantom{=}\\le-\\epsilon S(x_{t})+2L\\epsilon^{2}d+2\\epsilon\\sqrt{d}\\left(\\beta_{1}\\left(\\beta_{2}^{t}\\|\\nabla f(x_{0})\\|+\\displaystyle\\frac{2L\\epsilon\\sqrt{d}}{1-\\beta_{2}}\\right)+\\frac{(1-\\beta_{1})\\sigma}{\\sqrt{N}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, a telescope yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}S(x_{t})\\leq\\frac{f(x_{0})-f^{*}}{T\\epsilon}+\\frac{2\\beta_{1}\\beta_{2}\\sqrt{d}\\|\\nabla f(x_{0})\\|}{T(1-\\beta_{2})}+\\frac{4\\beta_{1}L\\epsilon d}{1-\\beta_{2}}+\\frac{2(1-\\beta_{1})\\sqrt{d}\\sigma}{\\sqrt{N}}+2L\\epsilon d.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The claims are supported with theoretical and empirical results ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the Conclusion section, we mentioned that Distributed Lion can be further improved when combined with compression techniques. Currently, a limitation is that D-Lion (Avg) and D-Lion (MaVo) perform inconsistently across datasets and benchmarks, and it will be good to understand why in future work. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We list our assumptions and results explicit in the theory section. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the benchmark, algorithm, and hyperparameters for reproducing ourresults. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: All the data we use are public. We will release code upon acceptance. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: answerYes ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the details for training and testing in the experiment section for reproducing our results. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: We average the results over 3 seeds and report the mean in Figure 2 for the CIFAR experiment. But for larger scale experiment, it is extremely computationally expensive to conduct the experiments multiple times, and it is known that the result is relatively stable. Hence we only run once for each large-scale experiment. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provided how many workers are needed for each experiment, the GPU resource can be arbitrary as long as it fits in memory. ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We think our paper confirms in every respect with the Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not think our work leads to any negative societal impact. ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: We think the paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we cite the data and benchmarks we use, and the baseline methods we compare against. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer:[NA] .   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}]