[{"figure_path": "wDirCeTIoz/figures/figures_0_1.jpg", "caption": "Figure 1: Illustration of Distributed-Lion. Each worker keeps its own optimizer state and applies the Lion optimizer individually to a binary update d<sub>i,t</sub> = Lion(x<sub>i</sub>, D<sub>i</sub>) (without the weight decay), then the server aggregates all d<sub>i,t</sub> to produce a binary \u2206<sub>t</sub> by majority vote (or an integer \u2206<sub>t</sub> by averaging) and send it back to all workers. The workers then apply \u2206<sub>t</sub> and weight decay to update their model parameters (Algorithm 1).", "description": "This figure illustrates the workflow of the Distributed Lion algorithm. Each worker maintains its own optimizer state and computes a binary update vector using the Lion optimizer. These vectors are then sent to a central server which aggregates them using either majority voting or averaging to produce a final update vector. This final vector is then sent back to the workers to update their model parameters. This process significantly reduces communication cost compared to traditional methods.", "section": "2 The Distributed Lion"}, {"figure_path": "wDirCeTIoz/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of Distributed-Lion. Each worker keeps its own optimizer state and applies the Lion optimizer individually to a binary update \n\u03b4i,t = Lion(xi, Di) (without the weight decay), then the server aggregates all \u03b4i,t to produce a binary \u2206t by majority vote (or an integer \u2206t by averaging) and send it back to all workers. The workers then apply \u2206t and weight decay to update their model parameters (Algorithm 1).", "description": "The figure illustrates the architecture of Distributed Lion. Multiple worker nodes each run a local instance of the Lion optimizer, producing a binary update vector. These vectors are sent to a central server, which aggregates them using either majority voting or averaging, resulting in a final update vector. The server then distributes this aggregated vector back to the worker nodes for model parameter updates. This process minimizes communication overhead by transmitting only low-precision vectors.", "section": "2 The Distributed Lion"}, {"figure_path": "wDirCeTIoz/figures/figures_7_1.jpg", "caption": "Figure 2: Performance of Distributed Lion v.s. baseline distributed optimizers on CIFAR-10 with 4, 8, 16, and 32 workers, each worker at each step runs on a local batch with size 32. All results are averaged over three seeds.", "description": "This figure compares the performance of Distributed Lion (with averaging and majority vote aggregation methods) against several baseline distributed optimizers on the CIFAR-10 dataset.  The experiment varies the number of workers (4, 8, 16, and 32), and each worker processes a local batch size of 32. The results, averaged over three random seeds, illustrate the test accuracy over 200 epochs for each method.  This helps to visualize the convergence speed and final accuracy of different optimization strategies in a distributed setting.", "section": "5.1 Comparing Distributed Lion Against Established Methods on CIFAR-10"}, {"figure_path": "wDirCeTIoz/figures/figures_7_2.jpg", "caption": "Figure 3: Performance of G-Lion, G-AdamW, Grad-Drop, DGC, TernGrad, and D-Lion (Avg/MaVo) v.s. the number of workers k.", "description": "This figure compares the performance of different distributed optimizers against the number of workers used.  The plot shows that the Global Lion (G-Lion) and Global AdamW (G-AdamW) optimizers consistently outperform most of the communication-efficient methods, specifically TernGrad, GradDrop, and DGC.  However, the Distributed Lion methods (D-Lion (Avg) and D-Lion (MaVo)), using either averaging or majority voting aggregation, demonstrate a competitive performance, particularly D-Lion(MaVo),  approaching the performance of the global methods. This highlights the effectiveness of Distributed Lion in reducing communication overhead while maintaining good performance.", "section": "5.1 Comparing Distributed Lion Against Established Methods on CIFAR-10"}, {"figure_path": "wDirCeTIoz/figures/figures_7_3.jpg", "caption": "Figure 4: Test Error v.s. Communication Bits per Iteration (closer to the lower-left is better). Note that we set G-Lion and G-AdamW are both 64, because they require 32 bits per parameter, and there are both worker-to-server and server-to-worker communications.", "description": "This figure compares the performance (test error) of various distributed optimization methods against their communication cost (bits per iteration).  It shows that Distributed Lion (MaVo and Avg) achieve a favorable trade-off, attaining comparable performance to global methods like G-Lion and G-AdamW while using significantly less communication bandwidth. Other low-bandwidth methods such as TernGrad, GradDrop, and DGC are also included for comparison.", "section": "5.1 Comparing Distributed Lion Against Established Methods on CIFAR-10"}]