[{"figure_path": "wDirCeTIoz/tables/tables_2_1.jpg", "caption": "Table 1: Minimum bandwidth requirements of different methods for a model with d parameters and n workers. For Deep Gradient Compression (DGC), \u03b7 denotes the compression rate (default: \u03b7 = 0.96).", "description": "This table compares the bandwidth requirements of various distributed training methods, including the proposed Distributed Lion and existing methods such as Global Lion/AdamW, TernGrad, and DGC.  It shows the amount of data transferred between workers and the server for both sending gradients and receiving updates.  The key takeaway is that Distributed Lion significantly reduces the communication overhead compared to the baseline methods.", "section": "2 The Distributed Lion"}, {"figure_path": "wDirCeTIoz/tables/tables_8_1.jpg", "caption": "Table 2: Results on ImageNet classification and OpenWebText language modeling. For ImageNet experiments, we report the Top-1 accuracy. For language modeling experiments, we report the validation perplexity. The best performance is marked with bold text, and the second best with an underline.", "description": "This table presents the results of experiments conducted on ImageNet (image classification) and OpenWebText (language modeling) datasets using different optimization methods: AdamW, G-Lion, D-Lion (MaVo), and D-Lion (Avg).  For ImageNet, Top-1 accuracy is reported. For language modeling, validation perplexity is shown.  The best performing method for each task and model size is highlighted in bold, while the second-best is underlined.  This table allows comparison of the performance and efficiency of different optimizers on large-scale tasks.", "section": "5.2 Scale to Larger Models on Larger Datasets"}, {"figure_path": "wDirCeTIoz/tables/tables_8_2.jpg", "caption": "Table 3: 3-Shot instruction finetuning downstream evaluation results on various datasets. We mark the best performance with bold text and the second one with an underline.", "description": "This table presents the results of a 3-shot instruction finetuning experiment on various downstream datasets.  The models were finetuned using different optimization methods: G-AdamW, G-Lion, D-Lion (MaVo), and D-Lion (Avg).  The table displays the performance of each method on several datasets, including Arc-Easy, Arc-Challenge, BoolQ, PIQA, SIQA, HellaSwag, and OBQA.  The best performing method for each dataset is highlighted in bold, and the second best is underlined.  This allows for a direct comparison of the performance and relative effectiveness of the different optimization methods in a few-shot learning context.", "section": "5.2 Scale to Larger Models on Larger Datasets"}, {"figure_path": "wDirCeTIoz/tables/tables_13_1.jpg", "caption": "Table 4: Hyperparameters for each method in Figure 2. Where LR represents learning rate and WD represents weight decay.", "description": "This table lists the hyperparameters used for each optimization method in the experiments shown in Figure 2 of the paper.  It includes the learning rate (LR), weight decay (WD), and compression rate for each method.  The compression rate is relevant for methods that employ gradient compression techniques (DGC, GradDrop).  The table clarifies the settings used to produce the results presented visually in the accompanying figure.", "section": "A Additional Experiment Details"}]