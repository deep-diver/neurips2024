{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper is foundational to the field of scaling laws for LLMs, introducing the concept and providing initial empirical observations that motivate the current work."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper significantly advanced the understanding of scaling laws by demonstrating the importance of compute-optimal training data size and model parameters, which is directly relevant to the current research."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced the Llama family of LLMs which serves as the basis for experiments in the current work and contributes to the context of the vocabulary scaling investigation."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper describes the Llama 2 models, a significant improvement to Llama, used as a key model for validation in the current work."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-06", "reason": "This paper introduced the Transformer architecture, which is the dominant architecture for large language models and thus forms the basis for the models used in this work."}]}