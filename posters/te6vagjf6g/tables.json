[{"figure_path": "te6VagJf6G/tables/tables_3_1.jpg", "caption": "Table 1: Benchmark results by COGEX models optimized for each dataset using the COTACS method, compared to the corresponding off-the-shelf Llama-2 checkpoint performing 2-shot reasoning using a BM25 retrieval index of 1000 exemplars. Results are also compared to a zero-shot Alpaca model fine-tuned from the same checkpoint. The top score per size is bolded. Colored cells indicate changes (gains, losses, or the same) relative to the best-performing non-COGEX baseline (Alpaca or 2-shot). Results show that COGEX with CoTACS outperforms the baselines on nearly every task and often does so even with only 10 examples.", "description": "This table presents a comparison of the performance of COGEX models (with COTACS optimization) against Llama-2 (with BM25 and 2-shot prompting) and Alpaca (zero-shot) baselines across various reasoning tasks.  It highlights the improvements achieved by COGEX, especially with a larger number of training examples, showcasing its superior performance in various task categories including classification, symbolic math, and commonsense reasoning.", "section": "Experiments and Results"}, {"figure_path": "te6VagJf6G/tables/tables_4_1.jpg", "caption": "Table 1: Benchmark results by COGEX models optimized for each dataset using the COTACS method, compared to the corresponding off-the-shelf Llama-2 checkpoint performing 2-shot reasoning using a BM25 retrieval index of 1000 exemplars. Results are also compared to a zero-shot Alpaca model fine-tuned from the same checkpoint. The top score per size is bolded. Colored cells indicate changes (gains, losses, or the same) relative to the best-performing non-COGEX baseline (Alpaca or 2-shot). Results show that COGEX with CoTACS outperforms the baselines on nearly every task and often does so even with only 10 examples.", "description": "This table compares the performance of COGEX models (fine-tuned using the COTACS algorithm) against baseline models (Llama-2 with BM25 and zero-shot Alpaca) across various reasoning tasks.  It shows the improvement achieved by COGEX, particularly when using 1000 training examples. Colored cells highlight performance differences compared to the best-performing baseline.", "section": "Experiments and Results"}, {"figure_path": "te6VagJf6G/tables/tables_5_1.jpg", "caption": "Table 2: Difference in performance by COTACS k = 3 comparing Llama-2 vs Code Llama COGEX models ('+' implies Llama-2 better). We see that Code Llama is more effective for some tasks but worse on others, while the 13B version performs worse than the 13B Llama-2 on all but 2 tasks.", "description": "This table compares the performance difference between Llama-2 and Code Llama models when using COTACS (k=3) across various tasks. It shows that Code Llama sometimes outperforms Llama-2 but not always, highlighting the task-specific nature of model effectiveness and indicating that the 13B Code Llama model is generally underperforming in comparison to its Llama-2 counterpart.", "section": "3 Experiments and Results"}]