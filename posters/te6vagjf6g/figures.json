[{"figure_path": "te6VagJf6G/figures/figures_1_1.jpg", "caption": "Figure 1: Example from the COGEX dataset automatically converted from an Alpaca (Taori et al., 2023) instance via LLM prompting. We train the model to receive the instruction and input and generate the Python program and function call (as an intermediate), before outputting the final dictionary that contains the answer and any intermediate reasoning steps.", "description": "This figure illustrates the transformation of an Alpaca instance into a COGEX instance.  An Alpaca instance consists of an instruction, input, and expected output. The COGEX instance expands this by generating a Python program designed to solve the problem, showing intermediate steps in a dictionary output.  This demonstrates how COGEX utilizes program generation and emulation to address reasoning tasks.", "section": "2 Approach"}, {"figure_path": "te6VagJf6G/figures/figures_6_1.jpg", "caption": "Figure 2: Change in COTACS performance using Llama-2 13B as we increase the number of training items from 5 to 1000 and program candidates from 3 to 300. Results are averaged across 1000 trials.", "description": "This figure displays the results of an ablation study on the COTACS algorithm, showing how the number of training examples and program candidates impact performance.  The results are presented across seven different tasks and averaged over 1000 trials. The x-axis represents the number of training items, while the y-axis represents the performance metric (likely accuracy).  Different colored lines represent different numbers of code candidates considered during the search phase of the COTACS algorithm. The figure helps to understand the trade-off between computational cost and model performance in relation to training data and program search space.", "section": "3 Experiments and Results"}, {"figure_path": "te6VagJf6G/figures/figures_7_1.jpg", "caption": "Figure 3: Example COGEX model-generated programs for Social IQa (Sap et al., 2019) questions. The left item fits well to a specific SocialIQa question pertaining to question-specific entities but does not generalize well to the dataset, while the right item applies more generally to cases such as the instance shown at the bottom, which does not pertain to character actions. Applying COTACS to identify a single program such as the right one shows to improve overall task accuracy.", "description": "This figure shows two example programs generated by the COGEX model for the Social IQa dataset. The left program is very specific to the example question, while the right program is more general and applicable to a wider range of questions.  The figure highlights the benefit of using the COTACS algorithm to select a single, generalizable program, improving overall accuracy compared to using question-specific programs.", "section": "3 Experiments and Results"}, {"figure_path": "te6VagJf6G/figures/figures_7_2.jpg", "caption": "Figure 5: Performance tradeoff between COTACS, which requires saving just a program string, and fine-tuning, which requires saving an entire checkpoint, as we increase the number of training examples. Although fine-tuning typically performs better with more data, COTACS provides an alternative that is lighter-weight and stronger at low-to-medium numbers of instances.", "description": "This figure compares the performance of COTACS (a program search method) and fine-tuning on various tasks with different numbers of training examples.  It shows that while fine-tuning generally achieves higher accuracy with more data, COTACS offers a competitive advantage, particularly when training data is limited. COTACS's advantage lies in its lightweight nature, requiring only a program string to be saved, unlike fine-tuning which demands saving an entire checkpoint.", "section": "3 Experiments and Results"}, {"figure_path": "te6VagJf6G/figures/figures_8_1.jpg", "caption": "Figure 5: Performance tradeoff between COTACS, which requires saving just a program string, and fine-tuning, which requires saving an entire checkpoint, as we increase the number of training examples. Although fine-tuning typically performs better with more data, COTACS provides an alternative that is lighter-weight and stronger at low-to-medium numbers of instances.", "description": "This figure compares the performance of COTACS and fine-tuning across multiple datasets as the number of training examples increases. It shows that fine-tuning generally outperforms COTACS with larger datasets, while COTACS is a more lightweight and effective alternative for smaller to medium sized datasets.", "section": "3 Experiments and Results"}, {"figure_path": "te6VagJf6G/figures/figures_8_2.jpg", "caption": "Figure 1: Example from the COGEX dataset automatically converted from an Alpaca (Taori et al., 2023) instance via LLM prompting. We train the model to receive the instruction and input and generate the Python program and function call (as an intermediate), before outputting the final dictionary that contains the answer and any intermediate reasoning steps.", "description": "This figure shows an example of how an Alpaca instance is converted into a COGEX instance.  The Alpaca instance contains an instruction and input. The COGEX instance shows the process of generating a Python program and function call, emulating the execution, and finally outputting a dictionary containing the answer and intermediate reasoning steps. This illustrates the core functionality of the COGEX approach, which involves using language models to generate and execute pseudo-programs to solve reasoning tasks.", "section": "2.1 Method: COGEX"}]