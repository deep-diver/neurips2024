{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, the large language model used as the base model for the COGEX experiments, significantly impacting the results and analysis."}, {"fullname_first_author": "Rohan Taori", "paper_title": "Alpaca: A strong, replicable instruction-following model", "publication_date": "2023-03-13", "reason": "The Alpaca dataset, a key resource in this work, is introduced and described in this paper, directly influencing the training and evaluation of the COGEX models."}, {"fullname_first_author": "Hyungjoo Chae", "paper_title": "Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models", "publication_date": "2024-04-02", "reason": "This paper explores a related approach of using language models to execute code, providing a relevant comparison and context for the COGEX method."}, {"fullname_first_author": "Lichang Chen", "paper_title": "InstructZero: Efficient instruction optimization for black-box large language models", "publication_date": "2023-06-03", "reason": "This paper investigates instruction optimization techniques, relevant to the COTACS program search method used in the COGEX framework."}, {"fullname_first_author": "Shivanshu Gupta", "paper_title": "Coverage-based example selection for in-context learning", "publication_date": "2023-12-01", "reason": "This paper addresses example selection strategies for in-context learning, a technique relevant to the COTACS method's program search and dataset optimization."}]}