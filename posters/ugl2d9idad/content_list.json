[{"type": "text", "text": "FilterNet: Harnessing Frequency Filters for Time Series Forecasting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kun $\\mathbf{Y}\\mathbf{i}^{1,2}$ , Jingru Fei3, Qi Zhang4, Hui $\\mathbf{He}^{3}$ , Shufeng $\\mathbf{Hao^{5}}$ , Defu Lian6, Wei $\\mathbf{Fan}^{7*}$ ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1North China Institute of Computing Technology, 2State Information Center of China 3Beijing Institute of Technology, 4Tongji University, 5Taiyuan University of Technology 6University of Science and Technology of China, 7University of Oxford ", "page_idx": 0}, {"type": "text", "text": "kunyi.cn $@$ gmail.com, {jingrufei, hehui617} $@$ bit.edu.cn, zhangqi_cs $@$ tongji.edu.cn haoshufeng $@$ tyut.edu.cn, liandefu $@$ ustc.edu.cn, weifan.oxford $@$ gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given the ubiquitous presence of time series data across various domains, precise forecasting of time series holds significant importance and finds widespread real-world applications such as energy, weather, healthcare, etc. While numerous forecasters have been proposed using different network architectures, the Transformer-based models have state-of-the-art performance in time series forecasting. However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series with thousands of points. In this paper, we explore a novel perspective of enlightening signal processing for deep time series forecasting. Inspired by the filtering process, we introduce one simple yet effective network, namely FilterNet, built upon our proposed learnable frequency fliters to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals. Concretely, we propose two kinds of learnable filters in the FilterNet: (i) Plain shaping fliter, that adopts a universal frequency kernel for signal filtering and temporal modeling; (ii) Contextual shaping filter, that utilizes filtered frequencies examined in terms of its compatibility with input signals for dependency learning. Equipped with the two filters, FilterNet can approximately surrogate the linear and attention mappings widely adopted in time series literature, while enjoying superb abilities in handling high-frequency noises and utilizing the whole frequency spectrum that is beneficial for forecasting. Finally, we conduct extensive experiments on eight time series forecasting benchmarks, and experimental results have demonstrated our superior performance in terms of both effectiveness and efficiency compared with state-of-the-art methods. Our code is available at 1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting has been playing a pivotal role across a multitude of contemporary applications, spanning diverse domains such as climate analysis [1], energy production [2], traffic flow patterns [3], financial markets [4], and various industrial systems [5]. The ubiquity and profound significance of time series data has recently garnered substantial research efforts, culminating in a plethora of deep learning forecasting models [6] that have significantly enhanced the domain of time series forecasting. ", "page_idx": 0}, {"type": "text", "text": "Previously, leveraging different kinds of deep neural networks derives a series of time series forecasting methods, such as Recurrent Neural Network-based methods including DeepAR [7], LSTNet [8], ", "page_idx": 0}, {"type": "image", "img_path": "ugL2D9idAD/tmp/4097763ff65ecc6452a3982dcf95223dbb136d19651141cda50b013215a91466.jpg", "img_caption": ["(a) The spectrum of input signal(b) iTransformer with MSE=1.1e-01 (c) FilterNet with MSE=2.7e-05 ", "Figure 1: Performance of Mean Squared Error (MSE) on a simple synthetic multi-frequency signal. More details about the experimental settings can be found in Appendix C.4. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Convolution Neural Network-based methods including TCN [9], SCINet [10], etc. Recently, however, with the continuing advancement of deep learning, two branches of methods that received particularly more attention have been steering the development of time series forecasting, i.e., Multilayer Perceptron (MLP)-based methods, such as N-BEATS [11], DLinear [12], and FreTS [13], and Transformerbased methods, such as Informer [14], Autoformer [15], PatchTST [16], and iTransformer [17]. While MLP-based models are capable of providing accurate forecasts, Transformer-based models continue to achieve state-of-the-art time series forecasting performance. ", "page_idx": 1}, {"type": "text", "text": "However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series composed of thousands of timesteps. In designing a very simple simulation experiment on the synthetic data only composed of a low-, middle- and high-frequency signal respectively (see Figure 1(a)), we find the state-of-the-art iTransformer [17] model performs much worse in forecasting (Figure 1(b) and Figure 1(c)). This observation shows that state-of-the-art Transformer-based model cannot utilize the full spectrum information, even for a naive signal of three different frequency components. In contrast, in the field of signal processing, a frequency filter enjoys many good properties such as frequency selectivity, signal conditioning, and multi-rate processing. These could have great potential in advancing the model\u2019s ability to extract key informative frequency patterns in time series forecasting. ", "page_idx": 1}, {"type": "text", "text": "Thus, inspired by the flitering process [18] in signal processing, in this paper, we introduce one simple yet effective framework, namely FilterNet, for effective time series forecasting. Specifically, we start by proposing two kinds of learnable filters as the key units in our framework: (i) Plain shaping fliter, which makes the naive but universal frequency kernel learnable for signal flitering and temporal modeling, and (ii) Contextual shaping fliter, which utilizes flitered frequencies examined in terms of its compatibility with input signals for dependency learning. The plain shaping filter is more likely to be adopted in predefined conditions and efficient in handling simple time series structures, while the contextual filter can adaptively weight the filtering process based on the changing conditions of input and thus have more flexibility in facing more complex situations. Besides, these two filters as the built-in functions of the FilterNet can also approximately surrogate the widely adopted linear mappings and attention mappings in time series literature [12, 14, 17]. This also illustrates the effectiveness of our FilterNet in forecasting by selectively passing or attenuating certain signal components while capturing the core time series structure with adequate learning expressiveness. Moreover, since filters are better fit in the stationary frequency domain, we let filters wrapped by two reversible transformations, i.e., instance normalization [19] and fast Fourier transform [20] to reduce the influence of non-stationarity and accomplish the domain transformation of time series respectively. In summary, our contributions can be listed as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In studying state-of-the-art deep Transformer-based time series forecasting models, an interesting observation from a simple simulation experiment motivates us to explore a novel perspective of enlightening signal processing techniques for deep time series forecasting.   \n\u2022 Inspired by the flitering process in signal processing, we introduce a simple yet effective network, FilterNet, built upon our proposed two learnable frequency filters to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals, thereby enhancing the forecasting performance.   \n\u2022 We conduct extensive experiments on eight time series forecasting benchmarks, and the results have demonstrated that our model achieves superior performance compared with state-of-the-art forecasting algorithms in terms of effectiveness and efficiency. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep Learning-based Time Series Forecasting In recent years, deep learning-based methods have gained prominence in time series forecasting due to their ability to capture nonlinear and complex correlations [21]. These methods have employed various network architectures to learn temporal dependencies, such as RNN [8, 7], TCN [9, 10], etc. Notably, MLP- and Transformerbased methods have achieved competitive performance, emerging as state-of-the-art approaches. N-HiTS [22] integrates multi-rate input sampling and hierarchical interpolation with MLPs to enhance univariate forecasting. DLinear [12] introduces a simple approach using a single-layer linear model to capture temporal relationships between input and output time series data. RLinear [23] utilizes linear mapping to model periodic features, demonstrating robustness across diverse periods with increasing input length. In contrast to the simple structure of MLPs, Transformer\u2019s advanced attention mechanisms [24] empower the models [14, 15, 25, 26] to capture intricate dependencies and longrange interactions. PatchTST [16] segments time series into patches as input tokens to the Transformer and maintaining channel independence. iTransformer [17] inverts the Transformer\u2019s structure by treating independent series as variate tokens to capture multivariate correlations through attention. ", "page_idx": 2}, {"type": "text", "text": "Time Series Modeling with Frequency Learning In recent developments, frequency technology has been increasingly integrated into deep learning models, significantly improving state-of-theart accuracy and efficiency in time series analysis [27]. These models leverage the benefits of frequency technology, such as high efficiency [28, 29] and energy compaction [13], to enhance forecasting capabilities. Concretely, Autoformer [15] introduces the auto-correlation mechanism, improving self-attention implemented with Fast Fourier Transforms (FFT). FEDformer [25] enhances attention with a FFT-based frequency approach, determining attentive weights from query and key spectrums and conducting weighted summation in the frequency domain. DEPTS [30] utilizes Discrete Fourier Transform (DFT) to extract periodic patterns and contribute them to forecasting. FiLM [31] employs Fourier analysis to retain historical information while flitering out noisy signals. FreTS [13] introduces a frequency-domain Multi-Layer Perceptrons (MLPs) to learn channel and temporal dependencies. FourierGNN [29] transfers the operations of graph neural networks (GNNs) from the time domain to the frequency domain. FITS [32] applies a low pass filter to the input data followed by complex-valued linear mapping in the frequency domain. ", "page_idx": 2}, {"type": "text", "text": "Unlike these methods, in this paper we propose a simple yet effective model FilterNet developed from a signal processing perspective, and apply all-pass frequency filters to design the network directly, rather than incorporating them into other network architectures, such as Transformers, MLPs, or GNNs, or utilizing them as low-pass filters, as done in FITS [32] and FiLM [31]. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Frequency Filters Frequency filters [33] are mathematical operators designed to modify the spectral content of signals. Specifically, given an input time series signal $x[n]$ with its corresponding Fourier transform $\\mathcal{X}[k]$ , a frequency filter $\\mathcal{H}[k]$ is applied to the signal to produce an output signal $y[n]$ with its corresponding Fourier transform $\\mathcal{\\bar{V}}[k]=\\mathcal{X}[k]\\mathcal{H}[k]$ . The frequency fliter $\\mathcal{H}[k]$ alters the amplitude and phase of specific frequency components in the input time series signal $x[n]$ according to its frequency response characteristics, thereby shaping the spectral content of the output signal. ", "page_idx": 2}, {"type": "text", "text": "According to the Convolution Theorem [34], the point-wise multiplication in the frequency domain corresponds to the circular convolution operation between two corresponding signals in the time domain. Consequently, the frequency filter process can be expressed in the time domain as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}[k]=\\mathcal{H}[k]\\mathcal{X}[k]\\leftrightarrow y[n]=h[n]\\circledast x[n],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h[n]$ is the inverse Fourier transform of $\\mathcal{H}[k]$ and $\\circledast$ denotes the circular convolution operation. This formulation underscores the intrinsic connections between the frequency filter process and the circular convolution in the time domain, and it indicates that the frequency fliter process can efficiently perform circular convolution operations. In time series forecasting, Transformer-based methods have achieved state-of-the-art performance, largely due to the self-attention mechanism [14, 15, 25, 17], which can be interpreted as a form of global circular convolution [35]. This perspective opens up the possibility of integrating frequency filter technologies, which are well-known for their ability to isolate and enhance specific signal components, to further improve time series forecasting models. ", "page_idx": 2}, {"type": "image", "img_path": "ugL2D9idAD/tmp/ba19ccc4bc5cb55aa692c910d8e472915245d3f7a273167c583760f3d45eb7ce.jpg", "img_caption": ["Figure 2: The overall architecture of FilterNet. (i) Instance normalization is employed to address the non-stationarity among time series data; (ii) The frequency filter block is applied to capture the temporal patterns, which has two different implementations, i.e., plain shaping filter and contextual shaping filter; (iii) Feed-forward network is adopted to project the temporal patterns extracted by frequency filter block back onto the time series data and make predictions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As aforementioned, frequency filters enjoy numerous advantageous properties for time series forecasting, functioning equivalently to circular convolution operations in the time domain. Therefore, we design the time series forecaster from the perspective of frequency filters. In this regard, we propose FilterNet, a forecasting architecture grounded in frequency filters. First, we introduce the overall architecture of FilterNet in Section 4.1, which primarily comprises the basic blocks and the frequency filter block. Second, we delve into the details of two types of frequency filter blocks: the plain shaping fliter presented in Section 4.2 and the contextual shaping fliter discussed in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overall architecture of FilterNet is depicted in Figure 2, which mainly consists of the instance tniomrem asleirziaetsi oinn ppuatr $\\mathbf{X}=[\\dot{X_{1}}^{1:L},\\dot{X}_{2}^{1:L},...,X_{N}^{1:L}]\\in\\mathbb{R}^{N\\times L}$ -fwoirthw atrhde  nneutwmobrekr.  oSfp evcairfiaicballleys, $N$ r aan gdi vtehne lookback window length $L$ , where $X_{N}^{1:L}\\in\\mathbb{R}^{L}$ denotes the -th variable, we employ FilterNet to predict the future \u03c4 time steps Y = [X1L+1:L+\u03c4, X2L+1:L+\u03c4, ... $\\mathbf{Y}=[\\lambda_{1}^{L+1:L+\\tau},X_{2}^{L+1:L+\\tau},...,X_{N}^{L+1:L+\\tau}]\\in\\mathbb{R}^{N\\times\\tau}$ XNL+1:L+\u03c4] \u2208RN\u00d7\u03c4. We provide further analysis about the architecture design of FilterNet in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Instance Normalization Non-stationarity is widely existing in time series data and poses a crucial challenge for accurate forecasting [19, 36]. Considering that time series data are typically collected over a long duration, these non-stationary sequences inevitably expose forecasting models to distribution shifts over time. Such shifts can result in performance degradation during testing due to the covariate shift or the conditional shift [37]. To address this problem, we utilize an instance normalization method, denoted as Norm, on the time series input $\\mathbf{X}$ , which can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Norm}(\\mathbf{X})=[\\frac{X_{i}^{1:L}-\\mathrm{Mean}_{L}(X_{i}^{1:L})}{\\mathrm{Std}_{L}(X_{i}^{1:L})}]_{i=1}^{N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\mathrm{Mean}}_{L}$ denotes the operation that calculates the mean value along the time dimension, and $\\mathrm{Std}_{L}$ represents the operation that calculates the standard deviation along the time dimension. ", "page_idx": 3}, {"type": "text", "text": "Correspondingly, the inverse instance normalization, denoted as InverseNorm, is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{InverseNorm}(\\mathbf{P})=[P_{i}^{L+1:L+\\tau}\\times\\mathrm{Std}_{L}(X_{i}^{1:L})+\\mathrm{Mean}_{L}(X_{i}^{1:L})]_{i=1}^{N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{P}=[P_{1}^{L+1:L+\\tau},P_{2}^{L+1:L+\\tau},...,P_{N}^{L+1:L+\\tau}]\\in\\mathbb{R}^{N\\times\\tau}$ P NL+1:L+\u03c4] \u2208RN\u00d7\u03c4 are the predictive values. ", "page_idx": 3}, {"type": "image", "img_path": "ugL2D9idAD/tmp/0852a222eb4792a2ee5a5159d209fcd1b8a94b6a71ae7b506d85aeffe5bce9f5.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: The structure of frequency fliters. (a) Plain shaping fliter: the plain shaping fliter is initialized randomly with channel-shared (left) or channel-unique (right) parameters, and then performs circular convolution (i.e., the symbol $\\circledast$ ) with the input time series; (b) Contextual shaping fliter: the contextual shaping filter firstly learns a data-dependent filter and then conducts multiplication (i.e., the symbol $\\odot$ ) with the frequency representation of the input time series. ", "page_idx": 4}, {"type": "text", "text": "Frequency Filter Block Previous representative works primarily leverage MLP architectures (e.g., DLinear [12], RLinear [23]) or Transformer architectures (e.g., PatchTST [16], iTransformer [17]) to model the temporal dependencies among time series data. As mentioned earlier, time series forecasters can be implemented through performing a frequency fliter process in the frequency domain, and thus we propose to directly apply the frequency fliter in the frequency domain, denoted as FilterBlock, to replace the aforementioned methods for modeling corresponding temporal dependencies, such as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FilterBlock}(\\mathbf{Z})=\\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{Z})\\mathcal{H}_{f i l t e r}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{F}$ is Fourier transform, ${\\mathcal{F}}^{-1}$ is inverse Fourier transform and $\\mathcal{H}_{f i l t e r}$ is the frequency filter. ", "page_idx": 4}, {"type": "text", "text": "Inspired by MLP that randomly initializes a learnable weight parameters and Transformer that learns the data-dependent attention scores from data (further explanations are provided in Appendix B), we introduce two types of frequency fliters, i.e., plain shaping fliter (PaiFilter) and contextual shaping filter (TexFilter). PaiFilter applies a random initialized learnable weight $\\mathcal{H}_{\\phi}$ to instantiate the frequency filter $\\mathcal{H}_{f i l t e r}$ , and then the frequency filter process is reformulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FilterBlock}(\\mathbf{Z})=\\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{Z})\\mathcal{H}_{\\phi}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "TexFilter learns a data-dependent frequency fliter $\\mathcal{H}_{\\varphi}(\\mathcal{F}(\\mathbf{Z}))$ from the input data by using a neural network $\\mathcal{H}_{\\varphi}()$ , and then the corresponding frequency filter process is reformulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FilterBlock}(\\mathbf{Z})=\\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{Z})\\mathcal{H}_{\\varphi}(\\mathcal{F}(\\mathbf{Z}))).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Feed-forward Network The frequency filter block has captured temporal dependencies among time series data, and then we employ a feed-forward network (FFN) to project them back onto the time series data and make predictions for the future $\\tau$ time steps. As the output $\\mathbf{P}$ of FFN are instance-normalized values, we conduct an inverse instance normalization operation (InverseNorm) on them and obtain the final predictions Y\u02c6. The entire process can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{P}=\\mathrm{FFN}(\\mathbf{S}),}\\\\ &{\\hat{\\mathbf{Y}}=\\mathrm{Inversel}\\mathrm{vorm}(\\mathbf{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Plain Shaping Filter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "PaiFilter instantiates the frequency filter by randomly initializing learnable parameters and then performing multiplication with the input time series. In general, for multivariate time series data, the channel-independence strategy in channel modeling has proven to be more effective compared to the channel-mixing strategy [12, 16]. Following this principle, we also adopt the channel-independence strategy for designing the frequency fliter. Specifically, we propose two types of plain shaping fliters: the universal type, where parameters are shared across different channels, and the individual type, where parameters are unique to each channel, as illustrated in Figure 3(a). ", "page_idx": 4}, {"type": "text", "text": "Given the time series input $\\mathbf{Z}\\in\\mathbb{R}^{N\\times L}$ and the plain shaping filter $\\mathcal{H}_{\\phi}$ , we apply PaiFilter by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mathcal{Z}}=\\mathcal{F}(\\mathbf{Z}),}\\\\ &{\\boldsymbol{S}=\\mathcal{Z}\\odot_{L}\\mathcal{H}_{\\phi},\\quad\\mathcal{H}_{\\phi}\\in\\{\\mathcal{H}_{\\phi}^{(U n i)},\\mathcal{H}_{\\phi}^{(I n d)}\\}}\\\\ &{\\mathbf{S}=\\mathcal{F}^{-1}(\\boldsymbol{S}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{F}$ is Fourier transform, ${\\mathcal{F}}^{-1}$ is inverse Fourier transform, $\\odot_{L}$ denotes the element-wise product $L$ $\\mathcal{H}_{\\phi}^{(U n i)}\\,\\in\\,\\mathbb{C}^{1\\times L}$ $\\mathcal{H}_{\\phi}^{(I n d)}\\,\\in\\,\\mathbb{C}^{N\\times\\bar{L}}$ is the individual plain shaping filter, and $\\mathbf{S}\\in\\mathbb{R}^{N\\times L}$ is the output of PaiFilter. We further compare and analyze the two types of PaiFilter in Section 5.3. ", "page_idx": 5}, {"type": "text", "text": "4.3 Contextual Shaping Filter ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In contrast to PaiFilter, which randomly initializes the parameters of frequency filters and fixes them after training, TexFilter learns the parameters generated from the input data, allowing for better adaptation to the data. Consequently, we devise a neural network $\\mathcal{H}_{\\varphi}$ that flexibly adjusts the frequency filter in response to the input data, as depicted in Figure 3(b). ", "page_idx": 5}, {"type": "text", "text": "Given the time series input $\\mathbf{Z}\\,\\in\\,\\mathbb{R}^{N\\times L}$ and its corresponding Fourier transform denoted as $\\mathcal{Z}=$ $\\mathcal{F}(\\mathbf{Z})\\,\\in\\,\\mathbb{C}^{N\\times L}$ , the network $\\mathcal{H}_{\\varphi}$ is utilized to derive the contextual shaping filter, expressed as $\\mathcal{H}_{\\varphi}:\\mathbb{C}^{N\\times L}\\mapsto\\mathbb{C}^{N\\times D}$ . First, it embeds the raw data by a linear dense operation $\\kappa:\\mathbb{C}^{L}\\mapsto\\mathbb{C}^{D}$ to improve the capability of modeling complex data. Then, it applies a series of complex-value multiplication with $K$ learnable parameters $\\b{\\mathscr{W}_{1:K}}\\in\\mathbb{C}^{1\\times D}$ yielding $\\sigma(\\kappa(\\mathcal{Z})\\odot\\mathcal{W}_{1:K})$ where $\\sigma$ is the activation function, and finally outputs $\\mathcal{H}_{\\varphi}(\\mathcal{Z})$ . Then we apply TexFilter by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\mathcal{Z}}=\\boldsymbol{\\mathcal{F}}(\\mathbf{Z}),}\\\\ &{\\boldsymbol{\\mathcal{E}}=\\kappa(\\boldsymbol{\\mathcal{Z}}),}\\\\ &{\\mathcal{H}_{\\varphi}(\\boldsymbol{\\mathcal{Z}})=\\sigma(\\boldsymbol{\\mathcal{E}}\\odot_{D}\\mathcal{W}_{1:K}),\\quad\\mathcal{W}_{1:K}=\\displaystyle\\prod_{i=1}^{K}\\mathcal{W}_{i}}\\\\ &{\\boldsymbol{S}=\\boldsymbol{\\mathcal{E}}\\odot_{D}\\mathcal{H}_{\\varphi}(\\boldsymbol{\\mathcal{Z}}),}\\\\ &{\\mathbf{S}=\\boldsymbol{\\mathcal{F}}^{-1}(\\boldsymbol{\\mathcal{S}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\odot_{D}$ denotes the element-wise product along $D$ dimension and ${\\bf S}\\in\\mathbb{R}^{N\\times D}$ is the output. The contextual shaping fliter can adaptively weight the flitering process based on the changing conditions of input and thus have more flexibility in facing more complex situations. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we extensively experiment with eight real-world time series benchmarks to assess the performance of our proposed FilterNet. Furthermore, we conduct thorough analytical experiments concerning the frequency filters to validate the effectiveness of our proposed framework. ", "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets We conduct empirical analyses on diverse datasets spanning multiple domains, including traffic, energy, and weather, among others. Specifically, we utilize datasets such as ETT datasets [14], Exchange [38], Traffic [15], Electricity [15], and Weather [15], consistent with prior studies on long time series forecasting [16, 17, 25]. We preprocess all datasets according to the methods outlined in [16, 17], and normalize them with the standard normalization method. We split the datasets into training, validation, and test sets in a 7:2:1 ratio. More dataset details are in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "Baselines We compare our proposed FilterNet with the representative and state-of-the-art models to evaluate their effectiveness for time series forecasting. We choose the baseline methods from four categories: (1) Frequency-based models, including FreTS [13] and FITS [32]; (2) TCN-based models, such as MICN [39] and TimesNet [40]; (3) MLP-based models, namely DLinear [12] and RLinear [23]; and (4) Transformer-based models, which include Informer [14], Autoformer [15], Pyraformer [26], FEDformer [25], PatchTST [16], and the more recent iTransformer [17] for comparison. Further details about the baselines can be found in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details All experiments are implemented using Pytorch 1.8 [41] and conducted on a single NVIDIA RTX 3080 10GB GPU. We employ MSE (Mean Squared Error) as the loss function and present MAE (Mean Absolute Errors) and MSE (Mean Squared Errors) results as the evaluation metrics. For further implementation details, please refer to Appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Forecasting results for prediction lengths $\\tau\\in\\{96,192,336,720\\}$ with lookback window length $L=96$ . The best results are in red and the second best are blue. Due to space limit, additional results with other baselines and under different lookback length are provided in Tables 4 and 5. ", "page_idx": 6}, {"type": "table", "img_path": "ugL2D9idAD/tmp/0736b25ab38c2f81c969d6cd50dec7bd6ff7f7a0087528e31d300229f9401f5e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the forecasting results of our FilterNet compared to several representative baselines on eight benchmarks with various prediction lengths in Table 1. Additional results with different lookback window lengths are reported in Appendix F and G.3. Table 1 demonstrates that our model consistently outperforms other baselines across different benchmarks. The average improvement of FilterNet over all baseline models is statistically significant at the confidence of $95\\%$ . Specifically, PaiFilter performs well on small datasets (e.g., ETTh1), while TexFilter excels on large datasets (e.g., Electricity) due to the ability to model the more complex and contextual correlations present in larger datasets. Also, the prediction performance of iTransformer [17], which achieves the best results on the Traffic dataset (862 variables) but not on smaller datasets, suggests that simpler structures may be more suitable for smaller datasets, while larger datasets require more contextual structures due to their complex relationships. Compared with FITS [32] built on low-pass filters, our model outperforms it validating an all-pass filter is more effective. Since PaiFilter is simple yet effective, the following FilterNet in the experimental section refer to PaiFilter unless otherwise stated. ", "page_idx": 6}, {"type": "text", "text": "5.3 Model Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this part, we conduct experiments to delve into a thorough exploration of frequency fliters, including their modeling capabilities, comparisons among different types of frequency filters, and the various factors impacting their performance. Detailed experimental settings are provided in Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "Modeling Capability of Frequency Filters Despite the simplicity of frequency fliter architecture, Table 1 demonstrates that this architecture can also achieve competitive performance. Hence, in this part, we perform experiments to explore the modeling capability of frequency filters. Particularly, given the significance of trend and seasonal signals in time series forecasting, we investigate the efficacy of simple filters in modeling these aspects. We generate a trend signal and a multi-period signal with noise, and then we leverage the frequency filters (i.e., PaiFilter) to perform training on the two signals respectively. Subsequently, we produce prediction values based on the trained frequency filters. Specifically, Figure 4(a) and 4(b) show that the filter can effectively model trend and periodic signals respectively even compared with state-of-the-art iTransformer [17] when the data contains noise. These results illustrate that the filter has excellent and robust modeling capabilities for trend and periodic signals which are important components for time series. This can also explain the effectiveness of FilterNet since the filters can perform well in such settings. ", "page_idx": 6}, {"type": "image", "img_path": "ugL2D9idAD/tmp/57400527f803d486edfe9b384eff4eb2cd2c86d8ecf30194e897d78209d675a1.jpg", "img_caption": ["Figure 4: Predictions produced by FilterNet on trend and multi-periodic signals with noises. When adding noises for interference, FilterNet can perform more robust forecasting than iTransformer [17]. ", "(a) Prediction on trend signals with noises (b) Prediction on multi-periodic signals with noises "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ugL2D9idAD/tmp/b3b4897a55a8172ca20d1b4368d0acb9fa40edc64010b225a1ac01111333a2dd.jpg", "table_caption": ["Table 2: Performance evaluation of forecasting using two different kinds of frequency filters on the ETTh1 and Exchange datasets with a lookback window size of 96 and the prediction lengths $\\tau\\in\\{96,192,336,720\\}$ . Results highlighted in red indicate the best performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Shared vs. Unique Filters Among Channels To analyze the different channel strategies of fliters, we further conduct experiments on the ETTh and Exchange datasets. Specifically, we compare forecasting performance under different prediction lengths between two different types of frequency filters, i.e., H(\u03d5Uni) a nd H(\u03d5Ind). In H(\u03d5U ${\\mathcal{H}}_{\\phi}^{(\\bar{U}n i)}$ , filters are shared across different channels, whereas $\\mathcal{H}_{\\phi}^{(I n d)}$ demonstrates that fliters shared among different channels consistently outperform across all prediction lengths. In addition, we visualize the prediction values predicted on the ETTh1 dataset by the two different types of fliters, as illustrated in Figure 11 (see Appendix G.1). The visualization reveals that the prediction values generated by filters shared among different channels exhibit a better fit than the unique fliters. Therefore, the strategy of channel sharing seems to be better suited for time series forecasting and filter designs, which is also validated in DLinear [12] and PatchTST [16]. ", "page_idx": 7}, {"type": "text", "text": "Visualization of Prediction We present a prediction showcase on ETTh1 dataset, as shown in Figure 5. We select iTransformer [17], PatchTST [16] as the representative compared methods. Comparing with these different state-of-the-art models, we can observe FilterNet delivers the most accurate predictions of future series variations, which has demonstrated superior performance. In addition, we include more visualization cases and please refer to Appendix G.3. ", "page_idx": 7}, {"type": "image", "img_path": "ugL2D9idAD/tmp/4e6fbbe17976dfb193425140b5e2de6daa14236c1e69eb65320997a00017a239.jpg", "img_caption": ["Figure 5: Visualization of prediction on the ETTh1 dataset with lookback and horizon length as 96. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Visualization of Frequency Filters To provide a comprehensive overview of the frequency response characteristics of frequency filters, we conduct visualization experiments on the Weather, ETTh, and Traffic datasets with the lookback window length of 96 and the prediction length of 96. The frequency response characteristics of learned fliters are visualized in Figure 7. From Figures 7(a) and 7(b), we can observe that compared with Transformer-based approaches (e.g., iTransformer [17], PatchTST [16]) tend to attenuate high-frequency components and preserve low-frequency information, FilterNet exhibits a more nuanced and adaptive filtering behavior that can be capable of attending to all frequency components. Figure 7(c) demonstrates that the main patterns of the Traffic dataset primarily resides in the low-frequency range. This observation also explains why iTransformer performs well on the Traffic dataset, despite its low-frequency nature. Overall, Figure 7 demonstrates that FilterNet possesses comprehensive processing capabilities. Moreover, visualization experiments conducted on the ETTm1 dataset across various prediction lengths, as shown in Figure 8, further illustrate the extensive processing abilities of FilterNet. Additional results conducted on different lookback window lengths and prediction lengths can be found in Appendix G.2. ", "page_idx": 7}, {"type": "image", "img_path": "ugL2D9idAD/tmp/a313e6e63a8224044889fdef2521d62393b04041ea2fb70d628116adb48a3b56.jpg", "img_caption": ["Figure 7: Spectrum visualizations of filters learned on the Weather, ETTh1, and Traffic datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ugL2D9idAD/tmp/9f50dce37eda10e8c7fbb98ae341a640f667004dcbb6d14905fe9d134e9108e0.jpg", "img_caption": ["Figure 8: Spectrum visualizations of filters learned on ETTm1 with different prediction lengths. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Efficiency Analysis The complexity of FilterNet is $\\mathcal{O}(\\operatorname{Log}L)$ where $L$ is the input length. To comprehensively assess efficiency, we evaluate it based on two dimensions: memory usage and training time. Specifically, we choose two different sizes of datasets: the Exchange (8 variables, 7588 timestamps) and Electricity datasets (321 variables, 26304 timestamps). We compare the efficiency of our FilterNet with the representative Transformer- and MLP-based methods under the same settings (lookback window length of 96 and prediction length of 96), and the results are shown in Figure 6. It highlights that FilterNet surpasses other Transformer models, regardless of dataset size. While our approach exhibits similar efficiency to DLinear, our effective results outperform its performance. In Appendix E, we further conduct ablation studies to validate the rationale of FilterNet designs. ", "page_idx": 8}, {"type": "image", "img_path": "ugL2D9idAD/tmp/d4537a48d5544a6f4e01b9423242561b7efefb51e9103f702bc92d0d6ba6bfa9.jpg", "img_caption": ["Figure 6: Model effectiveness and efficiency comparison on the Exchange and Electricity datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion Remarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we explore an interesting direction from a signal processing perspective and make a new attempt to apply frequency filters directly for time series forecasting. We propose a simple yet effective architecture, FilterNet, built upon our proposed two kinds of frequency fliters to accomplish the forecasting. Our comprehensive empirical experiments on eight benchmarks have validated the superiority of our proposed method in terms of effectiveness and efficiency. We also include many careful and in-depth model analyses of FilterNet and the internal filters, which demonstrate many good properties. We hope this work can facilitate more future research integrating signal processing techniques or flitering processes with deep learning on time series modeling and accurate forecasting. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported in part by the National Natural Science Foundation of China under Grant 92367110, the Shanghai Baiyulan TalentPlan Pujiang Project under Grant 23PJ1413800, and Shanxi Scholarship Council of China (2024-61). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting for worldwide stations with a unified deep model. Nat. Mac. Intell., 5(6):602\u2013611, 2023.   \n[2] Kun Yi, Qi Zhang, Hui He, Kaize Shi, Liang Hu, Ning An, and Zhendong Niu. Deep coupling network for multivariate time series forecasting. ACM Trans. Inf. Syst., 42(5):127:1\u2013127:28, 2024.   \n[3] Hui He, Qi Zhang, Simeng Bai, Kun Yi, and Zhendong Niu. CATN: cross attentive tree-aware network for multivariate time series forecasting. In AAAI, pages 4030\u20134038. AAAI Press, 2022.   \n[4] Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. Stock price prediction using the arima model. In 2014 UKSim-AMSS 16th international conference on computer modelling and simulation, pages 106\u2013112. IEEE, 2014.   \n[5] Nijat Mehdiyev, Johannes Lahann, Andreas Emrich, David Enke, Peter Fettke, and Peter Loos. Time series classification using deep learning for process planning: A case from the process industry. Procedia Computer Science, 114:242\u2013249, 2017.   \n[6] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194):20200209, 2021.   \n[7] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181\u20131191, 2020.   \n[8] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short-term temporal patterns with deep neural networks. In SIGIR, pages 95\u2013104, 2018.   \n[9] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. CoRR, abs/1803.01271, 2018.   \n[10] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022.   \n[11] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.   \n[12] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In AAAI, pages 11121\u201311128. AAAI Press, 2023.   \n[13] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In NeurIPS, 2023.   \n[14] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, pages 11106\u201311115, 2021.   \n[15] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In NeurIPS, pages 22419\u2013 22430, 2021.   \n[16] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.   \n[17] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In ICLR. OpenReview.net, 2024.   \n[18] Richard Asselin. Frequency fliter for time integrations. Monthly Weather Review, 100(6):487\u2013 490, 1972.   \n[19] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.   \n[20] Pierre Duhamel and Martin Vetterli. Fast fourier transforms: a tutorial review and a state of the art. Signal processing, 19(4):259\u2013299, 1990.   \n[21] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 379(2194):20200209, feb 2021.   \n[22] Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. CoRR, abs/2201.12886, 2022.   \n[23] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. CoRR, abs/2305.10721, 2023.   \n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998\u20136008, 2017.   \n[25] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022.   \n[26] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In ICLR, 2021.   \n[27] Kun Yi, Qi Zhang, Longbing Cao, Shoujin Wang, Guodong Long, Liang Hu, Hui He, Zhendong Niu, Wei Fan, and Hui Xiong. A survey on deep learning based time series analysis with frequency transformation. CoRR, abs/2302.02173, 2023.   \n[28] Wei Fan, Kun Yi, Hangting Ye, Zhiyuan Ning, Qi Zhang, and Ning An. Deep frequency derivative learning for non-stationary time series forecasting. In IJCAI, 2024.   \n[29] Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, and Zhendong Niu. Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective. In NeurIPS, 2023.   \n[30] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu. DEPTS: deep expansion learning for periodic time series forecasting. In ICLR. OpenReview.net, 2022.   \n[31] Tian Zhou, Ziqing Ma, Xue Wang, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, and Rong Jin. Film: Frequency improved legendre memory model for long-term time series forecasting. In NeurIPS, 2022.   \n[32] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: modeling time series with 10k parameters. CoRR, abs/2307.03756, 2023.   \n[33] Fausto Pedro Garc\u00eda M\u00e1rquez and Noor Zaman. Digital filters and signal processing. BoD\u2013 Books on Demand, 2013.   \n[34] RTMAC Lu. Algorithms for discrete Fourier transform and convolution. Springer, 1989.   \n[35] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. CoRR, abs/2111.13587, 2021.   \n[36] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881\u20139893, 2022.   \n[37] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Deeptime: Deep time-index meta-learning for non-stationary time-series forecasting. arXiv preprint arXiv:2207.06046, 2022.   \n[38] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short-term temporal patterns with deep neural networks. In SIGIR, pages 95\u2013104, 2018.   \n[39] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. MICN: multi-scale local and global context modeling for long-term series forecasting. In ICLR. OpenReview.net, 2023.   \n[40] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In ICLR. OpenReview.net, 2023.   \n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024\u20138035, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A More Analysis about the Architecture of FilterNet ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The Necessity of Instance Normalization Block From the frequency perspective, the mean value is equal to zero frequency component. Specifically, given a signal $x[n]$ with a length of $N$ , we can obtain its corresponding discrete Fourier transform $\\mathcal{X}[k]$ by: ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\mathcal{X}}[k]={\\frac{1}{N}}\\sum_{n=0}^{N-1}x[n]e^{2\\pi j n k/N}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $j$ is the imaginary unit. We set $k$ as 0 and then, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal X}[0]=\\frac1N\\sum_{n=0}^{N-1}x[n]e^{2\\pi j n0/N}}}\\\\ {{\\displaystyle=\\frac1N\\sum_{n=0}^{N-1}x[n]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "According the above equation, we can find that the mean value nN=\u221201 x[n] in the time domain is equal to the zero frequency component $\\mathcal{X}[0]$ in the frequency domain. Similarly, we can also view the standard deviation from the frequency perspective, and it is related to the power spectral density. ", "page_idx": 12}, {"type": "text", "text": "According to these analysis, the instance normalization is analogous to a form of data preprocessing. Given that fliters are primarily crafted to discern particular patterns within input data, while instance normalization aims to normalize each instance in a dataset, a function distinct from the conventional role of filters, we treat instance normalization as a distinct block within our FilterNet architecture. ", "page_idx": 12}, {"type": "text", "text": "Frequency Filter Block Recently, Transformer- and MLP-based methods have emerged as the two main paradigms for time series forecasting, exhibiting competitive performance compared to other model architectures. Building on prior work that conceptualizes self-attention and MLP architectures as forms of global convolution [35, 13], it becomes apparent that frequency filters hold promise for time series forecasting tasks. Just as self-attention mechanisms capture global dependencies and MLPs learn to convolve features across the entire input space, frequency filters offer a means to extract and emphasize specific temporal patterns and trends from time series data. By applying frequency filters to time series data, we can learn recurring patterns, trends, and periodic behaviors that are essential for forecasting future time series data and making accurate predictions. ", "page_idx": 12}, {"type": "text", "text": "Feed-forward Network Incorporating a feed-forward network within the FilterNet architecture is essential for enhancing the model\u2019s capacity to capture complex relationships and non-linear patterns within the data. While frequency filters excel at extracting specific frequency components and temporal patterns from time series data, they may not fully capture the intricate dependencies and higher-order interactions present in real-world datasets [17]. By integrating a feed-forward network, the model gains the ability to learn hierarchical representations and abstract patterns from the input data, allowing it to capture more nuanced relationships and make more accurate predictions. This combination of frequency filters and a feed-forward network leverages the strengths of both approaches, enabling the model to effectively process and analyze time series data across different frequency bands while accommodating the diverse and often nonlinear nature of temporal dynamics. Overall, the inclusion of a feed-forward network enriches the expressive power of FilterNet, leading to improved forecasting performance and robustness across various domains. ", "page_idx": 12}, {"type": "text", "text": "B Explanations about the Design of Two Filters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Self-attention mechanism is a highly data-dependent operation that both derives its parameters from data and subsequently applies these parameters back to the data. Concretely, given the input data $X$ , the self-attention can be formulated as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{SA}(Q,K,V)=\\mathrm{softmax}(\\frac{Q K^{T}}{\\sqrt{d_{k}}})V,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $Q$ (queries), $K$ (keys), and $V$ (values) are linear transformations of the input data $\\Chi$ , as: ", "page_idx": 12}, {"type": "equation", "text": "$$\nQ=W_{Q}X,K=W_{K}X,V=W_{V}X,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $W_{Q}$ , $W_{K}$ , and $W_{V}$ are learned weight matrices. Since the values $Q,K$ , and $V$ are derived directly from the input data $X$ , the attention scores SA are inherently dependent on the data. ", "page_idx": 13}, {"type": "text", "text": "Unlike self-attention mechanism that dynamically adapts to the input data during inference, MLPs maintain a consistent architecture regardless of the dataset characteristics. Specifically, for the input data $X$ , MLP can be formulated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathrm{MLP}}(X)=W X+b,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $W$ are the learned weights and $b$ are the learned biases. Once trained, the weights $W$ and biases $b$ remain static, meaning that they do not dynamically change in response to new data inputs. ", "page_idx": 13}, {"type": "text", "text": "MLPs are straightforward, less data-dependent models that apply fixed transformations to the input data, making them suitable for tasks with static relationships between the input data. In contrast, self-attention mechanisms are highly data-dependent, dynamically computing attention scores based on the input data to capture complex, context-specific dependencies, making them ideal for tasks requiring an understanding of sequential or structured data. ", "page_idx": 13}, {"type": "text", "text": "Inspired by the two paradigms, FilterNet designs two corresponding types of filters: plain shaping fliters and contextual shaping fliters. Plain shaping fliters offer stability and efficiency, making them suitable for tasks with static relationships. In contrast, contextual shaping fliters provide the flexibility to capture dynamic dependencies, excelling in tasks that require context-sensitive analysis. This dual approach allows FilterNet to effectively handle a wide range of data types and forecasting scenarios, combining the best aspects of both paradigms to achieve superior performance. ", "page_idx": 13}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We evaluate the performance of our proposed FilterNet on eight popular datasets, including Exchange, Weather, Traffic, Electricity, and ETT datasets. In detail, the Exchange2 dataset collects daily exchange rates of eight different countries including Singapore, Australia, British, Canada, Switzerland, China, Japan, and New Zealand ranging from 1990 to 2016. The Weather3 dataset, including 21 meteorological indicators such as air temperature and humidity, is collected every 10 minutes from the Weather Station of the Max Planck Institute for Biogeochemistry in 2020. The Traffic [15] dataset contains hourly traffic data measured by 862 sensors on San Francisco Bay area freeways, which has been collected since January 1, 2015. The Electricity4 dataset collects the hourly electricity consumption of 321 clients from 2012 to 2014. The ETT5 (Electricity Transformer Temperature) datasets contain two visions of the sub-dataset: ETTh and ETTm, collected from electricity transformers every 15 minutes and 1 hour between July 2016 and July 2018. Thus, in total we have 4 ETT datasets (ETTm1, ETTm2, ETTh1, and ETTh2) recording 7 features such as load and oil temperature. The details about these datasets are summarized in Table 3. ", "page_idx": 13}, {"type": "table", "img_path": "ugL2D9idAD/tmp/564a00f925a03e30c906ed55a9fbc68ec1b09910ee1bd744e917ec024e7d01d4.jpg", "table_caption": ["Table 3: The details of datasets. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C.2 Baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We choose twelve well-acknowledged and state-of-the-art models for comparison to evaluate the effectiveness of our proposed FilterNet for time series forecasting, including Frequency-based models, TCN-based models, MLP-based models, and Transformer-based models. We introduce these models as follows: ", "page_idx": 13}, {"type": "text", "text": "FreTS [13] introduces a novel direction of applying MLPs in the frequency domain to effectively capture the underlying patterns of time series, benefiting from global view and energy compaction. The official implementation is available at https://github.com/aikunyi/FreTS. To ensure fair and objective comparison, the results showed in Table 4 are obtained using instance normalization instead of min-max normalization in the original code. ", "page_idx": 14}, {"type": "text", "text": "FITS [32] performs time series analysis through interpolation in the complex frequency domain, enjoying low cost with 10k parameters. The official implementation is available at https://github. com/VEWOXIC/FITS. Because its original paper doesn\u2019t provide the forecasting results with the fixed lookback length $L=96$ , we report the performance of FITS with lookback length $L=96$ under five runs in Table 1. ", "page_idx": 14}, {"type": "text", "text": "MICN [39] employs multi-scale branch structure to model different potential patterns separately with linear complexity. It combines local features and global correlations to capture the overall view of time series. The official implementation is available at https://github.com/wanghq21/MICN. The experimental results showed in Table 4 follow its original paper. ", "page_idx": 14}, {"type": "text", "text": "TimesNet [40] transforms 1D time series into a set of 2D tensors based on multiple periods to analyse temporal variations. The above transformation allows the 2D-variations to be easily captured by 2D kernels with encoding the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively. The official implementation is available at https://github.com/thuml/ TimesNet. The results showed in Table 1 follow iTransformer [17] and the results showed in Table 5 follow RLinear [23]. ", "page_idx": 14}, {"type": "text", "text": "DLinear [12] utilizes a simple yet effective one-layer linear model to capture temporal relationships between input and output sequences. The official implementation is available at https://github. com/cure-lab/LTSF-Linear. We report the performance of DLinear with lookback length $L\\in$ $\\{96,336\\}$ under five runs in Table 1 and 5. ", "page_idx": 14}, {"type": "text", "text": "RLinear [23] uses linear mapping to model periodic features in multivariate time series with robustness for diverse periods when increasing the input length. It applies RevIN (reversible normalization) and CI (Channel Independent) to improve overall forecasting performance by simplifying learning about periodicity. The official implementation is available at https://github.com/plumprc/ RTSF. The experimental results showed in Table 1 follow iTransformer [17]. ", "page_idx": 14}, {"type": "text", "text": "Informer [14] enhances Transformer with KL-divergence based ProbSparse attention for $O(L\\log L)$ complexity, efficiently encoding dependencies among variables and introducing a novel architecture with a DMS forecasting strategy. The official implementation is available at https://github.com/ zhouhaoyi/Informer2020 and the experimental results showed in Table 4 follow FEDformer [25]. ", "page_idx": 14}, {"type": "text", "text": "Autoformer [15] employs a deep decomposition architecture with auto-correlation mechanism to extract seasonal and trend components from input series, embedding the series decomposition block as an inner operator. The official implementation is available at https://github.com/thuml/ Autoformer and the experimental results showed in Table 4 follow FEDformer [25]. ", "page_idx": 14}, {"type": "text", "text": "Pyraformer [26] introduces pyramidal attention module (PAM) with an $O(L)$ time and memory complexity where the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. The official implementation is available at https://github.com/ant-research/Pyraformer and the experimental results showed in Table 4 follow DLinear [12]. ", "page_idx": 14}, {"type": "text", "text": "FEDformer [25] implements sparse attention with low-rank approximation in frequency domain, enjoying linear computational complexity and memory cost. And it proposes mixture of experts decomposition to control the distribution shifting. The official implementation is available at https: //github.com/MAZiqing/FEDformer and the experimental results showed in Table 1 follow iTransformer [17]. ", "page_idx": 14}, {"type": "text", "text": "PatchTST [16] divides time series data into subseries-level patches to extract local semantic information and adopts channel-independence strategy where each channel shares the same embedding and Transformer weights across all the series. The official implementation is available at https://github.com/yuqinie98/PatchTST. The experimental results showed in Table 1 follow iTransformer [17]. And because iTransformer doesn\u2019t provide the prediction results with lookback length $L=336$ , we report the performance of PatchTST with lookback length $L=336$ under five runs in Table 5. ", "page_idx": 14}, {"type": "text", "text": "iTransformer [17] inverts the structure of Transformer without modifying any existing modules by encoding individual series into variate tokens. These tokens are utilized by the attention mechanism to capture multivariate correlations and FFNs are adopted for each variate token to learn nonlinear representations. The official implementation is available at https://github.com/thuml/ iTransformer. The experimental results showed in Table 1 follow its original paper. And because iTransformer doesn\u2019t provide the prediction results with lookback length $L=336$ , we report the performance of iTransformer with lookback length $L=336$ under five runs in Table 5. ", "page_idx": 15}, {"type": "text", "text": "C.3 Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The architecture of our FilterNet is very simple and has two main hyperparameters, i.e., the bandwidth of fliters and the hidden size of FFN. As shown in Figure 9, the bandwidth of the fliters corresponds to the lookback window length, so we select the lookback window length as the bandwidth accordingly. For the hidden size of FFN, we carefully tune the size over {64, 128, 256, 512}. Following the previous methods [16, 32], we use RevIN [19] as our instance normalization block. Besides, we carefully tune the hyperparameters including the batch size and learning rate on the validation set, and we choose the settings with the best performance. We tune the batch size over {4, 8, 16, 32} and tune the learning rate over {0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005}. ", "page_idx": 15}, {"type": "text", "text": "C.4 Experimental Settings for Simulation Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To evaluate the Transformer\u2019s modeling ability for different frequency spectrums, we generate a signal consisting of three different frequency components, i.e., low-frequency, middle-frequency, and high-frequency. We then apply both iTransformer and FilterNet to this signal, respectively, and compare the forecasting results with the ground truth. The results are presented in Figure 1. ", "page_idx": 15}, {"type": "text", "text": "C.5 Experimental Settings for Filters Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Modeling capability of frequency filters We generate two signals: a trend signal with Gaussian noise and a multi-periodic signal with Gaussian noise. We then apply PaiFilter to these signals with a lookback window length of 96 and a prediction length of 96. The results are displayed in Figure 4. ", "page_idx": 15}, {"type": "text", "text": "Visualization of Frequency Filters Given a filter $\\mathbf{H}\\,\\in\\,\\mathbb{R}^{1\\times L}$ , where $L$ is its bandwidth, we visualize the frequency response characteristic of the fliter by plotting the values in $\\mathbb{R}^{1\\times L}$ . First, we perform a Fourier transform on these values to obtain the spectrum, which includes the frequency and its corresponding amplitude. Finally, we visualize the spectrum, as shown in Figures 7, 8, and 12. ", "page_idx": 15}, {"type": "text", "text": "D Study of the Bandwidth of Frequency Filters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The bandwidth parameter (i.e., $L$ in Equation (8) and $D$ in Equation (9)) holds significant importance in the functionality of filters. In this part, we conduct experiments on the Weather dataset to delve into the impact of bandwidth on forecasting performance. We explore a range of bandwidth values within the set $\\{96,128,192,256,320,386,448,512\\}$ while keeping the lookback window length and prediction length constant. Specifically, we conduct experiments to evaluate the impact under three different combinations of lookback window length and prediction length, i.e., $96~\\rightarrow~96~$ , $96\\to192$ , and $192\\rightarrow192$ , and the results are represented in Figure 9. We observe clear trends in the relationship between bandwidth settings and lookback window length. Figure 9(a) and Figure 9(b) show that increasing the bandwidth results in minimal changes in forecasting performance. Figure 9(c) demonstrates that while forecasting performance fluctuates with increasing bandwidth, it is optimal when the bandwidth equals the lookback window length. These results indicate that using the lookback window length as the bandwidth is sufficient since the fliters can effectively model the data at this setting, and it also results in lower complexity. ", "page_idx": 15}, {"type": "text", "text": "E Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To validate the rationale behind the architectural design of our FilterNet, we conduct ablation studies on the ETTm1, ETTh1, and Electricity datasets. We evaluate the impact on the model\u2019s performance by eliminating the particular component of the FilterNet architecture. The evaluation results are present in Figure 10. In the figure, W/O Norm indicates that instance normalization and inverse instance normalization have been removed from FilterNet. W/O Filter signifies the removal of the filter block, and W/O FFN denotes the exclusion of the feed-forward network. The experiments are conducted with lookback window length of 96 and output length of 96. From the figure, we can conclude that each block is indispensable, as the removal of any component results in a noticeable decrease in performance. This highlights the critical role each block plays in the overall architecture of FilterNet, contributing to its effectiveness in time series forecasting. ", "page_idx": 15}, {"type": "image", "img_path": "ugL2D9idAD/tmp/f36e068147c9b83ea50d2a6337f73abb6aeb98eb37d144af0ecd3f170c7cbe8e.jpg", "img_caption": ["Figure 9: MSE and MAE of filters under different bandwidths on the Weather dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ugL2D9idAD/tmp/6a8c6a63498d6f0c4752136af4a691ed5b8591489711aea19dc3a9f83afd1bb4.jpg", "img_caption": ["Figure 10: Ablation Studies on the ETTh1, ETTm1, and Electricity datasets. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "F Additional Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 4 compares the performance of various methods with our FilterNet, demonstrating that our model consistently outperforms the others. To further assess the performance of FilterNet under different lookback window lengths, we conducted experiments on the ETTh1, ETTm1, Exchange, Weather, and Electricity datasets with the lookback window length of 336. The results, shown in Table 5, indicate that our model achieves the best performance across these datasets. ", "page_idx": 16}, {"type": "text", "text": "Table 4: Multivariate long-term forecasting results with prediction lengths $\\tau\\in\\{96,192,336,720\\}$ and fixed lookback length $L=96$ . The best results are in red and the second best are blue. ", "page_idx": 17}, {"type": "table", "img_path": "ugL2D9idAD/tmp/0d1c2c1e353c8669e1224f4055281d54a4eec3e2b8ff168a0d26af427c8bee95.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "ugL2D9idAD/tmp/011b63615991cfad0b17a13375b78f419638a5ac4ef6922a25686a909003a0cf.jpg", "table_caption": ["Table 5: Time series forecasting comparison. We set the lookback window size $L$ as 336 and the prediction length as $\\tau\\in\\{96,192,336,720\\}$ . The best results are in red and the second best are blue. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "ugL2D9idAD/tmp/8b3911915ad57058f5fdf4384fdb1db3bcf8393d941e54f543cd1a22e834998b.jpg", "img_caption": ["", "Figure 11: Visualizations on the ETTh1 dataset. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ugL2D9idAD/tmp/db8a9ed15692e7b3b6b622f6dcb701ccb13f220e3758829bd55ed8294db5a41e.jpg", "img_caption": ["Figure 12: Spectrum visualizations of the filters learned on the Electricity dataset with lookback window length of 336 and prediction lengths $\\tau\\in\\{96,192,336\\}$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "G Visualizations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 Visualization of Channel-shared vs Channel-unique Filters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further compare the channel-shared and channel-unique fliters, we visualize the prediction values by the corresponding filters. The results are shown in Figure 11. The figure demonstrates that the values predicted by channel-shared filters closely align with the ground truth compared to those predicted by channel-unique filters. This observation is consistent with the findings presented in Table 2, indicating the superiority of channel-shared filters. ", "page_idx": 18}, {"type": "text", "text": "G.2 Visualization of Frequency Filters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We further conduct visualization experiments to explore the learnable fliters under different lookback window lengths and prediction lengths. The experiments are performed on the Electricity dataset, and the results are illustrated in Figure 12. These figures illustrate that FilterNet possesses full spectrum learning capability, as the learnable filters exhibit values across the entire spectrum. Besides, we observe that the frequency primarily concentrates in the low and middle ranges which explains that some works based on low-pass filters can also achieve good performance. ", "page_idx": 18}, {"type": "text", "text": "G.3 Visualizations of Predictions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further offer an evident comparison of our model with the state-of-the-art models, we present supplementary prediction showcases on ETTm1 dataset, and the results are shown in 13. We choose the following representative models, including iTransformer [17], PatchTST [16], and DLinear [12], as the baselines. Comparing with these different types of state-of-the-art models, FilterNet delivers the most accurate predictions of future series variations, demonstrating superior performance. ", "page_idx": 18}, {"type": "image", "img_path": "ugL2D9idAD/tmp/a3561ed1462ebacd32b860e48a4b9641dca2b66266cf5df1ac7d46f706798502.jpg", "img_caption": ["Figure 13: Visualization of forecasting results on the ETTm1 dataset with lookback window length $L=96$ and prediction length $\\tau=96$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims are clearly written in the abstract and introduction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discussed the limitation of our method in the conclusion remarks ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: No theoretical proofs. ", "page_idx": 20}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We included the implementation details in the main paper and the appendix. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will include the source data and code. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please see implementation details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The average improvement of FilterNet over all baseline models is statistically significant at the confidence of $95\\%$ . ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please see implementation details. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have discussed the broader impact of time series forecasting in both abstract and introduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not have this risk. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We included it in implementation details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: N/A. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: N/A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: N/A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]