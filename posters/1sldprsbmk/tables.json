[{"figure_path": "1sLdprsbmk/tables/tables_4_1.jpg", "caption": "Table 1: Notation used in data generation (Section 3.1)", "description": "This table summarizes the notations used in the data generation section (Section 3.1) of the paper.  It defines various symbols representing the skill sets (all skills, training skills, held-out skills), topic sets (all topics, training topics, held-out topics), and the datasets generated for different numbers of skills (k=1,2,3). The table also specifies the size of each set and provides additional context about how the datasets were created.", "section": "3 Pipeline"}, {"figure_path": "1sLdprsbmk/tables/tables_5_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5.  DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4. It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  Each setting evaluates the model's ability to generalize and compose skills under different conditions, highlighting the impact of fine-tuning on held-out skills.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_6_1.jpg", "caption": "Table 3: Performance of fine-tuned Mistral-7B-Instruct-v0.2 on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5.  DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the results of the Mistral-7B-Instruct-v0.2 model after fine-tuning on different datasets (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3))  evaluated using the SKILL-MIX metric. The performance is measured by the Ratio of Full Marks and Skills Fraction for different numbers of skills (k=2,3,4,5) combined in short paragraphs. The evaluation is conducted across three settings: training skills and topics, held-out skills and topics, and all skills and topics.", "section": "4 Compositional Generalization for out-of-domain evaluations"}, {"figure_path": "1sLdprsbmk/tables/tables_7_1.jpg", "caption": "Table 4: SKILL-MIXall(k) performance of models fine-tuned on LLaMA-2-13B-Chat, graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different k. DSKILL-Mix (1, 2, 3) (8000 sample) denotes the randomly sub-sampled dataset from DSKILL-MIX (1, 2, 3) with size 8000.", "description": "This table presents the performance of LLaMA-2-13B-Chat models, fine-tuned on different datasets (combinations of SKILL-MIX data with k=1,2,3), evaluated on the SKILL-MIXall(k) metric (k=2,3,4,5).  The performance is measured using the Ratio of Full Marks and Skills Fraction. The table shows the impact of the size and composition of the fine-tuning dataset on the model's ability to combine k skills.  It includes results for a smaller dataset (8000 samples) for comparison.", "section": "4.3 Data requirement for inducing compositional generalization"}, {"figure_path": "1sLdprsbmk/tables/tables_8_1.jpg", "caption": "Table 5: (Comparison between GPT-4 and Claude-3 grader) SKILL-MIXall(k) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5.", "description": "This table presents a comparison of the performance of the fine-tuned Llama-2-13B-Chat model on the SKILL-MIXall(k) evaluation metric (k = 2, 3, 4, 5).  The key difference is that the evaluation was performed using two different graders: GPT-4 and Claude-3.  This allows for an assessment of the consistency and potential biases introduced by different grading approaches. The table shows the Ratio of Full Marks and Skills Fraction for each model and grader combination.", "section": "5 Discussions"}, {"figure_path": "1sLdprsbmk/tables/tables_8_2.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4. It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to combine), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  Each setting reflects different levels of generalization: in-domain, out-of-domain, and overall generalization. The table helps in understanding how well the model generalizes to unseen skill combinations after fine-tuning on different datasets.  The DSKILL-MIX(k) notation refers to the data used for fine-tuning, indicating the level of skill complexity (k) in the training data.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_9_1.jpg", "caption": "Table 7: (Filtering out common skills) SKILL-MIX(5) performance of models fine-tuned on LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model under SKILL-MIX evaluation with train, test, and all skills (SKILL-MIXtrain(5),SKILL-MIXheld-out(5),SKILL-MIXall(5) respectively). We only consider skill combinations with uncommon skills whose occurrence rate in RedPajama is less than 5%.", "description": "This table presents the results of evaluating the performance of fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 models on the SKILL-MIX(5) task.  The evaluation focuses on combinations of uncommon skills (skills with an occurrence rate in the RedPajama dataset of less than 5%). The table displays the Ratio of Full Marks and Skills Fraction for each model under three different evaluation settings: training skills, held-out skills, and all skills. This helps to analyze the models' ability to generalize skill composition to unseen skills.", "section": "4.2 Compositional generalization for out-of-domain evaluations"}, {"figure_path": "1sLdprsbmk/tables/tables_12_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation metric, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across three evaluation settings:  training skills and topics, held-out skills and topics, and all skills and topics. Each setting represents a different level of generalization and provides insights into the model's ability to compose skills, both seen and unseen during training. The table also includes the dataset used for fine-tuning (DSKILL-MIX(k)) which is also defined in Section 3.1 of the paper.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_13_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics. The table also indicates the fine-tuning dataset used for each model (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)) which varies in the number of skills (k) included in the training data.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_14_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across various settings. The settings include evaluations on training skills and topics, held-out skills and topics, and all skills and topics.  The table helps analyze the model's ability to generalize and compose skills both within and outside of its training data.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_15_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  The evaluation assesses the model's ability to combine k skills (k=2,3,4,5) in various settings. The settings include evaluating the model on the same training skills and topics it was fine-tuned on (in-domain), on held-out skills and topics (out-of-domain), and on all skills and topics. The results are presented as the Ratio of Full Marks and Skills Fraction, which are metrics reflecting the model's success rate in composing skills.  Different datasets (DSKILL-MIX(k)) used for fine-tuning are also indicated.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_16_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to combine), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  The data used for fine-tuning is indicated (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)), allowing for comparison of performance based on different training data. This helps analyze the model's ability to generalize to unseen skills (held-out) and to combine larger numbers of skills (higher k).", "section": "4.1 Compositional generalization for in-domain evaluations"}, {"figure_path": "1sLdprsbmk/tables/tables_17_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  The data used for fine-tuning is indicated (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)) and allows for comparison of the model's performance before and after fine-tuning with different datasets.  The table helps assess the model's ability to generalize and compose skills in various contexts.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_18_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table shows the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation for different numbers of skills (k=2, 3, 4, 5) under three different evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  The performance is measured using two metrics: Ratio of Full Marks and Skills Fraction. The table also shows the model's performance before and after fine-tuning on different datasets (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)).", "section": "4.1 Compositional generalization for in-domain evaluations"}, {"figure_path": "1sLdprsbmk/tables/tables_18_2.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills combined), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  Each setting reflects different levels of generalization, with held-out skills representing a more challenging out-of-distribution test.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_19_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to be combined), across three evaluation settings: using training skills and topics, held-out skills and topics, and all skills and topics.  Each setting assesses the model's ability to compose skills under different conditions, revealing its generalization capabilities.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_20_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to compose), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics. Each setting evaluates the model's ability to compose skills in different scenarios, allowing for a comprehensive analysis of its compositional generalization capabilities. The table also references Section 3.1 for details on data generation.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_21_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4. It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  Each setting further breaks down the results based on which dataset the model was fine-tuned on (DSKILL-MIX(1), DSKILL-MIX(1,2), or DSKILL-MIX(1,2,3)). This allows for a comparison of model performance under different training conditions and shows the impact of fine-tuning on compositional generalization.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_22_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  The data used for fine-tuning (DSKILL-MIX(k)) is also specified.  The table helps illustrate the impact of fine-tuning on the model's ability to compose varying numbers of skills, both those seen and unseen during training.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_23_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to be composed), across various settings. The settings include evaluations on training skills and topics, held-out skills and topics, and all skills and topics.  The table helps to analyze the model's ability to generalize compositional skills learned during training to unseen skill combinations. Different fine-tuning datasets (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)) are compared to understand the impact of training data richness on compositional generalization.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_24_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  The evaluation was performed across different settings (training, held-out, and all skills/topics) and varying numbers of skills (k=2,3,4,5). The results are shown as the \"Ratio of Full Marks\" and \"Skills Fraction\", reflecting the model's success in composing the specified skills.  DSKILL-MIX(k) refers to the dataset generated with full scores on the SKILL-MIX(k) evaluation. Section 3.1 provides more details about the data generation process.", "section": "4 Skill Composition Can Be Learned From Examples"}, {"figure_path": "1sLdprsbmk/tables/tables_25_1.jpg", "caption": "Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different k = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)", "description": "This table presents the performance of the fine-tuned LLaMA-2-13B-Chat model on the SKILL-MIX evaluation, graded by GPT-4.  It shows the \"Ratio of Full Marks\" and \"Skills Fraction\" for different values of k (number of skills to combine), across three evaluation settings: training skills and topics, held-out skills and topics, and all skills and topics.  The table allows comparison of the model's performance before and after fine-tuning on different datasets (DSKILL-MIX(1), DSKILL-MIX(1,2), DSKILL-MIX(1,2,3)).  It helps to analyze the model's ability to generalize to unseen skills and the impact of training data on its compositional generalization capabilities.", "section": "4 Skill Composition Can Be Learned From Examples"}]