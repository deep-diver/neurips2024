[{"figure_path": "PcyioHOmjq/figures/figures_1_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets.(b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the analysis of class distribution and model performance on different image-text datasets.  Part (a) demonstrates the highly imbalanced nature of class distribution across various datasets (LAION-400M, MetaCLIP-400M, LAION-2B, MetaCLIP-2.5B, YFCC-15M, CC-12M). Part (b) compares CLIP's performance with supervised learning models showing that CLIP is more robust to data imbalance. CLIP's accuracy is less affected by class frequency, and the correlation between a class's frequency and the model's predictions decreases with increasing data scale, suggesting the presence of implicit mechanisms that balance the learning process.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_1_2.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets.(b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of various image-text datasets and the models trained on them.  The left plot (a) demonstrates the highly imbalanced class distribution common across these datasets. The right plot (b) compares CLIP's performance to that of supervised learning models.  It shows that CLIP's accuracy is less affected by the frequency of classes in the training data, and the number of predictions per class is less correlated with class frequency than in supervised learning. This effect becomes more pronounced as the dataset size increases, suggesting that CLIP incorporates mechanisms to implicitly balance the learning signal.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_2_1.jpg", "caption": "Figure 2: Curation process and distribution of datasets used in our controlled study. Top: IN-Caps [27] augments train images of ImageNet with texts by querying Flickr with image URLs. The texts include title, description, and tags. Bottom: LAIONet [77] is a filtered subset of LAION-400M [73], obtained by matching ImageNet classes with captions and filtering by CLIP text encoder for disambiguation.", "description": "This figure illustrates the data curation processes and resulting data distributions for two datasets used in the controlled experiments: ImageNet-Captions (IN-Caps) and LAIONet.  IN-Caps augments ImageNet images with text captions obtained via Flickr API queries using image URLs as input.  The captions include titles, descriptions, and tags. LAIONet is a filtered subset of LAION-400M, where filtering is based on matching ImageNet classes with captions and using the CLIP text encoder to disambiguate and select only high-quality image-caption pairs.", "section": "2 Related work"}, {"figure_path": "PcyioHOmjq/figures/figures_3_1.jpg", "caption": "Figure 3: Results on IN-Caps about text descriptiveness and vocabulary size. 1) Increasing text descriptiveness improves both robustness (a) and discriminability (b) of CLIP, but the tendency varies if using less descriptive (template-based) supervision. 2) The gap between SL and CLIP (a) implies CLIP re-balances predictions, which is replicable by subsampling the vocabulary SL trains with.", "description": "This figure analyzes the impact of language supervision and vocabulary size on the robustness and discriminability of CLIP and supervised learning (SL) models.  It shows that more descriptive text supervision leads to better robustness and discriminability for CLIP, while the effect is less pronounced for less descriptive (template-based) supervision. The comparison between CLIP and SL highlights CLIP's implicit prediction re-balancing mechanism, which can be replicated in SL by subsampling the training vocabulary.", "section": "3.2 (Descriptive) language as supervision signal"}, {"figure_path": "PcyioHOmjq/figures/figures_5_1.jpg", "caption": "Figure 4: Results on LAIONet about data distribution (level of data imbalance, distribution shift, and data diversity). 1) Extreme data imbalance makes models more prone to bias (last column vs. others). 2) Distribution shift (\u25cf\u25cf vs., last column) harms discriminability but could improve robustness if pre-trained text head is used. 3) Higher data diversity (smaller threshold) also improves robustness.", "description": "This figure presents a comprehensive analysis of the impact of data distribution on the robustness of CLIP models to data imbalance.  It uses different variants of the LAIONet dataset to manipulate the level of data imbalance, distribution shift, and intra-class diversity. The results show that extreme data imbalance increases the risk of model bias. Distribution shifts can harm discriminability, but if a pre-trained text head is used, they can actually improve robustness. Finally, higher data diversity is shown to improve robustness.", "section": "3.4 Data distribution (level of imbalance, web distribution shift, and intra-class diversity)"}, {"figure_path": "PcyioHOmjq/figures/figures_5_2.jpg", "caption": "Figure 5: Results on LAIONet subsets about data scale and text encoder. 1) CLIP's discriminability (a) and robustness (b) co-improve as data scales up, and can be boosted by pre-trained heads. 2) A frozen head helps CLIP preserve intra-class variation (c) while not harming margins (d), which can be lost if fine-tuned. It is also unattainable by SL even using the same head. 3) Language pre-training using CLIP is more favorable for image-text tasks than pure language modeling (e.g., RoBERTa [51]).", "description": "This figure examines the effect of data scale and text encoder on CLIP's performance.  It shows that CLIP's discriminability and robustness to data imbalance improve with increasing data scale. Using pre-trained heads enhances these effects further.  The figure also compares using a frozen vs. fine-tuned text encoder, showing that a frozen pre-trained CLIP text encoder maintains intra-class variation better than a fine-tuned one.  Finally, it highlights that language pre-training with CLIP is superior to language modeling alone for image-text tasks.", "section": "3.5 Data scaling (also achievable via language pre-training)"}, {"figure_path": "PcyioHOmjq/figures/figures_6_1.jpg", "caption": "Figure 6: CLIP can benefit from open-world concepts. (a) Train on IN-Caps variants, and evaluate on 100 classes. (b) Train on YFCC-15M variants, and evaluate on 1K classes.", "description": "This figure shows that CLIP models benefit from the utilization of open-world concepts. In the left panel (a), CLIP models are trained on IN-Caps variants (ImageNet-Captions datasets with varying numbers of concepts/classes), and evaluated on 100 ImageNet classes. This shows that increasing the number of concepts improves robustness to data imbalance in the training datasets.  The right panel (b) follows a similar approach, but utilizes YFCC-15M datasets, which contain a larger number of concepts/classes, and evaluates performance on 1000 ImageNet classes.  Supervised learning models (SL) show a decreased performance due to inability to utilize the additional information provided by the open-world concepts, illustrating CLIP's advantage in handling data imbalance.", "section": "3.6 Utilization of open-world concepts"}, {"figure_path": "PcyioHOmjq/figures/figures_6_2.jpg", "caption": "Figure 7: Inspecting CLIP\u2019s failures and effects of data imbalance from NC-based metrics. 1) Fail classes of smaller-scale models (12/15M) are hardly discriminative to most classes, while larger-scale models (\u2265 400M) only fail on some nearest-neighbor classes. 2) Data imbalance is weakly correlated with most feature statistics except NC2w, denoting denser head and coarser tail classes in text space.", "description": "This figure analyzes the failure modes of CLIP models trained on different scales of data and how data imbalance affects these models.  The left panels (a) show correlations between various metrics (NC1, NC2M, NC2w, NC2, NC2nn) indicating compactness and separation of clusters in feature space, and per-class accuracy and frequency. It reveals that smaller models fail on many classes, while larger models mainly fail on classes that are close in feature space to other classes. The right panel (b) presents t-SNE visualization of CLIP text centers, highlighting that data imbalance leads to denser head and coarser tail classes in the text feature space. These findings suggest that scale and data distribution impact CLIP's robustness to data imbalance.", "section": "Understanding the feature distribution of CLIP pre-trained at scale"}, {"figure_path": "PcyioHOmjq/figures/figures_7_1.jpg", "caption": "Figure 8: An extreme case: we train SL models on IN-Caps variants that have tail classes trimmed to only one shot (a & b) or even zero shot (c & d), and evaluate the accuracy on the tail and other classes. CLIP with a frozen pre-trained text encoder shows superior generalization, which can be acquired by a SL model with fixed class prototypes from CLIP and vocabulary subsampling.", "description": "This figure shows the results of training supervised learning (SL) models on a highly imbalanced dataset (ImageNet-Captions with tail classes reduced to one or zero shots).  It compares the performance of several approaches: standard SL, SL using a frozen CLIP text encoder, SL with a frozen CLIP head and vocabulary subsampling, and CLIP. The results show that CLIP generalizes better to tail classes than SL, even under these extreme conditions.  Vocabulary subsampling helps SL achieve performance closer to CLIP, highlighting the importance of the dynamic vocabulary nature of the CLIP pretext task.", "section": "4.1 Data-imbalanced learning: an extreme case"}, {"figure_path": "PcyioHOmjq/figures/figures_8_1.jpg", "caption": "Figure 9: A case study of SL under the zero-shot tail setting. (a) SL models seek maximal margins between classifiers, and tail prototypes collapse together. Instead, CLIP has a healthier structure. (b) Using CLIP head solely is less effective, and voc. subsampling is needed for CLIP-like generalization.", "description": "This figure shows a comparison of the behavior of supervised learning (SL) and CLIP models when trained on a dataset with extremely few examples of certain classes (zero-shot tail setting).  Part (a) shows affinity matrices, illustrating the relationship between classifiers.  It highlights how in SL models, the tail class prototypes (representations of classes with few examples) cluster tightly together, indicating a lack of distinction between them.  In contrast, CLIP exhibits a healthier structure with more distinct representations. Part (b) presents distributions of model's per-class statistics, demonstrating that simply using a pre-trained CLIP head in SL is not sufficient for good generalization, while combining it with vocabulary subsampling can achieve performance similar to CLIP.", "section": "4.1 Data-imbalanced learning: an extreme case"}, {"figure_path": "PcyioHOmjq/figures/figures_8_2.jpg", "caption": "Figure 10: Transfer learning results of DINO variants pre-trained on LAIONet vs. vanilla DINO trained on ImageNet. Extreme data imbalance makes LAIONet much harder for DINO to learn transferrable representations. The vocabulary subsampling strategy effectively helps DINO alleviate such defects and generally match ImageNet-pretrained performance.", "description": "This figure compares the transfer learning performance of different DINO models.  The models are pre-trained on either ImageNet or a version of LAIONet that has an extreme class imbalance. The results show that the standard DINO model struggles to transfer well when trained on the imbalanced LAIONet data. However, a modified DINO model that uses a vocabulary subsampling strategy performs significantly better, achieving performance comparable to the model trained on balanced ImageNet data. This demonstrates the effectiveness of vocabulary subsampling in mitigating the negative effects of data imbalance on the transfer learning capability of the model.", "section": "4.2 Empowering self-supervised learning in-the-wild at scale"}, {"figure_path": "PcyioHOmjq/figures/figures_18_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets. (b) Compared to supervised learning (*SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of several image-text datasets (LAION-400M, MetaCLIP-400M, LAION-2B, MetaCLIP-2.5B, YFCC-15M, CC-12M, LAIONet-3M) and the models trained on them.  Subfigure (a) demonstrates that all datasets share a highly imbalanced class distribution. Subfigure (b) compares the performance of CLIP and supervised learning models. It shows that CLIP's performance is less affected by the class frequency of the training data, implying an implicit re-balancing mechanism.  The correlation between class-wise performance and frequency decreases as the dataset scale increases.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_18_2.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets. (b) Compared to supervised learning (*SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of several image-text datasets and the models trained on them.  Panel (a) demonstrates the highly imbalanced class distribution present across various datasets. Panel (b) compares the performance of CLIP models with supervised learning models. It highlights that CLIP is less affected by data imbalance, showing weaker correlation between a class's performance and its frequency in the training data. This effect is more pronounced as the scale of the training data increases, suggesting implicit re-balancing mechanisms within the CLIP training process.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_18_3.jpg", "caption": "Figure 11: Which is a better indicator for per-class statistics? (a) For less imbalanced IN-Caps, both Pearson's r [66] and Spearman's \u03c1 [78] can model the correlation between statistics well. (b & c) For extremely imbalanced datasets (e.g., LAIONet, YFCC-15M, and other web datasets), Pearson's r may fail even if class frequencies are processed to log scale. In contrast, Spearman's \u03c1 remains robust.", "description": "This figure compares the effectiveness of Pearson's r and Spearman's \u03c1 as correlation indicators for per-class statistics in datasets with varying levels of imbalance. It shows that while Pearson's r performs well on less imbalanced datasets, it fails to accurately reflect the correlation in extremely imbalanced datasets, even when using a log scale for class frequencies. In contrast, Spearman's \u03c1 consistently performs well across different levels of data imbalance, making it a more robust correlation measure in such cases.", "section": "A Extended discussions"}, {"figure_path": "PcyioHOmjq/figures/figures_19_1.jpg", "caption": "Figure 12: Correlation statistics of CLIP evaluated on broader sets of concepts. Models pre-trained at scale (\u2265 400M) remain robust on most datasets except fine-trained (e.g., CUB and Flowers) and domain-specific ones (e.g., EuroSAT). These data might be relatively rare on the web or have significant gaps with other data, thus hard to benefit from scaling or generalization from existing data.", "description": "This figure displays the correlation between per-class accuracy and the number of predictions for various CLIP models tested on several datasets (CUB, Flowers102, Places365, EuroSAT, DTD, Food101, Oxford Pets).  The models were pre-trained on different scales of data, and the graph shows that larger models (\u2265 400M) are more robust, but struggle with fine-grained or domain-specific datasets.  The results suggest that data scarcity and differences in data distribution may limit the ability to scale performance improvements.", "section": "3.6 Utilization of open-world concepts"}, {"figure_path": "PcyioHOmjq/figures/figures_20_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets.(b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of several large-scale image-text datasets and the models trained on them.  The left subplot (a) demonstrates the highly imbalanced class distribution shared across these datasets; classes are extremely unevenly represented. The right subplot (b) compares the performance of CLIP models to those trained with supervised learning (SL). It reveals that CLIP is less sensitive to the class imbalance than SL, as indicated by a weaker correlation between a class's accuracy and its frequency in the training data. Moreover, this insensitivity to imbalance seems to increase with the scale of the training data.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_20_2.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets.(b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of various image-text datasets and the models trained on them.  The left subplot (a) illustrates the highly imbalanced class distribution common across these datasets.  The right subplot (b) compares the performance of CLIP and supervised learning models.  CLIP shows less bias towards frequent classes and a weaker correlation between class performance and frequency, indicating an implicit re-balancing mechanism within CLIP. This effect becomes more pronounced as the dataset size increases.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_21_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets.(b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of different image-text datasets and the models trained on them.  Panel (a) demonstrates that a highly imbalanced class distribution is a common characteristic across various datasets. Panel (b) compares CLIP's performance to that of supervised learning models. It reveals that CLIP's accuracy is less affected by the class frequency in the training data, exhibiting a weaker correlation between class accuracy and frequency. This suggests that CLIP implicitly balances the learning signal, mitigating the impact of data imbalance.  The correlation further decreases as the scale of the training data increases.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_23_1.jpg", "caption": "Figure 16: Distribution of LAIONet subsets.", "description": "This figure shows the distribution of classes in various subsets of the LAIONet dataset. Different subsets are created by applying different thresholds to the text-definition similarity scores. The x-axis represents the classes ranked by frequency in the full LAIONet dataset (with a threshold of 0.7), and the y-axis shows the number of images per class. The different lines represent subsets of LAIONet with varying subsampling ratios.  The figure illustrates the impact of different subsampling strategies on the class distribution.", "section": "3.5 Data scaling (also achievable via language pre-training)"}, {"figure_path": "PcyioHOmjq/figures/figures_25_1.jpg", "caption": "Figure 17: Examples of the distribution of subsampled classes (bar plot), and per-class zero-shot accuracy (line plot) of CLIP (ViT-B/32) pre-trained accordingly (LAION-400M and MetaCLIP-400M). Both models show a weak correlation between class frequency and accuracy.", "description": "This figure shows the class frequency and zero-shot accuracy of two CLIP models (LAION-400M and MetaCLIP-400M) on a subset of ImageNet classes. The bar plot represents the frequency of each class in the pre-training data, while the line plot shows the zero-shot accuracy achieved by each model for each class.  The figure highlights the weak correlation between class frequency (how often a class appears in the training data) and zero-shot accuracy (how well the model can classify the class without any fine-tuning). This demonstrates that CLIP's performance is not heavily biased towards frequent classes, even with a highly imbalanced dataset.", "section": "E.1 Examples of class distribution and CLIP performance"}, {"figure_path": "PcyioHOmjq/figures/figures_26_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets. (b) Compared to supervised learning (*SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows the per-class statistics of several large-scale image-text datasets and the models trained on them.  The left panel (a) demonstrates that all datasets exhibit a highly imbalanced class distribution, meaning some classes have many more examples than others. The right panel (b) compares CLIP's performance to that of supervised learning models.  It reveals that CLIP is less sensitive to this class imbalance, exhibiting a weaker correlation between a class's performance and its frequency in the training data.  Moreover, this insensitivity to class imbalance becomes more pronounced as the scale of the training data increases. This suggests that CLIP employs implicit mechanisms to rebalance the learning signal.", "section": "1 Introduction"}, {"figure_path": "PcyioHOmjq/figures/figures_26_2.jpg", "caption": "Figure 3: Results on IN-Caps about text descriptiveness and vocabulary size. 1) Increasing text descriptiveness improves both robustness (a) and discriminability (b) of CLIP, but the tendency varies if using less descriptive (template-based) supervision. 2) The gap between SL and CLIP (a) implies CLIP re-balances predictions, which is replicable by subsampling the vocabulary SL trains with.", "description": "This figure shows the effects of text descriptiveness and vocabulary size on the robustness and discriminability of CLIP compared to supervised learning (SL).  Panel (a) shows the correlation between class-wise accuracy and frequency, revealing CLIP's improved robustness with more descriptive text and a smaller vocabulary, while SL shows more bias. Panel (b) shows that overall accuracy is better for CLIP when using more descriptive text, even with a smaller vocabulary.  The results suggest that a dynamic classification task with a smaller vocabulary, as in CLIP, helps improve robustness and discriminability. Subsampling the vocabulary in SL can replicate CLIP's performance.", "section": "3.2 (Descriptive) language as supervision signal"}, {"figure_path": "PcyioHOmjq/figures/figures_27_1.jpg", "caption": "Figure 20: t-SNE visualization of samples encoded by CLIP vision/text encoders in the multi-modal feature space (on ImageNet validation set). (a) Images encoded by CLIP vision encoder, and their class-wise mean features. Classes are subsampled. (b) Vision feature centers of all ImageNet classes. (c) Class templates encoded by CLIP text encoder, the same as Fig. 7b. Vision and text features are plotted separately due to the modality gap (despite being in the same feature space) [47].", "description": "This figure shows the t-distributed stochastic neighbor embedding (t-SNE) visualization of the multi-modal feature space learned by CLIP.  It visualizes samples and their class centers from both the vision and text encoders separately, highlighting the modality gap. Subfigure (a) shows a subset of samples and their corresponding vision class centers. Subfigure (b) presents all ImageNet class centers from the vision encoder, and subfigure (c) displays the text class centers which are the same as in figure 7b. The separation of vision and text feature spaces is emphasized.", "section": "3.7 Understanding the feature distribution of CLIP pre-trained at scale"}, {"figure_path": "PcyioHOmjq/figures/figures_28_1.jpg", "caption": "Figure 1: Per-class statistics of image-text datasets and models trained on top. (a) A highly imbalanced class distribution is shared across datasets. (b) Compared to supervised learning (* SL), CLIP's performance (measured by accuracy) is less biased by data frequency, and the classifier is notably uncorrelated (measured by model's number of prediction per class). Besides, the correlation narrows as data scales up. Both aspects indicate implicit re-balancing mechanisms exist in CLIP.", "description": "This figure shows a comparison of the class distribution and model performance for various image-text datasets and models trained on them. Part (a) illustrates that all datasets share a highly imbalanced class distribution, where some classes appear far more frequently than others. Part (b) shows that CLIP models are less affected by the data imbalance compared to supervised learning models, as indicated by weaker correlations between class performance and frequency.  This suggests CLIP possesses implicit mechanisms that handle class imbalance.", "section": "1 Introduction"}]