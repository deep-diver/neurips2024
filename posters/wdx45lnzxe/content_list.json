[{"type": "text", "text": "One-Layer Transformer Provably Learns One-Nearest Neighbor In Context ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zihao ${\\bf L i}^{1*}$ Yuan $\\mathbf{Cao^{2*}}$ Cheng Gao1 Yihan He1 Han Liu? Jason M. Klusowski1 Jianqing Fan1   Mengdi Wang1t ", "page_idx": 0}, {"type": "text", "text": "1Princeton University  2The University of Hong Kong 'Northwestern University {zihaoli,chenggao,yihan.he,jason.klusowski,jqfan,mengdiw}@princeton.edu yuancao@hku.hk hanliu@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have achieved great success in recent years. Interestingly, transformers have shown particularly strong in-context learning capability - even without fine-tuning, they are still able to solve unseen tasks well purely based on taskspecific prompts. In this paper, we study the capability of one-layer transformers in learning one of the most classical nonparametric estimators, the one-nearest neighbor prediction rule. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we show that, although the loss function is nonconvex when trained with gradient descent, a single softmax attention layer can successfully learn to behave like a one-nearest neighbor classifier. Our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms, and sheds light on the role of softmax attention in transformer models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have emerged as one of the most powerful machine learning models since its introduction in Vaswani et al. [2017], achieving remarkable success in various tasks, including natural language processing [Devlin et al., 2018, Achiam et al., 2023, Touvron et al., 2023], computer vision [Dosovitskiy et al., 2020, He et al., 2022, Saharia et al., 2022], reinforcement learning [Chen et al., 2021, Janner et al., 2021, Parisotto et al., 2020], and so on. One intriguing aspect of transformers is their exceptional In-Context Learning (ICL) capability [Garg et al., 2022, Min et al., 2022, Wei et al., 2023, Von Oswald et al., 2023, Xie et al., 2021, Akyirek et al., 2022]. It has been observed that transformers can effectively solve unseen tasks solely relying on task-specific prompts, without the need for fine-tuning. However, the underlying mechanisms and reasons behind the exceptional in-context learning capability of transformers remain largely unexplored, leaving a significant gap in our understanding of how and why transformers can be pretrained to exhibit such remarkable performance. ", "page_idx": 0}, {"type": "text", "text": "Several recent studies have attempted to understand in-context learning (ICL) through the lens of learning specific function classes. Notably, Garg et al. [2022] proposed a well-defined approach: the training data includes a demonstration prompt, consisting of a sequence of labeled data and a new unlabeled query. The in-context learning performance of a transformer is then evaluated based on its ability to successfully execute a machine-learning algorithm to predict the query data label using the prompt demonstration (i.e., the context). Based on such definition, several works such as Zhang et al. [2023], Huang et al. [2023], Chen et al. [2024] investigated ICL the optimization dynamics of transformers under in-context learning from a theoretical lens, but their studies are limited to linear regression prediction rules, which is a significant simplification of the transformer in-context learning task. Another line of work including Bai et al. [2024], Akyirek et al. [2022] investigated the expressiveness of transformers in context, but no optimization result is guaranteed. Whether transformers can handle more complicated ICL tasks under regular gradient-based training is still, in general, unknown. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we examine the ability of single-layer transformers to learn the one-nearest neighbor prediction rule. Our major contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We establish convergence guarantees as well as prediction accuracy guarantees of a single-layer transformer in learning from examples of one-nearest neighbor classification. Utilizing the softmax attention layer, we demonstrate that the training loss can be minimized to zero despite the highly non-convex loss function landscapes. We further justify our results with numerical simulations. \u00b7 Based on the optimization results, we further establish a behavior guarantee for the trained transformer, demonstrating its ability to act like a 1-NN predictor under data distribution shift. Our result thus serves as a concrete example of how transformers can learn nonparametric methods, surpassing the scope of previous literature focusing on linear regression. \u00b7 In our technical analysis, we make the key observation that although the transformer loss is highly nonconvex when learning from one-nearest neighbor, its optimization process can be controlled by a two-dimensional dynamic system when choosing a proper initialization. By analyzing the behavior of such a system, we establish the convergence result despite the curse of nonconvexity. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms and sheds light on the role of softmax attention in transformer models. To our knowledge, this is the first paper that establishes a provable result in both optimization and consecutive behavior under distribution shift for a softmax attention layer beyond the scope of linear prediction tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce the in-context learning data distribution based on the one-nearest neighbor data distribution and the setting of one-layer softmax attention transformers. Then, we discuss the training dynamics of transformers based on gradient descent. ", "page_idx": 1}, {"type": "text", "text": "2.1  In-Context Learning Framework: One-Nearest Neighbor ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In an In-Context Learning (ICL) instance, the model is given a prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}\\sim\\mathbb{P}_{\\mathrm{prompt}}$ and a query input $\\mathbf{x}_{N+1}\\sim\\mathbb{P}_{\\mathrm{query}}$ from some data distributions $\\mathbb{P}_{\\mathrm{prompt}}$ and $\\mathbb{P}_{\\mathrm{query}}$ ,where $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ are the input vectors, $\\left\\{\\mathbf{y}_{i}\\right\\}_{i\\in[N]}\\subset\\mathbb{R}$ are the corresponding labels (e.g. real-valued for regression, or $\\{+1,-1\\}$ -valued for binary classification), and ${\\bf x}_{N+1}$ is the query on which the model is required to make a prediction. Given a prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ , the prediction task is to predict an ground truth model $f(\\mathbf{x}_{N+1};\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]})$ that maps the query token ${\\bf x}_{N+1}$ to a real number. ", "page_idx": 1}, {"type": "text", "text": "In this work, we consider using transformers as the model to perform in-context learning. For a prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ of length $N$ and a query token ${\\bf x}_{N+1}$ , we consider use the following embedding: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{H}=[\\mathbf{h}_{1},\\mathbf{h}_{2},\\ldots,\\mathbf{h}_{N+1}]={\\left[\\begin{array}{l l l l l}{\\mathbf{x}_{1}}&{\\mathbf{x}_{2}}&{\\ldots}&{\\mathbf{x}_{N}}&{\\mathbf{x}_{N+1}}\\\\ {\\mathbf{y}_{1}}&{\\mathbf{y}_{2}}&{\\ldots}&{\\mathbf{y}_{N}}&{\\mathbf{0}}\\\\ {0}&{0}&{\\ldots}&{0}&{1}\\end{array}\\right]}\\in\\mathbb{R}^{(d+2)\\times(N+1)}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We use the notation of $\\mathbf{h}_{j}\\,=\\,[\\mathbf{x}_{j},\\mathbf{y}_{j},0]$ for $j\\,\\leq\\,N$ , and ${\\bf h}_{N+1}\\,=\\,[{\\bf x}_{N+1},0,1]$ . Here, $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ represents the input vectors, each associated with a corresponding label $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ , where $\\mathbf{y}_{i}\\in\\mathbb{R}$ is the label. Throughout this paper, the sequence $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ are referred to as the context or prompt exchangeably. The $(d+2)$ -th row serves as the indicator for the training token, which equals to O value for $i\\in[N]$ and 1 for $i=N+1$ , analogous to a positional embedding vector. Such an indicator allows the model to distinguish the query token from the context. Similar models have been studied in a line of recent works [Zhang et al., 2023, Huang et al., 2023, Chen et al., 2024, Bai et al., 2024, Akyirek et al., 2022] studying in-context learning of linear regression tasks. ", "page_idx": 1}, {"type": "image", "img_path": "WDX45LNZXE/tmp/6622390b72aad4f15d186e187717d01469af10e6ab80b0cd304d82aa0d39b9e5.jpg", "img_caption": ["Figure 1: Illustration of data distribution in Assumption 1 on $\\mathbb{S}^{2}$ and the corresponding ground-truth division of $\\mathrm{\\overline{{S}}^{2}}$ generated by one-nearest neighbor. (1) In the left panel, the red and blue points correspond to the $\\mathbf{x}_{i}$ with ${\\bf y}_{i}=1$ and $-1$ for $i\\in[N]$ , respectively, with $N=500$ . (2) In the right panel, the color of every point on the sphere is the same as its closest neighbor in $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ . The sphere is thus split into divisions by the one-nearest-neighbor decision rule. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Throughout this work, we focus on the case where the ground-truth prediction $f(\\mathbf{x}_{N+1};\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]})$ of the training data is constructed based on a One-Nearest Neighbor (1NN) data distribution, defined by the following definition. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (One-Nearest Neighbor Predictor). Given a prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ and aquery ${\\bf x}_{N+1}$ we define the one-nearest neighbor predictor by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{y}_{i^{*}}:=\\sum_{i=1}^{N}\\mathbb{1}(i=\\underset{j\\in[N]}{\\mathrm{argmin}}\\,\\|\\mathbf{x}_{N+1}-\\mathbf{x}_{j}\\|_{2})\\mathbf{y}_{i}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We also define $i^{*}=\\operatorname{argmin}_{i\\in[N]}\\|{\\bf x}_{N+1}-{\\bf x}_{i}\\|_{2}$ ", "page_idx": 2}, {"type": "text", "text": "Without lossof generality, we assume that $\\begin{array}{r}{\\mathrm{argmin}_{j\\in[N]}\\:\\|{\\bf x}_{N+1}-{\\bf x}_{j}\\|_{2}}\\end{array}$ is unique. Such assumption holds almost surely whenever $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ is sampled from a continuous distribution. Notably, for a fixed prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ and query ${\\bf x}_{N+1}$ , Definition 1 is identical to the nonparametric onenearest neighbor estimator [Peterson, 2009, Beyer et al., 1999], in which the algorithm outputs the label corresponding to the vector closest to the input, with the prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ as the training data in 1-NN. ", "page_idx": 2}, {"type": "text", "text": "Next, we discuss the distribution of the training dataset $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}\\cup\\{\\mathbf{x}_{N+1}\\}$ .Throughout the training process, we focus on the case in which $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ are independently sampled from a uniform distribution on a $d-1$ dimensional sphere $\\mathbb{S}^{d-1}$ \uff0c $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ is a zero-mean binary noise taken value in $\\{+1,-1\\}$ , with $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ and $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ being independent. Our data distribution assumption can be summarized formally by the following assumption: ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Training Distribution). For an embedding $\\mathbf{H}$ defined by Eq. (2.1), we focus on the following underlying training distribution: (i) The sequence $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ are sampled independently from a uniform distribution on a $d-1$ dimensional sphere $\\mathbb{S}^{d-1}\\subset\\overline{{\\mathbb{R}}}^{d}$ (ii)The labels $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ satisfies $\\mathbb{E}[{\\bf y}_{i}{\\bf y}_{j}|{\\bf x}_{1:N}]=0$ and $\\mathbb{E}[\\mathbf{y}_{i}^{2}|\\mathbf{x}_{1:N}]=1$ for all $i\\neq j,i,j\\in[N]$ . (ii) We have $\\mathbb{P}(\\mathbf{y}_{1:N}|\\mathbf{\\check{x}}_{1:N})=\\mathbb{P}(\\mathbf{y}_{1:N}|-\\mathbf{x}_{1:N})$ ", "page_idx": 2}, {"type": "text", "text": "Note that the case when $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ and $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ being independent when $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ : with $\\{\\mathbf{x}_{i}\\}_{i\\in[N]}$ are uniformly sampled from the sphere and $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ are randomly sampled from $\\{\\pm1\\}$ is an example of Assumption 1. We remark that by considering the training data distribution in Assumption 1, we aim to study the capability of transformers in learning one-nearest neighbor prediction rules starting from the cleanest possible setting. Despite the seemingly simple problem setting, we would like to point out that this data distribution is still challenging to study, especially because of the assumption that the second order moment of $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ and $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ are uncorrelated. Due to such uncorrelation, the classifier given by one-nearest neighbor models are rather complicated. For example, Fig. 1 illustrates the randomly generated context data and the corresponding one-nearest neighbor prediction regions with $d=3$ and $N=500$ , which clearly demonstrate the complexity of the training task. This further leads to a highly nonconvex and irregular objective function landscape, illustrated by Fig. 2. ", "page_idx": 2}, {"type": "image", "img_path": "WDX45LNZXE/tmp/7433a787b3b5889e01ddedbe4e41b896fa318c03bac674d2458803cb766ed7b1.jpg", "img_caption": ["Figure 2: Heatmap and landscape of loss function of single layer transformer when learning from one-nearest neighbor. The loss is defined in Eq. (2.5), generated by sampling 100 training sequences according to Assumption 1,with $d=N=4$ WeparametrizeWas $\\mathrm{diag}\\{\\xi_{1},\\ldots,\\xi_{1},0,\\xi_{2}\\}$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2  One-Layer Softmax Attention Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a simplified version of the one-layer transformer architecture [Vaswani et al., 2017] that processes any input sequence $\\mathbf{H}$ defined by Eq. (2.1) and outputs a scalar value: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}_{W}=\\mathbf{H}\\cdot\\mathrm{softmax}(\\mathbf{H}^{\\top}\\mathbf{W}_{K}^{\\top}\\mathbf{W}_{Q}\\mathbf{H}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where softmax(A) applies softmax operator on each column of the matrix A, i.e. $[\\mathrm{softmax}(\\mathbf{A})]_{i j}=$ $\\exp(A_{i j})/\\Sigma_{i}\\exp(A_{i j})$ . Our model is slightly different from the standard self-attention transformers, as we consider a frozen value matrix. However, we also claim that such practice is common in deep learning theory Fang et al. [2020], Lu et al. [2020], Mei et al. [2018]. We also merge the query and key matrices into one matrix denoted as $\\mathbf{W}$ , which is often taken in recent theoretical frameworks [Zhang et al., 2023, Huang et al., 2023, Jelassi et al., 2022, Tian et al., 2023]. The output of the model is defined by the $(d+1)$ -th element of the last column of $\\mathbf{H}\\mathbf{w}$ , with a closed form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1};\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]}):=[\\mathbf{H}_{\\mathbf{W}}]_{(d+1,N+1)}=\\frac{\\sum_{j=1}^{N}\\mathbf{y}_{j}\\exp(\\mathbf{h}_{j}^{\\top}\\mathbf{W}\\mathbf{h}_{N+1})}{\\sum_{j=1}^{N+1}\\exp(\\mathbf{h}_{j}^{\\top}\\mathbf{W}\\mathbf{h}_{N+1})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is the weighted mean of ${\\bf y}_{1},\\ldots,{\\bf y}_{N}$ . Here and after, we may occasionally suppress dependence on $\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]}$ and write $\\widehat{\\mathbf{y}}_{\\mathbf{W}}\\left(\\mathbf{x}_{N+1};\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]}\\right)$ as $\\widehat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})$ . Since the prediction takes only one entry of the token matrix output by the attention layer, actually only parts of $\\mathbf{W}$ affect the prediction. To see this, we denote ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\left(\\mathbf{W}_{11}\\quad\\mathbf{W}_{12}\\quad\\mathbf{W}_{13}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\mathbf{W}_{11}\\in\\mathbb{R}^{d\\times d}$ \uff0c $\\mathbf{W_{21}}\\in\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{31}\\in\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{12}\\in\\mathbb{R}^{d\\times1}$ \uff0c $\\mathbf{W}_{13}\\,\\in\\,\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{22},\\mathbf{W}_{23},\\mathbf{W}_{32}$ and $\\mathbf{W}_{33}\\in\\mathbb{R}$ . Then by Eq. 2.3, it is easy to see that $\\mathbf{W}_{i2}$ does not affect $\\widehat{\\bf y}{\\bf w}$ for $i\\in$ [3], which means we can simply take all these entries as zero in the following sections. Notably, for a fixed prompt-query pair $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ and $\\{{\\bf x}_{N+1}\\}$ , such an architecture allows an arbitrarily close approximation to the 1-NN model: consider $\\mathbf{W}_{11}^{k}=\\xi_{1}^{k}I_{d}$ with $\\xi_{1}^{k}$ goes to positive infinity, $\\mathbf{W}_{33}^{k}=\\xi_{2}^{k}$ such that $\\xi_{2}^{k}-\\xi_{1}^{k}$ converges to infinity, with the rest of $\\mathbf{W}_{i j}^{k}$ bounded,then $\\widehat{\\mathbf{y}}_{\\mathbf{W}^{k}}\\left(\\mathbf{x}_{N+1}\\right))$ converges to $\\mathbf{y}_{i^{*}}$ as $k$ goes to infinity. ", "page_idx": 3}, {"type": "text", "text": "2.3  Training Dynamics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To train the transformer model over the 1-NN task, we consider the Mean-Square Error (MSE) loss function. Specifically, the loss function is defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\mathbf{W})=\\frac{1}{2}\\mathbb{E}_{\\left\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\right\\}_{i\\in[N]},\\mathbf{x}_{N+1}}\\left[\\left(\\widehat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})-\\mathbf{y}_{i^{*}}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Above, the expectation is taken with respect to the sampled prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ and the query ${\\bf x}_{N+1}$ . Notably, when the underlying distribution for the prompt and query are defined by Assumption 1, this loss function is nonconvex with respect to W. Such nonconvexity makes the optimization hard to solve without further conditions such as PL condition or KL condition [Bierstone and Milman, 1988, Karimi et al., 2020]. We leave the proof of nonconvexity in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "We shall consider the behavior of gradient descent on the single-layer attention architecture w.r.t. the loss function in Eq. (2.5). The parameters are updated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{W}^{k+1}-\\mathbf{W}^{k}=\\frac{1}{\\eta}\\nabla_{\\mathbf{W}}L(\\mathbf{W}^{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We shall consider the following initialization for the gradient descent: ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Initialization). Let $\\sigma>0$ be a parameter We assume the following initialization: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{W}^{0}=\\left(\\!\\!\\begin{array}{c c}{0_{(d+1)\\times(d+1)}}&{0_{d+1}}\\\\ {0_{d+1}}&{-\\sigma}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here the parameter $\\sigma$ is similar to masking, which is widely applied in self-attention training process, and prevents the model from focusing on the zero-label for the query ${\\bf x}_{N+1}$ , e.g. Vaswani et al. [2017], Baade et al. [2022], Chang et al. [2022]. The reason we take the zero initialization for non-diagonal entries will be made clear when we describe the proof in Section 4. However, from a higher view, it is because we want to keep the model focusing on the inner product between different $\\mathbf{x}_{i}$ ,whichlargely reduces the complexity of the dynamic system under gradient descent and makes it tractable. We leave the question of convergence under alternative random initialization schemes for future work. ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we summarize the convergence of training loss and testing error respectively. In Section 3.1, we discuss the convergence of training loss under gradient descent. Specifically, we prove that with a proper initialization constant $\\sigma$ , gradient descent is able to minimize the loss function $L(\\mathbf{W})$ despite the nonconvexity. In Section 3.2, we further discuss the testing error of the trained transformer under distribution shift. Specifically, we consider a distribution $\\mathbb{P}_{\\mathrm{test}}$ for the prompt $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}\\cup\\left\\{\\mathbf{x}_{N+1}\\right\\}$ , which is different from the training data distribution $\\mathbb{P}_{\\mathrm{prompt}}\\otimes\\mathbb{P}_{\\mathrm{query}}$ ,and discuss the difference between the trained transformer and 1-NN predictor under such distribution shift. ", "page_idx": 4}, {"type": "text", "text": "3.1  Convergence of Gradient Descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we prove that under suitable initialization parameter $\\sigma$ , the loss function will converge to zero under gradient descent. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Convergence of Gradient Descent). Consider performing gradient descent of the softmax-attentiontransformermodel $\\widehat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})$ :Suppose the initialization satisfies Assumption 2 with $\\sigma>2(\\operatorname*{max}\\{\\log(N d),-\\log\\big(1-(N\\sqrt{d})^{\\frac{1}{d}}\\big),C_{d}\\big(1-\\frac{1}{2^{N}}\\big)\\})$ , where $C_{d}=\\mathrm{poly}(d)$ and the numberofcontext $N\\geq O({\\sqrt{d}}\\log d).$ then $L(\\mathbf{W}^{k})$ converges to $\\boldsymbol{O}$ ", "page_idx": 4}, {"type": "text", "text": "We leave the detailed proof in Appendix C. Theorem 1 shows that for the 1-NN data distribution, with a large enough initialization constant $\\sigma$ , the training loss of the transformer converges to zero under gradient descent. Here $\\sigma$ plays a role similar to the masking techniques in the self-attention training training process, in which $\\sigma$ is often set as infinity or an extremely large number. Such a technique has been widely accepted and shown to greatly accelerate the training process Vaswani et al. [2017], Devlin et al. [2018], Dosovitskiy et al. [2020]. We also compare our results to existing works. Zhang et al. [2023] studied linear prediction tasks under gradient flow, however, their analysis is limited to linear attention layers. Huang et al. [2023] was the first to study softmax attention optimization under gradient descent, but their prediction is limited to linear prediction tasks under a finite orthogonal dictionary. Chen et al. [2024] established optimization convergence results for one-layer multi-head attention transformers under gradient flow. On the contrary, our work studies gradient descent convergence for transformer under a nonparametric estimator, setting it apart from all previous studies. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 Results for New Task under Distribution shift ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we discuss the behavior of trained transformers under distribution shifts, i.e., how the model extrapolate beyond the training distribution. Following the definition in Garg et al. $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in][N]}\\sim\\mathbb{P}_{\\mathrm{prompt}}^{\\mathrm{train}}$ $\\mathbf{x}_{N+1}\\sim\\mathbb{P}_{\\mathrm{query}}^{\\mathrm{train}}$ . During inference, the prompts and queries are sampled from a new distribution $\\mathbb{P}^{\\mathrm{{test}}}$ . We study the behavior of the trained transformers under possible prompt and query shift, i.e. $\\mathbb{P}^{\\mathrm{test}}\\neq\\mathbb{P}_{\\mathrm{prompt}}^{\\mathrm{train}}\\otimes\\mathbb{P}_{\\mathrm{query}}^{\\mathrm{ptrain}}$ trained model is still similar to a 1-NN predictor even under a distribution shift. Before formally stating our result, let us introduce the following assumption on the testing distribution: ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Testing Distribution). We make the following assumption on $\\mathbb{P}^{t e s t}$ ", "page_idx": 5}, {"type": "text", "text": "Note that Assumption 3 only requires the label $\\mathbf{y}_{i}$ is bounded and $\\mathbf{x}_{i}$ is supported on a sphere. We also remind the reader that we do not assume independence between different $\\mathbf{x}_{i}$ Or $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ and $\\{\\mathbf{y}_{i}\\}_{i\\in[N+1]}$ . Now we are ready to summarize our result in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Resemblance to 1-NN predictor under Distribution Shift). Suppose Assumption $^{\\,l}$ and3 holdfor $\\mathbb{P}_{p r o m p t}^{t r a i n}\\otimes\\mathbb{P}_{q u e r y}^{t r a i n}$ and $\\mathbb{P}^{t e s t}$ If wedefine ", "page_idx": 5}, {"type": "equation", "text": "$A_{\\delta}:=\\big\\{\\|\\mathbf{x}_{j}-\\mathbf{x}_{N+1}\\|_{2}^{2}\\geq\\|\\mathbf{x}_{i^{*}}-\\mathbf{x}_{N+1}\\|_{2}^{2}+\\delta$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then,after $K$ -iterations of gradient descent, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]},\\mathbf{x}_{N+1}}\\big[\\big(\\widehat{\\mathbf{y}}_{\\mathbf{W}^{K}}(\\mathbf{x}_{N+1})-\\mathbf{y}_{i^{*}}\\big)^{2}\\big]\\leq O\\big(\\operatorname*{inf}_{\\delta}\\big\\{R^{2}N^{2}K^{-\\mathrm{poly}(N,d)\\delta}+R^{2}\\mathbb{P}^{t e s t}(A_{\\delta}^{c})\\big\\}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "here the expectation is takenw.t $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}\\cup\\{\\mathbf{x}_{N+1}\\}\\sim\\mathbb{P}^{t e s t}$ . Recall that $\\mathbf{y}_{i^{*}}$ is the $^{\\,l}$ NN predictor of ${\\bf x}_{N+1}$ which we defined in Definition $^{\\,I}$ ", "page_idx": 5}, {"type": "text", "text": "We leave the detailed proof in Appendix D. Let us discuss the implication of Theorem 2. The event $A_{\\delta}$ describes the situation when the query ${\\bf x}_{N+1}$ is located at an \"inner point\" away from its decision boundary, in which its distance to the nearest neighbor $\\mathbf{x}_{i}*$ is strictly larger than all other points. Such a quantity is similar to the margin condition in classification theory in deep learning Bartlett et al. [2017] and $k$ -NN literature Chaudhuri and Dasgupta [2014], where the optimal choice probability is strictly larger than all suboptimal choices. Specifically, if $\\mathbb{P}^{\\mathrm{test}}(A_{\\delta^{*}})=1$ for some $\\delta^{*}>0$ , i.e., the query ${\\bf x}_{N+1}$ is strictly bounded away from the decision boundary almost surely, then the $L_{2}$ distance between $\\widehat{\\mathbf{y}}_{\\mathbf{W}^{k}}$ and the 1-NN predictor will converge in a $O(R^{2}\\dot{K}^{-\\mathrm{poly}(N,d)\\delta^{*}})$ even under a shifted distribution. We also introduce the following corollary, in which we show that when $\\mathbf{y}_{i}$ only takes value in a finite integer set, resembling a classification task, the trained transformer behaves like a 1-NN predictor under an additional rounding operation. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 (Classfication of Trained Transformer). Suppose $\\mathbf{y}_{i}\\in[M]$ for someinteger $M\\geq0$ under $\\mathbb{P}^{t e s t}$ thenwehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t e s t}\\big(\\operatorname{Round}\\left(\\widehat{\\mathbf{y}}_{\\mathbf{W}^{k}}(\\mathbf{x}_{N+1})\\right)\\neq\\mathbf{y}_{i^{*}}\\big)\\leq O\\big(\\operatorname*{inf}_{\\delta}\\big\\{M^{2}N^{2}K^{-\\operatorname{poly}(N,d)\\delta}+M^{2}\\mathbb{P}^{t e s t}(A_{\\delta}^{c})\\big\\}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Round}(t):=\\mathbb{1}_{[t]<\\frac{1}{2}}\\lfloor t\\rfloor+\\mathbb{1}_{[t]\\geq\\frac{1}{2}}\\lceil t\\rceil,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e. the mapping from $t\\in\\mathbb R$ to its closest integer, and $A_{\\delta}$ is defined as in Theorem 2. Moreover, if there exists $\\delta^{*}>0$ suchthat $\\mathbb{P}^{t e s t}(A_{\\delta^{*}})=0,$ then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}^{t e s t}\\big(\\operatorname{Round}\\left(\\widehat{\\mathbf{y}}_{\\mathbf{W}^{k}}(\\mathbf{x}_{N+1})\\right)\\neq\\mathbf{y}_{i^{*}}\\big)=0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "whenever $\\begin{array}{r}{K\\ge O\\big(\\frac{\\log(M N)}{\\mathrm{poly}(N,d)\\delta^{*}}\\big)}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "We leave the detailed proof in Appendix D. Corollary 1 provides a convergence rate for the classification difference between 1-NN and the pretrained transformer. Notably, when ${\\bf x}_{N+1}$ is well separated from the decision boundary in the testing distribution $\\mathbb{P}_{\\mathrm{test}}$ , the trained transformer will behave exactly the same as the 1-NN classifier in (poly(N,d)\\* ) gradient steps for the pretrained transformer. Theorem 2 and Corollary 1 show that the trained transformer under gradient descent is robust to both query and prompt distribution shift in the test distribution $\\mathbb{P}^{\\mathrm{{test}}}$ , in the sense that it will maintain its resemblance to a 1-NN predictor in both prediction and classification task, thus extended the results in Zhang et al. [2023], Huang et al. [2023], Chen et al. [2024] to a nonparametric estimator. ", "page_idx": 6}, {"type": "text", "text": "4 Sketch of Proof ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we sketch the proof of Theorem 1 and highlight the techniques we used. The full proof is left to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Equivalence to a Two-Dimensional Dynamic System. Recall that $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ and the first and second moment of $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ are uncorrelated. Utilizing this uncorrelation between $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ and $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ , we can eliminate the reliance of the gradient on $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ since we are considering a population loss. Moreover, utilizing the structure of the initialization, we can prove by induction that all $\\mathbf{W}_{i j}$ will remain zero except for $\\mathbf{W}_{11}$ and $\\mathbf{W}_{33}$ . This shows that with a suitable initialization, the transformer model will only focus on the relationship between different tokens $\\mathbf{x}_{i}$ throughout the whole training process. Our findings can be summarized by the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 (Closed-Form Gradient). With the initialization in Assumption 2, the gradient of $L(\\mathbf{W}^{k})$ withrespectto $\\mathbf{W}_{11}$ can be written in the following form for all $k\\geq0$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})={\\mathbb{E}}{\\Bigg[}\\sum_{i=1}^{N}g_{i}^{k}{\\big(}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}{\\big)}\\cdot\\mathbf{x}_{i}\\mathbf{x}_{N+1}^{\\top}+g_{i^{*}}^{k}{\\big(}\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}{\\big)}\\cdot\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top}{\\Bigg]}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\{g_{i}^{k}(x)\\}_{i\\in[N]}\\cup\\{g_{i^{*}}^{k}(x)\\}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a set of functions. Here the expectation is taken with respect to $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ with $\\begin{array}{r}{{\\bf x}_{i^{*}}=\\mathrm{argmin}_{{\\bf x}\\in\\{{\\bf x}_{i}\\}_{i\\in[N]}}\\,\\|{\\bf x}-{\\bf x}_{N+1}\\|_{2}}\\end{array}$ sampled id.fro a uniform distribution on $\\mathbb{S}^{d-1}$ Moreover, we have $\\nabla_{\\mathbf{W}_{i j}}L(\\mathbf{W}^{k})=0$ for all $(i,j)\\in[3]\\times[3]$ and all $k\\geq0$ except for ${\\bf W}_{11}$ and $\\mathbf{W}_{33}$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 shows that we only need to consider $\\mathbf{W}_{11}$ and $\\mathbf{W}_{33}$ in our update since all other entries will remain zero during the whole learning process. Note that in Eq. (4.1), all nonlinearity comes from the inner product between $\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}$ and ${\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}$ . Recall that $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ are i.i.d. sampled from a uniform distribution supported on a $d-1$ -dimensional sphere $\\mathbb{S}^{d-1}$ , therefore, the distribution of $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ is rotational invariance, which means $\\begin{array}{r}{\\bigotimes_{i\\in[N+1]}\\mathrm{P}_{\\mathbf{x}_{i}}\\;=\\;\\bigotimes_{i\\in[N+1]}\\mathrm{P}_{U\\mathbf{x}_{i}}}\\end{array}$ for all orthogonal matrix $U\\in\\mathbb{R}^{d\\times d}$ Since the rtation of $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ does not change the inner prodcts $\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}\\}_{i\\in[N]}$ and ${\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}$ , from the structure of $\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})$ illustrated by Eq. (4.1), we shall always have $U\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})U^{\\top}=\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})$ , which shows $\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})\\,=\\,c_{k}I_{d}$ for some constant $c_{k}$ by simple algebra. We summarize our result in the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Two-Dimensional System). With the initialization in Assumption 2, there exists two sets of real numbers $\\{\\xi_{1}^{k}\\}_{k\\ge0}$ and $\\{\\xi_{2}^{k}\\}_{k\\geq0}$ such that $\\mathbf{W}^{k}$ has thefollowingform: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{W}^{k}=\\mathrm{diag}\\{\\underbrace{\\xi_{1}^{k},\\dots,\\xi_{1}^{k}}_{d\\,t i m e s},0,-\\xi_{2}^{k}\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With Lemma 2, we reduce the dimension of the original dynamic system in Eq. (2.6) from $(d+2)^{2}$ to 2. We now only need to focus on the evolution of $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ for all $k\\geq0$ ", "page_idx": 7}, {"type": "text", "text": "Convergence of the Dynamic System. Lemma 2 helps us largely reduce the dimension of the training dynamics. However, this does not make our question a trivial one, as the loss function is still highly nonconvex even when we only need to consider a two-dimensional subspace of $\\mathbb{R}^{(d+2)\\times(d+2)}$ To see this, we introduce the following lemma: ", "page_idx": 7}, {"type": "text", "text": "Lemma 3 (Nonconvexity of Transformer Optimization). When W lie in a two-dimensional subspace $\\mathbb{R}^{(d+2)\\times(d+2)}$ defnedby $\\mathbf{W}\\,=\\,\\mathrm{diag}\\{\\underbrace{\\xi_{1},\\dots,\\xi_{1}}_{\\textit{d i m e s}},0,-\\xi_{2}\\}$ the original losfunctiondefnedin d times ", "page_idx": 7}, {"type": "text", "text": "Eq. (2.5) is equivalent to the following: ", "page_idx": 7}, {"type": "equation", "text": "$$\nL(\\xi_{1},\\xi_{2}):=\\mathbb{E}\\bigg[\\bigg(\\frac{\\sum_{j=1}^{N}\\exp(\\xi_{1}\\langle\\mathbf{x}_{j},\\mathbf{x}_{N+1}\\rangle)\\mathbf{y}_{j}}{\\sum_{i=1}^{N}\\exp(\\xi_{1}\\langle\\mathbf{x}_{i},\\mathbf{x}_{N+1}\\rangle)+\\exp(\\xi_{1}-\\xi_{2})}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Such loss function is still nonconvex. ", "page_idx": 7}, {"type": "text", "text": "We leave the detailed proof in Appendix E. Nonconexity shown by Lemma 3 implies attaining the global minimum could be hard. Previous works such as Zhang et al. [2023] utilize conditions such as Polyak-Lojasiewicz inequality to analyze such systems, however, those conditions are not applicable in our setting, and a more delicate analysis for the evolution of $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ is needed. We characterize their behavior by the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4. For $\\xi_{1}^{k}\\ge0$ there exists constants $c_{1},c_{2},c_{3},c_{4}>0,$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{d}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})\\geq c_{1}\\cdot\\exp(-6\\xi_{1}^{k})-c_{2}\\cdot\\exp(2\\xi_{1}^{k}-\\xi_{2}^{k}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{d}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})\\leq c_{3}\\cdot\\exp\\left(\\mathrm{{poly}}(N,d)\\cdot\\xi_{1}^{k}\\right)-c_{4}\\cdot\\exp\\left(2(\\xi_{1}^{k}-\\xi_{2}^{k})\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 4 shows that there exits a constant $c_{b}\\in(0,1)$ , such that $\\xi_{1}^{k}$ will keep increasing with a scale of $\\Omega(\\eta\\log k)$ until $\\xi_{1}^{k}\\le c_{b}\\xi_{2}^{k}$ . With this ratio, we obtain the following lemma for the increment of $\\xi_{2}^{k}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 5. For $\\xi_{1}^{k}\\ge0$ there exits constant $c_{1}^{\\prime},\\,c_{2}^{\\prime}$ ,such that ", "page_idx": 7}, {"type": "equation", "text": "$$\nc_{1}^{\\prime}\\cdot\\exp(-\\mathrm{poly}(N,d)\\cdot\\xi_{2}^{k})\\leq\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\leq c_{2}^{\\prime}\\cdot\\exp(-\\mathrm{poly}(N,d)\\cdot\\xi_{2}^{k}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 5 shows that $\\xi_{2}^{k}$ will monotonically increase with a scale of $\\Omega(\\eta\\exp(\\xi_{2}^{k}))$ , which implies $\\xi_{2}^{k}=\\Omega(\\log k)$ CombiningLmma4and5weshowtha t $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ converge to infinity, with $\\xi_{1}^{k}$ maintaining a slower speed, as its decreases when getting closer to $\\xi_{2}^{k}$ from the below. Recall that the loss function is equivalent to Eq. (4.2) under the initialization specified in Assumption 2, which shows that $L(\\xi_{1}^{k},\\xi_{2}^{k})$ will converge to zero as long as $\\xi_{1}^{k}$ and $\\xi_{2}^{k}-\\bar{\\xi}_{1}^{k}$ both converges to infinity. We thus conclude our proof of $L(\\mathbf{W}^{k})$ eventually converges to it global minimum. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In previous sections, we have shown that with the initialization specified in Assumption 2, a single softmax attention layer transformer is able to learn the 1-NN predictor under gradient descent and remain robust under distribution shift. We now conduct experiments in a less restrictive setting and show that even without specific initialization and full-batch gradient descent, simple stochastic gradient descent updates with random parameter initialization for the parameters are still sufficient for the model to learn the 1-NN predictor. First, we investigate the convergence of single-head single-layer transformers [Vaswani et al., 2017] trained on 1-NN tasks. The training data are sampled fromIptrain and $\\mathbb{P}_{\\mathrm{auerv}}^{\\mathrm{train}}$ , defined in Assumption 1. We choose context length $N\\in\\{16,32,64\\}$ and input dimension $d\\in\\{8,16\\}$ . The model is trained on a dataset with a size of 10000, and an epoch number of 200o. To ensure our training convergence result holds beyond the gradient descent scheme, we choose SGD as our optimizer, with a batch size of 128 and a learning rate of 0.1. We use the random Gaussian as our initialization. Our results for the training loss convergence are summarized in the left panel of Fig. 3. The results show that the model converges to 1-NN predictor on the training data even under SGD and random initialization. Moreover, as the dimension $d$ and the length of contexts $N$ becomes larger, the convergence speed becomes slower. ", "page_idx": 7}, {"type": "image", "img_path": "WDX45LNZXE/tmp/8994071dc331b75642d61466abba7b84412ee73073f834d49ae5b58c830c758c.jpg", "img_caption": ["Figure 3: Prediction error for single softmax attention layer as a function of gradient iteration number. (1) The left panel shows the convergence of loss function during the training process. (2) The right pannel shows the MSE between the trained model and a 1-NN predictor on a well-separated testing dataset under distribution shift, as we discuss in Section 5. Curves and error bars in both panels are computed as twice the standard deviation based on 10 independent trials. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To verify our results on the distribution shift, we generate testing data sampled from a distribution difference from the training data, and report the mean square error between the model prediction and the 1-NN predictor. Furthermore, the testing data satisfies $\\mathbb{P}(A_{\\delta}^{*})=1$ ,with $\\delta^{*}$ specified as 0.1. Recall that we defined $A_{\\delta}$ in Theorem 2 as the event where ${\\bf x}_{N+1}$ is separated from the decision boundary with a distance of at least $\\delta$ . We leave the details of our data-generating process in Appendix B. We test the trained transformer model on this dataset once every epoch throughout the training process. Our results are summarized in the right panel of Fig. 3. The results show that the testing error decreases much faster than the training loss, due to the boundary separation condition, which the uniformly-sampled training data do not enjoy. Our result also coincides with our theoretical result in Theorem 2, showing that the trained transformers are robust under distribution shift, and benefits greatly from staying away from the decision boundary. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We investigate the ability of single-layer transformers to learn the one-nearest neighbor prediction rule, a classic nonparametric estimator. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we demonstrate that, despite the nonconvexity of the loss function during gradient descent training, a single softmax attention layer can successfully emulate a one-nearest neighbor predictor. We further show that the trained transformer is robust to the distribution shift of the testing data. As far as we know, this paper is the first to establish training convergence and behavior under distribution shifts for softmax attention transformers beyond the domain of linear predictors. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We thank the anonymous reviewers for their helpful comments. Yuan Cao is partially supported by NSFC 12301657 and Hong Kong RGC-ECS 27308624. Han Liu's research is partially supported by the NIH R01LM01372201. Jason M. Klusowski was supported in part by the National Science ", "page_idx": 8}, {"type": "text", "text": "Foundation through CAREER DMS-2239448, DMS-2054808 and HDR TRIPODS CCF-1934924. Jianqing Fan's research was partially supported by NSF grants DMS-2210833, DMS-2053832, and ONR grant N00014-22-1-2340. Mengdi Wang acknowledges the support by NSF IIS-2107304, NSF CPS-2312093,ONR 1006977 and Genmab. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob Abernethy, Alekh Agarwal, Teodor Vanislavov Marinov, and Manfred K Warmuth. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. In International Conference on Algorithmic Learning Theory, pages 3-46. PMLR, 2024.   \nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.   \nEkin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \nAlan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram transformer. arXiv preprint arXiv:2203.16691, 2022.   \nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024.   \nPeter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in neural information processing systems, 30, 2017.   \nKevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is \u201cnearest neighbor\" meaningful? In Database Theory\u2014ICDT'99: 7th International Conference Jerusalem, Israel, January 10-12, 1999 Procedings 7, pages 217-235. Springer, 1999.   \nEdward Bierstone and Pierre D Milman. Semianalytic and subanalytic sets. Publications Mathematiques de I'IHES, 67:5-42, 1988.   \nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315-11325, 2022.   \nKamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. Advances in Neural Information Processing Systems, 27, 2014.   \nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.   \nSiyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.   \nHerbert A David and Haikady N Nagaraja. Order statistics. John Wiley & Sons, 2004 ", "page_idx": 9}, {"type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018. ", "page_idx": 9}, {"type": "text", "text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \nCong Fang, Jason D. Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field framework for over-parameterized deep neural networks, 2020.   \nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.   \nAngeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason D Lee. How well can transformers emulate in-context newton's method? arXiv preprint arXiv:2403.03183, 2024.   \nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000-16009, 2022.   \nYu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \nMichael Janner, Qiyang Li, and Sergey Levine. Offine reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273-1286, 2021.   \nSamy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822-37836, 2022.   \nHong Jun Jeon, Jason D. Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context learning, 2024.   \nHamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-lojasiewicz condition, 2020.   \nYingcong Li, Muhammed Emrullah Idiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR, 2023a.   \nYuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. In International Conference on Machine Learning, pages 19689-19729. PMLR, 2023b.   \nYiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean-field analysis of deep resnet and beyond: Towards provable optimization via overparameterization from depth, 2020.   \nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33), July 2018. ISSN 1091-6490. doi: 10.1073/pnas.1806579115. URL http: //dx .doi .org/10 .1073/pnas. 1806579115.   \nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022.   \nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International conference on machine learning, pages 7487-7498. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Leif E Peterson. K-nearest neighbor. Scholarpedia, 4(2):1883, 2009 ", "page_idx": 10}, {"type": "text", "text": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.   \nYuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36:71911-71947, 2023.   \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023.   \nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.   \nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. ", "page_idx": 11}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In-Context Learning.  In-context learning refers to transformers\u2019 ability to solve unseen tasks without fine-tuning. Min et al. [2022] studied which aspects of the demonstrations contribute to end-task performance. Further, Garg et al. [2022] empirically investigated the ability of transformer architectures to learn a variety of function classes in context. From a theoretical perspective, Bai et al. [2024], Abernethy et al. [2024], Akyirek et al. [2022] studied the expressiveness of transformers to approximate statistical algorithms. Li et al. [2023a] studied the generation ability of transformers in ICL tasks. Jeon et al. [2024] studies the information-theoretical lower bound of in-context learning. However, none of these works studied the optimization process when training a transformer in context. ", "page_idx": 12}, {"type": "text", "text": "Optimization of Transformers.  There are various works that studied optimization for transformers. Among all these studies, Huang et al. [2023], Zhang et al. [2023], Chen et al. [2024] studied the optimization dynamics of transformers of learning linear prediction tasks in context. Specifically, Zhang et al., 2023 studied the gradient flow dynamics of linear self-attention model and obtained convergence results utilizing PL condition. Huang et al. [2023] studied linear prediction tasks with a softmax-attention layer trained with gradient descent, with a finite support prompt/query distribution. Chen et al. [2024] studied the gradient flow dynamics in training multi-head softmax attention on linear prediction tasks. Ahn et al. [2024], Giannou et al. [2024] studied transformers? ability to learn optimization methods. Prior works also studied transformer optimization beyond in-context learning tasks. Tian et al. [2023] studied a single-layer transformer with one self-attention layer plus one decoder layer, and proved that the attention layer acts as a scanning algorithm. Li et al. [2023b] studied the optimization of transformers in learning semantic structures. Jelassi et al. [2022] studies the spatial localization property of vision transformer in optimization. ", "page_idx": 12}, {"type": "text", "text": "Notations.We write $[N]=\\{1,\\dots,N\\}$ . For two vector $\\mathbf{x}^{1}=(x_{1}^{1},\\ldots,x_{d}^{1})$ and $\\mathbf{x}^{2}=(x_{1}^{2},\\ldots,x_{d}^{2})$ ta $\\textstyle\\sum_{i=1}^{d}x_{i}^{1}x_{i}^{2}$ $\\mathbf{x}^{1\\top}\\mathbf{x}^{2}$ $0_{n}$ ae $0_{m\\times n}$   \n$n$ $m\\times n$ $\\{\\mathbf{x}\\,\\in\\,\\mathbb{R}^{d}\\,:\\,\\|\\mathbf{x}\\|_{2}\\,=\\,1\\}$ $\\mathbb{S}^{d-1}$ For two series $\\{a_{k}\\}_{k\\ge0}$ and $\\{b_{k}\\}_{k\\ge0}$ , we write $a_{k}\\,=\\,\\Omega(b_{k})$ if there exists $0<C_{1},C_{2}$ such that $C_{1}\\cdot b_{k}\\,\\le\\,a_{k}\\,\\le\\,C_{2}\\cdot b_{k}$ . We write $\\bar{a}_{k}\\,=\\,O(b_{k})$ if there exists $C>0$ such that $a_{k}\\le C\\cdot b_{k}$ .We use $a_{k}\\mathrm{~=~}\\mathrm{poly}(b_{k})$ if there exits an $n$ -degree polynomial $P_{n}(x)$ such that $a_{k}=O(P_{n}(b_{k}))$ . For $(a_{k})_{k\\in[N]}$ , we define its permutation $\\left(a_{(k)}\\right)_{k\\in[N]}$ such that $a_{(1)}\\geq a_{(2)}\\geq\\ldots\\geq a_{(n)}$ We use $I_{d}$ to denote the $d$ -dimensional identity matrix and sometimes we also use $I$ when the dimension is clear from the context. Unless otherwise defined, we use lower case letters for scalars and vectors and use upper case letters for matrices. For a matrix A we denote its entry in the $i$ -th row and $j$ -th column by $[\\mathbf{A}]_{i j}$ ", "page_idx": 12}, {"type": "text", "text": "B  Data-Generating Process in Experiment ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In our experiment, we generate the testing dataset with context length $N$ and dimension $d$ with separation parameter $\\delta^{*}$ such that $\\begin{array}{r}{\\|\\mathbf{x}_{j}-\\mathbf{\\bar{x_{N+1}}}\\|_{2}^{2}\\geq\\|\\mathbf{x}_{i^{*}}-\\mathbf{x}_{N+1}\\|_{2}^{2}\\bar{+}\\,\\delta}\\end{array}$ for all $j\\in[N],j\\neq i^{*}$ by the following procedure: ", "page_idx": 12}, {"type": "text", "text": "(i) We sample $\\mathbf{x}_{i}$ from the uniform distribution on $\\mathbb{S}^{d-1}$ and $\\mathbf{y}_{i}\\sim\\mathcal{N}(0,1)$ for all $i\\in[N]$   \n(i)  We random sample $i^{*}\\in[N]$ by uniform distribution and set $\\mathbf{x}_{N+1}=\\mathbf{x}_{i^{*}}$ with the 1-NN label being $\\mathbf{y}_{i^{*}}$ .\uff0c   \n(ii) If $||\\mathbf{x}_{j}-\\mathbf{x}_{N+1}||_{2}^{2}\\le\\delta$ , set $\\mathbf{x}_{j}=-\\mathbf{x}_{j}$ ", "page_idx": 12}, {"type": "text", "text": "C Proof for Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we elaborate on the proof of Theorem 1. Our proof can be broken down into the following steps: ", "page_idx": 12}, {"type": "text", "text": "(i) With induction, we prove that the evolution dynamics of $\\mathbf{W}^{k}$ under gradient descent can be captured by a two-parameter dynamic system, parametrized by $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ ", "page_idx": 12}, {"type": "text", "text": "(i) By estimating the update dynamics for $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ we prove that with a proper initialization parameter $\\sigma$ , We will have $\\xi_{1}^{k},\\xi_{2}^{k}=\\Omega(\\overset{\\cdot}{\\log k})$ ,with $\\xi_{1}^{k}\\leq c\\cdot\\xi_{2}^{k}$ for a constant $c\\in(0,1)$   \n(i) With the non-asymptotic behavior of $\\xi_{1}$ and $\\xi_{2}$ determined, we further control the loss function $L(\\mathbf{W}^{k})$ and establish a convergence for the loss function. ", "page_idx": 13}, {"type": "text", "text": "In the following sections, we will discuss how we prove those three items in Section C.1, C.2 and C.3, respectively. ", "page_idx": 13}, {"type": "text", "text": "C.1  Dynamic of Gradient Descent ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove that we can characterize the evolution of $\\mathbf{W}$ under gradient descent with a two-parameter dynamic system. We proceed by mathematical induction: ", "page_idx": 13}, {"type": "text", "text": "(1) We prove that when $\\mathbf{W}^{k}$ is a diagonal matrix with $[\\mathbf{W}^{k}]_{[d]\\times[d]}=c_{k}I_{d}$ for some constant $c_{k}$ , we can have the following break downs of our proof: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}(\\mathbf{W}^{k+1}-\\mathbf{W}^{k})=\\mathrm{diag}\\{\\underbrace{\\Delta\\xi_{1}^{0},\\dots,\\Delta\\xi_{1}^{0}}_{\\mathrm{d\\,times}},0,-\\Delta\\xi_{2}^{0}\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Delta\\xi_{1}^{0}$ and $\\Delta\\xi_{2}^{0}$ are two positive constants. ", "page_idx": 13}, {"type": "text", "text": "(2) By $\\mathbf{W}^{k}$ being a diagonal matrix with $[\\mathbf{W}^{k}]_{[d]\\times[d]}=c_{k}I_{d}$ combined with Eq. (C.1), we prove that $\\mathbf{W}^{k+1}$ is diagonal matri and $[\\mathbf{W}^{k+1}]_{[d]\\times[d]}=c_{k+1}I_{d}.$ \uff1a ", "page_idx": 13}, {"type": "text", "text": "Now since (1) is naturally satisfied by the initialization in Assumption 2 for $k=0$ , we can conclude the proof by simply proving (1) for $k\\geq1$ ", "page_idx": 13}, {"type": "text", "text": "Proof for Step (1) of Induction.  Recall that our loss function can be written as ", "text_level": 1, "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\boldsymbol{\\Sigma}}(\\mathbf{W})=\\frac{1}{2}\\mathbb{E}_{\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]};\\mathbf{x}_{N+1}}\\left[\\left(\\widehat{\\mathbf{y}}\\mathbf{W}\\mathbf{(x}_{N+1})-f(\\mathbf{x}_{N+1};\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]})\\right)^{2}\\right]}\\\\ &{\\qquad=\\frac{1}{2}\\mathbb{E}_{\\{\\mathbf{x}_{i},\\mathbf{y}_{i}\\}_{i\\in[N]};\\mathbf{x}_{N+1}}\\left[\\left(\\frac{\\sum_{j=1}^{N}\\exp{\\left(\\mathbf{x}_{j}^{\\top}\\mathbf{W}_{11}\\mathbf{x}_{N+1}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\mathbf{x}_{N+1}+\\mathbf{x}_{j}\\top\\mathbf{W}_{13}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\right)}\\mathbf{y}_{j}}{\\sum_{j=1}^{N+1}\\exp{\\left(\\mathbf{x}_{j}^{\\top}\\mathbf{W}_{11}\\mathbf{x}_{N+1}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\mathbf{x}_{N+1}+\\mathbf{x}_{j}\\top\\mathbf{W}_{13}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\right)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\left(\\mathbf{W}_{11}\\quad\\mathbf{W}_{12}\\quad\\mathbf{W}_{13}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $\\mathbf{W}_{11}\\in\\mathbb{R}^{d\\times d}$ \uff0c $\\mathbf{W_{21}}\\in\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{31}\\in\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{12}\\in\\mathbb{R}^{d\\times1}$ \uff0c $\\mathbf{W}_{13}\\,\\in\\,\\mathbb{R}^{1\\times d}$ \uff0c $\\mathbf{W}_{22},\\mathbf{W}_{23},\\mathbf{W}_{32}$ and $\\mathbf{W}_{33}\\in\\mathbb{R}$ , and we make the additional definition of $\\mathbf{y}_{j}^{\\dagger}=\\mathbf{y}_{j}$ \uff0c $\\mathbf{W}_{j}^{\\dagger}=\\mathbf{W}_{23}$ \uff0c $\\mathbf{W}_{j}^{\\ddag}=\\mathbf{W}_{21}$ for $j\\in[N]$ and $\\mathbf{y}_{N+1}^{\\dagger}=1,\\mathbf{W}_{N+1}^{\\dagger}=\\mathbf{W}_{33},\\mathbf{W}_{N+1}^{\\dagger}=\\mathbf{W}_{31}$ . Throughout the rest of this paper, we will also adopt thenotationof ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}):=\\frac{\\exp\\left(\\mathbf{x}_{j}^{\\top}\\mathbf{W}_{11}\\mathbf{x}_{N+1}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\mathbf{x}_{N+1}+\\mathbf{x}_{j}\\top\\mathbf{W}_{13}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\right)}{\\sum_{i}\\exp\\left(\\mathbf{x}_{i}^{\\top}\\mathbf{W}_{11}\\mathbf{x}_{N+1}+\\mathbf{y}_{i}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\mathbf{x}_{N+1}+\\mathbf{x}_{i}\\top\\mathbf{W}_{13}+\\mathbf{y}_{j}^{\\dagger}\\mathbf{W}_{j}^{\\dagger}\\right)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "when there is no ambiguity, where $q:\\mathbf{W}\\times\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}\\times\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ . As we have discussed in Section 2.2, $\\mathbf{W}_{*2}$ does not affect the outcome of the transformer, therefore the second column will remain O throughout the training procedure. Now we will calculate the gradient $\\nabla_{\\mathbf{W}_{i j}}L(\\mathbf{W}^{0})$ respectively. Note that for $\\mathbf{f}_{\\theta}^{j}(x)=\\exp(\\mathbf{g}_{\\theta}^{j}(x))$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n7_{\\theta}\\left(\\frac{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(x)\\cdot\\mathbf{y}_{j}}{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})}-\\mathbf{y}_{i^{*}}\\right)^{2}=2\\left(\\frac{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})\\cdot\\mathbf{y}_{j}}{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})}-\\mathbf{y}_{i^{*}}\\right)\\cdot\\left\\{\\frac{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})\\mathbf{y}_{j}\\nabla_{\\theta}\\mathbf{g}_{\\theta}^{j}}{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})}-\\frac{\\sum_{j}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})\\nabla\\mathbf{g}_{\\theta}^{j}(\\mathbf{x})}{\\sum_{j=1}^{N+1}\\mathbf{f}_{\\theta}^{j}(\\mathbf{x})}\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "therefore, with some algebra, we have the following closed-form formula for $\\nabla_{\\mathbf{W}_{i j}}L(\\mathbf{W})$ respectively, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\mathbf{w}_{1},L}(\\mathbf{W})=\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=0}^{\\infty}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}\\mathbf{x}_{k+1}^{\\top})\\mathbf{y}_{j}-\\displaystyle\\sum_{j=1}^{\\infty}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}\\mathbf{x}_{k+1}^{\\top})\\bigg]\\left\\{\\displaystyle\\sum_{j=1}^{\\infty+n}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j})\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\cdot\\left\\{\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{j}-\\mathbf{y}_{k}^{\\top})\\right\\}\\bigg],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\bigg[\\displaystyle\\sum_{j=1}^{n}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{k+1}^{\\top})(\\mathbf{y}_{j})-\\displaystyle\\sum_{j=1}^{N}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{k+1}^{\\top})\\bigg]\\left\\{\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{k}^{\\top})\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\left\\{\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{y}_{j}-\\mathbf{y}_{k}^{\\top})\\right\\}\\bigg],}\\\\ &{\\zeta_{\\mathbf{w}_{1},L}(\\mathbf{W})=\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{j}-\\mathbf{y}_{k+1}^{\\top})\\bigg]\\left\\{\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{k}^{\\top})\\right\\}\\bigg],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\left\\{\\displaystyle\\sum_{j=1}^{n+1}q_{1}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}\\mathbf{x}_{l}^{\\top})-\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\mathbf{W}_{31}}L(\\mathbf{W})=\\mathbb{E}\\bigg[\\bigg\\{-\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j})\\}\\cdot\\mathbf{q}_{N+1}(\\mathbf{W})\\mathbf{x}_{N+1}^{\\top}\\bigg\\}\\cdot\\bigg\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}})\\bigg\\}\\bigg],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}_{33}}L(\\mathbf{W})=\\mathbb{E}\\bigg[\\bigg(-\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})\\mathbf{y}_{j}\\bigg)\\mathbf{q}_{N+1}(\\mathbf{W})\\cdot\\bigg\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}})\\bigg\\}\\bigg].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the induction assumption, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})=\\frac{\\mathbb{1}_{j\\neq N+1}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{j},\\mathbf{x}_{N+1}\\rangle)+\\mathbb{1}_{j=N+1}\\exp(\\xi_{1}^{k}\\|\\mathbf{x}_{N+1}\\|_{2}^{2}-\\xi_{2}^{k})}{\\sum_{i=1}^{N}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{i},\\mathbf{x}_{N+1}\\rangle)+\\exp(\\xi_{1}^{k}\\|\\mathbf{x}_{N+1}\\|_{2}^{2}-\\xi_{2}^{k})}}\\\\ {\\displaystyle=\\frac{\\mathbb{1}_{j\\neq N+1}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{j},\\mathbf{x}_{N+1}\\rangle)+\\mathbb{1}_{j=N+1}\\exp(\\xi_{1}^{k}-\\xi_{2}^{k})}{\\sum_{i=1}^{N}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{i},\\mathbf{x}_{N+1}\\rangle)+\\exp(\\xi_{1}^{k}-\\xi_{2}^{k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first equality comes from $\\|\\mathbf{x}_{N+1}\\|_{2}\\,=\\,1$ .Therefore, under our induction ${\\bf q}_{j}$ is only a function of $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ and $\\mathbf{W}^{k}$ , independent to $\\{\\mathbf{y}_{i}\\}_{i\\in[N]}$ . Now, with the closed form of $\\nabla_{\\mathbf{W}_{i j}}L(\\mathbf{W})$ , we can make the following calculation. First, we calculate $\\nabla_{\\mathbf{W}_{1}1}L(\\mathbf{W}^{k})$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle7_{{\\bf W}_{11}}L({\\bf W}^{k})={\\mathbb E}\\bigg[\\bigg\\{\\sum_{j=1}^{N+1}{\\bf q}_{j}({\\bf x},{\\bf W}^{k})({\\bf x}_{j}{\\bf x}_{N+1}^{\\top})({\\bf y}_{j}-{\\bf y}_{i^{*}})-\\big\\{\\sum_{j=1}^{N+1}{\\bf q}_{j}({\\bf x},{\\bf W}^{k})({\\bf x}_{j}{\\bf x}_{N+1}^{\\top})\\big\\}\\big\\{\\sum_{j=1}^{N+1}{\\bf q}_{j}({\\bf x},{\\bf W}^{k})({\\bf x}_{j},{\\bf x}_{N+1}^{\\top})\\bigg\\}\\bigg]}~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~\\cdot\\left\\{\\sum_{j=1}^{N+1}{\\bf q}_{j}({\\bf x},{\\bf W}^{k})({\\bf y}_{j}-{\\bf y}_{i^{*}})\\right\\}\\bigg]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n=\\mathbb{E}\\bigg[\\bigg\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})(\\mathbf{y}_{j}-\\mathbf{y}_{i}^{*})\\bigg\\}\\bigg\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{y}_{j}-\\mathbf{y}_{i}^{*})\\bigg\\}\\bigg]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $\\mathbf{q}(\\mathbf{x},\\mathbf{W}^{k})$ is only a function of $\\mathbf{x}$ by our induction assumption, we have ", "page_idx": 15}, {"type": "text", "text": "here ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{(i)}}=\\mathbb{E}\\Bigg[\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\Bigg(\\underbrace{y_{j}\\sum_{j=1}^{N+1}}_{j^{\\prime}=1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{y}_{j^{\\prime}}\\Bigg)\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})^{2}(\\mathbf{x}_{j}\\mathbf{\\times}_{N+1}^{\\top})\\Bigg],}\\\\ &{\\mathrm{(ii)}=-\\mathbb{E}\\Bigg[\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\mathbb{E}\\Bigg[\\mathbf{y}_{j}\\mathbf{y}_{i^{\\prime}}\\mathbf{x}_{1:N+1}\\Bigg]\\Bigg]}\\\\ &{=-\\mathbb{E}\\Bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\sum_{j=1}^{N+1}\\mathbf{1}_{j^{\\prime}=\\arg\\operatorname*{max}_{i\\in[N]}(\\mathbf{x}_{N+1},\\mathbf{x}_{i})}\\mathbb{E}[\\mathbf{y}_{j^{\\prime}}\\mathbf{y}_{j}|\\mathbf{x}_{1:N+1}]\\Bigg]}\\\\ &{=-\\mathbb{E}\\Bigg[\\mathbf{q}_{i}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{x}_{i}\\mathbf{\\times}_{N+1}^{\\top}\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{(iii)}=-\\mathbb{E}\\biggl[\\sum_{j,j^{\\prime}=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{q}_{j^{\\prime}}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\mathbf{y}_{i^{*}}\\mathbf{y}_{j^{\\prime}}\\biggr]}}\\\\ {{\\displaystyle~~~~=-\\mathbb{E}\\biggl[\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\biggr]}}\\\\ {{\\displaystyle\\mathrm{(iv)}=\\mathbb{E}\\biggl[\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\biggr],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "also for (2), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{2})=-\\mathbb{E}\\Bigg[\\Bigg\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top}\\Bigg\\}\\mathbb{E}\\Bigg[\\Bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{y}_{j}-\\mathbf{y}_{i}\\cdot)\\Bigg\\}^{2}\\Bigg|\\mathbf{x}_{1:N+1}\\Bigg]\\Bigg]}\\\\ {{\\mathrm{~}=-\\mathbb{E}\\Bigg[\\Bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top}\\Bigg\\}\\Bigg\\{\\displaystyle\\sum_{j,j^{\\prime}=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{q}_{j^{\\prime}}(\\mathbf{x},\\mathbf{W}^{k})\\mathbb{E}\\Bigg[\\big\\{\\mathbf{y}_{j}\\mathbf{y}_{j^{\\prime}}-(\\mathbf{y}_{j}+\\mathbf{y}_{j^{\\prime}})\\mathbf{y}_{i^{\\ast}}\\mathrm{~}+\\mathrm{~}}}\\\\ {{\\mathrm{~}=-\\mathbb{E}\\Bigg[\\bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top}\\Bigg\\}\\Bigg\\{1+\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})-\\displaystyle\\sum_{j,j^{\\prime}=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{q}_{j^{\\prime}}(\\mathbf{x},\\mathbf{W}^{k})\\mathbb{E}\\Bigg[(\\mathbf{y}_{j}\\cdot\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\mathbf{q}_{j^{\\prime}}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{b}\\Bigg\\}\\Bigg]}}\\\\ {{\\mathrm{~}=-\\mathbb{E}\\Bigg[\\Bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top}\\Bigg\\}\\Bigg\\{1+\\displaystyle\\sum_{j=1}^{N}\\mathbf \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$(1)+(2)$ gives us ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})=\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\bigg]-\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top})\\bigg]+\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\bigg(\\displaystyle\\sum_{j=1}^{N-1}\\mathbf{q}_{j^{*}}^{2}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\bigg)\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})(\\mathbf{x}_{j}\\mathbf{x}_{N+1}^{\\top})\\bigg)\\bigg].\\qquad\\qquad\\qquad\\mathrm{(C.10)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $\\nabla_{\\mathbf{W}_{21}}L(\\mathbf{W}_{k})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\mathbf{W}_{21}}L(\\mathbf{W}_{k})=\\mathbb{E}\\bigg[\\bigg\\{\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j}\\mathbf{x}_{N+1}^{\\top})(\\mathbf{y}_{j})-\\Big\\{\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j}\\mathbf{x}_{N+1}^{\\top})\\Big\\}\\bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j}\\mathbf{x}_{N+1}^{\\top})\\bigg\\}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\cdot\\left\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j}-\\mathbf{y}_{i}*)\\right\\}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "note that it is the expectation of multiplication of three linear functions of $\\mathbf{y}_{i}$ , therefore $\\nabla_{\\mathbf{W}_{21}}L(\\mathbf{W}_{k})$ equals to O by symmetry. For $\\nabla_{\\mathbf{W}_{31}}L(\\mathbf{W}_{k})$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\mathbf{W}_{31}}L(\\mathbf{W}_{k})=\\mathbb{E}\\bigg[\\bigg\\{-\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j})\\}\\cdot\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}_{k})\\mathbf{x}_{N+1}^{\\top}\\bigg\\}\\cdot\\bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}_{k})(\\mathbf{y}_{j}-\\mathbf{y}_{i}\\cdot\\mathbf{\\hat{y}})}\\\\ &{\\qquad\\qquad\\qquad=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here the second equality comes from $\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})$ being an even function of $\\mathbf{x}$ and $\\mathbf{x}$ has a symmetric distribution. For $\\nabla_{\\mathbf{W}_{13}}L(\\mathbf{W}_{k})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\nabla_{\\mathbf{W}_{13}}L(\\mathbf{W}_{k})=\\mathbb{E}\\bigg[\\bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{y}_{j})\\mathbf{x}_{j}-\\Big\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{x}_{j})\\Big\\}\\Big\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{y}_{j})\\Big\\}\\bigg\\}}\\\\ {\\displaystyle\\cdot\\left\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}})\\right\\}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here the second equality comes from $\\mathbf{q}_{j}(x,\\mathbf{W}_{k})$ is a function of $\\|\\mathbf{x}\\|^{2}$ and $\\mathbf{x}$ has a symmetric distribution. For $\\nabla_{\\mathbf{W}_{23}}L(\\mathbf{W}_{k})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}_{23}}L(\\mathbf{W}_{k})=\\left\\{\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{y}_{j})\\mathbf{y}_{j}-\\left\\{\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})\\mathbf{y}_{j}\\right\\}\\left\\{\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{W}^{K Q})(\\mathbf{y}_{j})\\right\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\cdot\\left\\{\\sum_{j=1}^{N+1}{\\bf q}_{j}({\\bf W}^{K Q})({\\bf y}_{j}-{\\bf y}_{b i^{\\ast}})\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "due to its symmetry in y. For $\\nabla_{\\mathbf{W}_{33}}L(\\mathbf{W}_{k})$ , we have the following calculation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{\\mathbf{W}_{33}}L(\\mathbf{W}_{k})=\\mathbb{E}\\bigg[\\bigg(-\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})\\mathbf{y}_{j}\\bigg)\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}_{k})\\cdot\\bigg\\{\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})(\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}})\\bigg\\}\\bigg]}}\\\\ &{}&{\\,=\\mathbb{E}\\bigg[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}_{k})\\bigg\\{\\mathbf{y}_{i^{*}}\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})\\mathbf{y}_{j}\\bigg)-\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}_{k})\\mathbf{y}_{j}\\bigg)^{2}\\bigg\\}\\bigg]}\\\\ &{}&{\\,=\\mathbb{E}\\bigg[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}_{k})\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}_{k})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}_{k})\\big\\}\\bigg]}\\end{array}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With all previous calculations, we know that $\\nabla_{\\mathbf{W}_{i j}}L(\\mathbf{W}^{k})$ equals O except for $\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})$ and $\\nabla_{\\mathbf{W}_{33}}L(\\mathbf{W}^{k})$ with our induction assumption. ", "page_idx": 17}, {"type": "text", "text": "Now we only need to prove that $\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W})$ is a diagonal matrix. Recall that $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ is identically uniformly distributed on $\\mathbb{S}^{d-1}$ , thus it naturally satisfies the following Assumption: ", "page_idx": 17}, {"type": "text", "text": "Assumption 4 (Rotational Invariance). We say the distribution of $\\mathbf{x}$ is rotationally invariance, $i f$ $\\mathbf{x}\\sim\\mathrm{P}_{x}$ and for every $U\\in{\\mathcal{O}}(d)$ where $\\scriptscriptstyle\\mathcal{O}$ is the group of orthogonal matrices, we have $\\mathrm{P}_{x}=U_{\\#}\\mathrm{P}_{x}$ ", "page_idx": 17}, {"type": "text", "text": "A straightforward lemma under Assumption 4 is the following. ", "page_idx": 17}, {"type": "text", "text": "Lemma6.Suppose $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ aresampled id.from $\\mathrm{P}_{x}$ and Assumption4 holds,thenwe have $\\begin{array}{r}{\\bigotimes_{i=1}^{N+1}\\mathrm{P}_{\\mathbf{x}_{i}}=\\bigotimes_{i=1}^{N+1}\\mathrm{P}_{U\\mathbf{x}_{i}}}\\end{array}$ forall $U\\in O(d)$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Since $\\mathbf{x}_{i^{*}}$ are sampled i.i.d. from $\\mathrm{P}_{x}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bigotimes_{i=1}^{N+1}\\mathrm{P}_{\\mathbf{x}_{i}}=\\bigotimes_{i=1}^{N+1}\\mathrm{P}_{U\\mathbf{x}_{i}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 7. When $\\mathbf{x}_{i}~\\sim~\\mathrm{P}_{x}$ \uff0c $\\mathrm{P}_{x}$ satisfies Assumption $^{4}$ and $\\mathbf{W}^{k}$ is a diagonal matrix with $[\\mathbf{W}^{k}]_{[d]\\times[d]}=c_{k}I_{d},$ we have $\\mathbb{E}[\\nabla_{\\mathbf{W}_{11}}L(\\mathbf{W}^{k})]=a_{k}\\cdot I_{d}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Note that by the induction assumption, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})=\\frac{\\mathbb{1}_{j\\neq N+1}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{j},\\mathbf{x}_{N+1}\\rangle)+\\mathbb{1}_{j=N+1}\\exp(\\xi_{1}^{k}-\\xi_{2}^{k})}{\\sum_{i=1}^{N}\\exp(\\xi_{1}^{k}\\langle\\mathbf{x}_{i},\\mathbf{x}_{N+1}\\rangle)+\\exp(\\xi_{1}^{k}-\\xi_{2}^{k})},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "therefore, $\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})=\\mathbf{q}_{j}(U\\mathbf{x},\\mathbf{W}^{k})$ for all $U\\in{\\mathcal{O}}(d)$ . By Eq. (C.10),there exits a set of functions ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\{g_{i}^{k}(x)\\}_{i\\in[N]}\\cup\\{g_{i^{*}}^{k}(x)\\}:\\mathbb{R}\\rightarrow\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{W}_{11}^{k}}L(\\mathbf{W})=\\sum_{i=1}^{N}g_{i}^{k}(\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1})\\cdot\\mathbf{x}_{i}\\mathbf{x}_{N+1}^{\\top}+g_{i^{*}}^{k}(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\cdot\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "here the expectation is taken with respect to $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ . By Lemma 6, we have $\\mathrm{P}_{\\mathbf{x}_{i}}\\otimes\\mathrm{P}_{\\mathbf{x}_{j}}=$ $\\mathrm{P}_{U{\\bf x}_{i}}\\otimes\\mathrm{P}_{U{\\bf x}_{j}}$ for any orthogonal matrix $U\\in{\\mathcal{O}}(d)$ ,therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[g_{i}^{k}({\\mathbf x}_{i}^{\\top}{\\mathbf x}_{N+1})\\cdot{\\mathbf x}_{i}{\\mathbf x}_{N+1}]=\\mathbb{E}[g_{i}^{k}((U{\\mathbf x}_{i})^{\\top}(U{\\mathbf x}_{N+1}))\\cdot(U{\\mathbf x}_{i})(U{\\mathbf x}_{N+1})^{\\top}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=U\\mathbb{E}[g_{i}^{k}({\\mathbf x}_{i}^{\\top}{\\mathbf x}_{N+1})\\cdot{\\mathbf x}_{i}{\\mathbf x}_{N+1}^{\\top}]U^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "here the first inequality comes from $\\mathrm{P}_{\\mathbf{x}_{i}}\\otimes\\mathrm{P}_{\\mathbf{x}_{j}}=\\mathrm{P}_{U\\mathbf{x}_{i}}\\otimes\\mathrm{P}_{U\\mathbf{x}_{j}}$ . Such identity holds for all orthogonal matrix $U$ , therefore $\\mathbb{E}[g_{i}^{k}({\\mathbf{x}_{i}^{\\top}}{\\mathbf{x}_{N+1}})\\cdot{\\mathbf{x}_{i}}{\\mathbf{x}_{N+1}}]$ must be a multiplication of the identity matrix $I_{d}$ Now, note that for all orthogonal matrix $U$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{i^{*}}=\\underset{\\mathbf{x}_{i}\\in\\{\\mathbf{x}_{j}\\}_{j\\in[N]}}{\\mathrm{argmin}}\\~~\\|\\mathbf{x}_{i}-\\mathbf{x}_{N+1}\\|_{2},}\\\\ &{U\\mathbf{x}_{i^{*}}=U\\underset{\\mathbf{x}_{i}\\in\\{\\mathbf{x}_{j}\\}_{j\\in[N]}}{\\mathrm{argmin}}~\\|U\\mathbf{x}_{i}-U\\mathbf{x}_{N+1}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{P}(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}|\\mathbf{x}_{1:(N+1)})=\\mathrm{P}(U\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top}U^{\\top}|U\\mathbf{x}_{1:N+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\mathrm{P}(\\mathbf{x}_{1:N+1})=\\mathrm{P}(U\\mathbf{x}_{1:N+1})$ , we have $\\mathrm P(\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1})=\\mathrm P(U\\mathbf x_{i^{*}}\\mathbf x_{N+1}^{\\top}U^{\\top})$ , therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[g_{i^{*}}^{k}({\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1})\\cdot{\\mathbf x}_{i^{*}}{\\mathbf x}_{N+1}^{\\top}]=\\mathbb{E}[g_{i^{*}}^{k}((U{\\mathbf x}_{i^{*}})^{\\top}(U{\\mathbf x}_{N+1}))\\cdot(U{\\mathbf x}_{i^{*}})(U{\\mathbf x}_{N+1})^{\\top}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=U\\mathbb{E}[g_{i^{*}}^{k}({\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1})\\cdot{\\mathbf x}_{i^{*}}{\\mathbf x}_{N+1}^{\\top}]U^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Again, since $U$ is an arbitrary orthogonal matrix, we conclude that $\\mathbb{E}[g_{i^{*}}^{k}(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\cdot\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top}]$ is a multiplication of the identity matrix. Summing everything together, we conclude the proof that $\\nabla_{\\mathbf{W}_{11}^{k}}\\bar{L}(\\mathbf{W}^{k})$ is a multiplication of an identity matrix. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "With Lemma 7, Eq. (2.6), and the previous calculations, we conclude our induction. ", "page_idx": 18}, {"type": "text", "text": "C.2Evolution of $\\xi_{1}$ and $\\xi_{2}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we characterize the training dynamics of $\\xi_{1}$ and $\\xi_{2}$ under gradient descent. Our results can be broken down into the following steps: ", "page_idx": 18}, {"type": "text", "text": "(1) We prove that when $\\xi_{1}^{0}$ and $\\xi_{2}^{0}$ are initialized as in Assumption 2 , with $\\sigma,N,d$ satisfy the condition in Theorem 1, we have $\\xi_{1}^{1}-\\xi_{1}^{0}\\geq0$   \n(2) We provide an uppe bound and lower bound for $\\xi_{1}^{k+1}-\\xi_{1}^{k}$ and $\\xi_{2}^{k+1}-\\xi_{2}^{k}$   \n(3)We prove that there exits constant $c_{1}\\in(0,1)$ such that $\\xi_{1}^{k}\\in(0,c_{1}\\xi_{2}^{k})$   \n(4) We prove that $\\xi_{2}^{k+1}-\\xi_{2}^{k}=\\Omega\\bigl(\\eta\\cdot\\mathrm{exp}(-\\mathrm{poly}(N,d)\\cdot\\xi_{2}^{k})\\bigr)$ , we thus conclude that $\\xi_{2}^{k}=$ $\\Omega(\\eta\\cdot\\log(\\mathrm{poly}(N,d))\\cdot\\log(k))$ ", "page_idx": 18}, {"type": "text", "text": "Combining (3) and (4), we conclude this section by proving that $\\xi_{1}^{k},\\xi_{2}^{k}$ and $\\xi_{2}^{k}-\\xi_{1}^{k}$ areboth of scale $\\Omega(\\log k)$ ", "page_idx": 18}, {"type": "text", "text": "Step (1): Initial incrementation of $\\xi_{1}^{0}$ . First, we prove that $\\xi_{1}^{1}-\\xi_{1}^{0}\\ge0$ . Note that by Lemma 7 and Eq. (C.10), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{d}{\\eta}(\\xi_{1}^{0}-\\xi_{1}^{1})=\\nabla_{\\mathbf{W}_{1},L}(\\mathbf{W}^{0})}}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{0})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\right]-\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{0})(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\bigg]}\\\\ &{\\qquad+\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{0})\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{0})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg)\\bigg]}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{0})\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{0})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg)\\bigg]}\\\\ &{\\leq-\\displaystyle\\frac{1}{N+1}\\mathbb{E}[\\mathbf{x}_{i^{*}}\\mathbf{x}_{N+1}^{\\top}]+\\frac{1}{(N+1)^{3}}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove that $\\xi_{1}^{0}-\\xi_{1}^{1}<0$ , we only need ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\frac1{N+1}\\mathbb{E}[{\\bf x}_{i^{*}}{\\bf x}_{N+1}^{\\top}]+\\frac1{(N+1)^{3}}\\mathbb{E}[{\\bf x}{\\bf x}^{\\top}]\\le0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, by Lemma 18, this can be guaranteed by $N\\geq O(\\sqrt{d}\\log d)$ ", "page_idx": 19}, {"type": "text", "text": "Step (2): Scale of $\\xi_{1}^{k+1}-\\xi_{1}^{k}$ and $\\xi_{2}^{k+1}-\\xi_{2}^{k}$ Note tha ey gradint uat witsti $\\eta$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})=\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\bigg]-\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg]}\\\\ &{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~-\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg)\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb E\\bigg[\\big\\{\\mathbf q_{i^{*}}(\\mathbf x,\\mathbf W)-\\displaystyle\\sum_{j=1}^{N}\\mathbf q_{j}^{2}(\\mathbf x,\\mathbf W)\\big\\}\\bigg\\{\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1}-\\displaystyle\\sum_{j=1}^{N+1}\\mathbf q_{j}(\\mathbf x,\\mathbf W)(\\mathbf x_{j}^{\\top}\\mathbf x_{N+1})\\bigg\\}\\bigg]}}\\\\ &{}&{+\\,\\mathbb E\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf q_{j}^{2}(\\mathbf x,\\mathbf W)\\big\\{\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1}-\\mathbf x_{j}^{\\top}\\mathbf x_{N+1}\\big\\}\\bigg]\\qquad\\qquad\\qquad(\\mathbf C.1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we also have the following estimation for the update of $\\xi_{3}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\eta(\\xi_{2}^{k+1}-\\xi_{2}^{k})=\\mathbb{E}[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\big\\}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\mathbb{E}\\big[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\cdot\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\cdot\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})\\big\\}\\big]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\cdot\\mathbf{q}_{N+1}^{2}(\\mathbf{x},\\mathbf{W}^{k})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which shows that in each iteration, $\\xi_{3}$ will at least increment by a scale of $\\eta\\cdot\\mathbb{E}[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})$ ${\\bf q}_{N+1}^{2}({\\bf x},{\\bf W}^{k})]$ . Combining Eq. (C.12), (C.11), we have the following estimation: ", "page_idx": 19}, {"type": "text", "text": "Lemma 8. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\cdot\\bigl\\{d(\\xi_{1}^{k+1}-\\xi_{1}^{k})+2(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\bigr\\}\\geq\\biggl(1-\\frac{1}{2^{N}}\\biggr)C_{d}\\exp(-6\\xi_{1}^{k})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $k\\geq0$ ", "page_idx": 19}, {"type": "text", "text": "Proof. First, note that we have the following relations: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\eta}\\cdot\\bigl\\{d(\\xi_{1}^{k+1}-\\xi_{1}^{k})+2(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\bigr\\}}\\\\ &{\\quad=\\mathbb{E}\\biggl[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\biggr]-\\mathbb{E}\\biggl[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\biggr]}\\\\ &{\\quad\\qquad\\qquad+\\mathbb{E}[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\bigl\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\bigr\\}]}\\\\ &{\\quad\\qquad\\qquad-\\,\\mathbb{E}\\biggl[\\bigl\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\bigr\\}\\biggr(\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\biggr)\\biggr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\bigg\\{(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}+1)-\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg\\}\\bigg]}\\\\ {\\displaystyle\\qquad\\qquad+\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\bigg]}\\\\ {\\displaystyle\\geq\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\cdot\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}+1)\\bigg]}\\\\ {\\displaystyle\\qquad\\qquad+\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "here the inequality comes from $\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\geq\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}$ for all $j\\in[N]$ . Moreover, note that when $\\xi_{1}\\geq0$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n1=\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})+\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W})\\leq N\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})+\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is equivalent to $\\begin{array}{r}{\\mathbf q_{i^{*}}(\\mathbf x,\\mathbf W)\\ge\\frac{1-\\mathbf q_{N+1}(\\mathbf x,\\mathbf W)}{N}}\\end{array}$ . Since ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})}{\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W})}=\\exp\\big(\\xi_{1}(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-1)+\\xi_{3}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\geq\\frac{1}{N+\\exp(\\xi_{1}(1-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})-\\xi_{3})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg[\\frac{N}{\\beta-1}q_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\bigg\\}\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\mathbf{q}_{i}^{2}\\big(\\mathbf{x},\\mathbf{W}^{\\top}\\big)\\displaystyle\\sum_{j=1}^{N}\\frac{\\mathbf{q}_{j}^{2}\\big(\\mathbf{x},\\mathbf{W}^{\\top}\\big)}{\\mathbf{q}_{j}^{2}\\big(\\mathbf{x},\\mathbf{W}^{\\top}\\big)}\\cdot\\big\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\bigg]}\\\\ &{\\ge\\mathbb{E}\\bigg[\\bigg(\\frac{1}{N+\\exp(\\xi_{1}(1-N\\tau_{k}^{\\top}\\mathbf{x}_{N+1})-\\xi_{3})}\\bigg)^{2}\\displaystyle\\sum_{j=1}^{N}\\exp\\left(2\\xi_{1}^{\\top}\\big(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\big)\\right)\\bigg\\}\\bigg\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}-}\\\\ &{\\ge\\frac{\\exp(-4\\xi_{1}^{\\top})}{(N+\\exp(\\xi_{1}^{\\top}-\\xi_{3}))^{2}}\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\big\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\big\\}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\ge0\\bigg]\\mathbb{P}\\big(\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\ge0\\big)}\\\\ &{\\ge\\Big(1-\\frac{1}{2^{N}}\\Big)\\left(N+\\exp(\\xi_{1}^{\\top}-\\xi_{2}^{\\top})\\right)^{2}}\\\\ &{\\ge\\Big(1-\\frac{1}{2^{N}}\\Big)C_{\\alpha}\\exp(-6\\xi_{1}^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "here $C_{d}$ is a constant that only pertains to $d$ . The first inequality comes from Eq. (C.16), and the third inequality comes from Lemma 17. Finally, note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\cdot\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W})\\big(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}+1\\big)\\bigg]\\geq0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we conclude the proof. ", "page_idx": 20}, {"type": "text", "text": "Next, we provide the following upper bound for the increment of $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ whenever $\\xi_{1}^{k}\\ge0$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 9. When $\\xi_{1}^{k}\\ge0$ and $N\\geq O(\\sqrt{d}\\log d)$ as in Theorem $^{\\,l}$ we have the folowing inequalities: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})\\leq\\frac{2N}{d}\\exp(-\\frac{4}{(N+1)^{2}}\\xi_{1}^{k})-\\frac{a_{n,d}}{d N^{3}e}\\exp(2(\\xi_{1}^{k}-\\xi_{2}^{k})),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $a_{n,d}=(2N{\\sqrt{d}})^{-{\\frac{2}{d-3}}}$ , and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\leq\\exp(2\\xi_{1}^{k}-\\xi_{2}^{k}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We first prove the first inequality. Recall that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})=\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\bigg\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})(\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1})\\bigg\\}\\bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "when $\\xi_{1}^{k}\\ge0$ , we have $\\begin{array}{r}{\\mathbf q_{i^{*}}(\\mathbf x,\\mathbf W)-\\sum_{j=1}^{N}\\mathbf q_{j}^{2}(\\mathbf x,\\mathbf W)\\geq0}\\end{array}$ , therefore ", "page_idx": 21}, {"type": "text", "text": "We have the following bound for (i) when $\\xi_{1}^{k}\\ge0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-(\\mathrm{iii})\\geq\\displaystyle\\frac{1}{e}\\frac{1}{\\left(2N\\sqrt{d}\\right)^{\\frac{2}{d-3}}}\\cdot\\mathbb{E}\\left[\\left\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\right\\}\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\middle|\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\geq1-a_{n,d}\\right]}\\\\ &{\\qquad\\geq\\displaystyle\\frac{1}{e}\\frac{1}{\\left(2N\\sqrt{d}\\right)^{\\frac{2}{d-3}}}\\cdot\\mathbb{E}\\left[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{q}_{N+1}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\right]}\\\\ &{\\qquad\\geq\\displaystyle\\frac{1}{e}\\frac{1}{\\left(2N\\sqrt{d}\\right)^{\\frac{2}{d-3}}}\\cdot\\frac{1}{N}\\cdot\\mathbb{E}[\\left(1-\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\right)\\mathbf{q}_{N+1}^{2}(\\mathbf{x},\\mathbf{W}^{k})]}\\\\ &{\\qquad\\geq\\displaystyle\\frac{1}{N^{3}}\\frac{1}{e}\\frac{1}{\\left(2N\\sqrt{d}\\right)^{\\frac{2}{d-3}}}\\exp(2(\\xi_{1}^{k}-\\xi_{2}^{k}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality comes from Lemma 19. Now we aim to bound (i) and (i) separately. First, wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{(i)}=\\mathbb{E}\\left[\\mathbf{q}_{i^{*}}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\sum_{j=1}^{N}\\frac{\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})}{\\mathbf{q}_{i^{*}}^{2}(\\mathbf{x},\\mathbf{W}^{k})}\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\right]}\\\\ {\\displaystyle\\ \\ \\ =\\mathbb{E}\\left[\\sum_{j=1}^{N}\\exp\\big(-2\\xi_{1}^{k}\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\big)\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq N\\exp\\Big(-2\\xi_{1}^{k}\\mathbb{E}[{\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1}-{\\mathbf x}_{j}^{\\top}{\\mathbf x}_{N+1}]\\Big)\\mathbb{E}[{\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1}-{\\mathbf x}_{j}^{\\top}{\\mathbf x}_{N+1}]}\\\\ &{=N\\exp(-2\\xi_{1}^{k}\\mathbb{E}[{\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1}])\\mathbb{E}[{\\mathbf x}_{i^{*}}^{\\top}{\\mathbf x}_{N+1}]}\\\\ &{\\leq N\\exp\\bigg(-\\frac{4}{(N+1)^{2}}\\xi_{1}^{k}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "here the first inequality comes from Jensen's inequality and the convexity of $x\\exp(-x)$ between $[0,2]$ and the second one comes frm $\\begin{array}{r}{\\mathbb{E}[{\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}]\\Big[\\geq\\frac{2}{(N+1)^{2}}}\\end{array}$ , wWhich is guaranteed by the condition of $N$ in Theorem 1 and Lemma 18. Thus we conclude the proof for the first inequality. For (i), denote the second largest order statistics in $\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\}_{i\\in[N]}$ by $\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}$ , we have the following inequalities: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(ii)}\\leq\\mathbb{E}\\bigg[\\big\\{\\mathbf{q}_{*}\\!\\cdot\\!(\\mathbf{x},\\mathbf{W})-\\mathbf{q}_{i^{*}}^{2}(\\mathbf{x},\\mathbf{W})\\big\\}\\bigg\\{\\sum_{j=1}^{N}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})\\big(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big)\\bigg\\}\\bigg]}\\\\ &{\\quad=\\mathbb{E}\\bigg[\\big(1-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\big)\\sum_{j=1}^{N}\\frac{\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})}{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})}\\big(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big)\\bigg]}\\\\ &{\\quad\\leq\\mathbb{E}\\bigg[\\operatorname*{min}\\{1,\\zeta_{k}\\}\\sum_{j=1}^{N}\\exp\\big(-\\xi_{1}^{k}\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\big)\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\bigg]}\\\\ &{\\quad\\leq N\\exp\\big(\\mathbb{E}[-\\xi_{1}^{k}\\big\\{\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}\\big\\}]\\big)\\mathbb{E}[\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}]}\\\\ &{\\quad\\leq N\\exp(-\\xi_{1}^{k}/2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we define a random variable $\\zeta_{k}$ by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\zeta_{k}=\\left(N-1\\right)\\exp\\left(\\xi_{1}^{k}\\cdot\\{\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\}\\right)+\\exp(\\xi_{1}^{k}\\cdot\\{1-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\}-\\xi_{2}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the second inequality comes from ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lnot-\\mathbf q_{i^{*}}(\\mathbf x,\\mathbf W^{k})\\leq1-\\frac{\\exp(\\xi_{1}^{k}\\cdot\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1})}{\\exp(\\xi_{1}^{k}-\\xi_{2}^{k})+(N-1)\\exp(\\xi_{1}^{k}\\cdot\\mathbf x_{(2)}^{\\top}\\mathbf x_{N+1})+\\exp(\\xi_{1}^{k}\\cdot\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1})}}\\\\ &{\\lphantom{\\sum}\\leq(N-1)\\exp\\left(\\xi_{1}^{k}\\cdot\\{\\mathbf x_{(2)}^{\\top}\\mathbf x_{N+1}-\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1}\\}\\right)+\\exp(\\xi_{1}^{k}\\cdot\\{1-\\mathbf x_{i^{*}}^{\\top}\\mathbf x_{N+1}\\}-\\xi_{2}^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the third inequality comes from Jensen's inequality. Combining We now aim to prove the second inequality. Recall that by Eq. (C.11), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})=\\mathbb{E}[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\big\\}],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "therefore we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\leq\\mathbb{E}[\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})\\big\\{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})-\\mathbf{q}_{i^{*}}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\big\\}]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\biggl[\\operatorname*{min}\\{1,\\zeta_{k}\\}\\frac{\\mathbf{q}_{N+1}(\\mathbf{x},\\mathbf{W}^{k})}{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})}\\biggr]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\biggl[\\frac{1}{1+\\exp\\big((\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}-1)\\xi_{1}^{k}+\\xi_{2}^{k}\\big)}\\biggr]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[\\exp\\big((1-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})\\xi_{1}^{k}-\\xi_{2}^{k}\\big)]}\\\\ &{\\qquad\\qquad\\leq\\exp(2\\xi_{1}^{k}-\\xi_{2}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus we conclude the proof of both inequalities ", "page_idx": 22}, {"type": "text", "text": "Lemma 10. When $\\xi_{1}^{k}\\ge0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d}{\\eta}(\\xi_{1}^{k+1}-\\xi_{1}^{k})\\geq\\bigg(1-\\frac{1}{2^{N}}\\bigg)C_{d}\\exp(-6\\xi_{1}^{k})-2\\exp(2\\xi_{1}^{k}-2\\xi_{2}^{k})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The next lemma provides a lower bound for the update of $\\xi_{3}$ . Notably, when $\\xi_{1}^{k}$ is close to $\\xi_{2}^{k},\\xi_{2}^{k}$ increases in a larger scale. ", "page_idx": 23}, {"type": "text", "text": "Lemma 11. When $\\xi_{1}^{k}\\ge0$ we have the following inequality holds. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\geq\\frac{1}{(N+1)^{3}e}\\exp{\\left(2a_{n,d}\\cdot\\xi_{1}^{k}-2\\xi_{2}^{k}\\right)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $a_{n,d}$ is a constant only pertains to n and $d$ that is defined inLemma9 ", "page_idx": 23}, {"type": "text", "text": "Proof. With Eq. (C.11), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\eta}(\\xi_{2}^{k+1}-\\xi_{2}^{k})\\geq\\mathbb{E}\\big[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W}^{k})\\cdot\\mathbf{q}_{N+1}^{2}(\\mathbf{x},\\mathbf{W}^{k})\\big]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}^{3}(\\mathbf{x},\\mathbf{W}^{k})\\bigg(\\frac{\\mathbf{q}_{N+1}^{2}(\\mathbf{x},\\mathbf{W}^{k})}{\\mathbf{q}_{i^{*}}^{2}(\\mathbf{x},\\mathbf{W}^{k})}\\bigg)\\bigg]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}^{3}(\\mathbf{x},\\mathbf{W}^{k})\\exp\\big(2\\xi_{1}^{k}(1-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})-2\\xi_{2}^{k}\\big)\\bigg]}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{e}\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}^{3}(\\mathbf{x},\\mathbf{W}^{k})\\exp\\big(2a_{n,d}\\cdot\\xi_{1}^{k}-2\\xi_{2}^{k}\\big)\\big|\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\geq a_{n,d}\\bigg]}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{(N+1)^{3}e}\\exp\\big(2a_{n,d}\\cdot\\xi_{1}^{k}-2\\xi_{2}^{k}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality comes from Eq. (C.14), second inequality comes from Lemma 19, the third inequalitycomes from $\\dot{\\xi}_{1}^{k}\\ge0$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Step (3): Upper Bound for $\\xi_{1}^{k}/\\xi_{2}^{k}$ . Now, combining Lemma 9 and 11, we immediately get the following result: ", "page_idx": 23}, {"type": "text", "text": "Lemma 12. If $\\begin{array}{r}{\\sigma=\\xi_{2}^{0}\\geq3\\log\\(\\frac{a_{n,d}}{2N^{4}d})=O\\big(\\operatorname*{max}\\{\\log(N d),-\\log\\big(1-(N\\sqrt{d})^{\\frac{1}{d}}\\big)\\}\\big)}\\end{array}$ and $\\xi_{1}^{k}\\ge0$ for all $k\\geq0$ then we willhave $\\xi_{1}^{k}\\le\\frac{7}{15}\\xi_{2}^{k}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemma 9, we know that whenever $\\xi_{1}^{k}\\ge0$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\big(\\xi_{1}^{k+1}-\\xi_{1}^{k}\\big)\\leq\\frac{2N}{d}\\exp(-\\xi_{1}^{k}/2)-\\frac{a_{n,d}}{d N^{3}e}\\exp(2(\\xi_{1}^{k}-\\xi_{2}^{k})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "therefore, if ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{2N}{d}\\exp(-\\xi_{1}^{k}/2)<\\frac{a_{n,d}}{d N^{3}e}\\exp(2(\\xi_{1}^{k}-\\xi_{2}^{k})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is equivalent to ", "page_idx": 23}, {"type": "equation", "text": "$$\n2.5\\xi_{1}^{k}>\\log(\\frac{a_{n,d}}{2N^{4}d})+\\xi_{2}^{k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we wilhave $\\xi_{1}^{k+1}-\\xi_{1}^{k}<0$ By Lemma 1, we ave $\\begin{array}{r}{\\xi_{2}^{k}\\ge\\xi_{2}^{0}\\ge3\\log(\\frac{a_{n,d}}{2N^{4}d})}\\end{array}$ Therefore k+1 will not increase as long as ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{15}{7}}\\xi_{1}^{k}\\geq\\xi_{2}^{k},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "finally, recall that ", "page_idx": 23}, {"type": "equation", "text": "$$\na_{n.d}=1-\\frac{1}{\\left(2N k_{d}\\right)^{\\frac{2}{d-3}}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and our result follows. ", "page_idx": 23}, {"type": "text", "text": "Step (4): Scale of $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ .Finally, we conclude the convergence of gradient descent with the following two Lemma: ", "page_idx": 24}, {"type": "text", "text": "Lemma 13. With $\\sigma$ and $N$ satisfying the condition in Theorem $^{\\,I}$ ,we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{1}^{k},\\xi_{2}^{k}=\\Omega(\\eta\\cdot\\mathrm{poly}(N,d)\\cdot\\log k),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $\\xi_{1}^{k}\\le\\frac{1}{2}\\xi_{2}^{k}$ holds for all $k\\geq0$ ", "page_idx": 24}, {"type": "text", "text": "Proof. We first establish a convergence rate for $\\xi_{2}$ . Note that by Lemma 9, Lemma 11 and Lemma 12, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{2}^{k+1}-\\xi_{2}^{k}\\leq\\eta\\cdot\\exp(-\\frac{1}{15}\\xi_{2}^{k}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{2}^{k+1}-\\xi_{2}^{k}\\ge\\eta\\cdot\\frac{1}{(N+1^{3}e)}\\exp(-2\\xi_{2}^{k})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "holds for all $k\\geq0$ . Therefore, we have $\\xi_{2}^{k}=\\Omega(\\eta\\cdot\\mathrm{poly}(N,d)\\cdot\\log k)$ for all $k\\geq0$ . Now we turn to $\\xi_{1}^{k}$ . By Lemma 11, when ", "page_idx": 24}, {"type": "equation", "text": "$$\n8\\xi_{1}^{k}\\leq\\xi_{2}^{k}-\\log{\\left(\\frac{C_{d}\\big(1-\\frac{1}{2^{N}}\\big)}{4}\\right)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{1}^{k+1}-\\xi_{1}^{k}\\geq\\frac{\\eta}{d}\\biggl(1-\\frac{1}{2^{N}}\\biggr)C_{d}\\exp(-6\\xi_{1}^{k})\\geq0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{\\xi_{2}^{0}=\\sigma\\geq2\\log\\left(\\frac{C_{d}\\left(1-\\frac{1}{2^{N}}\\right)}{4}\\right)}\\end{array}$ $\\xi_{2}^{k}$ monotonically increasing, and $\\xi_{1}^{1}\\ge0$ , by induction, we have $\\xi_{1}^{k}\\ge0$ for all $k$ , and $\\xi_{1}^{k}\\ge O(\\mathrm{poly}(N,d)\\log k)$ until $\\begin{array}{r}{\\xi_{1}^{k}\\ge\\frac{\\xi_{2}^{k}}{8}}\\end{array}$ . Meanwhile, Lemma 12 shows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{1}^{k+1}-\\xi_{1}^{k}\\leq\\frac{2N}{d}\\exp(-\\xi_{1}^{k}/2)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $k\\geq0$ , which implies $\\xi_{1}^{k}=O(\\mathrm{poly}(N,d)\\log k)$ . Therefore, $\\xi_{1}^{k}=\\Omega(\\mathrm{poly}(N,d)\\log k)$ . The results of $\\xi_{1}^{k}\\le\\frac{1}{2}\\xi_{2}^{k}$ follows by Lemma 12 and $\\xi_{1}^{k}\\ge0$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.3 Convergece of Loss Function $L(\\mathbf{W})$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We also have the following bound for the loss function. ", "page_idx": 24}, {"type": "text", "text": "Lemma 14. When W defined in Eq. (2.4) satisfies $\\mathbf{W}_{11}=\\xi_{1}I_{d}$ $\\mathbf{W}_{33}=-\\xi_{2}$ with $\\xi_{1}\\geq0$ andthe rest of items are all zero matrices, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\Bigg]\\leq O\\bigg(\\frac{N^{3}k_{d}^{2}}{\\xi_{1}}\\bigg)+\\exp(2\\xi_{1}-\\xi_{3})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\bigg[\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]=1+\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})\\bigg]-2\\mathbb{E}\\big[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\big]}}\\\\ &{}&{\\qquad=\\mathbb{E}[1-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})]+\\mathbb{E}\\bigg[\\displaystyle\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\bigg[\\sum_{j=1}^{N}\\mathbf{q}_{j}^{2}(\\mathbf{x},\\mathbf{W})-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\bigg]\\leq\\mathbb{E}[\\sum_{j=1}^{N}\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})]}\\\\ {\\displaystyle\\leq\\mathbb{E}\\big[-\\mathbf{q}_{N+1}(\\mathbf{W},\\mathbf{x})\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{z}\\big[1-\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\big]\\leq\\mathbb{E}\\Big[\\underset{j\\neq i^{*},j\\in[N+1]}{\\sum_{j\\in[N+\\beta]}}\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\bigg[\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})\\underset{j\\neq i^{*},j\\in[N+1]}{\\sum_{j\\in[N+1]}}\\frac{\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W})}{\\mathbf{q}_{i^{*}}(\\mathbf{x},\\mathbf{W})}\\bigg]}\\\\ &{\\qquad\\qquad\\leq(N-1)\\mathbb{E}\\bigg[\\exp\\big(\\xi_{1}\\cdot\\big\\{\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\big)\\bigg]+\\mathbb{E}\\big[\\exp(\\xi_{1}(1-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})-\\xi_{i^{*}}\\mathbf{x}_{N+1})\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathbf{q}_{i^{*}}\\leq1)}\\\\ &{\\qquad\\qquad\\qquad\\leq(N-1)\\mathbb{E}\\bigg[\\exp\\big(\\xi_{1}\\cdot\\big\\{\\mathbf{x}_{j}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\big\\}\\big)\\bigg]+\\exp(2\\xi_{1}-\\xi_{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 20, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]\\leq O\\bigg(\\frac{N^{3}k_{d}^{2}}{\\xi_{1}}\\bigg)+\\exp(2\\xi_{1}-\\xi_{3}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and we conclude the proof. ", "page_idx": 25}, {"type": "text", "text": "Now, combine Lemma 15 with our dynamic bounds for $\\xi_{1}^{k}$ and $\\xi_{2}^{k}$ developed in Section C.2, we have the following convergence result for the loss function. ", "page_idx": 25}, {"type": "text", "text": "Lemma 15. When $\\mathbf{W}^{K}$ defined in Eq. (2.4) is updated by gradient descent in Eq. (2.6), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]\\leq O\\bigg(\\frac{\\mathrm{poly}(N,d)}{\\log K}\\bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. By Lemma 15, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\bigg(\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]\\leq O\\bigg(\\frac{N^{3}k_{d}^{2}}{\\xi_{1}^{k}}\\bigg)+\\exp(2\\xi_{1}^{k}-\\xi_{3}^{k}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 13, we have $\\xi_{1}^{k}\\le\\frac{7}{15}\\xi_{2}^{k}$ , with $\\xi_{1}^{k}=\\Omega(\\mathrm{poly}(N,d)\\log k)$ . Thus we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg(\\displaystyle\\sum_{j=1}^{N+1}\\mathbf{q}(\\mathbf{x},\\mathbf{W}^{k})\\mathbf{y}_{j}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]\\leq O\\bigg(\\displaystyle\\frac{N^{3}k_{d}^{2}}{\\xi_{1}^{k}}\\bigg)+\\exp(-\\displaystyle\\frac{1}{15}\\xi_{3}^{k})}\\\\ &{\\phantom{=\\quad}=\\displaystyle\\frac{\\mathrm{poly}(N,d)}{\\log k}+\\displaystyle\\frac{1}{k^{1/15}}}\\\\ &{\\phantom{=\\quad}=\\displaystyle\\frac{\\mathrm{poly}(N,d)}{\\log k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D  Proof for Theorem 2 and Corollary 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we discuss the behavior of the pretrained transformer in tasks under distribution shift, and provide proof for Theorem 2 and Corollary 1 in Section 3.2. ", "page_idx": 25}, {"type": "text", "text": "D.1 Proof for Theorem 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide a proof for Theorem 2. The result comes from the following observation. First, we condition our analysis on the $A_{\\delta}$ , i.e. assuming that there exists a constant $\\delta$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{\\bf x}_{j}-{\\bf x}_{N+1}\\|_{2}^{2}\\geq\\delta+\\|{\\bf x}_{i^{*}}-{\\bf x}_{N+1}\\|_{2}^{2},\\qquad\\forall j\\not=i^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{y}(\\mathbf{W}^{k})-y_{*}|=R\\displaystyle\\left|\\sum_{j=1}^{N}\\mathbf{q}_{1}(\\mathbf{x},\\mathbf{W}^{k})y_{j}-y_{*}\\right|}\\\\ &{\\phantom{=}\\displaystyle\\left|\\sum_{j\\in[N,1]\\times\\mathcal{Y}_{j}\\cap\\mathcal{X}_{k}}\\mathbf{q}_{1}(\\mathbf{x},\\mathbf{W}^{k})(y_{j}-y_{*})\\right|+R(\\mathbf{q}_{1+1}(\\mathbf{x},\\mathbf{W}^{k})|)}\\\\ &{\\phantom{=}-2R\\,\\varepsilon\\|\\nabla\\!\\!\\!\\sum_{j=1}^{N}\\mathbf_{\\beta_{j}}\\!\\!\\!\\!\\!q_{1}(\\mathbf{x},\\mathbf{W}^{k})+R\\cdot\\mathbf{q}_{1}\\times(\\mathbf{N},\\mathbf{W}^{k})}\\\\ &{\\phantom{=}-2R\\,\\varepsilon\\|\\nabla\\!\\!\\!\\sum_{j=1}^{N}\\mathbf_{\\beta_{j}}\\!\\!\\!\\!\\!q_{1}(\\mathbf{x},\\mathbf{W}^{k})+R\\cdot\\mathbf{q}_{1+1}(\\mathbf{x},\\mathbf{W}^{k})}\\\\ &{\\leq2R\\,\\varepsilon\\|\\nabla\\!\\!\\!\\sum_{j\\in[N,1]\\times\\mathcal{Y}_{j}}\\mathbf{q}_{1}(\\mathbf{x},\\mathbf{W}^{k})+R\\cdot\\mathbf{q}_{1}\\times(\\mathbf{N},\\mathbf{W}^{k})}\\\\ &{\\phantom{=}-2R\\,\\varepsilon\\|\\nabla\\!\\!\\!\\sum_{j\\in[N,1]\\times\\mathcal{Y}_{j}}\\mathbf{q}_{1}(\\mathbf{x},\\mathbf{W}^{k})+R\\cdot\\mathbf{q}_{1}(\\mathbf{x},\\mathbf{W}^{k})}\\\\ &{=2R\\,\\varepsilon\\|\\nabla\\!\\!\\!\\sum_{j\\in[N,1]\\times\\mathcal{Y}_{j}}\\mathbf{q}_{1}(\\hat{\\mathbf{x}}^{k}\\times(\\mathbf{x}^{k}\\times(\\mathbf{x}^{k}\\times\\mathbf{x}_{N+1})+R\\cdot\\mathbf{p}\\times(\\hat{\\mathbf{k}}^{k}(1-\\mathbf{x}_{N+1})-\\mathbf{x}^{k}))}\\\\ &{\\leq2R\\cdot R\\cdot\\mathbf{p}(\\hat{\\mathbf{x}}^{k}\\cdot\\mathbf{A})+R\\cdot\\mathbf{exp}\\left(-\\frac{1}{15}\\mathbf{k}^{\\frac{1}{5}}\\right)}\\\\ &{=2R\\cdot\\mathbf{p}(-p\\\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i\\in[N]}$ ${\\bf x}_{N+1}$ $A_{\\delta}$ $\\xi_{1}^{K}\\le\\frac{7}{15}\\xi_{2}^{K}$ $\\mathbf{q}_{j}(\\mathbf{x},\\mathbf{W}^{k})$ for the $j$ -th label in $\\{\\mathbf{y}_{j}\\}_{j\\in[N+1]}$ Next, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{P}^{\\mathrm{esu}}}\\left[\\left(\\widehat{\\mathbf{y}}(\\mathbf{W}^{k})-\\mathbf{y}_{i^{*}}\\right)^{2}\\right]=\\mathbb{E}_{\\mathbb{P}^{\\mathrm{esu}}}\\left[\\left(\\widehat{\\mathbf{y}}(\\mathbf{W}^{k})-\\mathbf{y}_{i^{*}}\\right)^{2}\\mathbb{1}_{A_{\\delta}}\\right]+\\mathbb{E}_{\\mathbb{P}^{\\mathrm{esu}}}\\left[\\left(\\widehat{\\mathbf{y}}(\\mathbf{W}^{k})-\\mathbf{y}_{i^{*}}\\right)^{2}\\mathbb{1}_{A_{\\delta}^{c}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq O\\bigg(R^{2}N^{2}K^{-\\mathrm{poly}(N,d)\\delta}\\bigg)+4R^{2}\\cdot\\mathbb{P}^{\\mathrm{test}}(A_{\\delta}^{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the inequality comes from Eq. (D.2). Here the expectation is taken over the testing distribution $\\mathbb{P}^{\\mathrm{{test}}}$ . By taking inferior on the inequality above for all $\\delta>0$ , we conclude our proof for Theorem 2. ", "page_idx": 26}, {"type": "text", "text": "D.2  Proof for Corollary 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide the proof for Corollary 1. The first statement of Corollary 1 comes from Markov's inequality: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\mathrm{test}}\\big(\\mathrm{Round}\\left(\\hat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})\\right)\\neq\\mathbf{y}_{i^{*}}\\big)=\\mathbb{P}^{\\mathrm{test}}\\bigg(\\vert\\hat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})-\\mathbf{y}_{i^{*}}\\vert\\geq\\frac{1}{2}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4\\cdot\\mathbb{E}[\\vert\\hat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})-\\mathbf{y}_{i^{*}}\\vert^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O\\big(M N\\cdot\\underset{\\delta\\geq0}{\\operatorname*{inf}}\\left\\{K^{-\\mathrm{poly}(N,d)\\delta}+\\mathbb{P}^{\\mathrm{test}}(A_{\\delta}^{c})\\right\\}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality comes from Theorem 2. Next, we prove the second statement. Suppose $\\mathbb{P}^{\\mathrm{test}}(A_{\\delta^{*}})=1$ for some $\\delta^{*}>0$ . Then similar to the argument in Eq. D.2), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\mathbf{y}}(\\mathbf{W}^{k})-\\mathbf{y}_{i^{*}}\\right|\\leq O\\bigg(M N K^{-\\mathrm{poly}(N,d)\\delta^{*}}\\bigg)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "holds for all $\\left\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\right\\}\\cup\\left\\{\\mathbf{x}_{N+1}\\right\\}\\sim\\mathbb{P}^{\\mathrm{test}}$ almost surely. Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Round}\\left(\\widehat{\\mathbf{y}}_{\\mathbf{W}}(\\mathbf{x}_{N+1})\\right)=\\mathbf{y}_{i^{*}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "holds whenever ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\widehat{\\mathbf{y}}(\\mathbf{W}^{k})-\\mathbf{y}_{i^{*}}|<\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "therefore,i suffces to have $\\textstyle{\\frac{1}{2}}\\leq O{\\bigl(}M N K^{-\\mathrm{poly}(N,d)\\delta^{*}}{\\bigr)}$ , which is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\nK=O\\bigg(\\frac{\\log(M N)}{\\operatorname{poly}(N,d)\\delta^{*}}\\bigg).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E  Nonconvexity of Loss Function ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we show that the loss function is defined by Eq. (2.5). We prove by a special subspace $\\mathbf{W}\\in\\mathbb{R}^{(d+2)\\times(d+2)}$ defined bytwo scalars $\\xi_{1},\\xi_{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\mathrm{diag}\\{\\underbrace{\\xi_{1},\\dots,\\xi_{1}}_{d\\;\\mathrm{times}},0,\\xi_{2}\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By showing $L(\\mathbf{W})$ is nonconvex under such parametrization, we conclude our proof. ", "page_idx": 27}, {"type": "text", "text": "Lemma 16 (Nonconvexity of Transformer Optimization). When W is a two-dimensional subspace $\\mathbb{R}^{(d+2)\\times(\\dot{d}+2)}$ defnedby Eq.E.1,the orginal losfinctiondefnedin Eq 2.5 derates to thefollowing: ", "page_idx": 27}, {"type": "equation", "text": "$$\nL(\\xi_{1},\\xi_{2}):=\\mathbb{E}\\bigg[\\bigg(\\frac{\\sum_{j=1}^{N}\\exp(\\xi_{1}\\langle\\mathbf{x}_{j},\\mathbf{x}_{N+1}\\rangle)\\mathbf{y}_{j}}{\\sum_{i=1}^{N}\\exp(\\xi_{1}\\langle\\mathbf{x}_{i},\\mathbf{x}_{N+1}\\rangle)+\\exp(\\xi_{1}-\\xi_{2})}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use $\\mathbf{y}_{N+1}=0$ .Such loss function is still, in general, nonconvex. ", "page_idx": 27}, {"type": "text", "text": "Proof. The degeneracy of the original Eq. (2.5) to $L(\\xi_{1},\\xi_{2})$ can be shown by basic algebra. We only need to show the nonconvexity of $L(\\xi_{1},\\xi_{2})$ in our proof. Note that by Assumption 1, the gradient of $L(\\xi_{1},\\xi_{2})$ is defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{\\xi_{2}}L(0,\\xi_{2})=\\partial_{\\xi_{2}}\\mathbb{E}\\bigg[\\bigg(\\cfrac{\\sum_{j=1}^{N}\\mathbf{y}_{j}}{N+\\exp(-\\xi_{2})}-\\mathbf{y}_{i^{*}}\\bigg)^{2}\\bigg]}\\\\ &{\\phantom{=}=\\partial_{\\xi_{2}}\\mathbb{E}\\bigg[1-2\\cfrac{\\sum_{j=1}^{N}\\mathbf{y}_{j}\\mathbf{y}_{i^{*}}}{N+\\exp(-\\xi_{2})}+\\cfrac{(\\sum_{j=1}^{N}\\mathbf{y}_{j})^{2}}{(N+\\exp(-\\xi_{2}))^{2}}\\bigg]}\\\\ &{\\phantom{=}=\\partial_{\\xi_{2}}\\bigg\\{\\cfrac{N}{\\big(N+\\exp(-\\xi_{2})\\big)^{2}}-\\cfrac{2}{N+\\exp(-\\xi_{2})}\\bigg\\}}\\\\ &{\\phantom{=}=\\cfrac{-\\exp(-2\\xi_{2})}{\\big(N+\\exp(-\\xi_{2})\\big)^{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the third equation comes from ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\left[\\sum_{j=1}^{N}\\mathbf{y}_{j}\\mathbf{y}_{i^{*}}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{j=1}^{N}\\mathbf{y}_{j}\\mathbf{y}_{i^{*}}\\Bigg|\\{\\mathbf{x}_{i}\\}_{i\\in[N]}\\right]\\right]}\\quad}&{}\\\\ &{=\\mathbb{E}\\left[\\sum_{j=1}^{N}\\mathbf{1}_{j=i^{*}}\\mathbb{E}\\left[\\mathbf{y}_{j}\\mathbf{y}_{i^{*}}\\Bigg|\\{\\mathbf{x}_{i}\\}_{i\\in[N]}\\right]\\right]}\\\\ &{=\\mathbb{E}\\left[\\displaystyle\\sum_{j=1}^{N}\\mathbb{1}_{j=i^{*}}\\mathbb{E}\\left[\\mathbf{y}_{i^{*}}\\Bigg|\\{\\mathbf{x}_{i}\\}_{i\\in[N]}\\right]\\right]}\\\\ &{=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and $\\begin{array}{r}{\\mathbb{E}[(\\sum_{j=1}^{N}{\\bf y}_{i})^{2}]=\\mathbb{E}[\\sum_{j=1}^{N}{\\bf y}_{i}^{2}]=N}\\end{array}$ Eq. E.2 shows that when $\\begin{array}{r}{\\operatorname*{lim}_{\\xi_{2}\\to+\\infty}\\partial_{\\xi_{2}}L(0,\\xi_{2})=0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{\\xi_{2}\\rightarrow-\\infty}\\partial_{\\xi_{2}}L(0,\\xi_{2})=0.}\\end{array}$ If $L(\\xi_{1},\\xi_{2})$ is convex, then $\\partial_{\\xi_{2}}L(0,\\xi_{2})$ is monotonically increasing, which means $\\partial_{\\xi_{2}}L(0,\\xi_{2})=0$ for all $\\xi_{2}$ . However, this is clearly a contradiction. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "F Auxiliary Lemma ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma 17 (Distribution of Sphere Inner Product). With the assumption of $\\mathbf{x}_{i}$ sampled froma uniform distribution on the sphere, Let $\\tau$ be the cosine of the angle between an arbitrary $d$ -dimensional vector and a vector chosen uniformly at random from the unit sphere. Then the probability density functon of random variable T E [-1,1 is f(t)=(-) $\\textstyle f_{\\tau}(t)={\\frac{2\\Gamma({\\frac{d}{2}})}{{\\sqrt{\\pi}}\\Gamma({\\frac{d-1}{2}})}}\\cdot(1-t^{2})^{\\frac{d-3}{2}}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. For convenience, we assume that ${\\bf x}_{N+1}={\\bf e}_{1}$ . Note that this does not change the distribution of $\\mathbf{x}_{N+1}\\cdot\\mathbf{x}_{i}$ due to rotation invariance. Let $X_{i}\\sim{\\cal N}(0,1)$ ,define ", "page_idx": 28}, {"type": "equation", "text": "$$\nY_{1}=X_{1},\\dots Y_{d-1}=X_{d-1},Y_{d}={\\frac{X_{d}}{\\sqrt{\\sum_{i=1}^{d}X_{i}^{2}}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that $Y$ is distributed the same way as $\\mathbf{x}_{i}$ . Calculating the Jacobian: ", "page_idx": 28}, {"type": "equation", "text": "$$\nJ=\\left[\\begin{array}{c c c c c}{1}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{1}&&{\\vdots}&{\\vdots}\\\\ {\\vdots}&&{\\ddots}&{0}&{0}\\\\ {0}&{\\cdots}&{0}&{1}&{0}\\\\ {-\\frac{X_{1}X_{d}}{\\left[\\sum_{i=1}^{d}X_{i}^{2}\\right]^{3/2}}}&{\\cdots}&&{-\\frac{X_{1}X_{d-1}}{\\left[\\sum_{i=1}^{N}X_{i}^{2}\\right]^{3/2}}}&{\\frac{\\sum_{i=1}^{d}X_{i}^{2}}{\\left[\\sum_{i=1}^{d}X_{i}^{2}\\right]^{3/2}}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $J$ is of the form: ", "page_idx": 28}, {"type": "equation", "text": "$$\nJ=\\left[\\begin{array}{l l}{\\mathbf{I}}&{0}\\\\ {\\mathbf{a}}&{b}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the determinant is easily evaluated: ", "page_idx": 28}, {"type": "equation", "text": "$$\n|J|=\\frac{\\sum_{i=1}^{d}X_{i}^{2}}{\\left[\\sum_{i=1}^{d}X_{i}^{2}\\right]^{3/2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now to solve for the distribution of $Y_{N}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{\\tau}(y)=\\int_{-\\infty}^{\\infty}\\cdot\\cdot\\cdot\\int_{-\\infty}^{\\infty}\\frac{1}{|J|}f_{x}(\\mathbf{x})d\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\nf_{x}({\\bf x})=\\prod_{I=1}^{d}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x_{i}^{2}}{2}}=\\frac{1}{(2\\pi)^{d/2}}e^{-\\frac{1}{2}\\sum_{i=1}^{d}x_{i}^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Writing the distribution in terms of $t$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{\\tau}\\left(t\\right)=\\int_{-\\infty}^{\\infty}\\cdots\\int_{-\\infty}^{\\infty}\\frac{\\left[\\sum_{i=1}^{d-1}y_{i}^{2}+\\frac{Y_{d}^{2}}{1-t^{2}}\\sum_{i=1}^{d-1}y_{i}^{2}\\right]^{3/2}}{\\sum_{i=1}^{N-1}y_{i}^{2}}\\frac{1}{\\left(2\\pi\\right)^{d/2}}e^{-\\frac{1}{2}\\frac{1}{1-t^{2}}\\sum_{i=1}^{d-1}y_{i}^{2}}\\mathrm{d}y_{1}\\cdots\\mathrm{d}y_{d-1}}}\\\\ {{\\displaystyle f_{\\tau}\\left(t\\right)=\\int_{-\\infty}^{\\infty}\\cdots\\int_{-\\infty}^{\\infty}\\frac{1}{\\left(1-t^{2}\\right)^{3/2}}\\sqrt{\\frac{t-1}{i=1}}y_{i}^{2}\\frac{1}{\\left(2\\pi\\right)^{d/2}}e^{-\\frac{1}{2}\\frac{1}{1-t^{2}}\\sum_{i=1}^{d-1}y_{i}^{2}}\\mathrm{d}y_{1}\\cdots\\mathrm{d}y_{d-1}}}\\\\ {{\\displaystyle f_{\\tau}\\left(t\\right)=\\frac{1}{\\left(1-t^{2}\\right)^{3/2}}\\frac{1}{\\left(2\\pi\\right)^{N/2}}\\int_{-\\infty}^{\\infty}\\cdots\\int_{-\\infty}^{\\infty}\\sqrt{\\frac{t-1}{i=1}}y_{i}^{2}e^{-\\frac{1}{2}}\\frac{1}{1-t^{2}}\\sum_{i=1}^{d-1}y_{i}^{2}\\mathrm{d}y_{1}\\cdots\\mathrm{d}y_{d-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we can make a substitution to remove $y_{d}$ from inside the integral. Let $u_{n}=\\left(1-t^{2}\\right)^{-1/2}y_{n}$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\tau}\\left(t\\right)=\\frac{1}{\\left(1-t^{2}\\right)^{3/2}}\\frac{1}{\\left(2\\pi\\right)^{d/2}}\\left(1-t^{2}\\right)^{d/2}\\int_{-\\infty}^{\\infty}\\cdot\\cdot\\cdot\\int_{-\\infty}^{\\infty}\\sqrt{\\sum_{i=1}^{d-1}u_{i}^{2}e^{-\\frac{1}{2}}\\sum_{i=1}^{d-1}u_{i}^{2}}d u_{1}\\cdot\\cdot\\cdot d u_{d-1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\tau}\\left(t\\right)=\\left(1-t^{2}\\right)^{\\left(d-3\\right)/2}(2\\pi)^{-d/2}\\int_{-\\infty}^{\\infty}\\cdot\\cdot\\cdot\\int_{-\\infty}^{\\infty}\\sqrt{\\sum_{i=1}^{d-1}u_{i}^{2}e^{-\\frac{1}{2}\\sum_{i=1}^{d-1}u_{i}^{2}}d u_{1}\\cdot\\cdot\\cdot d u_{d-1}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The integral can be seen to be a constant so: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\tau}\\left(t\\right)=k_{d}\\left(1-t^{2}\\right)^{\\frac{d-3}{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "or for notational convenience: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\tau}(t)=k_{d}\\left(1-t^{2}\\right)^{\\frac{d-3}{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $f_{\\tau}({\\boldsymbol{y}})$ is a PDF and is defined for $0\\le t\\le1$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\int_{0}^{1}k_{d}\\left(1-t^{2}\\right)^{\\frac{d-3}{2}}d t=1,\\mathrm{where~}k_{d}=\\frac{1}{\\int_{0}^{1}\\left(1-y^{2}\\right)^{\\frac{d-3}{2}}d y}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore,frd >1, we have ka = r() ", "page_idx": 29}, {"type": "text", "text": "Lemma 18. With N = O(log d Vd), we have E[xxN+1] \u2265 (V21)2. ", "page_idx": 29}, {"type": "text", "text": "Proof. Note that we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}]\\ge\\mathbb{P}\\big(\\operatorname*{max}_{i\\in[N]}{\\bf x}_{i}^{\\top}{\\bf x}_{N+1}\\ge\\alpha\\big)\\cdot\\alpha+\\left\\{1-\\mathbb{P}\\big(\\operatorname*{max}_{i\\in[N]}{\\bf x}_{i}^{\\top}{\\bf x}_{N+1}\\ge\\alpha\\big)\\right\\}\\cdot(-1).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To ensure that $\\begin{array}{r}{\\mathbb{E}[{\\mathbf{x}}_{i^{*}}^{\\top}{\\mathbf{x}}_{N+1}]\\geq\\frac{2}{(N+1)^{2}}}\\end{array}$ , we only need ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\alpha\\mathbb{P}\\big(\\operatorname*{max}_{i\\in[N]}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\ge\\alpha\\big)-\\left\\{1-\\mathbb{P}\\big(\\operatorname*{max}_{i\\in[N]}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\ge\\alpha\\big)\\right\\}\\ge\\frac{2}{(N+1)^{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is equivalent to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb P\\big(\\operatorname*{max}_{i\\in[N]}\\mathbf x_{i}^{\\top}\\mathbf x_{N+1}\\geq\\alpha\\big)\\geq\\frac{1}{1+\\alpha}\\bigg(1+\\frac{2}{(N+1)^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma 17, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\operatorname*{max}_{i\\in[N]}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\ge\\alpha\\big)=1-\\Bigg(k_{d}\\int_{-1}^{\\alpha}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t\\Bigg)^{N}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we only need ", "page_idx": 30}, {"type": "equation", "text": "$$\n1-\\left(k_{d}\\int_{-1}^{\\alpha}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t\\right)^{N}=\\mathbb P\\big(\\operatorname*{max}_{i\\in[N]}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\geq\\alpha\\big)\\geq\\left(1+\\frac{2}{(N+1)^{2}}\\right)\\cdot\\frac{1}{1+\\alpha},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which suffices by ", "page_idx": 30}, {"type": "equation", "text": "$$\nN\\geq\\frac{\\log\\left(1-(1+\\frac{2}{(N+1)^{2}})\\cdot\\frac{1}{1+\\alpha}\\right)}{\\log(1-k_{d}\\int_{\\alpha}^{1}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{\\alpha}^{1}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t\\geq\\int_{\\alpha}^{1}(1-t)^{d-3}\\mathrm{d}t=\\frac{1}{d-2}(1-\\alpha)^{d-2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "therefore we only need ", "page_idx": 30}, {"type": "equation", "text": "$$\nN\\geq\\frac{\\log(\\frac{\\alpha}{1+\\alpha}-\\frac{\\alpha}{(1+\\alpha)(N+1)^{2}})}{\\frac{k_{d}}{d-2}(1-\\alpha)^{d-2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, let $\\alpha\\,=\\,{\\frac{1}{d-2}}$ , since $\\begin{array}{r}{k_{d}\\,=\\,\\frac{\\Gamma(\\frac{d}{2})}{\\Gamma(\\frac{d-1}{2})}\\,=\\,O(\\sqrt{d})}\\end{array}$ , we only need $\\begin{array}{r}{N\\,\\geq\\,O(\\frac{d}{e\\sqrt{d}}\\log(d-\\,2))\\,=\\,}\\end{array}$ $O({\\sqrt{d}}\\log d)$ ", "page_idx": 30}, {"type": "text", "text": "Lemma 19_ (Concentration upper bound  for $\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1})$ .When ${\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}$ isdefinedby $\\operatorname*{max}_{i\\in[N]}\\left\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\right\\}$ where $\\mathbf{x}_{i}$ are independently sampled from a uniform sphere distribution, wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\leq1-\\frac{1}{\\big(2N k_{d}\\big)^{\\frac{2}{d-3}}}\\bigg)\\geq\\frac{1}{e}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Lemma 17, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\leq\\alpha\\bigg)=\\bigg(1-k_{d}\\int_{\\alpha}^{1}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t\\bigg)^{N}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{\\alpha}^{1}(1-t^{2})^{\\frac{d-3}{2}}\\mathrm{d}t\\leq(1-\\alpha)(1-\\alpha^{2})^{\\frac{d-3}{2}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "since $\\begin{array}{r}{(1-\\frac{1}{N})^{N}}\\end{array}$ is monotonically increasing, we only need $\\begin{array}{r}{k_{d}(1-\\alpha)(1-\\alpha)^{\\frac{d-3}{2}}\\leq\\frac{1}{N}}\\end{array}$ .Setting ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha=1-\\frac{1}{\\left(2N k_{d}\\right)^{\\frac{2}{d-3}}}\\leq\\left(1-\\frac{1}{\\left(N k_{d}\\right)^{\\frac{2}{d-3}}}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "suffices. ", "page_idx": 30}, {"type": "text", "text": "Lemma 20. Suppose $\\{\\mathbf{x}_{i}\\}_{i\\in[N+1]}$ are ii.d. samples from a uniform distribution on a sphere $\\mathbb{R}^{d}$ with ${\\bf x}_{i^{*}}^{\\top}{\\bf x}_{N+1}$ and $\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}$ being thelgest dsecondlaest order statisting $\\{\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\}_{i\\in[N]}$ respectively. Then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\exp\\big(\\xi\\big(\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\big)\\big)\\bigg]\\leq O\\bigg(\\frac{N^{2}k_{d}^{2}}{\\xi}\\bigg)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\exp\\big(\\xi\\big(\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\big)\\big)\\bigg]=\\Omega\\bigg(\\frac{1}{\\xi}\\bigg),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\Omega(\\cdot)$ hides constant depends on $N$ and $d$ ", "page_idx": 30}, {"type": "text", "text": "Proof. We denote $\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}$ by $Y_{i},\\,{\\mathbf{x}}_{i^{*}}^{\\top}{\\mathbf{x}}_{N+1}$ by $Y_{i^{*}}$ and $\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}$ by $Y_{(2)}$ . By Lemma 17, we have the dnsty funetion f Y bein a(t) ka (1 t), where a() . Then by the joint distributionof order statistics David and Nagaraja,2004],the joint densityfnctin of $Y_{i^{*}}$ and $Y_{(2)}$ is ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{Y_{i^{*}},Y_{2}}(t)(y_{1},y_{2})=N(N-1)\\cdot f_{d}(y_{2})f_{d}(y_{1})\\cdot F_{d}^{N-2}(y_{2})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $-1\\leq y_{2}\\leq y_{1}\\leq1$ . Therefore, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boldsymbol{\\xi}\\Bigg[\\exp\\big(\\xi\\big(\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{N+1}\\big)\\big)\\Bigg]\\leq{N(N-1)k_{d}^{2}}\\int_{-1}^{1}\\int_{-1}^{y_{1}}(1-y_{2}^{2})^{\\frac{d-3}{2}}(1-y_{1}^{2})^{\\frac{d-3}{2}}\\exp\\big(\\xi(y_{2}-y_{1})\\big)d y_{2}}\\\\ &{\\leq\\frac{{N(N-1)}}{2}k_{d}^{2}\\int_{-1}^{1}\\int_{-1}^{y_{1}}(2-y_{1}^{2}-y_{2}^{2})\\exp\\big(\\xi(y_{2}-y_{1})\\big)\\mathrm{d}y_{2}\\mathrm{d}y_{1}}\\\\ &{\\leq{N(N-1)k_{d}^{2}}\\int_{-1}^{1}\\int_{-1-y_{1}}^{0}\\exp(\\xi y_{2})\\mathrm{d}y_{2}\\mathrm{d}y_{1}}\\\\ &{\\leq{N^{2}k_{d}^{2}}\\int_{-2}^{0}\\exp(\\xi y_{2})\\mathrm{d}y_{2}}\\\\ &{=O\\bigg(\\frac{{N^{2}k_{d}^{2}}}{{\\xi}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus we obtain the upper bound. Next, we establish a lower bound. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\exp\\big(\\xi\\big(\\mathbf{x}_{(2)}^{\\top}\\mathbf{x}_{N+1}-\\mathbf{x}_{i^{*}}^{\\top}\\mathbf{x}_{N+1}\\big)\\big)\\bigg]\\geq\\frac{N(N-1)}{2^{N}}k_{d}^{2}\\int_{0}^{1}(1-y_{1}^{2})^{d-3}\\int_{0}^{y_{1}}\\exp\\big(\\xi(y_{2}-y_{1})\\big)\\mathrm{d}y_{2}\\mathrm{d}y_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{N(N-1)}{2^{N}\\xi}k_{d}^{2}\\int_{0}^{1}(1-y_{1}^{2})^{d-3}\\big(1-\\exp(-\\xi y_{1})\\big)\\mathrm{d}y_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\frac{N(N-1)}{e^{52N}\\xi}k_{d}^{2}\\int_{1/\\sqrt{d}}^{2/\\sqrt{d}}\\big(1-\\exp(-\\xi/\\sqrt{d})\\big)\\mathrm{d}y_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{N(N-1)}{e^{52N}\\sqrt{d}\\xi}k_{d}^{2}(1-\\exp(-\\xi/\\sqrt{d}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our main claims are made clear in the abstract and introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is limited to the theoretical analysis of single-layer transformers under 1-NN contexts. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All assumptions and proofs are included in the main paper and the appendix. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All technical details are provided in the paper and the appendix ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 33}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We only use simulated data, and provided enough technical details for the data and code we used in the main paper and appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We specify all hyperparameters and optimizers in the main paper and appendix. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We include error bar obtained from 10 independent trials ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All experiments are conducted on a CPU cluster ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work conforms with the code of ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work discusses the theoretical performance of a well-known architecture, thus the social impacts are insignificant. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not introduce any new asset. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]