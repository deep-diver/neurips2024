[{"type": "text", "text": "DH-Fusion: Depth-Aware Hybrid Feature Fusion for Multimodal 3D Object Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion.   \n2 However, they neglect the factor of depth while designing the fusion strategy. In   \n3 this work, we for the first time point out that different modalities play different roles   \n4 as depth varies via statistical analysis and visualization. Based on this finding, we   \n5 propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the   \n6 weights of point cloud and RGB image modalities by introducing depth encoding   \n7 at both global and local levels. Specifically, the Depth-Aware Global Feature   \n8 Fusion (DGF) module adaptively adjusts the weights of image Bird\u2019s-Eye-View   \n9 (BEV) features in multi-modal global features via depth encoding. Furthermore,   \n10 to compensate for the information lost when transferring raw features to the BEV   \n11 space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which   \n2 adaptively adjusts the weights of original voxel features and multi-view image   \n13 features in multi-modal local features via depth encoding. Extensive experiments   \n4 on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous   \n15 state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion is more robust to   \n16 various kinds of corruptions, outperforming previous methods on nuScenes-C w.r.t.   \n17 both NDS and mAP. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 3D object detection has a wide range of applications in the fields of autonomous driving and robotics.   \n20 A large number of previous works have successfully focused on using a single modality, such as point   \n21 cloud or images, to design efficient 3D object detectors. However, the performance of these detectors   \n22 reaches a bottleneck due to the limitations of modality characteristics. For instance, the point cloud   \n23 modality can only provide rich geometric information while lacks detailed semantic information;   \n24 the image modality can only provide rich texture information while lacks three-dimensional spatial   \n25 information. To address the aforementioned issues, we are highly motivated to obtain comprehensive   \n26 information that represents objects by designing a LiDAR-camera 3D object detector.   \n27 In recent years, LiDAR-camera 3D object detection develops rapidly. Some works [1, 4, 28, 33, 67]   \n28 propose effective methods to integrate information from two modalities at the feature level. However,   \n29 they all overlook an important factor of depth in their fusion strategies. To understand how point   \n30 cloud and image information vary with depth, we first conduct statistical and visualization analysis   \n31 on the nuScenes-mini dataset [3], and find that: (1) The number of points representing objects at   \n32 near range is relatively large, which allows us to accurately determine the object\u2019s location, size, and   \n33 category, even without the aid of images. As shown in Fig. 1a, there is an average of 163.7 points per   \n34 object within 0-10 meters, which is a substantial number. We also visualize a car at 6.8 meters in   \n35 Fig. 1b $\\textcircled{1}$ and find it encompasses a considerable number of points, well representing the shape. In   \n36 contrast, some background noise in the image may interfere with detection (Fig. 1b $\\circledcirc$ ). (2) As the   \n37 depth increases, the number of points representing objects decreases rapidly. As shown in Fig. 1a,   \n38 the number of points within 30-50 meters falls below one per object, meaning that many objects are   \n39 even not represented by any points, such as the object at 42.1 meters in Fig. 1b $\\circled{3}$ . In contrast, the   \n40 complete objects may still be observed on the image, as in Fig. 1b $\\circledast$ , where the image information   \n41 becomes more important. To address the above problems, we propose a feature fusion strategy that   \n42 adaptively adjusts the importance of the two modalities based on depth.   \n43 Specifically, we propose a novel method for multi-modal 3D object detection, namely Depth-Aware   \n44 Hybrid Feature Fusion (DH-Fusion). The innovation lies in adaptively adjusting the weights of   \n45 features by introducing depth encoding to hybrid feature fusion at both global and local levels. The   \n46 fusion strategy consists of two crucial components: Depth-Aware Global Feature Fusion (DGF)   \n47 module and Depth-Aware Local Feature Fusion (DLF) module. In DGF, we take point cloud Bird\u2019s  \n48 Eye-View (BEV) features and image BEV features as inputs, and dynamically adjust the weights of   \n49 image BEV features based on depth during fusion by utilizing a global-fusion transformer encoder   \n50 with a depth encoder. To compensate for the information lost when transforming raw features to   \n51 BEV space, we enhance the fused BEV features at a lower cost by utilizing the original instance   \n52 features. In DLF, we obtain 3D boxes by utilizing a Region Proposal Network (RPN). Then, the   \n53 3D boxes are projected into both LiDAR voxel features and multi-view image features to crop out   \n54 corresponding local instance features with more detailed information. Afterward, we take these as   \n55 inputs and dynamically adjust the weights of local multi-view image features and local LiDAR voxel   \n56 features based on depth through the use of a local-fusion transformer encoder with the depth encoder.   \n57 In the end, we update local features for each object on the global feature map to enhance the detailed   \n58 instance information of multi-modal global features for detection. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/6f5f62d1d9e5e1b3b011bcd18ddd641e3ee6ee5bfd54e96919129afb6beacf83.jpg", "img_caption": ["Figure 1: Statistical and visualization analysis on the nuScenes-mini dataset. (a) The average numbers of points and pixels for each object at different depths. (b) Examples of near-range and long-range objects in images and point cloud. Points within the bounding boxes are colored red for observation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "59 Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "60 1. We for the first time point out that depth is an important factor to consider while fusing LiDAR   \n61 point cloud features and RGB image features for 3D object detection. From our statistical and   \n62 visualization analysis, we can see that image features play different roles as depth varies.   \n63 2. We propose a depth-aware hybrid feature fusion strategy that dynamically adjusts the weights of   \n64 features during feature fusion by introducing depth encoding at both global and local levels. The   \n65 above strategy can obtain high-quality features for detection, fully leveraging the advantages of   \n66 different modalities at various depths.   \n67 3. Our method is evaluated on the nuScenes [3] dataset and a more challenging nuScenes-C [13]   \n68 dataset, outperforming previous multi-modal methods and being robust to various kinds of data   \n69 corruptions. ", "page_idx": 1}, {"type": "text", "text": "70 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 Since our method is based on conducting 3D object detection using data from multiple modalities,   \n72 including point cloud and images, we briefly review recent works in the following fields: LiDAR  \n73 based 3D object detection, camera-based 3D object detection, and LiDAR-camera 3D object detection. ", "page_idx": 1}, {"type": "text", "text": "74 2.1 LiDAR-based 3D Object Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "75 LiDAR-based 3D object detectors only take the point cloud as input. Based on their different data   \n76 representations, they can be divided into point-based [44\u201346, 64, 65], voxel-based [12, 22, 61, 68, 71],   \n77 and point-voxel-based [17, 42, 43] methods. The feature extraction networks of point-based methods   \n78 typically extract features directly from the point cloud through a point-based backbone [40], such as   \n79 PointRCNN [44]. The voxel-based methods first convert the point cloud into voxels and then extract   \n80 voxel features through a 3D sparse convolution network [14], such as VoxelNet [71]. Point-voxel  \n81 based methods like PV-RCNN [42] combine the above two methods to extract and fuse point and   \n82 voxel features. The purpose of these approaches is to capture the geometric spatial information of the   \n83 point cloud. However, point cloud is sparse and incomplete, lacking detailed texture information,   \n84 which greatly limits the detection performance. ", "page_idx": 2}, {"type": "text", "text": "85 2.2 Camera-based 3D Object Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "86 Camera-based 3D object detectors only take images as inputs. Depending on the form of inputs,   \n87 they can be divided into monocular [2, 24, 32, 41, 47, 55], stereo [6, 25, 30, 48, 70], and multi-view   \n88 [19, 27, 56, 62] 3D object detectors. Early works like FCOS3D [55] input a monocular image and   \n89 utilize 2D object detectors to directly predict 3D bounding boxes, but these approaches have limited   \n90 capability in capturing spatial information. Subsequently, stereo and multi-view 3D object detectors   \n91 are proposed to obtain more precise depth information by constructing spatial relationships among   \n92 multiple images, such as Stereo RCNN [25] and BEVDet [19]. These methods successfully achieve   \n93 purely visual 3D object detection, but they do not perform as well as LiDAR-based methods, because   \n94 the spatial depth information provided by images is not as direct and precise as that provided by point   \n95 cloud. ", "page_idx": 2}, {"type": "text", "text": "96 2.3 LiDAR-Camera 3D Object Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 LiDAR-camera 3D object detectors take point cloud and images as inputs, and can be classified   \n98 into early-fusion-based [50, 52, 57, 59, 69], intermediate-fusion-based [1, 4, 28, 33, 67], and late  \n99 fusion-based [37, 38] 3D object detectors based on the location of multi-modal information fusion   \n100 [36].   \n101 Early-fusion-based methods perform at the point level, where the typical approach involves enhancing   \n102 the raw point cloud with semantic information extracted from images. PointPainting [50] and Fu  \n103 sionPainting [59] decorate the raw point cloud with semantic scores from 2D semantic segmentation.   \n104 Similarly, PointAugmenting [52] enhances the raw point cloud using features extracted from a 2D   \n105 semantic segmentation network. However, early-fusion-based methods are sensitive to alignment   \n106 errors between the two modalities.   \n107 Intermediate-fusion-based methods perform at the feature level. Transfusion [1] first proposes to   \n108 utilize the transformer for fine-grained fusion from LiDAR BEV features and multi-view image   \nfeatures. FUTR3D [5] encode each modality using deformable attention [73] in its own coordinate   \n110 and concatenate them for fusion. BEVFusion [28, 33] projects both point cloud and images to BEV   \n111 space for BEV feature fusion. SparseFusion [58] extracts instance-level features from both two   \nmodalities separately, and fuse them to perform detection. Similarly, ObjectFusion [4] utilizes 3D   \n113 proposals from LiDAR modality to extract instance-level features for fusion. CMT [60] proposes   \n114 the simultaneous interaction between the object queries and multi-modal features in the transformer   \n115 encoder and decoder. IS-Fusion [67] proposes feature fusion at both the instance level and scene   \nlevel. The intermediate-fusion-based methods gradually become a mainstream approach due to the   \n117 diversity of fusion strategies.   \n118 Late-fusion-based methods perform at the bounding box level. Typically, CLOCs [37] obtains 2D and   \n119 3D bounding boxes by separately using 2D and 3D object detectors, and then combine them to achieve   \n120 more accurate 3D bounding boxes. However, the interaction between modalities in late-fusion-based   \n121 methods is very limited, which constrains model performance.   \n122 These multi-modal methods successfully outperform single-modal methods. However, their feature   \n123 fusion methods do not take depth into account. In contrast, our approach introduces depth information   \n124 to guide the hybrid feature fusion, boosting the performance of the detector. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/1b8b9a5b6a0152ed9938477f650ecdf8fa863c739e85515e4bbda53ba3fa7db0.jpg", "img_caption": ["Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection. $\\otimes$ is the multiplication operation, and $\\widehat{\\textbf{M}}$ is the merge operation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "125 3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "126 In this section, we first give an overview of our proposed multi-modal 3D object detector, and then   \n127 provide a detailed introduction to our proposed feature fusion method. ", "page_idx": 3}, {"type": "text", "text": "128 3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "129 We propose a multi-modal 3D object detection method via Depth-Aware Hybrid Feature Fusion   \n130 (DH-Fusion). As illustrated in Fig. 2, our approach consists of two important feature fusion modules:   \n131 Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF). In the   \n132 following, we briefly describe the detection pipeline.   \n133 Inputs. First, we take the point cloud $P$ and multi-view images $I$ as inputs, where point cloud   \n134 consists of a set of points: $P=\\{P_{1},P_{2},\\cdot\\cdot\\cdot,P_{N_{l}}\\}$ , and each point has four dimensions: X-axis,   \n135 Y-axis, $Z$ -axis, and intensity; the multi-view images comprise $N_{c}$ images: $I=\\{I_{1},I_{2},\\cdot\\cdot\\cdot,I_{N_{c}}\\}$ ,   \n136 each image captured by its corresponding camera.   \n137 Input Encoding. For the point cloud $P$ , we use a 3D encoder to extract raw global voxel features   \n138 $\\nu_{O}^{\\hat{G}}$ ; for the multi-view images $I$ , we use a 2D encoder to extract image features of all views $\\mathcal{T}_{O}^{G}$ .   \n139 Hybrid Feature Fusion. Then, for voxel features $\\mathcal{V}_{O}^{G}$ , we compress the height dimension to obtain   \n140 point cloud BEV features $\\mathcal{V}_{B}^{G}$ ; for image features $\\mathcal{T}_{O}^{G}$ , we transform their perspective view to bird\u2019s   \n141 eye view to obtain image BEV features $\\mathcal{T}_{B}^{G}$ . To fully leverage the features from two modalities, we   \n142 design a DGF module that aims to dynamically adjust the weights of image BEV features based   \n143 on depth values during feature fusion. Please refer to Sec. 3.2 for more details. To compensate   \n144 for the information lost when transforming raw features to BEV space, we propose a DLF module   \n145 that, based on depth, utilizes the raw features to enhance the detailed information of each object   \n146 instance in global multi-modal features. It consists of three processes: local feature selection, local   \n147 feature fusion, and merging local features into global features. First, we obtain the local multi-modal   \n148 BEV features $\\mathcal{F}_{B}^{L}$ , local voxel features $\\nu_{O}^{L}$ , and local multi-view image features $\\mathcal{T}_{O}^{L}$ , by cropping the   \n149 corresponding global features based on the 3D boxes obtained from an RPN; then, it dynamically   \n150 and individually adjusts the weights of each local feature of $\\nu_{O}^{L}$ and $\\mathcal{T}_{O}^{L}$ based on depth values during   \n151 feature fusion; finally, we update local features for each object on the global feature map. Please   \n152 refer to Sec. 3.3 for more details. In this way, we obtain enhanced multi-modal global features for   \n153 detection.   \n154 Decoding. Based on the enhanced multi-modal global features $\\hat{\\mathcal{F}}_{B}^{G}$ that contain rich semantic and   \n155 spatial information, we utilize a transformer decoder and a detection head to predict the object   \n156 categories and 3D bounding boxes. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/7872c53c5a427f8fd49a0dcc87ba20c12b812671cd2133e8074de04dcc6bd833.jpg", "img_caption": ["Figure 3: Illustration of the DGF. It consists of a global fusion transformer with the depth encoder. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/93b3213e7ac1959c69649ca4a2396e7de6a57b6121d362de4732ed66de3276f9.jpg", "img_caption": ["Figure 4: Illustration of the DLF. It consists of a local feature selection module and a local fusion transformer with the depth encoder. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "157 3.2 Depth-Aware Global Feature Fusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "158 As shown in Fig. 3, the DGF module consists of a global-fusion transformer with a depth encoder. In   \n159 the following, we provide a detailed explanation of each component. ", "page_idx": 4}, {"type": "text", "text": "160 3.2.1 Depth Encoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "161 We introduce depth encoding (DE) in feature fusion to dynamically adjust the weights of image BEV   \n162 features during fusion. First, we build a depth matrix $M$ to store the depth value of each position   \n163 element $p_{k}$ represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{k}=\\{(x_{k},y_{k}):d_{k}\\},k\\in[1,n],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 where $(x_{k},y_{k})$ are the positional coordinates, $d_{k}$ is the depth value, and $n$ is the number of elements.   \n165 Then, we use Euclidean distance to calculate the distance between every element\u2019s spatial location   \n166 $(x_{k},y_{k})$ and the ego coordinate element\u2019s location $(x_{\\frac{n}{2}},y_{\\frac{n}{2}})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{k}=E((x_{k},y_{k}),(x_{\\frac{n}{2}},y_{\\frac{n}{2}})),k\\in[1,n],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 where we denote $E(\\cdot)$ as the Euclidean distance calculation. The depth matrix $M$ serves as a lookup   \n168 table to avoid redundant computation of depth values. Since the size of the BEV features is large and   \n169 the depth distribution is simple, to avoid introducing additional parameters, the depth encoding $D e$ is   \n170 obtained by applying sine and cosine functions [49] to the depth matrix. ", "page_idx": 4}, {"type": "text", "text": "171 3.2.2 Global-Fusion Transformer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "172 In the global-fusion transformer, we take the point cloud BEV features $\\mathcal{V}_{B}^{G}\\in\\mathbb{R}^{W\\times H\\times C}$ and image   \n173 BEV features $\\mathcal{T}_{B}^{G}\\in\\mathbb{R}^{W\\times H\\times C}$ as inputs, and integrate the depth encoding obtained above by multi  \n174 plying it with the point cloud BEV features, forming the query $Q_{\\mathcal{V}}^{G}=N(\\mathcal{V}_{B}^{G}\\times C o n v(D e))$ , where   \n175 $C o n v(\\cdot)$ is a convolution operation to align with the channels of $\\mathcal{V}_{B}^{G}$ , and $N(\\cdot)$ is a normalization   \n176 layer. The image BEV features are queried as the corresponding key $K_{\\mathcal{T}}^{G}$ and value $V_{\\mathcal{T}}^{G}$ . We utilize   \n177 the multi-head cross attention to achieve the interacted feature $\\hat{\\mathcal{V}}_{B}^{G}$ based on depth: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{V}}_{B}^{G}=C A(Q_{\\mathcal{V}}^{G},K_{\\mathcal{T}}^{G},V_{\\mathcal{T}}^{G}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 where $C A(\\cdot)$ indicates the multi-head cross attention. Afterward, we aggregate the information from   \n179 both modalities to obtain the fused features $\\mathcal{F}_{B}^{G}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}_{B}^{G}=N(F F N(N(\\hat{\\mathcal{V}}_{B}^{G}+\\mathcal{V}_{B}^{G}))+N(\\hat{\\mathcal{V}}_{B}^{G}+\\mathcal{V}_{B}^{G})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "180 where $N(\\cdot)$ is a normalization layer; $F F N(\\cdot)$ specifies a feed-forward network containing two   \n181 convolution operations. In this way, we obtain fused features in which the image features play   \n182 different roles as the depth varies. ", "page_idx": 5}, {"type": "text", "text": "183 3.3 Depth-Aware Local Feature Fusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "184 As shown in Fig. 4, the DLF module consists of a local feature selection and a local-fusion transformer   \n185 with the depth encoder. In the following, we provide a detailed explanation of each component. ", "page_idx": 5}, {"type": "text", "text": "186 3.3.1 Local Feature Selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "187 To compensate for the information lost when transforming point cloud features and image features to   \n188 BEV space, we enhance the instance details of fused BEV features $\\mathcal{F}_{B}^{G}$ using instance features from   \n189 raw voxel features $\\nu_{O}^{G}$ and multi-view image features ${\\mathcal{I}}_{O}^{G}$ . Specifically, we utilize an RPN to regress   \n190 $t\\ 3\\mathbf{D}$ boxes based on the BEV features $\\mathcal{F}_{B}^{G}$ . We directly crop the global fused BEV features $\\mathcal{F}_{B}^{G}$   \n191 based on the regressed 3D boxes to obtain the local fused BEV features $\\mathcal{F}_{B}^{L}\\in\\mathbb{R}^{c\\times t}$ . On the other   \n192 hand, we project the 3D boxes onto the raw voxel features and multi-view image features to obtain   \n193 their corresponding local features before global fusion, preserving richer information for each object   \n194 instance. Specifically, we utilize the voxel pooling operation [12], followed by a 3D convolution   \n195 operation and a linear layer, to extract local voxel features ${\\mathcal{V}}_{O}^{L}\\in\\mathbb{R}^{c\\times t}$ ; we transform the 3D boxes   \n196 from bird\u2019s eye view to perspective view, and utilize the RoI Align operation [15], followed by a   \n197 linear layer, to extract instance image features $\\mathcal{T}_{O}^{L}\\in\\mathbb{R}^{c\\times t}$ . By doing this, we obtain the hybrid   \n198 (before & after global fusion) local features, which will be sent to the subsequent fusion module. ", "page_idx": 5}, {"type": "text", "text": "199 3.3.2 Local-Fusion Transformer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "200 In the local-fusion transformer, the weights of each local raw feature are dynamically adjusted based   \n201 on depth values during feature fusion, and we update local features for each object on the global   \n202 feature map. Specifically, we take the local multi-modal BEV features $\\mathcal{F}_{B}^{L}$ , local voxel features $\\nu_{O}^{L}$ ,   \n203 and local multi-view image features $\\mathcal{T}_{O}^{L}$ as inputs, and integrate the depth encoding by multiplying   \n204 it with the local multi-modal BEV features, forming the query $Q_{\\mathcal{F}}^{L}$ . The local multi-view image   \n205 features and local voxel features are respectively queried as the corresponding key $K_{\\mathcal{T}}^{L},K_{\\mathcal{V}}^{L}$ and value   \n206 $V_{\\mathcal{T}}^{L},V_{\\mathcal{V}}^{L}$ . The two multi-head cross-attention modules are utilized to achieve the interacted features   \n207 $\\hat{Q}_{\\mathcal{F}}^{L},\\hat{Q}_{\\mathcal{F}}^{L^{\\prime}}$ . Note that the computation process of multi-head cross attention is similar to that described   \n208 in Sec. 3.2.2 and is omitted here. Afterward, we aggregate the above features: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{F}}_{B}^{L}=C o n v(C a t(\\hat{Q}_{\\mathcal{F}}^{L}+\\mathcal{F}_{B}^{L},\\hat{Q}_{\\mathcal{F}}^{L^{\\prime}}+\\mathcal{F}_{B}^{L^{\\prime}})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 where $C a t(\\cdot)$ is the concatenation operation; $C o n v(\\cdot)$ is used to align with the feature channels of   \n210 global fused BEV features $\\mathcal{F}_{B}^{G}$ . As a result, we obtain enhanced local features by dynamically calling   \n211 back rich information in raw modalities at various depths. Afterward, we update the global features   \n212 $\\mathcal{F}_{B}^{G}$ by inserting the enhanced local features at corresponding locations. ", "page_idx": 5}, {"type": "text", "text": "213 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 In this section, we will first introduce the dataset and evaluation metrics, followed by the implementa  \n215 tion details. Then, we will compare our method with the state-of-the-art methods on nuScenes and   \n216 also present results on a more challenging dataset of nuScenes-C with data corruptions. Finally, we   \n217 will show the ablation studies and qualitative results. More experiments are provided in Appendix   \n218 A.2. ", "page_idx": 5}, {"type": "text", "text": "219 4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "220 Datasets and evaluation metrics. We evaluate our proposed DH-Fusion on the nuScenes benchmark   \n221 [3] and a more challenging dataset of nuScenes-C [13] with data corruptions. nuScenes dataset   \n222 provides 700 scene sequences for training, 150 scene sequences for validation, and 150 scene   \n223 sequences for testing. Each sequence contains 40 frames of 32-beam LiDAR data, and each frame   \n224 has six corresponding images covering a 360-degree field of view. It offers calibration matrices that   \n225 facilitate accurate projection of 3D points onto 2D pixels, and contains 10 object categories that are   \n226 commonly encountered within autonomous driving. nuScenes-C dataset provides 27 corruptions   \n227 with 5 severities on the nuScenes validation set, including corruptions at the weather, sensor, motion,   \n228 object, and alignment level. We use the nuScenes detection scores (NDS) and mean Average Precision   \n229 (mAP) to evaluate our detection results, where NDS is a comprehensive metric in nuScenes that   \n230 combines object translation, scale, orientation, velocity, and attribute errors.   \n231 Implementation details. We implement the proposed DH-Fusion with PyTorch [39] under the   \n232 open-source framework MMDetection3D [10]. Specifically, for the LiDAR branch, we use VoxelNet   \n233 [71] with FPN [61] as the 3D encoder. The voxel size is set to $[0.075\\mathrm{m},0.075\\mathrm{m},0.1\\mathrm{m}]$ , and the range   \n234 of point cloud is $[-54\\mathrm{m},54\\mathrm{m}]$ along the X-axis, $[-54\\mathrm{m},54\\mathrm{m}]$ along the Y-axis, and $[-3\\mathrm{m},5\\mathrm{m}]$ along   \n235 the Z-axis. For the image branch, we use the ResNet18 [16], ResNet50 [16], and SwinTiny [34] with   \n236 FPN [29] as the 2D image encoder of DH-Fusion-light, -base, -large, respectively. Correspondingly,   \n237 the resolution of input images is resized to $256\\times704$ , $320\\times800$ , and $384\\times1056$ . Additionally, we   \n238 utilize BEVPoolV2 [18] to obtain image BEV features. Following [33], the feature size $W\\times H$ is set   \n239 to $180\\times180$ , the channel $C$ is set to 128, and the channel $c$ is also set to 128. The multi-head cross   \n240 attention is implemented with 8 heads, and the FFN contains 2 MLP layers with a hidden dimension   \n241 of 128. Following [58], the number of regressed 3D boxes $t$ is set to 200. More implementation   \n242 details are provided in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "243 4.2 Comparison to the State of the Art ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "244 Aiming for a fair comparison, we categorize previous methods based on the types of 2D backbones   \n245 into ResNet50-based, SwinTiny-based, and others, and provide three versions of our proposed method,   \n246 named DH-Fusion-light, DH-Fusion-base, and DH-Fusion-large. The results are shown in Tab. 1.   \n247 (1) Compared with the ResNet50-based methods, our DH-Fusion-base outperforms the top method   \n248 FocalFormer3D [7] by up to 1 pp w.r.t. NDS under the same configuration. Specifically, we reach   \n249 9 $74.0\\%$ w.r.t. NDS and $71.2\\%$ w.r.t. mAP on the validation set, and $74.7\\%$ w.r.t. NDS and $71.7\\%$   \n250 w.r.t. mAP on the test set, while maintaining comparable inference speed of 8.7 FPS on a 3090 GPU.   \n251 (2) Compared with the SwinTiny-based methods and others, our DH-Fusion-large outperforms the   \n252 top method IS-Fusion [67] under the same configuration, and runs $2\\mathbf{x}$ faster than it. Specifically, we   \n253 reach $74.4\\%$ w.r.t. NDS on the validation set, and $75.4\\%$ w.r.t. NDS on the test set, while achieving a   \n254 faster inference speed of 5.7 FPS on a 3090 GPU, indicating that our proposed method is both more   \n255 effective and efficient. (3) Furthermore, our DH-Fusion-light surpasses the typical BEVFusion [33]   \n256 by up to 1 pp w.r.t. all metrics using a lighter 2D backbone, and achieves a real-time inference speed   \n257 of 13.8 FPS. Overall, our method achieves higher detection accuracy and faster inference speed. ", "page_idx": 6}, {"type": "text", "text": "258 4.3 Robustness to Corruptions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "259 We further implement some experiments on the nuScenes-C [13] dataset to evaluate the model\u2019s   \n260 robustness under various corruptions, including changes in weather, data loss or temporal-spatial   \n261 misalignment in multi-modal inputs, etc. The results for different kinds of corruptions are shown   \n262 in Tab. 2, and more detailed results for each fine-grained corruption are shown in Appendix A.2.3.   \n263 We find that our DH-Fusion-light still achieves an average performance of $68.67\\%$ w.r.t. NDS and   \n264 $63.07\\%$ w.r.t. mAP under various corruptions, which only decreases by $4.63~\\mathrm{pp}$ w.r.t. NDS and   \n265 $6.68\\,\\mathrm{pp}$ w.r.t. mAP, compared to its performance without corruptions. Performance drop is smaller   \n266 than that observed with previous methods including BEVFusion [28] across all kinds of corruptions,   \n267 indicating that our DH-Fusion-light possesses superior robustness. Furthermore, we observe that our   \n268 DH-Fusion-light is particularly robust against weather and object corruptions, where the performance   \n269 drop is less than 3pp. The more stable performance indicates that our method is more friendly to   \n270 practical applications, where data corruption may occur. ", "page_idx": 6}, {"type": "text", "text": "271 4.4 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "272 We conduct ablation studies to first demonstrate the effect of each component of DH-Fusion, then   \n273 to demonstrate the effect of depth encoding in DGF and DLF, and finally to assess the impact of   \n274 multiplying depth encoding. All method variants are implemented on the nuScenes validation dataset.   \n275 Effect of DGF and DLF. To demonstrate the effect of DGF and DLF, we conduct experiments by   \n276 integrating the components one by one into the baseline, BEVFusion [33]. The results are shown   \n277 in Tab. 3. We find that our DGF improves the baseline performance by $1.0\\,\\mathrm{pp}$ w.r.t. NDS and 0.9   \n278 pp w.r.t. mAP. This demonstrates that dynamically adjusting the weights of the image BEV features   \n279 during fusion is effective for 3D object detection. Additionally, our DLF improves the baseline   \n280 performance by $1.3\\,\\mathrm{pp}$ w.r.t. NDS and 0.8 pp w.r.t. mAP, which indicates that dynamically adjusting   \n281 the weights of the local raw instance features based on depth during fusion effectively compensates   \n282 for the information loss caused by the transformation of global features into the BEV feature space.   \n283 The results of integrating both components show an improvement of 1.9 pp w.r.t. NDS and $1.3\\;\\mathrm{pp}$   \n284 w.r.t. mAP, well verifying the benefits of dynamically fusing global and local hybrid features based   \n285 on depth.   \n286 Effect of depth encoding in DGF and DLF. To evaluate the effectiveness of our depth encoding,   \n287 we conduct experiments where the depth encoding is removed from the DGF and DLF modules,   \n288 respectively. The results are shown in Tab. 4. When removing the depth encoding from Baseline $+\\mathrm{DGF}_{\\mathrm{}}$ ,   \n289 the performance drops by 0.6 pp w.r.t. NDS and 0.4 pp w.r.t. mAP. Similarly, when removing the   \n290 depth encoding from Baseline $+\\mathrm{DLF}$ , the performance also decreases by 1.1 pp w.r.t. NDS and 0.9 pp   \n291 w.r.t. mAP. These results indicate that our depth encoding is effective. Furthermore, we observe that   \n292 removing the depth encoding from the DLF module results in a larger performance drop, suggesting   \n293 that depth encoding plays a more crucial role in local feature fusion.   \n294 Impact of different operations for depth encoding. We conduct experiments with different   \n295 operations of depth encoding, including concatenation, summation, and multiplication. The results   \n296 in Tab. 5, show that the multiplication operation consistently outperforms the summation and   \n297 concatenation operations w.r.t. both metrics. The superior performance of multiplication can be   \n298 attributed to its ability to more effectively modulate the feature maps based on depth information.   \n299 Unlike summation, which simply shifts the feature values, or concatenation, which increases the   \n300 dimensionality without direct interaction, multiplication allows for more interaction between the   \n301 depth encoding and features, leading to better feature representation and ultimately improving the   \n302 detection performance. ", "page_idx": 6}, {"type": "table", "img_path": "gBOQ0ACqoO/tmp/792ce3010d00770d0a6a27472e5d7152bf8726172a3ba2e5dce4d8e884f6f581.jpg", "table_caption": ["Table 1: Comparisons with the state of the art on the nuScenes validation and test sets. FPS is measured on a 3090 GPU by default, and \\* denotes the inference speed on an A100 GPU referred from the original paper. Note that all results are obtained without any model ensemble or test time augmentation. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "gBOQ0ACqoO/tmp/7cadb4da58cb4da650915fe5bf68960f4c5998ea159b8cacd26a6c359f0e4341.jpg", "table_caption": ["Table 2: Robustness experiments on nuScenes-C. Numbers are NDS / mAP "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/1227a25ab76f6337bfb26fa700a62c0f32427703bdbcad3a546bc59a3dd5eba3.jpg", "img_caption": ["BEV image features in DGF vary with depth. ", "tures of BEVFusion and ours. We show the ground truth boxes in green, and the prediction boxes in blue. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "303 4.5 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "304 To better understand how depth encoding affects the feature fusion, in Fig. 5, we plot a curve to   \n305 observe how the attention weights applied on the image BEV features in our DGF module vary with   \n306 depth, and visualize the average attention map. It is evident that the weights of the image BEV   \n307 features stay low in near range, but go up significantly as depth increases when the depth is larger   \n308 than 40 meters. This trend supports our hypothesis that the image modality would become more   \n309 important as depth increases. In this way, our depth encoding allows the model to dynamically adjust   \n310 the weights of image BEV features based on depth.   \n311 We also compare the detection results of our DH-Fusion method with the baseline BEVFusion [33]   \n312 in Fig. 6, where we clearly find that our method better localizes those distant objects compared to   \n313 BEVFusion. These results demonstrate that our proposed multi-modal fusion strategy based on depth   \n314 is more effective for detection. Besides, we exhibit the corresponding BEV feature maps, where   \n315 our method shows a stronger feature response for the foreground objects, especially for distant ones.   \n316 That is why our feature fusion strategy can provide higher-quality detection results. More qualitative   \n317 results can be found in Appendix A.3. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "318 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 In this paper, we for the first time point out that different modalities play different roles as depth varies   \n320 via statistical analysis and visualization. Based on this finding, we propose a feature fusion strategy   \n321 for multi-modal 3D object detection, namely Depth-Aware Hybrid Feature Fusion (DH-Fusion), that   \n322 dynamically adjusts the weights of features during feature fusion by introducing depth encoding at   \n323 both global and local levels. Extensive experiments on the nuScenes dataset demonstrate that our   \n324 DH-Fusion method surpasses previous state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion   \n325 is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C   \n326 dataset w.r.t. both NDS and mAP. Our method uses an attention-based approach to interact with   \n327 the two modalities, making the detection results sensitive to modality loss. We plan to further   \n328 explore feature fusion methods that are robust to modality loss. Although our method improves   \n329 detection performance, emergency plans still need to be implemented in practical applications to   \n330 ensure personnel safety. ", "page_idx": 8}, {"type": "text", "text": "331 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "332 [1] Bai, X., Hu, Z., Zhu, X., Huang, Q., Chen, Y., Fu, H., Tai, C.L.: Transfusion: Robust lidar  \n333 camera fusion for 3d object detection with transformers. In: CVPR (2022)   \n334 [2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In:   \n335 ICCV (2019)   \n336 [3] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan,   \n337 G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: CVPR (2020)   \n338 [4] Cai, Q., Pan, Y., Yao, T., Ngo, C.W., Mei, T.: Objectfusion: Multi-modal 3d object detection   \n339 with object-centric fusion. In: ICCV (2023)   \n340 [5] Chen, X., Zhang, T., Wang, Y., Wang, Y., Zhao, H.: Futr3d: A unified sensor fusion framework   \n341 for 3d detection. In: CVPR (2023)   \n342 [6] Chen, Y., Liu, S., Shen, X., Jia, J.: Dsgn: Deep stereo geometry network for 3d object detection.   \n343 In: CVPR (2020)   \n344 [7] Chen, Y., Yu, Z., Chen, Y., Lan, S., Anandkumar, A., Jia, J., Alvarez, J.M.: Focalformer3d:   \n345 focusing on hard instance for 3d object detection. In: ICCV (2023)   \n346 [8] Chen, Z., Li, Z., Zhang, S., Fang, L., Jiang, Q., Zhao, F.: Deformable feature aggregation for   \n347 dynamic multi-modal 3d object detection. In: ECCV (2022)   \n348 [9] Chiu, H.k., Prioletti, A., Li, J., Bohg, J.: Probabilistic 3d multi-object tracking for autonomous   \n349 driving. arxiv 2020. arXiv preprint arXiv:2001.05673 (2020)   \n350 [10] Contributors, M.: MMDetection3D: OpenMMLab next-generation platform for general 3D   \n351 object detection. https://github.com/open-mmlab/mmdetection3d (2020)   \n352 [11] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical   \n353 image database. In: CVPR (2009)   \n354 [12] Deng, J., Shi, S., Li, P., Zhou, W., Zhang, Y., Li, H.: Voxel r-cnn: Towards high performance   \n355 voxel-based 3d object detection. In: AAAI (2021)   \n356 [13] Dong, Y., Kang, C., Zhang, J., Zhu, Z., Wang, Y., Yang, X., Su, H., Wei, X., Zhu, J.: Bench  \n357 marking robustness of 3d object detection to common corruptions. In: CVPR (2023)   \n358 [14] Graham, B., Engelcke, M., Van Der Maaten, L.: 3d semantic segmentation with submanifold   \n359 sparse convolutional networks. In: CVPR (2018)   \n360 [15] He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: CVPR (2017)   \n361 [16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR   \n362 (2016)   \n363 [17] Hu, J.S., Kuai, T., Waslander, S.L.: Point density-aware voxels for lidar 3d object detection. In:   \n364 CVPR (2022)   \n365 [18] Huang, J., Huang, G.: Bevpoolv2: A cutting-edge implementation of bevdet toward deployment.   \n366 arXiv:2211.17111 (2022)   \n367 [19] Huang, J., Huang, G., Zhu, Z., Ye, Y., Du, D.: Bevdet: High-performance multi-camera 3d   \n368 object detection in bird-eye-view. arXiv:2112.11790 (2021)   \n369 [20] Huang, J., Ye, Y., Liang, Z., Shan, Y., Du, D.: Detecting as labeling: Rethinking lidar-camera   \n370 fusion in 3d object detection. arXiv arXiv:2311.07152 (2023)   \n371 [21] Jiao, Y., Jie, Z., Chen, S., Chen, J., Ma, L., Jiang, Y.G.: Msmdfusion: Fusing lidar and camera   \n372 at multiple scales with multi-depth seeds for 3d object detection. In: CVPR (2023)   \n373 [22] Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders   \n374 for object detection from point clouds. In: CVPR (2019)   \n375 [23] Lee, Y., Hwang, J.w., Lee, S., Bae, Y., Park, J.: An energy and gpu-computation efficient   \n376 backbone network for real-time object detection. In: CVPR workshops (2019)   \n377 [24] Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X.: Gs3d: An efficient 3d object detection   \n378 framework for autonomous driving. In: CVPR (2019)   \n379 [25] Li, P., Chen, X., Shen, S.: Stereo r-cnn based 3d object detection for autonomous driving. In:   \n380 CVPR (2019)   \n381 [26] Li, Y., Chen, Y., Qi, X., Li, Z., Sun, J., Jia, J.: Unifying voxel-based representation with   \n382 transformer for 3d object detection. In: NeurIPS (2022)   \n383 [27] Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., Dai, J.: Bevformer: Learning   \n384 bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers. In:   \n385 ECCV (2022)   \n386 [28] Liang, T., Xie, H., Yu, K., Xia, Z., Lin, Z., Wang, Y., Tang, T., Wang, B., Tang, Z.: Bevfusion:   \n387 A simple and robust lidar-camera fusion framework. In: NeurIPS (2022)   \n388 [29] Lin, T.Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks   \n389 for object detection. In: CVPR (2017)   \n390 [30] Liu, Y., Wang, L., Liu, M.: Yolostereo3d: A step back to 2d for efficient stereo 3d detection. In:   \n391 ICRA. IEEE (2021)   \n392 [31] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer:   \n393 Hierarchical vision transformer using shifted windows. In: ICCV (2021)   \n394 [32] Liu, Z., Wu, Z., T\u00f3th, R.: Smoke: Single-stage monocular 3d object detection via keypoint   \n395 estimation. In: CVPR (2020)   \n396 [33] Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfusion: Multi-task   \n397 multi-sensor fusion with unified bird\u2019s-eye view representation. In: ICRA (2023)   \n398 [34] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In:   \n399 CVPR (2022)   \n400 [35] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint   \n401 arXiv:1711.05101 (2017)   \n402 [36] Mao, J., Shi, S., Wang, X., Li, H.: 3d object detection for autonomous driving: A comprehensive   \n403 survey. IJCV (2023)   \n404 [37] Pang, S., Morris, D., Radha, H.: Clocs: Camera-lidar object candidates fusion for 3d object   \n405 detection. In: IROS (2020)   \n406 [38] Pang, S., Morris, D., Radha, H.: Fast-clocs: Fast camera-lidar object candidates fusion for 3d   \n407 object detection. In: WACV (2022)   \n408 [39] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,   \n409 Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning   \n410 library. In: NeurIPS (2019)   \n411 [40] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point   \n412 sets in a metric space. In: NeurIPS (2017)   \n413 [41] Qin, Z., Wang, J., Lu, Y.: Monogrnet: A geometric reasoning network for monocular 3d object   \n414 localization. In: AAAI (2019)   \n415 [42] Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., Li, H.: Pv-rcnn: Point-voxel feature set   \n416 abstraction for 3d object detection. In: CVPR (2020)   \n417 [43] Shi, S., Jiang, L., Deng, J., Wang, Z., Guo, C., Shi, J., Wang, X., Li, H.: Pv-rcnn $^{++}$ : Point-voxel   \n418 feature set abstraction with local vector representation for 3d object detection. IJCV (2022)   \n419 [44] Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection from point   \n420 cloud. In: CVPR (2019)   \n421 [45] Shi, S., Wang, Z., Shi, J., Wang, X., Li, H.: From points to parts: 3d object detection from point   \n422 cloud with part-aware and part-aggregation network. IEEE TPAMI (2020)   \n423 [46] Shi, W., Rajkumar, R.: Point-gnn: Graph neural network for 3d object detection in a point cloud.   \n424 In: CVPR (2020)   \n425 [47] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposi  \n426 tion for monocular 3d object detection. In: ICCV (2021)   \n427 [48] Sun, J., Chen, L., Xie, Y., Zhang, S., Jiang, Q., Zhou, X., Bao, H.: Disp r-cnn: Stereo 3d object   \n428 detection via shape prior guided instance disparity estimation. In: CVPR (2020)   \n429 [49] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141.,   \n430 Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)   \n431 [50] Vora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for 3d object   \n432 detection. In: CVPR (2020)   \n433 [51] Wang, C.Y., Liao, H.Y.M., Wu, Y.H., Chen, P.Y., Hsieh, J.W., Yeh, I.H.: Cspnet: A new   \n434 backbone that can enhance learning capability of cnn. In: CVPR workshops (2020)   \n435 [52] Wang, C., Ma, C., Zhu, M., Yang, X.: Pointaugmenting: Cross-modal augmentation for 3d   \n436 object detection. In: CVPR (2021)   \n437 [53] Wang, H., Shi, C., Shi, S., Lei, M., Wang, S., He, D., Schiele, B., Wang, L.: Dsvt: Dynamic   \n438 sparse voxel transformer with rotated sets. In: CVPR (2023)   \n439 [54] Wang, H., Tang, H., Shi, S., Li, A., Li, Z., Schiele, B., Wang, L.: Unitr: A unified and efficient   \n440 multi-modal transformer for bird\u2019s-eye-view representation. In: ICCV (2023)   \n441 [55] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d   \n442 object detection. In: ICCV (2021)   \n443 [56] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object   \n444 detection from multi-view images via 3d-to-2d queries. In: Robot Learning (2022)   \n445 [57] Wu, H., Wen, C., Shi, S., Li, X., Wang, C.: Virtual sparse convolution for multimodal 3d object   \n446 detection. In: CVPR (2023)   \n447 [58] Xie, Y., Xu, C., Rakotosaona, M.J., Rim, P., Tombari, F., Keutzer, K., Tomizuka, M., Zhan, W.:   \n448 Sparsefusion: Fusing multi-modal sparse representations for multi-sensor 3d object detection.   \n449 In: ICCV (2023)   \n450 [59] Xu, S., Zhou, D., Fang, J., Yin, J., Bin, Z., Zhang, L.: Fusionpainting: Multimodal fusion with   \n451 adaptive attention for 3d object detection. In: ITSC (2021)   \n452 [60] Yan, J., Liu, Y., Sun, J., Jia, F., Li, S., Wang, T., Zhang, X.: Cross modal transformer via   \n453 coordinates encoding for 3d object dectection. In: ICCV (2023)   \n454 [61] Yan, Y., Mao, Y., Li, B.: Second: Sparsely embedded convolutional detection. Sensors (2018)   \n455 [62] Yang, C., Chen, Y., Tian, H., Tao, C., Zhu, X., Zhang, Z., Huang, G., Li, H., Qiao, Y., Lu, L.,   \n456 et al.: Bevformer v2: Adapting modern image backbones to bird\u2019s-eye-view recognition via   \n457 perspective supervision. In: CVPR (2023)   \n458 [63] Yang, H., Zhang, S., Huang, D., Wu, X., Zhu, H., He, T., Tang, S., Zhao, H., Qiu, Q., Lin, B.,   \n459 He, X., Ouyang, W.: Unipad: A universal pre-training paradigm for autonomous driving. In:   \n460 CVPR (2024)   \n461 [64] Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: Ipod: Intensive point-based object detector for point   \n462 cloud. arXiv:1812.05276 (2018)   \n463 [65] Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: Std: Sparse-to-dense 3d object detector for point   \n464 cloud. In: ICCV (2019)   \n465 [66] Yang, Z., Chen, J., Miao, Z., Li, W., Zhu, X., Zhang, L.: Deepinteraction: 3d object detection   \n466 via modality interaction. In: NeurIPS (2022)   \n467 [67] Yin, J., Shen, J., Chen, R., Li, W., Yang, R., Frossard, P., Wang, W.: Is-fusion: Instance-scene   \n468 collaborative fusion for multimodal 3d object detection. In: CVPR (2024)   \n469 [68] Yin, T., Zhou, X., Krahenbuhl, P.: Center-based 3d object detection and tracking. In: CVPR   \n470 (2021)   \n471 [69] Yin, T., Zhou, X., Kr\u00e4henb\u00fchl, P.: Multimodal virtual point 3d detection. In: NeurIPS (2021)   \n472 [70] You, Y., Wang, Y., Chao, W.L., Garg, D., Pleiss, G., Hariharan, B., Campbell, M., Wein  \n473 berger, K.Q.: Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving.   \n474 arXiv:1906.06310 (2019)   \n475 [71] Zhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object detection.   \n476 In: CVPR (2018)   \n477 [72] Zhu, B., Jiang, Z., Zhou, X., Li, Z., Yu, G.: Class-balanced grouping and sampling for point   \n478 cloud 3d object detection. arXiv preprint arXiv:1908.09492 (2019)   \n479 [73] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers   \n480 for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "481 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "83 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n84 paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The claims are clearly stated and are consistent with the theoretical and experimental results presented in the paper. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: We discuss the limitations of our method, specifically that using an attentionbased approach to interact with the two modalities makes the detection results sensitive to modality loss. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "532 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: We provide detailed theoretical statements and formulas along with their descriptions in the paper. ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "549 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide a detailed experimental setup in the paper, and the training and testing details are provided in the supplementary material to ensure the reproducibility of our results. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 14}, {"type": "text", "text": "588 to have some path to reproducing or verifying the results.   \n589 5. Open access to data and code   \n590 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n591 tions to faithfully reproduce the main experimental results, as described in supplemental   \n592 material?   \n593 Answer: [No]   \n594 Justification: We release the experimental details in the paper, and the code will be released   \n595 after the paper is accepted.   \n596 Guidelines:   \n597 \u2022 The answer NA means that paper does not include experiments requiring code.   \n598 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n599 public/guides/CodeSubmissionPolicy) for more details.   \n600 \u2022 While we encourage the release of code and data, we understand that this might not be   \n601 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n602 including code, unless this is central to the contribution (e.g., for a new open-source   \n603 benchmark).   \n604 \u2022 The instructions should contain the exact command and environment needed to run to   \n605 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n606 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n607 \u2022 The authors should provide instructions on data access and preparation, including how   \n608 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n609 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n610 proposed method and baselines. If only a subset of experiments are reproducible, they   \n611 should state which ones are omitted from the script and why.   \n612 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n613 versions (if applicable).   \n614 \u2022 Providing as much information as possible in supplemental material (appended to the   \n615 paper) is recommended, but including URLs to data and code is permitted.   \n616 6. Experimental Setting/Details ", "page_idx": 15}, {"type": "text", "text": "Justification: We provide a detailed experimental setup in the paper, and the training and testing details are provided in the supplementary material. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "629 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "637 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n638 dence intervals, or statistical significance tests, at least for the experiments that support   \n639 the main claims of the paper.   \n640 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n641 example, train/test split, initialization, random drawing of some parameter, or overall   \n642 run with given experimental conditions).   \n643 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n644 call to a library function, bootstrap, etc.)   \n645 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n646 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n647 of the mean.   \n648 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n649 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n650 of Normality of errors is not verified.   \n651 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n652 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n653 error rates).   \n654 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n655 they were calculated and reference the corresponding figures or tables in the text.   \n656 8. Experiments Compute Resources   \n657 Question: For each experiment, does the paper provide sufficient information on the com  \n658 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n659 the experiments?   \n660 Answer: [Yes]   \n661 Justification: We provide hardware computer resources for training and testing.   \n662 Guidelines:   \n663 \u2022 The answer NA means that the paper does not include experiments.   \n664 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n665 or cloud provider, including relevant memory and storage.   \n666 \u2022 The paper should provide the amount of compute required for each of the individual   \n667 experimental runs as well as estimate the total compute.   \n668 \u2022 The paper should disclose whether the full research project required more compute   \n669 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n670 didn\u2019t make it into the paper).   \n671 9. Code Of Ethics   \n672 Question: Does the research conducted in the paper conform, in every respect, with the   \n673 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n674 Answer: [Yes]   \n675 Justification: The research conducted in our paper complies with NeurIPS ethical standards   \n676 in all aspects.   \n677 Guidelines:   \n678 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n679 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n680 deviation from the Code of Ethics.   \n681 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n682 eration due to laws or regulations in their jurisdiction).   \n683 10. Broader Impacts   \n684 Question: Does the paper discuss both potential positive societal impacts and negative   \n685 societal impacts of the work performed?   \n686 Answer: [Yes]   \n687 Justification: We discuss that although our method has good performance, practical applica  \n689 Guidelines:   \n690 \u2022 The answer NA means that there is no societal impact of the work performed.   \n691 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n692 impact or why the paper does not address societal impact.   \n693 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n694 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n695 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n696 groups), privacy considerations, and security considerations.   \n697 \u2022 The conference expects that many papers will be foundational research and not tied   \n698 to particular applications, let alone deployments. However, if there is a direct path to   \n699 any negative applications, the authors should point it out. For example, it is legitimate   \n700 to point out that an improvement in the quality of generative models could be used to   \n701 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n702 that a generic algorithm for optimizing neural networks could enable people to train   \n703 models that generate Deepfakes faster.   \n704 \u2022 The authors should consider possible harms that could arise when the technology is   \n705 being used as intended and functioning correctly, harms that could arise when the   \n706 technology is being used as intended but gives incorrect results, and harms following   \n707 from (intentional or unintentional) misuse of the technology.   \n708 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n709 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n710 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n711 feedback over time, improving the efficiency and accessibility of ML).   \n712 11. Safeguards   \n713 Question: Does the paper describe safeguards that have been put in place for responsible   \n714 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n715 image generators, or scraped datasets)?   \n716 Answer: [NA]   \n717 Justification: The model of the paper dos not address the issues mentioned in the guidelines.   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper poses no such risks.   \n720 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n721 necessary safeguards to allow for controlled use of the model, for example by requiring   \n722 that users adhere to usage guidelines or restrictions to access the model or implementing   \n723 safety filters.   \n724 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n725 should describe how they avoided releasing unsafe images.   \n726 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n727 not require this, but we encourage authors to take this into account and make a best   \n728 faith effort.   \n729 12. Licenses for existing assets   \n730 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n731 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n732 properly respected?   \n733 Answer: [Yes]   \n734 Justification: We have annotated the cited papers and datasets in our paper.   \n735 Guidelines:   \n736 \u2022 The answer NA means that the paper does not use existing assets.   \n737 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n738 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n739 URL.   \n740 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n741 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n742 service of that source should be provided.   \n743 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n744 package should be provided. For popular datasets, paperswithcode.com/datasets   \n745 has curated licenses for some datasets. Their licensing guide can help determine the   \n746 license of a dataset.   \n747 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n748 the derived asset (if it has changed) should be provided.   \n749 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n750 the asset\u2019s creators.   \n751 13. New Assets   \n752 Question: Are new assets introduced in the paper well documented and is the documentation   \n753 provided alongside the assets?   \n754 Answer: [NA]   \n755 Justification: The paper does not release new assets   \n756 Guidelines:   \n757 \u2022 The answer NA means that the paper does not release new assets.   \n758 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n759 submissions via structured templates. This includes details about training, license,   \n760 limitations, etc.   \n761 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n762 asset is used.   \n763 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n764 create an anonymized URL or include an anonymized zip file.   \n765 14. Crowdsourcing and Research with Human Subjects   \n766 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n767 include the full text of instructions given to participants and screenshots, if applicable, as   \n768 well as details about compensation (if any)?   \n769 Answer: [NA]   \n770 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n771 Guidelines:   \n772 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n773 human subjects.   \n774 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n775 tion of the paper involves human subjects, then as much detail as possible should be   \n776 included in the main paper.   \n777 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n778 or other labor should be paid at least the minimum wage in the country of the data   \n779 collector.   \n780 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n781 Subjects   \n782 Question: Does the paper describe potential risks incurred by study participants, whether   \n783 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n784 approvals (or an equivalent approval/review based on the requirements of your country or   \n785 institution) were obtained?   \n786 Answer: [NA]   \n787 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n790 human subjects. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "799 A Appendix ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "800 A.1 Additional Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "801 During training, we adopt a one-stage strategy like DAL [20]. The whole pipeline is trained for a   \n802 total of 20 epochs with the AdamW optimizer [35] loading from the pre-trained weights from the   \n803 ImageNet [11] classification task only. Meanwhile, we use CBGS [72] to resample the training data,   \n804 and the one-cycle learning policy with a maximum learning rate of $2.0\\times10^{-4}$ . The batch size is set   \n805 to 8 on 4 3090 RTX GPUs. We adopt random filpping along both X and Y-axis, the random scaling in   \n806 [0.95, 1.05], and random rotation in $[-\\pi/8,\\pi/8]$ to augment the LiDAR data, and the random rotation   \n807 in $[-5.4^{\\circ},5.4^{\\circ}]$ and random resizing in [-0.06, 0.44] to augment the images. During evaluation, we   \n808 test a single model without any data augmentation on a single 3090 RTX GPU. ", "page_idx": 20}, {"type": "text", "text": "809 A.2 Additional Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "810 A.2.1 3D Multi-Object Tracking Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "811 We evaluate our DH-Fusion on the nuScenes tracking benchmark for 3D multi-object tracking (MOT)   \n812 task. Following ObjectFusion [4], we adopt the same tracking-by-detection algorithm that uses   \n813 velocity-based closest point distance matching, which is more effective than 3D Kalman filter [9].   \n814 For fair comparisons, we report the results of our DH-Fusion-light capable of real-time detection   \n815 on the nuScenes validation set, as shown in Tab. 6. We find that our DH-Fusion-light outperforms   \n816 BEVFusion [33] and ObjectFusion [4] by $2.0\\,\\mathrm{pp}$ and 0.6 pp w.r.t. AMOTA. These results demonstrate   \n817 that our DH-Fusion provides 3D detection boxes of higher quality, beneftiing the downstream task of   \n818 3D MOT. ", "page_idx": 20}, {"type": "table", "img_path": "gBOQ0ACqoO/tmp/39b3c4f8ec6747ea4dfb0729ffc748c9b423c6e92ccd08b4e43896b9f0b02e0e.jpg", "table_caption": ["Table 6: Comparisons on nuScenes validation set for 3D multi-object tracking. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "819 A.2.2 Evaluation at Different Depths ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "820 Since our fusion strategy is depth-aware, it is necessary to validate our method at different depths.   \n821 Following [4], we categorize annotation and prediction ego distances into three groups: Near (0-   \n822 $20\\mathrm{m})$ , Middle $(20{-}30\\mathrm{m})$ , and Far $(>\\!30\\mathrm{m})$ . As shown in Tab. 7, compared to ObjectFusion [4], our   \n823 DH-Fusion-light consistently improves performance across all depth ranges. Specifically, our method   \n824 achieves a $47.1\\;\\mathrm{mAP}$ in the long range $(>\\!30\\mathrm{m})$ , surpassing ObjectFusion by $5.5\\,\\mathrm{pp}$ w.r.t. mAP. These   \n825 results indicate that our method is more effective across different depths, especially in detecting   \n826 distant objects. ", "page_idx": 20}, {"type": "text", "text": "Table 7: Comparisons on nuScenes validation set at different depths. The numbers are mAP. ", "page_idx": 20}, {"type": "table", "img_path": "gBOQ0ACqoO/tmp/62d4822bf4f0aaf8fe8e75f165e64bda2f14645d2d654c6ad658d0844a148cb5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "827 A.2.3 Detailed Results on the nuScenes-C ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "828 We further provide the detailed results of each fine-grained corruption on nuScenes-C in Tab. 8. The   \n829 results are highly consistent with the average values of each kind of data corruption. ", "page_idx": 20}, {"type": "text", "text": "830 A.3 More Visualization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "831 As an extension of Fig. 6 in the manuscript, we provide additional examples of 3D object detection   \n832 results and BEV features from our baseline, BEVFusion [33], and our DH-Fusion. In various   \n833 samples, our method consistently achieves higher accuracy and recall in 3D detection results, with   \n834 stronger feature responses for distant objects compared to BEVFusion. These results demonstrate the   \n835 effectiveness of the proposed method in dynamically adjusting the weights of features based on depth   \n836 during fusion at both global and local levels. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Table 8: Comparisons for each corruption level on the nuScenes-C. Corruptions exist in both modalities by default. (L) means that only the point cloud modality has corruptions, and (C) means that only the image modality has corruptions. Numbers are NDS / mAP. ", "page_idx": 21}, {"type": "table", "img_path": "gBOQ0ACqoO/tmp/7ae5e228dd9fdae67fb943473645f0b34b62359939fce8cade6af1092bc01236.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/02145010e448d8df1b7a3f2962f6a9e31fb81888a42ce736ae4df907dee4878b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "gBOQ0ACqoO/tmp/a398bf3a588c7b1dd50e27ed99dd03c4baa289e65779a38f440ebb0d7557f11c.jpg", "img_caption": ["Figure 7: More examples of 3D object detection results and BEV features from BEVFusion and ours. We show the ground truth boxes in green, and the prediction boxes in blue. We use red circles to highlight the comparisons of ours with BEVFusion. "], "img_footnote": [], "page_idx": 22}]