[{"figure_path": "Vvcnqs8091/tables/tables_5_1.jpg", "caption": "Table 1: Small constant values are ignored for bubbles and peak memory of V-Min and V-Half. For 1F1B, \u03b4\u2070/\u03b4\u00b9 are redefined as the offsets between adjacent forward/backward passes. M represents the total activation memory of the entire model, and d is the number of devices.", "description": "This table summarizes the peak memory and number of bubbles for different pipeline building blocks (1F1B, V-Min, V-Half, and V-ZB).  It shows how the proposed V-shaped building blocks achieve significant memory reduction compared to the baseline 1F1B, while also controlling the number of bubbles.  The parameters \u03b4\u2070 and \u03b4\u00b9 represent offsets within the building blocks.", "section": "3.2 V-Shape Pipeline Schedules"}, {"figure_path": "Vvcnqs8091/tables/tables_6_1.jpg", "caption": "Table 2: Models used in experiments.", "description": "This table presents the specifications of four different large language models (LLMs) used in the paper's experiments.  Each model is characterized by its size (in billions of parameters), the number of layers in its transformer architecture, the number of attention heads per layer, the hidden layer size, and the number of GPUs used for training. The models range in size from 9.6 billion to 98.5 billion parameters, reflecting a wide range of scales relevant to modern LLM research.", "section": "4.1 Setup"}, {"figure_path": "Vvcnqs8091/tables/tables_8_1.jpg", "caption": "Table 3: V-Shape schedules combined with other memory saving methods.", "description": "This table presents the best results obtained from grid search experiments combining V-shape schedules (V-Min, V-Half, V-ZB) with other optimization techniques (1F1B, 1F1B-R, ZB-1P) for different model sizes and sequence lengths.  The table shows the best achieved MFU (model FLOPS utilization), along with the corresponding hyperparameters: DP (data parallelism), TP (tensor parallelism), PP (pipeline parallelism), and microbatch size (mbs). It demonstrates the relative performance of different scheduling approaches under various resource constraints.", "section": "4.4 Combining with Existing Techniques"}, {"figure_path": "Vvcnqs8091/tables/tables_13_1.jpg", "caption": "Table 4: Comparing Pipeline Schedules", "description": "This table presents a detailed comparison of various pipeline schedules (V-ZB, ZB-1P, V-Half, 1F1B, V-Min, 1F1B-R) across three different model sizes (9.6B, 21B, 38.5B) and varying numbers of microbatches.  For each schedule and model size, it shows the samples per second per GPU, the FLOPS utilization (MFU), peak memory usage, activation memory usage, and bubble rate. The data provides a quantitative comparison of the memory efficiency and throughput of different pipeline scheduling strategies.", "section": "4.2 Comparing Pipeline Schedules"}, {"figure_path": "Vvcnqs8091/tables/tables_14_1.jpg", "caption": "Table 5: Single-pass MFU gain when increasing microbatch size", "description": "This table shows the single-pass FLOPS utilization (MFU) for forward (F), backward (B), and weight (W) passes when the microbatch size is increased.  It demonstrates how the MFU changes with different model sizes (9.6B, 21B, and 38.5B parameters) and varying microbatch sizes. The average MFU across F, B, and W passes is also provided for each configuration.", "section": "D.2 Single-pass MFU Gain When Increasing Microbatch Size"}, {"figure_path": "Vvcnqs8091/tables/tables_14_2.jpg", "caption": "Table 3: V-Shape schedules combined with other memory saving methods.", "description": "This table presents the best performance achieved by various pipeline parallel scheduling methods (1F1B, 1F1B-R, ZB-1P, V-Half, V-Min, and V-ZB) when combined with other optimization techniques such as Flash Attention, Tensor Parallelism, Sequence Parallelism, and Distributed Optimizer.  The results are shown for different model sizes (98.5B) and sequence lengths (1024, 3072, 16384), highlighting the impact of each method on model parallel throughput and the optimal hyperparameter settings for each method (DP, TP, PP, and microbatch size).", "section": "4.4 Combining with Existing Techniques"}, {"figure_path": "Vvcnqs8091/tables/tables_15_1.jpg", "caption": "Table 7: MFU of grid search, with SequenceLength = 3072 and BatchSize = 640", "description": "This table presents the results of a grid search experiment to find the best hyperparameter settings for different pipeline parallelism schedules.  It shows the Maximum FLOPS Utilization (MFU) achieved for various combinations of data parallelism (DP), tensor parallelism (TP), pipeline parallelism (PP), and microbatch size, using a sequence length of 3072 and batch size of 640.  The goal was to identify the optimal configuration for each method (1F1B, 1F1B-R, ZB-1P, V-Half, V-Min, and V-ZB) in terms of MFU.", "section": "4.4 Combining with Existing Techniques"}, {"figure_path": "Vvcnqs8091/tables/tables_15_2.jpg", "caption": "Table 3: V-Shape schedules combined with other memory saving methods.", "description": "This table shows the best MFU (Million FLOPs Utilization) achieved by different pipeline parallel scheduling methods (1F1B, 1F1B-R, ZB-1P, V-Half, V-Min, V-ZB) when combined with other memory saving techniques (DP, TP, PP) for different model sizes and sequence lengths. The best parameters for each method (DP, TP, PP, and microbatch size) are also shown.  It highlights the performance improvements of V-shape schedules, particularly V-Half and V-ZB, in various scenarios depending on memory pressure and model size.", "section": "4.4 Combining with Existing Techniques"}, {"figure_path": "Vvcnqs8091/tables/tables_18_1.jpg", "caption": "Table 9: Offsets for V-Min", "description": "This table shows the offsets used in the V-Min building block for different numbers of devices (d).  The offsets are used to control the memory consumption and bubble rate of the pipeline schedule.  The table shows that the offsets are designed to be different depending on whether the number of devices is a multiple of 3 or not. This ensures that there are no collisions between passes of the building blocks when they are repeated to form the pipeline. The specific values of the offsets ensure balanced peak memory across devices. ", "section": "3.2 V-Shape Pipeline Schedules"}, {"figure_path": "Vvcnqs8091/tables/tables_18_2.jpg", "caption": "Table 1: Small constant values are ignored for bubbles and peak memory of V-Min and V-Half. For 1F1B, \u03b4\u2070/\u03b4\u00b9 are redefined as the offsets between adjacent forward/backward passes. M represents the total activation memory of the entire model, and d is the number of devices.", "description": "This table compares three different pipeline building blocks (1F1B, V-Min, V-Half) in terms of their peak memory consumption and number of bubbles. It shows that V-Min and V-Half significantly reduce the activation memory compared to 1F1B, while V-Half achieves near-zero bubbles.  The table highlights the trade-off between memory usage and pipeline efficiency for these different approaches.", "section": "3.2 V-Shape Pipeline Schedules"}]