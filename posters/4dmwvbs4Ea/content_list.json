[{"type": "text", "text": "Offline RL via Feature-Occupancy Gradient Ascent ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We study offline Reinforcement Learning in large infinite-horizon discounted   \n2 Markov Decision Processes (MDPs) when the reward and transition models are   \n3 linearly realizable under a known feature map. Starting from the classic linear  \n4 program formulation of the optimal control problem in MDPs, we develop a new   \n5 algorithm that performs a form of gradient ascent in the space of feature occupan  \n6 cies, defined as the expected feature vectors that can potentially be generated by   \n7 executing policies in the environment. We show that the resulting simple algorithm   \n8 satisfies strong computational and sample complexity guarantees, achieved under   \n9 the least restrictive data coverage assumptions known in the literature. In particu  \n10 lar, we show that the sample complexity of our method scales optimally with the   \n1 desired accuracy level and depends on a weak notion of coverage that only requires   \n12 the empirical feature covariance matrix to cover a single direction in the feature   \n13 space (as opposed to covering a full subspace). Additionally, our method is easy   \n14 to implement and requires no prior knowledge of the coverage ratio (or even an   \n15 upper bound on it), which altogether make it the strongest known algorithm for   \n16 this setting to date. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 We study Offline Reinforcement Learning (ORL) in sequential decision making problems whereby   \n19 a learner aims to find a near-optimal policy with sole access to a static dataset of interactions with   \n20 the underlying environment [Levine et al., 2020]. This line of work is naturally relevant to real  \n21 world tasks for which learning an accurate simulator of the environment is potentially intractable   \n22 or impossible, trial-and-error learning could have grave consequences, yet logged interaction data   \n2 is readily available. For example, in a high-stake application such as autonomous driving, building   \n2 a sufficiently accurate simulator for the vehicle and its environment would require modelling very   \n2 complex systems, which can be intractable both statistically and computationally. At the same time,   \n2 running experiments in the real world could endanger the lives of other road users or result in damages   \n27 to the vehicle. Yet, with the advent of tools for efficient sensory-data collection and processing, large   \n28 volumes of logged data from human drivers are readily available.   \n29 An efficient ORL method is one which finds a near-optimal policy after a tractable number of   \n30 elementary computations and samples from the dataset. It is well-known in this setting that the quality   \n31 of the solution has to heavily depend on the quality of the data, and in particular one cannot hope   \n32 to find a near-optimal policy if the data covers the space of states and actions poorly. To formalize   \n33 this intuition, many notions of data coverage have been proposed in the offilne RL literature, ranging   \n34 from a very restrictive uniform coverage assumption that requires the data-generating policy to cover   \n35 the entire state-action space [Munos and Szepesv\u00e1ri, 2008] to a variety of partial coverage conditions   \n36 whereby this exploratory condition is only required for state-action pairs that are of interest to the   \n37 optimal policy [Liu et al., 2020, Rashidinejad et al., 2021, Uehara and Sun, 2021, Zhan et al., 2022,   \n38 Rashidinejad et al., 2022, Li et al., 2024]. In the present work, we study the setting of linear Markov   \n39 Decision Processes (MDPs) [Jin et al., 2020, Yang and Wang, 2019] where the reward and transition   \n40 matrix admit a low rank structure in terms of a known feature map, and data-coverage assumptions   \n41 can be defined in the space of features. As shown by [Zanette et al., 2021], in this setting it is possible   \n42 to obtain strong guarantees if the offilne data is well-aligned with the expectation of the feature vector   \n43 generated by the optimal policy (as opposed to requiring alignment with the entire distribution of   \n44 features as required by other common offline RL methods [Jin et al., 2021, Xie et al., 2021, Uehara   \n45 and Sun, 2021, Zhang et al., 2022]). In the present paper, we propose a simple and efficient algorithm   \n46 that yields the best known sample complexity guarantees for this problem setting, all while only   \n47 requiring the weakest known data-coverage assumptions of Zanette et al. [2021].   \n48 Our approach is based on the LP formulation of optimal control in infinite-horizon discounted MDPs   \n49 due to Manne [1960], and more specifically on its low-dimensional saddle-point reparametrization   \n50 for linear MDPs proposed by Gabbianelli et al. [2024] (which itself builds on earlier work by Neu   \n51 and Okolo, 2023 and Bas-Serrano et al., 2021). Primal variables of this saddle-point objective   \n52 correspond to expectations of feature vectors under the state-action distribution of each policy (called   \n53 feature occupancies), and dual variables correspond to parameters of linear approximations of action  \n54 value functions. We design an algorithm based on the idea of optimizing the unconstrained primal   \n55 function that is derived from the saddle-point objective by eliminating the dual variables via a classic   \n56 dualization trick. More precisely, we design a sample-based estimator of the primal function and   \n57 optimize it via a variant of gradient ascent in the space of feature occupancies.   \n58 This approach is to be contrasted with the method of Gabbianelli et al. [2024], which instead optimized   \n59 the original saddle-point objective via stochastic primal-dual methods. Their algorithm interleaved a   \n60 sequence of \u201cpolicy improvement\u201d steps with an inner loop performing \u201cpolicy evaluation\u201d, which   \n61 resulted in a suboptimal use of sample transitions due to the costly inner loop. This issue was   \n62 addressed in the very recent work of Hong and Tewari [2024] who, instead of relying on stochastic   \n63 optimization, built an estimator of the saddle-point objective and optimized it via a deterministic   \n64 primal-dual method. Our approach is directly inspired by their idea of estimating the saddle-point   \n65 objective, but our algorithm design is significantly simpler: instead of directly optimizing the   \n66 primal function in terms of feature occupancies, Hong and Tewari [2024] relied on a sophisticated   \n67 reparametrization of the primal variables, and used a computationally involved procedure to update   \n68 the dual variables. Both of these steps required prior knowledge of a tight bound on the feature  \n69 coverage ratio of the optimal policy, which is typically not available in problems of practical interest.   \n70 Such knowledge is not required by our algorithm, thanks to the incorporation of a recently proposed   \n71 stabilization trick that we make use of in our algorithm [Jacobsen and Cutkosky, 2023, Neu and   \n72 Okolo, 2024]. We provide a more detailed discussion of these closely related works in Section 5.   \n73 Notation. We use boldface lowercase letters $\\mathbf{\\nabla}m$ to denote vectors and and bold uppercase $_M$ for   \n74 matrices. We define the Euclidean ball in $\\mathbb{R}^{d}$ of radius $D$ by $\\mathbb{B}_{d}(D)=\\{\\pmb{x}\\in\\mathbb{R}^{d}|\\left\\|\\pmb{\\tilde{x}}\\right\\|_{2}\\leq D\\}$ and   \n75 the $A$ -simplex over a finite set $\\boldsymbol{\\mathcal{A}}$ of cardinality $A$ as $\\Delta_{\\mathcal{A}}=\\{p\\in\\mathbb{R}_{+}^{A}|\\,\\|p\\|_{1}=1\\}$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "76 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "77 We consider infinite-horizon Discounted Markov Decision Processes (DMDPs) [Puterman, 1994]   \n78 of the form $(\\mathcal{X},\\mathcal{A},r,P,\\gamma)$ where $\\mathcal{X}$ denotes a finite (yet large) set of $X$ states and $\\boldsymbol{\\mathcal{A}}$ is a finite   \n79 action space of cardinality A = |A|. We refer to r \u2208[0, 1]XA as the reward vector, P \u2208R+XA\u00d7X   \n80 the transition matrix and $\\gamma\\,\\in\\,(0,1)$ the discount factor. For a state-action pair $(x,a)\\in\\dot{x}\\times\\mathcal{A}$   \n81 we also use the notation $r\\left(x,a\\right)=r[\\left(x,a\\right)]$ to denote the reward of taking action $a$ in state $x$ and   \n82 $p\\left(x^{\\prime}|x,a\\right)=P[\\left(x,a\\right),x^{\\prime}]$ as the probability of ending up in state $x^{\\prime}$ afterwards.   \n83 The MDP models a sequential decision making process where an agent interacts with its environment   \n84 as follows. For each step $k=0,1,2,\\cdot\\cdot\\cdot$ ,, the agent observes the current state $X_{k}$ of the environment   \n85 and then goes on to select its action $A_{k}$ . Based on this action in the current state, it receives a reward   \n86 $r\\left(X_{k},A_{k}\\right)$ , transits to a new state $X_{k+1}\\sim p\\left(\\cdot|X_{k},A_{k}\\right)$ and the process continues. The objective of   \n87 the agent is to find a decision-making rule that maximizes its total discounted reward when the initial   \n88 state $X_{0}$ is sampled according to a fixed initial-state distribution distribution $\\nu_{0}\\in\\Delta_{\\mathcal{X}}$ . Without loss   \n89 of generality, we assume that the initial state is fixed almost surely as $X_{0}=x_{0}$ , and use $\\pmb{\\nu}_{\\mathrm{0}}$ to refer to   \n90 the corresponding delta distribution. It is known that this objective can be achieved by executing a   \n91 stationary stochastic policy $\\pi:\\mathcal{X}\\to\\Delta_{\\mathcal{A}}$ , with $\\pi(a|x)$ denoting the probability of the agent selecting   \n92 action $A_{k}=a$ in state $X_{k}=x$ for all $k$ . We will use $\\Pi$ to denote the set of all such behavior rules   \n93 and will often simply call them policies. We define the normalized discounted return of each policy $\\pi$   \n94 as ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho\\left(\\pi\\right)=(1-\\gamma)\\,\\mathbb{E}_{\\nu_{0},\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}r\\left(x_{k},a_{k}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "95 where the role of the discount factor $\\gamma\\in(0,1)$ is to emphasize the importance of earlier rewards, and   \n96 the notation $\\mathbb{E}_{\\nu_{0},\\pi}\\left[\\cdot\\right]$ highlights that the initial state is sampled from $\\nu_{0}$ and all actions are sampled   \n97 according to the policy $\\pi$ . We will use $\\pi^{*}$ to denote any policy that maximizes the return.   \n98 We will consider the offilne RL setting where we are given access to a data set of $n$ sample transitions   \n99 $\\mathcal{D}_{n}=\\{(X_{i},A_{i},R_{i},X_{i}^{\\prime})\\}_{i=1}^{n}$ , where $X_{i}^{\\prime}\\sim p(\\cdot|X_{i},A_{i})$ is sampled independently for each $i$ and $R_{i}={}$   \n100 $r(X_{i},A_{i})$ . Otherwise, no assumption is made about the state-action pairs $(X_{i},A_{i})$ , and in particular   \n101 we do not require these to be generated by a fixed behavior policy or to be independent of each other.   \n102 For describing the approach we take towards solving this problem, we need to introduce some   \n103 further standard notations. The value function and action-value function associated with policy $\\pi$ are   \n104 respectively defined as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nv^{\\pi}\\left(x\\right)=\\mathbb{E}_{a\\sim\\pi\\left(\\cdot\\vert x\\right)}\\left[q^{\\pi}\\left(x,a\\right)\\right],\\quad q^{\\pi}\\left(x,a\\right)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}r\\left(x_{k},a_{k}\\right)\\right]x_{0}=x,a_{0}=a\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 and the state-occupancy and state-action-occupancy measures under $\\pi$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nu^{\\pi}\\left(x\\right)=\\sum_{a}\\mu^{\\pi}\\left(x,a\\right),\\quad\\mu^{\\pi}\\left(x,a\\right)=\\left(1-\\gamma\\right)\\mathbb{E}_{\\nu_{0},\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}\\mathbb{I}_{\\left\\{x_{k},a_{k}\\right\\}}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "106 The value functions and occupancy measures adhere to the following recursive equations, respectively   \n107 termed the Bellman equation and Bellman flow condition [Bellman, 1966]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{q}^{\\pi}=\\pmb{r}+\\gamma\\pmb{P}\\pmb{v}^{\\pi},\\qquad\\pmb{\\mu}^{\\pi}=\\pi\\circ[(1-\\gamma)\\,\\pmb{\\nu}_{0}+\\gamma\\pmb{P}^{\\top}\\pmb{\\mu}^{\\pi}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 Here, the composition operation $\\circ$ is defined so that for any policy $\\pi$ and state distribution $\\pmb{\\nu}\\in\\mathbb{R}^{X}$ ,   \n109 we have $\\left(\\pi\\circ\\pmb{\\nu}\\right)\\left(x,a\\right)=\\pi\\left(a|x\\right)\\nu\\left(x\\right)$ . Notice that we can express the return of $\\pi$ in terms of value   \n110 functions and occupancy measures as $\\dot{\\rho}\\left(\\pi\\right)=\\left(1-\\gamma\\right)\\langle\\pmb{\\nu}_{0},\\pmb{v}^{\\pi}\\rangle=\\langle\\pmb{\\mu}^{\\pi},\\pmb{r}\\rangle$ . On this note, for a given   \n111 target accuracy $\\varepsilon>0$ , we say policy $\\pi$ is $\\varepsilon$ -optimal if it satisfies $\\left\\langle\\pmb{\\mu}^{\\pi^{*}}-\\pmb{\\mu}^{\\pi},\\pmb{r}\\right\\rangle\\leq\\varepsilon$ .   \n112 In the present work, we well make use of the linear $M D P$ assumption due to Jin et al. [2020], Yang   \n113 and Wang [2019], which is defined formally as follows:   \n114 Definition 2.1 (Linear MDP). An MDP is called linear if both the transition and reward functions   \n115 can be expressed as a linear function of a given feature map $\\varphi:\\mathcal{X}\\times\\mathcal{A}\\to\\mathbb{R}^{d}$ . That is, there exist   \n116 $\\psi:\\mathcal{X}\\rightarrow\\mathbf{\\^{\\ast}}\\mathbb{R}^{d}$ and $\\omega\\in\\mathbb{R}^{d}$ such that, for every $x,x^{\\prime}\\in\\mathcal{X}$ and $a\\in{\\mathcal{A}}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nr(x,a)=\\langle\\varphi(x,a),\\omega\\rangle,\\:\\:\\:\\:\\:p\\left(x^{\\prime}|x,a\\right)=\\langle\\varphi(x,a),\\psi(x^{\\prime})\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "117 We denote by $\\Phi\\in\\mathbb{R}^{|\\mathcal{X}\\times\\mathcal{A}|\\times d}$ the feature matrix with rows given by $\\varphi(x,a)^{\\tau}$ and $\\Psi\\in\\mathbb{R}^{d\\times|\\mathcal{X}|}$ as t\u221ahe   \n118 weight matrix with columns $\\psi(x)$ . Further, we will assume that $\\|\\omega\\|_{2}\\leq{\\sqrt{d}}$ , that $\\|\\Psi\\pmb{v}\\|_{2}\\leq B\\sqrt{d}$   \n119 holds for all $\\pmb{v}\\in[-B,B]$ , and that all feature vectors satisfy $\\left|\\left|\\varphi(x,a)\\right|\\right|_{2}\\leq R$ for some $R\\geq1$ .   \n120 An immediate consequence of this assumption is that the action-value function of any policy $\\pi$ can   \n121 be written as a linear function of the features as $\\pmb q^{\\pi}=\\pmb{\\Phi}\\pmb{\\theta}^{\\pi}$ , with $\\pmb{\\theta}^{\\pi}=\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}^{\\pi}\\in\\mathbb{R}^{d}$ . For the   \n122 rest of the paper we explicitly assume that the feature matrix $\\Phi$ is full rank \u2013 which is enough to   \n123 ensure uniqueness of $\\theta^{\\pi}$ . It is common to assume that the feature dimension $d\\ll X$ such that the   \n124 transition operator is low-rank. As common in this setting, we will suppose throughout the paper that   \n125 the feature map $\\Phi$ is known.   \n126 Our algorithm design will be based on the linear programming formulation of MDPs, first proposed   \n127 in a number of papers in the 1960\u2019s [Manne, 1960, de Ghellinck, 1960, d\u2019Epenoux, 1963, Denardo,   \n128 1970]. This formulation frames the problem of finding an optimal control policy as the following pair   \n129 of primal and dual linear programs: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "130 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{maximize}\\quad\\langle\\mu,r\\rangle}\\\\ &{\\mathrm{subject~to}\\quad E^{\\top}\\mu=(1-\\gamma)\\nu_{0}+\\gamma P^{\\top}\\mu\\quad(1)\\qquad\\left\\{\\qquad\\begin{array}{r l}&{\\mathrm{minimize}\\quad(1-\\gamma)\\langle\\nu_{0},v\\rangle}\\\\ &{\\mathrm{subject~to}\\quad E v\\geq r+\\gamma P v.}\\end{array}\\right.}\\\\ &{\\mu\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "131 Here, the operator $\\pmb{{\\cal E}}\\in\\mathbb{R}^{X A\\times X}$ is defined such that for each $x,a$ and vectors $\\pmb{\\mu}\\in\\mathbb{R}^{X A},\\pmb{v}\\in\\mathbb{R}^{X}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(E^{\\top}\\mu\\right)\\left(x\\right)=\\sum_{a\\in{\\cal{A}}}\\mu\\left(x,a\\right),\\qquad\\left(E v\\right)\\left(x,a\\right)=v\\left(x\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 It is known that the occupancy measure of an optimal policy $\\pmb{\\mu}^{\\pi^{*}}$ is an optimal solution of the primal   \n133 LP (1). In fact, the feasible set of the primal is precisely the space of valid state-action occupancy   \n134 measures that can be induced by stationary policies. Therefore, given any feasible solution $\\pmb{\\mu}$ , we can   \n135 extract the inducing policy as $\\begin{array}{r}{\\dot{\\pi}_{\\mu}\\left(a|x\\right)=\\dot{\\mu}\\left(x,a\\right)/\\sum_{a^{\\prime}}\\mu\\left(x,\\bar{a^{\\prime}}\\right)}\\end{array}$ when $\\textstyle\\sum_{a}\\mu\\left(x,a\\right)\\neq0$ . Likewise,   \n136 the state value function of the optimal policy $\\pi^{*}$ is an  optimal solution to  the dual LP. That said, since   \n137 the LP features $X A$ variables and constraints, it cannot be solved directly in large MDPs.   \n138 In view of the above limitations, we consider the following reduced version of the above intractable   \n139 LPs due to Gabbianelli et al. [2024] (see also Neu and Okolo, 2023, Bas-Serrano et al., 2021): ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "140 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l l}{\\mathrm{maximize}}&{\\langle\\lambda,\\omega\\rangle}\\\\ {\\mathrm{subject~to}}&{E^{\\top}\\mu=(1-\\gamma)\\nu_{0}+\\gamma\\Psi^{\\top}\\lambda}\\\\ &{\\lambda=\\Phi^{\\top}\\mu}\\\\ &{\\mu\\geq0,}\\end{array}\\qquad\\left|\\begin{array}{l l}&{\\mathrm{minimize}\\quad(1-\\gamma)\\langle\\nu_{0},{\\boldsymbol v}\\rangle}\\\\ &{\\mathrm{subject~to}\\quad E v\\geq\\Phi\\theta}\\\\ &{\\theta=\\omega+\\gamma\\Psi v.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 In view of the second constraint of the primal LP (3), $\\lambda$ should be thought of as expectations of feature   \n142 vectors under occupancy measures, and we thus refer to them as feature occupancy vectors. Similarly,   \n143 the second constraint of the dual LP (4) suggests that $\\pmb{\\theta}$ should be thought of as parameters of the   \n144 approximate action-value function $\\begin{array}{r}{\\pmb{q}_{\\theta}=\\Phi\\pmb{\\theta}=\\Phi\\left(\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}\\right)=\\pmb{r}+\\gamma\\pmb{P v}}\\end{array}$ . We use $\\lambda^{\\pi^{*}}=\\Phi^{\\intercal}\\mu^{\\pi^{*}}$ to   \n145 denote the feature occupancy associated with the optimal policy $\\pi^{*}$ and $\\pmb{\\theta}^{\\pi^{*}}$ to denote the parameter  \n146 vector of the optimal action-value function $\\boldsymbol{q}^{\\pi^{*}}$ . The Lagrangian corresponding to the LPs is given as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}(\\lambda,\\mu;v,\\pmb{\\theta})=(1-\\gamma)\\langle\\pmb{\\nu}_{0},\\pmb{v}\\rangle+\\langle\\lambda,\\omega+\\gamma\\Psi v-\\pmb{\\theta}\\rangle+\\langle\\pmb{\\mu},\\Phi\\pmb{\\theta}-E v\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\lambda,\\omega\\rangle+\\langle\\pmb{v},(1-\\gamma)\\pmb{\\nu}_{0}+\\gamma\\Psi^{\\intercal}\\lambda-E^{\\intercal}\\mu\\rangle+\\langle\\pmb{\\theta},\\Phi^{\\intercal}\\mu-\\lambda\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 It is easy to verify that by the linear MDP property, the feasible sets of the above LPs coincide   \n148 with those of the original LPs in an appropriate sense, and their optimal solutions correspond to   \n149 the optimal state-action occupancy measure and state-value function respectively (see Appendix A).   \n150 In order to further reduce the complexity of the LPs above, we introduce a policy $\\pi$ and parametrize   \n151 the remaining high-dimensional variables $\\pmb{v}$ and $\\pmb{\\mu}$ as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{\\theta,\\pi}(s)=\\sum_{a}\\pi(a|s)\\left<\\theta,\\varphi(x,a)\\right>,\\qquad\\mu_{\\lambda,\\pi}(x,a)=\\pi(a|x)\\Bigl[(1-\\gamma)\\nu_{0}(x)+\\gamma\\langle\\psi(x),\\lambda\\rangle\\Bigr].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "152 Plugging this choice back into the Lagrangian, we obtain the objective ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda,\\pi;\\pmb\\theta)=\\mathfrak{L}(\\lambda,\\mu_{\\lambda,\\pi};v_{\\theta,\\pi},\\pmb\\theta)}\\\\ &{\\qquad\\qquad=(1-\\gamma)\\langle\\pmb\\nu_{0},\\pmb v_{\\theta,\\pi}\\rangle+\\langle\\lambda,\\omega+\\gamma\\pmb\\Psi v_{\\theta,\\pi}-\\pmb\\theta\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\lambda,\\omega\\rangle+\\langle\\pmb\\theta,\\pmb\\Phi^{\\intercal}\\mu_{\\lambda,\\pi}-\\lambda\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 It is easy to see that for any $\\pi$ and $\\lambda^{\\pi}\\;=\\;\\Phi^{\\top}\\mu^{\\pi}$ , we have $f(\\pmb{\\lambda}^{\\pi},\\pi;\\pmb{\\theta})\\;=\\;\\langle\\pmb{\\mu}^{\\pi},\\pmb{r}\\rangle$ for all $\\pmb\\theta\\ \\in$   \n154 $\\mathbb{R}^{d}$ . Furthermore, whenever $\\lambda\\ne\\,\\lambda^{\\pi}$ then the $\\pmb{\\theta}$ -player has a winning strategy that can force   \n155 min\u03b8 $f(\\lambda,\\pi;\\pmb{\\theta})=-\\infty$ . This (informally) suggests that an optimal policy can be found by solving the   \n156 unconstrained saddle-point optimization problem $\\begin{array}{r}{\\iota\\operatorname*{max}_{\\pmb{\\lambda}\\in\\mathbb{R}^{d},\\pi\\in\\Pi}\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{d}}f(\\pmb{\\lambda},\\pi;\\pmb{\\theta})}\\end{array}$ . Furthermore,   \n157 since the optimal policy can be written as I{a=argmaxb\u27e8\u03b8\u03c0\u2217,\u03c6(x,b)\u27e9}, it is sufficient to   \n158 consider softmax policies of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Pi\\left(D_{\\pi}\\right)=\\left\\{\\pi_{\\theta}\\left(a|x\\right)=\\frac{e^{\\langle\\varphi\\left(x,a\\right),\\theta\\rangle}}{\\sum_{a^{\\prime}}e^{\\langle\\varphi\\left(x,a^{\\prime}\\right),\\theta\\rangle}}\\,\\bigg|\\,\\theta\\in\\mathbb{B}_{d}(D_{\\pi})\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "159 which can approximate $\\pi^{*}$ to good precision when the diameter $D_{\\pi}$ is set to be large enough. This   \n160 parametrization effectively reduces the high-dimensional LP into a low-dimensional saddle-point   \n161 optimization problem. ", "page_idx": 3}, {"type": "text", "text": "162 3 Feature-occupancy gradient ascent for offline RL in linear MDPs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 A natural idea for developing RL methods is to build an empirical approximation of the function $f$   \n164 defined in the previous section, and use primal-dual methods to find a saddle-point of the resulting   \n165 approximation. For offline RL, this approach has been explored by Gabbianelli et al. [2024] and   \n166 Hong and Tewari [2024]. In this work, we develop an alternative approach that seeks to directly   \n167 optimize the return by approximately maximizing the unconstrained primal function $f^{*}:\\mathbb{R}^{d}\\times\\dot{\\Pi}$ ,   \n168 defined for each feature-occupancy vector $\\lambda$ and policy $\\pi$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf^{*}(\\pmb{\\lambda},\\pi)=\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{B}_{d}(D_{\\pmb{\\theta}})}f(\\pmb{\\lambda},\\pi;\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "169 for an appropriately chosen feasible set $\\mathbb{B}_{d}(D_{\\theta})$ . Given the discussion in the previous section, maxi  \n170 mizing this function with respect to $\\lambda$ and $\\pi$ is rightly expected to result in an optimal policy (which   \n171 intuition will be made formal in our analysis). Notably, the so-called objective $f$ in Equation (7)   \n172 depends on the transition weight matrix $\\Psi$ which is unknown in general. As we soon show, this   \n173 matrix dominates the loss of the $\\pmb{\\theta}$ -player and $\\lambda$ -player. Based on these observations, our approach   \n174 consists of building a well-chosen estimator $\\widehat{f}$ of $f$ , and then maximizing the associated primal   \n175 function ${\\widehat{f}}^{*}$ defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\widehat{f}}^{*}(\\lambda,\\pi)=\\operatorname*{min}_{\\theta\\in\\mathbb{B}_{d}(D_{\\theta})}{\\widehat{f}}(\\lambda;\\theta,\\pi).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 The objective $\\widehat{f}$ is built via a least-squares estimator inspired by the classic LSTD model estimate   \n177 of Bradtke and Barto [1996], Parr et al. [2008], which has been successfully used for analyzing   \n178 finite-horizon linear MDPs in a variety of recent works (e.g., Jin et al., 2020, Neu and Pike-Burke,   \n179 2020). In particular, we fit an estimator $\\widehat{\\Psi}$ of the true matrix $\\Psi$ using samples from the dataset   \n180 $\\mathcal{D}_{n}=\\{(\\bar{X_{i}},A_{i},R_{i},X_{i}^{\\prime})\\}_{i=1}^{n}$ as follows. Let $\\varphi_{i}=\\varphi(X_{i},A_{i})$ denote the feature vector of $(X_{i},A_{i})$   \n181 and $\\begin{array}{r}{\\mathbf{{A}}_{n}=\\beta I_{n}+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\varphi_{i}^{\\intercal}}\\end{array}$ the empirical feature covariance matrix. We define the regularized   \n182 least squares estimate of $\\Psi$ at $x\\in\\mathscr{X}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\psi}\\left(x\\right)=\\operatorname*{arg\\,min}_{\\psi\\left(x\\right)\\in\\mathbb{R}^{d}}\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\left\\langle\\varphi_{i},\\psi\\left(x\\right)\\right\\rangle-\\mathbb{I}_{\\left\\{x=X_{i}^{\\prime}\\right\\}}\\right)^{2}+\\beta\\left\\Vert\\psi\\left(x\\right)\\right\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 so that the estimate can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\Psi}}=\\sum_{x\\in\\mathcal{X}}\\widehat{\\psi}(x)\\pmb{e}_{x}^{\\intercal}=\\frac{1}{n}\\pmb{\\Lambda}_{n}^{-1}\\sum_{i=1}^{n}\\varphi_{i}\\pmb{e}_{X_{i}^{\\prime}}^{\\intercal}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 With this matrix at hand, we define $\\widehat{f}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{f}(\\lambda,\\pi;\\theta)=(1-\\gamma)\\langle\\nu_{0},v_{\\theta,\\pi}\\rangle+\\langle\\lambda,\\omega+\\gamma\\widehat{\\Psi}v_{\\theta,\\pi}-\\theta\\rangle=\\langle\\lambda,\\omega\\rangle+\\langle\\theta,\\Phi^{\\top}\\widehat{\\mu}_{\\lambda,\\pi}-\\lambda\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{\\mu}_{\\lambda,\\pi}(x,a)=\\pi(a|x)\\Big[(1-\\gamma)\\nu_{0}(x)+\\gamma\\langle\\widehat{\\psi}(x),\\lambda\\rangle\\Big]$ is a sample-based approximation of $\\mu_{\\lambda,\\pi}$ ", "page_idx": 4}, {"type": "text", "text": "186 For the purpose of optimization, we will employ appropriately chosen versions of mirror as  \n187 cent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] to iteratively optimize the pri  \n188 mal variables. Denoting the iterates for each $t\\,=\\,1,2,\\ldots,T$ by $\\lambda_{t}$ and $\\pi_{t}$ , and defining $\\theta_{t}\\,=$   \n189 arg $\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{B}_{d}(D_{\\pmb{\\theta}})}\\widehat{f}(\\pmb{\\lambda}_{t},\\bar{\\pi}_{t};\\pmb{\\theta})$ , the updates are defined as follows. Using $g_{\\lambda}(t)=\\nabla_{\\lambda_{t}}\\widehat{f}^{*}(\\bar{\\lambda_{t}},\\pi_{t})$ to   \n190 denote the gradient of ${\\widehat{f}}^{*}$ with respect to the feature occupancies, the first set of variables is updated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{t+1}=\\operatorname*{\\arg\\operatorname*{max}}_{\\lambda\\in\\mathbb{R}^{d}}\\Big\\{\\langle\\lambda,g_{\\lambda}(t)\\rangle-\\frac{1}{2\\eta}\\left\\Vert\\lambda-\\lambda_{t}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\left\\Vert\\lambda\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 where the first regularization term acts as proximal regularization (necessary for mirror-ascent-style   \n192 methods), and the second one has a stabilization effect whose role will be made clear later in the   \n193 analysis. The resulting update can be written in closed form, and is equivalent to a preconditioned   \n194 gradient-ascent step on $\\bar{f^{*}}$ . The policies are updated in each state-action pair $x,a$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{t+1}(a|x)=\\frac{\\pi_{t}(a|x)e^{\\alpha\\langle\\varphi(x,a),\\theta_{t}\\rangle}}{\\sum_{a^{\\prime}}\\pi_{t}(a^{\\prime}|x)e^{\\alpha\\langle\\varphi(x,a^{\\prime}),\\theta_{t}\\rangle}}=\\frac{\\pi_{1}(a|x)e^{\\alpha\\langle\\varphi(x,a),\\sum_{k=1}^{t}\\theta_{k}\\rangle}}{\\sum_{a^{\\prime}}\\pi_{1}(a^{\\prime}|x)e^{\\alpha\\langle\\varphi(x,a^{\\prime}),\\sum_{k=1}^{t}\\theta_{k}\\rangle}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Input: Learning rates $\\alpha,\\varrho,\\eta$ , initial points $\\lambda_{1}\\in\\mathbb{R}^{d},\\pi_{1}\\in\\Pi\\left(D_{\\pi}\\right),\\bar{\\theta}_{0}=\\mathbf{0}$ , and dataset $\\mathcal{D}_{n}$ .   \nfor $t=1$ to $T$ do // Value-parameter update Compute $\\begin{array}{r l}&{\\overline{{\\Phi^{\\tau}\\hat{\\mu}_{\\lambda_{t},\\pi_{t}}}}=(1-\\gamma)\\sum_{a}\\pi_{t}(a|x_{0})\\varphi(x_{0},a)+\\gamma\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{a}\\pi_{t}(a|X_{i}^{\\prime})\\varphi(X_{i}^{\\prime},a)\\left\\langle\\varphi_{i},\\Lambda_{n}^{-1}\\lambda_{t}\\right\\rangle}\\\\ &{\\theta_{t}=\\,\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{B}_{d}(D_{\\theta})}\\langle\\theta,\\Phi^{\\tau}\\hat{\\mu}_{\\lambda_{t},\\pi_{t}}-\\lambda_{t}\\rangle}\\end{array}$ // Policy update Update $\\bar{\\pmb{\\theta}}_{t}^{\\,^{\\star}}=\\bar{\\pmb{\\theta}}_{t-1}+\\pmb{\\theta}_{t}$ $\\bar{\\pi_{t+1}}=\\sigma\\left(\\alpha\\Phi\\bar{\\pmb{\\theta}_{t}}\\right)$ // Feature-occupancy update Compute\u03a8 v\u03b8t,\u03c0t = $\\begin{array}{r}{\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}^{-}=\\frac{1}{n}\\pmb{\\Lambda}_{n}^{-1}\\sum_{i=1}^{n}\\varphi_{i}v_{\\theta_{t},\\pi_{t}}(X_{i}^{\\prime})}\\end{array}$ Compute $\\pmb{g}_{\\pmb{\\lambda}}(t)=\\pmb{\\omega}+\\gamma\\widehat{\\pmb{\\Psi}}\\pmb{v}_{\\theta_{t},\\pi_{t}}-\\pmb{\\theta}_{t}$ $\\begin{array}{r}{\\lambda_{t+1}=\\frac{1}{1+\\varrho\\eta}\\left(\\lambda_{t}+\\eta\\Lambda_{n}\\pmb{g}_{\\pmb{\\lambda}}(t)\\right)}\\end{array}$   \nend for   \nreturn $\\pi_{J}$ with $J\\sim\\mathcal{U}(1,\\cdot\\cdot\\cdot\\,,T)$ . ", "page_idx": 5}, {"type": "text", "text": "195 corresponding to performing an entropy-regularized mirror ascent step in each state $x$ (cf. Neu et al.,   \n196 2017). We use the shorthand notation $\\begin{array}{r}{\\pi_{t+1}\\,=\\,\\sigma\\big(\\alpha\\Phi\\sum_{k=1}^{t}\\pmb{\\theta}_{k}\\big)}\\end{array}$ to denote the resulting softmax   \n197 policy, and note that it is fully specified by a $d$ -dimensional vector that can be stored compactly.   \n198 After the final iterate is computed, the algorithm picks the index $J$ uniformly at random and outputs   \n199 the policy $\\pi_{J}$ . We refer to the resulting algorithm as Feature-Occupancy Gradient AScent (FOGAS),   \n200 and present its detailed pseudocode featuring the explicit expressions of $\\lambda_{t}$ and $\\theta_{t}$ as Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "201 The following theorem states our main result regarding the performance of FOGAS. ", "page_idx": 5}, {"type": "text", "text": "202 Theorem 3.1. Let $\\pi_{1}$ be the uniform policy and $\\lambda_{1}=\\mathbf{0}$ . Also set $D_{\\theta}=\\sqrt{d}/\\left(1-\\gamma\\right)$ , $D_{\\pi}=\\alpha T D_{\\theta}$   \n203 and $\\delta>0.$ . Suppose that we run FOGAS for $\\begin{array}{r}{T\\geq\\frac{2R^{2}n\\log A}{\\log\\left(1/\\delta\\right)}}\\end{array}$ rounds with parameters $\\beta=R^{2}/d T$ as   \n204 well as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha=\\sqrt{\\frac{2\\left(1-\\gamma\\right)^{2}\\log A}{R^{2}d T}},\\;\\;\\;\\varrho=\\gamma\\sqrt{\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{\\left(1-\\gamma\\right)^{2}n}},\\;\\;\\;\\eta=\\sqrt{\\frac{\\left(1-\\gamma\\right)^{2}}{27R^{2}d^{2}T}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "205 Then, with probability at least $1-\\delta$ , the following bound is satisfied for any comparator policy $\\pi^{*}$   \n206 and the associated feature-occupancy vector $\\lambda^{\\pi^{*}}\\,\\bar{=}\\,\\Phi^{\\top}\\mu^{\\pi^{*}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right\\rangle\\right]=\\mathcal{O}\\left(\\frac{\\left\\lVert\\lambda^{\\pi^{*}}\\right\\rVert_{\\Lambda_{n}^{-1}}^{2}+1}{1-\\gamma}\\cdot\\sqrt{\\frac{d^{2}\\log{(2T/\\delta)}}{n}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 with the expectation taken with respect to the random index $J$ . ", "page_idx": 5}, {"type": "text", "text": "208 The most important factor in the bound of Theorem 3.1 is $\\|\\pmb{\\lambda}^{*}\\|_{\\pmb{\\Lambda}_{n}^{-1}}^{2}$ , which measures the extent   \n209 to which the data $\\mathcal{D}_{n}$ covers the comparator policy $\\pi^{*}$ in feature space. We accordingly refer to   \n210 this quantity as the feature coverage ratio between the policy $\\pi^{*}$ and the data set $\\mathcal{D}_{n}$ , and we   \n211 discuss its relationship with other notions of data coverage in Section 5. Notably, the bound holds   \n212 simultaneously for all comparator policies $\\pi^{*}$ , and thus it can be restated in an oracle-inequality form.   \n213 On the same note, FOGAS does not need any prior upper bounds on the comparator norm $\\left\\|\\lambda^{*}\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}$ ,   \n214 and in particular it does not project the iterates $\\lambda_{t}$ to a bounded set. These nontrivial properties are   \n215 enabled by a recently proposed stabilization trick due to Jacobsen and Cutkosky [2023] and Neu and   \n216 Okolo [2024], which amounts to augmenting the standard mirror-ascent update of Equation (9) with   \n217 the regularization term $\\frac{\\varrho}{2}\\left\\|\\pmb{\\lambda}\\right\\|_{\\pmb{\\Lambda}_{n}^{-1}}^{2}$ . Without this additional regularization, the bounds would feature   \n218 an additional factor of the order $\\!\\!\\!\\!\\!\\!\\frac{\\!\\!\\!\\!\\!1}{T}\\sum_{t=1}^{T}\\|\\pmb{\\lambda}_{t}\\|_{\\pmb{\\Lambda}_{n}^{-1}}^{2}$ , which cannot be controlled without projecting the   \n219 iterates and in any case make it impossible to prove a comparator-adaptive bound. We defer further   \n220 discussion of the result to Section 5. ", "page_idx": 5}, {"type": "text", "text": "221 4 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "222 This section is dedicated to proving our main result, Theorem 3.1. While we have defined FOGAS as a   \n223 \u201cprimal-only\u201d algorithm above, its analysis will be most convenient if we regard it as a primal-dual   \n224 algorithm with implicitly defined dual updates. In particular, we will view the updates of FOGAS   \n225 as a sequence of steps in a zero sum game between two teams of players: the max players that   \n226 control $\\lambda_{t}$ and $\\pi_{t}$ , and the min player that picks $\\pmb{\\theta}_{t}$ . The min player uses the simple best-response   \n227 strategy of picking $\\pmb{\\theta}_{t}=\\mathrm{\\arg\\,min}_{\\pmb{\\theta}\\in\\mathbb{B}_{d}\\left(D_{\\pmb{\\theta}}\\right)}\\,\\bar{\\hat{f}}(\\pmb{\\lambda},\\pi_{t})$ , and the other two players perform their updates   \n228 via appropriate versions mirror ascent on t heir respective objectives. Importantly, the updates of the   \n229 $\\lambda$ -player are based on the gradients of ${\\widehat{f}}^{*}$ , which satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{g}_{\\lambda}(t)=\\nabla_{\\lambda_{t}}\\widehat{f}^{*}(\\lambda_{t},\\pi_{t})=\\nabla_{\\lambda_{t}}\\left(\\operatorname*{min}_{\\theta\\in\\mathbb{B}_{d}(D_{\\theta})}\\widehat{f}(\\lambda_{t},\\pi_{t};\\theta)\\right)=\\nabla_{\\lambda_{t}}\\widehat{f}(\\lambda_{t},\\pi_{t};\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "230 where the last equality follows from an application of Danskin\u2019s theorem. This property enables   \n231 a major conceptual simplification that allows the interpretation of the updates as optimizing the   \n232 unconstrained primal ${\\widehat{f}}^{*}$ directly. We refer the interested reader to Chapter 6 of Bertsekas [1997] for   \n233 more context on such use of primal-dual analysis.   \n234 More concretely, we make use of an analysis technique first developed by Neu and Okolo [2023],   \n235 and further refined by Gabbianelli et al. [2024] and Hong and Tewari [2024]. The core idea is to   \n236 introduce the dynamic duality gap defined on a sequence of iterates $\\{(\\bar{\\lambda}_{t},\\pi_{t},\\bar{\\pmb{\\theta}}_{t})\\}_{t=1}^{T}$ produced by   \n237 some iterative method, and a set of well-chosen comparators $\\left(\\lambda^{*},\\pi^{*};\\{\\theta_{t}^{*}\\}_{t=1}^{T}\\right)$ as ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{T}\\left(\\lambda^{*},\\pi^{*};\\{\\pmb{\\theta}_{t}^{*}\\}_{t=1}^{T}\\right)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(f(\\pmb{\\lambda}^{*},\\pi^{*};\\pmb{\\theta}_{t})-f(\\pmb{\\lambda}_{t},\\pi_{t};\\pmb{\\theta}_{t}^{*})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "238 Similar to Lemma 4.1 of Gabbianelli et al. [2024], we show in Lemma 4.1 below that with an   \n239 appropriate choice of the comparator points, we can relate the gap to the expected suboptimality of   \n240 policy $\\pi_{J}$ where $J\\sim\\mathcal{U}(1,\\cdot\\cdot\\cdot\\,,T)$ . We leave the proof in Appendix B.1.1.   \n241 Lemma 4.1. Suppose that $D_{\\theta}\\;=\\;\\sqrt{d}/(1\\:-\\:\\gamma)$ . Choose $\\left(\\lambda^{\\ast},\\pi^{\\ast},\\theta_{t}^{\\ast}\\right)\\;=\\;\\left(\\Phi^{\\top}\\mu^{\\pi^{\\ast}},\\pi^{\\ast},\\theta^{\\pi_{t}}\\right)\\;\\in$   \n242 $\\mathbb{R}^{d}\\times\\Pi\\left(D_{\\pi}\\right)\\times\\mathbb{B}_{d}(D_{\\theta})$ for $t=1,\\cdot\\cdot\\cdot,T$ where $\\pmb{\\mu}^{\\pi^{*}}$ is a valid occupancy measure induced by $\\pi^{*}$ .   \n243 Then, ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{J}\\left[\\left\\langle\\pmb{\\mu}^{\\pi^{*}}-\\pmb{\\mu}^{\\pi_{J}},\\pmb{r}\\right\\rangle\\right]=\\mathfrak{G}_{T}\\left(\\pmb{\\Phi}^{\\top}\\pmb{\\mu}^{\\pi^{*}},\\pi^{*},\\left\\{\\pmb{\\theta}^{\\pi_{t}}\\right\\}_{t=1}^{T}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "244 We will show below that the dynamic duality gap can be written in terms of the regrets of each player   \n245 and an additional term related to the estimation error of $\\widehat{f}$ , and then proceed to provide bounds on all   \n246 of these quantities. Specifically, the regrets of each player with respect to each of their respective   \n247 comparators are defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Re_{T}\\left({\\boldsymbol{\\pi}}^{*}\\right)=\\sum_{t=1}^{T}\\sum_{x}{\\boldsymbol{\\nu}}^{*}(x)\\sum_{a}\\left({\\boldsymbol{\\pi}}^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a),}\\\\ {\\displaystyle\\Re_{T}\\left({\\boldsymbol{\\lambda}}^{*}\\right)=\\sum_{t=1}^{T}\\widehat{f}({\\boldsymbol{\\lambda}}^{*},\\pi_{t};\\theta_{t})-\\widehat{f}(\\boldsymbol{\\lambda}_{t},\\pi_{t};\\theta_{t})=\\sum_{t=1}^{T}\\langle{\\boldsymbol{\\lambda}}^{*}-\\boldsymbol{\\lambda}_{t},{\\boldsymbol{\\omega}}+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\rangle,}\\\\ {\\displaystyle\\Re_{T}\\left({\\boldsymbol{\\theta}}_{1:T}^{*}\\right)=\\sum_{t=1}^{T}\\widehat{f}(\\boldsymbol{\\lambda}_{t},\\pi_{t};\\theta_{t})-\\widehat{f}(\\boldsymbol{\\lambda}_{t},\\pi_{t};\\theta_{t}^{*})=\\sum_{t=1}^{T}\\langle\\theta_{t}-{\\boldsymbol{\\theta}}_{t}^{*},\\Phi^{\\intercal}\\widehat{\\mu}_{\\lambda_{t},\\pi_{t}}-\\boldsymbol{\\lambda}_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "248 where $\\pmb{\\nu}^{*}=(1-\\gamma)\\nu_{0}(\\pmb{x})+\\gamma\\langle\\pmb{\\psi}(\\pmb{x}),\\pmb{\\lambda}^{*}\\rangle$ . Furthermore, we define the gap-estimation error as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{err}_{\\widehat{\\Psi}}=\\sum_{t=1}^{T}\\bigl\\langle\\lambda^{*},\\bigl(\\Psi-\\widehat{\\Psi}\\bigr)v_{{\\theta}_{t},\\pi_{t}}\\bigr\\rangle+\\sum_{t=1}^{T}\\bigl\\langle\\lambda_{t},\\bigl(\\widehat{\\Psi}-\\Psi\\bigr)v_{{\\theta}_{t}^{*},\\pi_{t}}\\bigr\\rangle.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "249 The following lemma rewrites the duality gap using the above terms. ", "page_idx": 6}, {"type": "text", "text": "250 Lemma 4.2. The dynamic duality gap satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{T}(\\lambda^{*},\\pi^{*},\\theta_{1:T}^{*})=\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\pi^{*}\\right)+\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\lambda^{*}\\right)+\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\theta_{1:T}^{*}\\right)+\\frac{\\gamma}{T}e r r_{\\widehat{\\Psi}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "251 The proof directly follows from a straightforward calculation similar to the proof of Lemma 4.2   \n252 of Gabbianelli et al. [2024] and Section E.1 of Hong and Tewari [2024] which is reproduced in   \n253 Appendix B.1.2 for completeness. It remains to bound the regret of the players, as well as the   \n254 gap-estimation error. An obstacle we need to face in the analysis is that our bound of the latter error   \n255 term scale with $\\left.\\frac{1}{T}\\sum_{t=1}^{T}\\|\\pmb{\\lambda}_{t}\\|_{\\pmb{\\Lambda}_{n}^{-1}}^{2}\\right.$ , which is undesirable given our aspiration to achieve bounds that   \n256 scale only with the comparator norm $\\|\\pmb{\\lambda}^{*}\\|_{\\pmb{\\Lambda}_{n}^{-1}}^{2}$ without requiring prior upper bounds on this quantity   \n257 (that would enable us to project the iterates to a bounded domain). This challenge is addressed by   \n258 making use of the stabilization technique of Jacobsen and Cutkosky [2023] and Neu and Okolo   \n259 [2024] in the updates for the $\\lambda$ -player, which effectively eliminates these problematic terms. We   \n260 briefly outline the remaining parts of the analysis below. ", "page_idx": 7}, {"type": "text", "text": "261 4.1 Regret analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "262 The regrets of each player are respectively controlled by the following three lemmas. ", "page_idx": 7}, {"type": "text", "text": "263 Lemma 4.3. Suppose that $\\pmb{\\nu}^{*}\\in\\Delta_{\\mathcal{X}}$ . Let $\\pi_{1}$ be the uniform policy which selects all actions with   \n264 equal probability in each state. Under the conditions on the feature map in Definition 2.1, the regret   \n265 of the $\\pi$ -player against $\\pi^{*}$ satisfies $\\begin{array}{r}{\\frac{1}{T}\\Re_{T}\\left(\\pi^{*}\\right)\\leq\\frac{\\log A}{\\alpha T}+\\frac{\\alpha R^{2}D_{\\theta}^{2}}{2}}\\end{array}$   \n266 The proof is a standard application of the analysis of exponential-weight updates, stated as Lemma E.1.   \n267 Lemma 4.4. Let $\\lambda_{1}=\\mathbf{0}$ and $C=6\\beta\\left(d+D_{\\theta}^{2}\\right)+3d\\left(1+R D_{\\theta}\\right)^{2}+3\\gamma^{2}d R^{2}D_{\\theta}^{2}$ . Then, the regret   \n268 of the $\\lambda$ -player against any comparator $\\pmb{\\lambda}^{*}\\in\\mathbb{R}^{d}$ satisfies ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\Re_{T}\\left(\\mathbf{\\lambda}\\mathbf{\\lambda}^{*}\\right)\\leq\\left(\\frac{1}{2\\eta T}+\\frac{\\varrho}{2}\\right)\\left\\Vert\\mathbf{\\lambda}\\mathbf{\\right\\lambda}^{*}\\right\\Vert_{\\mathbf{\\Lambda}_{n}^{-1}}^{2}+\\frac{\\eta C}{2}-\\frac{\\varrho}{2T}\\sum_{t=1}^{T}\\left\\Vert\\mathbf{\\lambda}\\mathbf{\\right\\lambda}\\mathbf{\\right\\Vert}_{\\mathbf{\\Lambda}_{n}^{-1}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "269 The proof (provided in Appendix B.2.2) follows from applying the standard analysis of composite  \n270 objective mirror descent due to Duchi et al. [2010] (stated as Lemma C.1 in the Appendix) and the   \n271 bound $\\|\\mathbf{A}_{n}\\pmb{g}_{\\lambda}(t)\\|_{\\mathbf{A}_{n}^{-1}}^{2}\\leq C$ on the weighted norm of the gradients for all $t$ provided in Lemma C.2. ", "page_idx": 7}, {"type": "text", "text": "272 Lemma 4.5. Let $D_{\\theta}=\\sqrt{d}/\\left(1-\\gamma\\right)$ . The regret of the $\\theta$ -player satisfies $\\begin{array}{r}{\\frac{1}{T}\\Re_{T}\\left(\\pmb{\\theta}_{1:T}^{*}\\right)\\leq0}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "273 As we show in Appendix B.2.3, the above statement holds trivially thanks to the \u201cbest-response\u201d   \n274 definition of $\\theta_{t}$ . This concludes our regret analysis. ", "page_idx": 7}, {"type": "text", "text": "275 4.2 Bounding the gap-estimation error ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "276 The following statement (proved in Appendix B.3) provides a bound on $\\exp\\!\\mathbf{\\hat{\\Psi}}$ : ", "page_idx": 7}, {"type": "text", "text": "277 Lemma 4.6. Suppose that $\\|\\varphi(x,a)\\|_{2}\\,\\leq\\,R$ for all $(x,a)\\;\\in\\;\\mathcal{X}\\,\\times\\,\\mathcal{A},$ , $D_{\\theta}\\;=\\;\\sqrt{d}/(1\\:-\\:\\gamma)$ and   \n278 $\\alpha=\\sqrt{2\\left(1-\\gamma\\right)^{2}\\log A/R^{2}d T}$ to optimize $\\Re_{T}\\left(\\pi^{*}\\right)$ . Then, for any $\\begin{array}{r}{T\\geq\\frac{2R^{2}n\\log A}{\\log\\left(1/\\delta\\right)}}\\end{array}$ and and $\\xi\\geq0$ ,   \n279 the following holds with probability at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\ne r r_{\\widehat{\\Psi}}\\leq\\frac{1}{2\\xi}\\left(\\left\\|\\lambda^{*}\\right\\|_{\\Lambda_{n}^{-1}}^{2}+\\frac{1}{T}\\sum_{t=1}^{T}\\left\\|\\lambda_{t}\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\right)+T^{2}\\xi\\left(\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "280 4.3 The proof of Theorem 3.1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "281 The proof follows from applying Lemmas 4.1 and Lemma 4.2 when $\\begin{array}{r l}{(\\lambda^{*},\\pi^{*},\\theta_{t}^{*})}&{{}=}\\end{array}$   \n282 $\\left(\\Phi^{\\tau}\\mu^{\\pi^{*}},\\pi^{*},\\theta^{\\pi_{t}}\\right)\\ \\in\\ \\mathbb{R}^{d}\\times\\Pi\\left(D_{\\pi}\\right)\\times\\mathbb{B}_{d}(D_{\\theta})$ for $t\\,=\\,1,\\cdot\\cdot\\cdot\\,,T$ . Then, adding up the bounds   \n283 stated in Lemmas 4.3\u20134.6 under the respective conditions, yields ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right\\rangle\\right]\\leq\\sqrt{\\frac{d\\log\\left(1/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\left(\\frac{1}{2\\eta T}+\\frac{\\underline{{\\varrho}}}{2}+\\frac{\\gamma}{2\\xi T}\\right)\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta C}{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(\\frac{\\gamma}{\\xi T}-\\varrho\\right)\\displaystyle\\frac{1}{2T}\\sum_{t=1}^{T}\\left\\Vert\\lambda_{t}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\gamma T\\xi\\left(\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "284 Then, setting $\\begin{array}{r}{\\rho=\\frac{\\gamma}{\\xi T}}\\end{array}$ simplifies the second term and eliminates the third term. The claim then follows   \n285 after optimizing the hyperparameters, with the full details provided in Appendix B.4. ", "page_idx": 7}, {"type": "text", "text": "286 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "288 Relation with previous work. As discussed in the introduction, our work draws heavily on previous   \n289 contributions of Gabbianelli et al. [2024] and Hong and Tewari [2024]. In particular, our idea of   \n290 building a least-squares estimator of the transition function is directly borrowed from the latter of   \n291 these works, and our implicit update rule for $\\theta_{t}$ is also inspired by their work to a good extent. Their   \n292 approach, however, failed to reach the same degree of efficiency due to a number of suboptimal design   \n293 choices. First, they used an alternative parametrization of the feature occupancies which only allowed   \n294 them to work under a more restrictive coverage condition, so that their bounds depend on $\\|\\pmb{\\lambda}^{*}\\|_{\\pmb{\\Lambda}_{n}^{-2}}$   \n295 which can be much larger than the feature coverage ratio appearing in our bounds. Second, their   \n296 algorithm required a prior upper bound on this coverage parameter, with the guarantees scaling with   \n297 the bound rather than the actual coverage. Such bounds are typically difficult to obtain in practice.   \n298 Third, the implementation of their algorithm required intricate computational steps necessitated by   \n299 their feature-occupancy parametrization. Our work has successfully removed these limitations and   \n300 reduced the complexity of their method, thanks to a new primal-only analysis style that we hope will   \n301 find further uses in reinforcement learning.   \n302 Computational and statistical efficiency. As can be inferred from our main result, the sample com  \n303 plexity of finding an $\\varepsilon$ -optimal policy using our algorithm is of the order $d^{2}\\left\\lVert\\lambda^{*}\\right\\rVert_{\\Lambda_{n}^{-1}}^{2}/\\varepsilon^{2}\\left(\\bar{1}-\\gamma\\right)^{2}$ ,   \n304 which is optimal in terms of scaling with $\\varepsilon$ . The rate can be improved to scale linearly with the feature   \n305 coverage ratio $\\|\\pmb{\\lambda}^{*}\\|_{\\pmb{\\Lambda}_{n}^{-1}}$ , if a tight upper bound is known on it which can be used for hyperparameter   \n306 tuning. We find this scenario to be unlikely, and are curious to see if future work can attain this   \n307 improved scaling without such prior knowledge. As for computational complexity, we point out   \n308 that the cost of each iteration of our method scales linearly with the sample size $n$ , due to having to   \n309 compute the matrix-vector products $\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}$ . Indeed, the matrix\u03a8  is sparse with $n$ non-zero rows,   \n310 and as such computing this product takes linear time in $n$ . Since the iteration complexity of FOGAS   \n311 scales linearly with the sample size $n$ , this makes for an overall runtime complexity of order $n^{2}$ . This   \n312 limitation is of course shared with all methods using the same least-squares transition estimator for   \n313 the transition model, including all work that builds on Jin et al. [2020], but we nevertheless wonder if   \n314 a substantial improvement is possible on this front.   \n315 Data coverage assumptions. The only works we are aware of that scale with the feature-coverage   \n316 ratio $\\|\\pmb{\\lambda}^{*}\\|_{\\pmb{\\Lambda}_{n}^{-1}}$ are due to Zanette et al. [2021] and Gabbianelli et al. [2024]. The latter work only   \n317 achieves this bound under the assumption that the data is drawn i.i.d. from a fixed behavior policy   \n318 with known feature covariance matrix, which is a much more restricted setting that we consider   \n319 here. Such assumptions are not needed by Zanette et al. [2021], however their results are restricted   \n320 to the simpler finite-horizon MDP setting, and their algorithm is arguably more complex than ours.   \n321 Using our notation, their approach can be interpreted as solving a \u201cpessimistic\u201d version of the   \n322 the relaxed dual LP (4) that features some additional quadratic constraints. This approach is not   \n323 computationally viable for the infinite-horizon discounted case we consider, as it requires solving a   \n324 fixed-point equation with respect to the estimated transition operator (cf. Wei et al., 2021).   \n25 Possible extensions. Our approach can be extended and generalized in a variety of ways. First,   \n26 following Gabbianelli et al. [2024], we believe that it is straightforward to extend our analysis to   \n27 undiscounted infinite-horizon MDPs. Second, we similarly believe that an extension to constrained   \n28 MDPs is possible without major challenges, following Hong and Tewari [2024]. We did not pursue   \n29 these extensions because we believe that they add little additional insight. There are other potential   \n30 directions that we did not explore because we found them to be too ambitious for the moment.   \n31 These include extending our results beyond linear MDPs to other MDP models with linear function   \n32 approximation, including MDPs with low inherent Bellman rank (which may be within reach of   \n33 the current theory, c.f. Zanette et al., 2020), linearly $Q^{\\pi}$ -realizable MDPs (which are known to be   \n34 challenging, c.f. Weisz et al., 2022, 2024). Even more ambitiously, one can ask if it is possible to   \n35 extend our methods to work under more general notions of function approximation. This looks very   \n36 challenging given the central role of feature occupancies in our formalism, which are strictly tied to   \n37 linear function approximation. We are nevertheless optimistic that the ideas presented in this work   \n338 will find use in other contexts, possibly including nonlinear function approximation in the future. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "339 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "340 Y. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits.   \n341 Advances in neural information processing systems, 24, 2011.   \n342 J. Bas-Serrano, S. Curi, A. Krause, and G. Neu. Logistic q-learning. In International Conference on   \n343 Artificial Intelligence and Statistics, pages 3610\u20133618. PMLR, 2021.   \n344 A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex   \n345 optimization. Operations Research Letters, 31(3):167\u2013175, 2003.   \n346 R. Bellman. Dynamic programming. science, 153(3731):34\u201337, 1966.   \n347 D. P. Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):   \n348 334\u2013334, 1997.   \n349 S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning.   \n350 Machine Learning, 22:33\u201357, 1996.   \n351 N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n352 G. de Ghellinck. Les probl\u00e8mes de d\u00e9cisions s\u00e9quentielles. Cahiers du Centre d\u2019\u00c9tudes de Recherche   \n353 Op\u00e9rationnelle, 2:161\u2013179, 1960.   \n354 E. V. Denardo. On linear programming in a Markov decision problem. Management Science, 16(5):   \n355 281\u2013288, 1970.   \n356 F. d\u2019Epenoux. A probabilistic production and inventory problem. Management Science, 10(1):   \n357 98\u2013108, 1963.   \n358 J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In   \n359 COLT, volume 10, pages 14\u201326. Citeseer, 2010.   \n360 Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an   \n361 application to boosting. Journal of Computer and System Sciences, 55:119\u2013139, 1997.   \n362 G. Gabbianelli, G. Neu, M. Papini, and N. M. Okolo. Offilne primal-dual reinforcement learning for   \n363 linear mdps. In International Conference on Artificial Intelligence and Statistics, pages 3169\u20133177.   \n364 PMLR, 2024.   \n365 K. Hong and A. Tewari. A primal-dual algorithm for offilne constrained reinforcement learning with   \n366 low-rank mdps. arXiv preprint arXiv:2402.04493, 2024.   \n367 A. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In International   \n368 Conference on Machine Learning, pages 14590\u201314630. PMLR, 2023.   \n369 C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear   \n370 function approximation. In Conference on learning theory, pages 2137\u20132143. PMLR, 2020.   \n371 Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? In International   \n372 Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n373 T. Lattimore and C. Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n374 S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and   \n375 perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n376 G. Li, L. Shi, Y. Chen, Y. Chi, and Y. Wei. Settling the sample complexity of model-based offline   \n377 reinforcement learning. The Annals of Statistics, 52(1):233\u2013260, 2024.   \n378 N. Littlestone and M. Warmuth. The weighted majority algorithm. Information and Computation,   \n379 108:212\u2013261, 1994.   \n380 Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Provably good batch off-policy reinforcement   \n381 learning without great exploration. Advances in neural information processing systems, 33:   \n382 1264\u20131274, 2020.   \n383 A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259\u2013267,   \n384 1960.   \n385 R. Munos and C. Szepesv\u00e1ri. Finite-time bounds for fitted value iteration. Journal of Machine   \n386 Learning Research, 9(5), 2008.   \n387 A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley   \n388 Interscience, 1983.   \n389 G. Neu and N. Okolo. Efficient global planning in large MDPs via stochastic primal-dual optimization.   \n390 In International Conference on Algorithmic Learning Theory, pages 1101\u20131123. PMLR, 2023.   \n391 G. Neu and N. Okolo. Dealing with unbounded gradients in stochastic saddle-point optimization.   \n392 arXiv preprint arXiv:2402.13903, 2024.   \n393 G. Neu and C. Pike-Burke. A unifying view of optimism in episodic reinforcement learning. In   \n394 Advances in Neural Information Processing Systems, pages 1392\u20131403, 2020.   \n395 G. Neu, A. Jonsson, and V. G\u00f3mez. A unified view of entropy-regularized Markov decision processes.   \n396 arXiv preprint arXiv:1705.07798, 2017.   \n397 R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An analysis of linear models, linear   \n398 value-function approximation, and feature selection for reinforcement learning. In Proceedings of   \n399 the 25th international conference on Machine learning, pages 752\u2013759, 2008.   \n400 M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley   \n401 & Sons, 1994.   \n402 P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement learning and   \n403 imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:   \n404 11702\u201311716, 2021.   \n405 P. Rashidinejad, H. Zhu, K. Yang, S. Russell, and J. Jiao. Optimal conservative offilne rl with general   \n406 function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716, 2022.   \n407 S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms.   \n408 Cambridge university press, 2014.   \n409 M. Uehara and W. Sun. Pessimistic model-based offilne reinforcement learning under partial coverage.   \n410 arXiv preprint arXiv:2107.06226, 2021.   \n411 V. Vovk. Aggregating strategies. In Proceedings of the third annual workshop on Computational   \n412 learning theory (COLT), pages 371\u2013386, 1990.   \n413 C.-Y. Wei, M. J. Jahromi, H. Luo, and R. Jain. Learning infinite-horizon average-reward mdps with   \n414 linear function approximation. In International Conference on Artificial Intelligence and Statistics,   \n415 pages 3007\u20133015. PMLR, 2021.   \n416 G. Weisz, A. Gy\u00f6rgy, T. Kozuno, and C. Szepesv\u00e1ri. Confident approximate policy iteration for   \n417 efficient local planning in $q^{\\pi}$ -realizable mdps. Advances in Neural Information Processing Systems,   \n418 35:25547\u201325559, 2022.   \n419 G. Weisz, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. Online rl in linearly $q^{\\pi}$ -realizable mdps is as easy as in   \n420 linear mdps if you learn what to ignore. Advances in Neural Information Processing Systems, 36,   \n421 2024.   \n422 T. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offilne   \n423 reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.   \n424 L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive features. In   \n425 International conference on machine learning, pages 6995\u20137004. PMLR, 2019.   \n426 A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low   \n427 inherent bellman error. In International Conference on Machine Learning, pages 10978\u201310989.   \n428 PMLR, 2020.   \n429 A. Zanette, M. J. Wainwright, and E. Brunskill. Provable beneftis of actor-critic methods for offilne   \n430 reinforcement learning. Advances in neural information processing systems, 34:13626\u201313640,   \n431 2021.   \n432 W. Zhan, B. Huang, A. Huang, N. Jiang, and J. Lee. Offilne reinforcement learning with realizability   \n433 and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR,   \n434 2022.   \n435 X. Zhang, Y. Chen, X. Zhu, and W. Sun. Corruption-robust offline reinforcement learning. In   \n436 International Conference on Artificial Intelligence and Statistics, pages 5757\u20135773. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "437 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "438 A Missing proofs of Section 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "439 A.1 Properties of the relaxed LP ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "440 In this section we prove a basic result about the feasible sets of the relaxed linear programs defined in   \n441 Equations (3) and (4). We remark that similar results have been previously shown in Proposition 4 of   \n442 Bas-Serrano et al. [2021] and Appendix A.1 of Neu and Okolo [2023].   \n443 Lemma A.1. Suppose that the MDP satisfies the linear MDP assumption in the sense of Definition 2.1,   \n444 consider the relaxed linear programs $^3$ and 4 and their respective feasible sets: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{\\Phi}^{P}=\\left\\{(\\lambda,\\mu)\\in\\mathbb{R}^{d}\\times\\mathbb{R}_{+}^{X A}\\mid E^{\\top}\\mu=(1-\\gamma)\\nu_{0}+\\gamma\\Psi^{\\top}\\lambda,\\quad\\lambda=\\Phi^{\\top}\\mu\\right\\},}\\\\ &{\\mathcal{M}_{\\Phi}^{D}=\\left\\{(v,\\pmb{\\theta})\\in\\mathbb{R}^{X}\\times\\mathbb{R}^{d}\\mid E v\\geq\\Phi\\pmb{\\theta},\\quad\\pmb{\\theta}=\\omega+\\gamma\\Psi v\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "445 Then, the following statements hold: ", "page_idx": 12}, {"type": "text", "text": "46 \u2022 The set $\\mathcal{M}=\\left\\{\\pmb{\\mu}:(\\lambda,\\pmb{\\mu})\\in\\mathcal{M}_{\\Phi}^{P}\\right\\}$ coincides with the feasible set of the primal $L P$ (1). Fur  \n47 thermore, for all $(\\lambda^{*},\\mu^{*})\\in\\,a r g\\,m a x_{(\\lambda,\\mu)\\in\\mathcal{M}_{\\Phi}^{P}}\\left\\langle\\lambda,\\omega\\right\\rangle$ , we have that $\\pmb{\\mu}^{*}$ is the occupancy   \n48 measure of an optimal policy.   \n49 \u2022 The set $\\mathcal{V}=\\left\\{v:(v,\\theta)\\in\\mathcal{M}_{\\Phi}^{D}\\right\\}$ coincides with the feasible set of the dual $L P$ (2). Further  \n50 more, the optimal value function $\\pmb{v}^{\\pi^{*}}$ and the parameter vector $\\theta^{\\pi^{*}}$ satisfying $q^{\\pi^{*}}=\\Phi\\theta^{\\pi^{*}}$   \n451 satisfy $({\\boldsymbol{v}^{\\pi}}^{*},{\\boldsymbol{\\theta}^{\\pi}}^{*})\\in\\ a r g\\,m i n_{({\\boldsymbol{v}},{\\boldsymbol{\\theta}})\\in{\\mathcal{M}}_{\\Phi}^{D}}(1-\\gamma)\\,\\langle\\pmb{\\nu_{0}},{\\boldsymbol{v}}\\rangle$ .   \n452 Proof. We first show that for any feasible point $\\pmb{\\mu}$ of the LP (1), the tuple $(\\lambda,\\mu)$ is feasible for the   \n453 relaxed LP with $\\lambda=\\Phi^{\\top}\\pmb{\\mu}$ . This choice of $\\lambda$ satisfies the second primal constraint by definition, so it   \n454 remains to verify that the first constraint is also satisfied. Indeed, this follows from ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{\\Psi}^{\\top}\\pmb{\\lambda}=\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{\\Psi}^{\\top}\\pmb{\\Phi}^{\\top}\\pmb{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{P}^{\\top}\\pmb{\\mu}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "455 where we have used the linear MDP property to write $\\Psi^{\\top}\\Phi^{\\top}=P^{\\top}$ in the first step and that $\\pmb{\\mu}$ is a   \n456 valid occupancy measure in the last one. Conversely, supposing that $(\\lambda,\\mu)\\in\\mathcal{M}_{\\Phi}^{\\dot{P}}$ are feasible for   \n457 the relaxed LP, we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{P}^{\\top}\\pmb{\\mu}=\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{\\Psi}^{\\top}\\pmb{\\Phi}^{\\top}\\pmb{\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\pmb{E}^{\\top}\\pmb{\\mu}-(1-\\gamma)\\pmb{\\nu}_{0}-\\gamma\\pmb{\\Psi}^{\\top}\\lambda=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "458 thus verifying that $\\pmb{\\mu}$ is indeed a valid occupancy measure. Optimality of $(\\lambda^{*},\\mu^{*})$ follows from the   \n459 fact that for any $(\\dot{\\lambda_{}},\\mu)\\in\\mathcal{M}_{\\Phi}^{P}$ , we can write the LP objective as $\\langle\\lambda,\\omega\\rangle=\\langle\\mu,r\\rangle$ by the linear MDP   \n460 assumption, and the standard fact that any solution $\\pmb{\\mu}^{*}$ to the primal LP 1 is the occupancy measure   \n461 of an optimal policy (cf. Theorem 6.9.4 in Puterman, 1994). This concludes the first part of the proof.   \n462 For the second part of the proof, let us first consider a feasible solution $\\pmb{v}$ for the original dual LP (2).   \n463 Then, the choice $\\pmb{\\theta}=\\omega+\\gamma\\Psi\\pmb{v}$ satisfies the second dual constraint by definition. The first constraint   \n464 can be verified by writing ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nE\\pmb{v}-\\Phi\\theta=E\\pmb{v}-\\pmb{r}-\\gamma P\\pmb{v}\\ge0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "465 where we used the choice of $\\pmb{\\theta}$ in the first step and the feasibility of $\\pmb{v}$ for the original LP in the second   \n466 step. Conversely, supposing that $(\\pmb{\\theta},\\pmb{v})\\in\\dot{\\mathcal{M}}_{\\Phi}^{D}$ , we note that ", "page_idx": 12}, {"type": "equation", "text": "$$\nE v-r-\\gamma P v=E v-\\Phi\\pmb\\theta\\geq0,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "467 which implies the feasibility of $\\pmb{v}$ in the LP 2. Optimality of $v^{\\ast}$ for both LPs follows from the fact   \n468 that their objectives are identical, and the standard fact that $v^{\\ast}$ is an optimal solution of the dual   \n469 LP (2) (cf. Theorem 6.2.2 in Puterman, 1994). \u53e3 ", "page_idx": 12}, {"type": "text", "text": "470 B Missing proofs of Section 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "471 In this section, we provide performance guarantees for Algorithm 1 in terms of the expected subopti  \n472 mality of the output policy $\\pi_{J}$ , and in particular prove the lemmas provided in Section 4 in the main   \n473 text. Auxiliary lemmas and technical results for proving some of these are included in Appendix E. ", "page_idx": 13}, {"type": "text", "text": "474 B.1 Properties of the Dynamic Duality Gap ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "475 We first prove our claims regarding the dynamic duality gap introduced in Section 4 of the main text.   \n476 First, we relate the gap to the expected suboptimality (in terms of return) of $\\pi_{J}$ against a comparator   \n477 policy $\\pi^{*}$ in Appendix B.1.1. Next, we relate the dynamic duality gap to the average regret of each   \n478 player in Appendix B.1.2. ", "page_idx": 13}, {"type": "text", "text": "479 B.1.1 Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "480 By definition of the the dynamic duality gap, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{T}\\left(\\Phi^{\\tau}\\mu^{\\pi^{*}},\\pi^{*},\\{\\theta^{\\pi_{t}}\\}_{t=1}^{T}\\right)=\\frac{1}{T}\\sum_{t=1}^{T}f(\\Phi^{\\tau}\\mu^{\\pi^{*}},\\pi^{*};\\theta_{t})-f(\\lambda_{t},\\pi_{t};\\theta^{\\pi_{t}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "481 Considering the first term, we see that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\Phi^{\\top}{\\mu^{\\pi^{*}}},\\pi^{*};\\theta_{t})=\\left\\langle{\\Phi^{\\top}{\\mu^{\\pi^{*}}},\\omega}\\right\\rangle+\\left\\langle{\\theta_{t},\\Phi^{\\top}{\\mu_{\\lambda^{*},\\pi^{*}}}-\\Phi^{\\top}{\\mu^{\\pi^{*}}}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{(a)}{=}\\left\\langle{\\mu^{\\pi^{*}}},r\\right\\rangle+\\left\\langle{\\theta_{t},\\Phi^{\\top}{\\mu_{\\lambda^{*},\\pi^{*}}}-\\Phi^{\\top}{\\mu^{\\pi^{*}}}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{(b)}{=}\\left\\langle{\\mu^{\\pi^{*}}},r\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "482 where we have used $(a)$ the linear MDP property (definition 2.1) and $(b)$ the following relation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\lambda^{*},\\pi^{*}}(x,a)=\\pi^{*}(a|x)\\Big[(1-\\gamma)\\nu_{0}(x)+\\gamma\\left\\langle\\psi(x),\\Phi^{\\top}\\mu^{\\pi^{*}}\\right\\rangle\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\pi^{*}(a|x)\\Big[(1-\\gamma)\\nu_{0}(x)+\\gamma\\displaystyle\\sum_{x^{\\prime},a^{\\prime}}p\\left(x|x^{\\prime},a^{\\prime}\\right)\\mu^{\\pi^{*}}\\left(x^{\\prime},a^{\\prime}\\right)\\Big]=\\mu^{\\pi^{*}}(x,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "483 Now for the second term, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda_{t},\\pi_{t};\\theta^{\\pi_{t}})=(1-\\gamma)\\left\\langle\\nu_{0},v_{\\theta^{\\pi_{t}},\\pi_{t}}\\right\\rangle+\\langle\\lambda_{t},\\omega+\\gamma\\Psi v_{\\theta^{\\pi_{t}},\\pi_{t}}-\\theta^{\\pi_{t}}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\langle\\mu^{\\pi_{t}},r\\rangle+\\langle\\lambda_{t},\\omega+\\gamma\\Psi v^{\\pi_{t}}-\\theta^{\\pi_{t}}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\langle\\mu^{\\pi_{t}},r\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "484 where we have used the Bellman equations $\\begin{array}{r}{\\pmb{q}^{\\pi_{t}}=\\pmb{\\Phi}\\pmb{\\theta}_{t}^{\\pi}=\\pmb{r}+\\gamma\\pmb{P}\\pmb{v}^{\\pi_{t}}=\\pmb{\\Phi}\\left(\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}^{\\pi_{t}}\\right)}\\end{array}$ , which   \n485 together with the fact that $\\Phi$ is full rank implies that $\\pmb{\\theta}^{\\pi_{t}}=\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}^{\\pi_{t}}$ . Substituting the above   \n486 expressions for $f(\\Phi^{\\top}{\\boldsymbol{\\mu}}^{\\pi^{*}},\\pi^{*};\\pmb{\\theta}_{t})$ and $f(\\lambda_{t},\\pi_{t};\\pmb{\\theta}^{\\pi_{t}})$ in the dynamic duality gap and noting that $\\pi_{J}$   \n487 is such that $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle\\pmb{\\mu}^{\\pi_{t}},\\pmb{r}\\right\\rangle=\\mathbb{E}_{J}\\left[\\left\\langle\\pmb{\\mu}^{\\pi_{J}},\\pmb{r}\\right\\rangle\\right]}\\end{array}$ we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{T}\\left(\\Phi^{\\top}\\mu^{\\pi^{*}},\\pi^{*},\\{\\pmb{\\theta}^{\\pi_{t}}\\}_{t=1}^{T}\\right)=\\mathbb{E}_{J}\\left[\\left\\langle\\pmb{\\mu}^{\\pi^{*}}-\\pmb{\\mu}^{\\pi_{J}},\\pmb{r}\\right\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "488 This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "489 B.1.2 Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "490 Recall that for any comparator points $\\left(\\lambda^{*},\\pi^{*};\\{\\theta_{t}^{*}\\}_{t=1}^{T}\\right)$ , the dynamic duality gap is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{T}\\left(\\lambda^{*},\\pi^{*};\\{\\pmb{\\theta}_{t}^{*}\\}_{t=1}^{T}\\right)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(f(\\pmb{\\lambda}^{*},\\pi^{*};\\pmb{\\theta}_{t})-f(\\pmb{\\lambda}_{t},\\pi_{t};\\pmb{\\theta}_{t}^{*})\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491 Then, by adding and subtracting some terms we express the dynamic duality gap in terms of the   \n492 average loss of each player with respect to the objective $f(\\lambda,\\pi;\\^{}\\theta)$ . This gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathfrak{G}_{T}(\\lambda^{*},\\pi^{*},\\theta_{1:T}^{*})=\\frac{1}{T}\\sum_{t=1}^{T}f(\\lambda^{*},\\pi^{*};\\theta_{t})-f(\\lambda^{*},\\pi_{t};\\theta_{t})}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\frac{1}{T}\\sum_{t=1}^{T}f(\\lambda^{*},\\pi_{t};\\theta_{t})-f(\\lambda_{t},\\pi_{t};\\theta_{t})}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\frac{1}{T}\\sum_{t=1}^{T}f(\\lambda_{t},\\pi_{t};\\theta_{t})-f(\\lambda_{t},\\pi_{t};\\theta_{t}^{*}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "493 Consider the first set of terms from the above expression. By definition of $f$ in Equation (7), we   \n494 immediately obtain the instantaneous regret of the $\\pi$ -player as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\lambda^{*},\\pi^{*};\\boldsymbol{\\theta}_{t})-f(\\lambda^{*},\\pi_{t};\\boldsymbol{\\theta}_{t})=\\langle\\boldsymbol{\\theta}_{t},\\Phi^{\\intercal}\\boldsymbol{\\mu}_{\\lambda^{*},\\pi^{*}}-\\Phi^{\\intercal}\\boldsymbol{\\mu}_{\\lambda^{*},\\pi_{t}}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x}\\nu^{*}(x)\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{x}\\nu^{*}(x)\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "495 where $\\nu^{*}(x)=(1-\\gamma)\\nu_{0}(x)+\\gamma\\left\\langle\\psi(x),\\lambda^{*}\\right\\rangle$ . For the regret of the $\\lambda$ and $\\pmb{\\theta}$ -players, notice that we   \n496 can express the estimator $\\widehat{f}$ in terms of the objective $f$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{f}(\\lambda,\\pi;\\pmb{\\theta})=(1-\\gamma)\\left\\langle\\pmb{\\nu}_{0},\\pmb{v}_{\\pmb{\\theta},\\pi}\\right\\rangle+\\left\\langle\\lambda,\\omega+\\gamma\\widehat{\\Psi}\\pmb{v}_{\\pmb{\\theta},\\pi}-\\pmb{\\theta}\\right\\rangle}\\\\ &{\\quad\\quad\\quad=f(\\pmb{\\lambda},\\pi;\\pmb{\\theta})+\\gamma\\left\\langle\\pmb{\\lambda},\\widehat{\\Psi}\\pmb{v}_{\\theta,\\pi}-\\Psi\\pmb{v}_{\\theta,\\pi}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "497 Taking advantage of this relation, we now consider the last two set of terms in Equation (11). Indeed,   \n498 for the second set of terms in the equation, we write ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{*}(\\lambda^{*},\\pi_{t};\\boldsymbol{\\theta}_{t})-f(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t})}\\\\ &{\\qquad=\\widehat{f}(\\lambda^{*},\\pi_{t};\\boldsymbol{\\theta}_{t})-\\widehat{f}(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t})-\\gamma\\left\\langle\\lambda^{*},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle+\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle}\\\\ &{\\qquad=\\left\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\right\\rangle-\\gamma\\left\\langle\\lambda^{*},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle+\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "499 Notice that the last equality follows directly from definition of $\\widehat{f}$ . Along these lines, we can also   \n500 express the last set of terms in Equation (11) as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathrm{\\Lambda}}(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t})-f(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t}^{*})}\\\\ &{\\quad\\quad=\\widehat{f}(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t})-\\widehat{f}(\\lambda_{t},\\pi_{t};\\boldsymbol{\\theta}_{t}^{*})-\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle+\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t}^{*},\\pi_{t}}\\right\\rangle}\\\\ &{\\quad\\quad=\\left\\langle\\boldsymbol{\\theta}_{t}-\\boldsymbol{\\theta}_{t}^{*},\\Phi^{\\top}\\widehat{\\mu}\\lambda_{t},\\pi_{t}-\\lambda_{t}\\right\\rangle-\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t},\\pi_{t}}\\right\\rangle+\\gamma\\left\\langle\\lambda_{t},\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\Psi v_{\\theta_{t}^{*},\\pi_{t}}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "501 Plugging the above derivations in the dynamic duality gap, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathfrak{G}_{T}\\big(\\lambda^{*},\\pi^{*},\\theta_{1:T}^{*}\\big)=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{x}\\nu^{*}(x)\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a)}}\\\\ {{\\displaystyle\\ ~~}+\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\hat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\right\\rangle}\\\\ {{\\displaystyle~~~}+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\theta_{t}-\\theta_{t}^{*},\\Phi^{\\mathsf{r}}\\hat{\\mu}_{\\lambda_{t},\\pi_{t}}-\\lambda_{t}\\rangle}\\\\ {{\\displaystyle~~~}+\\frac{\\gamma}{T}\\sum_{t=1}^{T}\\Big\\langle\\lambda^{*},\\Psi v_{\\theta_{t},\\pi_{t}}-\\hat{\\Psi}v_{\\theta_{t},\\pi_{t}}\\Big\\rangle+\\frac{\\gamma}{T}\\sum_{t=1}^{T}\\Big\\langle\\lambda_{t},\\hat{\\Psi}v_{\\theta_{t}^{*},\\pi_{t}}-\\Psi v_{\\theta_{t}^{*},\\pi_{t}}\\Big\\rangle\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "502 This matches the claim of the lemma, thus completing the proof. ", "page_idx": 14}, {"type": "text", "text": "503 B.2 Bounding the Regret Terms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "504 In this section we provide the proofs of the our claims made in the main text about the regret of each   \n505 player\u2014precisely, Lemmas 4.3\u20134.5. ", "page_idx": 15}, {"type": "text", "text": "506 B.2.1 Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "507 Consider the regret of the $\\pi$ -player introduced in the main text as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re_{T}\\left(\\pi^{*}\\right)=\\displaystyle\\sum_{t=1}^{T}\\sum_{x}\\nu^{*}(x)\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a)}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}\\frac{\\sum_{x}\\nu^{*}(x)\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi^{*}\\left(\\cdot|x\\right)\\right|\\left|\\pi_{1}\\left(\\cdot|x\\right)\\right)}{\\alpha}+\\frac{\\alpha T R^{2}D_{\\theta}^{2}}{2}}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\leq}\\frac{\\log A}{\\alpha}+\\frac{\\alpha T R^{2}D_{\\theta}^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "508 We have used $(a)$ the standard Mirror descent analysis of softmax policy iterates recalled in   \n509 Lemma E.1 for completeness, and $(b)$ the fact that $\\pi_{1}$ is a uniform policy and $\\pmb{\\nu}^{*}\\in\\Delta_{\\mathcal{X}}$ . Dividing   \n510 the above expression by $T$ completes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "511 B.2.2 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "512 Recall the total regret of the $\\lambda$ -player against any fixed comparator $\\pmb{\\lambda}^{*}\\in\\mathbb{R}^{d}$ is given as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Re_{T}\\left(\\mathbf{\\lambda}^{*}\\right)=\\sum_{t=1}^{T}\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "513 Since the feature-occupancy updates of Algorithm 1 simply implements a version of the composite  \n514 objective mirror descent scheme due to Duchi et al. [2010] we apply the standard analysis of this   \n515 method (recalled as Lemma C.1 in Appendix $\\mathbf{C}$ ) to bound the instantaneous regret as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\rangle}\\\\ &{\\quad\\quad\\leq\\frac{\\|\\lambda_{t}-\\lambda^{*}\\|_{\\mathbf{A}_{n}^{-1}}^{2}-\\|\\lambda_{t+1}-\\lambda^{*}\\|_{\\mathbf{A}_{n}^{-1}}^{2}}{2\\eta}+\\frac{\\eta}{2}\\left\\|\\mathbf{A}_{n}g_{\\lambda}(t)\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}+\\frac{\\varrho}{2}\\left\\|\\lambda^{*}\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\left\\|\\lambda_{t+1}\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "516 Then, taking the sum for $t=1,\\cdot\\cdot\\cdot,T$ , evaluating the telescoping sums and upper-bounding some   \n517 negative terms by zero yields the expression ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\hat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\rangle}\\\\ &{\\quad\\displaystyle\\quad\\leq\\frac{\\|\\lambda_{1}-\\lambda^{*}\\|_{\\Lambda_{n}^{-1}}^{2}}{2\\eta}+\\frac{\\eta}{2}\\sum_{t=1}^{T}\\|\\Lambda_{n}g_{\\lambda}(t)\\|_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\varrho T}{2}\\,\\|\\lambda^{*}\\|_{\\Lambda_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\sum_{t=1}^{T}\\|\\lambda_{t}\\|_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\varrho}{2}\\,\\|\\lambda_{1}\\|_{\\Lambda_{n}^{-1}}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\frac{1}{2\\eta}+\\frac{\\varrho T}{2}\\right)\\|\\lambda^{*}\\|_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta}{2}\\sum_{t=1}^{T}\\|\\Lambda_{n}g_{\\lambda}(t)\\|_{\\Lambda_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\sum_{t=1}^{T}\\|\\lambda_{t}\\|_{\\Lambda_{n}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "518 In the equality, we have used that $\\lambda_{1}=\\mathbf{0}$ . Dividing the resulting term by $T$ gives the following   \n519 bound on the average regret: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\lambda^{*}\\right)\\leq\\left(\\frac{1}{2T\\eta}+\\frac{\\varrho}{2}\\right)\\left\\Vert\\lambda^{*}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta}{2T}\\sum_{t=1}^{T}\\left\\Vert\\mathbf{A}_{n}g_{\\lambda}(t)\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}-\\frac{\\varrho}{2T}\\sum_{t=1}^{T}\\left\\Vert\\lambda_{t}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "520 The proof is completed by applying Lemma C.2 to bound the norm of the gradients and plugging the   \n521 result into the bound above. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "522 B.2.3 Proof of Lemma 4.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "523 For the regret of the $\\pmb{\\theta}$ -player, first note that for any policy $\\pi$ with corresponding state-action value   \n524 function weights $\\pmb{\\theta}^{\\pi}=\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}^{\\pi}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}^{\\pi}\\|_{2}=\\|\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}^{\\pi}\\|_{2}\\leq\\|\\pmb{\\omega}\\|_{2}+\\gamma\\,\\|\\pmb{\\Psi}\\pmb{v}^{\\pi}\\|_{2}\\leq\\sqrt{d}+\\frac{\\gamma\\sqrt{d}}{(1-\\gamma)}=\\frac{\\sqrt{d}}{(1-\\gamma)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "525 where we have used the triangle inequality in the second line. The last inequality uses Definition 2.1   \n526 and the fact that $\\begin{array}{r}{\\|\\pmb{v}^{\\pi}\\|_{\\infty}\\leq\\frac{\\pmb{\\`}_{1}}{(1-\\gamma)}}\\end{array}$ since the rewards are bounded in $[0,1]$ . Thanks to this bound, we   \n527 can ensure that $\\pmb{\\theta}_{t}^{*}=\\pmb{\\theta}^{\\pi_{t}}\\in\\mathbb{B}_{d}(D_{\\pmb{\\theta}})$ holds with the choice $D_{\\theta}=\\sqrt{d}/\\left(1-\\gamma\\right)$ as required by the   \n528 lemma.Therefore, by construction of value-parameter updates in Algorithm 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\pmb{\\theta}_{t}-\\pmb{\\theta}_{t}^{*},\\pmb{\\Phi}^{\\top}\\widehat{\\mu}_{\\lambda_{t},\\pi_{t}}-\\pmb{\\lambda}_{t}\\rangle\\leq0\\qquad\\mathrm{for}\\,t=1,\\cdots\\,,T.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "529 This concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "530 B.3 Bounding the gap-estimation error ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "531 In this section, we provide the proof of Lemma 4.6 which bounds the gap-estimation error defined for   \n532 an arbitrary comparator sequence $(\\lambda^{*},\\pi_{t},\\pmb{\\theta}_{t}^{*})\\in\\mathbb{R}^{d}\\times\\Pi\\left(D_{\\pi}\\right)\\times\\mathbb{B}_{d}\\bar{(D_{\\theta})}$ for $t=1,\\dots,T$ as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{err}_{\\widehat{\\Psi}}=\\sum_{t=1}^{T}\\bigl\\langle\\lambda^{*},\\bigl(\\Psi-\\widehat{\\Psi}\\bigr)v_{{\\theta}_{t},\\pi_{t}}\\bigr\\rangle+\\sum_{t=1}^{T}\\bigl\\langle\\lambda_{t},\\bigl(\\widehat{\\Psi}-\\Psi\\bigr)v_{{\\theta}_{t}^{*},\\pi_{t}}\\bigr\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "533 We control the above term with the now-classic techniques developed by Jin et al. [2020] for bounding   \n534 model-estimation errors for linear MDPs. These results also make heavy use of self-normalized tail   \n535 inequalities as popularized by Abbasi-Yadkori et al. [2011] (see also Lattimore and Szepesv\u00e1ri, 2020).   \n536 To make this clear, we first note that, for any $\\lambda\\in\\mathbb{R}^{d}$ , $\\pmb{v}\\in\\mathbb{R}^{X}$ , and $\\xi>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle\\lambda,\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\rangle\\stackrel{(a)}{\\leq}\\left\\lVert\\lambda\\right\\rVert_{\\mathrm{A}_{n}^{-1}}\\left\\lVert\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\rVert_{\\mathrm{A}_{n}^{-1}}\\stackrel{(b)}{\\leq}\\frac{\\left\\lVert\\lambda\\right\\rVert_{\\mathrm{A}_{n}^{-1}}^{2}}{2T\\xi}+\\frac{T\\xi}{2}\\left\\lVert\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\rVert_{\\mathrm{A}_{n}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "537 Here, we have first used $(a)$ the Cauchy\u2013Schwarz inequality, and $(b)$ the inequality of arithmetic and   \n538 geometric means. Using this expression, we can upper-bound the gap estimation error as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{err}_{\\hat{\\Psi}}\\leq\\frac{\\Vert\\boldsymbol{\\lambda}^{*}\\Vert_{\\boldsymbol{\\Lambda}_{n}^{-1}}^{2}}{2\\xi}+\\displaystyle\\sum_{t=1}^{T}\\frac{\\Vert\\boldsymbol{\\lambda}_{t}\\Vert_{\\boldsymbol{\\Lambda}_{n}^{-1}}^{2}}{2T\\xi}}\\\\ &{\\qquad+\\displaystyle\\frac{T\\xi}{2}\\displaystyle\\sum_{t=1}^{T}\\Big\\Vert\\boldsymbol{\\Lambda}_{n}\\left(\\hat{\\Psi}-\\Psi\\right)\\boldsymbol{v}_{\\theta_{t},\\pi_{t}}\\Big\\Vert_{\\boldsymbol{\\Lambda}_{n}^{-1}}^{2}+\\displaystyle\\frac{T\\xi}{2}\\displaystyle\\sum_{t=1}^{T}\\Big\\Vert\\boldsymbol{\\Lambda}_{n}\\left(\\hat{\\Psi}-\\Psi\\right)\\boldsymbol{v}^{\\pi_{t}}\\Big\\Vert_{\\boldsymbol{\\Lambda}_{n}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "539 To control the last two terms in the bound, we employ two main lemmas stated below. ", "page_idx": 16}, {"type": "text", "text": "540 Lemma B.1. Let $\\pmb{v}\\in[-B,B]^{X}$ . With probability at least $1-\\delta$ , we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\Vert\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\Vert_{\\Lambda_{n}^{-1}}\\leq\\frac{2B}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+2\\log\\frac{1}{\\delta}}+B\\sqrt{d\\beta}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "541 Lemma B.2. Consider the function class, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\Big\\{v_{\\pi,\\theta}:\\mathcal{X}\\rightarrow[-R D_{\\theta},R D_{\\theta}]\\Big|\\pi\\in\\Pi\\left(D_{\\pi}\\right),\\pmb{\\theta}\\in\\mathbb{B}_{d}(D_{\\theta})\\Big\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "542 Let $D_{\\pi}=\\alpha T D_{\\theta}$ so that $\\pmb{v}_{\\theta_{t},\\pi_{t}}\\in\\mathcal{V}$ . For any $\\epsilon\\in(0,1).$ , with probability at least $1-\\delta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Lambda_{n}\\left(\\hat{\\Psi}-\\Psi\\right)v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\Lambda_{n}^{-1}}}\\\\ &{\\qquad\\quad\\leq\\displaystyle\\frac{2R D_{\\theta}}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+4d\\log\\left(1+\\frac{4\\alpha T R^{2}D_{\\theta}^{2}}{\\epsilon}\\right)+2\\log\\frac{1}{\\delta}}}\\\\ &{\\qquad\\qquad+\\displaystyle R D_{\\theta}\\sqrt{d\\beta}+\\left(\\sqrt{\\beta}+1\\right)\\epsilon\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "543 The rather tedious but otherwise standard proofs of the above lemmas are given in Appendix D. Now,   \n544 taking into account the fact that for $D_{\\theta}$ large enough $\\pmb{v}^{\\pi_{t}}\\in\\mathcal{V}$ yields Corollary B.3 below.   \n545 Corollary B.3. In the linear MDP setting described in \u221aDefinition 2.1, notice that $\\pmb{v}^{\\pi_{t}}=\\pmb{v}_{\\theta^{\\pi_{t}},\\pi_{t}}$ with   \n546 $\\pmb{\\theta}^{\\pi_{t}}=\\pmb{\\omega}+\\gamma\\pmb{\\Psi}\\pmb{v}_{\\theta^{\\pi_{t}},\\pi_{t}}$ . Furthermore, with $R D_{\\theta}=R\\sqrt{d}/(1-\\gamma)\\ge\\|\\pmb{v}^{\\pi_{t}}\\|_{\\infty}$ and $D_{\\pi}=\\alpha T D_{\\theta}$ we   \n547 have that $\\pmb{v}^{\\pi_{t}}\\in\\mathcal{V}$ . Therefore, for all $\\epsilon>0$ with probability at least $1-\\delta$ , the following holds: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\Vert\\Lambda_{n}\\left(\\hat{\\Psi}-\\Psi\\right)v^{\\pi}t\\right\\Vert_{\\Lambda_{n}^{-1}}}\\\\ {\\quad\\leq\\cfrac{2\\sqrt{d}}{\\sqrt{n}\\left(1-\\gamma\\right)}\\sqrt{d\\log\\left(1+\\cfrac{R^{2}}{d\\beta}\\right)+4d\\log\\left(1+\\cfrac{4\\alpha T R^{2}d}{\\epsilon\\left(1-\\gamma\\right)^{2}}\\right)+2\\log\\frac{1}{\\delta}}}\\\\ {\\quad+\\cfrac{d\\sqrt{\\beta}}{\\left(1-\\gamma\\right)}+\\left(\\sqrt{\\beta}+1\\right)\\epsilon\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "548 In the following, we apply these results to bound the last two terms in the right-hand side of Equa  \n549 tion (12). Precisely, using $D_{\\theta}=\\sqrt{d}/(1-\\gamma),\\alpha=\\sqrt{2\\log A/R^{2}D_{\\theta}^{2}T}=\\sqrt{2\\left(1-\\gamma\\right)^{2}\\log A/R^{2}d T}$   \n550 (which follows from \u221aoptimizing the regret of the $\\pi$ -player in Lemma 4.3), as well as $\\epsilon=$   \n551 4\u03b1R2d/ (1 \u2212\u03b3)2 = 32R2d \u221alog A and $\\beta\\,=\\,R^{2}/d T$ we have that in any round $t$ , with probabil  \n552 ity at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v_{\\theta_{t},\\pi_{t}}\\right|_{\\Lambda_{n}^{-1}}\\leq\\sqrt{\\frac{20d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\sqrt{\\frac{R^{2}d}{T\\left(1-\\gamma\\right)^{2}}}+\\sqrt{\\frac{R^{4}d\\log A}{T^{2}}}+\\sqrt{\\frac{32R^{2}d^{2}\\log A}{T\\left(1-\\gamma\\right)^{2}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "553 Likewise, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v^{\\pi_{t}}\\right|_{\\Lambda_{n}^{-1}}\\leq\\sqrt{\\frac{20d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\sqrt{\\frac{R^{2}d}{T\\left(1-\\gamma\\right)^{2}}}+\\sqrt{\\frac{R^{4}d\\log A}{T^{2}}}+\\sqrt{\\frac{32R^{2}d^{2}\\log A}{T\\left(1-\\gamma\\right)^{2}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "554 Then plugging in the above bounds in Equation (12) with $\\begin{array}{r}{T\\ \\geq\\ \\frac{2R^{2}n\\log A}{\\log(1/\\delta)}}\\end{array}$ 2lRo2gn( 1l/o\u03b4g )A , it follows that for   \n555 $\\begin{array}{r}{D_{\\widehat{\\Psi}}=\\sqrt{\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{err}_{\\widehat{\\Psi}}\\leq\\frac{\\|\\lambda^{*}\\|_{\\mathbf{A}_{n}^{-1}}^{2}}{2\\xi}+\\sum_{t=1}^{T}\\frac{\\|\\mathbf{\\lambda}_{t}\\|_{\\mathbf{A}_{n}^{-1}}^{2}}{2T\\xi}+T^{2}\\xi D_{\\widehat{\\Psi}}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "556 with probability at least $1-\\delta$ , thus proving the claim. ", "page_idx": 17}, {"type": "text", "text": "557 B.4 Full proof of Theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "558 To control the expected suboptimality of the output policy $\\pi_{J}$ of Algorithm 1, we study the repective   \n559 regret and gap-estimation error at the selected comparator points. Precisely, combining Lemma 4.1   \n560 and 4.2 when $(\\lambda^{\\ast},\\pi^{\\ast},\\pmb{\\theta}_{1:T}^{\\ast})=\\left(\\Phi^{\\intercal}\\pmb{\\mu}^{\\pi^{\\ast}},\\pi^{\\ast},\\pmb{\\theta}^{\\pi_{t}}\\right)\\bar{\\in}\\mathbb{R}^{d}\\times\\bar{\\Pi}\\left(D_{\\pi}\\right)\\times\\mathbb{B}_{d}(\\bar{D_{\\theta}})$ , we have that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right\\rangle\\right]=\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\pi^{*}\\right)+\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\lambda^{\\pi^{*}}\\right)+\\frac{1}{T}\\mathfrak{R}_{T}\\left(\\theta_{1:T}^{*}\\right)+\\frac{\\gamma}{T}\\mathrm{err}_{\\widehat{\\Psi}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "561 where, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Re_{T}\\left({\\boldsymbol{\\pi}}^{*}\\right)=\\sum_{t=1}^{T}\\sum_{x}{\\boldsymbol{\\nu}}^{*}(x)\\sum_{a}\\left({\\boldsymbol{\\pi}}^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a),}}\\\\ {~}\\\\ {{\\displaystyle\\Re_{T}\\left({\\boldsymbol{\\lambda}}^{\\pi^{*}}\\right)=\\sum_{t=1}^{T}\\langle{\\boldsymbol{\\lambda}}^{\\pi^{*}}-{\\boldsymbol{\\lambda}}_{t},{\\boldsymbol{\\omega}}+\\gamma\\hat{\\Psi}v_{\\theta_{t},\\pi_{t}}-{\\boldsymbol{\\theta}}_{t}\\rangle,}}\\\\ {~}\\\\ {{\\displaystyle\\Re_{T}\\left(\\theta_{1:T}^{*}\\right)=\\sum_{t=1}^{T}\\langle\\theta_{t}-\\theta_{t}^{\\pi_{t}},\\Phi^{\\top}\\hat{\\mu}_{\\lambda_{t},\\pi_{t}}-{\\boldsymbol{\\lambda}}_{t}\\rangle}}\\\\ {{\\displaystyle\\mathrm{err}_{\\hat{\\Psi}}=\\sum_{t=1}^{T}\\langle{\\boldsymbol{\\lambda}}^{\\pi^{*}},\\left({\\boldsymbol{\\Psi}}-\\hat{\\boldsymbol{\\Psi}}\\right)v_{\\theta_{t},\\pi_{t}}\\rangle+\\sum_{t=1}^{T}\\langle{\\boldsymbol{\\lambda}}_{t},\\left(\\hat{\\boldsymbol{\\Psi}}-\\boldsymbol{\\Psi}\\right)v^{\\pi_{t}}\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "562 Notice that for this choice of $\\lambda^{*}$ , by Definition 2 $.1\\;\\nu^{*}(x)=(1-\\gamma)\\nu_{0}(x)+\\gamma\\langle\\psi(x),\\pmb{\\mu}^{*}\\rangle=\\nu^{\\pi^{*}}(x)$   \n563 is a valid state occupancy measure. Next, introducing the bounds stated in Lemmas $4.3{-}4.6$ under   \n564 the required conditions $D_{\\theta}\\;=\\;\\sqrt{d}/\\left(1-\\gamma\\right)$ , $\\alpha\\,=\\,\\sqrt{2\\left(1-\\gamma\\right)^{2}\\log A/R^{2}d T}$ , $D_{\\pi}\\;=\\;\\alpha T D_{\\theta}\\;=$   \n565 $\\sqrt{2T\\log A/R^{2}}$ and $\\begin{array}{r}{T\\geq\\frac{2R^{2}n\\log A}{\\log\\left(1/\\delta\\right)}}\\end{array}$ , as well as $\\xi\\geq0$ and $\\begin{array}{r}{D_{\\widehat{\\Psi}}=\\sqrt{\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}}\\end{array}$ yields, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{J}\\left[\\left<\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right>\\right]\\leq\\sqrt{\\displaystyle\\frac{2R^{2}d\\log A}{\\left(1-\\gamma\\right)^{2}T}}}\\\\ &{\\phantom{\\mathbb{E}_{J}\\left[\\left<\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right>\\right]}+\\left(\\displaystyle\\frac{1}{2\\eta T}+\\displaystyle\\frac{\\varrho}{2}\\right)\\left\\|\\lambda^{\\pi^{*}}\\right\\|_{\\Lambda_{n}^{-1}}^{2}+\\displaystyle\\frac{\\eta C}{2}-\\frac{\\varrho}{2T}\\displaystyle\\sum_{t=1}^{T}\\left\\|\\lambda_{t}\\right\\|_{\\Lambda_{n}^{-1}}^{2}}\\\\ &{\\phantom{\\mathbb{E}_{J}\\left[\\left(\\left|\\lambda^{\\pi^{*}}\\right|_{\\Lambda_{n}^{-1}}^{2}+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\left\\|\\lambda_{t}\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\right)+\\gamma T\\xi D_{\\hat{\\Psi}}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "566 with probability at least $1\\!-\\!\\delta$ , where $C=6\\beta\\left(d+D_{\\theta}^{2}\\right)+3d\\left(1+R D_{\\theta}\\right)^{2}+3\\gamma^{2}d R^{2}D_{\\theta}^{2}$ . Rearranging   \n567 the bound and selecting $\\varrho=\\gamma/\\xi T$ to eliminate the (potentially large) norm of the iterates, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right\\rangle\\right]\\leq\\sqrt{\\frac{d\\log\\left(1/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\left(\\frac{1}{2\\eta T}+\\frac{\\underline{{\\rho}}}{2}+\\frac{\\gamma}{2\\xi T}\\right)\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta C}{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\frac{\\gamma}{\\xi T}-\\varrho\\right)\\displaystyle\\frac{1}{2T}\\sum_{t=1}^{T}\\left\\Vert\\lambda_{t}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\gamma T\\xi D_{\\widehat{\\Psi}}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\frac{d\\log\\left(1/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\left(\\frac{1}{2\\eta T}+\\frac{\\gamma}{\\xi T}\\right)\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta C}{2}+\\gamma T\\xi D_{\\widehat{\\Psi}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "568 Furthermore, choosing $\\xi=1/T D_{\\widehat{\\Psi}}$ i.e $\\varrho=\\gamma D_{\\widehat{\\Psi}}$ , we further simplify the above bound on the regret   \n569 in terms of the optimization error a rising from th e policy and feature occupancy updates as, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi_{J}},r\\right\\rangle\\right]\\leq\\sqrt{\\frac{d\\log\\left(1/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\frac{1}{2\\eta T}\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\eta C}{2}+\\gamma\\left(\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+1\\right)D_{\\hat{\\Psi}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "570 Moving our attention to our earlier bound on the norm of $g_{\\lambda}(t)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nC=6\\beta\\left(d+D_{\\theta}^{2}\\right)+3d\\left(1+R D_{\\theta}\\right)^{2}+3\\gamma^{2}d R^{2}D_{\\theta}^{2}\\leq\\frac{27R^{2}d^{2}}{\\left(1-\\gamma\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "571 The inequality follows from our earlier choice of $\\beta=R^{2}/d T$ and that $T\\geq1/d^{2}$ . Plugging the values   \n572 of $C$ and $D_{\\hat{\\Psi}}$ in the bound, then choosing $\\begin{array}{r}{\\eta=\\sqrt{\\frac{(1-\\gamma)^{2}}{27R^{2}d^{2}T}}}\\end{array}$ and using the condition $\\begin{array}{r}{T\\geq\\frac{2R^{2}n\\log A}{\\log\\left(1/\\delta\\right)}}\\end{array}$   \n573 we have that with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{J}\\left[\\left\\langle\\mu^{\\pi^{*}}-\\mu^{\\pi,\\,r},r\\right\\rangle\\right]\\leq\\sqrt{\\frac{d\\log\\left(1/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}+\\left(\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+1\\right)\\sqrt{\\frac{27d^{2}\\log\\left(1/\\delta\\right)}{8n\\log A\\left(1-\\gamma\\right)^{2}}}}&{}\\\\ {+\\left.\\gamma\\left(\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+1\\right)\\sqrt{\\frac{320d^{2}\\log\\left(2T/\\delta\\right)}{n\\left(1-\\gamma\\right)^{2}}}}&{}\\\\ {=\\mathcal{O}\\left(\\frac{\\left\\Vert\\lambda^{\\pi^{*}}\\right\\Vert_{\\Lambda_{n}^{-1}}^{2}+1}{\\left(1-\\gamma\\right)}\\sqrt{\\frac{d^{2}\\log\\left(2T/\\delta\\right)}{n}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "574 This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "575 C Missing proofs of Section B.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "576 Lemma C.1. (cf. Lemma $^{\\,l}$ of Duchi et al. [2010]) Let $\\pmb{g}_{\\pmb{\\lambda}}(t)=\\pmb{\\omega}+\\gamma\\widehat{\\pmb{\\Psi}}\\pmb{v}_{\\theta_{t},\\pi_{t}}-\\pmb{\\theta}_{t}$ Given $\\lambda_{1}=\\mathbf{0}$   \n577 and $\\varrho,\\eta>0$ and the sequence of iterates $\\{\\lambda_{t}\\}_{t=2}^{T}$ defined for $t=1,\\cdot\\cdot\\cdot,T$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{t+1}=\\left.a r g\\,m i n\\right\\{-\\left\\langle\\lambda,g_{\\lambda}(t)\\right\\rangle+\\frac{1}{2\\eta}\\left\\|\\lambda-\\lambda_{t}\\right\\|_{\\Lambda_{n}^{-1}}^{2}+\\frac{\\varrho}{2}\\left\\|\\lambda\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "578 Then, for any $\\pmb{\\lambda}^{*}\\in\\mathbb{R}^{d}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\lambda^{*}-\\lambda_{t},\\omega+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\rangle}\\\\ &{\\quad\\quad\\leq\\frac{\\|\\lambda_{t}-\\lambda^{*}\\|_{\\mathbf{A}_{n}^{-1}}^{2}-\\|\\lambda_{t+1}-\\lambda^{*}\\|_{\\mathbf{A}_{n}^{-1}}^{2}}{2\\eta}+\\frac{\\eta}{2}\\left\\|\\mathbf{A}_{n}g_{\\lambda}(t)\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}+\\frac{\\varrho}{2}\\left\\|\\lambda^{*}\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\left\\|\\lambda_{t+1}\\right\\|_{\\mathbf{A}_{n}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "579 Proof. The proof of Lemma C.1 follows directly from the referenced Lemma from Duchi et al.   \n580 [2010]. Consider, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{-\\lambda_{t}\\phi_{2}(t)+\\frac{\\gamma}{2}\\left\\{\\|\\boldsymbol{\\lambda}_{t+1}\\|_{\\infty}^{2}-\\frac{\\theta}{2}\\|^{1}\\boldsymbol{X}_{t+1}^{*}-\\frac{\\theta}{2}\\|^{1}\\right\\}\\left\\{\\boldsymbol{\\lambda}_{t+1}^{*}-\\boldsymbol{\\lambda}_{2}^{*}\\right\\}}\\\\ &{=\\left\\langle\\boldsymbol{\\lambda}_{t+1}-\\boldsymbol{\\lambda}_{t}-\\boldsymbol{\\mathcal{A}}_{{\\lambda}}(t)+\\frac{1}{\\eta}\\frac{1}{\\lambda}\\boldsymbol{X}_{t}^{-1}\\left(\\lambda_{t+1}-\\lambda_{3}\\right)+\\boldsymbol{\\mathcal{A}}_{{\\lambda}}^{-1}\\boldsymbol{\\lambda}_{t+1}\\right\\rangle+(\\lambda_{t+1}-\\lambda_{4},\\phi_{2}(t))}\\\\ &{-\\left\\langle\\boldsymbol{\\lambda}_{t+1}-\\boldsymbol{\\lambda}_{t},\\frac{\\theta}{2}\\frac{1}{\\lambda}\\boldsymbol{X}_{t}^{-1}\\boldsymbol{X}_{t}^{*}\\left(\\lambda_{t+1}-\\lambda_{1}\\right)+\\boldsymbol{\\mathcal{B}}_{{\\lambda}}^{-1}\\boldsymbol{\\lambda}_{t+1}\\right\\rangle+\\frac{\\eta}{2}\\left\\|\\boldsymbol{\\lambda}_{t+1}\\right\\|_{\\infty}^{2}-\\frac{\\theta}{2}\\left\\|\\boldsymbol{\\lambda}_{t}^{*}\\right\\|_{\\infty}^{2}}\\\\ {\\overset{(a)}{\\leq}\\left(\\lambda_{t+1}-\\lambda_{t},\\phi_{2}(t)\\right)-\\frac{1}{\\eta}\\left(\\lambda_{t+1}-\\lambda_{t},\\lambda_{t}^{*}-\\lambda_{1}\\right)}\\\\ &{+\\left(\\boldsymbol{\\mathcal{B}}^{-1}\\boldsymbol{\\lambda}_{t}^{*}\\boldsymbol{\\lambda}_{t+1}\\right)-\\frac{\\eta}{2}\\left\\|\\boldsymbol{\\lambda}_{t+1}\\right\\|_{\\infty}^{2}-\\frac{\\theta}{2}\\left\\|\\boldsymbol{\\lambda}_{t}^{*}\\right\\|_{\\infty}^{2}}\\\\ {\\overset{(b)}{\\leq}\\left(\\lambda_{t+1}-\\lambda_{t},\\phi_{2}(t)\\right)+\\frac{1}{\\eta}\\left(\\lambda_{t+1}-\\lambda_{t},\\overline{{\\lambda}}_{t}^{*}-(\\lambda_{1}+\\lambda_{1})\\right)}\\\\ &{ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "581 We have used ", "page_idx": 19}, {"type": "text", "text": "582 $(a)$ The first order optimality condition on Equation (14): ", "page_idx": 19}, {"type": "text", "text": "583 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left<\\lambda_{t+1}-\\lambda,-g_{\\lambda}(t)+\\frac{1}{\\eta}\\Lambda_{n}^{-1}\\left(\\lambda_{t+1}-\\lambda_{t}\\right)+\\varrho\\Lambda_{n}^{-1}\\lambda_{t+1}\\right>\\le0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "584 $(b)$ The relation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varrho\\left\\langle\\lambda^{*},\\mathbf{A}_{n}^{-1}\\lambda_{t+1}\\right\\rangle-\\frac{\\varrho}{2}\\left\\Vert\\lambda_{t+1}\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}-\\frac{\\varrho}{2}\\left\\Vert\\lambda^{*}\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}=-\\frac{\\varrho}{2}\\left\\Vert\\lambda_{t+1}-\\lambda^{*}\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}\\leq0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "585 $(c)$ By definition of the squared $L^{2}$ -norm for vectors $\\pmb{a}\\;=\\;\\mathbf{A}_{n}^{-1/2}\\left(\\lambda_{t+1}-\\lambda^{*}\\right)$ and $\\textit{\\textbf{b}}=$   \n586 $\\mathbf{A}_{n}^{-1/2}\\left(\\lambda_{t}-\\lambda_{t+1}\\right)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\pmb{a},\\pmb{b}\\rangle=\\frac{1}{2}\\left(-\\left\\|\\pmb{b}\\right\\|_{2}^{2}+\\left\\|\\pmb{a}+\\pmb{b}\\right\\|_{2}^{2}-\\left\\|\\pmb{a}\\right\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\textbf{\\em a}$ and $^{b}$ are well defined since $\\mathbf{\\boldsymbol{A}}_{n}$ is both symmetric and positive definite. ", "page_idx": 19}, {"type": "text", "text": "589 Rearranging the terms and plugging in $\\pmb{g}_{\\pmb{\\lambda}}(t)=\\pmb{\\omega}+\\gamma\\widehat{\\pmb{\\Psi}}\\pmb{v}_{\\theta_{t},\\pi_{t}}-\\pmb{\\theta}_{t}$ completes the proof. ", "page_idx": 20}, {"type": "text", "text": "590 Finally, we will use the following result that bounds the gradient norms appearing in the bound above.   \n591 Lemma C.2. Under the conditions of the linear MDP setting we have that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\Lambda_{n}g_{\\lambda}(t)\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\le6\\beta\\left(d+D_{\\theta}^{2}\\right)+3d\\left(1+R D_{\\theta}\\right)^{2}+3\\gamma^{2}d R^{2}D_{\\theta}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "592 Proof. Recall that for $t=1,\\cdots\\,,T\\,\\pmb{g_{\\pmb{\\lambda}}}(t)=\\pmb{\\omega}+\\gamma\\hat{\\pmb{\\Psi}}\\pmb{v_{\\theta_{t},\\pi_{t}}}-\\pmb{\\theta_{t}}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left|\\Lambda_{n}g_{\\lambda}(t)\\right|\\right|_{\\Lambda_{n}^{-1}}^{2}=\\left\\|\\Lambda_{n}\\!\\left[\\omega+\\gamma\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}-\\theta_{t}\\right]\\right\\|_{\\Lambda_{n}^{-1}}^{2}}\\\\ {\\displaystyle=\\left\\|\\beta\\left(\\omega-\\theta_{t}\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(r\\left(x_{i},a_{i}\\right)-\\langle\\varphi_{i},\\theta_{t}\\rangle\\right)+\\gamma\\Lambda_{n}\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\Lambda_{n}^{-1}}^{2}}\\\\ {\\displaystyle\\ \\leq3\\left\\|\\beta\\left(\\omega-\\theta_{t}\\right)\\right\\|_{\\Lambda_{n}^{-1}}^{2}+3\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(r\\left(x_{i},a_{i}\\right)-\\langle\\varphi_{i},\\theta_{t}\\rangle\\right)\\right\\|_{\\Lambda_{n}^{-1}}^{2}+3\\gamma^{2}\\left\\|\\Lambda_{n}\\widehat{\\Psi}v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\Lambda_{n}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "593 Now to bound each of the three terms, we use that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\beta\\left(\\omega-\\pmb{\\theta}_{t}\\right)\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\leq2\\beta^{2}\\left\\|\\mathbf{A}_{n}^{-1}\\right\\|_{2}\\left(d+D_{\\pmb{\\theta}}^{2}\\right)\\leq2\\beta\\left(d+D_{\\pmb{\\theta}}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "594 where the first inequality uses the assumption that $\\|\\omega\\|_{2}\\leq{\\sqrt{d}}$ (cf. Definition 2.1) and $\\pmb{\\theta}_{t}\\in\\mathbb{B}_{d}(D_{\\pmb{\\theta}})$ .   \n595 Next, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(r\\left(x_{i},a_{i}\\right)-\\langle\\varphi_{i},\\theta_{t}\\rangle\\right)\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\le\\frac{1}{n}\\sum_{i=1}^{n}\\|\\varphi_{i}\\|_{\\Lambda_{n}^{-1}}^{2}\\left|r\\left(x_{i},a_{i}\\right)-\\langle\\varphi_{i},\\theta_{t}\\rangle\\right|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "596 The last step follows from the fact that the rewards are bounded in $[0,1],\\,\\|\\varphi_{i}\\|\\le R,\\pmb\\theta_{t}\\in\\mathbb{B}_{d}(D_{\\pmb\\theta})$   \n597 and Equation (17). The last remaining term is bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\mathbf{A}_{n}\\widehat{\\pmb{\\Psi}}v_{\\pmb{\\theta}_{t},\\pi_{t}}\\right\\rVert_{\\mathbf{A}_{n}^{-1}}^{2}=\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}v_{\\pmb{\\theta}_{t},\\pi_{t}}\\left(x_{i}^{\\prime}\\right)\\right\\rVert_{\\mathbf{A}_{n}^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\lVert\\varphi_{i}\\right\\rVert_{\\mathbf{A}_{n}^{-1}}^{2}\\left\\lVert\\pmb{v}_{\\pmb{\\theta}_{t},\\pi_{t}}\\right\\rVert_{\\infty}^{2}\\leq d R^{2}D_{\\pmb{\\theta}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "598 Therefore, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\Lambda_{n}g_{\\lambda}(t)\\right\\|_{\\Lambda_{n}^{-1}}^{2}\\le6\\beta\\left(d+D_{\\theta}^{2}\\right)+3d\\left(1+R D_{\\theta}\\right)^{2}+3\\gamma^{2}d R^{2}D_{\\theta}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "599 and this completes the proof. ", "page_idx": 20}, {"type": "text", "text": "600 D Missing proofs of Section B.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "601 In this section, we prove the lemmas stated in Section B.3. ", "page_idx": 21}, {"type": "text", "text": "602 D.1 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "603 By definition of $\\mathbf{\\boldsymbol{A}}_{n}$ Section 3 and $\\widehat{\\Psi}$ in Equation (8), we can write: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v=\\Lambda_{n}\\left(\\frac{1}{n}\\Lambda_{n}^{-1}\\sum_{i=1}^{n}\\varphi_{i}e_{x_{i}^{\\prime}}^{\\top}\\right)v-\\left(\\beta I_{n}+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\varphi_{i}^{\\top}\\right)\\Psi v}\\\\ {\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}[v\\left(x_{i}^{\\prime}\\right)-\\langle p\\left(\\cdot|x_{i},a_{i}\\right),v\\rangle\\right]-\\beta\\Psi v}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "604 In the last equality we used definition 2.1 to write $\\varphi_{i}^{\\top}\\Psi\\;=\\;p\\left(\\cdot|x_{i},a_{i}\\right)^{\\top}$ . Let $\\xi_{i}~=~v\\left(x_{i}^{\\prime}\\right)-$   \n605 $\\langle p\\left(\\cdot|x_{i},a_{i}\\right),\\bar{\\pmb{v}}\\rangle$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{A}_{n}\\left(\\widehat{\\boldsymbol{\\Psi}}-\\boldsymbol{\\Psi}\\right)\\boldsymbol{v}\\right\\|_{\\mathbf{A}_{n}^{-1}}\\leq\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\xi_{i}\\right\\|_{\\mathbf{A}_{n}^{-1}}+\\left\\|\\beta\\boldsymbol{\\Psi}\\boldsymbol{v}\\right\\|_{\\mathbf{A}_{n}^{-1}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "606 We easily control the second term with the relation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\beta\\Psi v\\|_{\\mathbf{A}_{n}^{-1}}\\leq\\beta\\left\\|\\mathbf{A}_{n}^{-1/2}\\right\\|_{2}\\|\\Psi v\\|_{2}\\leq B\\sqrt{d\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "607 The last inequality follows from the fact that $\\left\\|\\mathbf{A}_{n}^{-1/2}\\right\\|_{2}\\leq1/\\sqrt{\\beta}$ and by definition 2.1 $\\|\\Psi\\boldsymbol{v}\\|_{2}\\leq$   \n608 $B{\\sqrt{d}}$ for $\\pmb{v}\\in[-B,B]^{X}$ .   \n609 Now, to handle the first term, let $D_{0}=\\emptyset$ . We construct a filtration $\\mathcal{F}_{i-1}=\\mathcal{D}_{i-1}\\cup(x_{i}^{0},x_{i},a_{i},r_{i})$   \n610 for $i=1,2,\\cdots\\,,n$ . Notice that by construction of the dataset $\\xi_{i}$ is a martingale difference sequence   \n611 (i.e $\\mathbb{E}\\left[\\xi_{i}\\left|\\mathcal{F}_{i-1}\\right.\\right]=0)$ taking values in the range $[-2B,2B]$ . Then, we can directly apply Lemma E.3   \n612 to obtain a bound on the first term as: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\xi_{i}\\right\\|_{\\mathbf{A}_{n}^{-1}}=\\displaystyle\\frac{1}{\\sqrt{n}}\\sqrt{\\left\\|\\displaystyle\\sum_{i=1}^{n}\\varphi_{i}\\xi_{i}\\right\\|_{\\left(n\\mathbf{A}_{n}\\right)^{-1}}^{2}}\\leq\\displaystyle\\frac{2B}{\\sqrt{n}}\\sqrt{2\\log\\left(\\frac{\\operatorname*{det}\\left(n\\mathbf{A}_{n}\\right)^{1/2}\\operatorname*{det}\\left(n\\beta I\\right)^{-1/2}}{\\delta}\\right)}}&{}&\\\\ {\\ \\ \\leq\\displaystyle\\frac{2B}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+2\\log\\frac{1}{\\delta}}.}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "613 with probability $1-\\delta$ . In the last inequality we have used the AM-GM inequality and bound on the   \n614 feature vectors: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(n\\mathbf{A}_{n}\\right)\\leq\\left({\\frac{\\operatorname{tr}\\left(n\\mathbf{A}_{n}\\right)}{d}}\\right)^{d}=\\left(n\\beta+{\\frac{\\operatorname{tr}\\left(\\sum_{i=1}^{n}\\varphi_{i}\\varphi_{i}^{\\intercal}\\right)}{d}}\\right)^{d}\\leq\\left(n\\beta+{\\frac{n R^{2}}{d}}\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "615 Putting everything together, we have that w.p $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\Vert\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\Vert_{\\Lambda_{n}^{-1}}\\leq\\frac{2B}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+2\\log\\frac{1}{\\delta}}+B\\sqrt{d\\beta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "616 This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "617 D.2 Proof of Lemma B.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "618 Unlike Lemma B.1, we now aim to control the error term $\\left\\|\\mathbf{A}_{n}\\left({\\widehat{\\Psi}}-\\Psi\\right)v\\right\\|_{\\mathbf{A}_{n}^{-1}}$ when $\\pmb{v}$ is random.   \n619 Also, notice that with $\\begin{array}{r}{\\pi_{1}\\left(a|x\\right)=\\frac{e^{\\langle\\varphi\\left(x,a\\right),\\mathbf{0}\\rangle}}{\\sum_{a^{\\prime}\\in\\mathcal{A}}e^{\\langle\\varphi\\left(x,a^{\\prime}\\right),\\mathbf{0}\\rangle}}}\\end{array}$ as the uniform policy, for $t=1,\\cdot\\cdot\\cdot,T$ we have   \n620 that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{t+1}(a|x)=\\frac{\\pi_{1}(a|x)e^{\\alpha\\left\\langle\\varphi(x,a),\\sum_{k=1}^{t}\\theta_{k}\\right\\rangle}}{\\sum_{a^{\\prime}}\\pi_{1}(a^{\\prime}|x)e^{\\alpha\\left\\langle\\varphi(x,a^{\\prime}),\\sum_{k=1}^{t}\\theta_{k}\\right\\rangle}}=\\frac{e^{\\left\\langle\\varphi(x,a),\\alpha\\sum_{k=0}^{t}\\theta_{k}\\right\\rangle}}{\\sum_{a^{\\prime}}e^{\\left\\langle\\varphi(x,a^{\\prime}),\\alpha\\sum_{k=0}^{t}\\theta_{k}\\right\\rangle}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "621 where ${\\pmb\\theta}_{0}={\\bf0}$ . Furthermore, since $\\{\\pmb{\\theta}_{t}\\}_{t=1}^{T}\\subset\\mathbb{B}_{d}(D_{\\pmb{\\theta}})$ , for any $\\begin{array}{r}{t\\left\\|\\alpha\\sum_{k=0}^{t}\\pmb{\\theta}_{k}\\right\\|_{2}\\leq\\alpha T D_{\\pmb{\\theta}}}\\end{array}$ . Hence,   \n622 with $D_{\\pi}=\\alpha T D_{\\theta}$ , $\\pi_{t}\\in\\Pi\\left(D_{\\pi}\\right)$ and $\\pmb{v}_{\\theta_{t},\\pi_{t}}\\in\\mathcal{V}$ .   \n623 Therefore, as we have seen in previous works [Jin et al., 2020, Hong and Tewari, 2024], the quantity   \n624 $\\left\\|\\mathbf{A}_{n}\\left({\\widehat{\\Psi}}-\\Psi\\right)v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\mathbf{A}_{n}^{-1}}$ can be cont lled without any dependence on the size of the state space   \n625 with a uniform covering argument over $\\nu$ . Let be an -cover of . That is, for , there   \n626 exists ${\\pmb v}^{\\prime}\\in C_{\\pmb{v}}$ such that $\\left\\|\\pmb{v}_{\\pi,\\pmb{\\theta}_{t}}-\\pmb{v}^{\\prime}\\right\\|_{\\infty}\\leq\\epsilon$ . Then, we can write: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\Lambda_{n}^{-1}}}\\\\ &{\\qquad\\leq\\left\\|\\Lambda_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v^{\\prime}\\right\\|_{\\Lambda_{n}^{-1}}+\\left\\|\\Lambda_{n}\\widehat{\\Psi}\\left(v_{\\theta_{t},\\pi_{t}}-v^{\\prime}\\right)\\right\\|_{\\Lambda_{n}^{-1}}+\\left\\|\\Lambda_{n}\\Psi\\left(v^{\\prime}-v_{\\theta_{t},\\pi_{t}}\\right)\\right\\|_{\\Lambda_{n}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "627 Consider the first term in the bound. Note that $\\pmb{v}^{\\prime}$ is still random with respect to uncertainty in the   \n628 learning process. However, due to the structure of $\\mathcal{V}$ we know that $C_{v}$ exists and has cardinality   \n629 $\\begin{array}{r}{\\log|C_{v}|=\\mathcal{O}\\left(d\\log\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)\\right)}\\end{array}$ (see Lemma E.6). Inspired by Lemma B.1, consider the event: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{\\Psi}}_{v}=\\left\\{\\operatorname{exists}v\\in C_{v}:\\left\\|\\mathbf{A}_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v\\right\\|_{\\mathbf{A}_{n}^{-1}}>\\frac{2R D_{\\theta}}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+2\\log\\frac{1}{\\delta^{\\prime}}}+R D_{\\theta}\\sqrt{d\\beta}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "630 Since $C_{v}\\subseteq\\nu$ , we know from Lemma B.1 that $\\mathbb{P}\\left(\\mathcal{E}_{v}\\right)\\le\\delta^{\\prime}$ . Now, taking the union bound over the   \n631 cover $C_{v}$ we have that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcup_{v\\in C_{v}}\\mathcal{E}_{v}\\right)\\leq|C_{v}|\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "632 Therefore for any ${\\pmb v}^{\\prime}\\in C_{\\pmb{v}}$ with probability at least $1-\\delta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\mathbf{A}_{n}\\left(\\widehat{\\Psi}-\\Psi\\right)v^{\\prime}\\right\\|_{\\Lambda_{n}^{-1}}}\\\\ {\\displaystyle\\leq\\frac{2R D\\theta}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+2\\log\\frac{\\left|C_{v}\\right|}{\\delta}}+R D_{\\theta}\\sqrt{d\\beta}}\\\\ {\\displaystyle\\leq\\frac{2R D_{\\theta}}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+4d\\log\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)+2\\log\\frac{1}{\\delta}}+R D_{\\theta}\\sqrt{d\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "633 Now, for the second term in Equation (16) we write, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\Vert\\mathbf{A}_{n}\\widehat{\\Psi}\\left(v_{\\theta_{t},\\pi_{t}}-v^{\\prime}\\right)\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}=\\left\\Vert\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(v_{\\theta_{t},\\pi_{t}}\\left(x_{i}^{\\prime}\\right)-v^{\\prime}\\left(x_{i}^{\\prime}\\right)\\right)\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}}&{}\\\\ {\\overset{(a)}{\\leq}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\vert v_{\\theta_{t},\\pi_{t}}\\left(x_{i}^{\\prime}\\right)-v^{\\prime}\\left(x_{i}^{\\prime}\\right)\\right\\vert^{2}\\left\\Vert\\varphi_{i}\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}}&{}\\\\ {\\leq\\epsilon^{2}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert\\varphi_{i}\\right\\Vert_{\\mathbf{A}_{n}^{-1}}^{2}\\leq\\epsilon^{2}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "634 We have used $(a)$ Jensen\u2019s inequality and $(b)$ since $\\textstyle\\mathbf{\\boldsymbol{\\Lambda}}_{n}\\succ0$ , the relation, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}^{\\top}\\mathbf{A}_{n}^{-1}\\varphi_{i}=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname{tr}\\left(\\mathbf{A}_{n}^{-1}\\varphi_{i}\\varphi_{i}^{\\top}\\right)=\\operatorname{tr}\\left(\\mathbf{A}_{n}^{-1}\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\varphi_{i}^{\\top}\\right)\\leq\\operatorname{tr}\\left(I\\right)=d.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "635 For the last term, notice that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\Lambda_{n}\\Psi\\left(v^{\\prime}-v_{\\theta_{t},\\pi_{t}}\\right)\\biggr\\rvert\\mathbf{k}_{n}^{-1}=\\left\\lVert\\beta\\Psi\\left(v^{\\prime}-v_{\\theta_{t},\\pi_{t}}\\right)+\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\Bigl[\\underset{x^{\\prime}}{\\sum}p\\left(x^{\\prime}|x_{i},a_{i}\\right)\\left(v^{\\prime}\\left(x^{\\prime}\\right)-v_{\\theta_{t},\\pi_{t}}\\left(x^{\\prime}\\right)\\right)\\Bigr]\\right\\rvert}\\\\ {\\displaystyle\\overset{(a)}{\\leq}\\epsilon\\sqrt{d\\beta}+\\sqrt{\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\Bigl[\\underset{x^{\\prime}}{\\sum}p\\left(x^{\\prime}|x_{i},a_{i}\\right)\\left(v^{\\prime}\\left(x^{\\prime}\\right)-v_{\\theta_{t},\\pi_{t}}\\left(x^{\\prime}\\right)\\right)\\right]\\right\\rVert_{\\mathbf{A}_{n}^{-1}}^{2}}}\\\\ {\\displaystyle\\overset{(b)}{\\leq}\\epsilon\\sqrt{d\\beta}+\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\lVert v^{\\prime}-v_{\\theta_{t},\\pi_{t}}\\right\\rVert_{\\infty}^{2}\\left\\lVert\\varphi_{i}\\right\\rVert_{\\mathbf{A}_{n}^{-1}}^{2}}}\\\\ {\\displaystyle\\overset{(c)}{\\leq}\\epsilon\\sqrt{d\\beta}+\\epsilon\\sqrt{d}=\\epsilon\\sqrt{d}\\left(\\sqrt{\\beta}+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "636 This follows from $(a)$ Equation (15) since $\\pmb{v}=\\pmb{v}^{\\prime}-\\pmb{v}_{\\theta_{t},\\pi_{t}}\\in[-\\epsilon,\\epsilon]^{X}$ and $(b)$ monotonicity of the   \n637 square root function as well as Jensen\u2019s inequality and $(c)$ Equation (17).   \n638 Finally, plugging the above results back into Equation (16), we have that with probability at least   \n639 $1-\\delta$ , ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Lambda_{n}\\left(\\hat{\\Psi}-\\Psi\\right)v_{\\theta_{t},\\pi_{t}}\\right\\|_{\\Lambda_{n}^{-1}}}\\\\ &{\\qquad\\leq\\frac{2R D_{\\theta}}{\\sqrt{n}}\\sqrt{d\\log\\left(1+\\frac{R^{2}}{d\\beta}\\right)+4d\\log\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)+2\\log\\frac{1}{\\delta}}}\\\\ &{\\qquad+\\left.R D_{\\theta}\\sqrt{d\\beta}+\\left(\\sqrt{\\beta}+1\\right)\\epsilon\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "640 The proof of Lemma B.2 is complete. ", "page_idx": 23}, {"type": "text", "text": "641 E Auxiliary Lemmas ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "642 Lemma E.1. Let $q_{1},\\cdots,q_{t}$ be a sequence of iterates satisfying $\\|\\pmb{q}_{t}\\|_{\\infty}\\,\\leq\\,R D_{\\pmb{\\theta}}$ by virtue of   \n643 definition 2.1 and $\\pmb{\\theta}_{t}\\in\\mathbb{B}_{d}(D_{\\pmb{\\theta}})$ . Given an initial policy $\\pi_{1}$ and learning rate $\\alpha>0$ , and sequence of   \n644 policies $\\{\\pi_{t}\\}_{t=2}^{T}$ defined as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{t+1}(a|x)=\\frac{\\pi_{t}(a|x)e^{\\alpha q_{t}(x,a)}}{\\sum_{a^{\\prime}}\\pi_{t}(a^{\\prime}|x)e^{\\alpha q_{t}(x,a^{\\prime})}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "645 Then, for any comparator policy $\\pi^{*}$ and $\\nu^{*}$ some state distribution, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{x}\\nu^{*}(x)\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a)\\leq\\frac{\\sum_{x}\\nu^{*}(x)\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi^{*}\\left(\\cdot|x\\right)\\right||\\pi_{1}\\left(\\cdot|x\\right)\\right)}{\\alpha}+\\frac{\\alpha T R^{2}D_{\\theta}^{2}}{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "646 The proof of the lemma follows from bounding the regret of the $\\pi$ -player in each state $x$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{a}\\left(\\pi^{*}(a|x)-\\pi_{t}(a|x)\\right)q_{t}(x,a)\\leq\\frac{\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi^{*}\\left(\\cdot|x\\right)\\|\\pi_{1}\\left(\\cdot|x\\right)\\right)}{\\alpha}+\\frac{\\alpha}{2}\\sum_{t=1}^{T}\\left\\|q_{t}(x,\\cdot)\\right\\|_{\\infty}^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "647 via the application of the standard analysis of the exponentially weighted forecaster of Vovk [1990],   \n648 Littlestone and Warmuth [1994], Freund and Schapire [1997] (see, e.g., Theorem 2.2 in Cesa-Bianchi   \n649 and Lugosi, 2006), and noting that $\\|q_{t}\\|_{\\infty}\\leq R\\bar{D_{\\theta}}$ for all $t$ .   \n650 Lemma E.2. Suppose that $\\|\\varphi(x,a)\\|_{2}\\leq R_{\\cdot}$ for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ . Let $\\pi\\theta,\\pi\\theta^{\\prime}$ be softmax policies.   \n651 Then, for all states $x\\in\\mathscr{X}$ we have that: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{a}|\\pi_{\\pmb{\\theta}}\\left(a|x\\right)-\\pi_{\\pmb{\\theta}^{\\prime}}\\left(a|x\\right)|\\leq R\\,\\|\\pmb{\\theta}-\\pmb{\\theta}^{\\prime}\\|_{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "652 holds for any $\\pmb{\\theta},\\pmb{\\theta}^{\\prime}\\in\\mathbb{R}^{d}$ . ", "page_idx": 24}, {"type": "text", "text": "653 Proof. Recall that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Pi\\left(D_{\\pi}\\right)=\\left\\{\\pi_{\\theta}\\left(a|x\\right)=\\frac{e^{\\langle\\varphi\\left(x,a\\right),\\theta\\rangle}}{\\sum_{a^{\\prime}}e^{\\langle\\varphi\\left(x,a^{\\prime}\\right),\\theta\\rangle}}\\,\\bigg|\\,\\theta\\in\\mathbb{B}_{d}(D_{\\pi})\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "654 For $\\pi_{\\pmb{\\theta}},\\pi_{\\pmb{\\theta}^{\\prime}}\\in\\Pi\\left(D_{\\pi}\\right)$ using Pinsker\u2019s inequality we have that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pi_{\\theta}\\left(\\cdot|x\\right)-\\pi_{\\theta^{\\prime}}\\left(\\cdot|x\\right)\\|_{1}\\le\\sqrt{2\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi_{\\theta}\\left(\\cdot|x\\right)\\|\\pi_{\\theta^{\\prime}}\\left(\\cdot|x\\right)\\right)}\\qquad\\mathrm{for}\\;x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "655 Furthermore, taking into account the specific structure of the policies, we can write: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{D}_{\\mathrm{KL}}\\left(\\pi_{\\theta}\\left(\\cdot|x\\right)\\middle|\\pi_{\\theta^{\\prime}}\\left(\\cdot|x\\right)\\right)=\\displaystyle\\sum_{a}\\pi_{\\theta}\\left(a|x\\right)\\log\\frac{\\pi_{\\theta}\\left(a|x\\right)}{\\pi_{\\theta^{\\prime}}\\left(a\\middle|x\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=-\\displaystyle\\sum_{a}\\pi_{\\theta}\\left(a|x\\right)\\left<\\varphi(x,a),\\theta^{\\prime}-\\theta\\right>+\\log\\frac{\\sum_{a}e^{\\left\\langle\\varphi(x,a),\\theta^{\\prime}\\right\\rangle}}{\\sum_{a}e^{\\left\\langle\\varphi(x,a),\\theta\\right\\rangle}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{=}-\\displaystyle\\sum_{a}\\pi_{\\theta}\\left(a|x\\right)\\left<\\varphi(x,a),\\theta^{\\prime}-\\theta\\right>+\\log\\sum_{a}\\pi_{\\theta}\\left(a|x\\right)e^{\\left\\langle\\varphi(x,a),\\theta^{\\prime}-\\theta\\right\\rangle}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(b)}{=}\\frac{R^{2}\\left\\lVert\\theta-\\theta^{\\prime}\\right\\rVert_{2}^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "656 using that $(a)$ the relation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\log\\frac{\\sum_{a}e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta^{\\prime}\\right\\rangle}}{\\sum_{a}e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta\\right\\rangle}}=\\log\\sum_{a}\\frac{e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta^{\\prime}\\right\\rangle}}{\\sum_{a^{\\prime}}e^{\\left\\langle\\varphi\\left(x,a^{\\prime}\\right),\\theta\\right\\rangle}}\\cdot\\frac{e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta\\right\\rangle}}{e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta\\right\\rangle}}=\\log\\sum_{a}\\pi_{\\theta}\\left(a|x\\right)e^{\\left\\langle\\varphi\\left(x,a\\right),\\theta^{\\prime}-\\theta\\right\\rangle},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "657 and $(b)$ Hoeffding\u2019s lemma (cf. Lemma A.1 of Cesa-Bianchi and Lugosi [2006]). The final statement   \n658 follows from substituting this result in Equation (18). \u53e3   \n659 Lemma E.3. (Self-Normalized Bound for Vector-Valued Martingales - Theorem 1 of Abbasi-Yadkori   \n660 et al. $[20l l]$ Let $\\{\\mathcal{F}_{i-1}\\}_{i=1}^{\\infty}$ be a flitration and $\\{\\xi_{i}\\}_{i=1}^{\\infty}\\,a$ real-valued stochastic process such that $\\xi_{i}$   \n661 for $i=1,\\cdot\\cdot\\cdot$ is zero-mean $\\left(i.e\\mathbb{E}\\left[\\xi_{i}\\left|\\mathcal{F}_{i-1}\\right.\\right]=0_{,}$ ) and conditionally $s$ -subgaussian for $s\\geq0$ . That is,   \n662 for all $b\\in\\mathbb{R}$ , ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb E\\left[e^{b\\xi_{i}}\\,|\\mathcal F_{i-1}\\right]\\leq e^{\\frac{b^{2}s^{2}}{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "663 Also, let $\\{\\varphi_{i}\\}_{i=1}^{\\infty}$ be $\\mathcal{F}_{i-1}$ -measurable. Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{n}\\varphi_{i}\\xi_{i}\\right\\|_{(n\\Lambda_{n})^{-1}}^{2}\\leq2s^{2}\\log\\left[\\frac{\\operatorname*{det}\\left(n\\Lambda_{n}\\right)^{1/2}\\operatorname*{det}\\left(n\\beta I\\right)^{-1/2}}{\\delta}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "664 Lemma E.4. (e.g. see Chapter 27 of Shalev-Shwartz and Ben-David [2014]) For all $\\epsilon>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\mathbb{B}_{d}(r),\\|\\cdot\\|_{\\infty}\\,,\\epsilon\\right)\\leq d\\log\\left(1+\\frac{2r}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "665 Corollary E.5. Under the conditions of Lemma E.2, for all $\\epsilon>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\Pi\\left(D_{\\pi}\\right),\\left\\Vert\\cdot\\right\\Vert_{\\infty,1},\\epsilon\\right)\\leq\\log\\mathcal{N}\\left(\\mathbb{B}_{d}(D_{\\pi}),\\left\\Vert\\cdot\\right\\Vert_{\\infty},\\frac{\\epsilon}{R}\\right)\\leq d\\log\\left(1+\\frac{2R D_{\\pi}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "666 Lemma E.6. Consider the function class, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\Big\\{v_{\\pi,\\theta}:\\mathcal{X}\\rightarrow[-R D_{\\theta},R D_{\\theta}]\\Big|\\pi\\in\\Pi\\left(D_{\\pi}\\right),\\pmb{\\theta}\\in\\mathbb{B}_{d}(D_{\\theta})\\Big\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "667 we have that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}(\\mathcal{V},\\left\\|\\cdot\\right\\|_{\\infty},\\epsilon)\\leq\\mathcal{N}\\left(\\Pi\\left(D_{\\pi}\\right),\\left\\|\\cdot\\right\\|_{\\infty,1},\\epsilon/2R D_{\\theta}\\right)\\times\\mathcal{N}\\left(\\mathbb{B}_{d}(D_{\\theta}),\\left\\|\\cdot\\right\\|_{2},\\epsilon/2R\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times}\\\\ &{\\qquad\\qquad\\quad\\log\\mathcal{N}(\\mathcal{V},\\left\\|\\cdot\\right\\|_{\\infty},\\epsilon)\\leq2d\\log\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "668 and, ", "page_idx": 25}, {"type": "text", "text": "669 Proof. Let $C_{\\pi}$ denote the $\\epsilon_{\\pi}$ -cover of $\\Pi\\left(D_{\\pi}\\right)$ with respect to the norm $\\|\\cdot\\|_{\\infty,1}$ and $C_{\\theta}$ the $\\epsilon_{\\theta}$ -cover   \n670 of $\\mathbb{B}_{d}(D_{\\theta})$ under the $L^{2}$ -norm. For $(\\pi,\\pmb{\\theta})\\in\\Pi\\left(D_{\\pi}\\right)\\times\\mathbb{B}_{d}(D_{\\pmb{\\theta}})$ and $(\\pi^{\\prime},\\pmb{\\theta}^{\\prime})\\in C_{\\pi}\\times C_{\\pmb{\\theta}}$ , it follows   \n671 that for any state $x\\in\\mathscr{X}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|v_{\\pi,\\theta}(s)-v_{\\pi^{\\prime},\\theta^{\\prime}}(s)|=\\displaystyle\\left|\\sum_{a\\in A}\\pi(a|x)\\left\\langle\\varphi(x,a),\\theta\\right\\rangle-\\pi^{\\prime}(a|x)\\left\\langle\\varphi(x,a),\\theta^{\\prime}\\right\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\left|\\sum_{a\\in A}\\left(\\pi(a|x)-\\pi^{\\prime}(a|x)\\right)\\left\\langle\\varphi(x,a),\\theta\\right\\rangle+\\sum_{a\\in A}\\pi^{\\prime}(a|x)\\left\\langle\\varphi(x,a),\\theta-\\theta^{\\prime}\\right\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq R D_{\\theta}\\displaystyle\\sum_{a\\in A}|\\pi(a|x)-\\pi^{\\prime}(a|x)|+R\\displaystyle\\sum_{a\\in A}\\pi^{\\prime}(a|x)\\left\\|\\theta-\\theta^{\\prime}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "672 Let $C_{v}=\\Big\\{v_{\\pi,\\theta}:\\mathcal{X}\\rightarrow[-R D_{\\theta},R D_{\\theta}]\\Big|\\pi\\in C_{\\pi},\\pmb{\\theta}\\in C_{\\theta}\\Big\\}$ . Then, $C_{v}$ is an $\\epsilon$ -cover of $\\mathcal{V}$ with respect   \n673 to the $L^{\\infty}$ -norm when $\\epsilon_{\\pi}\\,=\\,\\epsilon/2R D_{\\theta}$ and $\\epsilon_{\\theta}=\\epsilon/2R$ . Therefore, we can derive a bound on the   \n674 covering number of $C_{v}$ as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}\\left(\\mathcal{V},\\left\\|\\cdot\\right\\|_{\\infty},\\epsilon\\right)\\leq\\mathcal{N}\\left(\\Pi\\left(D_{\\pi}\\right),\\left\\|\\cdot\\right\\|_{\\infty,1},\\epsilon/2R D_{\\theta}\\right)\\times\\mathcal{N}\\left(\\mathbb{B}_{d}(D_{\\theta}),\\left\\|\\cdot\\right\\|_{2},\\epsilon/2R\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)^{d}\\left(1+\\frac{4R D_{\\theta}}{\\epsilon}\\right)^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "675 Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\mathcal{V},\\left\\Vert\\cdot\\right\\Vert_{\\infty},\\epsilon\\right)\\leq2d\\log\\left(1+\\frac{4R D_{\\pi}R D_{\\theta}}{\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "676 This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "677 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "678 1. Claims   \n679 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n680 paper\u2019s contributions and scope?   \n681 Answer: [Yes]   \n682 Justification:   \n683 Guidelines:   \n684 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n685 made in the paper.   \n686 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n687 contributions made in the paper and important assumptions and limitations. A No or   \n688 NA answer to this question will not be perceived well by the reviewers.   \n689 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n690 much the results can be expected to generalize to other settings.   \n691 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n692 are not attained by the paper.   \n693 2. Limitations   \n694 Question: Does the paper discuss the limitations of the work performed by the authors?   \n695 Answer: [Yes]   \n696 Justification:   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n699 the paper has limitations, but those are not discussed in the paper.   \n700 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n701 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n702 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n703 model well-specification, asymptotic approximations only holding locally). The authors   \n704 should reflect on how these assumptions might be violated in practice and what the   \n705 implications would be.   \n706 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n707 only tested on a few datasets or with a few runs. In general, empirical results often   \n708 depend on implicit assumptions, which should be articulated.   \n709 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n710 For example, a facial recognition algorithm may perform poorly when image resolution   \n711 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n712 used reliably to provide closed captions for online lectures because it fails to handle   \n713 technical jargon.   \n714 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n715 and how they scale with dataset size.   \n716 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n717 address problems of privacy and fairness.   \n718 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n719 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n720 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n721 judgment and recognize that individual actions in favor of transparency play an impor  \n722 tant role in developing norms that preserve the integrity of the community. Reviewers   \n723 will be specifically instructed to not penalize honesty concerning limitations.   \n724 3. Theory Assumptions and Proofs   \n25 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n26 a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "728 Justification:   \n729 Guidelines:   \n730 \u2022 The answer NA means that the paper does not include theoretical results.   \n731 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n732 referenced.   \n733 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n734 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n735 they appear in the supplemental material, the authors are encouraged to provide a short   \n736 proof sketch to provide intuition.   \n737 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n738 by formal proofs provided in appendix or supplemental material.   \n739 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "740 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "741 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n742 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n743 of the paper (regardless of whether the code and data are provided or not)?   \n744 Answer: [NA]   \n745 Justification: This is a theory paper, there are no experiments.   \n746 Guidelines: ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n8 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n9 well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n1 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n2 to make their results reproducible or verifiable.   \n3 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n4 For example, if the contribution is a novel architecture, describing the architecture fully   \n5 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n6 be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often   \n8 one good way to accomplish this, but reproducibility can also be provided via detailed   \n9 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n0 of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n2 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n3 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n4 nature of the contribution. For example   \n5 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n6 to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe   \n8 the architecture clearly and fully.   \n9 (c) If the contribution is a new model (e.g., a large language model), then there should   \n0 either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct   \n2 the dataset).   \n3 (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in   \n6 some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "78 5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "779 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n780 tions to faithfully reproduce the main experimental results, as described in supplemental   \n781 material?   \n782 Answer: [NA]   \n783 Justification: This is a theory paper, there is no data or code.   \n784 Guidelines:   \n785 \u2022 The answer NA means that paper does not include experiments requiring code.   \n786 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n787 public/guides/CodeSubmissionPolicy) for more details.   \n788 \u2022 While we encourage the release of code and data, we understand that this might not be   \n789 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n790 including code, unless this is central to the contribution (e.g., for a new open-source   \n791 benchmark).   \n792 \u2022 The instructions should contain the exact command and environment needed to run to   \n793 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n794 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n795 \u2022 The authors should provide instructions on data access and preparation, including how   \n796 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n797 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n798 proposed method and baselines. If only a subset of experiments are reproducible, they   \n799 should state which ones are omitted from the script and why.   \n800 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n801 versions (if applicable).   \n802 \u2022 Providing as much information as possible in supplemental material (appended to the   \n803 paper) is recommended, but including URLs to data and code is permitted.   \n804 6. Experimental Setting/Details   \n805 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n806 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n807 results?   \n808 Answer: [NA]   \n809 Justification: This is a theory paper, there are no experiments.   \n810 Guidelines:   \n811 \u2022 The answer NA means that the paper does not include experiments.   \n812 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n813 that is necessary to appreciate the results and make sense of them.   \n814 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n815 material.   \n816 7. Experiment Statistical Significance   \n817 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n818 information about the statistical significance of the experiments?   \n819 Answer: [NA]   \n820 Justification: This is a theory paper, there are no experiments.   \n821 Guidelines:   \n822 \u2022 The answer NA means that the paper does not include experiments.   \n823 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n824 dence intervals, or statistical significance tests, at least for the experiments that support   \n825 the main claims of the paper.   \n826 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n827 example, train/test split, initialization, random drawing of some parameter, or overall   \n828 run with given experimental conditions).   \n829 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n830 call to a library function, bootstrap, etc.)   \n831 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n832 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n833 of the mean.   \n834 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n835 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n836 of Normality of errors is not verified.   \n837 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n838 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n839 error rates).   \n840 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n841 they were calculated and reference the corresponding figures or tables in the text.   \n842 8. Experiments Compute Resources   \n843 Question: For each experiment, does the paper provide sufficient information on the com  \n844 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n845 the experiments?   \n846 Answer: [NA]   \n847 Justification: This is a theory paper, there are no experiments.   \n848 Guidelines:   \n849 \u2022 The answer NA means that the paper does not include experiments.   \n850 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n851 or cloud provider, including relevant memory and storage.   \n852 \u2022 The paper should provide the amount of compute required for each of the individual   \n853 experimental runs as well as estimate the total compute.   \n854 \u2022 The paper should disclose whether the full research project required more compute   \n855 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n856 didn\u2019t make it into the paper).   \n857 9. Code Of Ethics   \n858 Question: Does the research conducted in the paper conform, in every respect, with the   \n859 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n860 Answer: [Yes]   \n861 Justification:   \n862 Guidelines:   \n863 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n864 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n865 deviation from the Code of Ethics.   \n866 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n867 eration due to laws or regulations in their jurisdiction).   \n868 10. Broader Impacts   \n869 Question: Does the paper discuss both potential positive societal impacts and negative   \n870 societal impacts of the work performed?   \n871 Answer: [NA]   \n872 Justification: This is a theory paper, with no specific societal impacts foreseen in the near   \n873 future.   \n874 Guidelines:   \n875 \u2022 The answer NA means that there is no societal impact of the work performed.   \n876 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n877 impact or why the paper does not address societal impact.   \n878 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n879 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n880 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n881 groups), privacy considerations, and security considerations.   \n882 \u2022 The conference expects that many papers will be foundational research and not tied   \n883 to particular applications, let alone deployments. However, if there is a direct path to   \n884 any negative applications, the authors should point it out. For example, it is legitimate   \n885 to point out that an improvement in the quality of generative models could be used to   \n886 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n887 that a generic algorithm for optimizing neural networks could enable people to train   \n888 models that generate Deepfakes faster.   \n889 \u2022 The authors should consider possible harms that could arise when the technology is   \n890 being used as intended and functioning correctly, harms that could arise when the   \n891 technology is being used as intended but gives incorrect results, and harms following   \n892 from (intentional or unintentional) misuse of the technology.   \n893 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n894 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n895 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n896 feedback over time, improving the efficiency and accessibility of ML).   \n897 11. Safeguards   \n898 Question: Does the paper describe safeguards that have been put in place for responsible   \n899 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n900 image generators, or scraped datasets)?   \n901 Answer: [NA]   \n902 Justification: This is a theory paper, this question is not relevant.   \n903 Guidelines:   \n904 \u2022 The answer NA means that the paper poses no such risks.   \n905 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n906 necessary safeguards to allow for controlled use of the model, for example by requiring   \n907 that users adhere to usage guidelines or restrictions to access the model or implementing   \n908 safety filters.   \n909 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n910 should describe how they avoided releasing unsafe images.   \n911 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n912 not require this, but we encourage authors to take this into account and make a best   \n913 faith effort.   \n914 12. Licenses for existing assets   \n915 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n916 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n917 properly respected?   \n918 Answer: [NA]   \n919 Justification: This is a theory paper, there are no assets used.   \n920 Guidelines:   \n921 \u2022 The answer NA means that the paper does not use existing assets.   \n922 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n923 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n924 URL.   \n925 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n926 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n927 service of that source should be provided.   \n928 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n929 package should be provided. For popular datasets, paperswithcode.com/datasets   \n930 has curated licenses for some datasets. Their licensing guide can help determine the   \n931 license of a dataset.   \n932 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n933 the derived asset (if it has changed) should be provided.   \n934 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n935 the asset\u2019s creators.   \n936 13. New Assets   \n937 Question: Are new assets introduced in the paper well documented and is the documentation   \n938 provided alongside the assets?   \n939 Answer: [NA]   \n940 Justification: This is a theory paper, there are no new assets.   \n941 Guidelines:   \n942 \u2022 The answer NA means that the paper does not release new assets.   \n943 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n944 submissions via structured templates. This includes details about training, license,   \n945 limitations, etc.   \n946 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n947 asset is used.   \n948 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n949 create an anonymized URL or include an anonymized zip file.   \n950 14. Crowdsourcing and Research with Human Subjects   \n951 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n952 include the full text of instructions given to participants and screenshots, if applicable, as   \n953 well as details about compensation (if any)?   \n954 Answer: [NA]   \n955 Justification: This is a theory paper, we haven\u2019t used crowdsourced data or worked with   \n956 human subjects.   \n957 Guidelines:   \n958 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n959 human subjects.   \n960 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n961 tion of the paper involves human subjects, then as much detail as possible should be   \n962 included in the main paper.   \n963 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n964 or other labor should be paid at least the minimum wage in the country of the data   \n965 collector.   \n966 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n967 Subjects   \n968 Question: Does the paper describe potential risks incurred by study participants, whether   \n969 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n970 approvals (or an equivalent approval/review based on the requirements of your country or   \n971 institution) were obtained?   \n972 Answer: [NA]   \n973 Justification: This is a theory paper, we haven\u2019t worked with human subjects.   \n974 Guidelines:   \n975 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n976 human subjects.   \n977 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n978 may be required for any human subjects research. If you obtained IRB approval, you   \n979 should clearly state this in the paper.   \n980 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n981 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n982 guidelines for their institution.   \n983 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n984 applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]