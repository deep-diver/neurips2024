[{"figure_path": "lPDxPVS6ix/tables/tables_5_1.jpg", "caption": "Table 1: Comparison to prior work in the image domain.", "description": "This table compares SPEAR's performance against prior gradient inversion attacks on the ImageNet dataset.  It shows that SPEAR achieves significantly higher PSNR (Peak Signal-to-Noise Ratio) values, indicating better reconstruction quality, and is substantially faster than the other methods.", "section": "6 Empirical Evaluation"}, {"figure_path": "lPDxPVS6ix/tables/tables_6_1.jpg", "caption": "Table 1: Comparison to prior work in the image domain.", "description": "This table compares the performance of SPEAR against two other gradient inversion attack methods (CI-Net and Geiping et al.) on the ImageNet dataset.  The comparison considers the peak signal-to-noise ratio (PSNR), which is a metric for image reconstruction quality. Higher PSNR indicates better reconstruction quality. The table also compares the time each method takes to reconstruct one batch of images.  SPEAR significantly outperforms the other methods in terms of PSNR and speed.", "section": "6.1 Comparison to Prior Work"}, {"figure_path": "lPDxPVS6ix/tables/tables_7_1.jpg", "caption": "Table 2: Results vs prior work in the tabular domain.", "description": "This table compares SPEAR's performance against Tableak [8], a state-of-the-art gradient inversion attack in the tabular domain, on the ADULT dataset.  It evaluates the methods based on three metrics: Discrete Accuracy (Discr Acc %), measuring the percentage of correctly classified discrete features; Continuous Mean Absolute Error (Cont. MAE), indicating the average absolute difference between the reconstructed and original values for continuous features; and Time/Batch, showing the time taken for reconstruction per batch of data.", "section": "6.1 Comparison to Prior Work"}, {"figure_path": "lPDxPVS6ix/tables/tables_7_2.jpg", "caption": "Table 3: Reconstruction quality across 100 batches.", "description": "This table presents the reconstruction quality metrics (PSNR, LPIPS, Accuracy) and time per batch for different datasets (MNIST, CIFAR-10, TinyImageNet, and IMAGENET at two resolutions).  It showcases the effectiveness of SPEAR in achieving near-perfect reconstruction (99% accuracy) across various datasets and image sizes.", "section": "6.2 Main Results"}, {"figure_path": "lPDxPVS6ix/tables/tables_8_1.jpg", "caption": "Table 4: Effect of the attacked layer's depth l (1 \u2264 l \u2264 6) on reconstruction time and quality for 100 TINYIMAGENET batches of size b = 20.", "description": "This table shows the effect of targeting different layers of a neural network (from the first layer to the fifth) on the reconstruction performance of the SPEAR algorithm.  The results demonstrate that while SPEAR successfully reconstructs inputs to all layers, the computation time increases significantly as the target layer moves deeper into the network. It also shows a drop in reconstruction accuracy as the target layer is deeper.  The results are based on 100 batches of images from the TINYIMAGENET dataset, each batch containing 20 images.", "section": "6.2 Main Results"}, {"figure_path": "lPDxPVS6ix/tables/tables_8_2.jpg", "caption": "Table 5: Comparison between the reconstruction quality of Geiping et al. [1] and a version of SPEAR that uses Geiping et al. [1] to speed up its search procedure evaluated on 10 TINYIMAGENET batches.", "description": "This table compares the reconstruction quality (accuracy and PSNR) achieved by Geiping et al.'s method and a modified version of SPEAR that incorporates Geiping et al.'s approach to enhance its search efficiency.  The comparison is performed on 10 batches from the TINYIMAGENET dataset for two different network sizes (m=400 and m=2000) and batch sizes (b=50 and b=100).  It shows that SPEAR, when combined with the optimization of Geiping et al. can achieve significantly better PSNR values.", "section": "6.3 Scaling SPEAR via Optimization-based Attacks"}, {"figure_path": "lPDxPVS6ix/tables/tables_8_3.jpg", "caption": "Table 6: Comparison between the reconstructions on VGG16 for Geiping et al. [1], CPA [9], and SPEAR for 10 IMAGENET batches (b = 16).", "description": "This table compares the reconstruction quality of three different methods on the ImageNet dataset using a VGG16 convolutional neural network.  The methods compared are Geiping et al. [1] (a baseline approximate reconstruction method), CPA [9] + FI + Geiping et al. [1] (Cocktail Party Attack with Feature Inversion using Geiping et al. [1] for feature recovery), and SPEAR + FI + Geiping et al. [1] (the proposed method SPEAR combined with Feature Inversion and Geiping et al. [1]).  The quality is assessed using two metrics: LPIPS (lower is better) and Feature Similarity (higher is better).  The results show that SPEAR, combined with the other methods, achieves the best performance in both metrics, indicating superior reconstruction quality.", "section": "6.4 Feature Inversion in Convolutional Neural Networks"}, {"figure_path": "lPDxPVS6ix/tables/tables_16_1.jpg", "caption": "Table 1: Comparison to prior work in the image domain.", "description": "This table compares SPEAR's performance against two prior gradient inversion attacks (CI-Net and Geiping et al.) on the ImageNet dataset.  It shows that SPEAR achieves significantly higher PSNR (Peak Signal-to-Noise Ratio) values, indicating much better reconstruction quality, and is also substantially faster.  The comparison highlights SPEAR's superiority in terms of both accuracy and efficiency.", "section": "6.1 Comparison to Prior Work"}, {"figure_path": "lPDxPVS6ix/tables/tables_18_1.jpg", "caption": "Table 8: Mean reconstruction quality metrics across 100 batches for batches only containing samples from only one class in the same setting as Table 3.", "description": "This table presents the mean reconstruction quality metrics (PSNR, SSIM, MSE, LPIPS, and Accuracy) for 100 batches of images from the TINYIMAGENET dataset.  A key characteristic of these batches is that they only contain samples from a single class (label-homogeneous data). The results are compared against those from Table 3, which uses heterogeneous data.", "section": "E.2 Experiments on Label-Heterogeneous Data"}, {"figure_path": "lPDxPVS6ix/tables/tables_21_1.jpg", "caption": "Table 9: Reconstruction quality across 100 batches of size b = 20 computed on TINYIMAGENET for gradients computed with DPSGD [28] with different noise levels \u03c3 and gradient clipping levels C.", "description": "This table presents the results of reconstruction experiments using the SPEAR algorithm on the TINYIMAGENET dataset. The experiments involved adding different levels of noise (\u03c3) and gradient clipping (C) to the gradients during training using the DP-SGD defense mechanism. The table shows the PSNR and accuracy achieved by the SPEAR algorithm under various noise and clipping conditions. It demonstrates the robustness of the SPEAR algorithm to noise and clipping.", "section": "6 Empirical Evaluation"}, {"figure_path": "lPDxPVS6ix/tables/tables_22_1.jpg", "caption": "Table 10: Reconstruction quality across 100 FedAvg client updates computed on TINYIMAGENET batches of size b = 20 for different number of epochs \u03b5 and different mini batch sizes bmini.", "description": "This table presents the results of an experiment evaluating the performance of SPEAR against the FedAvg algorithm.  The experiment varied the number of local client epochs (\u03b5), and the size of mini-batches (bmini) used in the FedAvg updates while keeping the total number of data points used in each local update fixed at b=20.  The PSNR and accuracy are reported for each configuration. This demonstrates SPEAR's robustness and effectiveness even when applied to aggregated gradients from multiple local update steps.", "section": "6.2 Main Results"}, {"figure_path": "lPDxPVS6ix/tables/tables_22_2.jpg", "caption": "Table 11: Reconstruction quality across 100 FedAvg client updates computed on TINYIMAGENET batches of size b = 20 for different local client learning rates \u03b7.", "description": "This table presents the reconstruction quality (PSNR and Accuracy) achieved by SPEAR on the TINYIMAGENET dataset using FedAvg updates.  It shows the results for 100 batches of size 20, varying the local learning rate (\u03b7) across three values (0.1, 0.01, 0.001). The number of local epochs (E) and mini-batch size (bmini) are kept constant at 5 and 20 respectively.  The table demonstrates the robustness of SPEAR's performance across different learning rates.", "section": "6.2 Main Results"}]