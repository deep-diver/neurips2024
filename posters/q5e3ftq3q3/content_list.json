[{"type": "text", "text": "Almost Minimax Optimal Best Arm Identification in Piecewise Stationary Linear Bandits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunlong Hou ", "page_idx": 0}, {"type": "text", "text": "Vincent Y. F. Tan ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics Department of Mathematics National University of Singapore Department of Electrical and Computer Engineering yhou@u.nus.edu National University of Singapore vtan@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Zixin Zhong Data Science and Analytics Thrust Hong Kong University of Science and Technology (Guangzhou) zixinzhong@hkust-gz.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a novel piecewise stationary linear bandit (PSLB) model, where the environment randomly samples a context from an unknown probability distribution at each changepoint, and the quality of an arm is measured by its return averaged over all contexts. The contexts and their distribution, as well as the changepoints are unknown to the agent. We design Piecewise-Stationary $\\varepsilon$ -BestArmIdentification+ $(\\mathbf{PS}\\varepsilon\\mathbf{BAI^{+}})$ , an algorithm that is guaranteed to identify an $\\varepsilon$ -optimal arm with probability $\\geq1-\\delta$ and with a minimal number of samples. ${\\tt P S}\\bar{\\varepsilon}{\\tt B A I^{+}}$ consists of two subroutines, ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ and NAiVE $\\varepsilon$ -BAI $(\\mathrm{N}\\varepsilon\\mathrm{B}\\mathrm{A}\\mathrm{I})$ ,which are executed in parallel. $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ actively detects changepoints and aligns contexts to facilitate the arm identification process. When $\\mathrm{PS}\\varepsilon$ BAI and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ are utilized judiciously in parallel, ${\\tt P S}{\\tt E B A I}^{+}$ is shown to have a finite expected sample complexity. By proving a lower bound, we show the expected sample complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is optimal up to a logarithmic factor. We compare ${\\tt P S E B A I}^{+}$ to baseline algorithms using numerical experiments which demonstrate its efficiency. Both our analytical and numerical results corroborate that the efficacy of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is due to the delicate change detection and context alignment procedures embedded in $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In stochastic multi-armed bandits (MABs), an agent interacts with the environment at each time step. The agent pulls an arm and observes the corresponding return provided by the environment. The classical MAB framework assumes a stationary environment where the expected return of each arm remains unchanged over time. However, we usually face ever-changing environments in real life. For instance, in investment option selection and portfolio management, fund managers want to select a subset of good candidate portfolios. However, the market may be bullish, bearish, or in some other state. The transition between these states can be well-modelled as being stochastic. We wish to select portfolios that yield the best long-term option under such a piecewise stationary environment. Further examples such as one based on agriculture in the face of stochastically changing weather patterns are discussed in detail in Appendix A. These motivate us to formulate and investigate a piecewise stationary linearbandit(PSLB)model. ", "page_idx": 0}, {"type": "text", "text": "Our PSLB model is equipped with an arm set $\\mathcal{X}$ ,acontextset $\\Theta$ and a deterministic but unknown sequenceofchangepoints $\\mathcal{C}$ . At each changepoint, the environment samples a context $\\theta\\in\\Theta$ from an unknown probability distribution $P_{\\theta}$ , and the returns of arms may change when the context changes. The return of each arm under each context is determined by its feature $x\\in\\mathscr{X}$ and the context $\\theta$ . In particular, the expected return of an arm is the weighted sum $\\mu_{x}=\\mathbb{E}_{P_{\\theta}}[x^{\\top}\\theta]$ .While the sequence of changepoints, as well as the distribution and latent vectors of contexts are not known, the agent samples an arm and observes the corresponding return at each time step so that it can identify the best arm arg $\\operatorname*{max}_{x}\\mu_{x}$ up to some tolerance $\\varepsilon$ with probability $\\geq1-\\delta$ and with as few samples as possible. The agent's behavior does not affect the sequence of contexts that is drawn from $P_{\\theta}$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Main Contributions. We are the first to study the fixed-confidence best arm identification (BAI) problem in piecewise stationary bandits (PsB). Given $\\delta\\,>\\,0$ ,we say the arm with the highest expected return $\\mu^{*}$ is the best, and an arm is $\\varepsilon$ -optimal if its expected return is at least $\\mu^{*}-\\varepsilon$ We seek to design an $(\\varepsilon,\\delta)$ -PAC algorithm which can identify an $\\varepsilon$ -optimal arm with probability $\\geq1-\\delta$ in as few time steps as possible, i.e., with minimal sample complexity. ", "page_idx": 1}, {"type": "text", "text": "Our first contribution concerns the formulation of a novel PSLB model, where we measure the quality of an arm $x$ according to its expected return $\\mu_{x}=\\mathbb{E}_{\\theta\\sim P_{\\theta}}[x^{\\top}\\theta]$ for the following reasons. Consider that an arm is measured by its average return across time, which is a generalization of the definition in stationary bandits (SB). A notable feature of PSB models is that the context changes as time evolves, and hence the arm's average return across time also changes, in general. Hence, we aim to identify an arm whose average return across contexts is high, and benefits the agent for interacting with the environment in the long run after the arm identification task. We are thus inspired to introduce the distribution of contexts under the PSLB model, define the expected return $\\mu_{x}$ for each arm, and use this ensemble (non-time varying) statistic as the benchmark for what we seek to learn. The BAI task using this statistic is meaningful but challenging, as the agent needs to reliably estimate the context vectors, changepoints, and context distribution. ", "page_idx": 1}, {"type": "text", "text": "Secondly, we propose PIECEWISE-STATIONARY $\\varepsilon{\\mathrm{-}}\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ $({\\bf P}{\\bf S}\\varepsilon{\\bf B}{\\bf M}^{+})$ 0, an algorithm designed to tackle the BAI problem in our PSLB model. We prove that it is $(\\varepsilon,\\delta)$ -PAC and bound its sample complexity. ${\\tt P S E B A I^{+}}$ samples arms according to a suitably defined G-optimal allocation, and runs two algorithms: NAiVE $\\varepsilon$ BAI $({\\mathrm{N}}{\\varepsilon}{\\mathrm{B}}{\\mathrm{AI}})$ and $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ as subroutines in parallel to achieve efficiency and attain a bound on the sample complexity in expectation. ", "page_idx": 1}, {"type": "text", "text": "\u25cf Being a baseline but naive algorithm, the complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ grows linearly with the maximum length between two changepoints $L_{\\mathrm{max}}$ , motivating us to design a more effcient algorithm, $\\mathrm{PS}\\varepsilon$ BAI, to reduce the impact of $L_{\\mathrm{max}}$ ", "page_idx": 1}, {"type": "text", "text": "$\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ is equipped with two delicately designed subroutines LINEAR-CHANGE DETECTION (LCD) and LINEAR-CONTEXT ALIGNMENT (LCA) to actively detect changepoints and align contexts with those observed in the previous time steps respectively. Concretely, in terms of the design, $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ determines whether samples from two intervals are under the identical context via a sliding window mechanism, and detects changepoints and aligns contexts accordingly; this facilitates the estimation of context vectors and their distribution. Combining these elements into the design of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and analyzing them requires some care. On the theoretical side, we prove $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ identifies an $\\varepsilon$ -optimal arm faster than $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ with high probability. The success of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ relies on the LCD and LCA subroutines, while a minor drawback is that they require a non-vanishing failure probability budget which does not allow us to bound the sample complexity of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ in expectation. To achieve a complete theoretical understanding, we delicately design the ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm whose efficiency is inherited via the LCD and LCA procedures in $\\mathrm{PS}{\\varepsilon}\\mathbf{B}\\mathbf{A}\\mathbf{I}$ as well as the effective utilization of running $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ in parallel. ", "page_idx": 1}, {"type": "text", "text": "Thirdly, we derive a lower bound on the complexity of any $(\\varepsilon,\\delta)$ -PAC algorithm in PSLB models. To derive this bound, we first lower bound the complexity of an algorithm when the contextual information (and changepoints) are available, and then quantify the number of arm samples (and realized contexts) required to reliably infer an $\\varepsilon$ -best arm. We compare the upper bound of $\\bar{\\mathrm{PS}}\\varepsilon\\mathrm{BAI^{+}}$ and this generic lower bound in several instances. The matching (up to logarithmic terms) of bounds illustrate that our $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ algorithm is almost asymptotically optimal. ", "page_idx": 1}, {"type": "text", "text": "Lastly, we demonstrate the efficiency of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ with numerical experiments. The first half of our experiment shows that ${\\tt P S E B A I}^{+}$ is $(\\varepsilon,\\delta)$ -PAC and with significantly lower sample complexity compared to $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ , corroborating our theoretical findings. In the second half, we compare ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ to $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ , and two other benchmarks DISTRIBUTION $\\varepsilon$ -BAI $({\\mathrm{D}}{\\varepsilon}{\\mathrm{B}}{\\mathrm{AI}})$ and $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ .While contexts and changepoints are not available to ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ , they are observed by $\\scriptstyle\\mathrm{D}\\varepsilon\\mathrm{BAI}$ and $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ . Nevertheless, ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is still competitive compared to $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and $\\scriptstyle\\mathbf{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ in our empirical experiments. Hence, both experiments justify the necessity of the change detection and context alignment procedures for boosting the learning of contexts and their distributions, as well as the identification of the best arm. We also show empirically that misspecifications to the knowledge Oof $L_{\\mathrm{min}}$ and $L_{\\mathrm{max}}$ do not affect the performance of our algorithm significantly. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Related work. Best arm identification (BAI) and regret minimization (RM) are two fundamental problems in multi-armed bandits. In stationary linear bandits, [1, 2] focus on BAI and [3, 4] aim to solve the RM problem. An efficient algorithm can choose the G-optimal allocation or $\\mathcal{X Y}$ -allocation rule to quickly identify a good arm [1, 5]. Besides, [6, 7] focus on $\\varepsilon$ -optimal arm identification, which is a generalization of the standard BAI problem. ", "page_idx": 2}, {"type": "text", "text": "The BAI and RM problems are also studied in thr non-stationary bandits (NSB), where in contrast to the SB model, the context varies with time [8-10]. NSB can be largely divided into two classes: the drifting bandit (DB) model, where the context changes at each step [8], and PSB, where the context changes less frequently [10]. [11] provides an extensive discussion on the definition of NSB models. ", "page_idx": 2}, {"type": "text", "text": "On one hand, the RM problem have been investigated extensively in DB models [12, 13]. On the other hand, concerning the BAI task in DB models, [14] investigated BAI with a fixed-horizon, where the best arm has the highest average return over this horizon; [15] assumes the best arm remains unchanged after certain time step and explores the fixed-confidence setting. Besides, when the contextual information in NSB models is available, NSB models are known as contextual bandit (CB) models [16-18]. [16] showed that the contextual information accelerates the best arm identification process. More discussions on DB and CB models are presented in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "Moreover, the context can drift dramatically in a DB model while it remains unvarying in a SB model. Straddling between the DB and SB models, PSB models assume there is an interim stationary interval between each two consecutive changepoints, where the context remains unchanged. The context changes can be characterized in different ways and affect the performance of proposed algorithms in PSB models. For instance, a changepoint signals the return of at least one arm shift as in [10], and indicates the best arm changes as in [19]. ", "page_idx": 2}, {"type": "text", "text": "In PSB models, a large body of works focus on RM. While [10, 20-22] equip their algorithms with changepoint detection techniques to handle the context changes, [23] actively checks the quality of each arm. However, there is no existing work on the fixed-confidence BAI problem in a PSB model. To fill this gap in the literature, we design ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI^{+}}}$ for $\\varepsilon$ -optimal arm identification in our proposed PSLB model.We show $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ is almost asymptotically optimal by comparing its complexity to a generic lower bound of all algorithms, and validate its efficiency through numerical experiments. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For $m\\,\\in\\,\\mathbb{N}$ , let $[m]\\,:=\\,\\{1,2,\\dots,m\\}$ . For a finite set $S$ , let $\\Delta_{S}$ denote the $|S|$ -dim probability simplex on $S$ . Let $\\begin{array}{r}{A(\\mathbf{q}):=\\sum_{x\\in\\mathcal{X}}q_{x}x x^{\\top}}\\end{array}$ be the matrix induced by $\\mathbf{q}\\in\\Delta_{\\mathcal{X}}$ with $\\mathcal{X}\\subset\\mathbb{R}^{d}$ .An instance of piecewise stationary linear bandit is a tuple $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ . Specifically, $x\\in\\mathbb{R}^{d}$ is an arm (vector) and the arm set $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is composed of $|\\mathcal{X}|=K$ arms that spans $\\mathbb{R}^{d}$ . The latent vector matrix $\\Theta=(\\theta_{1}^{*},\\dots,\\theta_{N}^{*})\\in\\mathbb{R}^{d\\times N}$ contains $N$ latent column vectors where the $j^{\\mathrm{th}}$ column $\\theta_{j}^{*}$ is associated with context $j\\in[N]$ . For the sake of normalization, we assume $|x^{\\top}\\theta_{j}^{*}|\\leq1$ for all $x\\in\\mathcal{X},j\\in[N]$ . Let $P_{\\theta}$ denote the distribution (probability mass function) of the latent vectors and $p_{j}=P_{\\theta}[\\theta_{j}^{*}]$ . We represent the probabilities of latent vectors as $\\mathbf{p}=(p_{1},\\ldots,p_{N})\\in\\Delta_{N}$ The fixed but unknown sequence of changepoints ${\\mathcal{C}}:=(c_{1},c_{2},...)$ is an sequence of increasing positive integers $1=c_{1}<c_{2}<...$ , characterizing all the changepoints (time steps). ", "page_idx": 2}, {"type": "text", "text": "The return of arm $x$ under latent context $j$ is a random variable $Y\\;=\\;x^{\\top}\\theta_{j}^{*}\\;+\\;\\eta$ \uff0cwhere $\\eta$ is a zero-mean random variable (noise) supported on $[-1,1]$ , and the expected return of arm $x$ is $\\begin{array}{r}{\\mu_{x}:=\\mathbb{E}_{\\theta\\sim P_{\\theta}}[x^{\\top}\\theta]=\\sum_{j=1}^{N}P_{\\theta}[\\theta_{j}^{*}]x^{\\top}\\theta_{j}^{*}}\\end{array}$ Thbe $x^{*}:=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}\\mu_{x}$ with mean $\\mu^{*}:=\\mu_{x^{*}}$ . Given a slackness parameter , we define the set of $\\varepsilon$ -best arms $\\mathcal{X}_{\\varepsilon}:=\\{x\\in\\mathcal{X}:\\mu_{x}\\geq\\mu^{*}-\\varepsilon\\}$ . For each pair of arms $(x,\\tilde{x})\\in\\mathcal{X}^{2}$ , define the contextual mean gap between $x$ and $\\tilde{x}$ under latent context $j$ as $\\Delta_{j}(x,\\tilde{x}):=(x-\\tilde{x})^{\\top}\\theta_{j}^{*}$ and the mean gap between $x$ and $\\tilde{x}$ as $\\Delta(x,\\tilde{x}):=\\mu_{x}-\\mu_{\\tilde{x}}$ ", "page_idx": 2}, {"type": "text", "text": "Given $l\\in\\mathbb N$ the interval $(c_{l},\\ldots,c_{l+1}-1)$ is known as the $l^{\\mathrm{th}}$ stationary segment and its length is $L_{l}:=c_{l+1}-c_{l}$ . We assume $L_{\\mathrm{min}}\\le L_{l}\\le L_{\\mathrm{max}}$ . Let $l_{t}:=\\operatorname*{max}\\{l:c_{l}\\leq t\\}$ denote the number of stationary segments up to time step $t$ . At time step $t\\in[T]$ (see Dynamics 1), (i) If $t\\,\\in\\,{\\mathcal{C}}$ , the environment samples a latent vector $\\theta_{j_{t}}^{*}$ according to $P_{\\theta}$ , that is, it generates a (latent) context sample with $P_{\\theta}$ ; otherwise the latent vector remains unchanged, i.e. $\\theta_{j_{t}}^{*}=\\theta_{j_{t-1}}^{*}$ . The contexts $\\{\\theta_{j_{c_{l}}}^{*}\\}_{l\\in\\mathbb{N}}$ are sampled ii.d. from $P_{\\theta}$ at each changepoint $\\{c_{l}\\}_{l\\in\\mathbb{N}}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "(i) The agent pulls an arm $x_{t}$ and observes the stochastic return $Y_{t,x_{t}}=x_{t}^{\\top}\\theta_{j_{t}}^{*}+\\eta_{t}$ \uff0cwhere $\\eta_{t}$ is drawn independently from a distribution supported on $[-1,1]$ ", "page_idx": 3}, {"type": "text", "text": "The agent uses an online algorithm $\\pi:=\\{(\\pi_{t},\\tau_{t},r_{t})\\}_{t\\in\\mathbb{N}}$ to actively interact with the instance $\\Lambda$ and only has access to the arm set $\\mathcal{X}$ , number of latent vectors $N$ , the bounds on the length of each segment $L_{\\mathrm{min}}$ and $L_{\\mathrm{max}}$ , the slackness parameter $\\varepsilon$ , and the confidence parameter $\\delta$ ", "page_idx": 3}, {"type": "text", "text": "$\\bullet$ sampling rule $\\pi_{t}:\\mathcal{H}_{t}^{\\pi}:=\\big((x_{s}^{\\pi},Y_{s,x_{s}^{\\pi}})\\big)_{s\\in[t-1]}\\to\\mathcal{X}$ samples an arm $\\boldsymbol{x}_{t}^{\\pi}$ based on the observation history $\\mathcal{H}_{t}^{\\pi}$ and observe the corresponding random return $Y_{t,x_{t}^{\\pi}}$ .\uff0c $\\bullet$ stopping rule $\\tau_{t}:\\mathcal{H}_{t+1}^{\\pi}\\rightarrow\\{\\mathrm{STOP},\\mathrm{CONTINUE}\\}$ decides whether to stop or continue to execute given the observation history $\\varkappa_{t+1}^{\\pi}$ . The stopping time under algorithm $\\pi$ is denoted as $\\tau^{\\pi}$ .\uff0c \u00b7recommendation rule $r_{\\tau}:\\mathcal{H}_{\\tau+1}^{\\pi}\\to\\mathcal{X}$ recommends an arm ${\\hat{x}}^{\\pi}$ based on $\\mathcal{H}_{\\tau+1}^{\\pi}$ upon termination. The stopping time $\\tau^{\\pi}$ is the sample complexity of the algorithm $\\pi$ under instance $\\Lambda$ . The expected samplecomplexityis $\\mathbb{E}[\\tau^{\\pi}]$ , where the expectation is taken w.r.t. the random returns, the realization of the contexts governed by the latent vector distribution $P_{\\theta}$ , and the randomness of the algorithm $\\pi$ ", "page_idx": 3}, {"type": "text", "text": "An algorithm $\\pi$ is $(\\varepsilon,\\delta)$ -PAC (Probably Approximately-Correct) if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\hat{x}^{\\pi}\\in\\mathcal{X}_{\\varepsilon}]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our overarching goal in this paper is to devise an $(\\varepsilon,\\delta)$ -PAC algorithm that minimizes $\\tau^{\\pi}$ with high probability (w.h.p.) and in expectation. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1A Naive Baseline: $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first devise the NAivE $\\varepsilon$ -BEST ARM IDENTIFICATION (or $\\scriptstyle\\mathrm{N}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\mathrm{\\Omega}}$ ) algorithm (presented in Algorithm 2). In the design of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ , only the choice of confidence radius $\\tilde{\\rho}_{t}$ takes the potential context changes into consideration. Even though $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ does not attempt to detect potential changes in the context, it can identify an $\\varepsilon$ -optimal arm w.h.p. and is with a finite expected sample complexity. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1. Let $\\begin{array}{r}{\\Delta_{\\mathrm{min}}=\\operatorname*{min}_{x\\neq x^{*}}\\Delta(x^{*},x),}\\end{array}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{\\mathrm{V}}^{\\mathrm{N}}=\\frac{d}{\\left(\\varepsilon+\\Delta_{\\mathrm{min}}\\right)^{2}}\\ln\\frac{1}{\\delta}\\qquad a n d\\qquad T_{\\mathrm{D}}^{\\mathrm{N}}=\\frac{L_{\\mathrm{max}}}{\\left(\\varepsilon+\\Delta_{\\mathrm{min}}\\right)^{2}}\\ln\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The $\\mathrm{N}\\varepsilon$ BAI algorithm is $(\\varepsilon,\\delta)$ -PAC and its expected sample complexity is $\\tilde{O}(T_{\\mathrm{V}}^{\\mathrm{N}}+T_{\\mathrm{D}}^{\\mathrm{N}})$ ", "page_idx": 3}, {"type": "text", "text": "The upper bound in Proposition 3.1 (also see Appendix F) consists of two main terms. (i) As $\\mathrm{N}\\varepsilon$ BAI samples arms according to the G-optimal allocation (see Appendix D), the amount of samples needed to estimate the average of latent vectors $\\textstyle\\sum_{s=1}^{t}\\theta_{j_{s}}^{*}/t$ contributes to $T_{\\mathrm{V}}^{\\mathrm{N}}$ (i) $T_{\\mathrm{D}}^{\\mathrm{N}}$ quantifies how fast $\\textstyle\\sum_{s=1}^{t}\\theta_{j_{s}}^{*}/t$ convergesto the expectation of context vectors $\\textstyle\\sum_{j=1}^{t}p_{j}\\theta_{j}^{*}$ ", "page_idx": 3}, {"type": "text", "text": "The sample complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ grows linearly with $L_{\\mathrm{max}}$ , but we surmise that the sample complexity of a close-to-optimal algorithm should have a reduced dependence on $L_{\\mathrm{max}}$ ", "page_idx": 3}, {"type": "text", "text": "3.2  Piecewise-Stationary $\\underline{{\\boldsymbol{\\varepsilon}}}$ -Best Arm Identification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The algorithm PIECEWISE- $\\underline{{\\boldsymbol{S}}}$ TATIONARY $\\underline{{\\boldsymbol{\\varepsilon}}}$ -BEST ARM IDENTIFICATION (or $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{}$ is presented in Algorithm 1. By using a sliding window mechanism, $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ actively detects the changepoints and aligns the current latent context with contexts observed in the previous time steps via LINEAR-CHANGE DETECTION (or LCD) and LINEAR-CONTEXT ALIGNMENT (or LCA), which are presented in Algorithms 3 and 4 (see App. D.2.2), respectively. $\\mathrm{PS}\\varepsilon$ BAI consists of three phases: (i) Exploration phase (Exp): Estimate latent vectors and their distribution $P_{\\theta}$ (Lines 8 to 11 and 25); (ii) Change Detection phase (CD): Detect changepoints (Lines 12 to 16);   \n(i) Context Alignment phase (CA): Evaluate the current context and align it with the contexts observed in previous time steps (Lines 17 to 21). ", "page_idx": 3}, {"type": "text", "text": "At time $t$ ,we estimate $\\Theta$ and $\\mathbf{p}$ with $\\hat{\\Theta}_{t}=(\\hat{\\theta}_{t,1},\\dots,\\hat{\\theta}_{t,N})$ and $\\hat{\\mathbf{p}}_{t}=\\left(\\hat{p}_{t,1},\\ldots,\\hat{p}_{t,N}\\right)^{\\top}$ , respectively.1 We denote the empirical mean gap between $x$ and $\\tilde{x}$ under context $j$ as $\\hat{\\Delta}_{t,j}(x,\\tilde{x}):=(x-\\tilde{x})^{\\top}\\hat{\\theta}_{t,j}$ $\\mathrm{PS}{\\varepsilon}\\mathbf{B}\\mathbf{A}\\mathbf{I}$ first computes the $\\mathrm{G}$ -optimal allocation [1] $\\lambda^{*}$ on the arm set $\\mathcal{X}$ and its maximum possible stopping time $\\tau^{*}$ (Line 2). It initializes $\\mathrm{CD}_{\\mathrm{sample}}$ and $\\mathrm{CA}_{\\mathrm{id}}$ . CDsample collects samples to detect changepoints and $\\mathrm{CA}_{\\mathrm{id}}$ maintains a dictionary of {latent context index : identification samples} pairs (Line 3);2 $\\mathrm{CA}_{\\mathrm{id}}[j]$ is the sequence of CD samples used to identify latent context $j$ . It also initializes $\\mathcal{T}_{t,j}$ , the collection of time indices in $[t]$ in the Exp phases under estimated context $j$ Define ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{T}_{t}=\\bigcup_{j\\in[N]}\\mathcal{T}_{t,j},\\qquad T_{t}=|\\mathcal{T}_{t}|,\\qquad T_{t,j}=|\\mathcal{T}_{t,j}|,\\qquad\\forall j\\in[N].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "Q5e3ftQ3q3/tmp/528ab5a44d564a37f19e74c818b96659cc37dd7c2de9250bd7e730a81b112332.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "It then collects $\\frac w2$ samples and stores them in $\\mathrm{CA}_{\\mathrm{id}}$ , which is then used to identify the first latent context (Lines 5 to 6). ", "page_idx": 4}, {"type": "text", "text": "In the Exp phase, $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ firstly samples an arm $x_{t}$ with $\\lambda^{*}$ and observes the return $Y_{t,x_{t}}\\;=\\;$ $x_{t}^{\\top}\\theta_{j t}^{*}+\\overline{{\\eta_{t}}}$ (Line 9). It then updates the estimated context index and time collectors (Line 11). It also updates the estimates of value and probability of each context $j$ (Line 25) with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t,j}=\\frac{1}{T_{t,j}}\\sum_{s\\in\\mathcal{T}_{t,j}}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}\\qquad\\mathrm{and}\\qquad\\hat{p}_{t,j}=\\frac{T_{t,j}}{T_{t}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{\\theta}_{t,j}=\\mathbf{0}$ $T_{t,j}=0$ .Wwedfine $\\alpha_{t},\\,\\xi_{t},\\,\\beta_{t,j}$ and $\\hat{\\Delta}_{t,j}^{\\mathrm{clip_{2}}}(x,\\tilde{x})$ in Appendix D21 For ach pairs of arms $(x,\\tilde{x})$ , the confidence radius of $\\Delta(x,\\tilde{x})$ at time step $t$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{t}(x,\\tilde{x}):=2(\\alpha_{t}+\\xi_{t})+\\sum_{j=1}^{N}\\beta_{t,j}|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})|;\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\mathrm{PS}\\varepsilon$ BAI actively enters the CD phase every $\\gamma$ time steps (Line 12). It firstly adds a CD sample to $\\mathrm{CD}_{\\mathrm{sample}}$ (Line 13). Next, if there are suffcient CD samples (Line 15), the LCD subroutine (presented in Algorithm 3) is called and utilizes the most recent $w$ CD samples to check whether a changepoint just occurred (Line 16). $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ steps into the CA phase if a changepoint is detected, and skips the CA phase otherwise, which is illustrated by Figures I(b) and 1(a) respectively. ", "page_idx": 5}, {"type": "image", "img_path": "Q5e3ftQ3q3/tmp/ce6cbcd802ad9d892b2622a48173acdc8b42982706c26ec93f82d3907d0d957e.jpg", "img_caption": ["Figure 1: (a) No change alarm is raised during a stationary segment. The active CD samples are the input to the LCD subroutine at current time step $t$ (b) A changepoint is detected by LCD, followed by a CA phase and a statistics reversion step. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In the CA phase, ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ starts by resetting $\\mathrm{CD}_{\\mathrm{sample}}$ (Line 17), updating the count of time steps and recording the ending time of this CA phase (Line 18). Thereafter, the CA subroutine (presented in Algorithm 4) is invoked, which estimates the current latent context index $\\hat{j}_{t}$ and updates $\\mathrm{CA}_{\\mathrm{id}}$ (Line 19). If $\\hat{j}_{t}=N+1$ i.e., $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ identifies $N+1$ latent contexts, which is incorrect under instance $\\Lambda$ , it terminates and fails to identify an $\\varepsilon,$ -optimal arm (Line 20). Lastly, all empirical statistics are reverted to those from $(w(\\gamma+1)/2)$ time steps ago, i.e., the most recent $\\bar{(w\\gamma/2)}$ samples in the Exp phases are abandoned (Line 21). ", "page_idx": 5}, {"type": "text", "text": "The stopping rule is described in Lines 26 to 32. (I) If the following condition is satisfied (Line 26): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x:x\\neq x_{t}^{*}}\\hat{\\Delta}_{t}(x_{t}^{*},x)-\\rho_{t}(x_{t}^{*},x)\\geq-\\varepsilon\\quad\\mathrm{and}\\quad T_{t}\\geq\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\Big(\\frac{2}{\\delta_{d,T_{t}}}\\Big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the empirical mean gap $\\hat{\\Delta}_{t}(x_{t}^{*},x)\\,:=\\,(x_{t}^{*}\\,-\\,x)^{\\top}\\hat{\\Theta}_{t}\\hat{\\mathbf{p}}_{t}$ and $x_{t}^{*}\\;:=\\;\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\hat{\\Theta}_{t}\\hat{\\mathbf{p}}_{t}$ \uff0c $\\mathrm{PS}\\varepsilon$ BAI records the arm with the highest empirical mean as $\\hat{x}_{\\varepsilon}$ and the number of CD samples $t_{\\mathrm{CD}}$ (Lines 27 and 28) but does not terminate immediately. Besides, a mild forced arm pull procedure is in the second line of (3.4), which is inspired by Lemma E.1 and to ensure the performance of $\\mathrm{PS}{\\varepsilon}\\mathbf{B}\\mathbf{A}\\mathbf{I}$ $(\\mathbf{II})$ $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ will execute for another $(w\\gamma/2)$ time steps in which $w/2$ CD samples are collected; if no changepoint is detected with these $w/2$ CD samples, the recorded arm $\\hat{x}_{\\varepsilon}$ is recommended and $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ terminates (Lines 29 to 30). Part $(\\mathbf{II})$ of the stopping rule assures $\\mathrm{PS}\\varepsilon$ BAI does not terminate when a changepoint has occurred but has not been detected, as $\\mathrm{PS}\\varepsilon$ BAI may fail to identify an $\\varepsilon$ -optimal arm otherwise. ", "page_idx": 5}, {"type": "text", "text": "We remark that even though $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ uses the knowledge of $L_{\\mathrm{max}}$ , our experiments show that the performance of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ is robust to small misspecifications in $L_{\\mathrm{max}}$ (see Appendix O.2). Furthermore, the computational complexity of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ is computed in detail in Appendix D.4. The derived computational complexity indicates the proposed algorithm depends in a natural manner on the problem parameters such as $d,K,N$ , and $\\gamma$ . Lastly, thanks to the LCD and LCA subroutines, a slightly modified variant of $\\mathrm{PS}\\varepsilon$ BAI can also solve the \u201c $\\varepsilon$ -Best Arm Tuple identification problem\", which aims to identify an $\\varepsilon$ -best arm under each context; see Appendix Q for details. ", "page_idx": 5}, {"type": "text", "text": "3.2.1  Theoretical guarantee of $\\mathrm{PS}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To facilitate the analysis of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ we propose the following assumptions. Note that our $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ algorithm may still succeed to identify an $\\varepsilon$ -optimal arm w.h.p. when the assumptions do not hold. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (Distinguishability Condition). The agent can choose $w$ $\\gamma$ and $b$ suchthat $(l)\\,2b\\leq\\Delta_{c}$ where $\\begin{array}{r}{\\bar{\\Delta}_{c}:=\\operatorname*{min}_{\\theta_{j}^{*}\\neq\\theta_{\\widetilde{j}}^{*}}\\operatorname*{max}_{x\\in\\dot{\\mathcal{X}}}|x^{\\top}(\\theta_{j}^{*}-\\theta_{\\widetilde{j}}^{*})|}\\end{array}$ is the minimum gap between two contexts; and (2) $3w\\gamma\\leq L_{\\operatorname*{min}}$ . A possible choice is ", "page_idx": 6}, {"type": "equation", "text": "$$\nb\\!=\\!\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\!+\\!\\sqrt{\\Big(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\Big)^{2}\\!+\\!\\frac{24d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}\\quad w h e r e\\quad\\delta_{\\mathrm{FAE}}=\\frac{\\gamma\\delta}{4(\\tau^{*})^{2}K}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This assumption guarantees (i) $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ will not abandon all samples during the reversion procedure (Line 21 of Algorithm 1); (ii) each two latent vectors can be distinguished if the window size $w$ is sufficiently large (e.g., $L_{\\mathrm{min}}/6)$ . We clarify that this assumption is only for the rigor of theoretical guarantees and it holds provided that each stationary segment is sufficiently long; this is a feature of PSB models and similar assumptions are also present in existing works for their analyses [21, 10, 22]. We demonstrate the robustness of $\\mathrm{PS}\\varepsilon$ BAI to these parameters using experiments in Section 6. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Define the context distribution estimation $(D E)$ hardnessparameter ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x):=\\frac{L_{\\mathrm{max}}}{\\left(\\Delta(x^{\\ast},x)+\\varepsilon\\right)^{2}}\\bar{H}(x_{\\varepsilon},x)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{H}(x_{\\varepsilon},x):=\\left(\\sum_{j=1}^{N}\\sqrt{\\operatorname*{min}\\{16p_{j},1/4\\}}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\right)^{2}}\\end{array}$ Under Asmption $^{\\,l}$ bility at least $1-\\delta$ $\\mathrm{PS}\\varepsilon$ BAI identifies an $\\varepsilon$ -optimal arm and its sample complexity is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{O}\\Bigg(\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\frac{d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\underbrace{\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{1}{\\delta}}_{T_{\\mathrm{D}}(x_{\\varepsilon},x)}+\\underbrace{\\frac{N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{1}{\\delta}}_{T_{\\mathrm{R}}(x)}\\Bigg).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The upper bound comprises three terms which serve distinct purposes: ", "page_idx": 6}, {"type": "text", "text": "(i) Latent vector estimation (VE): $\\tilde{O}\\left(T_{\\mathrm{V}}(x)\\right)$ quantifies the bulk of samples needed to obtain a good estimate of latent context vectors such that the returns of $x_{\\varepsilon}$ and $x$ can be distinguished, where $x_{\\varepsilon}$ is an $\\varepsilon$ -best arm and $x\\notin\\mathcal{X}_{\\varepsilon}$ is a suboptimal arm. $T_{\\mathrm{V}}(x)$ recovers the sample complexity in the stationary linear bandits in [1], indicating that $\\mathrm{PS}\\varepsilon$ BAI estimates latent vectors efficiently. ", "page_idx": 6}, {"type": "text", "text": "(i) Context distribution estimation (DE): $\\tilde{O}\\left(T_{\\mathrm{D}}(x_{\\varepsilon},x)\\right)$ characterizes the bulk of samples needed to learn the distribution of latent context vectors.   \n(ii) Residual estimation (RE): $\\tilde{O}\\left(T_{\\mathrm{R}}(x)\\right)$ counts the remaining samples needed for VE and DE, in additionto $\\tilde{O}\\left(T_{\\mathrm{V}}+T_{\\mathrm{D}}\\right)$   \nBesides, the max operator is applied to exclude all suboptimal arms. We also see that $T_{\\mathrm{V}}(x)$ and ", "page_idx": 6}, {"type": "text", "text": "$T_{\\mathrm{D}}(x_{\\varepsilon},x)$ are similar to $T_{\\mathrm{V}}^{\\mathrm{N}}$ and $\\dot{T}_{\\mathrm{D}}^{\\mathrm{N}}$ in Proposition 3.1 respectively. ", "page_idx": 6}, {"type": "text", "text": "Firstly, the bound in (3.6) implies that, in an instance with smaller relaxed mean gap $\\Delta(x^{*},x)+\\varepsilon$ $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ terminates after a larger number of time steps; in other words, it is more difficult to identify an $\\varepsilon$ -optimal arm. In difficult instances with small $\\bar{\\Delta}(x^{*},x)+\\varepsilon$ , the different orders of this term in $T_{\\mathrm{V}}(x)$ $T_{\\mathrm{D}}(x_{\\varepsilon},x)$ and $T_{\\mathrm{R}}(x)$ indicate that, $T_{\\mathrm{R}}(x)$ is small compared to $T_{\\mathrm{V}}(x)$ and $T_{\\mathrm{D}}(x_{\\varepsilon},x)$ ", "page_idx": 6}, {"type": "text", "text": "Secondly, DE solely utilizes context samples generated with $P_{\\theta}$ and they are generated only at changepoints in $\\mathcal{C}$ , while all the observations in Exp phases facilitate VE. From this perspective, there are less samples that can be used for DE than for VE as $\\overline{{\\mathrm{PS}}}\\varepsilon$ BAI processes, and hence $T_{\\mathrm{D}}(x_{\\varepsilon},x)$ is supposed to be with larger order than $T_{\\mathrm{V}}(x)$ ", "page_idx": 6}, {"type": "text", "text": "Moreover,  for the purpose  of  DE, ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ needs  to  observe  context  samples   at $\\begin{array}{r}{\\tilde{O}\\bigl(\\frac{\\bar{H}(x_{\\varepsilon},x)}{(\\Delta(x^{*},x)+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}\\bigr)=\\tilde{O}\\bigl(\\frac{\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)}{L_{\\mathrm{max}}}\\ln\\frac{1}{\\delta}\\bigr)}\\end{array}$ changepoints where $L_{\\mathrm{max}}$ is the maximum engtho a stationary segment, leading us to $T_{\\mathrm{D}}(x_{\\varepsilon},x)$ . Close examination of the definition of $\\bar{H}(x_{\\varepsilon},x)$ reveals that both the vectors and their probabilities infuence the number of samples needed for DE. The comparison between $T_{\\mathrm{D}}(x_{\\varepsilon},x)$ and $T_{\\mathrm{{D}}}^{\\mathrm{{N}}}$ in Proposition 3.1 clearly indicates that $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ mitigates the influence of $L_{\\mathrm{max}}$ by detecting changepoints and aligning the detected context with observed ones, while $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ does not do so. ", "page_idx": 6}, {"type": "text", "text": "We have provided a high-probability result for $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ in Theorem 3.2. The design of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ (Line 7 of Algorithm 1) indicates that $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ will not recommend any arm if it does not terminate at time $\\tau^{*}$ . This result is nontrivial, as the high-probability result in Theorem 3.2 depends on the success of change detection (Algorithm 3) and context alignment (Algorithm 4), which requires a non-vanishing failure probability (e.g., $\\delta/2$ ). Thus, we cannot derive an upper bound on the expected sample complexity of $\\mathrm{PS}\\varepsilon$ BAI. We devise a solution by designing the Piecewise-Stationary $\\underline{{\\varepsilon}}_{\\mathrm{:}}$ -Best Arm Identification+ $(\\mathbf{PS}\\varepsilon\\mathbf{BAI}^{+})$ ) algorithm with a simple but effective trick. ", "page_idx": 7}, {"type": "text", "text": "The ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm samples one arm with the G-optimal allocation $\\lambda^{*}$ at each time step, with which Algorithms 1 and 2 are executed in parallel (detailed in Algorithm 5). This is feasible since $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ algorithms have the same sampling rule. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.3. The $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ algorithm is $(\\varepsilon,\\delta)$ -PAC and its expected sample complexity is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{O}\\bigg(\\operatorname*{min}\\left\\{\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon},x\\ne x_{\\varepsilon},x^{*}}T_{\\mathrm{V}}(x)+T_{\\mathrm{D}}(x_{\\varepsilon},x)+T_{\\mathrm{R}}(x),\\,T_{\\mathrm{V}}^{\\mathrm{N}}+T_{\\mathrm{D}}^{\\mathrm{N}}\\right\\}\\bigg).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ inherits the superiority of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ to adapt to the piecewise stationary environment, and employs the stopping rule of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ to maintain a finite expected sample complexity. As a result, the expected complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ in Theorem 3.3 is of the same order as the high-probability one of ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ in Theorem 3.2 and is not larger than the complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ in Proposition 3.1. We show how our results particularize to the stationary linear bandits BAI problem, as well as additional discussions on the upper bound, in Appendix $\\mathbf{P}$ ", "page_idx": 7}, {"type": "text", "text": "4  Lower Bound on the Sample Complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ , define the alternative instance $\\Lambda^{\\prime}=(\\mathcal{X},\\Theta^{\\prime},P_{\\theta^{\\prime}},\\mathcal{C})$ w.r.t. $\\Lambda$ , where $\\Theta^{\\prime}=$ $(\\theta_{1}^{\\prime},\\dots,\\theta_{n}^{\\prime})\\in\\mathbb{R}^{d\\times N}$ \uff0c $P_{\\theta^{\\prime}}[\\theta_{j}^{\\prime}]=P_{\\theta}[\\theta_{j}^{*}]$ , and there exists $x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\varepsilon}$ , such that $x_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta^{\\prime}\\sim P_{\\theta^{\\prime}}}[\\theta^{\\prime}]<$ $x^{\\top}\\mathbb{E}_{\\theta^{\\prime}\\sim P_{\\theta^{\\prime}}}[\\theta^{\\prime}]-\\epsilon$ for all $x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}$ . Let $\\mathrm{Alt}_{\\Theta}(\\Lambda)$ be set of all alternative instances (w.r.t. $\\Lambda$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. For all $(\\varepsilon,\\delta)$ -PAC algorithm $\\pi$ there exists an instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau_{\\pi}]\\geq\\operatorname*{max}\\left\\{T_{\\varepsilon}(\\Lambda)\\ln\\frac{1}{2.4\\delta},\\;c_{N_{c}}\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\varepsilon}(\\Lambda)^{-1}:=\\underset{\\{v_{j}\\}_{j=1}^{N}\\Lambda^{\\prime}\\in\\mathrm{Alt}_{\\Theta}(\\Lambda)}{\\operatorname*{max}}\\sum_{j,x}p_{j}v_{j,x}\\frac{(x^{\\top}(\\theta_{j}^{*}-\\theta_{j}^{\\prime}))^{2}}{2},\\quad a n d}\\\\ &{\\quad\\quad N_{\\mathcal{C}}:=\\underset{x\\neq x^{*}}{\\operatorname*{max}}\\frac{\\sum_{j}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon)^{2}}{(\\Delta(x^{*},x)+\\varepsilon)^{2}}\\ln\\frac{1}{4\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Recall that $c_{N_{C}}$ isthe $N_{\\mathcal{C}}$ -th changepoint in the changepoint sequence $\\mathcal{C}$ , which is lower bounded by $N_{\\mathcal{C}}L_{\\operatorname*{min}}$ and is $N_{\\mathcal{C}}L_{\\mathrm{max}}$ in the worst case. To derive the lower bound in Theorem 4.1, we investigate two environments different from the one defined in Section 2 (and as in Dynamics 1): ", "page_idx": 7}, {"type": "text", "text": "$\\bullet$ Dynamics 2: the agent observes the index of current context $j_{t}$ (i.e., contextual linear bandits); \u00b7 Dynamics 3: the agent observes the changepoints in $\\mathcal{C}$ and context vector $\\boldsymbol{\\theta}_{j_{t}}^{*}$ 's, and hence she solely needs to estimate the distribution of contexts. We bound the sample complexity of an $(\\varepsilon,\\delta)$ -BAI algorithm in Dynamics 2 and 3 respectively, which when combined, yield the lower bound in Theorem 4.1; this is detailed in Appendix M. ", "page_idx": 7}, {"type": "text", "text": "Note that $T_{\\varepsilon}(\\Lambda)^{-1}$ in the lower bound generalizes [16] to the setting of linear bandits. In addition, Theorem 4.1 can be reduced to a bound in stationary linear bandits with one latent vector [24] (see the discussion leading to (M.15)). ", "page_idx": 7}, {"type": "text", "text": "5  On the Asymptotic Optimality of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate the efficiency of our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm, we compare the upper bound on its expected sample complexity in Theorem 3.3 and the generic lower bound in Theorem 4.1 under specific instances below and in Appendix N. We also gain further insight into our $\\scriptstyle\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ algorithm. ", "page_idx": 7}, {"type": "image", "img_path": "Q5e3ftQ3q3/tmp/23dd27883c308590e5076ab881a5414cc9e7ff428e45a1e55fac6cccf8f99c76.jpg", "img_caption": ["Figure 2: Experimental results "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Example 1. Instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ is with(i) $2d-1$ arms: $x_{(1)}=\\mathbf{e}_{1}$ \uff0c $x_{(i)}={\\mathbf{e}}_{i},x_{(d+i-1)}=$ $\\mathbf{e}_{1}\\cos\\phi\\!+\\!\\mathbf{e}_{i}\\sin\\phi$ for all $i\\in\\{2,\\ldots,d\\}$ where $\\phi\\in[0,\\pi/4)$ (i) $2d-2$ contexts: $\\theta_{j\\pm}^{\\ast}={\\bf e}_{1}\\cos\\phi\\pm$ $\\mathbf{e}_{j+1}\\sin\\phi$ for all $j\\in[d-1]$ ,(ii)Contextdistribution: $p_{j}=1/N$ for all $j\\in[N]$ ", "page_idx": 8}, {"type": "text", "text": "Under the instance defined in Example 1, $x_{(i)}$ for all $i\\neq1$ is inferior to $x_{(1)}$ under all contexts and $x_{(i+d)}$ for all $i\\in[d-1]$ is marginally better than $x_{(1)}$ by $1-\\cos\\phi$ only under context $\\theta_{i+}^{*}$ and $\\Delta(x_{(1)},x_{(i+d)})=\\cos\\phi-\\cos^{2}\\phi$ . We expect ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ to discover this feature of the instances and quickly identify an $\\varepsilon$ -optimal arm with a course estimation of the context distribution. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.1. For the instance defined in Example $^{\\,l}$ wehave $\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)=\\tilde{O}(N L_{\\mathrm{max}})$ for all $(x_{\\varepsilon},x)\\in\\mathcal{X}_{\\varepsilon}\\times(\\mathcal{X}\\setminus\\mathcal{X}_{\\varepsilon})$ .In addition, $i f\\varepsilon<(\\cos\\phi)(1-\\cos\\phi)$ wehave ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\ \\tilde{\\Theta}\\bigg((1\\!+\\!f(\\phi))\\cdot\\frac{d}{(\\Delta_{x_{(1)},x_{(d+1)}}\\!+\\!\\varepsilon)^{2}}\\bigg),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbb{E}[\\tau]^{*}$ is theminimal expected sample complexity overall $(\\varepsilon,\\delta)$ -PAC algorithms and $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies $f(\\phi)\\rightarrow0$ as $\\phi\\to0^{+}$ . The upper bound in (5.1) is achieved by $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ ", "page_idx": 8}, {"type": "text", "text": "The order of $\\mathrm{H_{DE}}$ in Corollary 5.1 indicates that $T_{\\mathrm{D}}(x_{(1)},x_{(d+1)})\\,=\\,\\tilde{O}(N L_{\\mathrm{max}}\\ln(1/\\delta))$ is not dominating the sample complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ , suggesting that a coarse estimation of the context distribution is sufficient when $\\phi$ is small. In other words, ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ exploits the feature of instances and utilize samples mostly for estimate context vectors, which is again expected. ", "page_idx": 8}, {"type": "text", "text": "Corollary 5.1 implies that under such instances, the upper and lower bounds on the sample complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ match up to logarithmic factors, that is, the performance ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is near optimal. Besides, the bound of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ in Proposition 3.1 is with an extra additive term $L_{\\mathrm{max}}$ compared to the lower bound in Corollary 5.1, illustrating that $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ is suboptimal and again emphasizing the significance of detecting changes and aligning contexts for ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI^{+}}}$ to reduce the impact of $L_{\\mathrm{max}}$ ", "page_idx": 8}, {"type": "text", "text": "6   Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now evaluate the empirical performance of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ . We utilize the instance defined in Example 1 with $d=2$ $\\phi=\\pi/8$ We generate a changepoint sequence $\\mathcal{C}$ such that $c_{l+1}=c_{l}+L_{l}$ with $\\bar{L}_{\\mathrm{min}}\\,=\\,3\\times\\,10^{4}$ \uff0c $L_{\\mathrm{max}}\\,=\\,5\\times\\,\\mathrm{\\bar{1}0^{4}}$ \uff0c $\\mathbb{P}[L_{l}\\,=\\,L_{\\mathrm{min}}^{-}\\mathbf{\\bar{\\mu}}]\\,=\\,0.8$ $\\mathbb{P}[L_{l}\\,=\\,L_{\\mathrm{max}}]\\,=\\,0.2$ , and fix it throughout the whole set of experiments. We set the confidence parameter $\\delta=0.05$ and vary the slackness parameter $\\varepsilon$ from 0.04 to 0.6 (i.e., $\\varepsilon\\,=\\,0.03\\times1.35^{k}$ for $k\\,\\in\\,[12]\\rangle$ . We set $\\gamma=6$ the window size $w=L_{\\mathrm{min}}/(3\\gamma)$ and compute $b$ via (3.5) in Assumption 1.3 For each choice of algorithm and instance, we run 20 independent trials. All the code to reproduce our experiments can be found at https: //github.com/Y-Hou/BAI-in-PSLB.git. ", "page_idx": 8}, {"type": "text", "text": "We first compare ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ . Both algorithms succeed to identify an $\\varepsilon$ -optimal arm, while empirically, the complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I^{+}}}$ is $\\bar{<}1\\%$ of that of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ . The empirical averages and standard deviations of the sample complexities of both algorithms are presented in Figure 2(a). ", "page_idx": 8}, {"type": "text", "text": "Figure 2(a) illustrates that empirically, the termination and arm recommendation of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ are determined by the execution of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ as a subroutine, suggesting that in Theorem 3.3, the first term resulting from $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ actually determines the complexity of $\\bar{\\mathrm{PS}}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathrm{I^{+}}$ ", "page_idx": 9}, {"type": "text", "text": "Next, we test the efficacy of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ to learn and exploit the latent vectors and the distribution of contexts. Specifically, we run ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ under Dynamics 1 where neither the index nor the vector of current context is visible to the agent. We also run benchmark algorithms: $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and its variant $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ under Dynamics 3 where context vectors and changepoints are all observed; these two algorithms are detailed and analyzed in Appendix O.4. ", "page_idx": 9}, {"type": "text", "text": "As the changepoint sequence $\\mathcal{C}$ is fixed in a given instance and $t/L_{\\mathrm{max}}\\leq l_{t}\\leq t/L_{\\mathrm{min}}$ for all $t\\in\\mathbb{N}$ we regard the number of context samples $l_{\\tau}$ as a proxy of the sample complexity $\\tau$ . We present the number of context samples need by ${\\sf P S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ \uff0c $\\mathrm{N}\\varepsilon$ BAI, $\\scriptstyle\\mathrm{D}\\varepsilon\\mathrm{BAI}$ and $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ for arm identification w.r.t. $1/(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}$ in Figure 2(b). ", "page_idx": 9}, {"type": "text", "text": "Figure 2(b) contains three messages. First, the complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ scales as $1/(\\Delta_{\\mathrm{min}}+\\varepsilon)^{2}$ corroborating Theorem 3.3. Second, although ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ has access to neither context vectors nor changepoints, it needs roughly the same number of context samples as $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ \uff0c suggesting that it is competitive compared to these algorithms that have oracle information about the environment. Third, $\\scriptstyle\\mathbf{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ uses the confidence radius in (3.3) and terminates with fewer context samples compared to $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ , implying that the confidence radius is well-designed. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, when $\\varepsilon$ decreases from $0.03\\times1.35^{12}$ to $0.03\\times1.35^{9}$ , the complexity of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ almost remains unchanged while that of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathbf{B}\\mathrm{AI}$ increases rapidly as presented in instances 9 to 12 in Figure 2(a). Meanwhile, the number of context samples need by two algorithms are shown to be with the same pattern in Figure 2(b). This contrast indicates that the cost of distribution estimation ( $\\mathrm{\\nabla}T_{\\mathrm{D}}$ in (3.6)) for ${\\tt P S}{\\tt E B A I}^{+}$ has been significantly minimized compared to $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ ", "page_idx": 9}, {"type": "text", "text": "To summarize, we emphasize that the empirical superiority of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ over $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ implies that the efficacy of $\\mathrm{PS}_{\\mathcal{E}}\\mathrm{BAI^{+}}$ is inherited from $\\mathrm{PS}_{\\mathcal{E}\\mathrm{BAI}}$ . Our experiments show that actively exploiting the context information, via changepoint detection and context alignment (as in ${\\sf P S E B A I}^{\\dot{+}}$ and ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ facilitates identifying the $\\varepsilon,$ -optimal arm efficiently. ", "page_idx": 9}, {"type": "text", "text": "Similar to many existing algorithms in piecewise-stationary bandits [21, 10, 22], our algorithm requires Assumption 1 and the knowledge of $L_{\\mathrm{max}}$ These may not be available in practice. Thus, we conduct more experiments in Appendix O.2 to exhibit the robustness of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ . Specifically, in Appendix O.2, we conduct experiments for the case in which $L_{\\mathrm{max}}$ is misspecified. In Appendix O.3, we alter the change detection frequency $\\gamma$ so that $w$ and $b$ change accordingly. In both sets of experiments, the overall sample complexity of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ does not vary significantly and retains its superiority over $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ . We conclude that ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is robust to slight misspecifications in these parameters, as long as Assumption 1 is not severely violated. Please refer to Appendix $\\mathrm{o}$ for further details and experiments. ", "page_idx": 9}, {"type": "text", "text": "7  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a novel PSLB model and designed the ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm to identify an $\\varepsilon,$ -optimal arm with probability $\\geq1-\\delta$ . The efficacy of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ has been demonstrated both empirically and theoretically. We argued that this is due to the embedded change detection and context alignment procedures. There are several directions for further exploration. ", "page_idx": 9}, {"type": "text", "text": "Firstly, our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm provides a fairly general framework for algorithm design. For instance, in addition to utilization the $\\mathrm{G}$ -optimal allocation to sample arms as in ${\\tt P S}{\\tt E B A I^{+}}$ ,the $\\mathcal{X Y}$ -allocation and adaptive $\\mathcal{N V}$ -allocation [1] can also be considered. In other words, our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm can be generalized to form an entire class of algorithms for BAI in PSLB models. In addition, deriving instance-dependent guarantees is also of great interest. ", "page_idx": 9}, {"type": "text", "text": "Secondly, most of the literature on piecewise-stationary bandits [21, 10, 22] make assumptions to provide theoretical guarantees. It would be interesting to remove or reduce these assumptions under Our $\\varepsilon,$ -BAI problem setup, and yet still be able to provide similar theoretical guarantees. ", "page_idx": 9}, {"type": "text", "text": "Finally, we believe that it is possible to adapt our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm to the fixed-budget setting, i.e., to identify an $\\varepsilon$ -optimal arm with high probability in a fixed time horizon in PSLB models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements:  This work is funded by the Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00) and Tier 1 grants (A-8000189-01-00 and A-8000980-00-00). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marta Soare, Alessandro Lazaric, and R\u00e9mi Munos. Best-arm identification in linear bandits. In Proceedings of the 27th Advances in Neural Information Processing Systems, volume 27, pages 828-836, 2014.   \n[2] Junwen Yang and Vincent Tan. Minimax optimal fixed-budget best arm identification in linear bandits. Proceedings of the 36th Advances in Neural Information Processing Systems, 35:12253-12266, 2022.   \n[3]  Yasin Abbasi-yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Proceedings of the 24th Advances in Neural InformationProcessing Systems, volume 24. Curran Associates, Inc., 2011.   \n[4]  Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 176-184. PMLR, 2017. [5]  Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for transductive linear bandits. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Proceedings of the 32nd Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[6]  Marc Jourdan, Remy Degenne, and Emilie Kaufmann. An e-best-arm identification algorithm for fixed-confidence and beyond, November 2023. arXiv:2305.16041. [7]  R\u00e9my Degenne and Wouter M Koolen. Pure exploration with multiple correct answers. Proceedings of the 32nd Advances in Neural Information Processing Systems, 32, 2019. [8]  Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary rewards. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Proceedings of the 27th Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n[9]  Aur\u00e9lien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In Jyrki Kivinen, Csaba Szepesvari, Esko Ukkonen, and Thomas Zeugmann, editors, The 22nd International conference on Algorithmic learning theory, pages 174-188, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.   \n[10]  Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change detection for piecewise-stationary bandit. In Proceedings of the 22nd International Conference on Artificial Inteligence and Statistics, pages 418-427. PMLR, 2019.   \n[11]  Yueyang Liu, Xu Kuang, and Benjamin Van Roy. A definition of non-stationary bandits, Feb 2023. arXiv:2302.12202.   \n[12]  Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under nonstationarity. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1079-1087. PMLR, 2019.   \n[13]  Jonathan Shapiro, Carlos M Carvalho, and Pradeep Ravikumar. Thompson sampling in switching environments with bayesian online change point detection. In Proceedings of the 16th International Conference on Artificial Inteligence and Statistics, pages 442-450. Microtome Publishing, 2013.   \n[14] Zhihan Xiong, Romain Camilleri, Maryam Fazel, Lalit Jain, and Kevin Jamieson. A/B testing and best-arm identification for linear bandits with robustness to non-stationarity. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, PMLR 238, pages 1585-1593. PMLR, 2023.   \n[15]  Robin Allesiardo, Raphael F\u00e9raud, and Odalric-Ambrym Maillard. The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3:267-283, 2017.   \n[16]  Masahiro Kato and Kaito Ariu. The role of contextual information in best arm identification, 2021. arXiv:2106.14077.   \n[17] Yoan Russac, Christina Katsimerou, Dennis Bohle, Olivier Cappe, Aur\u00e9lien Garivier, and Wouter M Koolen. A/B/n testing with control in the presence of subpopulations. Procedings of the 34th Advances in Neural Information Processing Systems, 34:25100-25110, 2021.   \n[18]  Andrea Zanette, Kefan Dong, Jonathan N Lee, and Emma Brunskill. Design of experiments for stochastic contextual linear bandits. Proceedings of the 34th Advances in Neural Information Processing Systems, 34:22720-22731, 2021.   \n[19]  Yasin Abbasi- Yadkori, Andras Gyorgy, and Nevena Lazic. A new look at dynamic regret for non-stationary stochastic bandits. Journal of Machine Learning Research, 24(288):1-37, 2023.   \n[20] Jia Yuan Yu and Shie Mannor. Piecewise-stationary bandit problems with side observations. In Proceedings of the 26th International Conference on Machine Learning, ICML '09, page 1177-1184. Association for Computing Machinery, 2009.   \n[21] Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewisestationary multi-armed bandit problem. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 32, 2018.   \n[22] Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec. Efficient change-point detection for tackling piecewise-stationary bandits. The Journal of Machine Learning Research, 23(1):3337-3376, 2022.   \n[23]  Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknown number of distribution changes. In Proceedings of the 32nd Conference on Learning Theory, pages 138-158. PMLR, 2019.   \n[24]  Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Proceedings of the 34th Advances in Neural Information Processing Systems, volume 33, pages 10007-10017. Curran Associates, Inc., 2020.   \n[25]  Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear bandits. In The 23rd International Conference on Artificial Intelligence and Statistics, pages 746-755. PMLR, 2020.   \n[26] Masahiro Kato, Masaaki Imaizumi, Takuya Ishihara, and Toru Kitagawa. Asymptotically optimal fixed-budget best arm identification with variance-dependent bounds, 2023. arXiv:2302.02988.   \n[27]  Chao Qin and Daniel Russo. On adaptivity and confounding in contextual bandit experiments. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.   \n[28] Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of both worlds: Stochastic & adversarial best-arm identification. In Proceedings of the 31st Conference on Learning Theory, pages 918-949. PMLR, 2018.   \n[29]  Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2020.   \n[30] Kwang-Sung Jun, Kevin G Jamieson, Robert D Nowak, and Xiaojin Zhu. Top arm identification in multi-armed bandits with batch arm pulls. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pages 139-148, 2016.   \n[31] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17:1-42, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32]  Marc Jourdan and Remy Degenne. Choosing answers in epsilon-best-answer identification for linear bandits. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 10384-10430. PMLR, 17-23 Jul 2022. ", "page_idx": 12}, {"type": "text", "text": "[33]  Michael J. Todd. Minimum-Volume Ellipsoids: Theory and Algorithms. SIAM, 2016. ", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The contents of the appendices are organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u00b7 In Appendix A, we provide further motivating examples for our problem.   \n\u00b7 In Appendix B, we discuss the limitations of our method.   \n\u00b7 In Appendix C, we review more related works on drifting and contextual bandits.   \n\u00b7 In Appendix D, we provide more details about our algorithms - Appendix D.1: pseudo-code of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ in Algorithm 2. - Appendix D.2: more details of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ including the precise definition of the confidence radius $\\rho_{t}$ and details of LCD and LCA subroutines. - Appendix D.3: pseudo-code of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ in Algorithm 5. - Appendix D.4: the computational complexity of ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ in Algorithm 1.   \n\u00b7 In Appendix E: we provide a useful lemma for estimating the expected return of any arm in linear bandits when the sampling rule is according to the G-optimal allocation.   \n\u00b7 In Appendix F: we proof the upper bound on the expected sample complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$   \n\u00b7 In Appendices G to K: we detail the analysis of $\\mathrm{PS}\\varepsilon$ BAI, i.e., we provide the proof of the upper bound on its complexity in Theorem 3.2. - Appendix G: outline of the proof. - Appendix H: analysis of the Change Detection (CD) and Context Alignment (CA) procedures. - Appendix I: analysis of the estimation error by decomposing it into three terms: VectorEstimation Error (VE), Distribution-Estimation Error (DE), and Residual Estimation Error (RE). - Appendix J: proof of Theorem 3.2 based on the analysis above. - Appendix K: proof of technical lemmas that are utilized in the analysis of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$   \n\u00b7 In Appendix L: we prove the upper bound on the expected sample complexity of ${\\sf P S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ based on the analysis of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ and $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$   \n\u00b7 In Appendix M: we derive the lower bound on the expected sample complexity of any algorithm, i.e., proof Theorem 4.1.   \n\u00b7 In Appendix N: we provide more examples to compare the derived upper and lower bounds on the expected sample complexity, and illustrate the efficacy of our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm.   \n\u00b7 In Appendix O: we provide more details of numerical experiments.   \n\u00b7 In Appendix P: we provide more discussions on - the related methods for BAI in nonstationary bandits, - the instance-dependent upper bound, - the connection between the piecewise-stationary linear bandits model to the stationary linear bandits model, - the special case where $N=1$   \n\u00b7 In Appendix Q: we provide analytical results on the \u201cBest Arm Tuple Identification Problem\". ", "page_idx": 13}, {"type": "text", "text": "A Further Motivating Examples ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We elaborate on the some concrete real-life examples that motivate our problem setup of identifying the ensemble best arm in piecewise-stationary linear bandits. ", "page_idx": 13}, {"type": "text", "text": "In scenarios such as investment option selection and portfolio management also mentioned by [1o, 20], there is a multitude of options for fund managers to choose from and typically, they want to find, in the initial pure exploration process, a small subset of candidate portfolios (or even the \u201cbest\" portfolio) based on various economic indicators and the market performance of individual stocks before further exploitation. In a bearish market, more portfolios tend to incur losses; while in a bullish market, more portfolios tend to generate gains. The transition between these two contexts can be effected by stochastic factors, e.g., the weather, or the outbreak of a pandemic, making the market conditions (contexts) stochastic. In the face of these uncertainties (in the contexts and rewards), we wish to design and analyze algorithms that selected portfolio to yield the best long-term option under such a piecewise-stationary environment. ", "page_idx": 13}, {"type": "text", "text": "Crop rotation is another example. Since crop yields can be influenced by various factors, such as weather conditions (analogous to our stochastically generated contexts), selecting the most suitable crop to grow and harvest from is crucial. Given several candidate crops, crops of similar types (e.g.. potatoes and sweet potatoes) are correlated as they tend to favor similar conditions, thus, they can be modelled by bandits with a linear reward structure. Contextual factors, like weather conditions, are well-modelled as being stochastic. A fixed weather condition will last a period of time and it will not change suddenly. Domain knowledge from historical data/records provides us with prior knowledge on $L_{\\mathrm{min}}$ and $L_{\\mathrm{max}}$ . These observations dovetail with our model. Our objective is to choose the crop that offers the near-highest yield potential over a long time period (an ensemble $\\varepsilon$ -best arm) and is adaptable to local environment factors, such as weather patterns. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Similar to existing works on piecewise stationary bandits [21, 10, 22], we introduced some assumptions to provide theoretical guarantees for ${\\tt P S}{\\tt E B}\\bar{\\bf A}{\\bf I}^{+}$ although $\\mathrm{PS}\\varepsilon\\mathrm{BAI^{+}}$ has shown to be robust even in the absence of these assumptions. Since an algorithm may not need to differentiate two contexts with close latent vectors for identifying an $\\varepsilon$ -optimal arm, we surmise it is possible to weaken these assumptions. For this purpose, we will consider clustering the contexts into a few classes based on the distances between their latent vectors and design an algorithm that only aims to detect the change of context class, instead of the change of context. ", "page_idx": 14}, {"type": "text", "text": "C More Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In drifting bandits (DB), the regrets of algorithms are affected by the level of nonstationarity of the environment, which can be measured by various quantities, such as the total variation of the context sequence and the number of time steps when the return of at least one arm changes [23, 25]. ", "page_idx": 14}, {"type": "text", "text": "In contextual bandits (CB), where the contextual information is visible to the agent, [16, 26, 17, 27] aim to identify the best arm with the assumption that the context changes at every time step according to a fixed distribution, and the return of an arm is averaged across all contexts. We see that the context distribution is involved to measure the quality of arms. However, while the agent in CB models can observe the context information, the agent has no access to the contexts but still aims to identify an $\\varepsilon$ -optimal arm in our piecewise stationary linear bandit (PSLB) model. ", "page_idx": 14}, {"type": "text", "text": "In adversarial bandits, existing works pertaining to the BAI problem only explored the fixed-horizon setting [28]. Due to the difference between the fixed-confidence and fixed-horizon setting and the difference between adversarial and piecewise stationary bandits, their results cannot be trivially extended to solve the fixed-confidence BAI problem in piecewise stationary bandits. ", "page_idx": 14}, {"type": "text", "text": "D  More Details of Algorithms NeBAI, $\\mathrm{PS}\\varepsilon$ BAI and ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}^{+}$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All proposed algorithms make use of the well known G-optimal allocation (design) [1, 29], which is widely used in the linear bandits literature. The G-optimal allocation minimizes the maximal mean-squared prediction error in all directions [1]. Given an arm set $\\mathcal{X}$ , the G-optimal allocation $\\lambda^{*}$ is a distribution over the arm set, which is the minimizer of $g(\\lambda)=\\operatorname*{max}_{x\\in\\mathcal{X}}\\|\\dot{\\boldsymbol{x}}\\|_{A(\\lambda)^{-1}}^{2}$ where $\\begin{array}{r}{A(\\lambda)=\\sum_{x\\in\\mathcal{X}}{\\lambda(x)x x}^{\\top}}\\end{array}$ and $\\lambda\\in\\Delta_{\\mathcal{X}}$ . Interested readers may refer to Chapter 21 in [29]. ", "page_idx": 14}, {"type": "text", "text": "D.1The NAiVE $\\varepsilon$ -BEST ARM IDENTIFICATION $\\mathbf{\\nabla}[\\mathbf{N}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I})$ Algorithm ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As presented in Algorithm 2, $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ samples an arm with the G-optimal allocation at each time steps; its stopping rule is grounded on the property of $\\mathrm{G}$ -optimal allocation (see Lemma E.1) and affected by the maximum length of a single stationary segment $L_{\\mathrm{max}}$ ", "page_idx": 14}, {"type": "text", "text": "1: Input: the arm set $\\mathcal{X}$ : the phase length bounds $L_{\\mathrm{min}},L_{\\mathrm{max}}$ , the slackness parameter $\\varepsilon$ , the   \nconfidence parameter $\\delta$   \n2: Initialize: Compute the G-optimal allocation $\\lambda^{*}$   \n3: Compute $\\textstyle C_{3}={\\overline{{\\sum_{n=1}^{\\infty}n^{-3}}}}$ and $t^{*}=3d\\ln(6d K C_{3}/\\delta)$   \n4: Sample $t^{*}$ arms $\\{x_{s}\\}_{s=1}^{t^{*}}\\sim\\lambda^{*}$ and observe the associated returns $\\{Y_{s,x_{s}}\\}_{s=1}^{t^{*}}$ . Let $t=t^{*}$   \n5: Compute ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\theta}_{t}=\\displaystyle\\frac{1}{t}\\sum_{s=1}^{t}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}},\\quad\\dot{x}_{t}=\\arg\\operatorname*{max}_{x}x^{\\top}\\tilde{\\theta}_{t},}\\\\ &{\\tilde{\\rho}_{t}=\\sqrt{\\frac{8L_{\\operatorname*{max}}}{t}\\ln\\frac{4K C_{3}t^{3}}{\\delta}}+5\\sqrt{\\frac{d}{t}\\ln\\frac{4K C_{3}t^{3}}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "6: while $\\begin{array}{r}{\\dot{x}_{t}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon<\\operatorname*{max}_{x\\neq\\dot{x}_{t}}x^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t}}\\end{array}$ do   \n7: Sample an arm $x_{t}\\sim\\lambda^{*}$ and observe return $Y_{t,x_{t}}$ and let $t=t+1$   \n8: Update $\\tilde{\\theta}_{t},\\dot{x}_{t}$ and ${\\tilde{\\rho}}_{t}$ with (D.1).   \n9: end while   \n10: Recommend arm $\\dot{x}_{\\varepsilon}=\\dot{x}_{t}$ ", "page_idx": 15}, {"type": "text", "text": "D.2  Details about $\\mathrm{PS}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.2.1  Confidence radius utilized in $\\mathrm{PS}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For each pair of arms $(x,\\tilde{x})$ , the confidence radius of $\\Delta(x,\\tilde{x})$ at time step $t$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{t}(x,\\tilde{x}):=2(\\alpha_{t}+\\xi_{t})+\\sum_{j=1}^{N}\\beta_{t,j}|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a^{\\mathrm{clip_{2}}}:=\\operatorname*{min}\\{\\operatorname*{max}\\{a,-2\\},2\\}$ denotes the value of $a$ that is clipped to the interval $[-2,2]$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{t}:=5\\sqrt{\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}},\\ \\xi_{t}:=25\\sqrt{2}\\frac{N L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}},}\\\\ &{\\beta_{t,j}:=\\operatorname*{min}\\left\\{\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}},\\ 1\\right\\},}\\\\ &{\\phi_{t,j}:=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\ \\frac{1}{4}\\right\\},}\\\\ &{\\delta_{v,T_{t}}=\\frac{\\delta}{15K T_{t}^{3}},\\ \\delta_{m,T_{t}}=\\frac{\\delta}{15K N T_{t}^{3}},\\ \\delta_{d,T_{t}}=\\frac{\\delta}{15K T_{t}^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\zeta_{t}(x,\\tilde{x})$ minimizes the last summation. In the final theoretical upper bound on the sample complexity, wetake $\\zeta_{t}(x,\\tilde{x})=\\varepsilon$ for simplicity. In the experiment, we utilize ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\zeta_{t}(x,\\tilde{x})=\\operatorname*{arg\\,min}_{\\zeta_{t}(x,\\tilde{x})\\in\\mathbb{R}}\\sum_{j=1}^{N}\\beta_{t,j}(\\hat{\\Delta}_{t,j}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x}))^{2}=-\\frac{\\sum_{j=1}^{N}\\beta_{t,j}\\hat{\\Delta}_{t,j}(x,\\tilde{x})}{\\sum_{j=1}^{N}\\beta_{t,j}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for a simple and effective analytic expression in the experiment. ", "page_idx": 15}, {"type": "text", "text": "D.2.2  Subroutines of $\\mathrm{PS}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the LCD subroutine (Algorithm 3), two estimates $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{2}$ of the latent vectors are independently computed from the first and second halves of the input CD samples (Line 2). LCD only raises an alarm of changepoint (Line 4) when the difference between $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{2}$ is sufficiently large (Line 3) and indicates a changepoint occurs w.h.p. ", "page_idx": 15}, {"type": "text", "text": "The LCA subroutine (Algorithm 4) estimates the latent vector of current context and updates the dictionary $\\mathrm{CA}_{\\mathrm{id}}$ .Specifically, it firstly samples $w/2$ sampleswith $\\lambda^{*}$ (Line 2). It then checks ", "page_idx": 15}, {"type": "text", "text": "1: Input: arm set $\\mathcal{X}$ , detection window $w$ , threshold $b$ and $w$ arms with the associated observations   \n$[(\\bar{\\tilde{x}}_{1},Y_{s,\\tilde{x}_{1}}),\\ldots,(\\tilde{x}_{w},Y_{w,\\tilde{x}_{w}})]$   \n2: Compute $\\begin{array}{r}{\\tilde{\\theta}_{1}=\\frac{2}{w}\\sum_{s=1}^{\\frac{w}{2}}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}}\\end{array}$ and $\\begin{array}{r}{\\tilde{\\theta}_{2}=\\frac{2}{w}\\sum_{s=\\frac{w}{2}+1}^{w}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}.}\\end{array}$   \n3: if $\\exists x\\in\\mathcal{X}$ \uff0c $s.t.|x^{\\top}(\\tilde{\\theta}_{2}-\\tilde{\\theta}_{1})|>b$ then   \n4:Return True   \n5:else   \n6:Return False   \n7: end if ", "page_idx": 16}, {"type": "text", "text": "Algorithm 4 LINEAR-CONTEXT ALIGNMENT (LCA) 1: Input: arm set $\\mathcal{X}$ ,detection window $w$ , threshold $b$ and context ids $\\mathrm{CA}_{\\mathrm{id}}$   \n2: Sample $\\frac w2$ arms $\\{\\tilde{x}_{s}\\}_{s=1}^{\\frac{w}{2}}\\sim\\lambda^{*}$ adobereteasatd s $\\{Y_{s,\\tilde{x}_{s}}\\}_{s=1}^{\\frac{w}{2}}$ , where $\\{(x_{s},Y_{s,x_{s}})\\}_{s=t-\\frac{w}{2}+1}^{t}=\\{\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}}\\}_{s=1}^{\\frac{w}{2}}.$ 3: for $j\\in\\mathrm{CA}_{\\mathrm{id}}$ do   \n4: ifnot $\\mathrm{LCD}(\\mathcal{X},w,b,[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{\\frac{w}{2}}+\\mathrm{CA_{id}}[j])$ then   \n5: $\\mathrm{CA}_{\\mathrm{id}}[j]=\\left[\\left(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}}\\right)\\right]_{s=1}^{\\frac{w}{2}}$   \n6: Return $j,\\mathrm{CA_{\\mathrm{id}}}$   \n7:end if   \n8: end for   \n9: index $=\\mathrm{len(CA_{id})+1}.$   \n10: $\\mathrm{CA}_{\\mathrm{id}}[\\mathrm{index}]=[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{\\frac{w}{2}}.$   \n11: Return index, $\\mathrm{CA}_{\\mathrm{id}}$ ", "page_idx": 16}, {"type": "text", "text": "whether the current latent context has been visited in previous time steps by scanning through $\\mathrm{CA}_{\\mathrm{id}}$ (Line 3). In order to learn if the current context can be aligned with context $j$ , the LCD subroutine is called with the $w/2$ recent samples and $\\mathrm{CA}_{\\mathrm{id}}[j]$ as input: ", "page_idx": 16}, {"type": "text", "text": "(i) If the current context is aligned with $\\mathrm{CA}_{\\mathrm{id}}[j]$ (Line 4), the current latent context is $j$ w.h.p. Thus the LCA subroutine updates $\\mathrm{CA}_{\\mathrm{id}}[j]$ and returns the index $j$ (Lines 5 to 6), which would be $\\hat{j}_{t}$ in Line 19 of Algorithm 1. Note that in Lines 10 and 11 of Algorithm 1, all the collected samples will be assigned to context $\\hat{j}_{t}$ (until the next changing alarm) and will be used to estimate $\\theta_{\\hat{j}_{t}}^{*}$ and all $p_{j}$ Therefore, aligning contexts allows ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}$ to make good use of observation history. ", "page_idx": 16}, {"type": "text", "text": "(ii) If the current context is not aligned with any observed context, i.e., it has not been visited, $\\mathrm{CA}_{\\mathrm{id}}$ gets extended with a new index-samples pair and is returned along with the new index (Lines 9 to 11). ", "page_idx": 16}, {"type": "text", "text": "Tohelpunderstand ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm, we highlight the differences between $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ and ${\\tt P S}{\\tt E B}\\bar{\\bf A}{\\bf I}^{+}$ , and the differences between subroutines LINEAR-CONTEXT ALIGNMENT (LCA) and LINEAR-CONTEXT ALIGNMENT+ $(\\mathrm{LCA^{+}})$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 5 PIECEWISE-STATIONARY $\\varepsilon$ -BEST ARM IDENTIFICATION+( $\\mathbf{\\nabla}\\mathbf{P}\\mathbf{S}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+},$   \n1: Input: arm set $\\mathcal{X}$ , number of latent vectors $N$ , bounds on the segment lengths $L_{\\mathrm{min}}$ and   \nslackness parameter $\\varepsilon$ , confidence parameter $\\delta$ , sampling parameter $\\gamma$ , window size $w$   \n2: thrshie $b$ Compute theG-optmalocaton and   \n$\\lambda^{*}$ $\\begin{array}{r}{\\tau^{*}\\!=\\!\\frac{38400\\ln(80)N L_{\\mathrm{max}}}{\\varepsilon^{2}}\\ln\\frac{N^{2}K L_{\\mathrm{max}}}{\\delta\\varepsilon^{2}}.}\\end{array}$   \n3: Set $\\mathrm{CD}_{\\mathrm{sample}}=[\\ ]$ $\\mathrm{CA}_{\\mathrm{id}}=\\left\\{\\begin{array}{l l}{\\begin{array}{r l r l}\\end{array}}\\end{array}\\right\\}$ \uff0c $t_{\\mathrm{CD}}=+\\infty$ . Set $\\mathcal{T}_{t,j}=\\emptyset$ and initialize $\\mathcal{T}_{t},T_{t,j},T_{t}$ with (   \nfor all $t\\leq\\tau^{*}$ \uff0c $j\\in[N]$   \n$\\begin{array}{r}{C_{3}=\\sum_{n=1}^{\\bar{\\infty}}n^{-3}}\\end{array}$ $t^{*}=3d\\ln(6d K C_{3}/\\delta).$   \n$w/2$ $\\{x_{s}\\}_{s=1}^{w/2}\\sim\\lambda^{*}$ $\\{Y_{s,x_{s}}\\}_{s=1}^{w/2}$   \n6: Set $\\textstyle t={\\frac{w}{2}}$ $t_{\\mathrm{CA}}=w/2$ $\\mathrm{CA}_{\\mathrm{id}}=\\{1:[(x_{s},Y_{s,x_{s}})]_{s=1}^{w/2}\\},\\hat{j}_{t}=1$   \n7: Set ${\\dot{x}}_{t}=0$ $\\tilde{\\rho}_{t}=2\\varepsilon$ and $\\dot{x}_{\\varepsilon}=-\\infty$ . Compute $\\tilde{{\\boldsymbol{\\theta}}}_{t}$ with (D.1).   \n8: while True do   \n9: $t=t+1$   \n10: Sample an arm $x_{t}\\sim\\lambda^{*}$ and observe return $Y_{t,x_{t}}$   \n11: Compute $\\tilde{\\theta}_{t},\\ \\dot{x}_{t},\\ \\tilde{\\rho}_{t}=\\mathrm{EU}(t,\\tilde{\\theta}_{t-1},t^{*},C_{3})$   \n12: if $\\begin{array}{r}{\\dot{x}_{t}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\geq\\operatorname*{max}_{x\\neq\\dot{x}_{t}}x^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t}}\\end{array}$ then   \n13: $\\dot{x}_{\\varepsilon}=\\dot{x}_{t}$   \n14: Break   \n15: end if   \n16: if $t>\\tau^{*}$ then   \n17: Continue   \n18: end if   \n19: if mod $(t-t_{\\mathrm{CA}},\\gamma)\\neq0$ then   \n20: Update $\\hat{j}_{t}=\\hat{j}_{t-1}$ \uff0c $T_{t,\\hat{j}_{t}}=T_{t-1,\\hat{j}_{t}}\\cup\\{t\\},T_{t,j}=T_{t-1,j}$ for $j\\neq\\hat{j}_{t}$   \n21: else   \n22: $\\mathrm{CD}_{\\mathrm{sample}}=\\mathrm{CD}_{\\mathrm{sample}}+[(x_{t},Y_{t,x_{t}})].$   \n23: $|\\mathrm{CD}_{\\mathrm{sample}}|\\geq w$ then   \n24: ifLC $\\mathrm{:D}\\mathrm{(}\\mathcal{X},w,b,\\mathrm{CD}_{\\mathrm{sample}}[-w:])$ then   \n25: $\\mathrm{CD}_{\\mathrm{sample}}=\\left[\\begin{array}{l}{\\right]}$   \n26: $\\begin{array}{r}{t=t+\\frac{w}{2},t_{\\mathrm{CA}}=t,t_{\\mathrm{CD}}=+\\infty.}\\end{array}$   \n27: $\\hat{j}_{t},\\mathrm{CA}_{\\mathrm{id}}=\\mathrm{LCA}^{+}(\\mathcal{X},w,b,\\mathrm{CA}_{\\mathrm{id}})$   \n28: if $\\hat{j}_{t}=N+1$ then break.   \n29: Revert $\\begin{array}{r}{\\mathcal{T}_{t,j}=\\mathcal{T}_{t-\\frac{w(\\gamma+1)}{2},j}}\\end{array}$ for all $j\\in[N]$   \n30: end if   \n31: end if   \n32: end if   \n33: $\\dot{x}_{\\varepsilon}\\neq-\\infty$ then   \n34: Break   \n35: end if   \n36: Update the empirical estimates by (3.2) and (3.3).   \n37: if condition (3.4) is met and $t_{\\mathrm{CD}}=+\\infty$ then   \n38: Record $\\hat{x}_{\\varepsilon}=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\hat{\\Theta}_{t}\\hat{\\mathbf{p}}_{t}$   \n39: $t_{\\mathrm{CD}}=|\\mathrm{CD}_{\\mathrm{sample}}|$ \uff1a   \n40: else if $\\begin{array}{r}{t_{\\mathrm{CD}}=|\\mathrm{CD}_{\\mathrm{sample}}|-\\frac{w}{2}}\\end{array}$ then   \n41: $\\dot{x}_{\\varepsilon}=\\hat{x}_{\\varepsilon}$   \n42: Break   \n43: end if   \n44: end while   \n45: Recommend $\\mathring{x}_{\\varepsilon}=\\dot{x}_{\\varepsilon}$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 6 EsTIMATE UPDATE (EU) ", "page_idx": 18}, {"type": "text", "text": "1: Input: time step $t$ , vector $\\tilde{\\theta}_{t-1}$ , threshold $t^{*}$ and constant $C_{3}$   \n2: Compute $\\tilde{\\theta}_{t}=\\frac{1}{t}\\left[(t-1)\\tilde{\\theta}_{t-1}+A(\\lambda^{*})^{-1}x_{t}Y_{s,x_{t}}\\right],\\;\\dot{x}_{t}=0,\\;\\tilde{\\rho}_{t}=2\\varepsilon.$   \n3: if $t\\geq t^{*}$ then   \n4: Compute $\\dot{x}_{t}=\\operatorname*{arg\\,max}_{x\\in\\mathcal{X}}{x^{\\top}\\tilde{\\theta}_{t}},\\ \\tilde{\\rho}_{t}=\\sqrt{\\frac{8L_{\\operatorname*{max}}}{t}\\ln\\frac{4K C_{3}t^{3}}{\\delta}}+5\\sqrt{\\frac{d}{t}\\ln\\frac{4K C_{3}t^{3}}{\\delta}}.$   \n5: end if   \n6: Return ${\\tilde{\\theta}}_{t}$ $\\dot{{x}}_{t},\\ \\tilde{{\\rho}}_{t}$ ", "page_idx": 18}, {"type": "text", "text": "Algorithm 7 LINEAR-CONTEXT ALIGNMENT+ (LCA+) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: arm set $\\mathcal{X}$ , detection window $w$ , threshold $b$ , context ids $\\mathrm{CA}_{\\mathrm{id}}$ , time step $t_{-}$ vector $\\tilde{{\\boldsymbol{\\theta}}}_{t}.$ threshold $t^{*}$ and constant $C_{3}$ 2: Set $s=0$ \uff0c $\\dot{x}_{\\varepsilon}=-\\infty$ 3: for $s\\leq w/2$ do 4: $s=s+1$ $t=t+1$ 5: Sample an arm $\\tilde{x}_{s}\\sim\\lambda^{*}$ and observe return $Y_{s,\\tilde{x}_{s}}$ 6: Set $(x_{t},Y_{t,x_{t}})=(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})$ 7: Compute $\\tilde{{\\boldsymbol{\\theta}}}_{t}$ \uff0cct, $\\tilde{\\rho}_{t}=\\mathrm{EU}(t,\\bar{\\theta}_{t-1},t^{*},C_{3})$ 8: $\\begin{array}{r}{\\dot{x}_{t}^{\\top}\\bar{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\geq\\operatorname*{max}_{x\\neq\\dot{x}_{t}}x^{\\top}\\bar{\\theta}_{t}+\\tilde{\\rho}_{t}}\\end{array}$ then   \n9: ${\\dot{x}}_{\\varepsilon}={\\dot{x}}_{t},{\\mathrm{index}}=\\log(\\operatorname{CA}_{\\mathrm{id}})+1$   \n10: Return index, $\\mathrm{CA}_{\\mathrm{id}}$ \uff0c $\\dot{x}_{\\varepsilon},\\,\\tilde{\\theta}_{t}$   \n11: end if   \n12: end for   \n13: for $j\\in\\mathrm{CA}_{\\mathrm{id}}$ do   \n14: if not $\\mathrm{LCD}(w,b,[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{\\frac{w}{2}},\\mathrm{CA}_{\\mathrm{id}}[j])$ then   \n15: $\\mathrm{CA}_{\\mathrm{id}}[j]=\\left[\\left(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}}\\right)\\right]_{s=1}^{\\frac{w}{2}}$   \n16: Return $j,\\mathrm{CA_{\\mathrm{id}}}$   \n17: end if   \n18: end for   \n19: $\\mathrm{index=len(CA_{id})+1}$   \n20: $\\mathrm{CA}_{\\mathrm{id}}[\\mathrm{index}]=[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{\\frac{\\omega}{2}}.$   \n21: Return index, $\\mathrm{CA}_{\\mathrm{id}},\\,\\dot{x}_{\\varepsilon},\\,\\tilde{\\theta}_{t}$ ", "page_idx": 18}, {"type": "text", "text": "D.4   Computational Complexity of $\\mathrm{PS}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide analysis for the computational complexity of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ in this subsection. ", "page_idx": 18}, {"type": "text", "text": "We consider the number of these operations: arithmetic (addition and multiplication) operations, logic operations and comparison operations and we also regard $\\ln(\\cdot)$ $\\sqrt{\\cdot}$ and sampling from a distribution as onestep of operation. ", "page_idx": 18}, {"type": "text", "text": "We decompose the main loop of Algorithm 1 as follows, where the lines with $O(1)$ operations are omitted: ", "page_idx": 18}, {"type": "text", "text": "\u00b7 Exploration phase (Lines 8 to 11): $O(1)$   \n\u00b7 Change Detection phase (Lines 12 to 16): - The $L C D$ subroutine in Line 16 needs $O(w d^{2}+K d)$ operations.   \n\u00b7 Context Alignment phase (Lines 17 to 21): - The $L C A$ subroutine in Line 19 needs $O((w d^{2}+K d)N)$ operations, as the $L C D$ subroutine will be invoked $N$ times in the worst case in Line 4 of $L C A$ - The reversion procedure in Line 21 needs $O(\\gamma w d)$ operations.   \n\u00b7 The updating procedure and the stopping rule checking (Lines 25 to 32): - Updating $\\widehat{\\theta}_{t,j}$ needs $O(d^{2})$ operations, as we need to incorporate the latest sample into the estimate. ", "page_idx": 18}, {"type": "text", "text": "Updating $\\hat{p}_{t,j},j\\in[N]$ requires $N$ operations. ", "page_idx": 19}, {"type": "text", "text": "Updating the confidence radius and the empirical best arm need $O(K N)$ operations.   \nThe stopping condition in Equation (3.4) requires $O(K)$ operations. ", "page_idx": 19}, {"type": "text", "text": "We remark that some intermediate results can be stored to avoid repeated computations and we believe the algorithm is efficient overall. In particular, since the reward structure is linear, there are $K$ arms, and $N$ contexts, $O(d^{2}),O(K)$ ,and $O(N)$ operations probably cannot be be avoided. ", "page_idx": 19}, {"type": "text", "text": "From the above analysis, in the decreasing order of the number of operations: ", "page_idx": 19}, {"type": "text", "text": "\u00b7 The Change Detection phase requires the most budget as it will be invoked every $\\gamma$ time steps.   \n\u00b7 The Context Alignment phase requires the second many operations. While it requires many operations when it is implemented, it will not be called during a stationary segment.   \n\u00b7 The updating procedure uses small portion of the operations, as $w$ is usually much greater than $K$ and $N$   \n\u00b7 The exploration phase demands constant operations in each loop. ", "page_idx": 19}, {"type": "text", "text": "E Auxiliary results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma E.1. Let $X_{s}$ be the arm drawn with the $G$ optimal allocation $\\lambda^{*}$ at time step s and $Y_{s,x_{s}}=$ $x_{s}^{\\top}\\theta_{j_{s}}^{*}+\\eta_{s}$ be the corresponding return with $\\mathbb{E}\\eta_{s}=0$ $|x_{s}^{\\top}\\theta_{j_{s}}^{*}|\\leq1$ and $\\lvert\\eta_{s}\\rvert\\leq1$ Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\frac{1}{n}\\left|\\sum_{s=1}^{n}{x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}}-{x^{\\top}\\theta_{j_{s}}^{*}}\\right|\\geq\\epsilon\\right]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $\\begin{array}{r}{\\epsilon\\geq\\frac{d}{n}\\ln\\frac{2}{\\delta}+\\sqrt{\\left(\\frac{d}{n}\\ln\\frac{2}{\\delta}\\right)^{2}+\\frac{4d}{n}\\ln\\frac{2}{\\delta}}}\\end{array}$ . In addition, $\\begin{array}{r}{i f n\\geq\\frac{d}{4}\\ln\\frac{2}{\\delta}}\\end{array}$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\frac{1}{n}\\left|\\sum_{s=1}^{n}x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j_{s}}^{*}\\right|\\geq5\\sqrt{\\frac{d}{n}\\ln\\frac{2}{\\delta}}\\right]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Note that the arms are selected according to the $\\mathrm{G}$ -optimal design, and thus $\\mathbb{E}_{x\\sim\\lambda^{*}}[x x^{\\top}]=$ $A(\\lambda^{*})$ . Therefore, for all $s\\in[n]$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j_{s}}^{*}\\right]=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j_{s}}^{*}|}\\\\ &{\\leq|x^{\\top}A(\\lambda^{*})^{-1}x_{s}x_{s}^{\\top}\\theta_{j_{s}}^{*}|+|x^{\\top}A(\\lambda^{*})^{-1}x_{s}\\eta_{s}|+1}\\\\ &{\\leq|x^{\\top}A(\\lambda^{*})^{-1}x_{s}||x_{s}^{\\top}\\theta_{j_{s}}^{*}|+|x^{\\top}A(\\lambda^{*})^{-1}x_{s}||\\eta_{s}|+1}\\\\ &{\\leq2|x^{\\top}A(\\lambda^{*})^{-1}x_{s}|+1}\\\\ &{\\leq2\\|x\\|_{A(\\lambda^{*})^{-1}}\\|x_{s}\\|_{A(\\lambda^{*})^{-1}}+1}\\\\ &{\\leq3d}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we make use of the property of the $\\mathrm{G}$ -optimal allocation in the last inequality ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x\\|_{A(\\lambda^{*})^{-1}}^{2}\\leq d,\\;\\forall x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Additionally, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}(x_{s}^{\\top}\\theta_{j s}^{*}+\\eta_{s})\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}x_{s}^{\\top}\\theta_{j s}^{*}\\right)^{2}\\right]+\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}\\eta_{s}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad+\\,2\\mathbb{E}\\left[x^{\\top}A(\\lambda^{*})^{-1}x_{s}x_{s}^{\\top}\\theta_{j s}^{*}\\cdot x^{\\top}A(\\lambda^{*})^{-1}x_{s}\\eta_{s}\\right]}\\\\ &{\\overset{(a)}{=}\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}\\right)^{2}\\left(x_{s}^{\\top}\\theta_{j s}^{*}\\right)^{2}\\right]+\\mathbb{E}\\left[\\eta_{s}^{2}\\left(x^{\\top}A(\\lambda^{*})^{-1}x_{s}\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(b)}{\\leq}2\\mathbb{E}\\left[\\left(x^{\\top}A({\\lambda^{*}})^{-1}x_{s}\\right)^{2}\\right]}\\\\ &{\\overset{(c)}{=}2\\,x^{\\top}A({\\lambda^{*}})^{-1}x}\\\\ &{\\overset{(d)}{\\leq}2d}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we make use of the fact that $\\eta_{t}$ is zero-mean and is independent of other random variables in $(a)$ .\uff0c $|x_{s}^{\\top}\\theta_{j_{s}}^{*}|\\leq1$ and $\\mathbb{P}\\left[\\eta_{s}^{2}\\le1\\right]=1$ in ( $b);x_{s}\\sim\\lambda^{*}$ in $(c)$ ; and the property of the G-optimal allocation (E.3) in $(d)$ ", "page_idx": 20}, {"type": "text", "text": "According to the Bernstein's inequality with (E.1), (E.2) and (E.4) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathbb{P}\\left[\\displaystyle\\frac{1}{n}\\Big\\vert\\sum_{s=1}^{n}x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j_{s}}^{*}\\Big\\vert\\geq\\epsilon\\right]}\\\\ &{\\leq2\\exp\\left(-\\frac{\\frac{1}{2}(n\\epsilon)^{2}}{n\\cdot2d+d n\\epsilon}\\right)}\\\\ &{=2\\exp\\left(-\\frac{n\\epsilon^{2}}{4d+2d\\epsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In order to upper bound the error probability by $\\delta$ ,we need ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle2\\exp\\left(-\\frac{n\\epsilon^{2}}{4d+2d\\epsilon}\\right)\\leq\\delta}\\\\ {\\Rightarrow}&{\\epsilon\\geq\\displaystyle\\frac{d}{n}\\ln\\frac{2}{\\delta}+\\sqrt{\\left(\\frac{d}{n}\\ln\\frac{2}{\\delta}\\right)^{2}+\\frac{4d}{n}\\ln\\frac{2}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In addition, if $\\begin{array}{r}{n\\geq\\frac{d}{4}\\ln\\frac{2}{\\delta}}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n5{\\sqrt{\\frac{d}{n}\\ln\\frac{2}{\\delta}}}={\\frac{5}{2}}\\operatorname*{max}\\left\\{{\\frac{d}{n}}\\ln{\\frac{2}{\\delta}},{\\sqrt{\\frac{4d}{n}\\ln\\frac{2}{\\delta}}}\\right\\}\\geq{\\frac{d}{n}}\\ln{\\frac{2}{\\delta}}+{\\sqrt{\\left({\\frac{d}{n}}\\ln{\\frac{2}{\\delta}}\\right)^{2}+{\\frac{4d}{n}}\\ln{\\frac{2}{\\delta}}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 20}, {"type": "text", "text": "F  Analysis of $\\mathbf{N}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To analyze the theoretical performance of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ , we first show that it can identify an $\\varepsilon$ -optimal arm withprobability $1-\\delta$ and derive a high-probability upper bound on its stopping time in Lemma F.1. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.1 (High-probability upper bound of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ ).With probability $1-\\delta$ the NeBAI algorithm identifiesan $\\varepsilon$ -optimal armafteratmost ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\frac{L_{\\mathrm{max}}+d}{\\left(\\varepsilon+\\Delta_{\\mathrm{min}}\\right)^{2}}\\ln\\frac{1}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "time steps, where $\\Delta_{\\operatorname*{min}}=\\operatorname*{min}_{i\\neq i^{*}}\\Delta(x^{*}-x_{i})$ ", "page_idx": 20}, {"type": "text", "text": "Next, we prove that after a sufficiently large number of time steps, the probability that $\\mathrm{N}\\varepsilon$ BAI does not terminate is small in Lemma F.2. ", "page_idx": 20}, {"type": "text", "text": "Lemma F.2. Let ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{0}=\\frac{768\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}}\\ln\\frac{768K C_{3}(8L_{\\mathrm{max}}+25d)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}\\,\\delta}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "wih $\\begin{array}{r}{C_{3}=\\sum_{n=1}^{\\infty}n^{-3}}\\end{array}$ For $t\\geq T_{0}$ the probaity that $\\Nu\\varepsilon$ BAI does not terminate efore $t$ time steps is (\u03b1-1)C3(t/2)2. ", "page_idx": 20}, {"type": "text", "text": "Lastly, we apply Lemmas F.1 and F.2 to prove Proposition 3.1 ", "page_idx": 20}, {"type": "text", "text": "Proposition 3.1. Let $\\begin{array}{r}{\\Delta_{\\mathrm{min}}=\\operatorname*{min}_{x\\neq x^{*}}\\Delta(x^{*},x),}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{\\mathrm{V}}^{\\mathrm{N}}=\\frac{d}{\\left(\\varepsilon+\\Delta_{\\mathrm{min}}\\right)^{2}}\\ln\\frac{1}{\\delta}\\qquad a n d\\qquad T_{\\mathrm{D}}^{\\mathrm{N}}=\\frac{L_{\\mathrm{max}}}{\\left(\\varepsilon+\\Delta_{\\mathrm{min}}\\right)^{2}}\\ln\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The $\\mathrm{N}\\varepsilon$ BAI algorithm is $(\\varepsilon,\\delta)$ -PAC and its expected sample complexity is $\\tilde{O}(T_{\\mathrm{V}}^{\\mathrm{N}}+T_{\\mathrm{D}}^{\\mathrm{N}})$ ", "page_idx": 21}, {"type": "text", "text": "The detailed analysis is presented as below. ", "page_idx": 21}, {"type": "text", "text": "F.1 Detailed analysis of $\\mathrm{N}\\varepsilon$ BAI ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proofof Lemma $F.l$ . The result is to be proven with the following procedures. ", "page_idx": 21}, {"type": "text", "text": "First step: Prove $\\left|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-x^{\\top}\\tilde{\\theta}_{t}\\right|$ can be bounded with high probability. ", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{\\theta}_{t}=\\frac{1}{t}\\sum_{s=1}^{t}\\theta_{j_{s}}^{*}}\\end{array}$ . (i) For all $\\varepsilon>0$ , we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\mathbf{\\Lambda}\\middle|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta_{t}-x^{\\top}\\bar{\\theta}\\right|>\\varepsilon\\right]=\\mathbb{P}\\left[\\mathbf{\\Lambda}\\middle|t x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-x^{\\top}\\left(\\underset{l=1}{\\overset{N_{t}}{\\sum}}L_{l}\\theta^{*}j_{c_{l}}\\right)\\middle|>t\\varepsilon\\right]}\\\\ &{=\\mathbb{P}\\left[\\mathbf{\\Lambda}\\Bigg[\\underset{l=1}{\\overset{N_{t}}{\\sum}}L_{l}\\left(x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-L_{l}x^{\\top}\\theta^{*}j_{c_{l}}\\right)\\Bigg|>t\\varepsilon\\right]\\leq2\\exp\\left(-\\frac{t^{2}\\varepsilon^{2}}{2\\sum_{l=1}^{N_{t}}(2L_{l})^{2}}\\right)}\\\\ &{\\overset{(b)}{\\leq}2\\exp\\left(-\\frac{t\\varepsilon^{2}}{8L_{\\operatorname*{max}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta|\\leq1$ and $x^{\\top}\\theta^{*}j_{c_{l}}|\\leq1$ for all $l$ we obtain (a) by applying Hoeffding's inequality. We obtain (b) from the fact that $\\sum_{l=1}^{N_{t}}L_{l}=t$ and $L_{l}\\leq L_{\\mathrm{max}}$ for all $l$ In other words, for all $\\delta>0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\begin{array}{l}{\\left|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-x^{\\top}\\bar{\\theta}\\right|>\\sqrt{\\frac{8L_{\\operatorname*{max}}}{t}\\ln\\frac{2}{\\delta}}}\\end{array}\\right]\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(ii) Besides, according to Lemma E.1, if $\\begin{array}{r}{t\\geq\\frac{d}{4}\\ln\\frac{2}{\\delta}}\\end{array}$ ,wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Im\\left[\\left|x^{\\top}\\tilde{\\theta}_{t}-x^{\\top}\\bar{\\theta}_{t}\\right|\\ge5\\sqrt{\\frac{d}{t}\\ln\\frac{2}{\\delta}}\\right]=\\mathbb{P}\\left[\\frac{1}{t}\\left|\\sum_{s=1}^{t}x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j_{s}}^{*}\\right|\\ge5\\sqrt{\\frac{d}{t}\\ln\\frac{2}{\\delta}}\\right]\\le\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(i) For all $t>0$ , all $x\\in\\mathscr{X}$ , since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|{\\boldsymbol x}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-{\\boldsymbol x}^{\\top}\\tilde{\\theta}_{t}\\right|\\leq\\left|{\\boldsymbol x}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-{\\boldsymbol x}^{\\top}\\bar{\\theta}\\right|+\\left|{\\boldsymbol x}^{\\top}\\tilde{\\theta}_{t}-{\\boldsymbol x}^{\\top}\\bar{\\theta}_{t}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-x^{\\top}\\tilde{\\theta}_{t}\\right|>\\tilde{\\rho}_{t}(\\alpha)\\right]\\leq\\frac{\\delta}{K C_{\\alpha}t^{\\alpha}}\\quad\\mathrm{~when~}\\quad t\\geq\\frac{d}{4}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\rho}_{t}(\\alpha)=\\sqrt{\\frac{8L_{\\operatorname*{max}}}{t}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}+5\\sqrt{\\frac{d}{t}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}},\\quad C_{\\alpha}=\\sum_{n=1}^{\\infty}n^{-\\alpha},\\quad\\alpha>2.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity, we set $\\alpha\\,=\\,3$ and write $\\tilde{\\rho}_{t}(3)$ as ${\\tilde{\\rho}}_{t}$ in the following analysis. We now show that $\\begin{array}{r}{t\\,\\ge\\,\\frac{d}{4}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}\\end{array}$ holds when $\\begin{array}{r}{t\\,\\geq\\,3d\\ln{\\frac{6d K C_{3}}{\\delta}}}\\end{array}$ With the folowing lemma (the proof of whichis presented by the end of Appendix F.1). ", "page_idx": 21}, {"type": "text", "text": "Lemma F.3. Let $a,b,c>0$ and $a c\\ge1/2$ ,then ", "page_idx": 21}, {"type": "equation", "text": "$$\nt\\geq\\operatorname*{max}\\{2a\\ln b,\\,4a c\\ln(2a c)\\}\\;\\Rightarrow\\;t\\geq a\\ln(b t^{c}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With ", "page_idx": 21}, {"type": "equation", "text": "$$\na=\\frac{d}{4},\\quad b=\\frac{4K C_{\\alpha}}{\\delta}=\\frac{4K C_{3}}{\\delta},\\quad c=\\alpha=3,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma F.3 implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\nt\\geq4a c\\ln(2a b c)=3d\\ln{\\frac{6d K C_{3}}{\\delta}}\\,\\,\\Rightarrow\\,\\,t\\geq\\operatorname*{max}\\{2a\\ln b,\\,4a c\\ln(2a c)\\}\\,\\,\\Rightarrow\\,\\,t\\geq a\\ln(b t^{c}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define event $\\tilde{E}_{t}=\\bigcap_{x\\in\\mathcal{X}}\\left\\{\\left|x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-x^{\\top}\\tilde{\\theta}_{t}\\right|\\leq\\tilde{\\rho}_{t}(\\alpha)\\right\\}$ for $t\\geq t^{*}$ and $\\tilde{E}=\\bigcap_{t\\geq t^{*}}\\tilde{E}_{t}$ . We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\tilde{E}_{t}^{\\mathbf{c}}\\right]\\leq\\sum_{x\\in\\mathcal{X}}\\frac{\\delta}{K C_{\\alpha}t^{\\alpha}}=\\frac{\\delta}{C_{\\alpha}t^{\\alpha}},\\quad\\mathbb{P}\\left[\\tilde{E}^{\\mathbf{c}}\\right]\\leq\\sum_{t=t^{*}}^{\\infty}\\mathbb{P}\\left[\\tilde{E}_{t}^{\\mathbf{c}}\\right]\\leq\\sum_{t=1}^{\\infty}\\frac{\\delta}{C_{\\alpha}t^{\\alpha}}=\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We assume $\\tilde{E}$ holds in the second and third steps in this proof. ", "page_idx": 22}, {"type": "text", "text": "Second step: Prove $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ recommends an $\\varepsilon$ -optimal arm when it stops. ", "page_idx": 22}, {"type": "text", "text": "If the algorithm terminates and returns $\\dot{x}_{\\varepsilon}\\neq x^{*}$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{x}_{\\varepsilon}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\geq\\operatorname*{max}_{x\\neq\\dot{x}_{\\varepsilon}}x^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{x}_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta+\\varepsilon\\ge\\dot{x}_{\\varepsilon}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\quad\\mathrm{and}\\quad\\operatorname*{max}_{x\\neq\\hat{x}_{\\varepsilon}}\\tau^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t}\\ge\\operatorname*{max}_{x\\neq\\hat{x}_{\\varepsilon}}x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta\\ge x^{*^{\\top}}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{x}_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta+\\varepsilon\\geq{x^{*}}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "implying that $\\dot{x}_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ . Hence, we always have $\\dot{x}_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ ", "page_idx": 22}, {"type": "text", "text": "Third step: Derive the stopping time of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ ", "page_idx": 22}, {"type": "text", "text": "(i) Let $x_{(2)}=\\arg\\operatorname*{max}_{x\\neq x^{*}}x^{\\top}{\\mathbb{E}}_{\\theta\\sim P_{\\theta}}\\theta$ and $\\begin{array}{r}{\\Delta_{\\operatorname*{min}}=\\operatorname*{min}_{x\\neq x^{*}}\\Delta(x^{*}-x)=(x^{*}-x_{(2)})^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta}\\end{array}$ We apply the following Temma to show that $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ stops when ${\\tilde{\\rho}}_{t}$ is sufficiently small. ", "page_idx": 22}, {"type": "text", "text": "Lemma F.4 ([30, Lemma 3]). Denote by $\\hat{i}$ the index of the item with empirical mean is $i$ -th largest: i.e., $\\hat{w}(\\hat{1})\\geq...\\geq\\hat{w}(\\hat{L})$ .Assume that the empirical means of the arms are controlled by $\\varepsilon:i.e.$ \uff0c $\\forall i,|\\hat{w}(i)-w(i)|<\\varepsilon$ Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall i,w(i)-\\varepsilon\\leq\\hat{w}(\\hat{i})\\leq w(i)+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma F.4 implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\dot{x}_{t}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\ge{x^{*}}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-2\\tilde{\\rho}_{t}+\\varepsilon=x_{(2)}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-2\\tilde{\\rho}_{t}+\\varepsilon+\\Delta_{\\operatorname*{min}}\\quad\\mathrm{and}}\\\\ &{\\qquad\\quad\\displaystyle\\operatorname*{max}_{x\\ne x_{t}}\\!x^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t}\\le x_{(2)}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta+2\\tilde{\\rho}_{t}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $\\tilde{\\rho}_{t}\\leq(\\varepsilon+\\Delta_{\\operatorname*{min}})/4$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{x}_{t}^{\\top}\\tilde{\\theta}_{t}-\\tilde{\\rho}_{t}+\\varepsilon\\geq x_{(2)}^{\\top}\\tilde{\\theta}_{t}+\\frac{\\varepsilon+\\Delta_{\\operatorname*{min}}}{2}\\geq x_{(2)}^{\\top}\\tilde{\\theta}_{t}+2\\tilde{\\rho}_{t}\\geq\\operatorname*{max}_{x\\neq\\dot{x}_{t}}x^{\\top}\\tilde{\\theta}_{t}+\\tilde{\\rho}_{t},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which will lead to the termination of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ ", "page_idx": 22}, {"type": "text", "text": "(ii) According to the definition of ${\\tilde{\\rho}}_{t}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{\\rho}_{t}(\\alpha)\\leq(\\varepsilon+\\Delta_{\\operatorname*{min}})/4}\\\\ &{\\Leftrightarrow\\sqrt{\\frac{8L_{\\operatorname*{max}}}{t}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}+5\\sqrt{\\frac{d}{t}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}\\leq(\\varepsilon+\\Delta_{\\operatorname*{min}})/4}\\\\ &{\\Leftrightarrow\\sqrt{\\frac{1}{t}}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}\\left(\\sqrt{8L_{\\operatorname*{max}}}+5\\sqrt{d}\\right)\\leq(\\varepsilon+\\Delta_{\\operatorname*{min}})/4}\\\\ &{\\Leftrightarrow\\,t\\geq\\frac{\\left(\\sqrt{8L_{\\operatorname*{max}}}+5\\sqrt{d}\\right)^{2}}{(\\varepsilon+\\Delta_{\\operatorname*{min}})^{2}/16}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}\\\\ &{\\Leftarrow\\,t\\geq\\frac{32(8L_{\\operatorname*{max}}+25d)}{(\\varepsilon+\\Delta_{\\operatorname*{min}})^{2}}\\ln\\frac{4K C_{\\alpha}t^{\\alpha}}{\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Leftarrow\\,t\\geq\\frac{192\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}}\\ln\\frac{768K C_{3}\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}\\,\\delta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We apply Lemma F.3 to invert the last line above. With ", "page_idx": 23}, {"type": "equation", "text": "$$\na=\\frac{32\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}},\\quad b=\\frac{4K C_{\\alpha}}{\\delta},\\quad c=\\alpha=3,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma F.3 indicates that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad t\\geq4a c\\ln(2a b c)=\\frac{384\\left(8L_{\\operatorname*{max}}+25d\\right)}{\\left(\\varepsilon+\\Delta_{\\operatorname*{min}}\\right)^{2}}\\ln\\frac{768K C_{3}\\left(8L_{\\operatorname*{max}}+25d\\right)}{\\left(\\varepsilon+\\Delta_{\\operatorname*{min}}\\right)^{2}\\delta}}\\\\ &{\\Rightarrow\\,t\\geq\\operatorname*{max}\\{2a\\ln b,\\,4a c\\ln(2a c)\\}\\,\\Rightarrow\\,t\\geq a\\ln(b t^{c}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Altogether, we show that with probability $1-\\delta,\\mathrm{N}\\varepsilon\\mathrm{BAI}$ identifies an $\\varepsilon$ -optimal arm after at most ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{384(8L_{\\mathrm{max}}+25d)}{{(\\varepsilon+\\Delta_{\\mathrm{min}})}^{2}}\\ln\\frac{768K C_{3}(8L_{\\mathrm{max}}+25d)}{{(\\varepsilon+\\Delta_{\\mathrm{min}})}^{2}\\,\\delta}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "time steps. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma $F.2$ .To begin with, let ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{0}=\\frac{768\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}}\\ln\\frac{768K C_{3}\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}\\,\\delta}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any $T\\geq T_{0}$ , let $\\bar{T}=\\left\\lceil T/2\\right\\rceil$ ", "page_idx": 23}, {"type": "text", "text": "$\\underline{{\\boldsymbol{\\mathrm{(I)}}}}$ If $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ terminates within $\\bar{T}$ time steps, there is nothing to prove. ", "page_idx": 23}, {"type": "text", "text": "$(\\mathbf{II})$ Assume $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ does not terminates within $\\bar{T}$ time steps. According to the analysis in the proof of Lemma F.1, if $\\mathbf{N}\\varepsilon\\mathbf{B}$ AI does not terminate before $T$ time steps, i.e., the stopping time $\\tau$ satisfies that $\\tau\\geq\\bar{T}$ , then event $\\bigcap_{t=\\bar{T}+1}^{T-1}\\tilde{E}_{t}$ does not hold. This indicates ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\tau\\geq T|\\tau\\geq\\bar{T}\\right]\\leq\\mathbb{P}\\left[\\bigcup_{t=\\bar{T}+1}^{T-1}\\tilde{E}_{t}^{c}\\right]\\leq\\sum_{t=\\bar{T}}^{T-1}\\mathbb{P}\\left[\\tilde{E}_{t}^{c}\\right]\\leq\\sum_{t=\\bar{T}}^{T-1}\\frac{\\delta}{C_{\\alpha}t^{\\alpha}}\\leq\\frac{\\delta}{C_{\\alpha}}\\int_{\\bar{T}}^{T}t^{-\\alpha}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\delta}{(\\alpha-1)C_{\\alpha}}\\left(\\frac{1}{(\\bar{T})^{\\alpha-1}}-\\frac{1}{T^{\\alpha}}\\right)\\overset{(a)}{\\leq}\\frac{\\delta}{(\\alpha-1)C_{\\alpha}(T/2)^{\\alpha-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(a) results from the fact that $\\bar{T}\\geq T/2$ ", "page_idx": 23}, {"type": "text", "text": "Altogether, for $T~\\geq~T_{0}$ , the probability $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ does not terminate before $T$ time steps is $\\overline{{\\frac{\\delta}{(\\alpha-1)C_{\\alpha}(T/2)^{\\alpha-1}}}}$ ", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition 3.1. First, Tonelli's Theorem implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]=\\mathbb{E}\\left[\\int_{0}^{\\tau}1\\;\\mathrm{d}x\\right]=\\mathbb{E}\\left[\\int_{0}^{+\\infty}\\mathbb{I}(\\tau>x)\\;\\mathrm{d}x\\right]=\\int_{0}^{+\\infty}\\mathbb{E}\\left[\\mathbb{I}(\\tau>x)\\right]\\;\\mathrm{d}x=\\int_{0}^{+\\infty}\\mathbb{P}(\\tau>x)\\;\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we apply Lemma F.2 to show ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\tau\\leq T_{0}+\\mathbb{E}[\\tau|\\tau\\geq T_{0}+1]\\cdot\\mathbb{P}[\\tau\\geq T_{0}+1]\\leq T_{0}+\\int_{T_{0}}^{+\\infty}\\mathbb{P}(\\tau\\geq x)\\;\\mathrm{d}x}\\\\ &{\\quad\\leq T_{0}+\\int_{T_{0}}^{+\\infty}\\frac{\\delta}{(\\alpha-1)C_{\\alpha}(x/2)^{\\alpha-1}}\\,\\mathrm{d}x=T_{0}+\\frac{\\delta}{(\\alpha-1)(\\alpha-2)(T_{0}/2)^{\\alpha-2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Besides, Lemma F.1 indicates that $\\scriptstyle\\mathrm{N}\\varepsilon\\mathbf{B}\\mathrm{AI}$ can identify an $\\varepsilon$ -optimal arm with probability $1-\\delta$ \uff1a\u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma $F.3$ For $x>0$ , let ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=x-a\\ln(b x^{c})=x-a\\ln b-a c\\ln x=x\\left(1-{\\frac{a\\ln b}{x}}-{\\frac{a c\\ln x}{x}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)\\geq0~\\Leftarrow~\\left\\{\\begin{array}{l l}{\\frac{a\\ln b}{x}\\leq\\frac{1}{2}}\\\\ {\\frac{a c\\ln x}{x}\\leq\\frac{1}{2}}\\end{array}\\right.~\\Leftrightarrow~\\left\\{\\begin{array}{l l}{x\\geq2a\\ln b}\\\\ {x\\geq2a c\\ln x}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $d=2a c$ $x=d\\ln(d y)$ , then ", "page_idx": 24}, {"type": "text", "text": "$x\\geq2a c\\ln x=d\\ln x\\iff d\\ln(d y)\\geq d\\ln(d\\ln(d y))\\iff\\ln(d y)\\geq\\ln(d\\ln(d y))\\iff y\\geq\\ln(d y)$ Since $z^{0.4}\\geq\\ln z$ for all $z\\geq1$ wehave $\\ln(d y)\\leq(d y)^{0.4}\\leq\\sqrt{d y}$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\ny\\geq\\ln(d y)\\;\\Leftarrow\\;y\\geq{\\sqrt{d y}}\\;\\Leftarrow\\;y\\geq d\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "when $y d\\geq1$ . Hence, when $a c\\ge1/2$ \uff0c $y\\geq d=2a c\\geq1$ , we have $y\\geq\\ln(d y)$ . Furthermore, for $x$ such that $x\\geq\\operatorname*{max}\\{2a\\ln b,\\,4a c\\ln(2a c)\\}$ , we have $f(x)\\geq0$ ", "page_idx": 24}, {"type": "text", "text": "GProof Outline of Theorem 3.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "A proof outline of Theorem 3.2 is provided in this section. It consists of three steps: ", "page_idx": 24}, {"type": "text", "text": "Step 1: $\\mathrm{PS}\\varepsilon$ BAI (Algorithm 1) depends on the success of change detection and context alignment (Algorithm 3 and 4). We firstly upper bound the failure probability of these two subroutines via Lemma G.1, Lemma G.2, Lemma G.3 and summarized in Lemma G.4. More details about these two subroutines are provided in Appendix D.2.2 and the proof of the Lemmas are postponed to Appendix H. ", "page_idx": 24}, {"type": "text", "text": "Step 2: Subsequently, conditioned on their success, we provide a theoretical guarantee for the choice of the confidence radius $\\rho_{t}(x,\\tilde{x})$ for $\\Delta(x,\\tilde{x})$ at time step $t$ in Lemma G.5. The proof is detailed in Appendix I. ", "page_idx": 24}, {"type": "text", "text": "Step 3: Lastly, utilizing the above elements, we provide a sufficient condition for the stopping rule, and upper bound the number of time steps in Exp phases $T_{\\tau}$ via Lemma G.6 whose proof is presented in Appendix J. As the total number of time steps $\\tau$ is upper bounded by a constant multiple of $T_{\\tau}$ the high-probability upper bound on the stopping time $\\tau$ is obtained. This finishes the proof of Theorem 3.2 (please refer to Appendix J). ", "page_idx": 24}, {"type": "text", "text": "Step 1: We borrow the terminology in hypothesis testing to define the errors. Let the null hypothesis be: the algorithm has undergone a changepoint within the last w CD samples. For Algorithm 3, we will characterize the type I error: the algorithm has experienced a changepoint within the last w $C D$ samples but it fails to raise a changing alarm, and the type II error: a changepoint has not occurred butAlgorithm $^3$ raises a changing alarm. We refer to the event Failed Alarm (FA) and the event False Alarm Error (FAE) as that a type I error occurs and that a type $\\mathrm{II}$ error occurs, respectively: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{FA}_{l}:=\\{\\boldsymbol{\\mathrm{a\\,type\\,I\\,error\\,occurs\\;at\\,changepoint}}\\;c_{l}\\},l\\in\\mathbb{N},}\\\\ &{\\mathrm{FAE}_{t}:=\\{\\boldsymbol{\\mathrm{a\\,type\\,II\\,error\\,occurs\\,time\\,\\,step\\,}}t\\},t\\in\\mathcal{T}_{\\mathrm{FAE}},}\\\\ &{\\mathrm{FA}:=\\bigcup\\quad\\mathrm{FA}_{l}\\quad\\mathrm{~and~}\\quad\\mathrm{FAE}:=\\bigcup_{t\\in\\mathcal{T}_{\\mathrm{FAE}}}\\mathrm{FAE}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\ T_{\\mathrm{FAE}}\\,:=\\,\\{t:t$ in CD phase, $t\\,\\leq\\,\\tau,[t-w\\gamma,t]\\cap\\mathcal{C}\\,=\\,\\emptyset\\}$ and $\\tau$ is the stopping time. In term of Algorithm 4, define the event $\\mathrm{MI}_{l}\\,:=\\,\\{\\mathrm{Algorithm}\\,4\\,\\}$ misidentifies $\\theta_{j_{t}}^{*}\\},\\ l\\in\\mathbb{N}$ and $\\mathrm{MI}:=$ $\\textstyle\\bigcup_{\\{l:c_{l}\\leq\\tau\\}}\\operatorname{MI}_{l}$ . Define the good event ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{Good}:=\\{\\operatorname{FA}^{c}\\cap\\operatorname{FAE}^{c}\\cap\\operatorname{MI}^{c}\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We upper bound the failure probability of Goodin Lemma G.1, Lemma G.2, Lemma G.3 and conclude the results in Lemma G.4. ", "page_idx": 24}, {"type": "text", "text": "LemmaG1. For any $\\delta_{\\mathrm{FAE}}\\in(0,1)$ $\\begin{array}{r}{b\\geq\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{24}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}}\\end{array}$ ,LCD makes no false alarm before the stopping time $\\tau$ with probability at least ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathrm{FAE}^{c}]\\ge1-\\frac{\\tau}{\\gamma}K\\delta_{\\mathrm{FAE}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma G.1 can also be stated as, fix $b$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[{\\mathrm{FAE}}^{c}\\right]\\geq1-\\frac{\\tau}{\\gamma}2K\\exp\\left(-\\operatorname*{min}\\left\\{\\frac{3w b}{20d},\\frac{w b^{2}}{150d}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which indicates we can always decrease the error probability by enlarging the window size $w$ ", "page_idx": 25}, {"type": "text", "text": "Assume $c_{l}$ is observable, i.,. $\\theta_{j_{c_{l}}}^{*}\\neq\\theta_{j_{c_{l+1}}}^{*}$ , and denote $\\hat{c}_{l}$ as the alarm time of $c_{l}$ from Algorithm 3. ", "page_idx": 25}, {"type": "text", "text": "Lemma G.2. Conditional on $\\mathrm{FAE^{c}}$ for any $\\begin{array}{r l r}{\\delta_{\\mathrm{FA}}}&{{}\\in}&{(0,1),~~i f~~\\frac{\\Delta_{c}-b}{2}~~\\geq~~\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}~+}\\end{array}$ $\\begin{array}{r}{\\sqrt{\\left(\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}\\right)^{2}+\\frac{4d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}},}\\end{array}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[c_{l}\\leq\\hat{c}_{l}\\leq c_{l}+\\frac{w\\gamma}{2}\\vert\\hat{c}_{l}\\geq c_{l}\\right]\\geq1-\\delta_{\\mathrm{FA}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In addition, $\\mathbb{P}\\left[{\\mathrm{FA}}{\\left|{\\mathrm{FAE}^{c}}\\right.}\\right]\\le l_{\\tau}\\delta_{\\mathrm{FA}}$ ", "page_idx": 25}, {"type": "text", "text": "Lemma G.2 guarantees a prompt change alarm will be raised within $\\frac{w\\gamma}{2}$ time steps after a changepoint occurs. This also explains why $\\frac{w\\gamma}{2}$ samples are abandoned at line 16fAlgorithm 1 s that $\\mathcal{T}_{t,j}$ only keeps samples from context $j$ ", "page_idx": 25}, {"type": "text", "text": "Lemma G.3. Conditional on $\\mathrm{FAE^{c}}$ and $\\mathrm{FA}^{c}$ , based on the conditions in Lemma G.1 and Lemma G.2, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathrm{MI}|\\mathrm{FAE}^{c},\\mathrm{FA}^{c}\\right]\\leq l_{\\tau}\\left[(N-1)\\delta_{\\mathrm{FA}}+K\\delta_{\\mathrm{FAE}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The intuition of Lemma G.3 is simple: change alarm should not be raised when the samples are from the same context and, change alarms should be raised when the samples are from the other $(N-1)$ contexts. The error made by Algorithm 3 and 4 can be concluded as follows ", "page_idx": 25}, {"type": "text", "text": "Lemma G.4. Assume the instance satisfies Assumption $^{\\,l}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{Good}\\right]\\ge1-\\frac{\\delta}{2\\tau^{*}}\\ge1-\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Step 2: To give an upper bound on $T_{\\tau}$ , we firstly prove that the estimate of $\\Delta(x,\\tilde{x})$ at time $t$ is within distance $\\rho_{t}(x,\\tilde{x})$ from the ground truth w.h.p. Define the event CI as the estimates of all the mean gaps locate in the high-probability Confidence Interval ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{CI}_{t}:=\\{\\left|\\hat{\\Delta}_{t}(x,\\tilde{x})-\\Delta(x,\\tilde{x})\\right|\\leq\\rho_{t}(x,\\tilde{x}),\\forall x,\\tilde{x}\\in\\mathcal{X}\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $\\operatorname{CI}:=\\bigcap_{t}\\operatorname{CI}_{t}$ , where $\\rho_{t}$ is defined in (3.3). ", "page_idx": 25}, {"type": "text", "text": "Lemma G.5.If $\\begin{array}{r}{T_{t}\\ge\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}},}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "In addition, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\mathrm{CI}_{t}\\middle|\\mathrm{Good}\\right]\\geq1-(K\\delta_{v,T_{t}}+N\\delta_{d,T_{t}}+K N\\delta_{m,T_{t}})\\,.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathbb{P}\\left[\\mathrm{CI}\\middle|\\mathrm{Good}\\right]\\geq1-\\frac{\\delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In addition to an estimation over the latent vector which is suffcient under the stationary case $\\ {\\alpha}_{t}$ in $\\rho_{t}$ ), we also need to control the derivation between the empirical distribution $\\hat{\\mathbf{p}}_{t}$ and the ground truth $\\mathbf{p}$ $\\beta_{t}$ in $\\rho_{t}\\dot{}$ ), as well as the interactions between the latent vectors and the distribution ( $\\langle\\xi_{t}$ in $\\rho_{t}\\dot{,}$ This is reflected by $K\\delta_{v,T_{t}},N\\delta_{d,T_{t}}$ and $K N\\delta_{m,T_{t}}$ which bound the the failure probability of Vector Estimation, Distribution Estimation and Residual Estimation, respectively. ", "page_idx": 25}, {"type": "text", "text": "Step 3: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma G.6. Conditional on Good and CI, the recommended arm $\\hat{x}_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ andwhenAlgorithm1 terminates,theorderof $T_{t}$ is upper bounded by (3.6). ", "page_idx": 25}, {"type": "text", "text": "The detailed proof is presented in Appendix J. ", "page_idx": 25}, {"type": "text", "text": "H Analysis of $\\mathrm{PS}\\varepsilon$ BAI: Change Detection and Context Alignment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As the output of Algorithms 3 and 4 depends on random samples, the output may not meet our expectation with some probabilities. We will characterize the probabilities of the three \u201cbad\" events: FAE in Appendix H.1, FA in Appendix H.2 and MI in Appendix H.3; and finally upper bound the failure probability of the good event Good in this section. ", "page_idx": 26}, {"type": "text", "text": "Lemma G.4. Assume the instance satisfies Assumption $^{\\,l}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{Good}\\right]\\ge1-\\frac{\\delta}{2\\tau^{*}}\\ge1-\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Some notations are introduced first ", "page_idx": 26}, {"type": "text", "text": "$\\theta_{1:\\infty}:=(\\theta_{j_{t}}^{*})_{t=1}^{\\infty}$ is the latent vector sequence.   \n$\\hat{c}_{l}$ indicates the time step when we raise alarm for the $l^{\\mathrm{th}}$ changepoint. ", "page_idx": 26}, {"type": "text", "text": "According to the dynamics (see Dynamics 1) of the problem, at a changepoint $c_{l}\\in\\mathcal{C}$ , the next latent vector will be sampled from $P_{\\theta}$ . Therefore, it may happen that $\\theta_{j_{c_{l}}}^{*}=\\theta_{j_{c_{l}-1}}^{*}$ O\\*el- i.e,this changepoint Ct is hidden and these two consecutive stationary segments are observed as one stationary segment. In order to upper bound the errors made by Algorithm 3, we make the following assumption, which yields the worst-case scenario in terms of the error probability. ", "page_idx": 26}, {"type": "text", "text": "Assumptio 2. Given $\\theta_{1:\\infty}$ and hechangepint sequence C, $\\theta_{j_{c_{l}}}^{*}\\neq\\theta_{j_{c_{l+1}}}^{*},\\forall l\\in\\mathbb{N}.$ ", "page_idx": 26}, {"type": "text", "text": "In this section, we do not consider the randomness of $\\theta_{1:\\infty}$ , i.e., we consider a realization of the sequence.4 The results do not involve any parameter depending on the specific sequence and thus can be applied to any latent vector sequence. ", "page_idx": 26}, {"type": "text", "text": "H.1  False Alarm Error ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma G.1. For any $\\delta_{\\mathrm{FAE}}\\in(0,1)$ wih $\\begin{array}{r}{b\\geq\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{24}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}.}\\end{array}$ LCD makes no false alarm before the stopping time $\\tau$ with probability at least ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathrm{FAE}^{c}]\\ge1-\\frac{\\tau}{\\gamma}K\\delta_{\\mathrm{FAE}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma G.1. Note that within a stationary segment of length $t$ , the observations are i.i.d. and thus the time segments between two consecutive false alarm time $\\{\\hat{c}_{l}-\\hat{c}_{l-1}\\}_{l=1}^{t}$ (where $\\hat{c}_{0}=0$ are i.i.d. i.e., it is a renewal process. Thus, it is sufficient to bound the error probability under the stationary case, i.e. ${\\mathcal{C}}=\\varnothing$ and $\\theta_{1:\\infty}=(\\theta_{j_{t}}^{*}=\\theta_{j_{1}}^{*})_{t=1}^{\\infty}$ ", "page_idx": 26}, {"type": "text", "text": "Assume that we are under a stationary segment of length $t$ .We wish to upper bound the error $\\mathbb{P}[\\mathrm{FAE}]=\\mathbb{P}[\\hat{c}_{1}\\leq t]$ . This is given by Lemma H.1. ", "page_idx": 26}, {"type": "text", "text": "Lemma H.1. Given w $C D$ samples from the same context $[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{w}$ in a $C D$ phase at time step $t_{:}$ .Algorithm 3 makes a false alarm error with probability upper bounded by $K\\delta_{\\mathrm{FAE}}$ i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathrm{FAE}_{t}\\right]\\leq K\\delta_{\\mathrm{FAE}}}\\\\ {i f b\\geq\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{24}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Assume $\\gamma$ divides $t$ . In the case of a stationary segment, the input samples to Algorithm 3 are $\\{[(x_{s\\gamma},Y_{s\\gamma,x_{s\\gamma}})]_{s=k}^{k+w-1}\\}_{k=1}^{\\frac{t}{\\gamma}}$ ichaning alamisraised WeioretitalizationatL in Algorithm 1 for simplicity). Denote $\\Gamma_{i}(t):=\\{k\\gamma:k\\equiv i$ mod $w,k\\gamma\\leq t\\},i=0,1,\\dots,w-1$ Given any $i=0,\\ldots,w-1$ forany $k_{1}\\neq k_{2}\\in\\Gamma_{i}(t)$ thetwo lis of samples, $[(x_{s\\gamma},Y_{s\\gamma,x_{s\\gamma}})]_{s=k_{1}}^{k_{1}+w-1}$ $[(x_{s\\gamma},Y_{s\\gamma,x_{s\\gamma}})]_{s=k_{2}}^{k_{2}+w-1}$ , do not overlap (thus, they are independent). Therefore, $\\mathbb{P}[k]:=\\mathbb{P}[\\hat{c}_{1}=$ $k\\gamma+i,\\hat{c}_{1}\\in\\Gamma_{i}(t)]$ is a geometric distribution with parameter upper bounded by $K\\delta_{F A E}$ Wehave ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\hat{c}_{1}\\in\\Gamma_{i}(t)\\right]\\leq1-\\left(1-K\\delta_{\\mathrm{FAE}}\\right)^{|\\Gamma_{i}(t)|}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, the cumulative false alarm error is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\mathrm{FAE}]=\\mathbb{P}\\left[\\hat{c}_{1}\\leq t\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=0}^{w-1}\\left[1-(1-K\\delta_{\\mathrm{FAE}})^{|\\Gamma_{i}(t)|}\\right]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{i=0}^{w-1}|\\Gamma_{i}(t)|K\\delta_{\\mathrm{FAE}}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{t}{\\gamma}K\\delta_{\\mathrm{FAE}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma $H.l$ .According to Algorithm 3, given $w$ CD samples from the same context $[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{w}$ and $x\\in\\mathscr{X}$ , we need to bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Big|\\sum_{s=1}^{\\frac{w}{2}}\\boldsymbol{x}^{\\top}A(\\lambda^{*})^{-1}\\tilde{\\boldsymbol{x}}_{s}Y_{s,\\tilde{x}_{s}}-\\sum_{s=\\frac{w}{2}+1}^{w}\\boldsymbol{x}^{\\top}A(\\lambda^{*})^{-1}\\tilde{\\boldsymbol{x}}_{s}Y_{s,\\tilde{x}_{s}}\\Big|\\geq\\frac{w}{2}b\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proof is similar to Lemma E.1. For any $s\\in\\left[{\\frac{w}{2}}\\right]$ and $\\tilde{s}=s+\\frac{w}{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}-x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\tilde{s}}Y_{\\tilde{s},\\tilde{x}_{\\tilde{s}}}\\right]=0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\bar{x}_{s}}-x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\bar{s}}Y_{\\bar{s},\\bar{x}_{\\bar{s}}}|}\\\\ &{\\leq\\left|x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}\\tilde{x}_{s}^{\\top}\\theta_{\\bar{j}s}^{*}\\right|+\\left|x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}\\eta_{s}\\right|+\\left|x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\bar{s}}\\tilde{x}_{\\bar{s}}^{\\top}\\theta_{\\bar{j}s}^{*}\\right|+\\left|x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\bar{s}}\\eta_{\\bar{s}}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By making use of (E.4), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}-x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\tilde{s}}Y_{\\tilde{s},\\tilde{x}_{\\tilde{s}}}\\right)^{2}\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}\\right)^{2}\\right]+\\mathbb{E}\\left[\\left(x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\tilde{s}}Y_{\\tilde{s},\\tilde{x}_{\\tilde{s}}}\\right)^{2}\\right]}\\\\ &{\\quad\\quad\\ -\\ 2\\mathbb{E}\\left[x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}\\cdot x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{\\tilde{s}}Y_{\\tilde{s},\\tilde{x}_{\\tilde{s}}}\\right]}\\\\ &{\\ \\le4d+2\\mathbb{E}\\left[|x^{\\top}\\theta_{j_{s}}^{*}\\cdot x^{\\top}\\theta_{j_{\\tilde{s}}}^{*}|\\right]}\\\\ &{\\le6d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "According to the Bernstein's inequality ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\left[\\Big|\\displaystyle\\sum_{s=1}^{\\frac{\\overline{{s}}}{2}}\\Big(x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}-x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s+\\frac{w}{2}}Y_{s+\\frac{w}{2},\\tilde{x}_{s+\\frac{w}{2}}}\\Big)\\Big|\\geq\\frac{w}{2}\\epsilon\\right]}\\\\ &{\\leq2\\exp\\left(-\\frac{\\frac{1}{2}\\left(\\frac{w}{2}\\epsilon\\right)^{2}}{\\frac{w}{2}\\cdot6d+\\frac{4d}{3}\\frac{w}{2}\\epsilon}\\right)}\\\\ &{=2\\exp\\left(-\\frac{\\frac{w}{2}\\epsilon^{2}}{12d+\\frac{8d}{3}\\epsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In order to upper bound the error probability by $\\delta_{\\mathrm{FAE}}$ , we need ", "page_idx": 27}, {"type": "equation", "text": "$$\n2\\exp\\left(-\\frac{\\frac{w}{2}\\epsilon^{2}}{12d+\\frac{8d}{3}\\epsilon}\\right)\\le\\delta_{F A E}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Rightarrow\\;\\;\\epsilon\\geq\\frac{4d}{3\\cdot\\frac{w}{2}}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{4d}{3\\cdot\\frac{w}{2}}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+12\\cdot\\frac{2}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the choiceof $\\begin{array}{r}{b\\geq\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{24}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}}\\end{array}$ ,we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Big|\\frac{2}{w}\\sum_{s=1}^{\\frac{w}{2}}x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}-\\frac{2}{w}\\sum_{s=\\frac{w}{2}+1}^{w}x^{\\top}A(\\lambda^{*})^{-1}\\tilde{x}_{s}Y_{s,\\tilde{x}_{s}}\\Big|\\geq b\\right]\\leq\\delta_{\\mathrm{FAE}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "A union bound over the $K$ arms yields the final result. ", "page_idx": 28}, {"type": "text", "text": "H.2 Failed Alarm ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma G.2. Conditional on $\\mathrm{FAE^{c}}$ for any $\\begin{array}{r l r}{\\delta_{\\mathrm{FA}}}&{{}\\in}&{(0,1),}\\end{array}$ $\\begin{array}{r}{\\sqrt{\\left(\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}\\right)^{2}+\\frac{4d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}},}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[c_{l}\\leq\\hat{c}_{l}\\leq c_{l}+\\frac{w\\gamma}{2}\\big|\\hat{c}_{l}\\geq c_{l}\\right]\\geq1-\\delta_{\\mathrm{FA}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In addition, $\\mathbb{P}\\left[{\\mathrm{FA}}|{\\mathrm{FAE}^{c}}\\right]\\le l_{\\tau}\\delta_{\\mathrm{FA}}$ ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma G.2. Conditioned on $\\mathrm{FAE^{_{c}}}$ , the detection at the changepoints is independent. Thus we can assume there is only one changepoint $c_{1}$ within a certain number of consecutive time steps. ", "page_idx": 28}, {"type": "text", "text": "Algorithm3isgiven $w$ CD samples which are collected under two different context. Without loss of generality, we assume the sample selected at time $c_{1}$ is among the CD samples (otherwise, we can regard the time step of the first sample from the second context as $c_{1}$ ).Wewishtobound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[c_{1}\\leq\\hat{c}_{1}\\leq c_{1}+\\frac{\\gamma w}{2}\\middle|\\hat{c}_{1}\\geq c_{1}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is the probability of the event that, after a changepoint occurs, a changing alarm will be raised within $\\frac{w\\gamma}{2}$ time steps (or $\\frac w2$ CD samples). Here, $\\hat{c}_{1}\\geq c_{1}$ can be guaranteed as we are conditioning on that there is no false alarm error $\\boldsymbol{(\\mathrm{FAE}^{c\\cdot}}$ ", "page_idx": 28}, {"type": "text", "text": "The event $\\begin{array}{r}{c_{1}\\leq\\hat{c}_{1}\\leq c_{1}+\\frac{\\gamma w}{2}}\\end{array}$ indicates, at least one of the CD sample list in ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\left[(x_{c_{1}+(s-k)\\gamma},Y_{c_{1}+(s-k)\\gamma,x_{c_{1}+(s-k)\\gamma}})\\right]_{s=1}^{w}\\right\\}_{k=\\frac{w}{2}+1}^{w}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "will trigger the changing alarm in Algorithm 3. In particular, in Lemma H.2, we consider the failed arm probability when the CD samples are composed by exactly half samples from each contexts, $\\begin{array}{r}{\\Big[\\underset{\\big(x_{c_{1}+(s-1-\\frac{w}{2})\\gamma},Y_{c_{1}+(s-1-\\frac{w}{2})\\gamma,x_{c_{1}+(s-1-\\frac{w}{2})\\gamma}}\\big)}{\\Big\\therefore}\\Big]_{s=1}^{\\mathbf{\\hat{\\omega}}}.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma H.2. Given w $C D$ samples from the two different contexts $[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=1}^{w}$ in a CD phase, where $[(\\Tilde{x}_{s},Y_{s,\\Tilde{x}_{s}})]_{s=1}^{\\frac{w}{2}}$ is from latent vector $\\theta_{j}^{*}$ and $[(\\tilde{x}_{s},Y_{s,\\tilde{x}_{s}})]_{s=\\frac{w}{2}+1}^{w}$ is from latent vector $\\theta_{\\widetilde{j}}^{*}$ \uff0c Algorithm 3 raises a changing alarm with probability lower bounded by ", "page_idx": 28}, {"type": "text", "text": "$1-\\delta_{\\mathrm{FA}}$ $\\begin{array}{r}{i f\\frac{\\Delta_{c}-b}{2}\\geq\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}+\\sqrt{\\left(\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}\\right)^{2}+\\frac{4d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}}\\;w h e r e\\;\\Delta_{c}:=\\operatorname*{max}_{x\\in\\mathcal{X}}|x^{\\top}(\\theta_{j}^{*}-\\theta_{j}^{*})|\\;a^{*}\\leq b,}\\end{array}$ nd it is assumedtobegreaterthan $b$ ", "page_idx": 28}, {"type": "text", "text": "By making use of Lemma H.2, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[c_{1}\\leq\\hat{c}_{1}\\leq c_{1}+\\frac{w\\gamma}{2}\\big|\\hat{c}_{1}\\geq c_{1}\\right]}\\\\ &{\\quad=\\mathbb{P}\\left[\\exists k\\in\\{\\frac{w}{2}+1,\\dots,w\\},\\right.}\\\\ &{\\qquad\\quad\\left.\\mathrm{LCD}\\left(w,b,[(x_{c_{1}+(s-k)\\gamma},Y_{c_{1}+(s-k)\\gamma,x_{c_{1}+(s-k)\\gamma}})\\big|_{s=1}^{w}\\right)=\\mathrm{True}\\big|\\hat{c}_{1}\\geq c_{1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\geq\\mathbb{P}\\left[\\mathrm{LCD}\\left(w,b,[(x_{c_{1}+(s-1-\\frac{w}{2})\\gamma},Y_{c_{1}+(s-1-\\frac{w}{2})\\gamma,x_{c_{1}+(s-1-\\frac{w}{2})\\gamma}})]_{s=1}^{w}\\right)=\\mathrm{True}\\big|\\hat{c}_{1}\\geq c_{1}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{c}=\\operatorname*{max}_{x\\in\\mathcal{X}}|x^{\\top}(\\theta_{j}^{*}-\\theta_{\\tilde{j}}^{*})|}\\end{array}$ and $\\theta_{j}^{*},\\theta_{\\tilde{j}}^{*}$ are the latent vectors. Hence, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{FA}\\middle|\\mathrm{FAE}^{c}\\right]=1-\\mathbb{P}\\left[\\mathrm{FA}^{c}\\middle|\\mathrm{FAE}^{c}\\right]\\leq1-(1-\\delta_{\\mathrm{FA}})^{l_{\\tau}}\\leq l_{\\tau}\\delta_{\\mathrm{FA}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $H.2$ .According to the design of LCD, there exists an $x\\in\\mathscr{X}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Bigg[\\Bigg|\\frac{\\displaystyle\\frac{u}{\\displaystyle x}\\sum_{x=1}^{\\infty}x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{x}Y_{s,s}\\_{x}\\_{x}-\\frac{2}{w}\\underbrace{\\displaystyle\\sum_{s=1}^{w}x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{s}Y_{s,s}}_{\\displaystyle\\leq a^{\\top}\\mathbb{A}}\\Bigg|\\geq b\\Bigg]}\\\\ &{\\geq1-\\mathbb{P}\\Bigg[\\Bigg|\\frac{\\displaystyle\\frac{u}{\\displaystyle x}\\sum_{x=1}^{\\infty}x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{x}Y_{s,s}\\_{x}\\ \\_{x}\\underbrace{\\displaystyle\\sum_{s=1}^{w}x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{s}Y_{s,s}\\_{x}\\_{-\\frac{w}{2}}\\mathbb{r}(\\theta_{j}^{\\star}-\\theta_{j}^{\\star})}_{\\displaystyle\\geq b\\Bigg|\\ }}\\\\ &{\\qquad\\qquad\\geq\\left|\\frac{w}{\\displaystyle2}b-\\frac{w}{\\displaystyle2}x^{\\top}(\\theta_{j}^{\\star}-\\theta_{j}^{\\star})\\right|\\Bigg]}\\\\ &{=1-\\mathbb{P}\\Bigg[\\Bigg|\\frac{\\displaystyle\\frac{u}{\\displaystyle x}\\sum_{i=1}^{\\infty}\\left(x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{s}Y_{s,s}\\_{x}-x^{\\top}\\theta_{j}^{\\star}\\right)}{\\displaystyle\\sum_{s=1}^{w}\\left(x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{s}Y_{s,s}\\_{x}\\_{x}-x^{\\top}\\theta_{j}^{\\star}\\right)}\\underbrace{\\displaystyle\\sum_{s=\\frac{w}{2}+1}^{w}\\left(x^{\\top}A({x}^{\\star})^{-1}\\bar{x}_{s}Y_{s,s}\\_{x}-x^{\\top}\\theta_{j}^{\\star}\\right)}_{\\displaystyle\\geq b\\Bigg|\\mathrm{vart}\\Bigg\\}}\\\\ &{\\qquad\\qquad\\geq\\left|\\frac{w}{\\displaystyle2}b-\\frac{w}{\\displaystyle2}x^{\\top}(\\theta_{j}^{\\star}-\\theta_{j}^{\\star})\\right|\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where th last ineqult hls as $\\begin{array}{r}{\\frac{\\Delta_{c}-b}{2}\\geq\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}+\\sqrt{\\left(\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}\\right)^{2}+\\frac{4d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "H.3 Context Alignment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma G.3. Conditional on $\\mathrm{FAE^{c}}$ and $\\mathrm{FA}^{c}$ , based on the conditions in Lemma G.1 and Lemma G.2, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathrm{MI}|\\mathrm{FAE}^{c},\\mathrm{FA}^{c}\\right]\\leq l_{\\tau}\\left[(N-1)\\delta_{\\mathrm{FA}}+K\\delta_{\\mathrm{FAE}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma G.3. The error of the context alignment procedure can be derived from the FAE and FA analyses. Conditioned on $\\mathrm{FAE^{_{c}}}$ \uff0c $\\mathrm{FA}^{c}$ and that the previous $l-1$ contexts are correctly identified, i.e., $\\cap_{k=1}^{l-1}\\mathrm{MI}_{k}^{c}$ , we have the following statements. ", "page_idx": 29}, {"type": "text", "text": "Firstly, according to Lemma H.1, the change alarm at Line 4 in Algorithm 4 will not be triggered with probability at least $(1-K\\delta_{\\mathrm{FAE}})$ if the current context is context $j$ . This error has been taken into account in the FAE (see Remark H.3). ", "page_idx": 29}, {"type": "text", "text": "Secondly, if the current context is not $j$ (which will occur at most $N-1$ times), a change alarm will be raised with probability at least $1-\\delta_{\\mathrm{FA}}$ by Lemma G.2. ", "page_idx": 29}, {"type": "text", "text": "Therefore, the error probability at $c_{l}$ is upper bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{MI}_{l}|\\mathrm{FAE}^{c},\\mathrm{FA}^{c},\\cap_{k=1}^{l-1}\\mathrm{MI}_{k}^{c}\\right]\\leq(N-1)\\delta_{\\mathrm{FA}}+K\\delta_{\\mathrm{FAE}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and by a union bound, the cumulative error probability bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\mathrm{MI}|\\mathrm{FAE}^{c},\\mathrm{FA}^{c}\\right]=1-\\mathbb{P}\\left[\\mathrm{MI}^{c}|\\mathrm{FAE}^{c},\\mathrm{FA}^{c}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq1-(1-(N-1)\\delta_{\\mathrm{FA}}-K\\delta_{\\mathrm{FAE}})^{l_{\\tau}}}\\\\ &{\\qquad\\qquad\\qquad\\leq l_{\\tau}\\left((N-1)\\delta_{\\mathrm{FA}}+K\\delta_{\\mathrm{FAE}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Remark H.3.Note that $(I)$ wewill only bound theFAEwhen the $C D$ samplesarefromthesame context; (2) in the analysis of FAE, we bound the error probability on the whole horizon up to time $t$ as we assume there is no changepoint. In particular, there will be unused and redundant $\\scriptstyle{\\frac{w}{2}}K\\delta_{\\mathrm{FAE}}$ errors budget for FAE at each changepoint,which accumulates to $l_{\\tau}\\frac{w}{2}K\\delta_{\\mathrm{FAE}}$ before time step $\\tau$ Therefore, the second error term in (H.1), $l_{\\tau}K\\delta_{\\mathrm{FAE}}$ can be covered by the unused $l_{\\tau}\\frac{w}{2}K\\delta_{\\mathrm{FAE}}$ error budget from FAE, and thus it can be neglected in (H.1). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma G.4. According to Lemma G.1, G.2, G.3, Assumption 1 and Remark H.3, given a time $\\tau$ , the total failure probability is upper bounded by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\operatorname{Good}^{c}\\right]=\\mathbb{P}\\left[\\operatorname{FAE}\\cup\\operatorname{FA}\\cup\\operatorname{MI}\\right]}\\\\ &{\\phantom{\\mathbb{P}\\bigg[}\\leq\\frac{\\tau}{\\gamma}K\\delta_{\\mathrm{FAE}}+l_{\\tau}\\delta_{\\mathrm{FA}}+l_{\\tau}\\cdot(N-1)\\delta_{\\mathrm{FA}}}\\\\ &{\\phantom{\\mathbb{P}\\bigg[}=\\frac{\\tau}{\\gamma}K\\delta_{\\mathrm{FAE}}+l_{\\tau}N\\delta_{\\mathrm{FA}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, when $\\tau$ is upper bounded by $\\tau^{*}$ in Line 2 of Algorithm 1 ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tau\\leq\\tau^{*}=c_{0}\\frac{N L_{\\operatorname*{max}}}{\\varepsilon^{2}}\\ln\\frac{N^{2}K L_{\\operatorname*{max}}/\\varepsilon^{2}}{\\delta}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Iwhere $c_{0}=3\\cdot6400\\ln6400$ The num ber f changepoits ii $\\tau$ is uper ounded y $\\begin{array}{r}{l_{\\tau^{*}}\\leq\\frac{\\tau^{*}}{L_{\\operatorname*{min}}}}\\end{array}$ By Assumption 1 when $\\begin{array}{r}{b\\,=\\,\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{8d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{24}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}}\\end{array}$ S2AB and OFAE = 4(r\\*)2K , the conditions of Lemma G.1 are met. And we can upper bound $\\scriptstyle{\\frac{\\tau}{\\gamma}}K\\delta_{\\mathrm{FAE}}$ by $\\begin{array}{r}{\\frac{\\delta}{4\\tau^{*}}\\leq\\frac{\\delta}{4}}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "By making use of Assumption 1 and seting $\\begin{array}{r}{\\delta_{\\mathrm{FA}}=\\frac{\\delta}{4N l_{\\tau^{*}}\\tau^{*}}}\\end{array}$ we have $\\delta_{\\mathrm{FA}}>\\delta_{\\mathrm{FAE}}$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Delta_{c}-b}{2}\\geq\\frac{b}{2}\\geq\\frac{4d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}+\\sqrt{\\left(\\frac{4d}{3w}\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}\\right)^{2}+\\frac{6}{w}d\\ln\\frac{2}{\\delta_{\\mathrm{FAE}}}}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}+\\sqrt{\\left(\\frac{d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}\\right)^{2}+\\frac{4d}{w}\\ln\\frac{2}{\\delta_{\\mathrm{FA}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, Lemma G.2 can be applied. $l_{\\tau^{*}}N\\delta_{\\mathrm{FA}}$ can be upper bounded by $\\begin{array}{r}{\\frac{\\delta}{4\\tau^{*}}\\leq\\frac{\\delta}{4}}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "Therefore, according to (H.2), $\\begin{array}{r}{\\mathbb{P}[\\mathrm{Good}]\\ge1-\\frac{\\delta}{2\\tau^{*}}\\ge1-\\frac{\\delta}{2}.}\\end{array}$ ", "page_idx": 30}, {"type": "text", "text": "1 Analysis of $\\mathrm{PS}\\varepsilon$ BAI: Estimation Error ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma G.5 is proved in this section. We will upper bound these three error terms (see (I.1)): VE error in App I.1, DE error in App I.2, RE error in App I.3, and finally we will prove Lemma G.5 which upper bounds the failure probability of CI at the end of this section. ", "page_idx": 30}, {"type": "text", "text": "Lemma G.5. If $\\begin{array}{r}{T_{t}\\ge\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}},}\\end{array}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{CI}_{t}\\big|\\mathrm{Good}\\right]\\ge1-(K\\delta_{v,T_{t}}+N\\delta_{d,T_{t}}+K N\\delta_{m,T_{t}})\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In addition, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathrm{CI}\\vert\\mathrm{Good}\\right]\\ge1-\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Given any twoarms $x,\\tilde{x}\\in\\mathcal{X}$ , by the triangular inequality, the deviation between $\\hat{\\Delta}_{t}(x,\\tilde{x})$ and $\\Delta(x,\\tilde{x})$ can be upper bounded by three terms: the Vector-Estimation Error (VE) term, the DistributionEstimation Error (DE) term, and the Residual Estimation Error (RE) term: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{\\Delta}_{t}(x,\\tilde{x})-\\Delta(x,\\tilde{x})|}\\\\ &{=|(x-\\tilde{x})^{\\top}\\hat{\\Theta}_{t}\\hat{\\mathbf{p}}_{t}-(x-\\tilde{x})^{\\top}\\Theta\\mathbf{p}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underbrace{\\Big|\\displaystyle\\sum_{j=1}^{N}\\Big(\\hat{\\Delta}_{t,j}(x,\\tilde{x})-\\Delta_{j}(x,\\tilde{x})\\Big)\\,\\hat{p}_{t,j}\\Big|}_{\\mathrm{VE~term}}+\\underbrace{\\Big|\\displaystyle\\sum_{j=1}^{N}\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})(\\hat{p}_{t,j}-p_{j})\\Big|}_{\\mathrm{DE~term}}}\\\\ &{\\quad\\quad+\\underbrace{\\Big|\\displaystyle\\sum_{j=1}^{N}\\Big(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\Big)\\,(\\hat{p}_{t,j}-p_{j})\\Big|}_{\\mathrm{RE~term}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $a^{\\mathrm{clip}_{2}}:=\\mathrm{clip}_{2}(a):=\\operatorname*{min}\\{\\operatorname*{max}\\{a,-2\\},2\\}$ is a shorthand notation for the value of $a$ that is clipped to the interval $[-2,2]$ . The reason the value $\\hat{\\Delta}_{t,j}(x,\\tilde{x})$ is clipped is the ground truth $|\\Delta_{t,j}(\\bar{x},\\tilde{x})|\\leq2$ ", "page_idx": 31}, {"type": "text", "text": "Recall the event CI ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{CI}_{t}:=\\{\\left|\\hat{\\Delta}_{t}(x,\\tilde{x})-\\Delta(x,\\tilde{x})\\right|\\leq\\rho_{t}(x,\\tilde{x}),\\forall x,\\tilde{x}\\in\\mathcal{X}\\},}\\\\ &{\\mathrm{CI}:=\\displaystyle\\bigcap_{t}\\mathrm{CI}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and the confidence radius ", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho_{t}(x,\\tilde{x}):=2(\\alpha_{t}+\\xi_{t})+\\sum_{j=1}^{N}\\beta_{t,j}\\big|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})\\big|,}\\\\ {\\displaystyle\\alpha_{t}:=5\\sqrt{\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}},\\quad\\beta_{t,j}:=\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}},}\\\\ {\\displaystyle\\xi_{t}:=25\\sqrt{2}\\frac{N L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}},\\quad\\phi_{t,j}:=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\frac{1}{4}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and $\\zeta_{t}(x,\\tilde{x})\\quad\\in\\ \\ \\mathbb{R}$ can be any value.  In particular, it can be the value that minimizes $\\begin{array}{r}{\\sum_{j=1}^{N}\\beta_{t,j}|\\hat{\\Delta}_{t,j}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})|}\\end{array}$ or it can be taken as $\\varepsilon$ For simplicity, we will take $\\zeta_{t}(x,\\tilde{x})\\;=$ $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathrm{s}\\operatorname*{min}_{\\zeta_{t}(x,\\tilde{x})\\in\\mathbb{R}}\\sum_{j=1}^{N}\\beta_{t,j}(\\hat{\\Delta}_{t,j}(x,\\tilde{x})\\!+\\!\\zeta_{t}(x,\\tilde{x}))^{2}=-\\frac{\\sum_{j=1}^{N}\\beta_{t,j}\\hat{\\Delta}_{t,j}(x,\\tilde{x})}{\\sum_{j=1}^{N}\\beta_{t,j}}}\\end{array}$ Bt,(z,g) for asimle and effecive analytic expression in the algorithm. ", "page_idx": 31}, {"type": "text", "text": "1.1Vector-Estimation Error ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For the VE term $\\begin{array}{r}{\\bigg|\\sum_{j=1}^{N}\\left(\\hat{\\Delta}_{t,j}(x,\\tilde{x})-\\Delta_{j}(x,\\tilde{x})\\right)\\hat{p}_{t,j}\\bigg|;}\\end{array}$ note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Big|\\sum_{j=1}^{N}\\left(\\hat{\\Delta}_{t,j}(x,\\tilde{x})-\\Delta_{j}(x,\\tilde{x})\\right)\\hat{p}_{t,j}\\Big|\\leq|x^{\\top}(\\hat{\\Theta}_{t}-{\\Theta})\\hat{p}_{t}|+|\\tilde{x}^{\\top}(\\hat{\\Theta}_{t}-{\\Theta})\\hat{p}_{t}|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma I.1. Given $x\\in\\mathscr{X}$ and $\\begin{array}{r}{T_{t}\\ge\\frac{d}{4}\\ln\\frac{2}{\\delta_{v,T_{t}}}}\\end{array}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\lvert x^{\\top}(\\hat{\\Theta}_{t}-\\Theta)\\hat{\\mathbf{p}}_{t}\\rvert\\ge\\alpha_{t}\\big\\lvert\\mathrm{Good}\\right]\\le\\delta_{v,T_{t}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $I.I$ .By the definition of the estimators in (3.2), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x^{\\top}(\\hat{\\Theta}_{t}-\\Theta)\\hat{\\mathbf{p}}_{t}=\\sum_{j=1}^{N}x^{\\top}(\\hat{\\theta}_{t,j}-\\theta_{j}^{*})\\hat{\\mathbf{p}}_{t,j}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\sum_{j=1}^{N}x^{\\top}\\left(\\frac{1}{T_{t,j}}\\displaystyle\\sum_{s\\in\\mathcal{T}_{t,j}}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-\\theta_{j}^{*}\\right)\\frac{T_{t,j}}{T_{t}}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\frac{1}{T_{t}}\\sum_{j=1}^{N}\\sum_{s\\in\\mathcal{T}_{t,j}}x^{\\top}A(\\lambda^{*})^{-1}x_{s}Y_{s,x_{s}}-x^{\\top}\\theta_{j}^{*}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma E.1, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|x^{\\top}(\\hat{\\Theta}_{t}-\\Theta)\\hat{\\mathbf{p}}_{t}|\\ge\\alpha_{t}\\big|\\theta_{1:\\infty},\\mathrm{Good}\\right]\\le\\delta_{v,T_{t}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the property of conditional probability, we have the desired result. ", "page_idx": 32}, {"type": "text", "text": "Therefore, conditional on Good, with probability at least $1-K\\delta_{v,T_{t}}$ \uff0c $\\mathrm{VE}\\le2\\alpha_{t}$ for any $x,\\tilde{x}\\in\\mathcal{X}$ ", "page_idx": 32}, {"type": "text", "text": "1.2 Distribution-Estimation Error ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma I.2. Given Good and $\\begin{array}{r}{T_{t}\\geq\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ forany $j\\in[N]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\hat{p}_{t,j}-p_{j}|\\ge\\beta_{t,j}\\big|\\mathrm{Good}\\right]\\le\\delta_{d,T_{t}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Aditionall, with probability $1-N\\delta_{d,T_{t}}$ \uff0c ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Big|\\sum_{j=1}^{N}\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\big(\\hat{p}_{t,j}-p_{j}\\big)\\Big|\\leq\\sum_{j=1}^{N}\\beta_{t,j}|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})|\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\zeta_{t}(x,\\tilde{x})$ can be any value. ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma I.2. Given any $j\\in[N]$ , and the stationary segment $l$ , we denote $X_{j,l}:=\\hat{L}_{l}\\mathbb{1}\\{\\theta_{j_{l}}^{*}=$ $\\theta_{j}^{*}\\}-p_{j}\\hat{L}_{l}$ , where $\\hat{L}_{l}$ is the total length of the Exp phases in the th stationary segment. Note that $\\mathbb{E}[X_{j,l}|\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\operatorname{Good}]=0,|X_{j,l}|\\leq L_{\\operatorname*{max}}\\;a.s.$ and $\\mathrm{Var}[X_{j,l}|\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\mathrm{Good}]=p_{j}(1-p_{j})\\hat{L}_{l}^{2}\\leq$ $p_{j}(1-p_{j})\\hat{L}_{l}L_{\\mathrm{max}}$ . By Bernstein's inequality, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[|\\displaystyle\\sum_{l=1}^{l_{t}}X_{j,l}|\\geq\\epsilon|\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\mathrm{Good}\\right]\\leq2\\exp\\left(-\\frac{\\epsilon^{2}}{2\\sum_{l=1}^{l_{t}}\\mathrm{Var}[X_{j,l}|\\mathrm{Good}]+\\frac{2}{3}L_{\\operatorname*{max}}\\epsilon}\\right)}\\\\ &{\\mathbb{P}\\left[|\\displaystyle\\frac{1}{T_{t}}\\sum_{l=1}^{l_{t}}X_{j,l}|\\geq\\epsilon|\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\mathrm{Good}\\right]\\leq2\\exp\\left(-\\frac{T_{t}^{2}\\epsilon^{2}}{2\\sum_{l=1}^{l_{t}}\\mathrm{Var}[X_{j,l}|\\mathrm{Good}]+\\frac{2}{3}L_{\\operatorname*{max}}T_{t}\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[\\vert\\hat{p}_{t,j}-p_{j}\\vert\\ge\\epsilon\\big\\vert\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\mathrm{Good}\\big]\\le2\\exp\\left(-\\frac{T_{t}^{2}\\epsilon^{2}}{2\\sum_{l=1}^{l_{t}}\\mathrm{Var}\\big[X_{j,l}\\vert\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}},\\mathrm{Good}\\big]+\\frac{2}{3}L_{\\operatorname*{max}}T_{t}\\epsilon\\right)}\\right.}&{}\\\\ &{\\quad\\left.\\le2\\exp\\left(-\\frac{T_{t}^{2}\\epsilon^{2}}{2\\sum_{l=1}^{l_{t}}p_{j}(1-p_{j})\\hat{L}_{l}L_{\\operatorname*{max}}+\\frac{2}{3}L_{\\operatorname*{max}}T_{t}\\epsilon}\\right)\\right.}&{}\\\\ &{\\quad\\le2\\exp\\left(-\\frac{T_{t}\\epsilon^{2}}{2p_{j}(1-p_{j})L_{\\operatorname*{max}}+\\frac{2}{3}L_{\\operatorname*{max}}\\epsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As the last bound is independent of $\\{\\hat{L}_{l}\\}_{l=1}^{l_{t}}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\lvert\\hat{p}_{t,j}-p_{j}\\rvert\\ge\\epsilon\\big\\rvert\\mathrm{Good}\\right]\\le2\\exp\\left(-\\frac{T_{t}\\epsilon^{2}}{2p_{j}(1-p_{j})L_{\\mathrm{max}}+\\frac{2}{3}L_{\\mathrm{max}}\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If we want to upper bound the above by $\\delta_{d,T_{t}}\\in(0,1)$ ,i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\n2\\exp\\left(-\\frac{T_{t}\\epsilon^{2}}{2p_{j}(1-p_{j})L_{\\mathrm{max}}+\\frac{2}{3}L_{\\mathrm{max}}\\epsilon}\\right)\\leq\\delta_{d,T_{t}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we require ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\epsilon\\geq\\frac{\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}+\\sqrt{\\left(\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right)^{2}+2T_{t}p_{j}(1-p_{j})L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}{T_{t}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\epsilon=\\tilde{\\beta}_{t,j}:=\\frac{5}{2}\\operatorname*{max}\\left\\{\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "satisfies the condition, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\hat{p}_{t,j}-p_{j}|\\ge\\tilde{\\beta}_{t,j}\\big|\\mathrm{Good}\\right]\\le\\delta_{d,T_{t}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "A problem here is we do not have access to $p_{j}$ during the dynamics, therefore, we adopt Lemma K.1 to further upper bound $\\tilde{\\beta}_{t,j}$ by $\\beta_{t,j}$ ", "page_idx": 33}, {"type": "text", "text": "Lemma K.1. Given any $j\\in[N]$ and $\\begin{array}{r}{T_{t}\\ge\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\,\\tilde{\\beta}_{t,j}\\le\\beta_{t,j}.}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Therefore, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\hat{p}_{t,j}-p_{j}|\\ge\\beta_{t,j}\\big|\\mathrm{Good}\\right]\\le\\delta_{d,T_{t}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In addition, with probability at least $1-N\\delta_{d,T_{t}}$ , we can upper bound DE term as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Big|\\sum_{j=1}^{N}\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})(\\hat{p}_{t,j}-p_{j})\\Big|=\\Big|\\sum_{j=1}^{N}\\left(\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})\\right)(\\hat{p}_{t,j}-p_{j})\\Big|}}\\\\ &{}&{\\leq\\displaystyle\\sum_{j=1}^{N}\\beta_{t,j}\\big|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x})\\big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\zeta_{t}(x,\\tilde{x})\\in\\mathbb{R}$ can be any value. ", "page_idx": 33}, {"type": "text", "text": "1.3  Residual-Estimation Error ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "RE is composed by the produt of two dvations i.e. $\\left(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\right)$ and $(\\hat{p}_{t,j}-p_{j})$ as the time step $t$ becomes large, we expect it will converge to zero fast. Thus, it is sufficient to have a coarse estimation of it. ", "page_idx": 33}, {"type": "text", "text": "Lemma I.3. For any $x\\in\\mathscr{X}$ conditional on that $|\\hat{p}_{t,j}-p_{j}|\\le\\beta_{t,j},\\forall j\\in[N],$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Big|\\sum_{j=1}^{N}\\Big(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\Big)\\left(\\hat{p}_{t,j}-p_{j}\\right)\\Big|\\geq\\xi_{t}\\Big|\\mathrm{Good}\\right]\\leq N\\delta_{m,T_{t}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{\\xi_{t}:=25\\sqrt{2}\\frac{N L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma I.3. According to Lemma E.1, for any $x\\in\\mathcal{X},j\\in[N]$ wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|x^{\\top}(\\theta_{j}^{*}-\\hat{\\theta}_{t,j})|\\ge5\\sqrt{\\frac{d}{T_{t,j}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}|\\mathrm{Good}\\right]\\le\\delta_{m,T_{t}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r}{T_{t,j}\\ge\\frac{d}{4}\\ln\\frac{2}{\\delta_{m,T_{t}}}}\\end{array}$ . Note the fact that $\\Delta_{j}(x,\\tilde{x})\\in[-2,2]$ , hence, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\right|\\leq\\left|\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}(x,\\tilde{x})\\right|\\leq|x^{\\top}(\\theta_{j}^{*}-\\hat{\\theta}_{t,j})|+|\\tilde{x}^{\\top}(\\theta_{j}^{*}-\\hat{\\theta}_{t,j})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "And $|x^{\\top}(\\theta_{j}^{*}-\\hat{\\theta}_{t,j})|\\leq3d$ with probability 1. Denote ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\psi_{t,j,1}:=\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{d}{4T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}\\right\\},\\ \\psi_{t,j,2}:=\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We have5 ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big|\\displaystyle\\sum_{j=1}^{N}\\Big(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\Big)\\,(\\hat{p}_{t,j}-p_{j})\\Big|}\\\\ &{\\leq\\displaystyle\\sum_{j:\\psi_{t,j,1}>\\hat{p}_{t,j}}\\Big|\\left(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\right)\\beta_{t,j}\\Big|+\\displaystyle\\sum_{j:\\psi_{t,j,1}=\\hat{p}_{t,j}}\\Big|\\left(\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j,2}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\right)\\beta_{t,j}\\Big|}\\\\ &{\\quad\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\psi_{t,j,2}\\hat{\\textmd{s f r}}_{\\perp,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\underset{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}{\\sum_{\\substack{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}}\\right.}}}}\\left[\\left(\\Delta_{j}\\big(x,j\\big)-\\Delta_{j}^{\\mathrm{a}\\mathrm{a}\\mathrm{a}\\mathrm{a}\\left(x,z\\right)}\\right)\\right]\\Delta_{j_{\\left/{\\right.}}}\\right]}\\\\ &{\\stackrel{\\mathrm{(a)}}{\\leq}\\underset{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}{\\sum_{\\substack{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}}\\right.}}}}\\Bigg[\\left(\\Delta_{j}\\big(x,j\\big)-\\Delta_{j}^{\\mathrm{a}\\mathrm{a}\\mathrm{a}\\left(x,z\\right)}\\right)\\Delta_{j_{\\left/{\\right.}}}\\Bigg]+\\underset{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}{\\sum_{\\substack{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}}\\right.}}}}\\Bigg[\\left(\\Delta_{j}\\big(x,j\\big)-\\Delta_{j,\\left(2,\\left(1,\\right)\\right)}^{\\mathrm{a}\\mathrm{a}\\mathrm{b}\\left(j,\\right)}\\right)\\Delta_{j_{\\left/{\\right.}}}\\Bigg]}\\\\ &{\\quad+\\underset{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}{\\sum_{\\substack{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}}\\right.}}}}\\mathrm{L}^{\\mathrm{a}\\mathrm{a}\\left(x,j\\right)}\\Delta_{j_{\\left/{\\vphantom{{m a x}},k_{\\left/{\\operatorname*{max}}}\\right.}}}\\Bigg]+\\mathrm{1}\\Bigg[\\mathrm{v}_{j}^{\\mathrm{a}}-\\dot{\\delta}_{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}\\Bigg]}\\\\ &{\\stackrel{\\mathrm{(b)}}{\\leq}\\underset{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}\\right.}}}{\\sum_{\\substack{j_{\\left/{\\operatorname*{max}},k_{\\left/{\\operatorname*{max}}}\\right.}}}}\\frac{5}{2}\\cdot\\frac{5}{2}\\mathbb{L}_{n}^{2}\\ln_\n$$$$\n\\begin{array}{r l}&{\\quad_{\\mathbb{P}}\\int_{\\mathbb{R}^{n}}\\sum_{i=0}^{I}|x^{\\top}(\\theta^{\\star}-\\theta_{i})|_{\\mathbb{A}_{u}^{t}}+|y^{\\top}(\\theta^{\\star}-\\theta_{i})|_{\\mathbb{A}_{u}^{t}}}\\\\ &{\\stackrel{(a)\\sim}\\int_{\\mathbb{R}^{n+1}\\times\\mathbb{R}^{n}}\\sum_{i=0}^{I}\\frac{1}{2}\\sum_{t=1}^{n}\\log\\frac{1}{A_{u}}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\operatorname*{min}\\left\\{4:z^{\\top}\\sqrt{\\frac{d}{d t}}\\frac{1}{A_{u}}\\frac{2}{A_{u}}\\right\\},\\ \\frac{5}{2}\\cdot\\frac{5}{2^{n}}\\frac{C_{2}\\operatorname*{Lim}}{I_{1}}\\sin\\frac{2}{A_{u}}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\sum_{t=1}^{n}\\sqrt{\\frac{d}{d t}}\\int_{\\mathbb{R}^{n+1}}^{2}\\frac{1}{A_{u}}\\frac{2}{A_{u}}\\frac{2}{A_{u}}\\prod_{s=0}^{T}\\prod_{s=1}^{T}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\sum_{t=1}^{n}\\sqrt{\\frac{d}{d t}}\\int_{\\mathbb{R}^{n+1}}^{2}\\frac{2}{A_{u}}\\frac{2}{A_{u}}\\frac{2}{A_{u}}}\\\\ &{\\stackrel{(b)\\sim}\\int_{\\mathbb{R}^{n+1}}^{\\sum}\\sum_{i=0}^{I\\setminus j}\\frac{2\\sqrt{\\frac{d}{d t}}}{I_{1}}\\sin\\frac{2}{A_{u}}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\sum_{t=1}^{n}\\tan\\left\\{4,[\\frac{d}{d t}]_{\\mathbb{A}_{u}^{t}}\\frac{2}{A_{u}}\\right\\},\\ \\frac{2\\sqrt{2}\\sum_{t=1}^{n}\\sum_{t=1}^{2}\\dots\\sum_{t=1}^{2}}{I_{1}}\\frac{2}{A_{u}}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\sum_{t=1}^{n}\\sqrt{\\frac{d}{d t}}\\frac{2\\sqrt{\\frac{d}{d t}}}{I_{1} \n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(a)$ adopts (I1.7) and $(b)$ is obtained by the following derivations: ", "page_idx": 34}, {"type": "text", "text": "(1) $\\psi_{t,j,1}~~>~~\\hat{p}_{t,j}$ indicates $\\begin{array}{r c l}{\\hat{p}_{t,j}}&{<}&{\\frac{d}{4T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}&{<\\;\\;\\frac{25}{4}\\frac{L_{\\mathrm{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ and thus $\\beta_{t,j}\\leq\\ \\ \\frac{5}{2}$ $\\frac{5\\sqrt{2}}{2}\\,\\frac{L_{\\mathrm{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}$ .In addition, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\Delta_{j}(x,\\tilde{x})-\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x,\\tilde{x})\\right|\\leq4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\psi_{t,j,1}=\\hat{p}_{t,j}$ $\\begin{array}{r}{\\hat{p}_{t,j}\\,\\leq\\,\\frac{d}{4T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}\\end{array}$ $\\psi_{t,j,2}>\\hat{p}_{t,j}$ $\\begin{array}{r}{\\hat{p}_{t,j}<\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ 2,thus (I.6) can be aplied and \u03b2t, \u2264  \u00b7 2 T\\* ", "page_idx": 34}, {"type": "text", "text": "(3) $\\psi_{t,j,2}\\,=\\,\\hat{p}_{t,j}$ indices $\\begin{array}{r}{\\hat{p}_{t,j}\\;\\geq\\;\\frac{25}{16}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\;\\geq\\;\\frac{d}{4T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}\\end{array}$ $\\begin{array}{r}{\\beta_{t,j}\\le\\frac{5}{2}\\sqrt{\\frac{8\\hat{p}_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta}}}\\end{array}$ Therefore, conditional on Good, with probability at least $1-K N\\delta_{m,T_{t}}$ \uff0c $\\mathrm{RE}\\le2\\xi_{t}$ ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma G.5. By Lemma 1.1, 1.2 and 1.3, conditional on Good, with probability at least $1-\\left(K\\delta_{v,T_{t}}+N\\delta_{d,T_{t}}+K N\\delta_{m,T_{t}}\\right)$ $\\left|\\hat{\\Delta}_{t}(x,\\tilde{x})-\\Delta(x,\\tilde{x})\\right|\\le\\rho_{t}(x,\\tilde{x})$ . This finishes the proof of the first result in Lemma G.5. ", "page_idx": 35}, {"type": "text", "text": "We then accumulates all the error probabilities. By the choices of $\\begin{array}{r l r}{\\delta_{v,T_{t}}\\!}&{{}=}&{\\!\\frac{\\delta}{15K T_{t}^{3}},\\delta_{d,T_{t}}\\!}&{=}\\end{array}$ $\\begin{array}{r}{\\frac{\\delta}{15N T_{t}^{3}},\\delta_{m,T_{t}}=\\frac{\\delta}{15K N T_{t}^{3}},}\\end{array}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{T_{t}=1}^{\\infty}{K\\delta_{v,T_{t}}}+N{\\delta_{d,T_{t}}}+K N{\\delta_{m,T_{t}}}=\\displaystyle\\sum_{T_{t}=1}^{\\infty}{K\\cdot\\frac{\\delta}{15K T_{t}^{3}}}+N\\cdot\\frac{\\delta}{15N T_{t}^{3}}+K N\\cdot\\frac{\\delta}{15K N T_{t}^{3}}}\\\\ {\\displaystyle=\\sum_{T_{t}=1}^{\\infty}\\frac{3\\delta}{15T_{t}^{3}}}\\\\ {\\displaystyle\\leq\\frac{\\delta}{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that there is an reversion step at Line 18 Algorithm 1, for each $T\\in\\mathbb N$ , there are at most two time steps $t_{1}<t_{2}$ With $T_{t_{1}}=T_{t_{2}}$ .Therefore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\infty}K\\delta_{v,T_{t}}+N\\delta_{d,T_{t}}+K N\\delta_{m,T_{t}}\\leq2\\sum_{T_{t}=1}^{\\infty}K\\delta_{v,T_{t}}+N\\delta_{d,T_{t}}+K N\\delta_{m,T_{t}}\\leq{\\frac{\\delta}{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This proves the second statement. ", "page_idx": 35}, {"type": "text", "text": "J   Upper Bound of $\\mathrm{PS}\\varepsilon$ BAI: Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Theorem 3.2 is proved in this section by three steps. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Firstly, we show that the recommended arm $\\hat{x}_{\\varepsilon}$ is an $\\varepsilon$ -best arm upon the termination of the algorithm in Lemma J.1.   \n\u00b7 Secondly, we present a suffcient condition for the termination of the algorithm in terms of $T_{t}$ in Lemma G.6.   \n\u00b7 Lastly, we show that $T_{\\tau}$ is bounded by a constant fraction of $\\tau$ and thus, obtain an upper bound on T. ", "page_idx": 35}, {"type": "text", "text": "Step 1: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "According to Lemma G.5, $\\mathrm{CI}_{t}$ holds with probability $1\\!-\\!\\left(K\\delta_{v,T_{t}}\\!+\\!N\\delta_{d,T_{t}}\\!+\\!K N\\delta_{m,T_{t}}\\right)$ . Conditional on the good event Good in (G.1) and the event $\\mathrm{CI}_{t}$ (the mean gaps are well approximated at time step $t^{\\th}$ 0, we expect the recommended arm will be an $\\varepsilon$ -optimal arm. This is formalized in the following lemma. ", "page_idx": 35}, {"type": "text", "text": "Lemma J.1. Conditional on Good and $\\mathrm{CI}_{t}$ , if the algorithm stops at time step $t$ ,therecommended arm $\\hat{x}_{\\varepsilon}=x_{t}^{*}$ isan $\\varepsilon$ -best arm. ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma J.1. If $\\hat{x}_{\\varepsilon}=x^{\\ast}$ , the lemma holds. ", "page_idx": 35}, {"type": "text", "text": "If $\\hat{x}_{\\varepsilon}\\neq x^{*}$ , according to the termination condition (3.4) ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x:x\\neq\\hat{x}_{\\varepsilon}}\\hat{\\Delta}_{t}(\\hat{x}_{\\varepsilon},x)-\\rho_{t}(\\hat{x}_{\\varepsilon},x)>-\\varepsilon\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Delta(\\hat{x}_{\\varepsilon},x^{*})\\geq\\hat{\\Delta}_{t}(\\hat{x}_{\\varepsilon},x^{*})-\\rho_{t}(\\hat{x}_{\\varepsilon},x^{*})\\geq\\operatorname*{min}_{x:x\\neq\\hat{x}_{\\varepsilon}}\\hat{\\Delta}_{t}(\\hat{x}_{\\varepsilon},x)-\\rho_{t}(\\hat{x}_{\\varepsilon},x)>-\\varepsilon\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality holds due to $\\mathrm{CI}_{t}$ . This indicates $\\Delta(x^{*},\\hat{x}_{\\varepsilon})\\leq\\varepsilon$ and thus the recommended arm $\\hat{x}_{\\varepsilon}=x_{t}^{*}$ is an $\\varepsilon$ -best arm. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Step 2: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma G.6. Conditional on Good and CI, the recommended arm $\\hat{x}_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ and whenAlgorithm1 terminates, the order of $T_{t}$ is upper bounded by (3.6). ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma G.6. By Lemma J.1, $\\hat{x}_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ . Note that for any $x\\neq\\hat{x}_{\\varepsilon}$ ,when ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{2\\rho_{t}(x,\\hat{x}_{\\varepsilon})\\leq\\Delta(x,\\hat{x}_{\\varepsilon})+\\varepsilon,\\quad\\hat{x}_{\\varepsilon}\\neq x^{*}\\mathrm{~and~}x=x^{*},\\right.}\\\\ {\\left.\\rho_{t}(\\hat{x}_{\\varepsilon},x)+\\rho_{t}(x^{*},x)\\leq\\Delta(x^{*},x)+\\varepsilon,\\quad\\mathrm{otherwise}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By utilizing $\\mathrm{CI}_{t}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\!\\!\\!\\Delta_{t}(\\hat{x}_{\\varepsilon},x^{*})-\\rho_{t}(\\hat{x}_{\\varepsilon},x^{*})\\geq\\hat{\\Delta}_{t}(x^{*},\\hat{x}_{\\varepsilon})-\\rho_{t}(\\hat{x}_{\\varepsilon},x^{*})\\geq\\Delta_{t}(x^{*},\\hat{x}_{\\varepsilon})-2\\rho_{t}(x^{*},\\hat{x}_{\\varepsilon})\\geq-\\varepsilon,\\quad}\\\\ {\\hat{x}_{\\varepsilon}\\neq x^{*}\\mathrm{~and~}x=x^{*},\\quad}\\\\ {\\hat{\\Delta}_{t}(\\hat{x}_{\\varepsilon},x)-\\rho_{t}(\\hat{x}_{\\varepsilon},x)\\geq\\hat{\\Delta}_{t}(x^{*},x)-\\rho_{t}(\\hat{x}_{\\varepsilon},x)\\geq\\Delta(x^{*},x)-\\rho_{t}(x^{*},x)-\\rho_{t}(\\hat{x}_{\\varepsilon},x)\\geq-\\varepsilon}\\\\ {\\mathrm{~}\\quad\\mathrm{~otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, if (J.1) hold for all $x\\neq\\hat{x}_{\\varepsilon}$ , the algorithm must have stopped, i.e., it is sufficient to have the following for any $x\\neq\\hat{x}_{\\varepsilon}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\rho_{t}(x^{*},\\hat{x}_{\\varepsilon})\\leq\\frac{\\Delta(x^{*},\\hat{x}_{\\varepsilon})+\\varepsilon}{2},\\quad\\hat{x}_{\\varepsilon}\\neq x^{*}\\mathrm{~and~}x=x^{*},}\\\\ {\\displaystyle\\rho_{t}(\\hat{x}_{\\varepsilon},x)\\leq\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}\\,,\\displaystyle\\rho_{t}(x^{*},x)\\leq\\frac{\\Delta(x^{*},x)+\\varepsilon}{2},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma J.2 gives an upper bound on the total number of time steps in Exp phases at which (J.1) must hold for any two arms $x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon},x\\in\\mathcal{X}\\setminus\\{x_{\\varepsilon}\\}$ ", "page_idx": 36}, {"type": "text", "text": "Lemma J.2. For any two arms $(x_{\\varepsilon},x)$ where $x\\neq x_{\\varepsilon},x^{*}$ ,when ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{t}=\\frac{6400\\ln6400\\cdot d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{K d/\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}{\\delta}}\\\\ &{\\phantom{T}\\qquad\\qquad+6400\\ln6400\\cdot\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{N\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)}{\\delta}}\\\\ &{\\phantom{T}\\qquad\\qquad+\\frac{3200\\sqrt{2}\\ln3200\\sqrt{2}\\cdot N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{\\frac{K N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}}{\\delta}}\\\\ &{\\phantom{T}\\qquad\\qquad=\\tilde{O}\\left(\\frac{d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{1}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x):=\\frac{L_{\\mathrm{max}}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\left(\\sum_{j=1}^{N}\\sqrt{\\operatorname*{min}\\left\\{16p_{j},\\frac{1}{4}\\right\\}}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\right)^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "wemusthave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho_{t}(x_{\\varepsilon},x)\\leq\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By taking the maximum of (J.2) over all $\\varepsilon$ -best arms and the suboptimal arms, we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{T}:=\\underset{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon,x}\\not=x_{\\varepsilon},x^{*}}{\\operatorname*{max}}\\frac{6400\\ln6400\\cdot d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{K d/\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}{\\delta}}\\\\ &{\\qquad\\qquad+6400\\ln6400\\cdot\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{N\\mathrm{H}_{\\mathrm{DE}}\\left(x_{\\varepsilon},x\\right)}{\\delta}}\\\\ &{\\qquad\\qquad+\\frac{3200\\sqrt{2}\\ln3200\\sqrt{2}\\cdot N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{\\frac{K N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}}{\\delta}}\\\\ &{\\qquad=\\tilde{O}\\left(\\underset{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon,x}\\not=x_{\\varepsilon},x^{*}}{\\operatorname*{max}}\\frac{d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma J.2. Let $c_{v},c_{d},c_{m}\\,\\in\\,(0,1)$ be constants, satisfying $c_{v}+c_{d}+2c_{m}\\,=\\,1$ . By the definition of $\\rho_{t}(x_{\\varepsilon},x)$ in (3.3), it is sufficient to have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle2\\alpha_{t}\\leq c_{v}\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}}\\\\ {\\displaystyle\\sum_{j=1}^{N}\\beta_{t,j}|\\hat{\\Delta}_{t,j}^{\\mathrm{clip}_{2}}(x_{\\varepsilon},x)+\\zeta_{t}(x_{\\varepsilon},x)|\\leq(c_{d}+c_{m})\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}}\\\\ {\\displaystyle2\\xi_{t}\\leq c_{m}\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where  we  take $\\begin{array}{r l r}{\\zeta_{t}(x_{\\varepsilon},x)}&{{}=}&{\\varepsilon}\\end{array}$ for theoretical simplicity, but we adopt $\\begin{array}{r l}{\\zeta_{t}(x,\\tilde{x})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\zeta_{t}(x,\\tilde{x})\\in\\mathbb{R}}\\sum_{j=1}^{N}\\beta_{t,j}(\\hat{\\Delta}_{t,j}(x,\\tilde{x})+\\zeta_{t}(x,\\tilde{x}))^{2}=-\\frac{\\sum_{j=1}^{N}\\beta_{t,j}\\hat{\\Delta}_{t,j}(x,\\tilde{x})}{\\sum_{j=1}^{N}\\beta_{t,j}}}\\end{array}$ Bt,\u25b3t,5(e,3) in the algorithm, which still enjoys the theoretical guarantee. ", "page_idx": 37}, {"type": "text", "text": "VE Term According to the definition of $\\alpha_{t}$ , we need to bound: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle}&{\\displaystyle\\alpha_{t}\\leq c_{v}\\frac{\\Delta(x^{*},x)+\\varepsilon}{4}}\\\\ {\\displaystyle\\Leftrightarrow}&{5\\sqrt{\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}}\\leq c_{v}\\frac{\\Delta(x^{*},x)+\\varepsilon}{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By a coarse estimation, it is sufficient to have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau_{t}\\geq\\frac{\\bar{c}_{v}d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{K d/\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}{\\delta}=O\\left(\\frac{d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{K d/\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{c}_{v}=\\frac{6400}{c_{v}^{2}}\\ln\\frac{6400}{c_{v}^{2}}}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "DE Term The difficulty lies at the DE term. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{N}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)+\\zeta(\\alpha_{t},x)\\big\\rvert}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{N}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)+\\varepsilon\\big\\rvert}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{N}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)+\\varepsilon\\big\\rvert}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{N}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)+\\varepsilon\\big\\rvert}\\\\ &{\\quad+\\displaystyle\\sum_{j=1}^{N}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{H}_{2}}(x_{t},x)+\\varepsilon\\big\\rvert+\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)-\\Delta_{j}(x_{t},x)\\big\\rvert}\\\\ &{\\le\\displaystyle\\sum_{j=1,2\\nu,j=1,3,4,2}\\beta_{i,j}(2+\\varepsilon)+\\displaystyle\\sum_{j=1,\\nu,j=1,6}\\beta_{i,j}\\big\\lvert\\hat{A}_{t,j}^{\\mathrm{HH}_{2}}(x_{t},x)-\\Delta_{j}(x_{t},x)\\big\\rvert}\\\\ &{\\quad+\\displaystyle\\sum_{j=1,\\nu,j=1,\\nu,j=1,5}\\beta_{i,j}\\big\\lvert\\Delta_{t,j}(x_{t},x)+\\varepsilon\\big\\rvert}\\\\ &{\\le2\\varepsilon_{i,j}+\\displaystyle\\sum_{j=1,\\nu,j=1,3,4,j}\\beta_{j,j}\\big\\lvert\\Delta_{j}(x_{t},x)+\\varepsilon\\big\\rvert}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We willupper bound $2\\xi_{t}$ by (\\*,\u00b1a) frst; the second term will be included in the RE term and analyzed later. Therefore, it is sufficient to have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{j:\\psi_{t,j}=\\hat{p}_{t,j}}\\beta_{t,j}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\leq c_{d}\\frac{\\Delta(x^{*},x)+\\varepsilon}{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By the definition of $\\beta_{t,j}$ in (I.2) and Lemma K.3, we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\beta_{t,j}=\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\leq\\frac{5}{2}\\sqrt{\\frac{2\\operatorname*{min}\\left\\{16p_{j},\\frac{1}{4}\\right\\}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma K.3. If $\\psi_{t,j}=\\hat{p}_{t,j}$ then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\phi_{t,j}\\leq\\operatorname*{min}\\left\\lbrace16p_{j},\\frac{1}{4}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, in order for (J.4) to hold, it is sufficient to have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j:\\psi_{t,j}=\\bar{p}_{t,j}}{\\sum}\\frac{\\frac{5}{2}\\sqrt{\\frac{2\\operatorname*{min}\\big\\{16p_{j},\\frac{1}{4}\\big\\}L_{\\operatorname*{max}}}{2}}\\ln\\frac{2}{\\bar{\\delta}_{d,T_{t}}}\\big|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\leq c_{d}\\frac{\\Delta\\big(x^{*},x\\big)+\\varepsilon}{2}}\\\\ {\\Leftrightarrow}&{\\frac{5}{2}\\sqrt{\\frac{2L_{\\operatorname*{max}}}{T_{t}}}\\frac{2}{\\bar{\\delta}_{d,T_{t}}}\\underset{j:\\psi_{t,j}=\\bar{p}_{t,j}}{\\sum}\\sqrt{\\operatorname*{min}\\bigg\\{16p_{j},\\frac{1}{4}\\bigg\\}}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\leq c_{d}\\frac{\\Delta\\big(x^{*},x\\big)+\\varepsilon}{2}}\\\\ {\\Leftarrow}&{\\frac{5}{2}\\sqrt{\\frac{2L_{\\operatorname*{max}}}{T_{t}}}\\ln\\frac{2}{\\bar{\\delta}_{d,T_{t}}}\\underset{j=1}{\\sum}\\sqrt{\\operatorname*{min}\\bigg\\{16p_{j},\\frac{1}{4}\\bigg\\}}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\leq c_{d}\\frac{\\Delta\\big(x^{*},x\\big)+\\varepsilon}{2}}\\\\ {\\Leftarrow}&{T_{t}\\geq\\bar{c}_{d}\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{N\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)}{\\delta}=O\\left(\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{N\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{c}_{d}=\\frac{100}{c_{d}^{2}}\\ln\\frac{100}{c_{d}^{2}}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "RE Term By the definition of $\\xi_{t}$ in (I.2), it is sufficient to have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{25\\sqrt{2}\\frac{N L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}\\leq c_{m}\\frac{\\Delta\\left(x^{*},x\\right)+\\varepsilon}{2}}\\\\ {\\Leftarrow}&{T_{t}\\geq\\frac{\\bar{c}_{m}\\cdot N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln\\frac{\\frac{K N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}}{\\delta}=O\\left(\\frac{N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln\\frac{\\frac{K N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{c}_{m}=\\frac{400\\sqrt{2}}{{c}_{m}}\\ln\\frac{400\\sqrt{2}}{{c}_{m}}}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "By taking $\\begin{array}{r}{c_{v}=\\frac{3}{4},c_{d}=\\frac{1}{8}}\\end{array}$ and $\\begin{array}{r}{c_{m}={\\frac{1}{8}}}\\end{array}$ , the upper bound on $T_{t}$ can be concluded as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{t}=\\frac{6400\\ln\\left(6400-\\mathcal{A}\\right)}{\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}\\ln\\frac{K d\\left/\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}{\\delta}+6400\\ln6400\\cdot\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{K\\mathrm{H}_{\\mathrm{DE}}\\left(x_{\\varepsilon},x\\right)}{\\delta}}\\\\ &{\\qquad\\qquad+\\frac{3200\\sqrt{2}\\ln3200\\sqrt{2}\\cdot N\\mathrm{L}_{\\mathrm{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln\\frac{K\\mathrm{N}_{\\mathrm{L}}}{\\delta}}\\\\ &{\\qquad=O\\left(\\frac{d}{\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}\\ln\\frac{K d/\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}{\\delta}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{N\\mathrm{H}_{\\mathrm{DE}}\\left(x_{\\varepsilon},x\\right)}{\\delta}+\\frac{N L_{\\mathrm{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln\\frac{\\frac{K N L_{\\mathrm{max}}}{\\delta\\left(x^{*},x\\right)+\\varepsilon}}{\\delta}\\right)}\\\\ &{\\qquad=\\bar{O}\\left(\\frac{d}{\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{1}{\\delta}+\\frac{N L_{\\mathrm{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step 3: ", "page_idx": 38}, {"type": "text", "text": "Theorem 3.2. Define the context distribution estimation $(D E)$ hardnessparameter ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x):=\\frac{L_{\\mathrm{max}}}{\\left(\\Delta(x^{\\ast},x)+\\varepsilon\\right)^{2}}\\bar{H}(x_{\\varepsilon},x)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{H}(x_{\\varepsilon},x):=\\left(\\sum_{j=1}^{N}\\sqrt{\\operatorname*{min}\\{16p_{j},1/4\\}}|\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon|\\right)^{2}}\\end{array}$ UunderAssmpion $^{l}$ wih prohability at least $1-\\delta$ \uff0c $\\mathrm{PS}\\varepsilon$ BAI identifies an $\\varepsilon$ -optimal arm and its sample complexity is ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tilde{O}\\Bigg(\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\frac{d}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\underbrace{\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln\\frac{1}{\\delta}}_{T_{\\mathrm{D}}(x_{\\varepsilon},x)}+\\underbrace{\\frac{N L_{\\operatorname*{max}}}{\\Delta(x^{*},x)+\\varepsilon}\\ln\\frac{1}{\\delta}}_{T_{\\mathrm{R}}(x)}\\Bigg).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Theorem 3.2. By Assumption 1, and the choice of the parameters, Lemma G.4 indicates Good is guaranteed with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ ", "page_idx": 39}, {"type": "text", "text": "Note that some arm pulls are not counted in $T_{t}$ ", "page_idx": 39}, {"type": "text", "text": "\u00b7 In every $\\gamma$ time steps, Algorithm 1 enters the CD phase to select a CD sample.   \n\u00b7Whenachangepoint s detcted, lgorithm 1 sts into th CA phase, h $\\frac w2$ arms are sampled in Algorithm 4.   \n\u00b7 During the CA phase, we abandon $\\frac{w\\gamma}{2}$ samples. ", "page_idx": 39}, {"type": "text", "text": "Therefore, when $t$ is large (or after the first stationary segment) ", "page_idx": 39}, {"type": "equation", "text": "$$\nt\\leq T_{t}\\cdot\\frac{\\gamma}{\\gamma-1}\\frac{L_{\\operatorname*{min}}}{L_{\\operatorname*{min}}-w\\gamma-\\frac{w}{2}}\\leq T_{t}\\cdot\\frac{\\gamma}{\\gamma-1}\\frac{L_{\\operatorname*{min}}}{L_{\\operatorname*{min}}-\\frac{3w\\gamma}{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\gamma\\geq2$ $w\\gamma$ $\\frac{L_{\\mathrm{min}}}{3}$ $\\gamma=2$ $\\begin{array}{r}{w=\\frac{L_{\\mathrm{min}}}{6}}\\end{array}$ $t$ order as $T_{t}$ . By Lemma G.6, (3.6) holds with probability $1-\\delta$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "K  Analysis of $\\mathrm{PS}\\varepsilon$ BAI: Technical Lemmas ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma K.1. Given any $j\\in[N]$ and $\\begin{array}{r}{T_{t}\\ge\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\,\\tilde{\\beta}_{t,j}\\le\\beta_{t,j}.}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma K.1. Recall ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\phi_{t,j}=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\frac{1}{4}\\right\\}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The proof is scheduled in 2 steps. ", "page_idx": 39}, {"type": "text", "text": "Step 1: Upper bound $\\begin{array}{r}{\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\end{array}$   \nWhen $\\begin{array}{r}{\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\end{array}$ ,wehave $\\tilde{\\beta}_{t,j}=\\frac{5}{2}\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}$   \nAs $\\begin{array}{r}{p_{j}(1-p_{j})\\le\\frac{1}{4},\\forall p_{j}\\in(0,1)}\\end{array}$ this naive bound gives $\\begin{array}{r}{\\tilde{\\beta}_{t,j}\\leq\\frac{5}{2}\\sqrt{\\frac{2\\cdot\\frac{1}{4}\\cdot L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "f $T_{t,j}>0$ , according to Lemma K.2 ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\beta}_{t,j}=\\frac{5}{2}\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\\\ &{\\quad\\quad\\leq\\frac{5}{2}\\sqrt{\\frac{2p_{j}T_{t,j}L_{\\operatorname*{max}}}{T_{t}T_{t,j}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\\\ &{\\quad\\quad\\leq\\frac{5}{2}\\sqrt{\\frac{2\\hat{p}_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\cdot4\\operatorname*{max}\\left\\{1,\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t,j}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\}\\ln\\frac{2}{\\delta_{d,T_{t}}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "thus we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\tilde{\\beta}_{t,j}\\leq\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If $T_{t,j}=0$ , i.e., the latent context $j$ has never been observed and $\\hat{p}_{t,j}=0$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\beta}_{t,j}=\\frac{5}{2}\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\\\ &{\\quad\\le\\frac{5}{2}\\sqrt{\\frac{2\\tilde{\\beta}_{t,j}(1-\\tilde{\\beta}_{t,j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\\\ &{\\Rightarrow\\quad\\tilde{\\beta}_{j}(t,\\delta)\\le\\frac{\\frac{25}{8}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}{T_{t}+\\frac{25}{8}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\le\\frac{25}{8}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Takhetieao $\\begin{array}{r}{\\phi_{t,j}=\\operatorname*{min}\\bigl\\{\\frac{25L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\frac{1}{4}\\bigr\\}}\\end{array}$ and hat ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{25}{8}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\frac{25\\sqrt{2}L_{\\operatorname*{max}}}{2T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}=\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, (K.1) also holds for $T_{t,j}=0$ ", "page_idx": 40}, {"type": "text", "text": "To conclude, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\beta}_{t,j}=\\frac{5}{2}\\operatorname*{max}\\left\\{\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\right\\}}\\\\ &{\\quad\\leq\\frac{5}{2}\\operatorname*{max}\\left\\{\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Step 2: Simplify the bound. Recall that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi_{t,j}=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\frac{1}{4}\\right\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "As $\\begin{array}{r}{T_{t}\\geq\\frac{2L_{\\operatorname*{max}}}{9}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\sqrt{\\frac{2\\cdot\\frac{1}{4}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "According to the definition of \u03a6,if pt, \u2265 2m $\\begin{array}{r}{\\hat{p}_{t,j}\\,\\geq\\,\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\frac{5\\sqrt{2}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\sqrt{\\frac{2\\cdot4\\hat{p}_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If $\\begin{array}{r}{\\hat{p}_{t,j}\\,\\leq\\,\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\end{array}$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\leq\\frac{25\\sqrt{2}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}=\\sqrt{\\frac{2L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\cdot25\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\frac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\right\\}=\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This gives us the desired result. ", "page_idx": 40}, {"type": "text", "text": "Lemma K.2. When $\\begin{array}{r}{\\tilde{\\beta}_{t,j}=\\frac{5}{4}\\sqrt{\\frac{2p_{j}(1-p_{j})L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\end{array}$ and $T_{t,j}>0$ we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{p_{j}}{T_{t,j}}\\leq\\frac{4}{T_{t}}\\operatorname*{max}\\left\\{1,\\frac{25}{16}\\frac{L_{\\operatorname*{max}}}{T_{t,j}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof of Lemma K.2. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p_{j}}{T_{t,j}}=\\frac{p_{j}T_{t}}{T_{t,j}T_{t}}}\\\\ &{\\qquad\\le\\frac{\\hat{p}_{t,j}T_{t}+\\frac{5}{4}\\sqrt{2p_{j}(1-p_{j})L_{\\operatorname*{max}}T_{t}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}{T_{t,j}T_{t}}}\\\\ &{\\qquad=\\frac{1}{T_{t}}+\\frac{5}{4}\\sqrt{\\frac{p_{j}}{T_{t,j}}}\\cdot\\sqrt{\\frac{1}{T_{t,j}}-\\frac{p_{j}}{T_{t,j}}}\\cdot\\sqrt{\\frac{2L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}\\\\ {\\Rightarrow}&{\\frac{p_{j}}{T_{t,j}}\\le\\frac{4}{T_{t}}\\operatorname*{max}\\left\\{1,\\frac{25}{16}\\frac{L_{\\operatorname*{max}}}{T_{t,j}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma K.3. If $\\psi_{t,j}=\\hat{p}_{t,j}$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\phi_{t,j}\\leq\\operatorname*{min}\\left\\lbrace16p_{j},\\frac{1}{4}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma K.3. Recall to the definition of $\\phi_{t,j}$ in (I.2) and $\\psi_{t,j}$ in (I.8), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\phi_{t,j}=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\frac{1}{4}\\right\\},\\,\\psi_{t,j}=\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\hat{p}_{t,j}\\geq\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\quad\\phi_{t,j}=\\operatorname*{min}\\left\\{4\\hat{p}_{t,j},\\frac{1}{4}\\right\\}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We only need to upper bound $\\hat{p}_{t,j}$ ", "page_idx": 41}, {"type": "text", "text": "By (1.4), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{t,j}=\\hat{p}_{t,j}\\leq p_{j}+\\frac{5}{2}\\operatorname*{max}\\left\\{\\cfrac{L_{\\operatorname*{max}}}{3T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}},\\sqrt{\\frac{2p_{j}\\left(1-p_{j}\\right)L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}\\right\\}}\\\\ {\\Rightarrow\\;}&{\\psi_{t,j}\\leq p_{j}+\\frac{5}{2}\\operatorname*{max}\\left\\{\\cfrac{4}{75}\\psi_{t,j},\\sqrt{p_{j}\\left(1-p_{j}\\right)\\frac{8}{25}\\psi_{t,j}}\\right\\}}\\\\ &{\\qquad=p_{j}+\\operatorname*{max}\\left\\{\\cfrac{4}{15}\\psi_{t,j},\\sqrt{2p_{j}(1-p_{j})\\psi_{t,j}}\\right\\}}\\\\ {\\Rightarrow\\;}&{\\psi_{t,j}\\leq4p_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus $\\hat{p}_{t,j}=\\psi_{t,j}\\leq4p_{j}$ , and $\\phi_{t,j}\\leq\\operatorname*{min}\\left\\lbrace16p_{j},{\\frac{1}{4}}\\right\\rbrace$ ", "page_idx": 41}, {"type": "text", "text": "L Upper Bound of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ : Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Theorem 3.3. The $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ algorithm is $(\\varepsilon,\\delta)$ -PAC and its expected sample complexity is ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tilde{O}\\bigg(\\operatorname*{min}\\left\\{\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon},x\\ne x_{\\varepsilon},x^{*}}T_{\\mathrm{V}}(x)+T_{\\mathrm{D}}(x_{\\varepsilon},x)+T_{\\mathrm{R}}(x),\\,T_{\\mathrm{V}}^{\\mathrm{N}}+T_{\\mathrm{D}}^{\\mathrm{N}}\\right\\}\\bigg).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. We let $\\tau_{1}$ denote the stopping time of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ $\\tau_{2}$ denote the stopping time of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ and $\\tau$ denote the stopping time of $\\mathrm{PS}\\bar{\\varepsilon}\\bar{\\mathrm{B}}\\bar{\\mathrm{AI}}^{+}$ when an identical sequence of samples and returns are applied to them. By the design of these algorithms, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tau\\leq\\operatorname*{min}\\{\\tau_{1},\\tau_{2}\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Part I: Prove that $\\mathbb{P}(\\stackrel{\\circ}{x}_{\\varepsilon}\\notin\\mathcal{X}_{\\varepsilon})\\leq\\delta$ ", "page_idx": 41}, {"type": "text", "text": "Let event $\\mathcal{E}_{1}=1\\{\\tau_{1}<\\tau_{2}\\}\\cap1\\{\\mathrm{PS}\\varepsilon{\\mathrm{BAI}}$ returns arm $x_{\\varepsilon}$ when it terminates}. Recall that $\\dot{x}_{\\varepsilon}$ denotes the recommended arm of $\\Nu\\varepsilon$ BAI when it terminates and $\\mathring{x}_{\\varepsilon}$ denotes the recommended arm of ", "page_idx": 41}, {"type": "text", "text": "$\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ when it terminates. The design of algorithms indicates that: ", "page_idx": 42}, {"type": "text", "text": "Moreover, with the performance guarantees of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ and $\\mathrm{PS}\\varepsilon$ BAI(shown in Proposition 3.1 and Theorem 3.2 individually), we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}[\\mathring{x}_{\\varepsilon}\\ \\notin\\ \\mathcal{X}_{\\varepsilon}]=\\mathbb{P}[\\mathring{x}_{\\varepsilon}\\ \\notin\\ \\mathcal{X}_{\\varepsilon}|\\mathcal{E}_{1}]\\cdot\\mathbb{P}[\\mathcal{E}_{1}]+\\mathbb{P}(\\mathring{x}_{\\varepsilon}\\ \\notin\\ \\mathcal{X}_{\\varepsilon}|\\mathcal{E}_{1}^{c}]\\cdot\\mathbb{P}[\\mathcal{E}_{1}^{c}]}\\\\ &{}&{\\quad=\\mathbb{P}[x_{\\varepsilon}\\ \\notin\\ \\mathcal{X}_{\\varepsilon}|\\mathcal{E}_{1}]\\cdot\\mathbb{P}[\\mathcal{E}_{1}]+\\mathbb{P}[\\dot{x}_{\\varepsilon}\\notin\\ \\mathcal{X}_{\\varepsilon}|\\mathcal{E}_{1}^{c}]\\cdot\\mathbb{P}[\\mathcal{E}_{1}^{c}]}\\\\ &{}&{\\quad\\leq\\delta\\cdot\\mathbb{P}[\\mathcal{E}_{1}]+\\delta\\cdot\\mathbb{P}[\\mathcal{E}_{1}^{c}]=\\delta\\cdot(\\mathbb{P}[\\mathcal{E}_{1}]+\\mathbb{P}[\\mathcal{E}_{1}^{c}])=\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Part II: Derive the expected sample complexity of $\\scriptstyle\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ ", "page_idx": 42}, {"type": "text", "text": "We first derive the conditional expectation of the stopping time of $\\mathrm{PS}\\varepsilon$ BAI. By the same argument as in (J.6), $T_{t}\\leq t\\leq4T_{t}$ for any $t$ . Therefore, it is sufficient to upper bound $\\mathbb{E}[T_{\\tau_{1}}]$ and $\\mathbb{E}\\!\\left[\\tau_{1}\\right]$ can be upper bounded by $4\\mathbb{E}[T_{\\tau_{1}}]$ ", "page_idx": 42}, {"type": "text", "text": "According to Lemma G.6 and Lemma G.6, when both events Good and CI occur, $\\mathrm{PS}\\varepsilon$ BAI terminates With $T_{\\tau_{1}}$ satisfying $T_{\\tau_{1}}\\leq\\tilde{T}$ ,where $\\tilde{T}$ is defined in (J.3) and as below: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{T}:=\\underset{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon},x\\not=x_{\\varepsilon},x^{*}}{\\operatorname*{max}}\\frac{6400\\ln{6400}\\cdot d}{\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}\\ln{\\frac{K d/\\left(\\Delta\\left(x^{*},x\\right)+\\varepsilon\\right)^{2}}{\\delta}}}\\\\ &{\\qquad\\qquad\\qquad+6400\\ln{6400}\\cdot\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)\\ln{\\frac{N\\mathrm{H}_{\\mathrm{DE}}\\left(x_{\\varepsilon},x\\right)}{\\delta}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{3200\\sqrt{2}\\ln{3200\\sqrt{2}}\\cdot N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}\\ln{\\frac{\\frac{K N L_{\\operatorname*{max}}}{\\Delta\\left(x^{*},x\\right)+\\varepsilon}}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Given time step $t$ with $T_{t}\\geq2\\tilde{T}$ (thus $t\\geq8\\tilde{T}$ ), we will upper bound $\\mathbb{P}[\\tau_{1}>t]$ in the following. We firstly upper bound $\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}\\right]$ . Note that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}\\right]=\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}|T_{\\tau_{1}}\\ge\\frac{T_{t}}{2}+1\\right]\\mathbb{P}\\left[T_{\\tau_{1}}\\ge\\frac{T_{t}}{2}+1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}|T_{\\tau_{1}}<\\frac{T_{t}}{2}+1\\right]\\mathbb{P}\\left[T_{\\tau_{1}}<\\frac{T_{t}}{2}+1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\le\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}|T_{\\tau_{1}}\\ge\\frac{T_{t}}{2}+1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "$\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ fails to terminate with $T_{\\tau_{1}}\\leq\\frac{T_{t}}{2}$ , it implies that event $\\operatorname{Good}\\bigcap\\left(\\bigcap_{s\\in\\mathbb{Z}}\\operatorname{CI}_{s}\\right)$ fails, where $\\mathcal{T}=\\{s:\\frac{T_{t}}{2}+1\\leq T_{s}\\leq T_{t}\\}$ Hence, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[T_{n}>T_{1}\\right]\\leq\\mathbb{P}\\left[T_{n}>T_{1}\\middle|T_{n}\\geq\\frac{T_{1}}{2}+1\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[\\mathrm{Gode}^{*}\\bigcup_{s\\in\\mathbb{Z}^{*}}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[\\mathrm{Gode}^{*}\\right]+\\mathbb{P}\\left[\\mathrm{God}\\bigcap_{s\\in\\mathbb{Z}^{*}}\\right]\\Bigg[}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[\\mathrm{Gode}^{*}\\right]+\\mathbb{P}\\left[\\mathrm{God}\\bigcap_{s\\in\\mathbb{Z}^{*}}\\right]\\Bigg]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[\\mathrm{Gode}^{*}\\right]+\\mathbb{P}\\left[\\bigg(\\bigcup_{s\\in\\mathbb{Z}^{*}}\\bigg)\\left[\\mathrm{Gode}\\right]\\right]}\\\\ &{\\qquad\\qquad\\stackrel{(a)}{\\leq}\\frac{\\delta}{2r^{3}}+2\\frac{\\underset{s\\in\\mathbb{Z}^{*}}{\\sum}}{2r_{s\\in\\mathbb{Z}^{*}/2+1}}(K\\delta_{s,T_{1}}+N\\delta_{d,T_{2}}+K N\\delta_{m,T_{2}})}\\\\ &{\\qquad\\qquad\\stackrel{(b)}{\\leq}\\frac{\\delta}{2r^{3}}+2\\frac{\\underset{s\\in\\mathbb{Z}^{*}}{\\sum}}{r_{s\\in\\mathbb{Z}^{*}/2+1}}\\frac{\\delta}{\\sum_{T_{2}}^{3}}\\leq\\frac{\\delta}{2r^{3}}+2\\int_{r_{1/2}}^{r_{1}}\\frac{\\delta}{\\zeta\\zeta^{3}r^{3}}\\,\\mathrm{d}x=\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $(a)$ is obtained by applying Lemma G.5 to time steps in $\\mathcal{T}\\cap\\mathcal{T}_{t}$ (i.e., the Exp time steps in $\\mathcal{T}$ and note that there are at most two time steps $t_{1}<t_{2}$ with $T_{t_{1}}=T_{t_{2}}$ due to the reversion step; $(b)$ is derived by substituting $\\begin{array}{r}{\\delta_{v,T_{t}}=\\frac{\\delta}{15K T_{t}^{3}},\\delta_{d,T_{t}}=\\frac{\\delta}{15N T_{t}^{3}},\\delta_{m,\\bar{T_{t}}}=\\frac{\\delta}{15K N T_{t}^{3}}}\\end{array}$ Which are used to define the confidence radius $\\rho$ in (3.3). By using the fact that $4T_{\\tau_{1}}\\geq\\tau_{1}$ , for any $T_{t}\\geq2\\tilde{T}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\tau_{1}>4T_{t}\\right]\\le\\mathbb{P}\\left[T_{\\tau_{1}}>T_{t}\\right]\\le\\displaystyle\\frac{\\delta}{2\\tau^{*}}+\\frac{3\\delta}{5T_{t}^{2}}}\\\\ {\\Rightarrow}&{\\mathbb{P}\\left[\\tau_{1}>4T_{t}+i\\right]\\le\\displaystyle\\frac{\\delta}{2\\tau^{*}}+\\frac{3\\delta}{5T_{t}^{2}},\\quad i=0,1,2,3}\\\\ {\\Rightarrow}&{\\mathbb{P}\\left[\\tau_{1}>t\\right]\\le\\displaystyle\\frac{\\delta}{2\\tau^{*}}+\\frac{48\\delta}{5(t-4)^{2}},\\quad\\forall8\\tilde{T}\\le t\\le\\tau^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This indicates fot \u2265T,the probability thatBAIds nt stop afertme sepsis + 5) Therefore, by Tonelli's Theorem, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau_{1}|8\\tilde{T}<\\tau_{1}\\leq\\tau^{*}]\\leq\\displaystyle\\sum_{t=8\\tilde{T}}^{\\tau^{*}-1}\\mathbb{P}[\\tau_{1}>t]\\leq\\displaystyle\\sum_{t=8\\tilde{T}}^{\\tau^{*}-1}\\left(\\frac{\\delta}{2\\tau^{*}}+\\frac{48\\delta}{5(t-4)^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1+\\displaystyle\\int_{8\\tilde{T}-1}^{\\tau^{*}-1}\\frac{48\\delta}{5(t-4)^{2}}\\;\\mathrm{d}t\\leq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Next, we bound $\\mathbb{E}\\tau$ , the expected sample complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ $\\mathbb{E}\\tau$ can be decomposed as below: ", "text_level": 1, "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\tau\\leq8\\tilde{T}+\\mathbb{E}[\\tau|8\\tilde{T}<\\tau\\leq\\tau^{*}]+\\mathbb{E}[\\tau|\\tau>\\tau^{*}].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since $\\tau=\\operatorname*{min}\\{\\tau_{1},\\tau_{2}\\}$ and $\\mathbb{P}[\\operatorname*{min}\\{\\tau_{1},\\tau_{2}\\}>t]\\le\\mathbb{P}[\\tau_{1}>t]$ for all $t$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[\\tau|8\\tilde{T}<\\tau\\leq\\tau^{*}]=\\sum_{t=8\\tilde{T}}^{\\tau^{*}-1}\\mathbb{P}[\\tau>t]=\\sum_{t=8\\tilde{T}}^{\\tau^{*}-1}\\mathbb{P}[\\operatorname*{min}\\{\\tau_{1},\\tau_{2}\\}>t]}}\\\\ &{}&{\\leq\\displaystyle\\sum_{t=8\\tilde{T}}^{\\tau^{*}-1}\\mathbb{P}[\\tau_{1}>t]=\\mathbb{E}[\\tau_{1}|8\\tilde{T}<\\tau_{1}\\leq\\tau^{*}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Besides, since $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ will terminate after $\\tau^{*}$ time steps, i.e., $\\mathbb{P}(\\tau_{1}\\leq\\tau^{*})=1$ . We have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau|\\tau\\geq\\tau^{*}]=\\sum_{t=\\tau^{*}}^{\\infty}\\mathbb{P}[\\tau\\geq t]\\leq1+\\sum_{t=\\tau^{*}+1}^{\\infty}\\mathbb{P}[\\tau_{2}\\geq t]=1+\\mathbb{E}[\\tau_{2}|\\tau_{2}>\\tau^{*}].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Lemma I ndicates hat $\\begin{array}{r}{\\mathbb{P}[\\tau_{2}\\geq t]\\leq\\frac{\\delta}{(\\alpha-1)C_{3}(t/2)^{2}}}\\end{array}$ for all $t\\geq T_{0}$ where ", "page_idx": 43}, {"type": "equation", "text": "$$\nT_{0}=\\frac{768\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}}\\ln\\frac{768K C_{3}\\big(8L_{\\mathrm{max}}+25d\\big)}{\\big(\\varepsilon+\\Delta_{\\mathrm{min}}\\big)^{2}\\,\\delta},\\quad C_{3}=\\sum_{n=1}^{\\infty}n^{-3}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since the order of $T_{0}$ is apparently larger than $\\tau^{*}$ , we have $T_{0}\\leq\\tau^{*}$ . Then, with the same method as in the proof of Proposition 3.1, the conditional sample complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ can be bounded as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau_{2}|\\tau_{2}>\\tau^{*}]\\le\\int_{\\tau^{*}}^{+\\infty}\\mathbb{P}(\\tau_{2}\\ge x)\\,\\mathrm{d}x\\le\\int_{\\tau^{*}}^{+\\infty}\\frac{\\delta}{(\\alpha-1)C_{3}(x/2)^{2}}\\,\\mathrm{d}x=\\frac{\\delta}{C_{3}\\tau^{*}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Substituting terms in (L.2) with (L.1), (L.3), (L.4) and (L.5), we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\tau\\leq8\\tilde{T}+3+\\frac{\\delta}{C_{3}\\tau^{*}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Besides, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\tau\\le\\mathbb{E}\\tau_{2}\\overset{(a)}{\\le}T_{0}+\\frac{\\delta}{(\\alpha-1)(\\alpha-2)(T_{0}/2)^{\\alpha-2}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Where (a) is shown in (F.1). ", "page_idx": 43}, {"type": "text", "text": "Altogether, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}\\tau\\leq\\operatorname*{min}\\left\\{8\\tilde{T}+3+\\frac{\\delta}{C_{3}\\tau^{*}},\\,8\\tilde{T}+3+\\frac{\\delta}{C_{3}\\tau^{*}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "M Analysis of Lower Bound: Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "The dynamics for under our PSLB model is displayed in Dynamics 1. ", "page_idx": 44}, {"type": "table", "img_path": "Q5e3ftQ3q3/tmp/85726e3ff1a9be4d48cc62c3b8bb4b31f820a558ccf602e8edbda6e3ed360141.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "To derive the lower bound in Theorem 4.1, we investigate two environments different from the one defined in Section 2 (and as in Dynamics 1): ", "page_idx": 44}, {"type": "text", "text": "\u00b7 Dynamics 2: the agent observes the index of current context $j_{t}$ , and the environment reduces to contextual linear bandits; the definition of Dynamics 2 and the lower bound under it are detailed in Appendix M.1.   \n\u00b7 Dynamics 3: the agent observes the changepoints in $\\mathcal{C}$ and context vector $\\theta_{j_{t}}^{*}$ 's, and hence she solely needs to estimate the distribution of contexts; the definition of Dynamics 3 and the lower bound under it are detailed in Appendix M.2. ", "page_idx": 44}, {"type": "text", "text": "We first derive a lower bound for $(\\varepsilon,\\delta)$ -BAI algorithms in Dynamics 2, that is, in contextual linear bandits. ", "page_idx": 44}, {"type": "text", "text": "Corollary M.1. For any $(\\varepsilon,\\delta)$ -PAC algorithm $\\pi$ there exists an instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ with Dynamics 2 such that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]\\ge T_{\\varepsilon}(\\Lambda)\\log\\displaystyle\\frac{1}{2.4\\delta}\\cdot}\\\\ &{w h e r e\\quad T_{\\varepsilon}(\\Lambda)^{-1}=\\operatorname*{max}_{\\{v_{j}\\in\\Delta_{\\mathcal{X}}\\}_{j=1}^{N}\\Lambda^{\\prime}\\in\\mathrm{Alte}(\\Lambda)}\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\frac{(x^{\\top}(\\theta_{j}^{*}-\\theta_{j}^{\\prime}))^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "is defined in Theorem 4.1. In addition, when $|\\mathcal{X}_{\\varepsilon}|=1$ ", "page_idx": 44}, {"type": "equation", "text": "$$\nT_{\\varepsilon}(\\Lambda)=\\operatorname*{min}_{\\{v_{j}\\in\\Delta x\\}_{j=1}^{N}}\\operatorname*{max}_{x\\neq x^{*}}\\frac{\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This lower bound generalizes the result of [16] to the linear bandit setting. ", "page_idx": 44}, {"type": "text", "text": "We next study Dynamics 3. In this setting, the agent solely needs to estimate the distribution of contexts $P_{\\theta}$ with context samples. Once the agent obtains a good estimate of $P_{\\theta}$ , she can identify an $\\varepsilon$ -optimal arm w.h.p. Hence, the lower bound on the complexity of an $(\\varepsilon,\\delta)$ -BAI algorithm is the product of the minimum length of a stationary segment and the minimum number of context samples/changepoints needed for distribution estimation. ", "page_idx": 44}, {"type": "text", "text": "Corollary M.2. For any $(\\varepsilon,\\delta)$ -PAC algorithm $\\pi$ there exists an instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ with Dynamics 3 such that $\\mathbb{E}\\tau\\geq c_{N_{c}}$ , where $c_{N_{C}}$ is the $N_{\\mathcal{C}}^{t h}$ changepoint n the changepoint sequence $\\mathcal{C}$ ", "page_idx": 44}, {"type": "text", "text": "Altogether, the sample complexity of an $(\\varepsilon,\\delta)$ -BAI algorithm in Dynamics 2 and 3 build up the lower bound in Theorem 4.1. ", "page_idx": 45}, {"type": "text", "text": "M.1  Lower Bound for $\\varepsilon$ -BAI in Contextual Linear Bandits ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we consider a sub-problem where the index of the current context $j_{t}$ is revealed at eachtimestep $t$ . The original piecewise-stationary problem becomes a contextual problem whose dynamics is presented in Algorithm 2. As more information is provided to the agent, the lower ", "page_idx": 45}, {"type": "table", "img_path": "Q5e3ftQ3q3/tmp/5a017717566d54e8aba0fe6f715721d0924e73d99c14ab77f20d71f200abf8b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "bound for this sub-problem is smaller than the one for the original problem. ", "page_idx": 45}, {"type": "text", "text": "Corollary M.1. For any $(\\varepsilon,\\delta)$ -PAC algorithm $\\pi$ there exists an instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ with Dynamics 2 such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]\\ge T_{\\varepsilon}(\\Lambda)\\log\\displaystyle\\frac{1}{2.4\\delta}\\cdot}\\\\ &{w h e r e\\quad T_{\\varepsilon}(\\Lambda)^{-1}=\\operatorname*{max}_{\\{v_{j}\\in\\Delta_{\\mathcal{X}}\\}_{j=1}^{N}\\Lambda^{\\prime}\\in\\mathrm{Alte}(\\Lambda)}\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\frac{(x^{\\top}(\\theta_{j}^{*}-\\theta_{j}^{\\prime}))^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "is defined in Theorem 4.1. In addition, when $|\\mathcal{X}_{\\varepsilon}|=1$ ", "page_idx": 45}, {"type": "equation", "text": "$$\nT_{\\varepsilon}(\\Lambda)=\\operatorname*{min}_{\\{v_{j}\\in\\Delta x\\}_{j=1}^{N}}\\operatorname*{max}_{x\\neq x}\\frac{\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For simplicity, we consider the noise model is the Clipped Gaussian Distribution $C N(1)$ ,i.e., $\\eta\\sim C N(1)$ or $P_{\\eta}=C N(1)$ ", "page_idx": 45}, {"type": "text", "text": "Definition M.3. A random variable x follows the Clipped Gaussian Distribution with parameter $\\sigma$ denotedby $x\\sim C N(\\sigma)$ if it has the probability distribution function ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{(2\\Phi(\\sigma)-1)\\cdot{\\sqrt{2\\pi\\sigma^{2}}}}}\\exp\\left(-{\\frac{x^{2}}{2\\sigma^{2}}}\\right),\\quad\\forall x\\in[-1,1]\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\Phi(x)$ is the cumulative distribution function of the standard Gaussian distribution. ", "page_idx": 45}, {"type": "text", "text": "Some notations are introduced here: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 Given an instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ , define the alternative instance $\\Lambda^{\\prime}=(\\mathcal{X},\\Theta^{\\prime},P_{\\theta^{\\prime}},\\mathcal{C})$ with respect to $\\Lambda$ where $\\Theta^{\\prime}\\,=\\,(\\theta_{1}^{\\prime},\\ldots,\\theta_{n}^{\\prime})\\,\\in\\,\\mathbb{R}^{d\\times N}$ and $P_{\\theta^{\\prime}}[\\theta_{j}^{\\prime}]\\,=\\,P_{\\theta}[\\theta_{j}^{*}]$ , .t. there $\\exists x\\,\\in\\,{\\mathcal{X}}\\mid$ $\\begin{array}{r}{\\ensuremath{\\mathcal{X}}_{\\varepsilon},\\forall x_{\\varepsilon}\\in\\ensuremath{\\mathcal{X}}_{\\varepsilon},s.t.,x_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta^{\\prime}\\sim P_{\\theta^{\\prime}}}<x^{\\top}\\mathbb{E}_{\\theta^{\\prime}\\sim P_{\\theta^{\\prime}}}-\\varepsilon}\\end{array}$ We denote the set containing all the alternative instance (w.r.t. $\\Lambda$ )as $\\mathrm{Alt}_{\\Theta}(\\Lambda)$   \n\u00b7 $\\mathcal{H}_{t}=\\left(x_{s},Y_{s,x_{s}},j_{s}\\right)_{s=1}^{t-1}$ is the observation history up to but not include time $t$   \n\u00b7 $\\begin{array}{r}{N_{j,x}(t)=\\sum_{s=1}^{t}\\mathbb{1}\\{x_{s}=x,j_{s}=j\\}}\\end{array}$ is the number of times arm in which $x$ is sampled under context $j$ And $\\begin{array}{r}{\\bar{N}_{j}(t):=\\sum_{x\\in\\mathcal{X}}N_{j,x}(t)}\\end{array}$ is the number of times in which context $j$ appears.   \n\u00b7 $\\operatorname{kl}(p,q)=\\operatorname{KL}(\\operatorname{Bern}(p),\\operatorname{Bern}(q))$ is the $\\mathrm{KL}$ divergence between two Bernoulli distributions with parameters $p$ and $q$ ", "page_idx": 45}, {"type": "text", "text": "Therefore, the probability of the observation history $\\mathcal{H}_{t+1}=\\left(x_{s},Y_{s,x_{s}},j_{s}\\right)_{s=1}^{t}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\nP_{\\pi\\Lambda}\\left[(x_{s},Y_{s,x_{s}},j_{s})_{s=1}^{t}\\right]=\\prod_{l=1}^{l_{t}}P_{\\theta}[\\theta_{j_{c_{l}}}^{*}]\\prod_{s=0}^{L_{l}-1}\\pi(x_{c_{l}+s}|{\\mathcal H}_{c_{l}+s})P_{\\eta}(Y_{c_{l}+s,x_{c_{l}+s}}|x_{c_{l}+s},\\theta_{j_{c_{l}}}^{*})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then the log-likelihood between the two instance up to time $t$ , given the observed data, is ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathrm{t}}\\left[\\langle x,Y_{k},z_{k},j_{\\mathrm{t}}^{\\star}\\rangle\\right]_{\\mathbf{t}}}\\\\ &{=\\log\\frac{P_{k}}{P_{k}}\\!\\!\\!\\left[\\left\\langle x,Y_{k,n},j_{\\mathrm{t}}^{\\star}\\rangle\\!\\!\\right]_{\\mathbf{t}}^{\\mathrm{tand}}}\\\\ &{\\overset{(a)}{=}\\log\\left(\\frac{\\prod_{i=1}^{n}P_{k}(y_{i})}{\\prod_{i=1}^{n}P_{k}(y_{i})}\\prod_{i=1}^{n}\\overline{{\\{x\\left(x_{i+1}\\right)\\!\\!\\beta_{i+1}\\!\\!\\cdot\\!\\!\\gamma_{i}}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{\\overset{(b)}{=}\\log\\left(\\frac{\\prod_{i=1}^{n}\\widehat{\\gamma}_{i}\\left(y_{i}^{\\mathrm{D}}\\right)_{i=1}^{n}\\sigma\\left(x_{i+1},j_{\\mathrm{t}+1}\\right)\\sigma\\left(Y_{k+1}+x_{i+1},j_{\\mathrm{t}+1}\\right)}{\\prod_{i=1}^{n}P_{k}\\prod_{i=1}^{n}\\widehat{\\gamma}_{i}\\left(x_{i+1},j_{\\mathrm{t}+1}\\right)\\sigma\\left(x_{i+1},j_{\\mathrm{t}+1},\\ldots,x_{i+1},j_{\\mathrm{t}}\\right)}\\right)}\\\\ &{=\\log\\left(\\prod_{i=1}^{n}\\prod_{i=1}^{k-1}\\frac{\\prod_{i=1}^{n}P_{k}\\left(y_{i}\\right)_{\\mathrm{t}}\\left(x_{i+1},x_{i+1},\\ldots,x_{i}^{\\mathrm{t}}\\right)}{\\prod_{i=1}^{n}\\widehat{\\gamma}_{i}\\left(x_{i+1},x_{i+1},\\ldots,x_{i}^{\\mathrm{t}}\\right)\\sigma\\left(x_{i+1},\\ldots,x_{i}^{\\mathrm{t}},\\ldots,x_{i}^{\\mathrm{t}}\\right)}\\right)}\\\\ &{\\overset{(c)}{\\underset{\\mathrm{t\\toi}}{=}}\\log\\left(\\frac{\\exp\\left(-x_{i}^{\\mathrm{t}}\\right)+\\sum_{i=1}^{n}\\sigma\\left(2\\right)}{\\exp\\left(-(x_{i}^{\\mathrm{t}}\\right)+(y_{i}^{\\mathrm{t}})\\right)^{ \n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(a)$ utilizes $P_{\\theta}[\\theta_{j_{c_{l}}}^{*}]\\;=\\;P_{\\theta^{\\prime}}[\\theta_{j_{c_{l}}}^{\\prime}]$ \uff0c $(b)$ makes use of the relationship between the arm and observation and i (c) $\\vartheta_{j_{c_{l}}}:=\\theta_{j_{c_{l}}}^{*}-\\theta_{j_{c_{l}}}^{\\prime}$ . Thus, the expectation of the log-liklihood is in the following, the expectation is taken under instance $\\Lambda$ and algorithm $\\pi$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[L]=\\mathbb{E}\\left[\\frac{L_{1}}{\\sum_{i=1}^{I-1}}\\frac{x_{i}^{\\top}x_{i}+\\beta_{i,j}}{\\sum_{i=1}^{I-1}}\\frac{(\\beta_{j})_{i,j}(2\\gamma_{j,i}+x_{i+1}^{\\top}+\\beta_{j+1}^{\\top})}{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\frac{L_{1}}{\\sum_{i=1}^{I-1}}\\frac{(\\beta_{j}^{\\top}\\gamma_{i}x_{i+1}+x_{i+1}^{\\top})}{2}\\bar{\\alpha}_{i}\\right]}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}\\left[\\frac{L_{1}}{\\sum_{i=1}^{I-1}}\\bar{\\beta}_{i,i}\\left(\\sum_{u=1}^{I-1}x_{i+1}+x_{i+1}^{\\top}\\right)\\phi_{i,u}\\right]}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}\\left[\\sum_{i\\neq i}1\\{x_{i+1}=x\\}\\sum_{j=1}^{N}1\\{\\beta_{i,i}=j\\}\\sum_{i=1}^{L}\\bar{\\sigma}_{j,i}^{\\top}\\left(\\sum_{v=1}^{I-1}x_{i+1}+x_{i+1}^{\\top}\\right)\\phi_{j,v}\\right]}\\\\ &{\\quad=\\frac{1}{2}\\sum_{i\\neq i}^{N}\\sum_{m}\\mathbb{E}[\\tilde{N}_{i,i}(t)]{\\hat{\\sigma}_{j}^{\\top}}{x_{i}^{\\top}}^{\\top}\\phi_{j}}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}\\phi_{i}\\sum_{m}^{N}\\frac{\\mathbb{E}[N_{i}(t)]}{\\sum_{i=1}^{I}}\\sum_{k=1}^{\\mathbb{E}\\left[N_{i}(t)\\right]}{\\sum_{s=1}^{I-1}\\sum_{i=1}^{N}\\sum_{j=1}^{I}\\hat{\\sigma}_{j,k}^{\\top}}}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}[i\\sum_{i=1}^{N}\\frac{\\mathbb{E}[N_{i}(t)]}{\\sum_{i=1}^{I}}\\sum_{k=1}^{\\mathbb{E}\\left[N_{i}(t)\\right]}{\\sum_{s=1}^{I}\\sum_{k=1}^{I- \n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "As the lengths of all the stationary phases are upper bounded, i.e., $L_{l}\\,\\leq\\,L_{\\operatorname*{max}},\\forall l\\,\\in\\,\\mathbb{N}$ thenby Wald's Lemma, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[N_{j}(t)]}{\\mathbb{E}[t]}=\\frac{\\mathbb{E}[t]P_{\\theta}[\\theta_{j}^{*}]}{\\mathbb{E}[t]}=P_{\\theta}[\\theta_{j}^{*}]=p_{j}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The above also holds for $t=\\tau$ where $\\tau$ is a stopping time. According to Lemma 19 in [31], ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[L_{\\tau}]\\ge\\operatorname*{sup}_{\\boldsymbol{\\varepsilon}\\in\\mathcal{F}_{\\tau}}\\mathrm{kl}(P_{\\pi\\Lambda}[\\boldsymbol{\\mathcal{E}}],P_{\\pi\\Lambda^{\\prime}}[\\boldsymbol{\\mathcal{E}}])\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\mathscr{F}_{\\tau}=\\sigma(\\mathscr{H}_{\\tau+1})$ . In addition, let $\\begin{array}{r}{\\mathcal{E}=\\left\\{\\begin{array}{r l}\\end{array}\\right.}\\end{array}$ the recommended arm $\\hat{x}_{\\varepsilon}\\notin\\mathcal{X}_{\\varepsilon}\\}$ , as the algorithm $\\pi$ is $\\delta$ -PAC, then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname{kl}(P_{\\pi\\Lambda}[\\mathcal{E}],P_{\\pi\\Lambda^{\\prime}}[\\mathcal{E}])\\geq\\operatorname{kl}(1-\\delta,\\delta)\\geq\\log\\frac{1}{2.4\\delta}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "From (M.8), (M.9), (M.10) and (M.11), we conclude that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\mathbb{E}[\\tau]\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{E}\\left[N_{j,x}(\\tau)\\right]}{\\mathbb{E}[N_{j}(\\tau)]}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\\geq\\log\\frac{1}{2.4\\delta}}\\\\ {\\Rightarrow}&{\\underbracket{\\operatorname*{min}}_{N^{\\prime}\\in\\mathrm{Atte}(\\Lambda)}\\frac{1}{2}\\mathbb{E}[\\tau]\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{E}\\left[N_{j,x}(\\tau)\\right]}{\\mathbb{E}[N_{j}(\\tau)]}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\\geq\\log\\frac{1}{2.4\\delta}}\\\\ {\\Rightarrow}&{\\underbracket{\\operatorname*{max}}_{\\{\\sigma_{j}\\in\\Delta x\\}_{j=1}^{N}\\Lambda^{\\prime}\\in\\mathrm{Atte}(\\Lambda)}\\frac{1}{2}\\mathbb{E}[\\tau]\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\\geq\\log\\frac{1}{2.4\\delta}}\\\\ {\\Leftrightarrow}&{\\mathbb{E}[\\tau]\\geq T_{\\varepsilon}(\\Lambda)\\log\\displaystyle\\frac{1}{2.4\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where ", "page_idx": 47}, {"type": "equation", "text": "$$\nT_{\\varepsilon}(\\Lambda)^{-1}=\\operatorname*{max}_{\\{\\boldsymbol{v}_{j}\\in\\Delta_{x}\\}_{j=1}^{N}\\Lambda^{\\prime}\\in\\mathrm{Alte}(\\Lambda)}\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}\\boldsymbol{v}_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The solution to the above optimization problem is in general intractable, even for the stationary case [32]. We can establish a connection of the above problem to the stationary $\\varepsilon$ -bestidentification problem in linear bandits when we assume the change of the latent vectors are the same, i.e., $\\vartheta_{1}=\\ldots=\\vartheta_{N}$ ", "page_idx": 47}, {"type": "text", "text": "M.1.1 Connection with the Stationary Case ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In the alternative instance $\\operatorname{Alt}_{\\Theta}(\\Lambda),\\,\\exists x\\in\\mathcal{X}\\setminus\\mathcal{X}_{\\varepsilon}$ , for any arm $x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}$ ,s.t. ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{}}&{{\\displaystyle\\displaystyle\\sum_{j=1}^{N}p_{j}x_{\\ell}^{\\top}\\theta_{j}^{\\prime}+\\varepsilon<\\displaystyle\\sum_{j=1}^{N}p_{j}x^{\\top}\\theta_{j}^{\\prime}}}\\\\ {{\\displaystyle\\Leftrightarrow}}&{{\\displaystyle\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}^{\\prime}+\\varepsilon<0}}\\\\ {{\\displaystyle\\Leftrightarrow}}&{{\\displaystyle-\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}^{\\prime}+\\displaystyle\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}^{\\ast}>\\displaystyle\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}^{\\ast}+\\varepsilon}}\\\\ {{\\displaystyle\\Leftrightarrow}}&{{\\displaystyle\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}>\\displaystyle\\sum_{j=1}^{N}p_{j}(x_{\\ell}-x)^{\\top}\\theta_{j}^{\\ast}+\\varepsilon=\\Delta(x_{\\varepsilon},x)+\\varepsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, ", "page_idx": 47}, {"type": "text", "text": "$\\mathrm{Alt}_{\\Theta}(\\Lambda)$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n=\\left\\{(\\boldsymbol{\\mathcal{X}},\\boldsymbol{\\Theta}^{\\prime},P_{\\theta},\\boldsymbol{\\mathcal{C}}):\\boldsymbol{\\theta}_{j}^{\\prime}=\\boldsymbol{\\theta}_{j}^{*}-\\boldsymbol{\\vartheta}_{j},\\exists\\boldsymbol{x}\\in\\boldsymbol{\\mathcal{X}}\\setminus\\boldsymbol{\\mathcal{X}}_{\\varepsilon},\\sum_{j=1}^{N}p_{j}(x_{\\varepsilon}-x)^{\\top}\\boldsymbol{\\vartheta}_{j}>\\Delta(x_{\\varepsilon},x)+\\varepsilon,\\forall x_{\\varepsilon}\\in\\boldsymbol{\\mathcal{X}}_{\\varepsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Define $\\operatorname{Alt}(\\Lambda)_{\\mathrm{restricted}}\\subset\\operatorname{Alt}_{\\Theta}(\\Lambda)$ , with the additional constraint that $\\vartheta_{1}=\\vartheta_{2}=\\cdot\\cdot=\\vartheta_{N}$ . Then wehave ", "page_idx": 47}, {"type": "text", "text": "Alt(A)restricted ", "page_idx": 47}, {"type": "equation", "text": "$$\n=\\left\\{(\\boldsymbol{\\mathcal{X}},\\boldsymbol{\\Theta}^{\\prime},P_{\\theta},\\boldsymbol{\\mathcal{C}}):\\boldsymbol{\\theta}_{j}^{\\prime}=\\boldsymbol{\\theta}_{j}^{*}-\\boldsymbol{\\vartheta}_{1},\\exists x\\in\\boldsymbol{\\mathcal{X}}\\setminus\\mathcal{X}_{\\varepsilon},\\sum_{j=1}^{N}p_{j}(x_{\\varepsilon}-x)^{\\top}\\boldsymbol{\\vartheta}_{j}>\\Delta(x_{\\varepsilon},x)+\\varepsilon,\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that in stationary linear bandits, the instance can be characterized by the arm set $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and the latent vector $\\bar{\\theta}\\in\\mathbb{R}^{d}$ . Define the alternative instance in linear bandits [32] for the instance $(\\boldsymbol{\\mathcal{X}},\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta)$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Alt}((\\mathcal{X},\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta))_{\\mathrm{stationary}}}\\\\ &{=\\left\\{(\\mathcal{X},\\theta^{\\prime}):\\theta^{\\prime}=\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta-\\vartheta_{1},(x_{\\varepsilon}-x)^{\\top}\\vartheta_{1}>(x_{\\varepsilon}-x)^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}+\\varepsilon,\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\{v_{j}\\in\\Delta_{x}\\}_{j=1}^{N}}{\\operatorname*{max}}\\underset{N^{\\prime}\\in\\mathrm{Att}(\\Lambda)}{\\operatorname*{min}}\\,\\frac{1}{2}\\mathbb{E}[\\tau]\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}v_{j}^{\\top}x x^{\\top}\\vartheta_{j}}\\\\ &{\\leq\\underset{\\{v_{j}\\in\\Delta_{x}\\}_{j=1}^{N}}{\\operatorname*{max}}\\Lambda^{\\prime}\\epsilon\\mathrm{Att}(\\Lambda)\\mathrm{rentictictic}}\\end{array}}\\\\ &{\\overset{(a)}{=}\\underset{\\bar{v}\\in\\Delta_{x}}{\\operatorname*{max}}\\underset{N^{\\prime}\\in\\mathrm{Att}(\\Lambda)_{\\mathrm{rentictic}}}\\frac{1}{2}\\mathbb{E}[\\tau]\\vartheta_{1}^{\\top}\\left(\\underset{x\\in\\mathcal{X}}{\\sum_{j=1}^{N}}p_{j}v_{j,x}\\right)x x^{\\top}\\Bigg)\\vartheta_{1}}\\\\ &{=\\underset{\\bar{v}\\in\\Delta_{x}}{\\operatorname*{max}}\\Lambda^{\\prime}\\epsilon\\mathrm{Att}(\\Lambda)\\mathrm{renticticed}\\overset{1}{2}\\mathbb{E}[\\tau]\\vartheta_{1}^{\\top}\\left(\\underset{x\\in\\mathcal{X}}{\\sum_{i}}\\bar{v_{i}}x x^{\\top}\\right)\\vartheta_{1}}\\\\ &{=\\underset{\\bar{v}\\in\\Delta_{x}}{\\operatorname*{max}}\\Lambda^{\\prime}\\epsilon\\mathrm{Att}(\\Lambda)\\mathrm{rension}}\\underset{0\\leq t\\leq N}{\\operatorname*{max}}{\\operatorname*{max}}\\frac{1}{2}\\mathbb{E}[\\tau]\\vartheta_{1}^{\\top}\\left(\\underset{x\\in\\mathcal{X}}{\\sum_{i}}\\bar{v_{i}}x x^{\\top}\\right)\\vartheta_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where in $(a)$ wedenote $\\begin{array}{r}{\\bar{v}:=\\sum_{j=1}^{N}p_{j}v_{j}}\\end{array}$ as a mixture of $\\{v_{j}\\}_{j=1}^{N}$ . In other words, the max-min problem becomes the one for the $\\varepsilon$ -best arm identification problem in stationary linear bandits. According to [32], the solution to the last optimization problem above is in general intractable. ", "page_idx": 48}, {"type": "text", "text": "However, the optimization problem can be simplified under some simple cases, e.g., the set of $\\varepsilon$ -best arm is a singleton [32]. In the next subsection, a lower bound for the original problem with $\\textstyle{\\mathcal{X}}_{\\varepsilon}$ being a singleton will be derived. ", "page_idx": 48}, {"type": "text", "text": "M.1.2  A Simple Case: $|\\mathcal{X}_{\\varepsilon}|=1$ ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Assume that the set of $\\varepsilon$ -best arm is a singleton, i.e., $\\mathcal{X}_{\\varepsilon}~=~\\{x^{*}\\}$ , we will solve the original optimization problem: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda^{\\prime}\\in\\mathrm{Alt}_{\\Theta}(\\Lambda)}\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "we extend the procedures in [1] to the piecewise-stationary setup. ", "page_idx": 48}, {"type": "text", "text": "Lemma M.4. ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda^{\\prime}\\in\\mathrm{Alte}\\left(\\Lambda\\right)}\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\\geq\\frac{1}{2}\\operatorname*{min}_{x\\neq x^{*}}\\frac{\\left(\\Delta(x^{*},x)\\right)^{2}}{\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})^{-1}}^{2}}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Proof. Note that $\\Lambda^{\\prime}$ differs from $\\Lambda$ in the context matrix $\\Theta$ . By the derivations in (M.12), there exists arm $x\\neq x^{*}$ ,s.t., ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}p_{j}(x^{*}-x)^{\\top}\\vartheta_{j}>\\Delta(x_{\\varepsilon},x)+\\varepsilon\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, the optimization problem becomes ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\boldsymbol{\\vartheta}_{1},\\ldots,\\boldsymbol{\\vartheta}_{N}}\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\sum_{\\boldsymbol{x}\\in\\mathcal{X}}v_{j,\\boldsymbol{x}}\\vartheta_{j}^{\\top}\\boldsymbol{x}\\boldsymbol{x}^{\\top}\\boldsymbol{\\vartheta}_{j}}\\\\ {\\mathrm{~s.t.~}}&{\\displaystyle\\exists x\\in\\mathcal{X}\\setminus\\{x^{*}\\},\\displaystyle\\sum_{j=1}^{N}p_{j}(x^{*}-x)^{\\top}\\boldsymbol{\\vartheta}_{j}\\geq\\Delta(x^{*},\\boldsymbol{x})+\\varepsilon+a=:\\Delta_{a+\\varepsilon}(x^{*},\\boldsymbol{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $a>0$ . The Lagrangian function for this problem is ", "page_idx": 49}, {"type": "equation", "text": "$$\nL(\\vartheta_{1},\\ldots,\\vartheta_{N},\\lambda)=\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}+\\lambda(-\\sum_{j=1}^{N}p_{j}(x^{*}-x)^{\\top}\\vartheta_{j}+\\Delta_{a+\\varepsilon}(x^{*},x))\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial L}{\\partial\\vartheta_{j}}=0\\,\\,\\Rightarrow\\,\\,p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}x x x^{\\top}\\vartheta_{j}-\\lambda p_{j}(x^{*}-x)=0\\,\\,\\Rightarrow\\,\\,A(v_{j})^{1/2}\\vartheta_{j}=\\lambda A(v_{j})^{-1/2}(x^{*}-x)}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial\\lambda}=0\\,\\,\\Rightarrow\\,\\,\\displaystyle\\sum_{j=1}^{N}p_{j}(x^{*}-x)^{\\top}\\vartheta_{j}=\\Delta_{a+\\varepsilon}(x^{*},x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This indicates that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j}x_{j}^{\\top}x x^{\\top}\\vartheta_{j}=\\displaystyle\\sum_{j=1}^{N}p_{j}\\Vert\\vartheta_{j}\\Vert_{A(v_{j})}^{2}}&{}\\\\ &{\\ge\\displaystyle\\frac{\\left(\\displaystyle\\sum_{j=1}^{N}p_{j}\\Vert\\vartheta_{j}\\Vert_{A(v_{j})}\\Vert x^{*}-x\\Vert_{A(v_{j})}-1\\right)^{2}}{\\displaystyle\\sum_{j=1}^{N}w_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})}^{2}-1}}\\\\ &{=\\displaystyle\\frac{\\left(\\displaystyle\\sum_{j=1}^{N}p_{j}\\vartheta_{j}^{\\top}(x^{*}-x)\\right)^{2}}{\\displaystyle\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})}^{2}-1}}\\\\ &{=\\displaystyle\\frac{\\left(\\Delta_{a\\varepsilon}\\varepsilon(x^{*},x)\\right)^{2}}{\\displaystyle\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})}^{2}-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Let $a\\to0$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}p_{j}\\sum_{x\\in\\mathcal{X}}v_{j,x}\\vartheta_{j}^{\\top}x x^{\\top}\\vartheta_{j}\\geq\\frac{(\\Delta(x^{*},x)+\\varepsilon)^{2}}{\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})^{-1}}^{2}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Due to (M.13), we only require there exists $x\\neq x^{*}$ , such that the constraint is satisfied, therefore, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{j=1}^{N}p_{j}\\|\\vartheta_{j}\\|_{A(v_{j})}^{2}\\geq\\frac{1}{2}\\operatorname*{min}_{x\\neq x^{*}}\\frac{(\\Delta(x^{*},x)+\\varepsilon)^{2}}{\\sum_{j=1}^{N}p_{j}\\|x^{*}-x\\|_{A(v_{j})^{-1}}^{2}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By Lemma M.4, the stopping time can be lower bounded as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]\\ge2\\log\\frac{1}{2.4\\delta}\\operatorname*{min}_{\\{v_{j}\\in\\Delta_{x}\\}_{j=1}^{N}}\\operatorname*{max}_{x\\ne x^{*}}\\frac{\\sum_{j=1}^{N}p_{j}\\Vert x^{*}-x\\Vert_{A(v_{j})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This lower bound indicates that, (1) a good algorithm should actively detects and makes use of the contextual information to facilitate the arm identification process. (2) our lower bound extends the result of [16] to the linear bandits case. ", "page_idx": 49}, {"type": "text", "text": "The above lower bound can be further lower bounded if we restrict $v_{1}=\\ldots=v_{N}$ ", "page_idx": 49}, {"type": "text", "text": "Lemma M.5. Let $\\mathrm{SPD}(d):=\\{A:A\\in\\mathbb{R}^{d\\times d},A>0\\}$ denote the set of SPD matrices of dimension $d\\times d$ Given any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , define the function $f:\\mathrm{SPD}(d)\\rightarrow\\mathbb{R}$ $f(A)=x^{\\top}A^{-1}x,$ then $f$ is convex. ", "page_idx": 49}, {"type": "text", "text": "Proof of Lemma M.5. Given any $A,B\\in\\mathrm{SPD}(d)$ , define ", "page_idx": 49}, {"type": "equation", "text": "$$\ng:\\{t\\in\\mathbb{R}:A+t B\\in\\mathrm{SPD}(d)\\}\\to\\mathbb{R},g(t)=x^{\\top}(A+t B)^{-1}x\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "It is suffice to prove $g$ is convex. ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g^{\\prime}(t)=-x^{\\top}(A+t B)^{-1}B(A+t B)^{-1}x}}\\\\ {{g^{\\prime\\prime}(t)=2x^{\\top}(A+t B)^{-1}B(A+t B)^{-1}B(A+t B)^{-1}x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n=2\\left(x^{\\top}(A+t B)^{-1}B\\right)\\left(A+t B\\right)^{-1}\\left(B(A+t B)^{-1}x\\right)\\geq0\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, $g$ is convex. ", "page_idx": 50}, {"type": "text", "text": "Given Lemma M.5, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{N}p_{j}\\|x^{*}-x\\|_{\\boldsymbol A(v)}^{2}\\cdot1=\\left(x^{*}-x\\right)^{\\top}\\left(\\displaystyle\\sum_{j=1}^{N}p_{j}\\mathcal A(v_{j})^{-1}\\right)\\left(x^{*}-x\\right)}\\\\ {\\displaystyle}&{\\ge(x^{*}-x)^{\\top}\\left(\\displaystyle\\sum_{j=1}^{N}p_{j}\\mathcal A(v_{j})\\right)^{-1}\\left(x^{*}-x\\right)}\\\\ {\\displaystyle}&{=(x^{*}-x)^{\\top}\\left(\\displaystyle\\sum_{j=1}^{N}p_{j}\\displaystyle\\sum_{x\\in\\mathcal A}x\\right)^{\\top}\\left(x^{*}-x\\right)}\\\\ {\\displaystyle}&{=(x^{*}-x)^{\\top}\\left(\\displaystyle\\sum_{j=1}^{N}\\left(\\displaystyle\\sum_{j=1}^{N}p_{j,y,x}\\right)x x^{\\top}\\right)^{-1}\\left(x^{*}-x\\right)}\\\\ {\\displaystyle}&{=(x^{*}-x)^{\\top}\\left(\\displaystyle\\sum_{x\\in\\mathcal A}x\\displaystyle\\left(\\displaystyle\\sum_{j=1}^{N}p_{j,y,x}\\right)x x^{\\top}\\right)^{-1}\\left(x^{*}-x\\right)}\\\\ {\\displaystyle}&{=(x^{*}-x)^{\\top}A(v)^{-1}(x^{*}-x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{v}:=\\sum_{j=1}^{N}p_{j}v_{j}\\in\\Delta_{\\mathcal{X}}}\\end{array}$ and the inequality becomes equality when $v_{1}=\\cdots=v_{n}$ . Thus, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]\\geq2\\log\\frac{1}{2.4\\delta}\\operatorname*{min}_{\\{v_{j}\\in\\Delta_{\\mathcal{X}}\\}_{j=1}^{N}}\\underset{x\\neq x^{*}}{\\operatorname*{max}}\\frac{\\sum_{j=1}^{N}p_{j}\\left\\Vert x^{*}-x\\right\\Vert_{A(v_{j})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}}\\\\ &{\\phantom{\\sum_{j=2}^{N}\\log\\frac{1}{2.4\\delta}}\\frac{\\operatorname*{min}}{v\\in\\Delta_{\\mathcal{X}}}\\operatorname*{max}_{x\\neq x^{*}}\\frac{\\left\\Vert x^{*}-x\\right\\Vert_{A(\\bar{v})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The last term mimics the lower bound in the stationary linear bandits with the latent vector $\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\theta$ ", "page_idx": 50}, {"type": "text", "text": "In addition, if we let $p_{1}\\,=\\,1$ and $p_{j}\\,=\\,0$ for all $j\\neq1$ in the instance, our lower bound can be simplified to the lower bound in stationary linear bandits with latent vector $\\theta_{1}^{*}$ [24] ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]\\geq2\\log\\frac{1}{2.4\\delta}\\operatorname*{min}_{\\{v_{j}\\in\\Delta_{\\mathcal{X}}\\}_{j=1}^{N}}\\displaystyle\\operatorname*{max}_{x\\neq x^{*}}\\frac{\\sum_{j=1}^{N}p_{j}\\|x^{*}-x\\|_{A(v_{j})^{-1}}^{2}}{\\left(\\Delta(x^{*},x)+\\varepsilon\\right)^{2}}}\\\\ &{\\quad=2\\log\\frac{1}{2.4\\delta}\\displaystyle\\operatorname*{min}_{v_{1}\\in\\Delta_{\\mathcal{X}}}\\operatorname*{max}_{x\\neq x^{*}}\\frac{\\|x^{*}-x\\|_{A(v_{1})^{-1}}^{2}}{\\left(\\Delta_{1}(x^{*},x)+\\varepsilon\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "M.2  Lower Bound on the Number of changepoints ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this section, we consider an even easier problem: all the contextual information is known to the agent, except for the distribution $P_{\\theta}$ . The dynamics is displayed in Dynamics 3. ", "page_idx": 50}, {"type": "text", "text": "Dynamics 3 Dynamics for an easier problem   \n1: The instance: $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$   \n2: while the algorithm does not stop at time step $t$ do   \n3:if $t\\in\\mathcal{C}$ then   \n4: The agent acknowledges $t\\in\\mathcal{C}$   \n5: The environment samples $\\overline{{\\theta_{j_{t}}^{*}\\sim P_{\\theta}}}$   \n6: else   \n7: $\\theta_{j_{t}}^{*}=\\theta_{j_{t-1}}^{*}$ (thenvironent ds notchange)   \n8: end if   \n9: The agent observes $\\theta_{j_{t}}^{*}$   \n10: end while   \n11: Recommend an $\\varepsilon$ -best arm $\\hat{x}_{\\varepsilon}$ ", "page_idx": 50}, {"type": "text", "text": "We are going to consider the alternative instances with respect to the distribution on $\\theta$ ,i.e., $\\Lambda^{\\prime}=$ $(\\mathcal{X},\\Theta,P_{\\theta}^{\\prime},\\mathcal{C})$ . where $\\exists x\\,\\in\\,\\mathcal{X}\\,\\backslash\\,\\mathcal{X}_{\\varepsilon},\\forall x_{\\varepsilon}\\,\\in\\,\\mathcal{X}_{\\varepsilon},s.t.,x_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}^{\\prime}}^{\\star}\\,<\\,x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}^{\\prime}}-\\varepsilon$ . Denote the set of all the alternative instance (w.r.t. $\\Lambda$ as $\\mathrm{Alt}_{P}(\\boldsymbol{\\Lambda})$ . According to the Pinsker's inequality, let $\\begin{array}{r}{\\mathcal{E}:=\\left\\{\\begin{array}{l l}{\\begin{array}{r l r l}\\end{array}}\\end{array}\\right.}\\end{array}$ the recommended arm $\\hat{x}_{\\varepsilon}\\notin\\mathcal{X}_{\\varepsilon}\\}$ , then for any $\\delta$ PAC algorithm $\\pi$ \uff0c ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{P_{\\pi\\mathbb{A}}[\\varepsilon]+P_{\\pi\\Lambda^{\\prime}}[\\varepsilon^{\\prime}]\\geq\\frac{1}{2}\\exp\\left(-\\mathrm{KL}(P_{\\pi\\Lambda},P_{\\pi\\Lambda^{\\prime}})\\right),}&{\\forall\\Lambda^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}\\\\ &{\\Rightarrow}&{4\\delta\\geq\\exp\\left(-\\mathrm{KL}(P_{\\pi\\Lambda},P_{\\pi\\Lambda^{\\prime}})\\right),\\quad\\forall\\Lambda^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}\\\\ &{\\Rightarrow}&{\\mathrm{KL}(P_{\\pi\\Lambda},P_{\\pi\\Lambda^{\\prime}})\\geq\\ln\\frac{1}{4\\delta},\\quad\\forall\\Lambda^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}\\\\ &{\\Rightarrow}&{\\underset{N^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}{\\operatorname*{min}}\\mathrm{KL}(P_{\\pi\\Lambda},P_{\\pi\\Lambda^{\\prime}})\\geq\\ln\\frac{1}{4\\delta}}\\\\ &{\\Rightarrow}&{\\mathbb{E}[l_{l}]\\underset{N^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}{\\operatorname*{min}}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{\\prime})\\geq\\ln\\frac{1}{4\\delta}}\\\\ &{\\Rightarrow}&{\\mathbb{E}[l_{l}]\\geq\\frac{1}{\\operatorname*{min}_{N^{\\prime}\\in\\mathrm{Alt}P(\\Lambda)}}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{\\prime})\\ln\\frac{1}{4\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We will give an upper bound on the denominator. Note that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda^{\\prime}\\in\\mathrm{Alt}_{P}(\\Lambda)}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{\\prime})=\\operatorname*{min}_{x\\in\\mathcal{X}\\backslash\\mathcal{X}_{\\varepsilon}}\\operatorname*{min}_{\\Lambda_{x}}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{x})\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\Lambda_{x}$ is an alternative instance with distribution $P_{\\theta}^{x}$ s.t. $\\begin{array}{r}{x^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}^{x}}\\theta-\\varepsilon>x_{\\varepsilon}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}^{x}}\\theta,\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\end{array}$ Given any $x\\notin\\mathcal{X}_{\\varepsilon}$ , we denote the shorthand notation $q_{j}=P_{\\theta}^{x}[\\theta_{j}^{*}]$ .We have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}q_{j}\\Delta_{j}(x,x_{\\varepsilon})\\geq\\varepsilon+a,\\;\\mathrm{for}\\;\\mathrm{any}\\;a>0,x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Fix $a>0$ which is sufficiently small, we have the following optimization problem: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{q\\in\\Delta_{N}}\\sum_{j=1}^{N}p_{j}\\ln\\frac{p_{j}}{q_{j}}}}\\\\ &{}&\\\\ {s.t.\\,\\sum_{j=1}^{N}q_{j}=1}\\\\ &{}&\\\\ &{}&{\\sum_{j=1}^{N}q_{j}\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon+a\\leq0,\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "If such alternative distribution $q$ exists, let $L$ denote the augmented Lagrangian function ", "page_idx": 51}, {"type": "equation", "text": "$$\nL(q,\\lambda,\\{\\lambda_{x_{\\varepsilon}}\\}_{x_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}})=\\sum_{j=1}^{N}p_{j}\\ln\\frac{p_{j}}{q_{j}}+\\lambda\\Bigg(\\sum_{j=1}^{N}q_{j}-1\\Bigg)+\\sum_{x_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(q_{j}\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon+a)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By the KKT conditions, we have: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial q_{j}}=\\frac{-p_{j}}{q_{j}}+\\lambda+\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}\\Delta_{j}(x_{\\varepsilon},x)=0,\\;\\forall j\\in[N]}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial\\lambda}=\\sum_{j=1}^{N}q_{j}-1=0}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial\\lambda_{x_{\\varepsilon}}}=\\sum_{j=1}^{N}q_{j}\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon+a=0,\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "or $\\lambda_{x_{\\varepsilon}}\\,=\\,0$ forsome $x_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}$ , which indicates some conditions are not satisfied. The equations abovegive ", "page_idx": 51}, {"type": "equation", "text": "$$\nq_{j}=\\frac{p_{j}}{1+\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon+a)},\\;\\forall j\\in[N]\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By solving ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}\\frac{p_{j}}{1+\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon+a)}-1=0\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "which is a polynomial with $N-1$ degree. Thus, an explicit solution for any instance is not applicable. However, there are some cases where we can get an estimate of $\\lambda_{x_{\\varepsilon}}$ ", "page_idx": 52}, {"type": "text", "text": "Lemma M.6. Denote $j_{0}\\;=\\;\\arg\\operatorname*{min}_{j\\in[N]}\\Delta_{j}(x^{*},x)$ and $j_{1}\\,=\\,\\arg\\operatorname*{max}_{j\\in[N]}\\Delta_{j}\\bigl(x^{*},x\\bigr)$ When $|\\mathcal{X}_{\\varepsilon}|=1,\\,-\\Delta_{j_{0}}(x^{*},x)-2\\varepsilon>\\Delta_{j_{1}}(x^{*},x)>\\varepsilon$ and $\\begin{array}{r}{\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)^{3}\\leq0,}\\end{array}$ the solution to (M.20) is upper bounded by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\lambda_{i^{*}}\\leq\\frac{\\Delta(x^{*},x)+\\varepsilon+a}{\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)^{2}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof of Lemma M.6. When the $\\varepsilon$ -best arm set is a singleton, (M.20) becomes (where $a$ is sufficiently small) ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{N}\\frac{p_{j}}{1+\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)}-1=0\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "As (M.19) is a probability distribution, we need $1+\\lambda_{x^{*}}(\\Delta_{j_{0}}(x^{*},x)+\\varepsilon+a)\\geq0$ , which indicates $\\begin{array}{r}{\\lambda_{x^{*}}\\leq\\frac{-1}{\\Delta_{j}(x^{*},x)+\\varepsilon+a}}\\end{array}$ By the condion on $j_{0}$ and $j_{1}$ $\\dot{\\mathfrak{h}},0<\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)<1,\\forall j\\in[N]$ ", "page_idx": 52}, {"type": "text", "text": "Therefore, by using $\\textstyle{\\frac{1}{1+x}}\\,=\\,1\\,-\\,x\\,+\\,x^{2}\\,-\\,x^{3}\\,+\\,{\\frac{x^{4}}{1+x}}$ for $|x|<1$ we can expand the equation as follows ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle1=\\sum_{j=1}^{N}\\frac{p_{j}}{1+\\lambda_{j}+(\\Delta_{j}(x^{*},x)+\\varepsilon+a)}}&{}\\\\ {\\displaystyle}&{\\geq\\sum_{j=1}^{N}p_{j}\\left((1-\\lambda_{\\varepsilon^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)+(\\lambda_{\\varepsilon^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a))^{2}\\right.}\\\\ &{\\left.\\quad-(\\lambda_{\\varepsilon^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a))^{3})\\right)}\\\\ {\\displaystyle}&{\\Leftrightarrow}&{0\\ge-\\lambda_{\\varepsilon^{*}}^{2}\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a^{3})^{3}+\\lambda_{\\varepsilon^{*}}\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)^{3}}\\\\ &{\\quad-\\displaystyle\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)}\\\\ {\\displaystyle}&{\\Rightarrow\\ \\lambda_{\\varepsilon^{*}}\\leq\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)^{3}=\\frac{\\Delta(x^{*},x)+\\varepsilon+a}{\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In general, when we get the $\\{\\lambda_{x_{\\varepsilon}}\\}_{x_{\\varepsilon}\\in\\mathcal X_{\\varepsilon}}$ and plug it in (M.19), the alternative distribution $q$ is obtained. Finally, we let $a\\to0$ ", "page_idx": 52}, {"type": "text", "text": "This gives the lower bound for the number of changepoints that need to be observed. A coarse estimation of the KL divergence without solving (M.20) can be done as follows ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{N}p_{j}\\ln\\frac{p_{j}}{q_{j}}=\\displaystyle\\sum_{j=1}^{N}p_{j}\\ln\\left(1+\\displaystyle\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{j=1}^{N}p_{j}\\displaystyle\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta_{j}(x_{\\varepsilon},x)+\\varepsilon)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta(x_{\\varepsilon},x)+\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Note that the solution of $\\lambda_{x_{\\varepsilon}}$ dependson $\\Delta(x_{\\varepsilon},x)$ (e.g. Lemma M.6), so the final solution is of order $(\\Delta(x_{\\varepsilon},x)+\\varepsilon)^{2}$ for a given $x\\notin\\mathcal{X}$ . This is the solution to the inside minimization problem in (M.17). The final lower bound will be ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Lambda^{\\prime}\\in\\mathrm{Alt}_{P}(\\Lambda)}{\\mathrm{min}}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{\\prime})=\\underset{x\\in\\mathcal{X}\\backslash\\mathcal{X}_{\\varepsilon}}{\\mathrm{min}}\\underset{\\Lambda_{x}}{\\mathrm{min}}\\,\\mathrm{KL}(P_{\\theta},P_{\\theta}^{x})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{x\\not\\in\\mathcal{X}_{\\varepsilon}}{\\mathrm{min}}\\underset{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}{\\sum}\\,\\lambda_{x_{\\varepsilon}}(\\Delta(x_{\\varepsilon},x)+\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The lower bound is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}[l_{t}]\\ge\\operatorname*{max}_{x\\notin\\mathcal{X}_{\\varepsilon}}\\frac{1}{\\sum_{x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}}\\lambda_{x_{\\varepsilon}}(\\Delta(x_{\\varepsilon},x)+\\varepsilon)}\\ln\\frac{1}{4\\delta}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\lambda_{x_{\\varepsilon}}$ is the solution to (M.20) ", "page_idx": 53}, {"type": "text", "text": "With the setup in Lemma M.6, we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Lambda^{\\prime}\\in\\mathrm{Alt}_{P}(\\Lambda)}{\\mathrm{min}}\\mathrm{KL}(P_{\\theta},P_{\\theta}^{\\prime})=\\underset{x\\in\\mathcal{X}\\backslash\\mathcal{X}_{\\varepsilon}}{\\mathrm{min}}\\underset{\\Lambda_{x}}{\\mathrm{min}}\\,\\mathrm{KL}(P_{\\theta},P_{\\theta}^{x})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underset{x\\neq x_{\\varepsilon}}{\\mathrm{min}}\\,\\frac{(\\Delta(x^{*},x)+\\varepsilon)^{2}}{\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and the lower bound is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}[l_{t}]\\ge\\operatorname*{max}_{x\\ne x_{\\varepsilon}}\\frac{\\sum_{j=1}^{N}p_{j}(\\Delta_{j}(x^{*},x)+\\varepsilon)^{2}}{(\\Delta(x^{*},x)+\\varepsilon)^{2}}\\ln\\frac{1}{4\\delta}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Remark M.7. We give some comments on the existence and uniqueness of the solution to (M.20). ", "page_idx": 53}, {"type": "text", "text": "Existence: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 It is possible that (M.20) does not have a solution. For instance, consider a three-arm instance: $\\mathbf{\\Psi}_{x_{(1)}}=(1,0.5),x_{(2)}=(0.5,1),x_{(3)}=(0.6,0.6),\\theta_{1}^{*}=(1,0),\\theta_{2}^{*}=(0,1),P_{\\theta}=(0.5,0.5),\\varepsilon=0$ 0.1. We have $\\boldsymbol{x}_{(1)}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}=\\boldsymbol{x}_{(2)}{}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}=0.75$ and ${x_{(3)}}^{\\top}\\mathbb{E}_{\\theta\\sim P_{\\theta}}\\,=\\,0.6,$ thus $x_{(3)}$ is not an $\\varepsilon$ -best arm.Furthermore,there does not exist an alternative distribution $q$ such that $x_{(3)}$ is the best arm and neither $x_{(1)},x_{(2)}$ .s $\\varepsilon$ -best. Under such case, the lower bound on $\\mathbb{E}[l_{t}]$ .s $0$ (we regard $\\mathrm{min}_{x\\in\\varnothing}\\;f(x)=+\\infty$ by convention).   \n\u00b7 $I t$ is possible that (M.20) does not have a solution and it is unnecessary to estimate $P_{\\theta}$ . For instance, consider a two-arm instance: $x_{(1)}=(1,0.5),x_{(2)}=(0.5,0.1),\\theta_{1}^{*}=(1,0),\\theta_{2}^{*}=(0,1),P_{\\theta}=$ $(0.5,0.5),\\varepsilon=0.1.$ Arm $x_{(1)}$ is better than arm $x_{(2)}$ under all contexts. Therefore, no matter what $P_{\\theta}$ is, arm $x_{(1)}$ is the $\\varepsilon$ -best arm.Under suchcases, theremay exist an algorithm and it is suffcient for it to determine the best arm if the context vectors are well-approximated. There is no need to estimate $P_{\\theta}$   \n\u00b7 $A$ necessary condition for the existence of the solution of (M.20) is: there exists $x\\notin\\mathcal{X}_{\\varepsilon}$ for any $x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}$ there $\\exists j(x_{\\varepsilon})\\,\\in\\,[N],\\,s.t.\\Delta_{j(x_{\\varepsilon})}(x_{\\varepsilon},x)\\,<\\,-\\varepsilon$ In other words, for each $\\varepsilon$ -best arm $x_{\\varepsilon}$ \uff0c thereis at least one context in which the alternative arm $x$ is better than $x_{\\varepsilon}$ by at least $\\varepsilon$ $A$ suffcient condition for the existence of the solution of (M.20) is: there exists $x\\notin\\mathcal{X}_{\\varepsilon}$ and $j\\in[N]$ $s.t.\\Delta_{j}(x_{\\varepsilon},x)<-\\varepsilon_{\\cdot}$ $\\forall x_{\\varepsilon}\\in\\mathcal{X}_{\\varepsilon}$ . In other words, $x$ is better than any $\\varepsilon$ -best arm under context $j$ In the alternative instance, we can lift $P_{\\theta}^{\\prime}[\\theta_{j}^{*}]$ close to 1 so that arm $x$ becomes the $\\varepsilon$ -best arm and $\\mathcal{X}_{\\varepsilon}^{\\prime}\\cap\\mathcal{X}_{\\varepsilon}=\\emptyset$ ", "page_idx": 53}, {"type": "text", "text": "Uniqueness: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The uniqueness of the solution is not guaranteed, as the KKT conditions (M.18) is only a necessary condition for the solution. We need to look for the solution that minimizes the KL divergence. \u00b7 $A$ sufcient condition for the uniqueness of the solution is (if it exists, which indicates $\\exists j\\ \\in$ [N],s.t.\u25b3(x\\*, 2) <-): I& = 1. Specifcally denote f(>x ) ==1 I+>x-(\u25b3(\u00b1,)++a) wehave ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial f}{\\partial\\lambda_{x^{*}}}=\\sum_{j=1}^{N}\\frac{-p_{j}(1+\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a))}{1+\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}f}{\\partial\\lambda_{x^{*}}^{2}}=\\sum_{j=1}^{N}\\frac{2p_{j}(1+\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a))^{2}}{1+\\lambda_{x^{*}}(\\Delta_{j}(x^{*},x)+\\varepsilon+a)}>0\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "$f(0)=1$ \uff0c $\\begin{array}{r}{\\frac{\\partial f}{\\partial\\lambda_{x^{*}}}(0)=-1}\\end{array}$ and $\\frac{\\partial^{2}f}{\\partial\\lambda_{x^{*}}^{2}}>0$ $\\lambda_{x^{*}}>0$", "page_idx": 54}, {"type": "text", "text": "N  More Examples and Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In this section, we firstly provide one more example to illustrate the tightness of our derived upper bound lower bounds, indicating the efficiency of our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm. In addition, we can observe how the upper and lower bounds are affected by the level of piecewise non-stationarity and whether our ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ algorithm can reduce the infuence manifested by $L_{\\mathrm{max}}$ .Afterthat,we present a proof sketch of the results in Corollaries 5.1 and N.1 in Appendix N.1. ", "page_idx": 54}, {"type": "text", "text": "Example 2. Instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ is with ", "page_idx": 54}, {"type": "text", "text": "\u00b7 $d$ arms: $x_{(i)}={\\mathbf{e}}_{i},i\\in[d]$   \n\u00b7 $N\\,=\\,d$ contexts: $\\theta_{1}^{*}\\,=\\,(a,0,0,\\dots,0)^{\\top}$ \uff0c $\\theta_{2}^{*}\\,=\\,(a,b,b,\\ldots,b)^{\\top}\\,-\\,b\\mathbf{e}_{j},\\,j\\,\\geq\\,2,$ where $b>a>$ $\\varepsilon,b-a>\\varepsilon$   \n. Context distribution: $p_{j}=p,j\\ge2$ and $p_{1}=1-(N-1)p$ where $\\begin{array}{r}{p\\in(0,\\frac{a-\\varepsilon}{(N-2)b})}\\end{array}$ ", "page_idx": 54}, {"type": "text", "text": "Under Example 2, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(x_{(1)},x_{(i)})=(1-(N-2)p)\\cdot a-(N-2)p\\cdot(b-a)>\\varepsilon}\\\\ &{\\Delta_{j}(x_{(1)},x_{(i)})=-b+a<-\\varepsilon,\\quad i\\geq2,j\\neq1,i}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Thus, (1) $x_{(1)}$ is the unique $\\varepsilon$ -best arm; (2) $\\{x_{(i)}\\}_{i\\ge2}$ are equivalent, and $\\Delta_{\\operatorname*{min}}:=\\Delta(x_{(1)},x_{(i)})$ (3) for any $i\\geq2$ \uff0c $x_{(i)}$ can be an $\\varepsilon$ -optimal arm under some alternative distributions. ", "page_idx": 54}, {"type": "text", "text": "Corollary N.1. Firstly, for the instances defined in Example 2, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{H_{DE}}\\leq{\\frac{16(N-2)L_{\\operatorname*{max}}}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}}\\left((a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and the sample complexity of the ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is tight up to $(N L_{\\mathrm{max}}/L_{\\mathrm{min}})$ and logarithmic factors. We also further observe some specific instances: $(i)$ when $p\\to0^{+}$ with $\\begin{array}{r}{\\Delta_{\\mathrm{min}}=\\operatorname*{min}_{x\\neq x^{*}}\\Delta(x^{*},x)}\\end{array}$ we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\tilde{O}\\left(\\operatorname*{min}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon},\\:\\frac{L_{\\operatorname*{max}}+d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\bigcap\\Omega\\left\\{\\operatorname*{max}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}},\\frac{L_{\\operatorname*{min}}\\left(b-a-\\varepsilon\\right)}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "(i) When $\\begin{array}{r}{p\\rightarrow\\left(\\frac{a-\\varepsilon)}{(N-2)b}\\right)^{-}}\\end{array}$ and $(a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}=\\Omega(1),$ we have $\\begin{array}{r}{\\mathrm{H}_{\\mathrm{DE}}=\\frac{N L_{\\mathrm{max}}}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}}\\end{array}$ and ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\tilde{O}\\bigg(\\mathrm{min}\\left\\{\\mathrm{H}_{\\mathrm{DE}},\\frac{d+L_{\\mathrm{max}}}{\\varepsilon^{2}}\\right\\}\\bigg)\\bigcap\\Omega\\left(\\frac{d+L_{\\mathrm{min}}}{\\varepsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "The upper bounds are achieved by the ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI^{+}}}$ algorithm. ", "page_idx": 54}, {"type": "text", "text": "We can observe from Corollary N.1 that ", "page_idx": 54}, {"type": "text", "text": "In ae (i) hen $p\\to0^{+}$ \uff0c $\\begin{array}{r}{p_{1}\\to1-\\frac{(N-1)(a-\\varepsilon)}{(N-2)b}}\\end{array}$ and $\\begin{array}{r}{p_{j}\\to\\left(\\frac{a-\\varepsilon)}{(N-2)b}\\right)}\\end{array}$ to be non-stationary and $\\Delta_{\\operatorname*{min}}\\to\\varepsilon$ . We will obtain ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}\\tau}{\\ln(1/\\delta)}\\in\\tilde{\\Theta}\\left(\\frac{d}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "indicating that our algorithm can also reduce the impact of $L_{\\mathrm{max}}$ ", "page_idx": 54}, {"type": "text", "text": "\u00b7 In case (i), the upper and lower bounds are with the same order, and the difference is solely manifested by a additive term $L_{\\mathrm{max}}-L_{\\mathrm{min}}$ ,suggesting that ${\\sf P S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ is near optimal and again, PSeBAI+ mitigates the impact of Lmax\\* ", "page_idx": 55}, {"type": "text", "text": "N.1  Analysis of examples", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Recall the instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ in Example 1: ", "page_idx": 55}, {"type": "text", "text": "Example 1. Instance $\\Lambda=(\\mathcal{X},\\Theta,P_{\\theta},\\mathcal{C})$ is with (i) $2d-1$ arms: $x_{(1)}={\\mathbf{e}}_{1},x_{(i)}={\\mathbf{e}}_{i},x_{(d+i-1)}=$ $\\mathbf{e}_{1}$ COS $\\cdot\\phi{+}\\,\\mathbf{e}_{i}\\sin\\phi$ for all $i\\in\\{2,\\ldots,d\\}$ where $\\phi\\in[0,\\pi/4)$ ,(i) $2d-2$ contexts: $\\theta_{j\\pm}^{\\ast}={\\bf e}_{1}\\cos\\phi\\pm$ $\\mathbf{e}_{j+1}\\sin\\phi$ for all $j\\in[d-1]$ , (ii) Context distribution: $p_{j}=1/N$ for all $j\\in[N]$ ", "page_idx": 55}, {"type": "text", "text": "Corollary 5.1. For the instance defined in Example $^{\\,l}$ wehave $\\mathrm{H}_{\\mathrm{DE}}(x_{\\varepsilon},x)=\\tilde{O}(N L_{\\mathrm{max}})$ forall $(x_{\\varepsilon},x)\\in\\dot{\\mathcal{X}}_{\\varepsilon}\\times(\\mathcal{X}\\setminus\\mathcal{X}_{\\varepsilon})$ .In addition, $i f\\varepsilon<(\\cos\\phi)(1-\\cos\\phi)$ ,we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\ \\tilde{\\Theta}\\bigg((1\\!+\\!f(\\phi))\\cdot\\frac{d}{(\\Delta_{x_{(1)},x_{(d+1)}}\\!+\\!\\varepsilon)^{2}}\\bigg),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\mathbb{E}[\\tau]^{*}$ is the minimal expected sample complexity over all $(\\varepsilon,\\delta)$ -PAC algorithms and $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies $f(\\phi)\\rightarrow0$ as $\\phi\\to0^{+}$ . The upper bound in (5.1) is achieved by $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ ", "page_idx": 55}, {"type": "text", "text": "Proof of Corollary 5.1. As $\\mu_{x_{(1)}}=\\cos\\phi$ $\\mu_{x_{(i)}}=0,\\mu_{x_{(d+i)}}=\\cos^{2}\\phi$ for $i=2,\\ldots,d$ and $\\varepsilon\\in(1-$ $\\cos\\phi,\\cos\\phi)$ \uff0c $x_{(1)}$ is the best arm and $x_{(i)},i=2,\\ldots,d$ are not $\\varepsilon$ -best arms and $x_{(d+i)},i=2,\\ldots,d$ are $\\varepsilon$ -best arms. $\\Delta_{\\mathrm{min}}=\\cos\\phi-\\cos^{2}\\phi$ ", "page_idx": 55}, {"type": "text", "text": "When $\\varepsilon<\\cos\\phi-\\cos^{2}\\phi=\\Delta_{\\mathrm{min}}.$ $x_{(1)}$ is the unique $\\varepsilon$ -best arm. By solving (M.14), we see that there exists a continuous function $f(\\phi)$ such that $f(\\phi)\\rightarrow0$ as $\\phi\\rightarrow0$ and we can get the lower bound for the VE term as ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\Omega\\left((1+f(\\phi))\\cdot{\\frac{d}{(\\Delta(x_{(1)},x_{(d+1)})+\\varepsilon)^{2}}}\\ln{\\frac{1}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "As $(x_{(1)}-x_{(i)})^{\\top}\\theta_{j\\pm}^{*}>0$ for $i\\,=\\,2,\\ldots,d,j\\,\\in\\,[d-1]$ $x_{(i)}$ cannot be an $\\varepsilon$ -best arm under any alternative distribution, we only need to consider $x_{(d+i)},i=2,\\ldots,d$ , which are equivalent. Given any $x_{(d+i)}$ , by solving (M.20), we can upper bound ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\lambda_{x_{(1)}}\\leq\\frac{\\Delta_{\\operatorname*{min}}+\\varepsilon+\\sin^{2}\\phi}{-(\\Delta_{i+}(x_{(1)},x_{(d+i)})+\\varepsilon)(\\Delta_{i-}(x_{(1)},x_{(d+i)})+\\varepsilon)},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "and thus the number of change points or context samples can be lower bounded as ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[l_{\\tau}]\\geq\\frac{-\\big(\\Delta_{i+}\\big(x_{(1)},x_{(d+i)}\\big)-\\varepsilon\\big)\\big(\\Delta_{i-}\\big(x_{(1)},x_{(d+i)}\\big)+\\varepsilon\\big)}{\\big(\\Delta_{\\operatorname*{min}}+\\varepsilon+\\sin^{2}\\phi\\big)\\big(\\Delta_{\\operatorname*{min}}+\\varepsilon\\big)}}\\\\ &{\\qquad=\\frac{\\big(1-\\cos\\phi-\\varepsilon\\big)\\big(1+\\cos\\phi-2\\cos^{2}\\phi+\\varepsilon\\big)}{\\big(\\Delta_{\\operatorname*{min}}+\\varepsilon+\\sin^{2}\\phi\\big)\\big(\\Delta_{\\operatorname*{min}}+\\varepsilon\\big)}=\\frac{1-\\cos\\phi-\\varepsilon}{\\Delta_{\\operatorname*{min}}+\\varepsilon}=O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "According to Theorem 3.3, the upper bound is ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\cal O}\\left(\\frac{d}{(\\Delta(x_{(1)},x_{(d+1)})+\\varepsilon)^{2}}\\ln\\frac1\\delta+\\mathrm{H}_{\\mathrm{DE}}(\\Delta(x_{(1)},x_{(d+1)})\\ln\\frac1\\delta+\\frac{N L_{\\operatorname*{max}}}{\\Delta(x_{(1)},x_{(d+1)})+\\varepsilon}\\ln\\frac1\\delta\\right)}\\\\ &{=\\tilde{\\cal O}\\left(\\frac{d}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac1\\delta+\\mathrm{H}_{\\mathrm{DE}}(\\Delta(x_{(1)},x_{(d+1)})\\ln\\frac1\\delta+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\frac1\\delta\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mathrm{DE}}(\\Delta(x_{(1)},x_{(d+1)})=\\left\\{\\frac{16N L_{\\mathrm{max}},}{16N L_{\\mathrm{max}}(\\Delta_{\\mathrm{min}}+\\varepsilon+2(1-\\cos\\phi)/N)^{2}},\\right.\\ \\varepsilon<1-\\cos\\phi<\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "As $\\Delta_{\\operatorname*{min}}~=~\\cos\\phi\\,-\\,\\cos^{2}\\phi~=~\\cos\\phi(1\\,-\\,\\cos\\phi),$ thus $\\textstyle1\\,-\\,\\cos\\phi\\ =\\ \\frac{\\Delta_{\\operatorname*{min}}}{\\cos\\phi}\\ \\leq\\ \\sqrt{2}\\Delta_{\\operatorname*{min}}$ , and $\\mathrm{H}_{\\mathrm{DE}}(\\Delta(x_{(1)},x_{(d+1)})<144N L_{\\mathrm{max}}$ for any choice of $\\varepsilon$ ", "page_idx": 55}, {"type": "text", "text": "Hence, the upper bound is ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]=\\tilde{O}\\left(\\frac{d}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}+N L_{\\operatorname*{max}}\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\frac{1}{\\delta}\\right)}\\\\ &{\\quad\\quad=\\tilde{O}\\left(\\frac{d}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "As $\\phi\\rightarrow0$ , the first term (the VE term) dominates and it matches the lower bound in (N.1). Therefore, the upper bound is asymptotically tight up to logarithmic terms. \u53e3 ", "page_idx": 56}, {"type": "text", "text": "Corollary N.1. Firstly, for the instances defined in Example 2, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathrm{H_{DE}}\\leq\\frac{16(N-2)L_{\\operatorname*{max}}}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\Bigg((a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}\\Bigg),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and the sample complexity of the ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is tight up to $(N L_{\\mathrm{max}}/L_{\\mathrm{min}})$ and logarithmic factors. We also further observe some specific instances: $(i)$ when $p\\to0^{+}$ with $\\begin{array}{r}{\\Delta_{\\operatorname*{min}}=\\operatorname*{min}_{x\\neq x^{*}}\\Delta(x^{*},x)}\\end{array}$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\tilde{O}\\left(\\operatorname*{min}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon},\\:\\frac{L_{\\operatorname*{max}}+d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\bigcap\\Omega\\left\\{\\operatorname*{max}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}},\\frac{L_{\\operatorname*{min}}\\left(b-a-\\varepsilon\\right)}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\right\\}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "(i) When p \u2192(=2)) )and (a + e)2 + (b-\u03b1 - e)\u00b2 = S(1), we have HpE = $\\begin{array}{r}{\\mathrm{H}_{\\mathrm{DE}}=\\frac{N L_{\\mathrm{max}}}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}}\\end{array}$ and ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[\\tau]^{*}}{\\ln(1/\\delta)}\\in\\tilde{O}\\bigg(\\mathrm{min}\\left\\{\\mathrm{H}_{\\mathrm{DE}},\\frac{d+L_{\\mathrm{max}}}{\\varepsilon^{2}}\\right\\}\\bigg)\\bigcap\\Omega\\left(\\frac{d+L_{\\mathrm{min}}}{\\varepsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The upper bounds are achieved by the ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI^{+}}}$ algorithm. ", "page_idx": 56}, {"type": "text", "text": "Proof of Corollary N.1. Under Example 2, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(x_{(1)},x_{(i)})=(1-(N-2)p)\\cdot a-(N-2)p\\cdot(b-a)>\\varepsilon}\\\\ &{\\Delta_{j}(x_{(1)},x_{(i)})=-b+a<-\\varepsilon,\\quad i\\geq2,j\\neq1,i}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus, (1) $x_{(1)}$ is the unique $\\varepsilon$ -best arm; (2) $\\{x_{(i)}\\}_{i\\ge2}$ are equivalent, and $\\Delta_{\\operatorname*{min}}:=\\Delta(x_{(1)},x_{(i)})$ (3) for any $i\\geq2$ \uff0c $x_{(i)}$ can be an $\\varepsilon$ -best arm under some alternative distributions. ", "page_idx": 56}, {"type": "text", "text": "For any $i\\geq2$ by solving M.20) with $x_{\\varepsilon}=x_{(1)},x=x_{(i)}$ we obtain $\\begin{array}{r}{\\lambda_{x_{(1)}}=\\frac{\\Delta(x_{(1)},x_{(i)})+\\varepsilon}{(a+\\varepsilon)(b-a-\\varepsilon)}}\\end{array}$ and the alternative distribution ", "page_idx": 56}, {"type": "equation", "text": "$$\nP_{\\theta}^{\\prime}[\\theta_{j}^{*}]=\\frac{p_{j}}{1+\\lambda_{x_{(1)}}(\\Delta_{j}(x_{(1)},x_{(i)})+\\varepsilon)}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The lower bound on the expected number of changepoints is ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}[l_{\\tau}]\\geq\\frac{(a+\\varepsilon)(b-a-\\varepsilon)}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}\\ln\\frac{4}{\\delta}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus the time complexity is lower bounded by ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]\\geq L_{\\operatorname*{min}}\\cdot\\frac{(a+\\varepsilon)(b-a-\\varepsilon)}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}\\ln\\frac{4}{\\delta}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Furthermore, by solving (M.14), we obtain the lower bound on the expected sample complexity when thecontext index $j_{t}$ is revealed: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\tau]\\geq2\\log\\frac{1}{2.4\\delta}\\operatorname*{min}_{\\bar{v}\\in\\Delta_{K}}\\operatorname*{max}_{i\\neq1}\\frac{\\frac{1}{\\bar{v}_{1}}+\\frac{1}{\\bar{v}_{i}}}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}\\geq2\\log\\frac{1}{2.4\\delta}\\frac{(\\sqrt{d-1}+1)^{2}}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}}\\\\ &{\\qquad\\geq\\frac{d}{\\left(\\Delta(x_{(1)},x_{(i)})+\\varepsilon\\right)^{2}}\\cdot2\\log\\frac{1}{2.4\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We get the lower bound on the expected sample complexity: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]\\geq\\operatorname*{max}\\left\\{\\frac{d}{{(\\Delta_{\\operatorname*{min}}+\\varepsilon)}^{2}}\\cdot2\\log\\frac{1}{2.4\\delta},\\frac{L_{\\operatorname*{min}}(a+\\varepsilon)(b-a-\\varepsilon)}{{(\\Delta_{\\operatorname*{min}}+\\varepsilon)}^{2}}\\ln\\frac{4}{\\delta}\\right\\}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "According to Theorem 3.3, the upper bound on the expected sample complexity is ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\cal O}\\Bigg(\\operatorname*{min}\\left\\{\\displaystyle\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\displaystyle\\frac{1}{\\delta}+\\mathrm{H}_{\\mathrm{DE}}\\ln\\displaystyle\\frac{1}{\\delta}+\\displaystyle\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\displaystyle\\frac{1}{\\delta},\\,\\,\\displaystyle\\frac{L_{\\operatorname*{max}}+d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\displaystyle\\frac{1}{\\delta}\\right\\}\\Bigg)}\\\\ &{\\leq\\tilde{\\cal O}\\Bigg(\\operatorname*{min}\\left\\{\\displaystyle\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\displaystyle\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}\\Big((a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}\\Big)}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\displaystyle\\frac{1}{\\delta}\\right.}\\\\ &{\\qquad\\quad\\left.+\\displaystyle\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\displaystyle\\frac{1}{\\delta},\\,\\,\\displaystyle\\frac{L_{\\operatorname*{max}}+d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\displaystyle\\frac{1}{\\delta}\\right\\}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where we utilize ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H}_{\\mathrm{{DE}}}=\\frac{L_{\\mathrm{max}}}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\Bigg(\\sqrt{\\operatorname*{min}\\bigg\\{16p_{j},\\frac{1}{4}\\bigg\\}}|a+\\varepsilon|+\\sqrt{\\operatorname*{min}\\bigg\\{16p,\\frac{1}{4}\\bigg\\}}|a+\\varepsilon|}\\\\ &{\\mathrm{~\\\\\\}+(N-2)\\sqrt{\\operatorname*{min}\\bigg\\{16p,\\frac{1}{4}\\bigg\\}}|b-a-\\varepsilon|\\Bigg)^{2}}\\\\ &{\\leq\\frac{16\\left(N-2\\right)L_{\\operatorname*{max}}}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\Bigg((a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By comparing the lower bound in (N.2) and the upper bound (N.3), we conclude that ", "page_idx": 57}, {"type": "text", "text": "\u00b7 the sample complexity of ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ is tight up to NLmax and logarithmic factors.   \n\u00b7 When the mean gap is small, the sample complexity of $\\overrightarrow{\\mathrm{PS}}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}^{+}$ is dominated by the former term, i.e., the design of $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$   \n\u00b7 When $p\\,\\rightarrow\\,0^{+}$ , then $p_{1}\\,\\to\\,1,p_{j}\\,\\to\\,0$ for $j~\\ge~2$ , o the instance tends to be stationary and $\\Delta_{\\operatorname*{min}}\\to a$ . The lower bound in (N.2) becomes ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]\\geq\\operatorname*{max}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\cdot2\\log\\frac{1}{2.4\\delta},\\frac{L_{\\operatorname*{min}}(b-a-\\varepsilon)}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\frac{4}{\\delta}\\right\\}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and the upper bound in (N.3) turns into ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\operatorname*{min}\\left\\{\\frac{d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{\\Delta_{\\operatorname*{min}}+\\varepsilon}\\ln\\frac{1}{\\delta},\\ \\frac{L_{\\operatorname*{max}}+d}{\\left(\\Delta_{\\operatorname*{min}}+\\varepsilon\\right)^{2}}\\ln\\frac{1}{\\delta}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "The vector estimation term dominates the sample complexity and our upper bound is tight. ", "page_idx": 57}, {"type": "text", "text": "\u00b7 When $\\begin{array}{r}{p\\rightarrow\\left(\\frac{a-\\varepsilon)}{(N-2)b}\\right)^{-}}\\end{array}$ $\\begin{array}{r}{p_{1}\\to1-\\frac{(N-1)(a-\\varepsilon}{(N-2)b}}\\end{array}$ $\\begin{array}{r}{p_{j}\\to\\left(\\frac{a-\\varepsilon}{(N-2)b}\\right)}\\end{array}$ be non-stationary and $\\Delta_{\\operatorname*{min}}\\to\\varepsilon$ . The lower bound in (N.2) becomes ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau]\\ge\\operatorname*{max}\\left\\{\\frac{d}{4\\varepsilon^{2}}\\cdot2\\log\\frac{1}{2.4\\delta},\\;\\frac{L_{\\operatorname*{min}}(a+\\varepsilon)(b-a-\\varepsilon)}{4\\varepsilon^{2}}\\ln\\frac{4}{\\delta}\\right\\}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and the upper bound in (N.3) turns into ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\tilde{\\cal O}\\left(\\operatorname*{min}\\left\\{\\frac{d}{4\\varepsilon^{2}}\\ln\\frac{1}{\\delta}+\\mathrm{H}_{\\mathrm{DE}}\\cdot\\ln\\frac{1}{\\delta}+\\frac{N L_{\\operatorname*{max}}}{2\\varepsilon}\\ln\\frac{1}{\\delta},\\,\\,\\frac{L_{\\operatorname*{max}}+d}{4\\varepsilon^{2}}\\ln\\frac{1}{\\delta}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\mathrm{H_{DE}}$ is upper bounded by (N.4). ", "page_idx": 57}, {"type": "text", "text": "(1) If ${\\Big(}(a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}{\\Big)}=O(4\\varepsilon^{2}+N L_{\\mathrm{max}}).$ DE is independent of $\\varepsilon$ and VE increases as $\\varepsilon$ decreases, thus the expected sample complexity is dominated by vector estimation term when $\\varepsilon$ is small; (2) If $\\left((a+\\varepsilon)^{2}+(b-a-\\varepsilon)^{2}\\right)\\,=\\,\\Omega(1)$ the expected sample complexity is dominated by ", "page_idx": 57}, {"type": "text", "text": "0 Experimental Details ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "For the computation of the G-optimal allocation, we adopt the Wolfe-Atwood Algorithm with the Kumar-Yildirim start introduced in [33], where the input to the function is the arm set and the output is the G-optimal allocation. All experiments are conducted via MATLAB R2021b on a MacBook Pro with Apple M1 Pro chip and 16 GB memory. ", "page_idx": 58}, {"type": "text", "text": "To shorten the execution time, ", "page_idx": 58}, {"type": "text", "text": "\u00b7 Before $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ stops, $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ and $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ are conducted in parallel (i.e., we run ${\\bf P}{\\bf S}\\varepsilon{\\bf B}{\\bf A}{\\bf I}^{+},$ ) After $\\mathrm{PS}_{\\mathcal{E}\\mathrm{BAI}}$ stops, $\\scriptstyle\\mathrm{N}\\varepsilon\\mathbf{B}\\mathrm{AI}$ continues and is run in a batch manner, i.e., we sample $L_{\\mathrm{min}}$ samples according to the $\\mathrm{G}$ -optimal allocation one time and update the statistics. As the sample complexity of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ is of order at least $10^{7}$ and each stationary segment is of order $10^{4}$ , the effect of this batch sampling procedure can be largely ignored.   \n\u00b7 $\\scriptstyle\\mathrm{D}\\varepsilon\\mathrm{BAI}$ and $\\scriptstyle\\mathbf{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ are both conducted in segments, because the latent context vector can be observed by the two algorithms and the latent vector does not change within a stationary segment.   \n\u00b7 As the experiment in Section 6 illustrates that $\\mathrm{PS}{\\varepsilon}\\mathbf{B}\\mathbf{A}\\mathbf{I}$ outperforms $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ and dominates the ${\\mathrm{PS}}{\\varepsilon}{\\mathrm{BAI}}^{\\bar{+}}$ algorithm, we run $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ instead of ${\\tt P S}{\\tt E B A I^{+}}$ for the addition experiments in Section O.2 and Section O.3. ", "page_idx": 58}, {"type": "text", "text": "To increase the robustness of the algorithm, the window size for LCD is doubled. Note that this will only influence an absolute constant in the sample complexity of the proposed algorithm and the order of the sample complexity remains. ", "page_idx": 58}, {"type": "text", "text": "O.1 Modification of Confidence Radii in $\\mathrm{PS}\\varepsilon$ BAI and PS\u03b5BAI+ ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "During the proof of the upper bound, we have relaxed the absolute constants in the confidence radi to simplify the proof and increase the readability. In the experiments, we utilize the tighter confidence radi to gain better empirical performance. Note that when these tighter confidence radi are utilized by our algorithm, it still enjoys the current theoretical guarantee. The choice of confidence radii are asfollows: ", "page_idx": 58}, {"type": "text", "text": "$\\alpha_{t}$ : according to Lemma I.1 and Lemma E.1, $\\alpha$ can be tightened to be ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\alpha_{t}^{\\mathrm{alg}}=\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}+\\sqrt{\\left(\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}\\right)^{2}+\\frac{4d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}}\\leq5\\sqrt{\\frac{d}{T_{t}}\\ln\\frac{2}{\\delta_{v,T_{t}}}}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "$\\beta_{t,j}$ : according to (I.3) and Lemma K.1, $\\beta_{t,j}$ can be tightened to be ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t,j}^{\\mathrm{alg}}=\\operatorname*{min}\\left\\{\\frac{\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}+\\sqrt{\\left(\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right)^{2}+\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}{T_{t}},\\:1\\right\\},}\\\\ &{\\mathrm{where}\\ \\phi_{t,j}:=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\right\\},\\:\\frac{1}{4}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "$\\xi_{t}$ : instead of using $\\begin{array}{r}{25\\sqrt{2}\\frac{N L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{m,T_{t}}}}\\end{array}$ , we turn to bound the residual error by (I.9): ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi_{t}^{\\mathrm{alg}}=\\displaystyle\\sum_{j:\\psi_{t,j,1}>\\hat{p}_{t,j}}4\\cdot\\beta_{t,j}^{\\mathrm{alg}}+\\displaystyle\\sum_{j:\\psi_{t,j,1}=\\hat{p}_{t,j}}\\operatorname*{min}\\left\\{4,2\\xi_{t,j}^{\\mathrm{alg}}\\right\\}\\cdot\\beta_{t,j}^{\\mathrm{alg}}}\\\\ &{\\qquad+\\displaystyle\\sum_{j:\\psi_{t,j,3}=\\hat{p}_{t,j}}10\\sqrt{\\frac{d}{T_{t,j}}\\ln{\\frac{2}{\\delta_{m,T_{t}}}}}\\cdot\\beta_{t,j}^{\\mathrm{alg}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "image", "img_path": "Q5e3ftQ3q3/tmp/f180389a2ecff77afb09ac451309d002f894051d25ff9788e5557a31aceea62b.jpg", "img_caption": ["Figure 3: Misspeficied $L_{\\mathrm{max}}$ and $L_{\\mathrm{min}}$ "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "where ", "page_idx": 59}, {"type": "image", "img_path": "Q5e3ftQ3q3/tmp/5a1acdb6e57df8f3f6a266568afa3a3dd6a8374d31b0894f547ca3ead8e74cf6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "Ealg is obtained in a similar manner as \u03b1ag. Qalgal characterizes the confidence radi of each context at a finer level. ", "page_idx": 59}, {"type": "text", "text": "These finer confidence bounds can save a constant of 2.5 when $t$ is large. ", "page_idx": 59}, {"type": "text", "text": "O.2Misspecified $L_{\\mathrm{max}}$ and $L_{\\mathrm{min}}$ ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "As ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\mathrm{I}^{+}}$ requires the knowledge of $L_{\\mathrm{max}}$ , we empirically test the robustness of the algorithm towards $L_{\\mathrm{max}}$ on the instances in section 6. We run $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ with $\\tilde{L}_{\\operatorname*{min}}=\\nu L_{\\operatorname*{min}}$ and $\\tilde{L}_{\\mathrm{max}}=\\nu L_{\\mathrm{max}}$ where $\\nu=0.8$ or 1.2. An $\\varepsilon$ -best arm is recommended in all experiments. The sample complexities are presented in Figure 3. The result indicates that $\\mathrm{PS}{\\varepsilon}\\mathrm{BAI}$ (thus ${\\bf P}{\\bf S}\\varepsilon{\\bf B}{\\bf M}^{+},$ ) is robust towards the knowledge of $L_{\\mathrm{max}}$ and its superiority over $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ is maintained. ", "page_idx": 59}, {"type": "text", "text": "O.3Robustness towards $w$ and $b$ ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "According to the distinguishability condition (Assumption 1), point (2) indicates we can set $\\begin{array}{r}{w=\\frac{L_{\\mathrm{min}}}{3\\gamma}}\\end{array}$ where $\\gamma$ is the change detection frequency, and (3.5) indicates $w$ and $b$ are coupled. We denote $\\begin{array}{r}{\\tilde{w}:=\\frac{L_{\\mathrm{min}}}{18}}\\end{array}$ ", "page_idx": 59}, {"type": "text", "text": "To exam the robustness towards $w$ and $b$ ,we choose tovary the choice of $\\gamma$ , thus, $w$ and $b$ will change accordingly. Specifically, we select $\\gamma\\in\\{2,3,6,12\\}$ and the corresponding $w\\in\\{3\\tilde{w},2\\tilde{w},\\tilde{w},\\frac{\\tilde{w}}{2}\\}$ The other parameters remain unchanged. The experiment result is presented in Figure 4. When = 18,w=,l Assumption 1 is severely violated and the result is not desirable. ", "page_idx": 59}, {"type": "text", "text": "The result indicates that, while smaller $\\gamma$ and greater $w$ can result in slightly greater sample complexity, the overall sample complexity does not vary much and the superiority of our algorithm over the naive uniform sampling algorithm $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ is maintained. We conclude that our algorithm is robust against these choices, as long as Assumption 1 is not severely violated. ", "page_idx": 59}, {"type": "image", "img_path": "Q5e3ftQ3q3/tmp/fb7de4738c34bad1ab38d75650d9d8b514ac033fefd8b615a22c9804d177ff21.jpg", "img_caption": ["Figure 4: Robustness towards $w$ and $b$ "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "0.4 Benchmarks: D $\\varepsilon$ BAIand $\\scriptstyle\\mathbf{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "We present the Distribution $\\varepsilon$ -Best Arm Identification $({\\mathrm{D}}{\\varepsilon}{\\mathrm{B}}{\\mathrm{AI}})$ in Algorithm 8 and its variant $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ in Algorithm 9. According to Dynamics 3, the agent has access to the context vector $\\theta_{j_{t}}^{*}$ and the changepoint $t\\in\\mathcal{C}$ . Thus only the distribution $P_{\\theta}$ remains unknown and the agent needs to estimate it via the observed contexts. ", "page_idx": 60}, {"type": "text", "text": "Algorithm 8 DISTRIBUTION $\\varepsilon$ -BEST ARM IDENTIFICATION $({\\mathrm{D}}\\varepsilon{\\mathrm{B}}{\\mathrm{AI}})$ ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1: Input: the arm set $\\mathcal{X}$ , the latent vector matrix $\\Theta$ , the slackness parameter $\\varepsilon$ ,the confidence   \nparameter $\\delta$   \n2: Initialize: Compute the G-optimal allocation $\\lambda^{*}$ \uff1a   \n3: Compute $\\textstyle C_{3}={\\dot{\\sum}}_{n=1}^{\\infty}n^{-3}$   \n4: Observe 0\\*\\*   \n5: Compute ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ddot{p}_{t,j}=\\displaystyle\\sum_{l_{s}=1}^{l_{t}}\\frac{\\mathbb{1}\\{c_{l_{s}}=c_{j}\\}}{l_{t}},\\quad\\ddot{x}_{t}=\\underset{x\\in\\mathcal{X}}{\\mathrm{arg}\\,\\mathrm{max}}\\,x^{\\top}\\Theta\\ddot{p}_{t}}\\\\ &{\\ddot{\\beta}_{t}=\\sqrt{\\frac{1}{2l_{t}}\\ln\\frac{2C_{3}N l_{t}^{3}}{\\delta}},\\quad\\ddot{\\rho}_{t}(\\ddot{x}_{t},x)=\\ddot{\\beta}_{t}\\underset{\\zeta(\\ddot{x}_{t},x)\\in\\mathbb{R}}{\\mathrm{min}}\\underset{j=1}{\\overset{N}{\\sum}}|\\Delta_{j}(\\ddot{x}_{t},x)+\\zeta(\\ddot{x}_{t},x)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "6: while $\\begin{array}{r}{\\operatorname*{min}_{x\\neq\\ddot{x}_{t}}(\\ddot{x}_{t}-x)^{\\top}\\Theta\\ddot{p}_{t}-\\ddot{\\rho}_{t}<-\\varepsilon}\\end{array}$ or $t\\not\\in\\mathcal{C}$ do   \n7: Observe $\\boldsymbol{\\theta}_{j_{t}}^{*}$ and $\\mathbb{1}\\{t\\in\\mathcal{C}\\}$ update $t=t+1$   \n8: Update $\\ddot{x}_{t}$ and ${\\ddot{\\rho}}_{t}$ with (O.1) and update $l_{t}$ if $t\\in\\mathcal{C}$   \n9: end while   \n[0: Recommend arm $\\ddot{x}_{\\varepsilon}=\\ddot{x}_{t}$ ", "page_idx": 60}, {"type": "text", "text": "The design is straightforward except for $\\zeta(\\Ddot{x}_{t},x)$ . It minimizes the summation $\\zeta({\\ddot{x}}_{t},x)\\;:=$ $\\begin{array}{r}{\\arg\\operatorname*{min}_{y\\in\\mathbb{R}}\\sum_{j=1}^{N}|\\bar{\\Delta}_{j}(\\ddot{x}_{t},x)+\\zeta(\\ddot{x}_{t},x)|}\\end{array}$ . This trick has also been utilized in the design of $\\mathrm{PS}\\varepsilon\\mathrm{BAI}$ (see (I.5). It helps to better exploit the structure of the latent vectors $\\Theta$ and facilitates the identification process. For easy implementation, we choose a proxy $\\begin{array}{r}{\\zeta(\\ddot{x}_{t},x)=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}}\\sum_{j=1}^{N}(\\Delta_{j}(\\ddot{x}_{t},x)+}\\end{array}$ $\\begin{array}{r}{\\zeta(\\ddot{x}_{t},x))^{2}=-{\\frac{1}{N}}\\sum_{j=1}^{N}\\Delta_{j}(\\ddot{x}_{t},x)}\\end{array}$ ", "page_idx": 60}, {"type": "text", "text": "Algorithm 9 D1STRIBUTION $\\varepsilon$ -BEST ARM IDENTIFICATION- $\\beta$ $({\\bf D}\\varepsilon{\\bf B}{\\bf A}{\\bf I}_{\\beta})$ ", "page_idx": 61}, {"type": "text", "text": "1: Input: the arm set $\\mathcal{X}$ , the latent vector matrix $\\Theta$ , the slackness parameter $\\varepsilon$ , the confidence   \nparameter $\\delta$   \n2: Initialize: Compute the $\\mathrm{G}$ optimal allocation $\\lambda^{*}$ \uff1a   \n3: Compute $\\textstyle C_{3}={\\dot{\\sum}}_{n=1}^{\\infty}n^{-3}$ \uff1a   \n4: Observe 0\\*\\*   \n5: Compute ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\circ}{p}_{t,j}=\\underset{l_{s}=1}{\\overset{l_{t}}{\\sum}}\\frac{L_{l_{s}}\\mathbb{I}\\left\\{c_{l_{s}}=c_{j}\\right\\}}{t},\\quad\\overset{\\circ}{x}_{t}=\\underset{x\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\,x^{\\top}\\Theta\\overset{\\circ}{p}_{t},}\\\\ &{\\overset{\\circ}{\\beta}_{t,j}:=\\operatorname*{min}\\left\\{\\frac{\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{t}}}{\\lambda_{t}}+\\sqrt{\\left(\\frac{1}{3}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{t}}\\right)^{2}+\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}\\ln\\frac{2}{\\delta_{t}}}{t}},1\\right\\},}\\\\ &{\\overset{\\circ}{\\phi}_{t,j}:=\\operatorname*{min}\\left\\{4\\operatorname*{max}\\left\\{\\overset{\\circ}{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{t}\\ln\\frac{2}{\\delta_{t}}\\right\\},\\frac{1}{4}\\right\\},\\;\\delta_{t}=\\frac{\\delta}{C_{3}N t^{3}},}\\\\ &{\\overset{\\circ}{\\rho}_{t}(\\bar{x}_{t},x)=\\underset{\\zeta(\\bar{x}_{t},x)\\in\\mathbb{R}_{j}=1}{\\operatorname*{min}}\\underset{0}{\\overset{N}{\\sum}}\\left|\\Delta_{j}(\\bar{x}_{t},x)+\\zeta(\\bar{x},x)\\right|\\overset{\\circ}{\\beta}_{t,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "6:while $\\begin{array}{r}{\\operatorname*{min}_{x\\neq\\mathring{x}_{t}}(\\mathring{x}_{t}-x)^{\\top}\\Theta\\mathring{p}_{t}-\\mathring{\\rho}_{t}<-\\varepsilon}\\end{array}$ do   \n7: Observe $\\theta_{j_{t}}^{*}$ and $\\mathbb{1}\\{t\\in\\mathcal{C}\\}$ and let $t=t+1$   \n8: Update $\\mathbf{\\chi}_{x_{t}}^{\\circ}$ and $\\mathbf{\\Delta}_{\\rho_{t}}^{\\circ}$ with (0.2).   \n9: end while   \n10: Recommend arm $\\mathbf{\\omega}_{x_{\\varepsilon}}^{\\circ}=\\mathbf{\\omega}_{x_{t}}^{\\circ}$ ", "page_idx": 61}, {"type": "text", "text": "$\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ utilizes the techniques we have used to bound the deviation between the true distribution $p_{j}$ and the estimated ones $\\boldsymbol{\\stackrel{\\circ}{p}}_{t,j}$ for all $\\textit{j}\\in\\textit{[N]}$ .Similarly, we let $\\begin{array}{r l}{\\zeta({\\stackrel{\\circ}{x}},x)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\zeta(\\stackrel{\\circ}{x},x)\\in\\mathbb{R}}\\sum_{j=1}^{N}\\stackrel{\\circ}{\\beta}_{t,j}(\\Delta_{j}(\\stackrel{\\circ}{x}_{t},x)+\\zeta(\\stackrel{\\circ}{x},x))^{2}=-\\frac{\\sum_{j=1}^{N}\\stackrel{\\circ}{\\beta}_{t,j}\\Delta_{j}(\\stackrel{\\circ}{x}_{t},x)}{\\sum_{j=1}^{N}\\beta_{t,j}}}\\end{array}$ ", "page_idx": 61}, {"type": "text", "text": "As the theoretical guarantee of $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ can be derived following a similar manner as the proof of the DE term of $\\mathrm{PS}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ in Appendix I.2 and in the proof of Lemma G.6, for simplicity, we just present the result and omit the proof here here. ", "page_idx": 61}, {"type": "text", "text": "Theorem 0.1. $\\scriptstyle\\mathbf{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}_{\\beta}$ identifies an $\\varepsilon$ -best arm within ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X},x\\neq x_{\\varepsilon},x^{*}}\\frac{{L_{\\operatorname*{max}}H}^{2}(x_{\\varepsilon},x)}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "time steps with probability at least $1-\\delta$ and in expectation, where ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\stackrel{\\circ}{H}(x_{\\varepsilon},x):=\\operatorname*{min}_{\\zeta(\\frac{\\circ}{x_{\\varepsilon},x})\\in\\mathbb{R}}\\sum_{j=1}^{N}\\sqrt{\\operatorname*{min}\\{16p_{j},\\frac{1}{4}\\}}|\\Delta_{j}(\\frac{\\circ}{x_{\\varepsilon},x})+\\zeta(\\stackrel{\\circ}{x_{\\varepsilon},x})|.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "We present a theorem, along with a proof sketch, for the theoretical guarantee of $\\scriptstyle\\mathrm{D}\\varepsilon\\mathrm{BAI}$ below. ", "page_idx": 61}, {"type": "text", "text": "Theorem O.2. $\\scriptstyle\\mathrm{D}\\varepsilon\\mathbf{B}\\mathbf{A}\\mathbf{I}$ identifies an $\\varepsilon$ -best armwithin ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\operatorname*{max}_{x_{\\varepsilon}\\in\\mathcal{X},x\\neq x_{\\varepsilon},x^{*}}\\frac{L_{\\mathrm{max}}\\ddot{H}^{2}(x_{\\varepsilon},x)}{(\\Delta_{\\mathrm{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "timestepswithprobabilityatleast $1\\ -\\ \\delta$ andinexpectation,where $\\begin{array}{r l}{\\ddot{H}(x_{\\varepsilon},x)}&{{}:=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{\\zeta(\\ddot{x}_{\\varepsilon},x)\\in\\mathbb{R}}\\sum_{j=1}^{N}|\\Delta_{j}(\\ddot{x}_{\\varepsilon},x)+\\zeta(\\ddot{x}_{\\varepsilon},x)|}\\end{array}$ ", "page_idx": 61}, {"type": "text", "text": "Proof. For the sake of conciseness and simplicity, only a proof sketch is provided for this Theorem.   \nSimilar results can be found in the referred contents. The proof is composed by 4 steps.   \n\u00b7 Step 1: bound the deviation of $P_{\\theta}[\\theta_{j}^{\\ast}]$ by Lemma O.3 and Remark O.4.   \n\u00b7 Step 2: prove the recommended arm is an $\\varepsilon$ -best arm. Conditional on Step 1, we can prove this following the procedures in Lemma J.1.   \n\u00b7 Step 3: give a sufficient condition and then obtain a high probability upper bound. This can be seen from (J.1) and by solving ", "page_idx": 62}, {"type": "text", "text": "", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\ddot{\\rho}_{t}(\\ddot{x}_{t},x)=\\ddot{\\beta}_{t}\\sum_{j=1}^{N}|\\Delta_{j}(\\ddot{x}_{t},x)+\\zeta(\\ddot{x}_{t},x)|\\leq\\frac{\\Delta_{\\operatorname*{min}}+\\varepsilon}{2}}\\\\ {\\Rightarrow}&{l_{t}=\\tilde{O}\\left(\\frac{\\ddot{H}^{2}(x_{\\varepsilon},x)}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}\\right).}\\\\ {\\Rightarrow}&{t=\\tilde{O}\\left(\\frac{L_{\\operatorname*{max}}\\ddot{H}^{2}(x_{\\varepsilon},x)}{(\\Delta_{\\operatorname*{min}}+\\varepsilon)^{2}}\\ln\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "The high probability result is obtained by maximizing the above over $x_{\\varepsilon}\\in\\mathcal{X},x\\neq x_{\\varepsilon},x^{*}$ ", "page_idx": 62}, {"type": "text", "text": "\u00b7 Step 4: the expected result can be derived in a similar method as in the proof of $\\scriptstyle\\mathrm{N}\\varepsilon\\mathrm{BAI}$ in Appendix F. The expected sample complexity and the high-probability sample complexity is of the sameorder. ", "page_idx": 62}, {"type": "text", "text": "Lemma O.3. With probability at least $1\\,-\\,\\delta$ $\\operatorname*{sup}_{j\\in[N]}|p_{j}\\,-\\,\\ddot{p}_{t,j}|\\;\\leq\\;\\ddot{\\beta}_{t}$ for all $t\\,\\in\\,\\mathbb{N}$ where $\\begin{array}{r}{\\ddot{\\beta}_{t}=\\sqrt{\\frac{2}{l_{t}}\\ln\\frac{2}{\\delta_{l_{t}}}}}\\end{array}$ and oit=7 $\\begin{array}{r}{\\delta_{l_{t}}=\\frac{\\delta}{C_{3}l_{t}^{3}}}\\end{array}$ ", "page_idx": 62}, {"type": "text", "text": "Proof. We may define the cumulative distribution function (CDF) and the empirical CDF for $P_{\\theta}$ as $\\begin{array}{r}{F_{\\theta}(j)=\\underset{*=*}{\\sum}\\underset{k=1}{\\overset{j}{P}}{}_{\\theta}[\\theta_{j}^{*}]}\\end{array}$ and $\\begin{array}{r}{\\ddot{F}_{t}(j)=\\sum_{l_{s}=1}^{l_{t}}\\mathbb{1}\\{c_{l_{s}}=c_{j},l_{s}\\le j\\}}\\end{array}$ respectively for $j\\in[N]$ .According to the DKW inequality, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{j\\in[N]}\\left|\\ddot{F}_{t}(j)-F(j)\\right|\\ge\\epsilon\\right]\\le2\\exp\\left(-2l_{t}\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "By the implication ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon\\leq|p_{j}-\\ddot{p}_{t,j}|=|F(j)-F(j-1)-\\ddot{F}_{t}(j)+\\ddot{F}_{t}(j-1)|}\\\\ &{\\quad\\leq|F(j)-\\ddot{F}_{t}(j)|+|F(j-1)-\\ddot{F}_{t}(j-1)|}\\\\ {\\Rightarrow}&{|F(j)-\\ddot{F}_{t}(j)|\\geq\\frac{\\epsilon}{2}\\quad\\mathrm{or}\\quad|F(j-1)-\\ddot{F}_{t}(j-1)|\\geq\\frac{\\epsilon}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "we have that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\underset{j\\in[N]}{\\operatorname*{sup}}\\left|p_{j}-\\ddot{p}_{t,j}\\right|\\geq\\epsilon\\right]\\leq\\mathbb{P}\\left[\\underset{j\\in[N]}{\\operatorname*{sup}}\\left|\\ddot{F}_{t}(j)-F(j)\\right|\\geq\\frac{\\epsilon}{2}\\right]\\leq2\\exp\\left(-2l_{t}\\left(\\frac{\\epsilon}{2}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\exp\\left(-\\frac{l_{t}\\epsilon^{2}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "This is equivalent to ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\operatorname*{sup}_{j\\in[N]}\\left|p_{j}-\\ddot{p}_{t,j}\\right|\\geq\\ddot{\\beta}_{t}\\right]\\leq\\delta_{l_{t}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "A union bound gives an upper bound of the failure probability $\\textstyle\\sum_{l_{t}=1}^{\\infty}\\delta_{l_{t}}=\\delta$ ", "page_idx": 62}, {"type": "text", "text": "Remark O.4. The use of the DKW inequality gives a union bound over the deviation of the distribution estimationallcontexts.Thisavoidthe $N$ factorinthelogarithmin $\\ddot{\\beta}_{t}$ andisbeneficialwhenthe numberofcontexts $N$ is large. ", "page_idx": 62}, {"type": "text", "text": "When there are afew contexts, it is also possible to directly bound the deviation for each context via Azuma-Hoeffding's inequality, i.e., ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|p_{j}-\\ddot{p}_{t,j}|\\leq\\ddot{\\beta}_{t}\\right]\\leq\\delta_{l_{t}}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Where $\\begin{array}{r}{\\ddot{\\beta}_{t}:=\\sqrt{\\frac{1}{2l_{t}}\\ln\\frac{2C_{3}N l_{t}^{3}}{\\delta}}}\\end{array}$ A nion oundgivesthat with probabit a least $1\\!-\\!\\delta$ $|p_{j}-\\ddot{p}_{t,j}|\\leq\\ddot{\\beta}_{t}$ forall $t\\in\\mathbb{N},j\\in[N]$ .As this new confidence radius is the same as the one in Lemma O.3 up to constant and logarithmic terms, the sample complexity should be of the same order. ", "page_idx": 63}, {"type": "text", "text": "Weadopt this confdence radus $\\begin{array}{r}{\\ddot{\\beta}_{t}:=\\sqrt{\\frac{1}{2l_{t}}\\ln\\frac{2C_{3}N l_{t}^{3}}{\\delta}}}\\end{array}$ in the experiment. ", "page_idx": 63}, {"type": "text", "text": "P   Additional Discussions ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "In this section, we provide additional discussions on ", "page_idx": 63}, {"type": "text", "text": "\u00b7 the related methods for BAI in the nonstationary bandits literature in subsection P.1,   \n\u00b7 the upper on the sample complexity in Theorem 3.2 and Theorem 3.3 in subsection P.2,   \n\u00b7 the connection between the piecewise-stationary linear bandits model to the stationary linear bandits model in subsection P.3,   \n\u00b7 the special case where the number of context $N=1$ in subsection P.4, ", "page_idx": 63}, {"type": "text", "text": "P.1 Discussion on the Related Methods in Nonstationary Bandits ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "To the best of our knowledge, there is limited literature investigating the BAI in the nonstationary bandits setup (there is comparatively a much richer literature for regret minimization in non-stationary environments): [14] studies BAI in the fixed-horizon setup and [15] assumes the best arm remains unchanged after certain time step and explores the fixed-confidence setting. Both of the works are not directly comparable to our proposed piecewise-stationary setup. ", "page_idx": 63}, {"type": "text", "text": "Additionally, we provide strong baselines algorithms $D{\\varepsilon}B A I$ and $D\\varepsilon B A I_{\\beta}$ in Section 6, which are detailed in Section O.4. These two baselines are given oracle information about the context (see Dynamics 3), while ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is not. As indicated by Figure 2(b), ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ is competitive compared to these strong baselines and is much better than the naive approach. ", "page_idx": 63}, {"type": "text", "text": "P.2  More Discussions on the Upper Bound ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Firstly, we emphasize that the sample complexity in Theorem 3.2 and Theorem 3.3 are instancedependent, in particular, the term $\\mathrm{H_{DE}}$ characterizes the difficulty in estimating the distribution of the context. Furthermore, the presence of the gaps $\\Delta(x,x^{*})$ also underscores that our upper bounds are functions of the instance. ", "page_idx": 63}, {"type": "text", "text": "Secondly, our algorithm adopts the $\\mathrm{G}$ -optimal design, which is minimax optimal for the BAI in standard linear bandits problem [1, 2]. Although we have not proved it, we believe that the sample complexity of the proposed algorithm may not be instance-dependent optimal. ", "page_idx": 63}, {"type": "text", "text": "Lastly, we provide some remarks on the instance-dependent optimal algorithms: ", "page_idx": 63}, {"type": "text", "text": "(1) The G-optimal design is the cornerstone for the more efficient and adaptive rules like $\\mathcal{X Y}.$ allocation and $\\mathcal{X Y}$ -Adaptive Allocation in [1]. Therefore, our algorithm provides a framework for more sophisticated algorithms in the piecewise-stationary linear bandits problem with the BAI task. Empirically, one can attempt to replace the G-optimal design by the $\\mathcal{N V}$ -allocation during implementation, which can possibly yield empirical benefits. ", "page_idx": 63}, {"type": "text", "text": "(2) The current lower bound is established with two simpler problems (see Section 4), which is sufficient to show our algorithm is minimax optimal. However, a tighter instance-dependent lower bound based on the original problem may be required if one wishes to show an algorithm is instancedependent optimal. This can be challenging because the distribution of the arms, the contexts and the changepoints are unknown, and the characterization of the \"alternative instance\" (i.e., the $\\varepsilon$ -bestarm set is changed) is difficult. ", "page_idx": 63}, {"type": "text", "text": "To conclude, we believe an instance-dependent optimal algorithm is appealing and can lead to future research. ", "page_idx": 63}, {"type": "text", "text": "Firstly, we would like to clarify that, in the piecewise-stationary linear bandits model, the instance tends towards a stationary one when $\\operatorname*{max}_{j\\in[N]}p_{j}\\to1$ , instead of the scenario in which $L_{\\mathrm{max}}\\rightarrow\\infty$ ", "page_idx": 64}, {"type": "text", "text": "When $L_{\\mathrm{min}}$ and $L_{\\mathrm{max}}$ are large, estimating each latent vector becomes easier since there are more observations in a single stationary segment. However, the overall reward of an arm also depends on $P_{\\theta}$ , the distribution of latent context vectors, which is also assumed to be unknown in the setup. In order to estimate $P_{\\theta}$ , a learning agent can get one (unobservable) sample from $P_{\\theta}$ only at the (unobservable) change points. In other words, $L_{\\mathrm{max}}$ charaterizes the \u201csparsity\u201d of the context samples: the larger $L_{\\mathrm{max}}$ is, the sparser the context samples are. A larger $L_{\\mathrm{max}}$ indicates that samples from $P_{\\theta}$ are likely to be generated less frequently, which will result in the increase of the overall sample complexity. Consider an extreme example of Dynamics 3 where the context vector is revealed at every time step and $L_{\\mathrm{min}}$ and $L_{\\mathrm{max}}$ are very large. If at least $l_{\\tau}$ context samples from $P_{\\theta}$ are required to identify the best arm, then a sample complexity of $l_{\\tau}L_{\\mathrm{min}}$ is unavoidable in the our setup under this extreme case. ", "page_idx": 64}, {"type": "text", "text": "Secondly, while $\\operatorname*{max}_{j\\in[N]}p_{j}$ is important in characterizing the (non)stationarity of the instance, we would like to emphasize that both the distribution $P_{\\theta}$ and latent context vectors are essential in characterizing the sample complexity, as shown in the term $\\bar{H}(x_{\\epsilon},x)$ in Theorem 3.2. To further illustrate this, we provide a simple but illuminating instance as follows: The instance is composed by $N=3$ contexts and $K=2$ arms, and $\\Delta_{j}$ denotes the mean gap between arm 1 and arm 2 under context $j\\in[N]$ (to be specified below) and $\\begin{array}{r}{\\Delta:=\\sum_{j\\in[N]}p_{j}\\bar{\\Delta}_{j}}\\end{array}$ denotes the weighted mean gap between arm 1 and arm 2. The probability of context 1 is $\\begin{array}{r}{p_{1}^{\\bigstar}=\\operatorname*{max}_{j\\in[N]}p_{j}=1-p_{2}-p_{3}}\\end{array}$ , and $p_{1}$ is close to 1. Let $\\hat{x}$ denote the empirical version of the statistics $x$ in the foliowing arguments. ", "page_idx": 64}, {"type": "text", "text": "In such a case, consider this question: if an algorithm has only detected context 1 in the first $t$ time stepsand $t$ is large, should it stop or not? ", "page_idx": 64}, {"type": "text", "text": "As no change point has occurred, the current dynamics behaves similarly to that of a stationary bandit environment up till now. Thus the empirical probabilities of the 3 contexts are $\\hat{p}_{1}=1,\\hat{p}_{2}=\\hat{p}_{3}=0$ and the empirical mean gap under context 1 $\\hat{\\Delta}_{1}$ iscloseto $\\Delta_{1}$ . Consider the following two cases: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 Case 1: $\\Delta_{1}=p_{2}$ and $\\Delta_{2}=\\Delta_{3}=-p_{1}$ . The true weighted mean gap between arm 1 and arm   \n2 is $\\begin{array}{r}{\\Delta=\\sum_{j\\in[N]}p_{j}\\Delta_{j}\\,=\\,-p_{1}p_{3}\\,<\\,0}\\end{array}$ , and thus arm 2 is the best arm. This indicates that, in order to estimate $\\Delta_{2}$ and $\\Delta_{3}$ , an algorithm needs to observe contexts 2 and 3 despite their small probabilities. But currently $\\hat{p}_{2}\\,=\\,\\stackrel{\\cdot}{p}_{3}\\,=\\,0,\\hat{\\Delta}\\,=\\,\\hat{\\Delta}_{1}\\,\\approx\\,\\Delta_{1}\\,>\\,0$ , so the algorithm should not terminate.   \n\u00b7 Case 2: $\\Delta_{1}=1$ and $\\Delta_{2},\\Delta_{3}\\in[-2,2]$ . Thus $\\hat{\\Delta}\\approx\\Delta\\geq p_{1}-2p_{2}-2p_{3}>0$ . This indicates there is no need to observe contexts 2 and 3, because arm 1 is the best even in the worst case $(\\Delta_{2}\\,=\\,\\Delta_{3}\\,=\\,-2)$ . In this case, as long as the algorithm gets a good estimate of $\\Delta_{1}$ , it can confidently terminate. ", "page_idx": 64}, {"type": "text", "text": "As these two cases indicate, in addition to $P_{\\theta}$ , the latent vectors (which determine the means of the arms) are equally important for the stopping time. Our algorithm takes both factors into account, as reflected by the summation term in equation (3.4) and $\\hat{\\Delta}_{t}(x_{t}^{*},x)$ in (3.5) for the algorithm design, andby $\\bar{H}(\\bar{x}_{\\epsilon},x)$ for the theoretical upper bound. ", "page_idx": 64}, {"type": "text", "text": "We conclude that ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "\u00b7 The unknown distribution $P_{\\theta}$ and the latent vectors jointly determine the sample complexity. $\\bar{H}(x_{\\epsilon},x)$ in Theorem 3.2 characterizes their roles, where the contexts with larger probabilities have commensurately greater influence on the sample complexity of the algorithm. ", "page_idx": 64}, {"type": "text", "text": "\u00b7 When $\\operatorname*{max}_{j\\in[N]}p_{j}\\to1$ witthistaaita reduces to a stationary one. $H_{D E}(x_{\\epsilon},x)$ in Theorem 3.2 becomes $L_{\\mathrm{max}}/4$ , which implies that a constant number of change points need to be observed, and thus $T_{D}(x_{\\epsilon},x)$ can be regarded as a constant. The dominant term in our upper bound, the $T_{V}(x)$ term, recovers the upper bound in the stationary bandits,ta s, $\\operatorname*{max}_{x\\neq x^{*}}$ $\\overline{{(\\Delta(x^{*},x)\\!+\\!\\varepsilon)^{2}}}\\,\\ln(1/\\delta)$ [32]. ", "page_idx": 64}, {"type": "text", "text": "We onlyconsider $N>1$ as the bandit model is only truely piecewise-stationary when $N>1$ Thus, we did not specifically derive upper and lower bounds for the extreme case where $N=1$ Nevertheless, our analysis can cover this case due to the following reasons: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The (minimax) lower bound in Theorem 4.1 is not directly applicable since it is not derived for this extreme instance. However, if we step back to the analysis equation (M.16), the feasible set in the optimization problem is empty, and thus the lower bound on $\\mathbb{E}[l_{t}]$ $N_{C}$ in Theorem4.1) is 0. In this case, the lower bound in Theorem4.1 reduces to the lower bound in the stationary bandits.   \n\u00b7 Regarding the upper bound in Theorem 3.2, when $N=1$ , we have $p_{1}=1$ and $\\begin{array}{r}{\\dot{H}_{D E}=\\frac{L_{\\mathrm{max}}}{4}}\\end{array}$ Lmax . In this case, as $H_{D E}$ does not depend on the mean gaps and the mean gaps are among the fundamental quantities to characterize the difficulty of an instance, the $T_{D}$ term can be regarded as a constant. Therefore, the dominant term in the upper bound is $T_{V}$ . This recovers the bound in the stationary setup.   \n\u00b7 In addition, as we assume $N$ is an input to our algorithm, when we are aware $N=1$ , we can actually adopt the algorithms for the stationary setting, e.g., the $G$ -allocation or $\\mathcal{N V}$ -allocation rule [1]. ", "page_idx": 65}, {"type": "text", "text": "Q $\\varepsilon$ Best Arm Tuple Identification Problem ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "In the main paper, we consider the identification of an $\\varepsilon$ -best arm in terms of the \u201censemble\u201d quality $\\begin{array}{r}{\\mu_{x}:=\\mathbb{E}_{\\theta\\sim\\bar{P_{\\theta}}}\\bar{[}x^{\\top}\\theta]=\\sum_{j=1}^{N}P_{\\theta}[\\theta_{j}^{*}]x^{\\top}\\theta_{j}^{*}}\\end{array}$ The curious reader may wonder whether we can identify the $\\varepsilon$ -best arm tuple $\\mathcal{X}_{\\varepsilon}^{\\mathrm{tuple}}\\;:=\\;\\big(x_{1}^{\\varepsilon},\\ldots,x_{N}^{\\varepsilon}\\big)$ where $\\boldsymbol{x}_{j}^{\\varepsilon}$ is an $\\varepsilon$ -best arm under context $j$ ie.. $x_{j}^{\\varepsilon}\\in\\{x:x^{\\top}\\theta_{j}^{*}\\geq\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\theta_{j}^{*}-\\varepsilon\\}$ . Given the tools we developed in this manuscript, we answer this problem in the affirmative. ", "page_idx": 65}, {"type": "text", "text": "Intuitions: Let $x_{j}^{*}:=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\theta_{j}^{*}$ and $\\begin{array}{r}{\\Delta_{j}^{*}:=\\operatorname*{min}_{x\\neq x_{j}^{*}}\\Delta_{j}(x_{j}^{*},x)}\\end{array}$ . We expect to have an upper bound taking the form of ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\tilde{O}\\bigg(\\operatorname*{max}_{j\\in[N]}\\frac{L_{\\operatorname*{max}}}{p_{j}}\\cdot\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}L_{\\operatorname*{min}}}\\ln\\frac{1}{\\delta}\\bigg)\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\begin{array}{r}{O(\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}L_{\\operatorname*{min}}}\\ln\\frac{1}{\\delta})}\\end{array}$ is an upper bound on the number of context samples $j$ and context $j$ will occur once among every ; contexts in expectation. ", "page_idx": 65}, {"type": "text", "text": "Analysis of the problem: The change detection and context alignment subroutines are only effective Within $\\tau^{*}$ time steps (Line 2 in Algorithm 1). However, if a context is with small occurrence probability $p_{j}$ , it may not appear before $\\tau^{*}$ timesteps. ", "page_idx": 65}, {"type": "text", "text": "Regarding this scenario, we only expect to obtain a high-probability upper bound on the sample complexity, whereas the expected sample complexity requires more techniques beyond our parallel execution trick (which is used to design ${\\bf P}{\\bf S}{\\varepsilon}{\\bf B}{\\bf A}{\\bf I}^{+}$ ) and is an interesting direction left for future research. ", "page_idx": 65}, {"type": "text", "text": "Besides, it is more feasible to consider the identification of $\\varepsilon$ -best arms under contexts with high occurrence probability, i.e., we aim to identify ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\varepsilon,\\bar{p}}^{\\mathrm{tuple}}:=\\{\\boldsymbol{x}_{j}^{\\varepsilon}:p_{j}>\\bar{p}\\}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\bar{p}$ is a threshold on the occurrence probability of contexts. Let $\\Theta_{\\bar{p}}=\\{\\theta_{j}^{*}:p_{j}>\\bar{p}\\}$ denote those context with high occurrence probability. ", "page_idx": 65}, {"type": "text", "text": "Goal: We aim to devise an algorithm with the minimum sample complexity (arm pulls) to ascertain either (1) an $\\varepsilon$ -best arm undercontext $j$ or (2) $\\theta_{j}^{*}\\notin\\Theta_{\\bar{p}}$ ", "page_idx": 65}, {"type": "text", "text": "With the above goal, we expect to identify an $\\varepsilon$ -best arm for all $j$ with $\\theta_{j}^{*}\\in\\Theta_{\\bar{p}}$ ; for those $j$ with $\\theta_{j}^{\\ast}\\notin\\Theta_{\\bar{p}}$ , either an $\\varepsilon$ -best arm is identified or $\\theta_{j}^{\\ast}\\notin\\Theta_{\\bar{p}}$ is ascertained. ", "page_idx": 65}, {"type": "text", "text": "We propose Algorithm 10 for this problem, which is quite similar to Algorithm 1, except for a few changes: ", "page_idx": 65}, {"type": "text", "text": "1: Input: arm set $\\mathcal{X}$ , size of the set of latent vectors $N$ , bounds on the segment lengths $L_{\\mathrm{min}}$ and   \n$L_{\\mathrm{max}}$ , slackness parameter $\\varepsilon$ confidence parameter $\\delta$ , sampling parameter $\\gamma$ and window size $w$ \uff0c   \nthreshold $b$ probability threshold $\\bar{p}$   \n2: Initialize: Compute the $\\mathrm{G}$ -optimal allocation $\\lambda^{*}$ and $\\begin{array}{r}{\\tau^{*}=\\frac{38400\\ln(80)N L_{\\mathrm{max}}}{\\varepsilon^{2}}\\ln\\frac{N^{2}K L_{\\mathrm{max}}}{\\delta\\varepsilon^{2}}}\\end{array}$   \n$\\mathrm{Flag}=[N]$ and ${\\mathrm{Hold}}={\\big[}\\,{\\begin{array}{r l}\\end{array}}]$ and $\\mathrm{{Output}}=[\\,]$   \n3: Set $\\mathrm{CD}_{\\mathrm{sample}}=[\\mathrm{\\ensuremath{~],\\,CA_{id}}=\\left\\{\\begin{array}{l}{\\begin{array}{r l}\\end{array}}\\end{array}\\right\\}}$ .Set $t_{\\mathrm{CD}}=+\\infty$   \n4: Set $\\mathcal{T}_{t,j}=\\bar{\\emptyset}$ and initialize $\\mathcal{T}_{t}$ \uff0c $T_{t,j}$ \uff0c $T_{t}$ with (3.1) for all $t\\leq\\tau^{*}$ \uff0c $j\\in[N]$   \n5: Sample $\\frac w2$ arms $\\{x_{s}\\}_{s=1}^{\\frac{w}{2}}\\sim\\lambda^{*}$ and observe th asciatd s $\\begin{array}{r}{\\{Y_{s,x_{s}}\\}_{s=1}^{\\frac{w}{2}},t=\\frac{w}{2},t_{\\mathrm{CA}}=\\frac{w}{2}}\\end{array}$   \n6: $\\mathrm{CA}_{\\mathrm{id}}=\\{1:[(x_{s},Y_{s,x_{s}})]_{s=1}^{\\frac{w}{2}}\\},\\hat{j}_{t}=1.$   \n7: while $t\\leq\\tau^{*}$ and $\\operatorname{Flag}\\neq\\emptyset$ do   \n8: $t=t+1$   \n9: Sample an arm $x_{t}\\sim\\lambda^{*}$ and observe return $Y_{t,x_{t}}$   \n10: if  mod $(t-t_{\\mathrm{CA}},\\gamma)\\neq0$ then   \n11: Update $\\hat{j}_{t}=\\hat{j}_{t-1}$ \uff0c $T_{t,\\hat{j}_{t}}=T_{t-1,\\hat{j}_{t}}\\cup\\{t\\},T_{t,j}=T_{t-1,j}$ for $j\\neq\\hat{j}_{t}$   \n12: else   \n13: $\\mathrm{CD}_{\\mathrm{sample}}=\\mathrm{CD}_{\\mathrm{sample}}+[(x_{t},Y_{t,x_{t}})]$   \n14: Update $\\hat{j}_{t}=\\hat{j}_{t-1},T_{t,j}=T_{t-1,j}$ for all $j\\in[N]$   \n15: $|\\mathrm{CD}_{\\mathrm{sample}}|\\geq w$ then   \n16: if $\\mathtt{L C D}(\\mathcal{X},w,b,\\mathrm{CD}_{\\mathrm{sample}}[-w:])$ then   \n17: $\\mathrm{CD}_{\\mathrm{sample}}=\\left[\\begin{array}{l}{\\right]}$   \n18: $\\begin{array}{r}{t=t+\\frac{w}{2},t_{\\mathrm{CA}}=t,t_{\\mathrm{CD}}=+\\infty.}\\end{array}$   \n19: $\\boldsymbol{\\hat{j}}_{t},\\mathrm{CA}_{\\mathrm{id}}=\\mathrm{LCA}(\\mathcal{X},w,b,\\mathrm{CA}_{\\mathrm{id}}).$   \n20: $\\hat{j}_{t}=N+1$ then break.   \n21: Revert $\\begin{array}{r}{\\mathcal{T}_{t,j}=\\mathcal{T}_{t-\\frac{w(\\gamma+1)}{2},j}}\\end{array}$ for all $j\\in[N]$   \n22: end if   \n23: end if   \n24: end if   \n25: Update the estimates with (3.1), (3.2) and (Q.1).   \n26: if $\\exists j\\in$ Flag condition (Q.2) is met for $j$ and $t_{\\mathrm{CD}}=+\\infty$ then   \n27: for $j\\in\\operatorname{Flag}\\mathbf{d}\\mathbf{0}$   \n28: $\\begin{array}{r}{\\mathbf{if}\\operatorname*{min}_{x:x\\in\\mathcal{X}_{t,j}^{*}}\\hat{\\Delta}_{t,j}(x_{t,j}^{*},x)-\\alpha_{t,j}\\geq-\\varepsilon}\\end{array}$ then   \n29: Record $j$ and $\\hat{x}_{j,\\varepsilon}:=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\hat{\\theta}_{t,j}$ in Hold.   \n30: $\\mathrm{Flag}=\\mathrm{Flag}\\setminus\\dot{\\{}j\\}$ and $t_{\\mathrm{CD}}=|\\mathrm{CD}_{\\mathrm{sample}}|$   \n31: else if $\\hat{p}_{j}+\\beta_{t,j}<\\bar{p}$ then   \n32: Record $j$ and $\\hat{x}_{j,\\varepsilon}=\\mathrm{NAN}$ in Hold.   \n33: $\\mathrm{Flag}=\\mathrm{Flag}\\setminus\\{j\\}$ and $t_{\\mathrm{CD}}=|\\mathrm{CD}_{\\mathrm{sample}}|$   \n34: end if   \n35: end for   \n36: else if $\\mathit{t}_{\\mathrm{CD}}=\\lvert\\mathrm{CD}_{\\mathrm{sample}}\\rvert\\mathrm{~-~}\\frac{w}{2}$ then   \n37: for $(j,\\hat{x}_{j,\\varepsilon})$ in Hold do   \n38: $\\mathrm{Output}[j]=\\hat{x}_{j,\\varepsilon}$   \n39: end for   \n40: end if   \n41: end while   \n42: Recommend Output ", "page_idx": 66}, {"type": "text", "text": "\u00b7 On Line 7, the algorithm stops when an $\\varepsilon$ -best arm or $p_{j}\\leq\\bar{p}$ is identified for all contexts. \u00b7 On Line 25, it computes the confidence radii for the arms in context $j_{t}$ as well as the confidence radii for the occurrence probabilities ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\alpha_{t,j}=\\frac{d}{n}\\ln\\frac{2}{\\delta_{v,T_{t}}}+\\sqrt{\\left(\\frac{d}{n}\\ln\\frac{2}{\\delta_{v,T_{t}}}\\right)^{2}+\\frac{4d}{n}\\ln\\frac{2}{\\delta_{v,T_{t}}}},\n$$", "text_format": "latex", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t,j}:=\\operatorname*{min}\\bigg\\{\\frac{5}{2}\\sqrt{\\frac{2\\phi_{t,j}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}},1\\bigg\\},}\\\\ &{\\mathrm{where~}\\phi_{t,j}:=\\operatorname*{min}\\bigg\\{4\\operatorname*{max}\\bigg\\{\\hat{p}_{t,j},\\frac{25}{4}\\frac{L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}\\bigg\\},\\frac{1}{4}\\bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "\u00b7 On Line 26 to 40, the stopping rule is changed: \u00b7 Stopping condition (I) on Line 26 ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\underset{x:x\\neq x_{t,j}^{*}}{\\mathrm{min}}\\,\\hat{\\Delta}_{t,j}(x_{t,j}^{*},x)-\\alpha_{t,j}\\geq-\\varepsilon\\quad\\mathbf{or}\\quad\\hat{p}_{j}+\\beta_{t,j}<\\bar{p}\\right)}\\\\ &{\\mathrm{and}\\quad T_{t}\\geq(2L_{\\mathrm{max}}/9)\\ln(2/\\delta_{d,T_{t}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $x_{t,j}^{*}:=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}x^{\\top}\\hat{\\theta}_{t,j}$ ", "page_idx": 67}, {"type": "text", "text": "\u00b7 Lines 27'to 35: identify an $\\varepsilon$ -best arm or ascertain $p_{j}\\leq\\bar{p}$ among the remaining contexts, and record these observations in Hold for easy access in stopping condition $(\\mathbf{II})$ \u00b7 Lines 36 to 40: the recommended $\\varepsilon$ -best arm is recorded, where we adopt NAN to flag those contexts with small occurrence probabilities. ", "page_idx": 67}, {"type": "text", "text": "Theorem Q.1. Given an instance $\\Lambda$ with probability at least $1-\\delta$ Algorithm 10 can recommend an $\\varepsilon$ -best arm for all context $j\\in\\Theta_{\\bar{p}}$ with sample complexity ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\operatorname*{nax}\\left\\{\\tilde{O}\\left(\\operatorname*{max}_{\\theta_{j}^{*}\\in\\Theta_{p}}\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{p_{j}}\\right),\\tilde{O}\\left(\\operatorname*{max}_{\\theta_{j}^{*}\\notin\\Theta_{p/2}}\\frac{\\operatorname*{min}\\left\\{p_{j},\\frac{1}{64}\\right\\}L_{\\operatorname*{max}}\\ln(1/\\delta)}{(p_{j}-\\bar{p})^{2}}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "equation", "text": "$$\n\\tilde{\\cal O}\\left(\\operatorname*{max}_{\\theta_{j}^{*}\\in\\Theta_{\\tilde{p}/2}\\backslash\\Theta_{\\tilde{p}}}\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\right\\}\\frac{\\ln(1/\\delta)}{p_{j}},\\,\\frac{\\operatorname*{min}\\left\\{p_{j},\\frac{1}{64}\\right\\}L_{\\operatorname*{max}}\\ln(1/\\delta)}{(p_{j}-\\bar{p})^{2}}\\right\\}\\right)\\Bigg\\},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "which can be simplified as ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\Tilde{O}\\left(\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\varepsilon^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{\\Bar{p}}\\right).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Proof of Theorem Q.1. We provide a concise proof sketch for this theorem. ", "page_idx": 67}, {"type": "text", "text": "By adapting the stopping rule for best arm identification in stationary linear bandits to $\\varepsilon$ -best arm identification, we observe that the number of arm pulls needed for $\\varepsilon$ -best arm identification under context $j$ is ", "page_idx": 67}, {"type": "equation", "text": "$$\nT_{t,j}=\\tilde{O}\\bigg(\\frac{d}{\\big(\\Delta_{j}^{*}+\\varepsilon\\big)^{2}}\\ln\\frac{1}{\\delta}\\bigg).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "The remaining problem is to determine how many context samples/changepoints are needed such that the above number of arm samples can be achieved. ", "page_idx": 67}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\hat{p}_{t,j}=\\frac{T_{t,j}}{T_{t}}}\\end{array}$ (3.2) and Lemma I.2, we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\lvert T_{t,j}-T_{t}p_{j}\\rvert\\ge T_{t}\\beta_{t,j}\\lvert\\operatorname{Good}\\right]\\le\\delta_{d,T_{t}}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "In addition, we have an upper bound on $\\beta_{t,j}$ as in (J.5), so there would be sufficient arm samples for $\\varepsilon$ -best arm identification under context $j$ if ", "page_idx": 67}, {"type": "equation", "text": "$$\nT_{t}\\cdot\\binom{p_{j}-2\\cdot\\frac{5}{2}\\sqrt{\\frac{2\\operatorname*{min}\\big\\{16p_{j},\\frac{1}{4}\\big\\}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac{2}{\\delta_{d,T_{t}}}}}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\ge\\tilde{O}\\bigg(\\frac{d}{\\big(\\Delta_{j}^{*}+\\varepsilon\\big)^{2}}\\ln\\frac{1}{\\delta}\\bigg)\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "By solving this inequality in terms of $T_{t}$ , we obtain ", "page_idx": 67}, {"type": "equation", "text": "$$\nT_{t}=\\tilde{O}\\left(\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{p_{j}}\\right).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "As we aim to identify the $\\varepsilon$ -best arms under each context $\\theta_{j}^{*}\\in\\Theta_{\\bar{p}}$ , the upper bound is at least ", "page_idx": 68}, {"type": "equation", "text": "$$\nT_{t}=\\Tilde{O}\\left(\\operatorname*{max}_{\\theta_{j}^{*}\\in\\Theta_{\\bar{p}}}\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{p_{j}}\\right)\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "even if the set $\\Theta_{\\bar{p}}$ is given. ", "page_idx": 68}, {"type": "text", "text": "Similarly, for those contexts with small occurrence probability $\\theta_{j}^{\\ast}\\notin\\Theta_{\\bar{p}}$ , by Lemma I.2 and (J.5), we can identify them when ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{j}+2\\cdot\\cfrac52\\sqrt{\\cfrac{2\\operatorname*{min}\\left\\{16p_{j},\\frac14\\right\\}L_{\\operatorname*{max}}}{T_{t}}\\ln\\frac2{\\delta_{d,T_{t}}}}\\le\\bar{p}}\\\\ {\\Rightarrow}&{T_{t}=\\tilde{O}\\left(\\cfrac{\\operatorname*{min}\\left\\{p_{j},\\frac1{64}\\right\\}L_{\\operatorname*{max}}\\ln\\left(1/\\delta\\right)}{(p_{j}-\\bar{p})^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Careful readers may notice that the denominator depends on a \u201cprobability gap\", which can be very small. In practice, if we can actually identify the $\\varepsilon$ -best arm in those contexts, it is also acceptable. Therefore, we instead only choose not to identify the $\\varepsilon$ -best arm in those contexts $\\theta_{j}^{*}\\in\\Theta_{\\bar{p}/2}$ ,which yields an upper bound ", "page_idx": 68}, {"type": "equation", "text": "$$\nT_{t}=\\tilde{O}\\left(\\operatorname*{max}_{\\theta_{j}^{*}\\notin\\Theta_{\\bar{p}/2}}\\frac{\\operatorname*{min}\\left\\{p_{j},\\frac{1}{64}\\right\\}L_{\\operatorname*{max}}\\ln(1/\\delta)}{(p_{j}-\\bar{p})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "In tis case,the bound is at most O ( Lmuo Ia1/6) . For the rest contexts $\\theta_{j}^{*}\\notin\\Theta_{\\bar{p}/2}\\setminus\\Theta_{\\bar{p}}$ either identifying the $\\varepsilon$ -best arm or ascertaining its small occurrence probability suffices, that is, ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\Gamma_{t}=\\tilde{O}\\left(\\operatorname*{max}_{\\theta_{j}\\in\\Theta_{\\tilde{p}/2}\\backslash\\Theta_{\\tilde{p}}}\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\left(\\Delta_{j}^{*}+\\varepsilon\\right)^{2}}\\right\\}\\frac{\\ln(1/\\delta)}{p_{j}},\\,\\,\\frac{\\operatorname*{min}\\left\\{p_{j},\\frac{1}{64}\\right\\}L_{\\operatorname*{max}}\\ln(1/\\delta)}{(p_{j}-\\bar{p})^{2}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "The above bound is upper bounded by $\\begin{array}{r}{\\tilde{O}\\left(\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\varepsilon^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{\\bar{p}}\\right).}\\end{array}$ ", "page_idx": 68}, {"type": "text", "text": "By taking the maximum of (Q.3), (Q.4) and (Q.5), we can obtain a high-probability problemdependent upper bound on the sample complexity. In addition, we can get a high-probability problem-independent upper bound ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\Tilde{O}\\left(\\operatorname*{max}\\left\\{L_{\\operatorname*{max}},\\frac{d}{\\varepsilon^{2}}\\right\\}\\cdot\\frac{\\ln(1/\\delta)}{\\Bar{p}}\\right).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "By setting the threshold $\\bar{p}$ carefully, the algorithm is guaranteed to terminate before $\\tau^{*}$ given the good event Good (G.1). \u53e3 ", "page_idx": 68}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 69}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper's contributions and scope. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 69}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Justification: Empirical experiments are conducted to show that Assumption 1 is needed for theoretical proof and the proposed algorithm is robust to mild violations of the assumptions. The limitations are discussed in Appendix B. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications wouldbe.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 69}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 69}, {"type": "text", "text": "Justification: The problem setting is clearly introduced in Section 2 and all theoretical results are accompanied with proofs or proof outlines. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they. appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 70}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: The procedures of the algorithms are clearly presented. All experiment details are provided in Section 6 and Appendix O. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": ", The answer NA means that the paper does not include experiments.   \n\u2032 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n, While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 70}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: The code is released. Please refer to Section 6. Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 71}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: Please refer to Section 6 and Appendix O Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 71}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Justification: Error bars (standard deviations) are included in all experiment results ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 71}, {"type": "text", "text": "\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 72}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: Please see Appendix O. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 72}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Justification: The authors conform with the NeurIPS Code of Ethics Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPs Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 72}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 72}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Justification: The work is mainly theoretical, thus there is no identifiable societal impact. Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is ", "page_idx": 72}, {"type": "text", "text": "being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 73}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: The work is mainly theoretical. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 73}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: The MATLAB code for the computation of G-optimal allocation used existing code, and has been mentioned and cited in Appendix O. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 73}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not release new assets. ", "page_idx": 73}, {"type": "text", "text": "\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 74}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 74}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 74}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 74}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 74}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 74}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 74}]