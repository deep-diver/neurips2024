[{"type": "text", "text": "Consistency Diffusion Bridge Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guande $\\mathbf{H}\\mathbf{e}^{\\dagger1}$ ,\u2217 Kaiwen Zheng\u20201,\u2217 Jianfei Chen1, Fan Bao12, Jun Zhu\u2021123 ", "page_idx": 0}, {"type": "text", "text": "1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, THBI Lab 1Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China 2Shengshu Technology, Beijing 3Pazhou Lab (Huangpu), Guangzhou, China guande.he17@outlook.com; zkwthu@gmail.com; fan.bao@shengshu.ai; {jianfeic, dcszj}@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM\u2019s sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\\times$ to $50\\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64\\times64$ to $256\\times256$ , as well as supporting downstream tasks such as semantic interpolation in the data space. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) [53, 21, 60] have reached unprecedented levels as a family of generative models in various areas, including image generation [10, 50, 48], audio synthesis [5, 45], video generation [20], as well as image editing [41, 42], solving inverse problems [25, 56], and density estimation [59, 28, 37, 71]. In the era of AI-generated content, the stable training, scalability & state-of-the-art generation performance of DMs successfully make them serve as the fundamental component of large-scale, high-performance text-to-image [14] and text-to-video [18, 2] models. ", "page_idx": 0}, {"type": "text", "text": "A critical characteristic of diffusion models is their iterative sampling procedure, which progressively drives random noise into the data space. Although this paradigm yields a sample quality that stands out from other generation models, such as VAEs [29, 46], GANs [17], and Normalizing Flows [11, 12, 30], it also results in a notoriously lower sampling efficiency compared to other arts. In response to this, consistency models [58] have emerged as an attractive family of generative models by learning a consistency function that directly predicts the solution of a probability-flow ordinary differential equation (PF-ODE) at a certain starting timestep given any points in the ODE trajectory, designed to be a one-step generator that directly maps noise to data. Consistency models can be naturally integrated with diffusion models by adapting the score estimator of DMs to a consistency function of their PF-ODE via distillation [58, 26] or fine-tuning [15], showing promising performance for few-step generation in various applications like latent space [40] and video [64]. ", "page_idx": 0}, {"type": "image", "img_path": "FFJFGx78OK/tmp/1964643ff5df10a68e14e7922d26c5b5f01e1c23310660a2cba165d8f4333cd4.jpg", "img_caption": ["Figure 1: Illustration of consistency models (CMs) on PF-ODEs of diffusion models and our proposed consistency diffusion bridge models (CDBMs) building on PF-ODEs of diffusion bridges. Different from diffusion models, the PF-ODE of diffusion bridge is only well defined in $t<T$ due to the singularity induced by the fixed terminal endpoint. To this end, a valid input for CDBMs is some $\\pmb{x}_{t}$ for $t<T$ , which is typically obtained by one-step posterior sampling with a coarse estimation of $\\mathbf{\\nabla}_{x_{0}}$ with an initial network evaluation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the remarkable achievements in generation quality and better sampling efficiency, a fundamental limitation of diffusion models is that their prior distribution is usually restricted to a non-informative Gaussian noise, due to the nature of their underlying data to noise stochastic process. This characteristic may not always be desirable when adopting diffusion models in some scenarios with an informative non-Gaussian prior, such as image-to-image translation. Alternatively, an emergent family of generative models focuses on leveraging diffusion bridges, a series of altered diffusion processes conditioned on given endpoints, to model transport between two arbitrary distributions [44, 36, 33, 54, 51, 72, 7]. Among them, denoising diffusion bridge models (DDBMs) [72] study the reverse-time diffusion bridge conditioned on the terminal endpoint, and employ simulation-free, non-iterative training techniques for it, showing superior performance in application with coupled data pairs such as distribution translation compared to diffusion models. However, DDBMs generally require hundreds of network evaluations to produce samples with decent quality, even using an advanced high-order hybrid sampler, potentially hindering their deployments in real-world applications. ", "page_idx": 1}, {"type": "text", "text": "In this work, inspired by recent advances in consistency models with diffusion ODEs [58, 57, 15], we introduce consistency diffusion bridge models (CDBMs) and develop systematical techniques to learn the consistency function of the PF-ODEs in DDBMs for improved sampling efficiency. Firstly, to facilitate flexible integration of consistency models in DDBMs, we present a unified perspective on their design spaces, including noise schedule, prediction target, and network parameterizations, termed the same as in diffusion models [28, 24]. Additionally, we derive a first-order ODE solver based on the general-form noise schedule. This universal framework largely decouples the formulation of DDBMs and the corresponding consistency models from highly practical design spaces, allowing us to reuse the successful empirical choices of various diffusion bridges for CDBMs regardless of their different theoretical premises. On top of this, we then propose two paradigms for training CDBMs: consistency bridge distillation and consistency bridge training. This approach is free of dependence on a restricted form of noise schedule and the corresponding Euler ODE solver as in previous work [58], thus enhancing the practical versatility and extensibility of the CDBM framework. ", "page_idx": 1}, {"type": "text", "text": "We verify the effectiveness of CDBMs in two applications: image translation and image inpainting by distilling or fine-tuning DDBMs with various design spaces. Experimental results demonstrate that our approach can improve the sampling speed of DDBMs from $4\\times$ to $50\\times$ , in terms of the Fr\u00e9chet inception distance [19] (FID) evaluated with two-step generation. Meanwhile, given the same computational budget, CDBMs have better performance trade-offs compared to DDBMs, both quantitatively and qualitatively. CDBMs also retain the desirable properties of generative modeling, such as sample diversity and the ability to perform semantic interpolation in the data space. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given the data distribution $p_{\\mathrm{data}}(\\mathbf{\\boldsymbol{x}}),\\mathbf{\\boldsymbol{x}}\\in\\mathbb{R}^{m}$ , diffusion models [53, 21, 60] specify a forward-time diffusion process from an initial data distribution $p_{0}=p_{\\mathrm{data}}$ to a terminal distribution $p_{T}$ within a finite time horizon $t\\in[0,T]$ , defined by a stochastic differential equation (SDE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=\\pmb{f}(\\pmb{x}_{t},t)\\mathrm{d}t+g(t)\\mathrm{d}\\pmb{w}_{t},\\quad\\pmb{x}_{0}\\sim p_{0},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{w}_{t}$ is a standard Wiener process, $f:\\mathbb{R}^{m}\\times[0,T]\\rightarrow\\mathbb{R}^{m}$ and $g:[0,T]\\rightarrow\\mathbb{R}^{d}$ are drift and diffusion coefficients, respectively. The terminal distribution $p_{T}$ is usually designed to approximate a tractable prior $p_{\\mathrm{{prior}}}$ (e.g., standard Gaussian) with the appropriate choice of $\\boldsymbol{\\textbf{\\textit{f}}}$ and $g$ . The corresponding reverse SDE and the probability flow ordinary differential equation (PF-ODE) of the forward SDE in Eqn. (1) is given by [1, 60]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}x_{t}=[f(x_{t},t)-g^{2}(t)\\nabla\\log p_{t}(x_{t})]\\mathrm{d}t+g(t)\\mathrm{d}\\bar{w}_{t},\\quad x_{T}\\sim p_{T}\\approx p_{\\mathrm{prior}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[f(x_{t},t)-\\frac{1}{2}g^{2}(t)\\nabla\\log p_{t}(x_{t})\\right]\\mathrm{d}t,\\quad x_{T}\\sim p_{T}\\approx p_{\\mathrm{prior}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{\\pmb{w}}_{t}$ is a reverse-time standard Wiener process and $p_{t}(\\pmb{x}_{t})$ is the marginal distribution of $\\pmb{x}_{t}$ . Both the reverse SDE and PF-ODE can act as a generative model by sampling $x_{T}\\,\\sim\\,p_{\\mathrm{prior}}$ and simulating the trajectory from ${\\mathbf{}}x_{T}$ to $\\scriptstyle x_{0}$ . The major difficulty here is that the score function $\\nabla\\log p_{t}(\\pmb{x}_{t})$ remains unknown, which can be approximated by a neural network $\\boldsymbol{s}_{\\boldsymbol{\\theta}}(\\mathbf{\\boldsymbol{x}}_{t},t)$ with denoising score matching [63]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\in\\mathcal{U}(0,T)}\\mathbb{E}_{p_{0}(x_{0})p_{t|0}(x_{t}|x_{0})}\\left[\\lambda(t)||s_{\\theta}(x_{t},t)-\\nabla\\log p_{t|0}(x_{t}|x_{0})||_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{U}(0,T)$ is uniform distribution, $\\lambda(t)\\,>\\,0$ is a weighting function, and $p_{t|0}(\\mathbf{\\boldsymbol{x}}_{t}|\\mathbf{\\boldsymbol{x}}_{0})$ is the tirs aannsi tainoanl ykteircn eGl afruossmi $\\pmb{x}_{0}$ dtios $\\pmb{x}_{t}$ b. uAti coon ,s  two huesree ${\\bf\\nabla}f(t){\\bf x}_{t}$ $p_{t|0}(\\mathbf{\\boldsymbol{x}}_{t}|\\mathbf{\\boldsymbol{x}}_{0})$ $\\mathcal{N}(\\alpha_{t}\\pmb{x}_{0},\\sigma_{t}^{2}\\pmb{I})$ $\\begin{array}{r}{\\alpha_{t}=e^{\\int_{0}^{t}f(\\tau)\\mathrm{d}\\tau},\\sigma_{t}^{2}=\\alpha_{t}^{2}\\int_{0}^{t}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}}\\mathrm{d}\\tau}\\end{array}$ defined as the noise schedule [28]. The resulting score predictor $\\pmb{s}_{\\pmb{\\theta}}(\\pmb{x}_{t},t)$ can replace the true score function in Eqn. (2) and (3) to obtain the empirical diffusion SDE and ODE, which can be simulated by various SDE or ODE solvers [55, 38, 39, 16, 70]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Consistency Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a trajectory $\\{x_{t}\\}_{t=\\epsilon}^{T}$ with a fixed starting timestep $\\epsilon$ of a PF-ODE, consistency models [58] aim to learn the solution of the PF-ODE at $t=\\epsilon$ , also known as the consistency function, defined as $\\pmb{h}:(\\pmb{x}_{t},t)\\mapsto\\pmb{x}_{\\epsilon}$ . The optimization process for consistency models contains the online network $h_{\\theta}$ and a reference target network $h_{\\theta^{-}}$ , where $\\theta^{-}$ refers to $\\pmb{\\theta}$ with operation stopgrad, i.e., $\\theta^{-}=$ stopgrad $(\\pmb\\theta)$ . The networks are hand-designed to satisfy the boundary condition $h_{\\theta}(x_{\\epsilon},\\epsilon)=x_{\\epsilon}$ , which can be typically achieved with proper parameterization on the neural network. For PF-ODE taking the form in Eqn. (3) with a linear drift ${\\pmb f}(t){\\pmb x}_{t}$ , the overall learning objective of consistency models can be described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\in\\mathcal{U}(\\epsilon,T),r=r(t)}\\mathbb{E}_{p_{0}(x_{0})p_{t|0}({\\pmb x}_{t}|{\\pmb x}_{0})}\\left[\\lambda(t)d\\left(h_{\\pmb\\theta}({\\pmb x}_{t},t),h_{\\pmb\\theta^{-}}({\\pmb x}_{r},r)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\boldsymbol{r}}(t)$ is a function that specifies another timestep $r$ (usually with $t\\,>\\,r_{.}$ ), $d$ denotes some metric function with $\\forall\\pmb{x},\\pmb{y}:d(\\pmb{x},\\pmb{y})\\geq0$ and $d(\\pmb{x},\\pmb{y})\\stackrel{=}{=}0$ iff. $\\textbf{\\em x}=\\textbf{\\em y}$ . Here $\\hat{\\pmb{x}}_{r}$ is a function that estimates $\\begin{array}{r}{\\pmb{x}_{r}=\\pmb{x}_{t}+\\int_{t}^{r}\\frac{\\mathrm{d}\\pmb{x}_{\\tau}}{\\mathrm{d}\\tau}\\mathrm{d}\\tau}\\end{array}$ , which can be done by simulating the empirical diffusion ODE with a pre-trained score predictor $\\pmb{s}_{\\phi}(\\pmb{x}_{t},t)$ or empirical score estimator $-\\,{\\frac{\\mathbf{x}_{t}-\\alpha_{t}\\mathbf{x}_{0}}{\\sigma_{t}^{2}}}$ . The corresponding learning paradigms are named consistency distillation and consistency training, respectively. ", "page_idx": 2}, {"type": "text", "text": "2.3 Denoising Diffusion Bridge Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a data pair sampled from an arbitrary unknown joint distribution $(x,y)\\sim q_{\\mathrm{data}}(x,y),x,y\\in$ $\\mathbb{R}^{m}$ and let $\\mathbf{\\boldsymbol{x}}_{0}=\\mathbf{\\boldsymbol{x}}$ , denoising diffusion bridge models (DDBMs) [72] specify a stochastic process ", "page_idx": 2}, {"type": "text", "text": "that ensures $\\scriptstyle x_{T}\\;=\\;y$ almost surly via applying Doob\u2019s $h$ -transform [13, 47] on a reference diffusion process in Eqn. (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}x_{t}=\\left[f(x_{t},t)+g^{2}(t)\\nabla_{x_{t}}\\log p_{T|t}(x_{T}=y|x_{t})\\right]\\mathrm{d}t+g(t)\\mathrm{d}w_{t},\\;\\;(x_{0},x_{T})=(x,y)\\sim q_{\\mathrm{data}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{T|t}(\\pmb{x}_{T}\\,=\\,\\pmb{y}|\\pmb{x}_{t})$ is the transition kernel of the reference diffusion process from $t$ to $T$ , evaluated at $\\scriptstyle x_{T}\\;=\\;y$ . Denoting the marginal distribution of Eqn. (6) as $\\{q_{t}\\}_{t=0}^{T}$ , it can be shown that the forward bridge SDE in Eqn. (6) is characterized by the diffusion distribution conditioned on both endpoints, that is, $q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})=p_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})$ , which is an analytic Gaussian distribution. A generative model can be obtained by modeling $q_{t\\mid T}(\\mathbf{x}_{t}|\\mathbf{x}_{T}=\\pmb{y})$ , whose reverse SDE and PF-ODE are given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{i}x_{t}=\\left[f(x_{t},t)-g^{2}(t)\\left(\\nabla_{x_{t}}\\log q_{t|T}(x_{t}|x_{T}=y)-\\nabla_{x_{t}}\\log p_{T|t}(x_{T}=y|x_{t})\\right)\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\bar{w}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[f(x_{t},t)-g^{2}(t)\\left[\\frac{1}{2}\\nabla_{x_{t}}\\log q_{t|T}(x_{t}|x_{T}=y)-\\nabla_{x_{t}}\\log p_{T|t}(x_{T}=y|x_{t})\\right]\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The only unknown term remains is the score function $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}\\,=\\,\\pmb{y})$ , which can be estimated with a neural network $s_{\\theta}(\\pmb{x}_{t},t,\\pmb{y})$ via denoising bridge score matching (DBSM): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\bar{\\Sigma}}_{t\\in\\mathcal{U}(0,T)}\\mathbb{E}_{q_{\\mathrm{data}}(x,y)q_{t}|_{0}T(x_{t}|x_{0}=x,x_{T}=y)}\\left[\\lambda(t)\\|s_{\\theta}(x_{t},t,y)-\\nabla\\log q_{t}|_{0}T(x_{t}|x_{0}=x,x_{T}=y)\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Replacing $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}=\\pmb{y})$ in Eqn. (7) and (8) with the learned score predictor $\\pmb{s}_{\\theta}(\\pmb{x}_{t},t,\\pmb{y})$ would yield the empirical bridge SDE and ODE that could be solved for generation purposes. ", "page_idx": 3}, {"type": "text", "text": "3 Consistency Diffusion Bridge Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce consistency diffusion bridge models, extending the techniques of consistency models to DDBMs to further boost their performance and sample efficiency. Define the consistency function of the bridge ODE in Eqn. (8) as $\\pmb{h}:(\\pmb{x}_{t},t,\\pmb{y})\\mapsto\\pmb{x}_{\\epsilon}$ with a given starting timestep $\\epsilon$ , our goal is to learn the consistency function using a neural network $h_{\\theta}(\\cdot,\\cdot,y)$ with the following high-level objective similar to Eqn. (5): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\in\\mathcal{U}(\\epsilon,T),r=r(t)}\\mathbb{E}_{q_{\\mathrm{data}}(x,y)q_{t|0T}(x_{t}|x_{0}=x,x_{T}=y)}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y),h_{\\theta^{-}}(\\hat{x}_{r},r,y)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To begin with, we first present a unified view of the design spaces such as noise schedule, network parameterization $\\&$ precondition, as well as a general ODE solver for DDBMs. This allows us to: (1) decouple the successful practical designs of previous diffusion bridges from their different theoretical premises; (2) decouple the framework of consistency models from certain design choices of the corresponding PF-ODE, such as the reliance on VE schedule with Euler ODE solver of the original derivation of consistency models [58]. This would largely facilitate the development of consistency models that utilize the rich design spaces of existing diffusion bridges on DDBMs in a universal way. Then, we elaborate on two ways to train $h_{\\theta}$ based on different choices of $\\hat{\\pmb{x}}_{r}$ consistency bridge distillation, and consistency bridge training, with the proposed unified design spaces. ", "page_idx": 3}, {"type": "text", "text": "3.1 A Unified View on Design Spaces of DDBMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Noise Schedule We consider the linear drift ${\\bf\\nabla}f(t){\\bf x}_{t}$ and define: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha_{t}=e^{\\int_{0}^{t}f(\\tau)\\mathrm{d}\\tau},\\quad\\bar{\\alpha}_{t}=e^{-\\int_{t}^{T}f(\\tau)\\mathrm{d}\\tau},\\quad\\rho_{t}^{2}=\\int_{0}^{t}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}}\\mathrm{d}\\tau,\\quad\\bar{\\rho}_{t}^{2}=\\int_{t}^{T}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}}\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which aligns with the common notation of noise schedules used in diffusion models by denoting $\\sigma_{t}=\\alpha_{t}\\rho_{t}$ . Then we could express the analytic conditional distributions of DDBMs as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})=p_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})=\\mathcal{N}\\left(a_{t}\\pmb{x}_{T}+b_{t}\\pmb{x}_{0},c_{t}^{2}\\pmb{I}\\right),}\\\\ &{\\quad\\quad\\mathrm{where}\\quad a_{t}=\\frac{\\bar{\\alpha}_{t}\\rho_{t}^{2}}{\\rho_{T}^{2}},\\quad b_{t}=\\frac{\\alpha_{t}\\bar{\\rho}_{t}^{2}}{\\rho_{T}^{2}},\\quad c_{t}^{2}=\\frac{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}\\rho_{t}^{2}}{\\rho_{T}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The form of $q_{t|0T}$ is consistent with the original formulation of DDBM in [72]. Here, inspired by [6], we opt to adopt a more neat set of notations for enhanced compatibility. As shown in Table 1, with such notations, we could easily unify the design choices for diffusion bridges [33, 72, 6] that have shown effectiveness in various tasks and expeditiously employ consistency models on top of them. ", "page_idx": 3}, {"type": "table", "img_path": "FFJFGx78OK/tmp/bbc4fa8775d39cdce2748e8550e8f589176f9057890e7fc6338a398d618a41a2.jpg", "table_caption": ["Table 1: Specifications of design spaces in different diffusion bridges. The details of network parameterization are in Appendix B.4 due to space limit. "], "table_footnote": ["\u2020 Though I2SB is built on a discrete-time schedule for $\\overline{{T=1000}}$ timesteps, it can be converted to a continuous-time schedule on $t\\in[0,1]$ approximately by mapping $t$ to $t/(T-1)$ . \u2021 The authors change to the same VP schedule as Bridge-TTS with parameters $\\beta_{0}=0.1,\\beta_{d}=2$ in a revised version of their paper. "], "page_idx": 4}, {"type": "text", "text": "Network Parameterization & Precondition In practice, the neural network $F_{\\theta}$ in DBMs does not always directly regress to the target score function; instead, it can predict other equivalent quantities, such as the data predictor $\\begin{array}{r}{\\mathbf{\\emx}_{\\theta}=\\frac{\\mathbf{\\emx}_{t}-a_{t}\\mathbf{\\emx}_{T}+c_{t}^{2}\\mathbf{\\ems}_{\\theta}}{b_{t}}}\\end{array}$ for a Gaussian $\\mathcal{N}(a_{t}\\mathbf{\\Deltax}_{T}+b_{t}\\mathbf{x}_{0},c_{t}^{2}I)$ like $q_{t|0T}$ . Meanwhile, the inputs and outputs of the network $F_{\\theta}$ could be rescaled for a better-behaved optimization process, known as the network precondition. As shown in Table 1, we could consistently use $\\scriptstyle x_{0}$ as the prediction target with different choices of network precondition to unify the previous practical designs for DBMs. ", "page_idx": 4}, {"type": "text", "text": "PF-ODE and ODE Solver The validity of a consistency model relies on an underlying PF-ODE that shares the same marginal distribution with the forward process. In the original DDBM paper [72], the marginal preserving property of the proposed ODE is justified following an analogous logic from the derivation of the PF-ODE of diffusion models [60] with Kolmogorov forward equation. However, its validity suffers from doubts as there is a singularity at the deterministic starting point ${\\mathbf{}}x_{T}$ . Here, we provide a simple example to show that the ODE can indeed maintain the marginal distribution as long as we use a valid stochastic step to skip the singular point and start from $T-\\gamma$ for any $\\gamma>0$ . ", "page_idx": 4}, {"type": "text", "text": "Example 3.1. Assume $T=1$ and consider a simple Brownian Bridge between two fixed points $(x_{0},x_{1})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\frac{x_{1}-x_{t}}{1-t}\\mathrm{d}t+\\mathrm{d}w_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with marginal distribution $q_{t|01}(x_{t}|x_{0},x_{1})=\\mathcal{N}((1-t)x_{0}+t x_{1},t(1-t))$ . The ground-truth reverse SDE and PF-ODE are given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\frac{x_{t}-x_{0}}{t}\\mathrm{d}t+\\mathrm{d}\\bar{w}_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left({\\frac{1-2t}{2t(1-t)}}x_{t}+{\\frac{1}{2(1-t)}}x_{1}-{\\frac{1}{2t}}x_{0}\\right)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then first simulating the reverse SDE in Eqn. $(I4)$ from $t=1$ to $t=1-\\gamma$ for some $\\gamma\\in(0,1)$ and then starting to simulate the PF-ODE in Eqn. (15) will preserve the marginal distribution. ", "page_idx": 4}, {"type": "text", "text": "The detailed derivation can be found in Appendix. B.2. Therefore, the time horizon of the consistency model based on the bridge ODE needs to be set as $t\\in[\\epsilon,T-\\gamma]$ for some pre-specified $\\epsilon,\\gamma>0$ Additionally, the marginal preservation of the bridge ODE for more general diffusion bridges can be strictly justified by considering non-Markovian variants, as done in DBIM [69]. ", "page_idx": 4}, {"type": "text", "text": "Another crucial element for developing consistency models is the ODE solver, as a solver with a lower local error would yield lower error for consistency distillation, as well as the corresponding consistency training objectives [58, 57]. Inspired by the successful practice of advanced ODE solvers based on the Exponential Integrator (EI) [4, 22] in diffusion models, we present a first-order bridge ODE solver in a similar fashion: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 3.1. Given an initial value $\\pmb{x}_{t}$ at time $t>0$ , the first-order solver of the bridge ODE in Eqn. (8) from $t$ to $r\\in[0,t]$ with the noise schedule defined in Eqn. $(I I)$ is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf{x}}_{r}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}{\\bf{x}}_{t}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left[\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right){\\bf{x}}_{\\theta}({x}_{t},t,y)+\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)\\frac{y}{\\alpha_{T}}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We provide detailed derivation in the Appendix B.1. Typically, an EI-based solver enjoys a lower discretization error and therefore has better empirical performance [16, 38, 39, 67, 70]. Another notable advantage of this general form solver, as we will show in Section 3.3, is that it could naturally establish the connection between consistency training and consistency distillation for any noise schedules that take the form in Eqn. (11), eliminating the dependence of the VE schedule and the corresponding Euler ODE solver in the common derivation [58]. ", "page_idx": 5}, {"type": "text", "text": "3.2 Consistency Bridge Distillation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Analogous to consistency distillation with the empirical diffusion ODE, we could leverage a pretrained score predictor $s_{\\phi}(\\mathbf{x}_{t},t,\\mathbf{y})\\approx\\nabla_{\\mathbf{x}_{t}}\\log q_{t|T}(\\mathbf{x}_{t}|x_{T}=\\mathbf{y})$ to solve the empirical bridge ODE to obtain $\\hat{\\pmb{x}}_{r}$ , i.e., $\\hat{\\mathbf{x}}_{r}=\\hat{\\mathbf{x}}_{\\phi}(\\mathbf{x}_{t},t,r,\\pmb{y})$ , where $\\hat{\\pmb{x}}_{\\phi}$ is the update function of a one-step ODE solver with fixed $s_{\\phi}$ . We define the consistency bridge distillation (CBD) loss as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{L}_{\\mathrm{CBD}}^{\\mathrm{A}t_{\\mathrm{max}}}:=}&{(17)}\\\\ &{\\mathbb{E}_{t\\in U(\\epsilon,T-\\gamma),r=r(t)}\\mathbb{E}_{q_{\\mathrm{data}}(x,y)q_{t/0T}(x_{t}|x_{0}=x,x_{T}=y)}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y),h_{\\theta^{-}}(\\hat{x}_{\\phi}(x_{t},t,r,y),r,y)\\right)\\right]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t$ is sampled from the uniform distribution over $[\\epsilon,T-\\gamma],\\,r(t)$ is a function specifies another timestep $r$ such that $\\epsilon\\leq r<t$ with $\\Delta t_{\\operatorname*{max}}:=\\operatorname*{max}_{t}\\{\\dot{t}-r(t)\\}$ and $\\Delta t_{\\operatorname*{min}}:=\\operatorname*{min}_{t}\\bar{\\{}t-r(t)\\},\\lambda($ $\\lambda(t)$ is a positive weighting function, $d$ is some distance metric function with $\\forall\\pmb{x},\\pmb{y}:d(\\pmb{x},\\pmb{y})\\geq0$ and $\\bar{d}(\\pmb{x},\\pmb{y})=0$ iff. $\\mathbf{\\boldsymbol{x}}=\\mathbf{\\boldsymbol{y}}$ , and $\\pmb{\\theta}^{-}=\\mathrm{stopgrad}(\\pmb{\\theta})$ . Similarly to the case of consistency distillation in empirical diffusion ODEs, we have the following asymptotic analysis of the CBD objective: ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Given $\\Delta t_{\\operatorname*{max}}=\\operatorname*{max}_{t}\\{t-r(t)\\}$ and let $h_{\\phi}(\\cdot,\\cdot,\\cdot)$ be the consistency function of the empirical bridge ODE taking the form in Eqn. (8). Assume $h_{\\theta}$ is a Lipschitz function, i.e., there exists $L>0$ , such that for all $t\\in[\\epsilon,T-\\gamma],x_{1},x_{2},y,$ , we have $\\|h_{\\theta}(x_{1},t,{\\pmb y})-h_{\\theta}({\\pmb x}_{2},t,{\\pmb y})\\|_{2}\\leq$ $L\\|{\\pmb x}_{1}-{\\pmb x}_{2}\\|_{2}$ . Meanwhile, assume that for all $t,r\\in[\\epsilon,T-\\gamma]$ , $\\pmb{y}\\sim q_{\\mathrm{data}}(\\pmb{y}):=\\mathbb{E}_{\\pmb{x}}[q_{\\mathrm{data}}(\\pmb{x},\\pmb{y})]$ , the ODE solver $\\hat{x}_{\\phi}(\\cdot,t,r,y)$ has local error uniformly bounded by $O((t-r)^{p+1})$ with $p\\geq1$ . Then, $i f\\,\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=0,$ , we have: $\\begin{array}{r}{\\operatorname*{sup}_{t,x,y}\\|h_{\\theta}(\\mathbf{\\boldsymbol{x}},t,y)-h_{\\phi}(\\mathbf{\\boldsymbol{x}},t,y)\\|_{2}=O((\\Delta t_{\\operatorname*{max}})^{p})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The vast majority of the analysis can be done by directly following the proof in [58] with minor differences between the overlapped timestep intervals $\\{t,r(t)\\}$ for $t\\in[\\epsilon,T\\!-\\!\\gamma]$ used in Eqn. (17) and the fixed timestep intervals $\\{t_{n}\\}_{n=1}^{N}$ used in [58]. We include it in Appendix B.5 for completeness. In this work, unless otherwise stated, we use the first-order ODE solver in Eqn. (16) as $\\hat{\\pmb{x}}_{\\phi}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Consistency Bridge Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In addition to distilling from pre-trained score predictor $s_{\\phi}$ , consistency models can be trained [58, 57] or fine-tuned [15] by maintaining only one set of parameters $\\pmb{\\theta}$ . To accomplish this, we could leverage the unbiased score estimator: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log q_{t|T}(x_{t}|\\mathbf{x}_{T}=y)=\\mathbb{E}_{\\mathbf{x}_{0}}[\\nabla_{\\mathbf{x}_{t}}\\log q_{t|0T}(x_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T})|x_{t},x_{T}=y],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "that is, with a single sample $(x,y)\\,\\sim\\,q_{\\mathrm{data}}$ and $\\pmb{x}_{t}\\;\\sim\\;q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0}\\;=\\;\\pmb{x},\\pmb{x}_{T}\\;=\\;\\pmb{y})$ , the score $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}=\\pmb{y})$ can be estimated with $\\nabla_{\\pmb{x}_{t}}\\log q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})$ . Substituting such an estimation of $s_{\\phi}$ into the one-step ODE solver $\\hat{\\pmb{x}}_{\\phi}$ in Eqn. (17) with the transformation between data and score predictor $\\begin{array}{r}{\\mathbf{\\emx}_{\\phi}=\\frac{\\mathbf{\\emx}_{t}-a_{t}\\mathbf{\\emx}_{T}+c_{t}^{2}\\mathbf{\\ems}_{\\phi}}{b_{t}}}\\end{array}$ , we can obtain an alternative $\\hat{\\pmb{x}}_{r}$ that does not rely on the pre-trained $s_{\\phi}$ for any noise schedule taking the form in Eqn. (11) as follows (detail in Appendix B.3): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}_{r}=\\hat{\\pmb{x}}(\\pmb{x}_{t},t,r,\\pmb{x},\\pmb{y})=a_{r}\\pmb{y}+b_{r}\\pmb{x}+c_{r}z,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $a_{r},b_{r},c_{r}$ are defined as in Eqn. (11), and $\\begin{array}{r}{z\\;=\\;\\frac{\\pmb{x}_{t}-a_{t}\\pmb{y}-b_{t}\\pmb{x}}{c_{t}}\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\pmb{I})}\\end{array}$ . Based on this instantiation of $\\hat{\\pmb{x}}_{r}$ , we define the consistency bridge training (CBT) loss as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{L}_{\\mathrm{CBT}}^{\\mathrm{A}t_{\\mathrm{max}}}:=}&{(2\\mathrm{i}\\pi/\\Gamma)}\\\\ &{\\mathbb{E}_{t\\in U(\\epsilon,T-\\gamma),r=r(t)}\\mathbb{E}_{q_{\\mathrm{data}}(x,y)}\\left[\\lambda(t)d\\left(h_{\\theta}(a_{t}y+b_{t}x+c_{t}z,t,y),h_{\\theta^{-}}(a_{r}y+b_{r}x+c_{r}z,r,y)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $t,r(\\cdot),\\lambda(\\cdot),\\pmb{\\theta}^{-1}$ are defined the same as in Eqn. (17), and $z\\sim\\mathcal{N}(\\mathbf{0},I)$ is a shared Gaussian noise used in both $h_{\\theta}$ and $h_{\\theta^{-1}}$ . We have the following proposition demonstrating the connection between LC\u2206tBmTaxand LC\u2206tBmDaxwith the first-order one-step ODE solver: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.3. Given $\\Delta t_{\\operatorname*{max}}~=~\\operatorname*{max}_{t}\\{t\\,-\\,r(t)\\}$ and assume $d,h_{\\theta},f,g$ are twice continuously differentiable with bounded second derivatives, the weighting function $\\lambda(\\cdot)$ is bounded, and $\\mathbb{E}[\\|\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T})\\|_{2}^{2}]\\,<\\,\\infty$ . Meanwhile, assume that $\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}$ employs the one-step $O D E$ solver in Eqn. (16) with ground truth pre-trained score model, i.e., $\\forall t\\in[\\epsilon,T-\\gamma],\\pmb{y}\\sim q_{\\mathrm{data}}(\\pmb{y}):$ $s_{\\phi}(\\mathbf{x}_{t},t,\\mathbf{y})\\equiv\\nabla_{\\mathbf{x}_{t}}\\log q_{t|T}(\\mathbf{x}_{t}|\\mathbf{x}_{T}=\\mathbf{y})$ . Then, we have: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=\\mathcal{L}_{\\mathrm{CBT}}^{\\Delta t_{\\mathrm{max}}}+o(\\Delta t_{\\mathrm{max}})}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The core part of our analysis also follows [58], except the connection between the CBD & CBT objective relies on the proposed first-order ODE solver and the estimated $\\hat{\\pmb{x}}_{r}$ in Eqn. (19) with the general noise schedule for DDBM. We include the details in Appendix B.6. ", "page_idx": 6}, {"type": "text", "text": "3.4 Network Precondition and Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Network Precondition First, we focus on enforcing the boundary condition $\\begin{array}{r}{h_{\\theta}(x_{\\epsilon},\\epsilon,y)=x_{\\epsilon}}\\end{array}$ of our consistency bridge model, which can be done by designing a proper network precondition. Usually, a variable substitution $\\tilde{t}=t-\\epsilon$ could work in most cases. For example, for the precondition for $\\mathrm{I}^{2}\\mathrm{SB}$ in Table 1, we have $\\begin{array}{r}{\\pmb{x}_{\\epsilon}+\\sigma_{\\tilde{\\epsilon}}\\pmb{F_{\\theta}}\\,=\\,\\pmb{x}_{\\epsilon}+\\sqrt{\\int_{0}^{\\epsilon-\\epsilon}g^{2}(\\tau)\\mathrm{d}\\tau}\\,=\\,\\pmb{x}_{\\epsilon}}\\end{array}$ . Also, the common \u201cEDM\u201d [24] style precondition used in DDBM also satisfies $c_{\\mathrm{skip}}(\\tilde{\\epsilon})=1$ and $c_{\\mathrm{out}}(\\tilde{\\epsilon})=0$ . We also give a universal precondition to satisfy the boundary conditions based on the form of the ODE solver in Eqn. (16) in Appendix B.4 to cope with the case where the variable substitution is not applicable. ", "page_idx": 6}, {"type": "text", "text": "Sampling As explained in Section 3.1, the PF-ODE is only well-defined within the time horizon $0\\le t\\le T-\\gamma$ for some $\\gamma\\in(0,T)$ . Hence, the sampling of CDBMs should start with $x_{T-\\gamma}\\sim$ $q_{T-\\gamma|T}({\\pmb x}_{T-\\gamma}|{\\pmb x}_{T}={\\pmb y})$ , which can be obtained by simulating the reverse SDE in Eqn. (7) from $T$ to $T-\\gamma$ . Here we opt to use one first-order stochastic step, which is equivalent to performing posterior sampling, i.e., $\\begin{array}{r}{x_{T-\\gamma}\\,\\sim\\,q_{T-\\gamma|0T}(x_{T-\\gamma}|x_{0}\\,=\\,h_{\\theta}(\\dot{x}_{T},T,y),\\dot{x_{T}}=y)}\\end{array}$ . This sampling approach defaults to two NFEs (Number of Function Evaluations), which is aligned with the practical guideline that employing two-step sampling in CM allows for a better trade-off between quality and computation compared to other treatments such as scaling up models [15]. We could also alternate a forward noising step and a backward consistency step multiple times to further improve sample quality as consistency models do. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Task, Datasets, and Metrics In this work, we conduct experiments for CDBM on image-to-image translation and image inpainting tasks with various image resolutions and scales of the data set. For image-to-image translation, we use the Edges $\\rightarrow$ Handbags [23] with $64\\times64$ pixel resolution and DIODE-Outdoor [62] with $256\\times256$ pixel resolution. For image inpainting, we choose ImageNet [9] $256\\times256$ with a center mask of size $128\\times128$ . Regarding the evaluation metrics, we report the Fr\u00e9chet inception distance (FID) [19] for all datasets. Furthermore, following previous works [33, 72], we measure Inception Scores (IS) [3], LPIPS [68] and Mean Square Error (MSE) for image-to-image translation and Classifier Accuracy (CA) of a pre-trained ResNet50 for image-inpainting. The metrics are computed using the complete training set for Edges $\\rightarrow$ Handbags and DIODE-Outdoor, and a validation subset of 10,000 images for ImageNet. ", "page_idx": 6}, {"type": "text", "text": "Training Configurations We train CDBM in two ways: distill pre-trained DDBM with CBD or finetuning DDBM with CBT. We keep the noise schedule and prediction target of the pre-trained DDBM unchanged and modify the network precondition to satisfy the boundary condition. Specifically, we adopt the design space of DDBM-VP and $\\mathrm{I}^{2}\\mathrm{SB}$ in Table 1 on image-to-image translation and image inpainting, respectively. We specify complete training details in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Specification of Design Choices We illustrate the specific design choices for CDBM. In this work, we use $t\\in[\\epsilon,1-\\gamma]$ and set $\\epsilon=0.0001,\\gamma=0.001$ and sample $t$ uniformly during training. We employ two different sets of the timestep function ${\\boldsymbol{r}}(t)$ and the loss weighting $\\lambda(t)$ , also named the training schedule for CDBM. The first, following [58], specifies a constant quantity for $\\Delta t=t-r(t)$ with a simple loss weighting of $\\lambda(t)=1$ . The constant gap $\\Delta t$ is treated as a hyperparameter and we search it among $\\{1/9,1/18,1/36,1/60,1/80,1/120\\}$ . The other employs $r(t)$ that gradually shrinks $t-r(t)$ during the training process and a loss weighting of $\\begin{array}{r}{\\lambda(t)\\bar{=}\\,\\frac{1}{t-r(t)}}\\end{array}$ , which enjoys a better trade-off between faster convergence and performance [58, 57, 15]. Following [15], we use a sigmoid-style function $\\begin{array}{r}{r(t)=t(1-\\frac{1}{q^{\\lfloor\\mathrm{iters}/s\\rfloor}})(1+\\frac{k}{1+e^{b t}})}\\end{array}$ , where iters is the number of training iterations, $\\boldsymbol{q},\\boldsymbol{s},\\boldsymbol{k},\\boldsymbol{b}$ are hyperparameters. We use $q=2,k=8$ , and tune $b\\in\\{1,2,5,10,20,50\\}$ and $s\\in\\{5000,10000\\}$ . ", "page_idx": 7}, {"type": "table", "img_path": "FFJFGx78OK/tmp/ade5188d5d046fbfc3560946ef3959175048345bc010b86323cb7aaba45f6232.jpg", "table_caption": ["Table 2: Quantitative Results on the Image-to-Image Translation Task "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "FFJFGx78OK/tmp/40943e190305cf1f12f9b4a44dc83c45936510fa15650c95a0ebdf0d4dedb131.jpg", "img_caption": ["Figure 2: NFE-FID plot of CDBM and DDBM on ImageNet $256\\times256$ "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "FFJFGx78OK/tmp/c455f2210fae4d6a8a10c8e92281d51c4b9aa5bff7b816e83d695ad88828fa18.jpg", "img_caption": ["Figure 3: Ablation for hyperparameters of CDBM "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FFJFGx78OK/tmp/8f05b86ddfd6047d6964f678a202fd2710812a624c3cbe2e85ebbef66a31e347.jpg", "table_caption": ["Table 3: Quantitative Results on the Image Inpainting Task "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Results for Few-step Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present the quantitative results of CDBM on image-to-image translation and image inpainting tasks in Table 2 and Table 3. We adopt DDBM on the same noise schedule and network architecture, with the first-order ODE solver in Eqn. (16) as our main baseline (i.e., \u201cDDBM (ODE-1)\u201d). We report the performance of the baseline DDBM under different Number of Function Evaluations (NFE) as a reference for the sampling acceleration ratio (Reduction factor of NFE to achieve the same FID) of CDBM. Following [72, 33], we report the result of other baselines with $\\mathrm{NFE}\\geq40$ , which consists of diffusion-based methods, diffusion bridges with different formulations, or samplers. We mainly focus on the two-step generation scenario for CDBM, which is the minimal NFEs required for CDBM using the sampling procedure described in Section 3.4. ", "page_idx": 7}, {"type": "image", "img_path": "FFJFGx78OK/tmp/941c9d1ef264680f6f0adeaa6241ddaf7e5710debd3ffe13ab35e84501d3deea.jpg", "img_caption": ["Figure 4: Qualitative demonstration between DDBM and CDBM. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "FFJFGx78OK/tmp/37cc3f2288a8b48d02607993cb474e8939e836b9ee11fed4e0e22d9b0f7ede59.jpg", "img_caption": ["Figure 5: Example semantic interpolation result with CDBMs "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "For image-to-image translation, as shown in Table. 2, we first observed that our proposed firstorder ODE solver has superior performance compared to the hybrid high-order sampler used in DDBM [72]. On top of that, CDBM\u2019s FID at $\\mathrm{NFE}=2$ is close to or even better than DDBM\u2019s at NFE around 100 with the advanced ODE solver, achieving a sampling speed-up around $50\\times$ . This can be corroborated by the qualitative demonstration in Fig. 4, where CDBMs drastically reduce the blurring effect on DDBMs under few-step generation settings while enjoying realistic and faithful translation performance. ", "page_idx": 8}, {"type": "text", "text": "For image inpainting, as shown in Table. 3, the baseline ODE solver for DDBM achieves decent sample quality at $\\mathrm{NFE}\\,=\\,10$ . For CDBM, as shown in Fig. 2, the acceleration ratio is relatively modest in such a large-scale and challenging dataset, achieving close to a $4\\times$ increase in sampling speed. Notably, CBT\u2019s FID at $\\mathrm{NFE}\\,=\\,4$ matches DDBM at $\\mathrm{NFE}\\,=\\,10$ . Moreover, we find that CDBMs have better visual quality than DDBM given the same computation budget, as shown in Fig. 4 and Appendix D, which illustrates that CDBM yields a better quality-efficiency trade-off. ", "page_idx": 8}, {"type": "text", "text": "Meanwhile, we observe that fine-tuning DDBMs with CBT generally produces better results than CBD in all three data sets, demonstrating fine-tuning a pre-trained score model to a consistency function is a more promising solution with less computational and memory cost compared to distillation, which is consistent with recent findings [15]. We also conducted an ablation study for CBD and CBT under different training schedules (i.e., the combination of the timestep function ${\\boldsymbol{r}}(t)$ and the loss weighting $\\lambda(t))$ on ImageNet $256\\times256$ . As shown in Fig. 3, for a small timestep interval $t-r(t)$ , e.g., a small $\\Delta t$ in Fig. 3a or a large $b$ in Fig. 3b (detail in Appendix C.2), the performance is generally better but also suffers from training instability, indicated by the sharp increase in FID during training when $\\Delta t=1/120$ and $b=50$ . While for a large timestep interval, the performance at convergence is usually worse. In practice, we found that adopting the training schedule that gradually shrinks $r(t)-t$ with $b=20$ or 50 with CBT could work across all tasks, whereas CBD generally needs a meticulous design for $\\Delta t$ or $b$ to ensure stable training and satisfactory performance. ", "page_idx": 8}, {"type": "text", "text": "4.3 Semantic Interpolation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We show that CDBMs support performing downstream tasks, such as semantic interpolation, similar to diffusion models [55]. Recall that the sampling process for CDBM alternates between consistency function evaluation and forward sampling, we could track all noises and the corresponding timesteps to re-generate the same sample. By interpolating the noises of two sampling trajectories, we can obtain a series of samples lying between the semantics of two source samples, as shown in Fig. 5, which demonstrates that CDBMs have a wide range of generative modeling capabilities, such as sample diversity and semantic interpolation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce consistency diffusion bridge models (CDBMs) to address the sampling inefficiency of DDBMs and present two frameworks, consistency bridge distillation and consistency bridge training, to learn the consistency function of the DDBM\u2019s PF-ODE. Building on a unified view of design spaces and the corresponding general-form ODE solver, CDBM exhibits significant flexibility and adaptability, allowing for straightforward integration with previously established successful designs for diffusion bridges. Experimental evaluations across three datasets show that CDBM can effectively boost the sampling speed of DDBM by $4\\times$ to $50\\times$ . Furthermore, it achieves the saturated performance of DDBMs with less than five NFEs and possesses the broad capacity of generative models, such as sample diversity and semantic interpolation. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Broader Impact While significantly improving the sampling efficiency in the datasets we used, it remains to be explored how the proposed CDBM, along with the DDBM formulation, performs in datasets with larger-scale or more complex characteristics. Furthermore, the consistency model paradigm typically suffers from numerical instability and it would be a promising research direction to keep improving CDBM\u2019s performance from an optimization perspective. With enhanced sampling efficiency, CDBMs could contribute to more energy-efficient deployment of generative models, aligning with broader goals of sustainable AI development. However, it could also lower the cost associated with the potential misuse for creating deceptive content. We hope that our work will be enforced with certain ethical guidelines to prevent any form of harm. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Science and Technology Major Project (2021ZD0110502), NSFC Projects (Nos. 62350080, 62106122, 92248303, 92370124, 62350080, 62276149, U2341228, 62076147), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPlorer Prize. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[2] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.   \n[3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.   \n[4] Mari Paz Calvo and C\u00e9sar Palencia. A class of explicit multistep exponential integrators for semilinear problems. Numerische Mathematik, 102:367\u2013381, 2006.   \n[5] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021.   \n[6] Zehua Chen, Guande He, Kaiwen Zheng, Xu Tan, and Jun Zhu. Schrodinger bridges beat diffusion models on text-to-speech synthesis. arXiv preprint arXiv:2312.03491, 2023.   \n[7] Valentin De Bortoli, Guan-Horng Liu, Tianrong Chen, Evangelos A Theodorou, and Weilie Nie. Augmented bridge matching. arXiv preprint arXiv:2311.06978, 2023.   \n[8] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.   \n[10] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 8780\u2013 8794, 2021.   \n[11] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.   \n[12] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations, 2016.   \n[13] Joseph L Doob and JI Doob. Classical potential theory and its probabilistic counterpart, volume 262. Springer, 1984.   \n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \n[15] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024.   \n[16] Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudi. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. arXiv preprint arXiv:2305.14267, 2023.   \n[17] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, pages 2672\u20132680, 2014.   \n[18] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.   \n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 6626\u20136637, 2017.   \n[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851, 2020.   \n[22] Marlis Hochbruck, Alexander Ostermann, and Julia Schweitzer. Exponential rosenbrock-type methods. SIAM Journal on Numerical Analysis, 47(1):786\u2013803, 2009.   \n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.   \n[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, 2022.   \n[25] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Advances in Neural Information Processing Systems, 2022.   \n[26] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2024.   \n[27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[28] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances in Neural Information Processing Systems, 2021.   \n[29] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.   \n[30] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.   \n[31] Liangchen Li and Jiajun He. Bidirectional consistency models. arXiv preprint arXiv:2403.18035, 2024.   \n[32] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[33] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. $\\mathrm{I^{2}s b}$ : Image-to-image schr\u00f6dinger bridge. In International Conference on Machine Learning, pages 22042\u201322062. PMLR, 2023.   \n[34] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2019.   \n[35] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022.   \n[36] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International Conference on Machine Learning, pages 14429\u201314460. PMLR, 2022.   \n[38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information Processing Systems, 2022.   \n[39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[40] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.   \n[41] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022.   \n[42] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 16784\u201316804. PMLR, 2022.   \n[43] Stefano Peluchetti. Diffusion bridge mixture transports, schr\u00f6dinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):1\u201351, 2023.   \n[44] Stefano Peluchetti. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589, 2023.   \n[45] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei. Diffusion-based voice conversion with fast maximum likelihood sampling scheme. In International Conference on Learning Representations, 2022.   \n[46] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[47] L Chris G Rogers and David Williams. Diffusions, Markov processes and martingales: Volume 2, It\u00f4 calculus, volume 2. Cambridge university press, 2000.   \n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[49] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.   \n[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022.   \n[51] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Conditional simulation using diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1792\u20131802. PMLR, 2022.   \n[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \n[54] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1985\u20131995. PMLR, 2023.   \n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.   \n[56] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023.   \n[57] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024.   \n[58] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 32211\u201332252. PMLR, 2023.   \n[59] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In Advances in Neural Information Processing Systems, volume 34, pages 1415\u20131428, 2021.   \n[60] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[61] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. arXiv preprint arXiv:2203.08382, 2022.   \n[62] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019.   \n[63] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[64] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023.   \n[65] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In The Eleventh International Conference on Learning Representations, 2023.   \n[66] Yuji Wang, Zehua Chen, Xiaoyu Chen, Jun Zhu, and Jianfei Chen. Framebridge: Improving image-to-video generation with bridge models. arXiv preprint arXiv:2410.15371, 2024.   \n[67] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2023.   \n[68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[69] Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024.   \n[70] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[71] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation for diffusion odes. In International Conference on Machine Learning, pages 42363\u201342389. PMLR, 2023.   \n[72] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Diffusion Bridges Diffusion bridges [44, 36, 33, 54, 51, 72, 7, 6] are an emerging class of generative models with attractive flexibility in modeling the stochastic process between two arbitrary distributions. The flow matching [32], and its stochastic counterpart, bridge matching [44] assume the access of a joint distribution and an interpolation, or a forward process, between the samples, then, another SDE/ODE is learned to estimate the dynamics of the pre-defined interpolation, which can be used for generative modeling from non-Gaussian priors [33, 6, 72, 69, 66]. In particular, the forward process can be constructed via Doob\u2019s $h$ -transform [44, 36, 72]. Among them, DDBM [72] focuses on learning the reverse-time diffusion bridge conditioned on a particular terminal endpoint with denoising score matching, which has been shown to be equivalent to conducting a conditioned bridge matching that preserves the initial joint distribution [7]. Other works tackle solving the diffuion Schr\u00f6dinger Bridge problem, such as using iterative algorithms [8, 51, 43]. In this work, we use a unified view of design spaces on existing diffusion bridges, in particular, bridge matching methods, to decouple empirical choices from their different theoretical premises and properties and focus on developing the techniques of learning the consistency function of DDBM\u2019s PF-ODE with various established design choices for diffusion bridges. ", "page_idx": 14}, {"type": "text", "text": "Consistency Models Recent studies have continued to explore the effectiveness of consistency models [58]. For example, CTM [26] proposes to augment the prediction target from the starting point to the intermediate points along the PF-ODE trajectory from the input to this starting point. BCM [31] additionally expands the model to allow direct mapping at the PF-ODE trajectory points in both forward and reverse time. Beyond different formulations, several works aim to improve the performance of consistency training with theoretical and practical insights. iCT [57] systematically examines the design choices of consistency training and presents improved training schedule, loss weighting, distance metrics, etc. ECT [15] further leverages the insights to propose novel practical designs and show fine-tuning pre-trained diffusion models for learning consistency models yields decent performance with much lower computation compared to distillation. Unlike these works, we focus on constructing consistency models on top of the formulation of DDBMs with specialized design spaces and a sophisticated ODE solver for them. ", "page_idx": 14}, {"type": "text", "text": "B Additional Details for CDBM Formulation, CBD, and CBT ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Derivation of First-Order Bridge ODE Solver ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first review the first-order ODE solver in Section 3.1: ", "page_idx": 14}, {"type": "text", "text": "Proposition 3.1. Given an initial value $\\pmb{x}_{t}$ at time $t>0$ , the first-order solver of the bridge ODE in Eqn. (8) from $t$ to $r\\in[0,t]$ with the noise schedule defined in Eqn. $(I I)$ is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\bf{x}}_{r}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}{\\bf{x}}_{t}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left[\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right){\\bf{x}}_{\\theta}({\\bf{x}}_{t},t,{\\bf{y}})+\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)\\frac{{\\bf{y}}}{\\alpha_{T}}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall the PF-ODE of DDBM in Eqn. (8) with a linear drift ${\\bf\\nabla}f(t){\\bf x}_{t}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[f(t)x_{t}-g^{2}(t)\\left[\\frac{1}{2}\\nabla_{x_{t}}\\log q_{t|T}(x_{t}|x_{T}=y)-\\nabla_{x_{t}}\\log p_{T|t}(x_{T}=y|x_{t})\\right]\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Also recall the noise schedule in Eqn. (11) and the analytic form of $p_{t|0}$ and $p_{T|t}$ in diffusion models: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t|0}(\\pmb{x}_{t}|\\pmb{x}_{0})=\\mathcal{N}\\left(\\alpha_{t}\\pmb{x}_{0},\\alpha_{t}^{2}\\rho_{t}^{2}I\\right),\\quad p_{T|t}(\\pmb{x}_{T}|\\pmb{x}_{t})=\\mathcal{N}\\left(\\frac{\\alpha_{T}}{\\alpha_{t}}\\pmb{x}_{t},\\alpha_{T}^{2}(\\rho_{T}^{2}-\\rho_{t}^{2})I\\right),}\\\\ &{\\qquad\\qquad q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})=p_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0},\\pmb{x}_{T})=\\mathcal{N}\\left(a_{t}\\pmb{x}_{T}+b_{t}\\pmb{x}_{0},c_{t}^{2}I\\right),\\quad}\\\\ &{\\qquad\\qquad\\mathrm{where}\\quad a_{t}=\\frac{\\bar{\\alpha}_{t}\\rho_{t}^{2}}{\\rho_{T}^{2}},\\quad b_{t}=\\frac{\\alpha_{t}\\bar{\\rho}_{t}^{2}}{\\rho_{T}^{2}},\\quad c_{t}^{2}=\\frac{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}\\rho_{t}^{2}}{\\rho_{T}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We thus have the corresponding score functions and the score-data transformation for $\\scriptstyle{s_{\\theta}}$ that predicts \u2207xt log qt|0T : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{x}_{t}}\\log p_{T|t}(\\pmb{x}_{T}=\\pmb{y}|\\pmb{x}_{t})=-\\frac{\\pmb{x}_{t}-\\bar{\\alpha}_{t}\\pmb{y}}{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}_{t}}\\log q_{t|0T}(x_{t}|\\mathbf{x}_{0},\\mathbf{x}_{T}=y)=-\\frac{\\mathbf{x}_{t}-(\\alpha_{t}\\bar{\\rho}_{t}^{2}\\mathbf{x}_{0}+\\bar{\\alpha}_{t}\\rho_{t}^{2}\\mathbf{x}_{T})/\\rho_{T}^{2}}{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}\\rho_{t}^{2}/\\rho_{T}^{2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\ns_{\\theta}(x_{t},t,y)=-\\frac{x_{t}-(\\alpha_{t}\\bar{\\rho}_{t}^{2}x_{\\theta}(x_{t},t,y)+\\bar{\\alpha}_{t}\\rho_{t}^{2}x_{T})/\\rho_{T}^{2}}{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}\\rho_{t}^{2}/\\rho_{T}^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We use the data parameterization $\\pmb{x}_{\\theta}(\\pmb{x}_{t},t,\\pmb{y})$ in following discussions. For PF-ODE in Eqn. (21), substituting $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}=\\pmb{y})$ with Eqn. (25) and substituting $p_{T|t}(\\pmb{x}_{T}|\\pmb{x}_{t})$ in with Eqn. (23), we have the following after some simplification: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[f(t)x_{t}-\\frac{1}{2}g^{2}(t)\\frac{x_{t}-\\bar{\\alpha}_{t}y}{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}}+\\frac{1}{2}g^{2}(t)\\frac{x_{t}-\\alpha_{t}\\pmb{x}_{\\theta}(\\pmb{x}_{t},t,y)}{\\alpha_{t}^{2}\\rho_{t}^{2}}\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which shares the same form as the ODE in Bridge-TTS [6]. In the next discussions, we present an overview of deriving the first-order ODE solver and refer the reader to Appendix A.2 in [6] for details. ", "page_idx": 15}, {"type": "text", "text": "We begin by reviewing exponential integrators [4, 22], a key technique for developing advanced diffusion ODE solvers [16, 38, 39, 70]. Consider the following ODE: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pmb{x}_{t}=[a(t)\\pmb{x}_{t}+b(t)\\pmb{F}_{\\theta}(\\pmb{x}_{t},t)]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $F_{\\theta}$ is a $n$ -th differentiable parameterized function. By leveraging the \u201cvariation-of-constant\u201d formula, we could obtain a specific form of the solution of the ODE in Eqn. (27) (assume $r<t$ ): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{x}_{r}=e^{\\int_{t}^{r}a(\\tau)\\mathrm{d}\\tau}\\pmb{x}_{t}+\\int_{t}^{r}e^{\\int_{\\tau}^{r}a(s)\\mathrm{d}s}b(\\tau)\\pmb{F}_{\\theta}(\\pmb{x}_{\\tau},\\tau)\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The integral in Eqn. (28) only involves the function $F_{\\theta}$ , which helps reduce discretization errors. ", "page_idx": 15}, {"type": "text", "text": "With such a key methodology, we could derive the first-order solver for Eqn. (26). First, collecting the coefficients for $\\mathbf{\\Delta}\\mathbf{x}_{t},y,x_{\\theta}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left[\\left(f(t)-\\frac{g^{2}(t)}{2\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}}+\\frac{g^{2}(t)}{2\\alpha_{t}^{2}\\rho_{t}^{2}}\\right)x_{t}+\\frac{g^{2}(t)\\bar{\\alpha}_{t}}{2\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}}y-\\frac{g^{2}(t)}{2\\alpha_{t}\\rho_{t}^{2}}x_{\\theta}(x_{t},t,y)\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By setting: ", "page_idx": 15}, {"type": "equation", "text": "$$\na(t)=\\left(f(t)-\\frac{g^{2}(t)}{2\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}}+\\frac{g^{2}(t)}{2\\alpha_{t}^{2}\\rho_{t}^{2}}\\right),\\quad b_{1}(t)=\\frac{g^{2}(t)\\bar{\\alpha}_{t}}{2\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}},\\quad b_{2}(t)=\\frac{g^{2}(t)}{2\\alpha_{t}\\rho_{t}^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with correspondence to Eqn. (28), the exponential terms could be analytically given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne^{\\int_{t}^{r}a(\\tau)\\mathrm{d}\\tau}=\\frac{\\alpha_{r}\\sigma_{r}\\bar{\\sigma}_{r}}{\\alpha_{t}\\sigma_{t}\\bar{\\sigma}_{t}},\\quad e^{\\int_{\\tau}^{r}a(s)\\mathrm{d}s}=\\frac{\\alpha_{r}\\sigma_{r}\\bar{\\sigma}_{r}}{\\alpha_{\\tau}\\sigma_{\\tau}\\bar{\\sigma}_{\\tau}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The exact solution for Eqn. (29) is thus given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf x}_{r}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho\\bar{\\rho}_{t}}{\\bf x}_{t}+\\frac{\\bar{\\alpha}_{r}\\rho_{r}\\bar{\\rho}_{r}}{2}\\int_{t}^{r}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}\\rho_{\\tau}\\bar{\\rho}_{\\tau}^{3}}y\\mathrm{d}\\tau-\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{2}\\int_{t}^{r}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}\\rho_{\\tau}^{3}\\bar{\\rho}_{\\tau}}x_{\\theta}({\\bf x}_{\\tau},\\tau)\\mathrm{d}\\tau\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The integrals in Eqn. (31) (without considering $\\boldsymbol{x}_{\\theta}$ ) can be calculated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{t}^{r}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}\\rho_{\\tau}\\bar{\\rho}_{\\tau}^{3}}\\mathrm{d}\\tau=\\frac{2}{\\rho_{T}^{2}}\\left(\\frac{\\rho_{r}}{\\bar{\\rho}_{r}}-\\frac{\\rho_{t}}{\\bar{\\rho}_{t}}\\right),\\quad\\int_{t}^{r}\\frac{g^{2}(\\tau)}{\\alpha_{\\tau}^{2}\\sigma_{\\tau}^{3}\\bar{\\sigma}_{\\tau}}\\mathrm{d}\\tau=\\frac{2}{\\rho_{T}^{2}}\\left(\\frac{\\bar{\\rho}_{t}}{\\rho_{t}}-\\frac{\\bar{\\rho}_{r}}{\\rho_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, with the first order approximation $x_{\\theta}(x_{\\tau},\\tau)\\approx x_{\\theta}(x_{s},s)$ , we could obtain the first order solver in Eqn. (16). ", "page_idx": 15}, {"type": "text", "text": "B.2 An Illustration Example of the Validity of the Bridge ODE ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall the provided example in Section 3.1: ", "page_idx": 15}, {"type": "text", "text": "Example 3.1. Assume $T=1$ and consider a simple Brownian Bridge between two fixed points $(x_{0},x_{1})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\frac{x_{1}-x_{t}}{1-t}\\mathrm{d}t+\\mathrm{d}w_{t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with marginal distribution $q_{t|01}(x_{t}|x_{0},x_{1})=\\mathcal{N}((1-t)x_{0}+t x_{1},t(1-t))$ . The ground-truth reverse SDE and PF-ODE are given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathrm{d}x_{t}=\\frac{x_{t}-x_{0}}{t}\\mathrm{d}t+\\mathrm{d}\\bar{w}_{t},}\\\\ {\\displaystyle\\mathrm{d}x_{t}=\\left(\\frac{1-2t}{2t(1-t)}x_{t}+\\frac{1}{2(1-t)}x_{1}-\\frac{1}{2t}x_{0}\\right)\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then first simulating the reverse SDE in Eqn. (14) from $t=1$ to $t=1-\\gamma$ for some $\\gamma\\in(0,1)$ and then starting to simulate the $P F$ -ODE in Eqn. (15) will preserve the marginal distribution. ", "page_idx": 16}, {"type": "text", "text": "Proof. We first demonstrate the effect of the initial SDE step, according to Table 1 and the expression of the relevant score terms in Eqn. (23) and Eqn. (25), the ground-truth reverse SDE can be derived as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\frac{x_{t}-x_{0}}{t}\\mathrm{d}t+\\mathrm{d}\\bar{w}_{t}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the analytic solution of the reverse SDE in Eqn. (7) from time $t$ to time $s<t$ can be derived as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathrm{d}x_{t}-\\frac{1}{t}x_{t}\\mathrm{d}t=-\\frac{1}{t}x_{0}+\\mathrm{d}\\bar{w}_{t}}\\\\ {\\displaystyle\\Longleftrightarrow\\ \\mathrm{d}\\left(\\frac{1}{t}x_{t}\\right)=-\\frac{1}{t^{2}}x_{0}+\\frac{1}{t}\\mathrm{d}\\bar{w}_{t}}\\\\ {\\displaystyle\\Longleftrightarrow\\ \\frac{1}{s}x_{s}-\\frac{1}{t}x_{t}=\\left(\\frac{1}{s}-\\frac{1}{t}\\right)x_{0}+\\sqrt{\\frac{1}{s}-\\frac{1}{t}}\\epsilon,\\quad\\epsilon\\sim\\mathcal{N}(0,1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $t=1$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{s}=(1-s)x_{0}+s x_{1}+\\sqrt{s(1-s)}\\epsilon,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "i.e., $x_{s}$ has the same marginal as the forward process at time $s$ . Similarly, the ground-truth PF-ODE can be derived as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}x_{t}=\\left({\\frac{1-2t}{2t(1-t)}}x_{t}+{\\frac{1}{2(1-t)}}x_{1}-{\\frac{1}{2t}}x_{0}\\right)\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "whose analytic solution from time $t$ to time $s<t$ can be derived as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{d}x_{t}-\\frac{1-2t}{2t(1-t)}x_{t}\\mathrm{d}t=\\frac{1}{2(1-t)}x_{1}\\mathrm{d}t-\\frac{1}{2t}x_{0}\\mathrm{d}t}\\\\ &{\\implies\\mathrm{d}\\left(\\frac{1}{\\sqrt{t(1-t)}}x_{t}\\right)=\\frac{t}{2[t(1-t)]^{3/2}}x_{1}\\mathrm{d}t-\\frac{1-t}{2[t(1-t)]^{3/2}}x_{0}\\mathrm{d}t}\\\\ &{\\implies\\frac{1}{\\sqrt{s(1-s)}}x_{s}-\\frac{1}{\\sqrt{t(1-t)}}x_{t}=\\left(\\frac{s}{\\sqrt{s(1-s)}}-\\frac{t}{\\sqrt{t(1-t)}}\\right)x_{1}+\\left(\\frac{1-s}{\\sqrt{s(1-s)}}-\\frac{1-t}{\\sqrt{t(1-s)}}\\right)x_{2}\\mathrm{d}t}\\\\ &{\\implies x_{s}=\\frac{\\sqrt{s(1-s)}}{\\sqrt{t(1-t)}}x_{t}+\\left(s-\\frac{\\sqrt{s(1-s)}}{\\sqrt{t(1-t)}}t\\right)x_{1}+\\left(1-s-\\frac{\\sqrt{s(1-s)}}{\\sqrt{t(1-t)}}(1-t)\\right)x_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When $x_{t}\\sim\\mathcal{N}((1-t)x_{0}+t x_{1},t(1-t))$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{s}=\\displaystyle\\frac{\\sqrt{s(1-s)}}{\\sqrt{t(1-t)}}\\left((1-t)x_{0}+t x_{1}+\\sqrt{t(1-t)}\\epsilon\\right)+\\left(s-\\frac{\\sqrt{s(1-s)}}{\\sqrt{t(1-t)}}t\\right)x_{1}}}\\\\ {{\\mathrm{}\\,}}\\\\ {{\\,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, once the singularity is skipped by a stochastic step, following the PF-ODE reversely will preserve the marginals in this case. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.3 Derivation of the CBT Objective ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Given $(x,y)\\sim q_{\\mathrm{data}}(x,y),x_{t}\\sim q_{t|0T}(x_{t}|x_{0}=x,x_{T}=y)$ and an estimate of $\\hat{\\pmb{x}}_{r}=\\hat{\\pmb{x}}_{\\phi}(\\pmb{x}_{t},t,\\pmb{y})$ based on the pre-trained score predictor $s_{\\phi}$ with the first-order ODE solver in Eqn. (16), our goal is to derive the alternative estimation of $\\hat{\\pmb{x}}_{r}=\\hat{\\pmb{x}}(\\pmb{x}_{t},t,r,\\pmb{x},\\pmb{y})=a_{r}\\pmb{y}+b_{r}\\pmb{x}+c_{r}\\pmb{z}$ used in CBT, where $\\begin{array}{r}{z\\,=\\,\\frac{\\pmb{x}_{t}-a_{t}\\pmb{y}-b_{t}\\pmb{y}}{c_{t}}\\,\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})}\\end{array}$ and $a_{r},b_{r},c_{r}$ are defined in Eqn. (11). We begin with the estimator with pre-trained score model and first-order ODE solver: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf x}_{r}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}{\\bf x}_{t}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left[\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right){\\bf x}_{\\phi}({\\bf x}_{t},t,{\\bf y})+\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)\\frac{{\\bf y}}{\\alpha_{T}}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{\\nabla}\\boldsymbol{x}_{\\phi}$ is the equivalent data predictor of the score predictor $s_{\\phi}$ . By the transformation between data and score predictor xt\u2212atxbtT +ct s\u03d5 and substituting the score predictor s\u03d5 with the score estimator $\\nabla_{{\\pmb x}_{t}}q_{t\\vert0T}({\\pmb x}_{t}|{\\pmb x}_{0}={\\pmb x},{\\pmb x}_{T}={\\dot{\\pmb y}})$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf x}_{r}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}x_{t}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left[\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right)x+\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)\\frac{y}{\\alpha_{T}}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By expressing $\\pmb{x}_{t}=a_{t}\\pmb{y}+b_{t}\\pmb{x}+c_{t}\\pmb{z}$ , we could derive the corresponding coefficients for $\\mathbf{\\Delta}x,y,z$ on the right-hand side. ", "page_idx": 17}, {"type": "text", "text": "For $\\textit{\\textbf{y}}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}a_{t}+\\frac{\\alpha_{r}}{\\alpha_{T}\\rho_{T}^{2}}\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}\\frac{\\bar{\\alpha}_{t}\\rho_{t}^{2}}{\\rho_{T}^{2}}+\\frac{\\alpha_{r}}{\\alpha_{T}\\rho_{T}^{2}}\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)}\\\\ &{\\frac{(i)}{=}\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{T}\\bar{\\rho}_{t}}\\frac{\\rho_{t}}{\\rho_{T}^{2}}+\\frac{\\alpha_{r}}{\\alpha_{T}\\rho_{T}^{2}}\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)=\\frac{\\bar{\\alpha}_{r}\\rho_{r}^{2}}{\\rho_{T}^{2}}=a_{r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(i)$ is due to the fact $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\frac{\\alpha_{t}}{\\alpha_{T}}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "For $\\textbf{\\em x}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}b_{t}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right)=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}\\frac{\\alpha_{t}\\bar{\\rho}_{t}^{2}}{\\rho_{T}^{2}}+\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right)=\\frac{\\alpha_{r}\\bar{\\rho}_{r}^{2}}{\\rho_{T}^{2}}=b_{r}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $_{\\textit{z}}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}c_{t}=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}\\frac{\\alpha_{t}\\bar{\\rho}_{t}\\rho_{t}}{\\rho_{T}}=\\frac{\\alpha_{r}\\bar{\\rho}_{r}\\rho_{r}}{\\rho_{T}}=c_{r}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, we have the alternative model-free estimator $\\hat{\\pmb{x}}_{r}\\,=\\,\\hat{\\pmb{x}}(\\pmb{x}_{t},t,r,\\pmb{x},\\pmb{y})\\,=\\,a_{r}\\pmb{y}+b_{r}\\pmb{x}+c_{r}\\pmb{z}$ , where $z\\sim\\mathcal{N}(\\mathbf{0},I)$ is the same Gaussian noise used in sampling $\\pmb{x}_{t}=a_{t}\\pmb{y}+b_{t}\\pmb{x}+c_{t}\\pmb{z}$ . Substituting $\\hat{\\pmb{x}}_{\\phi}(\\pmb{x}_{t},t,r,\\pmb{y})$ in the CBD objective in Eqn. (17) with $\\hat{\\pmb{x}}({\\pmb x}_{t},t,r,\\pmb x,\\pmb y)$ gives the CBT objective in Eqn. (20). ", "page_idx": 17}, {"type": "text", "text": "B.4 Network Parameterization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, we show the detailed network parameterization for DDBM in Table. 1. Denote the neural network as $F_{\\theta}$ , the data predictor $\\pmb{x}_{\\pmb{\\theta}}(\\pmb{x},t,\\pmb{y})$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf x}_{\\theta}({\\bf x}_{t},t,y)=c_{\\mathrm{skip}}(t){\\bf x}_{t}+c_{\\mathrm{out}}(t){\\cal F}_{\\theta}(c_{\\mathrm{in}}(t){\\bf x}_{t},c_{\\mathrm{noise}}(t),y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\mathrm{in}}(t)=\\frac{1}{\\sqrt{a_{t}^{2}\\sigma_{T}^{2}+b_{t}^{2}\\sigma_{0}^{2}+2a_{t}b_{t}\\sigma_{0}\\tau+c_{t}}},\\quad c_{\\mathrm{out}}(t)=\\sqrt{a_{t}^{2}(\\sigma_{T}^{2}\\sigma_{0}^{2}-\\sigma_{0T}^{2})+\\sigma_{0}^{2}c_{t}c_{\\mathrm{in}}}(t),}\\\\ &{\\qquad\\qquad c_{\\mathrm{skip}}(t)=(b_{t}\\sigma_{0}^{2}+a_{t}\\sigma_{0T})c_{\\mathrm{in}}^{2}(t),\\quad c_{\\mathrm{noise}}(t)=\\frac{1}{4}\\log t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{t}=\\frac{\\bar{\\alpha}_{t}\\rho_{t}^{2}}{\\rho_{T}^{2}},\\quad b_{t}=\\frac{\\alpha_{t}\\bar{\\rho}_{t}^{2}}{\\rho_{T}^{2}},\\quad c_{t}=\\frac{\\alpha_{t}^{2}\\bar{\\rho}_{t}^{2}\\rho_{t}^{2}}{\\rho_{T}^{2}},\\quad\\sigma_{0}^{2}=\\mathrm{Var}[x_{0}],\\sigma_{T}^{2}=\\mathrm{Var}[x_{T}],\\sigma_{0T}=\\mathrm{Cov}[x_{0},x_{T}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It can be verified that, with the variable substitution $\\tilde{t}=t-\\epsilon$ , we have $a_{\\tilde{\\epsilon}}=0,b_{\\tilde{\\epsilon}}=1,c_{\\tilde{\\epsilon}}=0$ and thus have $c_{\\mathrm{skip}}(\\tilde{\\epsilon})=1$ and $c_{\\mathrm{out}}(\\tilde{\\epsilon})=0$ . ", "page_idx": 18}, {"type": "text", "text": "Meanwhile, we could generally parameterize the data predictor $\\pmb{x_{\\theta}}$ with the one-step first-order solver from $t$ to $\\epsilon$ , i.e.: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{\\theta}(x_{t},t,y)=\\frac{\\alpha_{\\epsilon}\\rho_{\\epsilon}\\bar{\\rho}_{\\epsilon}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}}x_{t}+\\frac{\\alpha_{\\epsilon}}{\\rho_{T}^{2}}\\left[\\left(\\bar{\\rho}_{\\epsilon}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{\\epsilon}\\bar{\\rho}_{\\epsilon}}{\\rho_{t}}\\right)x_{\\theta}(x_{t},t,y)+\\left(\\rho_{\\epsilon}^{2}-\\frac{\\rho_{t}\\rho_{\\epsilon}\\bar{\\rho}_{\\epsilon}}{\\bar{\\rho}_{t}}\\right)\\frac{y}{\\alpha_{T}}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which naturally satisfies $f(\\pmb{x}_{\\epsilon},\\epsilon,\\pmb{y})=\\pmb{x}_{\\epsilon}$ . ", "page_idx": 18}, {"type": "text", "text": "B.5 Asymptotic Analysis of CBD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 3.2. Given $\\Delta t_{\\operatorname*{max}}=\\operatorname*{max}_{t}\\{t-r(t)\\}$ and let $h_{\\phi}(\\cdot,\\cdot,\\cdot)$ be the consistency function of the empirical bridge ODE taking the form in Eqn. (8). Assume $h_{\\theta}$ is a Lipschitz function, i.e., there exists $L>0$ , such that for all $t\\in[\\epsilon,T-\\gamma],x_{1},x_{2},y,$ , we have $\\|h_{\\theta}(\\mathbf{x}_{1},t,\\pmb{y})-h_{\\theta}(\\mathbf{x}_{2},t,\\pmb{y})\\|_{2}\\leq$ $L\\|{\\pmb x}_{1}-{\\pmb x}_{2}\\|_{2}$ . Meanwhile, assume that for all $t$ $,r\\in[\\epsilon,T-\\gamma],\\pmb{y}\\sim q_{\\mathrm{data}}(\\pmb{y}):=\\mathbb{E}_{\\pmb{x}}[q_{\\mathrm{data}}(\\pmb{x},\\pmb{y})],$ , the ODE solver $\\hat{x}_{\\phi}(\\cdot,t,r,y)$ has local error uniformly bounded by $O((t-r)^{p+1})$ with $p\\geq1$ . Then, $i f\\,\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=0,$ , we have: $\\begin{array}{r}{\\operatorname*{sup}_{t,x,y}\\|h_{\\theta}(\\mathbf{\\boldsymbol{x}},t,y)-h_{\\phi}(\\mathbf{\\boldsymbol{x}},t,y)\\|_{2}=O((\\Delta t_{\\operatorname*{max}})^{p})}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Most of the proof directly follows the original consistency models analysis [58], with minor differences in the discrete timestep intervals (i.e., non-overlapped in [58] and overlapped in ours) and the form of marginal distribution between $p_{t}(\\pmb{x}_{t})$ for the diffusion ODE and $q_{t|T}(\\mathbf{\\bar{x}}_{t}|\\mathbf{x}_{T}=\\pmb{y})$ for the bridge ODE. ", "page_idx": 18}, {"type": "text", "text": "Proof. Given $\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=0$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{q_{\\mathrm{data}}(x,y)q_{t}|_{0T}(x_{t}|x_{0}=x,x_{T}=y)}\\mathbb{E}_{t,r}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y)-h_{\\theta^{-}}(\\hat{x}_{\\phi}(x_{t},t,r,y),r,y)\\right)\\right]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\lambda(t)~>~0$ , and for $t~\\in~[\\epsilon,T\\,-\\,\\gamma]$ , $q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0}\\ =\\ \\pmb{x},\\pmb{y}_{0}\\ =\\ \\pmb{y})$ takes the form of $\\mathcal{N}(a_{t}\\mathbf{\\Deltax}_{T}+b_{t}\\mathbf{\\Deltax}_{0},c_{t}\\pmb{I})$ with $c_{t}\\,>\\,0$ , which entails for any $\\pmb{x}_{t}$ , $t\\in[\\epsilon,T-\\gamma]$ , $q_{t|T}(\\mathbf{x}_{t}|\\mathbf{x}_{T}=\\pmb{y})=$ $\\mathbb{E}_{\\pmb{x}}[q_{t|0T}(\\pmb{x}_{t}|\\pmb{x}_{0}=\\pmb{x},\\pmb{x}_{T}=\\pmb{y})]>0.$ . Hence, Eqn. (41) implies that for all $t\\in[\\epsilon,T-\\gamma],({\\pmb x},{\\pmb y})\\sim$ $q_{\\mathrm{data}}(x,y),x_{t}\\sim q_{t|0T}(x_{t}|x_{0}=x,x_{T}=y)$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\left(h_{\\pmb{\\theta}}(\\pmb{x}_{t},t,y)-h_{\\pmb{\\theta}^{-}}(\\hat{x}_{\\phi}(\\pmb{x}_{t},t,r(t),\\pmb{y}),r(t),\\pmb{y})\\right)\\equiv0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the nature of the distance metric function $d$ and the stopgrad operator, we then have: ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{\\theta}(x_{t},t,y)\\equiv h_{\\theta^{-}}(\\hat{x}_{\\phi}(x_{t},t,r(t),y),r(t),y)\\equiv h_{\\theta}(\\hat{x}_{\\phi}(x_{t},t,r(t),y),r(t),y).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define the error term at timestep $t\\in[\\epsilon,T-\\gamma]$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{t}:=h_{\\theta}(x_{t},t,y)-h_{\\phi}(x_{t},t,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{t}=h_{\\theta}(x_{t},t,y)-h_{\\phi}(x_{t},t,y)}\\\\ &{\\quad=h_{\\theta}(\\hat{x}_{\\phi}(x_{t},t,r(t),y),r(t),y)-h_{\\phi}(x_{r(t)},r(t),y)}\\\\ &{\\quad=h_{\\theta}(\\hat{x}_{\\phi}(x_{t},t,r(t),y),r(t),y)-h_{\\theta}(x_{r(t)},r(t),y)}\\\\ &{\\quad\\quad+h_{\\theta}(x_{r(t)},r(t),y)-h_{\\phi}(x_{r(t)},r(t),y)}\\\\ &{\\quad\\quad=h_{\\theta}(\\hat{x}_{\\phi}(x_{t},t,r(t),y),r(t),y)-h_{\\theta}(x_{r(t)},r(t),y)+e_{r(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $h_{\\theta}$ is Lipschitz with constant $L$ and the ODE solver $\\hat{x}_{\\phi}(\\cdot,t,r,y)$ is bounded by $O((t-r)^{p+1})$ with $p\\geq1$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|e_{t}\\|_{2}\\leq\\|e_{r(t)}\\|_{2}+L\\|\\hat{\\mathbf{\\boldsymbol{x}}}_{\\phi}(\\mathbf{\\boldsymbol{x}}_{t},t,r(t),\\mathbf{\\boldsymbol{y}})-\\mathbf{\\boldsymbol{x}}_{r(t)}\\|_{2}}\\\\ &{\\quad\\quad=\\|e_{r(t)}\\|_{2}+L\\cdot O((t-r(t))^{p+1})}\\\\ &{\\quad\\quad=\\|e_{r(t)}\\|_{2}+O((t-r(t))^{p+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the boundary condition of the consistency function, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{\\epsilon}=h_{\\theta}(x_{\\epsilon},\\epsilon,y)-h_{\\phi}(x_{\\epsilon},\\epsilon,y)=x_{\\epsilon}-x_{\\epsilon}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Denote $r_{m}(t)$ as applying $r$ on $t$ for $m$ times, since $\\Delta t_{\\operatorname*{min}}=\\operatorname*{min}_{t}\\{t-r(t)\\}$ exists, there exists $N$ such that $r_{n}(t)=\\epsilon$ for $n\\geq N$ . We thus have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|e_{1}\\|_{2}\\leq\\big\\|e_{1}\\big\\|_{2}+\\displaystyle\\sum_{k=1}^{N}O\\big((r_{k-1}(t)-r_{k}(t))^{p+1}\\big)}}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}O\\big((r_{k-1}(t)-r_{k}(t))^{p+1}\\big)}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}\\big(r_{k-1}(t)-r_{k}(t)\\big)O\\big((r_{k-1}(t)-r_{k}(t))^{p}\\big)}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{N}\\big(r_{k-1}(t)-r_{k}(t)\\big)O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\big)}\\\\ &{=O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\displaystyle\\sum_{k=1}^{N}(r_{k-1}(t)-r_{k}(t))\\big)}\\\\ &{\\leq O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\big)(t-c)}\\\\ &{\\leq O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\big)(t-c)}\\\\ &{\\geq O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\big)(T-c)}\\\\ &{\\geq O\\big((\\Delta t_{\\operatorname*{max}})^{p}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.6 Connection between CBD & CBT ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 3.3. Given $\\Delta t_{\\operatorname*{max}}~=~\\operatorname*{max}_{t}\\{t\\,-\\,r(t)\\}$ and assume $d,h_{\\theta},f,g$ are twice continuously differentiable with bounded second derivatives, the weighting function $\\lambda(\\cdot)$ is bounded, and $\\mathbb{E}[\\|\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T})\\|_{2}^{2}]\\,<\\,\\infty$ . Meanwhile, assume that $\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}$ employs the one-step $O D E$ solver in Eqn. $(I\\!\\!\\!\\slash6)$ with ground truth pre-trained score model, i.e., $\\forall t\\in[\\epsilon,T-\\gamma],\\pmb{y}\\sim q_{\\mathrm{data}}(\\pmb{y}):$ $s_{\\phi}(\\mathbf{x}_{t},t,\\mathbf{y})\\equiv\\nabla_{\\mathbf{x}_{t}}\\log q_{t|T}(\\mathbf{x}_{t}|\\mathbf{x}_{T}=\\mathbf{y})$ . Then, we have: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=\\mathcal{L}_{\\mathrm{CBT}}^{\\Delta t_{\\mathrm{max}}}+o(\\dot{\\Delta t}_{\\mathrm{max}})}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "The core technique for building the connection between consistency distillation and consistency training with Taylor Expansion also directly follows [58]. The major difference lies in the form of the bridge ODE and the general noise schedule & the first-order ODE solver studied in our work. ", "page_idx": 19}, {"type": "text", "text": "Proof. First, for a twice continuously differentiable, multivariate, vector-valued function $\\pmb{h}(\\pmb{x},t,\\pmb{y})$ , denote $\\partial_{k}h({\\pmb x},t,{\\pmb y})$ as the Jacobian of $^h$ over the $k$ -th variable. Consider the CBD objective with first-order ODE solver in Eqn. (16) (ignore terms taking expectation for notation simplicity): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{CBD}}^{\\Delta t_{\\mathrm{max}}}=\\mathbb{E}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y),h_{\\theta^{-}}(k_{1}(t,r)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y,r,y)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{k_{1}(t,r)=\\frac{\\alpha_{r}\\rho_{r}\\bar{\\rho}_{r}}{\\alpha_{t}\\rho_{t}\\bar{\\rho}_{t}},k_{2}(t,r)=\\frac{\\alpha_{r}}{\\rho_{T}^{2}}\\left(\\bar{\\rho}_{r}^{2}-\\frac{\\bar{\\rho}_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\rho_{t}}\\right),k_{3}(t,r)=\\frac{\\alpha_{r}}{\\alpha_{T}\\rho_{T}^{2}}\\left(\\rho_{r}^{2}-\\frac{\\rho_{t}\\rho_{r}\\bar{\\rho}_{r}}{\\bar{\\rho}_{t}}\\right)}\\end{array}$ are coefficients of $\\mathbf{\\Delta}\\mathbf{x}_{t},\\mathbf{\\Delta}\\mathbf{x}_{\\phi},y$ in the first-order ODE solver in Eqn. (16), $\\scriptstyle x_{\\phi}$ is pre-trained data predictor. By applying first-order Taylor expansion on Eqn. (45), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}_{\\mathrm{CBD}}^{\\mathrm{At}_{\\mathrm{max}}}}\\\\ &{=\\mathbb{E}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y),h_{\\theta^{-}}(x_{t}+(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y,t+(r-t),y)\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\lambda(t)d\\left(h_{\\theta}(x_{t},t,y),h_{\\theta^{-}}(x_{t},t,y)+\\partial_{1}h_{\\theta^{-}}(x_{t},t,y)[(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y]\\right)\\right]}\\\\ &{\\quad+\\partial_{2}h_{\\theta^{-}}(x_{t},t,y)(r-t)+o(|t-r|))]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here the error term w.r.t. the first variable can be obtained by applying Taylor expansion on $k(t,r)=$ $k(t,t)+\\partial_{2}k(t,t)(r-t)+o(|t-r|)$ with $k_{1}(t,t)-1=0,\\bar{k}_{2}(t,\\bar{t})=\\bar{k}_{3}(t,\\bar{t})=0$ . By applying ", "page_idx": 19}, {"type": "text", "text": "Taylor expansion on $d$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{CBD}}^{\\mathrm{At}_{\\mathrm{max}}}}\\\\ &{=\\mathbb{E}\\{\\lambda(t)d(h_{\\theta}(x_{t},t,y),h_{\\theta-}(x_{t},t,y))+\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta-}(x_{t},t,y))[}\\\\ &{\\partial_{1}h_{\\theta}-(x_{t},t,y)[(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y]+\\partial_{2}h_{\\theta^{-}}(x_{t},t,y)(r-t)+o(|t-r|)]}\\\\ &{=\\mathbb{E}\\{\\lambda(t)d(h_{\\theta}(x_{t},t,y),h_{\\theta-}(x_{t},t,y))\\}}\\\\ &{\\quad+\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta-}(x_{t},t,y))[\\partial_{1}h_{\\theta-}(x_{t},t,y)[(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y])}\\\\ &{\\quad+\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta-}(x_{t},t,y))\\}\\partial_{2}h_{\\theta^{-}}(x_{t},t,y)(r-t)\\}+\\mathbb{E}\\{o(|t-r|)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we focus on the term related to the first-order ODE solver: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(k_{1}(t,r)-1){\\pmb x}_{t}+k_{2}(t,r){\\pmb x}_{\\phi}+k_{3}(t,r){\\pmb y}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the transformation between data and score predictor $\\begin{array}{r}{\\textbf x_{\\phi}~=~\\frac{\\textbf x_{t}-a_{t}\\textbf x_{T}+c_{t}^{2}\\textbf s_{\\phi}}{b_{t}}}\\end{array}$ , and substitute $\\pmb{s}_{\\phi}(\\pmb{x}_{t},t,\\pmb{y})$ with $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}=\\pmb{y})$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)\\frac{x_{t}-a_{t}x_{T}+c_{t}^{2}\\nabla_{x_{t}}\\log q_{t|T}(x_{t}|x_{T}=y)}{b_{t}}+k_{3}(t,r)y.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, substituting the score $\\nabla_{\\pmb{x}_{t}}\\log q_{t|T}(\\pmb{x}_{t}|\\pmb{x}_{T}=\\pmb{y})$ with the unbiased estimator: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\nabla_{x_{t}}\\log{q_{t}}_{\\rvert0T}(x_{t}\\vert x_{0},x_{T})\\vert x_{t},x_{T}=y]=\\mathbb{E}\\left[-\\frac{x_{t}-\\left(a_{t}x_{T}+b_{t}x_{0}\\right)}{c_{t}^{2}}\\vert x_{t},x_{T}=y\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta}-(x_{t},t,y))[\\partial_{1}h_{\\theta}-(x_{t},t,y)][(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)x_{\\phi}+k_{3}(t,r)y}\\\\ &{=\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta}-(x_{t},t,y))[\\partial_{1}h_{\\theta}-(x_{t},t,y)[}\\\\ &{\\quad(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)\\frac{x_{t}-a_{t}x_{T}+c_{t}^{2}\\mathbb{E}\\left[-\\frac{x_{t}-(a_{t}x_{T}+b_{t}x_{0})}{c_{t}^{2}}|x_{t},x_{T}=y\\right]}{b_{t}}+k_{3}(t,r)y}\\\\ &{\\quad\\frac{i}{2}\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta}-(x_{t},t,y))[\\partial_{1}h_{\\theta}-(x_{t},t,y)[}\\\\ &{\\quad(k_{1}(t,r)-1)x_{t}+k_{2}(t,r)\\frac{x_{t}-a_{t}x_{T}-c_{t}^{2}\\frac{x_{t}-(a_{t}x_{T}+b_{t}x_{0})}{c_{t}^{2}}+k_{3}(t,r)y]}\\\\ &{=\\mathbb{E}\\{\\lambda(t)\\partial_{2}d(h_{\\theta}(x_{t},t,y),h_{\\theta}-(x_{t},t,y))[\\partial_{1}h_{\\theta}-(x_{t},t,y)][h_{1}(t,r)x_{t}+k_{2}(t,r)x+k_{3}(t,r)y-x_{t}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ comes from the law of total expectation. Then we apply Taylor expansion in the reverse direction: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{GPB}}^{\\mathrm{CA}_{\\mathrm{mins}}}}\\\\ &{=\\mathbb{E}\\biggl\\{\\lambda(t)\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)\\}}\\\\ &{\\quad+\\mathbb{E}\\bigl\\{\\lambda(t)\\partial_{2}\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)[\\partial_{1}h_{\\theta}-(x_{t},t,y)[k_{1}(t,r)x_{t}+k_{2}(t,r)x+k_{3}(t,r)y-s}\\\\ &{\\quad+\\mathbb{E}\\bigl\\{\\lambda(t)\\partial_{2}d\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)\\}\\partial_{2}h_{\\theta}-(x_{t},t,y)(r-t)\\bigr)+\\mathbb{E}\\bigl\\{\\partial((t-r)[)\\bigr\\}\\cdot}\\\\ &{=\\mathbb{E}\\bigl\\{\\lambda(t)\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)\\}}\\\\ &{\\quad+\\partial_{2}d\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)[\\partial_{1}h_{\\theta}-(x_{t},t,y)[k_{1}(t,r)x_{t}+k_{2}(t,r)x+k_{3}(t,r)y-x_{t}]\\bigr\\}}\\\\ &{\\quad+\\partial_{2}d\\bigl(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)\\bigr)\\partial_{2}h_{\\theta}-(x_{t},t,y)(r-t)\\bigr\\}+\\mathbb{E}\\bigl\\{\\partial((t-r)[)\\bigr\\}}\\\\ &{=\\mathbb{E}\\bigl\\{\\lambda(t)\\bigl[\\partial(h_{0}(x_{t},t,y),h_{0}-(x_{t},t,y)+\\partial_{1}h_{0}-(x_{t},t,y)\\bigr)[k_{1}(t,r)x_{t}+k_{2}(t,r)x+k_{3}(t,r)y-x_{t}]}\\\\ &{\\quad+\\partial_{2}h_{\\theta}-(x_{t},t,y)(r-t)\\bigr)\\bigr\\}+\\mathbb{E}\\bigl\\{\\partial((t-r)[)\\bigr)}}\\\\ &{=\\mathbb{E}\\bigl\\{\\lambda(t)\\bigl|\\bigl(h_{0}(\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i i)$ follows the derivation in Eqn. (33) \u2013 Eqn. (36), and $z\\sim\\mathcal{N}(\\mathbf{0},I)$ . ", "page_idx": 20}, {"type": "text", "text": "C Additional Experimental Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Details of Training and Sampling Configurations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We train CDBMs based on a series of pre-trained DDBMs. For two image-to-image translation tasks, we directly use the pre-trained checkpoints provided by DDBM\u2019s [72] official repository.2 For image inpainting, we re-train a model with the same $\\mathrm{I}^{2}\\mathrm{SB}$ style noise schedule, network parameterization, and timestep scheme in Table. 1, as well as the overall network architecture. Unlike the training setup in $\\mathrm{I}^{2}\\mathrm{SB}$ , our network is conditioned on $\\scriptstyle x_{T}\\;=\\;y$ following DDBM and takes the class information of ImageNet as input, which we refer to as the base DDBM model for image inpainting on ImageNet. The model is initialized with the class-conditional version on ImageNet $256\\!\\times\\!256$ of guided diffusion [10]. We used a global batch size of 256 and a constant learning rate of 1e-5 with mixed precision (fp16) to train the model for 200k steps. We train the model with 8 NVIDIA A800 80G GPUs for 9.5 days, achieving the FID reported in Table. 3 with the first-order ODE solver in Eqn. (16). ", "page_idx": 21}, {"type": "text", "text": "For training CDBMs, we use a global batch size of 128 and a learning rate of 1e-5 with mixed precision (fp16) for all datasets using 8 NVIDIA A800 80G GPUs. For the constant training schedule $r(t)=t-\\Delta t$ , we train the model for $50\\mathrm{k}$ steps, while for the sigmoid-style training schedule, we train the model for $6s$ steps, e.g., 30k or 60k steps, due to numerical instability when $t-r(t)$ is small. For CBD, training a model for $50\\mathrm{k}$ steps on a dataset with $256\\times256$ resolution takes ${\\sim}2.5$ days, while CBT takes ${\\sim}1.5$ days. In this work, we normalize all images within $[-1,1]$ and adopt the RAdam [27, 34] optimizer. ", "page_idx": 21}, {"type": "text", "text": "For sampling, we use a uniform timestep for all baselines with the ODE solver and CDBM on two image-to-image translation tasks with $\\epsilon=0.0001,T=1.0$ . For CDBM on image inpainting on ImageNet, we manually assign the second timestep to $T-0.1$ and make other timesteps uniformly distributed between $[\\epsilon,T-0.1)$ , which we find yields better empirical performance on this task. ", "page_idx": 21}, {"type": "text", "text": "C.2 Details of Training Schedule for CDBM ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We illustrate the effect of the hyperparamter $b$ in the sigmoid-like training schedule $r(t)=t(1-$ $\\begin{array}{r}{\\frac{1}{q^{\\lfloor\\mathrm{iters}/s\\rfloor}})(1+\\frac{k}{1+e^{b t}})}\\end{array}$ . Note that we further manually enforce ${\\boldsymbol{r}}(t)$ to satisfy $\\Delta t_{\\mathrm{max}}$ and $\\Delta t_{\\mathrm{min}}$ . ", "page_idx": 21}, {"type": "image", "img_path": "FFJFGx78OK/tmp/df5aae506fe3cc6f7b3e6ed389219ea3d387ae52691e6a17b5121b953ce994c0.jpg", "img_caption": ["Figure 6: Illustration of the effect of the parameter $b$ on the sigmoid-style training schedule. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 License ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We list the used datasets, codes, and their licenses in Table 4. ", "page_idx": 21}, {"type": "table", "img_path": "FFJFGx78OK/tmp/a4bd1aad10dfcea3ce08fcb2257cb3f25f24983c037db35bdd2a0da989d1b1ed.jpg", "table_caption": ["Table 4: The used datasets, codes and their licenses. "], "table_footnote": ["2https://github.com/alexzhou907/DDBM "], "page_idx": 21}, {"type": "image", "img_path": "FFJFGx78OK/tmp/5428d50d4d5ec7bffe9e2ad26c69434902f039f0aab8b43d281032a33bfdf0e8.jpg", "img_caption": ["Figure 7: Additional Samples for Edges $\\rightarrow$ Handbags. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FFJFGx78OK/tmp/139fa87ccd06a567b0a793590ebf53d7bd58d459d063bba38b6d5ded9167c584.jpg", "img_caption": ["Figure 8: Additional Samples for DIODE-Outdoor. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "FFJFGx78OK/tmp/a6e50ad6161de66984ec603fd20099c3e2278d13bea14e3cd16e9e34ec01ab81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Condition ", "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/fe4a6c2e0b435824e39d40ac51e0d006027d1db8f7b961b63556f92b3949eae0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Ground Truth ", "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/149e867b5f483e0106e0fcf8f8b9c25d6a3510697e776eaf5de638c1e5cff52c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "DDBM, NFE=2 ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/61f4a27a9cca95f71a96c7d8d6bbc191c0ad56c1c6a66afcca2bd3f82eb43953.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "DDBM, NFE=8 ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/39141254f13edb0823cd863b2ea2cde643703044243b6fc723158d2a6ce51ee1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "CDBM, NFE=2 ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/82063787d6c9d5eac6c0e7005df369af537a100af6f8adbdba92b15ae7a05554.jpg", "img_caption": ["DDBM, NFE=10 "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/ab2b7fd074c722928e1b1ca053c496cc3c1fb9d2b5620c072d4e09cbe8cb7884.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "CDBM, NFE=10 ", "page_idx": 24}, {"type": "text", "text": "Figure 9: Additional Samples for ImageNet $256\\times256$ . ", "page_idx": 24}, {"type": "image", "img_path": "FFJFGx78OK/tmp/1b9090953553d442eae6d8f08df6ed737b132682ba7f99f1fb8cf533fb7e145c.jpg", "img_caption": ["Figure 10: Demonstration of sample diversity of the deterministic ODE sampler. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "FFJFGx78OK/tmp/7d117f60a5cf3e32beb25e9ec9a8a313dfabc46fa506b0f2f8b9ecac399a67c6.jpg", "img_caption": ["Figure 11: Qualitative comparison between CDBM and $\\mathrm{I}^{2}\\mathrm{SB}$ baseline on ImageNet $256\\times256$ . Note that here the base model of CDBM is different from the officially released checkpoint of $\\mathrm{I}^{2}\\mathrm{SB}$ we used for evaluation. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The discussion is located in the \u201cLimitations and Broad Impact\u201d section after the main paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Assumptions are provided with the propositions and the detailed derivation and proof is in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experimental configurations are included in Section 4 and Appendix C.   \nThe information we provided is sufficient to reproduce the results that support our claims. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The release of the code needs an official procedure related to the authors\u2019 affiliation, which is not approved yet. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The experiment configuration and details are included in Section 4 and Appendix C, which is sufficient to understand the results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The metrics for evaluating generative models are typically stable and do not require error bars. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided the information in Appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The discussion is located in the \u201cLimitations and Broad Impact\u201d section after the main paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work is conducted with common academic image datasets with model capability restricted with specific tasks. There is little chance posing risks for misuse. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Licenses for existing assets are listed in Appendix C.3. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]