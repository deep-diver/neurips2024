[{"type": "text", "text": "Intervention and Conditioning in Causal Bayesian Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sainyam Galhotra Computer Science Dept. Cornell University sg@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Joseph Y. Halpern Computer Science Dept. Cornell University halpern@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal models are crucial for understanding complex systems and identifying causal relationships among variables. Even though causal models are extremely popular, conditional probability calculation of formulas involving interventions pose significant challenges. In case of Causal Bayesian Networks (CBNs), Pearl assumes autonomy of mechanisms that determine interventions to calculate a range of probabilities. We show that by making simple yet often realistic independence assumptions, it is possible to uniquely estimate the probability of an interventional formula (including the well-studied notions of probability of sufficiency and necessity). We discuss when these assumptions are appropriate. Importantly, in many cases of interest, when the assumptions are appropriate, these probability estimates can be evaluated using observational data, which carries immense significance in scenarios where conducting experiments is impractical or unfeasible. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal models play a pivotal role in elucidating the causal relationships among variables. These models facilitate a principled approach to understanding how various factors interact and influence each other in complex systems. For instance, in epidemiology, causal models often help us understand the relationship between lifestyle choices and health outcomes (Greenland, Pearl, and Robins 1999); and in economics, they help to analyze the impact of policy changes on market dynamics (Hicks 1979). These examples underscore the versatility and utility of causal models for providing a formal representation of system variables. ", "page_idx": 0}, {"type": "text", "text": "Interventions and conditioning are the most fundamental procedures in the application of causal models, useful to examine and analyze causal mechanisms. For example, interventions help explain the outcome of complex ML systems (Galhotra, Pradhan, and Salimi 2021); and in AI-driven healthcare diagnostics, it is crucial to discern the effect of a particular intervention (like a change in treatment protocol) on patient outcomes (Greenland 1999). ", "page_idx": 0}, {"type": "text", "text": "Despite their utility, calculating the probabilities related to interventions and conditioning in tandem presents significant challenges. Indeed, it is not even clear what the semantics of queries involving counterfactuals is. Work in the AI literature has focused on two types of models: functional causal models1 and causal Bayesian networks (Pearl 2000). Both are typically described using directed acyclic graphs, where each node is associated with a variable. In a causal model, with each variable $Y$ associated with a non-root node, there is a deterministic (structural) equation, that gives the value of $Y$ as a function of the values of its parents; there is also a probability on the values of root nodes. In a CBN, like in a Bayesian network, each variable $Y$ is associated with a conditional probability table (cpt), that for each setting of the parents of $Y$ , gives the probability of $Y$ conditional on that setting. In a functional causal model, it is actually straightforward to determine the conditional probability of formulas involving interventions. In a CBN, this is far from true. Indeed, recent work of Beckers (2023) has shown that an approach given by Pearl (2000) to calculate these probabilities in a CBN is incorrect. 2 Pearl also calculates probabilities in a CBN by implicitly reducing the CBN to a family of functional causal models (see, e.g., (Pearl 2000, Theorem 9.2.10)), but he does not give an explicit reduction, nor does he give a formal definition of the probability of a formula in a CBN. Here, we do both. Using this approach leads to formulas having a range of probabilities in a CBN, whereas in a functional causal model, their probability is unique. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "But we take an additional significant step. Pearl assumes that mechanisms that determine how interventions work (which are given by the cpts in the case of CBNs and the structural equations in the case of causal models) are autonomous: as Pearl puts it, \u201cexternal changes affecting one equation do not imply changes to the others\u201d (Pearl 2000, p. 28). We model this autonomy formally by taking the equations to be independent of each other, in an appropriate space. As shown recently by Richardson, Peters, and Halpern (2024), taking the equations that characterize different variables to be independent is a necessary and sufficient condition for reproducing all the (conditional) independencies in the underlying Bayesian network, as determined by $d$ -separation (Pearl 1988). Thus, this independence seems like a natural and critical assumption to get CBNs and causal models to work as we would expect. ", "page_idx": 1}, {"type": "text", "text": "Here we assume that, not only are the equations that define different variables independent, but also the equations that give the values of a variable for different settings of its parents. We never need to consider the values of a variable for different settings of its parents in a standard Bayesian network, but this is necessary to determine the probability of a formula involving interventions, such as $X=0\\land Y=0\\land[X\\leftarrow1](Y=1)$ ( $X$ and $Y$ have value 0, but if $X$ is set to 1, $Y$ gets value 1). Taking these latter equations to be independent is not always appropriate;3 For example, there may be a latent exogenous variable that affects the value of $Y$ for different settings of $Y$ \u2019s parents. But if the parents of $Y$ (including exogenous variables) are all observable, and screen $Y$ off from the effects of all other variables, then the independence assumption seems appropriate. ", "page_idx": 1}, {"type": "text", "text": "Making these independence assumptions has significant benefits. For one thing, it allows us to uniquely identify the probability of queries in a CBN; rather than getting a range of values, we get a unique value. Moreover, for many formulas of interest (including the probability of necessity and probability of sufficiency (Pearl 2000), we can compute the probability by considering only conditional probabilities involving only a subset of endogenous and exogenous variables, which do not involve interventions. This means that these probabilities can be estimated from observational data, without requiring involving controlled experiments. This can have huge implications in settings where such experimental data is not available but the exogenous variables can be observed. ", "page_idx": 1}, {"type": "text", "text": "The rest of this paper is organized as follows. Section 2 reviews the formalism of causal models. Section 3 gives semantics to formulas in Causal Bayesian Networks (CBNs) and Section 4 shows that any CBN can be converted to a compatible causal model that satisfies the independence assumptions that we are interested in. We show how counterfactual probabilities of necessity and sufficiency can be simplified and calculated in the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Causal Models and CBNs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In a (functional) causal model (also called a structural equations model), the world is assumed to be described in terms of variables and their values. Some variables may have a causal influence on others. This influence is modeled by a set of structural equations. It is conceptually useful to split the variables into two sets: the exogenous variables, whose values are determined by factors outside the model, and the endogenous variables, whose values are ultimately determined by the exogenous variables. In some settings, exogenous variables can be observed; but they can never be intervened upon, as (by assumption) their values are determined by factors outside the model. Note that exogenous variables may involve latent factors that are not observable, and may even be unknown. For example, in an agricultural setting, we could have endogenous variables that describe crop produce, amount of fertilizers used, water consumption, and so on, and exogenous variables that describe weather conditions (which cannot be modified, but can be observed) and some latent factors, like the activity level of pollinators (which cannot be observed or measured). The structural equations describe how the values of endogenous variables are determined (e.g., how the water consumption depends on the weather conditions and the amount of fertilizer used). ", "page_idx": 2}, {"type": "text", "text": "Formally, a causal model $M$ is a pair $({\\mathcal{S}},{\\mathcal{F}})$ , where $\\boldsymbol{S}$ is a signature, which explicitly lists the endogenous and exogenous variables and characterizes their possible values, and $\\mathcal{F}$ defines a set of modifiable structural equations, relating the values of the variables. A signature $\\boldsymbol{S}$ is a tuple $(\\mathcal{U},\\mathcal{V},\\mathcal{R})$ , where $\\boldsymbol{\\mathcal{U}}$ is a set of exogenous variables, $\\nu$ is a set of endogenous variables, and $\\mathcal{R}$ associates with every variable $Y\\in\\mathcal{U}\\cup\\mathcal{V}$ a nonempty set ${\\mathcal{R}}(Y)$ of possible values for $Y$ (that is, the set of values over which $Y$ ranges). For simplicity, we assume that $\\nu$ is finite, as is ${\\mathcal{R}}(Y)$ for every endogenous variable $Y\\in\\mathcal{V}$ . $\\mathcal{F}$ associates with each endogenous variable $X\\in\\mathcal{V}$ a function denoted $F_{X}$ such that $F_{X}:(\\times_{U\\in\\mathcal{U}}\\mathcal{R}(U))\\times(\\times_{Y\\in\\mathcal{V}-\\{X\\}}\\mathcal{R}(Y))\\stackrel{*}{\\rightarrow}\\mathcal{R}(X)$ . This mathematical notation just makes precise the fact that $F_{X}$ determines the value of $X$ , given the values of all the other variables in $\\mathcal{U}\\cup\\mathcal{V}$ . ", "page_idx": 2}, {"type": "text", "text": "The structural equations define what happens in the presence of external interventions. Setting the value of some variable $X$ to $x$ in a causal model $M=(S,{\\mathcal{F}})$ results in a new causal model, denoted $M_{X\\leftarrow x}$ , which is identical to $M$ , except that the equation for $X$ in $\\mathcal{F}$ is replaced by $X=x$ . ", "page_idx": 2}, {"type": "text", "text": "Following most of the literature, we restrict attention here to what are called recursive (or acyclic) models. In such models, there is a total ordering $\\prec$ of the endogenous variables such that if $X\\prec Y$ , then $X$ is not causally influenced by $Y$ , that is, $F_{X}(\\ldots,y,\\ldots)=F_{X}(\\ldots,y^{\\prime},\\ldots)$ for all $y,y^{\\prime}\\in$ ${\\mathcal{R}}(Y)$ . If $X\\prec Y$ , then the value of $X$ may affect the value of $Y$ , but the value of $Y$ cannot affect the value of $X$ . It should be clear that if $M$ is an acyclic causal model, then given a context, that is, a setting $\\vec{u}$ for the exogenous variables in $\\boldsymbol{\\mathcal{U}}$ , there is a unique solution for all the equations. We simply solve for the variables in the order given by $\\prec$ . ", "page_idx": 2}, {"type": "text", "text": "A recursive causal model can be described by a dag (directed acyclic graph) whose nodes are labeled by variables, and there is an edge from $X$ to $Y$ if $X\\prec Y$ . We can assume without loss of generality that the equation for $Y$ involves only the parents of $Y$ in the dag. The roots of the dag are labeled by exogenous variables or endogenous variables with no parents; all the remaining nodes are labeled by endogenous variables.4 ", "page_idx": 2}, {"type": "text", "text": "A probabilistic (functional) causal model is a pair $(M,\\operatorname*{Pr})$ consisting of a causal model $M$ and a probability $\\mathrm{Pr}$ on the contexts of $M$ . In the rest of this paper, when we refer to a \u201ccausal model\u201d, we mean a probabilistic functional causal model, unless we explicitly say otherwise. ", "page_idx": 2}, {"type": "text", "text": "A causal Bayesian network (CBN) is a tuple $M\\,=\\,(S,\\mathcal{P})$ described by a signature $\\boldsymbol{S}$ , just like a causal model, and a collection $\\mathcal{P}$ of conditional probability tables (cpts), one for each (endogenous and exogenous) variable.5 For this paper, we focus on recursive CBNs that can be characterized by a dag, where there is a bijection between the nodes and the (exogenous and endogenous) variables. The cpt for a variable $X$ quantifies the effects of the parents of $X$ on $X$ . For example, if the parents of $X$ are $Y$ and $Z$ and all variables are binary, then the cpt for $X$ would have entries for all $j,k\\in$ $\\{0,1\\}^{2}$ , where the entry for $(j,k)$ describes $\\{\\Bar{P}r(X=0\\mid\\Bar{Y}=j,Z=k)$ . (There is no need to have an explicit entry for $P(X=1\\mid Y=j\\cap Z=k)$ , since this is just $1-P(X=0\\mid Y=j\\cap Z=k)$ .) The cpt for a root of the dag is just an unconditional probability, since a root has no parents. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Just as for causal models, we can also perform interventions in a CBN: intervening to set the value of some variable $X$ to $x$ in a CBN $M$ results in a new CBN, denoted $M_{X\\leftarrow x}$ , which is identical to $M$ , except that now $X$ has no parents; the cpt for $X$ just gives $X$ value $x$ with probability 1. ", "page_idx": 3}, {"type": "text", "text": "Note that we typically use the letter $M$ to refer to both non-probabilistic causal models and CBNs, while we use $\\mathrm{Pr}$ to refer to the probability on contexts in a probabilistic causal model. We use $P$ to refer to the probability in a cpt. It is also worth noting that a causal model can be viewed as a CBN; the equation $Y=F({\\vec{x}})$ can be identified with the entry $P(Y=F({\\vec{x}}))\\mid{\\vec{X}}={\\vec{x}})=1$ in a cpt. ", "page_idx": 3}, {"type": "text", "text": "3 Giving semantics to formulas in CBNs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 The problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the following (standard) language for reasoning about causality: Given a signature ${\\boldsymbol{S}}=$ $(\\mathcal{U},\\mathcal{V},\\mathcal{R})$ , a primitive event is a formula of the form $X\\ =\\ x$ , for $X\\ \\in\\ \\nu$ and $x\\,\\in\\,{\\mathcal{R}}(X)$ . A causal formula (over $\\boldsymbol{S}$ ) is one of the form $[Y_{1}\\;\\leftarrow\\;y_{1},\\..\\.\\;,Y_{k}\\;\\leftarrow\\;y_{k}]\\varphi$ , where $\\varphi$ is a Boolean combination of primitive events, $Y_{1},\\ldots,Y_{k}$ are distinct variables in $\\mathcal{V}$ , and $y_{i}\\,\\in\\,\\mathcal{R}(Y_{i})$ . Such a formula is abbreviated as $[\\vec{Y}\\leftarrow\\vec{y}]\\varphi$ . The special case where $k=0$ is abbreviated as $\\varphi$ . Intuitively, $[Y_{1}\\leftarrow y_{1},\\..\\.\\,,Y_{k}\\leftarrow y_{k}]\\dot{\\varphi}$ says that $\\varphi$ would hold if $Y_{i}$ were set to $y_{i}$ , for $i=1,\\ldots,k.\\;\\mathcal{L}(S)$ is the language consisting of Boolean combinations of causal formulas. We typically take the signature $\\boldsymbol{S}$ to be fixed, and just write $\\mathcal{L}$ . It will be convenient to consider a slightly richer language, that we denote ${\\mathcal{L}}^{+}(S)$ . It extends $\\mathcal{L}(S)$ by allowing primitive events $U\\,=\\,u$ , where $U\\in\\mathcal{U}$ , and also allowing interventions on exogenous variables.6 ", "page_idx": 3}, {"type": "text", "text": "A pair $(M,\\vec{u})$ consisting of a (non-probabilistic) causal model $M$ and a context $\\vec{u}$ is called a (causal) setting. A formula $\\varphi\\in\\bar{\\mathcal L}^{+}$ is either true or false in a setting. We write $(M,\\vec{u})\\vDash\\varphi$ if the causal formula $\\varphi$ is true in the setting $(M,\\vec{u})$ . The $\\vDash$ relation is defined inductively. $(M,\\vec{u})\\vDash X=x$ if the variable $X$ has value $x$ in the unique (since we are dealing with acyclic models) solution to the equations in $M$ in context $\\vec{u}$ (that is, the unique vector of values for the endogenous variables that simultaneously satisfies all equations in $M$ with the variables in $\\boldsymbol{\\mathcal{U}}$ set to $\\vec{u}_{\\ast}$ ). The truth of conjunctions and negations is defined in the standard way. Finally, $(M,\\vec{u})\\vDash[\\vec{Y}\\leftarrow\\vec{y}]\\varphi$ if $(M_{\\vec{Y}\\leftarrow\\vec{y}},\\vec{u}_{\\vec{Y}\\leftarrow\\vec{y}})\\,\\vDash\\varphi;$ , where $\\langle M_{\\vec{Y}\\leftarrow\\vec{y}}$ is identical to $M$ except that the equation for each endogenous variable $Y\\in{\\vec{Y}}$ is replaced by $Y=y^{*}$ , where $\\boldsymbol{y}^{*}\\in\\mathcal{R}(\\boldsymbol{Y})$ is the value in $\\vec{y}$ corresponding to $Y$ , and $\\vec{u}_{\\vec{Y}\\leftarrow\\vec{y}}$ is identical to $\\vec{u}$ , except that for each exogenous variable $U\\,\\in\\,{\\vec{Y}}$ , the component of $\\vec{u}$ corresponding to $U$ is replaced by $\\boldsymbol{u}^{*}$ , where $u^{*}\\in\\mathcal{R}(U)$ is the value in $\\vec{y}$ corresponding to $U$ . (We remark that in a CBN, intervening to set an exogenous variable $U$ to $u^{*}$ is just like any other intervention; we change the cpt for $U$ so that $u^{*}$ gets probability 1.) ", "page_idx": 3}, {"type": "text", "text": "In a probabilistic causal model $(M,\\operatorname*{Pr})$ , we can assign a probability to formulas in $\\mathcal{L}$ by taking the probability of a formula $\\varphi$ in $M$ , denoted $\\operatorname*{Pr}(\\varphi)$ , to be $\\operatorname*{Pr}(\\{\\vec{u}:(M,\\dot{\\vec{u}})\\in\\varphi\\})$ . Thus, the probability of $\\varphi$ in $M$ is simply the probability of the set of contexts in which $\\varphi$ is true; we can view each formula as corresponding to an event. ", "page_idx": 3}, {"type": "text", "text": "When we move to CBNs, things are not so straightforward. First, while we still have a probability on contexts, each context determines a probability on states, assignments of values to variables. A state clearly determines a truth value for formulas that do not involve interventions; call such formulas simple formulas. Thus, we can compute the truth of a simple formula $\\varphi$ in a context, and then using the probability of contexts, determine the probability of $\\varphi$ in a CBN $M$ . But what about a causal formula such as \u03c8 = [Y\u20d7 \u2190\u20d7y]\u03d5? Given a context \u20d7u, we can determine the model M \u2032 = MY\u20d7 \u2190y\u20d7. In $(M^{\\prime},\\vec{u})$ , $\\varphi$ is an event whose probability we can compute, as discussed above. We can (and will) take this probability to be the probability of the formula $\\psi$ in $(M,\\vec{u})$ . But note that $\\psi$ does not correspond to an event in $M$ , although we assign it a probability. ", "page_idx": 3}, {"type": "text", "text": "It gets harder to evaluate probability if we add another conjunct $\\psi^{\\prime}$ and consider the formula $\\psi\\wedge\\psi^{\\prime}$ . While we can use the procedure above to compute the probability of $\\psi$ and $\\psi^{\\prime}$ individually in $(M,\\vec{u})$ , what is the probability of the conjunction? Because such formulas do not correspond to events in $M$ , this is not obvious. We give one approach for defining the probability of a formula in a CBN by making one key assumption, which can be viewed as a generalization of Pearl\u2019s assumption. Pearl assumes that mechanisms that determine how interventions work (which are the cpts in the case of CBNs and the structural equations in the case of causal models) are autonomous; he takes that to mean \u201cit is conceivable to change one such relationship without changing the others\u201d (Pearl 2000, p. 22). We go further and assume, roughly speaking, that they are (probabilistically) independent. In a causal model, the mechanism for a given variable (specifically, the outcome after the intervention) is an event, so we can talk about mechanisms being independent. While it is not an event in a CBN, we nevertheless use the assumption that mechanisms are independent to guide how we determine the probability of formulas in $\\mathcal{L}$ in a CBN. ", "page_idx": 4}, {"type": "text", "text": "3.2 Independence of cpts and complete combinations of conditional events ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To describe our approach, we must first make clear what we mean by mechanisms (cpts) being independent. This has two components: the outcomes of cpts for different variables are independent, and for the cpt for a single variable Y, the outcomes for different settings of the parents of $Y$ are independent. Indeed, all these outcomes are mutually independent. We believe that these independence assumptions are quite reasonable and, capture the spirit of Bayesian networks. In fact, Richardson, Peters, and Halpern (2024) show that the assumption that cpts involving different variables are independent is equivalent to the (conditional) independence assumptions made in Bayesian networks (see Section 3.4 for further discussion). ", "page_idx": 4}, {"type": "text", "text": "In more detail, suppose that we have a variable $Y_{1}$ in a CBN $M$ with parents $X_{1}\\ldots,X_{m}$ . We want to consider events of the form $Y_{1}=y_{1}$ | $(X_{1}=x_{1},\\ldots,X_{m}=x_{m})$ , which we read ${}^{\\bullet\\bullet}Y_{1}=y_{1}$ given that $X_{1}=x_{1}$ , ..., and $X_{m}={x_{m}}^{*}$ \u201d. Such events have a probability, given by the cpts for $Y_{1}$ . We call such an event a conditional event for CBN $M$ . (Explicitly mentioning the CBN $M$ is necessary, since on the right-hand side of the conditional with left-hand side $Y$ , we have all the parents of $Y$ ; what the parents are depends on $M$ .) Roughly speaking, we identify such a conditional event with the formula $[X_{1}\\,\\leftarrow\\,j_{1},.\\,.\\,.\\,,X_{m}\\,\\leftarrow\\,j_{m}](Y_{1}\\,=\\,1)$ . This identification already hints at why we we care about conditional events (and their independence). Suppose for simplicity that $m\\,=\\,1$ . To determine the probability of a formula such as $X_{1}=0\\wedge\\bar{Y_{1}}^{\\circ}=0\\wedge[X_{1}\\stackrel{\\cdot}{\\leftarrow}1](Y_{1}=1)$ we need to apply both the entry in the cpt for $Y_{1}=0\\mid X_{1}=0$ and the entry for $Y_{1}=1\\mid X=1.$ They each give a probability; the probability of the formula $X_{1}=0\\land Y_{1}=0\\land[X_{1}\\leftarrow1](Y_{1}=1)$ is the probability that the conditional events $Y_{1}=0\\mid X_{1}=0$ and $Y_{1}=1\\mid X=1$ hold simultaneously. Our independence assumption implies that this probability is the product of the probability that each of them holds individually (which is given by the cpt for $Y_{1}$ ). ", "page_idx": 4}, {"type": "text", "text": "This is an instance of independence within a cpt; we want the conditional events in a cpt for a variable $Y$ for different settings of the parents of $Y$ to be independent. (Of course, conditional events for the same setting of the parents, such as $Y_{1}=0\\mid X_{1}={\\mathrm{\\bar{1}}}$ and $Y_{1}=1\\mid X_{1}=1$ , are not independent.) Independence for cpts of different variables is most easily explained by example: Suppose that $Y_{2}$ has parents $X_{1}$ and $X_{3}$ . Then we want the events $Y_{1}=0\\mid X_{1}=0$ and $Y_{2}=1\\mid$ $\\left'X_{1}=0,X_{3}=1\\right)$ ) to be independent. This independence assumption will be needed to compute the probability of formulas such as $[X_{1}\\leftarrow0](Y_{1}^{-}=0)\\land[X_{1}=\\bar{0},X_{3}=1](Y_{2}=_{-}1)$ . As we said, we in fact want to view all the relevant conditional events as mutually independent.7 ", "page_idx": 4}, {"type": "text", "text": "Although we use the term \u201cconditional event\u201d, these are not events in a CBN. On the other hand, in a causal model, there are corresponding notions that really do correspond to events. For example, the conditional event $Y_{1}\\;=\\;0\\;\\;|\\;\\;X_{1}\\;=\\;1$ corresponds to the set of contexts where the formula $[X_{1}\\leftarrow1](Y_{1}=1)$ is true. Starting with a CBN $M$ , we will be interested in causal models for which the probability $P(Y_{1}=0\\mid X_{1}=^{\\prime}1)$ , as given by the cpt for $Y_{1}$ in $M$ , is equal to the probability of the corresponding event in the causal model. ", "page_idx": 4}, {"type": "text", "text": "Going back to CBNs, define a complete combination of conditional events (ccce) for $M$ to be a conjunction consisting of the choice of one conditional event for $M$ for each endogenous variable ", "page_idx": 4}, {"type": "text", "text": "$X$ and each setting of the parents of $X$ . A fixed-context ccce (fccce) involves fewer conjuncts; we have only conditional events where for all the exogenous parents $U$ of a variable $X$ , the value of $U$ is the same as its value in the conjunct determining the value of $U$ (the examples should make clear what this means). ", "page_idx": 5}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Example 3.1. Consider the CBN M \u2217with the following dag: U where all variables are binary, and the cpts give the following probabilities: $P(U\\;=\\;0)\\;=\\;a,$ , $P(X=0\\mid U=0)=b,$ , $P(\\dot{X}=0\\mid U=1)=c_{*}$ , $P(Y=0\\mid X=0)=d,$ , and $P(Y=0\\mid X=$ $1)=e$ . Then a ccce consists of 5 conjuncts: ", "page_idx": 5}, {"type": "text", "text": "\u2022 one of $U=0$ and $U=1$ ;   \n\u2022 one of $X=0\\mid U=0$ and $X=1\\mid U=0,$ ;   \n\u2022 one of $X=0\\mid U=1$ and $X=1\\mid U=1,$ ;   \n\u2022 one of $Y=0\\mid X=0$ and $Y=1\\mid X=0$ ; and \u2022 one of $Y=0\\mid X=1$ and $Y=1\\mid X=1$ . ", "page_idx": 5}, {"type": "text", "text": "An fccce consist of only $^{4}$ conjuncts; it has only one of the second and third conjuncts of a ccce. In particular, if $U\\,=\\,0$ is a conjunct of the fccce, then we have neither $X\\;=\\;0\\;\\;|\\;\\;U\\;=\\;1$ nor $\\overset{\\vartriangle}{\\boldsymbol{X}}=1\\mid\\boldsymbol{U}=\\dot{\\boldsymbol{1}}$ as a conjunct; similarly, $i f U=1$ is a conjunct, then we have neither $\\dot{X}=0\\mid U=0$ nor $X=1\\mid U=0$ as a conjunct. (This is what we meant above by saying that each exogenous parent $U$ of $X$ must have the same value as in conjunct that determines $U$ \u2019s value.) ", "page_idx": 5}, {"type": "text", "text": "It is not hard to show that, in this case, there are 32 ccces and 16 fccces. Moreover, (in this example and in general) each fccce is equivalent to a disjunction of ccces. The number of ccces and fccces can be as high as doubly exponential (in the number of variables), each one involving exponentially many choices. For example, if a variable $Y$ has $n$ parents, each of them binary, there are $2^{n}$ possible settings of the parents of $Y$ , and we must choose one value of $Y$ for each of these $2^{n}$ settings, already giving us $2^{2^{n}}$ choices. It is easy to see that there is also a double-exponential upper bound. ", "page_idx": 5}, {"type": "text", "text": "If we think of a conditional event of the form $Z=1\\mid X=0,Y=0$ as saying \u201cif $X$ were (set to) 0 and $Y$ were (set to) 0, then $Z$ would be $1^{\\circ}$ , then given a ccce and a formula $\\varphi\\in{\\mathcal{L}}$ and context $\\vec{u}$ , we can determine if $\\varphi$ is true or false. We formalize this shortly. We can then take the probability of $\\varphi$ to be the sum of the probabilities of the ccces that make $\\varphi$ true. The probability of a ccce is determined by the corresponding entry of the cpt. Thus, if we further assume independence, we can determine the probability of each ccce, and hence the probability of any formula $\\varphi$ . We now give some informal examples of how this works, and then formalize the procedure in Section 3.3. ", "page_idx": 5}, {"type": "text", "text": "Example 3.2. In the CBN $M^{*}$ described in Example 3.1, there are two fccces where $\\varphi\\,=\\,X\\,=$ $0\\land Y=0\\land[X\\leftarrow1](Y=1)$ is t $\\begin{array}{r}{\\begin{array}{c c c}{{\\boldsymbol{u}}e\\colon({\\boldsymbol{a}})\\,{\\boldsymbol{U}}=0\\wedge\\big({\\boldsymbol{X}}=0\\mid{\\boldsymbol{U}}=0\\big)\\wedge\\big({\\boldsymbol{Y}}=0\\mid{\\boldsymbol{X}}=0\\big)\\wedge\\big({\\boldsymbol{Y}}=0\\big)}\\end{array}}\\end{array}$ $\\mid\\mid X=1\\rangle$ ); and (b) $\\dot{U}=1\\stackrel{.}{\\wedge}(X=0\\mid U=1)\\wedge(Y=0\\mid X=0)\\wedge\\dot{(}Y=1\\mid X=1).$ Each of these two fccces is the disjunction of two ccces, which extend the fccce by adding a fifth conjunct. For example, for the first fccce, we can add either the conjunct $X\\,=\\,0\\ \\vert\\ U\\,=\\,1$ or the conjunct $X=1\\mid U=1.$ . The total probability of these two fccces is $a b d(1-e)+(1-a)c d(1-e)$ ; this is the probability of $\\varphi$ in $M^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "We give one more example of this calculation. ", "page_idx": 5}, {"type": "text", "text": "Example 3.3. Consider the model CBN $M^{\\dagger}$ , which differs from $M^{*}$ in that now $U$ is also a parent of $Y$ ; the dag is shown below. $M^{*}$ and $M^{\\dagger}$ have the same cpts for $U$ and $X$ ; the cpt of $Y$ in $M^{\\dagger}$ is ${\\dot{P}}(Y=0\\mid{\\bar{U}}=0,X=0)=f_{1},$ , $P(Y=0\\mid U=0,X=\\mathrm{\\bar{1}})=f_{2}$ , $P(Y=0\\mid U=1,X=0)=$ $f_{3}$ $\\mathfrak{c}_{3},\\,P(Y=0\\mid U=1,X=1)=f_{4}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\textcircled{U}\\xrightarrow[\\textcircled{Y}]{}\\textcircled{X}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now there are 128 ccces, but only 16 fccces; the formula $\\varphi=X=0\\land Y=0\\land[X\\leftarrow1](Y=1)$ is true in only two of these fccces: (a) $)\\,{\\dot{U}}=0\\wedge(X=0\\mid U=0)\\wedge(Y=0\\mid(U=\\dot{0},X=\\dot{0}))\\wedge(\\dot{Y}=$ 1 | $\\left(U=0,X=1\\right)$ ); and (b) U = 1\u2227)X $=\\mathrm{~0~}\\,|\\,\\,U=1)\\wedge(Y=0\\ |\\ (U=1,X=0))\\wedge(\\mathrm{Y}$ $\\wedge\\,(Y=$ 1 | $\\left[U=1,X=1\\right)$ ). It is easy to check that ${\\operatorname*{Pr}}_{M^{\\dagger}}(\\varphi)=a b f_{1}(1-f_{2})+(1-a)c f_{3}(1-f_{4})$ . The calculation of the probability of $\\varphi$ is essentially the same in $M^{*}$ and $M^{\\dagger}$ . ", "page_idx": 5}, {"type": "text", "text": "We denote by $\\operatorname{Pr}_{M}(\\varphi)$ the probability of a formula $\\varphi$ in a CBN or causal model $M$ . (We provide a formal definition of $\\operatorname{Pr}_{M}(\\varphi)$ for a CBN $M$ at the end of Section 3.) ", "page_idx": 6}, {"type": "text", "text": "3.3 Giving semantics to formulas in CBNs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We already hinted in Examples 3.2 and 3.3 how we give semantics to formulas in CBNs. We now formalize this. ", "page_idx": 6}, {"type": "text", "text": "The first step is to show that a ccce (resp., fccce) determines the truth of a formula in ${\\mathcal{L}}^{+}(S)$ (resp., $\\mathcal{L}(S))$ in a causal model. To make this precise, we need a few definitions. We take the type of a CBN $M\\,=\\,(S,\\mathcal{P})$ , where $\\ensuremath{\\boldsymbol{S}}\\,=\\,(\\ensuremath{\\boldsymbol{\\mathcal{U}}},\\ensuremath{\\boldsymbol{\\mathcal{V}}},\\bar{\\mathcal{R}})$ to consist of its signature $\\boldsymbol{S}$ and, for each endogenous variable, a list of its parents (which is essentially given by the dag associated with $M$ , without the cpts). A causal model $M^{\\prime}=(S^{\\prime},{\\mathcal{F}}^{\\prime})$ has the same type as $M$ if $\\dot{S}^{\\prime}=(\\mathcal{U}\\cup\\mathcal{U}^{\\prime},\\mathcal{V},\\mathcal{R}^{\\prime})$ , where $\\mathcal{U}^{\\prime}$ is arbitrary, $\\mathcal{R}^{\\prime}\\vert_{\\mathcal{U}\\cup\\mathcal{V}}=\\mathcal{R}$ , and ${\\mathcal{F}}^{\\prime}$ is such that each endogenous variable $X$ depends on the same variables in $\\mathcal{U}\\cup\\mathcal{V}$ according to ${\\mathcal{F}}^{\\prime}$ as it does according to the type of $M$ (but may also depend on any subset of $\\mathcal{U}^{\\prime}$ ). ", "page_idx": 6}, {"type": "text", "text": "Definition 3.4. For the conditional event $Y=y\\mid$ ${\\mid(X_{1}=x_{1},\\ldots,X_{m}=x_{m})}$ , let the corresponding formula be $[X_{1}\\,\\leftarrow\\,x_{1},\\ldots,X_{m}\\,\\leftarrow\\,x_{m}](Y=y)$ . (Note that the corresponding formula may be in $\\mathcal{L}^{+}-\\mathcal{L}$ , since some of the $X_{i}s$ may be exogneous.) Let $\\varphi_{\\alpha}\\in{\\mathcal{L}}^{+}(S)$ , the formula corresponding to the ccce $\\alpha$ , be the conjunction of the formulas corresponding to the conditional events in $\\alpha$ . We can similarly define the formula corresponding to an fccce. ", "page_idx": 6}, {"type": "text", "text": "Example 3.5. In the model $M^{\\dagger}$ of Example 3.3, if $\\alpha$ is the fccce $U=0\\wedge(X=0\\mid U=0)\\wedge(Y=$ $0\\mid(\\bar{U^{\\big}}=0,X=0))\\land$ $\\lfloor Y=1\\mid$ $(U=0,X=1)$ ), then $\\varphi_{\\alpha}$ is $U=0\\wedge[U\\gets0]X=0\\wedge[U\\gets$ $0,X\\gets0](Y=0)\\wedge[U\\gets0,X\\gets1](Y=1)$ . ", "page_idx": 6}, {"type": "text", "text": "Say that a formula $\\psi$ is valid with respect to a $C B N M$ if $(M^{\\prime},\\vec{u})\\vDash\\psi$ for all causal settings $(M^{\\prime},\\vec{u})$ , where $M^{\\prime}$ is a causal model with the same type as $M$ . The following theorem makes precise the sense in which a ccce determines whether or not an arbitrary formula is true. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6. Given a CBN $M=(S,\\mathcal{P})$ and a ccce (resp., fccce) $\\alpha$ , then for all formulas $\\psi\\in$ ${\\mathcal{L}}^{+}(S)$ (resp., $\\psi\\,\\in\\,{\\mathcal{L}}(S))$ either $\\varphi_{\\alpha}\\,\\Rightarrow\\,\\psi$ is valid with respect to $M$ or $\\varphi_{\\alpha}\\,\\Rightarrow\\,\\neg\\psi$ is valid with respect to $M$ . ", "page_idx": 6}, {"type": "text", "text": "Proof: We show that if two causal models $M_{1}$ and $M_{2}$ have the same type as $M$ and $\\vec{u}_{1}$ and $\\vec{u}_{2}$ are contexts such that $(M_{1},\\vec{u}_{1})\\vDash\\varphi_{\\alpha}$ and $(M_{2},\\vec{u}_{2})\\,\\vDash\\,\\varphi_{\\alpha}$ , then for all formulas $\\psi\\,\\in\\,{\\mathcal{L}}^{+}(S)$ (resp., $\\psi\\in{\\mathcal{L}}(S))$ , we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left(M_{1},\\vec{u}_{1}\\right)\\vDash\\psi\\,\\o\\mathrm{iff}\\left(M_{2},\\vec{u}_{2}\\right)\\vDash\\psi.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The claimed result follows immediately. The details of the proof can be found in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Based on this result, we can take the probability of a formula $\\varphi\\;\\in\\;{\\mathcal{L}}^{+}(S)$ in a CBN $M$ to be the probability of the ccces that imply it. To make this precise, given a CBN $M$ , say that a probabilistic causal model $(M^{\\prime},\\operatorname*{Pr})$ is compatible with $M$ if $M^{\\prime}$ has the same type as $M$ , and the probability $\\mathrm{Pr}$ is such that all the cpts in $M$ get the right probability in ${\\dot{M^{\\prime}}}$ . More precisely, for each endogenous variable $Y$ in $M$ , if $X_{1},\\ldots,X_{k}$ are the parents of $Y$ in $M$ , then for each entry $P(Y\\;=\\;y\\;\\;|\\;\\;X_{1}\\;=\\;x_{1},\\ldots,X_{k}\\;=\\;x_{k})\\;=\\;a$ in the cpt for $Y$ , Pr is such that the corresponding formula $[X_{1}\\ \\leftarrow\\ x_{1},\\ldots,X_{k}\\ \\leftarrow\\ x_{k}](Y\\ =\\ y)$ gets probability $a$ . $(M^{\\prime},\\operatorname*{Pr})$ is $i$ -compatible with $M$ (the $i$ stands for independence) if it is compatible with $M$ and, in addition, $\\mathrm{Pr}$ is such that the events described by the formulas corresponding to entries for cpts for different variable (i.e. the set of contexts in $M$ that make these formulas true) are independent, as are the events described by the formulas corresponding to different entries for the cpt for a given variable. Thus, for example, if $(x_{1}^{\\prime},\\bar{\\dots},x_{k}^{\\prime})\\ \\bar{\\neq}\\ (x_{1}\\bar{,\\dots},x_{k})$ , then we want the events described by $[X_{1}\\ \\leftarrow\\ x_{1},\\ldots,X_{k}\\ \\leftarrow\\ x_{k}]({\\bar{Y}}\\ =\\ y)$ and $[X_{1}\\ \\leftarrow\\ x_{1}^{j},...\\,,X_{k}\\ \\leftarrow\\ x_{k}^{\\prime}](Y\\ =\\ y)$ to be independent (these are different entries of the cpt for $Y)$ ; and if $Y^{\\prime}\\;\\neq\\;Y$ and has parents $X_{1}^{\\prime},\\cdot\\cdot\\cdot,X_{m}^{\\prime}$ in $M$ , then we want the events described by $[X_{1}\\ \\leftarrow\\ x_{1},\\ldots,X_{k}\\ \\leftarrow\\ x_{k}](Y\\;=\\;y)$ and $[X_{1}^{\\prime}\\leftarrow\\stackrel{\\cdot\\cdot}{x_{1}^{\\prime}},...\\,,X_{m}\\leftarrow x_{m}^{\\prime}](Y^{\\prime}=y^{\\prime})$ to be independent (these are entries of cpts for different variables). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.7. Given a CBN $M$ and a formula $\\varphi\\in{\\mathcal{L}}^{+}(S)$ , the probability of $\\varphi$ is the same in all causal models $M^{\\prime}$ i-compatible with $M$ . ", "page_idx": 6}, {"type": "text", "text": "Proof: It follows from Theorem 3.6 that the probability of $\\varphi$ is the sum of the probabilities of the formulas $\\varphi_{\\alpha}$ for the ccces $\\alpha$ such that $\\varphi_{\\alpha}\\Rightarrow\\varphi$ is valid. It is immediate that these formulas have the same probability in all causal models i-compatible with $M$ . ", "page_idx": 7}, {"type": "text", "text": "Formally, we take $\\operatorname{Pr}_{M}(\\varphi)$ , the probability of $\\varphi$ in the CBN $M$ , to be $\\operatorname*{Pr}_{M^{\\prime}}(\\varphi)$ for a causal model $M^{\\prime}$ i-compatible with $M$ . By Theorem 3.7, it does not matter which causal model $M^{\\prime}$ i-compatible with $M$ we consider. Note for future reference that if we consider only causal models compatible with $M$ , dropping the independence assumption, we would get a range of probabilities. ", "page_idx": 7}, {"type": "text", "text": "3.4 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Four points are worth making: First, note that this way of assigning probabilities in a CBN $M$ always results in the probability of a formula $\\varphi\\in{\\mathcal{L}}^{+}$ being a sum of products of entries in the cpt. Thus, we can in principle compute the probabilities of (conditional) events involving interventions from observations of statistical frequencies (at least, as long as all settings of the parents of a variable in the relevant entries of the cpt have positive probability). ", "page_idx": 7}, {"type": "text", "text": "Second, the number of ccces may make the computation of the probability of a formula in a CBN seem unacceptably high. As the examples above shows, in practice, it is not so bad. For example, we typically do not actually have to deal with ccces. For one thing, it follows from Theorem 3.6 that to compute the probability of $\\varphi\\in{\\mathcal{L}}$ , it suffices to consider fccces. Moreover, when computing $\\operatorname{Pr}_{M}(\\varphi)$ where $\\varphi$ involves an intervention of the form $X\\leftarrow x$ , we can ignore the entries in the cpts involving $X$ , and for variables for which $X$ is a parent, we consider only entries in the cpts where $X\\,=\\,x$ . We can also take advantage of the structure of the formula whose probability we are interested in computing to further simplify the computation, although the details are beyond the scope of this paper. ", "page_idx": 7}, {"type": "text", "text": "Third, as mentioned above, a formula involving interventions does not correspond in an obvious way to an event in a CBN, but it does correspond to an event in a (functional) causal model. The key point is that in a causal model, a context not only determines a state; it determines a state for every intervention. We can view a formula involving interventions as an event in a space whose elements are functions from interventions to worlds. Since a context can be viewed this way, we can view a formula involving interventions as an event in such a space. This makes conditioning on arbitrary formulas in $\\mathcal{L}^{+}$ (with positive probability) in causal models well defined. By way of contrast, in a CBN, we can view a context as a function from interventions to distributions over worlds. Finally, it is worth asking how reasonable is the assumption that cpts are independent, that is, considering i-compatible causal models rather than just compatible causal models, which is what seems to have been done elsewhere in the literature (see, e.g., (Balke and Pearl 1994; Tian and Pearl 2000)). ", "page_idx": 7}, {"type": "text", "text": "As we said, Richardson, Peters, and Halpern (2024) show that the assumption that cpts involving different variables are independent is equivalent to the (conditional) independence assumptions made in Bayesian networks. More precisely, given a CBN $M$ , let $M^{\\prime}$ be the non-probabilistic causal model constructed above. They show that if the probability $\\mathrm{Pr^{\\prime}}$ makes interventions on different variables independent (i.e., if $\\operatorname*{Pr}^{\\prime}(\\vec{U},f_{1},\\dotsc,f_{m})\\,=\\,\\operatorname*{Pr}(\\vec{u})\\,\\times\\,\\operatorname*{Pr}_{Y_{1}}(f_{1})\\,\\times\\,\\dots\\,\\times\\,\\operatorname*{Pr}_{Y_{m}}(f_{m}),$ as in our construction), then all the conditional independencies implied by d-separation hold in $(M,\\ensuremath{\\mathrm{Pr}^{\\prime}})$ (see (Pearl 1988) for the formal definition of ${\\mathrm{d}}\\cdot$ -separation and further discussion). Conversely, if all the dependencies implied by d-separation hold in $(M,\\ensuremath{\\mathrm{Pr}}^{\\prime})$ , then $\\mathrm{Pr^{\\prime}}$ must make interventions on different variables independent. ", "page_idx": 7}, {"type": "text", "text": "This result says nothing about making interventions for different settings of the parents of a single variable independent. This is relevant only if we are interested in computing the probability of formulas such as $X=0{\\land}Y=0{\\land}[X\\leftarrow1](Y=1)$ , for which we need to consider (simultaneously) the cpt for $Y$ when $X=0$ and when $X=1$ . As discussed earlier, independence is reasonable in this case if we can observe all the parents of a variable $Y$ , and thus screen off $Y$ from the effects of all other variables (and other settings of the parents). We cannot always assume this, but in many realistic circumstances, we can. We give two general classes of examples where we can: ", "page_idx": 7}, {"type": "text", "text": "1. When debugging systems (including ml pipelines, database engines, or any general software) and network failures, users have access to all parameters related to the code and the execution environment (Fariha, Nath, and Meliou 2020; Kobayashi, Otomo, and Fukuda 2019; Galhotra, Fariha, Louren\u00b8co, Freire, Meliou, and Srivastava 2022). With this information, a causal graph over different blocks of code, its parameters, and other environment variables like information about background processes can be constructed. This means we can address queries like \u201cGiven that component A is faulty, with what probability would repairing component B solve the problem\u201d using our techniques. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "2. Manufacturing pipelines across various industries, such as semiconductor fabrication, pharmaceuticals, automobile assembly, and battery production, typically consist of a series of interconnected stages. Each of these stages is equipped with several sensors designed to monitor and measure critical environmental conditions that directly impact the production process. These sensors collect data about variables such as temperature, humidity, pressure, and other factors that can influence the quality, efficiency, and consistency of the final product. For instance, in semiconductor fabrication, precise control of environmental conditions like temperature and humidity is crucial to ensuring the integrity of the microchips produced $(\\mathrm{Wu}\\,2008)$ . Similarly, in pharmaceutical manufacturing, sensors monitor parameters like $\\mathrm{pH}$ levels and chemical concentrations to maintain the efficacy of the drugs being produced. Thus, we can answer queries like \u201cwhat is the probability that a temperature increase of 3 degrees Celsius would result in a poor quality product, given that the humidity is high?\u201d. ", "page_idx": 8}, {"type": "text", "text": "Other potential applications of our framework include (a) modeling player performance in sports by considering factors like injury, skill, and sports facilities, (b) urban planning scenarios to analyze the impact of zoning laws, interest rates, and other factors on house prices, and (c) modeling agriculture yield by considering variables like soil quality and weather conditions. ", "page_idx": 8}, {"type": "text", "text": "4 Converting a CBN to a (Probabilistic) Causal Model ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our semantics for formulas in CBNs reduced to considering their semantics in i-compatible causal models. It would be useful to show explicitly that such i-compatible causal models exist and how to construct them. That is the goal of this section. Balke and Pearl (1994) sketched how this could be done. We largely follow and formalize their construction. ", "page_idx": 8}, {"type": "text", "text": "Starting with a CBN $M$ , we want to construct an i-compatible probabilistic causal model $(M^{\\prime},\\ensuremath{\\mathrm{Pr}}^{\\prime})$ , where $M^{\\prime}$ has the same type as $M$ . To do this, for each endogenous variable $Y$ in $M$ with parents $X_{1},\\ldots,X_{n}$ , we add a new exogenous variable $U_{Y}$ ; $\\mathcal{R}(\\mathcal{U}_{Y})$ consists of all functions from $\\mathcal{R}\\bar{(}X_{1})\\times$ $\\cdot\\cdot\\cdot\\times\\mathcal{R}(X_{n})$ to $\\mathcal{R}(Y)$ . Balke and Pearl (1994) call such an exogenous variable a response function. (Response functions, in turn, are closely related to the potential response variables introduced by Rubin (1974).) We take $U_{Y}$ to be a parent of $Y$ (in addition to $X_{1},\\ldots,X_{n})$ . We replace the cpt for $Y$ be the following equation for $Y$ ; $F_{Y}(x_{1},\\ldots,x_{n},f)\\,=\\,f(x_{1},\\ldots,x_{n})$ , where $f$ is the value of $U_{Y}$ . Since $f$ is a function from $\\mathcal{R}(X_{1})\\times\\cdot\\cdot\\cdot\\times\\mathcal{R}(X_{n})$ to ${\\mathcal{R}}(Y)$ , this indeed gives a value of $Y$ , as desired. Let $Y_{1},\\ldots,Y_{m}$ be the endogenous variables in $M$ . We define the probability $\\mathrm{Pr^{\\prime}}$ on $\\mathcal{R}(\\mathcal{U})\\times$ $\\mathcal{R}(U_{Y_{1}})\\times\\cdot\\cdot\\cdot\\times\\mathcal{R}(U_{Y_{m}})$ by taking $\\mathrm{Pr}^{\\prime}(\\vec{u},f_{1},\\dotsc,f_{m})=\\mathrm{Pr}(\\vec{u})\\times\\Pi_{i=1,\\dotsc,m}\\operatorname*{Pr}_{Y_{i}}(f_{i})$ , where $\\mathrm{Pr}_{Y_{i}}$ reproduces the probability of the cpt for $Y_{i}$ . Specifically, for an endogenous variable $Y$ with parents $X_{1},\\ldots,X_{n}$ , $\\bar{\\operatorname*{m}}_{Y}(f)=\\bar{\\Pi}_{\\vec{x}\\in\\mathcal{R}(X_{1})\\times\\dots\\times\\mathcal{R}(X_{n})}\\bar{\\operatorname*{Pr}}(Y=f(x_{1},\\dots,x_{n})\\mid X_{1}=x_{1},\\dots,X_{n}=x_{n})$ ). This makes interventions for different settings of $X_{1},\\ldots,X_{n}$ independent, which is essentially what we assumed in the previous section when defining the probability of formulas in $\\mathcal{L}$ in $M_{0}$ , in addition to making interventions on different variables independent and independent of the context in $M$ . In any case, it is easy to see that this gives a well-defined probability on $\\mathcal{R}(\\mathcal{U})\\times\\mathcal{R}(U_{Y_{1}})\\times\\mathcal{R}(U_{Y_{m}}).$ , the contexts in $M^{\\prime}$ . Moreover, $M^{\\prime}$ is clearly a causal model with the same type as $M$ that is i-compatible with $M$ . ", "page_idx": 8}, {"type": "text", "text": "We can easily modify this construction to get a family of causal models compatible with $M$ , by loosening the requirements on $\\mathrm{Pr^{\\prime}}$ . While we do want the marginal of $\\mathrm{Pr^{\\prime}}$ on $\\boldsymbol{\\mathcal{U}}$ to agree with the marginal of $\\mathrm{Pr}$ on $\\boldsymbol{\\mathcal{U}}$ , and we want it to reproduce the probability of the cpt for each variable $Y_{i}$ (as defined above), there are no further independence requirements. If we do that, we get the bounds computed by Balke and Pearl (1994). The following example illustrates the impact of dropping the independence assumptions. ", "page_idx": 8}, {"type": "text", "text": "Example 4.1. Consider the CBN $M^{*}$ from Example 3.1 again. Using the notation from that example, suppose that $a\\,=\\,1$ and $b\\,=\\,d\\,=\\,1/2$ . Independence guarantees that the set of ccces that includes $U\\;=\\;0,$ , $X\\ =\\ 0\\ \\mid\\ U\\ =\\ 0,$ , and $Y\\;=\\;0\\;\\;|\\;\\;X\\;=\\;0$ has probability $a b d\\;=\\;1/4$ . But now consider a causal model $(M^{**},\\operatorname*{Pr}^{**})$ compatible with $M^{*}$ where the contexts are the same as in our construction, but the probability $\\mathrm{Pr^{**}}$ does not build in the independence assumptions of our construction. Recall that contexts in $M^{**}$ have the form $(u,f_{X},f_{Y})$ . Since we want $(M^{**},\\operatorname*{Pr}^{**})$ to be compatible with $M^{*}$ , we must have $\\mathrm{Pr}^{**}(\\{(u,f_{X},f_{U},f_{Y})\\::\\:u\\:=\\:0\\})\\:=\\:1,$ , $\\mathrm{Pr}^{**}(\\{(u,f_{X},f_{Y}):f_{X}(0)=0\\})=1/2,$ , and $\\mathrm{Pr}^{**}(\\{(u,f_{X},f_{Y}):f_{Y}(0)=0\\})=1/2$ , so that $\\mathrm{Pr^{**}}$ agrees with the three cpts. But this still leaves a lot of flexibility. For example, we might have $\\mathrm{Pr}^{**}(\\{(u,f_{X},f_{Y}):f_{X}(0)\\stackrel{\\cdot}{=}f_{Y}(0)=0\\}=P r^{**}(\\{(u,\\overleftarrow{f_{X}},f_{Y}):f_{X}(1)=\\acute{f}_{Y}(1)=1\\})$ = 1/2 (so that $\\begin{array}{r l r}&{}&{\\mathrm{~\\partial_{t}^{\\tau}~}\\mathrm{~\\partial_{t}^{\\tau}}\\mathrm{~\\partial_{x}^{\\tau}}\\mathrm{~\\partial_{y}^{\\tau}~}\\mathrm{~\\partial_{z}^{\\tau}}\\mathrm{~\\partial_{x}^{\\tau}(}\\boldsymbol{v}\\boldsymbol{\\jmath}-\\boldsymbol{\\jmath}_{Y}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}-\\boldsymbol{\\mathsf{\\Pi}}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}}\\\\ &{}&{\\mathrm{\\partial_{t}^{\\tau}}\\mathrm{\\partial_{\\boldsymbol{t}}^{\\mathrm{\\Delta}}}\\mathrm{\\partial_{\\boldsymbol{t}}^{\\tau}}\\mathrm{~\\partial_{\\boldsymbol{t}}^{\\tau}~}\\mathrm{\\partial_{\\boldsymbol{r}}^{\\tau}}\\mathrm{~\\partial_{\\boldsymbol{r}}^{\\tau}~}\\mathrm{\\partial_{\\boldsymbol{r}}^{\\tau}(}\\boldsymbol{0}_{1}^{\\tau}=\\boldsymbol{0}_{2}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}(1)=1\\boldsymbol{\\upgamma}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\mathrm{\\boldmath{\\cal~f}_{\\ell}^{\\tau}~}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}\\mathrm{\\Delta}\\boldsymbol{\\jmath}_{\\ell}^{\\tau}}\\\\ &{}&{\\mathrm{\\$ , $f_{Y}(1)=$ ${0}\\}=0.$ ). As shown in Example 3.2, ${\\operatorname*{Pr}}_{M^{*}}(X=0\\land Y=0\\land[X\\leftarrow1](Y=1))=1/.$ 4. However, it is easy to check that $\\operatorname*{Pr}_{M^{**}}(X\\,=\\,0\\land\\dot{Y}\\,=\\,0\\land[X\\,\\leftarrow\\,1](Y\\,=\\,1))\\,=\\,1/2$ . (Tian and Pearl (2000) give bounds on the range of probabilities for this formula, which is called the probability of necessity; see also Section $B$ and (Pearl 2000, Section 9.2).) ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Computing counterfactual probabilities ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we analyze counterfactual probabilities, introduced by Balke and Pearl (1994). Counterfactual probabilities have been widely used in several domains, including psychology (Hoerl, McCormack, and Beck 2011), epidemiology (Greenland and Robins 1999), and political science (Grynaviski 2013), to explain the effects on the outcome. More recently, they have proved useful in machine learning to explain the output of ML models (Beckers 2022). ", "page_idx": 9}, {"type": "text", "text": "Two types of counterfactual formulas that have proved particularly useful are the probability of necessity and the probability of sufficiency; we focus on them in this section. As discussed by Pearl (2000), counterfactual analysis is particularly useful when it comes to understanding the impact of a decision on the outcome. For example, we might be interested in the probability that an outcome $O$ would not have been favorable if $A$ were not true. This captures the extent to which $A$ is a necessary cause of $O$ . Similarly, we might be interested in whether $A$ is sufficient for $O$ : that is if $A$ were true, would $O$ necessarily be true? We now review the formal definitions of these notions; see (Pearl 2000) for more discussion. ", "page_idx": 9}, {"type": "text", "text": "Definition 5.1. Let $X$ and $Y$ be binary variables in a causal model or CBN M. ", "page_idx": 9}, {"type": "text", "text": "1. Probability of necessity of $X$ for $Y$ : $\\mathbf{P}\\mathbf{N}_{M}^{X,Y}=\\operatorname*{Pr}_{M}([X\\leftarrow0](Y=0)|X=1\\land Y=1).$ 2. Probability of sufficiency of $X$ for $Y$ $\\colon\\mathrm{PS}_{M}^{X,Y}=\\mathrm{Pr}_{M}([X\\leftarrow1](Y=1)\\mid X=0{\\land}Y=0)$ . 3. Probability of necessity and sufficiency of $X$ for $Y$ : ${\\mathrm{PNS}}_{M}^{X,Y}\\;=\\;{\\mathrm{Pr}}_{M}([X\\;\\leftarrow\\;1](Y\\;=$ $1)\\wedge[X\\leftarrow0](Y=0))$ . ", "page_idx": 9}, {"type": "text", "text": "Pearl (2000) gives examples showing that neither the probability of necessity nor the probability of sufficiency in a CBN can be identified; we can just determine a range for these probabilities. But with our independence assumptions, they can be identified, justifying our notation. Moreover, these probabilities can be computed using only conditional probabilities of (singly) exponentially many simple formulas (not involving interventions). Since these formulas do not involve interventions, they can be estimated from observational data, without requiring involving controlled experiments. Thus, our results and assumptions have significant practical implications. ", "page_idx": 9}, {"type": "text", "text": "Let $P a^{X}(Y)$ consist of all the parents of $Y$ other than $X$ . For a set $\\mathcal{Z}$ of variables, let $\\mathcal{T}_{\\mathcal{Z}}$ consist of all possible settings of the variables in $\\mathcal{Z}$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.2. If $M$ is a CBN where $Y$ is a child of $X$ , then ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{PN}_{M}^{X,Y}=\\sum_{c_{P a^{X}(Y)}^{j}\\in{\\mathcal{T}}_{P a^{X}(Y)}}\\mathrm{Pr}_{M}(P a^{X}(Y)=c_{P a^{X}(Y)}^{j}\\mid Y=1\\land X=1)}}\\\\ &{}&{\\mathrm{Pr}_{M}(Y=0\\mid X=0\\land P a^{X}(Y)=c_{P a^{X}(Y)}^{j});\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\sf P S}_{M}^{X,Y}=\\sum_{c_{P a^{X}(Y)}^{j}\\in{\\mathcal{T}}_{P a^{X}(Y)}}\\!\\!\\!\\!\\!\\operatorname*{Pr}_{M}(P a^{X}(Y)=c_{P a^{X}(Y)}^{j}\\mid Y=0\\land X=0)}\\\\ {{\\operatorname*{Pr}_{M}}(Y=1\\mid X=1\\land P a^{X}(Y)=c_{P a^{X}(Y)}^{j});\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\colon\\ensuremath{\\operatorname{PNS}}_{M}^{X,Y}=\\ensuremath{\\operatorname{PS}}_{M}^{X,Y}\\cdot\\ensuremath{\\operatorname{Pr}}_{M}(X=0\\wedge Y=0)+\\ensuremath{\\operatorname{PN}}_{M}^{X,Y}\\cdot\\ensuremath{\\operatorname{Pr}}_{M}(X=1\\wedge Y=1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We defer the proof of the theorem to Section B.1 in the appendix, where further extensions are also provided. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments: Halpern\u2019s work was supported in part by AFOSR grant FA23862114029, MURI grant W911NF-19-1-0217, ARO grant W911NF-22-1-0061, and NSF grant FMitF-2319186. Galhotra\u2019s work is supported by a grant from Infosys. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Balke, A. and J. Pearl (1994). Probabilistic evaluation of counterfactual queries. In Proc. Twelfth National Conference on Artificial Intelligence (AAAI \u201994), pp. 200\u2013207.   \nBeckers, S. (2022). Causal explanations and XAI. In Proc. First Conference on Causal Learning and Reasoning, Volume 177 of Proceedings of Machine Learning Research, pp. 90\u2013109.   \nBeckers, S. (2023). Disjunctive counterfactuals using causal models: a critical examination. Unpublished manuscript.   \nFariha, A., S. Nath, and A. Meliou (2020). Causality-guided adaptive interventional debugging. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, pp. 431\u2013446.   \nGalhotra, S., A. Fariha, R. Louren\u00b8co, J. Freire, A. Meliou, and D. Srivastava (2022). Dataprism: Exposing disconnect between data and systems. In Proceedings of the 2022 International Conference on Management of Data, pp. 217\u2013231.   \nGalhotra, S., R. Pradhan, and B. Salimi (2021). Explaining black-box algorithms using probabilistic contrastive counterfactuals. In Proceedings of the 2021 International Conference on Management of Data, pp. 577\u2013590.   \nGreenland, S. (1999). Relation of probability of causation to relative risk and doubling dose: a methodologic error that has become a social problem. American journal of public health 89(8), 1166\u20131169.   \nGreenland, S., J. Pearl, and J. M. Robins (1999). Causal diagrams for epidemiologic research. Epidemiology 10(1), 37\u201348.   \nGreenland, S. and J. M. Robins (1999). Epidemiology, justice, and the probability of causation. Jurimetrics 40, 321.   \nGrynaviski, E. (2013). Contrasts, counterfactuals, and causes. European Journal of International Relations 19(4), 823\u2013846.   \nHicks, J. (1979). Causality in economics. Basic Books.   \nHoerl, C., T. McCormack, and S. R. Beck (2011). Understanding Counterfactuals, Understanding Causation: Issues in Philosophy and Psychology. Oxford University Press.   \nKobayashi, S., K. Otomo, and K. Fukuda (2019). Causal analysis of network logs with layered protocols and topology knowledge. In 2019 15th International Conference on Network and Service Management (CNSM), pp. 1\u20139. IEEE.   \nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. San Francisco: Morgan Kaufmann.   \nPearl, J. (2000). Causality: Models, Reasoning, and Inference. New York: Cambridge University Press.   \nRichardson, O. E., S. Peters, and J. Y. Halpern (2024). Representing mechanism (in)dependence. In Proc. Advances in Nueral Information Processing Systems 37 (NeurIPS \u201924).   \nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66(5), 688\u2013701.   \nTian, J. and J. Pearl (2000). Probabilities of causation: bounds and identification. Annals of Mathematics and Artificial Intelligence 28, 287\u2013313.   \nWu, K. (2008). Modeling the semiconductor industry dynamics. Ph. D. thesis, Massachusetts Institute of Technology. ", "page_idx": 10}, {"type": "text", "text": "A Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof: As we said, we show that if two causal models $M_{1}$ and $M_{2}$ have the same type as $M$ and $\\vec{u}_{1}$ and $\\vec{u}_{2}$ are contexts such that $(M_{1},\\vec{u}_{1})\\vDash\\varphi_{\\alpha}$ and $(M_{2},\\vec{u}_{2})\\in\\varphi_{\\alpha}$ , then for all formulas $\\psi\\in{\\mathcal{L}}^{+}(S)$ (resp., $\\psi\\in{\\mathcal{L}}(S))$ , we have that (1) (as defined in the main text) holds. The claimed result follows immediately. ", "page_idx": 11}, {"type": "text", "text": "We give the proof in the case that $\\alpha$ is a ccce and $\\psi\\,\\in\\,{\\mathcal{L}}^{+}(S)$ . The modifications needed to deal with the case that $\\alpha$ is an fccce and $\\psi\\,\\in\\,{\\mathcal{L}}(S)$ are straightforward and left to the reader. Since $M$ is acyclic, we can order the exogenous and endogenous variables topologically. Let $X_{1},\\ldots,X_{m}$ be such an ordering. We first prove by induction on $j$ that, for all interventions $\\vec{Y}\\gets\\vec{y}$ (including the empty intervention) and $x_{j}\\in{\\mathcal{R}}(X_{j})$ $),\\,(M_{1},\\vec{u}_{1})\\,|=[\\vec{Y}\\leftarrow\\vec{y}](X_{j}=x_{j})$ iff $(M_{2},\\vec{u}_{2})\\vDash[\\vec{Y}\\leftarrow$ ${\\vec{y}}](X_{j}=x_{j})$ . ", "page_idx": 11}, {"type": "text", "text": "For the base case, $X_{1}$ must be exogenous, and hence have no parents. If $X_{1}$ is not one of the variables in $\\vec{Y}$ , then we must have $(M_{1},\\vec{u}_{1})\\vDash[\\vec{Y}\\gets\\vec{y}](X_{1}=x_{1})$ iff $(M_{1},\\vec{u}_{1})\\vDash(X_{1}=x_{1})$ , and similarly for $M_{2}$ ; since no variable in $\\vec{Y}$ is a parent of $X_{1}$ , intervening on $\\vec{Y}$ has no effect on $X_{1}$ . Since $(M_{1},\\vec{u}_{1})\\vDash\\varphi_{\\alpha}$ and $(M_{2},\\vec{u}_{2})\\in\\varphi_{\\alpha},I$ $M_{1}$ and $M_{2}$ agree on the values of variables in $\\boldsymbol{\\mathcal{U}}$ . Thus, $\\left(M_{1},\\vec{u}_{1}\\right)\\vDash\\left(X_{1}=x_{1}\\right)$ iff $\\left(M_{2},\\vec{u}_{2}\\right)\\vDash\\left(X_{1}=x_{1}\\right)$ . It follows that $(M_{2},\\vec{u}_{2})\\models[\\vec{Y}\\leftarrow\\vec{y}](X_{1}=x_{1})$ . $(M_{2},\\vec{u}_{2})\\models[\\vec{Y}\\leftarrow\\vec{y}](X_{1}=x_{1})$ , as desired. ", "page_idx": 11}, {"type": "text", "text": "On the other hand, if $X_{1}$ is one of the variables in $\\vec{Y}$ (which can happen only if the formula is in ${\\mathcal{L}}^{+}(S))$ , let $x^{*}$ be the value in $\\vec{y}$ corresponding to $X_{1}$ . In that case, the formula $[\\vec{Y}\\leftarrow\\vec{y}](X_{1}=x^{*})$ is valid with respect to $M$ . It follows that $(M_{1},\\vec{u}_{1})\\vDash[\\vec{Y}\\leftarrow\\vec{y}](X_{1}=x_{1})$ iff $x_{1}=x^{*}$ , and similarly for $(M_{2},\\vec{u}_{2})$ . The desired result follows. This completes the proof for the base case. ", "page_idx": 11}, {"type": "text", "text": "Now suppose that we have proved the result for $j\\,<\\,m$ . Let $Z_{1},\\ldots,Z_{k}$ be the parents of $X_{j+1}$ in $M$ . Since $X_{1},\\ldots,X_{m}$ is a topological sort, we must have $\\{Z_{1},\\ldots,Z_{k}\\}\\subseteq\\{X_{1},\\ldots,X_{j}\\}$ . Let $z_{1},\\ldots,z_{k}$ be values in $\\mathcal{R}(Z_{1}),\\ldots,\\mathcal{R}(Z_{k})$ , respectively, such that $(M_{1},\\vec{u}_{1})\\ensuremath{\\left|{=[\\vec{Y}\\leftarrow\\vec{y}](Z_{h}=z_{h})}\\right.}$ , for $h=1,\\ldots,k$ . By the induction hypothesis, $(M_{2},\\vec{u}_{2})\\vDash[\\vec{Y}\\gets\\vec{y}](Z_{h}=z_{h})$ , for $h=1,\\ldots,k$ . Moreover, it is easy to see that $({\\vec{Y}}\\gets{\\vec{y}}]\\varphi\\wedge[{\\vec{Y}}\\gets{\\vec{y}}]\\varphi^{\\prime}))\\Leftrightarrow[{\\vec{Y}}\\gets{\\vec{y}}](\\varphi\\wedge\\varphi^{\\prime})$ is valid with respect to $M$ . Thus, $(M_{1},\\vec{u}_{1})\\vDash[\\vec{Y}\\leftarrow\\vec{y}](Z_{1}=z_{1}\\land\\dotsc Z_{k}=z_{k})$ ) and similarly for $(M_{2},\\vec{u}_{2})$ . Moreover, since $Z_{1},\\ldots,Z_{k}$ are the parents of $X_{j+1}$ , it follows that $(M_{1},\\vec{u}_{1})\\,\\vline=\\,[\\vec{Y}\\,\\leftarrow\\,\\vec{y}](X_{j+1}\\,=\\,x_{j+1})$ iff $[Z_{1}\\;=\\;z_{1}\\wedge.\\,.\\,.\\,Z_{k}\\;=\\;\\bar{z}_{k})](X_{j+1}\\overset{\\cdot}{=}\\;x_{j+1})$ is a conjunct of $\\varphi_{\\alpha}$ . Since $(M_{1},\\vec{u}_{1})\\,\\vDash\\,\\varphi_{\\alpha}$ and $(M_{2},\\vec{u}_{2})\\vDash\\varphi_{\\alpha}$ , the desired result follows, completing the induction proof. ", "page_idx": 11}, {"type": "text", "text": "The argument that $(M_{1},\\vec{u}_{1})\\vDash[\\vec{Y}\\leftarrow\\vec{y}]\\psi$ iff $(M_{2},\\vec{u}_{2})\\vDash[\\vec{Y}\\leftarrow\\vec{y}]\\psi$ for arbitrary (simple) formulas $\\psi$ now follows from the fact that (as we already observed) $([\\vec{Y}\\leftarrow\\vec{y}]\\varphi\\land[\\vec{Y}\\leftarrow\\vec{y}]\\varphi^{\\prime}))\\Leftrightarrow[\\vec{Y}\\leftarrow$ $\\vec{y}](\\varphi\\wedge\\varphi^{\\prime})$ is valid with respect to $M$ , as are $([{\\vec{Y}}\\leftarrow{\\vec{y}}]\\varphi\\lor[{\\vec{Y}}\\leftarrow{\\vec{y}}]\\varphi^{\\prime}))\\Leftrightarrow[{\\vec{Y}}\\leftarrow{\\vec{y}}](\\varphi\\lor\\varphi^{\\prime})$ and $[{\\vec{Y}}\\leftarrow{\\vec{y}}]\\neg\\varphi\\Leftrightarrow\\neg[{\\vec{Y}}\\leftarrow{\\vec{y}}]\\varphi,$ . ", "page_idx": 11}, {"type": "text", "text": "Finally, we can deal with Boolean combinations of causal formulas by a straightforward induction.   \nThis completes the argument that (1) holds for all formulas in $\\psi\\in{\\mathcal{L}}^{\\dot{+}}(S)$ . ", "page_idx": 11}, {"type": "text", "text": "B Computing counterfactual probabilities (Missing Proof and Extension) ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We prove the calculation for the probability of sufficiency, $\\mathrm{PS}_{M}^{X,Y}$ . Essentially the same argument can be used to compute the probability of necessity, $\\mathrm{PN}_{M}^{X,Y}$ . Finally, for part (c), we use the representation of PNSXM,Y in terms of PSXM, and $\\mathrm{PN}_{M}^{X,Y}$ given in (Pearl 2000, Lemma 9.2.6). ", "page_idx": 11}, {"type": "text", "text": "Proof of Theorem $5.2\\;(b)$ . Let ${\\mathcal{Z}}={\\mathcal{U}}\\cup{\\mathcal{V}}\\setminus\\{X,Y\\}$ . $\\mathcal{T}_{\\mathcal{Z}}$ has $2^{n-2}$ settings, where $n=|\\mathcal{U}\\cup\\mathcal{V}|$ . For a setting $c\\in{\\mathcal{T}}_{\\mathcal{Z}}$ , let $c_{Z}$ be the setting of the variable $Z$ in $c$ . ", "page_idx": 11}, {"type": "text", "text": "By definition, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{PS}_{M}^{X,Y}=\\frac{\\operatorname*{Pr}_{M}((X=0)\\wedge(Y=0)\\wedge[X\\gets1](Y=1))}{\\operatorname*{Pr}_{M}((X=0)\\wedge(Y=0))}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Let the numerator $(X=0)\\land(Y=0)\\land[X\\leftarrow1](Y=1)$ be $\\psi$ . Then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}_{M}(\\psi)=}\\\\ &{\\sum_{c\\in{\\cal T}_{\\cal Z}}\\operatorname*{Pr}_{M}((X=0)\\wedge(Y=0)\\wedge[X\\gets1](Y=1)\\wedge\\bigwedge_{Z\\in{\\cal Z}}(Z=c_{Z})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We next compute the probability of ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\psi_{c}=(X=0)\\wedge(Y=0)\\wedge[X\\gets1](Y=1)\\wedge\\bigwedge_{Z\\in\\mathcal{Z}}(Z=c_{Z}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "From Theorem 3.6, it follows that in all causal models $M^{\\prime}$ compatible with $M$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{M^{\\prime}}(\\psi_{c})=\\sum_{\\varphi_{\\alpha}\\Longrightarrow\\;\\psi_{c}}\\operatorname*{Pr}_{M^{\\prime}}(\\varphi_{\\alpha}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now $\\varphi_{\\alpha}\\implies\\psi_{c}$ holds whenever $\\alpha$ contains the following events: ", "page_idx": 12}, {"type": "text", "text": "Let $S_{c}$ consist of all ccces that contain these four events, and let $\\varphi_{S_{c}}$ be the conjunction of the formulas corresponding to the events in $S_{c}$ . Then by Theorem 3.6, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\varphi_{c}\\Leftrightarrow\\bigvee_{\\alpha\\in S_{c}}\\varphi_{\\alpha}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since the formulas $\\varphi_{\\alpha}$ for distinct ccces in $S_{c}$ are mutually exclusive, we have that $\\mathrm{Pr}_{M^{\\prime}}(\\psi_{c})\\,=$ $\\operatorname*{Pr}_{M^{\\prime}}\\left(\\varphi_{S}\\right)$ . Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{c\\in\\mathbb{Z}_{\\mathcal{Z}}}{\\operatorname*{Pr}_{M^{\\prime}}}\\big(\\psi\\big)}\\\\ &{=\\underset{c\\in\\mathcal{T}_{\\mathcal{Z}}}{\\sum}\\operatorname*{Pr}_{M^{\\prime}}\\!\\big(\\varphi_{c}\\big)}\\\\ &{=\\underset{c\\in\\mathcal{T}_{\\mathcal{Z}}}{\\sum}\\operatorname*{Pr}_{M^{\\prime}}\\!\\big(X=0\\wedge Y=0\\wedge\\bigwedge_{Z\\in\\mathcal{Z}}(Z=c_{Z})\\wedge[X\\leftarrow1,P a^{X}(Y)\\leftarrow c_{P a^{X}(Y)}](Y=1)\\big)}\\\\ &{=\\underset{c_{P a}x_{(Y)}\\in\\mathcal{T}_{P a}x_{(Y)}}{\\sum}\\operatorname*{Pr}_{M^{\\prime}}\\!\\big(X=0\\wedge Y=0\\wedge P a^{X}(Y)\\leftarrow c_{P a}x_{(Y)}\\wedge[X\\leftarrow1,P a^{X}(Y)\\leftarrow c_{P a}x_{(Y)}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "If $M^{\\prime}$ is i-compatible with $M$ , then we can further conclude that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{c\\in\\mathcal{T}_{Z}}\\operatorname*{Pr}_{M^{\\prime}}(X=0\\land Y=0\\land\\bigwedge_{Z\\in\\mathcal{Z}}(Z=c_{Z})\\land[X\\leftarrow1,P a^{X}(Y)\\leftarrow c_{P a^{X}(Y)}](Y=1))}\\\\ &{=\\sum_{c_{P a}x_{(Y)}\\in\\mathcal{T}_{P a}x_{(Y)}}\\operatorname*{Pr}_{M^{\\prime}}(X=0\\land P a^{X}(Y)=c_{P a}x_{(Y)})\\operatorname*{Pr}_{M^{\\prime}}(Y=0\\mid X=0\\land P a^{X}(Y)=c_{P a}x_{(Y)})}\\\\ &{\\qquad\\qquad\\qquad\\operatorname*{Pr}_{M^{\\prime}}(Y=1\\mid X=1\\land P a^{X}(Y)=c_{P a}x_{(Y)})}\\\\ &{=\\sum_{c_{P a}x_{(Y)}\\in\\mathcal{T}_{P a}x_{(Y)}}\\operatorname*{Pr}_{M^{\\prime}}(Y=0\\land X=0\\land P a^{X}(Y)=c_{P a}x_{(Y)})\\operatorname*{Pr}_{M^{\\prime}}[Y=1\\mid X=1\\land P a^{X}(Y)=c_{P a}x_{(Y)}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $\\operatorname*{Pr}_{M^{\\prime}}(\\psi)=\\operatorname*{Pr}_{M}(\\psi)$ , substituting the expression for $\\operatorname{Pr}_{M}(\\psi)$ into (2), we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n:\\frac{\\sum_{c_{P a}x_{(Y)}\\in\\mathcal{T}_{P a}x_{(Y)}}\\operatorname*{Pr}(Y=0\\land X=0\\land P a^{X}(Y)=c_{P a}x_{(Y)})\\operatorname*{Pr}(Y=1\\mid X=1\\land P a^{X}(Y)={\\phi})}{\\operatorname*{Pr}(X=0\\land Y=0)}\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We can extend Theorem 5.2 to the case where $Y$ is any descendant of $X$ (not necessarily a child of $X$ ). In this case, the term involving $P a(Y)$ would change to the set of the ancestors of $Y$ at the same level as $X$ in the topological ordering of the variables. We can further extend Theorem 5.2 to arbitrary formulas $\\psi$ , where $\\operatorname*{Pr}(\\psi)$ can be determined by calculating the probability of formulas that do not involve interventions (although they may involve conditional probabilities), and thus can be determined using only observational information. The key idea of the proof is to convert $\\psi$ to a disjunction of conjunctions, where the disjuncts are mutually exclusive and have the form $\\psi_{i}=\\bar{\\psi_{i0}\\,\\wedge\\,}\\left(\\bigwedge_{j\\in\\{1,\\dots,r\\}}\\bar{\\psi_{i j}}\\right)$ , where $\\psi_{i0}=\\left(\\bigwedge_{j\\in\\{1,\\ldots,s\\}}(Z_{i j}=z_{i j})\\right)$ is a simple formula (with no intervention), and $\\psi_{i j}$ for $j>0$ has the form $[\\vec{X}_{j}\\gets\\vec{x}_{j}](\\bigwedge_{k\\in\\{1,\\dots,t\\}}Y_{i j k}=y_{i j k})$ , where $Y_{i j k}$ is a descendant of $\\vec{X_{j}}$ in $M$ , so that we can apply the ideas in the proof of Theorem 5.2 to each disjunct separately. In terms of complexity, we show that $\\operatorname*{Pr}(\\psi)$ can be estimated in $O(m\\cdot2^{n r^{*}})$ conditional probability calculations, where $r^{*}$ is the maximum number of conjuncts in a disjunction $\\psi_{i}$ that involve at least one intervention, and $m$ is the number of disjuncts in the DNF. Unfortunately, for an arbitrary formula $\\psi$ , determining $\\operatorname*{Pr}(\\psi)$ may involve doubly-exponentially many conditional probabilities. We defer details to Section ?? in the appendix. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1. Given a CBN $M=(S,\\mathcal{P})$ and an arbitrary formila $\\psi$ , then $\\operatorname*{Pr}(\\psi)$ can be determined by taking the probability of formulas that do not involve interventions (although they may involve conditional probabilities), and thus can be determined using only observational information. ", "page_idx": 13}, {"type": "text", "text": "We now show that we can calculate the probability of an arbitrary formula $\\psi$ in terms of conditional probabilities that can be estimated from observational data. To prove this result, we first convert $\\psi$ to an equivalent formula in a canonical form. Specifically, it has the form $\\psi_{1}\\lor\\cdots\\lor\\psi_{m}$ , where the $\\psi_{i}\\mathbf{s}$ are mutually exclusive and each $\\psi_{i}$ is a conjunction of the form $\\psi_{i0}\\wedge\\cdot\\cdot\\cdot\\wedge\\psi_{i r_{i}}$ , where $\\psi_{i0}$ is a simple formula and for $1\\le j\\le r_{i}$ , $\\psi_{i j}$ is a formula of the form $[\\vec{X_{j}}\\leftarrow\\vec{x}_{j}](\\bigwedge_{k\\in\\{1,...,t_{i j}\\}}Y_{i j k}=$ $y_{i j k})$ , and the interventions are all distinct. This conversion just involves standard propositional reasoning and two properties which hold under the semantics described in Section 3. The first is that $[Y\\stackrel{.}{\\leftarrow}y]\\varphi\\wedge[\\bar{Y}\\stackrel{.}{\\leftarrow}y]\\varphi^{\\prime}$ is equivalent to $[Y\\leftarrow y](\\varphi\\land\\varphi^{\\prime})$ . The second is that $\\neg[Y\\leftarrow y]\\varphi$ is equivalent to $[Y\\leftarrow y]\\neg\\varphi$ . ", "page_idx": 13}, {"type": "text", "text": "Ignore for now the requirements that the disjuncts be mutually exclusive, that all interventions be distinct, and that there be no leading formulas involving interventions. Using standard propositional reasoning, we can transform a formula $\\varphi$ to an equivalent formula in DNF, where the literals are either simple formulas or intervention formulas (i.e., formulas of the form $[X\\leftarrow x]\\varphi)$ . Of course, the disjuncts may not be mutually exclusive. Again, using straightforward propositional reasoning, we can convert the formula to a DNF where the disjuncts are mutually exclusive. Rather than writing out the tedious details, we give an example. Consider a formula of the form $\\left(\\varphi_{1}\\wedge\\varphi_{2}\\right)\\vee\\left(\\varphi_{3}\\wedge\\varphi_{4}\\right)$ . This is propositionally equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\varphi_{1}\\wedge\\varphi_{2}\\wedge\\varphi_{3}\\wedge\\varphi_{4}\\right)\\vee\\left(\\varphi_{1}\\wedge\\varphi_{2}\\wedge\\lnot\\varphi_{3}\\wedge\\varphi_{4}\\right)\\vee\\left(\\varphi_{1}\\wedge\\varphi_{2}\\wedge\\varphi_{3}\\wedge\\lnot\\varphi_{4}\\right)\\vee\\left(\\varphi_{1}\\wedge\\varphi_{2}\\wedge\\lnot\\varphi_{3}\\wedge\\lnot\\varphi_{4}\\right)}\\\\ &{\\vee\\big(\\neg\\varphi_{1}\\wedge\\varphi_{2}\\wedge\\varphi_{3}\\wedge\\varphi_{4}\\big)\\vee\\left(\\varphi_{1}\\wedge\\lnot\\varphi_{2}\\wedge\\varphi_{3}\\wedge\\varphi_{4}\\right)\\vee\\left(\\neg\\varphi_{1}\\wedge\\lnot\\varphi_{2}\\wedge\\varphi_{3}\\wedge\\varphi_{4}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can now apply the two equivalences mentioned above to remove leading negations from intervention formulas and to ensure that, in each disjunct, all interventions are distinct. These transformations maintain the fact that the disjuncts are mutually exclusive. ", "page_idx": 13}, {"type": "text", "text": "Since the disjuncts in $\\psi$ are mutually exclusive, the probability of $\\psi$ is the sum of the probabilities of the disjuncts; that is, $\\begin{array}{r}{\\operatorname*{Pr}(\\psi)=\\sum_{i\\in\\{1,...,m\\}}\\operatorname*{Pr}(\\psi_{i}^{\\bar{}})}\\end{array}$ . To compute the probability of a disjunct $\\psi_{i}$ , we first simplify it using the following two observations. First, if a formula involves an intervention $[X\\leftarrow x]$ on some variable $X$ such that $X$ is also set to $x$ in the formula, such as $(X=x\\land[X\\leftarrow$ $\\dot{x},Z\\gets\\dot{z}](Y=1))$ , the intervention $X\\gets x$ is redundant and can be dropped; for example, $X=$ $0\\land[X\\leftarrow0,Z\\leftarrow1](Y=1)$ is equivalent to $X=0\\land[Z\\leftarrow1](Y=1)$ . Second, if an intervention formula does not contain a descendant of the intervened variables, such as $\\psi=[\\vec{X}\\leftarrow\\vec{x}](\\psi_{1}\\land\\psi_{2})$ , where all variables in $\\psi_{1}$ are non-descendants of the variables in $\\vec{X}$ , then the variables in $\\psi_{1}$ are not affected by the intervention, so $\\psi_{1}$ can be pulled out of the scope of the intervention; that is, $\\psi$ is equivalent to $\\psi_{1}\\wedge[X\\leftarrow x](\\psi_{2})$ . Using these observations, we remove all interventions that are redundant and pull formulas involving only non-descendants of the intervened variables out of the intervention formula. ", "page_idx": 13}, {"type": "text", "text": "After this simplification, without loss of generality, the disjunct $\\psi_{i}$ is a conjunction of formulas $\\psi_{i0}\\wedge\\Big(\\bigwedge_{j\\in\\{1,...,r_{i}\\}}\\psi_{i j}\\Big)$ , where $\\psi_{i0}\\,=\\,\\Bigl(\\Lambda_{j\\in\\{1,\\dots,s_{i}\\}}(Z_{i j}=z_{i j})\\Bigr)$ is a simple formula (with no intervention), and $\\psi_{i j}$ for $j>0$ has the form $[\\vec{X}_{j}\\gets\\vec{x}_{j}](\\bigwedge_{k\\in\\{1,...,t_{i j}\\}}Y_{i j k}=y_{i j k})$ , where $Y_{i j k}$ is a descendant of some variable in $\\vec{X_{j}}$ in $M$ . The following theorem proves the result for $\\psi_{i}$ , which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.2. If $M$ is a CBN and $\\psi_{i}=\\psi_{i0}\\wedge\\left(\\bigwedge_{j\\in\\{1,\\dots,r_{i}\\}}\\psi_{i j}\\right)$ , where $\\psi_{i}$ contains no redundant interventions, $\\begin{array}{r}{\\psi_{i0}=\\left(\\bigwedge_{j\\in\\{1,...,s_{i}\\}}(Z_{i j}=z_{i j})\\right)}\\end{array}$ is a simple formula (with no interventions), and $\\psi_{i j}$ for $j>0$ has the form $[\\vec{X}_{j}\\gets\\vec{x}_{j}](\\bigwedge_{k\\in\\{1,...,t_{i j}\\}}Y_{i j k}=y_{i j k})$ , where $Y_{i j k}$ is a descendant of some variable in $\\vec{X_{j}}$ in $M$ , then $\\operatorname*{Pr}(\\psi_{i})$ can be computed by determining the probability of formulas that do not involve an intervention. ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof proceeds along lines very similar to the proof of Theorem 5.2. ", "page_idx": 14}, {"type": "text", "text": "Let $\\bar{\\mathcal Z}=\\cup_{j=1}^{s_{i}}Z_{i j}$ , $\\mathcal{Z}=\\mathcal{U}\\cup\\mathcal{V}\\setminus\\bar{\\mathcal{Z}}$ , and $\\bar{z}=\\{z_{i j}:j\\in\\{1,\\ldots,s_{i}\\}\\}$ . $\\mathcal{T}_{\\mathcal{Z}}$ has $2^{|\\mathcal{Z}|}$ settings. For a setting $c\\in{\\mathcal{T}}_{\\mathcal{Z}}$ , let $c_{Z}$ be the setting of the variable $Z$ in $c$ . We use $P a_{A}(Z)$ to denote $A\\cap P a(Z)$ , i.e., the set of parents of $Z$ in $A$ and $\\bar{z}_{A}$ to denote the values in $\\bar{z}$ for all variables in $A$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{c\\in T_{\\mathcal{Z}}}\\operatorname*{Pr}_{M}\\left(\\underset{j\\in\\{1,\\dots,s_{i}\\}}{\\bigwedge}(Z_{i j}=z_{i j})\\wedge\\left(\\underset{j\\in\\{1,\\dots,r_{i}\\}}{\\bigwedge}[\\vec{X}_{j}\\gets\\vec{x}_{j}](\\underset{k\\in\\{1,\\dots,t_{i j}\\}}{\\bigwedge}Y_{i j k}=y_{i j k})\\right)\\wedge\\underset{z\\in\\mathcal{Z}}{\\bigwedge}(Z=c)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We next compute the probability of ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{\\nu}_{i c}=\\biggl(\\bigwedge_{j\\in\\{1,\\dots,s_{i}\\}}(Z_{i j}=z_{i j})\\wedge\\biggl(\\bigwedge_{j\\in\\{1,\\dots,r_{i}\\}}[\\vec{X}_{j}\\leftarrow\\vec{x}_{j}](\\bigwedge_{k\\in\\{1,\\dots,t_{i j}\\}}Y_{i j k}=y_{i j k})\\biggr)\\wedge\\bigwedge_{Z\\in\\mathcal{Z}}(Z=c_{Z})\\biggr).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Theorem 3.6, it follows that in all causal models $M^{\\prime}$ compatible with $M$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{M^{\\prime}}(\\psi_{i c})=\\sum_{\\varphi_{\\alpha}\\Longrightarrow\\;\\psi_{i c}}\\operatorname*{Pr}_{M^{\\prime}}(\\varphi_{\\alpha}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now $\\varphi_{\\alpha}\\implies\\psi_{i c}$ holds whenever $\\alpha$ contains the following events: ", "page_idx": 14}, {"type": "text", "text": "1. $(Z_{i j}=z_{i j}\\mid P a(Z_{i j})=c_{P a(Z_{i j})})$ , for all $j\\in\\{1,\\ldots,s_{i}\\}$ ;   \n2. $\\left(Z=c_{Z}\\;\\middle|\\;P a z(Z)=c_{P a z(Z)},P a_{\\bar{z}}(Z)=\\bar{z}_{P a_{\\bar{z}}(Z)}\\right),\\mathrm{for~all~}Z\\in\\mathcal{Z};$   \n3. $(X=c_{X}^{j}\\mid P a(X)=c_{P a(X)}^{j})$ , for all $X\\in\\mathcal{X}_{j}^{\\prime}$ , where $\\mathcal{X}_{j}^{\\prime}$ consists of all descendants of the intervened variables in $\\vec{X_{j}}$ other than the variables in $\\vec{X_{j}}$ and $c^{j}\\in T_{j}^{\\prime}$ , the set of settings of the variables in $\\mathcal{U}\\cup\\mathcal{V}$ , where the following variables are fixed as follows: (a) $\\begin{array}{r l}&{\\vec{X_{j}}=\\vec{x}_{j},}\\\\ &{Y_{i j k}=y_{i j k}\\mathrm{~for~all~}k\\in\\{1,\\dots,t_{i j}\\},}\\\\ &{Z_{i k}=z_{i k}\\mathrm{~for~}Z_{i k}\\notin(\\vec{X_{j}}\\cup\\mathcal{X}_{j}^{\\prime}),k\\in\\{1,\\dots,s_{i}\\}.}\\\\ &{Z=c_{Z}\\mathrm{~for~all~}Z\\in\\mathcal{Z}\\mathrm{~and~}Z\\not\\in(\\vec{X_{j}}\\cup\\mathcal{X}_{j}^{\\prime}).}\\end{array}$ (b) (c) (d) ", "page_idx": 14}, {"type": "text", "text": "Intuitively, $\\mathcal{T}_{j}^{\\prime}$ captures all possible post-intervention settings of all variables that are descendants of $\\vec{X_{j}}$ , while fixing $Y_{i j k^{\\mathrm{s}}}$ as $y_{i j k}$ . By fixing the third set of events, $(X=c_{X}^{j}|P a(X)=c_{P a(X)}^{j})$ for all $X\\in\\mathcal{X}_{j}^{\\prime}$ , we ensure that all events involving descendants of $\\vec{X_{j}}$ are consistent with respect to one of the post-intervention settings $c^{j}\\in\\mathcal T_{j}^{\\prime}$ . These events represent the effects of interventions in $\\vec{X_{j}}\\gets\\vec{x_{j}}$ on its descendants. For example, consider a causal graph as shown below and $\\psi_{i}=$ $[X_{1}\\overleftarrow{\\leftarrow}1,\\bar{X_{3}}\\leftarrow1](Y=1)$ . ", "page_idx": 14}, {"type": "image", "img_path": "DC28Fpk76s/tmp/401afc6f8b15b8b7c59996fe94eccc7b960c21f4d740a4a14dc86c4b35e6b0e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "In this case, $\\vec{X}_{j}=\\{X_{1}\\leftarrow1,X_{3}\\leftarrow1\\}$ . By the conditions mentioned above, $\\alpha$ must contain one of the two events ( $\\left(Y=1\\mid X_{2}=0,X_{3}=\\right.$ 1) \u2227 $\\setminus\\left(X_{2}=0\\mid X_{1}=1\\right)$ ) or $((Y=1\\mid X_{2}=1,X_{3}=$ $1)\\wedge(X_{2}=1\\mid X_{1}=1)\\rangle$ , because ${\\mathcal{T}}_{1}^{\\prime}=\\{\\{X_{1}=1,X_{2}=0,X_{3}=1,Y=1\\}$ , $\\{X_{1}=1,X_{2}=$ $1,X_{3}=1,Y=1\\}\\}$ . This condition ensures that if $X_{1}=1$ and $X_{3}=1$ , then $\\varphi_{\\alpha}$ implies $Y=1$ . It is easy to see that if $\\alpha$ does not contain either of these two events, then it must contain ( $(Y=0\\mid$ $X_{2}=0,X_{3}=1)\\wedge(X_{2}=0\\mid X_{1}=1)$ ) or $((Y=0\\mid X_{2}=1,X_{3}=1)\\land(X_{2}=1\\mid X_{1}=1)$ )), in which case $\\varphi_{\\alpha}$ does not imply $\\psi_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "Let $S_{c}$ consist of all ccces that contain these events, and let $\\varphi_{S_{c}}$ be the conjunction of the formulas corresponding to the events in $S_{c}$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varphi s_{c}=}&{\\bigg(\\underset{\\mathcal{T}^{*}\\in\\{1,\\dots,s_{i}\\}}{\\bigwedge}\\big[P a(Z_{i j^{\\prime}})\\gets c_{P a(Z_{i j^{\\prime}})}\\big]\\big(Z_{i j^{\\prime}}=z_{i j^{\\prime}}\\big)\\bigg)}\\\\ &{\\wedge\\bigg(\\underset{\\mathcal{T}^{*}\\in\\mathcal{Z}}{\\bigwedge}\\big[P a_{Z}\\big(Z\\big)\\gets c_{P a_{Z}(Z)}\\wedge P a_{Z}\\big(Z\\big)\\gets\\bar{z}_{P a_{Z}(Z)}\\big]\\big(Z=c_{Z}\\big)\\bigg)}\\\\ &{\\wedge\\underset{\\mathcal{T}^{*}\\in\\{1,\\dots,r_{i}\\}}{\\bigwedge}\\Bigg(\\underset{\\mathcal{S}^{*}\\in\\mathcal{T}_{i}^{*}}{\\bigwedge}\\big[P a(X)\\gets c_{P a(X)}^{j}\\big](X=c_{X}^{j})\\bigg)\\Bigg)}\\\\ {=}&{\\bigg(\\underset{\\mathcal{T}^{*}\\in\\{1,\\dots,s_{i}\\}}{\\bigwedge}\\big[P a\\big(Z_{i j^{\\prime}}\\big)\\gets c_{P a(Z_{i j^{\\prime}})}\\big]\\big(Z_{i j^{\\prime}}=z_{i j^{\\prime}}\\big)\\bigg)}\\\\ &{\\wedge\\bigg(\\underset{\\mathcal{S}^{*}\\in\\mathcal{Z}}{\\bigwedge}\\big[P a_{Z}\\big(Z\\big)\\gets c_{P a_{Z}(Z)}\\wedge P a_{Z}\\big(Z\\big)\\gets\\bar{z}_{P a_{Z}(Z)}\\big]\\big(Z=c_{Z}\\big)\\bigg)}\\\\ &{\\wedge\\bigg(\\underset{\\mathcal{S}^{*}\\in\\mathcal{Z}}{\\bigwedge}\\Bigg[\\underset{\\mathcal{S}\\in\\mathcal{Z}}{\\nabla}\\big(1\\!\\!-\\!e_{\\partial_{Z}(Z)}\\wedge P a_{Z}\\big)\\Big(Z\\!-\\!\\bar{z}_{P a(X)}\\big)\\big(X=c_{X}^{j}\\big)\\bigg)\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then by Theorem 3.6, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varphi_{S_{c}}\\Leftrightarrow\\bigvee_{\\alpha\\in S_{c}}\\varphi_{\\alpha}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the formulas $\\varphi_{\\alpha}$ for distinct ccces in $S_{c}$ are mutually exclusive, we have that $\\mathrm{Pr}_{M^{\\prime}}(\\psi_{i c})=$ $\\operatorname*{Pr}_{M^{\\prime}}\\left(\\varphi_{S}\\right)$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}_{M^{\\prime}}(\\psi_{i})}\\\\ &{=\\displaystyle\\sum_{c\\in\\mathcal{T}_{\\mathcal{Z}}}\\operatorname*{Pr}_{N^{\\prime}}(\\psi_{i c})}\\\\ &{=\\displaystyle\\sum_{\\substack{c\\in\\mathcal{T}_{\\mathcal{Z}},\\,\\,}}\\operatorname*{Pr}_{M^{\\prime}}\\left(\\bigwedge_{\\substack{j^{\\prime}\\in\\{1,\\dots,s_{i}\\}}}(Z_{i j^{\\prime}}=z_{i j^{\\prime}})\\wedge\\bigwedge_{\\substack{z\\in\\mathcal{Z}}}(Z=c_{Z})\\right.}\\\\ &{\\quad\\left.c^{j}\\in\\mathcal{T}_{j}^{\\prime}:j\\in\\{1,\\dots,r_{i}\\}\\right.\\qquad\\qquad\\qquad\\left.\\bigwedge_{\\substack{X\\in\\mathcal{X}_{l}^{\\prime},\\,l\\in\\{1,\\dots,r_{i}\\}}}[P a(X)\\leftarrow c_{P a(X)}^{l}](X=c_{X}^{l})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can further simplify this expression. Specifically, we can get rid of $[P a(X)\\leftarrow c_{P a(X)}^{j}](X=$ $c_{X}^{j})$ for all descendants $X$ of some $Y_{i j k}$ with $k\\in\\{1,\\ldots,t_{i j}\\}$ and $j\\,\\in\\,\\{1,\\ldots,s_{i}\\}$ . We leave the details to the reader. The expression above may be infeasible for some combinations of settings $c\\in{\\mathcal{T}}_{\\mathcal{Z}}$ and $c^{l}$ for all $l\\,\\in\\,\\{1,\\dots,r_{i}\\}$ . For example $[X\\,\\leftarrow\\,1](Y\\,=\\,0)\\land[X\\,\\leftarrow\\,1](Y\\,=\\,1)$ has zero probability. Furthermore, certain formulas in $\\land_{X\\in\\mathcal{X}_{l}^{\\prime},l\\in\\{1,\\dots,r_{i}\\}}[P a(X)\\leftarrow c_{P a(X)}^{l}](X=c_{X}^{l})$ may be duplicates, and some interventions may be redundant. We need to drop the duplicates and redundant interventions before further simplifying the expression. For ease of exposition, we assume that the expression is feasible, all conjuncts in $\\tilde{\\Lambda_{X\\in\\mathcal{X}_{l}^{\\prime},l\\in\\{1,...,r_{i}\\}}}[P a(X)\\gets c_{P a(X)}^{l}](X=c_{X}^{l})$ are distinct, and all interventions are non-redundant. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "If $M^{\\prime}$ is i-compatible with $M$ , then we can further conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}_{M^{\\prime}}\\{\\psi_{i}\\}}\\\\ &{=\\underset{\\textit{c}\\in\\mathcal{C}_{T_{j}^{\\prime},\\hat{\\jmath}\\in\\mathbb{E}_{1},\\ldots,r}^{\\sum}}{\\sum}\\left(\\operatorname*{Pr}_{M^{\\prime}}\\left(\\underset{\\textit{j}\\in\\mathcal{X}_{1},\\ldots,s_{i}}{\\bigwedge}(Z_{i^{j}}=z_{i^{j}})\\wedge\\underset{\\textit{2}\\in\\mathcal{Z}}{\\bigwedge}(Z=c_{2})\\right)\\right.}\\\\ &{\\quad\\quad\\left.\\sim^{j}\\epsilon\\frac{c(z_{i})}{\\epsilon_{i}T_{j}^{\\prime}\\sharp\\in\\{1,\\ldots,r\\}}\\times\\operatorname*{Pr}_{M^{\\prime}}\\left(\\underset{\\textit{x}\\in\\mathcal{X}_{1}^{\\prime}}{\\bigwedge}\\ \\big[P a(X)\\leftarrow c_{P_{a}(X)}^{j}\\big](X=c_{X}^{j})\\right)\\right)}\\\\ &{=\\underset{\\textit{c}\\in\\mathcal{C}_{T_{j}^{\\prime},\\hat{\\jmath}\\in\\mathbb{E}_{1},\\ldots,r}^{\\sum}}{\\sum}\\left(\\operatorname*{Pr}_{M^{\\prime}}\\left(\\underset{\\textit{j}\\in\\{1,\\ldots,s_{i}\\}}{\\bigwedge}\\ \\big(Z_{i^{j}}=z_{i^{j}}\\big)\\wedge\\underset{\\textit{2}\\in\\mathcal{Z}}{\\bigwedge}(Z=c_{2})\\right)\\right.}\\\\ &{\\quad\\quad\\quad\\left.\\underset{\\textit{L}\\in\\mathcal{X}_{1}^{\\prime}}{\\sum}\\ \\operatorname*{Pr}_{M^{\\prime}}(X=c_{X}^{j}\\ |\\ P a(X)\\leftarrow c_{P_{a}(X)}^{j})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\operatorname*{Pr}_{M^{\\prime}}(\\psi_{i})=\\operatorname*{Pr}_{M}(\\psi_{i})$ , we get the desired result. ", "page_idx": 16}, {"type": "text", "text": "In terms of complexity, each intervention $\\vec{X_{j}}\\gets\\vec{x_{j}}$ requires at most $2^{n}$ different settings in the set $\\mathcal{T}_{j}^{\\prime}$ . Therefore, the expression above for $\\operatorname*{Pr}_{M}(\\psi_{i})$ has $O(2^{n(r_{i}+1)})$ setting combinations in the summation and $O(n r_{i}{+}1)$ conditional probability calculations for each such setting. This shows that an arbitrary formula $\\psi$ can be evaluated in terms of $O(m(n r^{*}+1)2^{n(r^{*}+1)})$ conditional probability calculations, where $r^{*}$ is the maximum number of conjuncts in a disjunction $\\psi_{i}$ that involve at least one intervention, and $m$ is the number of disjuncts in the DNF. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have summarized the key claims in the introduction and abstract, provided relevant references wherever we referenced prior work. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Section 3.4 discusses the implications of our assumptions. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We have added all theorems and proofs (some of which are in the supplementary material). ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not have experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We do not have experimental results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 18}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not have experimental results. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not have experimental results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not have experiment results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have preserved anonymity. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not see any direct societal impact of the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: No such risks identified. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification:The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]