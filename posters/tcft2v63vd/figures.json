[{"figure_path": "Tcft2V63Vd/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of our motivation and the identified weight norm variance pattern. Fig. (a): For two channels with weight vectors of similar direction, the gradient of the former layer is proportional to the weight norm of the latter layer and vice versa. As empirical evidence indicated, the weight norm increases during training, and the weight norm disparity between channels increases. Fig. (b): The two different patterns we identified of how weight norm variance would change during training. For wide layers, the weight norm variance keeps increasing, which we call the increase to saturate pattern (IS). For narrow layers, the weight norm starts to decrease after reaching a high point, which we call the decrease to saturate pattern (DS).", "description": "This figure illustrates the Matthew effect between channels, showing how weight norm variance changes during training for wide and narrow layers.  (a) demonstrates the relationship between weight norm, gradients, and the increasing disparity between channels over time. (b) shows two distinct patterns: the increase-to-saturate (IS) pattern for wide layers and the decrease-to-saturate (DS) pattern for narrow layers, forming the basis for determining layer width sufficiency.", "section": "3 Different Patterns between Wide and Narrow Layers"}, {"figure_path": "Tcft2V63Vd/figures/figures_4_1.jpg", "caption": "Figure 2: Weight norm variance change during training of GCN [17] on cora [35] and GRU [6] on enwiki8 [22]. We change the width of the second layer in GCN and the input-hidden weight of the second layer in GRU. As the width increases, the weight norm variance during training shows two different patterns. For the wide layer, the weight norm variance increases and stays at a high level until convergence, which we call an increase to saturate (IS) pattern. For the narrow layer, the weight norm variance decreases after the initial increase, which we call the decrease to saturate (DS) pattern.", "description": "This figure shows the weight norm variance changes during the training process of GCN and GRU models with varying layer widths. It demonstrates two distinct patterns: the \"increase to saturate\" (IS) pattern for wide layers and the \"decrease to saturate\" (DS) pattern for narrow layers. The IS pattern indicates that the weight norm variance consistently increases and remains high, suggesting sufficient layer width. Conversely, the DS pattern shows an initial increase followed by a decrease in variance, implying that the layer could benefit from increased width.", "section": "3 Different Patterns between Wide and Narrow Layers"}, {"figure_path": "Tcft2V63Vd/figures/figures_4_2.jpg", "caption": "Figure 3: Weight norm variance between different channels of the MLP at the 5th layer of a tiny ViT trained on CIFAR-10. The left figure corresponds to the result where the MLP size is set as 512, and the right figure corresponds to the result where the MLP size is set as 32.", "description": "This figure shows the weight norm variance change during training for two different MLP sizes (32 and 512) within the 5th layer of a Tiny Vision Transformer (ViT) model trained on the CIFAR-10 dataset.  The left panel (MLP size=512) exhibits an \"increase to saturate\" (IS) pattern, where variance steadily increases and plateaus. The right panel (MLP size=32) shows a \"decrease to saturate\" (DS) pattern, with variance initially rising, then declining to a stable level. This illustrates the paper's core finding that wider layers demonstrate the IS pattern while narrower layers exhibit the DS pattern.", "section": "3 Different Patterns between Wide and Narrow Layers"}, {"figure_path": "Tcft2V63Vd/figures/figures_5_1.jpg", "caption": "Figure 4: The variance of weight norm ||w|| \\cdot ||w<sup>l+1</sup>|| at different layers of VGG-16 and ResNet18 trained on CIFAR-10. The y-axis corresponds to the variance while the x-axis corresponds to the epochs trained after initialization. From left to right is the result for the first layer, the layer in the middle, and the last layer. For more results please refer to Appendix A", "description": "The figure shows the variance of weight norm across different layers of VGG-16 and ResNet18 during training on CIFAR-10.  The y-axis represents the variance of weight norm, and the x-axis represents the number of training epochs.  Different layers exhibit distinct patterns in weight norm variance change over epochs. The patterns shown are intended to illustrate the difference in variance patterns between wide and narrow layers.  The patterns observed across the layers of VGG and ResNet architectures support the paper's claims regarding the relationship between layer width and weight norm variance.", "section": "4 Three training Stages from Random Initialization to Convergence"}, {"figure_path": "Tcft2V63Vd/figures/figures_5_2.jpg", "caption": "Figure 5: Density plots of the weight norm of different layers of a ResNet-18 trained on CIFAR10. The x-axis corresponds to the weight norm, and the y-axis corresponds to the density. For clarity, we omit the label for the y-axis. From top to bottom, each density plot corresponds to different epochs, we demonstrate how the weight norm distribution of different layers changes during training.", "description": "This figure shows the distribution of weight norms for different layers (1, 3, 7, 11, and 17) of a ResNet-18 model trained on CIFAR-10 at various epochs. Each row represents a different epoch, visualizing how the weight norm distribution changes across layers and over time during the training process.", "section": "4 Three training Stages from Random Initialization to Convergence"}, {"figure_path": "Tcft2V63Vd/figures/figures_5_3.jpg", "caption": "Figure 6: Ratio of weight elements where the sign is changed compared to the initialization during training different models on CIFAR-10. We show the result for the first 10 epochs. After the first 5 epochs, the sign of at least 10% weights changes, indicating a dramatic change in the weight space.", "description": "This figure shows the percentage of weight elements that changed sign during the first 10 epochs of training for four different CNN architectures (VGG11, VGG16, VGG19, and ResNet18) on the CIFAR-10 dataset.  It demonstrates that a significant portion of weights change sign early in training, highlighting the dynamic nature of the weight space during the initial learning phase. The rapid changes suggest a period of chaotic exploration before the network settles into a more stable solution.", "section": "4.1 Stage 1: Weight Variance Stay Low Shortly after Initialization"}, {"figure_path": "Tcft2V63Vd/figures/figures_6_1.jpg", "caption": "Figure 7: (a): The loss, accuracy, and weight norm variance between neurons at 5-th layer of a VGG16 trained on CIFAR-10. The model is trained for 150 epochs, after the second stage of training mentioned in Sec. 4.2 the model has achieved a high performance at around 20 epoch. For more details please refer to Appendix B. (b): We demonstrate the correlation between the weight norm at the 6-th epoch and the weight norm after the last epoch. The x-axis corresponds to the weight norm after the last epoch (150-th epoch). For the upper plot, the y-axis corresponds to the change of weight norm from 6-th epoch to 4-th epoch. For the lower plot, the y-axis corresponds to the weight norm at the 6-th plot. Each point represents a neuron at the first layer of VGG16.", "description": "This figure shows the training dynamics of a VGG16 network on the CIFAR-10 dataset.  Panel (a) displays the loss, accuracy, and weight norm variance of the 5th layer over 150 epochs.  It highlights the rapid performance increase during the second training stage (around epoch 20), correlating with a drastic increase in weight norm variance. Panel (b) demonstrates the strong correlation between weight norm at epoch 6 and the final weight norm, suggesting the early training stages are crucial in determining the final weight distribution.", "section": "4 Three training Stages from Random Initialization to Convergence"}, {"figure_path": "Tcft2V63Vd/figures/figures_7_1.jpg", "caption": "Figure 8: The cosine similarity between weight vectors at the same layer of a VGG16 trained on CIFAR-10. The value at i-th row and j-th column correspond to the cosine similarity between the weight vectors of the i-th neuron and the j-th neuron. Note that we sort the neurons by their weight norm in descending order which means the 0-th neuron is the neuron with the largest weight norm. The results show that weights for neurons at the 5-th layer of VGG16 are almost orthogonal.", "description": "This figure shows the cosine similarity between weight vectors of neurons in the same layer of a VGG16 network trained on CIFAR-10. Neurons are sorted by weight norm in descending order (highest weight norm at index 0). The heatmaps visualize the cosine similarity between neuron weight vectors.  The figure demonstrates how the similarity patterns differ across layers with different training behaviors (IS and DS patterns). The 5th layer, displaying near-orthogonality (values close to 0), is a representative example of the DS pattern; the first and 13th layers show different similarity structures.", "section": "Stage 3: Variance Decreases or Increases to Saturation at Different Layers"}, {"figure_path": "Tcft2V63Vd/figures/figures_12_1.jpg", "caption": "Figure 9: The weight norm variance between neurons at each layer of VGG16 during its training on CIFAR10.", "description": "This figure shows the weight norm variance between neurons at each layer of a VGG16 network trained on the CIFAR-10 dataset.  Each subplot represents a different layer of the network, and the x-axis shows the training epochs, while the y-axis displays the variance of the weight norms. The plots illustrate how the weight norm variance changes over the course of training for each layer.  Analyzing these plots can reveal insights into the training dynamics of each layer and how the width of the layers affects the variance.", "section": "A.1 The Weight Norm Variance of Each Layer During Training"}, {"figure_path": "Tcft2V63Vd/figures/figures_12_2.jpg", "caption": "Figure 5: Density plots of the weight norm of different layers of a ResNet-18 trained on CIFAR10. The x-axis corresponds to the weight norm, and the y-axis corresponds to the density. For clarity, we omit the label for the y-axis. From top to bottom, each density plot corresponds to different epochs, we demonstrate how the weight norm distribution of different layers changes during training.", "description": "This figure shows the distribution of weight norms for different layers (1, 3, 7, 11, 17) of a ResNet-18 model trained on the CIFAR-10 dataset at various training epochs.  The x-axis represents the weight norm, and the y-axis represents the density of neurons with that weight norm.  The figure illustrates how the weight norm distribution evolves across different layers and training stages.", "section": "4 Three training Stages from Random Initialization to Convergence"}, {"figure_path": "Tcft2V63Vd/figures/figures_13_1.jpg", "caption": "Figure 11: The weight norm distribution of each layer of VGG16 trained on CIFAR10. The number on the left indicates the epoch.", "description": "This figure visualizes the distribution of weight norms across different channels within each layer of a VGG16 convolutional neural network trained on the CIFAR-10 dataset.  Each subplot represents a different layer of the network. Within each subplot, there are multiple density plots, one for each epoch of training. The x-axis represents the weight norm, and the y-axis represents the density of channels with that particular weight norm.  The figure helps illustrate how the distribution of weight norms changes across layers and over the course of training.", "section": "A.2 The weight Norm Distribution of Each Layer"}, {"figure_path": "Tcft2V63Vd/figures/figures_13_2.jpg", "caption": "Figure 12: The cosine similarity of weight vectors corresponding to different neurons at each layer of VGG16 trained on CIFAR10.", "description": "This figure visualizes the cosine similarity between weight vectors for different neurons within the same layer of a VGG16 model trained on the CIFAR-10 dataset.  Each subplot represents a different layer (1-13). The color intensity in each heatmap indicates the cosine similarity; red represents high similarity, blue represents low similarity, and white represents near-zero similarity. This helps to show how similar the learned features within a layer are.  Wide layers might have more neurons with similar weight vectors (redder heatmaps), while narrow layers may show more scattered weight vectors (bluer heatmaps).", "section": "3 Different Patterns between Wide and Narrow Layers"}, {"figure_path": "Tcft2V63Vd/figures/figures_13_3.jpg", "caption": "Figure 13: The results of weight variance for ResNet20.", "description": "This figure shows the weight norm variance change during the training of ResNet20.  Each subplot corresponds to a different layer of the network (Layer 1, Layer 4, Layer 8, Layer 15, Layer 16). The x-axis represents the training epochs, and the y-axis represents the weight variance.  The plots illustrate the patterns of weight norm variance changes for different layers during training, showing whether each layer follows an increasing to saturate (IS) pattern or a decreasing to saturate (DS) pattern.", "section": "A.4 Results with small ResNet-20"}, {"figure_path": "Tcft2V63Vd/figures/figures_14_1.jpg", "caption": "Figure 13: The results of weight variance for ResNet20.", "description": "This figure shows the weight norm variance across different channels for each layer (1, 4, 8, 15, 16) of a ResNet20 model during training.  Each subplot shows how the variance changes over training epochs. This illustrates the 'decrease to saturate' (DS) and 'increase to saturate' (IS) patterns observed in narrow and wide layers, respectively. The patterns observed can help determine if a layer is appropriately sized or requires adjustment.", "section": "A.4 Results with small ResNet-20"}, {"figure_path": "Tcft2V63Vd/figures/figures_14_2.jpg", "caption": "Figure 13: The results of weight variance for ResNet20.", "description": "This figure shows the weight norm variance change during training for each layer of a ResNet-20 model.  Each subplot represents a layer of the network. The x-axis represents training epochs, and the y-axis represents the weight norm variance. The plot visualizes how the weight norm variance changes over the training process for each layer, illustrating the dynamics of weight distribution learning within the ResNet-20 architecture. This is particularly relevant to the paper's analysis of wide vs. narrow layers and their distinct training patterns.", "section": "A.4 Results with small ResNet-20"}]