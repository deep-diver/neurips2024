[{"figure_path": "Tcft2V63Vd/tables/tables_1_1.jpg", "caption": "Table 1: We list some different training dynamics phenomena.", "description": "This table summarizes various phenomena observed during the training of neural networks, including the critical period, the frequency principle, grokking, double descent, and the authors' findings on weight norm variance.  Each phenomenon is described, and the metric used to measure it is specified. This helps to contextualize the authors' work within the existing literature on neural network training dynamics.", "section": "2 Related Works"}, {"figure_path": "Tcft2V63Vd/tables/tables_7_1.jpg", "caption": "Table 2: We employ IFM [5] to merge the neurons (channels) across the layers of a VGG16 trained on CIFAR-10. We list the original width, the width after merging, and the weight norm variance pattern in the third stage.", "description": "This table presents the results of applying the IFM (Insufficient Feature Map) algorithm to merge neurons (channels) within layers of a VGG16 model trained on the CIFAR-10 dataset.  It compares the original width of each layer to the width after neuron merging using the IFM technique. The final column indicates whether each layer exhibits an \"IS\" (Increase to Saturate) or \"DS\" (Decrease to Saturate) pattern of weight norm variance during the third training stage.  The IS/DS pattern is an indicator of layer width sufficiency discussed in the paper.", "section": "4.3 Stage 3: Variance Decreases or Increases to Saturation at Different Layers"}, {"figure_path": "Tcft2V63Vd/tables/tables_8_1.jpg", "caption": "Table 3: In this table, we report the number of parameters, FLOPs, and the testing accuracy of the original VGG and ResNet models and our width-adjusted models. Each result is averaged over 10 runs with a different random seed. For a fair comparison, we make the FLOPs of the width-adjusted models close to the original model with nearly 40% parameters reduced.", "description": "This table presents a comparison of the original and width-adjusted versions of VGG16, VGG19, ResNet18, and ResNet50 models.  It shows the number of parameters, FLOPs (floating-point operations), and top-1 accuracy on CIFAR-10 and CIFAR-100 datasets. The width-adjusted models are designed to have similar FLOPs to the original models but with approximately 40% fewer parameters.  The results demonstrate the impact of the width adjustment on model efficiency and performance.", "section": "5 Width Modification on CNNs for Fewer Parameters and Better Performance"}, {"figure_path": "Tcft2V63Vd/tables/tables_8_2.jpg", "caption": "Table 4: The result of VGG16 and ResNet18 trained on Tiny-ImageNet. Each result is averaged over 10 runs. Each model is trained for 90 epochs. More details are in Appendix B.", "description": "This table shows the validation accuracy of original and streamline-width VGG16 and ResNet18 models trained on TinyImageNet dataset. The streamline width models have adjusted width across layers to reduce parameters while maintaining similar FLOPs to the original models. Each result represents an average of 10 independent runs, with each model trained for 90 epochs. More detailed information about training and model configurations is available in Appendix B.", "section": "5 Width Modification on CNNs for Fewer Parameters and Better Performance"}, {"figure_path": "Tcft2V63Vd/tables/tables_14_1.jpg", "caption": "Table 3: In this table, we report the number of parameters, FLOPs, and the testing accuracy of the original VGG and ResNet models and our width-adjusted models. Each result is averaged over 10 runs with a different random seed. For a fair comparison, we make the FLOPs of the width-adjusted models close to the original model with nearly 40% parameters reduced.", "description": "This table presents a comparison of the original VGG and ResNet models with the proposed width-adjusted models.  It shows the number of parameters, FLOPs (floating-point operations), and top-1 accuracy for CIFAR-10 and CIFAR-100 datasets.  The width-adjusted models aim to reduce parameters while maintaining similar FLOPs to the originals.  The results are averaged over 10 runs, each with a different random seed, ensuring robustness.  The goal is to show that the adjusted models improve efficiency (fewer parameters) without significant performance loss.", "section": "5 Width Modification on CNNs for Fewer Parameters and Better Performance"}]