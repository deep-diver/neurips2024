[{"figure_path": "nw6ANsC66G/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy (%) achieved on the CIFAR-10 dataset by PFPT and other baselines.", "description": "This table presents the classification accuracy achieved by different federated prompt-tuning methods on the CIFAR-10 dataset under various data distribution settings.  The methods compared include FEDAVG-PT, FEDPROX-PT, SCAFFOLD-PT, FEDOPT-PT, PFEDPG, GMM-PT, and the proposed PFPT.  The results are shown for three scenarios: data with Dirichlet distribution (\u03b1 = 0.5 and \u03b1 = 0.1) and imbalanced data.  The table highlights the superior performance of the proposed PFPT method across all settings.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_7_2.jpg", "caption": "Table 2: Accuracy (%) achieved on the CIFAR-100 dataset by PFPT and other baselines.", "description": "This table presents the classification accuracy on CIFAR-100 dataset for different algorithms under three scenarios:  data partition with \u03b1 = 0.5, data partition with \u03b1 = 0.1, and imbalanced data partition.  The algorithms compared include FEDAVG-PT, FEDPROX-PT, SCAFFOLD-PT, FEDOPT-PT, PFEDPG-PT, GMM-PT, and the proposed PFPT. The results show the effectiveness of PFPT across different data heterogeneity levels.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_7_3.jpg", "caption": "Table 3: Accuracy (%) achieved on the TinyImageNet dataset by PFPT and other baselines.", "description": "This table presents the classification accuracy achieved by the proposed Probabilistic Federated Prompt Tuning (PFPT) method and several baseline methods on the TinyImageNet dataset under three different data partition scenarios:  a setting with a Dirichlet distribution parameter \u03b1 = 0.5, another with \u03b1 = 0.1, and a highly imbalanced data setting. The results show the average accuracy and standard deviation over five independent runs. The table compares PFPT's performance against baselines such as FEDAVG-PT, FEDPROX-PT, SCAFFOLD-PT, FEDOPT-PT, PFEDPG, and GMM-PT.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_8_1.jpg", "caption": "Table 4: Accuracy (%) achieved on the synthetic 4-dataset, which combines data from MNIST-M [53], Fashion-MNIST [54], CINIC-10 [55], and MMAFEDB\u00b9, by PFPT and other baselines.", "description": "This table presents the classification accuracy achieved by different federated prompt-tuning methods on a synthetic dataset combining MNIST-M, Fashion-MNIST, CINIC-10, and MMAFEDB.  The results are shown for three different data distributions:  Dirichlet distribution with \u03b1 = 0.5, Dirichlet distribution with \u03b1 = 0.1, and an imbalanced distribution. The table compares the performance of PFPT (the proposed method) against several baseline methods, including FEDAVG-PT, FEDPROX-PT, SCAFFOLD-PT, FEDOPT-PT, PFEDPG, and GMM-PT.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_9_1.jpg", "caption": "Table 5: Accuracy (%) achieved on long-tailed datasets by PFPT, CREFF [58] and FEDIC [57].", "description": "This table shows the accuracy achieved by PFPT and two other state-of-the-art methods (CREFF and FEDIC) on long-tailed versions of CIFAR-100 and ImageNet datasets.  The long-tailed datasets are created using an imbalance factor (IF) which controls the skew in the class distribution.  The results demonstrate PFPT's superiority in handling imbalanced data, especially with high IF values, highlighting its robustness and effectiveness compared to existing approaches.", "section": "4.2 Non-IID Data with Globally Skewed Class Distributions"}, {"figure_path": "nw6ANsC66G/tables/tables_17_1.jpg", "caption": "Table 6: Accuracy (%) achieved on our synthetic 4-dataset by our proposed PFPT algorithm and other baselines improved with a similar client-specific prompt selection mechanism. The symbol + indicates the improved variant of the original baseline.", "description": "This table compares the performance of the proposed PFPT algorithm against other baseline algorithms on a synthetic 4-dataset.  The baselines have been improved by incorporating a client-specific prompt selection mechanism. The table shows the accuracy achieved under different data distribution scenarios (\u03b1 = 0.5, \u03b1 = 0.1, and imbalance).  The results demonstrate the superior performance of PFPT across all scenarios.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_17_2.jpg", "caption": "Table 7: Accuracy (%) achieved on the TinyImageNet dataset by our proposed PFPT algorithm and other baselines improved with a similar client-specific prompt selection mechanism. The symbol + indicates the improved variant of the original baseline.", "description": "This table shows the accuracy achieved by different federated prompt tuning methods on the TinyImageNet dataset.  The baselines (FEDAVG-PT, FEDPROX-PT, FEDOPT-PT, GMM-PT) have been improved by incorporating a client-specific prompt selection mechanism. The results demonstrate the superior performance of the proposed PFPT (Probabilistic Federated Prompt Tuning) algorithm across different data heterogeneity levels (\u03b1 = 0.5, \u03b1 = 0.1, and imbalanced).", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_17_3.jpg", "caption": "Table 8: Accuracy (%) achieved on the all dataset by FEDAVG with Adapter-Tuning, FEDPROX with Adapter-Tuning, and our proposed PFPT algorithm.", "description": "This table compares the performance of PFPT against two other federated learning methods (FEDAVG and FEDPROX) that incorporate adapter-tuning. The results are reported for three different data settings (\u03b1 = 0.5, \u03b1 = 0.1, and imbalance) and four datasets (CIFAR10, CIFAR100, TinyImageNet, and a synthetic 4-dataset).  The table shows that PFPT consistently outperforms the adapter-tuning methods across all settings and datasets, demonstrating its effectiveness in handling non-IID data and data imbalance.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy (%) achieved on the CIFAR-10 dataset by PFPT and other baselines.", "description": "This table shows the accuracy achieved on the CIFAR-10 dataset using different federated prompt-tuning methods, including the proposed PFPT method and several baselines under different data heterogeneity settings (controlled by the Dirichlet parameter \u03b1 and data imbalance). The results highlight the superior performance of PFPT, particularly when dealing with extreme data heterogeneity.", "section": "4 Empirical Results"}]