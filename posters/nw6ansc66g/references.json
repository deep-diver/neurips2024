{"references": [{"fullname_first_author": "Jakub Kone\u010dn\u00fd", "paper_title": "Federated Optimization: Distributed Machine Learning for On-Device Intelligence", "publication_date": "2016-10-27", "reason": "This paper is foundational to the field of federated learning, introducing the core concepts and challenges of collaborative training across decentralized systems."}, {"fullname_first_author": "H. Brendan McMahan", "paper_title": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "publication_date": "2017-01-01", "reason": "This paper presents the FedAvg algorithm, a cornerstone of federated learning, which enables efficient model aggregation across multiple clients with limited communication bandwidth."}, {"fullname_first_author": "Peter Kairouz", "paper_title": "Advances and Open Problems in Federated Learning", "publication_date": "2019-12-02", "reason": "This survey paper provides a comprehensive overview of the federated learning landscape, highlighting key challenges and promising future directions."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "This paper introduces BERT, a foundational model in natural language processing, upon which many subsequent prompt-tuning methods rely."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2020-10-21", "reason": "This paper introduces the Vision Transformer (ViT), a powerful architecture for image recognition that is used in the target paper's experiments, demonstrating the effectiveness of prompt tuning in computer vision."}]}