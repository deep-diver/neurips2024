[{"figure_path": "nw6ANsC66G/figures/figures_1_1.jpg", "caption": "Figure 1: Test Accuracy (%) achieved on the CIFAR-10 dataset by solving Eq. (3) via centralizing data, using FEDAVG, and using FEDPROX on (orange) full-model (FM) and (blue) prompt-tuning (PT) setups. The evaluation is performed under (left) a standard (non-extreme) heterogeneous data partition; and (right) an extremely imbalanced data partitioning scheme (see Section 4).", "description": "This figure presents the test accuracy achieved on the CIFAR-10 dataset using three different methods: centralized training, FEDAVG, and FEDPROX.  Two training approaches are compared: full-model tuning and prompt tuning. The left panel shows results under a standard heterogeneous data partition, while the right panel shows results under an extremely imbalanced data partition. The figure highlights the significant performance drop of full-model tuning in the imbalanced setting, underscoring its instability in extreme data scenarios, while prompt tuning shows more resilience.", "section": "1 Introduction"}, {"figure_path": "nw6ANsC66G/figures/figures_3_1.jpg", "caption": "Figure 2: Workflow of Probabilistic Federated Prompt Aggregation: (left) each client selects a subset of prompts from the global set of summarizing prompts using the prompt-selection mechanism adapted from [49], and fine-tune them using local data; and (right) the server collects all local prompt sets and updates the global summarizing prompts that aggregate similar local prompts. This is achieved by our proposed probabilistic federated prompt aggregation (PFPT) algorithm.", "description": "This figure illustrates the workflow of the Probabilistic Federated Prompt Aggregation (PFPT) algorithm. The left side shows the local phase where each client selects a subset of prompts from a global set, fine-tunes them using local data, and then sends the updated prompts to the server.  The right side shows the global phase where the server aggregates the local prompt sets and updates the global summarizing prompts by using the PFPT algorithm, which then broadcasts the updated prompts back to the clients. This iterative process helps to combat data heterogeneity in federated learning.", "section": "3 Probabilistic Federated Prompt-Tuning"}, {"figure_path": "nw6ANsC66G/figures/figures_8_1.jpg", "caption": "Figure 3: t-SNE plots of the (learned) summarizing prompts of PFPT on CIFAR-100 over 120 communication iterations with different heterogeneity settings. Yellow triangles denote the centroids of the t-SNE embeddings of the prompts. The dashed red line visualizes their trajectories.", "description": "This figure visualizes the learned summarizing prompts' movement over 120 communication rounds using t-SNE for dimensionality reduction.  The plots show the trajectories of the prompts for three different data heterogeneity scenarios (\u03b1 = 0.5, \u03b1 = 0.1, and imbalanced data), as well as a centralized learning scenario (for comparison). The yellow triangles indicate the centroids of the prompts at various stages, and the dashed red lines trace their movement over time. The plots illustrate how the prompt distribution evolves as the model trains under different data conditions, offering insights into the prompt convergence and diversity in federated learning.", "section": "3.2 Probabilistic Prompt Model"}, {"figure_path": "nw6ANsC66G/figures/figures_8_2.jpg", "caption": "Figure 4: Variations in CIFAR-100 global prompt pool size across 120 communication rounds under different heterogeneity settings.", "description": "The figure shows the change in the number of global prompts used in CIFAR-100 experiments across 120 communication rounds under three different data heterogeneity settings (\u03b1 = 0.5, \u03b1 = 0.1, and imbalanced data).  The shaded area represents the variability in the number of prompts. The plot demonstrates how the prompt pool size evolves over the course of federated learning, offering insights into the algorithm's adaptation to varying data heterogeneity levels. The plot shows that with more data heterogeneity (lower \u03b1 value), the prompt pool shrinks slower, which suggests that the heterogeneity requires more prompts to characterize the data.", "section": "4 Empirical Results"}, {"figure_path": "nw6ANsC66G/figures/figures_16_1.jpg", "caption": "Figure 3: t-SNE plots of the (learned) summarizing prompts of PFPT on CIFAR-100 over 120 communication iterations with different heterogeneity settings. Yellow triangles denote the centroids of the t-SNE embeddings of the prompts. The dashed red line visualizes their trajectories.", "description": "This figure visualizes the learned summarizing prompts' movement over 120 communication rounds using t-SNE for dimensionality reduction. Three different data heterogeneity settings are shown, along with their centroids and trajectories. The plot helps understand how prompts evolve and diversify during training across various heterogeneity levels.", "section": "Prompt Convergence and Diversity"}]