[{"figure_path": "HkMCCFrYkT/figures/figures_0_1.jpg", "caption": "Figure 1: HDR-GS vs. HDR-NeRF. Our HDR-GS achieves better PSNR in dB, SSIM, and LPIPS performance with shorter training time in minutes and faster inference speed in fps.", "description": "This figure compares the performance of HDR-GS and HDR-NeRF across multiple metrics: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), LPIPS (Learned Perceptual Image Patch Similarity), training time, and inference speed.  HDR-GS shows significant improvements in all metrics compared to HDR-NeRF.  Specifically, HDR-GS achieves a higher PSNR, SSIM, and lower LPIPS, indicating better image quality.  Furthermore, HDR-GS boasts much faster inference speed (frames per second) and shorter training time (minutes), making it significantly more efficient.", "section": "1 Introduction"}, {"figure_path": "HkMCCFrYkT/figures/figures_1_1.jpg", "caption": "Figure 2: Comparisons of point clouds (left) and rendered views (right) between the original 3DGS [15] (top) and our HDR-GS (bottom). (i) 3DGS [15] renders blurry LDR views when training with images under different exposures. Its point clouds suffer from severe color distortion and can not accurately represent the scene. In addition, 3DGS cannot control the exposure of the rendered images. (ii) Our HDR-GS can not only reconstruct clear HDR images with 3D consistency but also render LDR views with controllable exposure time At.", "description": "This figure compares the point clouds and rendered views of 3DGS and HDR-GS.  The top row shows the results from the original 3DGS method, illustrating blurry LDR views and color distortions in the point cloud, stemming from training on images with varying exposures.  The inability of 3DGS to control exposure is also highlighted. The bottom row presents the HDR-GS results, demonstrating clear HDR images with 3D consistency and the ability to render LDR views with user-specified exposure times.", "section": "Introduction"}, {"figure_path": "HkMCCFrYkT/figures/figures_3_1.jpg", "caption": "Figure 3: Pipeline of our method. (a) SfM [17] algorithm is used to recalibrate camera parameters and initialize 3D Gaussians. (b) Dual Dynamic Range Gaussian point clouds use spherical harmonics to model the HDR color. Three MLPs are employed to tone-map the LDR color from the HDR color and user input exposure time. (c) The HDR and LDR colors are fed into two Parallel Differentiable Rasterization to render the HDR and LDR views.", "description": "This figure illustrates the pipeline of the HDR-GS method.  It shows three stages: (a) recalibration and initialization using SfM to obtain camera parameters and initialize 3D Gaussian point clouds; (b) a dual dynamic range Gaussian point cloud model that uses spherical harmonics for HDR color and MLPs for tone mapping LDR colors from HDR colors with a controllable exposure time; and (c) parallel differentiable rasterization processes for rendering both HDR and LDR views from the model.", "section": "3 Method"}, {"figure_path": "HkMCCFrYkT/figures/figures_6_1.jpg", "caption": "Figure 4: LDR visual comparisons on the synthetic scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.", "description": "This figure compares the LDR visual results of different novel view synthesis methods (NeRF, 3DGS, NeRF-W, HDR-NeRF, and HDR-GS) on two synthetic scenes (\"dog\" and \"sofa\") with two different exposure times (\u0394t = 8s and \u0394t = 0.5s).  The figure highlights how HDR-GS (the authors' method) produces significantly clearer images with better exposure control and detail preservation, compared to existing methods that suffer from artifacts like blurry regions, black spots, and inaccurate exposure rendering.  The ground truth images are also shown for comparison.", "section": "4.2 Quantitative Results"}, {"figure_path": "HkMCCFrYkT/figures/figures_7_1.jpg", "caption": "Figure 5: LDR visual comparisons on the real scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.", "description": "This figure compares the LDR visual results of different novel view synthesis methods (NeRF, 3DGS, NeRF-W, HDR-NeRF, and HDR-GS) on real-world scenes with two different exposure times (\u0394t = 0.17s and \u0394t = 0.1s).  It showcases how previous methods like NeRF, 3DGS, and NeRF-W often introduce artifacts such as black spots or blurry images, especially when dealing with challenging lighting conditions. In contrast, HDR-NeRF shows improvement but still suffers from blurriness in some areas. The proposed HDR-GS method demonstrates significantly better performance, producing cleaner and more detailed images with better exposure control, closely resembling the ground truth images.", "section": "4.3 Qualitative Results"}, {"figure_path": "HkMCCFrYkT/figures/figures_8_1.jpg", "caption": "Figure 4: LDR visual comparisons on the synthetic scenes. Previous methods introduce unpleasant black spots or render blurry images. Our method controls the exposure better while reconstructing more detailed structures.", "description": "This figure compares the LDR visual results of different novel view synthesis methods on synthetic scenes.  The top row shows the ground truth images.  The middle rows show the results from HDR-NeRF and the proposed HDR-GS method. The bottom row shows zoomed-in regions highlighting the differences in detail and clarity.  HDR-NeRF produces some black spots and blurry areas, while HDR-GS is able to reconstruct clearer images and better control the exposure.", "section": "4.2 Quantitative Results"}]