[{"figure_path": "HkMCCFrYkT/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative results on the synthetic datasets. Metrics are averaged over all scenes. LDR-OE denotes the LDR results with exposure time t1, t3, and t5. LDR-NE denotes the LDR results with exposure time t2 and t4. HDR denotes the HDR results. HDR-GS yields the best results on all tracks.", "description": "This table presents a quantitative comparison of different novel view synthesis (NVS) methods on synthetic datasets.  Metrics include training time, inference speed (frames per second), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  The results are broken down into Low Dynamic Range (LDR) results obtained with exposure times t1, t3, t5 (LDR-OE) and t2, t4 (LDR-NE), and High Dynamic Range (HDR) results.  The table highlights the superior performance of HDR-GS across all metrics.", "section": "4.2 Quantitative Results"}, {"figure_path": "HkMCCFrYkT/tables/tables_7_1.jpg", "caption": "Table 2: Quantitative results on the real datasets. Metrics are averaged across all scenes. LDR-OE represents the LDR results with exposure time t1, t3, and t5. LDR-NE denotes the LDR results with exposure time t2 and t4. HDR refers to the HDR results. HDR-GS yields the best results on all tracks.", "description": "This table presents a quantitative comparison of different novel view synthesis (NVS) methods on real-world datasets.  It shows the performance of several methods, including NeRF, 3DGS, NeRF-W, and HDR-NeRF, against the proposed HDR-GS method. The metrics used for comparison are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity).  The table is divided into two main sections: LDR-OE (Low Dynamic Range - Optimized Exposure), which uses images with exposure times t1, t3, and t5; and LDR-NE (Low Dynamic Range - Non-Optimized Exposure), which uses images with exposure times t2 and t4.  HDR-GS demonstrates superior performance across all metrics and exposure scenarios.", "section": "4.2 Quantitative Results"}, {"figure_path": "HkMCCFrYkT/tables/tables_8_1.jpg", "caption": "Table 1: Quantitative results on the synthetic datasets. Metrics are averaged over all scenes. LDR-OE denotes the LDR results with exposure time t1, t3, and t5. LDR-NE denotes the LDR results with exposure time t2 and t4. HDR denotes the HDR results. HDR-GS yields the best results on all tracks.", "description": "This table presents a quantitative comparison of the proposed HDR-GS model against other state-of-the-art models on synthetic datasets.  It evaluates performance using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), training time (in minutes), and inference speed (frames per second).  The results are broken down for Low Dynamic Range (LDR) images using exposure times t1, t3, t5 (LDR-OE) and exposure times t2, t4 (LDR-NE), and for High Dynamic Range (HDR) images.  HDR-GS demonstrates superior results across all metrics.", "section": "4.2 Quantitative Results"}]