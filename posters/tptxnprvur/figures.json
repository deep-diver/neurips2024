[{"figure_path": "TPtXnpRvur/figures/figures_1_1.jpg", "caption": "Figure 1: Performance and efficiency comparison among SD-based Real-ISR methods. (a). Performance comparison on the DrealSR benchmark [51]. Metrics like LPIPS and NIQE, where smaller scores indicate better image quality, are inverted and normalized for display. OSEDiff achieves leading scores on most metrics with only one diffusion step. (b). Model efficiency comparison. The inference time is tested on an A100 GPU with 512 \u00d7 512 input image size. OSEDiff has the fewest trainable parameters and is over 100 times faster than StableSR [42].", "description": "This figure compares the performance and efficiency of OSEDiff against other state-of-the-art real-world image super-resolution (Real-ISR) methods based on Stable Diffusion (SD).  The left panel (a) shows a radar chart comparing various image quality metrics, demonstrating that OSEDiff achieves top performance using only one diffusion step, unlike other methods needing dozens or hundreds of steps. The right panel (b) presents a comparison in terms of model size, trainable parameters, and inference time, showing that OSEDiff is substantially more efficient than other methods.", "section": "1 Introduction"}, {"figure_path": "TPtXnpRvur/figures/figures_3_1.jpg", "caption": "Figure 2: The training framework of OSEDiff. The LQ image is passed through a trainable encoder Ee, a LORA finetuned diffusion network e\u03b8 and a frozen decoder De to obtain the desired HQ image. In addition, text prompts are extracted from the LQ image and input to the diffusion network to stimulate its generation capacity. Meanwhile, the output of the diffusion network e\u03b8 will be sent to two regularizer networks (a frozen pre-trained one and a fine-tuned one), where variational score distillation is performed in latent space to ensure that the output of e\u03b8 follows HQ natural image distribution. The regularization loss will be back-propagated to update Ee and e\u03b8. Once training is finished, only Ee, e\u03b8 and De will be used in inference.", "description": "This figure illustrates the training framework of the OSEDiff model. It shows how the low-quality (LQ) image is processed through an encoder, a finetuned diffusion network, and a decoder to generate a high-quality (HQ) image.  The process includes text prompt extraction, variational score distillation (VSD) in the latent space, and the use of both a frozen pre-trained regularizer and a fine-tuned regularizer. The diagram clearly shows the flow of data and gradients throughout the network, explaining the key components of the OSEDiff model's training process.", "section": "3.2 One-Step Effective Diffusion Network"}, {"figure_path": "TPtXnpRvur/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparisons of different Real-ISR methods. Please zoom in for a better view.", "description": "This figure presents visual comparisons of different Real-ISR methods on two real-world image examples (a face and a leaf).  It showcases the differences in detail preservation and realism between the various methods, highlighting OSEDiff's ability to reproduce high-quality and realistic details even with only one diffusion step.  The figure emphasizes OSEDiff's competitive results against multi-step methods such as StableSR, DiffBIR, SeeSR, PASD, ResShift, and SinSR.", "section": "4.2 Comparison with State-of-the-Arts"}, {"figure_path": "TPtXnpRvur/figures/figures_9_1.jpg", "caption": "Figure 4: The impact of different prompt extraction methods. Please zoom in for a better view.", "description": "This figure compares the results of using three different text prompt extraction methods with the OSEDiff model on a real-world LQ image. The first method uses no text prompt, the second method uses DAPE (degradation-aware prompt extraction), and the third method uses LLaVA-v1.5. The image shows that using a text prompt significantly improves the quality of the generated HQ images, especially using LLaVA-v1.5 that provides a more detailed description of the image resulting in a more rich and detailed output.", "section": "4.3 Ablation Study"}, {"figure_path": "TPtXnpRvur/figures/figures_16_1.jpg", "caption": "Figure 5: Qualitative comparisons between OSEDiff and GAN-based Real-ISR methods. Please zoom in for a better view.", "description": "This figure presents a qualitative comparison of image super-resolution results generated by OSEDiff and four other GAN-based methods (BSRGAN, Real-ESRGAN, LDL, and FeMaSR).  Each row shows a zoomed-in low-quality (LQ) image and the corresponding high-quality (HQ) images produced by each method.  The purpose is to visually demonstrate that OSEDiff produces images with finer details and better visual quality compared to these GAN-based methods.", "section": "4 Experiments"}, {"figure_path": "TPtXnpRvur/figures/figures_16_2.jpg", "caption": "Figure 1: Performance and efficiency comparison among SD-based Real-ISR methods. (a). Performance comparison on the DrealSR benchmark [51]. Metrics like LPIPS and NIQE, where smaller scores indicate better image quality, are inverted and normalized for display. OSEDiff achieves leading scores on most metrics with only one diffusion step. (b). Model efficiency comparison. The inference time is tested on an A100 GPU with 512 \u00d7 512 input image size. OSEDiff has the fewest trainable parameters and is over 100 times faster than StableSR [42].", "description": "This figure compares OSEDiff against other state-of-the-art SD-based Real-ISR methods in terms of both performance and efficiency.  The left panel (a) shows that OSEDiff achieves the best performance across most metrics on the DrealSR benchmark, using only a single diffusion step. The right panel (b) illustrates OSEDiff's significant efficiency advantage, exhibiting the fewest trainable parameters and an over 100x speed improvement compared to StableSR.", "section": "1 Introduction"}, {"figure_path": "TPtXnpRvur/figures/figures_17_1.jpg", "caption": "Figure 1: Performance and efficiency comparison among SD-based Real-ISR methods. (a). Performance comparison on the DrealSR benchmark [51]. Metrics like LPIPS and NIQE, where smaller scores indicate better image quality, are inverted and normalized for display. OSEDiff achieves leading scores on most metrics with only one diffusion step. (b). Model efficiency comparison. The inference time is tested on an A100 GPU with 512 \u00d7 512 input image size. OSEDiff has the fewest trainable parameters and is over 100 times faster than StableSR [42].", "description": "This figure presents a comparison of the performance and efficiency of OSEDiff against other state-of-the-art SD-based Real-ISR methods.  Subfigure (a) shows a performance comparison using the DrealSR benchmark, highlighting OSEDiff's superior performance across multiple metrics, achieved with only one diffusion step. Subfigure (b) compares the efficiency of the models, demonstrating that OSEDiff is significantly faster and requires fewer trainable parameters than competing methods.", "section": "1 Introduction"}]