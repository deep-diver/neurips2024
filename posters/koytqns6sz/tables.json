[{"figure_path": "KoyTqNs6SZ/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of discretization (approximation) schemes of prior work and our methods under various assumptions. All methods achieve a O(\u20ac) additive approximation to any pricing curve. Here, M means Monotonicity, F means that there are a Finite (m) number of types, S means that the valuation curves satisfy a L-Lipschitz-like Smoothness condition (Assumption 1), and D means that they satisfy a Diminishing returns condition (Assumption 2). The O notation suppresses log dependencies when there is already a polynomial dependence on a parameter. Prior work has exponential dependence in either N or \u20ac\u00af\u00b9. We wish to do better since (i) typically, the number of data N is very large and (ii) we need \u20ac \u2192 0 as T\u2192 \u221e to achieve sublinear regret.", "description": "This table compares different discretization schemes for revenue-optimal data pricing, highlighting the assumptions made (monotonicity, finite types, smoothness, diminishing returns), the size of the discretization, and the relevant theorem or reference.  It shows that the proposed methods have significantly better dependence on the approximation parameter (epsilon) and the number of data points (N) compared to prior work.", "section": "1. Discretization (approximation) schemes for revenue-optimal data pricing"}, {"figure_path": "KoyTqNs6SZ/tables/tables_3_1.jpg", "caption": "Table 2: Comparison of regret and time complexity of our online learning methods when paired with our discretization schemes and schemes from prior work. See Table 1 for a description of the assumptions. All methods, including [14, 25] achieve \u00d5(m\u221aT) regret in the stochastic setting.", "description": "This table compares the regret bounds and computational complexity per iteration of online learning algorithms for data pricing in both stochastic and adversarial settings.  It shows the performance of the authors' algorithms using different discretization schemes under various assumptions (monotonicity, finite types, smoothness, diminishing returns).  The results highlight the trade-off between regret and computational cost depending on the setting and assumptions.", "section": "Online learning in the stochastic setting"}, {"figure_path": "KoyTqNs6SZ/tables/tables_30_1.jpg", "caption": "Table 1: Comparison of discretization (approximation) schemes of prior work and our methods under various assumptions. All methods achieve a O(\u20ac) additive approximation to any pricing curve. Here, M means Monotonicity, F means that there are a Finite (m) number of types, S means that the valuation curves satisfy a L-Lipschitz-like Smoothness condition (Assumption 1), and D means that they satisfy a Diminishing returns condition (Assumption 2). The O notation suppresses log dependencies when there is already a polynomial dependence on a parameter. Prior work has exponential dependence in either N or \u20ac\u00af\u00b9. We wish to do better since (i) typically, the number of data N is very large and (ii) we need \u20ac \u2192 0 as T\u2192 \u221e to achieve sublinear regret.", "description": "This table compares different discretization schemes for approximating revenue-optimal pricing curves, highlighting the trade-offs between the size of the discretization and the assumptions made about the valuation curves (monotonicity, finite types, smoothness, diminishing returns). It shows that the proposed methods achieve better scaling with the approximation parameter compared to prior work, which is crucial for online learning settings where the parameter needs to approach 0 as the time horizon increases.", "section": "Discretization (approximation) schemes for revenue-optimal data pricing"}]