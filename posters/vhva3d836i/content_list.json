[{"type": "text", "text": "WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haipeng Luo1\u2217 Qingfeng Sun2\u2217 Can $\\mathbf{X}\\mathbf{u}^{2\\dagger}$ Pu Zhao2 ", "page_idx": 0}, {"type": "text", "text": "Qingwei Lin2 Jianguang Lou2 Shifeng Chen3 \u2020 Yansong Tang1 \u2020 Weizhu Chen2 ", "page_idx": 0}, {"type": "text", "text": "1Shenzhen International Graduate School, Tsinghua University 2Microsoft Corporation 3Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences {luohp24@mails., tang.yansong@sz.}tsinghua.edu.cn, {shifeng.chen}@siat.ac.cn {qins,caxu,puzhao,qlin,jlou,wzchen}@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent work demonstrates that, post-training large language models with opendomain instruction following data have achieved colossal success. Simultaneously, human Chatbot Arena has emerged as one of the most reasonable benchmarks for model evaluation and developmental guidance. However, the processes of manually curating high-quality training data and utilizing online human evaluation platforms are both expensive and limited. To mitigate the manual and temporal costs associated with post-training, this paper introduces a Simulated Chatbot Arena named WizardArena, which is fully based on and powered by open-source LLMs. For evaluation scenario, WizardArena can efficiently predict accurate performance rankings among different models based on offline test set. For the training scenario, we propose Arena Learning, an innovative offilne strategy that simulates iterative arena battles among various state-of-the-art models on a large scale of instruction data using AI-driven annotations to evaluate and leverage battle results, thus continuously enhancing the weaknesses of the target model through both supervised fine-tuning and reinforcement learning. Experimental results demonstrate that our WizardArena aligns closely with the online human arena rankings, and our models, trained on extensive offline battle data through Arena Learning, demonstrate marked improvements in performance across the SFT, DPO, and PPO stages. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, the field of natural language processing (NLP) has witnessed a remarkable transformation, driven by the rapid advancements in large language models (LLMs). These models, trained on vast amounts of text data, have demonstrated an exceptional ability to understand, generate, and interact with human language in a wide range of tasks [1\u20133]. One of the most exciting applications of LLMs has been in the realm of conversational AI [4\u20139], where they have been utilized to create powerful chatbots capable of engaging in naturalistic dialogues. One of the key factors contributing to the success of LLM-powered chatbots is the ability to leverage large-scale high-quality instruction following data for effective post-training [10\u201314]. By exposing these models to a diverse range of conversational tasks and instructional scenarios, researchers have been able to imbue them with a deep understanding of how to effectively communicate and assist humans. ", "page_idx": 0}, {"type": "image", "img_path": "VHva3d836i/tmp/01802d2032f1b20dc02403521050304912f1f4a2061592c944b8305112f37eb6.jpg", "img_caption": ["Figure 1: Overview of Running Example. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this context, the emergence of the Chatbot Arena [15, 16] has been a significant development. This is a platform that facilitates the assessment and comparison of different chatbot models by pitting them against each other in a series of conversational challenges and rank with Elo rating system [17]. By leveraging a diverse set of human evaluators, the Chatbot Arena provides a more robust and comprehensive evaluation of chatbot performance, going beyond the limitations of traditional benchmarking approaches. At the same time, they also opened up some real direct chat and battle preferences data [18], which have been proven to be valuable resources for model post-training and developmental guidance [19]. However, the manual nature of the human-based evaluation process poses its own set of challenges: Manually orchestrating and coordinating the interactions between chatbots and human evaluators can be time-consuming and resource-intensive, limiting the scale and frequency of evaluation and training data opensource cycles. On the other hand, due to their priority limitations [20], most models are unable to participate in arena evaluations, making it impossible to directly guide the development of the target model on it. So, the need for a more efficient and scalable arena-based pipeline to chatbot post-training and evaluation has become increasingly pressing. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, this paper introduces a novel approach called WizardArena, a simulated offline chatbot arena that is fully based on and powered by AI LLMs without human evaluators. The primary objective of WizardArena is to mitigate the manual and temporal costs associated with post-training LLMs while retaining the benefits of arena-based evaluation and training. As the running example shown in the Figure 1, the key is that WizardArena can efficiently predict accurate performance rankings among different battle models based on a powerful \u201cjudge model\u201d, which could automatically imitate the manner of human annotators in judging a responses pair of two models and provide rankings, scores, and explanation. ", "page_idx": 1}, {"type": "text", "text": "In the training scenario, We innovatively propose Arena Learning strategy to simulate arena battles among target model (referred to as WizardLM- $\\beta$ ) and various state-of-the-art models on a large scale of instruction data. These synthetic battle results are then used to enhance WizardLM- $\\cdot\\beta$ through some training strategies, including supervised fine-tuning (SFT), direct preference optimization (DPO) [21], and proximal policy optimization (PPO) [22], enabling it to learn from the strengths and weaknesses of other models. Furthermore, Arena Learning introduces an iterative battle and training process, where the WizardLM- $\\cdot\\beta$ is continuously updated and re-evaluated against SOTA models. This process allows for the WizardLM- $\\cdot\\beta$ to iteratively improve and adapt to the evolving landscape of the arena, ensuring that it remains competitive and up-to-date with the latest advancements in the field. ", "page_idx": 1}, {"type": "text", "text": "In the evaluation scenario, we firstly contribute a carefully prepared offline testset, it effectively balances the diversity and complexity of evaluation. By automating the pair judgement process with \u201cjudge model\u201d, WizardArena significantly reducing the associated costs and priority limitations, and could produce the Elo rankings and detailed win/loss/tie statistics. ", "page_idx": 1}, {"type": "text", "text": "The experimental results demonstrate that the Elo rankings produced by WizardArena align closely with the LMSys Chatbot Arena. This finding validates the effectiveness of WizardArena as a reliable and cost-effective alternative to human-based evaluation platforms. Moreover, the models trained on the extensive battle data generated by Arena Learning exhibit significant performance improvements during the SFT, DPO, and PPO stages. In three iterative loops, our model can also scale up with more training data and achieve better performance. These results highlight the value and power of Arena Learning in post-training, which leverages the collective knowledge and capabilities of multiple models to drive the WizardLM- $\\beta$ \u2019s performance to new heights. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce Arena Learning, a novel AI powered method which help us build an efficient data flywheel for large language models post-training by simulating offline chatbot arena, which leverages AI annotator to mitigate the manual and temporal costs.   \n\u2022 We contribute a carefully prepared offilne testset of AI-based WizardArena, and demonstrate the highly consistent in accurately predicting Elo rankings among different LLMs compared to human-based LMSys Chatbot Arena.   \n\u2022 Experimental results demonstrate the effectiveness of Arena Learning in producing largescale synthetic data to continuously improve WizardLM- $\\beta$ , through various training strategies including SFT, DPO, and PPO. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 LLM Benchmarks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Large Language Models (LLMs) have transformed the way people interact with computing systems and are extensively used in everyday life and work [23]. The existing benchmarks [24\u201326] are mainly divided into two categories: 1) Specialized tasks. Knowledge and Capability: MMLU [27], CMMLU [28], and C-Eval [29]; Reasoning: ARC [30], HellaSwag [31], PIQA[32], GSM8k [33], MATH [34]; Programming: HumanEval [35], MBPP [36], LiveCodeBench [37]; Safety and Truthfulness: ToxicChat [38], OLID [39], BIG-Bench [40], TruthfulQA [41]. They focus on assessing LLM performance in specific areas. 2) General tasks: like MT-Bench [15, 42] and AlpacaEval [43], encompass categories such as writing, role-playing, and mathematics, highlighting the models\u2019 comprehensive abilities and multi-turn dialogue performance. ", "page_idx": 2}, {"type": "text", "text": "Real-world benchmarks, (i.e., LMSYS ChatBot Arena [44] and Allenai WildBench [45]) use anonymous battles, ELO [17, 46] rankings, and human judgments, but have time delay and often do not timely reflect the models\u2019 true performance and require large time and human labor intensive. Additionally, most models overfit on leaderboards like MT-Bench [15], OpenLLM leaderboard [47, 48], showing inconsistent performance with real-world ChatBot scenarios and low differentiation among models. Therefore, we have developed Simulated Offilne WizardArena, which not only effectively differentiates model performance but also aligns closely with the online live ChatBot Arena [44], making it suitable for selecting the optimal models while significantly enhancing model post-training through battle data. ", "page_idx": 2}, {"type": "text", "text": "2.2 Large Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLMs have made significant strides in Natural Language Processing (NLP), serving as a versatile foundation for numerous applications [23, 49, 50]. These models, which often contain hundreds of billions of parameters, are trained on expansive text datasets. Notable examples include OpenAI\u2019s GPT-3 and GPT-4 [4, 51], Anthropic\u2019s Claude [52], Google\u2019s PaLM [53, 54], Gemini [6], and DeepMind\u2019s Chinchilla [55]. The AI field has recently seen a surge in open-source LLMs, providing public access to model codes and parameters. Notable releases include BigScience\u2019s BLOOM [56], Mistral AI\u2019s Mistral [57], Meta\u2019s Llama family [3, 58] and GAL [59], Tsinghua University\u2019s ChatGLM [60], TII\u2019s Falcon [61] and Yi [62]. New entries such as Baichuan, Qwen [7], DeepSeek [63], and Llemma [64] have also emerged. Presently, models like Alpaca [11], Vicuna [10], Guanaco [65], Orca [66], OpenChat [13], Tulu2 [67], WizardLM [12], and Zephyr [68] are being developed through supervised fine-tuning based on Llama [3, 58] and Mistral [57]. ", "page_idx": 2}, {"type": "text", "text": "The alignment performance of Large Language Models (LLMs) is significantly influenced by the quality of Supervised Fine-Tuning (SFT) data, which encompasses task difficulty [66], query complexity [12, 69], semantic diversity [11, 14], and sample size [70]. For instance, [11] generates diverse queries through self-instruct [71] methods, while [12] enhances model alignment by increasing query complexity. [66] boosts NLP task performance by optimizing FLAN [72] queries and responses with specialized LLMs, and [14] has introduced UltraChat. To select data efficiently, some strategies like IFD [73], INSTAG [74], DEITA [75], MODS [76], and ALPAGASUS [77] are ", "page_idx": 2}, {"type": "image", "img_path": "VHva3d836i/tmp/048872be4ab074583f60a6f6ac8af1c66cc3ededf56db81782bc923d2572a67b.jpg", "img_caption": ["Figure 2: Overview of Arena Learning post-training data flywheel and WizardArena evaluation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "adopted. [66] employs ChatGPT to label instructional data, ensuring both diversity and complexity.   \nHere, we select training data using the judge pair method with different models. ", "page_idx": 3}, {"type": "text", "text": "To better adapt to preferences beyond SFT, models are trained with feedback-based methods like RLHF and RLAIF [2, 52, 58, 78, 79], employing Proximal Policy Optimization (PPO) [80] to align with model preferences. RLEIF [81] combines instruction evolution and reinforcement learning to enhance the mathematical reasoning capabilities of the model. Due to RLHF\u2019s complexity and instability, simpler alternatives like DPO [21], RRHF [82], KTO [83], IPO [84], sDPO [85], and ORPO [86] are utilized. DPO [21] merges reward modeling with preference learning, RRHF [82] uses ranking loss to prioritize preferred answers, and KTO [83] operates without needing paired preference datasets. In this paper, we use DPO and PPO for model post-training. ", "page_idx": 3}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we elaborate on the details of the proposed WizardArena and Arena Learning strategy. As illustrated in Figure 2, the pipeline mainly contains three components: Offline Pair-size LLM Battle Arena, Model Evaluation, and Model Training. ", "page_idx": 3}, {"type": "text", "text": "3.1 ChatBot Arena and Elo Ranking ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Chatbot Arena is a pioneering platform that has revolutionized the way chatbot models are evaluated and compared. It facilitates the assessment of different chatbot models by pitting them against each other in a series of conversational challenges. At the core of this Arena lies the concept of Elo rankings, a widely adopted rating system originally devised for chess players. Elo rankings [17] are used to quantify the relative performance of chatbot models based on a series of head-to-head battles. Each model is initially assigned an arbitrary Elo rating, which is then updated after every battle based on the outcome (win, loss, or tie) and the rating difference between the competing models. If a higher-rated model defeats a lower-rated one, its Elo rating increases slightly, while the loser\u2019s rating decreases by a corresponding amount. ", "page_idx": 3}, {"type": "text", "text": "3.2 Using a Powerful LLM as Judge to Simulate Human Annotators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "At the core of the simulated arena battles in WizardArena lies a powerful LLM that serves as the \u201cjudge model\u201d. This judge model is specifically prompted and adjusted by us on a diverse range of conversational pair data, enabling it to assess the quality, relevance, and appropriateness of the models\u2019 responses objectively and consistently. The judge model\u2019s role is to analyze and compare the responses provided by the competing models for each instruction or conversational sample. It considers various factors, such as coherence, factual accuracy, context-awareness, and overall quality, to determine whether one response is superior to the other or if they are of comparable quality (a tie). We show the details of its judgement prompt in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "3.3 Evaluation LLMs with WizardArena ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.3.1 Constructing the Offline Test Set ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To accurately evaluate the performance of chatbot models and predict their Elo rankings, WizardArena relies on a carefully curated offline mixed test set, which is designed to strike a balance between diversity and complexity, ensuring a comprehensive assessment of the models\u2019 capabilities across a wide range of conversational scenarios. This test set consists of the following two subsets: ", "page_idx": 4}, {"type": "text", "text": "Diverse Subset The diverse subset of the test set is constructed to capture a broad range of topics, styles, and conversational contexts. To achieve this, we employs text clustering techniques on a large corpus of instructions and conversational data. The clustering process begins by representing all the instructions in a conversation as a high-dimensional vector using state-of-the-art embedding models. These vectors capture the semantic and contextual information within the text, enabling the clustering algorithm to group similar samples together. Once the clustering is complete, we selects a representative sample from each cluster, ensuring that the diverse subset of the test set capture a broad range of scenarios. This approach helps to mitigate potential biases or blindspots that may arise from relying solely on simply random sampling. ", "page_idx": 4}, {"type": "text", "text": "Hard Subset This subset is specifically designed to challenge the capabilities of even the most advanced chatbot models. To construct this subset, we leverages the power of LLMs to predict the difficulty level of each instruction. We then selects the top-ranking samples according to the predicted difficulty scores, ensuring that the hard subset of the test set comprises the most challenging and complex scenarios. This data serves as a rigorous benchmark for evaluating the robustness and capability of chatbot models in handling intricate and nuanced conversational tasks. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Simulating Offline Battling and Ranking Models on Test Set ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the above \u201cjudge\u201d model and the offilne test set in place, WizardArena proceeds to evaluate the performance of various chatbot models through a series of pair-wise battles. The outcomes of the battles are then used to compute the Elo rankings of the participating chatbot models. WizardArena adopts the same Elo rating system used in LMSys Chatbot Arena, which has proven effective in ranking players or entities based on their head-to-head performance. ", "page_idx": 4}, {"type": "text", "text": "3.4 Iterative Training LLMs through Arena Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.4.1 Collecting Large-Scale Instruction Data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To facilitate leveraging the simulated arena battles among models to train WizardLM- $\\beta$ , Arena Learning relies on a large-scale corpus of conversational data $D$ . The data collection process involves several stages of filtering, cleaning, and deduplication to ensure the quality and diversity of the instruction data. The simulated arena battle outcomes are then used to generate training data for the WizardLM- ${}_{/\\beta}$ , tailored to different training strategies: supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). We split the data equally into some parts $D=\\{D_{0},D_{1},D_{2},...,D_{N}\\}$ for following iterative training and updates respectively. ", "page_idx": 4}, {"type": "text", "text": "3.4.2 Iterative Battle and Model Updating ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Arena Learning employs an iterative process for training and improving the WizardLM- $\\beta$ . After each round of simulated arena battles and training data generation, the WizardLM- ${}\\cdot{\\beta}$ is updated using the appropriate training strategies (SFT, DPO, and/or PPO). This updated model is then re-introduced into the arena, where it battles against the other SOTA models once again. This iterative process allows the WizardLM- $\\beta$ to continuously improve and adapt to the evolving landscape of the arena. As the model becomes stronger, the simulated battles become more challenging, forcing the WizardLM- $\\cdot\\beta$ to push its boundaries and learn from the latest strategies and capabilities exhibited by the other models. Additionally, the iterative nature of Arena Learning enables the researchers to monitor the progress and performance of the WizardLM- $\\beta$ over time, providing valuable insights into the effectiveness of the different training strategies and potential areas for further improvement or refinement. ", "page_idx": 4}, {"type": "text", "text": "The following is the first iterative loop: For SFT, we first train the initial version of WizardLM- $\\cdot\\beta$ with $D_{0}$ , then select some state-of-the-art LLMs which ranking top on WizardArena testset, following we battle WizardLM- $\\cdot\\beta$ with them on $D_{1}$ , and focus on extracting instances where the WizardLM- $\\cdot\\beta$ \u2019s response is considered inferior to the winning model\u2019s response, as determined by the judge model. These instances are collected, and the winning model\u2019s response is used as the target output for fine-tuning the next WizardLM- $\\beta$ -SFT model. For DPO, we use WizardLM- $\\cdot\\beta$ -SFT to battle with SOTA LLMs on $D_{2}$ , and then we treat win and loss responses as the $<$ choice, reject $>$ pairs to training the WizardLM- $\\beta$ -DPO model. For PPO, we leverage the same battle process on $D_{3}$ to obtain the $<$ choice, reject $>$ pairs to train the reward model and WizardLM- $\\beta$ -PPO. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In the second training iteration, we select the latest best WizardLM- $\\beta$ on the WizardArena test set as the initial model, and adopt above battles on $D_{4}$ , $D_{5}$ , and $D_{6}$ to get the training data of next version of SFT, DPO, and PPO models respectively. We will stop the iteration when we find the model can\u2019t achieve significantly better performance than previous iteration. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Source Data. We collected some instructions from open available datasets (i.e., Alpaca [11], FLAN [72], LMSYS-Chat-1M [87], OpenOrca [88], WizardLM [12]), and optimized them using the following steps: first, we flitered out all illegal conversations; second, we removed conversations with instruction lengths of less than 10; third, we eliminated duplicate instructions with prefixes of 10; next, we employed the MinHashLSH technique [89] (with a threshold of 0.4 and 128 permutation functions) for data deduplication; subsequently, we excluded instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, LMSYS-hard [90], MT-Bench [15], AlpacaEval [43], OpenLLM Leaderboard [27, 31, 41, 47, 91]) to prevent data leakage. Finally, we removed all non-English instructions. After completing these steps, we obtained the refined 276K dataset $D$ . ", "page_idx": 5}, {"type": "text", "text": "Offilne Diverse & Hard WizardArena test set. Firstly, we processed the source data using K-Means clustering into 500 categories. From each category, we randomly selected two samples to construct 1,000 diversity samples, named as the Offilne-Diverse WizardArena. Additionally, 20 samples from each category were selected at random to form a data set of 10,000 entries, we then used GPT-4-1106- preview to rate each instruction on a difficulty scale from 0 to 10 in descending order, and selected the top 1,000 entries to create the hard test set, denoted as the Offline-Hard WizardArena. Detailed scoring prompt is provided in Appendix B. The Offline-Mix WizardArena combines the Diverse and Hard test sets in 2,000 samples. Different from Arena-Hard-v1.0 [90], which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data. ", "page_idx": 5}, {"type": "text", "text": "LLM Battle. We selected 15 popular models from the LMSYS ChatBot Arena and conducted pairwise battles in the Offilne-Mix WizardArena. Llama3-70B-Chat [58] served as the \u201cjudge\u201d model, with the higher-scoring model declared the winner. To maintain consistency, the detailed judgement prompt is sourced from [15, 92] provided in Appendix A. Following LMSYS Chatbot Arena, we adopt the Bradley-Terry model [93] to calculate the final scores for each model. To mitigate potential position bias, we used a two-game setup, swapping the models between the first and second positions for each instance [92]. We use multiple bootstraps (i.e., 100), and select the median as the model\u2019s ELO score. The $95\\%$ CI is determined from the $2.5\\%$ to $97.5\\%$ range of confidence interval scores. ", "page_idx": 5}, {"type": "text", "text": "Training Data. 1) we random sample 10k ShareGPT data to train a initial model WizardLM- $\\ensuremath{\\boldsymbol}{\\cdot}\\ensuremath{\\boldsymbol}{\\beta}{-}\\ensuremath{\\boldsymbol}{I_{0}}$ . 2) we randomly split the $D$ into nine slices, each $D_{i}$ contains around 30K multi-turn conversations. 3) In the first iteration $I_{1}$ , we inference three SOTA models Command $\\mathbf{R+}$ , Qwen1.5-72B-chat [94], and OpenChat-3.5 [13] as reference models for battle and We also inference our WizardLM- $\\ensuremath{\\beta}{-}I_{0}$ on $D_{1}$ . 4) We employ the judge model to judge between WizardLM- $\\ensuremath{\\beta}_{0}J_{0}$ and each reference model. 5) The winning reference model\u2019s responses (threshold score $>1.0$ for maintaining the distinction) are used as the target output to train WizardLM-SFT- $\\beta{-}I_{1}$ . 6) Immediately, we use this as initial model and re-battle with three SOTA models on $D_{2}$ and $D_{3}$ to produce the training data of WizardLM-DPO- $\\beta{-}I_{1}$ and WizardLM-PPO- $\\beta{-}I_{1}$ respectively. The best model from $I_{1}$ will be the initial model of second iteration $I_{2}$ , and the third one $I_{3}$ also operates the same way. Finally, we obtain $56.5\\mathrm{K}$ ( $D_{0}$ 10K, $D_{1}$ 20K, $D_{4}\\,\\,14\\mathrm{K}$ , $D_{7}$ 12.5K) data for SFT, 57.3K $\\langle D_{2}\\,20.4\\mathrm{K}$ , $D_{5}$ 19.3K, $D_{8}$ 17.6K) data for DPO, 57.3K $(D_{3}\\;20.4\\mathrm{K}$ , $D_{6}$ 19.3K, $\\boldsymbol{D}_{9}$ 17.6K) data for PPO reward model, and 90k for PPO. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. We apply our method to the Mistral-7B [57] for post-training, use Llama3- 70B-Chat [58] as judge models in WizardArena. In supervised fine-tuning, we trained three epochs with a learning rate of 5e-6, a batch size of 128, and a sequence length of 4096. For PPO reward model training, Mistral-7B was trained for one epoch at a learning rate of 1e-6. In PPO training, the learning rate was 1e-7 for one epoch with a KL coefficient of 0.4, and for DPO training, it was 5e-7 for two epochs with a beta of 0.3. We used the DeepSpeed [95] and TRL [96] for SFT and RL. ", "page_idx": 5}, {"type": "table", "img_path": "VHva3d836i/tmp/9cbd72e9ebdaea5ba3ace4a4e01ce87b31f87910eade017bc9009c1cddfc8c1b.jpg", "table_caption": ["Table 1: The ELO rankings results of 22 models on LMSYS ChatBot Arena EN, MT-Bench, OffilneDiverse, Offline-Hard, and Offline-Mix (Diverse & Hard). "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: The performance comparison of 15 popular Figure 4: Win rates (w/o tie) of models in models across MT-Bench, normalized LMSYS ChatBot WizardArena-Mix. Each model involved in $2\\mathrm{k}\\times20$ Arena, and normalized WizardArena. battles (#samples num $\\times$ #models num). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 and Figure 3 presents the rankings for 16 popular models across several evaluation benchmarks: LMSYS ChatBot Arena-EN [44], MT-Bench [15], and three Offline-Diverse, Offilne-Hard, and Offline-Mix (Diverse & Hard) of WizardArena. The results reveal that employing the LMSYS ChatBot Arena as the reference benchmark in the real-world scenarios, WizardArena displays the good ranking consistency, however MT-Bench shows the large fluctuations. We find some models that perform well on MT-Bench, such as GPT-3.5-Turbo-0613 [4] and DeepSeek-LLM-67B-Chat [9], rank lower in the LMSYS ChatBot Arena. In addition, there is a significant difference in performance between WizardArena diverse and hard subsets, with Vicuna-33B [10] and Qwen1.5-32B-Chat [94] being more effective in diverse tasks, while Tulu-2-DPO-70B [67] and Nous-Hermes-2-MixtureDPO [98] achieving better results in hard tasks. ", "page_idx": 6}, {"type": "text", "text": "Table 2 illustrates that the Offilne WizardArena-Mix significantly outperforms MT-Bench across several consistent metrics, calculated from the evaluation results of the models in Table 1: a $45\\%$ higher Spearman Correlation, a $74\\%$ increase in Human Agreement with $95\\%$ CI, and a $60\\%$ improvement in Differentiation with $95\\%$ CI. It achieves totally $92.80\\%$ consistency with the LMSYS ChatBot Arena , closely matching the $91.57\\%$ consistency of ", "page_idx": 7}, {"type": "text", "text": "Arena-Hard-v1.0 [90]. Details of these metrics are provided in Appendix C. In contrast to MT-Bench and Arena-Hard-v1.0, using proprietary models (i.e. GPT-4) to judge, our approach employs current SOTA open-source models like Llama3-70B-Chat, which has a significantly lower cost. Moreover, the Offline ArenaMix, which combines Diverse and Hard test sets, achieves $3.27\\%$ higher average consistency than ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "VHva3d836i/tmp/196000d729c93115ba59ecdbe9ff7150514bc8cbba179e95fed75034729253b5.jpg", "table_caption": ["Table 2: The consistency of MT-Bench, ArenaHard-v1.0, and Offline WizardArena compared with the LMSYS ChatBot Arena. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "only using the Diverse-Arena and $1.8\\%$ higher than the LMSys Arena-Hard- $\\cdot\\mathrm{v}0.1$ , indicating greater consistency with the Online LMSYS ChatBot Arena. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Explore the consistency between Llama3-70B-Chat and GPT-4 as judging models in the Offilne-Mix Arena. Using multiple bootstraps (e.g., 100), we select the median as the model\u2019s ELO score. ", "page_idx": 7}, {"type": "table", "img_path": "VHva3d836i/tmp/3951da623e3be24c0898a24adee3549d09a5b5a0b0b8c852b42225d0f5a0b40d.jpg", "table_caption": ["Table 3: Explores data selection strategies for the SFT stage, using 10k samples for each method except for the Original $D_{1}$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "VHva3d836i/tmp/d36ea14a4a9ef919cf49c70c9178367c138648483072e8bb7b238108653f19c2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Can Arena Learning improve models performance via post-training? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 demonstrates the impact of using the Arena Learning method post-train Wizard- $\\cdot\\beta$ models during the SFT, DPO and PPO stages. In the SFT stage, Wizard- $\\beta{-}I_{1}$ , compared to Wizard- $\\cdot\\beta$ - $I_{0}$ , achieved a 0.57-point increase on MT-Bench, a 188-point rise in WizardArena-Mix ELO, and advanced 8 places in the ELO rankings. In the RL stage, WizardLM- $\\beta$ -PPO- $I_{1}$ outperformed Wizard- $\\beta{-}I_{1}$ by 0.31 points on MT-Bench, increased its ELO score in the WizardArena-Mix by 142 points, and moved up 6 places. WizardLM- $\\cdot\\beta$ -DPO- $J_{1}$ improved by 0.37 points on MT-Bench, and 135 points on WizardArena-Mix ELO, and advanced five places. WizardLM- $\\cdot\\beta$ -DPO-PPO- $J_{1}$ even achieves a 0.42-point increase on MT-Bench, a 156-point rise on WizardArena-Mix ELO and climbed seven places, outperforming both WizardLM- $\\beta$ -DPO- $I_{1}$ and WizardLM- $\\beta$ -PPO- $I_{1}$ individually. This indicates that continued PPO training based on DPO can further boost model performance. Figure 4 also provides a detailed comparison of the win rates between different models, our final 7B WizardLM- $\\ensuremath{\\beta}_{-}I_{3}$ achieve performance that is very close to Qwen1.5-32B-Chat. ", "page_idx": 7}, {"type": "text", "text": "Above results highlight that continuous battle with SOTA models with Arena Learning and updating weights with new selected data can progressively enhance model capacities compared to its rivals. Hence, Arena Learning builds an effective data flywheel and utilizing the Arena Learning can significantly improve model performance in post-training. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Data Selection strategy. To explore the efficiency of our pair-judge data selection method, we compare it with some widely used data selection strategies. In Table 3, We use 10k samples for each method except for the Original $D_{1}$ . to ensure a fair comparison, the pair-judge battle method only conducts battles between WizardLM- $\\beta$ -SFT- $I_{0}$ and Command $\\mathbf{R+}$ . The data where WizardLM- $\\cdot\\beta$ - SFT- $I_{0}$ loses are selected, with the corresponding responses taken from Command $\\mathrm{R}+$ . Additionally, the responses for instructions selected by IFD and INSTAG are also derived from Command $\\mathrm{R}+$ , rather than the original existing responses. ", "page_idx": 7}, {"type": "text", "text": "The results indicate that data selected via the pair-judge method yielded a 22-point improvement in the Offline-Mix Arena ELO over the all original 30k data, surpassed the diversity-based K-Means Cluster method by 25 points, and exceeded the instruction complexity-based INSTAG [74] method by 11 points. On MT-bench, the pair-judge method also demonstrated superior performance, with improvements of 0.17 points over Original, 0.13 points over K-Means Cluster, and 0.06 points over INSTAG. This advantage is attributed to that the pair-judge method focuses on instructions where the base model underperforms, particularly in diverse and complex tasks, effectively addressing the model\u2019s weaknesses. These results underscore the effectiveness of the pair-judge method in selecting high-quality data during the SFT phase to target and strengthen the weakness of the base model. ", "page_idx": 8}, {"type": "text", "text": "Llama3-Chat Judge and GPT-4 Judge Consistency. In most previous works, people were accustomed to use GPT-4 as a judge for evaluation or generating synthetic data, but the GPT-4 API cost required for large-scale data flywheel is enormous for most research and production scenarios. Therefore, we explore whether it is possible to replace GPT-4 with advanced open source models. Table 4 explores the consistency between Llama3-70B-Chat and GPT-4 as judge models in the Offilne-Mix Arena. Using GPT-4 judge\u2019s ELO as the reference benchmark, the Spearman correlation coefficient between Llama3-70B-Chat judge and GPT-4 judge is $95.81\\%$ , and the Human Agreement with $95\\%$ CI is $88.46\\%$ . The overall average consistency between the two judge models is $92.14\\%$ . Consequently, employing Llama3-70B-Instruct as a cost-effective judge model achieves high consistency with both GPT-4 and LMSYS ChatBot Arena by human judgment, ensuring the reliability of the WizardArena evaluation and post-training with Arena Learning in this paper. ", "page_idx": 8}, {"type": "table", "img_path": "VHva3d836i/tmp/97798bb67cb3c391de113dc2b96c79f0c6ebf96f278e00bf8cb4a3607a389d2d.jpg", "table_caption": ["Table 5: Explore different alignment strategies for models in SFT and RL stages. We utilize three slices of data for SFT, DPO, and PPO training. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "VHva3d836i/tmp/fc6860dd756691ab28547afb8a0dca8545f61e4743b6f6853061e4880c6fdd58.jpg", "table_caption": ["Table 6: Explore our model\u2019s performance across various benchmarks, including the OpenLLM Leaderboard, GSM8k, AlpacaEval2.0 and MT-Bench. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Training strategy. Table 5 explores the impact of different training strategies in the first round during the SFT, DPO, and PPO stages. Iterative application of the pair-judge method consistently boosts SFT model performance, exemplified by the Offilne-Arena Mix ELO score rising from 1063 to 1124 and the MT-bench score from 6.98 to 7.15. These outcomes confirm the effectiveness and scalability of the pair-judge approach for SFT data selection. In the RL stage, by continuing the post-training of DPO and PPO on top of SFT, the Offline-Arena Mix ELO score significantly increased by 135 points and 142 points, and MT-bench improved by 0.37 points and 0.31 points. Futhermore, SFT $+\\,\\mathrm{DPO}+\\mathrm{PPO}$ showed a modest 0.05-point improvement on MT-bench compared to $\\mathrm{SFT}+\\mathrm{DPO}$ , but obviously increased by 21 points on Offline-Arena Mix ELO. These findings suggest that the continuous application of reinforcement learning strategies can further boost the model\u2019s intrinsic capabilities. Above results indicate that the data derived from the pair-judge battle method not only significantly enhanced the SFT phase training but also provided high-quality data pairs for the RL phase, continuously improving the training outcomes for DPO and PPO. ", "page_idx": 8}, {"type": "text", "text": "Scaling Iterative SFT, DPO, PPO train", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "ing. Figure 5 explores the iterative training processes of SFT, DPO, and PPO, where $I_{i}$ represents the $i$ -th iteration. The results highlight that continuous battle with WizardAerna and updating can progressively enhance model performance. Specifically, from SFT- $J_{0}$ to DPO- $J_{3}$ or $\\mathrm{PPO}{-}I_{3}$ , the WizardArena ELO score increased from 875 to 1274, achieves a huge gain of 399 points, and the MT-Bench score also rises ", "page_idx": 8}, {"type": "image", "img_path": "VHva3d836i/tmp/8a8ac4df196b46ee46a023da006719608ce983b608255fa32c74d234aea56eaa.jpg", "img_caption": ["Figure 5: Explore the iterative training processes of SFT, DPO, and PPO. $I_{i}$ represents the $i$ -th iteration. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "from 6.41 to 7.81, achieves an increase of 1.4 points. These findings underscore the effectiveness and scalability of the Arena Learning iterative training method in post-training LLMs. ", "page_idx": 8}, {"type": "table", "img_path": "VHva3d836i/tmp/68d88e8f6649a875f669a8318c906c1d6b791847334c5c3988047e5be7dcb5a5.jpg", "table_caption": ["Table 7: Explore the quantity of Choose and Reject responses for each battle model across various rounds during the DPO stages. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Count of data selected from each battle model during DPO. Table 7 summarizes the sources of Choose and Reject responses during the DPO data construction. Command $\\mathrm{R}+$ selected $9.5\\mathrm{k}$ , $8.8\\mathbf{k}$ , and $7.8\\mathbf{k}$ data as Choose responses across three rounds, totaling $26.1\\mathrm{k}$ . The corresponding Reject responses were 1.1k, $0.9\\mathbf{k}$ , and $0.9\\mathbf{k}$ , totaling $2.9\\mathbf{k}$ . WizardLM- $\\cdot\\beta$ -SFT selected 1.6k, 1.9k, and $2.5\\mathrm{k}$ Choose responses across three rounds, totaling $6.0\\mathrm{k}(6.0\\mathrm{k}$ vs. $57.3\\mathrm{k})$ , with corresponding Reject responses of $8.5\\mathrm{k}$ , $6.4\\mathrm{k}$ , and $4.2\\mathrm{k}$ , totaling 19.1k(19.1k vs. 57.3k). This indicates that as WizardLM- $\\beta$ -SFT improved through iterative training, the number of Choose responses increased, while Reject responses decreased. ", "page_idx": 9}, {"type": "text", "text": "Llama3-70B-Chat vs. Human Judge. To reduce time and annotation costs, we randomly selected 200 samples from WizardArena-Mix (100 diverse, 100 challenging). We evaluated four models: WizardLM- $\\beta$ -PPO- $I_{3}$ (reference model), OpenChat-3.5, Command $\\mathbf{R+}$ , and Qwen1.5-72B-Chat (battle models), using Llama3-70BChat and professional human annotators , with results shown in Table 8. Llama3-70B-Chat\u2019s win rates for WizardLM- $\\beta$ -PPO- $.I_{3}$ against Command $\\mathbf{R+}$ , Qwen1.5- 72B-Chat, and OpenChat-3.5 were $34.1\\%$ , $41.3\\%$ , and $79.7\\%$ , closely matching human evaluations $(31.8\\%$ , $37.7\\%$ , $82.1\\%)$ . The high consistency between them further validates Llama3-70B-Chat\u2019s reliability and accuracy as a judge model in WizardArena. ", "page_idx": 9}, {"type": "table", "img_path": "VHva3d836i/tmp/9e7464bf8a361019d1051194d05f34f60778cfd3bf13be26c08712ca0ffd3929.jpg", "table_caption": ["Table 8: The win/tie/loss counts of WizardLM- $\\beta$ -PPO- $.I_{3}$ against Command $\\mathbf{R+}$ , Qwen1.5-72B-Chat, OpenChat-3.5 evaluated by Llama3 70B Chat Judge and Human Judge. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Performance on more benchmarks. Table 6 showcases our model\u2019s performance at the first iteration across various benchmarks, including the OpenLLM Leaderboard, GSM8k, AlpacaEval2.0, and MT-Bench for SFT, DPO and PPO stages. Utilizing the WizardArena method to produce training data has markedly improved model performance in both SFT and RL stages. Specifically, WizardLM- $\\beta$ -SFT- $I_{1}$ exceeds WizardLM- $\\cdot\\beta$ -SFT- $.I_{0}$ by 7.03 average points. More impressively, WizardLM- $\\beta$ -PPO- $I_{1}$ not only surpasses WizardLM- $\\beta.$ -SFT- $I_{0}$ by 8.74 points but also exceeds WizardLM- $\\beta$ -SFT- $I_{1}$ by 1.71 points and outperforms Openchat-3.5 by 4.64 points. Particularly in the reasoning tasks, WizardLM- $\\cdot\\beta$ -PPO- $J_{1}$ shows a 7.88 point increase on MMLU and a significant 20.26 point gain on GSM8k compared to WizardLM- $\\beta.$ -SFT- $I_{0}$ , demonstrating that the our method effectively enhances the model\u2019s weaknesses. The detailed scores of WizardLM- $\\beta{-}I_{1}$ SFT, DPO, PPO in the 8 subtasks of MT-Bench refer to the Figure 6. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, this paper presents WizardArena, a simulated offilne chatbot arena that utilizes AI LLMs to eliminate the manual and temporal costs associated with post-training LLMs, while preserving the beneftis of arena-based evaluation and training. The effectiveness of WizardArena is validated through the high consistency in predicting Elo rankings among different LLMs compared to the human-based LMSys Chatbot Arena. Furthermore, the model trained on synthetic data generated by Arena Learning strategy exhibits significant performance improvements across various training strategies. This work showcases the potential of WizardArena as a cost-effective and reliable alternative to human-based evaluation and data production platforms for post-training chatbot models. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Broader Impacts. If the judge model fails to accurately imitate human evaluators, the generated rankings and training data may be compromised. Moreover, similar to the other LLMs, our model could also generate potentially unethical or misleading information sometimes. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by Young Elite Scientists Sponsorship Program by CAST (No.   \n2024QNRC003). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[4] OpenAI. Gpt-4 technical report, 2023.   \n[5] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.   \n[6] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Shusheng Yang, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv, abs/2309.16609, 2023.   \n[8] BaichuanAI. Baichuan.   \n[9] DeepseekAI. Deepseek.   \n[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality, March 2023.   \n[11] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023.   \n[12] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.   \n[13] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023.   \n[14] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.   \n[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.   \n[17] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \n[18] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2024.   \n[19] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, 2023.   \n[20] LMSYS. Lmsys chatbot arena: Live and community-driven llm evaluation.   \n[21] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. ArXiv, abs/2305.18290, 2023.   \n[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[23] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \n[24] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1\u201345, 2024.   \n[25] Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, and Jian-Yun Nie. A user-centric benchmark for evaluating large language models. arXiv preprint arXiv:2404.13940, 2024.   \n[26] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736, 2023.   \n[27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[28] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023.   \n[29] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[31] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[32] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432\u20137439. AAAI Press, 2020.   \n[33] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[34] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \n[35] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[36] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021.   \n[37] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.   \n[38] Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389, 2023.   \n[39] Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav Nakov. Solid: A large-scale semi-supervised dataset for offensive language identification, 2021.   \n[40] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.   \n[41] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.   \n[42] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024.   \n[43] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.   \n[44] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.   \n[45] Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha Ravichander, Valentina Pyatkin, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild, 2024.   \n[46] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model evaluation, 2023.   \n[47] Edward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface. co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.   \n[48] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.   \n[49] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023.   \n[50] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i et al. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. ", "page_idx": 13}, {"type": "text", "text": "[52] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. ", "page_idx": 13}, {"type": "text", "text": "[53] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.   \n[54] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[55] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022.   \n[56] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   \n[57] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[59] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.   \n[60] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.   \n[61] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.   \n[62] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.   \n[63] DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu, Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. ArXiv, abs/2401.02954, 2024.   \n[64] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.   \n[65] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.   \n[66] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.   \n[67] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.   \n[68] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment. ArXiv, abs/2310.16944, 2023.   \n[69] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.   \n[70] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024.   \n[71] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.   \n[72] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 22631\u201322648. PMLR, 2023.   \n[73] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023.   \n[74] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, and Chang Zhou. # instag: Instruction tagging for diversity and complexity analysis. arXiv preprint arXiv:2308.07074, 2023.   \n[75] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685, 2023.   \n[76] Qianlong Du, Chengqing Zong, and Jiajun Zhang. Mods: Model-oriented data selection for instruction tuning. arXiv preprint arXiv:2311.15653, 2023.   \n[77] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023.   \n[78] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.   \n[79] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[80] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.   \n[81] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.   \n[82] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.   \n[83] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024.   \n[84] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. ArXiv, abs/2310.12036, 2023.   \n[85] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. sdpo: Don\u2019t use your data all at once. ArXiv, abs/2403.19270, 2024.   \n[86] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. ArXiv, abs/2403.07691, 2024.   \n[87] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Haotong Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset. ArXiv, abs/2309.11998, 2023.   \n[88] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/ Open-Orca/OpenOrca, 2023.   \n[89] Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. ArXiv, abs/1407.4416, 2014.   \n[90] Tianle\\* Li, Wei-Lin\\* Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024.   \n[91] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.   \n[92] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. ArXiv, abs/2305.17926, 2023.   \n[93] Julien Fageot, Sadegh Farhadkhani, L\u00ea Nguy\u00ean Hoang, and Oscar Villemaud. Generalized bradley-terry models for score estimation from paired comparisons. In AAAI Conference on Artificial Intelligence, 2023.   \n[94] Ali. Qwen.   \n[95] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale. SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315, 2022.   \n[96] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/ trl, 2020.   \n[97] Cohere Inc. Cohere: Large language models for your business.   \n[98] Teknium, theemozilla, karan4d, and huemin_art. Nous hermes 2 mixtral 8x7b dpo. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Models Pair-wise Judgement Prompt ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "VHva3d836i/tmp/96a07d9f7c4b749bedce46c1cce889384f36502dd18e46f97e50d8e33c5a0dd6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B GPT-4 Scoring Prompt ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Example 2: GPT-4 Scoring Prompt ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "[System] ", "page_idx": 17}, {"type": "text", "text": "We are interested in understanding how well the following input prompts can evaluate an AI assistant\u2019s proficiency in problem-solving ability, creativity, or adherence to real-world facts. Your task is to assess each prompt within a conversation based on its potential to gauge the AI\u2019s capabilities effectively in these areas. ", "page_idx": 17}, {"type": "text", "text": "There are multiple rounds of user prompts in the conversation, please rate each user prompt individually. ", "page_idx": 17}, {"type": "text", "text": "Guidelines for Scoring: ", "page_idx": 17}, {"type": "text", "text": "\u2022 High Score (8-10): Reserved for prompts that are particularly challenging and excellently designed to assess AI proficiency. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Medium Score (4-7): Given to prompts that have a moderate potential to assess the AI\u2019s capabilities. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Low Score (1-3): Allocated to prompts that are either too easy, ambiguous, or do not adequately assess the AI\u2019s capabilities. ", "page_idx": 17}, {"type": "text", "text": "our input is a JSON list format, where each element is a dictionary: - idx: represent which round of user prompts in a conversation. - value: represent a round of user prompts within a conversation ", "page_idx": 17}, {"type": "text", "text": "Your output is in JSON list format, where each element is a dictionary:   \n- idx: represent which round of user prompts in a conversation.   \n- value: represent detailed scoring reasons for the round of user prompts within a conversation. You should carry out the two steps: ", "page_idx": 17}, {"type": "text", "text": "1. Assess the Potential: Consider how challenging the prompt is, and how well it can assess an AI\u2019s problem-solving skills, creativity, or factual accuracy. Briefly explain your reasoning. ", "page_idx": 17}, {"type": "text", "text": "2. Assign a Score: Assign a score on a scale of 1 to 10, with a higher score representing a higher potential to evaluate the AI assistant\u2019s proficiency effectively. Use double square brackets to format your scores, like so: [[5]]. ", "page_idx": 17}, {"type": "text", "text": "Ensure to critically evaluate each prompt and avoid giving high scores to prompts that are ambiguous or too straightforward. ", "page_idx": 17}, {"type": "text", "text": "Here is an example.   \n[### input ###]: [{\"idx\": 0, \"value\": \"Please show me how to serve a ReactJS app from a simple ExpressJS server. Use typescript.\"}] ", "page_idx": 17}, {"type": "text", "text": "[### output ###]: $[\\{^{\\dag}\\mathrm{idx}^{\\dag};\\,0$ , \"value\": \"1. Assess the Potential: This prompt is a good test of the AI\u2019s problem-solving skills and its adherence to real-world facts. The AI would need to provide a step-by-step guide on how to serve a ReactJS app from an ExpressJS server using TypeScript, which requires a good understanding of these technologies. However, it doesn\u2019t directly test the AI\u2019s creativity. $\\mathfrak{w}2$ . Assign a Score: This prompt has a high potential to assess the AI\u2019s problem-solving skills and adherence to real-world facts, but it doesn\u2019t test creativity. Therefore, I would assign a score of [[8]].\"}] ", "page_idx": 17}, {"type": "text", "text": "It is important to note that the output must be in JSON format and be parsed correctly by the JSON tool. ", "page_idx": 17}, {"type": "text", "text": "[### input ###]: {Your_JSON_INSTRUCTION} ", "page_idx": 17}, {"type": "text", "text": "[### output ###]: ", "page_idx": 17}, {"type": "text", "text": "C Three consistency metrics between two Arenas ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To more effectively align the online arena (i.e. LMSYS ChatBot Arena) with real-world human preferences and to enhance differentiation among models, we developed a simulated offline arena. This platform is designed to evaluate the actual performance of the models and to facilitate the selection of optimal model checkpoints. We use several key criteria [90] that define an effective benchmark for evaluating Large Language Models (LLMs) in chatbot applications, aiming to enable meaningful functional comparisons across different models. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Alignment with Human Preference $:$ The benchmarks should maintain high alignment with real-world human preferences in responses to the diverse and hard instructions, ensuring that the models\u2019 outputs meet user expectations. \u2022 Ranking Accuracy: The benchmark should align closely with the reference standard to ensure that the rankings of different models on the leaderboard are reliable and accurate. \u2022 Differentiation: The benchmark should be capable of accurately differentiating the performance of various models by providing confidence intervals with minimal overlap. This feature is crucial to ensure that the more effective models can be reliably distinguished. ", "page_idx": 18}, {"type": "text", "text": "We define the alignment of Benchmark $A$ with reference to Benchmark $B$ , for a model pair $(m_{1},m_{2})$ that $B$ can confidently differentiate, using the following formulation: ", "page_idx": 18}, {"type": "text", "text": "The agreement score, $s(m_{1},m_{2})$ , is determined as: ", "page_idx": 18}, {"type": "text", "text": "$s(m_{1},m_{2})=\\binom{1.0}{-1.0}$ if $A$ confidently separates $m_{1}$ from $m_{2}$ and their ranking aligns with $B$ if $A$ confidently separates $m_{1}$ from $m_{2}$ and their ranking conflicts with $B$ if A cannot confidently separate $m_{1}$ from $m_{2}$ ", "page_idx": 18}, {"type": "text", "text": "To assess ranking accuracy, we employed Spearman\u2019s rank correlation coefficient to analyze the correlation between the two sets of ranking data. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho=1-\\frac{6\\sum d_{i}^{2}}{n(n^{2}-1)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\rho$ is the Spearman\u2019s rank correlation coefficient, $d_{i}$ is the difference between the ranks of corresponding variables, and $n$ is the number of observations. ", "page_idx": 18}, {"type": "text", "text": "We define the differentiation of models based on their performance scores, which are represented by confidence intervals $C I_{1}$ and $C I_{2}$ via bootstrapping. If the two confidence intervals do not overlap, then models $M_{1}$ and $M_{2}$ are considered to be separable. ", "page_idx": 18}, {"type": "equation", "text": "$$\nC I_{1}\\cap C I_{2}=\\emptyset\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D The Radar plot of MT-Bench ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "VHva3d836i/tmp/a833691f34899d294953f1414f3f28b7a55690cc9a57589efa3436867dad230c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Radar plot showing detailed scores of WizardLM- $\\cdot\\beta$ -SFT, DPO, PPO at the first iteration in the eight subtasks of MT-Bench. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 20}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 20}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 20}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 20}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 20}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, please refer to the abstract and introduction of this article for details. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, please refer to the conclusion of this article for details. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, please refer to the approach of this article for details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes, please refer to the appendix source code of this article for details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 22}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, please refer to the conclusion of this article for details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, please refer to the conclusion of this article for details. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, please refer to the experiments of this article for details. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer:[NA] Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]