{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "This paper introduces Flamingo, a foundational visual language model that significantly influenced the design and capabilities of many subsequent multimodal LLMs, including the model presented in this paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-29", "reason": "BLIP-2 is a highly influential multimodal LLM that improved the efficiency of previous models by using frozen image encoders and introducing Q-Former, a method referenced in this paper's discussion of efficient aggregation."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-01-01", "reason": "Qwen-VL is another important multimodal LLM that this paper uses for comparison, especially regarding its use of efficient learnable queries for visual information aggregation."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper introduces visual instruction tuning, a key method used for training and improving multimodal LLMs, which is directly relevant to this paper's approach."}, {"fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "publication_date": "2019-01-01", "reason": "The GQA dataset is a benchmark dataset used in the evaluation of this paper's model, making it an important reference in assessing the model's performance."}]}