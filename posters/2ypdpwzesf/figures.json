[{"figure_path": "2YPdpWzEsF/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the average normalized accuracy (MMB, TextVQA, GQA). PR means Perceiver Resampler, which utilize the learnable query as information aggregator. Our method achieves highest accuracy comparing with the others while maintaining high training speed.", "description": "This figure compares the average normalized accuracy of different vision-language connectors on three benchmark datasets (MMB, TextVQA, GQA) against the step time (seconds per step).  The connectors compared include AcFormer (the proposed method), MLP, C-Abstractor, Pooling, and PR (Perceiver Resampler).  The graph shows that AcFormer achieves the highest accuracy across all datasets, while maintaining a relatively fast training speed, outperforming the other methods in both accuracy and efficiency.  The number of visual tokens used by each method is also indicated, showing that AcFormer uses significantly fewer tokens than some other methods while still maintaining high accuracy. The figure also visually represents the information aggregation mechanism using visual anchors in Vision Transformer, which is the core idea of the AcFormer method.", "section": "1 Introduction"}, {"figure_path": "2YPdpWzEsF/figures/figures_3_1.jpg", "caption": "Figure 2: Visualizations of the visual feature map and attention map pertaining to the [CLS] token. Here we select 10 layers in Vision Transformer to show their output. We present the attention maps corresponding to the [CLS] token in the final layer. Notably, special tokens within both the feature map and attention map are identified using red circles. These marked points are referred to as \u201cvisual anchors\u201d. Details can be found in Section 3.2.", "description": "This figure visualizes the feature maps and attention maps from 10 different layers of a Vision Transformer model, focusing on the [CLS] token which represents the global image information.  The visualizations highlight specific tokens (marked with red circles) that consistently show strong activation in both the feature maps and attention maps across multiple layers.  These tokens are identified as \"visual anchors\", demonstrating regions of the image that are particularly important for understanding the scene. The figure supports the paper's claim that these visual anchors provide crucial information aggregation.", "section": "3.2 Visual Anchors"}, {"figure_path": "2YPdpWzEsF/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of Anchor Former (AcFormer). We propose our token selection algorithm code in detail at Section D.", "description": "This figure illustrates the architecture of the proposed Anchor Former (AcFormer) method. AcFormer is a novel vision-language connector that leverages visual anchors to improve the efficiency and accuracy of Multimodal Large Language Models (MLLMs).  The figure shows the process, starting with the vision feature extraction, then visual anchor selection by Anchor Selector, followed by Information Aggregation Module using cross-attention with selected anchors, and finally passing the aggregated information to the Large Language Model.  It also compares AcFormer with other existing vision-language connectors, highlighting the advantages of using visual anchors and showing differences in flexibility and utilization of prior knowledge.", "section": "3.3 Anchor Former"}, {"figure_path": "2YPdpWzEsF/figures/figures_13_1.jpg", "caption": "Figure 2: Visualizations of the visual feature map and attention map pertaining to the [CLS] token. Here we select 10 layers in Vision Transformer to show their output. We present the attention maps corresponding to the [CLS] token in the final layer. Notably, special tokens within both the feature map and attention map are identified using red circles. These marked points are referred to as \u201cvisual anchors\u201d. Details can be found in Section 3.2.", "description": "This figure visualizes the feature maps and attention maps from 10 different layers of a Vision Transformer.  The visualization focuses on the [CLS] token, which is a special token used to represent the global context of the image. Red circles highlight special tokens called \"visual anchors\", which the authors identify as important for information aggregation.  The figure supports the authors' claim that visual anchors exist and can be identified, providing evidence for their AcFormer method.", "section": "3.2 Visual Anchors"}, {"figure_path": "2YPdpWzEsF/figures/figures_14_1.jpg", "caption": "Figure 2: Visualizations of the visual feature map and attention map pertaining to the [CLS] token. Here we select 10 layers in Vision Transformer to show their output. We present the attention maps corresponding to the [CLS] token in the final layer. Notably, special tokens within both the feature map and attention map are identified using red circles. These marked points are referred to as \u201cvisual anchors\u201d. Details can be found in Section 3.2.", "description": "This figure visualizes feature maps and attention maps from different layers of a Vision Transformer for two example images.  The visualizations highlight specific tokens (marked with red circles) that consistently appear activated across multiple layers both in feature maps and attention maps. These tokens, termed \"visual anchors\", are central to the aggregation of visual information within the transformer, and are the foundation of the paper's proposed AcFormer method.", "section": "3.2 Visual Anchors"}, {"figure_path": "2YPdpWzEsF/figures/figures_14_2.jpg", "caption": "Figure 6: More samples for visualization of the visual anchors.", "description": "This figure shows more examples of visualizations of feature maps and attention maps from different layers of a vision transformer, supporting the paper's claim of the existence and importance of \"visual anchors\" in these models.  The consistent pattern of activation across different images further validates the hypothesis that these anchors play a significant role in information aggregation.", "section": "3.2 Visual Anchors"}, {"figure_path": "2YPdpWzEsF/figures/figures_15_1.jpg", "caption": "Figure 7: Detailed code for the anchor selector within Anchor Former (AcFormer).", "description": "This figure shows the detailed Python code for the anchor selection algorithm used in the Anchor Former. The code iterates through the attention map to find the most salient visual tokens (anchors) and selects them as information aggregators for the vision-language connector.  The input includes the visual feature map, attention map and the desired number of tokens. The algorithm uses a progressive search strategy to ensure diverse anchor selection and to avoid redundancy.", "section": "3.3 Anchor Former"}]