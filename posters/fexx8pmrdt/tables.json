[{"figure_path": "FExX8pMrdT/tables/tables_3_1.jpg", "caption": "Table 2: Results of Naive RAG, Human writing and AutoSurvey. Both of AutoSurvey and Naive RAG use Claude-haiku as the writer. Note that human writing surveys used for evaluation are excluded during the retrieval process.", "description": "This table presents a comparison of the performance of three different methods for generating surveys: Naive RAG, human writing, and AutoSurvey.  The comparison is made across four different survey lengths (8k, 16k, 32k, and 64k tokens) and includes metrics for citation quality (recall and precision), content quality (coverage, structure, and relevance), and speed.  Claude-haiku was used as the LLM writer for both Naive RAG and AutoSurvey. Notably, human-written surveys used for evaluation were excluded from the retrieval process when generating the results for Naive RAG and AutoSurvey.", "section": "Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_5_1.jpg", "caption": "Table 1: Content Quality Criteria.", "description": "This table lists the five-point scoring rubric used to evaluate the content quality of the generated surveys. The scoring criteria are broken down into three sub-indicators: Coverage, Structure, and Relevance, each with a detailed description for each score (1-5).", "section": "3 Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_5_2.jpg", "caption": "Table 2: Results of Naive RAG, Human writing and AutoSurvey. Both of AutoSurvey and Naive RAG use Claude-haiku as the writer. Note that human writing surveys used for evaluation are excluded during the retrieval process.", "description": "This table presents the results of an experiment comparing three different methods for generating academic surveys: human writing, a naive RAG approach, and the proposed AutoSurvey method.  The table shows performance metrics for each method across four different survey lengths (8k, 16k, 32k, and 64k tokens). The metrics reported include speed, citation quality (recall and precision), and content quality (coverage, structure, relevance, and average).  This allows for a comparison of the efficiency, accuracy, and overall quality of the three methods for generating academic surveys.", "section": "3 Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study results for AutoSurvey with different components removed.", "description": "This table presents the results of an ablation study conducted on the AutoSurvey model.  The study systematically removes key components of the model (retrieval mechanism and reflection phase) to assess their individual contributions to the overall performance.  The table shows the performance metrics for citation quality (recall and precision) and content quality (coverage, structure, relevance, and average) for each ablation variant compared to the full AutoSurvey model.", "section": "3 Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_7_2.jpg", "caption": "Table 4: Performance of AutoSurvey with different base LLM writers.", "description": "This table shows the performance of AutoSurvey when using different Large Language Models (LLMs) as the base writer.  The results demonstrate the impact of choosing different LLMs on both citation quality (recall and precision) and content quality (coverage, structure, and relevance).  The table allows for a comparison of AutoSurvey's performance using GPT-4, Claude-haiku, and Gemini-1.5-pro, against human-written surveys. The average content quality score is presented to summarize the overall quality achieved by each LLM.", "section": "3 Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_7_3.jpg", "caption": "Table 5: Performances given different references.", "description": "This table presents the accuracy results of four different methods in answering multiple-choice questions related to a survey topic.  The methods include: \n\n1. **Direct:** Answering questions without any additional context.\n2. **Naive RAG-based LLMs:** Answering questions with a basic retrieval augmented generation approach, using LLMs.\n3. **Upper-bound:** Answering questions with access to all relevant papers, representing the best possible performance.\n4. **AutoSurvey:** Answering questions using the proposed AutoSurvey methodology.\n\nThe accuracy is measured as a percentage, with error bars indicating variability.  The results demonstrate how AutoSurvey improves accuracy compared to direct answers and a naive RAG-based approach, although not reaching the performance level of the upper-bound (having access to all relevant information).", "section": "3 Experiments"}, {"figure_path": "FExX8pMrdT/tables/tables_8_1.jpg", "caption": "Table 6: Performance of AutoSurvey with different base LLM writers.", "description": "This table presents the results of an ablation study conducted to evaluate the performance of AutoSurvey when using different Large Language Models (LLMs) as the base writer for generating surveys.  The results are shown as recall scores (in percentage) at different points (20%, 40%, 60%, 80%, and 100%) in the generation process.  It shows a comparison between the naive RAG (Retrieval-Augmented Generation) approach and AutoSurvey, highlighting AutoSurvey's relative stability and better performance across various stages of the text generation process.  The table helps demonstrate the robustness of AutoSurvey across different LLMs.", "section": "Main Results"}, {"figure_path": "FExX8pMrdT/tables/tables_13_1.jpg", "caption": "Table 7: Survey Table", "description": "This table lists 20 different surveys selected for the evaluation of AutoSurvey.  Each row represents a survey paper, showing its topic, title, and the number of citations it received from Google Scholar. The surveys were chosen to cover a broad range of topics within the field of large language models (LLMs) and to include surveys with varying citation counts.", "section": "A Detail of Topics and Human-writing Surveys"}, {"figure_path": "FExX8pMrdT/tables/tables_14_1.jpg", "caption": "Table 8: Cost of AutoSurvey", "description": "This table shows the average cost to generate a 32k-tokens survey using three different LLMs: Claude-haiku, Gemini-1.5-pro, and GPT-4.  The table indicates the input tokens, output tokens, and the cost in dollars for each LLM.", "section": "Cost Analysis"}]