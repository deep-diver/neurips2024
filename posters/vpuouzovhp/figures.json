[{"figure_path": "VpuOuZOVhP/figures/figures_1_1.jpg", "caption": "Figure 1: Different long-tailed data augmentation paradigms. (a) The traditional augmentation paradigm randomly samples augmentations from the fixed strategy. (b) The strategy fixed augmentation paradigm samples augmentations from the fixed strategy according to the data distribution. (c) The LLM-driven augmentation paradigm combines LLMs with long-tailed learning to learn the optimal augmentation strategy.", "description": "This figure illustrates three different paradigms for long-tailed data augmentation. (a) shows the traditional approach, where augmentations are randomly selected from a fixed set. (b) presents a strategy-fixed approach, where augmentations are selected based on the data distribution. (c) introduces the LLM-driven approach proposed in the paper, which leverages large language models to learn an optimal augmentation strategy.", "section": "1 Introduction"}, {"figure_path": "VpuOuZOVhP/figures/figures_3_1.jpg", "caption": "Figure 2: Strategy generation paradigm of SimpleLLM.", "description": "This figure illustrates the SimpleLLM framework, which uses prompts to guide a large language model (LLM) in generating data augmentation strategies.  A non-parametric prompt is fed to the LLM, which then outputs an augmentation strategy.  This strategy is subsequently applied to a long-tailed model for training. The process leverages the LLM's capabilities to automatically create data augmentation strategies for improved performance in long-tailed learning scenarios.", "section": "3 LLM \u00d7 LTL: Can LLMs Provide DA Strategies for Long-tailed Learning?"}, {"figure_path": "VpuOuZOVhP/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of LLM-AutoDA. LLM-AutoDA leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions.", "description": "LLM-AutoDA uses a two-module framework. First, an LLM-based augmentation strategy generation module uses prompt engineering to automatically generate augmentation strategies based on prior knowledge and store them in a repository. These strategies are evaluated by applying them to the original imbalanced data, creating an augmented dataset for training a long-tailed learning model.  The performance on a validation set acts as a reward signal, updating the strategy generation model iteratively. This process continues until convergence or a computational budget is reached. The second module is a long-tailed learning training and evaluation module, which trains and evaluates the long-tailed learning model using the generated augmentation strategies.", "section": "4 LLM-AutoDA: A Resourceful Adviser for Long-tailed Learning"}, {"figure_path": "VpuOuZOVhP/figures/figures_6_1.jpg", "caption": "Figure 4: Average accuracy (%) on CIFAR-100-LT dataset (Imbalance ratio=100) with CUDA and DODA. SimpleLLM is comparable to the effectiveness of CUDA and DODA when combined with long-tailed learning baselines.", "description": "This figure compares the performance of different data augmentation methods (CUDA, DODA, and SimpleLLM) when combined with various long-tailed learning baselines on the CIFAR-100-LT dataset with an imbalance ratio of 100.  It shows that SimpleLLM, despite its simplicity, achieves comparable results to the more sophisticated CUDA and DODA methods. This suggests that LLMs can be effective in generating data augmentation strategies for long-tailed learning.", "section": "5.1 Experimental Settings"}, {"figure_path": "VpuOuZOVhP/figures/figures_7_1.jpg", "caption": "Figure 5: Impact of different LLMs on the performance of long-tailed learning models.", "description": "This figure shows the results of experiments comparing the performance of three different large language models (LLMs) in a long-tailed learning scenario.  The x-axis represents the generation number in the evolutionary process, and the y-axis represents the Top-1 accuracy. The three LLMs used are GPT-3.5, GPT-4, and Claude-3-Opus. The figure demonstrates that each LLM exhibits a similar trend in performance, achieving high accuracy near the strategy with scores around 12, and showing performance degradation when using augmentation strategies with excessively high scores.", "section": "5.3 More Analysis and Discussion"}, {"figure_path": "VpuOuZOVhP/figures/figures_8_1.jpg", "caption": "Figure 8: Visualization of the process of finding the optimal solution for different augmentation paradigms.", "description": "This figure visualizes the loss landscape for different data augmentation strategies. The x and y axes represent the augmentation operator and intensity, respectively, and the z-axis represents the loss.  The figure shows that traditional fixed-strategy methods (like CUDA and DODA) search for optimal strategies only within a limited region (shown as the red and green planes), while LLM-AutoDA explores a larger search space and is able to find a global optimal solution (the lowest point on the surface).", "section": "5.3 More Analysis and Discussion"}, {"figure_path": "VpuOuZOVhP/figures/figures_15_1.jpg", "caption": "Figure 9: Accuracy (%) on more imbalanced CIFAR-100-LT dataset (Imbalance ratio=200) with SOTA DA methods.", "description": "This figure compares the performance of different data augmentation methods on a highly imbalanced version of the CIFAR-100 dataset (imbalance ratio of 200).  It shows Top-1 accuracy achieved by several long-tailed learning baselines (CE, CE-DRW, LDAM-DRW, BS, RIDE, and BCL) when combined with three data augmentation techniques: CUDA, DODA, and the proposed LLM-AutoDA.  The results demonstrate LLM-AutoDA's superior performance across various baselines, highlighting its effectiveness in addressing long-tailed problems with extreme class imbalance.", "section": "D.1 Highly Imbalanced Scenarios"}, {"figure_path": "VpuOuZOVhP/figures/figures_16_1.jpg", "caption": "Figure 10: Trends in the augmentation intensities and number of times different strategies are selected.", "description": "This figure visualizes how the selection of data augmentation strategies changes over training epochs.  It shows the frequency with which different augmentation methods (Rotate, Gaussian Blur, Invert, etc.) are chosen and the intensity at which they're applied.  The data suggest that the model learns to prefer certain augmentation strategies over others as training progresses, demonstrating adaptation and learning within the augmentation strategy generation process.", "section": "D.2 Visualization of Selection Process"}]