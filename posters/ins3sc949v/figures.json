[{"figure_path": "iNS3SC949v/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Unified view of deep MIL models. Depending on how instances interact with each other in (a), we devise three different families of methods: (b), (c), (d).", "description": "This figure shows a unified view of deep multiple instance learning (MIL) models.  It categorizes them into three families based on how instances within a bag interact: (b) instances treated independently; (c) only global interactions (e.g., using transformers); (d) both global and local interactions (e.g., combining transformers with graph neural networks).  The figure visually represents the different model architectures, highlighting the key differences in how instance-level information is processed and aggregated to produce bag-level predictions.", "section": "3 Background: A unified view of deep MIL approaches"}, {"figure_path": "iNS3SC949v/figures/figures_3_1.jpg", "caption": "Figure 2: WSIs are divided into patches. CT scans are provided as slices. They often show spatial dependencies: in a WSI, a patch is usually surrounded by patches with the same label, while in a CT scan, a slice is usually surrounded by slices with the same label. The red color indicates malignant/hemorrhage patches/slices.", "description": "This figure shows two examples of medical images used in the paper which are Whole Slide Images (WSIs) and Computed Tomography (CT) scans.  WSIs are divided into patches and CT scans into slices.  The figure highlights that neighboring image instances (patches or slices) often share the same label. This observation motivates the use of local dependencies in modeling instance labels for improved localization in Multiple Instance Learning (MIL).", "section": "4 Method: Introducing smoothness in the attention values"}, {"figure_path": "iNS3SC949v/figures/figures_5_1.jpg", "caption": "Figure 3: Smooth Attention Multiple Instance Learning. (a) The well-known model in [17], which we build upon. (b): only local interactions are considered by applying the proposed smooth operator Sm in the aggregation part. (c): both global and local interactions are considered by applying Sm both in the transformer and in the aggregation parts.", "description": "This figure shows three different model architectures for multiple instance learning (MIL).  (a) is the baseline ABMIL model, which processes instances independently. (b) incorporates the proposed \"smooth operator\" (Sm) to model local dependencies between instances, enhancing localization.  (c) extends (b) by adding a transformer to capture global dependencies among instances.", "section": "4.3 The proposed model"}, {"figure_path": "iNS3SC949v/figures/figures_7_1.jpg", "caption": "Figure 4: Attention histograms on CAMELYON16. First/second rows show models without/with global interactions. SmAP and SmTAP stand out at separating positive and negative instances.", "description": "This figure presents histograms of attention values for different MIL models on the CAMELYON16 dataset.  The histograms separately show the distribution of attention values for positive and negative instances.  The results highlight that SmAP and SmTAP effectively separate positive and negative instances, unlike other models which show more overlap.", "section": "5.1 Localization: instance level results"}, {"figure_path": "iNS3SC949v/figures/figures_7_2.jpg", "caption": "Figure 5: Attention maps on CAMELYON16. The novel SmTAP produces the most accurate one.", "description": "This figure compares the attention maps generated by four different methods on a WSI from the CAMELYON16 dataset.  The ground truth patch labels are shown for comparison.  The SmTAP model produces the attention map that most closely resembles the ground truth, highlighting its superior localization performance. Other methods such as TransMIL, GTP, and CAMIL show less accurate localization and more ambiguity in identifying the regions of interest.", "section": "5.1 Localization: instance level results"}, {"figure_path": "iNS3SC949v/figures/figures_9_1.jpg", "caption": "Figure 6: Influence of the trade-off parameter \u03b1 (left) and of spectral normalization (right) in CAMELYON16. Setting \u03b1 > 0 improves upon the baseline ABMIL (\u03b1 = 0) and is a trade-off between better localization results (lower \u03b1) or better classification results (higher \u03b1). Likewise, Sm without spectral normalization already improves the results upon the baseline (ABMIL), but the best performance is obtained when they are used together.", "description": "This figure shows the ablation study on the influence of the trade-off parameter \u03b1 and spectral normalization on the performance of the proposed method. The left panel shows how different values of \u03b1 affect the instance-level and bag-level AUROC scores. The right panel shows how using spectral normalization impacts instance-level and bag-level AUROC scores, compared to not using it. The results indicate that using the smooth operator improves performance in both localization and classification tasks, and that spectral normalization further enhances the performance.", "section": "5.3 Ablation study"}, {"figure_path": "iNS3SC949v/figures/figures_15_1.jpg", "caption": "Figure 7: Influence of the number of steps T used to approximate Sm in RSNA and CAMELYON16. ABMIL corresponds to T = 0. Using T = 10 is enough to closely match the performance of the exact form (T = \u221e).", "description": "This figure shows the impact of the number of iterations (T) used in approximating the smooth operator (Sm) on the model's performance.  The x-axis represents the number of iterations (T), with \u221e representing the exact solution. The y-axis shows the instance and bag AUROC for the RSNA and CAMELYON16 datasets.  The results indicate that using T = 10 provides a close approximation to the exact solution (T = \u221e), suggesting that a smaller number of iterations can be used without significantly impacting the model's performance.  ABMIL represents the baseline case where the smooth operator is not used (T=0).", "section": "5.3 Ablation study"}, {"figure_path": "iNS3SC949v/figures/figures_17_1.jpg", "caption": "Figure 3: Smooth Attention Multiple Instance Learning. (a) The well-known model in [17], which we build upon. (b): only local interactions are considered by applying the proposed smooth operator Sm in the aggregation part. (c): both global and local interactions are considered by applying Sm both in the transformer and in the aggregation parts.", "description": "This figure shows three different architectures for multiple instance learning (MIL).  The first (a) is a baseline architecture (ABMIL) that processes instances independently. The second (b) adds the proposed smooth operator (Sm) to model local dependencies only after the instance-level embeddings have been aggregated. The third (c) includes both global dependencies (through a transformer) and local dependencies by incorporating the smooth operator before both aggregation and global interaction modeling steps. This figure illustrates the progressive addition of complexity and the flexible nature of the proposed smooth operator, allowing for use in conjunction with other mechanisms to address global interactions.", "section": "4.3 The proposed model"}, {"figure_path": "iNS3SC949v/figures/figures_21_1.jpg", "caption": "Figure 10: PANDA attention maps.", "description": "This figure displays attention maps generated by several Multiple Instance Learning (MIL) models on a Whole Slide Image (WSI) from the PANDA dataset.  Each column represents a different MIL model: SmTAP, TransMIL, SETMIL, GTP, CAMIL, SmAP, ABMIL, CLAM, DSMIL, and DFTD-MIL.  The color intensity in each map indicates the attention score assigned to each patch within the WSI.  Brighter red signifies a higher attention score. The figure visually demonstrates how different MIL models focus on different regions of the WSI when trying to predict the presence of prostate cancer.", "section": "5 Experiments"}, {"figure_path": "iNS3SC949v/figures/figures_22_1.jpg", "caption": "Figure 10: PANDA attention maps.", "description": "This figure shows attention maps generated by different MIL methods on a Whole Slide Image (WSI) from the PANDA dataset.  The top row displays the original WSI and its corresponding patch labels. The subsequent rows illustrate the attention maps produced by SmTAP, TransMIL, SETMIL, GTP, and CAMIL (top half) and SmAP, ABMIL, CLAM, DSMIL, and DFTD-MIL (bottom half). Each attention map visually represents the model's prediction of patch-level labels, with colors indicating confidence levels of positive or negative labels.", "section": "5 Experiments"}, {"figure_path": "iNS3SC949v/figures/figures_22_2.jpg", "caption": "Figure 12: Ground truth and SmAP attention maps of three different WSIs from CAMELYON16. As expected theoretically, a larger \u03b1 produces smoother attention maps.", "description": "This figure compares the ground truth attention maps with the attention maps produced by ABMIL and SmAP with different values of \u03b1 (0.1, 0.5, and 0.9). It shows that increasing \u03b1 leads to smoother attention maps, as predicted by the theory. The results are shown for three different WSIs from the CAMELYON16 dataset.", "section": "5.3 Ablation study"}, {"figure_path": "iNS3SC949v/figures/figures_23_1.jpg", "caption": "Figure 4: Attention histograms on CAMELYON16. First/second rows show models without/with global interactions. SmAP and SmTAP stand out at separating positive and negative instances.", "description": "This figure displays attention histograms for various MIL models on the CAMELYON16 dataset.  It visualizes the distribution of attention values assigned to positive versus negative instances for each model.  The histograms highlight the effectiveness of SmAP and SmTAP in clearly separating positive and negative instance attention values. The top row presents models without global interactions, and the bottom row shows models incorporating global interactions.", "section": "5.1 Localization: instance level results"}, {"figure_path": "iNS3SC949v/figures/figures_23_2.jpg", "caption": "Figure 4: Attention histograms on CAMELYON16. First/second rows show models without/with global interactions. SmAP and SmTAP stand out at separating positive and negative instances.", "description": "This figure shows the attention histograms for different MIL models on the CAMELYON16 dataset. The histograms visualize the distribution of attention values assigned to positive and negative instances by each model.  The top row displays models without global interactions, and the bottom row displays models with global interactions.  The figure highlights that SmAP and SmTAP are particularly effective at separating positive and negative instances, indicating superior performance in instance-level classification.", "section": "5.1 Localization: instance level results"}]