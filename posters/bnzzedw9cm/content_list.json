[{"type": "text", "text": "Decoupled Kullback-Leibler Divergence Loss ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiequan Cui1 Zhuotao Tian4 Zhisheng Zhong2 Xiaojuan $\\mathbf{Q}\\mathbf{i}^{3}$ Bei $\\mathbf{Y}\\mathbf{u}^{2}$ Hanwang Zhang1 ", "page_idx": 0}, {"type": "text", "text": "Nanyang Technological University1 The Chinese University of Hong Kong2 The University of Hong Kong3 HIT(SZ)4 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we delve deeper into the Kullback\u2013Leibler (KL) Divergence loss and mathematically prove that it is equivalent to the Decoupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. Thanks to the decomposed formulation of DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of KL/DKL in scenarios like knowledge distillation by breaking its asymmetric optimization property. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce class-wise global information into KL/DKL to mitigate bias from individual samples. With these two enhancements, we derive the Improved Kullback\u2013Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training, and knowledge distillation tasks. The proposed approach achieves new state-of-the-art adversarial robustness on the public leaderboard \u2014 RobustBench and competitive performance on knowledge distillation, demonstrating the substantial practical merits. Our code is available at https://github.com/jiequancui/DKL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Loss functions are a critical component of training deep models. Cross-Entropy loss is particularly important in image classification tasks [28, 55, 59, 20, 44, 12], while Mean Square Error (MSE) loss is commonly used in regression tasks [51, 27, 25]. Contrastive loss [7, 26, 8, 23, 4, 16, 17] has emerged as a popular objective for representation learning. The selection of an appropriate loss function can exert a substantial influence on a model\u2019s performance. Therefore, the development of effective loss functions [3, 43, 73, 63, 36, 2, 65, 58, 15] remains a critical research topic in the fields of computer vision and machine learning. ", "page_idx": 0}, {"type": "text", "text": "Kullback-Leibler (KL) Divergence quantifies the degree of dissimilarity between a probability distribution and a reference distribution. As one of the most frequently used loss functions, it finds application in various scenarios, such as adversarial training [71, 66, 14, 35], knowledge distillation [33, 6, 73], incremental learning [5, 42], and robustness on out-of-distribution data [29]. Although many of these studies incorporate KL Divergence loss as part of their algorithms, they may not thoroughly investigate the underlying mechanisms of the loss function. To bridge this gap, our paper aims to elucidate the working mechanism of KL Divergence regarding gradient optimization. ", "page_idx": 0}, {"type": "text", "text": "Our study focuses on the analysis of Kullback\u2013Leibler (KL) Divergence loss from the perspective of gradient optimization. For models with softmax activation, we provide theoretical proof that it is equivalent to the Decoupled Kullback\u2013Leibler (DKL) Divergence loss which comprises a weighted Mean Square Error (wMSE) loss and a Cross-Entropy loss with soft labels. Figures 1(a) and (b) reveal the equivalence between KL and DKL losses regarding gradient backpropagation. With the decomposed formulation, it becomes more convenient to analyze how the KL loss works in training optimization. ", "page_idx": 0}, {"type": "image", "img_path": "bnZZedw9CM/tmp/59fca5823c45d1e7201fa2ad69c51a7ad60e564196c94a4028f839109a0c2d52.jpg", "img_caption": ["Figure 1: Comparisons of gradient backpropagation between KL, DKL, and IKL losses. DKL loss is equivalent to KL loss regarding backward optimization. $\\mathcal{M}$ and $\\mathcal{N}$ can be the same one (like in adversarial training) or two separate (like in knowledge distillation) models determined by application scenarios. Similarly, $x_{m}$ , $x_{n}\\in X$ can also be the same one (like in knowledge distillation) or two different (like in adversarial training) images. $o_{m}$ , $o_{n}$ are logits output with which the probability vectors are obtained when applying the softmax activation. Black arrows represent the forward process while colored arrows indicate the backward process driven by the corresponding loss functions in the same color. \u201cwMSE\u201d is a weighted Mean Square Error (MSE) loss. \u201cw\u00afMSE\u201d is incorporated with class-wise global information. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We have identified potential issues of KL loss with the newly derived DKL loss. Specifically, its gradient optimization is asymmetric regarding the inputs. As illustrated in Figure 1(b), the gradients on $o_{m}$ and $o_{n}$ are asymmetric and driven by the wMSE and Cross-Entropy individually. This optimization asymmetry can lead to the wMSE component being ignored in certain scenarios, such as knowledge distillation where $o_{m}$ is the logits of the teacher model and detached from gradient backpropagation. Fortunately, it is convenient to address this issue with the decoupled formulation of DKL loss by breaking the asymmetric optimization property. As evidenced by Figure 1(c), enabling gradient on $o_{n}$ from wMSE alleviates this problem. ", "page_idx": 1}, {"type": "text", "text": "Moreover, wMSE component is guided by sample-wise predictions. Hard examples with incorrect prediction scores can lead to challenging optimization. We thus insert class-wise global information to regularize the training process. Integrating DKL with these two points, we derive the Improved Kullback\u2013Leibler (IKL) Divergence loss. ", "page_idx": 1}, {"type": "image", "img_path": "bnZZedw9CM/tmp/d698a35bac6181501d640ecaa1dface91f51a45d04caa0392e865143d2bf73ee.jpg", "img_caption": ["Figure 2: We achieve SOTA robustness on CIFAR-100. \u201cstar\u201d represents our method while \u201ccircle\u201d denotes previous methods. \u201cBlack\u201d means adversarial training with image preprocessing only including random crop and flip, \u201cBlue\u201d is for methods with AutoAug or CutMix, and \u201cred\u201d represents methods using synthesized data. AA is short for Auto-Attack [10]. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To demonstrate the effectiveness of our proposed IKL loss, we evaluate it with adversarial training and knowledge distillation tasks. Our experimental results on CIFAR-10/100 and ImageNet show that the IKL loss achieves new state-of-the-art robustness on the public leaderboard of RobustBench 1. Comparisons with previous methods on adversarial robustness are shown in Figure 2. ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions of our work are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We reveal that the KL loss is mathematically equivalent to a composite of a weighted MSE (wMSE) loss and a Cross-Entropy loss employing soft labels.   \n\u2022 Based on our analysis, we propose two modifications for enhancement: breaking its asymmetric optimization and incorporating class-wise global information, deriving the Improved Kullback\u2013Leibler (IKL) loss.   \n\u2022 With the proposed IKL loss, we obtain the state-of-the-art adversarial robustness on RobustBench and competitive knowledge distillation performance on CIFAR-10/100 and ImageNet. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adversarial Robustness. Since the identification of adversarial examples by Szegedy et al. [57], the security of deep neural networks (DNNs) has gained significant attention, and ensuring the reliability of DNNs has become a prominent topic in the machine learning community. Adversarial training [46], being the most effective method, stands out due to its consistently high performance. ", "page_idx": 2}, {"type": "text", "text": "Adversarial training incorporates adversarial examples into the training process. Madary et al. [46] propose the adoption of the universal first-order adversary, specifically the PGD attack, in adversarial training. Zhang et al. [71] trade off the accuracy and robustness by the KL loss. Wu et al. [66] introduce adversarial weight perturbation to explicitly regulate the flatness of the weight loss landscape. Cui et al. [14] leverage guidance from naturally-trained models to regularize the decision boundary in adversarial training. Additionally, various other techniques [35] focusing on optimization or training aspects have also been developed. Besides, recently, several works [22, 64, 1] have explored the use of data augmentation techniques to improve adversarial training. We have explored the mechanism of KL loss for adversarial robustness in this paper. The effectiveness of the proposed IKL loss is tested in both settings with and without synthesized data [38]. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation. The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [33]. It involves extracting \u201cdark knowledge\u201d from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the KL loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective technique significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [33], several advanced methods, including logits-based [9, 21, 48, 67, 72, 73, 34] and features-based approaches [53, 61, 30, 70, 6, 31, 32, 39, 49, 50, 68], have been introduced. This paper decouples the KL loss into a new formulation, i.e., DKL, and addresses the limitation of KL loss for application scenarios like knowledge distillation. ", "page_idx": 2}, {"type": "text", "text": "Other Applications of KL Divergence Loss. In semi-supervised learning, the KL loss acts as a consistency loss between the outputs of weakly and strongly augmented images [56, 60]. In continual learning, KL loss helps retain previous knowledge by encouraging consistency between the outputs of pre-trained and newly updated models [5, 42]. Additionally, enforcing consistency between outputs from different augmented views enhances model robustness to out-of-distribution data [29]. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we begin by introducing the preliminary mathematical notations in Section 3.1. Theoretical analysis of the equivalence between KL and DKL losses is presented in Section 3.2. Finally, we propose the IKL loss to address potential limitations of KL/DKL in Section 3.3, followed by a case study with additional analysis in Section 3.4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition of KL Divergence. Kullback-Leibler (KL) Divergence measures the differences between two probability distributions. For distributions $P$ and $Q$ of a continuous random variable, It is defined to be the integral: ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{K L}(P||Q)=\\int_{-\\infty}^{+\\infty}p(x)*\\log\\frac{p(x)}{q(x)}d x,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p$ and $q$ denote the probability densities of $P$ and $Q$ . ", "page_idx": 2}, {"type": "text", "text": "The KL loss is one of the most widely used objectives in deep learning, applied across various contexts involving categorical distributions. This paper primarily examines its role in adversarial training and knowledge distillation tasks. ", "page_idx": 2}, {"type": "text", "text": "In adversarial training, the KL loss improves model robustness by aligning the output probability distribution of adversarial examples with that of their corresponding clean images, thus minimizing output changes despite input perturbations. In knowledge distillation, the KL loss enables a student model to mimic the behavior of a teacher model, facilitating knowledge transfer that enhances the student model\u2019s generalization performance. ", "page_idx": 2}, {"type": "text", "text": "Applications of KL Loss in Deep Learning. We consider image classification models that predict probability vectors using the softmax activation. Let $\\mathbf{o}_{i}\\in\\mathbb{R}^{C}$ represent the logits output from a model given an input image $x_{i}\\in X$ , where $C$ denotes the number of classes. The predicted probability vector is $\\mathbf{s}_{i}\\in\\mathbb{R}^{C}$ , computed as $\\mathbf{s}_{i}=s o f t m a x(\\mathbf{o}_{i})$ . The values $\\mathbf{o}_{i}^{j}$ and $\\mathbf{s}_{i}^{j}$ correspond to the logits and probabilities for the $j$ -th class, respectively. The KL loss is often used to encourage similarity between $\\mathbf{s}_{m}$ and $\\mathbf{s}_{n}$ in various scenarios, resulting in the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{K L}(x_{m},x_{n})=\\sum_{j=1}^{C}\\mathbf{s}_{m}^{j}*\\log\\frac{\\mathbf{s}_{m}^{j}}{\\mathbf{s}_{n}^{j}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, in adversarial training, $x_{m}$ represents a clean image, while $x_{n}$ is its corresponding adversarial example. In knowledge distillation, $x_{m}$ and $x_{n}$ are the same image, but they are input separately to the teacher and student models. Notably, in the knowledge distillation process, $\\mathbf{s}_{m}$ is detached from gradient backpropagation, as the teacher model is pre-trained and fixed during training. ", "page_idx": 3}, {"type": "text", "text": "3.2 Decoupled Kullback-Leibler Divergence Loss ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Previous works [33, 73, 71, 14] have incorporated the KL loss into their algorithms without investigating its underlying mechanism. This paper aims to uncover the driving force behind gradient optimization by analyzing the KL loss function. With the backpropagation rule in training optimization, the derivative gradients are as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\partial\\mathcal{L}_{K L}}{\\partial\\mathbf{o}_{m}^{j}}}&{=}&{\\displaystyle\\sum_{k=1}^{C}(\\big(\\Delta\\mathbf{m}_{j,k}-\\Delta\\mathbf{n}_{j,k}\\big)*\\big(\\mathbf{s}_{m}^{k}*\\mathbf{s}_{m}^{j}\\big)),}\\\\ {\\displaystyle\\frac{\\partial\\mathcal{L}_{K L}}{\\partial\\mathbf{o}_{n}^{j}}}&{=}&{\\displaystyle\\mathbf{s}_{m}^{j}*\\big(\\mathbf{s}_{n}^{j}-1\\big)+\\mathbf{s}_{n}^{j}*\\big(1-\\mathbf{s}_{m}^{j}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta\\mathbf{m}_{j,k}=\\mathbf{o}_{m}^{j}-\\mathbf{o}_{m}^{k}$ , and $\\Delta\\mathbf{n}_{j,k}=\\mathbf{o}_{n}^{j}-\\mathbf{o}_{n}^{k}$ ", "page_idx": 3}, {"type": "text", "text": "Leveraging the antiderivative technique alongside the structured gradient information, we introduce a novel formulation called the Decoupled Kullback-Leibler (DKL) Divergence loss, as presented in Theorem 1. The DKL loss is designed to be equivalent to the KL loss while offering a more analytically tractable alternative for further exploration and study. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 From the perspective of gradient optimization, the Kullback-Leibler (KL) Divergence loss is equivalent to the following Decoupled Kullback-Leibler (DKL) Divergence loss when $\\alpha=1$ and $\\beta=1$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D K L}(x_{m},x_{n})=\\underbrace{\\frac{\\alpha}{4}\\|\\sqrt{S(\\mathbf{w}_{m})}(\\Delta\\mathbf{m}-S(\\Delta\\mathbf{n}))\\|^{2}}_{\\mathrm{weighted}\\;\\mathbf{MSE}\\;(\\mathrm{wMSE})}\\underbrace{-\\beta\\cdot S(\\mathbf{s}_{m}^{\\top})\\cdot\\log\\mathbf{s}_{n}}_{\\mathrm{Cross-Entropy}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal S(\\cdot)$ arnedp e. nStsu mopmeartaitoion ni,s $\\mathbf{s}_{m}^{\\top}$ e ids  ftroar ntshpe orseed uocf $\\mathbf{s}_{m}$ , $\\mathbf{w}_{m}^{j,k}=\\mathbf{s}_{m}^{j}*\\mathbf{s}_{m}^{k}$ , $\\Delta\\mathbf{m}_{j,k}=$ $\\mathbf{o}_{m}^{j}-\\mathbf{o}_{m}^{k}$ $\\Delta\\mathbf{n}_{j,k}=\\mathbf{o}_{n}^{j}-\\mathbf{o}_{n}^{k}$ $\\|\\cdot\\|^{2}$ ", "page_idx": 3}, {"type": "text", "text": "Proof See Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Interpretation. With Theorem 1, we know that KL loss is equivalent to DKL loss regarding gradient optimization, i.e., DKL loss produces the same gradients as $K L$ loss given the same inputs. Therefore, KL loss can be interpreted as a composition of a wMSE loss and a Cross-Entropy loss. This is the first work to reveal the precise quantitative relationships between KL, Cross-Entropy, and MSE losses. Upon examining this new formulation, we identify two potential issues with the KL loss. ", "page_idx": 3}, {"type": "text", "text": "Asymmetirc Optimization. As shown in Eqs. (3) and (4), gradient optimization is asymmetric for $\\mathbf{o}_{m}$ and $\\mathbf{o}_{n}$ . The wMSE and Cross-Entropy losses in Theorem 1 are complementary and collaboratively work together to make $\\mathbf{o}_{m}$ and $\\mathbf{o}_{n}$ similar. Nevertheless, the asymmetric optimization can cause the wMSE component to be neglected or overlooked when $\\mathbf{o}_{m}$ is detached from gradient backpropagation, which is the case for knowledge distillation, potentially leading to performance degradation. ", "page_idx": 3}, {"type": "text", "text": "Sample-wise Prediction Bias. As shown in Eq. (5), ${\\bf w}_{m}$ in wMSE component is conditioned on the prediction score of $x_{m}$ . However, sample-wise predictions can be subject to significant variance. ", "page_idx": 3}, {"type": "text", "text": "Incorrect prediction of hard examples or outliers will mislead the optimization and result in unstable training. Our study in Sections 3.4 and 4.4 indicates that the choice of ${\\bf w}_{m}$ significantly affects adversarial robustness. ", "page_idx": 4}, {"type": "text", "text": "3.3 Improved Kullback-Leibler Divergence Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Based on the analysis in Section 3.2, we propose an Improved Kullback-Leibler (IKL) Divergence loss. Distinguished from DKL in Theorem 1, we make the following improvements: 1) breaking the asymmetric optimization property; 2) inserting class-wise global information to mitigate sample-wise bias. The details are presented as follows. ", "page_idx": 4}, {"type": "text", "text": "Breaking the Asymmetric Optimization Property. As shown in Eq. (5), the wMSE component encourages $\\mathbf{o}_{m}$ to resemble $\\mathbf{o}_{n}$ by capturing second-order information, specifically the differences between logits for each pair of classes. Each addend in wMSE only involves logits of two classes. We refer to this property as locality. On the other hand, the Cross-Entropy loss ensures that $\\mathbf{s}_{n}$ and $\\mathbf{s}_{m}$ produce similar predicted scores. Each addend in the Cross-Entropy gathers all class logits. We refer to this property as globality. Two loss terms collaboratively work together to make $\\mathbf{o}_{n}$ and $\\mathbf{o}_{m}$ similar in locality and globality. Discarding any one of them can lead to performance degradation. ", "page_idx": 4}, {"type": "text", "text": "However, because of the asymmetric optimization property of KL/DKL, the unexpected case can occur when $\\mathbf{s}_{m}$ is detached from the gradient backpropagation (scenarios like knowledge distillation), in which the formulation will be: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D K L-K D}(x_{m},x_{n})=\\underbrace{\\frac{\\alpha}{4}\\|\\sqrt{S(\\mathbf{w}_{m})}(S(\\Delta\\mathbf{m})-S(\\Delta\\mathbf{n}))\\|^{2}}_{\\mathrm{weighted}\\;\\mathrm{MSE}\\;\\mathrm{(wMSE)}}\\underbrace{-\\beta\\cdot S(\\mathbf{s}_{m}^{\\top})\\cdot\\log\\mathbf{s}_{n}}_{\\mathrm{Cross-Entropy}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As indicated by Eq. (6), the wMSE component loss takes no effect on training optimization since all sub-components of wMSE are detached from gradient propagation, which can potentially hurt the model performance. Knowledge distillation exactly matches this case because the teacher model is fixed during knowledge distillation training. Thanks to the decomposition of DKL formulation, we address this issue by breaking the asymmetric optimization property, i.e., enabling the gradients of ${\\mathcal{S}}(\\Delta\\mathbf{n})$ in Eq. (5). Then, the updated formulation of Eq. (6) becomes, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{D K L-K D}(x_{m},x_{n})=\\underbrace{\\frac{\\alpha}{4}\\|\\sqrt{S(\\mathbf{w}_{m})}(\\mathcal{S}(\\Delta\\mathbf{m})-\\Delta\\mathbf{n})\\|^{2}}_{\\mathrm{weighted}\\;\\mathrm{MSE}\\;\\mathrm{(wMSE)}}\\underbrace{-\\beta\\cdot\\mathcal{S}(\\mathbf{s}_{m}^{\\top})\\cdot\\log{\\mathbf{s}_{n}}}_{\\mathrm{Cross-Entropy}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After enabling the gradients of ${\\mathcal{S}}(\\Delta\\mathbf{n})$ in Eq. (5), wMSE will produce symmetric gradients on $o_{n}$ and $o_{m}$ . Regarding the knowledge distillation, wMSE can output gradient on $o_{n}$ and promote the training optimization demonstrated by Eq. (7). ", "page_idx": 4}, {"type": "text", "text": "Inserting Class-wise Global Information. Recall in Theorem 1, ${\\bf w}_{m}$ in Eq. (5) is calculated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{w}_{m}^{j,k}=\\mathbf{s}_{m}^{j}*\\mathbf{s}_{m}^{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It indicates that ${\\bf w}_{m}$ depends on the sample-wise prediction scores. Nevertheless, the model cannot output correct predictions when dealing with outliers or hard examples in training. In this case, wMSE will attach the most importance on the predicted class $\\hat{y}=\\arg\\operatorname*{max}\\mathcal{M}(x_{m})$ rather than the ground-truth class, which misleads the optimization and makes the training unstable. ", "page_idx": 4}, {"type": "text", "text": "We thus insert class-wise global information into wMSE component, replacing ${\\bf w}_{m}$ with $\\bar{\\bf w}_{y}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\bf w}_{y}^{j,k}=\\bar{\\bf s}_{y}^{j}*\\bar{\\bf s}_{y}^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $y$ is ground-truth label of $x_{m}$ , $\\begin{array}{r}{\\overline{{\\bf s}}_{y}=\\frac{1}{|X_{y}|}\\sum_{x_{i}\\in X_{y}}{\\bf s}_{i}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The class-wise global information injected by $\\bar{\\bf w}_{y}$ can act as a regularization to enhance intra-class consistency and mitigate biases that may arise from sample noises. Especially, in the late stage of training, $\\bar{\\bf w}_{y}$ can always provide correct predictions, beneftiing the optimization of w\u00afMSE component. ", "page_idx": 4}, {"type": "table", "img_path": "", "table_caption": ["Table 1: Ablation study on \u201cGI\u201d and \u201cBA\u201d with DKL loss. \u201cGI\u201d represents \u201cInserting Global Information\u201d, and \u201cBA\u201d indicates \u201cBreaking Asymmetric Optimization\u201d. \u201cClean\u201d is the test accuracy of clean images and \u201cAA\u201d is the robustness under Auto-Attack. CIFAR-100 is used for the adversarial training task and ImageNet is adopted for the knowledge distillation task. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "bnZZedw9CM/tmp/e1ac89c2480aa64c8f04bd753ba19bb719510d50eb41c90e1e88750d0ab3a12f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Visualization comparisons. (a) $\\scriptstyle\\mathrm{t-SNE}$ visualization of the model trained by IKL-AT on CIFAR-100; (b) t-SNE visualization of the model trained by TRADES on CIFAR-100. (c) Class margin differences between models trained by IKL-AT and TRADES. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To this end, we derive the IKL loss in Eq. (10) by incorporating the two designs, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{I K L}(x_{m},x_{n})=}&{\\underbrace{\\frac{\\alpha}{4}\\|\\sqrt{S(\\bar{\\mathbf{w}}_{y})}(\\Delta\\mathbf{m}-\\Delta\\mathbf{n})\\|^{2}}_{\\mathrm{weighted}\\,\\mathbf{MSE}\\,(\\bar{\\mathbf{w}}\\mathbf{MSE})}\\underbrace{-\\beta\\cdot S(\\mathbf{s}_{m}^{\\top})\\cdot\\log\\mathbf{s}_{n}}_{\\mathrm{Cross-Entropy}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $y$ is the ground-truth label for $x_{m}$ , $\\bar{\\mathbf{w}}_{y}\\,\\in\\,\\mathbb{R}^{C\\times C}$ is the weights for class $y$ calculated with Eq. (9). ", "page_idx": 5}, {"type": "text", "text": "3.4 A Case Study and Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A Case Study. We empirically examine each component of IKL on CIFAR-100 with the adversarial training task and on ImageNet with the knowledge distillation task. Ablation experimental results and their setting descriptions are listed in Table 1. In the implementation, for adversarial training, we use improved TRADES [71] as our baseline that combines with AWP [66] and uses an increasing epsilon schedule [1]. For knowledge distillation, we use the official code from DKD. The comparison between (a) and (b) shows that DKL can achieve comparable performance, confirming the equivalence to KL. The comparisons among (b), (c), and (d) confirm the effectiveness of the \u201cGI\u201d and \u201cBA\". ", "page_idx": 5}, {"type": "text", "text": "Analysis on Inserting Class-wise Global Information. As evidenced by Table 1, class-wise global information plays an important role in adversarial robustness. The mean probability vector ${\\bar{s}}_{y}$ of all samples in the class $y$ is more robust than the sample-wise probability vector. During training, once the model gives incorrect predictions for hard samples or outliers, ${\\bf w}_{m}$ in Eq. (5) will wrongly guide the optimization. Adoption of $\\bar{\\bf w}_{y}$ in Eq. (10) can mitigate the issue and meanwhile enhance intra-class consistency. ", "page_idx": 5}, {"type": "text", "text": "To visualize the effectiveness of inserting class-wise global information, we define the boundary margin for class $y$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Margin}_{y}=\\bar{s}_{y}[y]-\\operatorname*{max}_{k\\neq y}\\bar{s}_{y}[k].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We plot the margin differences between models trained by IKL-AT and TRADES on CIFAR-100. As shown in Figure 3c, almost all class margin differences are positive, demonstrating that there are larger decision boundary margins for the IKL-AT model. Such larger margins lead to stronger robustness. This phenomenon is coherent with our experimental results in Section 4.1. ", "page_idx": 5}, {"type": "table", "img_path": "bnZZedw9CM/tmp/f8caa131afc8d121b2714ee95a4b40561848d85633761bf01756dd140d911f6c.jpg", "table_caption": ["Table 2: Test accuracy $(\\%)$ of clean images and robustness $(\\%)$ under AutoAttack on CIFAR-100. All results are the average over three trials. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "bnZZedw9CM/tmp/0d84b7c0ba904bbf2f68847f10dc3bdb85c010b76fc517057bf99b29d31e5371.jpg", "table_caption": ["Table 3: Test accuracy $(\\%)$ of clean images and robustness $(\\%)$ ) under AutoAttack on CIFAR-10. Average over three trials are listed. "], "table_footnote": ["We also randomly sample 20 classes in CIFAR-100 for t-SNE visualization. The numbers in the pictures are class indexes. For each sampled class, we collect the feature representation of natural images and adversarial examples with the validation set. The visualization by t-SNE is shown in Figures 3b and 3a. Compared with TRADES that trained with KL loss, features by IKL-AT models are more compact and separable. "], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To verify the effectiveness of our IKL loss, we conduct experiments on CIFAR-10, CIFAR100, and ImageNet for adversarial training (Section 4.1) and knowledge distillation (Sections 4.2 and 4.3). More ablation studies are included in Section 4.4. ", "page_idx": 6}, {"type": "text", "text": "4.1 Adversarial Robustness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental Settings. We use an improved version of TRADES [71] as our baseline, which incorporates AWP [66] and adopts an increasing epsilon schedule. SGD optimizer with a momentum of 0.9 is used. We use the cosine learning rate strategy with an initial learning rate of 0.2 and train models 200 epochs. The batch size is 128, the weight decay is 5e-4 and the perturbation size $\\epsilon$ is set to 8/255. Following previous work [71, 14], standard data augmentation including random crops and random horizontal flip is performed for data preprocessing. Models are trained with 4 Nvidia GeForce 3090 GPUs. ", "page_idx": 6}, {"type": "text", "text": "Under the setting of training with synthesized data by generative models, we strictly follow the training configurations in DM-AT [64] for fair comparisons. Our implementations are based on their open-sourced code. We only replace the KL loss with our IKL loss. ", "page_idx": 6}, {"type": "text", "text": "Datasets and Evaluation. Following previous work [66, 14], CIFAR-10 and CIFAR-100 are used for the adversarial training task. we report the clean accuracy on natural images and adversarial robustness under Auto-Attack [10] with epsilon 8/255. ", "page_idx": 6}, {"type": "table", "img_path": "bnZZedw9CM/tmp/fc4c56c582a2bba0e9fa5cd0fa4ea5fd24063e3ed480a39ae37e4c4c61f37ad6.jpg", "table_caption": ["Table 4: Top-1 accuracy $(\\%)$ on the ImageNet validation and training speed (sec/iteration) comparisons. Training speed is calculated on 4 Nvidia GeForce 3090 GPUs with a batch of 512 224x224 images. All results are the average over three trials. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Comparison Methods. To compare with previous methods, we categorize them into two groups according to the different types of data preprocessing: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Methods with basic augmentation, i.e., random crops and random horizontal flip.   \n\u2022 Methods using augmentation with generative models or Auto-Aug [11], CutMix [69]. ", "page_idx": 7}, {"type": "text", "text": "Comparisons with State-of-the-art on CIFAR-100. On CIFAR-100, with the basic augmentations setting, we compare with AWP, LBGAT, LAS-AT, and ACAT. The experimental results are summarized in Table 2. Our WRN-34-10 models trained with IKL loss do a better trade-off between natural accuracy and adversarial robustness. With ${\\frac{\\alpha}{4}}\\,=\\,5$ and $\\beta\\,=\\,5$ , the model achieves $65.76\\%$ top-1 accuracy on natural images while $31.91\\%$ adversarial robustness under Auto-Attack. An interesting phenomenon is that IKL-AT is complementary to data augmentation strategies, like AutoAug, without any specific designs, which is different from the previous observation that the data augmentation strategy hardly benefits adversarial training [66]. With AutoAug, we obtain $32.53\\%$ adversarial robustness, achieving new state-of-the-art under the setting without extra real or generated data. ", "page_idx": 7}, {"type": "text", "text": "We follow DM-AT [64] to take advantage of synthesized images generated by the popular diffusion models [38]. With 50M generated images, we create new state-of-the-art with WideResNet-28-10, achieving $73.85\\%$ top-1 natural accuracy and $39.18\\%$ adversarial robustness under Auto-Attack. ", "page_idx": 7}, {"type": "text", "text": "Comparison with State-of-the-art on CIFAR-10. Experimental results on CIFAR-10 are listed in Table 3. With the basic augmentation setting, our model achieves $84.80\\%$ top-1 accuracy on natural images and $57.09\\%$ robustness, outperforming AWP by $0.92\\%$ on robustness. With extra generated data, we improve the state-of-the-art by $0.44\\%$ , achieving $67.75\\%$ robustness. ", "page_idx": 7}, {"type": "text", "text": "4.2 Knowledge Distillation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Evaluation. Following previous work [6, 61], we conduct experiments on CIFAR100 [40] and ImageNet [54] to show the advantages of IKL on knowledge distillation. For evaluation, we report top-1 accuracy on CIFAR-100 and ImageNet validation. The training speed of different methods is also discussed. ", "page_idx": 7}, {"type": "text", "text": "Experimental Settings. We follow the experimental settings in DKD. Our implementation for knowledge distillation is based on their open-sourced code. Models are trained with 1 and 8 Nvidia GeForce 3090 GPUs on CIFAR and ImageNet separately. ", "page_idx": 7}, {"type": "text", "text": "Specifically, on CIFAR-100, we train all models for 240 epochs with a learning rate that decayed by 0.1 at the 150th, 180th, and 210th epoch. We initialize the learning rate to 0.01 for MobileNet and ShuffleNet, and 0.05 for other models. The batch size is 64 for all models. We train all models three times and report the mean accuracy. On ImageNet, we use the standard training that trains the model for 100 epochs and decays the learning rate for every 30 epochs. We initialize the learning rate to 0.2 and set the batch size to 512. ", "page_idx": 7}, {"type": "text", "text": "For both CIFAR-100 and ImageNet, we consider the distillation among the architectures having the same unit structures, like ResNet56 and ResNet20, VGGNet13 and VGGNet8. On the other hand, we also explore the distillation among architectures made up of different unit structures, like WideResNet and ShuffleNet, VggNet and MobileNet-V2. ", "page_idx": 7}, {"type": "table", "img_path": "bnZZedw9CM/tmp/04f2ee7a28cc390fe74501fbd2f03481d6497c7e93f6e1835fc2b73716d9eb67.jpg", "table_caption": ["Table 5: Peformance $(\\%)$ on imbalanced data, i.e., the ImageNet-LT. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Comparison Methods. According to the information extracted from the teacher model in distillation training, knowledge distillation methods can be divided into two categories: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Features-based methods [53, 61, 6, 30]. This kind of method makes use of features from different layers of the teacher model, which can need extra parameters and high training computational costs.   \n\u2022 Logits-based methods [33, 73]. This kind of method only makes use of the logits output of the teacher model, which does not require knowing the architectures of the teacher model and thus is more general in practice. ", "page_idx": 8}, {"type": "text", "text": "Comparison with State-of-the-art on CIFAR-100. Experimental results on CIFAR-100 are summarized in Table 13 and Table 14 (in Appendix). Table 13 lists the comparisons with previous methods under the setting that the architectures of the teacher and student have the same unit structures. Models trained by IKL-KD can achieve comparable or better performance in all considered settings. Specifically, we achieve the best performance in 4 out of 6 training settings. Table 14 shows the comparisons with previous methods under the setting that the architectures of the teacher and student have different unit structures. We achieve the best performance in 3 out of 5 training configurations. ", "page_idx": 8}, {"type": "text", "text": "Comparison with State-of-the-art on ImageNet. We empirically show the comparisons with other methods on ImageNet in Table 4. With a ResNet34 teacher, our ResNet18 achieves $71.91\\%$ top-1 accuracy. With a ResNet50 teacher, our MobileNet achieves $72.84\\%$ top-1 accuracy. Models trained by IKL-KD surpass all previous methods while saving ${\\bf38\\%}$ and ${\\bf52\\%}$ computation costs for ResNet34\u2013ResNet18 and ResNet50\u2013MobileNet distillation training respectively when compared with ReviewKD [6]. ", "page_idx": 8}, {"type": "text", "text": "4.3 Knowledge Distillation on Imbalanced Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Data often follows a long-tailed distribution. Tackling the long-tailed recognition problem is essential for real-world applications. Lots of research has contributed to algorithms and theories [3, 19, 37, 47, 17, 16, 18] on the problem. In this work, we examine how the knowledge distillation with our IKL loss affects model performance on imbalanced data, i.e., ImageNet-LT [45]. We train ResNets models 90 epochs with Random-Resized-Crop and horizontal filp as image pre-processing. Following previous work [13], we report the top-1 accuracy on Many-shot, Meidum-shot, Few-shot, and All classes. As shown in Table 5, IKL-KD consistently outperforms KL-KD on imbalanced data. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation on $\\alpha$ and $\\beta$ for Adversarial Robustness. Thanks to the decomposition of the DKL loss formulation, the two components (wMSE and Cross-Entropy) of IKL can be manipulated independently. We empirically study the effects of hyper-parameters of $\\alpha$ and $\\beta$ on CIFAR-100 for adversarial robustness. Clean accuracy on natural data and robustness under AA [10] are reported in Table 7 and Table 8. Reasonable $\\alpha$ and $\\beta$ should be chosen for the best trade-off between natural accuracy and adversarial robustness. ", "page_idx": 8}, {"type": "text", "text": "Ablation on Temperature $\\left(\\tau\\right)$ for Global Information. As discussed in Section 3.3, the incorporated class-wise global information is proposed to promote intra-class consistency and mitigate the biases from sample noises. When calculating the $\\Bar{w}_{y}$ and ${\\bar{s}}_{y}$ , a temperature $\\tau$ could be applied before getting sample probability vectors. We summarize the experimental results in Table 9 for ablation of $\\tau$ . Interestingly, we observe that models usually exhibit higher performance on clean images with a higher $\\tau$ . There are even $5.75\\%$ improvements of clear accuracy while keeping comparable robustness when changing $\\tau=1$ to $\\tau=4$ , which implies the vast importance of weights in wMSE component of DKL/KL for adversarial robustness. To achieve the strongest robustness, we finally choose $\\tau=4$ as illustrated by empirical study. ", "page_idx": 8}, {"type": "table", "img_path": "bnZZedw9CM/tmp/f39a112f2f6f7bf9432a0e9ffcfd6284c602a7a66b47b805a14fd1affad6c1d9.jpg", "table_caption": ["Table 6: Ablation study on hyper-parameters of IKL. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "bnZZedw9CM/tmp/48369c642eec3e46c2420c64388b9813b12ed275db61fe6f6f83a8f582b96f64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Ablation on Various Perturbation Size $\\epsilon$ . We evaluate model robustness with unknown perturbation size $\\epsilon$ in training under Auto-Attack. The experimental results are summarized in Table 10. As shown in Table 10, model robustness decreases significantly as the $\\epsilon$ increases for both the TRADES model and our model. Nevertheless, our model achieves stronger robustness than the TRADES model under all of $\\epsilon$ , outperforming TRADES by $1.34\\%$ on average robustness. The experimental results demonstrate the super advantages of models adversarially trained with our IKL loss. ", "page_idx": 9}, {"type": "text", "text": "Robustness under Other Attacks. Auto-Attack is currently one of the strongest attack methods. It ensembles several adversarial attack methods including APGD-CE, APGD-DLR, FAB, and Square Attack. To show the effectiveness of our IKL loss, we also evaluate our models under PGD and CW attacks with 10 and 20 iterations. The perturbation size and step size are set to 8/255 and 2/255 respectively. As shown in Table 11, with increasing iterations from 10 to 20, our models show similar robustness, demonstrating that our models don\u2019t suffer from obfuscated gradients problem. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have investigated the mechanism of Kullback-Leibler (KL) Divergence loss in terms of gradient optimization. Based on our analysis, we decouple the KL loss into a weighted Mean Square Error (wMSE) loss and a Cross-Entropy loss with soft labels. The new formulation is named Decoupled Kullback-Leibler (DKL) Divergence loss. To address the spotted issues of KL/DKL, we make two improvements that break the asymmetric optimization property and incorporate class-wise global information, deriving the Improved Kullback-Leibler (IKL) Divergence loss. Experimental results on CIFAR-10/100 and ImageNet show that we create new state-of-the-art adversarial robustness and competitive performance on knowledge distillation. This underscores the efficacy of our Innovative KL (IKL) loss technique. The KL loss exhibits a wide range of applications. As part of our future work, we aim to explore and highlight the versatility of IKL in various other scenarios, like robustness on out-of-distribution data, and incremental learning. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sravanti Addepalli, Samyak Jain, and Venkatesh Babu Radhakrishnan. Efficient and effective augmentation strategy for adversarial training. In NeurIPS, 2022.   \n[2] Maxim Berman, Amal Rannen Triki, and Matthew B Blaschko. The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In CVPR, pages 4413\u20134421, 2018.   \n[3] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. NeurIPS, 2019. [4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, NeurIPS, 2020.   \n[5] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, pages 532\u2013547, 2018. [6] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In CVPR, 2021. [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.   \n[8] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. CoRR, 2020.   \n[9] Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In ICCV, pages 4794\u20134802, 2019.   \n[10] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML. PMLR, 2020.   \n[11] Ekin D. Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. In CVPR, 2019.   \n[12] Jiequan Cui, Pengguang Chen, Ruiyu Li, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast and practical neural architecture search. In ICCV, pages 6509\u20136518, 2019.   \n[13] Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and Jiaya Jia. Reslt: Residual learning for long-tailed recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2022.   \n[14] Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. Learnable boundary guided adversarial training. In ICCV, 2021.   \n[15] Jiequan Cui, Yuhui Yuan, Zhisheng Zhong, Zhuotao Tian, Han Hu, Stephen Lin, and Jiaya Jia. Region rebalance for long-tailed semantic segmentation. arXiv preprint arXiv:2204.01969, 2022.   \n[16] Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In ICCV, pages 715\u2013724, 2021.   \n[17] Jiequan Cui, Zhisheng Zhong, Zhuotao Tian, Shu Liu, Bei Yu, and Jiaya Jia. Generalized parametric contrastive learning. arXiv preprint arXiv:2209.12400, 2022.   \n[18] Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, and Hanwang Zhang. Classes are not equal: An empirical study on image recognition fairness. In CVPR, pages 23283\u201323292, 2024.   \n[19] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In CVPR, pages 9268\u20139277, 2019.   \n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[21] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In ICML, pages 1607\u20131616. PMLR, 2018.   \n[22] Sven Gowal, Sylvestre-Alvise Rebuff,i Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A Mann. Improving robustness using generated data. 2021.   \n[23] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent - A new approach to selfsupervised learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, NeurIPS, 2020.   \n[24] Zhiwei Hao, Jianyuan Guo, Kai Han, Han Hu, Chang Xu, and Yunhe Wang. Vanillakd: Revisit the power of vanilla knowledge distillation from small scale to large scale. arXiv preprint arXiv:2305.15781, 2023.   \n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009, 2022.   \n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.   \n[27] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In CVPR, pages 2961\u20132969, 2017.   \n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[29] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.   \n[30] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In ICCV, 2019.   \n[31] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1921\u20131930, 2019.   \n[32] Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In AAAI, pages 3779\u20133787, 2019.   \n[33] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. 2015.   \n[34] Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge distillation from a stronger teacher. In NeurIPS, 2022.   \n[35] Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Las-at: Adversarial training with learnable attack strategy. In CVPR, 2022.   \n[36] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, pages 694\u2013711. Springer, 2016.   \n[37] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.   \n[38] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.   \n[39] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. NeurIPS, 2018.   \n[40] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.   \n[41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[42] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. NeurIPS, 2017.   \n[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.   \n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012\u201310022, 2021.   \n[45] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2537\u20132546, 2019.   \n[46] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[47] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.   \n[48] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In AAAI, pages 5191\u20135198, 2020.   \n[49] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In CVPR, pages 3967\u20133976, 2019.   \n[50] Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning Zhang. Correlation congruence for knowledge distillation. In ICCV, pages 5007\u20135016, 2019.   \n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.   \n[52] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In ICML. PMLR, 2020.   \n[53] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.   \n[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein. ImageNet large scale visual recognition challenge. IJCV, 2015.   \n[55] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.   \n[56] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NeurIPS, 33:596\u2013608, 2020.   \n[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.   \n[58] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan. Equalization loss for long-tailed object recognition. In CVPR, pages 11662\u201311671, 2020.   \n[59] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.   \n[60] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NeurIPS, 30, 2017.   \n[61] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In ICLR, 2020.   \n[62] Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu, Hengshuang Zhao, Bei Yu, Ming-Chang Yang, and Jiaya Jia. Adaptive perspective distillation for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):1372\u20131387, 2022.   \n[63] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In ICLR, 2020.   \n[64] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. arXiv preprint arXiv:2302.04638, 2023.   \n[65] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, pages 499\u2013515. Springer, 2016.   \n[66] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958\u20132969, 2020.   \n[67] Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student optimization in one generation. In CVPR, pages 2859\u20132868, 2019.   \n[68] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In CVPR, pages 4133\u20134141, 2017.   \n[69] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.   \n[70] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.   \n[71] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML. PMLR, 2019.   \n[72] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR, pages 4320\u20134328, 2018.   \n[73] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In CVPR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof to Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To demonstrate that DKL in Eq. (5) is equivalent to KL in Eq. (2) for training optimization, we prove that DKL and KL produce the same gradients when given the same inputs. ", "page_idx": 14}, {"type": "text", "text": "For KL loss, we have the following derivatives according to the chain rule: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\frac{\\partial w_{i}}{\\partial x_{j}}}&{=}&{w_{0}s_{0}-w_{1}s_{0}-\\nu_{1},}\\\\ {\\frac{\\partial w_{i}}{\\partial x_{j}}}&{=}&{\\nu_{1}\\;,}\\\\ {\\frac{\\partial C_{S X}}{\\partial x_{i}}}&{=}&{\\frac{\\varepsilon_{\\mathrm{SX}}}{\\partial x_{i}}\\;\\delta_{i j}^{(i)}+\\;\\delta_{i k}^{\\phantom{\\dagger}}(1-\\kappa_{0}^{i})}\\\\ {\\frac{\\partial C_{S X}}{\\partial\\rho_{0}}}&{=}&{\\frac{\\varepsilon_{\\mathrm{SX}}}{\\partial x_{i}}\\;\\frac{\\partial w_{i}}{\\partial x_{j}}+\\;\\sum_{s=1}^{r}\\frac{\\varepsilon_{\\mathrm{SX}}}{\\partial x_{i}}\\;\\frac{\\partial w_{i}}{\\partial x_{0}}}\\\\ &{=}&{(w_{0}\\varepsilon_{i}^{\\dagger}-\\log_{s}^{\\dagger}+1)\\;\\nu_{i k}^{s}\\;\\frac{\\sqrt{\\nu}_{i k}}{\\rho_{0}}+\\;\\sum_{s=1}^{r}(\\log_{s}^{\\dagger}-\\log_{s}^{\\dagger}+1)\\;\\varepsilon_{\\mathrm{SX}}(s_{i}^{s})}\\\\ &{=}&{\\frac{\\varepsilon}{\\mathrm{SY}}(\\log_{s}^{\\dagger}-\\log_{s}^{\\dagger}-(1\\zeta_{s}^{s}-1)\\log_{s}^{\\dagger})-(\\log_{s}^{\\dagger}-\\log_{s}^{\\dagger}))\\;(\\nu_{i k}^{s}+\\nu_{i k}^{s})}\\\\ &{=}&{\\frac{\\varepsilon_{\\mathrm{SY}}}{\\varepsilon_{\\mathrm{SY}}}((w_{i}-\\log_{s}^{\\dagger}-(\\alpha_{i}^{\\dagger}-\\upsilon_{j}))+(\\nu_{i k}^{s}-\\log_{s}^{\\dagger}))}\\\\ &{=}&{\\frac{\\varepsilon_{\\mathrm{SY}}}{\\varepsilon_{\\mathrm{SY}}}((\\log_{s}-\\Delta_{i k})-\\log_{s})\\;\\nu_{i k}^{s}\\;}\\\\ &{=}&{\\frac{\\varepsilon_{\\mathrm{SY}}}{\\varepsilon_{\\mathrm{SY}}}((\\Delta\\mu_{0},\\mu_{0}-\\Delta_{i k}))\\;\\nu_{i k}\\frac{\\partial w_{i}}{\\partial x\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For DKL los, we expand the Eq. (5) as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}_{D K L}(x_{m},x_{n})}&{=}&{\\displaystyle\\underbrace{\\frac{\\alpha}{4}\\|\\sqrt{S(\\mathbf{w}_{m})}(\\Delta\\mathbf{m}-\\mathcal{S}(\\Delta\\mathbf{n}))\\|^{2}}_{\\mathrm{weighted}\\,\\mathrm{MSE}\\,(\\mathrm{wMSE})}\\underbrace{-\\beta\\cdot\\mathcal{S}(\\mathbf{s}_{m}^{\\top})\\cdot\\log{\\bf s}_{n}}_{\\mathrm{Cross:Eutrepy}}}\\\\ &{=}&{\\displaystyle\\frac{\\alpha}{4}\\sum_{j=1}^{C}\\sum_{k=1}^{C}((\\Delta\\mathbf{m}_{j,k}-\\mathcal{S}(\\Delta\\mathbf{n}_{j,k}))^{2}*\\mathcal{S}(\\mathbf{w}_{m}^{\\prime,k}))-\\beta\\sum_{j=1}^{C}\\mathcal{S}(\\mathbf{s}_{m}^{j})*\\log\\mathbf{s}_{n}^{j},}\\\\ &&{.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the chain rule, we obtain the following equations: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{D K L}}{\\partial\\mathbf{o}_{n}^{i}}\\quad=\\quad\\beta*\\mathbf{s}_{m}^{i}*\\left(\\mathbf{s}_{n}^{i}-1\\right)+\\mathbf{s}_{n}^{i}*\\left(1-\\mathbf{s}_{m}^{i}\\right)}\\\\ {\\frac{\\partial\\mathcal{L}_{D K L}}{\\partial\\mathbf{o}_{m}^{i}}\\quad=\\quad\\frac{\\alpha}{4}*2*\\left(\\sum_{j}(\\Delta\\mathbf{m}_{j,i}-\\Delta\\mathbf{n}_{j,i})*\\left(-\\mathbf{w}_{m}^{j,i}\\right)+\\sum_{k}^{C}(\\Delta\\mathbf{m}_{i,k}-\\Delta\\mathbf{n}_{i,k})*\\mathbf{w}_{m}^{i,k}\\right)}\\\\ {\\quad=\\quad\\alpha*\\sum_{j}(\\Delta\\mathbf{m}_{i,j}-\\Delta\\mathbf{n}_{i,j})*\\mathbf{w}_{m}^{i,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "bnZZedw9CM/tmp/ca78cd1a21b73ca62f84cbb1f00476a42549641470d44e5924abc83621f99ce3.jpg", "table_caption": ["Table 12: New state-of-the-art on public leaderboard RobustBench [10]. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "bnZZedw9CM/tmp/f08a143c1efcbdbd203dc418bac10c1fe700d693dfe0606391b70853097342df.jpg", "table_caption": ["Table 13: Top-1 accuracy $(\\%)$ on the CIFAR-100 validation. Teachers and students are in the same architectures. All results are the average over three trials. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "bnZZedw9CM/tmp/894d7b1ab979b35a568189535965fb024a3529713c12e59be579f32750c3f175.jpg", "table_caption": ["Table 14: Top-1 accuracy $(\\%)$ on the CIFAR-100 validation. Teachers and students are in different architectures. All results are the average over 3 trials. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Comparing Eq. (12) and Eq. (15), Eq. (13) and Eq. (16), we conclude that DKL loss and KL loss have the same derivatives given the same inputs. Thus, DKL loss is equivalent to KL loss in terms of gradient optimization. ", "page_idx": 15}, {"type": "text", "text": "A.2 New state-of-the-art robustness on CIFAR-100/10 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Robustbench is the most popular benchmark for adversarial robust models in the community. It evaluates the performance of models by the Auto-Attack. Auto-Attack [10] is an ensemble of different kinds of attack methods and is considered the most effective method to test the robustness of models. ", "page_idx": 15}, {"type": "text", "text": "We achieve new state-of-the-art robustness on CIFAR-10 and CIFAR-100 under both settings w/ and w/o generated data. As shown in Table 12, on CIFAR-100 without extra generated data, we achieve $32.53\\%$ robustness, outperforming the previous best result by $\\mathbf{0.68\\%}$ while saving $33.3\\%$ computational cost. With generated data, our model boosts performance to $73.85\\%$ natural accuracy, surpassing the previous best result by $1.27\\%$ while maintaining the strongest robustness. More detailed comparisons can be accessed on the public leaderboard https://robustbench.github. io/. ", "page_idx": 15}, {"type": "text", "text": "Table 15: Comparisons with strong training settings on ImageNet for knowledge distillation. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "bnZZedw9CM/tmp/289fb61b992ba1881ed37747cba99db527d58c1d2e3094916160f036703dab04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Comparisons on CIFAR-100 for Knowledge Distillation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We experiment on CIFAR-100 with the following cases: 1) the teacher and student models have the same unit network architectures; 2) the teacher and student models have different unit network architectures. The results are listed in Table 13 and Table 14. We have achieved the best results in 4 out of 6 and 3 out of 5 experimental settings respectively. ", "page_idx": 16}, {"type": "text", "text": "Moreover, we follow the concurrent work [24] and conduct experiments with BEiT-Large as the teacher and ResNet-50 as the student under a strong training scheme, the experimental results are summarized in Table 15. The model trained by IKL-KD shows slightly better results. ", "page_idx": 16}, {"type": "text", "text": "A.4 Other Applications with IKL ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Semisupervised learning. We use the open-sourced code from https://github.com/ microsoft/Semi-supervised-learning and conduct semi-supervised experiments on CIFAR100 with FixMatch and Mean-Teacher methods. Specifically, each class has 2 labeled images and 500 unlabeled images. All default training hyper-parameters are used for fair comparisons. We only replace the consistency loss with our IKL loss. As shown in Table 16, with our IKL loss, the Mean-Teacher method even surpasses the FixMatch. ", "page_idx": 16}, {"type": "table", "img_path": "bnZZedw9CM/tmp/ba05481ccc4856da199dfa74c225d3b3d6ebea0cfff7289e0299ac04ab3984b3.jpg", "table_caption": ["Table 16: Semi-supervised Learning on CIFAR-100 with ViT-small backbone. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Semantic segmentation distillation. We conduct ablation on the semantic segmentation distillation task. We use the APD [62] as our baseline for their open-sourced code. All default hyper-parameters are adopted. We only replace the original KL loss with our IKL loss. As shown in Table 17, we achieve better performance with the IKL loss function, demonstrating that the IKL loss can be complementary to other techniques in semantic segmentation distillation. ", "page_idx": 16}, {"type": "table", "img_path": "bnZZedw9CM/tmp/40863bf347fe2688f0c2ad8a15e7aebf96e365b7d250cd1dfbffb19953df3654.jpg", "table_caption": ["Table 17: Semantic segmentation distillation with APD on ADE20K. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.5 Complexity of IKL ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Compared with the KL divergence loss, IKL loss is required to update the global class-wise prediction scores $W\\,\\in\\,\\mathbb{R}^{C\\times C}$ where $C$ is the number of classes during training. This extra computational cost can be nearly ignored when compared with the model forward and backward. Algorithm 1 ", "page_idx": 16}, {"type": "text", "text": "shows the implementation of our IKL loss in Pytorch style. On dense prediction tasks like semantic segmentation, $\\Delta_{a}$ and $\\Delta_{b}$ can require large GPU Memory. Here, we also provide the memory-efficient implementations for $w$ MSE loss component, which is listed in Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "bnZZedw9CM/tmp/962fb14205d42cd96b9be0dcd6aab1557f88a19dbf7f917de378b7e81bb9cbe5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Memory efficient implementation for wMSE_loss in Pytorch style. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: logitsa, $l o g i t s_{b}\\in\\mathbb{R}^{B\\times C}$ , one-hot label Y , $W\\in\\mathbb{R}^{C\\times C}$ ;   \nclass_scores $=$ one-hot $@$ W;   \nloss_a $=$ (class_scores \\* logitsa \\* logitsa).sum(dim=1) \\* 2 - torch.pow((class_scores \\* $l o g i t s_{a}).\\mathrm{sum}(\\mathrm{dim}{=}1),2)\\ '2;$ ;   \nloss_ $\\mathbf{\\Phi}_{)}\\,=\\,\\mathbf{\\Phi}(\\mathbf{\\Phi}$ class_scores \\* logitsb \\* logitsb).sum(dim $^{=1}$ ) \\* 2 - torch.pow((class_scores \\* logitsb) $.\\mathrm{sum}(\\mathrm{dim}{=}1),2)\\ '2$ ;   \nloss_ex $=$ (class_scores \\* logitsa \\* logitsb).sum(dim $_{|=1}$ ) \\* 4 - (class_scores \\* logitsa).sum(dim=1) \\* (class_scores \\* logitsb).sum(dim=1) \\* 4;   \n$\\mathrm{wMSE\\_loss}=\\frac{1}{4}*(\\mathrm{loss\\_a}+\\mathrm{loss\\_b}-\\mathrm{loss\\_ex})$ ).mean();   \nreturn wMSE_loss. ", "page_idx": 17}, {"type": "text", "text": "A.6 Connection between IKL and the Jensen-Shannon (JS) Divergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "With the following JS divergence loss, ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ S D(P||Q)=\\frac{1}{2}K L(P||M)+\\frac{1}{2}K L(Q||M),\\quad M=\\frac{1}{2}P+\\frac{1}{2}Q.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We calculate its derivatives regarding $o_{n}$ (the student logits), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\frac{\\partial\\mathcal{L}_{J S D}}{\\partial\\mathbf{o}_{n}^{i}}}&{=}&{\\displaystyle\\sum_{j=1}^{C}\\mathbf{w}_{n}^{i,j}(\\Delta\\mathbf{n}_{i,j}-\\Delta\\mathbf{m}^{\\prime}{}_{i,j})}\\\\ {\\displaystyle S o f t m a x(o_{m^{\\prime}})}&{=}&{\\displaystyle\\frac{1}{2}s_{n}+\\frac{1}{2}s_{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{o}_{m}$ is the logits from the teacher model, ${\\bf o}_{m^{\\prime}}$ is a virtual logits satisfying Eq. (19), $\\mathbf{s}_{m}=$ Softmax $\\left(\\mathbf{o}_{m}\\right)$ , sn = Softmax(on), $\\Delta\\mathbf{m}^{\\prime}{}_{i,j}=\\mathbf{o}_{m^{\\prime}}^{i}-\\mathbf{o}_{m^{\\prime}}^{j}$ , $\\Delta\\mathbf{n}_{i,j}=\\mathbf{o}_{n}^{i}-\\mathbf{o}_{n}^{j}$ . ", "page_idx": 17}, {"type": "text", "text": "Correspondingly, the derivatives of IKL loss regrading $o_{n}$ (the student logits), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{I K L}}{\\partial\\mathbf{o}_{n}^{i}}=\\alpha\\sum_{j=1}^{C}\\mathbf{w}_{m}^{i,j}(\\Delta\\mathbf{n}_{i,j}-\\Delta\\mathbf{m}_{i,j})+\\underbrace{\\beta*\\mathbf{s}_{m}^{i}*(\\mathbf{s}_{n}^{i}-1)+\\mathbf{s}_{n}^{i}*(1-\\mathbf{s}_{m}^{i})}_{\\mathrm{Fffecte~of~Cross.Fntranv}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Compared with IKL loss, the problem for JSD divergence in knowledge distillation is that: The soft labels from the teacher models often embed dark knowledge and facilitate the optimization of the student models. However, there are no effects of the cross-entropy loss with the soft labels from the teacher model, which can be the underlying reason that JSD is worse than KD and IKL-KD. ", "page_idx": 17}, {"type": "text", "text": "As shown in Table 18, we also empirically demonstrate that IKL loss performs better than JSD divergence on the knowledge distillation task. ", "page_idx": 17}, {"type": "table", "img_path": "bnZZedw9CM/tmp/6db22225f96ba702d192b0290c87c34b09e58a533c5bacfd92711e7d153c8712.jpg", "table_caption": ["Table 18: Comparisons between KL, IKL, and JSD on ImageNet-LT. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.7 Licenses ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All the datasets we considered are publicly available, we list their licenses and URLs as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 CIFAR-10 [41]: MIT License, https://www.cs.toronto.edu/\\~kriz/cifar.html.   \n\u2022 CIFAR-100 [41]: MIT License, https://www.cs.toronto.edu/\\~kriz/cifar.html.   \n\u2022 ImageNet [54]: Non-commercial, http://image-net.org. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We theoretically prove that the Kullback-Leibler (KL) Divergence loss is equivalent to a Decoupled Kullback-Leibler (DKL) loss regarding gradient optimization. Based on this analysis, we improve KL/DKL loss on adversarial training and knowledge distillation tasks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The conclusion and limitation section is included. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors ", "page_idx": 18}, {"type": "text", "text": "should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our proof to Theorem 1 is in Appendix A.1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Section 4. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same ", "page_idx": 19}, {"type": "text", "text": "dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 20}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide a link for our code and models in the paper. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Section 4 and ablations for hyper-parameters in the Appendix 4.4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Error bars are included in Table 14 of the Appendix. Due to the heavy computation, other tables don\u2019t provide error bars. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: On adversarial training, Each run with basic augmentations takes around 2 days using 4GPUs while 5 days using 8 GPUs for adversarial training with generated data. On knowledge distillation, 8 Nvidia GeForce 3090 GPUs are used on ImageNet. Each run takes about 1 day for our method. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our research conforms NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper is fundamental research in adversarial robustness and knowledge distillation and there is no obvious societal impact. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our models don\u2019t suffer from a high risk for misuse. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Proper citations are added to the paper. Licenses for used data is included in Appendix A.7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing or human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing or human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]