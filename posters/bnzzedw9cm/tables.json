[{"figure_path": "bnZZedw9CM/tables/tables_6_1.jpg", "caption": "Table 2: Test accuracy (%) of clean images and robustness (%) under AutoAttack on CIFAR-100. All results are the average over three trials.", "description": "This table presents the performance of various adversarial training methods on the CIFAR-100 dataset.  The \"Clean\" column shows the accuracy on clean images, while the \"AA\" column shows the robustness against the AutoAttack, a strong adversarial attack.  The table compares different methods (AWP, LBGAT, LAS-AT, ACAT, IKL-AT, and DM-AT), architectures (WRN-34-10 and WRN-28-10), and augmentation types (Basic and AutoAug) to show the impact of each on the model's robustness.  The results using generated data are also included.", "section": "4.1 Adversarial Robustness"}, {"figure_path": "bnZZedw9CM/tables/tables_6_2.jpg", "caption": "Table 3: Test accuracy (%) of clean images and robustness (%) under AutoAttack on CIFAR-10. Average over three trials are listed.", "description": "This table presents the performance comparison of different adversarial training methods on the CIFAR-10 dataset.  The evaluation metrics are clean test accuracy and robustness against the AutoAttack, which is a strong, comprehensive adversarial attack.  The results are averaged over three independent trials to provide a reliable estimate of performance.  The table shows that IKL-AT achieves competitive results compared to other methods under different augmentation strategies.", "section": "4.1 Adversarial Robustness"}, {"figure_path": "bnZZedw9CM/tables/tables_7_1.jpg", "caption": "Table 4: Top-1 accuracy (%) on the ImageNet validation and training speed (sec/iteration) comparisons. Training speed is calculated on 4 Nvidia GeForce 3090 GPUs with a batch of 512 224x224 images. All results are the average over three trials.", "description": "This table compares the Top-1 accuracy and training speed of different knowledge distillation methods on the ImageNet validation set.  It shows results for various teacher and student model architectures using different distillation techniques (features-based and logits-based) with and without extra parameters.  The training speed is measured using 4 Nvidia GeForce 3090 GPUs with a batch size of 512. Results are averaged over three trials.", "section": "4.2 Knowledge Distillation"}, {"figure_path": "bnZZedw9CM/tables/tables_8_1.jpg", "caption": "Table 5: Peformance (%) on imbalanced data, i.e., the ImageNet-LT.", "description": "This table presents the performance comparison of different knowledge distillation methods on the ImageNet-LT dataset, which is an imbalanced dataset.  It shows the top-1 accuracy for each method broken down by the number of samples in each class (Many, Medium, Few) and overall (All). The methods compared include baseline models without knowledge distillation and those using KL-KD and IKL-KD.", "section": "4.3 Knowledge Distillation on Imbalanced Data"}, {"figure_path": "bnZZedw9CM/tables/tables_9_1.jpg", "caption": "Table 1: Ablation study on \u201cGI\u201d and \u201cBA", "description": "This table presents the results of an ablation study conducted to evaluate the impact of two modifications, 'Inserting Global Information' (GI) and 'Breaking Asymmetric Optimization' (BA), on the performance of the DKL loss.  It shows the test accuracy on clean images and robustness under AutoAttack on CIFAR-100 for the adversarial training task and ImageNet for the knowledge distillation task.  The table allows for comparison between different configurations of GI and BA to determine their individual contributions to the improvement provided by the modified loss.", "section": "3.4 A Case Study and Analysis"}, {"figure_path": "bnZZedw9CM/tables/tables_9_2.jpg", "caption": "Table 10: Ablation study of e.", "description": "This ablation study investigates the impact of the hyperparameter epsilon (e), representing the perturbation size in adversarial training, on the performance of the IKL-AT and TRADES methods.  It shows clean test accuracy and AutoAttack robustness for different epsilon values, demonstrating how the choice of epsilon affects the model's robustness.  The average robustness across all epsilon values is compared between IKL-AT and TRADES. ", "section": "4.1 Adversarial Robustness"}, {"figure_path": "bnZZedw9CM/tables/tables_15_1.jpg", "caption": "Table 12: New state-of-the-art on public leaderboard RobustBench [10].", "description": "This table presents the state-of-the-art results achieved by the proposed IKL-AT method on the RobustBench public leaderboard. It compares the performance of IKL-AT with previous state-of-the-art methods under different experimental settings, including with and without generated data, and with basic and AutoAug augmentation strategies.  The table highlights the improved clean accuracy and adversarial robustness achieved by IKL-AT, along with the computational savings gained.", "section": "4.1 Adversarial Robustness"}, {"figure_path": "bnZZedw9CM/tables/tables_15_2.jpg", "caption": "Table 13: Top-1 accuracy (%) on the CIFAR-100 validation. Teachers and students are in the same architectures. All results are the average over three trials.", "description": "This table presents the Top-1 accuracy results of knowledge distillation experiments performed on the CIFAR-100 validation dataset.  The experiments compare different knowledge distillation methods (FitNet, RKD, CRD, OFD, ReviewKD, DKD, KD, IKL-KD) using various teacher-student model architectures where both teacher and student models have the same network architecture. The results are averaged across three independent trials.", "section": "4.2 Knowledge Distillation"}, {"figure_path": "bnZZedw9CM/tables/tables_15_3.jpg", "caption": "Table 14: Top-1 accuracy (%) on the CIFAR-100 validation. Teachers and students are in different architectures. All results are the average over 3 trials.", "description": "This table presents the Top-1 accuracy results of knowledge distillation experiments conducted on the CIFAR-100 validation dataset.  The key aspect is that it compares different knowledge distillation methods (FitNet, RKD, CRD, OFD, ReviewKD, DKD, KD, and IKL-KD) across various teacher-student architecture pairings.  The teacher and student models used in each experiment belong to different architectural families (ResNet32x4, WRN-40-2, VGG13, ResNet50, ResNet32x4, ShuffleNet-V1, MobileNet-V2, ShuffleNet-V2). This allows for a comprehensive comparison of the effectiveness of different distillation methods when dealing with architectural heterogeneity. The table reports the average top-1 accuracy over three independent trials.", "section": "4.2 Knowledge Distillation"}, {"figure_path": "bnZZedw9CM/tables/tables_16_1.jpg", "caption": "Table 15: Comparisons with strong training settings on ImageNet for knowledge distillation.", "description": "This table presents a comparison of the top-1 accuracy achieved by different knowledge distillation methods on the ImageNet dataset.  The methods compared include KD, DKD, DIST, and the proposed IKL-KD.  The strong training settings used likely involved techniques like data augmentation and larger batch sizes to push model performance.", "section": "4.2 Knowledge Distillation"}, {"figure_path": "bnZZedw9CM/tables/tables_16_2.jpg", "caption": "Table 16: Semi-supervised Learning on CIFAR-100 with ViT-small backbone.", "description": "This table presents the results of semi-supervised learning experiments conducted on the CIFAR-100 dataset using a ViT-small backbone.  Two different methods, FixMatch and Mean-Teacher, were employed, each with variations in pseudo-labeling techniques (hard or soft) and consistency loss functions (cross-entropy, KL loss, or IKL loss). The table shows the top-1 accuracy achieved in the last epoch of training for each configuration.  The results highlight the effectiveness of using soft pseudo-labels and the IKL loss for improved performance in semi-supervised learning.", "section": "A.4 Other Applications with IKL"}, {"figure_path": "bnZZedw9CM/tables/tables_16_3.jpg", "caption": "Table 17: Semantic segmentation distillation with APD on ADE20K.", "description": "This table presents the results of semantic segmentation distillation experiments using the APD method on the ADE20K dataset.  It compares the performance of three approaches: a baseline using only a student network, APD with KL loss, and APD with the proposed IKL loss.  The table shows the teacher and student network architectures used, and the resulting teacher and student mIoU (mean Intersection over Union) scores, demonstrating that IKL loss improves upon KL loss in this context.", "section": "A.4 Other Applications with IKL"}, {"figure_path": "bnZZedw9CM/tables/tables_17_1.jpg", "caption": "Table 1: Ablation study on \u201cGI\u201d and \u201cBA", "description": "This ablation study investigates the effects of two modifications to the Decoupled Kullback-Leibler (DKL) loss: \"Inserting Global Information\" (GI) and \"Breaking Asymmetric Optimization\" (BA). It evaluates the impact of these modifications on the performance of adversarial training and knowledge distillation using CIFAR-100 and ImageNet datasets, respectively. Results are presented for different combinations of GI and BA, showing the test accuracy on clean images and the robustness under Auto-Attack (AA) for adversarial training, and the Top-1 accuracy for knowledge distillation.", "section": "3.3 Improved Kullback-Leibler Divergence Loss"}, {"figure_path": "bnZZedw9CM/tables/tables_18_1.jpg", "caption": "Table 18: Comparisons between KL, IKL, and JSD on ImageNet-LT.", "description": "This table compares the performance of KL, IKL, and JSD methods on the ImageNet-LT dataset, which is an imbalanced dataset. The comparison is done for both self-distillation and knowledge distillation tasks. For self-distillation, ResNet-50 is used as both teacher and student. For knowledge distillation, ResNet-50 is used as the student and ResNeXt-101 is used as the teacher. The results show that IKL outperforms KL and JSD for both tasks.", "section": "A.4 Other Applications with IKL"}]