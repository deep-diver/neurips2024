[{"figure_path": "06Vt6f2js7/figures/figures_0_1.jpg", "caption": "Figure 1: Diverse visual content generated by SyncTweedies: A diffusion synchronization process applicable to various downstream tasks without finetuning.", "description": "This figure showcases various examples of visual content generated using the SyncTweedies method.  It demonstrates the model's ability to generate diverse outputs, including ambiguous images (like different viewpoints of a single object), panoramic images (360\u00b0 views), 3D mesh textures (applying textures onto 3D models), and 3D Gaussian splat textures. Notably, the method achieves this without requiring any fine-tuning for specific visual content, highlighting its generality and efficiency.", "section": "Abstract"}, {"figure_path": "06Vt6f2js7/figures/figures_3_1.jpg", "caption": "Figure 2: Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables {wi}, while the right diagram illustrates directly denoising a canonical variable z.", "description": "This figure illustrates the different ways to synchronize multiple diffusion processes.  The left side shows how denoising happens individually in multiple instance spaces (Wi) and are synchronized in the canonical space (Z).  The right side shows denoising directly in the canonical space. The figure helps visualize the three main cases (Case 1, Case 2, Case 3) of diffusion synchronization described in the paper.", "section": "3 Diffusion Synchronization"}, {"figure_path": "06Vt6f2js7/figures/figures_3_2.jpg", "caption": "Figure 2: Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables {wi}, while the right diagram illustrates directly denoising a canonical variable z.", "description": "The figure shows two diagrams illustrating the processes of diffusion synchronization.  The left diagram (a) shows the denoising process applied to instance variables (wi).  The right diagram (b) shows the denoising process applied directly to a canonical variable z.  The diagrams illustrate the different approaches to synchronizing multiple diffusion processes.", "section": "3 Diffusion Synchronization"}, {"figure_path": "06Vt6f2js7/figures/figures_5_1.jpg", "caption": "Figure 3: Qualitative results of ambiguous image generation. While all diffusion synchronization processes show identical results with 1-to-1 projections, Case 1, Case 3 and Visual Anagrams [18] (Case 4) exhibit degraded performance when the projections are 1-to-n. Notably, SyncTweedies can be applied to the widest range of projections, including n-to-1 projections, where Case 5 fails to generate plausible outputs.", "description": "This figure shows the qualitative results of ambiguous image generation using different diffusion synchronization methods.  The results demonstrate the performance of each method with 1-to-1, 1-to-n, and n-to-1 projections.  SyncTweedies shows the best performance across different projection types, especially with n-to-1 projections, where other methods fail to generate satisfactory results.", "section": "3.4 Comparison Across the Diffusion Synchronization Processes"}, {"figure_path": "06Vt6f2js7/figures/figures_14_1.jpg", "caption": "Figure 6: Qualitative results of 3D mesh texturing. SyncTweedies and SyncMVD [35] exhibit comparable results, outperforming other baselines. Finetuning-based method [63] produces images without fine details as it was trained on a dataset with coarse texture images. The optimization-based method [62] tends to produce unrealistic and high saturation textures, while iterative-view-updating-based methods [10, 44] show view inconsistencies.", "description": "This figure shows the qualitative results of 3D mesh texturing using different methods.  SyncTweedies and SyncMVD generate the most realistic results, while finetuning-based methods (Paint3D) lack detail, optimization-based methods (Paint-it) have unnatural colors and high contrast, and iterative-view-updating methods (TEXTURE, Text2Tex) show inconsistencies across views.", "section": "A Qualitative Results"}, {"figure_path": "06Vt6f2js7/figures/figures_15_1.jpg", "caption": "Figure 7: Qualitative results of depth-to-360-panorama generation. SyncTweedies and Case 5 generate consistent and high-fidelity panoramas as observed in the 1-to-n projection experiment in Section 3.4.3. MVDiffusion [56] fails to generalize to out-of-domain scenes and generates suboptimal panoramas.", "description": "This figure displays the qualitative results of generating 360\u00b0 panoramas from depth maps using different diffusion synchronization methods.  SyncTweedies and Case 5 demonstrate superior performance, producing realistic and detailed panoramas. In contrast, MVDiffusion [56], a finetuned model, struggles to generate realistic images for scenes it wasn't specifically trained on.  Other methods (Cases 1, 3, and 4) show significant artifacts and reduced visual quality.", "section": "A.2 Depth-to-360-Panorama Generation"}, {"figure_path": "06Vt6f2js7/figures/figures_16_1.jpg", "caption": "Figure 8: Qualitative results of 3D Gaussian splats [26] texturing. [S*] is a prefix prompt. We use \"Make it to\" for IN2N [19] and \"A photo of\" for the other methods. Case 5 tends to lose details due to the variance reduction issue, whereas SyncTweedies generates realistic images by avoiding this issue. The optimization-based methods [41, 52] produce high contrast, unnatural colors, and the iterative view updating method [19] yields suboptimal outputs due to error accumulation.", "description": "This figure shows the qualitative results of 3D Gaussian splats texturing using different methods. SyncTweedies outperforms other methods in terms of generating high-fidelity results with intricate details, while other methods like optimization-based methods show artifacts and iterative view updating methods fail to preserve fine details.", "section": "A Qualitative Results"}, {"figure_path": "06Vt6f2js7/figures/figures_17_1.jpg", "caption": "Figure 9: Illustration of unprojection and aggregation operation. The figure shows the synchronization process using the 3D mesh texturing application as an example. The left figure depicts the unprojection operation, where the instance space variables are unprojected into the canonical space. The right figure illustrates the aggregation operation, where the unprojected samples are averaged in the canonical space.", "description": "This figure illustrates the unprojection and aggregation operations in the SyncTweedies framework. The unprojection operation maps data points from instance spaces (multiple views of a 3D mesh) back to the canonical space (the texture image). The aggregation operation averages these data points to obtain a consistent representation in the canonical space.", "section": "B Details on Experiments"}, {"figure_path": "06Vt6f2js7/figures/figures_21_1.jpg", "caption": "Figure 10: Qualitative results of arbitrary-sized image generation. All diffusion synchronization processes generate identical results in the 1-to-1 projection.", "description": "The figure shows the qualitative results of applying different diffusion synchronization processes to generate arbitrary-sized images using a 1-to-1 projection.  It demonstrates that the results across all five methods (Case 1, SyncTweedies (Case 2), MultiDiffusion [5], Case 3, Case 4, and Case 5) are visually identical for this specific projection type.  This supports the paper's findings about the mathematical equivalence of the methods under these conditions.", "section": "C Arbitray-Sized Image Generation"}, {"figure_path": "06Vt6f2js7/figures/figures_22_1.jpg", "caption": "Figure 1: Diverse visual content generated by SyncTweedies: A diffusion synchronization process applicable to various downstream tasks without finetuning.", "description": "This figure showcases the diverse visual content generated by the SyncTweedies model.  It demonstrates the model's ability to generate various image types such as 360\u00b0 panoramas, 3D mesh textures, and 3D Gaussian splat textures. Importantly, the model achieves this without requiring any additional fine-tuning for each specific downstream task, highlighting its generalizability and efficiency.", "section": "Abstract"}, {"figure_path": "06Vt6f2js7/figures/figures_22_2.jpg", "caption": "Figure 1: Diverse visual content generated by SyncTweedies: A diffusion synchronization process applicable to various downstream tasks without finetuning.", "description": "This figure showcases various examples of visual content generated using the SyncTweedies framework.  The top row shows a 360-degree panorama created from an input depth map. The middle row demonstrates texturing a 3D mesh model with different objects. The bottom row shows textures generated for 3D Gaussian splat models. The diversity of generated images highlights the framework's broad applicability across different visual content generation tasks, even without fine-tuning.", "section": "Abstract"}, {"figure_path": "06Vt6f2js7/figures/figures_23_1.jpg", "caption": "Figure 13: Qualitative results of 3D mesh texture editing. We edit the textures of the 3D meshes generated from Genies [1] using SyncTweedies.", "description": "This figure shows three examples of 3D mesh texture editing using the SyncTweedies method.  The input is a 3D model generated by Genies [1], and SyncTweedies is used to edit the texture of the model. Each row shows the original texture, and three variations created by SyncTweedies in response to a new text prompt. The results demonstrate SyncTweedies' ability to generate high-quality, diverse, and realistic texture edits on 3D models.", "section": "E 3D Mesh Texture Editing and Diversity"}, {"figure_path": "06Vt6f2js7/figures/figures_23_2.jpg", "caption": "Figure 14: Diversity comparison. Optimization-based method Paint-it [62] (Left) and diffusion-synchronization-based method, SyncTweedies (Right). SyncTweedies generates more diverse images.", "description": "This figure compares the diversity of images generated by the optimization-based method Paint-it and the proposed diffusion synchronization method, SyncTweedies.  Both methods are applied to generate textures for the same 3D objects (clock, axe, rabbit, handbag). The images produced by Paint-it show less variation in color and texture, while those produced by SyncTweedies display a much wider range of styles and appearances, highlighting SyncTweedies' superior ability to generate diverse visual content.", "section": "E 3D Mesh Texture Editing and Diversity"}, {"figure_path": "06Vt6f2js7/figures/figures_26_1.jpg", "caption": "Figure 3: Qualitative results of ambiguous image generation. While all diffusion synchronization processes show identical results with 1-to-1 projections, Case 1, Case 3 and Visual Anagrams [18] (Case 4) exhibit degraded performance when the projections are 1-to-n. Notably, SyncTweedies can be applied to the widest range of projections, including n-to-1 projections, where Case 5 fails to generate plausible outputs.", "description": "This figure shows qualitative results comparing different diffusion synchronization methods for generating ambiguous images.  The results demonstrate that while all methods perform similarly with simple 1-to-1 projections (one input to one output), SyncTweedies outperforms the others with more complex projection scenarios (one-to-many and many-to-one).  This highlights SyncTweedies' robustness and broader applicability.", "section": "3.4 Comparison Across the Diffusion Synchronization Processes"}, {"figure_path": "06Vt6f2js7/figures/figures_26_2.jpg", "caption": "Figure 2: Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables {wi}, while the right diagram illustrates directly denoising a canonical variable z.", "description": "This figure illustrates the five different approaches to diffusion synchronization. The left side shows the denoising process for individual instance spaces, while the right side illustrates the denoising process directly in the canonical space.  Each approach varies in when the aggregation of results from multiple instance spaces occurs (before noise prediction, after Tweedie's formula approximation, or after the final deterministic denoising step), leading to different characteristics and performance.", "section": "3 Diffusion Synchronization"}, {"figure_path": "06Vt6f2js7/figures/figures_26_3.jpg", "caption": "Figure 2: Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables {wi}, while the right diagram illustrates directly denoising a canonical variable z.", "description": "This figure illustrates the five different diffusion synchronization processes discussed in the paper. The left side shows the process of denoising in the instance spaces, while the right side shows the process of directly denoising in the canonical space. The figure highlights the different ways in which the denoising processes can be synchronized, depending on the timing of the aggregation operation.", "section": "3 Diffusion Synchronization"}]