[{"type": "text", "text": "SyncTweedies: A General Generative Framework Based on Synchronized Diffusions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jaihoon Kim\u2217 Juil Koo\u2217 Kyeongmin Yeo\u2217 Minhyuk Sung KAIST {jh27kim,63days,aaaaa,mhsung}@kaist.ac.kr ", "page_idx": 0}, {"type": "image", "img_path": "06Vt6f2js7/tmp/f17e0e06d9d061546a1744a3b926ef7166f07c27abc20357b1051708f1db3c00.jpg", "img_caption": ["Figure 1: Diverse visual content generated by SyncTweedies: A diffusion synchronization process applicable to various downstream tasks without finetuning. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a general diffusion synchronization framework for generating diverse visual content, including ambiguous images, panorama images, 3D mesh textures, and 3D Gaussian splats textures, using a pretrained image diffusion model. We first present an analysis of various scenarios for synchronizing multiple diffusion processes through a canonical space. Based on the analysis, we introduce a synchronized diffusion method, SyncTweedies, which averages the outputs of Tweedie\u2019s formula while conducting denoising in multiple instance spaces. Compared to previous work that achieves synchronization through finetuning, SyncTweedies is a zero-shot method that does not require any finetuning, preserving the rich prior of diffusion models trained on Internet-scale image datasets without overfitting to specific domains. We verify that SyncTweedies offers the broadest applicability to diverse applications and superior performance compared to the previous state-of-the-art for each application. Our project page is at https://synctweedies.github.io. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Image diffusion models [47, 38] have shown unprecedented ability to generate plausible images that are indistinguishable from real ones. The generative power of these models stems not only from their capacity to learn from a vast diversity of potential data but also from being trained on Internet-scale image datasets [49, 51]. ", "page_idx": 1}, {"type": "text", "text": "Our goal is to expand the capabilities of pretrained image diffusion models to produce a wide range of 2D and 3D visual content, including panoramic images and textures for 3D objects, as shown in Figure 1, without the need to train diffusion models for each specific visual content. Despite the existence of general image datasets on the scale of billions [49], collecting other forms of visual data at this scale is not feasible. Nonetheless, most visual content can be converted into a regular image of a specific size through certain mappings, such as projecting for panoramic images and rendering for textures of 3D objects. Thus, we employ such a bridging function between each type of visual content and images, along with pretrained image diffusion models [47, 38]. ", "page_idx": 1}, {"type": "text", "text": "We introduce a general generative framework that generates data points in the desired visual content space\u2014referred to as canonical space\u2014by combining the denoising process of diffusion models in the conventional image space\u2014referred to as instance spaces. Given the bridging functions connecting the canonical space and instance spaces, we first explore performing individual denoising processes in each instance space while synchronizing them in the canonical space via the mapping. Another approach is to denoise directly in the canonical space, although it is not immediately feasible due to the absence of diffusion models trained on the canonical space. We investigate redirecting the noise prediction to the instance spaces but aggregating the outputs later in the canonical space. ", "page_idx": 1}, {"type": "text", "text": "Depending on the timing of aggregating the outputs of computation in the instance spaces, we identify five main possible options for the diffusion synchronization processes. Previous works [5, 18, 35] have investigated each of the possible cases only for specific applications, and none of them have analyzed and compared them across a range of applications. For the first time, we present a general framework for diffusion synchronization processes, within which the previous works [5, 18, 35] are contextualized as specific cases. We then present extensive analyses of different choices of diffusion synchronization processes. Based on the analyses, we demonstrate that the approach, which conducts denoising processes in instance spaces (not the canonical space) and synchronizes the outputs of Tweedie\u2019s formula [46] in the canonical space, provides the broadest applicability across a range of applications and the best performance. We name this approach SyncTweedies and showcase its superior performance in multiple visual content creation tasks compared with previous state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "Previous works [56, 34, 52, 63] finetune pretrained diffusion models to generate new types of outputs such as $360^{\\circ}$ panorama images and 3D mesh texture images. However, this approach requires a large quantity of target content for high-quality outputs which is prohibitively expensive to acquire. When it comes to generating visual content that can be parameterized into an image, a notable zero-shot approach not utilizing diffusion synchronization is Score Distillation Sampling (SDS) [41], which has shown particular effectiveness in 3D generation and texturing [31, 60, 62, 37]. However, this alternative application of diffusion models has been observed to produce suboptimal results and also requires a high CFG [22] weight for convergence, leading to over-saturation. For 3D mesh texture generation, specifically, an approach that iteratively updates each view image has also been explored in multiple previous works [10, 44, 8, 23, 17]. However, the accumulation of errors over iterations has been identified as a challenge. We demonstrate that our diffusion-synchronization-based approach outperforms these methods in terms of generation quality across various applications. ", "page_idx": 1}, {"type": "text", "text": "Overall, our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose, for the first time, a general generative framework for diffusion synchronization processes.   \n\u2022 Through extensive analyses of various options for diffusion synchronization processes, including previous works [35, 18, 5, 33], we identify that the approach which synchronizes the outputs of Tweedie\u2019s formula and performs denoising in the instance space, SyncTweedies, offers the broadest applicability and superior performance.   \n\u2022 In our experiments, we verify the superior performance and versatility of SyncTweedies across diverse applications, including texturing on 3D meshes and Gaussian Splats [26], and depth-to360-panorama generation. Compared to the previous state-of-the-art methods based on finetuning, optimization, and iterative updates, SyncTweedies demonstrates significantly better results. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a generative process that samples data within a space we term the canonical space $\\mathcal{Z}$ , where a pretrained diffusion model is not provided. Instead, we leverage diffusion models trained in other spaces called the instance spaces $\\{\\mathcal{W}_{i}\\}_{i=1:N}$ , where a subset of the canonical space can be instantiated into each of them via a mapping: $f_{i}:\\mathcal{Z}\\to\\mathcal{W}_{i}$ ; we refer to this mapping as the projection. Let $g_{i}$ denote the unprojection, which is the inverse of $f_{i}$ , mapping the instance space to a subset of the canonical space. We assume that the entire canonical space $\\mathcal{Z}$ can be expressed as a composition of multiple instance spaces $\\mathcal{W}_{i}$ , meaning that for any data point $\\mathbf{z}\\in{\\mathcal{Z}}$ , there exist $\\left\\{\\mathbf{w}_{i}\\mid\\bar{\\mathbf{w}}_{i}\\in\\mathcal{W}_{i}\\right\\}_{i=1:N}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}=\\mathcal{A}\\left(\\left\\{g_{i}(\\mathbf{w}_{i})\\right\\}_{i=1:N}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{A}}$ is an aggregation function that averages the data points from the multiple instance spaces in the canonical space. Our objective is to introduce a general framework for the generative process in the canonical space by integrating multiple denoising processes from different instance spaces through synchronization. ", "page_idx": 2}, {"type": "text", "text": "3 Diffusion Synchronization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first outline the denoising procedure of DDIM [53] and then present possible options for diffusion synchronization processes based on it. ", "page_idx": 2}, {"type": "text", "text": "3.1 Denoising Process of DDIM [53] ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Song et al. [53] have proposed DDIM, a generalized denoising process that controls the level of randomness during denoising. In DDIM [53], the posterior of the forward process is represented as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\sigma_{t}}\\left(\\mathbf{x}^{(t-1)}|\\mathbf{x}^{(t)},\\mathbf{x}^{(0)}\\right)=\\mathcal{N}\\left(\\psi_{\\sigma_{t}}^{(t)}(\\mathbf{x}^{(t)},\\mathbf{x}^{(0)}),\\sigma_{t}^{2}\\mathbf{I}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi_{\\sigma_{t}}^{(t)}(\\mathbf{x}^{(t)},\\mathbf{x}^{(0)})=\\sqrt{\\alpha_{t-1}}\\mathbf{x}^{(0)}+\\sqrt{\\frac{1-\\alpha_{t-1}-\\sigma_{t}^{2}}{1-\\alpha_{t}}}\\cdot(\\mathbf{x}^{(t)}-\\sqrt{\\alpha_{t}}\\mathbf{x}^{(0)})}\\end{array}$ 1\u2212\u03b11t\u2212\u2212\u03b11\u2212\u03c3t2\u00b7 (x(t) \u2212\u221a\u03b1tx(0)) and \u03c3t is a hyperparameter determining the level of randomness. In this paper, we consider a deterministic process where $\\sigma_{t}=0$ for all $t$ , thus $\\psi_{\\sigma_{t}=0}^{(t)}$ will be denoted as $\\psi^{(t)}$ for simplicity. During denoising process, to sample $\\mathbf{x}^{(t-1)}$ from its unknown original clean data point $\\mathbf{x}^{(0)}$ , we estimate $\\mathbf{x}^{(0)}$ using Tweedie\u2019s formula [46]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}^{(0)}\\simeq\\phi^{(t)}(\\mathbf{x}^{(t)},\\epsilon_{\\theta}(\\mathbf{x}^{(t)}))=\\frac{\\mathbf{x}^{(t)}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}(\\mathbf{x}^{(t)})}{\\sqrt{\\alpha_{t}}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon_{\\theta}$ is a noise prediction network, and for simplicity, the time input and condition term in $\\epsilon_{\\theta}$ are dropped. In short, each deterministic denoising step of DDIM [53] is expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}^{(t-1)}=\\boldsymbol{\\psi}^{(t)}(\\mathbf{x}^{(t)},\\boldsymbol{\\phi}^{(t)}(\\mathbf{x}^{(t)},\\epsilon_{\\theta}(\\mathbf{x}^{(t)}))).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Diffusion Synchronization Processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now explore various scenarios of sampling $\\mathbf{z}\\in{\\mathcal{Z}}$ by leveraging the composition of multiple denoising processes in the instance spaces $\\{\\mathcal{W}_{i}\\}_{i=1:N}$ . Consider the denoising step of the diffusion model at each time step $t$ in each instance space $\\mathcal{W}_{i}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{w}_{i}^{(t-1)}=\\boldsymbol{\\psi}^{(t)}(\\mathbf{w}_{i}^{(t)},\\boldsymbol{\\phi}^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A na\u00efve approach to generating data in the canonical space through the denoising processes in instance spaces would be to perform the processes independently in each instance space and then aggregate the final denoised outputs in the canonical space at the end using the averaging function $\\boldsymbol{\\mathcal{A}}$ . However, this approach results in poor outcomes that lack consistency across outputs in different instance spaces. Hence, we propose to synchronize the denoising processes at each time step $t$ through the unprojection operation $g_{i}$ from each instance space to the canonical space and the aggregation operation $\\boldsymbol{\\mathcal{A}}$ , after which the results will be back-projected via the projection operation $f_{i}$ to each instance space again. Note that, as described in Equation 4, the estimated mean of the posterior distribution $\\bar{\\psi}^{(t)}\\bar{(\\cdot,\\cdot)}$ involves multiple layers of computations: noise prediction $\\epsilon_{\\theta}(\\cdot)$ , Tweedie\u2019s formula [46] $\\phi^{(t)}(\\cdot,\\cdot)$ approximating the final output $\\mathbf{x}^{(0)}$ each time step, and the final linear combination $\\psi^{(t)}(\\cdot,\\cdot)$ . Synchronization through the sequence of unprojection $g_{i}$ , aggregation in the canonical space $\\boldsymbol{\\mathcal{A}}$ , and projection $f_{i}$ can thus be performed after each layer of these computations, resulting in the following three cases: ", "page_idx": 2}, {"type": "image", "img_path": "06Vt6f2js7/tmp/7f3742ece005d8aade5f8bf8ccbcb752d3ce6363a2dd28a0a201b130a0e26aad.jpg", "img_caption": ["(a) Instance variable denoising process "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "06Vt6f2js7/tmp/ad395491f17bbadace03dd3dd108917bca6907db6dee21c542e7ac273eead674.jpg", "img_caption": ["(b) Canonical variable denoising process "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Diagrams of diffusion synchronization processes. The left diagram depicts denoising instance variables $\\{\\mathbf{w}_{i}\\}$ , while the right diagram illustrates directly denoising a canonical variable ${\\bf z}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~1:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(\\mathcal{A}(\\left\\{g_{j}\\big(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})\\big)\\right\\}_{j=1}^{N})))))}\\\\ &{\\mathrm{Case~2:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}\\big(\\mathcal{A}\\big(\\{g_{j}\\big(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\big)\\}_{j=1}^{N}\\big)\\big)}\\\\ &{\\mathrm{Case~3:~}\\mathbf{w}_{i}^{(t-1)}=f_{i}\\big(\\mathcal{A}\\big(\\mathcal{A}\\big(\\{g_{j}\\big(\\psi^{(t)}(\\mathbf{w}_{j}^{(t)},\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\big)\\big)\\}_{j=1}^{N}\\big)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In each case, we highlight the computation layer to be synchronized in red. ", "page_idx": 3}, {"type": "text", "text": "Another notable approach is to conduct the denoising process directly on the canonical space: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}^{(t-1)}=\\boldsymbol{\\psi}^{(t)}(\\mathbf{z}^{(t)},\\boldsymbol{\\phi}^{(t)}(\\mathbf{z}^{(t)},\\mathbf{\\epsilon}_{\\epsilon_{\\theta}(\\mathbf{z}^{(t)}))})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "although it is not directly feasible because the noise prediction network in the canonical space $\\epsilon_{\\theta}(\\mathbf{z}^{(t)})$ is not available. Nevertheless, it can be achieved by redirecting the noise prediction to the instance spaces as follows: ", "page_idx": 3}, {"type": "text", "text": "(a) project the intermediate noisy data point $\\mathbf{z}^{(t)}$ from the canonical space to each instance space, resulting in $f_{i}(\\mathbf{z}^{(t)})$ ,   \n(b) apply a subsequence of the operations: $\\epsilon_{\\theta},\\,\\phi^{(t)}$ , and $\\psi^{(t)}$ ,   \n(c) unproject the outputs back to the canonical space via $g_{i}$ and then average them using the aggregation function $\\mathcal{A}$ , and   \n(d) perform the remaining operations in the canonical space. ", "page_idx": 3}, {"type": "text", "text": "Such an approach of performing the denoising process in the canonical space leads to the following two additional cases depending on the subsequence of operations at step (b): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~4:~}\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},\\phi^{(t)}(\\mathbf{z}^{(t)},\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))\\}_{i=1}^{N}))))}\\\\ &{\\mathrm{Case~5:~}\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},\\mathcal{A}(\\{g_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\}_{i=1}^{N})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Illustration of the aforementioned diffusion synchronization processes are shown in Figure 2. Note the analogy between Cases 1 and 4, and Cases 2 and 5 in terms of the variable averaged in the canonical space with the aggregation operator $\\mathcal{A}$ : either the outputs of $\\epsilon_{\\theta}(\\cdot)$ or $\\phi^{(t)}(\\cdot,\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "While it is also feasible to conduct the aggregation $\\mathcal{A}$ multiple times with the output of different layers within a single denoising step, and to denoise data both in instance spaces and the canonical space, we empirically find that such more convoluted cases perform worse. In Appendix $_\\mathrm{H}$ , we detail our exploration of all possible cases and present experimental analyses. ", "page_idx": 3}, {"type": "text", "text": "3.3 Connection to Previous Diffusion Synchronization Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Below, we first review previous works each corresponding to a specific case of the aforementioned possible diffusion synchronization processes while focusing on a specific application. Then, we discuss finetuning-based approaches and their limitations. In Section 4, we also review literature targeting the same applications but without diffusion synchronization. ", "page_idx": 3}, {"type": "text", "text": "3.3.1 Zero-Shot-Based Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Ambiguous Image Generation. Ambiguous images are images that exhibit different appearances under certain transformations, such as a $90^{\\circ}$ rotation or flipping. They can be generated through a diffusion synchronization process, considering both the canonical space $\\mathcal{Z}$ and instance spaces $\\{\\mathcal{W}_{i}\\}_{i=1:N}$ as the same space of the image, with the projection operation $f_{i}$ representing the transformation producing each appearance. Visual Anagrams [18] uses Case 4 which aggregates the noise predictions $\\epsilon_{\\theta}(\\cdot)$ to generate ambiguous images. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Arbitrary-Sized Image Generation. In arbitrary-sized image generation, the canonical space $\\mathcal{Z}$ is the space of the arbitrary-sized image, while the instance spaces $\\bar{\\{\\mathcal{W}_{i}\\}}_{i=1:N}$ are overlapping patches across the arbitrary-sized image, matching the resolution of the images that the pretrained image diffusion model can generate. The projection operation $f_{i}$ corresponds to the cropping operation applied to each patch. MultiDiffusion [5] and SyncDiffusion [29] introduce arbitrary-sized image generation methods using Case 3, averaging the mean of the posterior distribution $\\psi^{(t)}(\\cdot,\\cdot)$ . ", "page_idx": 4}, {"type": "text", "text": "Mesh Texturing. In 3D mesh texturing, the texture image space serves as the canonical space $\\mathcal{Z}$ , and the rendered images from each view serve as the instance spaces $\\{\\mathcal{W}_{i}\\}_{i=1:N}$ . The projection operation $f_{i}$ corresponds to rendering 3D textured meshes into 2D images. SyncMVD [35] proposes a 3D mesh texturing method by leveraging Case 5, which performs denoising in the canonical space and unprojects the outputs of Tweedie\u2019s formula [46] $\\bar{\\phi^{(t)}(\\cdot,\\cdot)}$ . ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Finetuning-Based Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In addition to the aforementioned works, there have been attempts to achieve synchronization through finetuning. In multi-view image generation, SyncDreamer [34] and MVDream [52] finetune pretrained image diffusion models to achieve consistency across different views. MVDiffusion [56] and DiffCollage [65] generate $360^{\\circ}$ panorama images through finetuning. Additionally, Paint3D [63] trains an encoder to directly generate 3D mesh texture images in the UV space. However, these finetuning-based methods use target sample datasets [16, 9, 15, 13] that are smaller by orders of magnitude compared to Internet-scale image datasets [49], e.g., 10K panorama images [9] vs. 5B images [49]. As a result, they are prone to oveftiting and losing the rich prior and generalizability of pretrained image diffusion models [47, 48]. Additionally, the poor quality of textures in most 3D model datasets results in unsatisfactory texturing outcomes, even when using relatively large-scale datasets [16, 15]. In our experiments, we demonstrate that our zero-shot synchronization method, fully leveraging the pretrained model without bias toward a specific dataset, provides the best realism and widest diversity, assessed by FID and KID, compared to the finetuning-based methods. ", "page_idx": 4}, {"type": "table", "img_path": "06Vt6f2js7/tmp/69a729e2bbf15f9ce2780f038eb32af2d3c59e528f7e5959780345cee6c65c1f.jpg", "table_caption": ["Table 1: A quantitative comparison in ambiguous image generation. KID [6] is scaled by $10^{3}$ . For each row, we highlight the column whose value is within $95\\%$ of the best. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.4 Comparison Across the Diffusion Synchronization Processes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we compare the five cases of diffusion synchronization processes in Section 3.2 and analyze their characteristics through various toy experiments. ", "page_idx": 4}, {"type": "text", "text": "3.4.1 Toy Experiment Setup: Ambiguous Image Generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the toy experiment setup, we employ the task of generating ambiguous images introduced by Geng et al. [18] (see Section 3.3.1 for descriptions of ambiguous images). In this setup, we consider two-view ambiguous image generation, where two different transformations are applied, each producing a distinct appearance. Note that one of the transformations is an identity transformation, while the other is chosen to simulate different scenarios of mapping pixels from the canonical space to the instance space: 1-to-1, 1-to- ${\\boldsymbol{n}}$ , and $n$ -to-1 projection. In 1-to-1 and $n$ -to-1 projections, we use the 10 transformations from Visual Anagrams [18], while for the 1-to- ${\\cdot n}$ projection, we apply rotation transformations with randomly sampled angles. For all projection cases, we use the 95 prompts from [18]. For more details on the experiment setups, refer to Appendix B.1. ", "page_idx": 4}, {"type": "image", "img_path": "06Vt6f2js7/tmp/59bd8647522527b62b94b05b63d2a3328ad51c00363cf74a9f297c52c9859d75.jpg", "img_caption": ["Figure 3: Qualitative results of ambiguous image generation. While all diffusion synchronization processes show identical results with 1-to-1 projections, Case 1, Case 3 and Visual Anagrams [18] (Case 4) exhibit degraded performance when the projections are 1-to- ${\\mathbf{\\nabla}}n$ . Notably, SyncTweedies can be applied to the widest range of projections, including $n$ -to-1 projections, where Case 5 fails to generate plausible outputs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.4.2 1-to-1 Projection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In 1-to-1 projection case, the five cases of diffusion synchronization processes become identical, as shown in Appendix D. The quantitative and qualitative results of diffusion synchronization processes are presented in Table 1 and the first row of Figure 3, respectively, where the fully denoised instance variables, $\\mathbf{w}_{1}^{(0)}$ and w2 , are displayed side by side. The results confirm that all diffusion synchronization processes produce the same outputs. ", "page_idx": 5}, {"type": "text", "text": "3.4.3 1-to- $n$ Projection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We further investigate the five cases of diffusion synchronization processes with different transformations for ambiguous images. It is important to note that all the transformations previously mentioned are perfectly invertible, meaning: $f_{i}(g_{i}(\\mathbf{w}_{i}))=\\mathbf{w}_{i}$ . However, in certain applications, the projection $f_{i}$ is often not a function but an 1-to- $n$ mapping, thus not allowing its inverse. For example, consider generating a texture image of a 3D object while treating the texture image space as the canonical space and the rendered image spaces as instance spaces. When mapping each pixel of a specific view image to a pixel in the texture image in the rendering process\u2014with nearest neighbor sampling, one pixel in the texture space can be projected to multiple pixels. Hence, the unprojection $g_{i}$ cannot be a perfect inverse of the projection $f_{i}$ but can only be an approximation, making the reprojection error $\\lVert\\mathbf{w}_{i}-f_{i}\\big(g_{i}\\big(\\mathbf{w}_{i}\\big)\\big)\\rVert$ small. This violates the initial conditions required for the proof in Appendix D that states Cases 1-5 become identical, and we observe that such a case of having 1-to- $\\cdot n$ projection $f_{i}$ can significantly impact the diffusion synchronization process. ", "page_idx": 5}, {"type": "text", "text": "As a toy experiment setup illustrating such a case with ambiguous image generation, we replace the 1-to-1 transformations used in Section 3.4.2 to rotation transformations with nearest-neighbor sampling. We randomly select an angle and rotate an inner circle of the image while leaving the rest of the region unchanged. Due to discretization, rotating an image followed by an inverse rotation may not perfectly restore the original image. ", "page_idx": 5}, {"type": "text", "text": "The second row of Table 1 and Figure 3 present the quantitative and qualitative results of 1-to$n$ projection experiment. Note that the performance of Case 1 and Visual Anagrams [18] (Case ", "page_idx": 5}, {"type": "text", "text": "4), which aggregate the predicted noises $\\epsilon_{\\theta}(\\cdot)$ from either instance variables $\\mathbf{w}_{i}^{(t)}$ or a projected canonical variable $f_{i}(\\mathbf{z}^{(t)})$ respectively, significantly declines. Also, the performance of Case 3, which aggregates the posterior means $\\psi^{(t)}(\\cdot,\\cdot)$ , shows a minor decline. The quality of Cases 2 and 5, however, remain almost unchanged. This highlights that the denoising process is highly sensitive to the predicted noise and to the intermediate noisy data points, while it is much more robust to the outputs of Tweedie\u2019s formula [46] $\\phi^{(t)}(\\cdot,\\cdot)$ , the prediction of the final clean data point at an intermediate stage. ", "page_idx": 6}, {"type": "text", "text": "3.4.4 $n$ -to-1 Projection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Then, do the results above conclude that both Cases 2 and 5 are suitable for all applications? Lastly, we consider the case when the projection $f_{i}$ also involves an $n$ -to-1 mapping. Such a scenario can arise when coloring not a solid mesh but a neural 3D representation rendered with the volume rendering equation [25, 26, 39]. Due to the nature of volume rendering, which involves sampling multiple points along a ray and taking a weighted sum of their information, the projection operation $f_{i}$ includes an $n$ -to-1 mapping. Note that this case also violates the initial conditions of the proof in ", "page_idx": 6}, {"type": "text", "text": "Appendix D, which states that the diffusion synchronization cases become identical under specific initial conditions. Additionally, Case 5 results in poor outcomes due to a variance decrease issue. Let $\\{\\mathbf{x}_{i}\\}_{i=1:N}$ be random variables, each sampled from $\\mathbf{x}_{i}\\sim\\mathcal{N}(\\pmb{\\mu}_{i},\\sigma_{t}^{2}\\mathbf{I})$ , and $\\begin{array}{r}{{\\bf x}=\\sum_{i=1}^{N}w_{i}{\\bf x}_{i}}\\end{array}$ be the weighted sum, where $0\\leq w_{i}\\leq1$ and $\\textstyle\\sum_{i=1}^{N}w_{i}=1$ . Then, $\\mathbf{x}$ also follows the Gaussian distribution $\\begin{array}{r}{\\mathbf{x}\\sim\\mathcal{N}\\left(\\sum_{i=1}^{N}w_{i}\\pmb{\\mu}_{i},\\sum_{i=1}^{N}w_{i}^{2}\\sigma_{t}^{2}\\mathbf{I}\\right)}\\end{array}$ .  From the triangle inequality [40], the sum of squares is always less than or equal to the square of the sum: $\\begin{array}{r}{\\sum_{i=1}^{N}w_{i}^{2}\\le(\\sum_{i=1}^{N}w_{i})^{2}=1}\\end{array}$ , implying that the variance of $\\mathbf{x}$ is mostly less than the variance of $\\mathbf{x}_{i}$ . ", "page_idx": 6}, {"type": "text", "text": "Consequently, when $f_{i}$ includes an $n$ -to-1 mapping, the variance of $\\mathbf{w}_{i}^{(t)}$ , computed as a weighted sum over multiple points in the canonical space, is mostly less than the variance of $\\mathbf{z}^{(t)}$ . Thus, the final output of Case 5 becomes blurry and coarse since each intermediate noisy latent in instance spaces $\\bar{\\mathbf{w}_{i}^{(t)}}$ experiences a decrease in variance compared to that of $\\mathbf{z}^{(t)}$ . ", "page_idx": 6}, {"type": "text", "text": "We validate our analysis with another toy experiment, where we use the same set of transformations used by Geng et al. [18] but with a multiplane image (MPI) [57] as the canonical space. The image of each instance space is rendered by first averaging colors in the multiplane of the canonical space and then applying the transformation. Ten planes are used for the multiplane image representation in our experiments. The results are presented in the third row of Table 1 and Figure 3. Notably, Case 5 fails to produce plausible images like the other cases, whereas Case 2 still generates realistic images. ", "page_idx": 6}, {"type": "text", "text": "Table 2 below summarizes suitable cases for each projection type. Note that Case 2 is the only case that is applicable to any type of projection function. Since Case 2 involves averaging the outputs of Tweedie\u2019s formula in the instance spaces, we name this case SyncTweedies. Experimental results with additional applications are demonstrated in Section 5, and analysis of all possible cases is presented in Appendix H. ", "page_idx": 6}, {"type": "table", "img_path": "06Vt6f2js7/tmp/a4cf64dbeb085cef228855491b3d2ef03dab7eefcc7686aef836579167d516ee.jpg", "table_caption": ["Table 2: Analysis of diffusion synchronization processes on different projection scenarios. SyncTweedies offers the broadest range of applications. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In addition to Section 3.3.1 introducing previous works on diffusion synchronization, in this section, we review other previous works that utilize pretrained diffusion models in different ways to generate or edit visual content. ", "page_idx": 6}, {"type": "text", "text": "Optimization-Based Methods. Poole et al. [41] first introduced Score Distillation Sampling (SDS), which facilitates data sampling in a canonical space by leveraging the loss function of the diffusion model training and performing gradient descent. This idea, originally introduced for 3D generation [58, 31, 55], has been widely applied to various applications, including vector image generation [24], ambiguous image generation [7], mesh texturing [37, 11, 62], mesh deformation [61], and 4D generation [32, 3]. Subsequent works [20, 27, 28] also proposed modified loss functions not to generate data but to edit existing data while preserving their identities. This approach, exploiting diffusion models not for denoising but for gradient-descent-based updating, generally produces less realistic outcomes and is more time-consuming compared to denoising-based generation. ", "page_idx": 7}, {"type": "text", "text": "Iterative View Updating Methods. Particularly for 3D object/scene texturing and editing, there are approaches to iteratively update each view image and subsequently refine the 3D object/scene. TEXTure [44], Text2Tex [10], and TexFusion [8] are previous works that sequentially update a partial texture image from each view and unproject it onto the 3D object mesh. For texturing 3D scene meshes, Text2Room [23] and SceneScape [17] take a similar approach and update scene textures sequentially. Instruct-NeRF2NeRF [19] proposed to edit a 3D scene by iteratively replacing each view image used in the reconstruction process. However, sequentially updating the canonical sample leads to error accumulations, resulting in blurriness or inconsistency across different views. ", "page_idx": 7}, {"type": "text", "text": "Utilization of One-Step Predictions. Previous works have utilized the outputs of Tweedie\u2019s formula to restore images [12, 66] and to guide the generation process [4, 29]. However, the one-step predicted samples are used to compute the gradient from a predefined loss function to guide the sampling process, rather than for synchronization, which differentiates from our approach. ", "page_idx": 7}, {"type": "text", "text": "Concurrent works [50, 59] also average the outputs of Tweedie\u2019s formula, similar to our approach, but they focus only on specific applications. For the first time, we present a general framework for diffusion synchronization and provide a comprehensive analysis of different synchronization methods across various applications. ", "page_idx": 7}, {"type": "table", "img_path": "06Vt6f2js7/tmp/a12aa9bb504d62aa75e1a6c7856b52992cb8d844923f08bf2e4dcd61a27d7d3e.jpg", "table_caption": ["Table 3: A quantitative comparison in 3D mesh texturing. KID is scaled by 103. The best in each row is highlighted by bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Applications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We quantitatively and qualitatively compare SyncTweedies with the other diffusion synchronization processes, as well as the state-of-the-art methods of each application: 3D mesh texturing (Section 5.1), depth-to-360-panorama generation (Section 5.2) ,and 3D Gaussian splats [26] texturing (Section 5.3). Additional experiments and detailed setups are provided in Appendix, including (1) additional qualitative results, (2) implementation details of each application, (3) arbitrary-sized image generation, (4) 3D mesh texture editing and diversity comparison, (6) runtime and VRAM usage comparisons, and (7) user preference evaluations. ", "page_idx": 7}, {"type": "table", "img_path": "06Vt6f2js7/tmp/e84bd4d46faef9769827d715e84e90c2767ea85f0c45a18d153da647a584e1c5.jpg", "table_caption": ["Table 4: A quantitative comparison in depth-to-360-panorama application. KID is scaled by $10^{3}$ . The best in each row is highlighted by bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Experiment Setup. In the case of instance variable denoising processes introduced in Section 3.2 (Cases 1-3), we initialize instance variables by projecting an initial canonical latent $\\mathbf{z}^{(T)}$ sampled from a standard Gaussian distribution $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ : wi(T )\u2190fi(z(T )). For n-to-1 projection cases (e.g.,3D Gaussian splats texturing), the instance variables are directly initialized from a standard Gaussian distribution which can avoid the variance decrease issue discussed in Section 3.4.4. ", "page_idx": 8}, {"type": "text", "text": "For instance space denoising processes, the final canonical variables are obtained by synchronizing the fully denoised instance variables at the end of the diffusion synchronization processes. Refer to Section 3.3.1 for the detailed definition of the canonical space $\\mathcal{Z}$ , the instance spaces $\\{\\mathcal{W}_{i}\\}_{i=1:N}$ , the projection operation $f_{i}$ , and the unprojection operation $g_{i}$ in each application. ", "page_idx": 8}, {"type": "text", "text": "Evaluation Setup. Across all applications, we compute FID [21] and KID [6] to assess the fidelity of the generated images and CLIP similarity [42] (CLIP-S) to evaluate text alignment. We use a depth-conditioned ControlNet [64] as the pretrained image diffusion model. ", "page_idx": 8}, {"type": "text", "text": "5.1 3D Mesh Texturing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In 3D mesh texturing, projection operation $f_{i}$ is a rendering function which outputs perspective view images from a 3D mesh with a texture image. This operation represents a 1-to- ${\\cdot n}$ projection due to discretization. We evaluate five diffusion synchronization cases along with Paint3D [63], a finetuning-based method, Paint-it [62], an optimization-based method, and TEXTure [44] and Text2Tex [10], which are iterative-view-updating-based methods. We use 429 pairs of meshes and prompts used in TEXTure [44] and Text2Tex [10]. ", "page_idx": 8}, {"type": "text", "text": "Results. We present quantitative and qualitative results in Table 3 and Figure 4, respectively. The results in Table 3 align with the observations shown in the 1-to- ${\\mathbf{\\nabla}}n$ projection case discussed in Section 3.4.3. SyncTweedies and SyncMVD [35] outperform other baselines across all metrics, but ours demonstrates superior performance compared to SyncMVD. ", "page_idx": 8}, {"type": "text", "text": "Notably, SyncTweedies outperforms Paint3D [63], a finetuning-based method, indicating that finetuning with a relatively small set of synthetic 3D objects [16] is not sufficient for realistic texture generation. This is further evidenced by the cartoonish texture of the car in row 1 of Figure 4. Optimization-based and iterative-view-updating-based methods produce unrealistic texture images, often exhibiting high saturation and visible seams, as seen in the baseball glove and light bulb in rows 2 and 3 of Figure 4. These issues are also reflected in the relatively high FID and KID scores in Table 3. See Appendix A for additional qualitative results. ", "page_idx": 8}, {"type": "text", "text": "5.2 Depth-to-360-Panorama ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We generate $360^{\\circ}$ panorama images from $360^{\\circ}$ depth maps obtained from the 360MonoDepth [43] dataset. Here, $f_{i}$ projects a $360^{\\circ}$ panorama to a perspective view image, which is an 1-to- ${\\cdot n}$ projection due to discretization. We compare SyncTweedies with previous diffusion-synchronization-based methods [5, 18, 35] and MVDiffusion [56], which is finetuned using 3D scenes in the ScanNet [13] dataset. We generate a total of $500\\;360^{\\circ}$ panorama images at $0^{\\circ}$ elevation, with a field of view of $72^{\\circ}$ . ", "page_idx": 8}, {"type": "text", "text": "Results. We report quantitative results of the five diffusion synchronization processes discussed in Section 3.2 in Table 4. Table 4 demonstrates a trend consistent with the 1-to- ${\\mathbf{\\nabla}}n$ projection toy experiment results shown in Section 3.4.3. Specifically, SyncTweedies and Case 5, which synchronize the outputs of Tweedie\u2019s formula $\\phi^{(t)}(\\cdot,\\cdot)$ , exhibit the best performance. Notably, SyncTweedies demonstrates slightly superior performance across all metrics. On the other hand, MVDiffusion [56], which is finetuned using indoor scenes, fails to adapt to new, unseen domains and shows inferior results. The qualitative results are presented in Appendix A due to page limit. ", "page_idx": 8}, {"type": "table", "img_path": "06Vt6f2js7/tmp/9ed2303fc78ed81d398a306f2d3c357f0e50885082518e88f80896a1c8140869.jpg", "table_caption": ["Table 5: A quantitative comparison in 3D Gaussian splats [26] texturing. KID is scaled by $10^{3}$ . The best in each row is highlighted by bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 3D Gaussian Splats Texturing ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lastly, to verify the difference between SyncTweedies and Case 5 both of which demonstrate applicability up to 1-to- $^{n}$ projections as outlined in Section 3.4.3, we explore texturing 3D Gaussian Splats [26], exemplifying an $n$ -to-1 projection case. In 3D Gaussian splats texturing, the projection operation $f_{i}$ is an $n$ -to-1 case, characterized by a volumetric rendering function [25]. This function computes a weighted sum of $n$ 3D Gaussian splats in the canonical space to render a pixel in the instance space. Note that in 3D Gaussian splats texturing, the unprojection $g_{i}$ and the aggregation $\\boldsymbol{\\mathcal{A}}$ operation are performed using optimization. ", "page_idx": 9}, {"type": "text", "text": "While recent 3D generative models [55, 54] generate plausible 3D objects represented as 3D Gaussian splats, they often lack fine details in the appearance. We validate the effectiveness of SyncTweedies on pretrained 3D Gaussian splats [26] from the Synthetic NeRF dataset [39]. We use 50 views for texture generation and evaluate the results from 150 unseen views. For baselines, we evaluate diffusion-synchronization-based methods, the optimization-based methods, SDS [41], MVDreamSDS [52], and the iterative-view-updating-based method, Instruct-NeRF2NeRF (IN2N) [19]. ", "page_idx": 9}, {"type": "text", "text": "Results. Table 5 and Figure 5 present quantitative and qualitative comparisons of 3D Gaussian splats [26] texturing. SyncTweedies, unaffected by the variance decrease issue, outperforms Case 5 both quantitatively and qualitatively, which is consistent with the observations from the toy experiments in Section 3.4.4. When compared to other baselines based on optimization (SDS [41] and MVDream-SDS [52]) and iterative view updating (IN2N [19]), ours outperforms across all metrics, especially by a large margin in FID [21]. As shown in Figure 5, optimization-based methods tend to generate textures with high saturation, while the iterative-view-updating-based method produces textures lacking fine details. Additional qualitative results are shown in Appendix A. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have explored various scenarios of diffusion synchronization and evaluated their performance across a range of applications, including ambiguous image generation, panorama generation, and texturing on 3D mesh and 3D Gaussian splats. Our analysis shows that SyncTweedies, which averages the outputs of Tweedie\u2019s formula while conducting denoising in multiple instance spaces, offers the best performance and the widest applicability. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Societal Impacts. Despite the superior performance of SyncTweedies across diverse applications, updating both the geometry and appearance of 3D objects remains an open problem. Also, since the pretrained image diffusion model may have been trained with uncurated images, SyncTweedies might inadvertently produce harmful content. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thank you to Phillip Y. Lee for valuable discussions on diffusion synchronization, and to Jisung Hwang for providing the 3D mesh renderer. This work was supported by the NRF grant (RS-2023- 00209723), IITP grants (RS-2022-II220594, RS-2023-00227592, RS-2024-00399817), and KEIT grant (RS-2024-00423625), all funded by the Korean government (MSIT and MOTIE), as well as grants from the DRB-KAIST SketchTheFuture Research Center, NAVER-Intel Co-Lab, Hyundai NGV, KT, and Samsung Electronics. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luma AI. Genie. ", "page_idx": 10}, {"type": "text", "text": "[2] Franz Aurenhammer. Voronoi Diagrams\u2014a survey of a fundamental geometric data structure. CSUR, 1991.   \n[3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4D-fy: Text-to-4d generation using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984, 2023.   \n[4] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In CVPR, 2023.   \n[5] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023.   \n[6] Miko\u0142aj Bin\u00b4kowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018.   \n[7] Ryan Burgert, Xiang Li, Abe Leite, Kanchana Ranasinghe, and Michael S Ryoo. Diffusion illusions: Hiding images in plain sight. arXiv preprint arXiv:2312.03817, 2023.   \n[8] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In CVPR, 2023.   \n[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In International Conference on 3D Vision (3DV), 2017.   \n[10] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nie\u00dfner. Text2tex: Text-driven texture synthesis via diffusion models. In ICCV, 2023.   \n[11] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22246\u201322256, 2023.   \n[12] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In ICLR, 2023.   \n[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.   \n[14] DeepFloyd. Deepfloyd if. https://www.deepfloyd.ai/deepfloyd-if/.   \n[15] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of $10\\mathrm{m}{+3}\\mathrm{d}$ objects. In NeurIPS, 2024.   \n[16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In CVPR, 2023.   \n[17] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. In NeurIPS, 2024.   \n[18] Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. arXiv preprint arXiv:2311.17919, 2023.   \n[19] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instructnerf2nerf: Editing 3d scenes with instructions. In ICCV, 2023.   \n[20] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In ICCV, 2023.   \n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2018.   \n[22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS, 2021.   \n[23] Lukas H\u00f6llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In ICCV, 2023.   \n[24] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In CVPR, 2023.   \n[25] James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM TOG, 1984.   \n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023.   \n[27] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. In NeurIPS, 2024.   \n[28] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior distillation sampling. In CVPR, 2024.   \n[29] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. In NeurIPS, 2023.   \n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.   \n[31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3d content creation. In CVPR, 2023.   \n[32] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint arXiv:2312.13763, 2023.   \n[33] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022.   \n[34] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. SyncDreamer: Generating multiview-consistent images from a single-view image. In ICLR, 2023.   \n[35] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. arXiv preprint arXiv:2311.12891, 2023.   \n[36] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021.   \n[37] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for shapeguided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12663\u201312673, 2023.   \n[38] Midjourney. Midjourney. https://www.midjourney.com/.   \n[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2021.   \n[40] Dragoslav S Mitrinovic, Josip Pecaric, and Arlington M Fink. Classical and new inequalities in analysis. Springer Science & Business Media, 2013.   \n[41] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. In ICLR, 2023.   \n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[43] Manuel Rey-Area, Mingze Yuan, and Christian Richardt. 360MonoDepth: High-resolution 360deg monocular depth estimation. In CVPR, 2022.   \n[44] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. ACM TOG, 2023.   \n[45] Daniel Ritchie. Rudimentary framework for running two-alternative forced choice (2afc) perceptual studies on mechanical turk.   \n[46] Herbert E Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory. Springer, 1956.   \n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   \n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. In NeurIPS, 2022.   \n[49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022.   \n[50] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. 2024.   \n[51] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.   \n[52] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. In ICLR, 2024.   \n[53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021.   \n[54] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024.   \n[55] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In ICLR, 2023.   \n[56] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. In NeurIPS, 2023.   \n[57] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In CVPR, 2020.   \n[58] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. In CVPR, 2023.   \n[59] Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steven M Seitz, Ira Kemelmacher-Shlizerman, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, and Aleksander Holynski. Generative powers of ten. In CVPR, 2024.   \n[60] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2024.   \n[61] Seungwoo Yoo, Kunho Kim, Vladimir G Kim, and Minhyuk Sung. As-Plausible-As-Possible: PlausibilityAware Mesh Deformation Using 2D Diffusion Priors. In CVPR, 2024.   \n[62] Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. Paint-it: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. arXiv preprint arXiv:2312.11360, 2023.   \n[63] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, BIN FU, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In CVPR, 2024.   \n[64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, 2023.   \n[65] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In CVPR, 2023.   \n[66] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In CVPR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Qualitative Results 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 3D Mesh Texturing 15   \nA.2 Depth-to-360-Panorama Generation 16   \nA.3 3D Gaussian Splats Texturing . . 17 ", "page_idx": 13}, {"type": "text", "text": "B Details on Experiments 17 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Details on Ambiguous Image Generation \u2014 Section 3.4.1 . . 18   \nB.2 Details on 3D Mesh Texturing \u2014 Section 5.1 . . 19   \nB.3 Details on Depth-to-360-Panorama Generation \u2014 Section 5.2 20   \nB.4 Details on 3D Gaussian Splats Texturing \u2014 Section 5.3 . . . 20 ", "page_idx": 13}, {"type": "text", "text": "C Arbitray-Sized Image Generation 21 ", "page_idx": 13}, {"type": "text", "text": "D 1-to-1 Projection 23 ", "page_idx": 13}, {"type": "text", "text": "E 3D Mesh Texture Editing and Diversity 25 ", "page_idx": 13}, {"type": "text", "text": "E.1 3D Mesh Texture Editing . 25   \nE.2 Diversity of SyncTweedies 25 ", "page_idx": 13}, {"type": "text", "text": "F Runtime and VRAM Usage Comparison 25 ", "page_idx": 13}, {"type": "text", "text": "G User Study 26 ", "page_idx": 13}, {"type": "text", "text": "H Analysis of Diffusion Synchronization Processes 28 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "H.1 Overview 28   \nH.2 Instance Variable Denoising Process 29   \nH.3 Canonical Variable Denoising Process . . 30   \nH.4 Combined Variable Denoising Process 31   \nH.5 Quantitative Results . . 31 ", "page_idx": 13}, {"type": "text", "text": "A Qualitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 3D Mesh Texturing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As shown in Figure 6, SyncTweedies and SyncMVD [35] generate the most realistic output images, aligning with the results of $n$ -to-1 projection scenarios discussed in Section 3.4.3. Notably, Paint3D [63], a finetuing-based method, produces inferior textures, losing fine-details, as seen in the appearance of the clock in row 3 and the patterns of the ladybug in row 5. This demonstrates the challenge of acquiring a sufficient amount of high-quality texture images for satisfactory results. The optimization-based method [62] tends to produce images with high-contrast, unnatural colors, as evidenced in rows 4 and 6. Lastly, the iterative-view-updating-based methods [44, 10] show inconsistencies across views noticeable in the dumpster in row 1 and the television in row 9. ", "page_idx": 14}, {"type": "image", "img_path": "06Vt6f2js7/tmp/3520016887c68171ee8d47d903d3868a069277b2ebd36e8df1e6e6c97e7d4596.jpg", "img_caption": ["Figure 6: Qualitative results of 3D mesh texturing. SyncTweedies and SyncMVD [35] exhibit comparable results, outperforming other baselines. Finetuning-based method [63] produces images without fine details as it was trained on a dataset with coarse texture images. The optimization-based method [62] tends to produce unrealistic and high saturation textures, while iterative-view-updatingbased methods [10, 44] show view inconsistencies. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Depth-to-360-Panorama Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As shown in Figure 7, SyncTweedies and Case 5 demonstrate the best results, aligning well with the input depth maps, with SyncTweedies showing a slightly better alignment as indicated by the red arrow in Figure 7. On the other hand, MVDiffusion [56], which is finetuned with the depth maps of indoor 3D scenes from the ScanNet [13] dataset, produces suboptimal results and fails to generate realistic $360^{\\circ}$ panoramas for out-of-domain scenes. This demonstrates that MVDiffusion [56] is overftiting to the scenes encountered during finetuning, resulting in a loss of generalizability. Cases 1 and 4, which aggregate the predicted noise $\\epsilon_{\\theta}(\\cdot)$ , produce noisy outputs. Case 3 yields suboptimal panoramas, characterized by monochromatic appearances and a lack of detail. ", "page_idx": 15}, {"type": "image", "img_path": "06Vt6f2js7/tmp/884b54b7b6f9c6e491c1be62fe974af7a3857b954ead780c8f02a5bf1303fb1a.jpg", "img_caption": ["Figure 7: Qualitative results of depth-to-360-panorama generation. SyncTweedies and Case 5 generate consistent and high-fidelity panoramas as observed in the 1-to- ${\\cdot n}$ projection experiment in Section 3.4.3. MVDiffusion [56] fails to generalize to out-of-domain scenes and generates suboptimal panoramas. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8 shows that SyncTweedies generates high-fidelity results with intricate details, such as the carvings of an excavator in row 2, while Case 5 lacks fine details. Optimization-based methods, SDS [41] and MVDream-SDS [52], produce artifacts characterized by high saturation, such as the corns in row 1 and the carrots in row 4. Notably, a finetuning-based method, MVDreamSDS [52], shows inferior quality to SDS. As discussed in Section 3.3.2, the poor quality of textures in the finetuning dataset [16] results in quality degradation. Iterative-view-updating-based method, IN2N [19], fails to preserve fine details, such as the head of the microphone in row 7. ", "page_idx": 16}, {"type": "image", "img_path": "06Vt6f2js7/tmp/c430c65710d1a9c70cdcf864188bec30e2384331471da47ce0bc3e94f22ef3f9.jpg", "img_caption": ["Figure 8: Qualitative results of 3D Gaussian splats [26] texturing. $[\\mathsf{S}^{\\ast}]$ is a prefix prompt. We use \u201cMake it to\u201d for IN2N [19] and $\\bullet\\,\\bullet\\,\\mathsf{A}$ photo of\u201d for the other methods. Case 5 tends to lose details due to the variance decrease issue, whereas SyncTweedies generates realistic images by avoiding this issue. The optimization-based methods [41, 52] produce high contrast, unnatural colors, and the iterative view updating method [19] yields suboptimal outputs due to error accumulation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Details on Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide details on the experiments discussed in Section 5 of the main paper. For all diffusion synchronization processes, we use a fully deterministic DDIM [53] sampling with 30 steps, unless specified otherwise. ", "page_idx": 16}, {"type": "text", "text": "We use DeepFloyd [14] as the pretrained diffusion model for the ambiguous image generation which denoises images in the pixel space. For the depth-to-360-panorama generation, 3D mesh texturing, and 3D Gaussian splats texturing, we employ a pretrained depth-conditioned ControlNet [64] which is based on a latent diffusion model, specifically Stable Diffusion [47]. For applications utilizing ControlNet, synchronization during the intermediate steps of diffusion synchronization processes occurs within the same latent space, except for 3D Gaussian splats texturing. In the case of 3D Gaussian splats texturing, synchronization takes place in the RGB space, and detailed explanations are provided in Section B.4. ", "page_idx": 16}, {"type": "text", "text": "In the 1-to- $n$ projection cases, each instance space sample is unprojected into the canonical space, resulting in $N$ unprojected samples, $\\{g_{i}(\\mathbf{w}_{i}^{(t)})\\}_{i=1}^{N}$ , where $N$ is the number of views. The canonical space sample $\\mathbf{z}^{(t)}$ is then obtained by averaging these unprojected samples. The averaging can be weighted based on the visibility from each view. An illustration of the process is shown in Figure 9. ", "page_idx": 16}, {"type": "image", "img_path": "06Vt6f2js7/tmp/cf908ea0eab4dc7502b3e523248e33d337e785be99b452574e6433290d14d123.jpg", "img_caption": ["Figure 9: Illustration of unprojection and aggregation operation. The figure shows the synchronization process using the 3D mesh texturing application as an example. The left figure depicts the unprojection operation, where the instance space variables are unprojected into the canonical space. The right figure illustrates the aggregation operation, where the unprojected samples are averaged in the canonical space. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Evaluation Metrics. For all applications, we evaluate diversity and fidelity of the generated images using FID [21] and KID [6]. These metrics compute scores based on the distance between the distribution of the generated image set and that of the reference image set, with the reference set forming the target distribution. Refer to each application section for detailed description of constructing the generated image set and the reference image set. ", "page_idx": 17}, {"type": "text", "text": "To evaluate the text alignment of the generated images, we report CLIP similarity score [42] (CLIP-S) which measures the similarity between the generated images $\\mathbf{w}_{i}^{(0)}$ and their corresponding text prompts $p_{i}$ in CLIP [42] embedding space. Additionally, in the ambiguous image generation, we report CLIP alignment score (CLIP-A) and CLIP concealment score (CLIP-C) following previous work, Visual Anagrams [18]. To compute the metrics, we begin by calculating a CLIP similarity matrix ${\\bf S}\\in\\mathbb{R}^{N\\times N}$ from $N$ pairs of transformations and text prompts: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i j}=E_{\\mathrm{img}}(f_{i}(\\mathbf{z}^{(0)}))^{T}E_{\\mathrm{text}}(p_{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $E_{\\mathrm{img}}(\\cdot)$ and $E_{\\mathrm{text}}(\\cdot)$ are the image encoder and the text encoder of the pretrained CLIP model [42], respectively. CLIP-A quantifies the worst alignment among the corresponding image-text pairs, specifically computed as min diag(S). However, this metric does not account for misalignment failure cases, where $p_{i}$ is visualized in $\\mathbf{w}_{j}^{(0)}$ for $i\\neq j$ . CLIP-C considers alignment of an (a) image (prompt) to all prompts (images) by normalizing the similarity matrix S using softmax: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\mathrm{tr}(\\mathrm{softmax}(\\mathbf{S}/\\tau)),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\operatorname{tr}(\\cdot)$ denotes the trace of a matrix, and $\\tau$ is the temperature parameter of CLIP [42]. We set $\\tau$ to 0.07. ", "page_idx": 17}, {"type": "text", "text": "B.1 Details on Ambiguous Image Generation \u2014 Section 3.4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the details of the ambiguous image generation experiments in Section 3.4.1. Quantitative and qualitative results are presented in Table 1 and Figure 3. ", "page_idx": 17}, {"type": "text", "text": "Evaluation Setup. To evaluate the fidelity of the generated images using FID [21] and KID [6], we create a reference set consisting of 5,000 generated images from Stable Diffusion 1.5 [47] with the same text prompts used in the generation of ambiguous images. ", "page_idx": 17}, {"type": "text", "text": "Implementation Details. We use DeepFloyd [14] which is a two-stage cascaded pixel-space diffusion model. In the first stage, we generate $64\\times64$ images that are upscaled to $256\\times256$ images in the subsequent stage. ", "page_idx": 17}, {"type": "text", "text": "Definition of Operations. In the context of ambiguous image generation, both the instance variables $\\left\\{\\mathbf{w}_{i}\\right\\}_{i=1:N}$ and canonical variables ${\\bf z}$ share the same image space. However, instance variables exhibit different appearances from the canonical variable upon applying certain transformations. ", "page_idx": 18}, {"type": "text", "text": "In the 1-to-1 projection case, we use the 10 transformations used in Visual Anagrams [18], all of which are 1-to-1 mappings. The projection operation $f_{i}$ is defined as the transformation itself, and the unprojection operation $g_{i}$ is defined as the inverse of the transformation matrix. ", "page_idx": 18}, {"type": "text", "text": "In the scenario of 1-to- ${\\cdot n}$ projection, we employ inner circle rotation as the projection operation $f_{i}$ . This involves rotating the pixels within an inner circle of an image while keeping the outer pixels unchanged. The unprojection operation $g_{i}$ is the inverse of $f_{i}$ . We use 14 inner circle rotation transformations, with rotation angles evenly spaced in the range $[45^{\\circ},175^{\\circ}]$ . For evaluation, we utilize the same 95 prompts as in the 1-to-1 case for each transformation, generating $14\\times95=1,350$ ambiguous images. After applying a rotation transformation, the grid of the rotated image does not align with the original image grid. Thus, we use the nearest-neighbor sampling to retrieve pixel colors from the original image to the rotated image. This sampling process leads to a scenario where a single pixel in the original image $\\mathbf{z}$ can be mapped to multiple pixels in the rotated image $\\mathbf{w}_{i}$ , which is an 1-to- ${\\cdot n}$ mapping. ", "page_idx": 18}, {"type": "text", "text": "For $n$ -to-1 projection, we use the same transformations and text prompts as in the 1-to-1 projection experiment, thus resulting in a total of $10\\times95\\,=\\,950$ ambiguous images. The only difference from the 1-to-1 projection experiment is that the canonical space variable ${\\bf z}$ is now represented as multiplane images (MPI) [57], where a collection of planes $\\{\\bar{\\mathbf{p}_{j}}\\}_{j=1:M}$ represents a single canonical variable. Specifically, we compute $\\mathbf{z}$ by averaging the multiplane images: $\\begin{array}{r}{{\\bf z}={\\frac{1}{M}}\\sum_{j=1}^{M}{\\bf p}_{j}}\\end{array}$ M j=1 pj. In the context of $n$ -to-1 projection, we substitute the sequence of the unprojection $g_{i}$ and the aggregation $\\boldsymbol{\\mathcal{A}}$ operation with an optimization process. The multiplane images $\\bar{\\{\\mathbf{p}_{j}\\}}$ are optimized using the following objective function: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{\\mathbf{p}_{j}\\}}\\sum_{i}^{N}\\left|f_{i}\\left(\\frac{1}{M}\\sum_{j=1}^{M}\\mathbf{p}_{j}\\right)-\\mathbf{w}_{i}\\right|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where wet set the number of planes $M=10$ . ", "page_idx": 18}, {"type": "text", "text": "B.2 Details on 3D Mesh Texturing \u2014 Section 5.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide details of the 3D mesh texturing experiments presented in Section 5.1. Quantitative and qualitative results are shown in Table 3 and Figure 6. ", "page_idx": 18}, {"type": "text", "text": "Evaluation Setup. We use 429 mesh and prompt pairs collected from previous works, TEXTure [44] and Text2Tex [10]. For texture generation, we use eight views sampled around the object with $45^{\\circ}$ intervals at $0^{\\circ}$ elevation. Two additional views are sampled at $0^{\\circ}$ and $180^{\\circ}$ azimuths with $30^{\\circ}$ elevation. For evaluation, we render each 3D mesh to ten perspective views with randomly sampled azimuths at $0^{\\circ}$ elevation, resulting $10\\times429=4{,}290$ images. Following SyncMVD [35], the reference set images are generated by ControlNet [64] using the same depth maps and text prompts used in the texture generation. ", "page_idx": 18}, {"type": "text", "text": "Implementation Details. The resolution of the latent texture image is $1,536\\times1,536$ , and that of the latent perspective view images is $96\\times96$ . In the RGB space, the resolution of the texture image is $1,024\\times1,024$ and that of the perspective view images is $768\\times768$ . ", "page_idx": 18}, {"type": "text", "text": "We adopt two approaches introduced in SyncMVD [35]: Voronoi-diagram-based filling [2] and modified self-attention layers. First, the high resolution of the latent texture image results in a texture image with sparse pixel distribution. To address this issue, we propagate the unprojected pixels to the visible regions of the texture image using the Voronoi-diagram-based filling. Second, spatially distant views tend to generate inconsistent outputs. Therefore, we adopt the modified self-attention mechanism that attends to other views when computing the attention output. ", "page_idx": 18}, {"type": "text", "text": "Definition of Operations. In the 3D mesh texturing, the canonical variable $\\mathbf{z}$ is the texture image of a 3D mesh, and the instance variables $\\{\\mathbf{w}_{i}\\}_{i=1:N}$ are rendered images from the 3D mesh. The projection operation $f_{i}$ is a rendering function where nearest-neighbor sampling is utilized to retrieve the color from the texture image to perspective view images. ", "page_idx": 18}, {"type": "text", "text": "The unprojection operation $g_{i}$ is performed using optimization where the texture image ${\\bf z}$ is updated to minimize the rendering loss with the multi-view images $\\left\\{\\mathbf{w}_{i}\\right\\}_{i=1:N}$ . The projection operation of 3D mesh texturing may involve mapping one pixel in the texture image $\\mathbf{z}$ to multiple pixels in a rendered image $\\mathbf{w}_{i}$ . Hence, this application corresponds to the 1-to- ${\\cdot n}$ projection case as in Section 3.4.3. ", "page_idx": 19}, {"type": "text", "text": "B.3 Details on Depth-to-360-Panorama Generation \u2014 Section 5.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide details of the depth-to-360-panorama generation experiments presented in Section 5.2.   \nRefer to Table 4 and Figure 7 for quantitative and qualitative results. ", "page_idx": 19}, {"type": "text", "text": "Evaluation Setup. We evaluate SyncTweedies and the baselines on 500 pairs of $360^{\\circ}$ panorama images and depth maps randomly sampled from the 360MonoDepth [43] dataset. For each $360^{\\circ}$ panorama image, we generate a text prompt using the output of BLIP [30] by providing a perspective view image of the panorama as input. ", "page_idx": 19}, {"type": "text", "text": "In the $360^{\\circ}$ panorama generation, we use eight perspective views by evenly sampling azimuths with $45^{\\circ}$ intervals at $0^{\\circ}$ elevation. Each perspective view has a field of view of $72^{\\circ}$ for diffusionsynchronization-based methods and $90^{\\circ}$ for MVDiffusion [56]. For evaluation, we project the generated $360^{\\circ}$ panorama image to ten perspective views with randomly sampled azimuths at $0^{\\circ}$ elevation and a field of view of $60^{\\circ}$ . Similarly, the reference set images are obtained by projecting each ground truth $360^{\\circ}$ panorama image into ten perspective views with azimuths randomly sampled and at $0^{\\circ}$ elevation. In total, we use $500\\times10=5,000$ perspective view images for evaluation. ", "page_idx": 19}, {"type": "text", "text": "Implementation Details. We set the resolution of a latent panorama image to $2{,}048\\times4{,}096$ and that of the latent perspective view images to $64\\times64$ . In the RGB space, a panorama image has a resolution of $1,024\\times2{,}048$ , and perspective view images have a resolution of $512\\times512$ . As done in the 3D mesh texturing, we apply the Voronoi-diagram-based filling [2] after each unprojection operation and employ the modified self-attention mechanism. ", "page_idx": 19}, {"type": "text", "text": "Definition of Operations. In the $360^{\\circ}$ panorama generation, the canonical variable $\\mathbf{z}$ represents a $360^{\\circ}$ panorama image, while the instance variables $\\bar{\\{\\mathbf{w}_{i}\\}}_{i=1:N}$ correspond to perspective views of the panorama. The mappings between the panorama image and the perspective views are computed as follows: First, we unproject the pixels of the perspective view image to the 3D space. Then, we apply two rotation matrices based on the azimuth and elevation angles. The pixels are then reprojected onto the surface of a unit sphere, represented as longitudes and latitudes. These spherical coordinates are finally converted to 2D coordinates on the panorama image. ", "page_idx": 19}, {"type": "text", "text": "Given the mappings, the projection operation $f_{i}$ samples colors from the panorama image using the nearest-neighbor method. Since a single pixel of a panorama image z can be mapped to multiple pixels of a perspective view image $\\mathbf{w}_{i}$ , the $360^{\\circ}$ panorama generation is a 1-to- ${\\cdot n}$ projection case, as discussed in Section 3.4.3. ", "page_idx": 19}, {"type": "text", "text": "B.4 Details on 3D Gaussian Splats Texturing \u2014 Section 5.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide details of the 3D Gaussian splats texturing experiment presented in Section 5.3. Quantitative and qualitative results are provided in Table 5 and Figure 8. ", "page_idx": 19}, {"type": "text", "text": "Evaluation Setup. For evaluation, we use 3D Gaussian splats trained with multi-view images from the Synthetic NeRF dataset [39], consisting of 8 objects. We generate 40 textured 3D Gaussian splats by utilizing five different prompts per scene. We use 50 views for texture generation and 150 unseen views for evaluation. ", "page_idx": 19}, {"type": "text", "text": "Implementation Details. As described in Section B, we employ ControlNet [64] which denoises latent images. To render the latent images, we replace the spherical harmonics coefficients of a 3D Gaussian splats to a 4-channel latent vector. For the optimization, we run 2,000 iterations with a learning rate of 0.025. When applicable, we perform the optimization in RGB space by decoding the latent variables for diffusion-synchronization-based methods. ", "page_idx": 19}, {"type": "text", "text": "Definition of Operations. The canonical variables $\\left\\{\\mathbf{z}_{j}\\right\\}_{j=1:M}$ are 3D Gaussian splats and the instance space variables $\\left\\{\\mathbf{w}_{i}\\right\\}_{i=1:N}$ are the rendered images from the 3D Gaussian splats. The projection operation $f_{i}$ is a volume rendering function [25, 26] where the colors (latent vectors) of multiple 3D Gaussian splats are composited to render a pixel. This corresponds to the $n$ -to-1 projection as discussed in Section 3.4.4. In 3D Gaussian splats texturing, only the colors of 3D Gaussian splats $\\mathbf{z}=\\{\\mathbf{s}_{j}\\}_{j=1:M}$ are optimized from multi-view images $\\{\\mathbf{w}_{i}\\}_{i=1:N}$ , while keeping other parameters, such as positions, fixed, as done in the $n$ -to-1 experiment in Section 3.4.4. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "06Vt6f2js7/tmp/183ba8c91728171e82851dec94b14c71bdd199cd2a9aafbe220807dd526768c7.jpg", "table_caption": ["Table 6: A quantitative comparison in arbitrary-sized image generation. KID is scaled by $10^{3}$ For each row, we highlight the column whose value is within $95\\%$ of the best. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Arbitray-Sized Image Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to the 1-to-1 projection case presented in Section 3.4.2, we present arbitrary-sized image generation. In contrast to depth-to-360-panorama generation, which corresponds to the 1-to- $n$ projection case, arbitrary-sized image generation is a 1-to-1 projection case. ", "page_idx": 20}, {"type": "text", "text": "Evaluation Setup. We follow the evaluation setup used in SyncDiffusion [29]. Using Stable Diffusion 2.0 [47] as the pretrained diffusion model, we generate 500 arbitrary-sized images of $512\\times3$ , 072 resolution per prompt. With six text prompts from SyncDiffusion [29], we generate a total of $500\\times6=3,$ , 000 arbitrary-sized images. For quantitative evaluation, we report FID [21], KID [6], and CLIP-S [42]. Each generated arbitrary-sized image is randomly cropped to partial view images with $512\\times512$ resolution. For the reference set, we generate 3, 000 images with a resolution of $512\\times512$ from the pretrained diffusion model using the same text prompts. ", "page_idx": 20}, {"type": "text", "text": "Implementation Details. The resolution of latent arbitrary-sized image is $64\\times384$ , and the resolution of an instance space sample is $64\\times64$ . We use deterministic DDIM [53] sampling with 50 steps. ", "page_idx": 20}, {"type": "text", "text": "Definition of Operations. The projection operation $f_{i}$ corresponds to cropping a partial view of the arbitrary-sized image, which is a 1-to-1 projection. The unprojection operation $g_{i}$ is the inverse of the $f_{i}$ which pastes the partial view image onto the canvas of the arbitrary-sized image. ", "page_idx": 20}, {"type": "text", "text": "Results. We report quantitative results in Table 6 and qualitative results in Figure 10. As mathematically proven in Section D, the quantitative results show that all diffusion synchronization cases exhibit comparable performances, which aligns with the observations from the 1-to-1 experiment in Section 3.4.2. This is further supported by the qualitative results in Figure 10, where all cases produce identical arbitrary-sized images, indicating that any option can be used when the projection is 1-to-1. ", "page_idx": 20}, {"type": "text", "text": "Results using Gaudi Intel-v2. Additionally, we present qualitative results of arbitrary-sized image generation using Intel Gaudi-v2 in Figure 11, along with a comparison of computation times between Intel Gaudi-v2 and NVIDIA A6000 in Figure 12. We observe that Intel Gaudi-v2 achieves 1.8 to 1.9 times faster runtimes compared to the NVIDIA A6000. ", "page_idx": 20}, {"type": "image", "img_path": "06Vt6f2js7/tmp/e097f51fc8b141daf5bf2bc1c20f850a399a65b92451dcae5b3551accaf101d4.jpg", "img_caption": ["Figure 10: Qualitative results of arbitrary-sized image generation. All diffusion synchronization processes generate identical results in the 1-to-1 projection. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "06Vt6f2js7/tmp/f59086d0102f4417cd801c8d47ff182a4a8db876a5d9a2d726448e26c646749f.jpg", "img_caption": ["Figure 11: Qualitative results of arbitrary-sized image generation using Intel Gaudi-v2. SyncTweedies (Case 2) is used for all text prompts. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "06Vt6f2js7/tmp/08133e6324c080b702741aec084e0171c2ad321972c917bcc4912a4945c082f7.jpg", "img_caption": ["Figure 12: Runtime comparison of NVIDIA RTX A6000 and Intel Gaudiv2. We use four different width sizes for the arbitrary-sized images: {512, 1024, $2048,3072\\dot{\\}$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D 1-to-1 Projection ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "It is mathematically guaranteed that Cases 1-5 become identical when the mappings are 1-to-1 and noises are initialized by projecting from the canonical space $\\mathbf{w}_{i}^{(T)}=f_{i}(\\mathbf{z}^{(T)})$ , where $\\mathbf{z}^{(T)}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . Note that $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ are linear operations and commutative with other linear operation such as $f_{i},A$ , and $g_{i}$ . Assume the following conditions hold: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathbf{z}}^{(T)}=\\mathcal{A}(\\{g_{i}(f_{i}(\\boldsymbol{\\mathbf{z}}^{(T)}))\\}),}\\\\ {\\mathcal{A}(\\{g_{i}(\\mathbf{w}_{i})\\})=\\mathcal{A}(\\{g_{i}(f_{i}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j})\\})))\\})\\quad\\forall\\{\\mathbf{w}\\}_{i=1}^{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Based on induction, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},\\phi^{(t)}(\\mathbf{z}^{(t)},\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(f_{i}((\\mathbf{z}^{(t)}))))\\})))}\\\\ &{\\quad\\quad=\\psi^{(t)}(\\mathbf{z}^{(t)},\\phi^{(t)}(\\mathcal{A}(\\{g_{i}(f_{i}(\\mathbf{z}^{(t)}))\\}),\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))\\})))}\\\\ &{\\quad\\quad=\\psi^{(t)}(\\mathbf{z}^{(t)},\\mathcal{A}(\\{g_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))\\})))}\\\\ &{\\quad\\quad=\\psi^{(t)}(\\mathcal{A}(\\{g_{i}(\\{g_{i}(\\mathbf{z}^{(t)})\\})\\},\\mathcal{A}(\\{g_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\}))}\\\\ &{\\quad\\quad=\\mathcal{A}(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\}))}\\\\ &{\\quad\\quad=\\mathcal{A}(\\{g_{i}(f_{i}(\\mathbf{z}(f_{i}(\\mathbf{z}^{(t)})(f_{j}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{j}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{j}(\\mathbf{z}^{(t)}))))))\\})}\\\\ &{\\quad\\quad=\\mathcal{A}(\\{g_{i}(f_{i}(\\mathbf{z}(f^{(t)}(f_{j}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{j}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{j}(\\mathbf{z}^{(t)}))))))\\}))}\\\\ &{\\quad\\quad=\\mathcal{A}(\\{g_{i}(f_{i}(\\mathbf{z}(f^{\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equality holds the induction hypothesis. This proves that Cases 4-5 are identical. For instance variable denoising cases we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\quad\\quad=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(f_{i}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\})),f_{i}(A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\quad\\quad=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(A(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})))\\})))}\\\\ &{\\quad\\quad=\\psi^{(t)}(f_{i}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\})),f_{i}(A(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\quad\\quad=f_{i}(A(\\{g_{j}(\\psi^{(t)}(\\mathbf{w}_{j}^{(t)},\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})))\\})))}\\\\ &{\\quad\\quad=f_{i}(A(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))))\\}))}\\\\ &{\\quad\\quad=f_{i}(A(\\{g_{j}(f_{j}(A(\\{g_{k}(\\psi^{(t)}(\\mathbf{w}_{k}^{(t)},\\phi^{(t)}(\\mathbf{w}_{k}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{k}^{(t)})))\\})))\\}))}\\\\ &{\\quad\\quad=f_{i}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t-1)})\\}))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equality holds the induction hypothesis. This proves that Cases 1-3 are identical. Lastly, based on the definition of the projection operation, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{i}^{(t-1)}=f_{i}(\\mathbf{z}^{(t-1)})}\\\\ &{\\quad\\quad\\quad=f_{i}(A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\}))))}\\\\ &{\\quad\\quad\\quad=f_{i}(A(\\{g_{j}(\\psi^{(t)}(\\mathbf{w}_{j}^{(t)},\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})))\\})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This proves that canonical variable denoising cases (Cases 4-5) are equivalent to Case 3. ", "page_idx": 23}, {"type": "text", "text": "We validate the proof both qualitatively and quantitatively in applications with 1-to-1 projection: ambiguous image generation and arbitrary-sized image generation in Section 3.4.2 and Section C, respectively, where all cases generate identical results. ", "page_idx": 23}, {"type": "image", "img_path": "06Vt6f2js7/tmp/837f17612b5625b8a66c77b37ff6d3cd0c40e9c07d3259199a5ade4175fa5fbc.jpg", "img_caption": ["Figure 13: Qualitative results of 3D mesh texture editing. We edit the textures of the 3D meshes generated from Genies [1] using SyncTweedies. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "06Vt6f2js7/tmp/e8b05fabdd600551f08fed09a4103f6f2140b63c36909ff0d71e585c17557cc4.jpg", "img_caption": ["Figure 14: Diversity comparison. Optimization-based method Paint-it [62] (Left) and diffusionsynchronization-based method, SyncTweedies (Right). SyncTweedies generates more diverse images. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E 3D Mesh Texture Editing and Diversity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we extend the 3D mesh texture generation from Section 5.1 and present a texture editing application, along with a diversity comparison of SyncTweedies to the optimization-based method Paint-it [62]. ", "page_idx": 24}, {"type": "text", "text": "E.1 3D Mesh Texture Editing ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Despite the recent successes of 3D generation models [1, 34], the textures of the generated 3D meshes often lack fine details. We utilize SyncTweedies to edit the textures of the generated 3D meshes, and enhance the texture quality. Specifically, we use the 3D meshes generated from a text-to-3D model, Genie [1]. ", "page_idx": 24}, {"type": "text", "text": "We follow SDEdit [36] to edit the textures of 3D meshes. We begin by adding noise at an intermediate time $t^{\\prime}$ to the texture image of the 3D mesh and then perform a reverse process starting from $t^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "Implementation Details. We set the CFG weight [22] to 30 and $t^{\\prime}$ to 0.8. For other settings, we follow the 3D mesh texture generation experiment presented in Section 5.1. ", "page_idx": 24}, {"type": "text", "text": "Results. We present qualitative results of 3D mesh texture editing in Figure 13. The 3D meshes edited with SyncTweedies exhibit fine details, including grafftii on the car in row 1, paintings on the lantern in row 2, and the intricate shells of the turtle in row 3. ", "page_idx": 24}, {"type": "text", "text": "E.2 Diversity of SyncTweedies ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 14, we present qualitative results of 3D mesh texturing using the optimization-based method (Paint-it [62]) and SyncTweedies with different random seeds. SyncTweedies generates more diverse texture images compared to Paint-it. ", "page_idx": 24}, {"type": "text", "text": "F Runtime and VRAM Usage Comparison ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 7: A runtime comparison in 3D mesh texturing and 3D Gaussian splats texturing applications. The best in each row is highlighted by bold. ", "page_idx": 24}, {"type": "table", "img_path": "06Vt6f2js7/tmp/d7d8b5a8f6ec5e91e4d6bb5b78434de3b5b3ecf84927fa4a41469796b2bf1955.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "06Vt6f2js7/tmp/fc710dd5a765cee851e7f88955b645bb9c873bb8d2d703110309044639cf875f.jpg", "table_caption": ["Table 8: A VRAM usage comparison in 3D mesh texturing and 3D Gaussian splats texturing applications. The best in each row is highlighted by bold. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "As discussed in Section 4, one of the advantages of diffusion synchronization processes is the fast computational speed. We compare the runtime performance of SyncTweedies with optimizationbased and iterative-view-updating-based methods in the 3D mesh texturing and the 3D Gaussian splats texturing. The quantitative results are presented in Table 7. ", "page_idx": 25}, {"type": "text", "text": "In the 3D mesh texturing application, SyncTweedies shows faster running time than other baselines except TEXTure [44] which shows comparable running time. However, TEXTure [44] generates suboptimal texture outputs as observed in Table 3 and Figure 6. The finetuning-based method Paint3D [63] shows a comparable running time to SyncTweedies, but it shows inferior quality, as seen in Table 3 and Figure 6. Another iterative-view-updating-based method, Text2Tex [10], improves quality of texture image by integrating a refinement module, but this comes at the cost of additional overhead in terms of running time. In contrast, SyncTweedies achieves running times that are 7 times faster than Text2Tex and even outperforms across all metrics as shown in Table 3. Lastly, SyncTweedies shows 11 times faster running time when compared to Paint-it [62], an optimization-based method. ", "page_idx": 25}, {"type": "text", "text": "In the 3D Gaussian splats texturing, SyncTweedies achieves the fastest running time. SyncTweedies is 3 times faster than the iterative-view-updating-based method IN2N [19], and 8 times faster than the optimization-based method, SDS [41]. This shows that SyncTweedies not only generates high-fidelity textures, but also excels other baselines in computational speed. We use the NVIDIA RTX A6000 for the runtime comparisons. ", "page_idx": 25}, {"type": "text", "text": "Additionally, in Table 8, we present results comparing the VRAM usage of SyncTweedies and the baselines, where our SyncTweedies requires around 6-9 GiB of memory, making it suitable for most GPUs. ", "page_idx": 25}, {"type": "text", "text": "G User Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conduct user studies to evaluate the textures of the generated 3D Gaussian splats [26] through Amazon\u2019s Mechanical Turk. Following the methodology of Ritchie [45], participants were presented with input text prompts and randomly sampled output images generated by our method and the baseline methods. Participants are asked to choose the most plausible image that aligns with the given text prompt. In Table 9, our results are the most preferred in the human evaluations compared to the other baselines. ", "page_idx": 25}, {"type": "text", "text": "Details on User Study. We conduct separate user studies comparing our method to diffusionsynchronization-based methods, optimization-based methods (SDS [41], MVDream-SDS [52]), and iterative-view-updating-based method (IN2N [19]). For each user study, we use 20 images in a shuffled order including five vigilance tasks. We collect survey responses only from participants who pass the vigilance tasks. Specifically, 94 out of 100 participants passed in the test with Case 5, 90 out of 100 passed with SDS [41], 95 out of 100 passed with MVDream-SDS [52], and 92 out of 100 passed with IN2N [19]. Screenshots of the user study, including an example of vigilance tasks, are shown in Figure 15. ", "page_idx": 25}, {"type": "table", "img_path": "06Vt6f2js7/tmp/1a53b4f4bd55930829e6287d1929c6dc0e3ebbb115618568f18f834b356e922b.jpg", "table_caption": ["Table 9: User study results in 3D Gaussian splats texturing application. SyncTweedies is the most preferred method over the baselines from human evaluators. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "06Vt6f2js7/tmp/68610326882900c5d60a9c55491d9d1bf420831b892a7aac4ecea018ec3ca1e7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 15: 3D Gaussian splats texturing user study screenshots. A screenshot of a main problem (left) and a vigilance task (right) is shown. ", "page_idx": 26}, {"type": "image", "img_path": "06Vt6f2js7/tmp/c6f63dacdbe2c82bcaf4ae918a358915521deb973e35fb851dbdabe556109623.jpg", "img_caption": ["(g) Instance variable denoising trajectory 4 "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "06Vt6f2js7/tmp/8e44248fc5a979926a1fefe6cbefc4e412eaec6d2b47b14f2571be5e55ee8b5c.jpg", "img_caption": ["(h) Canonical variable denoising trajectory 4 "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 16: Diagrams of diffusion synchronization processes. All feasible trajectories of the instance variable denoising process (left) and the canonical variable denoising process (right). Each row shares the same trajectory with different variables denoised. ", "page_idx": 26}, {"type": "text", "text": "H Analysis of Diffusion Synchronization Processes ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As outlined in Section 3.4.4, we present a comprehensive analysis of all possible diffusion synchronization processes, including the representative five diffusion synchronization processes introduced in Section 3.2. Following the main paper, we categorize diffusion synchronization processes into two types: the instance variable denoising process, where instance variables $\\{\\mathbf{w}_{i}^{(t)}\\}$ are denoised, and the canonical variable denoising process, which denoises a canonical variable $\\mathbf{z}^{(t)}$ directly. Unlike the representative cases, other all feasible cases either take inconsistent inputs when computing $\\epsilon_{\\theta}(\\cdot)$ , $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ or conduct the aggregation $\\boldsymbol{\\mathcal{A}}$ multiple times. Additionally, for a more exhaustive analysis, we introduce another type of diffusion synchronization processes, named the combined variable denoising process, which denoises $\\{\\mathbf{w}_{i}^{(t)}\\}$ and $\\mathbf{z}^{(t)}$ together. ", "page_idx": 27}, {"type": "text", "text": "We present a total of 46 feasible cases for the instance variable denoising process, 8 for the canonical variable denoising process, and an additional 6 representative cases for the combined variable denoising process. We provide instance variable denoising cases in Section H.2, and canonical variable denoising cases in Section H.3. Additionally, the six representative cases for the combined variable denoising process are detailed in Section H.4. We conduct a quantitative comparison of all listed cases following the experiment setup outlined in Section 3.4.1, and the results are presented in Section H.5. ", "page_idx": 27}, {"type": "text", "text": "H.1 Overview ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide the representative trajectories in Figure 16, where (a)-(b), (c)-(d), (e)-(f), and (g)-(h) follow the same trajectory but differ in the denoising variable, either instance or canonical, respectively. In each denoising case, there are $2^{2}=4$ possible trajectories determined by whether $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ are computed in the canonical space or instance space. This is because among the three computation layers\u2014 $\\cdot\\boldsymbol{\\epsilon}_{\\theta}(\\cdot)$ , $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot),$ \u2014only the last two operations can be computed in both the canonical space and the instance space unlike noise prediction which is only available in the instance space. Table 10 summarizes the computation spaces of $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\bar{\\psi^{(t)}(\\cdot,\\cdot)}$ , along with their corresponding trajectories. ", "page_idx": 27}, {"type": "table", "img_path": "06Vt6f2js7/tmp/306a8da9985a9d1d522be431d4b2128f42a15bbfdbff7af32f9af78d9395caac.jpg", "table_caption": ["Table 10: Computation space of each denoising trajectory. $\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ can be computed in both instance space $\\mathcal{W}_{i}$ or canonical space $\\mathcal{Z}$ , whereas noise prediction $\\epsilon_{\\theta}(\\cdot)$ can only be computed in the instance space. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Next, we introduce an additional operator ${\\mathcal{F}}_{i}$ that synchronizes instance variables. This operator unprojects a set of instance variables and averages them in the canonical space. Subsequently, the aggregated variables are reprojected to the instance space: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i}(\\{\\mathbf{w}_{j}\\}_{j=1:N})=f_{i}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j})\\}_{j=1:N})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The red arrows in the diagrams of Figure 16 indicate the potential incorporation of ${\\mathcal{F}}_{i}$ . Thus, a total of $2^{N}$ different cases can be derived from a trajectory marked by $N$ red arrows, depending on whether ${\\mathcal{F}}_{i}$ is applied to each variable or not. ", "page_idx": 27}, {"type": "text", "text": "Lastly, we review the five representative diffusion synchronization processes discussed in Section 3.2, along with two additional denoising processes: an instance variable denoising process that proceeds without synchronization (No Synchronization) and a canonical variable denoising process that averages the outputs of $\\psi^{(t)}(\\cdot,\\cdot)$ (Case 6): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{2hronization:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))}\\\\ &{\\quad\\quad\\mathrm{Case~1:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))}\\\\ &{\\quad\\quad\\mathrm{Case~2:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~3:~}\\mathbf{w}_{i}^{(t-1)}=\\mathcal{F}_{i}(\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))}\\\\ &{\\mathrm{Case~4:~}\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},\\phi^{(t)}(\\mathbf{z}^{(t)},A(\\{g_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))\\})))}\\\\ &{\\mathrm{Case~5:~}\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},A(\\{g_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\}))}\\\\ &{\\mathrm{Case~6:~}\\mathbf{z}^{(t-1)}=A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))))\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that Case 3 and 6 are identical except for the initialization, which can be either $\\{\\mathbf{w}_{i}^{(T)}\\}$ or $\\mathbf{z}^{(T)}$ . For the independent instance variable denoising process (No Synchronization), synchronization is only applied at the end of the denoising process. ", "page_idx": 28}, {"type": "text", "text": "H.2 Instance Variable Denoising Process ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here, we explore all possible instance variable denoising processes. Here, the canonical space $\\mathcal{Z}$ is employed to synchronize the outputs of $\\epsilon_{\\theta}(\\cdot),\\,\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ in the instance spaces. ", "page_idx": 28}, {"type": "text", "text": "Following the trajectory 1 shown in part (a) of Figure 16, marked by five red arrows, there are a total of $2^{5}={\\bar{3}}2$ possible denoising processes. This includes the independent instance variable denoising process (No Synchronization), where ${\\mathcal{F}}_{i}$ is not applied at any red arrow. Additionally, the three representative instance variable denoising processes, Cases 1-3, are also included, along with Cases 7-34 which are presented below: ", "page_idx": 28}, {"type": "text", "text": "$\\operatorname{Case}\\,7:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}))))$ $\\operatorname{Case}\\,8:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}))))$ ) $\\operatorname{Case}9:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))$ Ca $\\mathbf{:}10:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))$ ) Case $11:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))$ Case $12:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ ) Case $13:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ Case $14:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))$ Case $15:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ ) Case $16:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))$ Case $17:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ ) Cas $\\begin{array}{r}{\\geq18\\;:{\\mathbf w}_{i}^{(t-1)}=\\psi^{(t)}({\\mathbf w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}({\\mathbf w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}({\\mathbf w}_{i}^{(t)})))))}\\end{array}$ Cas $:19:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}))))))$ Case $20:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))$ Case $21:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}))$ ) Case $:22\\,:\\,\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))$ Case $23:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ ) Case $24:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))$ Case $25:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))$ Case $26:\\mathbf{w}_{i}^{(t-1)}=\\mathcal{F}_{i}(\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))$ Case $27:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))$ Cas $\\mathtt{e}\\;28:\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)})))))$ C $\\mathrm{ase~29:}\\ \\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~30:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}))))))}\\\\ &{\\mathrm{Case~31:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))))}\\\\ &{\\mathrm{Case~32:~}\\mathbf{w}_{i}^{(t-1)}=\\mathcal{F}_{i}(\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))}\\\\ &{\\mathrm{Case~33:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))}\\\\ &{\\mathrm{Case~34:~}\\mathbf{w}_{i}^{(t-1)}=\\mathcal{F}_{i}(\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, four cases are derived from the trajectory 2 shown in part (c) of Figure 16. These correspond to Cases 35-38 below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~35:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\mathrm{Case~36:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{j}^{(t)}))\\})))))}\\\\ &{\\mathrm{Case~37:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),f_{i}(\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\mathrm{Case~38:~}\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{i}^{(t)}),f_{i}(\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The trajectory 3 shown in part (e) of Figure 16 accounts for two cases, corresponding to Cases 39-40 below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case\\39:}\\ \\mathbf{w}_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))\\}))))}\\\\ &{\\mathrm{Case\\40:}\\ \\mathbf{w}_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\phi^{(t)}(A(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),A(\\{g_{j}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{j}^{(t)})))\\})))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lastly, the trajectory 4 shown in part (g) of Figure 16 includes Cases 41-48 below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:41:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})))\\})))}\\\\ &{:42:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{j}^{(t)}))))\\})))}\\\\ &{:43:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)}))))\\})))}\\\\ &{:44:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathbf{w}_{j}^{(t)},\\mathcal{F}_{i}(\\epsilon_{\\theta}(\\mathcal{F}_{i}(\\mathbf{w}_{j}^{(t)}))))\\})))}\\\\ &{:45:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathcal{F}_{i}(\\mathbf{w}_{j}^{(t)}),\\epsilon_{\\theta}(\\mathbf{w}_{j}^{(t)})))\\})))}\\\\ &{:46:\\textbf w_{i}^{(t-1)}=f_{i}(\\psi^{(t)}(\\mathcal{A}(\\{g_{j}(\\mathbf{w}_{j}^{(t)})\\}),\\mathcal{A}(\\{g_{j}(\\phi^{(t)}(\\mathcal{F}_{i}(\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "H.3 Canonical Variable Denoising Process ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here, we present all possible canonical variable denoising processes. Due to the absence of noise prediction in the canonical space, a process first redirects canonical variable $\\mathbf{z}^{(t)}$ to the instance spaces where a subsequence of operations $\\epsilon_{\\theta}(\\cdot),\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ are computed. ", "page_idx": 29}, {"type": "text", "text": "We exclude the application of ${\\mathcal{F}}_{i}$ to $\\mathbf{w}_{i}^{(t)}\\gets f_{i}(\\mathbf{z}^{(t)})$ , as the variable remains unchanged after the operation. Therefore, applying ${\\mathcal{F}}_{i}$ to $\\mathbf{w}_{i}^{(t)}\\gets f_{i}(\\mathbf{z}^{(t)})$ for the inputs of $\\epsilon_{\\theta}(\\cdot),\\,\\phi^{(t)}(\\cdot,\\cdot)$ and $\\psi^{(t)}(\\cdot,\\cdot)$ is not considered. ", "page_idx": 29}, {"type": "text", "text": "Case 4 which belongs to the trajectory 3, is visualized in part (f) of Figure 16. Cases 5 and 49 derive from the trajectory 4 which are shown in part (h) of Figure 16. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Case\\;49:}\\;\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},A(\\{g_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))\\})))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the trajectory 1, $2^{2}=4$ cases are possible, as shown in part (b) of Figure 16. This includes Case 6 along with Cases 50-52 below: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{50:\\mathbf{z}^{(t-1)}=A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))))\\}))}\\\\ &{51:\\mathbf{z}^{(t-1)}=A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))))\\})}\\\\ &{52:\\mathbf{z}^{(t-1)}=A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\mathcal{F}_{i}(\\phi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),\\mathcal{F}_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)}))))))\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lastly, trajectory 2, shown in part (d) of Figure 16, encompasses one possible case, corresponding to Case 53: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\langle\\,\\mathsf{S}3\\,:\\,\\mathbf{z}^{(t-1)}=A(\\{g_{i}(\\psi^{(t)}(f_{i}(\\mathbf{z}^{(t)}),f_{i}(\\phi^{(t)}(\\mathbf{z}^{(t)},\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z}^{(t)})))\\}))))\\})).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "H.4 Combined Variable Denoising Process ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we introduce combined variable denoising processes where both instance and canonical variables are denoised. This process synchronizes instance variables and a canonical variable by aggregating the unprojected instance variables and the canonical variable in the canonical space. ", "page_idx": 30}, {"type": "text", "text": "For clarity, we introduce additional operations below. $\\delta_{\\mathcal{Z}}(\\cdot)$ takes a variable in the canonical space $\\mathbf{z}\\in{\\mathcal{Z}}$ , projects it into the instance spaces, predicts noises in those spaces, and aggregates them back in the canonical space after the unprojection. $\\Phi_{\\mathcal{Z}}^{(t)}(\\cdot)$ then computes Tweedie\u2019s formula [46] based on the noise term computed by $\\delta_{\\mathcal{Z}}(\\cdot)$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\delta_{\\mathcal{Z}}(\\mathbf{z})=\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(f_{i}(\\mathbf{z})))\\})}}\\\\ {{\\Phi_{\\mathcal{Z}}^{(t)}(\\mathbf{z})=\\phi^{(t)}(\\mathbf{z},\\delta(\\mathbf{z})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similarly, given a set of variables in the instance spaces $\\{\\mathbf{w}_{i}\\}$ , the following operators aggregate the unprojected outputs of $\\psi^{(t)}(\\cdot,\\cdot),\\epsilon_{\\theta}(\\cdot)$ and $\\phi^{(t)}(\\cdot,\\cdot)$ in the canonical space: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{\\mathcal{W}_{i}}^{(t)}(\\{\\mathbf{w}_{i}\\})=\\mathcal{A}(\\{g_{i}(\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))\\})\\})}\\\\ &{\\delta_{\\mathcal{W}_{i}}(\\{\\mathbf{w}_{i}\\})=\\mathcal{A}(\\{g_{i}(\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)}))\\})}\\\\ &{\\Phi_{\\mathcal{W}_{i}}^{(t)}(\\{\\mathbf{w}_{i}\\})=\\mathcal{A}(\\{g_{i}(\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},\\epsilon_{\\theta}(\\mathbf{w}_{i}^{(t)})))\\})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We present joint variable denoising cases on the representative cases discussed in Section $_\\mathrm{H}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Case~}54\\,:\\,\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},\\phi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(A(\\{\\delta_{\\mathcal{W}_{i}}(\\{\\mathbf{w}_{i}^{(t)}\\}),\\delta_{\\mathcal{Z}}(\\mathbf{z}^{(t)})\\}))))}\\\\ &{\\mathrm{Case~}55\\,:\\,\\mathbf{w}_{i}^{(t-1)}=\\psi^{(t)}(\\mathbf{w}_{i}^{(t)},f_{i}(A(\\{\\Phi_{\\mathcal{W}_{i}}^{(t)}(\\{\\mathbf{w}_{i}^{(t)}\\}),\\Phi_{\\mathcal{Z}}^{(t)}(\\mathbf{z}^{(t)})\\})))}\\\\ &{\\mathrm{Case~}56\\,:\\,\\mathbf{w}_{i}^{(t-1)}=f_{i}(A(\\{\\Psi_{\\mathcal{W}_{i}}^{(t)}(\\{\\mathbf{w}_{i}^{(t)}\\}),\\mathbf{z}^{(t-1)}\\}))}\\\\ &{\\mathrm{Case~}57\\,:\\,\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},\\phi^{(t)}(\\mathbf{z}^{(t)},A(\\{\\delta_{\\mathcal{Z}}(\\mathbf{z}^{(t)}),\\delta_{\\mathcal{W}_{i}}(\\{\\mathbf{w}_{i}^{(t)}\\})\\})))}\\\\ &{\\mathrm{Case~}58\\,:\\,\\mathbf{z}^{(t-1)}=\\psi^{(t)}(\\mathbf{z}^{(t)},A(\\{\\Phi_{\\mathcal{W}_{i}}^{(t)}(\\{f_{i}(\\mathbf{z}^{(t)})\\}),\\Phi_{\\mathcal{W}_{i}}^{(t)}(\\{\\mathbf{w}_{i}^{(t)}\\})\\})}\\\\ &{\\mathrm{Case~}59\\,:\\,\\mathbf{z}_{t-1}=A(\\{\\Psi_{\\mathcal{W}_{i}}^{(t)}(\\{f_{i}(\\mathbf{z}^{(t)})\\}),A(\\{g_{i}(\\mathbf{w}_{i}^{(t-1)})\\})\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Cases 54-59 correspond to the combined variable denoising processes from Cases 1-6, respectively. In each of the above cases, we highlight the terms already present in the original representative case in orange and newly added variable to be synchronized together in purple. ", "page_idx": 30}, {"type": "text", "text": "H.5 Quantitative Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In Table 11, we present the quantitative results of the 60 diffusion synchronization processes listed above. We follow the same toy experiment setup described in Section 3.4.1 and Section B.1. As outlined in Section H.1, for all instance variable denoising processes, including the independent denoising case (No Synchronization), we perform the final synchronization at the end of the denoising process. For $n$ -to-1 projection, we utilize $M=10$ multiplane images as done in Section 3.4.4. ", "page_idx": 30}, {"type": "text", "text": "We report the quantitative results of all cases in Table 11. The results align with the observations of Table 1. In the 1-to-1 projection scenario, most diffusion synchronization processes exhibit similar performances. Except for Cases 55-56, the combined variable denoising processes (Cases 54-59) show suboptimal performances with FID [21] scores over 100. This indicates that denoising either instance variables or a canonical variable is sufficient to produce satisfactory and consistent results. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "When it comes to the 1-to- $^{\\cdot n}$ projection scenario, Cases 2 and 5 outperform the others, with some exceptions such as Cases 11 and 35. This trend is also consistent with the results in Section 3.4.3, highlighting the effectiveness of synchronizing the outputs of Tweedie\u2019s formula [46] $\\phi^{(t)}(\\cdot,\\cdot)$ even when compared to more complex diffusion synchronization processes. ", "page_idx": 31}, {"type": "text", "text": "Lastly, in the $n$ -to-1 projection scenario, Case 2 (SyncTweedies) is the only one that outperforms the others across all metrics. ", "page_idx": 31}, {"type": "text", "text": "In conclusion, as shown in Table 11, Case 2 (SyncTweedies) distinctly exhibits superior performance across various projection scenarios, outperforming even more convoluted cases. ", "page_idx": 31}, {"type": "table", "img_path": "06Vt6f2js7/tmp/d59046533a8367a6cce40b24ab0da6a0a14db486676deaa21bd4fcf3fded9dc9.jpg", "table_caption": ["Table 11: A quantitative comparison of all cases in ambiguous image generation. KID [6] is scaled by $10^{3}$ . For each column, we highlight the row whose value is within $95\\%$ of the best. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "06Vt6f2js7/tmp/fa3b9606c45555954b477d95baedeb6221ddedc029e53e9ff5f9eeefb563f743.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "06Vt6f2js7/tmp/2aca3e8a26e33098413b5000ca133c600d26b931775d117cbbcb6931cdabe51d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the claims made in the abstract and introduction throughout the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: In the last section of the main paper, we address the limitation of our work. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the results shown in the paper are obtained through experiments. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We provide experiment setups and implementation details in the main paper and the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The code is publicly released. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide detailed experiment setups in the main paper and the appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: Due to our limited computational resources, we were unable to report error bars. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide a runtime comparison of ours and other baselines in the appendix. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: This work conforms with the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We discuss societal impacts in the last section of the main paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: NA. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have properly cited all sources. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: NA. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide detailed setups for our user study in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We conducted the user study with IRB approval. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]