[{"figure_path": "wl44W8xpc7/tables/tables_2_1.jpg", "caption": "Table 1: Infinitesimal generators of the Affine group Aff(2). The set of six generators {L1,\u2026\u2026, L6} forms a basis of the corresponding Lie algebra.", "description": "This table lists the six infinitesimal generators of the Affine group Aff(2), which are used to parameterize affine transformations in two dimensions.  Each generator represents a specific type of transformation (translation along the x or y axis, scaling along the x or y axis, or shearing). The table shows the expression for each generator, the resulting one-parameter group of transformations it produces, and a short description of each transformation.", "section": "2.1 Images and Their Symmetries"}, {"figure_path": "wl44W8xpc7/tables/tables_7_1.jpg", "caption": "Table 2: Comparison with other symmetry discovery methods.", "description": "This table compares the proposed method with other state-of-the-art symmetry discovery methods.  The comparison focuses on what is being learned (transformation scales vs. symmetry generators), the prior knowledge assumed (completely known, partially known, or unknown symmetry generators), whether the symmetry is implicit or explicit in the dataset, how the symmetry is learned, and whether the methods were tested on real-world or toy datasets. The table highlights the advantages of the proposed method in terms of its minimal assumptions and ability to work with high-dimensional real-world data.", "section": "Related Work"}, {"figure_path": "wl44W8xpc7/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with other symmetry discovery methods.", "description": "This table compares the proposed method with other symmetry discovery methods, highlighting the differences in what is learned (transformation scales or subgroups), assumptions about prior knowledge (complete, partial, or none), and the viewpoint on symmetry (implicit or explicit).  It emphasizes the proposed method's advantages in handling high-dimensional real-world datasets and reducing the dimensionality of the search space.", "section": "Related Work"}, {"figure_path": "wl44W8xpc7/tables/tables_8_2.jpg", "caption": "Table 5: LPS of PDEs.", "description": "This table lists the Lie Point Symmetries (LPS) for several partial differential equations (PDEs): KdV, KS, Burgers, nKdV, and cKdV.  For each PDE, the table shows the infinitesimal generators representing the symmetries.  These symmetries describe transformations of the independent and dependent variables that leave the PDE invariant. The notation (\u03be, \u03bc) indicates the transformations on the independent variable x (\u03be) and the dependent variable u (\u03bc).", "section": "5.2 PDEs"}, {"figure_path": "wl44W8xpc7/tables/tables_8_3.jpg", "caption": "Table 3: Test accuracy in CIFAR-10.", "description": "This table presents the test accuracy results (%) for image classification using CIFAR-10 dataset. Four different methods are compared: no augmentation, default augmentation (horizontal flip and random crop), augmentation using affine transformations, and augmentation using learned symmetries.  The results show that both affine and learned symmetry augmentations significantly improve the accuracy compared to no augmentation and default augmentation.", "section": "5.1 Images"}, {"figure_path": "wl44W8xpc7/tables/tables_9_1.jpg", "caption": "Table 6: Test NMSE comparison of augmentation using LPS and AS in FNO learning for cKdV.", "description": "This table presents the results of an experiment comparing different data augmentation methods for training Fourier Neural Operators (FNOs) on the cylindrical Korteweg-de Vries (cKdV) equation.  The methods compared are no augmentation, augmentation using Lie Point Symmetries (LPS), augmentation using Approximate Symmetries (AS), and augmentation using both LPS and AS.  The table shows the Normalized Mean Squared Error (NMSE) for two different dataset sizes (25 and 27 data points).  Lower NMSE indicates better performance.", "section": "5.2 PDEs"}, {"figure_path": "wl44W8xpc7/tables/tables_15_1.jpg", "caption": "Table 7: Comparison of augmentation methods with different numbers of data for KdV and KS", "description": "This table compares the performance of three different augmentation methods (None, Ground-truth, and Ours) on two partial differential equations (KdV and KS) with varying amounts of training data (25, 27, and 29 data points).  The \"None\" column represents no augmentation. \"Ground-truth\" utilizes augmentations based on known symmetries.  The \"Ours\" column uses augmentations generated by the proposed method.  The results show the NMSE (Normalized Mean Squared Error) for each method and dataset size, demonstrating how the proposed method compares to using known symmetries and no augmentation at all.", "section": "5.2 PDEs"}, {"figure_path": "wl44W8xpc7/tables/tables_16_1.jpg", "caption": "Table 8: Comparison of augmentation methods with different numbers of data for nKdV and cKdV", "description": "This table compares the performance of different data augmentation methods on two variations of the Korteweg-de Vries equation (nKdV and cKdV).  The \"None\" column represents no augmentation, while the \"Ours\" column shows the results when using the learned symmetries from the proposed method. The Normalized Mean Squared Error (NMSE) is reported for various dataset sizes (2<sup>5</sup>, 2<sup>7</sup>, 2<sup>9</sup> data points). Lower NMSE values indicate better model performance.", "section": "5.2 PDEs"}, {"figure_path": "wl44W8xpc7/tables/tables_19_1.jpg", "caption": "Table 9: Test NMSE comparison of augmentation using Whittaker-Shannon interpolation and bilinear interpolation.", "description": "This table presents a comparison of the test Normalized Mean Squared Error (NMSE) for the Kuramoto-Sivashinsky (KS) equation using three different augmentation methods: No augmentation, Whittaker-Shannon interpolation, and bilinear interpolation.  The results show a significant improvement in NMSE when using Whittaker-Shannon interpolation compared to the other two methods.", "section": "C Experiment Results"}, {"figure_path": "wl44W8xpc7/tables/tables_21_1.jpg", "caption": "Table 10: The number of correctly learned symmetries across values of wsym, wLips, and wortho.", "description": "This table presents the results of a hyperparameter study on the weights of the three loss functions (symmetry loss, orthonormality loss, Lipschitz loss) used in the symmetry learning algorithm.  It shows how many of the three expected symmetries were correctly learned in the first three slots of the model for various combinations of the weights.  This helps determine appropriate weighting of the losses for optimal symmetry discovery. ", "section": "F Hyperparameter analysis"}]