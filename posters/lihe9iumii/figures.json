[{"figure_path": "liHe9iumIi/figures/figures_3_1.jpg", "caption": "Figure 1: FewViewGS pipeline. Our method consists of a multi-stage training scheme of (a) pre-training, (b) intermediate, and (c) tuning stages. Top right: pre-training / tuning. At the beginning and end, Gaussians are optimized solely on the known input views, utilizing color re-rendering loss and regularization terms on total opacity and local appearance. Bottom right: intermediate. Correspondences are first extracted from the pairs of training images and projected onto the virtual sampled views. Given the projected and virtual renders, color, geometry, and semantic losses are calculated at the projected pixels in the new views.", "description": "This figure illustrates the FewViewGS pipeline, a multi-stage training approach for novel view synthesis. It shows three stages: pre-training (using only known views), intermediate (introducing correspondence-driven losses for novel views sampled between known views), and tuning (fine-tuning on known views). The pre-training and tuning stages focus on optimizing Gaussians using color re-rendering loss and regularization. The intermediate stage leverages correspondences between known views to project and render virtual views, using color, geometry, and semantic losses to enforce consistency between known and novel views.", "section": "3 Method"}, {"figure_path": "liHe9iumIi/figures/figures_6_1.jpg", "caption": "Figure 2: Qualitative comparison on DTU [12] and LLFF [19] datasets. The results show that RegNeRF tends to produce blurred outcomes. 3DGS and DNGaussian introduce artifacts in the novel view. In contrast, our method generates better qualitative results.", "description": "This figure displays a qualitative comparison of novel view synthesis results on the DTU and LLFF datasets using four different methods: 3DGS, RegNeRF, DNGaussian, and the proposed FewViewGS method.  Ground truth images are also shown for comparison. The results illustrate that FewViewGS produces sharper, more artifact-free images compared to the other methods, especially in challenging scenarios where other methods lead to blurred or distorted outputs.", "section": "4 Experiments"}, {"figure_path": "liHe9iumIi/figures/figures_7_1.jpg", "caption": "Figure 2: Qualitative comparison on DTU [12] and LLFF [19] datasets. The results show that RegNeRF tends to produce blurred outcomes. 3DGS and DNGaussian introduce artifacts in the novel view. In contrast, our method generates better qualitative results.", "description": "This figure shows a qualitative comparison of novel view synthesis results on the DTU and LLFF datasets using different methods: 3DGS, RegNeRF, DNGaussian, and the proposed FewViewGS method.  The results highlight that FewViewGS produces sharper, more artifact-free images compared to the other methods, particularly in areas with less supervision.", "section": "4 Experiments"}, {"figure_path": "liHe9iumIi/figures/figures_13_1.jpg", "caption": "Figure 1: FewViewGS pipeline. Our method consists of a multi-stage training scheme of (a) pre-training, (b) intermediate, and (c) tuning stages. Top right: pre-training / tuning. At the beginning and end, Gaussians are optimized solely on the known input views, utilizing color re-rendering loss and regularization terms on total opacity and local appearance. Bottom right: intermediate. Correspondences are first extracted from the pairs of training images and projected onto the virtual sampled views. Given the projected and virtual renders, color, geometry, and semantic losses are calculated at the projected pixels in the new views.", "description": "This figure illustrates the multi-stage training process of the FewViewGS method.  It shows three stages: pre-training, intermediate, and tuning.  The pre-training and tuning stages optimize 3D Gaussians using only known views, focusing on color re-rendering loss and regularization. The intermediate stage leverages correspondences between training image pairs, warping them to novel views sampled between them. This stage applies color, geometry, and semantic losses to ensure consistency between novel and known views.", "section": "3 Method"}, {"figure_path": "liHe9iumIi/figures/figures_14_1.jpg", "caption": "Figure 2: Qualitative comparison on DTU [12] and LLFF [19] datasets. The results show that RegNeRF tends to produce blurred outcomes. 3DGS and DNGaussian introduce artifacts in the novel view. In contrast, our method generates better qualitative results.", "description": "This figure compares the qualitative results of novel view synthesis on the DTU and LLFF datasets using different methods, including RegNeRF, 3DGS, DNGaussian, and the proposed FewViewGS method.  It highlights that FewViewGS produces sharper and more artifact-free results than the other methods, addressing the limitations of RegNeRF's blurriness and artifacts in 3DGS and DNGaussian, especially in novel view generation.", "section": "4 Experiments"}, {"figure_path": "liHe9iumIi/figures/figures_14_2.jpg", "caption": "Figure 5: Comparison of the results with our proposed multi-view alignment.", "description": "This figure shows the qualitative comparison of novel view synthesis results using different combinations of loss functions.  The first image shows results from the baseline 3D Gaussian Splatting (3DGS). Subsequent images show the improvements achieved by adding geometric consistency loss, then color consistency loss, and finally semantic consistency loss. The last image shows the ground truth. The improvements in visual quality are evident as more loss terms are incorporated, demonstrating the effectiveness of the multi-view consistency constraint in enhancing the realism and coherence of the synthesized novel views.", "section": "3 Method"}, {"figure_path": "liHe9iumIi/figures/figures_14_3.jpg", "caption": "Figure 6: Comparison of the results with different feature networks.", "description": "This figure compares the visual results of novel view synthesis using different feature networks for semantic loss in FewViewGS.  It demonstrates the impact of the choice of feature extractor on the final image quality, showing results with CLIP, DINOv2, and VGG16 features, and comparing these to the ground truth (GT) and the baseline 3DGS method.", "section": "4.3 Ablation Study"}, {"figure_path": "liHe9iumIi/figures/figures_14_4.jpg", "caption": "Figure 7: Comparison of the matched results with different feature-matching algorithms. Roma[9] and SIFT[17] implement feature matching between the Imagel and Image2.", "description": "This figure compares the performance of two feature matching algorithms, RoMa and SIFT, in finding correspondences between two images of similar objects (Image 1 and Image 2).  RoMa demonstrates superior matching capabilities with more accurate and numerous correspondences highlighted in red lines, compared to SIFT, which shows significantly fewer and less precise matches. This difference in matching quality directly affects the subsequent novel view synthesis process.  The more accurate matches provided by RoMa contribute to improved visual coherence and reduce artifacts in the final rendered novel view.", "section": "3.2 Novel View Consistency"}, {"figure_path": "liHe9iumIi/figures/figures_15_1.jpg", "caption": "Figure 8: Comparison of the results without and with feature matching results.", "description": "This figure shows an ablation study on feature matching. The leftmost image shows the results using 3DGS without feature matching. The next two images show the results with feature matching, with the middle image showing results when using only single-view correspondences (i.e., no multi-view consistency). The rightmost image shows the ground truth. The results show that feature matching greatly improves the results, with multi-view consistency further improving the results. This demonstrates the importance of feature matching and multi-view consistency for accurate novel view synthesis.", "section": "3.3 Locality Preserving Regularization"}, {"figure_path": "liHe9iumIi/figures/figures_15_2.jpg", "caption": "Figure 9: Comparison between single-stage and multi-stage training with our multi-view consistency constraints.", "description": "This figure shows a comparison of novel view synthesis results using single-stage training versus multi-stage training, highlighting the improvements achieved by enforcing multi-view consistency constraints in the multi-stage approach.  The single-stage training leads to blurry results, especially noticeable in the Smurf and grocery scenes, indicating issues with depth and view consistency.  In contrast, the multi-stage training produces sharper, more realistic novel views, demonstrating the effectiveness of the proposed method in handling few-shot scenarios. The ground truth (GT) images are provided for reference.", "section": "3.4 Multi-stage Training Scheme"}]