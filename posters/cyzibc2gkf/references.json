{"references": [{"fullname_first_author": "David Abel", "paper_title": "A theory of state abstraction for reinforcement learning", "publication_date": "2019-01-01", "reason": "This paper introduces a theoretical framework for state abstraction in reinforcement learning, which is a core concept used in the current paper's approach to off-policy evaluation."}, {"fullname_first_author": "Cameron Allen", "paper_title": "Learning Markov state abstractions for deep reinforcement learning", "publication_date": "2021-01-01", "reason": "This paper develops methods for learning Markov state abstractions, a key component of the proposed STAR framework for consistent off-policy evaluation."}, {"fullname_first_author": "Lihong Li", "paper_title": "Towards a unified theory of state abstraction for MDPs", "publication_date": "2015-01-01", "reason": "This paper provides a theoretical foundation for state abstraction in Markov decision processes (MDPs), which is fundamental to the approach of abstract reward processes."}, {"fullname_first_author": "Richard S Sutton", "paper_title": "Reinforcement Learning: An Introduction", "publication_date": "2018-01-01", "reason": "This book is a foundational text in reinforcement learning, providing the necessary background and context for understanding the concepts and challenges in off-policy evaluation."}, {"fullname_first_author": "Nan Jiang", "paper_title": "Doubly robust off-policy value evaluation for reinforcement learning", "publication_date": "2016-01-01", "reason": "This paper introduces a doubly robust method for off-policy evaluation, which is an important existing technique related to the proposed STAR framework."}]}