[{"type": "text", "text": "Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shreyas Chaudhari University of Massachusetts schaudhari@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Ameet Deshpande Princeton University asd@cs.princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Bruno Castro da Silva University of Massachusetts bsilva@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Philip S. Thomas University of Massachusetts pthomas@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for off-policy evaluation (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators\u2014which include existing OPE methods as special cases\u2014that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call abstract reward processes (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Within reinforcement learning (RL), off-policy evaluation (OPE) is the foundational challenge of evaluating the performance, $J(\\pi)$ , of policies $\\pi$ that are different from the ones used to generate data. OPE methods are a general-purpose tool that can be used as part of a local policy search algorithm [45] to provide insight about policies similar to the current policy, or as a tool to evaluate policies without requiring their actual deployment for high-risk applications like those in healthcare [38], education [35, 15], and recommendation systems [5, 7]. Despite many recent advances in OPE, existing methods struggle to give accurate predictions for many real-world applications [52], showing the need for new perspectives on OPE. ", "page_idx": 0}, {"type": "text", "text": "Existing methods can be broadly divided into two categories: importance sampling (IS) based and model-based [58]. IS-based methods are typically consistent (i.e., their predictions converge probabilistically to the correct value in the limit as the amount of data approaches infinity), but have variance that increases exponentially with the horizon [29, 31]. Model-based methods have lower variance but often introduce bias due to model class mismatch and are not generally guaranteed to be consistent [36, 10]. A third set of methods, which we call mixture methods, combine the predictions obtained from both of these categories [22, 53]. However, in some cases, combining the predictions also combines the drawbacks\u2014high variance and bias. This leads us to ask: Can we develop a framework for OPE that yields predictions that are both consistent and low variance? ", "page_idx": 0}, {"type": "image", "img_path": "cYZibc2gKf/tmp/9d92ac66b0b5e348b79bcecca603196563352b315b3075e75ddd8acf4b9cab51.jpg", "img_caption": ["Figure 1: (a): MDP $M$ and policy $\\pi_{b}$ are transformed into a discrete abstract reward process (ARP) using a state abstraction function $\\phi$ . The ARP aggregates rewards (denoted by stars) and transition probabilities from all states that map to each abstract state. (b): A model of the ARP for the evaluation policy $\\pi_{e}$ is constructed by: reweighting data generated by $\\pi_{b}$ with importance weights $\\rho$ (middle), applying the state abstraction function $\\phi$ , and performing weighted maximum likelihood estimation of the ARP (right). The expected return of a model of this ARP estimated from off-policy data is a consistent estimator of the expected return of $\\pi_{e}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a new framework that attains this goal by combining the machinery underlying IS-based and model-based approaches (not just their predictions). Our proposed framework is a fundamentally different approach to OPE that incorporates importance sampling into model learning for OPE. Our approach is motivated by the intuition that humans build small mental models of their environment to plan and predict, selectively abstracting away information that is not relevant to the problem at hand [54]. Similarly, complex sequential decision processes can be distilled into compact models that hold sufficient information for (off-)policy evaluation. Specifically, we propose creating small tabular models (even for continuous environments), which we call abstract reward processes (ARPs), customized for the problem at hand and for the policy being evaluated. We call this framework for constructing a range of ARPs state-abstract reward processes (STAR). ", "page_idx": 1}, {"type": "text", "text": "Idea Summary: A Markov decision process (MDP) combined with a policy $\\pi$ induces a Markov chain with rewards, called a Markov reward process (MRP). A model of this process can be estimated to evaluate policy $\\pi$ . However, two main challenges arise: (1) like any model-based approach, estimating an MRP can introduce asymptotic bias if the chosen model class cannot represent the underlying MRP; and (2) the model of the MRP must be accurately estimated from off-policy data, i.e., data generated by a behavior policy $\\pi_{b}$ that differs from the evaluation policy $\\pi_{e}$ . The proposed framework addresses both challenges by modeling a special instantiation of an MRP, as detailed next. ", "page_idx": 1}, {"type": "text", "text": "First, to address potential model class mismatch, we propose mapping the large and possibly continuous set of states of the MDP to a finite set of abstract states using a discrete state abstraction function. We then represent the resulting MRP defined over abstract states, referred to as the abstract reward process (ARP), using tabular models. Since the ARP is finite, tabular models can represent it accurately, as depicted in Figure 1(a). However, using a discrete state abstraction may lead to loss of state information that can potentially introduce modeling errors. Surprisingly, we prove that despite state information being abstracted away, the maximum likelihood model of an ARP estimated from on-policy data provides consistent estimates of the expected return of the behavior policy $\\pi_{b}$ . ", "page_idx": 1}, {"type": "text", "text": "Next, to estimate a model of the ARP corresponding to $\\pi_{e}$ from data generated by $\\pi_{b}$ , we reweight occurrences of abstract states in the off-policy dataset using importance sampling, see Figure 1(b). In expectation, this has the effect of updating the abstract state visitation counts to reflect those resulting from the policy being evaluated. We prove that the weighted maximum likelihood estimate of a model of the ARP estimated from this off-policy dataset provides consistent estimates of the performance of the evaluation policy.The integration of importance sampling into model estimation permits a favorable interpretation of weight clipping for mitigating variance (see Section 4.1). ", "page_idx": 1}, {"type": "text", "text": "The STAR framework offers two adjustable knobs\u2014the state abstraction function, and the amount of weight clipping\u2014that instantiate a range of OPE estimators. Varying the configurations of these knobs results in different bias-variance trade-offs for OPE, with existing OPE methods forming special cases of the range of estimators that lie within this framework. ", "page_idx": 1}, {"type": "text", "text": "Contributions: We empirically evaluate estimators instantiated in this framework on synthetic domains and a healthcare simulator built from real-world ICU data, where the best STAR estimator significantly outperforms baselines in all cases, and even the median STAR estimator surpasses baselines in seven out of twelve cases. It must be emphasized that this work does not propose a specific estimator for OPE; rather, it introduces a fundamentally different framework that offers fresh insights on approaches for off-policy evaluation. These insights open up exciting avenues for new research questions and future directions. In this paper, we introduce: ", "page_idx": 2}, {"type": "text", "text": "(1) the first model-based approach for OPE that guarantees asympotic correctness of the estimates without model class assumptions, even for continuous state MDPs (Theorem 4.1).   \n(2) the concept of abstract reward processes for consistent OPE. ARPs abstract away the complexity of the underlying problem, and distill sufficient information for accurate policy evaluation (Theorem 3.1). Being finite, they can be consistently estimated.   \n(3) a generalizing framework that provides a fresh perspective on OPE by merging the machinery of model-based and IS-based approaches. The framework offers two tunable knobs, various configurations of which instantiate a range of OPE estimators with varying bias-variance characteristics. Existing model-free and model-based methods are special cases in this framework. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An MDP is a tuple $\\boldsymbol{M}\\,:=\\,(\\boldsymbol{S},\\boldsymbol{\\mathcal{A}},p,r,\\gamma,\\eta)$ where $\\boldsymbol{S}$ is the set of states, $S_{t}$ is the state at time $t~\\in~\\{0,1,\\ldots\\}$ , $\\boldsymbol{\\mathcal{A}}$ is the set actions, $A_{t}$ is the action at time $t$ , $p~:~\\mathcal{S}\\,\\times\\,\\mathcal{A}\\,\\times\\,\\mathcal{S}~\\to~[0,1]$ is the transition function that characterizes state transition dynamics according to $p(s,a,s^{\\prime}):=$ $\\operatorname*{Pr}(S_{t+1}{=}s^{\\prime}|S_{t}{=}s,A_{t}{=}a)$ , $r\\,:\\,S\\,\\times\\,A\\,\\rightarrow\\,\\mathbb{R}$ is the reward function that characterizes rewards according to $\\boldsymbol{r}(s,a)~:=~\\mathbb{E}[R_{t}|S_{t}\\!=\\!s,A_{t}\\!=\\!a]$ , $\\gamma~\\in~[0,1]$ is the reward discount parameter, and $\\eta\\,:\\,S\\,\\to\\,[0,1]$ characterizes the initial state distribution according to $\\eta(s):=\\operatorname{\\bar{Pr}}(S_{0}{=}s)$ .1 A policy $\\pi:S\\times A\\rightarrow[0,1]$ characterizes how actions can be selected given the current state according to $\\pi(s,a):=\\operatorname*{Pr}(A_{t}{=}a|\\dot{S}_{t}{=}s)$ . We consider finite horizon MDPs [49] where episodes terminate by some (unspecified) time $T\\in\\mathbb N$ \u2014which is common in practical applications of OPE. For simplicity, we set $\\gamma=1$ , allowing us to omit $\\gamma$ terms. ", "page_idx": 2}, {"type": "text", "text": "For OPE, a dataset $\\mathcal{D}_{n}^{(\\pi_{b})}$ is collected by deploying a behavior policy $\\pi_{b}$ on the MDP $M$ . The dataset of $n$ logged trajectories is denoted by $\\mathcal{D}_{n}^{(\\pi_{b})}:=\\{H^{i}\\}_{i=1}^{n}$ where each $H^{i}:=(S_{0}^{i},A_{0}^{i},R_{0}^{i},S_{1}^{i},\\dots)$ represents an independent trajectory generated by executing $\\pi_{b}$ . The performance of an evaluation policy $\\pi_{e}$ is its expected return, denoted by2 ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\pi_{e}):=\\mathbb{E}\\left[\\sum_{t=1}^{T}R_{t};\\pi_{e}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The problem of off-policy evaluation entails estimating $J(\\pi_{e})$ with access only to data $\\mathcal{D}_{n}^{(\\pi_{b})}$ , generated by a behavior policy $\\pi_{b}$ , without additional interaction with the MDP. To ensure that samples in $\\dot{\\mathcal{D}}_{n}^{(\\pi_{b})}$ are sufficiently informative, we make the common assumption that any outcome under $\\pi_{e}$ has non-negligible probability of occurring under $\\pi_{b}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1. There exists an (unknown) $\\varepsilon\\ \\geq\\ 0$ such that for all $s\\ \\in\\ S$ and $a\\ \\in\\ {\\mathcal{A}}$ , $(\\pi_{b}(s,\\bar{a})<\\varepsilon)\\Longrightarrow(\\pi_{e}(s,a)=0)$ . ", "page_idx": 2}, {"type": "text", "text": "Background: For a detailed review of OPE methods, we refer the reader to surveys by Voloshin et al. [58] and Uehara et al. [56]. Concepts fundamental to this approach are briefly introduced here. ", "page_idx": 2}, {"type": "text", "text": "1. Importance Sampling: Importance sampling [25] enables unbiased estimation of the expected value, $\\mathbb{E}[f(X)]$ , of a function $f$ applied to a random variable $X\\sim p$ , given samples of a different random variable $Y\\sim\\ q$ . The importance sampling estimator is $\\bar{(}p(Y)/q(\\bar{Y}))f(Y)$ , where $p(Y)/q(Y)$ is a term called an importance weight. This technique can provide unbiased estimates (i.e., $\\mathbb{E}\\ [(p(Y)/q(Y))f(Y)]=\\mathbb{E}\\left[f(X)\\right])$ and has proven effective for variance reduction in Monte Carlo sampling [44] and for model-free OPE in RL [42]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2. State Abstraction: State abstraction aims to reduce the size of the state space by grouping together similar states in a way that does not change the essence of the underlying problem [28, 43, 1]. A state abstraction function $\\phi:S\\rightarrow{\\mathcal{Z}}$ lies in the set of functions $\\phi\\in\\Phi$ that map each state $s\\in S$ to an abstract state $z\\in{\\mathcal{Z}}$ . We consider abstraction functions that partition $\\boldsymbol{S}$ into disjoint sets, where $\\mathcal{Z}$ is a finite set. ", "page_idx": 3}, {"type": "text", "text": "Notation: Indicator functions are abbreviated for clarity. For example, $\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}:=\\mathbf{1}\\{\\phi(S_{t+1}^{(i)})=$ $z_{.}^{\\prime},\\phi(S_{t}^{(i)})=z\\}$ denotes the occurrence of abstract states $z$ and $z^{\\prime}$ at time steps $t$ and $t+1$ in the $i^{\\mathrm{th}}$ logged trajectory, with ${\\bf1}_{t}^{i}\\{z\\}$ defined correspondingly. The expected return of $\\pi$ when obtained from $O$ \u2014where $O$ may be the MDP, or an ARP\u2014is denoted by $\\bar{J}(\\pi;O)$ . The sample estimate of a variable $y$ estimated from $n$ samples is denoted by $\\hat{y}_{n}$ . Summation limits are often dropped for brevity, with $\\textstyle\\sum_{t}$ denoting $\\textstyle\\sum_{t=0}^{T}$ and $\\textstyle\\sum_{i}$ denoting $\\textstyle\\sum_{i=1}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "2.1 Markov Reward Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A Markov reward process (MRP) extends the idea of a Markov chain by associating states with rewards. Formally, an MRP is a tuple $(\\boldsymbol{\\mathcal{X}},p,\\boldsymbol{r},\\gamma,\\eta)$ where $\\mathcal{X}$ is the set of states of the MRP, $X_{t}$ is the state at time $t$ , $p:\\mathcal{X}\\times\\mathcal{X}\\to[0,1]$ is the transition function where $p(x,x^{\\prime}):=\\operatorname*{Pr}(X_{t+1}{=}x^{\\prime}|X_{t}{=}x)$ $r:\\mathcal{X}\\rightarrow\\mathbb{R}$ is the reward function where $r(x):=\\mathbb{E}[R_{t}|X_{t}{=}x]$ , $\\gamma\\in[0,1]$ is the discount factor, and $\\eta:\\mathcal{X}\\rightarrow[0,1]$ is the starting state distribution. We consider finite horizon MRPs where episodes terminate by some (unspecified) timestep and set $\\gamma=1$ . ", "page_idx": 3}, {"type": "text", "text": "A specific MRP is induced by the use of a fixed policy $\\pi$ on an MDP $M$ , where $\\mathcal{X}=\\mathcal{S}$ . The resulting transition and reward functions, denoted by $p^{\\pi}$ and $r^{\\pi}$ respectively, are: ", "page_idx": 3}, {"type": "equation", "text": "$$\np^{\\pi}(x,x^{\\prime})=\\frac{\\sum_{t}\\operatorname*{Pr}(S_{t+1}=x^{\\prime},S_{t}=x;\\pi)}{\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)},\\quad r^{\\pi}(x)=\\frac{\\sum_{t}\\mathbb{E}\\left[R_{t}|S_{t}=x;\\pi\\right]\\operatorname*{Pr}(S_{t}=x;\\pi)}{\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Markov property [37] allows for further simplification of the above expressions (detailed in Appendix A.1), but this form is most conducive to our subsequent discussion. In this work, we focus on a specific instantiation of an MRP, described in the next section, where the set of states $\\mathcal{X}$ of the MRP are outputs of a state abstraction function $\\phi\\in\\Phi$ . ", "page_idx": 3}, {"type": "text", "text": "3 Abstract Reward Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An abstract reward process is a Markov reward process\u2014derived from MDP $M$ and policy $\\pi$ and defined over abstract states\u2014that we use to evaluate $\\pi$ . The ARP provides two primary benefits for policy evaluation: (1) it preserves sufficient information to exactly evaluate the policy $\\pi$ , and (2) the ARP can be consistently estimated from data. In this section, we formalize the concept of an ARP, and highlight the theoretical and practical benefits of using ARPs for policy evaluation. ", "page_idx": 3}, {"type": "text", "text": "Given a state abstraction function $\\phi:S\\rightarrow{\\mathcal{Z}}$ , the ARP $\\Re_{\\phi}^{\\pi}$ is defined such that $\\mathcal X=\\mathcal Z$ . Formally, $\\Re_{\\phi}^{\\pi}$ is an MRP $(\\mathcal{Z},\\mathrm{P}_{\\phi}^{\\pi},\\mathrm{R}_{\\phi}^{\\pi},\\eta_{\\phi})$ , with $\\gamma=1$ (see Appendix A.1 for a discussion on termination in ARPs and MRPs). The components of the ARP are defined over abstract states as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime})\\!:=\\!\\frac{\\sum_{t}\\mathrm{Pr}(\\phi(S_{t+1})\\!=\\!z^{\\prime},\\phi(S_{t})\\!=\\!z;\\pi)}{\\sum_{t}\\mathrm{Pr}(\\phi(S_{t})\\!=\\!z;\\pi)},\\mathrm{R}_{\\phi}^{\\pi}(z)\\!:=\\!\\frac{\\sum_{t}\\mathbb{E}[R_{t}|\\phi(S_{t})\\!=\\!z;\\pi]\\operatorname*{Pr}(\\phi(S_{t})\\!=\\!z;\\pi)}{\\sum_{t}\\mathrm{Pr}(\\phi(S_{t})\\!=\\!z;\\pi)},\\mathrm{R}_{\\phi}^{\\pi}(z)\\!:=\\!\\frac{\\sum_{t}\\mathrm{Pr}(\\phi(S_{t})\\!=\\!\\theta)\\!\\!\\!\\!\\operatorname{Pr}(\\phi(S_{t})\\!=\\!z;\\pi)}{\\sum_{t}\\mathrm{Pr}(\\phi(S_{t})\\!=\\!z;\\pi)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\eta_{\\phi}(z):=\\mathrm{Pr}(\\phi(S_{0})=z)$ . These expressions cannot be simplified further, unlike the case of an MRP [2]. Since $\\mathcal{Z}$ is a finite set, i.e., the abstract states are discrete, the components of the ARP can be represented by matrices (we use uppercase letters to emphasize this). The expected return of $\\Re_{\\phi}^{\\pi}$ can be computed efficiently using a linear solver to evaluate the expression $J(\\pi;\\Re_{\\phi}^{\\pi}):=(\\mathrm{I}-\\mathrm{P}_{\\phi}^{\\pi})^{-1}\\dot{\\mathrm{R}}_{\\phi}^{\\pi}\\eta_{\\phi}$ , or via Monte Carlo rollouts of the reward process. ", "page_idx": 3}, {"type": "text", "text": "ARPs are Performance Preserving: The expected return of an ARP has a surprising property: even though some state information is abstracted away to create simple discrete abstract states, the finite ", "page_idx": 3}, {"type": "text", "text": "ARP, derived from a possibly continuous and complex MDP, preserves sufficient information about the performance of the policy that defines the ARP for all $\\phi\\in\\Phi$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. $\\forall\\,\\phi\\in\\Phi$ , the performance of a policy $\\pi$ is equal to the expected return of the abstract reward process $\\Re_{\\phi}^{\\pi}$ defined from MDP $M$ , i.e., $J(\\pi;\\Re_{\\phi}^{\\pi})=J(\\pi;M)$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "The result holds for the ground-truth ARP $\\Re_{\\phi}^{\\pi}$ . In practice, a model of the ARP must be estimated from data. Next, we describe how the choice of defining an ARP over discrete abstract states eliminates model class mismatch, enabling asymptotically correct estimation of the ARP from data. ", "page_idx": 4}, {"type": "text", "text": "Eliminating Model Class Mismatch: Methods that learn models from data make an assumption about the class of models used to represent the data. A significant challenge is that of model class mismatch, where this assumed model class is often unable to represent the true data distribution. As an example, a neural network parameterizing a univariate Gaussian distribution cannot accurately represent data generated from a bimodal distribution. In the context of this work, the transition function of an ARP may specify arbitrary probability distributions over discrete abstract states, necessitating a careful selection of the model class. Tabular models are capable of representing any distribution over discrete variables. Therefore, using tabular models when estimating an ARP from data ensures that there is no model class mismatch. This is why we employ state abstraction functions $\\phi\\in\\Phi$ that partition the state space into a finite number of disjoint sets, or discrete abstract states. The abstraction functions may be viewed as: (a) a discrete clustering of the state space, or (b) a discretization of continuous states. ", "page_idx": 4}, {"type": "text", "text": "While this addresses model class mismatch, the use of a discrete state abstraction may itself be a source of modeling error. Mapping groups of (possibly continuous) states to discrete abstract states loses information about the state of the MDP. A process defined over the abstract states cannot in general capture the full complexity of the underlying MDP and policy. Nonetheless, Theorem 3.1 guarantees that the ARP is performance-preserving, ensuring that the use of discrete state abstractions is not a source of error for policy evaluation. Additionally, since we have eliminated model class mismatch, a perfect model of the ARP can be asymptotically estimated. ", "page_idx": 4}, {"type": "text", "text": "To estimate the ARP from $\\mathcal{D}_{n}^{(\\pi_{b})}$ , apply the state abstraction function to states in $\\mathcal{D}_{n}^{(\\pi)}$ to map them to the abstract state space. Denote the maximum likelihood estimate of the model of the ARP obtained from the dataset (with abstract states) by $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi}{:=}(\\mathcal{Z},\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi},\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi},\\widehat{\\eta}_{n,\\phi})$ . The components take the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})=\\frac{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}}{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}};\\quad\\widehat{\\mathbf{R}}_{n,\\phi}^{\\pi}(z)=\\frac{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}R_{t}^{i}}{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}};\\quad\\widehat{\\eta}_{n,\\phi}^{\\pi}(z)=\\frac{\\sum_{i=1}^{n}\\mathbf{1}^{i}\\{z_{0}=z\\}}{n}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Asymptotic Correctness: With access to on-policy data $\\mathcal{D}_{n}^{(\\pi)}$ , the following result states that ARPs enable consistent model-based estimation of the policy\u2019s performance. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. $\\forall\\phi\\in\\Phi$ , the expected return of the maximum likelihood estimate $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi}$ converges almost surely to the expected return of the policy $\\pi$ , i.e., $J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})\\xrightarrow{a.s.}J(\\pi;M)$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "As the amount of data $(n)$ increases, the estimate of $J(\\pi;M)$ becomes increasingly accurate, i.e., the return estimate is consistent. This result holds for all $\\phi\\in\\Phi$ . It implies that even an arbitrarily small model derived from a large, complex sequential decision-making problem will not introduce asymptotic bias. However, this theoretical guarantee requires on-policy data and so does not directly assist us in off-policy evaluation. To that end, we introduce a procedure for estimation of the ARP from off-policy data that merges the machinery of IS-based and model-based methods. ", "page_idx": 4}, {"type": "text", "text": "3.1 Estimation from Off-Policy Data: Weighted Maximum Likelihood Estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present a method for consistent estimation of the ARP corresponding to the evaluation policy $\\pi_{e}$ from off-policy data $\\mathcal{D}_{n}^{(\\pi_{b})}$ . It relies on the following intuition: ", "page_idx": 4}, {"type": "text", "text": "The expected value of the indicator function of an event represents the probability of that event.   \nUse importance sampling to approximate the probability of that event under a different distribution. ", "page_idx": 5}, {"type": "text", "text": "To estimate an ARP from off-policy data, assign importance weights $\\rho_{0:t}$ to the abstract states $(Z_{t}\\;:=\\;\\phi(S_{t}))$ in the dataset $\\mathcal{D}_{n}^{(\\pi_{b})}$ . Let $H_{t}\\;:=\\;\\left(S_{0},A_{0},R_{0},\\ldots,S_{t-1},A_{t-1},R_{t-1},S_{t},A_{t}\\right)$ denote a sub-trajectory up to time $t$ . The importance weight $\\rho_{0:t}$ is then the ratio of the probability of $H_{t}$ under $\\pi_{e}$ and $\\pi_{b}$ , i.e., $\\begin{array}{r}{\\rho_{0:t}\\ :=\\ \\frac{\\mathrm{Pr}(H_{t};\\pi_{e})}{\\mathrm{Pr}(H_{t};\\pi_{b})}\\ =\\ \\prod_{j=0}^{t}\\frac{\\pi_{e}(S_{j},A_{j})}{\\pi_{b}(S_{j},A_{j})}}\\end{array}$ \u03c0\u03c0e((SSj,,AAj)).3 The maximum likelihood estimate (MLE) of $\\Re_{\\phi}^{\\pi_{e}}$ obtained from the weighted off-policy data is denoted by $\\widehat{\\Re}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\;:=\\;$ $\\left(\\mathcal{Z},\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}},\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}},\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\right)$ , where $\\begin{array}{r}{\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}(z)=\\frac{\\sum_{i=1}^{n}\\mathbf{1}^{i}\\{z_{0}=z\\}}{n}}\\end{array}$ remains unchanged, and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})=\\frac{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}}{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}\\rho_{0:t}},\\qquad\\widehat{\\mathbf{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)=\\frac{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}\\rho_{0:t}R_{t}^{i}}{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}\\rho_{0:t}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This estimation is a form of weighted maximum likelihood estimation [13]. Including the importance ratios in the numerator and denominator of the estimated transition and reward functions of the ARP enables estimation from off-policy data generated by $\\pi_{b}$ . The estimated model of the ARP is consistent and, as shown next, allows for consistent off-policy evaluation. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3. Under Assumption 2.1, the weighted maximum likelihood estimate $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}$ converges almost surely to the ground-truth ARP $\\Re_{\\phi}^{\\pi_{e}}$ , i.e. ,R \u03c0n,b\u03d5\u2192\u03c0e\u2212a.s\u2192. R\u03c0\u03d5e . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "4 Off-Policy Evaluation with ARPs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "iTs hae ne axspyecmteptdo rtiectuarllny  ocf $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}$ tiism aa tce oonfs ,t  aess tipmera tLe eomf tmhae  3p.e3r.formance of policy $\\pi_{e}$ since $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}$ $\\Re_{\\phi}^{\\pi_{e}}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. The expected return of the ARP $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}$ (built from off-policy data) converges almost surely to the expected return of $\\pi_{e}$ , i.e., $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}})\\stackrel{a.s.}{\\longrightarrow}J(\\pi_{e};M)$ .   \nProof. See Appendix B.4. ", "page_idx": 5}, {"type": "text", "text": "To our knowledge, this is the first instance of a model-based OPE method that comes with the theoretical guarantee of consistent performance estimates, even for continuous problems and without model class assumptions. So far, we have achieved one of the starting goals\u2014that of consistency. However, the use of importance weights $\\rho_{0:t}$ for weighted MLE is expected to introduce high variance. Next, we discuss methods to mitigate the variance of IS. ", "page_idx": 5}, {"type": "text", "text": "4.1 Variance Reduction: Leveraging Markovness of the State Abstraction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A common technique for mitigating the variance of IS-based methods for OPE is clipping the importance weights to the c most recent ratios [18, 3, 20], i.e., \u03c1(t\u2212c+1)+:t :=  it=(t\u2212c+1)+ \u03c0\u03c0be((SSii,,AAii)), where $(t-c+1)^{+}{:=}\\operatorname*{max}(t-c+1,0)$ . This is often a bad approximation for classical IS-based methods, as it implies that only the $c$ most recent actions affect the reward distribution at any timestep, which rarely holds true in practice. In STAR, importance weights are incorporated into model estimation, resulting in a more reasonable implication of weight clipping. ", "page_idx": 5}, {"type": "text", "text": "By importance weighting the abstract-state occurrences as described in Equation (5), clipping importance weights, in this case, implies that the c most recent abstract states are sufficient to determine the current abstract-state transition and reward distributions. This allows actions from the distant past to influence the current reward, as the effects of actions propagate through the abstract state transitions, unlike in IS-based methods. This condition, that a recent history of abstract states is sufficient to predict the current abstract state transition distribution, often approximately holds in practice as discussed in POMDP literature [30, 39]. While the approximation may introduce asymptotic bias in exchange for reduced variance, certain abstraction functions that satisfy specific conditions can incur no asymptotic bias. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Weight Clipping without Asymptotic Bias: Intuitively, the use of $c$ -clipped importance weights, $\\rho_{(t-c+1)^{+}:t}$ , updates the estimated distribution of the previous $c$ abstract states\u2014as if under the evaluation policy\u2014while leaving the ones before unchanged. $c\\cdot$ -clipping does not introduce asymptotic bias when the previous $c$ abstract states form a sufficient statistic for predicting the current abstract state transition distribution. This notion of conditional independence from history given the recent past is referred to as the Markovness of the abstraction function $\\phi$ . We posit that there exist abstraction functions that are $c$ -th order Markov [51, 9]. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.2 $\\scriptstyle{\\overrightarrow{c}}$ -th order Markov). The abstraction function $\\phi$ is $c$ -th order Markov if $\\mathrm{Pr}(\\phi(S_{t+1})|\\phi(S_{t}),\\cdot\\cdot\\cdot\\,,\\phi(S_{(t-c+1)^{+}});\\pi)\\!=\\!\\mathrm{Pr}(\\phi(S_{t+1})|\\phi(S_{t}),\\cdot\\cdot\\cdot\\,,\\phi(S_{0});\\pi)$ for $\\pi\\in\\left\\{\\pi_{b},\\pi_{e}\\right\\}$ . LetR \u03c0\u03d5,bc\u2192\u03c0edenote the ARP estimated using c-clipped importance weights, \u03c1(t\u2212c+1)+:t, in place of $\\rho_{0:t}$ in Equation (5). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3. Given a c-th order Markov $\\phi$ , the expected return of the abstract reward process $\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ converges almost surely to the expected return of $\\pi_{e}$ , i.e., $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})\\stackrel{a.s.}{\\longrightarrow}J(\\bar{\\pi_{e}};M)$ . Proof. See Appendix B.5. ", "page_idx": 6}, {"type": "text", "text": "Even when $\\phi$ does not satisfy the above condition, weight clipping proves to be a practical approximation and results in low mean squared prediction error, as demonstrated empirically in Section 5. The steps for performing off-policy evaluation by estimating $\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ are highlighted in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "4.2 Fantastic $\\phi$ \u2019s and Where to Find Them ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Discrete abstraction functions that are $c$ -th order Markov with small values of $c$ represent the most suitable abstractions for enabling asymptotically correct, low-variance off-policy evaluation using STAR. An automated approach to discovering such abstraction functions, however, remains elusive. In a manner reminiscent of the options framework [50], wherein one might consider the usefulness of options before having methods for constructing options automatically, this work emphasizes the remarkable effectiveness of state abstractions used in abstract reward processes for OPE. It motivates a research area akin to option discovery: abstraction discovery for OPE. ", "page_idx": 6}, {"type": "text", "text": "We expect the following factors to play an important role in the search for good abstraction functions: (a) state-visitation distributions of $\\pi_{b}$ and $\\pi_{e}$ , determining the granularity of abstraction in different parts of the state set, and (b) the distribution shift in abstract state visitation induced by the two policies, determining the extent of weight clipping that can be applied. Both of these are affected by properties of the underlying MDP, in particular the transition function, and in our initial analyses, we observe varying effects of similar abstractions across different MDPs (Appendix C.2). ", "page_idx": 6}, {"type": "text", "text": "We observe that a simple approach of randomly initializing centroids and applying $k$ -means clustering [34, 33], where each cluster denotes a discrete abstract state, results in abstractions that provide competitive OPE performance, often significantly outperforming existing methods. We call this naive clustering-based abstraction method CluSTAR, and use it for our experiments. In some cases, abstraction by aggregation of states can increase the difficulty of estimation of the transition function. For example, aggregation of two states with deterministic transitions\u2014which can be estimated perfectly from a single observation of those transitions\u2014creates stochastic transitions between abstract states. However, in general, state aggregation tends to simplify estimation by increasing the effective sample size [21]. ", "page_idx": 6}, {"type": "text", "text": "Recovering Existing OPE Methods from STAR: Different configurations of $(\\phi,c)$ induce different ARPs, $\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ . For certain configurations of $(\\phi,c)$ : ", "page_idx": 6}, {"type": "text", "text": "\u2022 $|\\mathcal{Z}|=1$ and no weight clipping: Mapping all states to a single abstract state yields the weighted per-decision importance sampling (WPDIS) estimator [42].   \n\u2022 $\\mathcal{Z}=S$ and $c=1$ : Amounts to no state abstraction, and yields the maximum likelihood estimate of the MRP over states. The MRP is a combination of the approximate-model estimator [40] that directly estimates the model dynamics with the evaluation policy. ", "page_idx": 6}, {"type": "text", "text": "The recovery of these familiar estimators at the endpoints of STAR highlights the unifying nature of the framework. More importantly, the intermediate configurations of $\\bar{(\\phi,c)}$ uncover a whole new set of OPE estimators. The ARPs in this space often inherit a mixture of the favorable properties of both of the endpoints. Consequently, the framework yields estimators that can significantly outperform existing methods, as shown empirically in the next section. ", "page_idx": 7}, {"type": "text", "text": "5 Empirical Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we (A) analyze the performance of the set of estimators (ARPs) induced by STAR across different configurations of $(\\phi,c)$ , and $\\bf{(B)}$ compare the performance of the best and median ARPs from this set against existing OPE methods to demonstrate that estimators encompassed by STAR often outperform prior OPE methods. We use the following domains for OPE: (1) CartPole [49]: A classic control domain in OpenAI Gym [6]. (2) ICU-Sepsis [8]: An MDP that simulates treatment of sepsis in the ICU. ICU-Sepsis is built from real-world medical records obtained from the MIMIC-III dataset [24], using a modified version of the process described by Komorowski et al. [26]. (3) Asterix from the MinAtar testbed [61]: A miniaturized version of the Atari game Asterix. Details about each domain, and about the behavior and evaluation policies can be found in the Appendix C. The code is available at: https://github.com/shreyasc-13/STAR. ", "page_idx": 7}, {"type": "text", "text": "(A) Estimator Selection: Estimator selection presents a significant challenge for OPE [48] due to the unavailability of a validation set. To select STAR estimators to compare against existing methods, we first report the performance of the set of the ARPs induced by STAR, across a range of configurations of $(\\phi,c)$ . We highlight the performance of the best and median estimators from this set. For reference, we compare the mean squared prediction errors from the estimated ARPs against WPDIS and approximatemodel estimator (MBased), the two endpoints of STAR, as shown in Figure 2 on the CartPole domain. The state abstraction is performed with CluSTAR with the number of centroids $|\\mathcal{Z}|\\,\\in\\,\\{2,4,8,16,32,64,128\\}$ and the weight clipping factor $c\\in\\{1,2,3,4,5\\}$ defining 35 ARPs, where the performance of each is indicated by . This range of $(\\phi,c)$ is picked based on the intuition that (a) larger values of $c$ are expected to introduce high variance, and (b) this range of $|\\mathcal{Z}|$ covers a variety of granularities of state abstraction (for a continuous problem). The results are averages across 200 trails. The prediction errors for the estimated ARPs are competitive with baselines. This suggests that an average estimator in STAR is competitive with existing OPE methods, and even better performance may be attained by using specialized methods for abstraction discovery and estimator selection [55]. Figure 4, in the Appendix, provides a detailed breakdown of the performance of each ARP in the set considered, presented as a heatmap. This highlights patterns observed for varying values of $|\\mathcal{Z}|$ and $c$ across different domains. Further discussion and details about the best-performing configurations of $(\\phi,c)$ are deferred to Appendix C.2. ", "page_idx": 7}, {"type": "image", "img_path": "cYZibc2gKf/tmp/e4f24ca9fddfbd32601dd0ce125e9272c1abff194cda273cd8a09c19652b2ba2.jpg", "img_caption": ["Figure 2: Mean squared prediction errors of the estimated ARPs for the set of hyperparameters swept over for CartPole. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "(B) ARPs Can Outperform Existing Methods for OPE: We compare against representative methods from the main categories of OPE methods: (a) IS-based methods: Vanilla IS, Per-Decision IS, Weighted IS, Weighted Per-Decision IS [42]; (b) Model-based methods: that approximate a model of the MDP\u2014MBased [49]\u2014and directly estimate off-policy Q-values\u2014FQE and MRDR [27, 11]; and (c) Mixture methods: DR [22] and MAGIC [53], which blend estimates from the methods in the aforementioned categories. We use implementations from the Caltech OPE Benchmarking Suite (COBS) [58] for all methods. The representative method for minmax style estimators, IH [31], is designed for the infinite horizon setting and performs poorly with $\\gamma=1$ [63]. Due to the instability of IH estimates in our experiments, we excluded it from comparison. ", "page_idx": 7}, {"type": "text", "text": "Results [Figure 3 and 4]: The ranges of $|{\\mathcal{Z}}|$ and $c$ for each domain, which induces a set of ARPs that we consider, are detailed in Appendix C.2. Figure 4 in Appendix C.2 demonstrates the performance of each ARP from that set as a heatmap, showing that the best configuration of $(\\phi,c)$ varies with the dataset size for each domain. In critical applications like sepsis treatment, as simulated in ICUSepsis, where incorrect policy performance estimates can be catastrophic, the best estimators in ", "page_idx": 7}, {"type": "image", "img_path": "cYZibc2gKf/tmp/d64f5dad3e984671deeb0b0ef8d5fbbbafa907b73120d0725d49fb05579d081a.jpg", "img_caption": ["Figure 3: Mean squared prediction errors of best and median ARPs from STAR compared against existing OPE methods. The empirically estimated bias-variance decomposition of the error is shown. The results are averaged over 200 trials, with error bars indicating standard error. Note: For ICUSepsis, regression-based methods (MRDR and Q-Reg) were computationally intractable due to the large state set, as the corresponding Weighted Least Squares methods for regression were too slow. In all domains and across all datasizes, the best ARP in STAR outperforms baselines in all cases, and the even the median estimator does so in 7 out of 12 cases. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "STAR achieve prediction errors that are an order of magnitude lower than the best baseline. These results in Figure 3 emphasize that highly compact ARPs that distill complex sequential processes are particularly effective for off-policy evaluation. The best ARP in STAR for each domain abstracts: (a) the continuous state space of CartPole to $|\\mathcal{Z}|=32$ abstract states, (b) the 747 states of ICU-Sepsis to $|\\mathcal{Z}|=16$ abstract states, and (c) the 400 states of Asterix to $|\\mathcal{Z}|=8$ abstract states, to construct compact finite ARPs for OPE. Furthermore, the horizon length of CartPole is 50, and episodes in ICU-Sepsis and Asterix go up to 120 and 75 timesteps respectively. Such relatively long horizons have proven to be challenging for prior OPE methods. STAR leverages ARPs to enable OPE at scales commonly seen in practice. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The problem of off-policy evaluation (OPE) has been extensively studied due to its relevance for practical applications of reinforcement learning [38, 35]. Extensive surveys on the topic, both theoretical [56] and empirical [58, 14] delineate the numerous approaches to the problem. Modelbased approaches for OPE have proven effective [32, 62] but are restricted by the model class used. In this work, we use state abstraction to define compact models called abstract reward processes, and demonstrate their effectiveness as off-policy estimators. Historically, state abstraction research has focused on grouping similar states in a way that does not change the essence of the underlying problem [28, 43, 1], to reduce the complexity of the problem. However, the use of state abstractions for OPE remains under-explored. Pavse and Hanna [41] show that the use of state abstraction with marginalized importance sampling achieves variance reduction in high-dimensional state spaces, but their approach does not use abstraction to construct models. Jiang et al. [23] study abstraction selection for modelbased RL, balancing model complexity and policy value suboptimality. Abstraction discovery or learning has focussed on discretization of continuous state spaces to reduce problem complexity [47], and distilling the Markov features [2] or reward relevant features [12, 59]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have introduced a new framework for consistent model-based off-policy evaluation. This framework leverages state abstraction to prevent model class mismatch along with importance sampling to consistently learn models from off-policy data. Unlike traditional model-based methods, our approach eliminates the need for model class assumptions and provides theoretical guarantees for the obtained performance estimates. Moreover, using state abstraction increases the effective sample size [21], which is particularly beneficial in limited data regimes. Importantly, this work presents a framework with a new approach to OPE, rather than a specific new method. Estimators that lie within this framework significantly outperform existing OPE methods, with the best estimator consistently outperforming all baselines, as demonstrated in our empirical evaluation. ", "page_idx": 9}, {"type": "text", "text": "The framework has two main limitations: it requires knowledge of the probabilities of observed actions under the behavior policy, which may not always be available, and a principled method for selecting well-performing configurations of the abstraction function and the weight clipping factor remain elusive. Combining this work with regression IS [19] would be a practical extension that addresses the first limitation. Additionally, a data-driven approach to automated estimator selection based on characteristics of the domain, dataset sizes, and other factors, as suggested by Su et al. [48], would enhance its practical application. ", "page_idx": 9}, {"type": "text", "text": "Our findings indicate that even a simple class of abstraction functions can provide competitive OPE performance. We theoretically demonstrate the existence of certain abstraction functions that may offer better performance. Investigating the properties of abstraction functions and developing automated approaches to abstraction discovery for ARPs are promising directions for future work on creating high-performing OPE methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements We thank Yash Chandak, Mohammad Ghavamzadeh and Dhawal Gupta for their helpful discussions and feedback on this work. We would also like to thank Josiah Hanna, Bo Liu, Cameron Allen, and the anonymous reviewers for their feedback. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] David Abel. A theory of state abstraction for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9876\u20139877, 2019.   \n[2] Cameron Allen, Neev Parikh, Omer Gottesman, and George Konidaris. Learning Markov state abstractions for deep reinforcement learning. Advances in Neural Information Processing Systems, 34:8229\u20138241, 2021.   \n[3] Oliver Bembom and Mark J van der Laan. Data-adaptive selection of the truncation level for inverse-probability-of-treatment-weighted estimators. 2008.   \n[4] Patrick Billingsley. Convergence of Probability Measures. John Wiley & Sons, 2013.   \n[5] L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(11), 2013.   \n[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym. arXiv Preprint arXiv:1606.01540, 2016.   \n[7] Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. Advances in Neural Information Processing Systems, 24, 2011.   \n[8] Kartik Choudhary, Dhawal Gupta, and Philip S. Thomas. ICU-Sepsis: A benchmark MDP built from real medical data, 2024.   \n[9] Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosef.i Provable reinforcement learning with a short-term memory. In Proceedings of the International Conference on Machine Learning, pages 5832\u20135850. PMLR, 2022.   \n[10] Amir-Massoud Farahmand and Csaba Szepesv\u00e1ri. Model selection in reinforcement learning. Machine Learning, 85(3):299\u2013332, 2011.   \n[11] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In Proceedings of the International Conference on Machine Learning, pages 1447\u20131456. PMLR, 2018.   \n[12] Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes. arXiv Preprint arXiv:1207.4114, 2012.   \n[13] Chris Field and B Smith. Robust estimation: A weighted maximum likelihood approach. International Statistical Review/Revue Internationale de Statistique, pages 405\u2013424, 1994.   \n[14] Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R Zhang, Yutian Chen, Aviral Kumar, et al. Benchmarks for deep off-policy evaluation. arXiv Preprint arXiv:2103.16596, 2021.   \n[15] Ge Gao, Song Ju, Markel Sanz Ausin, and Min Chi. Hope: Human-centric off-policy evaluation for e-learning and healthcare. arXiv Preprint arXiv:2302.09212, 2023.   \n[16] LC Evans-RF Gariepy and F Ronald. Measure theory and fine properties of functions, Revised edition. CRC Press, 2015.   \n[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[18] Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long horizon off-policy policy evaluation. Advances in Neural Information Processing Systems, 30, 2017.   \n[19] Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an estimated behavior policy. In Proceedings of the International Conference on Machine Learning, pages 2605\u20132613. PMLR, 2019.   \n[20] Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295\u2013311, 2008.   \n[21] Nan Jiang. Notes on state abstractions, 2018. URL https://nanjiang.cs.illinois.edu/ files/cs598/note4.pdf.   \n[22] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 652\u2013661. PMLR, 2016.   \n[23] Nan Jiang, Alex Kulesza, and Satinder Singh. Abstraction selection in model-based reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 179\u2013188. PMLR, 2015.   \n[24] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. MIMIC-III, a freely accessible critical care database. Scientific Data, 3(1):1\u20139, 2016.   \n[25] Herman Kahn and Theodore E Harris. Estimation of particle transmission by random sampling. National Bureau of Standards Applied Mathematics Series, 12:27\u201330, 1951.   \n[26] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature Medicine, 24(11):1716\u20131720, 2018.   \n[27] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Proceedings of the International Conference on Machine Learning, pages 3703\u20133712. PMLR, 2019.   \n[28] Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for MDPs. Artificial Intelligence and Machine Learning (AI&M), 1(2):3, 2006.   \n[29] Lihong Li, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Toward minimax off-policy value estimation. In Proceedings of the Artificial Intelligence and Statistics Conference, pages 608\u2013616. PMLR, 2015.   \n[30] Michael Littman and Richard S. Sutton. Predictive representations of state. Advances in Neural Information Processing Systems, 14, 2001.   \n[31] Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. Advances in Neural Information Processing Systems, 31, 2018.   \n[32] Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A Faisal, Finale Doshi-Velez, and Emma Brunskill. Representation balancing MDPs for off-policy policy evaluation. Advances in Neural Information Processing Systems, 31, 2018.   \n[33] Stuart Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28(2):129\u2013137, 1982.   \n[34] James MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281\u2013297, Oakland, CA, USA, 1967. University of California Press.   \n[35] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offilne policy evaluation across representations with applications to educational games. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, volume 1077, 2014.   \n[36] Vukosi N Marivate. Improved Empirical Methods in Reinforcement-Learning Evaluation. Rutgers The State University of New Jersey, School of Graduate Studies, 2015.   \n[37] Andrei Andreevich Markov. The theory of algorithms. Trudy Matematicheskogo Instituta Imeni VA Steklova, 42:3\u2013375, 1954.   \n[38] Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Research Group. Marginal mean models for dynamic regimes. Journal of the American Statistical Association, 96(456):1410\u20131423, 2001.   \n[39] Khanh Xuan Nguyen. Converting POMDPs into MDPs using history representation. Engineering Archive, 2021.   \n[40] Cosmin Paduraru. Off-Policy Evaluation in Markov Decision Processes. PhD thesis, 2013.   \n[41] Brahma S Pavse and Josiah P Hanna. Scaling marginalized importance sampling to highdimensional state-spaces via state abstraction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9417\u20139425, 2023.   \n[42] Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80, 2000.   \n[43] Balaraman Ravindran. An Algebraic Approach to Abstraction in Reinforcement Learning. University of Massachusetts Amherst, 2004.   \n[44] Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo Method. John Wiley & Sons, 2016.   \n[45] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the International Conference on Machine Learning, pages 1889\u20131897. PMLR, 2015.   \n[46] Pranab K Sen and Julio M Singer. Large Sample Methods in Statistics: An Introduction with Applications, volume 25. CRC Press, 1994.   \n[47] Sean R Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization for episodic reinforcement learning in metric spaces. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 3(3):1\u201344, 2019.   \n[48] Yi Su, Pavithra Srinath, and Akshay Krishnamurthy. Adaptive estimator selection for offpolicy evaluation. In Proceedings of the International Conference on Machine Learning, pages 9196\u20139205. PMLR, 2020.   \n[49] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.   \n[50] Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2): 181\u2013211, 1999.   \n[51] Erik N Talvitie. Simple Partial Models for Complex Dynamical Systems. PhD thesis, University of Michigan, 2010.   \n[52] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In Proceedings of the Machine Learning for Healthcare Conference, pages 2\u201335. PMLR, 2021.   \n[53] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In Proceedings of the International Conference on Machine Learning, pages 2139\u20132148. PMLR, 2016.   \n[54] Edward C Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189, 1948.   \n[55] Takuma Udagawa, Haruka Kiyohara, Yusuke Narita, Yuta Saito, and Kei Tateno. Policyadaptive estimator selection for off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10025\u201310033, 2023.   \n[56] Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. arXiv Preprint arXiv:2212.06355, 2022.   \n[57] Aad W Van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.   \n[58] Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation for reinforcement learning. arXiv Preprint arXiv:1911.06854, 2019.   \n[59] Tongzhou Wang, Simon S Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian. Denoised MDPs: Learning world models better than the world itself. arXiv Preprint arXiv:2206.15477, 2022.   \n[60] Neil A. Weiss. A Course in Probability. Addison-Wesley, Boston, 2005. ISBN 0-321-18954-X.   \n[61] Kenny Young and Tian Tian. MinAtar: An Atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv Preprint arXiv:1903.03176, 2019.   \n[62] Michael R Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and optimization. arXiv Preprint arXiv:2104.13877, 2021.   \n[63] Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offilne estimation of stationary values. arXiv Preprint arXiv:2002.09072, 2020.   \n[64] Daniel Zwillinger and Stephen Kokoska. CRC Standard Probability and Statistics Tables and Formulae. CRC Press, 1999. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consistency and Almost Sure Convergence Let $\\hat{\\theta}_{n}$ denote the estimator of a statistic $\\theta$ , estimated from $n$ data points. As the amount of data approaches infinity, i.e., as $n\\to\\infty$ , the estimator is said to converge almost surely to $\\theta$ if and only if ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\operatorname*{lim}_{n\\to\\infty}{\\hat{\\theta}}_{n}=\\theta\\right)=1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We write $\\theta_{n}\\xrightarrow[]{\\mathrm{a.s.}}\\theta$ to denote that the sequence $\\theta_{n}$ converges almost surely to $\\theta$ . An estimator that converges almost surely to the true value of the statistic is said to be (strongly) consistent. ", "page_idx": 13}, {"type": "text", "text": "Continuous Mapping Theorem The continuous mapping theorem [4, 57] states that if a sequence of random variables $(X_{i})_{i=1}^{n}$ converges almost surely to a random variable $X$ , a function $f$ has discontinuity points $D_{f}$ , and $\\operatorname*{Pr}(X\\,\\,\\bar{\\epsilon}\\,\\,D_{f})=0$ , then the sequence $\\left(f(X_{i})\\right)_{i=1}^{n}$ converges almost surely to $f(X)$ . ", "page_idx": 13}, {"type": "text", "text": "Tower Rule For random variables $X$ and $Y$ defined on the same probability space, the tower rule states that the expected value of the conditional expected value of $X$ given $Y$ is the same as the expected value of $X$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[X\\mid Y\\right]\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1 Additional Notes on MRPs and ARPs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we provide additional details regarding the definitions of MDPs and MRPs. Specifically, we show how the expression of the transition and reward functions of the MRP can be simplified when $\\mathcal{X}=\\mathcal{S}$ and discuss how to handle termination in ARPs that are derived from finite-horizon MDPs. ", "page_idx": 13}, {"type": "text", "text": "A.1.1 Simplification of Equation 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The expressions for the components of a Markov reward process (MRP) can be simplified by using the Markov property. The transition function simplifies as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p^{\\pi}(x,x^{\\prime})=\\frac{\\sum_{t}\\mathrm{Pr}(S_{t+1}=x^{\\prime},S_{t}=x;\\pi)}{\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\frac{\\sum_{t}\\mathrm{Pr}(S_{t+1}=x^{\\prime}|\\ S_{t}=x;\\pi)\\,\\mathrm{Pr}(S_{t}=x;\\pi)}{\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\frac{\\sum_{t}\\sum_{a\\in A}\\mathrm{Pr}(S_{t+1}=x^{\\prime}|\\ S_{t}=x,\\pi)}{\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi)}\\,\\mathrm{Pr}(\\boldsymbol{A_{t}=a}|S_{t}=x)\\,\\mathrm{Pr}(S_{t}=x;\\pi)}\\\\ &{\\qquad=\\frac{\\sum_{t}\\sum_{a\\in A}p(x,a,x^{\\prime})\\pi(x,a)\\,\\mathrm{Pr}(S_{t}=x;\\pi)}{\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\frac{\\sum_{a\\in A}p(x,a,x^{\\prime})\\pi(x,a)\\,(\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi))}{\\sum_{t}\\mathrm{Pr}(S_{t}=x,\\pi)}}\\\\ &{\\qquad=\\sum_{t}\\frac{p(x,t)\\,(x,\\pi)}{\\sum_{t}\\mathrm{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\frac{\\sum_{t}p(x,a,x^{\\prime})\\pi(x,\\pi)}{\\sum_{t}\\mathrm{Gr}(x,a,x^{\\prime})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly, the reward function simplifies as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{\\pi}(x)=\\!\\frac{\\sum_{t}\\mathbb{E}\\left[R_{t}\\middle|S_{t}=x;\\pi\\right]\\operatorname*{Pr}(S_{t}=x;\\pi)}{\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\!\\frac{\\sum_{t}\\sum_{a\\in A}r(x,a)\\pi(x,a)\\operatorname*{Pr}(S_{t}=x;\\pi)}{\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\!\\frac{\\sum_{a\\in A}r(x,a)\\pi(x,a)\\left(\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)\\right)}{\\sum_{t}\\operatorname*{Pr}(S_{t}=x;\\pi)}}\\\\ &{\\qquad=\\!\\sum_{a\\in A}r(x,a)\\pi(x,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1.2 Handling termination in an MDPs, MRPs, and ARPs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Each episode in a finite-horizon MDP concludes by some timestep $T\\;\\in\\;\\mathbb{N}$ , referred to as the termination of that episode. In practice, there are two common ways of modeling termination of episodes in finite-horizon MDPs: 1) including an absorbing state $s_{\\infty}$ in the state set, or 2) introducing a termination function $\\beta:S\\rightarrow[0,1]$ representing the probability of termination of an episode from each state, i.e., $\\beta(s):=\\operatorname*{Pr}({\\mathrm{terminate}}|S_{t}=s)$ . In the first case, by timestep $T$ the process transitions into the absorbing $s_{\\infty}$ after which it continually transitions back to $s_{\\infty}$ getting a reward of zero. In the second case, after timestep $T$ the process stops and there are no subsequent samples. Expressions involving sums over samples in an epsiode are denoted correspondingly, for example, the expected return $\\begin{array}{r}{J(\\pi):=\\mathbb{E}[\\sum_{t=1}^{\\infty}R_{t};\\pi]}\\end{array}$ in the first case, and $\\begin{array}{r}{J(\\pi):=\\mathbb{E}[\\sum_{t=1}^{T}R_{t};\\pi]}\\end{array}$ in the second. Both approaches are mathematically equivalent and correspondingly, MRPs induced by the combination of finite-horizon MDPs with a policy can model termination in either way. ", "page_idx": 14}, {"type": "text", "text": "However, termination in ARPs requires special attention. The introduction of an absorbing abstract state $z_{\\infty}$ places a condition on the abstraction function $\\phi$ \u2014it requires the abstraction function to map only $s_{\\infty}$ , and no other state, to $z_{\\infty}$ . In this work, we implement termination in code by estimating abstract termination functions that represent the probability of termination from each abstract state. Denote the abstract termination function of an ARP by $\\beta_{\\phi}^{\\pi}\\,:\\,\\mathcal{Z}\\,\\rightarrow\\,[0,1]$ . It must be noted that both approaches continue to be mathematically equivalent. The probability of transitioning from any abstract state $z$ to $z_{\\infty}$ is the same as the the probability of termination from that state, i.e., $\\beta_{\\phi}^{\\bar{\\pi}}(z)=\\mathrm{P}_{\\phi}^{\\pi}(z,z_{\\infty})$ . Note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{P}_{\\phi}^{\\pi}(z_{\\infty},z)=\\left\\{0\\quad\\mathrm{if~}z\\neq z_{\\infty}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, the reward function is zero for the absorbing abstract state: $\\mathrm{R}_{\\phi}^{\\pi}(z_{\\infty})=0$ . It is mathematically more succint to model termination with $z_{\\infty}$ , and we use this (equivalent) approach in our theoretical analysis. ", "page_idx": 14}, {"type": "text", "text": "In our implementation, $z_{\\infty}$ is not defined and thus the components of $\\Re_{\\phi}$ do not take $z_{\\infty}$ as input. The equivalant form of the expected return of an ARP, that models a termination function, is then given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\,\\left[\\mathrm{P}_{\\phi}^{\\pi}\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\,\\left[(\\mathrm{P}_{\\phi}^{\\pi})^{2}\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\dots\\right]\\right]}}\\\\ {{=\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\mathrm{P}_{\\phi}^{\\pi}\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)^{2}(\\mathrm{P}_{\\phi}^{\\pi})^{2}\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}+\\dots}}\\\\ {{=\\displaystyle\\sum_{k=0}^{\\infty}\\left(\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\mathrm{P}_{\\phi}^{\\pi}\\right)^{k}\\mathrm{R}_{\\phi}^{\\pi}\\eta_{\\phi}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Correspondingly, the closed form expression for the expected return that accounts for the termination function is given by, ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=\\left(\\mathrm{I}-\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\mathrm{P}_{\\phi}^{\\pi}\\right)^{-1}\\mathrm{R}_{\\phi}^{\\pi}(z)\\eta_{\\phi}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\left(\\mathrm{I}-\\left(\\mathrm{I}-\\mathrm{diag}(\\beta_{\\phi}^{\\pi})\\right)\\mathrm{P}_{\\phi}^{\\pi}\\right)$ is an invertible matrix, as the left hand side of the expression, $J(\\pi;\\Re_{\\phi}^{\\pi})$ , is equal to the expected return $J(\\pi;M)$ of $\\pi$ by Theorem 3.1, which is bounded. ", "page_idx": 14}, {"type": "text", "text": "B Proofs of Theoretical Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide proofs of the theorems and lemmas in the main text. We start with Theorem 3.1 that states that ARPs are performance-preserving. ", "page_idx": 15}, {"type": "text", "text": "B.1 ARPs are Performance Preserving ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 3.1. $\\forall\\,\\phi\\in\\Phi$ , the performance of a policy $\\pi$ is equal to the expected return of the abstract reward process $\\Re_{\\phi}^{\\pi}$ defined from MDP $M$ , i.e., $J(\\pi;\\Re_{\\phi}^{\\pi})=J(\\pi;M)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. This result states that despite the use of a discrete state abstraction, the expected return of the ARP $\\Re_{\\phi}^{\\pi}$ is equal to the performance of policy $\\pi$ . To prove this, we leverage two equivalent forms of defining the expected return of a policy. The first form: ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=\\sum_{t}\\mathbb{E}[R_{t};\\pi],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "defines the expected return as a sum of the expected value of the reward at each timestep, where the distribution of rewards at each timestep is governed by the components of the ARP, namely, $\\eta_{\\phi},\\mathrm{P_{\\phi}^{\\pi}}$ and $\\mathrm{R}_{\\phi}^{\\pi}$ . The second, equivalent, form of expressing the expected return is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=\\sum_{t}\\sum_{z\\in\\mathcal{Z}}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\mathbb{E}[R_{t}|Z_{t}{=}z;\\pi]=\\sum_{t}\\sum_{z\\in\\mathcal{Z}}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the expected return is denoted as a sum of the reward at each abstract state mutliplied by the visitation frequency of that abstract state. Both Equations 7 and 8 are equivalent, i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=\\sum_{t}\\mathbb{E}[R_{t};\\pi]=\\sum_{t}\\sum_{z\\in\\mathcal{Z}}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\mathrm{R}_{\\phi}^{\\pi}(z).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we denote the terms in Equation 8 in terms of the states of the underlying MDP to show the final result. The reward function $\\mathrm{R}_{\\phi}^{\\pi}$ can be expressed as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{R}_{\\phi}^{\\pi}(z):=\\frac{\\sum_{t}\\mathbb{E}[R_{t}|\\phi(S_{t})=z;\\pi]\\operatorname{Pr}(\\phi(S_{t})=z;\\pi)}{\\sum_{t}\\operatorname{Pr}(\\phi(S_{t})=z;\\pi)}}\\\\ &{\\quad\\quad\\quad=\\frac{\\sum_{t}\\sum_{s\\in S}1\\{\\phi(s)=z\\}\\mathbb{E}[R_{t}|S_{t}=s;\\pi]\\operatorname{Pr}(S_{t}=s;\\pi)}{\\sum_{t}\\sum_{s\\in S}1\\{\\phi(s)=z\\}\\operatorname{Pr}(S_{t}=s;\\pi)}}\\\\ &{\\quad\\quad\\quad=\\frac{\\sum_{t}\\sum_{s\\in S,a\\in A,1}\\{\\phi(s)=z\\}r(s,a)\\pi(s,a)\\operatorname{Pr}(S_{t}=s;\\pi)}{\\sum_{t}\\sum_{s\\in S}1\\{\\phi(s)=z\\}\\operatorname{Pr}(S_{t}=s;\\pi)}}\\\\ &{\\quad\\quad\\quad=\\frac{\\sum_{s\\in S,a\\in A}1\\{\\phi(s)=z\\}r(s,a)\\pi(s,a)(\\sum_{t}\\operatorname{Pr}(S_{t}=s;\\pi))}{\\sum_{s\\in S}1\\{\\phi(s)=z\\}\\operatorname{Pr}(S_{t}=s;\\pi)}}\\\\ &{\\quad\\quad\\quad=\\frac{\\sum_{s\\in S,a\\in A}1\\{\\phi(s)=z\\}r(s,a)\\pi(s,a)(\\sum_{t}\\operatorname{Pr}(S_{t}=s;\\pi))}{\\sum_{s\\in S}1\\{\\phi(s)=z\\}\\left(\\sum_{t}\\operatorname{Pr}(S_{t}=s;\\pi)\\right)}}\\\\ &{\\quad\\quad\\quad=\\frac{\\sum_{s\\in S,a\\in A}\\psi^{\\pi}(s)\\pi(s,a)\\operatorname{Pr}(s,a)\\mathbb{I}\\{\\phi(s)=z\\}}{\\sum_{s\\in S}\\psi^{\\pi}(s)\\equiv1\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the undiscounted state distribution [49, Section 9.2] under policy $\\pi$ is denoted by $\\psi^{\\pi}(s)\\propto$ $\\begin{array}{r}{\\sum_{t}\\operatorname*{Pr}(S_{t}=s;\\pi)}\\end{array}$ , where $\\psi^{\\pi}(s_{\\infty})$ is set to be proportional to any constant value. The normalization constant, $\\kappa$ , that makes $\\psi^{\\pi}(s)$ a valid distribution cancels out from both the numerator and the denominator of the above expression. The term $\\operatorname*{Pr}(Z_{t}=z;\\pi)$ can be expressed in terms of states as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(Z_{t}=z;\\pi)=\\sum_{s\\in S}\\operatorname*{Pr}(S_{t}=s;\\pi)\\mathbf{1}\\{\\phi(s)=z\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Substituting these expressions into Equation 8 gives, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma(\\pi;\\mathfrak{R}_{\\omega}^{n})=\\displaystyle\\sum_{t\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}}\\mathrm{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\theta}^{n}(z)}\\\\ &{=\\displaystyle\\sum_{t\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}}\\left(\\sum_{\\varphi\\in\\mathcal{E}}\\mathrm{Pr}(S_{t}=s;\\pi)1\\{\\phi(s)=z\\}\\right)\\left(\\frac{\\sum_{\\epsilon\\in S\\to\\delta}\\mathrm{e}\\mathcal{E}^{\\pi}(s)\\pi^{\\alpha}(s,a)\\gamma(s,a)\\}\\{\\phi(s)=z\\}\\right.}\\\\ &{=\\displaystyle\\sum_{s\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}\\in\\mathcal{E}}\\mathrm{Pr}(S_{t}=s;\\pi)1\\{\\phi(s)=z\\}\\right)\\left(\\frac{\\sum_{\\epsilon\\in S\\to\\delta}\\mathrm{e}\\mathcal{E}^{\\pi}(s)\\pi^{\\alpha}(s,a)\\gamma(s,a)\\}\\{\\phi(s)=z\\}\\right.}\\\\ &{=\\displaystyle\\sum_{s\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}\\in\\mathcal{E}}\\mathrm{Pr}(S_{t}=s;\\pi)1\\{\\phi(s)=z\\}\\right)\\left(\\frac{\\sum_{\\epsilon\\in S\\to\\delta}\\mathrm{e}\\mathcal{V}^{\\pi}(s)\\pi^{\\alpha}(s,a)\\gamma(s,a)\\}\\{\\phi(s)=z\\}\\right.}\\\\ &{=\\displaystyle\\sum_{s\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}\\in\\mathcal{E}}\\mathrm{e}\\mathcal{V}^{\\pi}(s)1\\{\\phi(s)=z\\}\\right)\\left(\\frac{\\sum_{\\epsilon\\in S\\to\\delta}\\mathrm{e}\\mathcal{V}^{\\pi}(s)\\pi^{\\alpha}(s,a)\\Gamma(s,a)}{\\sum_{\\epsilon\\in S\\to\\delta}\\mathrm{e}^{\\pi}(s)\\}\\{\\phi(s)=z\\}\\right)}\\\\ &{=\\displaystyle\\sum_{s\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}\\in\\mathcal{E}}\\mathrm{e}\\mathcal{V}^{\\pi}(s)\\pi(s,a)\\gamma(s,a)1\\{\\phi(s)=z\\}}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{s\\mathop{\\varepsilon}^{n}\\in\\mathcal{E}}\\displaystyle\\sum_{c^{0}\\in\\mathcal{E}}\\mathrm{Pr}(S_{t}=s;\\pi)\\pi(s,a) \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) follows from the law of total probability [64]. ", "page_idx": 16}, {"type": "text", "text": "We have established that ARPs are performance-preserving. Next, we prove Lemma 3.2, which states that the performance estimate obtained from an estimated model of the ARP converges almost surely to the performance of the policy that induces it. In order to do so, we first introduce properties that will be useful in the proof of Lemma 3.2. ", "page_idx": 16}, {"type": "text", "text": "B.2 Estimation of ARPs from On-Policy Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "At a high level, we prove Lemma 3.2 by first studying the almost sure convergence of the estimated transition function, reward function, and initial state distributions. Once the almost sure convergence of these components has been established, we reason about the implications for the convergence of the policy performance predictions that result from these terms. We begin by studying the almost sure convergence of the transition function. ", "page_idx": 16}, {"type": "text", "text": "Property B.2. For all abstract states $z\\in{\\mathcal{Z}}$ and $z^{\\prime}\\in\\mathcal{Z}$ , if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})\\xrightarrow{a.s.}\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})=\\frac{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}}{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}}\\end{array}$ and $\\begin{array}{r}{Y_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}}\\end{array}$ . We can then rewrite $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})=\\frac{X_{n}}{Y_{n}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is a continuous function of $X_{n}$ and $Y_{n}$ when $Y_{n}\\,>\\,0$ . At a high level, we will show what $X_{n}$ and $Y_{n}$ each converge to almost surely, and will then apply the continuous mapping theorem to reason about the almost sure convergence of $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})$ . ", "page_idx": 16}, {"type": "text", "text": "However, there may be some abstract states $\\tilde{Z}\\subset\\mathcal{Z}$ that are not reached and thus are not observed in the data. Such abstract states pose a problem: If $\\tilde{z}\\,\\in\\,\\tilde{Z}$ , then $Y_{n}$ for $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(\\tilde{z},z^{\\prime})$ will be zero, and so $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(\\tilde{z},z^{\\prime})$ will be undefined and not a continuous function of $X_{n}$ and $Y_{n}$ (which must be handled appropriately in the proof of consistency). First, to ensure that the ARP is well-defined even when $\\tilde{Z}$ is not empty, for abstract states $\\tilde{z}\\in\\tilde{Z}$ (abstract states that were not observed in the data), special values can be hardcoded into the transition function, reward functions, and initial state distribution so that the components of the ARP continue to be well-defined.4 In particular, for $\\tilde{z}\\in\\tilde{Z}$ , the transition function can be set to self-transition back to $\\tilde{z}$ with probability 1, i.e., $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(\\tilde{z},\\tilde{z})=1$ and $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(\\tilde{z},\\bar{z})=0$ for all $\\Bar{z}\\notin\\tilde{Z}$ , the reward function is set to $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(\\tilde{z})=0$ and the initial abstract state distribution is $\\widehat{\\eta}_{n,\\phi}^{\\pi}(\\tilde{z})=0$ for all $\\tilde{z}\\in\\tilde{Z}$ .5 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "To reason about the almost sure convergence of $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})$ , first consider $\\operatorname*{lim}_{n\\to\\infty}X_{n}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{n\\to\\infty}X_{n}=\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}}}\\\\ &{}&{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{}&{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\displaystyle{=\\operatorname*{lim}_{n\\to\\infty}\\sum_{t}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}$ is independent across episodes (for each $i$ ), has bounded variance, and has the same mean for each episode. By Kolmogorov\u2019s strong law of large numbers [46], ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}\\xrightarrow{\\mathrm{a.s.}}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This convergence guarantee holds for all $t$ . From the definition of almost sure convergence, this means that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathbb{N},\\,\\operatorname*{Pr}\\left(\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\big\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\big\\}=\\mathbb{E}\\big[\\mathbf{1}\\big\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\big\\};\\pi\\big]\\right)=1,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the expectation results from the use of the policy $\\pi$ that generated dataset $\\mathcal{D}_{n}^{(\\pi)}$ . By the countable additivity property of probability measures (and the fact that the number of time steps is countable), this implies the following (notice that $\\forall t\\in\\mathbb{N}$ is now inside the statement of probability): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\forall t\\in\\mathbb{N},\\,\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\big\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\big\\}=\\mathbb{E}\\big[\\mathbf{1}\\big\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\big\\};\\pi\\big]\\right)=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{t}\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}=\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi\\right]\\right)=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, by the dominated convergence theorem [16, Theorem 1.19], the summation over time and limit over $n$ can be interchanged, giving: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\operatorname*{lim}_{n\\to\\infty}\\sum_{t}{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}=\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi\\right]\\right)=1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Returning to\u2212\u2192notation, this means that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z,Z_{t+1}^{i}=z^{\\prime}\\}\\right)\\xrightarrow{\\mathrm{a.s.}}\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "4The particular behavior of the ARP in these unobserved abstract states is not of consequence. Since they are unreached, the behavior in these abstract states does not impact the expected return of the ARP. However, we wish to ensure that the transition function, reward function, and initial state distribution still represent a well-defined ARP. 5Note that the initial state distribution is not actually a problem like the transition and reward functions\u2014the previous definitions already ensured that $\\widehat{\\eta}_{n,\\phi}^{\\pi}(\\tilde{z})=0$ . ", "page_idx": 17}, {"type": "text", "text": "The expected value of the indicator is equal to the probability of the event, and so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t}\\mathbb{E}\\left[{\\bf1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi\\right]=\\sum_{t}\\operatorname*{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This, combined with the fact that the left hand side of Equation 15 is $X_{n}$ means that ", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{n}\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\sum_{t}\\operatorname*{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, these same exact steps can be followed to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\nY_{n}\\;{\\xrightarrow{\\mathrm{a.s.}}}\\;\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\Psi_{i}=(X_{i},Y_{i})$ , so that $\\Psi_{n}=(X_{i},Y_{i})_{i=1}^{n}$ is a sequence of vector-valued random variables. Since $\\begin{array}{r}{X_{n}\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\sum_{t}\\operatorname*{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi)}\\end{array}$ and $\\begin{array}{r}{Y_{n}\\xrightarrow{\\mathrm{a.s.}}\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)}\\end{array}$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Psi_{n}\\xrightarrow{\\mathrm{a.s.}}\\left(\\sum_{t}\\operatorname*{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi),\\,\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consider the function $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ , defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x,y)={\\left\\{\\frac{x}{y}\\right.}{\\overset{\\underset{\\mathrm{~}}{x}}{\\sim}}{\\left.{\\underset{\\mathrm{~}}{y}}\\right.}{\\mathrm{~}}{\\mathrm{if~}}y\\not=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that discontinuities of $f$ may occur when $y=0$ . Applying the continuous mapping theorem, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\Psi_{n})\\xrightarrow{\\mathrm{a.s.}}\\frac{\\sum_{t}\\operatorname*{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi)}{\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "if6 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)=0\\right)=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In our case, $\\begin{array}{r}{\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)}\\end{array}$ is a constant, and so the condition in Equation 22 is simply: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Also, Equation 21 can be simplified to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})\\xrightarrow{\\mathrm{a.s.}}\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $f(\\Psi_{n})\\,=\\,X_{n}/Y_{n}\\,=\\,\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})$ ,7 and $\\begin{array}{r}{\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime})\\,=\\,\\frac{\\sum_{t}\\mathrm{Pr}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi)}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi)}}\\end{array}$ by Equation 3. Restating this conclusion, we have that if ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})\\xrightarrow{\\mathrm{a.s.}}\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which establishes the property. ", "page_idx": 18}, {"type": "text", "text": "Next, we present a property that establishes the almost sure convergence of the reward function estimate. ", "page_idx": 18}, {"type": "text", "text": "Property B.3. For all abstract states $z\\in{\\mathcal{Z}}$ , if ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)\\xrightarrow{a.s.}\\mathrm{R}_{\\phi}^{\\pi}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)=\\frac{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}}{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}}\\end{array}$ and $\\begin{array}{r}{Y_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}}\\end{array}$ . We will show that $X_{n}$ and $Y_{n}$ converge almost surely, and then apply the continuous mapping theorem to reason about the almost sure convergence of $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)$ . ", "page_idx": 19}, {"type": "text", "text": "To ensure that the ARP is well-defined even when $\\tilde{Z}$ is not empty, for abstract states $\\tilde{z}\\in\\tilde{Z}$ (abstract states that were not observed in the data), the reward function can be set to $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(\\tilde{z})=0$ , as elaborated in the proof of Property B.2. ", "page_idx": 19}, {"type": "text", "text": "Consider $\\operatorname*{lim}_{n\\to\\infty}X_{n}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}X_{n}=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\displaystyle\\sum_{t}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that ${\\bf1}\\{Z_{t}^{i}=z\\}R_{t}^{i}$ is independent across episodes (for each $i$ ), has bounded variance, and has the same mean for each episode. By Kolmogorov\u2019s strong law of large numbers [46], ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}\\xrightarrow{\\mathrm{as.}}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z\\}R_{t};\\pi\\right]}}\\\\ &{=\\displaystyle\\sum_{r\\in\\mathbb{R},\\,\\xi\\in\\mathcal{Z}}\\mathrm{Pr}(R_{t}=r,Z_{t}=\\xi;\\pi)\\mathbf{1}\\{Z_{t}=z\\}r}\\\\ &{=\\displaystyle\\sum_{r\\in\\mathbb{R}}\\mathrm{Pr}(R_{t}=r,Z_{t}=z;\\pi)r}\\\\ &{=\\displaystyle\\left(\\sum_{r\\in\\mathbb{R}}r\\,\\mathrm{Pr}(R_{t}=r\\mid Z_{t}=z;\\pi)\\right)\\mathrm{Pr}(Z_{t}=z;\\pi)}\\\\ &{=\\displaystyle\\mathbb{E}\\left[R_{t}\\mid Z_{t}=z;\\pi\\right]\\mathrm{Pr}(Z_{t}=z;\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This convergence guarantee holds for all $t$ . As in the proof of Property B.2, by the countable additivity property of probability measures, in conjunction with the dominated convergence theorem [16, Theorem 1.19], this implies that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\operatorname*{lim}_{n\\to\\infty}\\sum_{t}{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}=\\sum_{t}\\mathbb{E}\\left[R_{t}\\mid Z_{t}=z;\\pi\\right]\\operatorname*{Pr}(Z_{t}=z;\\pi)\\right)=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Returning to $\\xrightarrow{\\mathrm{a.s.}}$ notation, this means that ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{n}\\xrightarrow{\\mathrm{a.s.}}\\sum_{t}\\mathbb{E}\\left[R_{t}\\ |\\ Z_{t}=z;\\pi\\right]\\operatorname*{Pr}(Z_{t}=z;\\pi).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, these same exact steps can be followed to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\nY_{n}\\;{\\xrightarrow{\\mathrm{a.s.}}}\\;\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\Psi_{i}\\,=\\,(X_{i},Y_{i})$ , so that $\\Psi_{n}\\,=\\,(X_{i},Y_{i})_{i=1}^{n}$ is a sequence of vector-valued random variables. Considering the function $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ , defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x,y)=\\left\\{{\\frac{x}{y}}\\begin{array}{l l}{{\\mathrm{if~}}y\\neq0}\\\\ {{\\widehat{\\mathrm{R}}}_{n,\\phi}^{\\pi}(z)}&{{\\mathrm{otherwise.}}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that discontinuities of $f$ may occur when $y=0$ . Applying the continuous mapping theorem, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\Psi_{n})\\xrightarrow{\\mathrm{a.s.}}\\frac{\\sum_{t}\\mathbb{E}\\left[R_{t}\\mid Z_{t}=z;\\pi\\right]\\operatorname*{Pr}(Z_{t}=z;\\pi)}{\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "if ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, when $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi)\\neq0}\\end{array}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)\\xrightarrow{\\mathrm{a.s.}}\\mathrm{R}_{\\phi}^{\\pi}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which establishes the property. ", "page_idx": 20}, {"type": "text", "text": "Next, we present a property that establishes the almost sure convergence of the initial state distribution. ", "page_idx": 20}, {"type": "text", "text": "Property B.4. For all abstract states $z\\in\\mathcal{Z},\\,\\widehat{\\eta}_{n,\\phi}(z)\\overset{a.s.}{\\longrightarrow}\\eta_{\\phi}(z).$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\eta}_{n,\\phi}(z)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{0}^{i}=z\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{0}^{i}=z\\}}\\end{array}$ . We will show that $X_{n}$ converges almost surely, and then apply the continuous mapping theorem to reason about the almost sure convergence of $\\widehat{\\eta}_{n,\\phi}(z)$ . Note that even when $\\tilde{Z}$ is not empty, the definition of $\\widehat{\\eta}_{n,\\phi}^{\\pi}$ ensures that it is well-defined. ", "page_idx": 20}, {"type": "text", "text": "Consider $\\operatorname*{lim}_{n\\to\\infty}X_{n}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}X_{n}=\\operatorname*{lim}_{n\\to\\infty}{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{0}^{i}=z\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ${\\bf1}\\{Z_{0}^{i}=z\\}$ is independent across episodes (for each $i$ ), has bounded variance, and has the same mean for each episode. By Kolmogorov\u2019s strong law of large numbers [46], ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{n}{\\bf1}\\{Z_{0}^{i}=z\\}\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}{\\mathbb{E}}\\left[{\\bf1}\\{Z_{0}=z\\}\\right]=\\mathrm{Pr}(Z_{0}=z).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In contrast to the proofs of the previous two properties, this proof holds for all $z\\in{\\mathcal{Z}}$ , not just the abstract states that have non-zero probability of occuring. This implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\eta}_{n,\\phi}(z)\\xrightarrow{\\mathrm{a.s.}}\\eta_{\\phi}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which establishes the property. ", "page_idx": 20}, {"type": "text", "text": "Having established properties that will be useful in the proof of Lemma 3.2, we now turn to proving the lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma 3.2. $\\forall\\phi\\in\\Phi$ , the expected return of the maximum likelihood estimate $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi}$ converges almost surely to the expected return of the policy $\\pi$ , i.e., $J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})\\xrightarrow{a.s.}J(\\pi;M)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. From Theorem 3.1, we have that $J(\\pi;\\Re_{\\phi}^{\\pi})=J(\\pi;M)$ . Thus, to prove this result, we only need to show that $J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})\\xrightarrow{\\mathrm{a.s.}}J(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})$ . Several of the preliminary results required to estalish this result were provided in Properties B.2, B.3, and B.4. Specifically, Properties B.2 and B.3 establish that for all abstract states $z\\in{\\mathcal{Z}}$ and $z^{\\prime}\\in\\mathcal{Z}$ , if ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)\\neq0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}(z,z^{\\prime})\\xrightarrow{\\mathrm{a.s.}}\\mathrm{P}_{\\phi}^{\\pi}(z,z^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)\\xrightarrow{\\mathrm{a.s.}}\\mathrm{R}_{\\phi}^{\\pi}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, Property B.4 establishes that for all abstract states $z\\in{\\mathcal{Z}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\eta}_{n,\\phi}(z)\\xrightarrow{\\mathrm{a.s.}}\\eta_{\\phi}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The expected return $J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})$ is a continuous function of $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi}:=(\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi},\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi},\\widehat{\\eta}_{n,\\phi})$ , given by $\\begin{array}{r}{J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})\\,=\\,\\left(\\mathbb{I}-\\widehat{\\mathbb{P}}_{n,\\phi}^{\\pi}\\right)^{-1}\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}\\widehat{\\eta}_{n,\\phi}\\,=\\,\\sum_{z}\\sum_{t}\\widehat{\\mathrm{Pr}}(Z_{t}\\,=\\,z;\\pi)\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)^{\\mathtt{S}}}\\end{array}$ where $\\hat{\\mathrm{Pr}}$ denotes the empirical estimate of the corresponding probability. ", "page_idx": 21}, {"type": "text", "text": "Note that the term $\\begin{array}{r}{\\sum_{t}\\hat{\\mathrm{Pr}}(Z_{t}=z;\\pi)=\\left[\\left(\\mathrm{I}-\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}\\right)^{-1}\\widehat{\\eta}_{n,\\phi}\\right]_{z}}\\end{array}$ , i.e., the empirical estimate of the sum of probabilities of encountering the abstract state $z$ over all timesteps is equal to value of the vector $\\left(\\mathrm{I}-\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi}\\right)^{-1}\\widehat{\\eta}_{n,\\phi}$ at $z$ . By the continuous mapping theorem, if $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi)\\neq0}\\end{array}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t}\\hat{\\operatorname*{Pr}}(Z_{t}=z;\\pi)\\xrightarrow{\\mathrm{a.s.}}\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{Z}=\\{z:\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi)=0\\}}\\end{array}$ , where $\\bar{Z}\\subset\\mathcal{Z}$ , be the set of abstract states that will never be observed empirically as they have no probability of being visited under $\\pi$ .9The expression for the expected return can be divided into two terms: (1) the first term sums over abstract states for which $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi)=0}\\end{array}$ , and the values of the components of the ARP\u2014in particular, $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}$ \u2014have been specially hardcoded as detailed in Property B.2, and (2) the second term sums over the abstract states for which $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi)\\neq0}\\end{array}$ and thus Equation 41 holds. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})=\\displaystyle\\sum_{z\\in\\bar{Z}}\\displaystyle\\sum_{t}\\hat{\\mathrm{Pr}}(Z_{t}=z;\\pi)\\underbrace{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)}_{=0}+\\displaystyle\\sum_{z\\in\\mathcal{Z}\\backslash\\bar{Z}}\\displaystyle\\sum_{t}\\hat{\\mathrm{Pr}}(Z_{t}=z;\\pi)\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{z\\in\\mathcal{Z}\\backslash\\bar{Z}}\\displaystyle\\sum_{t}\\hat{\\mathrm{Pr}}(Z_{t}=z;\\pi)\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As detailed in Property B.2, $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi}(z)=0$ for $z\\in\\tilde{Z}$ and thus $\\forall z\\in{\\bar{Z}}$ , making the first term equal to zero. The constituents of the second term converge almost surely: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\varepsilon\\ge\\sqrt{2}}{\\sum}\\underset{t}{\\sum}\\hat{\\mathbf{F}}\\mathbf{\\tilde{r}}(Z_{t}=z;\\pi)\\hat{\\mathbf{R}}_{n,\\phi}^{\\pi}(z)\\xrightarrow{\\lambda\\infty}\\underset{\\varepsilon\\in\\mathcal{Z}\\backslash\\left\\mathcal{Z}}{\\sum}\\underset{t}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z)}&{}&{(44)}\\\\ &{=\\underset{z\\in\\mathcal{Z}\\backslash\\left\\mathcal{Z}}{\\sum}\\underset{t}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z)+0}&{(45)}\\\\ &{=\\underset{z\\in\\mathcal{Z}\\backslash\\left\\mathcal{Z}}{\\sum}\\underset{t}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z)+\\underset{z\\in\\mathcal{Z}}{\\sum}\\underset{t}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\right)\\mathbf{I}}\\\\ &{=\\underset{z\\in\\mathcal{Z}\\backslash\\left\\mathcal{Z}}{\\sum}\\underset{t}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z)}&{(46)}\\\\ &{=\\underset{z\\in\\mathcal{Z}\\backslash\\left\\mathcal{Z}}{\\sum}\\mathbf{Pr}(Z_{t}=z;\\pi)\\mathbb{R}_{\\phi}^{\\pi}(z)}&{(47)}\\\\ &{=J(\\pi;\\mathbb{R}_{\\phi}^{\\pi}).}&{(48)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then obtain the final result: ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ(\\pi;\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi})\\stackrel{\\mathrm{\\scriptsize~a.s.}}{\\longrightarrow}J(\\pi;\\mathfrak{R}_{\\phi}^{\\pi})=J(\\pi;M).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.3 Estimation of ARPs from Off-Policy Data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "So far, we have shown that the expected return of the estimated ARP converges almost surely to the expected return of the policy that induced it, i.e., consistent on-policy evaluation. Next, we show that a model of the ARP can be consistently estimated from off-policy data. This requires us to prove that transition functions estimated using off-policy data, incorporating importance weights in their estimation, converge almost surely to the true transition functions induced by the evaluation policy, and that the same holds for the reward functions and initial state distributions. ", "page_idx": 22}, {"type": "text", "text": "Property B.6. For all abstract states $z\\in{\\mathcal{Z}}$ and $z^{\\prime}\\in\\mathcal{Z}$ , $i f$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi_{b})\\neq0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})\\stackrel{a.s.}{\\longrightarrow}\\mathrm{P}_{\\phi}^{\\pi_{e}}(z,z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})=\\frac{\\sum_{i,t}{\\mathbf{1}}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}}{\\sum_{i,t}{\\mathbf{1}}_{t}^{i}\\{z\\}\\rho_{0:t}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similar to the proof structure of Property B.2, let $\\begin{array}{r l r}{X_{n}}&{{}:=}&{\\frac{1}{n}\\sum_{i,t}{\\bf1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}}\\end{array}$ and $Y_{n}~:=$ $\\textstyle{\\frac{1}{n}}\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}\\rho_{0:t}$ . For abstract states $\\tilde{Z}\\subset\\mathcal{Z}$ that are never reached and thus are not observed in the data, special values can be hardcoded into the transition function, reward functions, and initial state distribution so that the components of the ARP continue to be well-defined. In particular, for $\\tilde{z}\\,\\in\\,\\tilde{Z}$ , the transition function can be set to self-transition back to $\\tilde{z}$ with probability 1, i.e., $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(\\tilde{z},\\tilde{z})=1$ and $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(\\tilde{z},\\bar{z})=0$ for all $\\Bar{z}\\notin\\tilde{Z}$ , the reward function is set to $\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(\\widetilde{z})=0$ and the initial abstract state distribution is $\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(\\widetilde{z})=0$ for all $\\tilde{z}\\in\\tilde{Z}$ . ", "page_idx": 22}, {"type": "text", "text": "Consider $\\operatorname*{lim}_{n\\to\\infty}X_{n}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{}&{\\underset{n\\to\\infty}{\\operatorname*{lim}}X_{n}=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\frac{1}{n}\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}}\\\\ &{\\quad\\quad\\quad\\quad=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\sum_{t}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}$ is independent across episodes (for each $i$ ), has bounded variance, and has the same mean for each episode. By Kolmogorov\u2019s strong law of large numbers [46], ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}\\xrightarrow{\\mathrm{a.s.}}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\}\\rho_{0:t};\\pi_{b}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The importance weights change the distribution over which the expectation is computed, as below: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathbb E}\\{1\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\}\\rho_{0:t};\\pi_{b}\\}}}\\\\ {{\\displaystyle=\\sum_{t}\\sum_{S_{0t+1},\\atop A_{0:t}}\\left(\\eta(S_{0})\\prod_{l=0}^{t}\\pi_{b}(S_{l},A_{l})p(S_{l},A_{l},S_{l+1})\\right)\\mathbf{1}\\{\\phi(S_{t})=z,\\phi(S_{t+1})=z^{\\prime}\\}\\left(\\prod_{j=0}^{t}\\frac{\\pi_{e}(S_{j},A_{j})}{\\pi_{b}(S_{j},A_{j})}\\right)}}\\\\ {{\\displaystyle=\\sum_{t}\\sum_{S_{0t+1},\\atop A_{0:t}}\\left(\\eta(S_{0})\\prod_{l=0}^{t}\\pi_{e}(S_{l},A_{l})p(S_{l},A_{l},S_{l+1})\\right)\\mathbf{1}\\{\\phi(S_{t})=z,\\phi(S_{t+1})=z^{\\prime}\\}}}\\\\ {{\\displaystyle=\\sum_{t}\\mathbb{E}\\left[{1\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi_{e}}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This convergence guarantee holds for all $t$ . By the countable additivity property of probability measures in conjunction with the dominated convergence theorem [16], this implies that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\cal X}_{n}\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\sum_{t}\\mathbb{E}\\left[{\\bf1}\\{{\\cal Z}_{t}=z,{\\cal Z}_{t+1}=z^{\\prime}\\};\\pi_{e}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, these same exact steps can be followed to show that ", "page_idx": 23}, {"type": "equation", "text": "$$\nY_{n}\\ {\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}}\\ \\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z\\};\\pi_{e}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\Psi_{i}\\,=\\,(X_{i},Y_{i})$ , so that $\\Psi_{n}\\,=\\,(X_{i},Y_{i})_{i=1}^{n}$ is a sequence of vector-valued random variables. Considering the function $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ , defined as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(x,y)={\\left\\{\\frac{x}{y}\\right.}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\overset{}{\\underset{}{\\partial}}}{\\vdots}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that discontinuities of $f$ may occur when $y=0$ . Applying the continuous mapping theorem, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(\\Psi_{n})\\xrightarrow{\\mathrm{a.s.}}\\frac{\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\};\\pi_{e}\\right]}{\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z\\};\\pi_{e}\\right]}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "if $\\begin{array}{r}{\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z\\};\\pi_{e}\\right]\\neq0}\\end{array}$ . This implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\mathrm{P}_{\\phi}^{\\pi_{e}}(z,z^{\\prime}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "when $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi_{b})\\neq0}\\end{array}$ , which establishes the property. ", "page_idx": 23}, {"type": "text", "text": "Next, we present the properties that establish almost sure convergence of the reward function estimate and the initial state distribution estimated from off-policy data. This follows a similar proof structure to their corresponding on-policy properties, with the key difference being the use of importance weights in the estimation. ", "page_idx": 23}, {"type": "text", "text": "Property B.7. For all abstract states $z\\in{\\mathcal{Z}}$ , if ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi_{b})\\neq0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\xrightarrow{a.s.}\\mathrm{R}_{\\phi}^{\\pi_{e}}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)=\\frac{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}\\rho_{0:t}}{\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}\\rho_{0:t}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}\\rho_{0:t}}\\end{array}$ and $\\begin{array}{r}{Y_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{t}\\mathbf{1}\\{Z_{t}^{i}\\,=\\,z\\}\\rho_{0:t}}\\end{array}$ . Following the exact steps from Property B.3, we can show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{t}^{i}=z\\}R_{t}^{i}\\xrightarrow{\\mathbf{a}\\times}_{\\mathbb{R}}\\mathbf{1}\\{Z_{t}=z\\}R_{t}\\rho_{\\tau_{0};\\tau}\\pi_{b}]}\\\\ &{\\quad=\\mathbb{E}\\big[\\mathbf{1}\\{Z_{t}=z\\}R_{t};\\pi_{c}\\big]}\\\\ &{\\quad=\\displaystyle\\sum_{r\\in\\mathbb{R},\\,\\xi\\in\\mathbb{Z}}\\mathrm{Pr}(R_{t}=r,Z_{t}=\\bar{z};\\pi_{e})\\mathbf{1}\\{Z_{t}=z\\}r}\\\\ &{\\quad=\\displaystyle\\sum_{r\\in\\mathbb{R}}\\mathrm{Pr}(R_{t}=r,Z_{t}=z;\\pi_{e})r}\\\\ &{\\quad=\\displaystyle\\left(\\sum_{r\\in\\mathbb{R}}r\\mathbf{Pr}(R_{t}=r\\mid Z_{t}=z;\\pi_{e})\\right)\\mathrm{Pr}(Z_{t}=z;\\pi)}\\\\ &{\\quad=\\mathbb{E}\\big[R_{t}\\mid Z_{t}=z;\\pi\\big]\\mathrm{Pr}(Z_{t}=z;\\pi_{e}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This convergence guarantee holds for all $t$ . By the countable additivity property of probability measures, in conjunction with the dominated convergence theorem, this implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{n}\\ {\\overset{\\mathrm{a.s.}}{\\longrightarrow}}\\ \\sum_{t}\\mathbb{E}\\left[R_{t}\\ |\\ Z_{t}=z;\\pi_{e}\\right]\\operatorname*{Pr}(Z_{t}=z;\\pi_{e}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, these same exact steps can be followed to show that ", "page_idx": 24}, {"type": "equation", "text": "$$\nY_{n}\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi_{e}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Noting that $\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}=\\frac{X_{n}}{Y_{n}}}\\end{array}$ , we can apply the continuous mapping theorem, as done in the proof of Property B.3, to show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\xrightarrow{\\mathrm{a.s.}}\\mathrm{R}_{\\phi}^{\\pi_{e}}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "when $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi_{b})\\neq0}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Property B.8. For all abstract states $z\\in{\\mathcal{Z}}$ , $\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\stackrel{a.s.}{\\longrightarrow}\\eta_{\\phi}^{\\pi_{e}}(z).$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{0}^{i}=z\\}\\rho_{0}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r}{X_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{Z_{0}^{i}=z\\}\\rho_{0}}\\end{array}$ . Following the exact steps from Property B.4, we can show that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\mathbf{1}\\{Z_{t}^{i}=z\\}\\xrightarrow{\\mathrm{a.s.}}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{0}=z\\}\\rho_{0}\\right]=\\operatorname*{Pr}(Z_{0}=z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This proof holds for all $z\\in{\\mathcal{Z}}$ , not just the abstract states that have non-zero probability of occuring. This implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\eta_{\\phi}^{\\pi_{e}}(z).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 3.3. Under Assumption 2.1, the weighted maximum likelihood estimate $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}$ converges almost surely to the ground-truth ARP $\\Re_{\\phi}^{\\pi_{e}}$ , i.e., $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\xrightarrow{a.s.}\\mathfrak{R}_{\\phi}^{\\pi_{e}}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. The results required to establish this result are provided in Properties B.6, B.7, and B.8. Specifically, the properties establish that if ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t}\\operatorname*{Pr}(Z_{t}=z;\\pi_{b})\\neq0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\Gamma}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})\\xrightarrow{\\mathrm{\\scriptsize~a.s.}}\\mathrm{P}_{\\phi}^{\\pi_{e}}(z,z^{\\prime});\\quad\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\xrightarrow{\\mathrm{\\scriptsize~a.s.}}\\mathrm{R}_{\\phi}^{\\pi_{e}}(z);\\quad\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)\\xrightarrow{\\mathrm{\\scriptsize~a.s.}}\\eta_{\\phi}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "giving the final result $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\xrightarrow{\\mathrm{a.s.}}\\mathfrak{R}_{\\phi}^{\\pi_{e}}$ , while the values of the components for $z\\in\\tilde{Z}$ are hardcoded as detailed in Property B.6. ", "page_idx": 24}, {"type": "text", "text": "B.4 Off-Policy Evaluation with ARPs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem 4.1. The expected return of the ARP $\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}$ (built from off-policy data) converges almost surely to the expected return of $\\pi_{e}$ , i.e., $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}})\\;\\xrightarrow{a.s.}J(\\pi_{e};M)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. The expected return $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}})$ is a continuous function of $\\begin{array}{r l}{\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}}}&{{}:=}\\end{array}$ $(\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}},\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}},\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}})$ , given by $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}})\\;=\\;\\left(\\mathbb{I}-\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\right)^{-1}\\widehat{\\mathrm{R}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}\\widehat{\\eta}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}$ . Similar to the proof of Lemma 3.2, by the continuous mapping theorem and Lemma 3.3, we obtain the final result that enables off-policy evaluation with ARPs: ", "page_idx": 24}, {"type": "equation", "text": "$$\nJ(\\pi_{e};\\widehat{\\mathfrak{R}}_{n,\\phi}^{\\pi_{b}\\to\\pi_{e}})\\stackrel{\\mathrm{\\scriptsize~a.s.}}{\\longrightarrow}J(\\pi_{e};\\mathfrak{R}_{\\phi}^{\\pi_{e}})=J(\\pi_{e};M).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.5 Variance Reduction: Leveraging Markovness of the State Abstraction ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Variance in estimation of the model of the ARP from off-policy can be reduced by clipping the importance weights. We now show that when the state abstraction function is $c$ -th order Markov, clipping the importance weights to the $c$ most recent terms continues to provide a consistent off-policy expected return estimate. ", "page_idx": 25}, {"type": "text", "text": "Theorem 4.3. Given a c-th order Markov $\\phi$ , the expected return of the abstract reward process $\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ converges almost surely to the expected return of $\\pi_{e}$ , i.e., $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})\\stackrel{a.s.}{\\longrightarrow}J(\\bar{\\pi_{e}};M)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. We only need to show that for a state abstraction function $\\phi\\in\\Phi$ that is $c$ -th order Markov, i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{v}_{\\mathrm{r}}(\\phi(S_{t+1})|\\phi(S_{t}),\\phi(S_{t-1}),\\dots,\\phi(S_{(t-c+1)^{+}});\\pi)=\\mathrm{Pr}(\\phi(S_{t+1})|\\phi(S_{t}),\\phi(S_{t-1}),\\dots,\\phi(S_{1}),\\phi(S_{0});\\pi)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $\\pi\\in\\left\\{\\pi_{b},\\pi_{e}\\right\\}$ the following holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}\\xrightarrow{\\mathrm{~a.s.}}\\mathfrak{R}_{\\phi}^{\\pi_{e}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The remaining steps to show that the return estimates are consistent follow from Theorem 4.1. For brevity, we denote by $Z_{t}:=\\phi(S_{t})$ for the rest of the proof. Consider the expression for the transition function without any weight clipping, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime}):=\\frac{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{0:t}}{\\sum_{i,t}\\mathbf{1}_{t}^{i}\\{z\\}\\rho_{0:t}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As shown in Property B.6, if $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi_{b})\\neq0}\\end{array}$ , as $n\\to\\infty$ the numerator of $\\widehat{\\mathrm{P}}_{n,\\phi}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime})$ converges almost surely to $\\begin{array}{r}{\\sum_{t}\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\}\\rho_{0:t};\\pi_{b}\\right]}\\end{array}$ . When the abstract states are $c$ -th order Markov, ", "page_idx": 25}, {"type": "text", "text": "Using the tower rule [60], $\\mathbb{E}\\left[\\mathbf{1}\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\}\\rho_{0:t};\\pi_{b}\\right]$ can be re-written as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z\\right\\}\\rho_{0;t};\\tau_{b}\\right.}\\\\ &{=\\mathbb{E}\\left[\\left.\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\right\\}\\rho_{0;(t-c)+\\rho}(t-c+1)^{+}t\\right\\}^{-}\\pi_{b}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\right\\}\\rho_{0;(t-c)+\\rho(t-c+1)+\\tau}\\right|\\left.\\left(Z_{t}\\right)_{i=(t-c+1)+}^{t};\\pi_{b}\\right\\};\\pi_{b}\\right]}\\\\ &{\\stackrel{{,}=}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\right\\}\\rho_{(t-c+1)^{+}t}\\left.\\left|\\left(Z_{t}\\right)_{i=(t-c+1)+}^{t};\\pi_{b}\\right|\\mathbb{E}\\left[\\rho_{0;(t-c)+\\tau}\\left.\\left(Z_{t}\\right)_{i=(t-c-1)+\\tau}^{t};\\pi_{b}\\right]\\right\\}\\right.\\right.}\\\\ &{\\left.\\left.-\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\right\\}\\rho_{(t-c+1)+\\tau}\\left.\\left(Z_{t}\\right)_{i=(t-c+1)+}^{t};\\pi_{b}\\right\\};\\pi_{b}\\right]\\right.\\right.}\\\\ &{\\left.\\left.\\qquad\\mathbb{E}\\left[\\left.\\mathbb{E}\\left[\\rho_{0;(t-c)+\\tau}\\left.\\left|\\left(Z_{t}\\right)_{i=(t-c+1)+\\tau}^{t};\\pi_{b}\\right\\};\\pi_{b}\\right]\\right.\\right.\\right.}\\\\ &{=\\mathbb{E}\\left[\\left.\\mathbb{I}\\left\\{Z_{t}=z,Z_{t+1}=z^{\\prime}\\right\\}\\rho_{(t-c+1)^{+}t};\\pi_{b}\\right]\\underbrace{\\mathbb{E}\\left[\\rho_{0;(t-c)+\\tau},\\pi_{b}\\right]}_{=1}}\\\\ &{\\stackrel{{,}=}\\mathbb{P}_{\\mathbb{T}}(Z_{t}=z,Z_{t+1}=z^{\\prime};\\pi_{b}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(a)$ follows from Equation 63 and $(b)$ follows from Lemma 3.3. Thus when the abstract states are $c$ -th order Markov, the numerator estimated with $c$ -clipped importance weights almost surely converges to the same value as if the importance weights were not clipped. The value to which the denominator converges similarly undergoing a change of measure by the use of clipped importance weights, i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[{\\bf1}\\{Z_{t}=z\\}\\rho_{0:t};\\pi_{b}\\right]=\\mathbb{E}\\left[{\\bf1}\\{Z_{t}=z\\}\\rho_{(t-c+1)^{+}:t};\\pi_{b}\\right]=\\mathrm{Pr}(Z_{t}=z;\\pi_{e}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "when $\\phi$ is $c$ -th order Markov. Note that the expression for $\\widehat{\\mathrm{P}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}(z,z^{\\prime}):=\\frac{\\sum_{i,t}{\\bf1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{(t-c+1)^{+};t}}{\\sum_{i,t}{\\bf1}_{t}^{i}\\{z\\}\\rho_{(t-c+1)^{+};t}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\begin{array}{r l r}{X_{n}\\!}&{{}:=}&{\\!\\frac{1}{n}\\sum_{i,t}{\\bf1}_{t}^{i}\\{z,z^{\\prime}\\}\\rho_{(t-c+1)^{+}:t}}\\end{array}$ and $\\begin{array}{r l r}{Y_{n}}&{{}:=}&{\\frac{1}{n}\\sum_{i,t}{\\bf1}_{t}^{i}\\{z\\}\\rho_{(t-c+1)^{+}:t}}\\end{array}$ . We can write $\\begin{array}{r}{\\widehat{\\mathrm{P}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}(z,z^{\\prime})\\,=\\,\\frac{X_{n}}{Y_{n}}}\\end{array}$ . Following the exact steps from Lemma 3.3, that entail a careful application of the continuous mapping theorem, we can show that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{P}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}(z,z^{\\prime})\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}\\mathrm{P}_{\\phi}^{\\pi_{e}}(z,z^{\\prime}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "if $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi_{b})\\neq0.}\\end{array}$ . Similar derivations can be followed for the reward function and the initial st ate distribution, leading to the required result: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}\\ \\xrightarrow{\\mathrm{a.s.}}\\mathfrak{R}_{\\phi}^{\\pi_{e}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "when $\\begin{array}{r}{\\sum_{t}\\mathrm{Pr}(Z_{t}=z;\\pi_{b})\\neq0}\\end{array}$ , that enables almost sure convergence of the expected return estimate: ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})\\stackrel{\\mathrm{\\scriptsize~a.s.}}{\\longrightarrow}J(\\pi_{e};\\mathfrak{R}_{\\phi}^{\\pi_{e}})=J(\\pi_{e};M).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C Empirical Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we provide additional empirical details for the experiments presented in the main paper. An overall step-by-step algorithm for STAR is as follows: ", "page_idx": 27}, {"type": "text", "text": "Algorithm 1 Overview of ${\\mathsf{S T A R}}(\\phi,c)$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Input: \u03c0e, \u03c0b, D(n\u03c0b)   \n1. Apply state abstraction to $\\mathcal{D}_{n}^{(\\pi_{b})}$ and compute importance weights: $\\begin{array}{r}{\\overleftarrow{\\mathrm{\\boldmath~v~}}i,t:\\mathrm{Store}\\left(Z_{t}^{(i)}=\\phi\\left(S_{t}^{(i)}\\right),A_{t}^{(i)},\\overbar{R}_{t}^{(i)},\\rho_{(t-c+1)+:t}^{(i)}=\\overleftarrow{\\prod}_{j=(t-c+1)^{+}}^{t}\\frac{\\pi_{e}(S_{j}^{(i)},A_{j}^{(i)})}{\\pi_{b}(S_{j}^{(i)},A_{j}^{(i)})}\\right)}\\end{array}$   \n2. Estimate the components of the ARP $\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}$ $\\begin{array}{r l}&{\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{c}}=\\left(\\widehat{\\mathbf{P}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{c}},\\widehat{\\mathbf{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{c}},\\widehat{\\eta}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}\\right)\\mathrm{~where},}\\\\ &{\\qquad\\widehat{\\mathbf{P}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}=\\frac{\\sum_{i,t}\\mathbf{1}\\left\\{\\phi(S_{t}^{(i)})=z,\\phi(S_{t+1}^{(i)})=z^{\\prime}\\right\\}\\rho_{(t-c+1)^{+};t t}}{\\sum_{i,t}\\mathbf{1}\\left\\{\\phi(S_{t}^{(i)})=z\\right\\}\\rho_{(t-c+1)^{+};t t}},}\\\\ &{\\qquad\\widehat{\\mathbf{R}}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}=\\frac{\\sum_{i,t}\\mathbf{1}\\left\\{\\phi(S_{t}^{(i)})=z\\right\\}\\rho_{(t-c+1)^{+};t t}R_{t}^{(i)}}{\\sum_{i,t}\\mathbf{1}\\left\\{\\phi(S_{t}^{(i)})=z\\right\\}\\rho_{(t-c+1)^{+};t t}},}\\\\ &{\\qquad\\widehat{\\eta}_{\\phi,c}^{\\pi_{b}\\rightarrow\\pi_{e}}(z)=\\frac{\\sum_{i=1}^{n}\\mathbf{1}\\left\\{\\phi(S_{0}^{(i)})=z\\right\\}}{n}}\\end{array}$   \n3. Compute the expected return $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})$ from the ARP. $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}):=(\\boldsymbol{\\mathrm{I}}-\\widehat{\\boldsymbol{\\mathrm{P}}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})^{-1}\\widehat{\\boldsymbol{\\mathrm{R}}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}\\widehat{\\eta}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}}$   \nOutput: $J(\\pi_{e};\\widehat{\\mathfrak{R}}_{\\phi,c}^{\\pi_{b}\\to\\pi_{e}})$ ", "page_idx": 27}, {"type": "text", "text": "C.1 Domains ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We perform empirical evaluations on a range of domains that consist of continuous domains and domains with large state spaces with long horizons. The domains are as follows: ", "page_idx": 27}, {"type": "text", "text": "CartPole The CartPole domain is a classic control problem from OpenAI Gym [6]. The task is to balance a pole on a cart by moving the cart left or right. The state space is continuous, and the action space is discrete. We use the standard CartPole environment from OpenAI Gym. In the experiments, $\\pi_{b}$ is uniformly random, i.e., the left and right actions are each taken with probability 0.5. The evaluation policy $\\pi_{e}$ is a policy that takes the action right with probability 0.9 when the pole is leaning left, and right with probability 0.1 when the pole is leaning right. This results in a policy that is not optimal, but is somewhat successful at balancing the pole. ", "page_idx": 27}, {"type": "text", "text": "ICU-Sepsis The ICU-Sepsis domain simulates the treatment of sepsis in the ICU. Built from the MIMIC-III database [24] and drawing from the analysis of Komorowski et al. [26], it consists of 747 states that denote the status of a patient and 25 possible actions that denote possible medical interventions. At the end of each episode, if the patient survives, a reward of $+1$ is given, while death corresponds to a reward of 0, with all intermediate rewards also being 0. This results in the expected return of a policy corresponding to the probability that a randomly selected patient will survive. In the experiments, $\\pi_{e}$ is set to an expert policy, provided with the domain and included in the submitted codebase, while $\\pi_{b}$ is a policy that is a more stochastic version of the expert policy constructed by temperature scaling [17] the action probabilities of expert policy with a temperature parameter $\\tau=2$ . ", "page_idx": 27}, {"type": "text", "text": "Asterix The Asterix domain is a miniaturized version of the Atari game Asterix where the task is to collect items while avoiding enemies. We use the implementation of the game from the MinAtar testbed [61], where the dimension of each state is $10\\times10\\times4$ , and the action set consists of six actions. The data collecting policy $\\pi_{b}$ is uniformly random, while the evaluation policy $\\pi_{e}$ picks actions with non-uniform skewed probabilities. ", "page_idx": 27}, {"type": "image", "img_path": "cYZibc2gKf/tmp/50bdd8134eed1d99269ede5234552740246d9d06db422ba82827dcb5348d3858.jpg", "img_caption": ["C.2 Additional Results for Estimator Selection "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 4: Heatmap of the log mean squared error (MSE) of the OPE estimators for the CartPole, Asterix, and ICU-Sepsis domains. The vertical axis represents the number of abstract states $|\\mathcal{Z}|$ , and the horizontal axis represents the value of the hyperparameter $c$ . The color intensity indicates the log MSE, with lower values denoting better performance. Note that the variation with $|\\mathcal{Z}|$ is strongly influenced by the class of abstraction functions used, which in this work is CluSTAR. ", "page_idx": 28}, {"type": "text", "text": "For each domain, we evaluate the OPE performance of the estimated ARP induced by varying configurations of $(\\phi,c)$ . For the class of abstraction function, we observe that the simple method CluSTAR performs well across all domains, and hence we use it for all experiments. CluSTARtakes an input a single hyperparameter, the number of centroids initialized, denoted by $|\\mathcal{Z}|$ . We evaluate the following configurations of $\\mathcal{Z}$ and $c$ for each domain: ", "page_idx": 28}, {"type": "text", "text": "1. CartPole: 35 estimators - $|\\mathcal{Z}|\\in\\{2,4,8,16,32,64,128\\},c\\in\\{1,2,3,4,5\\}.$   \n2. ICU-Sepsis: 25 estimators - $|\\mathcal{Z}|\\,\\in\\,\\{2,4,8,16,32\\}$ , $c\\in\\{1,2,3,4,5\\}$ . ICU-Sepsis with $|\\mathcal{Z}|=32$ and $n=5000$ is excluded for computational reasons.   \n3. Asterix: 25 estimators - $|\\mathcal{Z}|\\in\\{2,4,8,16,32\\}$ , $c\\in\\{1,2,3,4,5\\}.$ ", "page_idx": 28}, {"type": "text", "text": "In Figure 4 we report the log MSE for each estimator. ", "page_idx": 28}, {"type": "text", "text": "Effect of $|\\mathcal{Z}|$ : For the class of abstraction functions induced by CluSTAR, the effect of $|\\mathcal{Z}|$ varies across domains. In some domains, such as ICU-Sepsis, the estimators obtained by varying this hyperparameter show similar performance. However, in domains like Asterix and CartPole, performance is more sensitive to the choice of $|\\mathcal{Z}|$ , with larger values performing better. ", "page_idx": 28}, {"type": "text", "text": "Effect of $c$ : It is expected that large values of $c$ will lead to higher variance of the estimates. In low data regimes, small values of $c$ result in relatively lower MSE as the variance is lowered at the cost of increased bias. As the amount of data increases, the best value of $c$ also increases. This effect is most pronounced in the Asterix domain, and to a lesser extent in the CartPole domain. ", "page_idx": 28}, {"type": "text", "text": "The key takeaway from Figure 4 is that the optimal $|Z|$ and $c$ are a function of the amount of data $(n)$ , the characteristics of the domain and the class of abstraction functions used. ", "page_idx": 28}, {"type": "text", "text": "C.3 Compute ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "All experiments were run for 200 seeds each, on 3 domains in total. Each run took between 3 hours to 3 days (depending on the domain) and this duration includes offilne data collection. The experiments ", "page_idx": 28}, {"type": "text", "text": "were run using 32 threads on Xeon E5-2680 CPUs on a computing cluster, bringing the total compute time to roughly 45000 compute hours. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: All claims are contributions are supported by corresponding theorems and empirical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The limitations are highlighted both in the concluding discussion and as a separate \u201cLimitations\u201d section in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All proofs are provided in the supplementary material, and assumptions states clearly in the main text. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code has been included with the submission. Random seeds are set and fixed across multiple trials, along with the source code of the domain used. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Anonymized code is submitted as a .zip flie with the submission. The codebase will be made public upon acceptance. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: There is a specific section, in the main text and supplementary material, dedicated to choice of hyperparameters along with necessary experimental details. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Standard error is reported for main experiments, with the number of trials specified. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Details about the compute resources are provided in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Naive application of the framework to safety-critical applications could lead to unintended consequences. A discussion on the limitations and potential impacts of the proposed framework is provided in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Both positive impacts, in terms of strong results for OPE and potential negative impacts, in terms of naive application of the framework to safety-critical applications, are discussed in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Original creators of all domains used and all prior work are duely cited in the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The code and related artifacts, such as pre-trained policies, are well documented and provided alongside the submission. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]