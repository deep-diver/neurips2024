[{"figure_path": "YZoGmJrOS9/tables/tables_1_1.jpg", "caption": "Table 1: Summary of tasks. Each regression target fo(xi) is either parametrized by a randomly sampled \u03b8 or directly computed/sampled as detailed above.", "description": "This table summarizes the characteristics of six different regression tasks used in the paper's experiments.  For each task, it specifies the input dimension (dim), the number of data points (points), the distribution of the input data (x distribution), how the target output is calculated (y calculation), and any task-specific parameters (Task-specific).  The tasks range in complexity from simple linear regression to more complex non-linear functions like decision trees and sparse parity.", "section": "3 Methods"}, {"figure_path": "YZoGmJrOS9/tables/tables_4_1.jpg", "caption": "Table 2: A summary of the primary architectural differences between GPT-2, Llama, and Mamba. We examine all variations between GPT-2 and Llama and all variations between Llama and Mamba.", "description": "This table summarizes the key architectural differences between three transformer models: GPT-2, Llama, and Mamba.  It highlights variations in positional embedding (absolute, RoPE, or none), feed-forward network architecture (2-layer MLP, convolutional MLP, or none), attention mechanism (Multi-Query Multi-Head, or Mamba Mixer), and normalization (Layer Norm or RMS Norm).  The paper uses this information to guide the creation and comparison of hybrid models.", "section": "3.2 Architectures"}, {"figure_path": "YZoGmJrOS9/tables/tables_5_1.jpg", "caption": "Table 1: Summary of tasks. Each regression target fo(xi) is either parametrized by a randomly sampled \u03b8 or directly computed/sampled as detailed above.", "description": "This table summarizes the characteristics of six regression tasks used in the paper's experiments.  For each task, it lists the input dimension (dim), the number of data points (points), the distribution of input data (x distribution), how the target variable is calculated or sampled (y calculation / parameter distribution), and any task-specific parameters (Task-specific).  The tasks range in complexity from simple linear regression to more complex decision trees, allowing for a comprehensive evaluation of the models' in-context learning capabilities.", "section": "3 Methods"}, {"figure_path": "YZoGmJrOS9/tables/tables_5_2.jpg", "caption": "Table 1: Summary of tasks. Each regression target fo(xi) is either parametrized by a randomly sampled \u03b8 or directly computed/sampled as detailed above.", "description": "This table summarizes the characteristics of six different regression tasks used in the paper's experiments.  Each task is defined by its input dimension, the number of data points used, the distribution of input features (x), how the target output (y) is calculated or sampled, and any task-specific hyperparameters. The tasks range in complexity from simple linear regression to more complex tasks involving non-linear relationships and sparsity.", "section": "3 Methods"}, {"figure_path": "YZoGmJrOS9/tables/tables_8_1.jpg", "caption": "Table 3: ICL Regression Scores for each architecture on each task, averaged over many sampled functions, with 95% confidence intervals in the headers for each row. Best-in-task values are in boldface except when not statistically significant from another architecture. GPT-2/Llama hybrids were not evaluated on Sparse Parity due to compute constraints and lack of supporting evidence that they should succeed. *These models were used as the baseline for this task.", "description": "This table presents the ICL Regression Scores for various transformer models (GPT-2, Llama, Mamba, and their hybrids) across six different regression tasks.  The scores represent the average performance of each model on each task, considering many randomly sampled functions.  Confidence intervals (95%) are provided to show the statistical significance of the results. The table highlights the best-performing model for each task, and indicates when models' performance is not significantly better than others. Note that GPT-2/Llama hybrids were excluded from Sparse Parity due to computational limitations, and the models marked with an asterisk (*) served as baselines for their respective tasks.", "section": "3.3 Evaluation"}]