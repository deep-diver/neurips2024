[{"figure_path": "8APPypS0yN/figures/figures_8_1.jpg", "caption": "Figure 2: Accuracy for GNNK and GNN10 Tinhofer over 1000 epochs on synthetic datasets Cycles-pin and CSL-pin and real-world datasets MCF-7 [63, 37] and Peptites-func [55, 17].", "description": "The figure shows the accuracy for two different models over 1000 epochs for four different datasets (two synthetic and two real-world). The first model is a standard GNN with multiple layers (the number of layers is specified for each dataset), while the second model is a 1-layer GNN that utilizes the Tinhofer individualization scheme. The results demonstrate that the GNN with Tinhofer individualization converges faster and more stably, particularly for real-world datasets, where the standard GNN struggles to reach high accuracy within the tested 1000 epochs.  This highlights the benefit of node individualization in improving the expressivity of shallow GNNs.", "section": "5.1 Expressivity"}, {"figure_path": "8APPypS0yN/figures/figures_8_2.jpg", "caption": "Figure 3: Difference between test and training accuracy for GNNs with the RNI, RP and Tinhofer individualization schemes, on datasets of CSL and 3-regular graphs of various sizes.", "description": "This figure displays the difference between training and testing accuracy for Graph Neural Networks (GNNs) using three different node individualization schemes (RNI, RP, and Tinhofer). The results are shown for two types of graphs: CSL graphs (circular skip link) and 3-regular graphs. Different sizes of graphs are tested (n = 16, 17, 32, 41, 64, 83). The x-axis represents the number of samples used in training, while the y-axis represents the difference between training and test accuracy. The figure helps to illustrate how the sample complexity varies depending on the individualization scheme and graph structure.", "section": "5.2 VC dimension"}, {"figure_path": "8APPypS0yN/figures/figures_16_1.jpg", "caption": "Figure 1: Comparison between Tinhofer and Tinhoferw. Panel (a): Two graphs, where letters indicate initial node labels. The Tinhofer algorithm finds a canonical ordering on the two graphs. Panel (b): The Tinhofer scheme concatenates the position of the node in the ordering to the node label. The relabeled graphs have edit distance 3. Panel (c): The Tinhoferw scheme concatenates the position of the node within its WL color class. The edit distance remains 1, as in the original graphs.", "description": "This figure compares the Tinhofer and the improved Tinhoferw node individualization schemes.  It shows how the Tinhofer scheme, by concatenating node position in a canonical ordering to node labels, can significantly increase the edit distance between similar graphs, making them harder to distinguish with a GNN. Conversely, the Tinhoferw scheme, by only concatenating the position within the Weisfeiler-Lehman color class, preserves the edit distance, illustrating its superior ability to maintain graph similarity while achieving individualization.", "section": "5.1 Expressivity"}, {"figure_path": "8APPypS0yN/figures/figures_22_1.jpg", "caption": "Figure 5: Covering numbers for the NCI dataset", "description": "This figure shows the covering numbers for the NCI dataset using different node individualization schemes. The x-axis represents the log10 of epsilon (\u03b5), and the y-axis represents the covering number.  The curves show how the number of balls of radius \u03b5 needed to cover the space of functions (represented by different individualization schemes) changes with \u03b5. The comparison provides insights into the sample complexity of graph neural networks using different node individualization schemes.", "section": "5.4 Covering numbers"}, {"figure_path": "8APPypS0yN/figures/figures_23_1.jpg", "caption": "Figure 5: Covering numbers for the NCI dataset", "description": "This figure shows the covering numbers for the NCI dataset using different node individualization schemes. The x-axis represents the logarithm of epsilon (\u03b5), which is a parameter related to the accuracy of the covering. The y-axis represents the covering number, which is a measure of the complexity of the function class. Each subplot corresponds to a different individualization scheme: None, RP, RNI, Tinhofer, Tinhoferw, and LPE. The blue and orange lines represent the covering number for the 1-norm and \u221e-norm, respectively. The figure shows that the covering numbers for the Tinhofer and Tinhoferw schemes are significantly lower than those for the other schemes, suggesting that these schemes lead to simpler function classes and potentially better generalization performance.", "section": "5.4 Covering numbers"}, {"figure_path": "8APPypS0yN/figures/figures_23_2.jpg", "caption": "Figure 5: Covering numbers for the NCI dataset", "description": "This figure shows the covering numbers for the NCI dataset using different node individualization schemes. The x-axis represents the log10 of epsilon (\u03b5), which is a parameter used in calculating covering numbers. The y-axis shows the covering number, indicating the minimum number of balls of radius \u03b5 needed to cover the set of graphs. The different lines represent different node individualization schemes: None, RP, RNI, Tinhofer, Tinhoferw, and LPE.  The plot compares the 1-norm and \u221e-norm covering numbers for each scheme, offering insights into the sample complexity of graph neural networks (GNNs) under various individualization strategies.", "section": "5.4 Covering numbers"}, {"figure_path": "8APPypS0yN/figures/figures_23_3.jpg", "caption": "Figure 3: Difference between test and training accuracy for GNNs with the RNI, RP and Tinhofer individualization schemes, on datasets of CSL and 3-regular graphs of various sizes.", "description": "This figure displays the difference between training and testing accuracy for GNNs using three different individualization schemes (RNI, RP, and Tinhofer) on two types of graphs: CSL (circular skip link) graphs and 3-regular graphs.  Each type of graph is tested at various sizes (indicated by 'n' representing the number of nodes). The plots show how the generalization gap (difference between train and test accuracy) changes as the amount of training data increases, for each individualization method and graph type. The goal is to show how different individualization schemes impact the generalization capability of GNNs. The finding is that Tinhofer tends to exhibit smaller generalization gaps, particularly for the CSL graphs, showing its greater efficiency and effectiveness.", "section": "5.2 VC dimension"}]