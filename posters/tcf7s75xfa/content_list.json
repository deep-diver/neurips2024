[{"type": "text", "text": "Physics-Informed Variational State-Space Gaussian Processes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Oliver Hamelijnck University of Warwick oliver.hamelijnck@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Arno Solin Aalto University arno.solin@aalto.fi ", "page_idx": 0}, {"type": "text", "text": "Theodoros Damoulas University of Warwick t.damoulas@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, nonlinear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Physical modelling is integral in modern science and engineering with applications from climate modelling [61] to options pricing [6]. Here, the key formalism to inject mechanistic physical knowledge are differential equations (DEs), which given initial and/or boundary values, are typically solved numerically [8]. In contrast machine learning is data-driven, and aims to learn latent functions from observations. However the increasing availability of data has spurred interest in combining these traditional mechanistic models with data-driven methods through physics-informed machine learning. These hybrids approaches aim to improve predictive accuracy, computational efficiency by leveraging both physical inductive biases with observations [30, 44]. ", "page_idx": 0}, {"type": "text", "text": "A principled way to incorporate prior physical knowledge is through Gaussian processes (GPs). GPs are stochastic processes and are a data-centric approach that facilitates the quantification of uncertainty. Recently AUTOIP was proposed in order to integrate non-linear physics into GPs [40], where solutions to ordinary and partial differential equations (ODEs, PDEs) are observed at a finite set of collocation points. This is an extension of the probabilistic meshless method (PMM, [12]) to the variational setting such that non linear equations can be incorporated. Similarly, [4] introduced HELMHOLTZ-GP, that constructs GP priors that adhere to curl and divergence-free constraints. Such properties are required for the successful modelling of electromagnetic fields [60] and ocean currents through the Helmholtz decomposition [4]. These approaches enable the incorporation of physics but incur a cubic computational complexity from needlessly computing full covariance matrices, as illustrated in Fig. 1. For ODEs (time-series setting), extended Kalman smoothers incorporate non-linear physics (EKS) [64, 34] and recover popular ODE solvers whilst achieving linear-in-time complexity through state-space GPs [57, 25]. ", "page_idx": 0}, {"type": "image", "img_path": "tCf7S75xFa/tmp/5b03e0bc13ee799728608e772290c73664ffb457daf01645906b0f6a4d1ffa9b.jpg", "img_caption": ["Figure 1: The state-space formalism allows for linear-time inference in the temporal dimension. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work we propose a unified physics informed state-space GP (PHYSS-GP) that is a probabilistic models where mechanistic/physics knowledge is incorporated as an inductive bias. We can handle both linear and non-linear PDEs and ODEs whilst maintaining linear-in-time computational efficiency. We additionally derive a state-space variational inference algorithm that further reduces the computational cost in the spatial dimension. We recover EKS, PMM, and HELMHOLTZ-GP as special cases, and outperform AUTOIP in terms of computational efficiency and predictive performance. In summary: ", "page_idx": 1}, {"type": "text", "text": "1. We derive a state-space GP that can handle spatio-temporal derivatives with a computational complexity that is linear in the temporal dimension.   \n2. With this we derive a unifying state-space variational inference framework that allows the incorporation of both linear and non-linear PDEs whilst achieving a linear-in-time complexity and recovering state-of-the-art methods such as EKS, PMM and HELMHOLTZ-GP.   \n3. We further explore three approximations, namely a structured variational posterior, spatial sparsity, and spatial minibatching, that reduce the cubic spatial computational costs to linear.   \n4. We showcase our methods on a variety of synthetic and real-world experiments and outperform the current state-of-the-art methods AUTOIP and HELMHOLTZ-GP both in terms computational and predictive performance. ", "page_idx": 1}, {"type": "text", "text": "Code to reproduce experiments is available at https://github.com/ohamelijnck/physs_gp. ", "page_idx": 1}, {"type": "text", "text": "2 Background on Gaussian Processes ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Gaussian processes A GP is a distribution on an infinite collection of random variables such that any finite subset is jointly Gaussian [50]. Given observations $\\mathbf{X}\\in\\mathbb{R}^{N\\times F}$ and $\\mathbf{y}\\in\\mathbb{R}^{N}$ then ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\mathbf{y},f\\,|\\,\\pmb{\\theta})=\\prod_{n}^{N}p(y_{n}\\,|\\,f(\\mathbf{x}_{n}),\\pmb{\\theta})\\,p(f\\,|\\,\\pmb{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is a joint model where $p(f\\mid\\pmb\\theta)$ is a zero mean GP prior with kernel ${\\bf K}(\\cdot,\\cdot),\\;\\;f({\\bf X})\\;\\;\\sim$ $p\\big(f(\\bar{\\mathbf{X}})\\,\\big|\\,0,\\mathbf{K}(\\mathbf{X},\\mathbf{X})\\big)$ , and $\\pmb{\\theta}$ are (hyper) parameters. We are primarily concerned with the spatiotemporal setting where we observe $N_{\\mathrm{t}}$ temporal and $N_{\\mathrm{s}}$ spatial observations $x_{\\mathrm{{t,s}}}\\in\\mathbb{R}$ , $y_{\\mathrm{t,s}}\\,\\in\\,\\mathbb{R}$ on a spatio-temporal grid. Under a Gaussian likelihood, all quantities for inference and training are available analytically and, na\u00efvely, carry a dominant computational cost of $\\mathcal{O}((N_{\\mathrm{t}}\\,N_{\\mathrm{s}})^{3})$ . For time series data, an efficient way to construct a GP over $f$ (and its time derivatives) is through the state-space representation of GPs. Given a Markov kernel, the temporal GP prior can be written as the solution of a discretised linear time-invariant stochastic differential equation (LTI-SDE), which at time $k$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{f}}_{k+1}=\\mathbf{A}\\,\\bar{\\mathbf{f}}_{k}+q_{k}\\quad\\mathrm{and}\\quad y_{k}\\,|\\,\\bar{\\mathbf{f}}_{k}\\sim p(y_{k}\\,|\\,\\mathbf{H}\\,\\bar{\\mathbf{f}}_{k}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{A}$ is a transition matrix, is Gaussian noise, $\\mathbf{H}$ is an observation matrix, and $\\bar{\\mathbf{f}}$ is a $d\\!.$ - dimensional vector of temporal derivatives $\\begin{array}{r}{\\bar{f}=[f(\\cdot),\\frac{\\partial f(\\cdot)}{\\partial x},\\frac{\\partial^{2}f(\\cdot)}{\\partial x^{2}},\\cdot\\cdot\\cdot]^{\\top}}\\end{array}$ . With appropriately designed states, matrices and densities, SDEs of this form represent a large class of GP models, and Kalman smoothing enables inference in $O(N_{\\mathrm{t}}\\,d^{3})$ , see [55]. In the spatio-temporal setting, when the kernel matrix decomposes as a Kronecker product $\\mathbf{K}=\\mathbf{K}_{t}\\otimes\\mathbf{K}_{s}$ , then with a Markov time kernel, a state space form is admitted. This takes a particularly convenient form where the state is $\\bar{\\mathbf{f}}_{t}=[\\bar{f}((\\mathbf{X}_{s})_{1},\\bar{t}),\\cdot\\cdot\\cdot\\mathbf{\\Lambda},\\bar{f}((\\mathbf{X}_{s})_{N s},t)]^{\\top}$ , and inference requires $\\mathcal{O}(N_{\\mathrm{t}}(N_{\\mathrm{s}}\\,d)^{3})$ , see [59]. ", "page_idx": 1}, {"type": "text", "text": "Derivative Gaussian processes One main appeal of GPs is that they are closed under linear operators. Let $\\begin{array}{r}{\\mathcal{D}\\left[\\cdot\\right]=\\bar{\\mathcal{D}}_{\\mathrm{t}}\\,\\mathcal{D}_{\\mathrm{s}}\\left[\\cdot\\right]}\\end{array}$ be linear functional that computes $D=d_{t}\\,d_{s}$ space-time derivatives with $\\begin{array}{r}{D_{\\mathrm{t}}\\left[\\cdot\\right]=\\left[\\cdot,\\frac{\\partial\\cdot}{\\partial t},\\frac{\\partial^{2}\\cdot}{\\partial t^{2}},\\cdot\\cdot\\cdot\\right]}\\end{array}$ and $\\begin{array}{r}{D_{\\mathrm{s}}\\left[\\cdot\\right]=\\left[\\cdot,\\frac{\\partial\\cdot}{\\partial s},\\frac{\\partial^{2}\\cdot}{\\partial s^{2}},\\cdot\\cdot\\cdot\\right]}\\end{array}$ , then at a finite set of index points, the joint prior between f and its time and spatial derivatives is ", "page_idx": 1}, {"type": "equation", "text": "$$\np(\\bar{f}(\\mathbf{X}))=\\mathrm{N}\\left(\\,\\mathcal{D}\\,\\mathbf{f}\\,\\mid\\,\\mathbf{0},\\,\\mathcal{D}\\,\\mathbf{K}(\\mathbf{X},\\mathbf{X})\\,\\mathcal{D}^{*}\\,\\right)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\bar{f}(\\mathbf{X})={\\mathcal{D}}\\,f(\\mathbf{X})$ and $\\mathcal{D}^{*}$ is the adjoint of $\\mathcal{D}$ , meaning it operates on the second argument of the kernel [54]. When jointly modelling a single time and space derivative ${\\mathrm{\\Delta}d}_{t}={d}_{s}=1\\mathrm{\\Delta}$ ) the latent functions are $\\begin{array}{r}{\\bar{\\bf f}=[{\\bf f},\\frac{\\partial{\\bf f}}{\\partial\\mathrm{s}},\\frac{\\partial{\\bf f}}{\\partial\\mathrm{t}},\\frac{\\partial^{2}{\\bf f}}{\\partial\\mathrm{t}\\,\\partial\\mathrm{s}}]^{\\top}}\\end{array}$ and the kernel is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{K}}=\\mathcal{D}\\mathbf{K}(\\mathbf{X},\\mathbf{X})\\mathcal{D}^{*}=\\left[\\begin{array}{c c c c c}{\\mathbf{K}}&{-}&{-}&{-}&{-}\\\\ {\\frac{\\partial}{\\partial s}\\,\\mathbf{K}}&{\\frac{\\partial}{\\partial s}\\,\\mathbf{K}\\,\\frac{\\partial}{\\partial s}\\,\\overline{{\\,}}}&{-}&{-}\\\\ {\\frac{\\partial}{\\partial t}\\,\\mathbf{K}}&{\\frac{\\partial}{\\partial t}\\,\\mathbf{K}\\,\\frac{\\partial}{\\partial s}\\,\\overline{{\\,}}}&{\\frac{\\partial}{\\partial t}\\,\\mathbf{K}\\,\\frac{\\partial}{\\partial t}^{\\top}}&{-}\\\\ {\\frac{\\partial^{2}}{\\partial t\\,\\partial s}\\,\\mathbf{K}}&{\\frac{\\partial^{2}}{\\partial t\\,\\partial s}\\,\\mathbf{K}\\,\\frac{\\partial}{\\partial s}^{\\top}}&{\\frac{\\partial^{2}}{\\partial t\\,\\partial s}\\,\\mathbf{K}\\,\\frac{\\partial^{\\,\\top}}{\\partial t}^{\\top}}&{\\frac{\\partial^{2}}{\\partial t\\,\\partial s}\\,\\mathbf{K}\\frac{\\partial^{2}}{\\partial t\\,\\partial s}^{\\top}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is a multi-output prior whose samples are paths of $f$ with its corresponding derivatives. This prior is commonly known as a derivative GP and has found applications in monotonic GPs [51], input-dependent noise [41, 66] and explicitly modelling derivatives [58, 17, 43]. State-space GPs can be employed in the temporal setting since the underlying state computes $f(\\mathbf{x})$ with its corresponding time derivatives. In Sec. 3.1, we extend this to the spatio-temporal setting. ", "page_idx": 2}, {"type": "text", "text": "3 Physics-Informed State-Space Gaussian Processes (PHYSS-GP) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now propose a flexible generative model for incorporating information from both data observations and (non-linear) physical mechanics. We consider general non-linear evolution equations of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\ng(\\mathcal{N}_{\\theta}\\,f)=\\frac{\\partial f}{\\partial t}-\\mathcal{N}_{\\theta}\\,f=0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with appropriate boundary conditions, where $f:\\mathbb{R}^{F}\\,\\rightarrow\\,\\mathbb{R}$ is the latent quantity of interest and $\\mathcal{N}_{\\theta}$ is a non-linear differential operator [49]. We assume that $g:\\mathbb{R}^{P\\cdot D}\\overset{\\cdot}{\\rightarrow}\\mathbb{R}$ is measurable, and is well-defined such that there are sensible solutions to the differential equation [25]. We wish to place a GP prior over $f$ and update our beliefs after \u2018observing\u2019 that it should follow the solution of the differential equation. In general this is intractable and can only be handled approximately. By viewing Eqn. (4) as a loss function that measures the residual between $\\textstyle{\\frac{\\partial f}{\\partial t}}$ and the operator $\\mathcal{N}_{\\theta}\\,f$ then the right hand side (0) are virtual observations. The PDE can now be observed at a finite set of locations known as collocation points. This is a soft constraint (i.e. f is not guaranteed to follow the differential equation), but it can handle non-linear and linear mechanisms. However, there are special cases, namely curl and divergence-free constraints, that can be solved exactly. This follows from properties of vectors fields, where $f$ defines a potential function where linear combinations of its partial derivatives define vector fields that enforce these properties. To handle both of these situations we propose the following generative model ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbf{F}_{n}=\\mathbf{W}\\cdot\\left[\\,\\bar{f}_{q}(\\mathbf{X}_{n})\\,\\right]^{\\top}}_{\\mathrm{Linear}\\,\\mathrm{Mixing}},\\ \\underbrace{\\bar{f}_{q}\\sim\\mathcal{G P}(\\mathbf{0},\\bar{\\mathbf{K}}_{q})}_{\\mathrm{Independent}\\,\\mathrm{GP}\\mathrm{\\,Priors}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbf{y}_{n}^{(\\mathcal{O})}=\\mathbf{H}_{\\mathcal{O}}\\,\\mathbf{F}_{n}+\\epsilon_{\\mathcal{O}}}_{\\mathrm{Data}},\\,\\,\\underbrace{\\mathbf{0}_{n}^{(\\mathcal{C})}=g(\\mathbf{F}_{n})}_{\\mathrm{Collocation\\,Points}},\\,\\,\\underbrace{\\mathbf{y}_{n}^{(\\mathcal{B})}=\\mathbf{H}_{\\mathcal{B}}\\,\\mathbf{F}_{n}+\\epsilon_{\\mathcal{B}}}_{\\mathrm{Boundary\\,Values}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{f}_{q}$ are derivative GPs (see Eqn. (3)) that are linearly mixed by $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{(\\mathrm{P}\\,D)\\times(\\mathrm{Q}\\,D)}$ , and $\\mathbf{Y}^{(\\mathcal{O})},\\bar{\\mathbf{0}}^{(\\mathcal{C})}\\in\\mathbb{R}^{\\mathrm{N}\\times\\mathrm{P}}$ are observations and collocation points over the $\\mathrm{P}$ outputs and $\\mathbf{Y}^{(B)}\\in\\mathbb{R}^{\\operatorname{N}\\times(\\operatorname{P}D)}$ are boundary values over the derivatives of each output. The observation matrices $\\mathbf{H}_{O},\\mathbf{H}_{B}$ simply select the relevant parts of ${\\mathbf{F}}_{n}$ . For further details on notation see App. A. In many case we want to observe the solution of the differential equation exactly, however in some cases it may be required to add observation noise $\\epsilon_{\\mathcal{C}}$ to the collocation points, whether for numerical reasons or to model inexact mechanics. This is a flexible generative model where different assumptions and approximations will lead to various physics informed methods such as AUTOIP, EKS, PMM, and HELMHOLTZ-GP that we will develop state space algorithms for. Additionally it is possible to learn missing physics by parameterising unknown terms in Eqn. (4) through the GP priors in Eqn. (6) (see App. B.2). ", "page_idx": 2}, {"type": "text", "text": "Example 3.1 (EKS Prior and PMM). We recover EKS style generative models (see Hennig et al. [25]) when the mixing weight is identity $\\textbf{W}=\\textbf{I}$ , and $\\epsilon_{\\mathcal{C}},\\epsilon_{B}\\;\\rightarrow\\;0$ , and the non-linear transform $g$ is linearised. Let the prior be Markov $\\begin{array}{r}{p(\\bar{\\bf f})\\,=\\,\\prod_{k}^{N_{\\mathrm{t}}}p(\\bar{\\bf f}_{k}\\,|\\,\\bar{\\bf f}_{k-1})}\\end{array}$ with marginals $p(\\bar{\\bf f}_{k})\\,=$ $\\mathrm{N}\\left(\\Bar{\\mathbf{f}}_{k}\\mid\\mathbf{m}_{k}^{-},\\mathbf{P}_{k}^{-}\\right)$ . By taking a first-order Taylor linearisation $\\begin{array}{r}{g(\\bar{\\mathbf{f}}_{k})\\simeq g(\\mathbf{m}_{k}^{-})+\\frac{\\partial g(\\mathbf{m}_{k}^{-})}{\\partial\\mathbf{m}_{k}^{-}}\\,\\delta\\bar{\\mathbf{f}}_{k}}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "$\\delta\\mathbf{\\bar{f}}_{k}\\sim\\mathrm{N}\\left(\\mathbf{0},\\,\\mathbf{P}_{k}^{-}\\,\\right)$ the joint is ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\left[\\mathbf{\\overline{{f}}}_{k}\\right])\\simeq\\mathrm{N}\\left(\\left[\\mathbf{\\overline{{f}}}_{k}\\right]\\;|\\;\\left[\\mathbf{\\underline{{m}}}_{k}^{-}\\right],\\left[\\mathbf{\\underline{{I}}}_{g(\\mathbf{m}_{k}^{-})}^{\\mathbf{I}}\\right],\\;\\left[\\mathbf{\\underline{{J}}}_{g(\\mathbf{m}_{k}^{-})}^{\\mathbf{I}}\\right]\\;\\mathbf{P}_{k}^{-}\\;\\left[\\mathbf{\\underline{{J}}}_{g(\\mathbf{m}_{k}^{-})}^{\\mathbf{I}}\\right]^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is now a form that can directly be implemented into an extended Kalman smoothing algorithm [62]. When $\\mathrm{Q}>1$ the state $\\bar{\\mathbf{f}}$ is constructed by stacking the individual states of each latent [55]. With linear ODEs EKS coincides with PMM. ", "page_idx": 3}, {"type": "text", "text": "Example 3.2 (HELMHOLTZ-GP and Curl and Divergence-Free Vector Fields in 2D). Let $\\textrm{\\textbf{v}}=$ $[v_{t},v_{s_{1}},v_{s_{2}}]$ denote a 3D-vector field, then curl indicates the tendency of a vector field to rotate and divergence at a specific point indicates the tendency of the field to spread out. Curl and divergence-free fields follow ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\times\\mathbf{v}=0~\\,({\\mathrm{curl~free}}),\\,\\,\\,\\nabla\\cdot\\mathbf{v}\\,\\,\\,\\mathbf{\\xi}=0~\\,({\\mathrm{div.~free}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\nabla=[{\\frac{\\partial}{\\partial{\\ t}}},{\\frac{\\partial}{\\partial{\\bf s}_{I}}},{\\frac{\\partial}{\\partial{\\bf s}_{2}}}]}\\end{array}$ . Two basic properties of vector fields state that the divergence of a curl field and the curl of a derivative field are zero [3]. Let $[f_{1},f_{2}]$ be scalar potential functions then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{v}_{\\mathrm{curl}}=\\nabla f_{1}\\ \\ (\\mathrm{curl\\;free}),\\ \\ \\mathbf{v}_{\\mathrm{div}}=\\nabla\\times\\nabla\\ f_{2}\\ \\ (\\mathrm{div.\\;free})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "define curl and divergence-free fields. In 2D this simplifies to using the grad and rot operators over $\\mathbf{v}=[v_{s_{1}},v_{s_{2}}]$ (see [4]). Placing GP priors over $f_{q}$ we incorporate this into Eqn. (6) by defining ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{W}_{\\mathrm{grad}}=\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]\\,\\mathbf{H},\\,\\,\\,\\mathbf{W}_{\\mathrm{rot}}=\\left[\\begin{array}{l l}{0}&{1}\\\\ {-1}&{0}\\end{array}\\right]\\,\\mathbf{H}\\,\\,\\,\\mathrm{where}\\,\\,\\,\\mathbf{H}\\,\\,\\,\\mathrm{selects}\\,\\,\\,\\left[\\frac{\\partial f}{\\partial s_{I}},\\frac{\\partial f}{\\partial s_{\\mathcal{Q}}}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "HELMHOLTZ-GP is defined as the sum of GP priors over 2D curl and divergence-free fields [4]. ", "page_idx": 3}, {"type": "text", "text": "3.1 A Spatio-Temporal State-Space Prior ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The generative model in Eqn. (6) contains two complications: i) it includes potential non-lineararities, and ii) the independent priors are defined over latent functions with their partial derivatives which substantially increases the computational complexity. We wish to tackle both issues through statespace algorithms that are linear-in-time. We begin by deriving a state-space model that observes derivatives across space and time (see App. A.3 for the simpler time-series setting). In Sec. 3.2 we further derive a state-space variational lower bound that will enable computational speeds up in the spatial dimension. ", "page_idx": 3}, {"type": "text", "text": "First, we show how Kronecker structure in the kernel allows us to rewrite the model as the solution to an LTI-SDE. From the definition of $\\mathcal{D}$ , the separable covariance matrix has a repetitive structure that can be represented through a Kronecker product. The gram matrix is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}\\,\\mathbf{K}(\\mathbf{x},\\mathbf{x})\\,\\mathcal{D}^{*}=\\mathbf{K}_{\\mathrm{t}}^{\\mathcal{D}}(\\mathbf{x}_{\\mathrm{t}},\\mathbf{x}_{\\mathrm{t}})\\,\\otimes\\,\\mathbf{K}_{\\mathrm{s}}^{\\mathcal{D}}(\\mathbf{x}_{\\mathrm{s}},\\mathbf{x}_{\\mathrm{s}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{K}_{\\cdot}^{\\mathcal{D}.}=\\left[\\tilde{\\mathcal{D}}_{\\cdot}\\mathbf{K}_{\\cdot}\\quad\\tilde{\\mathcal{D}}_{\\cdot}\\tilde{\\mathcal{D}}_{\\cdot}^{*}\\right]$ and $\\tilde{\\mathcal{D}}.[\\cdot]=(\\mathcal{D}.[\\cdot])_{1}$ : excludes the underlying latent function. To find a Kronecker form of the gram matrix over $\\mathbf{X}$ , we will exploit the fact that $\\mathbf{X}$ is on a spatiotemporal grid and that the kernel is separable. Due to the separable structure a derivative over either the spatio (or temporal) dimension only affects the corresponding kernel, and so when considering $\\mathbf{X}$ , the gram matrix is still Kronecker structured: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{\\partial}{\\partial s}}\\,\\mathbf{K}(\\mathbf{x},\\mathbf{x})=\\mathbf{K}_{t}(\\mathbf{x},\\mathbf{x})\\cdot{\\frac{\\partial}{\\partial s}}\\,\\mathbf{K}_{s}(\\mathbf{x},\\mathbf{x})\\Rightarrow{\\frac{\\partial}{\\partial s}}\\,\\mathbf{K}(\\mathbf{X},\\mathbf{X})=\\mathbf{K}_{t}(\\mathbf{X}_{t},\\mathbf{X}_{t})\\otimes{\\frac{\\partial}{\\partial s}}\\,\\mathbf{K}_{s}(\\mathbf{X}_{s},\\mathbf{X}_{s}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The full prior over (a permuted) $\\mathbf{X}$ is now given as ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\bar{f}(\\mathbf{X}))\\cong\\mathbf{N}\\left(\\mathbf{0},\\,\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{t},\\mathbf{X}_{t})\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{s},\\mathbf{X}_{s})\\,\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is the form of a spatio-temporal Gaussian process with derivative kernels that can be immediately cast into a state-space form as in Eqn. (2) where $\\mathbf{H}=\\mathbf{I}$ , as we want to observe the whole state, not just f. The marginal likelihood and the GP posterior can now be computed using standard Kalman flitering and smoothing algorithms with a computational time of $O(N_{\\mathrm{t}}^{-}\\cdot(N_{\\mathrm{s}}\\cdot d_{s}\\bar{\\cdot}d)^{3})$ . Inference in PHYSS-GP now follows Ex. 3.1 by recognising that the filtering state consists of the spatial points with there spatio-temporal derivatives. The EKS prior in Ex. 3.1 can now be simply extended to the PDE setting by placing colocation points on a spatio-temporal grid [35]. ", "page_idx": 3}, {"type": "text", "text": "We now derive a variational lower bound for PHYSS-GP that maintains the computational beneftis of state-space GPs. This acts as an alternative way of handling the non-linearity of $g$ in Eqn. (6), and will also enable the reduction of the cubic spatial computation complexity in Sec. 4. We start by focusing on the single latent function setting $\\left(\\mathrm{Q}=1\\right)$ ) and collect all terms that relate to observations in Eqn. (6) with $\\begin{array}{r}{p({\\bar{\\bf Y^{'}}}|\\,\\bar{\\bf f})\\,=\\,\\prod_{n}^{N}\\,p({\\bf y}_{n}^{(\\mathcal{O})}|{\\bf H}_{\\mathcal{O}}\\,{\\bf F}_{n})\\,p({\\bf0}_{n}^{(\\mathcal{C})}|g({\\bf F}_{n}))\\,p({\\bf y}_{n}^{(\\mathcal{B})}|{\\bf H}_{\\mathcal{B}}\\,{\\bf F}_{n})}\\end{array}$ . VI frames inference as the minimisation of the Kullback\u2013Leibler divergence between the true posterior and an approximate posterior, which leads the optimisation of the ELBO [28]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{q(\\bar{\\mathbf{f}}\\mid\\xi)}{\\arg\\operatorname*{max}}\\,\\mathcal{L}=\\mathbb{E}_{q(\\bar{\\mathbf{f}})}\\left[\\log\\frac{p(\\mathbf{Y}\\mid\\bar{\\mathbf{f}})\\,p(\\bar{\\mathbf{f}})}{q(\\bar{\\mathbf{f}})}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we define the approximate posterior $q(\\mathbf{f}\\mid\\boldsymbol{\\xi})\\triangleq\\mathrm{N}\\left(\\textbf{f}|\\textbf{m},\\mathbf{S}\\right)$ as a free-form Gaussian with $\\pmb{\\xi}=(\\mathbf{m},\\mathbf{S})$ and $\\mathbf{m}\\in\\dot{\\mathbb{R}}^{D\\,N\\times1}$ , $\\mathbf{S}\\in\\mathbb{R}^{D\\,N\\times\\hat{D}\\,N}$ . The aim is to represent the approximate posterior as a state-space GP posterior, which will enable efficient computation of the whole evidence lower bound (ELBO). We will achieve this through the use of natural gradients. The natural gradient preconditions the standard gradient with the inverse Fisher matrix, meaning the information geometry of the parameter space is taken into account, leading to faster convergence and superior performance [2, 31, 27]. For Gaussian approximate posteriors the natural gradient has a simple form [26] ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\lambda_{k-1}+\\beta\\,\\frac{\\partial\\mathcal{L}}{\\partial\\mu_{k}}=\\left(1-\\beta\\right)\\widetilde{\\lambda}_{k-1}+\\beta\\,\\frac{\\partial\\mathrm{ELL}}{\\partial\\mu_{k}}+\\eta=\\widetilde{\\lambda}+\\eta\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\lambda}=(\\mathbf{S}^{-1}\\mathbf{m},\\mathbf{\\mathrm{1/2}}\\,\\mathbf{S}^{-1})$ and $\\pmb{\\mu}=(\\mathbf{m},\\mathbf{m}\\,\\mathbf{m}^{\\top}+\\mathbf{S})$ are the natural and expectation parameterisations. This is known as conjugate variational inference (CVI) as\u03bb represent the natural parameters for the conjugate prior $\\eta$ [31, 10, 20, 71]. For now, we will assum e that the likelihood is conjugate to ensure that $[\\bar{\\lambda}_{k}]_{2}$ is $p.s.d$ , this will be relaxed in Sec. 5. The derivative of the ELL is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{ELL}}{\\partial[\\mu]_{\\mathcal{Q}}}=\\sum_{\\mathrm{t,s}}^{N_{\\mathrm{t}},N_{\\mathrm{s}}}\\!\\frac{\\partial}{\\partial[\\mu]_{\\mathcal{Q}}}\\,\\mathbb{E}_{q}\\left[\\log p(\\mathbf{Y}_{(\\mathrm{t,s})}\\,|\\,\\bar{\\mathbf{f}}_{(\\mathrm{t,s})})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is under $q(\\bar{\\bf f}_{(\\mathrm{t,s})})$ , a $D$ dimensional Gaussian over the spatio-temporal derivatives at location $\\mathbf{x}_{\\mathrm{t,s}}$ . Within the sum, the only elements of $[\\mu]_{2}$ whose gradient will propagate through the expectation are the $D\\times D$ elements corresponding to these locations. These points are unique and so $\\bar{\\frac{\\partial\\mathrm{ELL}}{\\partial[\\mu]_{\\it2}}}$ has some (permutated) block-diagonal structure, hence Eqn. (14) can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\bar{\\mathbf{f}})\\propto\\prod_{t}^{N t}\\left[\\mathrm{N}(\\widetilde{\\mathbf{Y}}_{t}\\,|\\,\\bar{\\mathbf{f}}_{t},\\widetilde{\\mathbf{V}}_{t})\\right]\\,p(\\bar{\\mathbf{f}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widetilde{\\mathbf{Y}}_{t}$ is $D$ -dimensional. The natural gradient update, i.e. $q(\\bar{\\bf f}_{t})$ in moment parameterisation, can now be computed using Kalman smoothing in $\\mathcal{O}(N_{\\mathrm{t}}\\cdot(N_{\\mathrm{s}}\\cdot d_{s}\\cdot d)^{3})$ . Collecting $\\bar{\\tilde{\\mathbf{Y}}}=\\mathrm{vec}(\\,[\\tilde{\\mathbf{Y}}_{t}]\\,),\\tilde{\\mathbf{V}}=$ blkdiag $\\left(\\left[\\tilde{\\widetilde{\\mathbf{V}}}_{t}\\right]\\right)$ , then the ELBO can also be computed efficiently by substituting this form of $q(\\bar{\\bf f}_{t})$ in ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\sum_{\\mathbf{t},\\mathbf{s}}^{N_{\\mathrm{t}},N_{\\mathrm{s}}}\\mathbb{E}_{q(\\bar{\\mathbf{f}}_{(\\mathbf{t},\\mathbf{s})})}\\left[\\log p(\\mathbf{Y}_{(\\mathbf{t},\\mathbf{s})}\\mid\\bar{\\mathbf{f}}_{(\\mathbf{t},\\mathbf{s})})\\right]-\\sum_{t}^{N t}\\mathbb{E}_{q(\\bar{\\mathbf{t}}_{t})}\\left[\\log\\mathrm{N}(\\widetilde{\\mathbf{Y}}_{t}\\,|\\,\\bar{\\mathbf{f}}_{t},\\widetilde{\\mathbf{V}}_{t})\\right]+\\log p(\\widetilde{\\mathbf{Y}}\\mid\\widetilde{\\mathbf{V}})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first two terms only depend on $q(\\mathcal{D}\\,{\\bf f}_{t})$ and the final term is simply a by-product of running the Kalman fliter, leading to a dominant computational complexity of $\\bar{\\mathcal{O}}(\\mathrm{\\bar{N}}\\cdot(\\bar{N_{\\mathrm{s}}}\\cdot d_{s}\\cdot d)^{3})$ . This cost is linear in the datapoints $(N)$ because the expected log likelihood above decomposes across all spatio-temporal locations. In summary we have shown that natural gradient is equivalent updating a block-diagonal likelihood that decomposes across time; hence the approximate posterior is computable via Kalman smoothing algorithms. Extending to multiple latent functions $\\left(\\mathrm{Q}>1\\right)$ ) we define a full Gaussian approximate posterior that captures all correlations between the latent functions $q(\\bar{\\mathbf{f}}_{1},\\bar{\\mathbf{\\Gamma}}^{\\cdot\\,\\cdot\\,},\\bar{\\mathbf{f}}_{\\mathrm{Q}})\\,\\triangleq\\,\\mathrm{N}\\left(\\,\\bar{\\mathbf{f}}_{1},\\bar{\\mathbf{\\Gamma}}^{\\cdot\\,\\cdot\\,},\\bar{\\mathbf{f}}_{\\mathrm{Q}}\\mid\\mathbf{m},\\,\\mathbf{S}\\,\\right)$ where $\\mathbf{m}\\,\\in\\,\\mathbb{R}^{(N\\times Q)\\times1}$ , $\\mathbf{S}\\ \\in\\ \\mathbb{R}^{(N\\times Q)\\times(N\\times Q)}$ . All the observation models in Eqn. (6) decompose across data points, hence Eqn. (16) is still block-diagonal and decomposes across time, except now each component is of dimension $\\mathrm{Q}\\times N_{\\mathrm{t}}$ as it encodes the correlations of spatial points and their spatio-temporal derivatives across the latent functions. We denote this model as PHYSS-VGP and PHYSS-EKS when using a EKS prior (see Ex. 3.1). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Let the approximate posterior be (full) Gaussian $q(\\bar{\\mathbf{f}}_{1},\\cdot\\cdot\\cdot\\mathbf{\\bar{\\rho}},\\bar{\\mathbf{f}}_{\\mathrm{Q}})$ \u225c= $\\mathrm{~N~}\\big(\\:\\bar{\\mathbf{f}}_{1},\\cdots\\:,\\bar{\\mathbf{f}}_{\\mathrm{Q}}\\:|\\:\\mathbf{m},\\:\\mathbf{S}\\big)$ where $\\textbf{m}\\in\\ \\mathbb{R}^{(N\\times Q)\\times1}$ , $\\textbf{S}\\in\\ \\mathbb{R}^{(N\\times Q)\\times(N\\times Q)}$ . When $g$ is linear $a$ single natural gradient step with $\\beta=1$ recovers the optimal solution $p(\\bar{\\mathbf{f}}_{1},\\cdot\\cdot\\cdot\\mathbf{\\sigma},\\bar{\\mathbf{f}}_{\\mathrm{Q}}\\,|\\,\\mathbf{Y})$ . ", "page_idx": 5}, {"type": "text", "text": "We prove this in App. A.5.4. This result not only demonstrates the optimality of our proposed inference scheme in the linear Gaussian setting, but confirms that we recover batch models like PMM and HELMHOLTZ-GP, as well as EKS (see Ex. 3.1). ", "page_idx": 5}, {"type": "text", "text": "4 Reducing the Spatial Computational Complexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now propose three approaches that reduce the cubic computational complexity in the number of spatial derivatives and locations. The first augments the process with inducing points that alleviate cubic costs associated with $N_{\\mathrm{s}}$ . The second is a structured variational approximation that defines the approximate posterior only over the temporal prior and alleviates cubic costs associated with $d_{s}$ . Finally, we introduce spatial mini-batching that alleviates linear $N_{\\mathrm{s}}$ costs. When used in conjunction, the dominant computation cost is $\\mathcal{O}\\left(N_{\\mathrm{t}}\\cdot\\mathbf{\\bar{\\boldsymbol{d}}}_{s}\\cdot(M_{s}\\cdot d_{t})^{3}\\right)$ . These approximations are not only useful for the state-space setting and can readily be applied to reduce the computational complexity for batch variational models (such as AUTOIP). See App. B.1 for more details. ", "page_idx": 5}, {"type": "text", "text": "Spatio-Temporal Inducing Points (PHYSS-SVGP) In this first approximation, denoted by PHYSSSVGP, we augment the full prior $p(\\bar{\\mathbf{f}})$ with inducing points. By defining these inducing points on a spatio-temporal grid, we will show that we can still exploit Markov conjugate operations through natural gradients. Let $\\bar{\\mathbf{u}}=\\mathcal{D}\\,\\mathbf{u}\\in\\mathbb{R}^{M\\times D}$ be inducing points at locations $\\mathbf{\\bar{Z}}\\in\\dot{\\mathbb{R}}^{M\\times F}$ . From the standard SVGP formulation [27], the ELBO is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{\\,q(\\bar{\\mathbf{f}},\\bar{\\mathbf{u}})}\\left[\\log\\frac{p(\\mathbf{Y}\\,|\\,\\bar{\\mathbf{f}})\\,p(\\bar{\\mathbf{u}})}{q(\\bar{\\mathbf{u}})}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $q(\\bar{\\mathbf{f}},\\bar{\\mathbf{u}})=p(\\bar{\\mathbf{f}}\\mid\\bar{\\mathbf{u}})\\,q(\\bar{\\mathbf{u}})$ . By defining the inducing points on a spatio-temporal grid at temporal locations $\\mathbf{X}_{\\mathrm{t}}\\in\\mathbb{R}_{\\mathrm{t}}^{N}$ and spatial $\\mathbf{Z_{\\mathrm{s}}}\\in\\mathbb{R}^{M_{s}\\times(F-1)}$ then the marginal $p(\\bar{\\bf f}\\mid\\bar{\\bf u})$ is Gaussian with mean ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{\\mathbf{F}\\,|\\,\\mathbf{U}}=\\left[\\mathbf{I}\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}})\\,(\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}}))^{-1}\\right]\\bar{\\mathbf{u}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and variance given in Eqn. (41). This Kronecker structure allows us to again \u2018decouple\u2019 space and time, leading to natural gradient updates with block size $M_{s}\\times D$ , reducing the computational complexity to $\\bar{\\mathcal{O}}(N\\,(M_{s}\\cdot d_{s}\\bar{\\cdot}\\,d)^{3})$ ). For full details, see App. A.5.1. ", "page_idx": 5}, {"type": "text", "text": "Structured Variational Inference (PHYSS- $\\mathbf{SVGP_{H}}$ ) This second approximation, denoted as PHYSS$\\mathrm{SVGP_{H}}$ , defines the inducing points only over the temporal derivatives. This is a useful approximation as it can drastically reduce the size of the filter state, making it more computationally and memory efficient. We begin by defining the joint prior as ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\mathbf{F},\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})=p(\\mathbf{F}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\\,p(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p(\\mathbf{F}\\mid\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})$ is a Gaussian conditional with mean ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sharp\\left[{\\mathbf F}\\;|\\;\\mathcal{D}_{\\mathbf{t}}\\,{\\mathbf f}\\right]=\\left[{\\mathbf I}\\otimes\\widetilde{\\mathbf{K}}_{s}^{\\mathcal{D}}(\\mathbf{X}_{s},\\mathbf{X}_{s})\\,\\mathbf{K}_{s}(\\mathbf{Z}_{s},\\mathbf{Z}_{s})^{-1}\\right]\\mathcal{D}_{\\mathbf{t}}\\,{\\mathbf f},\\;\\mathrm{~with~}\\widetilde{\\mathbf{K}}_{s}^{\\mathcal{D}}(\\mathbf{X}_{s},\\mathbf{X}_{s})=\\left[{\\mathbf K}_{s}(\\mathbf{X}_{s},\\mathbf{Z}_{s})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We then define a structured variational posterior ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\bar{\\mathbf{f}},\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\\triangleq p(\\mathbf{F}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\\,q(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Substituting this into the ELBO we see that all the terms with the prior spatial derivatives cancel ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q}\\left[\\log\\frac{p(\\mathbf{Y}\\,|\\,\\bar{\\mathbf{f}})\\,p(\\bar{\\mathbf{f}}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\\,p(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})}{p(\\mathbf{E}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})\\,q(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{f})}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Again, the marginal $q(\\mathcal{D}_{\\mathrm{t}}\\,{\\bf f})$ maintains Kronecker structure, enabling Markov conjugate operations, leading to a computational cost of $\\mathcal{O}(N\\cdot d_{s}\\cdot(N_{\\mathrm{s}}\\cdot d)^{3})$ , see App. A.5.2. These variational approximations can simply be applied to non-state-space variational approximation, see App. B.1. ", "page_idx": 5}, {"type": "text", "text": "Spatial Mini-Batching A standard approach for handling big data is through mini-batching where the ELL is approximated using only a data subsample [27]. Directly appling mini-batching would be of little computation benefti because computation of the ELBO requires running a Kalman smoother that iterates through all time points. Instead, we mini-batch by subsampling $B_{\\mathrm{s}}$ spatial points ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\centering\\mathrm{ELL}\\approx\\sum_{\\mathrm{t}}^{N_{\\mathrm{t}}}\\frac{N_{\\mathrm{s}}}{B_{\\mathrm{s}}}\\sum_{i}^{B_{\\mathrm{s}}}\\mathbb{E}_{q}\\left[\\log p(\\mathbf{Y}_{\\mathrm{t,s}}\\,|\\,\\bar{\\mathbf{f}}_{\\mathrm{t,}i})\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $i$ is uniformly sampled. We used in conjunction with PHYSS-SVGP and $\\mathrm{^{>}H Y S S-S V G P_{H}}$ , this results in dominant costs of $\\mathcal{O}(N_{\\mathrm{t}}\\,(M_{s}\\cdot d_{s}\\cdot d)^{\\bullet})$ and $\\mathcal{O}\\left(N_{\\mathrm{t}}\\cdot d_{s}\\cdot(M_{s}\\cdot d)^{3}\\right)$ when $B_{s}\\ll N_{\\mathrm{s}}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Handling the PSD Constraint ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As discussed in Sec. 3.2 when the differential equation is non-linear, the model is no longer conjugate and the resulting natural gradients are not guaranteed to result in $p.s.d$ updates. This issue has received some attention in the literature [53, 63, 39], but these approaches do not maintain an efficient conjugate representation. One distinction is [71], which uses the Gauss-Newton approximation to maintain conjugate operations. We now extend this to support spatial inducing points and non-linear transformations. Due to space we focus on PHYSS-SVGP, but see App. A.5.3 for further details. The troublesome term for the natural gradient update in Eqn. (14) is the Jacobian of the ELL w.r.t. to the second expectation parameter; which is not guaranteed to be $p.s.d$ unless the ELL is log convex [39]. Focusing at a single location $n=(t,s)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{ELL}_{n}}{\\partial[\\mu_{k}]_{\\mathcal{Q}}}=\\frac{\\partial}{\\partial\\mathbf{S}_{u}}\\mathbb{E}_{q(\\bar{\\mathbf{u}}_{t})}\\left[\\mathbb{E}_{p(\\bar{\\mathbf{f}}_{n}|\\bar{\\mathbf{u}}_{t})}\\left[\\log p(\\mathbf{Y}_{n}\\mid\\bar{\\mathbf{f}}_{n})\\right]\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we apply the Bonnet\u2019s and Price\u2019s theorem [38] to bring the differential inside the expectation and make a Gauss-Newton [19] approximation ensuring that the Jacobian is $p.s.d$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{EL}}{\\partial[\\mu_{t}]}\\approx\\sum_{n,p}^{N}\\mathbb{E}_{q(\\tilde{\\mathbf{u}}_{t})}\\left[\\mathbf{J}_{n,p}^{\\top}\\mathbf{H}_{n,p}\\mathbf{J}_{n,p}\\right],\\;\\;\\mathrm{where}\\;\\;\\mathbf{J}_{n,p}=\\frac{\\partial g_{n}(\\mu_{n})}{\\partial\\bar{\\mathbf{u}}_{t}},\\;\\;\\mathbf{H}_{n,p}=\\frac{\\mathrm{d}^{2}\\mathrm{log}\\;p(\\mathbf{Y}_{n}\\mid g_{n})}{\\mathrm{d}^{2}g_{n}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $g_{n,p}\\;=\\;g(\\bar{\\mathbf{u}}_{n})$ (Eqn. (4)) and $\\mu_{n}$ is the mean of $p(\\bar{\\bf f}_{n}\\mid\\bar{\\bf u}_{t})$ (Eqn. (19)). When using spatial mini-batching Eqn. (21) is also subsampled. ", "page_idx": 6}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "From the optimality of natural gradients, in the conjugate setting, we exactly recover batch GP based models such as [67, 29, 4]. Our inference scheme also applies to models that do not require derivative information i.e. in $d_{t}={d_{s}}=1$ . As a special case, we recover [20], but we have extended the inference scheme to support spatial mini-batching, allowing big spatial datasets to be used. The linear weighting matrix can be used to define a linear model of coregionalisation and its variants [7, 76, 42, 65] and through appropriately designed functionals also non-linear variants [72]. ", "page_idx": 6}, {"type": "text", "text": "In \u00c1lvarez et al. [75] GP priors over the solution of differential equations are obtained through a stochastic forcing term but they only consider situations where the Greens function is available. In [22, 23, 56, 33], efficient state-space algorithms are derived but are limited to the temporal setting only. Similarly, Heinonen et al. [24], learn a \u2018free-form ODE\u2019. In the spatio-temporal setting Kr\u00e4mer et al. [35] and Duffin et al. [14] (which builds [18]) derive extended Kalman fliter algorithms. Additionally there are approaches to constraining GPs by linear differential equations [37, 1, 5]. More generally than [4] in [21] GP priors over the solutions to linear PDEs with constant coefficients are derived. ", "page_idx": 6}, {"type": "text", "text": "Beyond GP based models, physics informed neural networks (PINNs) incorporate physics by constructing a loss function between the network and the differential equation at a finite set of collocation points [48]. This amounts to a highly complex optimisation problem [36] bringing difficulties for training [69, 70] and uncertainty quantification (UQ) [16]. Current approaches to quantifying uncertainty in PINNs are based on dropout [74] and conformal predictions [47]. In recent years UQ and deep learning has received much attention however is limited by its computational cost [45]. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 1: Test performance on the simulated damped pendulum. Time is the total wall clock time in seconds. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Test performance on the magnetic field strength experiment. Results are computed w.r.t. to the first output. Time is the average epoch time ", "page_idx": 7}, {"type": "image", "img_path": "tCf7S75xFa/tmp/f9ade47df9c9c8a161a913758e1b092a55e52970bf5ec8c352e375b998f4e2ff.jpg", "img_caption": ["Figure 2: Curl free synthetic example. The left panel displays the learnt scalar potential functions by PHYSS-GP with $N_{\\mathrm{s}}=20$ , and the right panel illustrates the associated vector field. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now examine the performance of our PHYSS-GP methods on multiple synthetic and real-world datasets. We compare against a batch GP (no physical knowledge) and current state-of-art methods AUTOIP and HELMHOLTZ-GP. We provide more details on all experiments in App. B. ", "page_idx": 7}, {"type": "text", "text": "Non-linear Damped Pendulum In this first synthetic example, we consider learning the non-linear dynamics of a damped swinging pendulum. This is described by a second-order differential equation ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}^{2}\\theta}{\\mathrm{d}t^{2}}}+\\sin\\left(\\,\\theta\\,\\right)+b\\,{\\frac{\\mathrm{d}\\theta}{\\mathrm{dt}}}=0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $b>0$ . The first term is a non-linear forcing term, and the third is the damping term. With $b=0.2$ we simulate a solution using Euler\u2019s method [9] and generate 20 points in $\\bar{t}\\,\\in\\,[0,6]$ for training and 200 in $t\\in[6,30]$ for testing, with additive Gaussian noise of variance 0.01. ", "page_idx": 7}, {"type": "text", "text": "We are interested in $i,$ ) the effect of the number of collocation points, $i i$ ) the effect of the optimisation algorithm. To answer these questions, we compare against AUTOIP on [10, 100, 500, 1000] collocation points, with and without whitening. Results are tabulated in Table 1. As expected, the predictive RMSE of all models decreases as the number of collocation points increases. Due to the cubic complexity of AUTOIP, the total time significantly increases as the number of collocation points increases. For example, when using 1000 collocation points, AUTOIP is $\\approx39$ times slower than PHYSS-GP. Interestingly, the un-whitened case performs poorly, possibly due to the nonlinearity of the differential equation making optimisation difficult. This indicates that either whitening or natural gradients are required to handle the non-linearity arising due to the differential equation. ", "page_idx": 7}, {"type": "text", "text": "Curl-free Magnetic Field Strength In this experiment, we consider modelling the magnetic field strength of a dipole $\\boldsymbol{H}(\\boldsymbol{r})\\,=\\,-\\nabla\\psi(\\boldsymbol{r})$ , where $\\psi(r)\\,=\\,\\mathbf{m}{\\cdot}\\mathbf{r}\\big/|\\mathbf{r}|^{3}$ is a scalar potential function [11]. Labelling the input dimensions as \u2018time\u2019, \u2018space\u2019 and $\\mathrel{\\mathop:}z^{\\bullet}$ , we let ${\\bf m}=[0,1,0]$ and generate observations from a spatio-temporal grid with $N_{\\mathrm{t}}=50$ , and $N_{\\mathrm{s}}=[5,10,20]$ , at $z=1$ . $\\bar{H}(\\mathbf{r})$ is a curl-free field and so we compare the curl free part of HELMHOLTZ-GP against PHYSS-GP and its variants. HELMHOLTZ-GP and PHYSS-GP are equivalent models (as this is the conjugate setting, ", "page_idx": 7}, {"type": "text", "text": "Table 3: Test performance on the diffusion-reaction system. Time is the total wall clock time in seconds. PHYSS-EKS significantly outperforms all models, and due to the EKS prior only requires a 1 epoch for inference. $\\mathrm{PHYSS-SVGP_{H}}$ achieves the same performance as AUTOIP but is over twice as fast. ", "page_idx": 8}, {"type": "table", "img_path": "tCf7S75xFa/tmp/473b6822e47f8afc65f30a82cd1f8fca25ee26ed6294bcdd64fc5d376b4daacf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 3.1), and recover the same posterior and predictive distribution (up to numerical precision). However, due to the cubic-in-time complexity HELMHOLTZ-GP, at larger spatial sizes, is over 2-times slower. The hierarchical approximation is substantially faster than PHYSS-GP and performs similarly. As expected when introducing sparsity both PHYSS-SVGP and PHYSS- $S\\mathrm{VGP_{H}}$ are even faster; however, this is compensated by a slight drop in predictive performance. See Fig. 2 and Table 2. ", "page_idx": 8}, {"type": "image", "img_path": "tCf7S75xFa/tmp/fc942cd7e519ae805742f5ee07be5002ebe190f3c654e89557186752944dce1a.jpg", "img_caption": ["Figure 3: Results on the diffusion reaction system. The top row denotes the predictive mean, and the bottom the $95\\%$ confidence intervals. The white line denotes where the training data ends. Only PHYSS-EKS captures the sharp boundaries, due to the IWP kernel. $\\mathrm{PHYSS-SVGP_{H}}$ recovers a similar solution to AUTOIP but at half the computational cost. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Diffusion-Reaction System Consider a diffusion-reaction system given by an Allen-Cahn equation ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\frac{\\partial u}{\\partial t}}-0.00001\\,{\\frac{\\mathrm{d}^{2}u}{\\mathrm{d}x^{2}}}+5\\,u^{3}-5\\,u=0\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $x\\in[-1,1]$ , $t\\in[0,1]$ , $u(0,x)=x^{2}\\,\\cos(\\pi\\,x)$ , $u(t,-1)=u(t,1)$ and $\\begin{array}{r}{\\frac{\\partial u}{\\partial x}(t,-1)=\\frac{\\partial u}{\\partial x}(t,1)}\\end{array}$ Following [40], we use the solution provided by [49] and sample 256 training examples from $t\\in[0,0.\\dot{2}8]$ . We compare PHYSS-EKS (where $g$ is linerized in the EKS prior), PHYSS-SVGP and $\\mathrm{PHYSS-SVGP_{H}}$ against AUTOIP. Following [40], we use a learning rate of 0.001 for Adam. For AUTOIP, we place 100 collocation points across the whole input domain on a regular grid. For both PHYSS-SVGP, and $\\mathrm{PHYSS-SVGP_{H}}$ we require more collocation points in the temporal dimension and place them on a regular grid of size $20\\times10$ . For PHYSS-EKS we use an integrated Wiener kernel (IWP) on time [55] and place $100\\times40$ collocation points. We are unable to place more collocation for AUTOIP due to computational limits. Results are presented in Fig. 3 and Table 3. PHYSS-EKS requires only a single epoch and can better handle the sharp boundaries. Our method $\\mathrm{PHYSS-SVGP_{H}}$ is over twice as fast as AUTOIP whilst achieving similar predictive RMSE. ", "page_idx": 8}, {"type": "text", "text": "Ocean Currents We now model oceanic currents in the Gulf of Mexico. We follow [4] and use the dataset provided by D\u2019Asaro et al. [15] that has information from over 1, 000 buoys. We focus on the region in long. $[-90,-84.5]$ , lat. [26, 30] on 2016-02-25, by computing hourly averages. This results in $N=42$ , 243 observations, and we construct a test-train split on 0.1 per cent of the data. It is infeasible to run HELMHOLTZ-GP due to data size (in Berlinghieri et al. [4], observations from only 19 buoys are used with $N=55$ ). However, we run $\\mathrm{PHYSS-SVGP_{H}}$ with 50 spatial inducing points and a spatial mini-batch size of 10, and plot results in Fig. 4. Our predictions are in excellent agreement with the test data, achieving an RMSE of 0.14, NLPD of $-0.52$ , CRPS of 0.078, and an average run-time of $1.86(s)$ per epoch. ", "page_idx": 8}, {"type": "image", "img_path": "tCf7S75xFa/tmp/37af9472bb8e0e3cc65063811afb221e3dc562a9f5d98bf64b9a1e6d606705aa.jpg", "img_caption": ["Figure 4: Predicted ocean currents by $\\mathrm{PHYSS-SVGP_{H}}$ . True observations are in grey, and predictions in green. The thickness of the line represents uncertainty and is computed by the L2 norm of the standard deviations across both outputs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a physics-informed state-space GP that integrates observational data with physical knowledge. Within the variational inference framework, we derived a computationally efficient algorithm that uses Kalman smoothing to achieve linear-in-time costs. To gain further computational speed-ups, we proposed three approximations with inducing points, spatial mini batching and structured variational posteriors. When used in conjunction, they allow us to handle large-scale spatiotemporal problems. The bottleneck is always the state size, where nearest neighbours GPs [13, 73] could be explored. For highly non-linear problems, future directions could explore deep approaches [52] or more flexible kernel families [68]. One limitation is the use of the collocation method which is only enforcing the differential equation point wise, whilst future work could look at the more general methods of weighted residuals [46]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "OH acknowledges funding from The Alan Turing Institute PhD fellowship programme and the UKRI Turing AI Fellowship (EP/V02678X/1). AS acknowledges support from the Research Council of Finland (339730). TD acknowledges support from UKRI Turing AI Acceleration Fellowship (EP/V02678X/1) and a Turing Impact Award from the Alan Turing Institute. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC-BY) license to any Author Accepted Manuscript version arising from this submission. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] C. G. Albert. Gaussian processes for data fulfliling linear differential equations. In Proceedings, volume 33, page 5. MDPI, 2019.   \n[2] S.-i. Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251\u2013276, 1998.   \n[3] G. B. Arfken, H. J. Weber, and F. E. Harris. Mathematical Methods for Physicists: A Comprehensive Guide. Academic Press, 2011.   \n[4] R. Berlinghieri, B. L. Trippe, D. R. Burt, R. Giordano, K. Srinivasan, T. \u00d6zg\u00f6kmen, J. Xia, and T. Broderick. Gaussian processes at the Helm (holtz): A more fluid model for ocean currents. arXiv preprint arXiv:2302.10364, 2023.   \n[5] A. Besginow and M. Lange-Hegermann. Constraining Gaussian processes to systems of linear ordinary differential equations. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 29386\u201329399. Curran Associates, Inc., 2022.   \n[6] F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political Economy, 81(3):637\u2013654, 1973.   \n[7] E. V. Bonilla, K. Chai, and C. Williams. Multi-task Gaussian process prediction. In Advances in Neural Information Processing Systems 20 (NIPS). Curran Associates, Inc., 2008.   \n[8] D. Borthwick. Introduction to Partial Differential Equations. Universitext. Springer International Publishing, 2017.   \n[9] J. Butcher. Numerical Methods for Ordinary Differential Equations. Wiley, 2016.   \n[10] P. E. Chang, W. J. Wilkinson, M. E. Khan, and A. Solin. Fast variational learning in state-space Gaussian process models. In 30th IEEE International Workshop on Machine Learning for Signal Processing (MLSP), pages 1\u20136. IEEE, 2020.   \n[11] T. Chow. Introduction to Electromagnetic Theory: A Modern Perspective. Jones and Bartlett Publishers, 2006.   \n[12] J. Cockayne, C. Oates, T. Sullivan, and M. Girolami. Probabilistic numerical methods for PDE-constrained Bayesian inverse problems. AIP Conference Proceedings, 1853(1), 06 2017.   \n[13] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets. Journal of the American Statistical Association, 111(514):800\u2013812, 2016.   \n[14] C. Duffin, E. Cripps, T. Stemler, and M. Girolami. Low-rank statistical finite elements for scalable model-data synthesis. Journal of Computational Physics, 463:111261, 2022.   \n[15] E. D\u2019Asaro, C. Guigand, A. Haza, H. Huntley, G. Novelli, T. \u00d6zg\u00f6kmen, and E. Ryan. Lagrangian submesoscale experiment (LASER) surface drifters, interpolated to 15-minute intervals, 2017. URL https://data.gulfresearchinitiative.org/data/R4.x265.237:0001.   \n[16] C. Edwards. Neural networks learn to speed up simulations. Communications of the ACM, 65: 27\u201329, 04 2022.   \n[17] D. Eriksson, K. Dong, E. Lee, D. Bindel, and A. G. Wilson. Scaling Gaussian process regression with derivatives. In Advances in Neural Information Processing Systems 31 (NeurIPS). Curran Associates, Inc., 2018.   \n[18] M. Girolami, E. Febrianto, G. Yin, and F. Cirak. The statistical finite element method (statFEM) for coherent synthesis of observation data and model predictions. Computer Methods in Applied Mechanics and Engineering, 375:113533, 2021.   \n[19] G. H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate. SIAM Journal on Numerical Analysis, 10(2):413\u2013432, 1973.   \n[20] O. Hamelijnck, W. J. Wilkinson, N. A. Loppi, A. Solin, and T. Damoulas. Spatio-temporal variational Gaussian processes. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc., 2021.   \n[21] M. Harkonen, M. Lange-Hegermann, and B. Raita. Gaussian process priors for systems of linear partial differential equations with constant coefficients. In International Conference on Machine Learning, pages 12587\u201312615. PMLR, 2023.   \n[22] J. Hartikainen and S. S\u00e4rkk\u00e4. Sequential inference for latent force models. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI\u201911, page 311\u2013318, Arlington, Virginia, USA, 2011. AUAI Press.   \n[23] J. Hartikainen, M. Sepp\u00e4nen, and S. S\u00e4rkk\u00e4. State-space inference for non-linear latent force models with application to satellite orbit prediction. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML\u201912, page 723\u2013730. Omnipress, 2012.   \n[24] M. Heinonen, \u00c7agatay Yildiz, H. Mannerstr\u00f6m, J. Intosalmi, and H. L\u00e4hdesm\u00e4ki. Learning unknown ODE models with Gaussian processes. In International Conference on Machine Learning (ICML), 2018.   \n[25] P. Hennig, M. Osborne, and H. Kersting. Probabilistic Numerics: Computation as Machine Learning. Cambridge University Press, 2022.   \n[26] J. Hensman, M. Rattray, and N. D. Lawrence. Fast variational inference in the conjugate exponential family. In Advances in Neural Information Processing Systems (NIPS), pages 2888\u20132896. Curran Associates Inc., 2012.   \n[27] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI), pages 282\u2013290. AUAI Press, 2013.   \n[28] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14, 2013.   \n[29] C. Jidling, N. Wahlstr\u00f6m, A. Wills, and T. B. Sch\u00f6n. Linearly constrained Gaussian processes. In Advances in Neural Information Processing Systems 30 (NeurIPS). Curran Associates, Inc., 2017.   \n[30] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physicsinformed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021.   \n[31] M. E. Khan and W. Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.   \n[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXv preprint arXiv:1412.6980, 2014.   \n[33] N. Kr\u00e4mer and P. Hennig. Linear-time probabilistic solution of boundary value problems. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 11160\u201311171. Curran Associates, Inc., 2021.   \n[34] N. Kr\u00e4mer and P. Hennig. Stable implementation of probabilistic ODE solvers. Journal of Machine Learning Research, 25(111):1\u201329, 2024.   \n[35] N. Kr\u00e4mer, J. Schmidt, and P. Hennig. Probabilistic numerical method of lines for timedependent partial differential equations. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 625\u2013639. PMLR, 2022.   \n[36] A. S. Krishnapriyan, A. Gholami, S. Zhe, R. Kirby, and M. W. Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems 34 (NeurIPS), 2021.   \n[37] M. Lange-Hegermann. Algorithmic linearly constrained Gaussian processes. In Advances in Neural Information Processing Systems 31 (NeurIPS), pages 2137\u20132148. Curran Associates, Inc., 2018.   \n[38] W. Lin, M. E. Khan, and M. Schmidt. Stein\u2019s lemma for the reparameterization trick with exponential family mixtures. arXiv preprint arXiv:1910.13398, 2019.   \n[39] W. Lin, M. Schmidt, and M. E. Khan. Handling the positive-definite constraint in the Bayesian learning rule. In Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2020.   \n[40] D. Long, Z. Wang, A. Krishnapriyan, R. Kirby, S. Zhe, and M. Mahoney. AutoIP: A united framework to integrate physics into Gaussian processes. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research. PMLR, 17\u201323 Jul 2022.   \n[41] A. Mchutchon and C. Rasmussen. Gaussian process training with input noise. In Advances in Neural Information Processing Systems 25 (NeurIPS). Curran Associates, Inc., 2011.   \n[42] P. Moreno-Mu\u00f1oz, A. Art\u00e9s, and M. \u00c1lvarez. Heterogeneous multi-output Gaussian process prediction. In Advances in Neural Information Processing Systems 31 (NeurIPS). Curran Associates, Inc., 2018.   \n[43] M. Padidar, X. Zhu, L. Huang, J. R. Gardner, and D. S. Bindel. Scaling Gaussian processes with derivative information using variational inference. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc., 2021.   \n[44] I. Pan, L. R. Mason, and O. K. Matar. Data-centric engineering: integrating simulation, machine learning and statistics. challenges and opportunities. Chemical Engineering Science, 249: 117271, 2022.   \n[45] T. Papamarkou, M. Skoularidou, K. Palla, L. Aitchison, J. Arbel, D. Dunson, M. Filippone, V. Fortuin, P. Hennig, J. M. Hern\u00e1ndez-Lobato, A. Hubin, A. Immer, T. Karaletsos, M. E. Khan, A. Kristiadi, Y. Li, S. Mandt, C. Nemeth, M. A. Osborne, T. G. J. Rudner, D. R\u00fcgamer, Y. W. Teh, M. Welling, A. G. Wilson, and R. Zhang. Position: Bayesian deep learning is needed in the age of large-scale AI. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 39556\u201339586. PMLR, 2024.   \n[46] M. Pf\u00f6rtner, I. Steinwart, P. Hennig, and J. Wenger. Physics-informed Gaussian process regression generalizes linear PDE solvers. arXiv preprint arXiv:2212.12474, 2022.   \n[47] L. Podina, M. T. Rad, and M. Kohandel. Conformalized physics-informed neural networks. In ICLR 2024 Workshop on AI4Differential Equations in Science, 2024. URL https:// openreview.net/forum?id $\\equiv$ ZoFWS818qG.   \n[48] M. Raissi and G. E. Karniadakis. Hidden physics models: Machine learning of nonlinear partial differential equations. Journal of Computational Physics, 2017.   \n[49] M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019.   \n[50] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, MA, USA, 2006.   \n[51] J. Riihim\u00e4ki and A. Vehtari. Gaussian processes with monotonicity information. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), volume 9 of Proceedings of Machine Learning Research. PMLR, 2010.   \n[52] H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In Advances in Neural Information Processing Systems 30 (NeurIPS), pages 4588\u2013 4599. Curran Associates, Inc., 2017.   \n[53] H. Salimbeni, S. Eleftheriadis, and J. Hensman. Natural gradients in practice: Non-conjugate variational inference in gaussian process models. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.   \n[54] S. S\u00e4rkk\u00e4. Linear operators and stochastic partial differential equations in gaussian process regression. In Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II, ICANN\u201911, page 151\u2013158. Springer-Verlag, 2011.   \n[55] S. S\u00e4rkk\u00e4 and A. Solin. Applied Stochastic Differential Equations. Cambridge University Press, 2019.   \n[56] J. Schmidt, N. Kr\u00e4mer, and P. Hennig. A probabilistic state space model for joint inference from differential equations and data. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 12374\u201312385. Curran Associates, Inc., 2021.   \n[57] M. Schober, D. K. Duvenaud, and P. Hennig. Probabilistic ODE solvers with Runge-Kutta means. In Advances in Neural Information Processing Systems 27 (NeurIPS). Curran Associates, Inc., 2014.   \n[58] E. Solak, R. Murray-smith, W. Leithead, D. Leith, and C. Rasmussen. Derivative observations in Gaussian process models of dynamic systems. In Advances in Neural Information Processing Systems 15 (NIPS). MIT Press, 2002.   \n[59] A. Solin. Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression. Doctoral thesis, Aalto University, 2016.   \n[60] A. Solin, M. Kok, N. Wahlstr\u00f6m, T. B. Sch\u00f6n, and S. S\u00e4rkk\u00e4. Modeling and interpolation of the ambient magnetic field by Gaussian processes. Transactions on Robotics, 34(4):1112\u20131127, 2018.   \n[61] T. Stocker. Introduction to Climate Modelling. Springer Science & Business Media, 2011.   \n[62] S. S\u00e4rkk\u00e4. Bayesian Filtering and Smoothing. Institute of Mathematical Statistics Textbooks. Cambridge University Press, 2013.   \n[63] M.-N. Tran, D. H. Nguyen, and D. Nguyen. Variational Bayes on manifolds. Statistics and Computing, 31:1\u201317, 2021.   \n[64] F. Tronarp, H. Kersting, S. S\u00e4rkk\u00e4, and P. Hennig. Probabilistic solutions to ordinary differential equations as nonlinear Bayesian filtering: a new perspective. Statistics and Computing, 29: 1297\u20131315, 2018.   \n[65] J. Vanhatalo, M. Hartmann, and L. Veneranta. Additive Multivariate Gaussian Processes for Joint Species Distribution Modeling with Heterogeneous Data. Bayesian Analysis, 15:415 \u2013 447, 2020.   \n[66] C. Villacampa-Calvo, B. Zald\u00edvar, E. C. Garrido-Merch\u00e1n, and D. Hern\u00e1ndez-Lobato. Multiclass Gaussian process classification with noisy inputs. Journal of Machine Learning Research, 22(1), 2021.   \n[67] N. Wahlstr\u00f6m, M. Kok, T. B. Sch\u00f6n, and F. Gustafsson. Modeling magnetic fields using Gaussian processes. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3522\u20133526, 2013.   \n[68] K. Wang, O. Hamelijnck, T. Damoulas, and M. Steel. Non-separable non-stationary random fields. In Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 9887\u20139897. PMLR, 2020.   \n[69] S. Wang, X. Yu, and P. Perdikaris. When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022.   \n[70] S. Wang, S. Sankaran, and P. Perdikaris. Respecting causality for training physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 421:116813, 2024.   \n[71] W. J. Wilkinson, S. S\u00e4rkk\u00e4, and A. Solin. Bayes-Newton methods for approximate Bayesian inference with PSD guarantees. Journal of Machine Learning Research, 24(83):1\u201350, 2023.   \n[72] A. G. Wilson, D. A. Knowles, and Z. Ghahramani. Gaussian process regression networks. In Proceedings of the 29th International Coference on International Conference on Machine Learning, page 1139\u20131146, 2012.   \n[73] L. Wu, G. Pleiss, and J. P. Cunningham. Variational nearest neighbor Gaussian process. In Proceedings of the 39th International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research, pages 24114\u201324130. PMLR, 2022.   \n[74] D. Zhang, L. Lu, L. Guo, and G. E. Karniadakis. Quantifying total uncertainty in physicsinformed neural networks for solving forward and inverse stochastic problems. Journal of Computational Physics, 397:108850, 2019.   \n[75] M. \u00c1lvarez, D. Luengo, and N. D. Lawrence. Latent force models. In Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics (AISTATS), volume 5 of Proceedings of Machine Learning Research, pages 9\u201316. PMLR, 2009.   \n[76] M. A. \u00c1lvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning, 4(3):195\u2013266, 2012. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Variational Approximation Derivation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Overview of Notation ", "page_idx": 15}, {"type": "table", "img_path": "tCf7S75xFa/tmp/9bcc97f2652b92e93ce7fa1360e15d2a88ac7687bcb7fce397b5c5f5d139465b.jpg", "table_caption": ["Table 4: Table of Notation "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Layout of Vectors and Matrices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use a numerator layout for derivatives. Let $Q$ denote the number of independent latent functions and $D$ the number of derivatives computed, and let $f_{q,d}$ denote the latent GP for $d^{\\ast}$ th derivative of the $q$ \u2019th latent function. We will need to keep track of the permutation of our data w.r.t. to space, time, and latent functions. Inspired by \u2018row-major\u2019 and \u2018column-major\u2019 layouts, we will use the following terminology that describes the ordering of the data across latent functions and time and space: ", "page_idx": 15}, {"type": "text", "text": "\u2022 latent-data: $\\mathbf{F}\\,=\\,\\mathbf{F}_{\\mathrm{ld}}\\,=\\,[\\mathbf{F}_{1}(\\mathbf{X}),\\cdots,\\mathbf{F}_{Q}(\\mathbf{X})]$ with $\\mathbf{F}_{q}(\\mathbf{X})\\,=\\,[\\mathbf{f}_{q,1}(\\mathbf{X}),\\cdots,\\mathbf{f}_{q,D}(\\mathbf{X})]$ which is ordered by stacking each of the latent functions on top of each other. \u2022 data-latent: $\\mathbf{F}_{\\mathrm{dl}}\\;=\\;[\\mathbf{F}_{1}(\\mathbf{X}_{n}),\\cdots\\,,\\mathbf{F}_{Q}(\\mathbf{X}_{n})]_{n}^{N}$ which is ordered by stacking the latent functions evaluated at each data point across all data points. \u2022 time-space: $[\\mathbf{f}(\\mathbf{X}_{\\mathrm{t}}^{\\mathrm{(st)}})]_{\\mathrm{t}}^{N_{\\mathrm{t}}}$ which is ordered by stacking each of the input points at each time point on top of each other. This is only applicable when there is a single latent function. \u2022 latent-time-space: $\\big[\\mathbf{F}_{1}\\big(\\mathbf{X}_{1}^{\\mathrm{(st)}}\\big),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{F}_{1}\\big(\\mathbf{X}_{N_{\\mathrm{t}}}^{\\mathrm{(st)}}\\big),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{F}_{Q}\\big(\\mathbf{X}_{1}^{\\mathrm{(st)}}\\big),\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{F}_{Q}\\big(\\mathbf{X}_{N_{\\mathrm{t}}}^{\\mathrm{(st)}}\\big)\\big]$ ", "page_idx": 15}, {"type": "text", "text": "\u2022 time-latent-space: $[\\mathbf{F}_{1}(\\mathbf{X}_{\\mathrm{t}}^{(\\mathrm{st})}),\\cdot\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{F}_{Q}(\\mathbf{X}_{\\mathrm{t}}^{(\\mathrm{st})})]_{\\mathrm{t}}^{N_{\\mathrm{t}}}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The default order will be latent-data (and latent-time-space for spatio-temporal problems). Since all of these are just simple permutations of each other, there exists a permutation matrix that permutes between any two of the layouts above. We use the function $\\pi$ to denote a function that performs this permutation such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{F}_{\\mathrm{dl}}=\\pi_{\\mathrm{ld}\\rightarrow\\mathrm{dl}}(\\mathbf{F}_{\\mathrm{ld}})}\\\\ {\\mathbf{F}_{\\mathrm{ld}}=\\pi_{\\mathrm{dl}\\rightarrow\\mathrm{ld}}(\\mathbf{F}_{\\mathrm{dl}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Timeseries Setting - Single Latent Function ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let $\\mathbf{X}\\in\\mathbb{R}^{N\\times D}$ , $\\mathbf{Y}\\in\\mathbb{R}^{N\\times P}$ be input-output observations across $P$ outputs, where $N=N_{\\mathrm{t}}$ . For now, we only consider the case where $Q=1$ . We assume that $f$ has a state-space representation, and we denote its state with its D time derivatives as F(X) = [f(X), \u2202f\u2202(XX ), \u22022\u2202fX(2X ), \u00b7 \u00b7 \u00b7 ] in latent-data format. The vector ${\\bf F}({\\bf X})$ is of dimension $(N\\times D)$ . We also use the notation $\\mathbf{F}_{n}=\\mathbf{\\dot{F}}(\\mathbf{X}_{n})$ , which is a $D$ -dimensional vector of the derivatives at location $\\mathbf{X}_{n}$ . The joint model is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left[\\,\\prod_{n}^{N}p(\\mathbf{Y}_{n}\\,|\\,\\mathbf{F}_{n},\\mathbf{DE})\\,\\right]p(\\mathbf{F}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "At this point, we place no particular restriction on the form of the likelihood, aside from decomposing across $N$ . The prior $p(\\mathbf{F})$ is a multivariate GP of dimension $N\\times D$ ", "page_idx": 16}, {"type": "equation", "text": "$$\np(\\mathbf{F})=\\mathrm{N}\\left(\\,\\mathbf{F}\\,\\mid\\,\\mathbf{0},\\,{\\mathcal{D}}\\,\\mathbf{K}(\\mathbf{x},\\mathbf{x})\\,{\\mathcal{D}}^{*}\\,\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $q(\\mathbf{F})$ be a free-form multivariate Gaussian of the same dimension as $p(\\mathbf{F})$ then the corresponding ELBO is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{L}=\\mathbb{E}_{\\,q({\\bf F})}\\left[\\log\\frac{p({\\bf Y}\\mid{\\bf F},\\mathrm{DE})\\,p({\\bf F})}{q({\\bf F})}\\right],}}\\\\ {\\displaystyle{\\quad=\\mathbb{E}_{\\,q({\\bf F})}\\left[\\log p({\\bf Y}\\mid{\\bf F},\\mathrm{DE})\\right]-{\\mathcal{K L}}\\left[\\,q({\\bf F})\\,||\\,p({\\bf F})\\,\\right],}}\\\\ {\\displaystyle{\\quad=\\sum_{n}^{N}\\mathbb{E}_{\\,q({\\bf F}_{n})}\\left[\\log p({\\bf Y}_{n}\\mid{\\bf F}_{n},\\mathrm{DE})\\right]-{\\mathcal{K L}}\\left[\\,q({\\bf F})\\,||\\,p({\\bf F})\\,\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the marginal $q(\\mathbf{F}_{n})$ is a $D$ -dimensional Gaussian corresponding to the $n$ \u2019th observation. The natural gradients are ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widetilde{\\lambda}\\leftarrow\\left(1-\\beta\\right)\\widetilde{\\lambda}+\\beta\\,\\frac{\\partial\\mathrm{ELL}}{\\partial\\mu}\\biggr\\}\\,\\,\\,\\mathrm{Surrogate~likelihood~update}}\\\\ {\\lambda\\leftarrow\\widetilde{\\lambda}+\\eta\\qquad\\qquad\\qquad\\biggr\\}\\,\\,\\mathrm{Surrogate~model~update}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\boldsymbol{\\widetilde{\\lambda}}=\\left[|\\widetilde{\\boldsymbol{\\lambda}}|_{1},[\\widetilde{\\boldsymbol{\\lambda}}]_{2}\\right]^{\\top}$ and $[\\widetilde{\\lambda}]_{1}$ is an $(N\\times D)$ vector and $[\\widetilde{\\lambda}]_{2}$ an $(N\\times D)\\times(N\\times D)$ matrix. Eqn. (29) is a sum of natural parameters, and so is the conjugate Bayesian update. Naively computing this would yield no computation speed up as the computation cost would be cubic $\\mathcal{O}(N^{\\frac{5}{3}})$ . However, the natural parameters of the likelihood $(\\widetilde{\\lambda})$ are guaranteed to be block diagonal, one block per data point (if $\\widetilde{\\lambda}_{0}$ is initialised as so). This immediately implies that Eqn. (29) can be computed using efficient Kalman filter and smoothing algorithms. The structure of $\\widetilde{\\lambda}$ depends on the gradient of the expected log-likelihood $\\frac{\\partial\\mathrm{ELL}}{\\partial\\pmb{\\mu}}$ . Expanding this out ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{ELL}}{\\partial[\\mu]_{\\mathcal{Q}}}=\\sum_{n}^{N}\\frac{\\partial\\mathbb{E}_{\\mathbf{\\phi}_{q}(\\mathbf{F}_{n})}\\left[\\log p(\\mathbf{Y}_{n}\\,|\\,\\mathbf{F}_{n},\\mathbf{DE})\\right]}{\\partial[\\mu]_{\\mathcal{Q}}}=\\sum_{n}^{N}\\Tilde{\\mu}_{n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where each component ${\\widetilde{\\pmb{\\mu}}}_{n}$ is a $(N\\times D)\\times(N\\times D)$ matrix that only has $D\\times D$ non-zero entries; as these are the only elements that directly affect ${\\mathbf{F}}_{n}$ . Collecting all these submatrices into a block diagonal matrix, we have a matrix in data-latent format, however, \u2202\u2202E\u00b5LL is in latent-data, and so all we need to do is permute by $\\mathbf{P}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{ELL}}{\\partial\\mu}=\\pi_{\\mathrm{dl}\\to\\mathrm{ld}}\\left(\\,\\mathfrak{b l k d i a g}[\\,\\widetilde{\\mu}_{1},\\cdot\\cdot\\cdot\\,,\\widetilde{\\mu}_{N}\\,]\\,\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Converting from natural to moment paramerisation the surrogate update is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{q({\\bf F})\\propto\\mathrm{N}\\left(\\,\\widetilde{{\\bf Y}}\\mid{\\bf F},\\,\\widetilde{{\\bf V}}\\,\\right)\\,p({\\bf F})}\\ ~}}\\\\ {{\\displaystyle~~~~~~~~=\\left[\\,\\prod_{n}^{N}\\,\\mathrm{N}\\left(\\,\\widetilde{{\\bf Y}}_{n}\\mid{\\bf F}_{n},\\,\\widetilde{{\\bf V}}_{n}\\,\\right)\\,\\right]\\,p({\\bf F})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widetilde{\\mathbf{Y}}_{n}$ is a $D$ -dimensional vector, and $\\widetilde{{\\mathbf V}}_{n}$ is a $D\\times D$ matrix, and efficient Kalman flitering and smoot hing algorithms can be used to com pute the surrogate model update. Substituting $q(\\mathbf{F})$ back into the ELBO it further simplifies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathbb{E}_{q(\\mathbf{F})}\\left[\\log\\frac{p(\\mathbf{Y}\\mid\\mathbf{F},\\mathrm{DE})\\,p(\\mathbf{F}^{\\top}p(\\widetilde{\\mathbf{Y}}\\mid\\widetilde{\\mathbf{V}})}{\\mathbf{N}\\left(\\widetilde{\\mathbf{Y}}\\mid\\mathbf{F},\\widetilde{\\mathbf{V}}\\right)\\,p(\\mathbf{F}^{\\top})}\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log p(\\mathbf{Y}_{n}\\mid\\mathbf{F}_{n},\\mathrm{DE})\\right]-\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log\\mathrm{N}\\left(\\,\\widetilde{\\mathbf{Y}}_{n}\\mid\\mathbf{F}_{n},\\widetilde{\\mathbf{V}}_{n}\\right)\\right]+\\log p(\\widetilde{\\mathbf{Y}}\\mid\\widetilde{\\mathbf{V}})}\\\\ &{\\quad=\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log p(\\mathbf{Y}_{n}\\mid\\mathbf{F}_{n},\\mathrm{DE})\\right]-\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log\\mathrm{N}\\left(\\,\\widetilde{\\mathbf{Y}}_{n}\\mid\\mathbf{F}_{n},\\widetilde{\\mathbf{V}}_{n}\\right)\\right]+\\log p(\\widetilde{\\mathbf{Y}}\\mid\\widetilde{\\mathbf{V}})}\\\\ &{\\quad=\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log p(\\mathbf{Y}_{n}\\mid\\mathbf{F}_{n},\\mathrm{DE})\\right]-\\displaystyle\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log\\mathrm{N}\\left(\\,\\widetilde{\\mathbf{Y}}_{n}\\mid\\mathbf{F}_{n},\\widetilde{\\mathbf{V}}_{n}\\right)\\right]+\\log p(\\widetilde{\\mathbf{Y}}\\mid\\widetilde{\\mathbf{V}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "each term can be computed efficiently as the by-product of the Kalman filtering and smoothing algorithm used to compute $q(\\mathbf{F})$ . ", "page_idx": 17}, {"type": "text", "text": "A.4 Timeseries Setting - Multiple Latent Functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now generalise the previous section to handle multiple independent latent functions, i.e. $\\mathrm{Q}>0$ . The model prior now has the form ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\mathbf{F})=\\prod_{q}^{\\mathrm{~Q~}}p(\\mathbf{F}_{q})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p(\\mathbf{F}_{q})$ is a prior over $\\mathbf{f}_{q,1}$ and its $D$ partial deriatives. We consider two approaches: a mean-field approximate posterior and a full Gaussian. ", "page_idx": 17}, {"type": "text", "text": "The first approach defined mean-field approximate posterior $\\begin{array}{r}{q(\\mathbf{F})\\triangleq\\prod_{q}^{\\mathrm{Q}}q(\\mathbf{F}_{q})}\\end{array}$ where each $q(\\mathbf{F}_{q})$ is a free-form Gaussian of dimension $(N\\times D)$ . The natural gradient updates are now simply applied to each component $q(\\mathbf{F}_{q})$ separately, and we essentially follow the update set out in App. A.3. ", "page_idx": 17}, {"type": "text", "text": "The second approach is a full-Gaussian approximate posterior where $q(\\mathbf{F})$ is a $(\\mathrm{Q}\\,\\times\\,D\\,\\times\\,N)$ - dimensional free-form Gaussian. In this case the ELL is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{ELL}=\\sum_{n}^{N}\\mathbb{E}_{q(\\mathbf{F}_{n})}\\left[\\log p(\\mathbf{Y}_{n}\\,|\\,\\mathbf{F}_{n},\\mathbf{D}\\mathbf{E})\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $q(\\mathbf{F}_{n})$ is of dimension $(\\mathrm{Q}\\times D)$ . This implies that the gradient of the ELL $\\begin{array}{r}{\\frac{\\partial\\mathrm{ELL}}{\\partial[\\pmb{\\mu}]_{2}}=\\sum_{n}^{N}\\widetilde{\\pmb{\\mu}}_{n}}\\end{array}$ where ${\\widetilde{\\pmb{\\mu}}}_{n}$ now has $(\\mathrm{Q}\\times D)\\times(\\mathrm{Q}\\times D)$ non-zero entries. Switching to moment parameterisatio n ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathbf{F})\\propto\\left[\\prod_{n}^{N}\\,\\mathrm{N}\\left(\\,\\widetilde{\\mathbf{Y}}_{n}\\mid\\mathbf{F}_{n},\\,\\widetilde{\\mathbf{V}}_{n}\\,\\right)\\,\\right]p(\\mathbf{F})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widetilde{\\mathbf{Y}}_{n}$ is of dimension $\\left(Q\\times D\\right)$ and $\\widetilde{{\\mathbf V}}_{n}$ is $(\\mathrm{Q}\\times D)\\times(\\mathrm{Q}\\times D)$ . We can still use state-space algorit hms by simply stacking the states c orresponding to each $\\mathbf{F}_{q}$ [55]. ", "page_idx": 17}, {"type": "text", "text": "A.5 Spatio-temporal Data - Single Latent Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now turn to the spatio-temporal setting where $\\mathbf{X},\\mathbf{Y}$ are spatio-temporal observations on a spatiotemporal grid ordered in time-space format. We now derive the conjugate variational algorithm for PHYSS-SVGP and PHYSS- $\\mathrm{SVGP_{H}}$ . The algorithms for PHYSS-GP and PHYSS- $\\mathrm{VGP_{H}}$ are recovered as special cases when $\\mathbf{Z}=\\mathbf{X}_{\\mathrm{s}}$ . ", "page_idx": 17}, {"type": "text", "text": "A.5.1 Spatial Derivative Inducing Points ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We follow the standard sparse variational GP procedure and augment that prior with inducing points $\\mathbf{U}=\\mathcal{D}\\mathbf{u}$ at locations $\\mathbf{Z}$ . We require that the inducing points are defined on a spatial-temporal grid at the same temporal points as the data $\\mathbf{X}$ , such that $\\mathbf{Z}=[\\mathbf{Z}_{t}]_{t}^{N_{\\mathrm{t}}}$ . This is required to ensure Kronecker structure between the inducing points and the data. The joint model is ", "page_idx": 18}, {"type": "equation", "text": "$$\np(\\mathbf{Y}\\,|\\,\\mathbf{F})\\,p(\\mathbf{F}\\,|\\,\\mathbf{U})\\,p(\\mathbf{U})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p({\\bf U})=\\mathrm{N}\\left(~{\\bf U}~|~{\\bf0},~{\\bf K}_{t}^{\\mathcal{D}}({\\bf X_{t}},{\\bf X_{t}})\\otimes{\\bf K}_{s}^{\\mathcal{D}}({\\bf Z_{s}},{\\bf Z_{s}})~\\right),}\\\\ {p({\\bf F}\\mid{\\bf U})=\\mathrm{N}\\left(~{\\bf F}~|~{\\boldsymbol\\mu_{{\\bf F}\\mid~{\\bf U}}},~\\Sigma_{{\\bf F}\\mid~{\\bf U}}~\\right)~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the conditional mean and covariance are given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathbf{F}}_{|\\mathbf{\\epsilon}|}=\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{t}},\\mathbf{X}_{\\mathbf{t}})\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\right]\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{t}},\\mathbf{X}_{\\mathbf{t}})\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\right]^{-1}\\mathbf{U}}\\\\ &{\\quad\\quad\\quad=\\left[\\mathbf{I}\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\,(\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}}))^{-1}\\right]\\mathbf{U},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\mathbf{F}\\,|\\mathbf{\\Gamma}}\\mathbf{\\Gamma}\\mathbf{\\Gamma}=\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{\\mathrm{t}},\\mathbf{X}_{\\mathrm{t}})\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathrm{s}},\\mathbf{X}_{\\mathrm{s}})\\right]}\\\\ &{\\qquad\\qquad-\\left[\\mathbf{K}_{\\mathbf{X},\\mathbf{Z}}^{\\otimes}\\right]\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{\\mathrm{t}},\\mathbf{X}_{\\mathrm{t}})\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}})\\right]^{-1}\\left[\\mathbf{K}_{\\mathbf{X},\\mathbf{Z}}^{\\otimes}\\right]^{\\intercal}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{K}_{\\mathbf{X},\\mathbf{Z}}^{\\otimes}=\\left[\\mathbf{K}_{t}^{D}(\\mathbf{X}_{\\mathrm{t}},\\mathbf{X}_{\\mathrm{t}})\\otimes\\mathbf{K}_{s}^{D}(\\mathbf{X}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}})\\right]$ which simplifies to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{U}}=\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{t}},\\mathbf{X}_{\\mathbf{t}})\\otimes\\left[\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{X}_{\\mathbf{s}})-\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\,\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})^{-1}\\,\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{X}_{\\mathbf{s}})\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Due to the Kronecker structure, the marginal at time $t$ only depends on the inducing points in that time slice so we can still get a CVI-style update that can be computed using a state-space model. To see why we again look at the Jacobian of the ELL: \u2202\u2202[E\u00b5L]L2 = $\\begin{array}{r}{\\frac{\\partial\\mathrm{ELL}}{\\partial[\\pmb{\\mu}]_{\\mathcal{Z}}}=\\dot{\\sum}_{n}^{N}\\,\\widetilde{\\pmb{\\mu}}_{n}}\\end{array}$ n\u00b5n where\u00b5n now has $(D\\times M)\\times(D\\times M)$ non-zero entries, which corresponding to needed all $M$ spatial inducing points with there derivatives to predict at a single time point. This is similar to the time series setting, except we have now predicted in space to compute marginals of $q(\\mathbf{F})$ . To be complete, we write that the marginal $q(\\mathbf{U})$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{U})\\propto\\left[\\prod_{t}^{N_{\\mathrm{t}}}\\mathrm{N}\\left(\\widetilde{\\mathbf{Y}}_{t}\\mid\\mathbf{F}_{t},\\,\\widetilde{\\mathbf{V}}_{t}\\right)\\right]p(\\mathbf{F})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\widetilde{\\mathbf{Y}}_{t}$ and $\\mathbf{F}_{t}$ are vectors of dimension $(D\\times M)$ , and $\\widetilde{\\mathbf{V}}_{t}$ is a matrix of dimension $(D\\times$ $M)\\times(D\\times M)$ . The marginals $q(\\mathbf{U}_{t})$ , and the corresponding marginal likelihood $p(\\widetilde{\\mathbf{Y}}\\,|\\,\\widetilde{\\mathbf{V}})$ can be computed by running a Kalman filter and smoother in $\\mathcal{O}(N_{\\mathrm{t}}\\cdot(M_{s}\\cdot d_{s}\\cdot d)^{3})$ . The marginal $q(\\mathbf{F})=\\mathbf{\\bar{N}}\\left(\\mathbf{F}\\mid\\bar{\\mu}_{\\mathbf{F}},\\,\\Sigma_{\\mathbf{F}}\\right)$ where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{\\mathbf{F}}=\\left[\\mathbf{I}\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}})\\,(\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathrm{s}},\\mathbf{Z}_{\\mathrm{s}}))^{-1}\\,\\right]\\mathbf{m}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\mathbf{F}}=\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{U}}\\!+\\!\\left[\\mathbf{I}\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\,(\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}}))^{-1}\\right]\\,\\mathbf{S}\\,\\left[\\mathbf{I}\\otimes\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{X}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}})\\,(\\mathbf{K}_{s}^{\\mathcal{D}}(\\mathbf{Z}_{\\mathbf{s}},\\mathbf{Z}_{\\mathbf{s}}))^{-1}\\right]^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.5.2 Structured Approximate Posterior With Spatial Inducing Points ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now derive the algorithm for the case of the structured approximate posterior with spatial inducing points. The key is to define the free-form approximate posterior over the inducing points and their temporal derivatives and then use the model conditional to compute the spatial derivatives. The model is ", "page_idx": 18}, {"type": "equation", "text": "$$\np(\\mathbf{Y}\\,|\\,\\mathbf{F})\\,p(\\mathbf{F}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{u})\\,p(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{u}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Each term is ", "page_idx": 18}, {"type": "equation", "text": "$$\np(\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{u})=\\mathrm{N}\\left(\\,\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{u}\\,\\,\\vert\\,\\,\\mathbf{0},\\,\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{K}(\\mathbf{Z},\\mathbf{Z})\\,\\mathcal{D}_{\\mathbf{t}}^{*}\\,\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\np(\\mathbf{F}\\,|\\,\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{u})=\\mathrm{N}\\left(\\mathbf{F}\\,\\,|\\,\\,\\mu_{\\mathbf{F}\\,|\\,\\mathbf{U}_{t}},\\,\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{U}_{t}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathbf{F}\\,|\\,\\mathbf{U}_{t}}=\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{t},\\mathbf{X}_{t})\\otimes\\widetilde{\\mathbf{K}}_{s}^{\\mathcal{D}}(\\mathbf{X}_{s},\\mathbf{X}_{s})\\right]\\left[\\mathbf{K}_{t}^{\\mathcal{D}}(\\mathbf{X}_{t},\\mathbf{X}_{t})\\otimes\\mathbf{K}_{s}(\\mathbf{Z}_{s},\\mathbf{Z}_{s})\\right]^{-1}\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{u}}\\\\ &{\\qquad\\qquad=\\left[\\mathbf{I}\\otimes\\widetilde{\\mathbf{K}}_{s}^{\\mathcal{D}}(\\mathbf{X}_{s},\\mathbf{X}_{s})\\,\\mathbf{K}_{s}(\\mathbf{Z}_{s},\\mathbf{Z}_{s})^{-1}\\right]\\mathcal{D}_{\\mathbf{t}}\\,\\mathbf{u}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\mathbf{F}\\mid\\mathbf{U}_{t}}=\\left[\\mathbf{K}_{t}^{D}(\\mathbf{X}_{\\mathrm{t}},\\mathbf{X}_{\\mathrm{t}})\\otimes\\mathbf{K}_{s}^{D}(\\mathbf{X}_{\\mathrm{s}},\\mathbf{X}_{\\mathrm{s}})\\right]}\\\\ &{\\qquad\\qquad-\\left[\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{Z}_{s}}^{\\otimes}\\right]\\left[\\mathbf{K}_{t}^{D}(\\mathbf{X}_{\\mathrm{t}},\\mathbf{X}_{\\mathrm{t}})\\otimes\\mathbf{K}_{s}(\\mathbf{Z}_{s},\\mathbf{Z}_{s})\\right]^{-1}\\left[\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{Z}_{s}}^{\\otimes}\\right]^{\\intercal}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\widetilde{\\mathbf{K}}_{{\\mathbf{X}},{\\mathbf{Z}_{s}}}^{\\otimes}=\\mathbf{K}_{t}^{\\mathcal{D}}({\\mathbf{X}_{\\mathrm{t}}},{\\mathbf{X}_{\\mathrm{t}}})\\otimes\\widetilde{\\mathbf{K}}_{s}^{\\mathcal{D}}({\\mathbf{X}_{\\mathrm{s}}},{\\mathbf{Z}_{s\\mathrm{s}}})$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\bf K}_{s}^{\\mathcal{D}}({\\bf X}_{\\mathrm{s}},{\\bf X}_{\\mathrm{s}})=\\left[{\\bf K}_{s}({\\bf X}_{s},{\\bf Z}_{s})\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The approximate posterior is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{F},\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{u})=p(\\mathbf{F}\\,|\\,\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{u})\\,q(\\mathcal{D}_{\\mathrm{t}}\\,\\mathbf{u})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $q(\\mathcal{D}_{\\mathrm{t}}{\\bf\\Psi}{\\bf u}$ is a free-form Gaussian of dimension $(N d\\times N_{\\mathrm{t}}\\times M_{s})$ . The rest of the derivation simply follows App. A.5.1 by simpling substituting $\\widetilde{\\bf K}_{s}^{\\mathcal{D}}({\\bf X}_{\\mathrm{s}},{\\bf Z}_{s_{\\mathrm{s}}})$ into the corresponding conditionals. The final result is that the approximate posterior decomposes as ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{U})\\propto\\left[\\prod_{t}^{N_{\\mathrm{t}}}\\mathrm{N}\\left(\\widetilde{\\mathbf{Y}}_{t}\\mid\\mathbf{F}_{t},\\widetilde{\\mathbf{V}}_{t}\\right)\\right]p(\\mathbf{F})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\widetilde{\\mathbf{Y}}_{t}$ and $\\mathbf{F}_{t}$ are vectors of dimension $(d_{t}\\,\\times\\,M_{s})$ , and $\\widetilde{\\mathbf{V}}_{t}$ is a matrix of dimension $(d_{t}\\,\\times$ $M_{s})\\times\\left(d_{t}\\times M_{s}\\right)$ . The marginals $q(\\mathbf{U}_{t})$ , and the corresponding marginal likelihood $p(\\widetilde{\\mathbf{Y}}\\,|\\,\\widetilde{\\mathbf{V}})$ can be computed by running a Kalman filter and smoother in $\\mathcal{O}(\\bar{N_{\\mathrm{t}}}\\cdot(\\bar{M_{s}}\\cdot d)^{3})$ , which co mp ared to App. A.5.1 is not cubic in the number of spatial derivatives. ", "page_idx": 19}, {"type": "text", "text": "A.5.3 Gauss-Newton Natural Gradient Approximation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now provide the full derivation of the Gauss-Newton approximation of the natural gradient used to ensure $p.s.d$ updates. We will make use of the following identities, known as the Bonnet and Price theorems (see, [38]), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\partial}{\\partial\\mu}\\mathbb{E}_{q({\\bf f}\\mid\\boldsymbol{\\mu},\\Sigma)}\\left[\\,\\ell({\\bf f})\\,\\right]=\\mathbb{E}_{\\boldsymbol{q}({\\bf f}\\mid\\boldsymbol{\\mu},\\Sigma)}\\left[\\,\\frac{\\partial}{\\partial{\\bf f}}\\,\\ell({\\bf f})\\,\\right]}\\ ~}\\\\ {{\\displaystyle\\frac{\\partial}{\\partial\\Sigma}\\mathbb{E}_{q({\\bf f}\\mid\\boldsymbol{\\mu},\\Sigma)}\\left[\\,\\ell({\\bf f})\\,\\right]=\\frac{1}{2}\\,\\mathbb{E}_{\\boldsymbol{q}({\\bf f}\\mid\\boldsymbol{\\mu},\\Sigma)}\\left[\\,\\frac{\\partial^{2}}{\\partial{\\bf f}\\,\\partial{\\bf f}^{\\top}}\\,\\ell({\\bf f})\\,\\right]}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which describes how to bring derivatives inside expectations. To ease notations, we work with a more general description of the model presented in the main paper, where we have multiple independent latent functions and use $T_{p}$ to denote likelihood-specific functions which, for example, can be used to represent DE or as the identity of standard Gaussian likelihoods. The model is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\mathbf{u}_{q})=\\mathrm{N}\\left(\\,\\mathbf{u}_{q}\\mid0,\\,\\mathbf{K}_{q}\\,\\right)\\qquad\\qquad}\\\\ {p(\\mathbf{f}_{q}\\mid\\mathbf{u}_{q})=\\mathrm{N}\\left(\\,\\mathbf{f}_{q}\\mid\\mu_{\\mathbf{f}_{q}\\mid\\mathbf{u}_{q}},\\,\\Sigma_{\\mathbf{f}_{q}\\mid\\mathbf{u}_{q}}\\,\\right)\\qquad\\qquad}\\\\ {\\mathbf{Y}_{n,q}=p(\\mathbf{Y}_{n,q}\\mid T_{p}(\\mathbf{f}_{n,1},\\dots,\\mathbf{f}_{n,Q}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the shapes are $\\mathbf{u}_{q}\\,\\in\\,\\mathbb{R}^{M}$ , $\\mathbf{f}_{q}^{}\\,\\in\\,\\mathbb{R}^{N}$ , $T_{p}:\\mathbb{R}^{Q}\\rightarrow\\mathbb{R}^{P}$ , $\\mathbf{Y}\\,\\in\\,\\mathbb{R}^{N\\times P}$ , and $\\mathbf{Y}_{n,p}\\,\\in\\,\\mathbb{R}$ . The variational approximation is ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{U})=\\mathrm{N}\\left(\\mathbf{U}\\mid\\mathbf{m},\\mathbf{S}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{U}=[\\mathbf{u}_{1},\\cdots,\\mathbf{u}_{Q}]$ , $\\mathbf{m}\\in\\mathbb{R}^{Q M\\times1}$ and $\\mathbf{S}\\in\\mathbb{R}^{Q M\\times Q M}$ . Let $\\mathbf{F}=[\\mathbf{f}_{1},\\ldots,\\mathbf{f}_{Q}]$ . The expected log-likelihood of the variational approximation is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ELL}=\\mathbb{E}_{q(\\mathbf{U})}\\left[\\mathbb{E}_{p(\\mathbf{F}\\mid\\mathbf{U})}\\left[\\sum_{n,p}\\log p(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{n,p}\\mathbb{E}_{q(\\mathbf{U})}\\left[\\mathbb{E}_{p(\\mathbf{F}_{n}\\mid\\mathbf{U})}\\left[\\log p(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{n,p}\\mathbb{E}_{q(\\mathbf{U}_{k})}\\left[\\mathbb{E}_{p(\\mathbf{F}_{n}\\mid\\mathbf{U}_{k})}\\left[\\log p(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $k=t(n)$ is the time period associated with data $\\mathbf{X}_{n}$ . Here $\\mathbf{U}_{k}$ are the spatial inducing points at time $t(n)$ and hence $\\mathbf{U}_{k}\\in\\mathbb{R}^{Q M_{s}}$ . We need to compute ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial\\mathrm{gt\\mathrm{L}}}{\\partial\\mathbf{S}}}=\\sum_{n,p}\\partial_{\\mathbf{S}}^{\\mathsf{L}}\\mathbb{E}_{q}(\\mathbf{U}_{k})\\left[\\mathbb{E}_{p}(\\mathbf{r}_{*}\\mid\\mathbf{U}_{k})\\left[\\log(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]}\\\\ &{=\\underset{n,p}{\\sum}\\mathbf{P}_{k}\\cdot\\frac{\\partial}{\\partial\\mathbf{S}_{k}}\\mathbb{E}_{q(\\mathbf{U}_{k})}\\left[\\mathbb{E}_{p(\\mathbf{r}_{*}\\mid\\mathbf{U}_{k})}\\left[\\log p(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]\\cdot\\mathbf{P}_{k}^{\\mathsf{L}}}\\\\ &{=\\underset{n,p}{\\sum}\\mathbf{P}_{k}\\cdot\\mathbb{E}_{q(\\mathbf{U}_{k})}\\left[\\frac{\\partial^{2}}{\\partial\\mathbf{U}_{k}\\,\\partial\\mathbf{U}_{k}^{\\mathsf{L}}}\\,\\mathbb{E}_{p(\\mathbf{r}_{*}\\mid\\mathbf{U}_{k})}\\left[\\log p(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}))\\right]\\right]\\cdot\\mathbf{P}_{k}^{\\mathsf{L}}}\\\\ &{\\frac{\\partial\\mathrm{dut}}{\\partial\\mathbf{S}_{k}}\\Bigg\\searrow\\mathbb{E}_{q}\\mathbb{E}_{q}(\\mathbf{U}_{k})\\left[\\frac{\\partial^{2}}{\\partial\\mathbf{U}_{k}\\,\\partial\\mathbf{U}_{k}^{\\mathsf{L}}}\\log(\\mathbf{Y}_{n,p}\\mid T_{p}(\\mathbf{F}_{n,p}^{*}))\\right]\\cdot\\mathbf{P}_{k}^{\\mathsf{L}}}\\\\ &{\\stackrel{\\mathrm{Gasshow~}}{\\approx}\\underset{n,p}{\\sum}\\mathbf{P}_{k}\\cdot\\mathbb{E}_{q(\\mathbf{U}_{k})}\\left[\\left[\\frac{\\partial\\,T_{p}(\\mathbf{F}_{n,p}^{*})}{\\partial\\mathbf{U}_{k}}\\right]^{\\mathsf{T}}\\,\\frac{\\partial^{2}\\log p(\\mathbf{Y}_{n,p}\\mid T_{p})}{\\partial T_{p}\\,\\partial T_{p}^{\\mathsf{T}}}\\left[\\frac{\\partial\\,T_{p}(\\mathbf{F}_{n,p}^{*})}{\\partial\\mathbf{U}_{k}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the shapes are ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbf{J}\\in Q M_{s}\\times1}\\\\ {\\mathbf{H}_{k}\\in1\\times1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\mathbf{P}_{k}$ is a permutation matrix that permutes from data-latent to latent-data format. In implementation, we do not need to perform this permutation as we only require the blocks \u2202\u2202ESLL but write it here for completeness. ", "page_idx": 20}, {"type": "text", "text": "A.5.4 Optimality of Natural Gradients In Linear Models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem A.1. Consider a linear multi-task model of the form ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{q}(\\cdot)\\sim\\mathcal{G P}(0,\\mathbf{K}_{q})}\\\\ &{\\widetilde{\\mathbf{f}}(\\mathbf{x})=[\\mathbf{f}_{q}(\\mathbf{x})]_{q=1}^{\\mathrm{Q}}}\\\\ &{\\;\\;\\mathbf{Y}_{n}=\\mathbf{W}\\,\\widetilde{\\mathbf{f}}(\\mathbf{X}_{n})+\\psi\\;\\;w h e r e\\;\\;\\psi\\sim\\mathrm{N}\\left(\\,\\mathbf{0},\\,b l k d i a g\\left(\\,[\\sigma_{p}^{2}]_{p=1}^{\\mathrm{P}}\\,\\right)\\,\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then under a full Gaussian variational approximate posterior ", "page_idx": 20}, {"type": "equation", "text": "$$\nq(\\widetilde{\\mathbf{f}})\\triangleq\\mathrm{N}\\left(\\widetilde{\\textbf{f}}|\\textbf{m},\\mathbf{S}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{m}\\in\\mathbb{R}^{(N\\times Q)\\times1},\\mathbf{S}\\in\\mathbb{R}^{(N\\times Q)\\times(N\\times Q)}$ then the natural gradient update with a learning rate of 1 recovers the optimal solution $p(\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. To prove this we first derive the natural parameters of the posterior $p(\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y})$ . We then derive the closed form expression of the natural parameter update and show that they recover that of the posterior. Let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{F}(\\mathbf{X})=\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{f}}\\left(\\mathbf{X}\\right)\\sim\\mathrm{N}\\left(\\mathbf{F}(\\mathbf{X})\\mid\\mathbf{0},\\,\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}=\\mathfrak{b l k d i a g}\\left(\\left[\\mathbf{K}_{q}\\right]_{q=1}^{\\mathrm{Q}}\\right)$ and $\\widetilde{\\mathbf{W}}=\\mathbf{W}\\otimes\\mathbf{I}$ then the posterior $p(\\mathbf{F}(\\mathbf{X})\\mid\\mathbf{Y})$ is Gaussian ", "page_idx": 20}, {"type": "equation", "text": "$$\np(\\mathbf{F}(\\mathbf{X})\\,|\\,\\mathbf{Y})=\\operatorname{N}\\left(\\,\\mathbf{F}(\\mathbf{X})\\,\\mid\\,\\mu_{\\mathbf{F}\\,|\\,\\mathbf{Y}}\\,,\\,\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{Y}}\\,\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathbf{F}\\,|\\,\\mathbf{Y}}=\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]\\,\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}+\\Phi\\right]^{-1}\\,\\mathbf{Y}}\\\\ &{\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{Y}}=\\left[\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]^{-1}+\\Phi^{-1}\\right]^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The covariance matrix can be simplified by invoking Woodbury\u2019s identity twice ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{Y}}=\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]-\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]\\left[\\Phi+\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]\\right]^{-1}\\left[\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]}\\\\ &{\\qquad=\\widetilde{\\mathbf{W}}\\,\\left[\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}-\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\left[\\Phi+\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right]^{-1}\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}\\right]\\widetilde{\\mathbf{W}}^{\\top}}\\\\ &{\\qquad=\\widetilde{\\mathbf{W}}\\,\\left[\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}+\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}^{-1}\\right]^{-1}\\widetilde{\\mathbf{W}}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the mean can be expressed as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathbf{F}\\,|\\,\\mathbf{Y}}=\\Sigma_{\\mathbf{F}\\,|\\,\\mathbf{Y}}\\,\\Phi^{-1}\\mathbf{Y}}\\\\ &{\\qquad\\quad=\\widetilde{\\mathbf{W}}\\,\\left[\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}+\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}^{-1}\\right]^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\mathbf{Y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we can immediately read off the posterior $p(\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y})$ as $p(\\mathbf{F}(\\mathbf{X})\\,|\\,\\mathbf{Y})=p(\\widetilde{\\mathbf{W}}\\,\\widetilde{\\mathbf{f}}\\,|\\,\\mathbf{Y})$ is simply a transformed version ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y})=\\mathrm{N}\\left(\\widetilde{\\mathbf{f}}\\mid\\left[\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\widetilde{\\mathbf{W}}^{\\top}+\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}^{-1}\\right]^{-1}\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\mathbf{Y},\\,\\left[\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\widetilde{\\mathbf{W}}^{\\top}+\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}^{-1}\\right]^{-1}\\right)}\\\\ &{\\qquad\\qquad=\\mathrm{N}\\left(\\widetilde{\\mathbf{f}}\\mid\\mu_{\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y}},\\,\\Sigma_{\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "whose natural parameters are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda_{\\widetilde{\\mathbf{f}}\\,|\\,\\mathbf{Y}}=\\left[\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\mathbf{Y},-\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}-\\frac{1}{2}\\,\\widetilde{\\mathbf{K}}_{\\mathbf{X},\\mathbf{X}}^{-1}\\right]^{\\top}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now derive the closed form expression of the natural gradient update with a learning rate of 1, and show that it recovers $\\lambda_{\\widetilde{\\mathbf{f}}\\,|\\,\\mathbf{Y}}$ . The expected log likelihood (ELL) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ELL}=\\mathbb{E}_{q(\\widetilde{\\mathbf{f}})}\\left[\\log\\mathrm{N}\\left(\\mathbf{Y}\\mid\\widetilde{\\mathbf{W}}\\widetilde{\\mathbf{f}},\\Phi\\right)\\right]}\\\\ &{\\quad\\quad=\\log\\mathrm{N}\\left(\\mathbf{Y}\\mid\\widetilde{\\mathbf{W}}\\widetilde{\\mathbf{f}},\\Phi\\right)-\\frac{1}{2}\\operatorname{Tr}\\left[\\Phi^{-1}\\widetilde{\\mathbf{W}}\\,\\mathbf{S}\\widetilde{\\mathbf{W}}^{\\top}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The required derivatives are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial_{\\mathrm{ELL}}}{\\partial\\mathbf{m}}=-\\frac{1}{2}\\,\\frac{\\partial}{\\partial\\mathbf{m}}\\left[\\left(\\mathbf{Y}-\\widetilde{\\mathbf{W}}\\,\\mathbf{m}\\right)^{\\intercal}\\Phi^{-1}\\left(\\mathbf{Y}-\\widetilde{\\mathbf{W}}\\,\\mathbf{m}\\right)\\right]}\\\\ {=\\widetilde{\\mathbf{W}}^{\\intercal}\\,\\Phi^{-1}\\left(\\mathbf{Y}-\\widetilde{\\mathbf{W}}\\,\\mathbf{m}\\right)\\qquad\\qquad\\qquad\\mathbf{\\Phi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last follows because $\\Phi$ is symmetric and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathrm{ELL}}{\\partial\\mathbf{S}}=-\\frac{1}{2}\\,\\frac{\\partial}{\\partial\\mathbf{S}}\\left[\\mathrm{Tr}\\left[\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}\\,\\mathbf{S}\\,\\widetilde{\\mathbf{W}}^{\\top}\\,\\right]\\right]}\\\\ &{\\quad\\quad\\quad=-\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The natural gradient is now given as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathrm{ELL}}{\\partial\\mu_{\\widetilde{\\mathbf{f}}\\,|\\,\\mathbf{Y}}}=\\left[\\frac{\\partial\\mathrm{EL}}{\\partial\\mathbf{m}}-2\\,\\frac{\\partial\\mathrm{EL}}{\\partial\\mathbf{S}}^{\\top}\\,\\mathbf{m}\\right]=\\left[\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\left(\\mathbf{Y}-\\widetilde{\\mathbf{W}}\\,\\mathbf{m}\\right)-\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}\\,\\mathbf{m}\\right]}\\\\ &{\\qquad\\qquad=\\left[\\widetilde{\\mathbf{W}}^{\\top}\\,\\Phi^{-1}\\,\\mathbf{Y}\\right]}\\\\ &{\\qquad\\qquad=\\left[\\mathbf{-}\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}\\,\\Phi^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The natural gradient update with a learning rate of 1 is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda_{q(\\widetilde{\\mathbf{f}})}=\\frac{\\partial\\mathrm{ELL}}{\\partial\\pmb{\\mu}_{\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y}}}+\\lambda_{p(\\widetilde{\\mathbf{f}})}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda_{p(\\widetilde{\\mathbf{f}})}=\\left[\\mathbf{0},-\\frac{1}{2}\\mathbf{K}^{-1}\\right]^{\\top}}\\end{array}$ are the natural parameters of the prior $p(\\widetilde{\\mathbf{f}})$ , hence after the update the natural parameters are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{q(\\widetilde{\\mathbf{f}})}=\\left[\\widetilde{\\mathbf{W}}^{\\top}\\,{\\Phi}^{-1}\\,\\mathbf{Y}\\right.}\\\\ {\\left.-\\frac{1}{2}\\,\\widetilde{\\mathbf{W}}\\,{\\Phi}^{-1}\\,\\widetilde{\\mathbf{W}}^{\\top}-\\frac{1}{2}\\mathbf{K}^{-1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which recover those of $p(\\widetilde{\\mathbf{f}}\\mid\\mathbf{Y})$ , and hence we recover the optimal posterior. ", "page_idx": 21}, {"type": "text", "text": "B Further Experimental Details and Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 An extension of AUTOIP ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "If one drops the requirement for state-space representations then the approximations proposed in Sec. 4 directly define approximations to the variational GP defined by Eqn. (13), and hence directly extend AUTOIP. For example on the non-linear damped pendulum in Sec. 7 we run this extension of AUTOIP with whitening and 50 inducing points for $C=1000$ and achieve an RMSE of $0.06\\pm0.001$ and running time of $158.16\\pm0.34$ , clearly improving the running time against AUTOIP. However the benefit of our methods is that PHYSS-GP remains linear in temporal dimensions which is vital for applications that are highly structured in time [20]. ", "page_idx": 22}, {"type": "text", "text": "B.2 Modelling Unknown Physics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Modelling of missing physics can be handled by parameterising unknown terms with GPs. For example take a simple non-linear pendulum ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\frac{d^{2}\\theta}{d t^{2}}}+\\sin(\\theta)=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now consider that the the $\\displaystyle\\sin(\\theta)$ is unknown and we would like to learn it. If we define the our differential equation in Eqn. (4) as ", "page_idx": 22}, {"type": "equation", "text": "$$\ng={\\frac{d^{2}\\,f_{1}}{d t^{2}}}+f_{2}(t)=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where both $f_{1}(\\cdot),f_{2}(\\cdot)$ are latent GPs that we wish to learn. We now construct 300 observations for training from the solution of Eqn. (75) across the range $[0,30]$ and 1000 for testing. We run PHYSS-GP and compare the similarity of the learnt latent GP $f_{2}(\\cdot)$ to the true function at the test locations and achieve an RMSE of 0.068 indicating we have recovered the latent force/unknown physics well. ", "page_idx": 22}, {"type": "text", "text": "B.3 Monotonic Timeseries ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This first example showcases the effectiveness of PHYSS-GP in learning monotonic functions. Monotonicity information is expressed by regularising the first derivative to be positive at a set of collocation points [51]: ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\mathbf{Y}\\mid\\mathbf{f})=\\operatorname{N}\\left(\\mathbf{Y}\\mid\\mathbf{f},\\,\\sigma_{y}^{2}\\,\\right),\\;\\;p(\\mathbf{0}\\mid{\\frac{\\partial\\mathbf{f}}{\\partial t}})}&{=\\Phi({\\frac{\\partial\\mathbf{f}}{\\partial t}}\\cdot{\\frac{1}{v}})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Phi(\\cdot)$ is a Gaussian cumulative distribution function, and $v=1e-1$ is a tuning parameter that controls the steepness of the step function. We plot predictive distributions of (batch) GP and PHYSS-GP in Fig. 5. The GP fits data and does not learn a monotonic function. However, using 300 collocation points, PHYSS-GP is able to include the additional information and learn a monotonic function whilst running 1.5 times faster. ", "page_idx": 22}, {"type": "image", "img_path": "tCf7S75xFa/tmp/d6770dea85ceffd80b41120c487f994c36002ec37c14346dd13c6419a4efb47e.jpg", "img_caption": ["Figure 5: Predictive distributions of GP and PHYSS-GP on the monotonic function in App. B. The GP cannot incorporate monotonicity information and fits the data. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "tCf7S75xFa/tmp/ad8f503c8e35c514c365db40a8d73117c2ba7ae15bdbd4cb0b202c374c9ce66e.jpg", "img_caption": ["Figure 6: Predictive distributions on the Damped Pendulum. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.4 Non-linear Damped Pendulum ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. All were optimised for 1000 epochs using Adam [32] with a learning rate of 0.01. Both the GP and AUTOIP had an RBF kernel (following Long et al. [40]) and PHYSS-GP used a Mat\u00e9rn- $\\eta_{2}$ ; all with a lengthscale of 1.0. The observation noise was initialised to 0.01 and the collocation 0.001. Both were fixed for the first $40\\%$ of training and then released. Predictive distribution of PHYSS-GP and AUTOIP are plotted in Fig. 6. ", "page_idx": 23}, {"type": "text", "text": "B.5 Curl-free Magnetic Field Strength ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. All models are run for 5000 epochs using Adam with a learning rate of 0.01, and use a Mat\u00e9rn- $3/2$ kernel on time, with ARD RBF kernels on the spatial dimensions, with a lengthscale of 0.1 across all. The Gaussian likelihood is initialised with a variance of 0.01 and held for $\\bar{4}0\\%$ of training. All our methods used a natural gradient learning rate of 1.0 as this is the conjugate setting. ", "page_idx": 23}, {"type": "text", "text": "B.6 Diffusion-Reaction System ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We use data provided by [49] under an MIT license. All models were run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. Our method PHYSS-SVGP and $\\mathrm{PHYSS-SVGP_{H}}$ use a Mat\u00e9rn 72 kernel on time and an RBF of space, both initialised with a lengthscale of 0.1. We place the collocation points on a regular grid of size $20\\times10$ and use $M_{s}=20$ spatial inducing points. We pretrain for 100 iterations using a natural gradient learning rate of 0.01 and after use a learning rate 0.1 for the remaining 19000 iterations. AUTOIP uses a RBF kernel on both time and space with a lengthscale of 0.1. We place the collocation points on a regular grid of size $10\\times10$ . All models use Adam with a learning rate 0.001 and train for a total of 20000 iterations. ", "page_idx": 23}, {"type": "text", "text": "B.7 Ocean Currents ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our method $\\mathrm{PHYSS-SVGP_{H}}$ was run using an Nvidia Titan RTX GPU and an Intel Core i5 CPU. We ran for 10000 iterations, using Adam with a learning rate of 0.01. For natural gradients with used a learning rate of 0.1. We used a Mat\u00e9rn- $\\ensuremath{3}/\\ensuremath{2}$ kernel on time and RBF kernels on both spatial dimensions with lengthscales [24.0, 1.0, 1.0]. We used 100 spatial inducing points and a spatial mini-batch size of 10. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have both theoretically (Theorem 3.1), and empirically demonstrated our claims (Sec. 7). We provide computational complexities for all proposed models showing a linear in the temporal dimension performance. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: A discussion of two main limitations (collocation method, and spatial scaling) and future work is provided in the conclusion. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The assumptions of Theorem 3.1 are provided in Sec. 3 with full proof (correct to the best of our knowledge) provided in the appendix App. A.5.4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Experimental details, sufficient to reproduce the results, are provided both in the main paper Sec. 7 and the appendix App. B. Code reproducing all methods and results is provided in the supplementary material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Code for data downloading and processing for train test splits, as well as code reproducing all methods and results is provided in the supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 27}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All required details are provided in App. B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: We followed established published results for experimental setups, which did not include error bars. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Specific details on the CPUs and GPUs used for experiments are provided in App. B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and we conform with this in every respect. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the beneftis of physics informed machine learning in our introduction. We do not envisage any negative societal implications. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All existing methods and datasets used are open and cited throughout. Any licenses are explicitly stated in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We release our code under CC-BY 4.0. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]