[{"figure_path": "Cth1PyCwZt/figures/figures_5_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the correlation between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and various LLMs on the ENEM 2023 exam.  The plot shows that identical accuracy scores can correspond to different ability levels according to IRT, highlighting IRT's ability to capture more nuanced performance patterns than accuracy alone.  Each LLM's performance is represented by a closed curve encompassing the central 90% of its data points, generated across multiple shuffles of the answer order, thus controlling for position bias. The human data points are shown as a light blue background cloud.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_6_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of different LLMs across various questions in four different exams (Languages and Codes, Natural Sciences, Humanities, and Mathematics). The questions are ordered from easiest to hardest based on their difficulty parameter (\u03b2). Each cell represents the probability that a given LLM answered a specific question correctly (averaged over 30 shuffles of answer options). Darker cells denote higher probabilities (more often correct). The figure helps to compare the response patterns of different LLMs and highlight differences in their performance on questions of varying difficulty levels across different exams.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_7_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the distribution of Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and several LLMs on the ENEM 2023 exam.  Each point represents a single LLM's performance across 30 different shuffles of the answer choices.  The light blue points represent the human data. The figure shows that LLMs exhibit a range of performance, with some having similar accuracy and ability scores to humans and others deviating significantly.  The scatter and density plots help visualize the relationship between accuracy and ability, highlighting how IRT provides a more nuanced understanding of LLM performance compared to simple accuracy.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_7_2.jpg", "caption": "Figure 4: Total Fisher information of the exams and the IRT scores (95% Confidence Interval (CI)) for LLMs. LLM datapoints are computed from different shuffles.", "description": "This figure displays the Fisher Information for each exam (Humanities, Languages, Natural Sciences, and Mathematics) and the IRT scores for various LLMs.  The top panel shows the Fisher Information, indicating the amount of information each exam provides for estimating the ability of test-takers. The bottom panel presents the IRT scores for the LLMs, with error bars representing the 95% confidence intervals obtained through multiple shuffles of answer options.  This allows comparison between LLMs' performance and the average human performance on each exam, considering the reliability of the exam to measure the models' ability.", "section": "5.3 Reliability of IRT scores for LLMs"}, {"figure_path": "Cth1PyCwZt/figures/figures_8_1.jpg", "caption": "Figure 5: Discrimination Indices for questions in the 2023 exam for both Humans and LLMs.", "description": "This figure displays the distribution of discrimination indices for questions in the 2023 ENEM exam, separately for humans and LLMs. The discrimination index measures how well a question distinguishes between high and low scorers.  The figure helps to understand the quality and reliability of the exam questions for LLMs in various subject areas.  A high discrimination index suggests a good question that effectively measures ability, while a low index suggests a poorly discriminating question or potential issues with the model or exam.", "section": "5.3 Reliability of IRT scores for LLMs"}, {"figure_path": "Cth1PyCwZt/figures/figures_16_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and LLMs on the ENEM 2023 exam.  Each point represents a single LLM run, with multiple points per LLM representing different shuffles of answer order. The figure helps visualize how LLMs' performance varies across different accuracy levels and IRT scores, offering insights into the differences between LLM and human performance patterns on this exam.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_24_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test-takers and various Large Language Models (LLMs) on the ENEM 2023 exam. Each point represents the performance of a single LLM across multiple question order shuffles. The light blue background points show the distribution of human scores. The figure shows that for a given accuracy score, there is a significant variation in IRT scores indicating IRT is a more nuanced metric and can reveal differences in performance not captured by CTT scores alone.  The majority of the LLMs' performances overlap with human scores, particularly in Humanities, Languages and Natural Sciences.  However, the LLMs show more variability in CTT scores than in IRT scores.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_25_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for human test takers and several LLMs on the 2023 ENEM exam.  The plot reveals the distribution of CTT and IRT scores for both humans and LLMs, highlighting the differences in performance patterns. Each closed curve represents the central 90% of the IRT score distribution for a given LLM. The light blue points represent the human test-takers. The figure indicates that LLMs often show different patterns of accuracy scores compared to human test takers even when their overall accuracy is similar, demonstrating the limitations of using accuracy alone as a metric for evaluating LLM performance.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_25_2.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and various LLMs on the ENEM 2023 exam.  The plot visually demonstrates that LLMs, even with similar accuracy scores, can exhibit varying IRT scores, reflecting differences in their response patterns.  The background points represent human performance, providing a benchmark for comparison with LLM results. The closed curves around each set of LLM data points show the 90% central region of the LLM's distribution, highlighting the variability in their performance across multiple runs.", "section": "5 Results"}, {"figure_path": "Cth1PyCwZt/figures/figures_26_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and several LLMs on the 2023 ENEM exam.  Each point represents the performance of an LLM (or a human) on the exam, with the x-axis representing the accuracy and the y-axis representing the ability. The light blue background points are the data for humans. Each colored curve represents the 90% central region of an LLM's data points across 30 shuffles of the order of answer options. The figure visually demonstrates how the same accuracy scores can result in different ability levels based on response patterns. It allows comparison of LLM ability levels with those of humans.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_27_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of various LLMs across different exam sections. Each cell represents the probability of a given LLM correctly answering a particular question, averaged over 30 shuffles of the answer choices. Darker cells indicate higher probabilities of correct answers, while lighter cells suggest lower probabilities. The questions are ordered by increasing difficulty, allowing for the identification of patterns in how LLMs approach questions of varying difficulty levels. The visualization helps analyze if LLMs exhibit human-like response patterns on exams.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_27_2.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of different LLMs across questions sorted by difficulty. Darker colors represent higher accuracy. Each row represents an LLM, and each column represents a question. The figure helps to identify patterns and differences in how various LLMs answer questions of varying difficulty levels.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_28_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of various LLMs across different exam sections. Each cell's darkness represents the probability of a correct answer, averaged across multiple shuffles of question order.  The questions are ordered by their difficulty (beta value) as determined by IRT modeling. Darker cells show higher probabilities of correct responses, indicating stronger performance in easier questions. The figure allows for visual comparison of LLMs' response patterns based on the arrangement of easy and hard questions.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_28_2.jpg", "caption": "Figure 11: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot.", "description": "This figure shows the response patterns of different LLMs (Large Language Models) on questions from the 2023 ENEM exam.  Each row represents a specific LLM, and each column represents a question. The questions are ordered by difficulty (\u03b2 value), making it easy to see which questions each LLM answered correctly more often. The darker the cell, the more often that LLM answered the question correctly across 30 runs, with the answer options shuffled each time to minimize bias.  This visualization helps understand the strengths and weaknesses of each model in answering questions of varying difficulty. The models shown are open-source models and GPT-3.5, tested without any instructions (zero-shot setting).", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_29_1.jpg", "caption": "Figure 3: Distribution of l\u2082 and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the distribution of l\u2082 and IRT scores for both humans and LLMs on the ENEM 2023 exam.  The light blue points represent the human data, providing a baseline for comparison. Each colored closed curve represents a different Large Language Model (LLM), showing the 90% central distribution for each model across 30 shuffles of the answer order.  The x-axis represents the l\u2082 score (goodness-of-fit), measuring how well the LLM's responses match the expected patterns of the IRT model.  The y-axis represents the IRT score (ability), showing the LLM's estimated ability level according to the model. Deviations from the human distribution (light blue) may indicate non-human-like behavior or unreliable exam scores for those models.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_29_2.jpg", "caption": "Figure 3: Distribution of l\u2082 and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the distribution of l\u2082 and IRT scores for both humans and LLMs, using four different exams (Humanities, Languages and Codes, Natural Sciences, and Mathematics).  The LLMs used are non-instructed, tuned, open source models, plus GPT3.5. Each LLM's data points are calculated from 30 different shuffles of the answer order.  The plot helps visualize the relationship between a model's accuracy (IRT score) and its consistency with the model's expectations (l\u2082 score).  Deviations from the expected distribution highlight models that are not consistently human-like in their response patterns, even if they achieve high accuracy.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_29_3.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test-takers and several Large Language Models (LLMs) on the 2023 ENEM exam.  Each point represents a model's performance on one of the 30 shuffles of answer options, demonstrating the range of performance variability. The light blue background points represent human performance data. The figure helps visually compare the performance spread of LLMs and humans, showing how the IRT score provides a more nuanced measure of ability than the simple accuracy score.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_30_1.jpg", "caption": "Figure 3: Distribution of l\u2082 and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the distribution of l2 and IRT scores for both humans and LLMs on the ENEM 2023 exam.  The data for LLMs is derived from 30 different shuffles of the answer options to account for potential answer position biases. The plot highlights the differences in response patterns between humans and LLMs, showing that LLMs' IRT scores can be similar to human average ability but not identical, suggesting different underlying reasoning processes. The figure also indicates the variability in accuracy scores versus IRT scores for LLMs.", "section": "5 Results"}, {"figure_path": "Cth1PyCwZt/figures/figures_31_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "The figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and LLMs on the ENEM 2023 exam.  Each point represents the performance of a single LLM or human on the exam, accounting for variations in the order of the answer choices through 30 shuffles.  The plot helps to visually compare human and LLM performance across both metrics, demonstrating that identical CTT scores can lead to different IRT scores. This highlights IRT's ability to consider the pattern of correct and incorrect answers, which is not captured solely by the accuracy metric.  The spread of data points reveals the variability in both accuracy and IRT score for LLMs, compared to the more clustered distribution of human results.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_32_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test-takers and various LLMs on the 2023 ENEM exam.  The plot shows that identical accuracy scores can correspond to different IRT scores, highlighting IRT's ability to differentiate performance beyond simple accuracy. LLMs' performance varies across exams, and differences in accuracy and IRT scores are observed. This suggests that IRT provides a more nuanced evaluation of model performance compared to accuracy alone.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_32_2.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test takers and various LLMs on the 2023 ENEM exam.  Each point represents the performance of a single LLM run on the exam, with multiple points for each LLM reflecting the variability introduced by shuffling the order of the answer choices. The light blue points represent human performance. The figure provides insights into how accurately different LLMs model human-like response patterns on the exam.", "section": "5.1 Accuracy vs Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_33_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the correlation between the Classical Test Theory (CTT) score (accuracy) and the Item Response Theory (IRT) score (ability) for both humans and various LLMs.  The light blue points represent human test takers, while each colored curve shows the distribution of CTT and IRT scores for a specific LLM, based on 30 different random orderings of answer choices. The figure highlights the differences between the scores and the variability of LLM performance compared to human consistency. It suggests that IRT provides a more nuanced assessment of LLM performance than accuracy alone.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_33_2.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and various LLMs on the 2023 ENEM exam.  Each point represents the performance of an LLM or human test-taker, with the x-axis indicating accuracy and the y-axis representing the IRT ability score.  The different colored curves represent the performance distributions of different LLMs.  The light blue points represent the human data.  The figure allows a visual comparison of the performance of LLMs compared to humans, revealing discrepancies despite similar accuracy scores.", "section": "5.1 Accuracy vs Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_34_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test-takers and various LLMs on the 2023 ENEM exam.  Each closed curve represents the central 90% of the IRT score distribution for a particular LLM, illustrating the variability in their performance.  The light blue points represent human test-takers. The figure highlights the differences between LLMs and humans in their performance and the variability among LLMs. Importantly, the figure shows that identical accuracy scores can correspond to different ability levels.", "section": "5.1 Accuracy vs Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_35_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "The figure shows the response patterns of LLMs for the 2023 ENEM exam. Each cell (i,j) represents the probability that LLM i answered question j correctly, where probabilities are computed over the 30 shuffles of answer choices. Darker cells mean that the model answered the questions correctly more often. Questions are ordered by increasing difficulty, from left to right.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_35_2.jpg", "caption": "Figure 26: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\u03b2 value). LLMs are instructed tuned open source models with four-shot.", "description": "This figure shows the response patterns of various LLMs on the 2023 ENEM exam. Each cell represents the probability that a given LLM answered a question correctly, averaged over 30 shuffles of the answer choices. Darker cells indicate more frequent correct answers. Questions are sorted by their difficulty level (beta values) from easiest to hardest. This visualization allows for comparison of LLM performance across different exams and highlights potential differences in how LLMs answer easy versus hard questions.", "section": "A.6.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_36_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "The figure visualizes the response patterns of different LLMs across various questions, sorted by difficulty. Darker shades represent higher accuracy in answering those questions across 30 shuffles of the answer options. This allows to compare the performance of different models in terms of their response consistency and pattern similarity to human-like answers.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_36_2.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of different LLMs across various questions, ordered by difficulty. Darker cells indicate higher probabilities of correct answers. It showcases the performance of both open-source models and GPT-3.5 under four-shot prompting, highlighting variations in model responses based on question difficulty.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_37_1.jpg", "caption": "Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\u03b2 value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.", "description": "This figure visualizes the response patterns of different LLMs across questions ordered by difficulty. Each cell represents the probability of an LLM answering a question correctly, averaged over 30 shuffles of answer choices. Darker cells indicate higher probabilities of correct answers. This helps to understand the LLM's ability to handle questions of varying difficulty and reveals patterns in their response behaviors.", "section": "5.2 Response Patterns"}, {"figure_path": "Cth1PyCwZt/figures/figures_37_2.jpg", "caption": "Figure 26: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\u03b2 value). LLMs are instructed tuned open source models with four-shot.", "description": "This figure visualizes the response patterns of various Large Language Models (LLMs) across different questions from the 2022 ENEM exam. Each row represents an LLM, and each column represents a question. The questions are ordered by difficulty, with easier questions on the left and harder questions on the right. The darkness of each cell indicates the probability of the LLM answering the corresponding question correctly, averaged over multiple trials. Darker cells indicate a higher probability of correct answers.  The figure helps in understanding how different LLMs approach questions of varying difficulty levels and reveals patterns in their response behavior.  This aids in identifying LLMs that exhibit human-like response patterns compared to others with more machine-like patterns.", "section": "A.8 Response Patterns for 2022"}, {"figure_path": "Cth1PyCwZt/figures/figures_38_1.jpg", "caption": "Figure 3: Distribution of l\u2082 and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the distribution of l\u2082 and IRT scores for both humans and LLMs on the ENEM 2023 exam.  The light blue points represent the human data. Each closed curve represents a different LLM, showing the central 90% of its distribution from 30 shuffles of answer options. The plot illustrates the differences in response patterns between LLMs and humans, highlighting the limitations of using only accuracy as a metric for evaluating LLM performance.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_38_2.jpg", "caption": "Figure 31: Distribution of l\u2082 and IRT scores for humans and LLMs in the ENEM 2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the distribution of l\u2082 and IRT scores for both humans and LLMs on the ENEM 2022 exam.  The data for LLMs includes scores from multiple runs with shuffled answer order to account for randomness.  The plot visually represents the relationship between the goodness-of-fit of the LLMs' responses to the IRT model (l\u2082 score) and their estimated ability (IRT score). This allows for a visual comparison of how well the LLMs' performance aligns with human-like response patterns on the exam.", "section": "A.9 Comparing IRT \u03b8 and l\u2082 for 2022"}, {"figure_path": "Cth1PyCwZt/figures/figures_38_3.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure displays the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and various LLMs on the 2023 ENEM exam. Each point represents an LLM's performance after shuffling the order of answer choices 30 times. The figure helps to visualize how LLMs perform compared to humans in terms of both accuracy and ability level, highlighting the differences in performance patterns between LLMs and humans.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_39_1.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both humans and LLMs on the ENEM 2023 exam.  The light blue points represent human performance. Each closed curve represents the performance of a single LLM, showing the central 90% of the LLM's scores across 30 shuffles of the order of answer choices. The figure highlights that LLMs exhibit varying degrees of consistency with human-like performance patterns.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_39_2.jpg", "caption": "Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the relationship between Classical Test Theory (CTT) scores (accuracy) and Item Response Theory (IRT) scores (ability) for both human test takers and various Large Language Models (LLMs) on the 2023 ENEM exam.  The plot visualizes the distribution of scores for each model with 90% confidence intervals shown. LLMs are evaluated using four-shot prompting, and 30 different shuffles of answer options are used to generate LLM datapoints for a more robust assessment. The goal is to compare the performance of LLMs to humans and identify differences in their response patterns.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_39_3.jpg", "caption": "Figure 3: Distribution of l\u2082 and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.", "description": "This figure shows the distribution of l\u2082 and IRT scores for both humans and LLMs on the ENEM 2023 exam.  The light blue background represents the distribution of human scores, providing a benchmark for comparison. Each colored curve represents a different LLM, with multiple data points generated from shuffling the answer choices to account for possible bias. This visualization allows us to compare not only accuracy but also response patterns, revealing how well the LLMs' performance aligns with expected human behavior.", "section": "5.1 Accuracy vs. Ability Level"}, {"figure_path": "Cth1PyCwZt/figures/figures_40_1.jpg", "caption": "Figure 37: Question 108 Natural Sciences", "description": "The figure shows a circuit with three identical incandescent light bulbs (L1, L2, L3), each in parallel with a resistor (R). These parallel sets are connected in series, and all bulbs have the same brightness when connected to a power supply. The question in the paper discusses the brightness of the bulbs when L2 burns out.", "section": "A.10 Examples of non-discriminating and highly discriminating items for the 2023 Natural Sciences exam"}, {"figure_path": "Cth1PyCwZt/figures/figures_41_1.jpg", "caption": "Figure 38: Question 109 Natural Sciences", "description": "The figure shows a truck carrying a load (box M) secured by two horizontal ropes (1 and 2). The truck is moving horizontally to the right.  The question in the paper relates to determining the tensions in the ropes (T1 and T2) when the truck is accelerating and braking, given its mass, acceleration, braking, and coefficient of static friction.", "section": "A.10.2 Highly discriminative questions"}]