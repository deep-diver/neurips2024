[{"type": "text", "text": "GeNIe: Generative Hard Negative Images Through Diffusion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Data augmentation is crucial in training deep models, preventing them from over  \n2 fitting to limited data. Recent advances in generative AI, e.g., diffusion mod  \n3 els, have enabled more sophisticated augmentation techniques that produce data   \n4 resembling natural images. We introduce GeNIe a novel augmentation method   \n5 which leverages a latent diffusion model conditioned on a text prompt to combine   \n6 two contrasting data points (an image from the source category and a text prompt   \n7 from the target category) to generate challenging augmentations. To achieve this,   \n8 we adjust the noise level (equivalently, number of diffusion iterations) to ensure   \n9 the generated image retains low-level and background features from the source   \n10 image while representing the target category, resulting in a hard negative sample   \n1 for the source category. We further automate and enhance GeNIe by adaptively   \n12 adjusting the noise level selection on a per image basis (coined as GeNIe-Ada),   \n13 leading to further performance improvements. Our extensive experiments, in both   \n14 few-shot and long-tail distribution settings, demonstrate the effectiveness of our   \n15 novel augmentation method and its superior performance over the prior art. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Augmentation has become an integral part of training deep learning models, particularly when faced   \n18 with limited training data. For instance, when it comes to image classification with limited number   \n19 of samples per class, model generalization ability can be significantly hindered. Simple transfor  \n20 mations like rotation, cropping, and adjustments in brightness artificially diversify the training set,   \n21 offering the model a more comprehensive grasp of potential data variations. Hence, augmentation   \n22 can serve as a practical strategy to boost the model\u2019s learning capacity, minimizing the risk of overfit  \n23 ting and facilitating effective knowledge transfer from limited labelled data to real-world scenarios.   \n24 Various image augmentation methods, encompassing standard transformations, and learning-based   \n25 approaches have been proposed [16, 15, 110, 111, 100]. Some augmentation strategies combine two   \n26 images possibly from two different categories to generate a new sample image. The simplest ones   \n27 in this category are MixUp [111] and CutMix [110] where two images are combined in the pixel   \n28 space. However, the resulting augmentations often do not lie within the manifold of natural images   \n29 and act as out-of-distribution samples that will not be encountered during testing.   \n30 Recently, leveraging generative models for data augmentation has gained an upsurge of attention   \n31 [100, 83, 63, 35]. These interesting studies, either based on fine-tuning or prompt engineering of   \n32 diffusion models, are mostly focused on generating generic augmentations without considering the   \n33 impact of other classes and incorporating that information into the generative process for a classifi  \n34 cation context. We take a different approach to generate challenging augmentations near the decision   \n35 boundaries of a downstream classifier. Inspired by diffusion-based image editing methods [67, 63]   \n36 some of which are previously used for data augmentation, we propose to use conditional latent dif  \n37 fusion models [81] for generating hard negative images. Our core idea (coined as GeNIe) is to   \n38 sample source images from various categories and prompt the diffusion model with a contradictory   \n39 text corresponding to a different target category. We demonstrate that the choice of noise level (or   \n40 equivalently number of iterations) for the diffusion process plays a pivotal role in generating images   \n41 that semantically belong to the target category while retaining low-level features from the source   \n42 image. We argue that these generated samples serve as hard negatives [108, 65] for the source cat  \n43 egory (or from a dual perspective hard positives for the target category). To further enhance GeNIe,   \n44 we propose an adaptive noise level selection strategy (dubbed as GeNIe-Ada) enabling it to adjust   \n45 noise levels automatically per sample.   \n46 To establish the impact of GeNIe, we focus on two challenging scenarios: long-tail and few-shot   \n47 settings. In real-world applications, data often follows a long-tail distribution, where common sce  \n48 narios dominate and rare occurrences are underrepresented. For instance, a person jaywalking a   \n49 highway causes models to struggle with such unusual scenarios. Combating such a bias or lack of   \n50 sufficient data samples during model training is essential in building robust models for self-driving   \n51 cars or surveillance systems, to name a few. Same challenge arises in few-shot learning settings   \n52 where the model has to learn from only a handful of samples. Our extensive quantitative and qual  \n53 itative experimentation, on a suite of few-shot and long-tail distribution settings, corroborate the   \n54 effectiveness of the proposed novel augmentation method (GeNIe, GeNIe-Ada) in generating hard   \n55 negatives, corroborating its significant impact on categories with a limited number of samples. A   \n56 high-level sketch of GeNIe is illustrated in Fig. 1. Our main contributions are summarized below:   \n57 - We introduce GeNIe, a novel yet elegantly simple diffusion-based augmentation method to cre  \n58 ate challenging augmentations in the manifold of natural images. For the first time, to our best   \n59 knowledge, GeNIe achieves this by combining two sources of information (a source image, and a   \n60 contradictory target prompt) through a noise-level adjustment mechanism.   \n61 - We further extend GeNIe by automating the noise-level adjustment strategy on a per-sample basis   \n62 (called GeNIe-Ada), to enable generating hard negative samples in the context of image classifica  \n63 tion, leading also to further performance enhancement.   \n64 - To substantiate the impact of GeNIe, we present a suit of quantitative and qualitative results in  \n65 cluding extensive experimentation on two challenging tasks: few-shot and long tail distribution   \n66 settings corroborating that GeNIe (and its extension GeNIe-Ada) significantly improve the down  \n67 stream classification performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/88ffafa25b7c1f0398bc0014fd39de201e276f2ffad58931dd032c0d6e0c8e7a.jpg", "img_caption": ["Figure 1: Generative Hard Negative Images Through Diffusion (GeNIe): generates hard negative images that belong to the target category but are similar to the source image from low-level feature and contextual perspectives. GeNIe starts from a source image passing it through a partial noise addition process, and conditioning it on a different target category. By controlling the amount of noise, the reverse latent diffusion process generates images that serve as hard negatives for the source category. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Data Augmentations. Simple flipping, cropping, colour jittering, and blurring are some forms of   \n70 image augmentations [91]. These augmentations are commonly adopted in training deep learning   \n71 models. However, using these data augmentations is not trivial in some domains. For example,   \n72 using blurring might remove important low-level information from medical images. More advanced   \n73 approaches, such as MixUp [111] and CutMix [110], mix images and their labels accordingly [37,   \n74 59, 47, 17]. However, the resulting augmentations are not natural images anymore, and thus, act   \n75 as out-of-distribution samples that will not be seen at test time. Another strand of research tailors   \n76 the augmentation strategy through a learning process to fit the training data [23, 16, 15]. Unlike the   \n77 above methods, we propose to utilize pre-trained latent diffusion models to generate hard negatives   \n78 (in contrast to generic augmentations) through a noise adaptation strategy discussed in Section 3.   \n79 Data Augmentation with Generative Models. Using synthesized images from generative models   \n80 to augment training data has been studied before in many domains [30, 86], including domain adap  \n81 tation [41], visual alignment [71], and mitigation of dataset bias [88, 36, 73]. For example, [73]   \n82 introduces a methodology aimed at enhancing test set evaluation through augmentation. While pre  \n83 vious methods predominantly relied on GANs [114, 51, 101] as the generative model, more recent   \n84 studies promote using diffusion models to augment the data [81, 35, 89, 100, 4, 62, 83, 42, 28, 26, 8].   \n85 More specifically, [100, 83, 35, 4] study the effectiveness of text-to-image diffusion models in data   \n86 augmentation by diversification of each class with synthetic images. [100] leverages a text-to-image   \n87 diffusion model and fine-tunes it on the downstream dataset using textual-inversion [31] to increase   \n88 the diversity of existing samples. [83] also utilizes a text-to-image diffusion model, but with a BLIP   \n89 [53] model to generate meaningful captions from the existing images. [42] utilizes diffusion models   \n90 for augmentation to correct model mistakes. [28] uses CLIP [76] to filter generated images. [26]   \n91 utilizes text-based diffusion and a large language model (LLM) to diversify the training data. [8]   \n92 uses an LLM to generate text descriptions of failure modes associated with spurious correlations,   \n93 which are then used to generate synthetic data through generative models. The challenge here is that   \n94 the LLM has little understanding of such failure scenarios and contexts.   \n95 We take a completely different approach here, without replying on any extra source of information   \n96 (e.g., through an LLM). Inspired by image editing approaches such as Boomerang [63] and SDEdit   \n97 [67], we propose to adaptively guide a latent diffusion model to generate hard negatives images   \n98 [65, 108] on a per-sample basis per category. In a nutshell, the aforementioned studies focus on im  \n99 proving the diversity of each class with effective prompts and diffusion models, however, we focus   \n100 on generating effective hard negative samples for each class by combining two sources of contra  \n101 dicting information (images from the source category and text prompt from the target category).   \n102 Language Guided Recognition Models. Vision-Language foundation models (VLMs) [2, 76, 81,   \n103 84, 77, 78] utilize human language to guide the generation of images or to extract features from   \n104 images that are aligned with human language. For example, CLIP [76] shows decent zero-shot   \n105 performance on many downstream tasks by matching images to their text descriptions. Some recent   \n106 works improve the utilization of human language in the prompt [25, 72], and others use a diffusion   \n107 model directly as a classifier [49]. Similar to the above, we use a foundation model (Stable Diffusion   \n108 1.5 [81]) to improve the downstream task. Concretely, we utilize category names of the downstream   \n109 tasks to augment their associate training data with hard negative samples.   \n110 Few-Shot Learning. In Few-shot Learning (FSL), we pre-train a model with abundant data to learn   \n111 a rich representation, then fine-tune it on new tasks with only a few available samples. In supervised   \n112 FSL [10, 1, 74, 109, 27, 54, 95, 116, 92], pretraining is done on a labeled dataset, whereas in   \n113 unsupervised FSL [43, 103, 61, 75, 3, 46, 39, 66, 90] the pre-training has to be conducted on an   \n114 unlabeled dataset. We assess the impact of GeNIe on a number of few-shot scenarios and state-of  \n115 the-art baselines by accentuating on its impact on the few-shot inference stage. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "116 3 Proposed Method: GeNIe ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 Given a source image $X_{S}$ from category $\\mathbf{S}=<$ <source category $>$ , we are interested in generating a   \n118 target image $X_{r}$ from category $T=<$ target category $>$ . In doing so, we intend to ensure the low  \n119 level visual features or background context of the source image are preserved, so that we generate   \n120 samples that would serve as hard negatives for the source image. To this aim, we adopt a conditional   \n121 latent diffusion model (such as Stable Diffusion, [81]) conditioned on a text prompt of the following   \n122 format \u201cA photo of a $T=<$ target category>\u201d.   \n123 Key Idea. GeNIe in its basic form is a simple yet effective augmentation sample generator for   \n124 improving a classifier $f_{\\theta}(.)$ with the following two key aspects: (i) inspired by [63, 67] instead of   \n125 adding the full amount of noise $\\sigma_{m a x}$ and going through all $N_{m a x}$ (being typically 50) steps of   \n126 denoising, we use less amount of noise ( $r\\sigma_{m a x}$ , with $r\\,\\in\\,(0,1))$ ) and consequently fewer number   \n127 of denoising iterations $\\left\\langle\\left[r N_{m a x}\\right]\\right\\rangle$ ; (ii) we prompt the diffusion model with a $P$ mandating a target   \n128 category $T$ different than the source $S$ . Hence, we denote the conditional diffusion process as   \n129 $X_{r}=\\tt S T D i f f(X_{S},P,r)$ . In such a construct, the proximity of the final decoded image $X_{r}$ to the   \n130 source image $X_{S}$ or the target category defined through the text prompt $P$ depends on $r$ . Hence, by   \n131 controlling the amount of noise, we can generate images that blend characteristics of both the text   \n132 prompt $P$ and the source image $X_{S}$ . If we do not provide much of visual details in the text prompt   \n133 (e.g., desired background, etc.), we expect the decoded image $X_{r}$ to follow the details of $X_{S}$ while   \n134 reflecting the semantics of the text prompt $P$ . We argue, and demonstrate later, that the newly   \n135 generated samples can serve as hard negative examples for the source category $S$ since they share   \n136 the low-level features of $X_{S}$ while representing the semantics of the target category, $T$ . Notably, the   \n137 source category $S$ can be randomly sampled or be carefully extracted from the confusion matrix of   \n138 $f_{\\theta}(.)$ based on real training data. The latter might result in even harder negative samples being now   \n139 cognizant of model confusions. Finally, we will append our initial dataset with the newly generated   \n140 hard negative samples through GeNIe and (re)train the classifier model.   \n141 Enhancing GeNIe: GeNIe-Ada. One of the remarkable aspects of GeNIe lies in its simple applica  \n142 tion, requiring only $X_{S}$ , $P$ , and $r$ . However, selecting the appropriate value for $r$ poses a challenge   \n143 as it profoundly influences the outcome. When $r$ is small, the resulting $X_{r}$ tends to closely resemble   \n144 $X_{S}$ , and conversely, when $r$ is large (closer to 1), it tends to resemble the semantics of the target   \n145 category. This phenomenon arises because a smaller noise level restricts the capacity of the diffusion   \n146 model to deviate from the semantics of the input $X_{S}$ . Thus, a critical question emerges: how can we   \n147 select $r$ for a particular source image to generate samples that preserve the low-level semantics of   \n148 the source category $S$ in $X_{S}$ while effectively representing the semantics of the target category $T?$   \n149 We propose a method to determine an ideal value for $r$ .   \n150 Our intuition suggests that by varying the noise ratio $r$ from 0 to 1, $X_{r}$ will progressively resemble   \n151 category $S$ in the beginning and category $T$ towards the end. However, somewhere between 0   \n152 and 1, $X_{r}$ will undergo a rapid transition from category $S$ to $T$ . This phenomenon is empirically   \n153 observed in our experiments with varying $r$ , as depicted in Fig. 2. Although the exact reason for this   \n154 rapid change remains uncertain, one possible explanation is that the intermediate points between   \n155 two categories reside far from the natural image manifold, thus, challenging the diffusion model\u2019s   \n156 capability to generate them. Ideally, we should select $r$ corresponding to just after this rapid semantic   \n157 transition, as at this point, $X_{r}$ exhibits the highest similarity to the source image while belonging to   \n158 the target category.   \n159 We propose to trace the semantic trajectory between $X_{S}$ and $X_{T}$ through the lens of the classifier   \n160 $f_{\\theta}({\\bar{.}})$ . As shown in Algorithm 1, assuming access to the classifier backbone $f_{\\theta}(.)$ and at least one   \n161 example $X_{T}$ from the target category, we convert both $X_{S}$ and $X_{T}$ into their respective latent vectors   \n162 $Z_{S}$ and $Z_{T}$ by passing them through $f_{\\theta}(.)$ . Then, we sample $M$ values for $r$ uniformly distributed   \n163 $\\in(0,1)$ , generating their corresponding $X_{r}$ and their latent vectors $Z_{r}$ for all those $r$ . Subsequently,   \n164 we calculate $\\begin{array}{r}{d_{r}=\\frac{(Z_{r}-Z_{S})^{T}(Z_{T}-Z_{S})}{||Z_{T}-Z_{S}||_{2}}}\\end{array}$ as the distance between $Z_{r}$ and $Z_{S}$ projected onto the vector   \n165 connecting $Z_{S}$ and $Z_{T}$ . Our hypothesis posits that the rapid semantic transition corresponds to a   \n166 sharp change in this projected distance. Therefore, we sample $n$ values for $r$ uniformly distributed   \n167 between 0 and 1, and analyze the variations in $d_{r}$ . We identify the largest gap in $d_{r}$ and select the $r$   \n168 value just after the gap when increasing $r$ , as detailed in Algorithm 1 and illustrated in Fig. 3. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/b66ec78eb655178cddd048facfc834dd3c7b127b9f677a7511e95d32273def65.jpg", "img_caption": ["Figure 2: Effect of noise ratio, $r$ , in GeNIe: we employ GeNIe to generate augmentations for the target classes (motorcycle and cat) with varying $r$ . Smaller $r$ yields images closely resembling the source semantics, creating an inconsistency with the intended target label. By tracing $r$ from 0 to 1, augmentations gradually transition from source image characteristics to the target category. However, a distinct shift from the source to the target occurs at a specific $r$ that may vary for different source images or target categories. For more examples, please refer to Fig. A4. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/4d579ddd761aaae9c4b343494a7baeda00d4cdcfe2d4691470942b3ab5139d95.jpg", "img_caption": ["Figure 3: GeNIe-Ada: To choose $r$ adaptively for each (source image, target category) pair, we propose tracing the semantic trajectory from $Z_{S}$ (source image embeddings) to $Z_{T}$ (target embeddings) through the lens of the classifier $f_{\\theta}(\\cdot)$ (Algorithm 1). We adaptively select the sample right after the largest semantic shift. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 Since the impact of augmentation is more pronounced when the training data is limited, we evaluate   \n171 the impact of GeNIe on Few-Shot classification in Section 4.1, Long-Tailed classification in Sec  \n172 tion 4.2, and fine-grained classification in Section A.2. For GeNIe-Ada in all scenarios, we utilize   \n173 GeNIe to generate augmentations from the noise level set $\\{0.5,0.6,0.7,0.8,0.9\\}$ . The selection of   \n174 the appropriate noise level per source image and target is adaptive, achieved through Algorithm 1.   \n175 Baselines. We use Stable Diffusion 1.5 [81] as our base diffusion model. In all settings,   \n176 we use the same prompt format to generate images for the target class: i.e., \u201cA photo of a   \n177 <target category $>^{,}$ , where we replace the target category with the target category label.   \n178 We generate $512\\times512$ images for all methods. For fairness in comparison, we generate the same   \n179 number of new images for each class. We use a single NVIDIA RTX 3090 for image generation.   \n180 We consider 4 diffusion-based baselines and a suite of traditional data augmentation baselines:   \n181 Img2Img [63, 67]: We sample an image from a target class, add noise to its latent representation and   \n182 then pass it along with a prompt for the target category through reverse diffusion. The focus here is   \n183 on a target class for which we generate extra positive samples. Adding large amount of noise leads   \n184 to generating an image less similar to the original image. We use two different noise magnitudes for   \n185 this baseline: $r=0.3$ and $r=0.7$ and denote them by $\\mathtt{I m g2I m g}^{L}$ and $\\mathtt{I m g2I m g}^{H}$ , respectively.   \n186 Txt2Img [4, 35]: For this baseline, we omit the forward diffusion process and only use the reverse   \n187 process starting from a text prompt for the target class of interest. This is similar to the base text  \n188 to-image generation strategy adopted in [81, 35, 89, 4, 62]. Fig. 4 illustrates a set of generated   \n189 augmentation examples for Txt2Img, Img2Img, and GeNIe.   \n190 DAFusion [100]: In this method, an embedding is optimized with a set of images for each class to   \n191 correspond to the classes in the dataset. This approach is introduced in Textual Inversion [32]. We   \n192 optimize an embedding for 5000 iterations for each class in the dataset, followed by augmentation   \n193 similar as the DAFusion method.   \n194 Cap2Aug[83]: It is a recent diffusion-based data augmentation strategy that uses image captions as   \n195 text prompts for an image-to-image diffusion model.   \n196 Traditional Data Augmentation: We consider both weak and strong traditional augmentations.   \n197 More specifically, for weak augmentation we use random resize crop with scaling $\\in[0.2,1.0]$ and   \n198 horizontal flipping. For strong augmentation, we consider random color jitter, random grayscale,   \n199 and Gaussian blur. For the sake of completeness, we also compare against data augmentations such   \n200 as CutMix [110] and MixUp [111] that combine two images together. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "201 4.1 Few-shot Classification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "202 We assess the impact of GeNIe compared to other augmentations in a number of few-shot classifica  \n203 tion (FSL) scenarios, where the model has to learn only from the samples contained in the ( $N$ -way,   \n204 $K$ -shot) support set and infer on the query set. Note that this corresponds to an inference-only FSL   \n205 setting where a pretraining stage on an abundant dataset is discarded. The goal is to assess how well   \n206 the model can benefit from the augmentations while keeping the original $N\\times K$ samples intact.   \n207 Datasets. We conduct our few-shot experiments on two most commonly adopted few-shot classi  \n208 fication datasets: mini-Imagenet [79] and tiered-Imagenet [80]. mini-Imagenet is a subset of Ima  \n209 geNet [22] for few-shot classification. It contains 100 classes with 600 samples each. We follow   \n210 the predominantly adopted settings of [79, 10] where we split the entire dataset into 64 classes for   \n211 training, 16 for validation and 20 for testing. tiered-Imagenet is a larger subset of ImageNet with   \n212 608 classes and a total of 779, 165 images, which are grouped into 34 higher-level nodes in the Im  \n213 ageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of   \n214 training, validation, and testing nodes, and the corresponding classes form the respective meta-sets.   \n215 Evaluation. To quantify the impact of different augmentation methods, we evaluate the test-set ac  \n216 curacies of a state-of-the-art unsupervised few-shot learning method with GeNIe and compare them   \n217 against the accuracies obtained using other augmentation methods. Specifically, we use UniSiam   \n218 [61] pre-trained with ResNet-18, ResNet-34 and ResNet-50 backbones and follow its evaluation   \n219 strategy of fine-tuning a logistic regressor to perform ( $N$ -way, $K$ -shot) classification on the test sets   \n220 of mini- and tiered-Imagenet. Following [79], an episode consists of a labeled support-set and an un  \n221 labelled query-set. The support-set contains $N$ randomly sampled classes where each class contains   \n222 $K$ samples, whereas the query-set contains $Q$ randomly sampled unlabeled images per class. We   \n223 conduct our experiments on the two most commonly adopted settings: (5-way, 1-shot) and (5-way,   \n224 5-shot) classification settings. Following the literature, we sample 16-shots per class for the query   \n225 set in both settings. We report the test accuracies along with the $95\\%$ confidence interval over 600   \n226 and 1000 episodes for mini-ImageNet and tiered-ImageNet, respectively.   \n227 Implementation Details: GeNIe generates augmented images for each class using images from all   \n228 other classes as the source image. We use $r=0.8$ in our experiments. We generate 4 samples per   \n229 class as augmentations in the 5-way, 1-shot setting and 20 samples per class as augmentations in the   \n230 5-way, 5-shot setting. For the sake of a fair comparison, we ensure that the total number of labelled   \n231 samples in the support set after augmentation remains the same across all different traditional and   \n232 generative augmentation methodologies. Due to the expensive training of embeddings for each class   \n233 in each episode, we only evaluated the DA-Fusion baseline on the first 100 episodes.   \n234 Results: The results on mini-Imagenet and tiered-Imagenet for both (5-way, 1 and 5-shot) set  \n235 tings are summarized in Table 1 and Table 2, respectively. Regardless of the choice of back  \n236 bone, we observe that GeNIe helps consistently improve UniSiam\u2019s performance and outperform   \n237 other supervised and unsupervised few-shot classification methods as well as other diffusion-based   \n238 [100, 63, 82, 35] and classical [110, 111] data augmentation techniques on both datasets, across both   \n239 (5-way, 1 and 5-shot) settings. Our noise adaptive method of selecting optimal augmentations per   \n240 source image (GeNIe-Ada) further improves GeNIe\u2019s performance across all three backbones, both   \n241 few-shot settings, and both datasets (mini and tiered-Imagenet). Few-shot accuracies for ResNet  \n242 34 computed on tieredImagenet are reported in Section A.3 of the appendix. Note that employing   \n243 CutMix and MixUp seems to lead to performance degradation compared to weak augmentations,   \n244 probably due to overfitting since these methods can only choose from 4 other classes to mix. ", "page_idx": 4}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/36621f2160db02aa4d9b832a01c43f36bd262f3337f6be43953264861afeb935.jpg", "img_caption": ["Figure 4: Visualization of Generative Samples: We compare GeNIe with two baselines: $\\mathtt{I m g2I m g}^{L}$ augmentation: both image and text prompt are from the same category. Adding noise does not change the image much, so they are not hard examples. Txt2Img augmentation: We simply use the text prompt only to generate an image for the desired category (e.g., using a text2image method). Such images may be far from the domain of our task since the generation is not informed by any visual data from our task. GeNIe augmentation: We use the target category name in the text prompt only along with the source image. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/4743448a6dc3c4aa906efcb6e427ead4cef3e433f1d599ea9a0857bbf16aab35.jpg", "table_caption": ["Table 1: mini-ImageNet: We use our augmentations on (5-way, 1-shot) and (5-way, 5-shot) few-shot settings of mini-Imagenet dataset with 3 different backbones (ResNet-18, 34, and 50). We compare with various baselines and show that our augmentations with UniSiam outperform all the baselines including $\\mathrm{Txt2Img}$ and DAFusion augmentation. The number of generated images per class is 4 for 1-shot and 20 for 5-shot settings. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "245 4.2 Long-Tailed Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "246 We evaluate our method on long-tailed data, where the number of instances per class is unbalanced,   \n247 with most categories having limited samples (tail). Our goal is to mitigate this bias by augmenting   \n248 the tail of the distribution with generated samples. We evaluate GeNIe using two different backbones   \n249 and methods: the ViT architecture with LViT [107], and ResNet50 with VL-LTR [97].   \n250 Following LViT [107], we first train an MAE [34] and ViT on the unbalanced dataset without any   \n251 augmentation. Next, we train the Balanced Fine-Tuning stage of LViT by incorporating the aug  \n252 mentation data generated using GeNIe or other baselines. For ResNet50, we use VL-LTR code to   \n253 fine-tune the CLIP [76] ResNet50 pretrained backbone with generated augmentations by GeNIe.   \n254 Dataset: We perform experiments on ImageNet-LT [60]. It contains 115.8K images from 1, 000   \n255 categories. The number of images per class varies from 1280 to 5. Imagenet-LT classes can be   \n256 divided into 3 groups: \u201cFew\u201d with less than 20 images, \u201cMed\u201d with $20-100$ images, and \u201cMany\u201d   \n257 with more than 100 images. Imagenet-LT uses the same validation set as ImageNet. We augment   \n258 \u201cFew\u201d categories only and limit the number of generated images to 50 samples per class. For GeNIe,   \n259 instead of randomly sampling the source images from other classes, we use a confusion matrix on   \n260 the training data to find the top-4 most confused classes and only consider those classes for random   \n261 sampling of the source image. The source category may be from \u201cMany\u201d, \u201cMed\u201d, or \u201cFew sets\u201d.   \n262 Results: Augmenting training data with GeNIe-Ada improves accuracy on the \u201cFew\u201d set by $11.7\\%$   \n263 and $4.4\\%$ compared with LViT only and LViT with Txt2Img augmentation baselines respectively.   \n264 In ResNet50, GeNIe-Ada outperforms Cap2Aug baseline in \u201cFew\u201d categories by $7.6\\%$ . The results   \n265 are summarized in Table 3. Please refer to Section A.4 for implementation details. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "266 4.3 Ablation and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "267 Semantic Shift from Source to Target Class. The core motivation behind GeNIe-Ada is that by   \n268 varying the noise ratio $r$ from 0 to 1, augmented sample $X_{r}$ will progressively shift its semantic cat  \n269 egory from source $(S)$ in the beginning to target category $(T)$ towards the end. However, somewhere   \n270 between 0 and 1, $X_{r}$ will undergo a rapid transition from $S$ to $T$ . To demonstrate this hypothesis   \n271 empirically, in Figs. 5 and A5, we visualize pairs of source images and target categories with their re  \n272 spective GeNIe generated augmentations for different noise ratios $r$ , along with their corresponding ", "page_idx": 6}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/0ef23eda2ccb19472a69098e33d04797e37f8a52c5c02f45c6c9e098b9ffc546.jpg", "table_caption": ["Table 2: tiered-ImageNet: Accuracies $(\\%\\pm\\mathrm{\\std})$ for 5-way, 1-shot and 5-way, 5-shot classification settings on the test-set. We compare against various SOTA supervised and unsupervised few-shot classification baselines as well as other augmentation methods, with UniSiam [61] pre-trained ResNet-18,50 backbones. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Long-Tailed ImageNet-LT: We compare different augmentation methods on ImageNet-LT and report Top-1 accuracy for \u201cFew\u201d, \u201cMedium\u201d, and \u201cMany\u201d sets. On the \u201cFew\u201d set and LiVT method, our augmentations improve the accuracy by 11.7 points compared to LiVT original augmentation and 4.4 points . ", "page_idx": 7}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/41a7f33f1f021208a2eabf19b76b83f43f5a7c6bdc1f3e9f97da71a244a32414.jpg", "img_caption": ["Figure 5: Embedding visualizations of generative augmentations: We pass all generative augmentations through DINOv2 ViT-G (serving as an oracle) to extract their corresponding embeddings and visualize them with PCA. As shown, the extent of semantic shifts varies based on both the source image and the target class. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "273 PCA-projected embedding scatter plots (on the far left). We extract embeddings for all the images   \n274 using a DINOv2 ViT-G pretrained backbone, which we assume as an oracle model in identifying   \n275 the right category. We observe that as $r$ increases from 0.3 to 0.8, the images transition to embody   \n276 more of the target category\u2019s semantics while preserving the contextual features of the source image.   \n277 This transition of semantics can also be observed in the embedding plots (on the left) where they   \n278 consistently shift from the proximity of the source image (blue star) to the target class\u2019s centroid   \n279 (red cross) as the noise ratio $r$ increases. The sparse distribution of points within $r=[0.4,0.6]$ for   \n280 the first image and $r=[0.2,0.4]$ for the second image aligns with our intuition of a rapid transition   \n281 from category $S$ to $T$ , thus empirically affirming our motivation behind GeNIe-Ada.   \n282 To further establish this, in Fig. 6, we demonstrate the efficacy of GeNIe in generating hard negatives   \n283 at the decision boundaries of an SVM classifier, which is trained on the labelled support set of   \n284 the few-shot tasks of mini-Imagenet, without any augmentations. We then plot source and target   \n285 class probabilities $(P(Y_{S}|X_{r})$ and $P(Y_{T}|X_{r})$ , respectively) of the generated augmentation samples   \n286 $X_{r}$ . For both $r\\:=\\:0.6$ and 0.7, there is significant overlap between $P(Y_{S}|\\bar{X_{r}})$ and $P(Y_{T}|\\bar{X}_{r})$ ,   \n287 making it difficult for the classifier to decide the correct class. On the right-hand-side, GeNIe-Ada   \n288 automatically selects the best $r$ resulting in the most overlap between the two distributions, thus   \n289 offering the hardest negative sample among the considered $r$ values (for more details see A.1).   \n290 Note that a large overlap between distributions is not sufficient to call the generated samples hard   \n291 negatives because they should also belong to the target category. This is, however, confirmed by the   \n292 high Oracle accuracy in Table 4 (elaborated in detail in the following paragraph) which verifies that   \n293 majority of the generated augmentation samples do belong to the target category. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/d7071bfce14a2a07bd4cde7fae80fce26ae68f8a63788e9aa6416e76781b1c4f.jpg", "img_caption": ["Figure 6: Why GeNIe augmentations are challenging? While deciding which class the generated augmentations $\\left(X_{r}\\right)$ belong to is already difficult within $r\\bar{=}\\,[\\bar{0}.6,0.7]$ (due to high overlap between $P(Y_{S}|X_{r}^{\\widecheck{}})$ and $P(Y_{T}|\\dot{X}_{r}))$ , GeNIe-Ada selects the best noise threshold $(r^{*})$ offering the hardest negative sample. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Effect of Noise in GeNIe: We use the same setting as in Table 1 to study the effect of the amount of noise. As expected (also shown in Fig 5), small noise results in worse accuracy since some generated images may be from the source category rather than the target one. For $r\\,=\\,0.5$ only $73\\%$ of the generated data is from the target category. This behaviour is also shown in Fig. 2. Notably, reducing the noise level below 0.7 is associated with a decline in oracle accuracy and subsequent degradation in the performance of the final fewshot model. Note that the high oracle accuracy of GeNIe-Ada demonstrates its capability to adaptively select the noise level per source and target, ensuring semantic consistency with the intended target. ", "page_idx": 8}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/ad0922484e10b746aea4d3d68d8f6b2f08e4369117ffe25bd6d82608bb563ef4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "294 Label consistency of the generated samples. The choice of noise ratio $r$ is important in producing   \n295 hard negative examples. In Table 4, we present the accuracy of the GeNIe model across various noise   \n296 ratios, alongside the oracle accuracy, which is an ImageNet pre-trained DeiT-Base [98] classifier.   \n297 We observe a decline in the label consistency of generated data (quantified by the performance of   \n298 the oracle model) when decreasing the noise level. Reducing $r$ also results in a degradation in the   \n299 performance of the final few-shot model $(87.2\\%\\rightarrow77.6\\%)$ corroborating that an appropriate choice   \n300 of $r$ plays a crucial role in our design strategy. We investigate this further in the following paragraph.   \n301 Effect of Noise in GeNIe. We examine the impact of noise on the performance of the few-shot   \n302 model in Table 4. Noise levels $r\\in[0.7,0.8]$ yield the best performance. Conversely, utilizing noise   \n303 levels below 0.7 diminishes performance due to label inconsistency, as is demonstrated in Table 4   \n304 and Fig 5. As such, determining the appropriate noise level is pivotal for the performance of GeNIe   \n305 to be able to generate challenging hard negatives while maintaining label consistency. An alternative   \n306 approach to finding the optimal noise level involves using GeNIe-Ada to adaptively select the noise   \n307 level for each source image and target class. As demonstrated in Tables 4 and A1, GeNIe-Ada   \n308 achieves performance that is comparable to or surpasses that of GeNIe with fixed noise levels. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "309 5 Concluding Remarks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 GeNIe, for the first time to our knowledge, combines contradictory sources of information (a source   \n311 image, and a different target category prompt) through a noise adjustment strategy into a conditional   \n312 latent diffusion model to generate challenging augmentations, which can serve as hard negatives.   \n313 Limitation. The required time to create augmentations through GeNIe is on par with any typical   \n314 diffusion-based competitors [4, 35]; however, this is naturally slower than traditional augmentation   \n315 techniques [110, 111]. This is not a bottleneck in offline augmentation strategies, but can be con  \n316 sidered a limiting factor in real-time scenarios. Recent studies are already mitigating this through   \n317 advancements in diffusion model efficiency [87, 68, 58]. Another challenge present in any genera  \n318 tive AI-based augmentation technique is the domain shift between the distribution of training data   \n319 and the downstream context they might be used for augmentation. A possible remedy is to fine-tune   \n320 the diffusion backbone on a rather small dataset from the downstream task.   \n321 Broader Impact. We believe ideas from GeNIe can have a significant impact when it comes to gen  \n322 erating hard augmentations challenging and thus enhancing downstream tasks beyond classification.   \n323 At the same time, just like any other generative model, GeNIe can also introduce inherent biases   \n324 stemming from the training data used to build its diffusion backbone, which can reflect and amplify   \n325 societal prejudices or inaccuracies. Therefore, it is crucial to carefully mitigate potential biases in   \n326 generative models such as GeNIe to ensure a fair and ethical deployment of deep learning systems. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "327 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "328 [1] Afrasiyabi, A., Lalonde, J.F., Gagne\u00b4, C.: Associative alignment for few-shot image classifi  \n329 cation. In: ECCV (2019)   \n330 [2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A.,   \n331 Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Saman  \n332 gooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,   \n333 S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: a   \n334 visual language model for few-shot learning (2022)   \n335 [3] Antoniou, A., Storkey, A.: Assume, augment and learn: Unsupervised few-shot meta  \n336 learning via random labels and data augmentation. arxiv:1902.09884 (2019)   \n337 [4] Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., Fleet, D.J.: Synthetic data from diffusion   \n338 models improves imagenet classification (2023)   \n339 [5] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 \u2013 mining discriminative components   \n340 with random forests. In: European Conference on Computer Vision (2014)   \n341 [6] Cai, J., Wang, Y., Hwang, J.N., et al.: Ace: Ally complementary experts for solving long  \n342 tailed recognition in one-shot. In: ICCV. pp. 112\u2013121 (2021)   \n343 [7] Cao, K., Wei, C., Gaidon, A., Arechiga, N., Ma, T.: Learning imbalanced datasets with label  \n344 distribution-aware margin loss. NeurIPS 32 (2019)   \n345 [8] Chegini, A., Feizi, S.: Identifying and mitigating model failures through few-shot clip-aided   \n346 diffusion generation. arXiv preprint arXiv:2312.05464 (2023)   \n347 [9] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning   \n348 of visual representations. In: ICML (2020)   \n349 [10] Chen, W.Y., Liu, Y.C., Kira, Z., Wang, Y.C.F., Huang, J.B.: A closer look at few-shot classi  \n350 fication. In: ICLR (2019)   \n351 [11] Chen, W., Si, C., Wang, W., Wang, L., Wang, Z., Tan, T.: Few-shot learning with part discov  \n352 ery and augmentation from unlabeled images. arXiv preprint arXiv:2105.11874 (2021)   \n353 [12] Chen, X., He, K.: Exploring simple siamese representation learning. In: CVPR (2021)   \n354 [13] Chen, Z., Ge, J., Zhan, H., Huang, S., Wang, D.: Pareto self-supervised training for few-shot   \n355 learning. In: CVPR (2021)   \n356 [14] Chen, Z., Fu, Y., Wang, Y.X., Ma, L., Liu, W., Hebert, M.: Image deformation meta-networks   \n357 for one-shot learning. In: CVPR (2019)   \n358 [15] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmen  \n359 tation policies from data (2019)   \n360 [16] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data aug  \n361 mentation with a reduced search space (2019)   \n362 [17] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data aug  \n363 mentation with a reduced search space. In: Larochelle, H., Ranzato, M., Hadsell, R.,   \n364 Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33,   \n365 pp. 18613\u201318624. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/   \n366 paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf   \n367 [18] Cui, J., Liu, S., Tian, Z., Zhong, Z., Jia, J.: Reslt: Residual learning for long-tailed recogni  \n368 tion. IEEE transactions on pattern analysis and machine intelligence 45(3), 3695\u20133706 (2022)   \n369 [19] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: Proceedings of   \n370 the IEEE/CVF international conference on computer vision. pp. 715\u2013724 (2021)   \n371 [20] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: ICCV. pp.   \n372 715\u2013724 (2021)   \n373 [21] Cui, Y., Jia, M., Lin, T.Y., Song, Y., Belongie, S.: Class-balanced loss based on effective   \n374 number of samples. In: CVPR. pp. 9268\u20139277 (2019)   \n375 [22] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierar  \n376 chical image database. In: 2009 IEEE conference on computer vision and pattern recognition.   \n377 pp. 248\u2013255. Ieee (2009)   \n378 [23] Ding, M., An, B., Xu, Y., Satheesh, A., Huang, F.: SAFLEX: Self-adaptive augmentation via   \n379 feature label extrapolation. In: The Twelfth International Conference on Learning Represen  \n380 tations (2024), https://openreview.net/forum?id=qL6brrBDk2   \n381 [24] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De  \n382 hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is   \n383 worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)   \n384 [25] Dunlap, L., Mohri, C., Zhang, H., Guillory, D., Darrell, T., Gonzalez, J.E., Rohrbach, A.,   \n385 Raghunathan, A.: Using language to extend to unseen domains. International Conference on   \n386 Learning Representations (ICLR) (2023)   \n387 [26] Dunlap, L., Umino, A., Zhang, H., Yang, J., Gonzalez, J.E., Darrell, T.: Diversify your vision   \n388 datasets with automatic diffusion-based augmentation (2023)   \n389 [27] Dvornik, N., Mairal, J., Schmid, C.: Diversity with cooperation: Ensemble methods for few  \n390 shot classification. In: ICCV (2019)   \n391 [28] Feng, C.M., Yu, K., Liu, Y., Khan, S., Zuo, W.: Diverse data augmentation with diffusions   \n392 for effective test-time prompt tuning (2023)   \n393 [29] Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep   \n394 networks. In: Proceedings of the 34th International Conference on Machine Learning. pp.   \n395 1126\u20131135 (2017)   \n396 [30] Frid-Adar, M., Diamant, I., Klang, E., Amitai, M., Goldberger, J., Greenspan, H.: Gan  \n397 based synthetic medical image augmentation for increased cnn performance in liver lesion   \n398 classification. Neurocomputing (2018)   \n399 [31] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.:   \n400 An image is worth one word: Personalizing text-to-image generation using textual inversion.   \n401 arXiv preprint arXiv:2208.01618 (2022)   \n402 [32] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or,   \n403 D.: An image is worth one word: Personalizing text-to-image generation using textual   \n404 inversion (2022). https://doi.org/10.48550/ARXIV.2208.01618, https://arxiv.org/abs/   \n405 2208.01618   \n406 [33] He, K., Chen, X., Xie, S., Li, Y., Dolla\u00b4r, P., Girshick, R.B.: Masked autoencoders are scalable   \n407 vision learners. In: CVPR. pp. 15979\u201315988. IEEE (2022)   \n408 [34] He, K., Chen, X., Xie, S., Li, Y., Dolla\u00b4r, P., Girshick, R.: Masked autoencoders are scalable   \n409 vision learners (2021)   \n410 [35] He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., Qi, X.: Is synthetic data from   \n411 generative models ready for image recognition? arXiv preprint arXiv:2210.07574 (2022)   \n412 [36] Hemmat, R.A., Pezeshki, M., Bordes, F., Drozdzal, M., Romero-Soriano, A.: Feedback  \n413 guided data synthesis for imbalanced classification (2023)   \n414 [37] Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: AugMix:   \n415 A simple data processing method to improve robustness and uncertainty. Proceedings of the   \n416 International Conference on Learning Representations (ICLR) (2020)   \n417 [38] Hong, Y., Zhang, J., Sun, Z., Yan, K.: Safa: Sample-adaptive feature augmentation for long  \n418 tailed image classification. In: ECCV (2022)   \n419 [39] Hsu, K., Levine, S., Finn, C.: Unsupervised learning via meta-learning. In: ICLR (2018)   \n420 [40] Hu, W., Jiang, X., Liu, J., Yang, Y., Tian, H.: Meta-dm: Applications of diffusion models on   \n421 few-shot learning (2023)   \n422 [41] Huang, S.W., Lin, C.T., Chen, S.P., an Po-Hao Hsu, Y.Y.W., Lai, S.H.: Auggan: Cross do  \n423 main adaptation with gan-based data augmentation. European Conference on Computer Vi  \n424 sion (2018)   \n425 [42] Jain, S., Lawrence, H., Moitra, A., Madry, A.: Distilling model failures as directions in latent   \n426 space. In: ArXiv preprint arXiv:2206.14754 (2022)   \n427 [43] Jang, H., Lee, H., Shin, J.: Unsupervised meta-learning via few-shot pseudo-supervised con  \n428 trastive learning. In: The Eleventh International Conference on Learning Representations   \n429 (2022)   \n430 [44] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.: Decoupling rep  \n431 resentation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217 (2019)   \n432 [45] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.: Decoupling   \n433 representation and classifier for long-tailed recognition. In: ICLR (2020)   \n434 [46] Khodadadeh, S., Boloni, L., Shah, M.: Unsupervised meta-learning for few-shot image clas  \n435 sification. In: NeurIPS (2019)   \n436 [47] Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for   \n437 optimal mixup. In: International Conference on Machine Learning. pp. 5275\u20135285. PMLR   \n438 (2020)   \n439 [48] Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3D object representations for fine-grained catego  \n440 rization. In: Workshop on 3D Representation and Recognition. Sydney, Australia (2013)   \n441 [49] Li, A.C., Prabhudesai, M., Duggal, S., Brown, E., Pathak, D.: Your diffusion model is secretly   \n442 a zero-shot classifier (2023)   \n443 [50] Li, B., Han, Z., Li, H., Fu, H., Zhang, C.: Trustworthy long-tailed classification. In: CVPR.   \n444 pp. 6970\u20136979 (2022)   \n445 [51] Li, D., Ling, H., Kim, S.W., Kreis, K., Barriuso, A., Fidler, S., Torralba, A.: Bigdatasetgan:   \n446 Synthesizing imagenet with pixel-wise annotations (2022)   \n447 [52] Li, J., Tan, Z., Wan, J., Lei, Z., Guo, G.: Nested collaborative learning for long-tailed visual   \n448 recognition. In: CVPR. pp. 6949\u20136958 (2022)   \n449 [53] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified   \n450 vision-language understanding and generation (2022)   \n451 [54] Li, K., Zhang, Y., Li, K., Fu, Y.: Adversarial feature hallucination networks for few-shot   \n452 learning. In: CVPR (2020)   \n453 [55] Li, M., Cheung, Y.m., Lu, Y., et al.: Long-tailed visual recognition via gaussian clouded logit   \n454 adjustment. In: CVPR. pp. 6929\u20136938 (2022)   \n455 [56] Li, T., Cao, P., Yuan, Y., Fan, L., Yang, Y., Feris, R.S., Indyk, P., Katabi, D.: Targeted   \n456 supervised contrastive learning for long-tailed recognition. In: CVPR. pp. 6918\u20136928 (2022)   \n457 [57] Liu, B., Cao, Y., Lin, Y., Li, Q., Zhang, Z., Long, M., Hu, H.: Negative margin matters:   \n458 Understanding margin in few-shot classification. In: ECCV (2020)   \n459 [58] Liu, X., Zhang, X., Ma, J., Peng, J., et al.: Instaflow: One step is enough for high-quality   \n460 diffusion-based text-to-image generation. In: The Twelfth International Conference on Learn  \n461 ing Representations (2023)   \n462 [59] Liu, Z., Li, S., Wu, D., Liu, Z., Chen, Z., Wu, L., Li, S.Z.: Automix: Unveiling the power of   \n463 mixup for stronger classifiers. In: Computer Vision\u2013ECCV 2022: 17th European Conference,   \n464 Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV. pp. 441\u2013458. Springer (2022)   \n465 [60] Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., Yu, S.X.: Large-scale long-tailed recognition   \n466 in an open world. In: CVPR (2019)   \n467 [61] Lu, Y., Wen, L., Liu, J., Liu, Y., Tian, X.: Self-supervision can be a good few-shot learner. In:   \n468 European Conference on Computer Vision. pp. 740\u2013758. Springer (2022)   \n469 [62] Luo, X.J., Wang, S., Wu, Z., Sakaridis, C., Cheng, Y., Fan, D.P., Gool, L.V.: Camdiff: Cam  \n470 ouflage image augmentation via diffusion model (2023)   \n471 [63] Luzi, L., Siahkoohi, A., Mayer, P.M., Casco-Rodriguez, J., Baraniuk, R.: Boomerang: Local   \n472 sampling on image manifolds using diffusion models (2022)   \n473 [64] Maji, S., Rahtu, E., Kannala, J., Blaschko, M.B., Vedaldi, A.: Fine-grained visual classifica  \n474 tion of aircraft. arXiv preprint arXiv:1306.5151 (2013)   \n475 [65] Mao, J., Xiao, T., Jiang, Y., Cao, Z.: What can help pedestrian detection? (2017)   \n476 [66] Medina, C., Devos, A., Grossglauser, M.: Self-supervised prototypical transfer learning for   \n477 few-shot classification. In: ICMLW (2020)   \n478 [67] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided image   \n479 synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073   \n480 (2021)   \n481 [68] Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., Salimans, T.: On distilla  \n482 tion of guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer   \n483 Vision and Pattern Recognition. pp. 14297\u201314306 (2023)   \n484 [69] Menon, A.K., Jayasumana, S., Rawat, A.S., Jain, H., Veit, A., Kumar, S.: Long-tail learning   \n485 via logit adjustment. In: ICLR (2021)   \n486 [70] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P.,   \n487 Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang,   \n488 P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal,   \n489 J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without   \n490 supervision (2023)   \n491 [71] Peebles, W., Zhu, J.Y., Zhang, R., Torralba, A., Efros, A., Shechtman, E.: Gan-supervised   \n492 dense visual alignment. In: CVPR (2022)   \n493 [72] Petryk, S., Dunlap, L., Nasseri, K., Gonzalez, J., Darrell, T., Rohrbach, A.: On guid  \n494 ing visual attention with language specification. In: Conference on Computer Vision and   \n495 Pattern Recognition (CVPR) (2022). https://doi.org/10.48550/ARXIV.2202.08926, https:   \n496 //arxiv.org/abs/2202.08926   \n497 [73] Prabhu, V., Yenamandra, S., Chattopadhyay, P., Hoffman, J.: Lance: Stress-testing visual   \n498 models by generating language-guided counterfactual images. Advances in Neural Informa  \n499 tion Processing Systems 36 (2024)   \n500 [74] Qiao, S., Liu, C., Shen, W., Yuille, A.: Few-shot image recognition by predicting parameters   \n501 from activations. In: CVPR (2018)   \n502 [75] Qin, T., Li, W., Shi, Y., Yang, G.: Unsupervised few-shot learning via distribution shift-based   \n503 augmentation. arxiv:2004.05805 (2020)   \n504 [76] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,   \n505 A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models   \n506 from natural language supervision. In: ICML (2021)   \n507 [77] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image   \n508 generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022)   \n509 [78] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.:   \n510 Zero-shot text-to-image generation. In: ICML (2021)   \n511 [79] Ravi, S., Larochelle, H.: Optimization as a model for few-shot learning. In: ICLR (2017)   \n512 [80] Ren, M., Ravi, S., Triantafillou, E., Snell, J., Swersky, K., Tenenbaum, J.B., Larochelle,   \n513 H., Zemel, R.S.: Meta-learning for semi-supervised few-shot classification. In: International   \n514 Conference on Learning Representations (2018), https://openreview.net/forum?id=   \n515 HJcSzz-CZ   \n516 [81] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn  \n517 thesis with latent diffusion models. In: CVPR (2022)   \n518 [82] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn  \n519 thesis with latent diffusion models (2021)   \n520 [83] Roy, A., Shah, A., Shah, K., Roy, A., Chellappa, R.: Cap2aug: Caption guided image to   \n521 image data augmentation (2023)   \n522 [84] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gon  \n523 tijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion   \n524 models with deep language understanding. Advances in Neural Information Processing Sys  \n525 tems 35, 36479\u201336494 (2022)   \n526 [85] Samuel, D., Chechik, G.: Distributional robustness loss for long-tail learning. In: ICCV   \n527 (2021)   \n528 [86] Sankaranarayanan, S., Balaji, Y., Castillo, C.D., Chellappa, R.: Generate to adapt: Aligning   \n529 domains using generative adversarial networks. Conference on Computer Vision and Pattern   \n530 Recognition (CVPR) (2018)   \n531 [87] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. arXiv   \n532 preprint arXiv:2311.17042 (2023)   \n533 [88] Sharmanska, V., Hendricks, L.A., Darrell, T., Quadrianto, N.: Contrastive examples for ad  \n534 dressing the tyranny of the majority. CoRR abs/2004.06524 (2020), https://arxiv.org/   \n535 abs/2004.06524   \n536 [89] Shipard, J., Wiliem, A., Thanh, K.N., Xiang, W., Fookes, C.: Boosting zero-shot classification   \n537 with synthetic data diversity via stable diffusion. arXiv preprint arXiv:2302.03298 (2023)   \n538 [90] Shirekar, O.K., Singh, A., Jamali-Rad, H.: Self-attention message passing for contrastive   \n539 few-shot learning. In: Proceedings of the IEEE/CVF Winter Conference on Applications of   \n540 Computer Vision (WACV). pp. 5426\u20135436 (January 2023)   \n541 [91] Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning.   \n542 Journal of big data 6(1), 1\u201348 (2019)   \n543 [92] Singh, A.R., Jamali-Rad, H.: Transductive decoupled variational inference for few-shot   \n544 classification. Transactions on Machine Learning Research (2023), https://openreview.   \n545 net/forum?id=bomdTc9HyL   \n546 [93] Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning. In: Advances   \n547 in Neural Information Processing Systems (2017)   \n548 [94] Su, J.C., Maji, S., Hariharan, B.: When does self-supervision improve few-shot learning? In:   \n549 ECCV (2020)   \n550 [95] Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M.: Learning to compare:   \n551 Relation network for few-shot learning. In: CVPR (2018)   \n552 [96] Tang, K., Huang, J., Zhang, H.: Long-tailed classification by keeping the good and removing   \n553 the bad momentum causal effect. NeurIPS 33, 1513\u20131524 (2020)   \n554 [97] Tian, C., Wang, W., Zhu, X., Dai, J., Qiao, Y.: Vl-ltr: Learning class-wise visual-linguistic   \n555 representation for long-tailed visual recognition. In: ECCV 2022 (2022)   \n556 [98] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Je\u00b4gou, H.: Training data  \n557 efficient image transformers and distillation through attention (2021)   \n558 [99] Touvron, H., Cord, M., Je\u00b4gou, H.: Deit iii: Revenge of the vit. In: ECCV (2022)   \n559 [100] Trabucco, B., Doherty, K., Gurinas, M.A., Salakhutdinov, R.: Effective data augmentation   \n560 with diffusion models. In: The Twelfth International Conference on Learning Representations   \n561 (2024), https://openreview.net/forum?id=ZWzUA9zeAg   \n562 [101] Tritrong, N., Rewatbowornwong, P., Suwajanakorn, S.: Repurposing gans for one-shot se  \n563 mantic part segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition   \n564 (CVPR) (2021)   \n565 [102] Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd birds-200-2011   \n566 dataset (2011)   \n567 [103] Wang, H., Deng, Z.H.: Contrastive prototypical network with wasserstein confidence penalty.   \n568 In: European Conference on Computer Vision. pp. 665\u2013682. Springer (2022)   \n569 [104] Wang, H., Fu, S., He, X., Fang, H., Liu, Z., Hu, H.: Towards calibrated hyper-sphere repre  \n570 sentation via distribution overlap coefficient for long-tailed learning. In: ECCV (2022)   \n571 [105] Wang, X., Lian, L., Miao, Z., Liu, Z., Yu, S.X.: Long-tailed recognition by routing diverse   \n572 distribution-aware experts. In: ICLR. OpenReview.net (2021)   \n573 [106] Xu, Y., Li, Y.L., Li, J., Lu, C.: Constructing balance from imbalance for long-tailed image   \n574 recognition. In: ECCV. pp. 38\u201356. Springer (2022)   \n575 [107] Xu, Z., Liu, R., Yang, S., Chai, Z., Yuan, C.: Learning imbalanced data with vision trans  \n576 formers (2023)   \n577 [108] Xuan, H., Stylianou, A., Liu, X., Pless, R.: Hard negative examples are hard, but useful   \n578 (2021)   \n579 [109] Ye, H.J., Hu, H., Zhan, D.C., Sha, F.: Few-shot learning via embedding adaptation with   \n580 set-to-set functions. In: CVPR (2020)   \n581 [110] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to   \n582 train strong classifiers with localizable features. In: ICCV. pp. 6023\u20136032 (2019)   \n583 [111] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk mini  \n584 mization. In: ICLR (2018)   \n585 [112] Zhang, S., Li, Z., Yan, S., He, X., Sun, J.: Distribution alignment: A unified framework for   \n586 long-tail visual recognition. In: CVPR. pp. 2361\u20132370 (2021)   \n587 [113] Zhang, Y., Hooi, B., Hong, L., Feng, J.: Test-agnostic long-tailed recognition by test-time   \n588 aggregating diverse experts with self-supervision. arXiv preprint arXiv:2107.09249 (2021)   \n589 [114] Zhang, Y., Ling, H., Gao, J., Yin, K., Lafleche, J.F., Barriuso, A., Torralba, A., Fidler, S.:   \n590 Datasetgan: Efficient labeled data factory with minimal human effort. In: CVPR (2021)   \n591 [115] Zhong, Z., Cui, J., Liu, S., Jia, J.: Improving calibration for long-tailed recognition. In:   \n592 CVPR. pp. 16489\u201316498. Computer Vision Foundation / IEEE (2021)   \n593 [116] Zhou, Z., Qiu, X., Xie, J., Wu, J., Zhang, C.: Binocular mutual learning for improving few  \n594 shot classification. In: ICCV (2021)   \n595 [117] Zhu, J., Wang, Z., Chen, J., Chen, Y.P.P., Jiang, Y.G.: Balanced contrastive learning for long  \n596 tailed visual recognition. In: CVPR. pp. 6908\u20136917 (2022) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "598 A.1 Analyzing GeNIe, GeNIe-Ada\u2019s Class-Probabilities ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "599 The core aim of GeNIe and GeNIe-Ada is to address the failure modes of a classifier   \n600 by generating challenging samples located near the decision boundary of each class pair,   \n601 which facilitates the learning process in effectively enhancing the decision boundary between   \n602 classes. As summarized in Table 4 and illustrated in Fig. 5, we have empirically corrob  \n603 orated that GeNIe and GeNIe-Ada can respectively produce samples $X_{r},X_{r^{*}}$ that are nega  \ntive with respect to the source image $X_{S}$ , while semantically belonging to the class $T$ . To ", "page_idx": 15}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/9e006accb94f15b53f6d7ae8b7ac9be47f03ae2d2fb0268b7010952a5d12ba56.jpg", "img_caption": ["Figure A1: $P(Y_{S}|X_{r})$ and $P(Y_{T}|X_{r})$ for $r\\in\\{0.5,0.6,0.7,0.8,0.9\\}$ . On average, the classifier confidently predicts the source class more than the target class for $X_{r}$ for $r\\;=\\;0.5$ , and vice-versa for $r\\,=\\,0.8,0.9$ . However, for $r=0.6,0.7$ , the classifier struggles to classify $X_{r}$ , indicating that the augmented samples are located closer to the decision boundary. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "further analyze the effectiveness of GeNIe and GeNIe-Ada, we compare the source classprobabilities $P(Y_{S}|X_{r})$ and target-class probabilities $P(Y_{S}|X_{r})$ of augmented samples $X_{r}$ . 607 To compute these class probabilities, we first fit an SVM classifier ", "page_idx": 15}, {"type": "text", "text": "604   \n605   \n606   \n608   \n609   \n610   \n611   \n612   \n613   \n614   \n615   \n616   \n617   \n618 ", "page_idx": 15}, {"type": "text", "text": "(as followed in UniSiam [61]) only on the labelled support set embeddings of each episode in the miniImagenet test dataset. Then, we perform inference using each episode\u2019s SVM classifier on its respective $X_{r}$ \u2019s and extract its class probabilities of belonging to its source class $S$ and target class $T$ . These per augmentation-sample source and target class probabilities are then averaged for each episode for each $r\\,\\in\\,\\{0.5,0.6,0.7,0.8,0.9\\}$ in the case of GeNIe and for the optimal $r\\,=\\,r^{*}$ per sample in the case of GeNIe-Ada, plotted as density plots in Fig. A1, Fig. A2, respectively. Fig. A1 illustrates that $\\dot{P}(\\bar{Y}_{S}|X_{r})$ and $P(Y_{T}|\\bar{X}_{r})$ have significant overlap in the case of $r\\in\\{0.6,0.7\\}$ indicating class-confusion for $X_{r}$ . ", "page_idx": 15}, {"type": "text", "text": "619 Furthermore, Fig. A2 illustrates that when using the optimal $r=r^{*}$   \n620 found by GeNIe-Ada per sample, $P(Y_{S}|X_{r})$ and $P(\\bar{Y}_{T}|X_{r})$ signif  \n621 icantly overlap around probability scores of $0.2\\mathrm{~-~}0.45$ , indicating   \n622 class confusion for GeNIe-Ada augmentations. This corroborates   \n623 with our analysis in Section 4.3, Table 4 and additionally empiri  \n624 cally proves that the augmented samples generated by GeNIe for   \n625 $r\\in\\bar{\\{0.6,0.7\\}}$ and GeNIe-Ada for $r=r^{*}$ are actually located near   \n626 the decision boundary of each class pair. ", "page_idx": 15}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/a44981bd6531b079f12859fbc490d56f0c753548dc19f0e54cc4bc06dfce9819.jpg", "img_caption": ["Figure A2: Significant overlap between $P\\overline{{(Y_{S}|X_{r^{*}})}}$ and $\\dot{P[\\gamma_{T}|}X_{r^{*}})$ indicates high classconfusion for augmented samples generated by GeNIe-Ada. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "627 A.2 Fine-grained Few-shot Classification ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "628 To further investigate the impact of the proposed method, we compare GeNIe with other text-based   \n629 data augmentation techniques across four distinct fine-grained datasets in a 20-way, 1-shot classifi  \n630 cation setting. We employ the pre-trained DINOV2 ViT-G [70] backbone as a feature extractor to   \n631 derive features from training images. Subsequently, an SVM classifier is trained on these features,   \n632 and we report the Top-1 accuracy of the model on the test set.   \n633 Datasets: We assess our method on several datasets: Food101 [5] with 101 classes of various foods,   \n634 CUB200 [102] with 200 bird species classes, Cars196 [48] with 196 car model classes, and FGVC  \n635 Aircraft [64] with 41 aircraft manufacturer classes. We provide detailed information around fine  \n636 grained datasets in Table A2. The reported metric is the average Top-1 accuracy over 100 episodes.   \n637 Each episode involves sampling 20 classes and 1-shot from the training set, with the final model   \n638 evaluated on the respective test set.   \n639 Implementation Details: We enhance the basic prompt by incorporating the superclass name for   \n640 the fine-grained dataset: \u201cA photo of a $<$ <target class>, a type of $<$ <superclass>\u201d. For instance,   \n641 in the food dataset and the burger class, our prompt reads: \u201cA photo of a burger, a type of food.\u201d No   \n642 additional augmentation is used for generative methods in this context. We generate 19 samples for   \n643 both cases of our method and also the baseline with weak augmentation.   \n644 Results: Table A1 summarizes the results. GeNIe helps outperform all other baselines and aug  \n645 mentations, including $\\mathrm{Txt2Img}$ , by margins upto $0.5\\%$ on CUB200 [102], $6.6\\%$ on Cars196 [48],   \n646 $0.1\\%$ on Food101 [5] and $5.3\\%$ on FGVC-Aircraft [64]. Notably, GeNIe exhibits great effectiveness   \n647 in more challenging datasets, outperforming the baseline with traditional augmentation by about   \n648 $38\\%$ for the Cars dataset and by roughly $17\\bar{\\%}$ for the Aircraft dataset. It can be observed here that   \n649 GeNIe-Ada performs on-par with GeNIe with a fixed noise level, eliminating the necessity for noise   \n650 level search in GeNIe. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/3d8a3e38889d2f90f4454c543e5294707b93b996b6ad1dcd81fd74a6ee6ae3a4.jpg", "table_caption": ["Table A1: Few-shot Learning on Fine-grained dataset: We utilize an SVM classifier trained atop the DINOV2 ViT-G pretrained backbone, reporting Top-1 accuracy for the test set of each dataset. The baseline is an SVM trained on the same backbone using weak augmentation. Across all datasets, GeNIe surpasses this baseline. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/417dfe95afc580afb92a801c306279d5560406c7b20c9fecd512053af699c99d.jpg", "table_caption": ["Table A2: Train and test split details of the fine-grained datasets. We use the provided train set for few-shot task generation, and the provided test sets for our evaluation. For the Aircraft dataset we use manufacturer hierarchy. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "651 A.3 Few-shot Classification with ResNet-34 on tieredImagenet ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "652 We follow the same evaluation protocol here as mentioned in section 4.1. As summarized in Ta  \n653 ble A3, GeNIe and GeNIe-Ada outperform all other classical and generative data augmentation   \n654 techniques. ", "page_idx": 16}, {"type": "text", "text": "655 A.4 Additional details of Long-Tail experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "656 We present a comprehensive version of Table 3 to benchmark the performance with different back  \n657 bone architectures (e.g., ResNet50) and to compare against previous long-tail baselines; this is de  \n658 tailed in Table A4.   \n659 Implementation Details of LViT: We download the pre-trained ViT-B of LViT [107] and finetune   \n660 it with Bal-BCE loss proposed therein on the augmented dataset. Training takes 2 hours on four   \n661 NVIDIA RTX 3090 GPUs. We use the same hyperparameters as in [107] for finetuning: 100 epochs,   \n662 $l r=0.008$ , batch size of 1024, CutMix and MixUp for the data augmentation.   \n663 Implementation Details of VL-LTR: We use the official code of VL-LTR [97] for our experiments.   \n664 We use a pre-trained CLIP ResNet-50 backbone. We followed the hyperparameters reported in VL  \n665 LTR [97]. We augment only \u201cFew\u201d category and train the backbone with the VL-LTR [97] method.   \n666 Training takes 4 hours on 8 NVIDIA RTX 3090 GPUs. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/b89a166592609c28f98bb4a6a08b8d37f1e328ca7139646afbd30f904cd26346.jpg", "table_caption": ["Table A3: tiered-ImageNet: Accuracies $(\\%\\pm\\mathrm{\\std})$ for 5-way, 1-shot and 5-way, 5-shot classification settings on the test-set. We compare against various SOTA supervised and unsupervised few-shot classification baselines as well as other augmentation methods, with UniSiam [61] pre-trained ResNet-34 backbone. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "667 A.5 More Visualizations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "668 Additional qualitative results resembling the style presented in Fig. 4 are presented in Fig. A3, and   \n669 more visuals akin to Fig. 2 can be found in Fig. A4. Moreover, we also present more visualization   \n670 similar to the style in Fig. 5 in Fig. A5. ", "page_idx": 17}, {"type": "table", "img_path": "rC0OM4Jm4v/tmp/3a5dacb7cecacc4417ee3756d659ca4da4d49f471a4c92af20b291ea772bc1a2.jpg", "table_caption": ["Table A4: Long-Tailed ImageNet-LT: We compare different augmentation methods on ImageNet-LT and report Top-1 accuracy for \u201cFew\u201d, \u201cMedium\u201d, and \u201cMany\u201d sets. $^\\dagger$ indicates results with ResNeXt50. $^{*}$ : indicates training with 384 resolution so is not directly comparable with other methods with 224 resolution. On the \u201cFew\u201d set and LiVT method, our augmentations improve the accuracy by 11.7 points compared to LiVT original augmentation and 4.4 points compared to $\\mathtt{T x t2I m g}$ . "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/81dc5e21ae4856e5b2bf505929f9d1830152d3cb7a4a10f884c3a6dbcfff1eb1.jpg", "img_caption": ["Figure A3: Visualization of Generative Samples: More visualization akin to Fig. 4. We compare GeNIe with two baselines: $\\mathtt{I m g2I m g}^{L}$ augmentation uses both image and text prompt from the same category, resulting in less challenging examples. Txt2Img augmentation generates images based solely on a text prompt, potentially deviating from the task\u2019s visual domain. GeNIe augmentation incorporates the target category name in the text prompt along with the source image, producing desired images with an optimal amount of noise, and balancing the impact of the source image and text prompt. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/dcbbb75e1c5cfc5dc89e9abc9a6faf1a85853b6382173375ef9c742f24331e80.jpg", "img_caption": ["Figure A4: Effect of noise in GeNIe: Akin to Fig. 2, we use GeNIe to create augmentations with varying noise levels. As is illustrated in the examples above, a reduced amount of noise leads to images closely mirroring the semantics of the source images, causing a misalignment with the intended target label. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "rC0OM4Jm4v/tmp/35e92153177b4330bf580c7b810e1ebdab42f3c73293211df5fdefcfda6ec4f6.jpg", "img_caption": ["Figure A5: Effect of noise in GeNIe: Similar to Fig. 5, we pass all the generated augmentations through the DinoV2 ViT-G model, which acts as our oracle model, to obtain their associated embeddings. Subsequently, we employ PCA for visualization purposes. The visualization reveals that the magnitude of semantic transformations is contingent upon both the source image and the specified target category. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "6721. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "673 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s   \n674 contributions and scope?   \n675 Answer: [Yes]   \n676 Justification: We demonstrate the effectiveness of our augmentation method through empirical   \n677 comparison with four different generative augmentation baselines across two scenarios: few-shot   \n678 and long-tail classification. Additionally, we perform analytical experiments on our augmented   \n679 samples to illustrate their nature as hard negatives.   \n680 Guidelines:   \n681 \u2022 The answer NA means that the abstract and introduction do not include the claims made in the   \n682 paper.   \n683 \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions   \n684 made in the paper and important assumptions and limitations. A No or NA answer to this question   \n685 will not be perceived well by the reviewers.   \n686 \u2022 The claims made should match theoretical and experimental results, and reflect how much the   \n687 results can be expected to generalize to other settings.   \n688 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not   \n689 attained by the paper.   \n6902. Limitations   \n691 Question: Does the paper discuss the limitations of the work performed by the authors?   \n692 Answer: [Yes]   \n693 Justification: We discuss about the limitations of our method in Sec 5   \n694 Guidelines:   \n695 \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper   \n696 has limitations, but those are not discussed in the paper.   \n697 \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n698 \u2022 The paper should point out any strong assumptions and how robust the results are to violations of   \n699 these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,   \n700 asymptotic approximations only holding locally). The authors should reflect on how these as  \n701 sumptions might be violated in practice and what the implications would be.   \n702 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested   \n703 on a few datasets or with a few runs. In general, empirical results often depend on implicit   \n704 assumptions, which should be articulated.   \n705 \u2022 The authors should reflect on the factors that influence the performance of the approach. For   \n706 example, a facial recognition algorithm may perform poorly when image resolution is low or   \n707 images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide   \n708 closed captions for online lectures because it fails to handle technical jargon.   \n709 \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they   \n710 scale with dataset size.   \n711 \u2022 If applicable, the authors should discuss possible limitations of their approach to address prob  \n712 lems of privacy and fairness.   \n713 \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers   \n714 as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t   \n715 acknowledged in the paper. The authors should use their best judgment and recognize that indi  \n716 vidual actions in favor of transparency play an important role in developing norms that preserve   \n717 the integrity of the community. Reviewers will be specifically instructed to not penalize honesty   \n718 concerning limitations.   \n7193. Theory Assumptions and Proofs   \n720 Question: For each theoretical result, does the paper provide the full set of assumptions and a   \n721 complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "722 Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "727 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 23}, {"type": "text", "text": "728 \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in   \n729 the supplemental material, the authors are encouraged to provide a short proof sketch to provide   \n730 intuition.   \n731 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal   \n732 proofs provided in appendix or supplemental material.   \n733 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n7344. Experimental Result Reproducibility   \n735 Question: Does the paper fully disclose all the information needed to reproduce the main exper  \n736 imental results of the paper to the extent that it affects the main claims and/or conclusions of the   \n737 paper (regardless of whether the code and data are provided or not)?   \n738 Answer: [Yes]   \n739 Justification: We provide implementation details in each experimental section. Additionally, we   \n740 include the code as supplementary material and plan to release it publicly.   \n741 Guidelines:   \n742 \u2022 The answer NA means that the paper does not include experiments.   \n743 \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the   \n744 reviewers: Making the paper reproducible is important, regardless of whether the code and data   \n745 are provided or not.   \n746 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make   \n747 their results reproducible or verifiable.   \n748 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For exam  \n749 ple, if the contribution is a novel architecture, describing the architecture fully might suffice, or if   \n750 the contribution is a specific model and empirical evaluation, it may be necessary to either make   \n751 it possible for others to replicate the model with the same dataset, or provide access to the model.   \n752 In general. releasing code and data is often one good way to accomplish this, but reproducibility   \n753 can also be provided via detailed instructions for how to replicate the results, access to a hosted   \n754 model (e.g., in the case of a large language model), releasing of a model checkpoint, or other   \n755 means that are appropriate to the research performed.   \n756 \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to   \n757 provide some reasonable avenue for reproducibility, which may depend on the nature of the con  \n758 tribution. For example   \n759 (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce   \n760 that algorithm.   \n761 (b) If the contribution is primarily a new model architecture, the paper should describe the architec  \n762 ture clearly and fully.   \n763 (c) If the contribution is a new model (e.g., a large language model), then there should either be a   \n764 way to access this model for reproducing the results or a way to reproduce the model (e.g., with   \n765 an open-source dataset or instructions for how to construct the dataset).   \n766 (d) We recognize that reproducibility may be tricky in some cases, in which case authors are wel  \n767 come to describe the particular way they provide for reproducibility. In the case of closed-source   \n768 models, it may be that access to the model is limited in some way (e.g., to registered users), but   \n769 it should be possible for other researchers to have some path to reproducing or verifying the   \n770 results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "7715. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "772 Question: Does the paper provide open access to the data and code, with sufficient instructions to   \n773 faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "774 Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "775 Justification: We provide implementation details in each experimental section. Additionally, we   \n776 include the code as supplementary material and plan to release it publicly.   \n777 Guidelines:   \n778 \u2022 The answer NA means that paper does not include experiments requiring code.   \n779 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/   \n780 guides/CodeSubmissionPolicy) for more details.   \n781 \u2022 While we encourage the release of code and data, we understand that this might not be possible,   \n782 so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless   \n783 this is central to the contribution (e.g., for a new open-source benchmark).   \n784 \u2022 The instructions should contain the exact command and environment needed to run to reproduce   \n785 the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/   \n786 guides/CodeSubmissionPolicy) for more details.   \n787 \u2022 The authors should provide instructions on data access and preparation, including how to access   \n788 the raw data, preprocessed data, intermediate data, and generated data, etc.   \n789 \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed   \n790 method and baselines. If only a subset of experiments are reproducible, they should state which   \n791 ones are omitted from the script and why.   \n792 \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if   \n793 applicable).   \n794 \u2022 Providing as much information as possible in supplemental material (appended to the paper) is   \n795 recommended, but including URLs to data and code is permitted.   \n7966. Experimental Setting/Details   \n797 Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,   \n798 how they were chosen, type of optimizer, etc.) necessary to understand the results?   \n799 Answer: [Yes]   \nJustification: We provide implementation details and dataset details in each experimental section.   \n801 Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n803 \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is   \n804 necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.   \n8067. Experiment Statistical Significance   \n807 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n808 information about the statistical significance of the experiments?   \n809 Answer: [Yes]   \n810 Justification: We repeat few-shot training for 600 episodes on mini-ImageNet and 1000 episodes   \n811 on tiered-ImageNet, reporting the mean and variance for each method.   \n812 Guidelines:   \n813 \u2022 The answer NA means that the paper does not include experiments.   \n814 \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence inter  \n815 vals, or statistical significance tests, at least for the experiments that support the main claims of   \n816 the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for exam  \n818 ple, train/test split, initialization, random drawing of some parameter, or overall run with given   \n819 experimental conditions).   \n820 \u2022 The method for calculating the error bars should be explained (closed form formula, call to a   \n821 library function, bootstrap, etc.)   \n822 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n823 \u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n824 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report   \n825 a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is   \n826 not verified.   \n827 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures sym  \n828 metric error bars that would yield results that are out of range (e.g. negative error rates).   \n829 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were   \n830 calculated and reference the corresponding figures or tables in the text.   \n8318. Experiments Compute Resources   \n832 Question: For each experiment, does the paper provide sufficient information on the computer   \n833 resources (type of compute workers, memory, time of execution) needed to reproduce the experi  \n834 ments?   \n835 Answer: [Yes]   \n836 Justification: We provide implementation and dataset details in each experimental section. Ad  \n837 ditionally, we elaborate on the required resources, including GPUs and training hours, for each   \n838 experiment.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not include experiments.   \n841 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud   \n842 provider, including relevant memory and storage.   \n843 \u2022 The paper should provide the amount of compute required for each of the individual experimental   \n844 runs as well as estimate the total compute.   \n845 \u2022 The paper should disclose whether the full research project required more compute than the ex  \n846 periments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into   \n847 the paper).   \n8489. Code Of Ethics   \n849 Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS   \n850 Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n851 Answer: [Yes]   \n852 Justification: We reviewed the NeurIPS Code of Ethics.   \n853 Guidelines:   \n854 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n855 \u2022 If the authors answer No, they should explain the special circumstances that require a deviation   \n856 from the Code of Ethics.   \n857 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due   \n858 to laws or regulations in their jurisdiction).   \n85910. Broader Impacts   \n860 Question: Does the paper discuss both potential positive societal impacts and negative societal   \n861 impacts of the work performed?   \n862 Answer: [Yes]   \n863 Justification: We discuss about broader impact in Conclusion.   \n864 Guidelines:   \n865 \u2022 The answer NA means that there is no societal impact of the work performed.   \n866 \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or   \n867 why the paper does not address societal impact.   \n868 \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., dis  \n869 information, generating fake profiles, surveillance), fairness considerations (e.g., deployment of   \n870 technologies that could make decisions that unfairly impact specific groups), privacy considera  \n871 tions, and security considerations.   \n872 \u2022 The conference expects that many papers will be foundational research and not tied to particular   \n873 applications, let alone deployments. However, if there is a direct path to any negative applications,   \n874 the authors should point it out. For example, it is legitimate to point out that an improvement in   \n875 the quality of generative models could be used to generate deepfakes for disinformation. On the   \n876 other hand, it is not needed to point out that a generic algorithm for optimizing neural networks   \n877 could enable people to train models that generate Deepfakes faster.   \n878 \u2022 The authors should consider possible harms that could arise when the technology is being used   \n879 as intended and functioning correctly, harms that could arise when the technology is being used   \n880 as intended but gives incorrect results, and harms following from (intentional or unintentional)   \n881 misuse of the technology.   \n882 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies   \n883 (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for moni  \n884 toring misuse, mechanisms to monitor how a system learns from feedback over time, improving   \n885 the efficiency and accessibility of ML).   \n88611. Safeguards   \n887 Question: Does the paper describe safeguards that have been put in place for responsible release of   \n888 data or models that have a high risk for misuse (e.g., pretrained language models, image generators,   \n889 or scraped datasets)?   \n890 Answer: [NA]   \n891 Justification: We believe our work does not have such risks.   \n892 Guidelines:   \n893 \u2022 The answer NA means that the paper poses no such risks.   \n894 \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary   \n895 safeguards to allow for controlled use of the model, for example by requiring that users adhere to   \n896 usage guidelines or restrictions to access the model or implementing safety filters.   \n897 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should   \n898 describe how they avoided releasing unsafe images.   \n899 \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require   \n900 this, but we encourage authors to take this into account and make a best faith effort.   \n90112. Licenses for existing assets   \n902 Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,   \n903 properly credited and are the license and terms of use explicitly mentioned and properly respected?   \n904 Answer: [Yes]   \n905 Justification: We cited all datasets and code used in our paper.   \n906 Guidelines:   \n907 \u2022 The answer NA means that the paper does not use existing assets.   \n908 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n909 \u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n910 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n911 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of   \n912 that source should be provided.   \n913 \u2022 If assets are released, the license, copyright information, and terms of use in the package should   \n914 be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for   \n915 some datasets. Their licensing guide can help determine the license of a dataset.   \n916 \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived   \n917 asset (if it has changed) should be provided.   \n918 \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s   \n919 creators.   \n92013. New Assets   \n921 Question: Are new assets introduced in the paper well documented and is the documentation pro  \n922 vided alongside the assets?   \n923 Answer: [NA]   \n924 Justification: We do not release new assets.   \n925 Guidelines:   \n926 \u2022 The answer NA means that the paper does not release new assets.   \n927 \u2022 Researchers should communicate the details of the dataset/code/model as part of their submis  \n928 sions via structured templates. This includes details about training, license, limitations, etc.   \n929 \u2022 The paper should discuss whether and how consent was obtained from people whose asset is   \n930 used.   \n931 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an   \n932 anonymized URL or include an anonymized zip file.   \n93314. Crowdsourcing and Research with Human Subjects   \n934 Question: For crowdsourcing experiments and research with human subjects, does the paper in  \n935 clude the full text of instructions given to participants and screenshots, if applicable, as well as   \n936 details about compensation (if any)?   \n937 Answer: [NA]   \n938 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n939 Guidelines:   \n940 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human   \n941 subjects.   \n942 \u2022 Including this information in the supplemental material is fine, but if the main contribution of the   \n943 paper involves human subjects, then as much detail as possible should be included in the main   \n944 paper.   \n945 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other   \n946 labor should be paid at least the minimum wage in the country of the data collector.   \n94715. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub  \n948 jects   \n949 Question: Does the paper describe potential risks incurred by study participants, whether such   \n950 risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or   \n951 an equivalent approval/review based on the requirements of your country or institution) were ob  \n952 tained?   \n953 Answer: [NA]   \n954 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n955 Guidelines:   \n956 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human   \n957 subjects.   \n958 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be   \n959 required for any human subjects research. If you obtained IRB approval, you should clearly state   \n960 this in the paper.   \n961 \u2022 We recognize that the procedures for this may vary significantly between institutions and loca  \n962 tions, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their   \n963 institution.   \n964 \u2022 For initial submissions, do not include any information that would break anonymity (if applica  \n965 ble), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]