[{"figure_path": "aIyNLWXuDO/figures/figures_0_1.jpg", "caption": "Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only) models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our new Abacus Embeddings (right) dramatically improve generalization to unseen digit lengths. The interior of the red square denotes the training distribution. Accuracies are averaged over three trials.", "description": "This figure compares the zero-shot accuracy of a depth-16 transformer decoder-only model on addition problems using two different types of positional embeddings: FIRE (state-of-the-art) and Abacus (the authors' new method).  The models were trained on numbers with up to 20 digits. The heatmaps show the accuracy for different lengths of operands one and two.  The key finding is that Abacus embeddings significantly improve generalization to longer, unseen operand lengths (outside the red square), demonstrating better performance than FIRE embeddings.", "section": "Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_4_1.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure presents a comparison of the performance of different transformer models on addition tasks. The left panel shows the accuracy of three models (standard transformer, standard transformer with input injection, and looped transformer) trained on datasets with 20-digit operands using three types of embeddings (Abacus, FIRE, NOPE).  The results show the Abacus embeddings achieve the highest accuracy. The right panel shows similar results for models trained on 40-digit operands. The main finding is that the looped transformer models with the Abacus and FIRE embeddings exhibit significantly higher accuracy compared to the other models.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_6_1.jpg", "caption": "Figure 4: Varying the size of the recurrent block, while maintaining an effective depth of 16 and training on size 20 data. We see that a recurrent model with eight layers in the recurrent block and two recurrences is the most accurate of all effective depth 16 models, halving the error rate of a standard model with input injection in the OOD evaluation. (See Figure 17 for results with FIRE and NOPE.)", "description": "This figure displays the results of an ablation study on the effect of varying the size of the recurrent block in a transformer model on the accuracy of addition problems.  The experiment kept the effective depth of the model consistent at 16 layers.  The x-axis shows different configurations of layers within the recurrent block and the number of times that block is repeated (recurrences).  The y-axis represents the exact match accuracy.  Two accuracy metrics are shown: out-of-distribution (OOD) accuracy and extreme out-of-distribution (100+ digit OOD) accuracy.  The results demonstrate that a model with 8 layers in the recurrent block and 2 recurrences achieves the best accuracy, significantly outperforming a standard transformer model with input injection.", "section": "3.2 Recurrence In Transformers Boosts Performance"}, {"figure_path": "aIyNLWXuDO/figures/figures_6_2.jpg", "caption": "Figure 5: Model which have 8 layers in recurrent block and 2 recurrences, trained on size 20 addition and subtraction data, each line is the average of 3 models. We see that it is possible to have extreme generalization whilst learning multiple tasks.", "description": "This figure shows the accuracy of models trained on both addition and subtraction problems with operand lengths up to 20 digits.  The models use a recurrent architecture (8 layers in the recurrent block, 2 recurrences). The plot demonstrates the model's ability to generalize to much larger problems (extrapolation) in both addition and subtraction accurately, even beyond the largest operand lengths seen during training.  The shaded gray region indicates the operand length range included in the training dataset.", "section": "4.1 Addition and Subtraction"}, {"figure_path": "aIyNLWXuDO/figures/figures_7_1.jpg", "caption": "Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only) models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our new Abacus Embeddings (right) dramatically improve generalization to unseen digit lengths. The interior of the red square denotes the training distribution. Accuracies are averaged over three trials.", "description": "This figure displays the zero-shot accuracy of a depth-16 transformer model on addition problems.  The x and y axes represent the length of the two operands.  The color intensity indicates the accuracy, with darker shades showing higher accuracy. The left panel shows results using state-of-the-art FIRE embeddings, while the right panel shows the results obtained with the proposed Abacus Embeddings. The red square indicates the training data distribution.  Abacus embeddings show significantly better generalization to longer operands compared to the FIRE embeddings.", "section": "Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_8_1.jpg", "caption": "Figure 7: Exact match accuracy of standard transformer of depth 16 with input injection, trained on up to size 20 data. The red square denotes in distribution testing. Combining Abacus Embeddings with FIRE or RoPE embeddings improves out of distribution accuracy for addition, over the baseline models without Abacus Embeddings.", "description": "This figure displays the exact match accuracy for addition problems solved by a standard transformer model with a depth of 16 and input injection. The model was trained on datasets with operands up to 20 digits.  Four different experimental setups are compared: using Abacus embeddings alone, FIRE embeddings alone, Abacus + FIRE embeddings, and Abacus + RoPE embeddings. The red square shows the in-distribution accuracy (accuracy on problems with operand lengths less than or equal to 20).  The figure highlights that combining Abacus embeddings with either FIRE or RoPE improves the model's accuracy, especially for out-of-distribution problems (operand lengths greater than 20).", "section": "4 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_14_1.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure displays the mean exact match accuracy for addition problems, comparing the performance of three different transformer architectures (standard, standard with input injection, and looped transformer) across three different positional embedding techniques (Abacus, FIRE, and NoPE).  The left panel shows results for models trained on datasets with operands up to 20 digits, while the right panel shows results for models trained on datasets with operands up to 40 digits.  The results highlight the improved accuracy achieved by Abacus embeddings and the further accuracy gains obtained using looped transformer architectures, especially when generalizing to unseen operand lengths.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_15_1.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure displays the performance comparison of different transformer models on addition tasks.  The left panel shows the results for models trained on datasets with operands up to 20 digits, while the right panel shows results for models trained on datasets with operands up to 40 digits.  The models varied in their architectures (standard, input injection, and looped transformers) and used different positional embeddings (Abacus, FIRE, and NoPE).  The figure highlights the improved performance of the Abacus embeddings and the benefits of using recurrent looped transformer architectures.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_15_2.jpg", "caption": "Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only) models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our new Abacus Embeddings (right) dramatically improve generalization to unseen digit lengths. The interior of the red square denotes the training distribution. Accuracies are averaged over three trials.", "description": "This figure displays the zero-shot accuracy of a depth sixteen transformer model on addition problems. The model was trained on operands of up to 20 digits.  The left panel shows results using state-of-the-art FIRE embeddings, while the right panel shows results obtained with the novel Abacus embeddings introduced in the paper. The red square highlights the training data distribution. The Abacus embeddings significantly improve generalization to longer operands (beyond the 20 digits seen during training) compared to FIRE embeddings.  Accuracy is averaged over three trials.", "section": "Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_16_1.jpg", "caption": "Figure 10: Left: Exact match accuracy of five models trained on size 20 data, generalizing well to 120 digit addition, an extrapolation of 6x. Right: Exact match accuracy of five models trained on size 20 data, offset randomization hyperparameter k = 25, 50, 75 and 100. Only showing the accuracy for operands of the same length.", "description": "This figure shows the results of experiments evaluating length generalization in addition tasks using Abacus embeddings. The left panel demonstrates that models trained on 20-digit numbers can generalize to 120-digit addition problems with high accuracy, exceeding the previous state-of-the-art generalization factor by a significant margin. The right panel explores the impact of the hyperparameter 'k' on generalization. The results suggest that increasing 'k' improves the model's ability to extrapolate to longer addition problems. This hyperparameter controls the range of positional embeddings and its increase allows the models to handle longer sequences beyond the training data.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_16_2.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure compares the performance of different transformer models on addition tasks. The left panel shows the accuracy of three depth-16 models trained on datasets with operands up to 20 digits, using different embedding methods (Abacus, FIRE, NOPE).  The right panel presents accuracy results for the same models on 40-digit operands, this time using looped transformers and standard transformers. The results highlight the superior performance of the Abacus embeddings and the benefits of recurrent architectures.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_17_1.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings. Looped transformer (LT): Weight tied decoder layers, with input injection and progressive loss. Standard Transformer (ST): Stacked decoder only layers. Standard Transformer with Input Injection (ST w/ II): Standard Transformer with input features added to the hidden representation between each decoder layer.", "description": "The left plot shows the mean accuracy of three different transformer models (standard, standard with input injection, and looped) on addition problems with operands up to 20 digits, each using different embedding techniques (Abacus, FIRE, and NOPE).  Abacus embeddings significantly improve performance. The right plot compares the same models but trained on data with operands up to 40 digits. Again, Abacus embeddings are advantageous, and the looped transformer architecture provides notable gains in accuracy, especially when combined with FIRE or NOPE embeddings.", "section": "3 Abacus Embeddings Help Align Digits"}, {"figure_path": "aIyNLWXuDO/figures/figures_18_1.jpg", "caption": "Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only) models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our new Abacus Embeddings (right) dramatically improve generalization to unseen digit lengths. The interior of the red square denotes the training distribution. Accuracies are averaged over three trials.", "description": "This figure compares the performance of a depth-16 transformer model with two different types of positional embeddings on an addition task. The models were trained on numbers with up to 20 digits.  The left side shows the results using FIRE embeddings (a state-of-the-art method), while the right side shows the results using the novel Abacus embeddings. The figure demonstrates that Abacus embeddings significantly improve the model's ability to generalize to addition problems with longer numbers (beyond the 20 digits seen during training). The red square highlights the training distribution.", "section": "Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_19_1.jpg", "caption": "Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only) models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our new Abacus Embeddings (right) dramatically improve generalization to unseen digit lengths. The interior of the red square denotes the training distribution. Accuracies are averaged over three trials.", "description": "This figure shows the zero-shot accuracy of a depth-16 transformer decoder-only model on addition problems.  The model was trained on numbers up to 20 digits long. The left side displays results using state-of-the-art FIRE embeddings, while the right shows the improved results using the authors' new Abacus embeddings.  The red square highlights the training data distribution, showing significantly better generalization to longer, unseen digit lengths with Abacus embeddings.", "section": "1 Introduction"}, {"figure_path": "aIyNLWXuDO/figures/figures_19_2.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure presents a comparison of the performance of different transformer models on addition problems.  The left panel shows the accuracy of three models (standard transformer, standard transformer with input injection, and looped transformer) trained on datasets with operands up to 20 digits.  The right panel shows accuracy for models with operands up to 40 digits. In both cases, accuracy is shown for different embedding methods (Abacus, FIRE, NoPE). The results demonstrate that Abacus embeddings improve performance in both settings and that recurrent models further improve performance.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_20_1.jpg", "caption": "Figure 4: Varying the size of the recurrent block, while maintaining an effective depth of 16 and training on size 20 data. We see that a recurrent model with eight layers in the recurrent block and two recurrences is the most accurate of all effective depth 16 models, halving the error rate of a standard model with input injection in the OOD evaluation. (See Figure 17 for results with FIRE and NOPE.)", "description": "This figure displays the results of an ablation study on the effect of varying the size of the recurrent block within a transformer model on the accuracy of addition.  The experiment maintains a constant effective depth of 16 and uses a training dataset with operands up to 20 digits.  The results show that a model with eight layers in the recurrent block and two recurrences achieves the best out-of-distribution (OOD) accuracy, significantly outperforming a standard transformer with input injection.  The improvements are highlighted through a comparison of accuracy scores across different block sizes, demonstrating the optimal balance between block size and recurrence number for enhanced performance.", "section": "3.2 Recurrence In Transformers Boosts Performance"}, {"figure_path": "aIyNLWXuDO/figures/figures_20_2.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "The figure compares the performance of different transformer models (standard, standard with input injection, and looped transformer) on an addition task, using different positional embeddings (Abacus, FIRE, and NoPE).  The left panel shows results for models trained on data with operands up to 20 digits, while the right panel shows results for models trained on data with operands up to 40 digits.  The results highlight the superiority of Abacus embeddings and demonstrate that recurrent architectures further improve accuracy.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_20_3.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure displays the mean exact match accuracy for addition problems, comparing different models (standard transformer, standard transformer with input injection, and looped transformer) and positional embeddings (Abacus, FIRE, and NOPE).  The left panel shows results for models trained on datasets with operands up to 20 digits, while the right panel shows results for models trained on datasets with operands up to 40 digits.  The figure highlights that Abacus Embeddings generally improve accuracy, and that recurrent models further boost performance, especially on larger problems (out-of-distribution).", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_21_1.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure presents a comparison of the performance of different transformer architectures and positional embedding methods on an addition task. The left panel shows the results for models trained on datasets with operands of up to 20 digits, while the right panel shows the results for models trained on datasets with operands of up to 40 digits. The results demonstrate that Abacus embeddings consistently improve accuracy, and that recurrent looped transformer architectures further enhance accuracy, especially for larger operand sizes.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_21_2.jpg", "caption": "Figure 3: Left: Mean exact match accuracy of three models of depth sixteen on size 20 data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NOPE Embeddings. Right: Mean exact match accuracy of three models of effective depth sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.", "description": "This figure displays the mean exact match accuracy for addition problems, comparing three different transformer architectures (Standard Transformer, Standard Transformer with Input Injection, and Looped Transformer) and three different positional embeddings (Abacus, FIRE, and NOPE).  The left panel shows results for models trained on data with operands up to 20 digits, while the right panel shows results for models trained on data with operands up to 40 digits.  The results demonstrate the effectiveness of Abacus Embeddings compared to FIRE and NOPE, and the improvement in performance offered by recurrent architectures in both experiments.", "section": "3 Achieving Length Generalization for Addition"}, {"figure_path": "aIyNLWXuDO/figures/figures_21_3.jpg", "caption": "Figure 7: Exact match accuracy of standard transformer of depth 16 with input injection, trained on up to size 20 data. The red square denotes in distribution testing. Combining Abacus Embeddings with FIRE or RoPE embeddings improves out of distribution accuracy for addition, over the baseline models without Abacus Embeddings.", "description": "This figure compares the performance of different transformer models on an addition task.  The models use various positional embeddings (Abacus, FIRE, RoPE),  with and without input injection.  The results show that combining Abacus embeddings with either FIRE or RoPE embeddings significantly improves the out-of-distribution accuracy compared to using only FIRE or RoPE embeddings.", "section": "4 Abacus and Relative Embeddings"}]