[{"figure_path": "aIyNLWXuDO/tables/tables_7_1.jpg", "caption": "Table 1: Exact match accuracy for sorting with various positional embeddings. All results are percentages of the test set and all models here are standard transformers with eight layers.", "description": "This table presents the exact match accuracy for a sorting task using different positional embeddings (FIRE, Abacus, and a combination of both). The accuracy is measured on out-of-distribution (OOD) data, with variations in the number length and array length.  The results highlight the performance of standard transformers with eight layers, demonstrating how different embeddings affect the model's ability to generalize to unseen data.", "section": "4.3 Array Sorting"}, {"figure_path": "aIyNLWXuDO/tables/tables_7_2.jpg", "caption": "Table 2: Accuracy for sorting with various architectures for sorting. ST denotes standard transformer, ST w/ II denotes standard transformer with input injection, and LT denotes looped transformer models. The standard transformer has the best exact match accuracy. When measuring the accuracy on identifying only the minimum element of the array, looped transformers outperform all others. All results are percentages of the test set.", "description": "This table presents the accuracy of different transformer architectures on a sorting task.  The accuracy is measured in two ways: exact string match (all elements correctly sorted) and minimum element only (only the minimum element is correctly identified).  The table compares the performance of standard transformers, standard transformers with input injection, and looped transformers, highlighting that looped transformers excel at identifying the minimum element, while the standard transformer achieves the highest accuracy on exact string matching.", "section": "4.3 Array Sorting"}, {"figure_path": "aIyNLWXuDO/tables/tables_22_1.jpg", "caption": "Table 3: Number of parameters, to the nearest million, in a model with Abacus Embeddings and input injection.", "description": "This table shows the number of parameters (in millions) for different looped transformer model configurations.  The configurations vary in the number of layers in the recurrent block and the number of recurrences. Abacus Embeddings and input injection are used in all models listed in this table.", "section": "A.7.3 Varying Effective Depth"}, {"figure_path": "aIyNLWXuDO/tables/tables_22_2.jpg", "caption": "Table 4: Default number of Nvidia GPU hours used to train a model.", "description": "This table shows the default number of Nvidia GPU hours used for training and testing different models on various datasets (Addition, Bitwise OR, Sorting, Multiplication).  It provides a sense of the computational resources required for each task.", "section": "A.8 Experimental Setting Details"}, {"figure_path": "aIyNLWXuDO/tables/tables_23_1.jpg", "caption": "Table 5: Default hyperparameter values.", "description": "This table lists the default hyperparameter values used in the experiments of the paper.  It includes settings for various aspects of the model architecture and training process, such as hidden size, embedding size, optimizer, learning rate schedule, activation function, normalization layer, and the offset randomization hyperparameter (k). These settings are crucial for reproducibility and understanding the experimental setup.", "section": "A.8 Hyperparameters"}]