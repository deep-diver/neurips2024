[{"type": "text", "text": "Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on at  \n2 tributes that have high spurious correlation with the target. This can degrade the   \n3 performance on underrepresented (or minority) groups that lack these attributes,   \n4 posing significant challenges for both out-of-distribution generalization and fair  \n5 ness objectives. Many studies aim to improve robustness to spurious correlation,   \n6 yet nearly all require group annotation for training and/or model selection. This   \n7 constrains their applicability in situations where the nature of the spurious correla  \n8 tion is not known, or when group labels for certain spurious attributes are either   \n9 insufficient or completely absent. To meet the demand for effectively enhancing the   \n10 model robustness under minimal assumptions about group annotation, we propose   \n11 Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses   \n12 from a trained model to construct a balanced dataset of high-loss and low-loss   \n13 samples in which the training data group imbalance is mitigated. This results in   \n14 a significant robustness to group shifts when equipped with a simple mechanism   \n15 of last layer retraining. Furthermore, by utilizing environment inference methods   \n6 for creating diverse environments with correlation shifts, EVaLS can potentially   \n17 eliminate the need for group annotation in the validation data. In such a context, the   \n8 worst environment accuracy acts as a reliable surrogate throughout the retraining   \n19 process for tuning hyperparameters and finding a model that performs well across   \n20 diverse group shifts. EVaLS effectively achieves group robustness, showing that   \n21 group annotation is not necessary even for validation. It is a fast, straightforward,   \n22 and effective approach that reaches near-optimal worst group accuracy without   \n23 needing group annotations, marking a new chapter in the robustness of trained   \n4 models against spurious correlation. ", "page_idx": 0}, {"type": "text", "text": "25 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "26 Training deep learning models using Empirical Risk Minimization (ERM) on a dataset, poses the   \n27 risk of relying on spurious correlation. These are correlations between certain patterns in the   \n28 training dataset and the target (e.g., the class label in a classification task) despite lacking any causal   \n29 relationship. Learning such correlations as shortcuts can negatively impact the models\u2019 accuracy on   \n30 minority groups that do not contain the spurious patterns associated with the target [1, 2]. This problem   \n31 leads to concerns regarding fairness [3], and can also cause a marked reduction in the performance.   \n32 This occurs particularly when minority groups, which are underrepresented during training, become   \n33 overrepresented at the time of testing, as a result of shifts within the subpopulations [4]. Hence,   \n34 ensuring robustness to group shifts and developing methods that improve worst group accuracy   \n35 (WGA) is crucial for achieving both fairness and robustness in the realm of deep learning.   \n36 Many studies have proposed solutions to address this challenge. A promising line of research   \n37 focuses on increasing the contribution of minority groups in the model\u2019s training [1, 5\u20137]. A strong   \n38 assumption that is considered by some previous works is having access to group annotations for   \n39 training or fully/partially fine-tuning a pretrained model [8, 7, 1]. The study by Kirichenko et al. [1]   \n40 proposes that retraining the last layer of a model on a dataset which is balanced in terms of group   \n41 annotation can effectively enhance the model\u2019s robustness against shifts in spurious correlation. While   \n42 these works have shown tremendous robustness performance, their assumption for the availability of   \n43 group annotation restricts their usage.   \n44 In many real-world applications, the process of labeling samples according to their respective groups   \n45 can be prohibitively expensive, and sometimes impractical, especially when all minority groups   \n46 may not be identifiable beforehand. A widely adopted strategy in these situations involves the   \n47 indirect inference of various groups, followed by the training of models using a loss function that is   \n48 balanced across groups [5, 9, 10, 4]. The loss value of the model or its similar metrics is a popular   \n49 signal for recognizing minority groups [5, 9\u201311]. While most of these techniques necessitate full   \n50 training of a model, Qiu et al. [9] attempt to adapt the DFR method [1] with the aim of preserving   \n51 computational efficiency while simultaneously improving robustness to group shift. However, this   \n52 method still requires group annotations of the validation set for model selection and hyperparameter   \n53 tuning. Consequently, this constitutes a restrictive assumption when adequate annotations for certain   \n54 groups are not supplied. It also applies to situations where some shortcut attributes are completely   \n55 unidentified.   \n56 In this study, we present a novel strategy that effectively mitigates reliance on spurious correlation,   \n57 completely eliminating the need for group annotations during both training and retraining. More   \n58 interestingly, we provide empirical evidence indicating that group annotations are not necessary,   \n59 even for model selection. We show that assembling a diverse collection of environments for model   \n60 selection, which reflect group shifts can serve as an effective alternative approach. Our proposed   \n61 method, Environment-based Validation and Loss-based Sampling (EVaLS), is a technique that   \n62 strengthens the robustness of trained models against spurious correlation, all without relying on group   \n63 annotations. EVaLS is pioneering in its ability to eliminate the need for group annotations at every   \n64 phase, including the model selection step. EVaLS posits that in the absence of group annotations, a   \n65 set of environments showcasing group shifts is sufficient. Worst Environment Accuracy (WEA) could   \n66 then be utilized for model selection. Our findings demonstrate that utilizing environment inference   \n67 methods [12], or even dividing the validation data based on the predictions of a random linear layer   \n68 atop a trained model\u2019s feature space can markedly enhance group robustness. Figure 1 demonstrates   \n69 the overall procedure of the main parts of EVaLS.   \n70 Our empirical observations support prior research which suggests that high-loss data points in a   \n71 trained model may signal the presence of minority groups [5, 9, 10]. Our method, EVaLS, evenly   \n72 selects from both high-loss and low-loss data to form a balanced dataset that is used for last-layer   \n73 retraining. We offer theoretical explanations for the effectiveness of this approach in addressing group   \n74 imbalances, and experimentally show the superiority of our efficient solution to the previous strategies.   \n75 Comprehensive experiments conducted on spurious correlation benchmarks such as CelebA [13],   \n76 Waterbirds [7], and UrbanCars [14], demonstrate that EVaLS achieves optimal accuracy. Moreover,   \n77 when group annotations are accessible solely for model selection, our approach, EVaLS-GL, exhibits   \n78 enhanced performance against various distribution shifts, including attribute imbalance, as seen in   \n79 MultiNLI [15], and class imbalance, exemplified by CivilComments [16]. We further present a   \n80 new dataset, Dominoes Colored-MNIST-FashionMNIST, which depicts a situation featuring multiple   \n81 independent shortcuts, that group annotations are only available for part of them (see Section 2.2). In   \n82 this setting, we show that strategies with lower levels of group supervision are paradoxically more   \n83 effective in mitigating the reliance on both known and unknown shortcuts. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "84 The main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "85 \u2022 We present EVaLS, a simple yet effective approach that enhances model robustness against   \n86 spurious correlation without relying on ground-truth group annotations.   \n87 \u2022 We offer both theoretical and practical insights on how balanced sampling from high-loss and   \n88 low-loss samples can result in a dataset in which the group imbalance is notably mitigated.   \n89 \u2022 Using simple environment inference techniques, EVaLS leverages worst environment accu  \n90 racy as a reliable indicator for model selection.   \n91 \u2022 EVaLS attains near-optimal worst group accuracies or even exceeds them in spurious   \n92 correlation benchmarks, all with zero group annotations.   \n93 \u2022 When group annotations are available for model selection, EVaLS delivers state-of-the-art   \n94 performance across a variety of subpopulation shift benchmarks.   \n95 \u2022 We introduce a new dataset consisting of two spurious features in which partial supervision   \n96 may negatively impact the performance of the underrepresented groups. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 2.1 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 We assume a general setting of a supervised learning problem with distinct data partitions $\\mathcal{D}^{\\mathrm{tr}}$ for   \n100 training, $\\mathcal{D}^{\\mathrm{val}}$ for validation, and $\\mathcal{D}^{\\mathrm{test}}$ for final evaluation. Each dataset comprises a set of paired   \n101 samples $(x,y)$ , where $x\\,\\in\\,{\\mathcal{X}}$ represents the data and $y\\,\\in\\,\\mathcal{V}$ denotes the corresponding labels.   \n102 Conventionally, $\\mathcal{D}^{\\mathrm{tr}}$ , $\\mathcal{D}^{\\mathrm{val}}$ , and $\\mathcal{D}^{\\mathrm{test}}$ are assumed to be uniformly sampled from the same distribution.   \n103 However, this idealized assumption does not hold in many real-world problems where distribution   \n104 shift is inevitable. In this context, we consider the sub-population shift problem [4]. In a general   \n105 form of this setting, it is assumed that data samples consist of different groups $\\mathcal{G}_{i}$ , where each   \n106 group comprises samples that share a property. More specifically, the overall data distribution   \n107 $\\begin{array}{r}{\\bar{p}(x,\\bar{y})=\\bar{\\sum_{i}{\\alpha_{i}}}p_{i}(x,\\bar{y})}\\end{array}$ is a composition of individual group distributions $p_{i}(x,y)$ weighted by their   \n108 respective proportions $\\alpha_{i}$ , where $\\textstyle\\sum_{i}\\alpha_{i}=1$ . In this work, we assume that $\\mathcal{D}^{\\mathrm{tr}}$ , $\\mathcal{D}^{\\mathrm{val}}$ , and $\\mathcal{D}^{\\mathrm{test}}$ are   \n109 composed of identical groups but with a different set of mixing coefficients $\\{\\alpha_{i}\\}$ . It is noteworthy   \n110 that the validation set may have approximately identical coefficients to those of the training or testing   \n111 sets, or it may have entirely different coefficients.   \n112 Several kinds of subpopulation shifts are defined in the literature, including class imbalance, attribute   \n113 imbalance, and spurious correlation [4]. Class imbalance refers to the cases where there is a difference   \n114 between the proportion of samples from each class, while attribute imbalance occurs when instances   \n115 with a certain attribute are underrepresented in the training data, even though this attribute may not   \n116 necessarily be a reliable predictor of the label. On the other hand, spurious correlation occurs when   \n117 various groups are differentiated by spurious attributes that are partially predictive and correlated with   \n118 class labels but are causally irrelevant. More precisely, we can consider a set of spurious attributes   \n119 $\\boldsymbol{S}$ that partition the data into $\\vert S\\vert\\times\\vert\\mathcal{V}\\vert$ groups. When the concurrence of a spurious attribute with a   \n120 label is significantly higher than its correlation with other labels, that spurious attribute could become   \n121 predictive of the label, resulting in deep models relying on the spurious attributes as shortcuts instead   \n122 of the core ones. This is followed by a decrease in the model\u2019s performance on groups that do not   \n123 have this attribute.   \n124 Given a class, the group containing samples with correlated spurious attributes is referred to as   \n125 majority group of that class, while the other groups are called the minority groups. As an example,   \n126 in the Waterbirds dataset [7], for which the task is to classify images of birds into landbird and   \n127 waterbird, there are spurious attributes {water background, land background}. Each background is   \n128 spuriously correlated with its associated label, decompose the data into two majority groups waterbird   \n129 on water background, and landbird on land background, and two minority groups waterbird on land   \n130 background and landbird on water background. Our goal is to make the classifier robust to spurious   \n131 attributes by increasing performance for all groups. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "132 2.2 Robustness of a Trained Model to an Unknown Shortcut ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "133 In scenarios where group annotations are absent, traditional methods that depend on these annotations   \n134 for training or model selection become infeasible. Moreover, as previously discussed by [14], when   \n135 data contains multiple spurious attributes and annotations are only available for some of them, such   \n136 methods would make the model robust only to the known spurious attributes. To explore such complex   \n137 scenarios, we introduce the Dominoes Colored-MNIST-FashionMNIST (Dominoes CMF) dataset   \n138 (Figure 3(d)). Drawing inspiration from Pagliardini et al. [17] and Arjovsky et al. [18], Dominoes   \n139 CMF merges an image from CIFAR10 [19] at the top with a colored (red or green) MNIST [20] or   \n140 FashionMNIST [21] image at the bottom. The primary label is derived from the CIFAR10 image,   \n141 while the bottom part introduces two independent spurious attributes: color and shape. Although   \n142 annotations for shape are provided for training and model selection, color remains an unknown   \n143 variable until testing. For more details on the dataset refer to the Appendix.   \n144 The illustrations in Figure 3(a-c) depict the outlined scenario. A classifier trained using ERM is   \n145 dependent on both spurious features (Figure 3(b)). Yet, achieving robustness against one spurious   \n146 correlation (Figure 3(c)), does not ensure robustness against both (Figure 3(a)). In Section 4 we   \n147 show that our method, which does not rely on the group annotations of the identified group, achieves   \n148 enhanced robustness against both spurious correlations, outperforming strategies that depend on the   \n149 known group\u2019s information. ", "page_idx": 2}, {"type": "image", "img_path": "wH36UKML4x/tmp/c2ac7d86057263d01703cc59cc7e5ebf3397d4db951ff3c654450b9d9108e1d6.jpg", "img_caption": ["Figure 1: Overview of the proposed method. Given an ERM-trained model (similar to DFR [1]), the following steps are performed: (a) we randomly split the held-out dataset into train and validation splits. (b) An environment inference method is utilized to infer diverse environments from the validation split. (c) We evaluate train split samples on the initial ERM classifier and sort high-loss and low-loss samples of each class for loss-based sampling. (d) Finally, we perform last-layer retraining on the loss-based selected samples. Each retraining setting (e.g. different $k$ for loss-based sampling) is validated based on the worst accuracy of the inferred environments. Note that majority and minority groups are shown with dark and light colors for better visualization, but are not known in our setting. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 3 Environment-based Validation and Loss-based Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 Our method, EVaLS, is designed to improve the robustness of deep learning models to group shifts   \n152 without the need for group annotation. In line with the DFR [1] approach, we utilize a classifier   \n153 defined as $f=h_{\\phi}\\circ g_{\\theta}$ , where $g_{\\theta}$ represents a deep neural network serving as a feature extractor, and   \n154 $h_{\\phi}$ denotes a linear classifier. The classifier is initially trained with the ERM objective on the training   \n155 dataset $\\mathcal{D}^{t r}$ . Subsequently, we freeze the feature extractor $g_{\\boldsymbol{\\theta}}$ and focus solely on retraining the last   \n156 linear layer $h_{\\phi}$ using the validation dataset $\\mathcal{D}^{\\mathrm{val}}$ as a held-out dataset.   \n157 We randomly divide the validation set $\\mathcal{D}^{\\mathrm{val}}$ into two subsets, $\\mathcal{D}^{L L}$ and $\\mathcal{D}^{M S}$ which are used for last   \n158 layer training and model selection, respectively. In Section 3.1 we explain how to sample a subset   \n159 of $\\mathcal{D}^{L L}$ that statistically handles the group shifts inherent in the dataset. In Section 3.2 we describe   \n160 how $\\mathcal{D}^{M S}$ is divided into different environments that are later used for model selection. The optimal   \n161 number of selected samples from $\\mathcal{D}^{L L}$ and other hyperparameters is determined based on the worst   \n162 environment accuracies among environments that are obtained from $\\mathcal{D}^{M S}$ . By combining our novel   \n163 sampling and validation strategy, we aim to provide a robust linear classifier $h_{\\phi^{*}}$ that significantly   \n164 improves the accuracy of underrepresented groups without requiring group annotations of training   \n165 or validation sets. Figure 1 illustrates the comprehensive workflow of the EVaLS methodology.   \n166 Finally in Section 3.3, we provide theoretical support for the loss-based sampling procedure and its   \n167 effectiveness. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "168 3.1 Loss-Based Instance Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "169 Following previous works [5, 10, 9], we use the loss value as an indicator for identifying minority   \n170 groups. We first evaluate classifier $f$ on samples within $\\mathcal{D}^{L L}$ and choose $k$ samples with the highest   \n171 and lowest loss values for a given $k$ . By combining these $2k$ samples from each class, we construct a   \n172 balanced set $\\mathcal{D}^{b a l a n c e d}$ , consisting of high-loss and low-loss samples (see Figure 1(c)). $\\mathcal{D}^{b a l a n c e d}$ is   \n173 then used for the training of the last layer of the model.   \n174 As depicted in Figure 2, the proportion of minority samples among various percentiles of samples   \n175 with the highest loss values increases as we select a smaller subset of samples with the highest loss.   \n176 This suggests that high and low-loss samples could serve as effective representatives of minority   \n177 and majority groups, respectively. In Section 3.3, we offer theoretical insights explaining why this   \n178 approach could lead to the creation of group-balanced data. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "179 3.2 Partitioning Validation Set into Environments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "180 Contrary to common assumptions and practices in the field, precise group labels for the validation   \n181 set are not essential for training models robust to spurious correlations. Our empirical findings,   \n182 detailed in Section 4, reveal that partitioning the validation set into environments that exhibit sig  \n183 nificant subpopulation shifts can be used for model selection. Under these conditions, the worst   \n184 environment accuracy (WEA) emerges as a viable metric for selecting the most effective model and   \n185 hyperparameters.   \n186 The concept of an environment, as frequently discussed in the invariant learning literature, denotes   \n187 partitions of data that exhibit different distributions. A model that consistently excels across these   \n188 varied environments, achieving impressive worst environment accuracy (WEA), is likely to perform   \n189 equally well across different groups in the test set. Several methods for inferring environments   \n190 with notable distribution shifts have been introduced [12, 22]. Environment Inference for Invariant   \n191 Learning (EIIL) [12], leverages the predictions from an earlier trained ERM model to divide the data   \n192 into two distinct environments that significantly deviate from the invariant learning principle proposed   \n193 by Arjovsky et al. [18], thus creating environments with distribution shifts. Initially, EIIL is employed   \n194 to split $\\mathcal{D}^{\\check{M}S}$ into two environments. Subsequently, each environment is further divided based on   \n195 sample labels, resulting in $2\\times|\\mathcal{V}|$ environments. To measure the difference between the distribution   \n196 of environments, we define group shift of a class as the absolute difference in the proportion of   \n197 a minority group between two environments of that class. A higher group shift suggests a more   \n198 distinct separation between environments. As detailed in the Appendix, environments inferred by   \n199 EIIL demonstrate an average group shift of $28.7\\%$ over datasets with spurious correlation. Further   \n200 information about EIIL and the group shift quantities for each dataset can be found in the Appendix.   \n201 We demonstrate that even more straightforward techniques, such as applying a random linear layer   \n202 over the feature embedding space and distinguishing environments based on correctly and incorrectly   \n203 classified samples of each class, can be effective to an extent in several cases (See Appendix E.2).   \n204 It underscores that the feature space of a trained model is a valuable resource of information for   \n205 identifying groups affected by spurious correlations. This supports the logic of previous research that   \n206 employs clustering [23] or contrastive methods [24] in this space to differentiate between groups. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "207 3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "208 In this subsection, we provide theoretical insights into why loss-based sampling in a class can be   \n209 utilized to create a balanced dataset of each group under sufficient conditions. We will show the close   \n210 relationship between the existence of a balanced dataset and the difference between the minority vs.   \n211 majority group means, calculated based on the logits of an ERM-trained classifier. Such logits are   \n212 known to depend on spurious features. Hence the mentioned group mean difference is expected to be   \n213 high if spurious features are present in the dataset.   \n214 Consider a binary classification problem with a cross-entropy loss function. Let logits be denoted as   \n215 $L$ . Because loss is a monotonic function of logits, the tails of the distribution of loss across samples   \n216 are equivalent to that of the logits in each class.   \n217 We assume that in feature space (output of $g_{\\boldsymbol{\\theta}}$ ) samples from the minority and majority of a class are   \n218 derived from Gaussian distributions. So, we can consider $\\mathcal{N}(\\mu_{\\mathrm{min}},\\sigma_{\\mathrm{min}}^{2})$ and $\\bar{N}(\\mu_{\\mathrm{maj}},\\sigma_{\\mathrm{maj}}^{2})$ as the   \n219 distribution of minority and majority samples in logits space.   \n220 Proposition 3.1 (Feasiblity Of Loss-based Group Balancing). Suppose that $L$ is derived from the   \n221 mixture of two distributions $\\mathcal{N}(\\mu_{m i n},\\sigma_{m i n}^{2})$ and $\\mathcal{N}(\\mu_{m a j},\\sigma_{m a j}^{2})$ with proportion of $\\varepsilon$ and $1-\\varepsilon$ ,   \n222 respectively, where $\\varepsilon\\leq\\frac{1}{2}$ . Under sufficient (see App.C) and necessary conditions on $\\mu_{m i n}$ , $\\mu_{m a j}$ ,   \n223 $\\sigma_{m i n}$ and $\\sigma_{m a j}$ including inequality $^{\\,l}$ , there exists $\\alpha$ and $\\beta$ such that restricting $L$ to the $\\alpha$ -left and ", "page_idx": 4}, {"type": "image", "img_path": "wH36UKML4x/tmp/c6cfb56651f6f12cb2bf04b3fe15f2ee4debbd6e57cc1fad7eb6211178587181.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: The proportion of minority(majority) samples across different classes within various percentages of $\\bar{\\mathcal{D}}^{L L}$ samples with highest (lowest) loss for the Waterbirds (a) and CelebA (b) datasets. Minority group samples are more prevalent among high-loss samples, while majority group samples dominate the low-loss areas. The error bars are calculated across three ERM models. 1 ", "page_idx": 5}, {"type": "text", "text": "224 $\\beta$ -right tails of its distribution results in a group-balanced distribution; in which both components   \n225 are equally represented. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon\\geq\\mathrm{sigmoid}\\Bigg(-\\frac{\\left(\\mu_{\\mathrm{maj}}-\\mu_{\\mathrm{min}}\\right)^{2}}{2(\\sigma_{\\mathrm{maj}}^{2}-\\sigma_{\\mathrm{min}}^{2})}-\\log\\Big(\\frac{\\sigma_{\\mathrm{maj}}}{\\sigma_{\\mathrm{min}}}\\Big)\\Bigg)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "226 We provide an outline for proof of Proposition 3.1 here and leave the complete and formal proof and   \n227 also exact bounds to Appendix C. We also analyze the conditions and effects of spurious correlation   \n228 in satisfying these conditions. To proceed with the outline we first define a key concept to outline our   \n229 proof. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Proportional Density Difference). For any interval $I=(a,b]$ and a mixture distribution $\\varepsilon P_{1}(x)+(1-\\varepsilon)P_{2}(x)$ , the proportional density difference is defined as the difference of accumulation of two component distributions in the interval $I$ and is denoted by $\\Delta_{\\varepsilon}P_{m i x t u r e}(I)$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{\\varepsilon}P_{m i x t u r e}(I)\\triangleq\\varepsilon P_{1}\\!\\left(x\\in I\\right)-(1-\\varepsilon)P_{2}\\!\\left(x\\in I\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "230 Proof outline Our proof proceeds with three steps. First, we reformulate the theorem as an equality   \n231 of left- and right-tail proportional distribution differences. In other words, we show that the more   \n232 mass the minority distribution has on one tail, the more mass the majority distribution must have on   \n233 the other tail. Afterward, supposing $\\mu_{\\mathrm{min}}<\\mu_{\\mathrm{maj}}$ WOLG, we propose a proper range for $\\beta$ values   \n234 on the right tail. We show that when $\\sigma_{\\mathrm{maj}}\\le\\sigma_{\\mathrm{min}}$ , values for $\\alpha$ trivially exist that can overcome the   \n235 imbalance between the two distributions. In the last step, for the case in which the variance of the   \n236 majority is higher than the minority, we discuss a necessary and sufficient condition for the existence   \n237 of $\\alpha$ and $\\beta$ based on the left-tail proportional density difference using the properties of its derivative   \n238 with respect to $\\alpha$ .   \n239 Condition 1 suggests that for a given degree of spurious correlation $\\epsilon$ and variations $\\sigma_{\\mathrm{maj}},\\sigma_{\\mathrm{min}}$ , an   \n240 essential prerequisite for the efficacy of loss-based sampling is a sufficiently large disparity between   \n241 the mean distributions of minority and majority samples, denoted by $||\\mu_{\\mathrm{maj}}-\\bar{\\mu_{\\mathrm{min}}}||^{2}$ . This indicates   \n242 that the groups should be distinctly separable in the logits space.   \n243 Although the parameters $\\alpha$ and $\\beta$ are theoretically established under certain conditions, their actual   \n244 values are undetermined. Therefore, validation data is necessary to ascertain them. For practicality   \n245 and simplicity in this study, we consider that $\\alpha=\\beta$ and explore its corresponding sample number   \n246 (the count of high- and low-loss samples) from a predefined set of possibilities. By leveraging the   \n247 worst environment accuracy, as elaborated in Section 3.2, we identify the optimal candidate that   \n248 ensures uniform accuracy across all environments. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "249 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "250 In this section, we evaluate the effectiveness of our proposed method through comprehensive experi  \n251 ments on multiple datasets and compare it with various methods and baselines. We begin by briefly   \n252 describing evaluation datasets and then introduce baselines and comparative methods. Finally, we   \n253 report and fully explain the results.   \n254 Datasets Our method, along with other baselines, is evaluated on Waterbirds [7], CelebA [13],   \n255 UrbanCars [14], CivilComments [16], and MultiNLI [15]. As per the study by Yang et al. [4],   \n256 Waterbirds, CelebA, and UrbanCars among these datasets exhibit spurious correlation. Among the   \n257 rest, CivilComments has class and attribute imbalance, whereas MultiNLI exhibits attribute imbalance.   \n258 For additional details on the datasets, please refer to the Appendix.   \n259 Baselines We compare our method with four baselines in addition to standard ERM. GroupDRO [7]   \n260 trains a model on the data with the objective of minimizing its average loss on the minority samples.   \n261 This method requires group labels of both the training and validation sets. DFR [1] argues that   \n262 models trained with ERM are capable of extracting the core features of images. Thus, it first trains a   \n263 model with ERM, and retrains only the last linear classifier layer on a group-balanced subset of the   \n264 validation or the held-out training data. While DFR reduces the number of group-annotated samples,   \n265 it still requires group labels in the training phase. $\\mathbf{GroupDRO+EIIL}$ [12] infers environments of   \n266 the training set and trains a model with GroupDRO on the inferred environments. JTT [5] first trains   \n267 a model with ERM on the dataset, and then retrains it on the dataset by upweighting the samples that   \n268 were misclassified by the initial ERM model. AFR [9] trains a model with ERM on a portion of the   \n269 training set, and retrains the classifier on the weighted held-out training data. The weights assigned to   \n270 retraining samples are based on the loss of the ERM model, upweighting samples from the minority   \n271 groups. Group $\\mathrm{DRO}+\\mathrm{EIIL}$ , JTT and AFR remove the reliance on group annotation in the training   \n272 phase. However, unlike our method, they all require group labels for model selection.   \n273 Setup Similar to all the works mentioned in Section 4, we use ResNet-50 [25] pretrained on   \n274 ImageNet [26] for image classification tasks. We used random crop and random horizontal flip   \n275 as data augmentation, similar to [1]. For a fair comparison with the baselines, we did not employ   \n276 any data augmentation techniques in the process of retraining the last layer of the model. For the   \n277 CivilComments and MultiNLI, we use pretrained BERT [27] and crop sentences to 220 tokens length.   \n278 In EvaLS, we use the implementation of EIIL by spuco package [28] for environments inference on   \n279 the model selection set with 20000 steps, SGD optimizer, and learning rate $10^{-2}$ for all datasets.   \n280 Model selection and hyper-parameter fine-tuning are done according to the worst environment(or   \n281 group if annotations are assumed to be available) accuracy on the validation set. For each dataset,   \n282 we assess the performance of our model in two cases: fine-tuning the ERM classifier or retraining it.   \n283 For all datasets except MultiNLI, retraining yielded better validation results. We report the results   \n284 of our experiments in two settings: (i) EVaLS, which incorporates loss-based instance sampling for   \n285 training the last layer, and environment inference for model selection. (ii) EVaLS-GL, similar to   \n286 EVaLS except in using ground-truth group labels for model selection. For more details on the ERM   \n287 training and last layer re-training hyperparameters refer to the Appendix.   \n289 The results of our experiments along with the reported results on GroupDRO [7], DFR [1], JTT [5],   \n290 and AFR [9] on five datasets are shown in Table 1. The reported results for GroupDRO, DFR, JTT,   \n291 and AFR except those for the UrbanCars are taken from Qiu et al. [9]. For EIIL $^+$ Group DRO, the   \n292 results are reported from Zhang et al. [24]. We report only the worst group accuracy of methods in   \n293 Table 1. The average group accuracies are documented in the Appendix. The Group Info column   \n294 shows whether group annotation is required for training or model selection entry for each method.   \n295 When compared to other methods with the same level of supervision, EVaLS-GL outperforms on four   \n296 of the five datasets, achieving near-optimal worst group accuracy on Waterbirds, demonstrating the   \n297 effectiveness of loss-based sample selection compared to the weighting scheme in AFR [9]. Given   \n298 that AFR employs exponential weights with a temperature parameter to assign a positive weight   \n299 to all samples, proportional to the model\u2019s assigned probability of the correct class, an increase   \n300 in the number of low-loss samples will lead to a corresponding rise in their cumulative weight.   \n301 Consequently, in situations where spurious correlation is high and an uptick in majority samples leads   \n302 to a greater proportion of low-loss over high-loss samples, determining an appropriate parameter   \n303 becomes challenging.   \n304 The comparison between EVaLS and Group $\\mathrm{DRO}+\\mathrm{EIIL}$ indicates that when environments are   \n305 available instead of groups, our method, which uses environments solely for model selection and   \n306 utilizes loss-based sampling, is more effective than GroupDRO, a potent invariant learning method,   \n307 which uses this annotation for training.   \n308 Regarding the UrbanCars, which contains an un-annotated spurious attribute, Li et al. [14] has shown   \n309 that shortcut mitigation methods often struggle to address multiple shortcuts simultaneously. Notably,   \n310 techniques such as DFR [1] which are designed to reduce reliance on a specific shortcut feature,   \n311 cannot make the model robust to an unknown shortcut. In contrast, our experiments suggest that   \n312 loss-based methods can mitigate the impact of both labeled and unlabeled shortcut features more   \n313 effectively. Also, in the case of CivilComments, which is viewed as a benchmark for class imbalance,   \n314 EVaLS-GL exceeds all prior methods, even those with complete group annotation, thanks to the class   \n315 balancing for the training of the last layer.   \n316 Our evaluation of EVaLS is based on the spurious correlation benchmarks. This is because, in   \n317 other instances of subpopulation shift, the attributes that differ across groups are not predictive of   \n318 the label, thereby reducing the visibility of these attributes\u2019 effects in the model\u2019s final layers [29].   \n319 Consequently, EIIL, which depends on output logits for prediction, might not effectively separate   \n320 the groups. This observation is further supported by our findings related to the degree of group   \n321 shift between the environments inferred by EIIL for each class in the CivilComments and MultiNLI   \n322 datasets. The average group shift (defined in the Section 3.2) in the environments of the minority   \n323 class of CivilComments is only $5.6_{\\pm0.8}\\%$ . Also, environments associated with Classes 1 and 2 in   \n324 MultiNLI show only $1.1_{\\pm0.3}\\%$ and $1.9{\\scriptstyle\\pm1.0}\\%$ group shift respectively. More results and ablation   \n325 studies can be found in the Appendix.   \n326 Mitigating Multiple Shortcut Attributes To evaluate the performance of our method in the case   \n327 of unknown spurious correlations, we train a ResNet-18 [25] model on the Dominoes-CMF dataset.   \n328 We apply DFR [1], EVaLS-GL, and EVaLS on top of the trained ERMs to assess their ability to   \n329 mitigate multiple shortcuts. For the last layer training set, we consider the MNIST/Fashion-MNIST   \n330 feature as the known group label, and the color as the unknown attribute. The results are shown in   \n331 Table 2. To clarify, we calculate the worst-group accuracy on the validation set considering only the   \n332 label of one shortcut, i.e., the lowest accuracy among the four groups based on the combination of the   \n333 target label and the single known shortcut label. Note that EVaLS does not require group annotations.   \n334 Our results confirm findings by Li et al. [14], suggesting that methods using group labels mitigate   \n335 reliance on the known shortcut but not necessarily on the unknown one. EVaLS-GL mitigates this   \n336 phenomenon using its loss-based sampling approach, but surprisingly EVaLS even outperforms   \n337 EVaLS-GL. Combining a loss-based sampling approach for last layer training and environment-based   \n338 model selection, results in a completely group-annotation-free method in a multi-shortcut setting and   \n339 successfully re-weights features to perform well with respect to both spurious attributes. ", "page_idx": 5}, {"type": "image", "img_path": "wH36UKML4x/tmp/5513277bf0f52527bdff0fa5232b62654f6b865922ab88346205035e59c9dc30.jpg", "img_caption": ["Figure 3: Two spurious correlations in a dataset. (a) If both spurious attributes are known, they can be utilized to fti a classifier that captures the essential attributes. (b) In the absence of knowledge about both spurious attributes, the model would depend on them for classification, leading to incorrect classification of minority samples. (c) If one spurious attribute is unknown (Spurious 2), the model becomes robust only to the known spurious correlation (Spurious 1), but it still underperforms on minority samples. (d) The Dominoes-CMF dataset, which contains two spurious attributes. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "wH36UKML4x/tmp/de47a44980b2c0757fa93dd5fc6f3a4f151cb19091663f5bac7646a59710cb01.jpg", "table_caption": ["Table 1: A comparison of the worst group accuracy across various methods, ours included, on five datasets. The Group Info column indicates if each method utilizes group labels of the training/validation data, with $\\mathcal{M}$ denoting that group information is employed during both the training and validation stages. Bold numbers are the highest results overall, while underlined ones are the best among methods that may require group annotation only for model selection. CivilComments is class imbalanced, MultiNLI has imbalanced attributes, and the other three datasets have spurious correlations. The $\\times$ sign indicates that the dataset is out of the scope of the method. The mean and standard deviation are calculated over three runs with different seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "wH36UKML4x/tmp/e654ecb03fd9f0869f53b30edf92db51bad129baea005680723d44d33683ddfd.jpg", "table_caption": ["Table 2: Worst test group accuracy of ERM, DFR, EVaLS, and EVaLS-GL on the Dominoes-CMF Dataset. The mean and standard deviation are calculated based on runs with three distinct seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "340 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "341 This study presents EVaLS, a novel approach to improve robustness to spurious correlations with   \n342 zero group annotation. EVaLS uses loss-based sampling to create a balanced training dataset that   \n343 effectively disrupts spurious correlations and employs EIIL to infer environments for model selection.   \n344 We also explore situations with multiple spurious correlations where not all spurious factors are   \n345 known. In this context, we introduce Dominoes-CMF, a dataset in which two factors are spuriously   \n346 correlated with the label, but only one is identified. Our findings suggest that EVaLS attains near  \n347 optimal worst test group accuracy on spurious correlation datasets. We also present EVaLS-GL, which   \n348 needs group labels only for model selection. Our empirical tests on various datasets demonstrate   \n349 EVaLS-GL outperforms state-of-the-art methods requiring group data during evaluation or training.   \n350 Note that this paper remains consistent with the findings of Lin et al. [30]. Our approach does not   \n351 involve identifying spurious attributes without auxiliary information. Instead, the objective is to make   \n352 a trained model robust against its reliance on shortcuts. Specifically, conditioning on what a trained   \n353 model learns, we ascertain that both the loss value and the model\u2019s feature space are instrumental in   \n354 mitigating shortcuts and effectuating notable shifts among groups.   \n355 EVaLS and EVaLS-GL may struggle with small datasets due to a low number of selected samples   \n356 for the last layer training. Also, as environment inference from the last layer features is not effective   \n357 for all types of subpopulation shifts, EVaLS is limited to datasets with spurious correlation. Similar   \n358 to other methods in the field, EVaLS prioritizes the worst group accuracy at the cost of less average   \n359 accuracy. Additionally, a notable variance has been observed in some of our experiments.   \n360 EVaLS represents a significant advancement in the development of methods for enhancing model   \n361 fairness and robustness without prior knowledge about group annotations. Future work could explore   \n362 developing environment inference methods effective for other types of subpopulation shift, such as   \n363 attribute and class imbalance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "364 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "365 [1] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is suffi  \n366 cient for robustness to spurious correlations. In The Eleventh International Conference on Learn  \n367 ing Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ Zb6c8A-Fghk.   \n368 [2] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar. Towards last-layer retraining for   \n369 group robustness with fewer annotations. In Thirty-seventh Conference on Neural Information   \n370 Processing Systems, 2023. URL https://openreview.net/forum?id=kshC3NOP6h.   \n371 [3] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness   \n372 without demographics in repeated loss minimization. In International Conference on Machine   \n373 Learning, pages 1929\u20131938. PMLR, 2018.   \n374 [4] Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer   \n375 look at subpopulation shift. In International Conference on Machine Learning, 2023.   \n376 [5] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori   \n377 Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without   \n378 training group information. In Marina Meila and Tong Zhang, editors, Proceedings of the   \n379 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine   \n380 Learning Research, pages 6781\u20136792. PMLR, 18\u201324 Jul 2021. URL https://proceedings.   \n381 mlr.press/v139/liu21f.html.   \n382 [6] Yu Yang, Eric Gan, Gintare Karolina Dziugaite, and Baharan Mirzasoleiman. Identifying   \n383 spurious biases early in training through the lens of simplicity bias. ArXiv, abs/2305.18761,   \n384 2023. URL https://api.semanticscholar.org/CorpusID:258967752.   \n385 [7] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust   \n386 neural networks. In International Conference on Learning Representations, 2019.   \n387 [8] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving   \n388 worst-group accuracy with spurious attribute estimation. In International Conference on   \n389 Learning Representations, 2021.   \n390 [9] Shikai Qiu, Andres Potapczynski, Pavel Izmailov, and Andrew Gordon Wilson. Simple and fast   \n391 group robustness by automatic feature reweighting. In International Conference on Machine   \n392 Learning, pages 28448\u201328467. PMLR, 2023.   \n393 [10] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:   \n394 De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems,   \n395 33:20673\u201320684, 2020.   \n396 [11] Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast, HamidReza Yaghoubi   \n397 Araghi, and Mahdieh Soleymani Baghshah. Decompose-and-compose: A compositional   \n398 approach to mitigating spurious correlation. CoRR, abs/2402.18919, 2024. doi: 10.48550/   \n399 ARXIV.2402.18919. URL https://doi.org/10.48550/arXiv.2402.18919.   \n400 [12] Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant   \n401 learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International   \n402 Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,   \n403 pages 2189\u20132200. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/   \n404 creager21a.html.   \n405 [13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the   \n406 wild. 2015 IEEE International Conference on Computer Vision (ICCV), pages 3730\u20133738,   \n407 2014. URL https://api.semanticscholar.org/CorpusID:459456.   \n408 [14] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer,   \n409 Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where   \n410 mitigating one amplifies others. In Proceedings of the IEEE/CVF Conference on Computer   \n411 Vision and Pattern Recognition (CVPR), pages 20071\u201320082, June 2023.   \n412 [15] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus   \n413 for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.   \n414 [16] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced   \n415 metrics for measuring unintended bias with real data for text classification. In Companion   \n416 proceedings of the 2019 world wide web conference, pages 491\u2013500, 2019.   \n417 [17] Matteo Pagliardini, Martin Jaggi, Fran\u00e7ois Fleuret, and Sai Praneeth Karimireddy. Agree to   \n418 disagree: Diversity through disagreement for better transferability. In The Eleventh International   \n419 Conference on Learning Representations, 2022.   \n420 [18] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini  \n421 mization, 2020.   \n422 [19] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32\u201333, 2009.   \n423 URL https://www.cs.toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n424 [20] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http:   \n425 //yann.lecun.com/exdb/mnist/.   \n426 [21] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image   \n427 dataset for benchmarking machine learning algorithms, 2017. URL http://arxiv.   \n428 org/abs/1708.07747. cite arxiv:1708.07747Comment: Dataset is freely available at   \n429 https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion  \n430 mnist.s3-website.eu-central-1.amazonaws.com/.   \n431 [22] Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization.   \n432 In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on   \n433 Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6804\u2013   \n434 6814. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/liu21h.   \n435 html.   \n436 [23] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\u00e9. No subclass   \n437 left behind: Fine-grained robustness in coarse-grained classification problems. Advances in   \n438 Neural Information Processing Systems, 33:19339\u201319352, 2020.   \n439 [24] Michael Zhang, Nimit Sharad Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher R\u00e9.   \n440 Correct-n-contrast: A contrastive approach for improving robustness to spurious correlations.   \n441 In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.   \n442 URL https://openreview.net/forum?id=Q41kl_DwS3Y.   \n443 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n444 recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   \n445 pages 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.   \n446 [26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng   \n447 Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual   \n448 recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n449 [27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of   \n450 deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and   \n451 Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter   \n452 of the Association for Computational Linguistics: Human Language Technologies, Volume 1   \n453 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association   \n454 for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.   \n455 org/N19-1423.   \n456 [28] Siddharth Joshi, Yu Yang, Yihao Xue, Wenhan Yang, and Baharan Mirzasoleiman. To  \n457 wards mitigating spurious correlations in the wild: A benchmark & a more realistic dataset.   \n458 ArXiv, abs/2306.11957, 2023. URL https://api.semanticscholar.org/CorpusID:   \n459 259211935.   \n460 [29] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and   \n461 Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh   \n462 International Conference on Learning Representations, 2023. URL https://openreview.   \n463 net/forum?id=APuPRxjHvZ.   \n464 [30] Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without   \n465 environment partition? In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,   \n466 editors, Advances in Neural Information Processing Systems, volume 35, pages 24529\u201324542.   \n467 Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/   \n468 paper/2022/file/9b77f07301b1ef1fe810aae96c12cb7b-Paper-Conference.pdf.   \n469 [31] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui.   \n470 Towards out-of-distribution generalization: A survey. ArXiv, abs/2108.13624, 2021. URL   \n471 https://api.semanticscholar.org/CorpusID:237364121.   \n472 [32] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Information-theoretic bias reduction via   \n473 causal view of spurious correlation. In Proceedings of the AAAI Conference on Artificial   \n474 Intelligence, volume 36, pages 2180\u20132188, 2022.   \n475 [33] Yuzhen Mao, Zhun Deng, Huaxiu Yao, Ting Ye, Kenji Kawaguchi, and James Zou. Last-layer   \n476 fairness fine-tuning is simple and effective for neural networks. arXiv preprint arXiv:2304.03935,   \n477 2023.   \n478 [34] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai   \n479 Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapo  \n480 lation (rex). In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International   \n481 Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,   \n482 pages 5815\u20135826. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/   \n483 krueger21a.html.   \n484 [35] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for   \n485 out-of-distribution generalization. In International Conference on Machine Learning, pages   \n486 18347\u201318377. PMLR, 2022.   \n487 [36] Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron Courville. Systematic generalisation   \n488 with group invariant predictions. In International Conference on Learning Representations,   \n489 2021. URL https://openreview.net/forum?id $\\equiv$ b9PoimzZFJ.   \n490 [37] Matteo Pagliardini, Martin Jaggi, Fran\u00e7ois Fleuret, and Sai Praneeth Karimireddy. Agree   \n491 to disagree: Diversity through disagreement for better transferability. arXiv preprint,   \n492 arXiv:2202.04414, 2022.   \n493 [38] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli.   \n494 The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing   \n495 Systems, 33:9573\u20139585, 2020.   \n496 [39] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay   \n497 Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,   \n498 Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure   \n499 Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.   \n500 Wilds: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang,   \n501 editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of   \n502 Proceedings of Machine Learning Research, pages 5637\u20135664. PMLR, 18\u201324 Jul 2021. URL   \n503 https://proceedings.mlr.press/v139/koh21a.html.   \n504 [40] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings   \n505 of the European Conference on Computer Vision (ECCV), September 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "507 Robustness to spurious correlation is a critical concern across various machine learning subfields.   \n508 It is a form of out-of-distribution generalization [31] where the distribution shift arises from the   \n509 disproportionate representation of minority groups\u2014those instances that are devoid of the correlated   \n510 spurious patterns associated with their labels [4]. The issue of spurious correlation also intersects   \n511 with the discourse on fairness in machine learning. [32, 33]. Past studies have proposed a range of strategies to mitigate the models\u2019 reliance on spurious correlation. Broadly speaking, these methods can be categorized according to the degree of supervision they   \n514 require regarding group labels. Invariant learning (IL) methods [18, 34, 35] operate under the assumption of having access to a collection of environments that comprise group shift. By imposing invariant conditions on these environments, IL methods strive to create classifiers robust against group-sensitive features. IRM [18] is   \n518 designed to learn a feature extractor, which, when utilized, guarantees the existence of a classifier that   \n519 would be optimal in all training environments. VREx [34] aims to decrease the risk variance among   \n520 different training environments. PGI [36] works by minimizing the distance between the expected   \n521 softmax distribution of labels, conditioned on inputs across both majority and minority environments.   \n522 Lastly, Fishr [35] focuses on bringing the variance of risk gradients closer together across different   \n523 training environments. For scenarios which environments are not available, environment inference   \n524 methods [12, 22] are used to obtain a set of environments. Creager et al. [12] introduce environment inference for invariant learning (EIIL), which tries to partition samples into two groups such that the objective of IRM [18] is maximized. HRM [22] aims to optimize both an environment inference module and an invariant prediction module jointly, with the goal of achieving an invariant predictor. When group annotations are accessible, various methods leverage this information to equalize the   \n529 impact of different groups on the model\u2019s loss. The Group Distributionally Robust Optimization   \n530 (GDRO) approach [7], for instance, focuses on optimizing the loss for the worst-performing group   \n531 during training. Kirichenko et al. [1] has shown that models can still learn and extract core data   \n532 features even in the presence high spurious correlation. Consequently, They suggest that retraining   \n533 just the last layer of a model initially trained with Empirical Risk Minimization (ERM) can effectively   \n534 reduce reliance on spurious correlation for predicting class labels. This method, termed Deep Feature Re-weighting (DFR), has been validated as not only highly effective but also significantly more efficient than earlier techniques that necessitated retraining the full model [8, 7]. However, availability   \n537 of group annotations is considered a serious restrictive assumption. Several recent studies have endeavored to enhance model robustness against spurious correlation, even in the absence of group annotations [5, 24, 9, 2, 6]. Liu et al. [5] introduce a two-stage method   \n540 that involves training a model using ERM for a number of epochs before retraining it to give more   \n541 weight to misclassified samples. The study by Zhang et al. [24] employs the same two-stage training   \n542 process, but with a twist for the second stage: they utilize contrastive methods. The goal is to   \n543 bring samples from the same class but with divergent predictions closer in the feature space, while   \n544 simultaneously increasing the separation between samples from different classes that have similar   \n545 predictions. Another method, known as automatic feature reweighting (AFR) [9], reweights the last   \n546 layer of an ERM-pretrained model to favor samples that the original model was less accurate on.   \n547 LaBonte et al. [2] refine the last layer of an ERM-trained model through class-balanced finetuning,   \n548 identifying challenging data points by comparing the classifier\u2019s predictions with those of an early  \n549 stopped version. While these methods have significantly reduced the reliance on group annotations, some are still required for validation and model selection. This remains a constraint, particularly   \n551 when the spurious correlation is completely unknown. ", "page_idx": 12}, {"type": "text", "text": "552 For making a trained model robust to spurious correlation with zero group annotations, recently,   \n553 LaBonte et al. [2] have empirically demonstrated that the class-balanced retraining of a model   \n554 pretrained with ERM can effectively improve the WGA for certain datasets. However, this approach   \n555 fails in datasets with a high degree of spurious correlation. ", "page_idx": 12}, {"type": "table", "img_path": "wH36UKML4x/tmp/3f9ee9a8712eb11d4473e4da8c7bc2f935341241f2de01a0d44736691a3e8797.jpg", "table_caption": ["Table 3: The average and variation percentage $(\\%)$ (across 3 seeds) of group shift between the inferred environments using EIIL [12] for each class, which is the absolute difference between the proportion of a minority group in the two environments of a class. Higher group shift indicates better separation of environments. In most cases, a significant group shift is observed between the inferred environments. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "556 B Environment Inference for Invariant Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "557 Consider the training dataset $\\mathcal{D}^{t r}=\\{(\\boldsymbol{x}^{(i)},\\boldsymbol{y}^{(i)})|\\boldsymbol{x}^{(i)}\\in\\mathcal{X},\\boldsymbol{y}^{(i)}\\in\\mathcal{Y}\\}$ , where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ represent the   \n558 input and output spaces, respectively. This dataset can be partitioned into different environments   \n559 $\\mathcal{E}^{\\bar{t r}}=\\{e_{1},...,e_{n}\\}$ , such that for any $i\\neq j$ , the data distribution in $e_{i}$ and $e_{j}$ differs. The objective   \n560 of invariant learning is to train a predictor that performs consistently across all environments in ${\\mathcal{E}}^{t r}$ .   \n561 Under certain conditions, this predictor is also expected to perform well on $e^{t s t}$ , a test environment   \n562 with a distribution distinct from the training data. Invariant Risk Minimization (IRM) [18] approaches   \n563 this problem by learning a feature extractor $\\Phi(.)$ such that a classifier $\\omega(.)$ exists, where $\\omega\\circ\\Phi(.)$   \n564 performs consistently across all training environments. The practical implementation of the IRM   \n565 objective is to minimize ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{e\\in\\mathcal{E}^{t r}}R^{e}(\\Phi)+\\lambda||\\nabla_{\\bar{\\omega}}R^{e}(\\bar{\\omega}\\circ\\Phi)||^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "566 where $\\bar{\\omega}$ is a constant scalar with a value of $1.0,\\ \\lambda$ is a hyperparameter, and $R^{e}(f)\\ \\,=$   \n567 $\\mathbf{E}_{(x,y)\\sim p_{e}}[l(f(x),y)]$ is referred to as the risk on environment $e$ .   \n568 In real-world scenarios, training environments might not always be available. To address this,   \n569 Environment Inference for Invariant Learning (EIIL) [12] partitions samples into two environments   \n570 in a way that maximizes the objective in Eq 2.   \n571 During the training phase, the EIIL algorithm replaces the hard assignment of environments to   \n572 samples with a soft assignment $\\mathbf{q}_{i}(e)=p(e|(x^{(i)},y^{(i)})$ , where $\\mathbf{q}_{i}$ is learnable. Consequently, the   \n573 relaxed version of the risk function is defined as $\\begin{array}{r}{\\tilde{R}^{e}(\\Phi)=\\frac{1}{N}\\sum_{i}^{N}\\mathbf{q}_{i}(e)[l(\\Phi(x^{(i)}),y^{(i)})]}\\end{array}$ . Given a   \n574 model that has been trained with ERM on the dataset, EIIL optimizes ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{q}^{*}=\\arg\\operatorname*{max}_{\\mathbf{q}}||\\nabla_{\\bar{\\omega}}\\tilde{R}^{e}(\\bar{\\omega}\\circ\\Phi)||.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "575 As discussed in Creager et al. [12], using a biased base model $\\Phi$ could lead to environments exhibiting   \n576 varying degrees of spurious correlation. During the inference phase, the soft assignment is converted   \n577 to a hard assignment. The average group shift between the inferred environments using EIIL is   \n578 illustrated in Table 3. ", "page_idx": 13}, {"type": "text", "text": "579 C Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "580 In this section, we establish a more formal description of loss-based sampling for balanced dataset   \n581 creation and then prove it. We thoroughly analyze the close relationship between the availability of   \n582 the balanced dataset and the gap between spurious features of minority and majority groups.   \n583 Consider a binary classification problem with a cross-entropy loss function. Let logits be denoted   \n584 as $L$ . Because loss is a monotonic function of logits, the tails of the distribution of loss across   \n585 samples are equivalent to that of the logits in each class. We assume that in feature space (output   \n586 of $g_{\\boldsymbol{\\theta}}{\\overset{}{\\underset{}{}}}$ ) samples from the minority and majority of a class are derived from Gaussian distributions   \n587 $\\tilde{\\mathcal{N}}(h_{\\operatorname*{min}},t_{\\operatorname*{min}}^{2^{\\star}}I_{d})$ and $\\mathcal{N}(h_{\\operatorname*{maj}},t_{\\operatorname*{maj}}^{2}\\dot{I}_{d})$ , respectively. Before diving into the group balance problem we   \n588 initially show that the distribution of minority and majority samples in the logit space (output of $h_{\\phi}$ )   \n589 are Gaussian too.   \n590 Lemma C.1 (Gaussain Distribution of Logits). if $\\ 'Z\\sim\\mathcal{N}(h,t^{2}I_{d})$ in feature space and $W\\in\\mathbb{R}^{d}$   \n591 then logits $L=\\langle W,Z\\rangle\\sim{\\mathcal{N}}{\\big(}W h,\\,t^{2}\\left\\|W\\right\\|^{2}{\\big)}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "592 Proof. Let $Z\\sim\\mathcal{N}(h,t^{2}I_{d})$ . ", "page_idx": 14}, {"type": "text", "text": "593 Consider the linear combination $L\\,=\\,\\langle W,Z\\rangle\\,=\\,W^{T}Z$ , where $W\\,\\in\\,\\mathbb{R}^{d}$ which is a univariate   \n594 gaussian. ", "page_idx": 14}, {"type": "text", "text": "595 To find the distribution of $L$ , we need to determine its mean and variance. ", "page_idx": 14}, {"type": "text", "text": "596 1. Mean of $L$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{\\mathbb{E}}[L]=\\operatorname{\\mathbb{E}}[\\langle W,Z\\rangle]=\\operatorname{\\mathbb{E}}[W^{T}Z]=W^{T}\\operatorname{\\mathbb{E}}[Z]=W^{T}h=\\langle W,h\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "597 Therefore, the mean of $L$ is $W h$ . ", "page_idx": 14}, {"type": "text", "text": "598 2. Variance of $L$ : ", "page_idx": 14}, {"type": "text", "text": "599 The variance of $L$ can be computed using the properties of covariance. Recall that if $Z\\sim\\mathcal{N}(h,t^{2}I_{d})$ ,   \n600 then the covariance matrix of $Z$ is $t^{2}I_{d}$ . ", "page_idx": 14}, {"type": "text", "text": "601 The variance of the linear combination $L=W^{T}Z$ is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}(L)=\\mathrm{Var}(W^{T}Z)=W^{T}\\mathrm{Cov}(Z)W.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "602 Given $\\mathrm{Cov}(Z)=t^{2}I_{d}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}(L)=W^{T}(t^{2}I_{d})W=t^{2}W^{T}I_{d}W=t^{2}\\left\\|W\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "603 where $\\|W\\|$ denotes the Euclidean norm of $W$ ", "page_idx": 14}, {"type": "text", "text": "604 Combining the mean and variance results, we conclude that $L$ is normally distributed with mean $W h$   \n605 and variance $t^{2}\\left\\|W\\right\\|^{2}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nL=\\left\\langle W,Z\\right\\rangle\\sim\\mathcal{N}(W h,t^{2}\\left\\lVert W\\right\\rVert^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "606 Thus, we have proved that if $Z\\sim\\mathcal{N}(h,t^{2}I_{d})$ , then the logits $L=\\langle W,Z\\rangle$ follow the distribution   \n607 $\\mathcal{N}(W h,t^{2}\\left\\lVert W\\right\\rVert^{2})$ . \u53e3   \n608 From now on, we consider $\\mathcal{N}(\\mu_{\\mathrm{min}},\\sigma_{\\mathrm{min}}^{2})$ and $\\mathcal{N}(\\mu_{\\mathrm{maj}},\\sigma_{\\mathrm{maj}}^{2})$ as the distribution of minority and   \n609 majority samples in logits space.   \n610 Next, we prove the more formal version of the main proposition 3.1 which describes the existence of   \n611 a balanced dataset, only after we define a key concept, proportional density difference (illustrated in   \n612 figure 4) to outline our proof. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Definition C.1 (Proportional Density Difference). For any interval $I=(a,b]$ and a mixture distribution $\\varepsilon P_{1}(x)+(1-\\varepsilon)P_{2}(x),$ , proportional density difference is defined by the difference of accumulation of two component distributions in the interval $I$ and is denoted by $\\Delta_{\\varepsilon}P_{m i x t u r e}(I)$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\varepsilon}P_{m i x t u r e}(I)\\triangleq\\varepsilon P_{1}\\!\\left(x\\in I\\right)-(1-\\varepsilon)P_{2}\\!\\left(x\\in I\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "613 Definition C.2 (Tail Proportional Density Difference). For a mixture distribution $\\varepsilon P_{1}(x)+(1-$   \n614 $\\varepsilon)P_{2}(x)$ , we define tail $\\mathbf{\\Omega}_{L}(\\alpha)$ as $\\Delta_{\\varepsilon}P_{m i x t u r e}\\Big((-\\infty,\\alpha]\\Big)$ and tail $\\begin{array}{r}{\\phantom{\\omega}_{R}(\\beta)\\,a s-\\Delta_{\\varepsilon}P_{m i x t u r e}\\Big((\\beta,+\\infty)\\Big)}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Corollary C.1. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{t a i l_{L}(\\alpha)=\\varepsilon F^{1}(\\alpha)-(1-\\varepsilon)F^{2}(\\beta)}}\\\\ {{t a i l_{R}(\\alpha)=(1-\\varepsilon)\\big[1-F^{2}(\\beta)\\big]-\\varepsilon\\big[1-F^{1}(\\beta)\\big]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "615 where $F^{1}$ and $F^{2}$ are CDF of two component distributions. ", "page_idx": 15}, {"type": "image", "img_path": "wH36UKML4x/tmp/a53236d23f3920a154f232d9e0ca9fcf3cab81f84cd8c41f789c45d78b974c60.jpg", "img_caption": ["Figure 4: (a) Illustration of proportion density difference C.1, (b) equation of $t a i l_{L}(\\alpha)=t a i l_{R}(\\beta)$ at C.2. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "616 Proposition C.1 (Feasiblity Of Loss-based Group Balancing). Suppose that $L$ is derived from   \n617 the mixture of two distributions $\\mathcal{N}(\\mu_{m i n},\\sigma_{m i n}^{2})$ and $\\mathcal{N}(\\mu_{m a j},\\sigma_{m a j}^{\\bar{2}})$ with proportion of $\\varepsilon$ and $1-\\varepsilon$ ,   \n618 respectively, where $\\varepsilon\\leq\\frac{1}{2}$ . There exists $\\alpha$ and $\\beta$ such that restricting $L$ to the $\\alpha$ -left and $\\beta$ -right tails   \n619 of its distribution results in a group-balanced distribution if and only if $\\dot{\\sigma}_{m i n}\\geq\\sigma_{m a j}$ or ", "page_idx": 15}, {"type": "equation", "text": "$$\nt a i l_{L}(\\frac{-B+\\sqrt{\\Delta}}{2A})>0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "620 and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon\\geq s i g m o i d\\Bigg(-\\frac{\\left(\\mu_{m a j}-\\mu_{m i n}\\right)^{2}}{2(\\sigma_{m a j}^{2}-\\sigma_{m i n}^{2})}-\\log\\left(\\frac{\\sigma_{m a j}}{\\sigma_{m i n}}\\right)\\Bigg)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "621 where $\\begin{array}{l l l}{{A}}&{{=}}&{{\\left(\\frac{1}{2\\sigma_{m a j}^{2}}\\ -\\ \\frac{1}{2\\sigma_{m i n}^{2}}\\right)}}\\end{array}$ , $\\begin{array}{r c l}{B}&{=}&{\\bigg(\\frac{\\mu_{m i n}}{\\sigma_{m i n}^{2}}~-~\\frac{\\mu_{m a j}}{\\sigma_{m a j}^{2}}\\bigg)}\\end{array}$ and $\\begin{array}{r l r}{\\Delta}&{{}\\!=\\!}&{{\\frac{(\\mu_{m i n}-\\mu_{m a j})^{2}}{\\sigma_{m i n}^{2}\\sigma_{m a j}^{2}}}\\,-\\,4\\Big[\\log\\Big({\\frac{\\sigma_{m a j}}{\\sigma_{m i n}}}\\Big)\\,+}\\end{array}$   \n622 $\\begin{array}{r}{\\log\\left(\\frac{\\epsilon}{1-\\epsilon}\\right)\\biggr[\\frac{1}{2\\sigma_{m a j}^{2}}-\\frac{1}{2\\sigma_{m i n}^{2}}\\biggr]}\\end{array}$   \n623 Proof outline Our proof proceeds with three steps. First, we reformulate the theorem as an equality   \n624 of left- and right-tail proportional distribution differences. In other words, we show that the more   \n625 mass the minority distribution has on one tail, the more mass the majority distribution must have on   \n626 the other tail. Afterward, supposing $\\mu_{\\mathrm{min}}<\\mu_{\\mathrm{maj}}$ WOLG, we propose a proper range for $\\beta$ values   \n627 on the right tail. We show that when $\\sigma_{\\mathrm{maj}}\\le\\sigma_{\\mathrm{min}}$ , values for $\\alpha$ trivially exist that can overcome the   \n628 imbalance between the two distributions. In the last step, for the case in which the variance of the   \n629 majority is higher than the minority, we discuss a necessary and sufficient condition for the existence   \n630 of $\\alpha$ and $\\beta$ based on the left-tail proportional density difference using the properties of its derivative   \n631 with respect to $\\alpha$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "632 Step 1 Reformulating the problem based on proportional distribution difference. ", "page_idx": 16}, {"type": "text", "text": "633 We introduce a utility random variable Logit Value Tier as $T$ , which is defined as a function of a   \n634 random variable $L$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\alpha,\\beta}=\\left\\{\\!\\!\\begin{array}{l l}{{H i g h}}&{{\\mathrm{if~}L\\geq\\beta}}\\\\ {{M i d}}&{{\\mathrm{if~}\\alpha<L<\\beta}}\\\\ {{L o w}}&{{\\mathrm{if~}L\\leq\\alpha}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "635 We can rewrite the problem in formal form as finding an $\\alpha$ and $\\beta$ which satisfies the following   \n636 equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nP\\Big(g=\\operatorname*{min}\\Big|T_{\\alpha,\\beta}\\neq\\ M i d\\Big)=P\\Big(g=\\operatorname*{maj}\\Big|T_{\\alpha,\\beta}\\neq M i d\\Big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "637 Equation 5 now can be rewritten to a more suitable form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{F(\\ell=\\operatorname*{min}_{1}\\ldots\\psi,\\,\\mathbb{A}\\,\\mathbb{A})=F\\left(\\ell=\\psi_{0}-\\psi\\otimes\\mathbb{A}\\right)\\!\\!\\!\\!\\!\\!-\\!F(\\ell=\\psi)}\\\\ &{\\iff}&{F\\Big(T_{\\alpha,\\ell},\\,\\hat{\\psi}\\,\\mathbb{A}\\,\\mathbb{A}|\\phi|_{\\ell=0}\\!\\!\\!\\!\\Big)\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "638 We can see the left side of equation 17 is just a function of alpha. The same goes for the right side of   \n639 the equation which is a function of $\\beta$ .   \n640 Rewriting the left side of the equation as $t a i l_{L}(\\alpha)$ and right side as $t a i l_{R}(\\beta)$ , the problem is now   \n641 reduced to finding an $\\alpha$ and $\\beta$ that satisfies ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nt a i l_{L}(\\alpha)=t a i l_{R}(\\beta)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "642 which is shown in figure 4. ", "page_idx": 16}, {"type": "text", "text": "643 Before reaching out to step two we discuss the properties of $t a i l_{L}$ and $t a i l_{R}$ in Lemma C.2. ", "page_idx": 16}, {"type": "text", "text": "644 Lemma C.2. tail $\\mathbf{\\Omega}_{L}(\\alpha)$ and tail $_R^{}(\\beta)$ are continuous functions and $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\to-\\infty}t a i l_{L}(\\alpha)~=~0,}\\end{array}$ ,   \n645 $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\rightarrow+\\infty}t a i l_{L}(\\alpha)=2\\varepsilon-1<0}\\end{array}$ , $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\to+\\infty}t a i l_{R}(\\beta)=0}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\rightarrow-\\infty}t a i l_{R}(\\beta)=1-2\\varepsilon>0}\\end{array}$ .   \n646 ", "page_idx": 16}, {"type": "text", "text": "647 Proof. Simply proved by the definition of tail functions and properties of CDF. ", "page_idx": 16}, {"type": "text", "text": "648 Step 2 Solving the equation 18 for simple cases. ", "page_idx": 16}, {"type": "text", "text": "649 Lemma C.3. tai $\\begin{array}{r}{{}^{l}R\\big(\\mu_{m a j}\\big)>\\frac{1}{2}-\\varepsilon\\geq0}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t a i l_{R}(\\mu_{\\mathrm{maj}})=(1-\\varepsilon)\\Bigl[1-F^{\\mathrm{maj}}(\\mu_{\\mathrm{maj}})\\Bigr]-\\varepsilon\\Bigl[1-F^{\\mathrm{min}}(\\mu_{\\mathrm{maj}})\\Bigr]}\\\\ &{\\qquad\\qquad=(1-\\varepsilon)\\Bigl[1-\\phi(0)\\Bigr]-\\varepsilon\\Bigl[1-\\phi\\bigl(\\frac{\\mu_{\\mathrm{maj}}-\\mu_{\\mathrm{min}}}{\\sigma_{\\mathrm{min}}}\\bigr)\\Bigr]}\\\\ &{\\qquad\\qquad>\\frac{(1-\\varepsilon)}2-\\varepsilon\\bigl(1-\\frac12\\bigr)=\\frac{1-2\\varepsilon}2=\\frac12-\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "650 ", "page_idx": 17}, {"type": "text", "text": "651 Corollary C.2. Because tail $R$ is continuous and $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\to+\\infty}t a i l_{R}(\\beta)=0;}\\end{array}$ , based on the mean value   \n652 theorem, any value between zero and $\\frac{(1\\!-\\!2\\varepsilon)}{2}$ is obtainable by selecting a $\\beta$ in $[\\mu_{2},+\\infty)$ .   \n653 According to the previous corollary C.2 finding a positive $t a i l_{L}(\\alpha)$ will satisfy our need. to find a   \n654 suitable point, we employ derivatives and properties of relative PDFs to maximize tail $\\mathbf{\\Omega}_{L}(\\alpha)$ and find   \n655 a positive value. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}t a i l_{L}(\\alpha)}{\\mathrm{d}\\alpha}=\\varepsilon f^{\\mathrm{min}}(\\alpha)-(1-\\varepsilon)f^{\\mathrm{maj}}(\\alpha)=\\varepsilon f^{\\mathrm{maj}}(\\alpha)\\Big[\\frac{f^{\\mathrm{min}}(\\alpha)}{f^{\\mathrm{maj}}(\\alpha)}-\\frac{1-\\varepsilon}{\\varepsilon}\\Big]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "656 The term [ ff  mmianj((\u03b1\u03b1)) \u22121\u03b5\u2212 \u03b5] has the same sign with derivative of tailL(\u03b1), also it\u2019s roots are critical   \n657 points of $t a i l_{L}$ , analyzing characteristics of log $\\frac{f^{\\operatorname*{min}}(\\alpha)}{f^{\\operatorname*{maj}}(\\alpha)}$ is the key insight to find a proper $\\alpha$ value. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log f^{\\operatorname*{min}}(\\alpha)-\\log f^{\\operatorname*{maj}}(\\alpha)=\\log\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Rightarrow\\log\\left(\\frac{\\sigma_{\\mathrm{maj}}}{\\sigma_{\\mathrm{min}}}\\right)-\\log\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)-\\frac{(\\alpha-\\mu_{\\mathrm{min}})^{2}}{2\\sigma_{\\mathrm{min}}^{2}}+\\frac{(\\alpha-\\mu_{\\mathrm{maj}})^{2}}{2\\sigma_{\\mathrm{maj}}^{2}}=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Rightarrow\\bigg(\\frac{1}{2\\sigma_{\\mathrm{maj}}^{2}}-\\frac{1}{2\\sigma_{\\mathrm{min}}^{2}}\\bigg)\\alpha^{2}+\\bigg(\\frac{\\mu_{\\mathrm{min}}}{\\sigma_{\\mathrm{min}}^{2}}-\\frac{\\mu_{\\mathrm{mij}}}{\\sigma_{\\mathrm{maj}}^{2}}\\bigg)\\alpha+\\bigg[\\frac{\\mu_{\\mathrm{maj}}^{2}}{2\\sigma_{\\mathrm{maj}}^{2}}-\\frac{\\mu_{\\mathrm{min}}^{2}}{2\\sigma_{\\mathrm{min}}^{2}}+\\log\\bigg(\\frac{\\sigma_{\\mathrm{maj}}}{\\sigma_{\\mathrm{min}}}\\bigg)+\\log\\bigg(\\frac{\\epsilon}{1-\\epsilon}\\bigg)\\bigg]=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "658 Because $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\to-\\infty}t a i l_{L}(\\alpha)=0}\\end{array}$ and $\\mathrm{lim}_{\\beta\\to+\\infty}\\,t a i l_{R}(\\beta)<0$ to have a positive $t a i l_{L}(\\alpha)$ we need   \n659 to have an interval which $\\frac{\\mathrm{d}t a i l_{L}(\\alpha)}{\\mathrm{d}\\alpha}$ is positive, for a second degree polynomial like $a x^{2}+b x+c$ to   \n660 have positive value, either $a\\geq0$ or $\\Delta>0$ , in our case $a$ is $\\left({\\frac{1}{\\sigma_{\\mathrm{maj}}^{2}}}-{\\frac{1}{\\sigma_{\\mathrm{min}}^{2}}}\\right)$ . if $\\sigma_{\\mathrm{min}}\\geq\\sigma_{\\mathrm{maj}}$ then $a\\geq0$   \n661 and the minority CDF function will dominate the majority CDF function in the left-side tail and by   \n662 choosing a negative number with big enough absolute value for alpha and $t a i l_{L}(\\alpha)$ will be positive. ", "page_idx": 17}, {"type": "text", "text": "663 Step 3 Solving equation $I8$ for special case $\\sigma_{m i n}<\\sigma_{m a j}$ In case of $\\sigma_{\\mathrm{min}}\\le\\sigma_{\\mathrm{maj}}$ , having $\\Delta>0$ $t a i l_{L}(\\alpha)$ $\\textstyle\\big(\\frac{-b-\\sqrt{\\Delta}}{2a},\\frac{-b+\\sqrt{\\Delta}}{2a}\\big)$ 665 maximum of $t a i l_{L}$ is either in $-\\infty$ or in $\\begin{array}{r}{\\frac{-b+\\sqrt{\\Delta}}{2a_{.}}}\\end{array}$ . Having $\\begin{array}{r}{t a i l_{L}\\big(\\frac{-b+\\sqrt{\\Delta}}{2a}\\big)\\,>\\,0\\,}\\end{array}$ next to $\\Delta\\:>\\:0$ 666 condition, would be the necessary and also sufficient in this case. ", "page_idx": 17}, {"type": "equation", "text": "$$\nB^{2}=\\frac{\\mu_{\\mathrm{min}}^{2}}{\\sigma_{\\mathrm{min}}^{4}}+\\frac{\\mu_{\\mathrm{maj}}^{2}}{\\sigma_{\\mathrm{maj}}^{4}}-2\\frac{\\mu_{\\mathrm{maj}}\\mu_{\\mathrm{min}}}{\\sigma_{\\mathrm{maj}}^{2}\\sigma_{\\mathrm{min}}^{2}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n4A C=\\frac{\\mu_{\\mathrm{min}}^{2}}{\\sigma_{\\mathrm{min}}^{4}}-\\frac{\\mu_{\\mathrm{min}}^{2}}{\\sigma_{\\mathrm{miq}}^{2}\\sigma_{\\mathrm{min}}^{2}}-\\frac{\\mu_{\\mathrm{maj}}^{2}}{\\sigma_{\\mathrm{maj}}^{2}\\sigma_{\\mathrm{min}}^{2}}+\\frac{\\mu_{\\mathrm{maj}}^{2}}{\\sigma_{\\mathrm{maj}}^{4}}+4\\left[\\log\\left(\\frac{\\sigma_{\\mathrm{mij}}}{\\sigma_{\\mathrm{min}}}\\right)+\\log\\left(\\frac{\\epsilon}{1-\\epsilon}\\right)\\right]\\left[\\frac{1}{2\\sigma_{\\mathrm{maj}}^{2}}-\\frac{1}{2\\sigma_{\\mathrm{min}}^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "wH36UKML4x/tmp/1724a9f3bb7ebbfea5319c90c2f038d1f380b61c53e9c84f5ab6f8fd993b7b92.jpg", "img_caption": ["Figure 5: Tail thresholds for three cases: (a) minority group variance is less than majority $(\\sigma_{\\mathrm{min}}<$ $\\sigma_{\\mathrm{maj}}),$ ), (b) the variance of two groups are equal $(\\sigma_{\\mathrm{min}}=\\sigma_{\\mathrm{maj}}$ ) and (c) the variance of the minority group is more than majority $(\\sigma_{\\mathrm{min}}>\\sigma_{\\mathrm{maj}})$ ). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "wH36UKML4x/tmp/95070af764c06c62da70faca663fa4ddbf39c9e81af6256a1d733ab1ec0b6b57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "667 Next, we investigate properties of the conditions of the proposition C.1 in case of $\\sigma_{\\mathrm{maj}}~<~\\sigma_{\\mathrm{min}}$ .   \n668 Schematic interpretation of these conditions is presented in figure 6.   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 As equation 5 indicates, the minority group is not allowed to be too underrepresented. This especially has a direct relation with the difference of means. The more mean values of groups are different, the more imbalance can be mitigated through loss-based sampling. Mean value difference is especially affected by the spurious correlation, it escalates as the model relies on spurious correlation and also when the spurious features between groups are too different. \u2022 On the other hand condition 4 is more complex and doesn\u2019t have a simple closed form, we analytically describe its behaviors by fixating the means and calculating the valid values for $\\varepsilon$ . As the results show in figure 6, most of $\\varepsilon$ are feasible in for $\\sigma_{\\mathrm{min}}<\\Delta\\mu$ as we can see the possible region declines with an increase of $\\sigma_{\\mathrm{min}}$ and valid $\\varepsilon$ values cease to exist. ", "page_idx": 18}, {"type": "image", "img_path": "wH36UKML4x/tmp/8ecc90ef07d6549d30f725e28b7d56fa983038f0f2ec749798e55ccc89274e74.jpg", "img_caption": ["Figure 6: (a) Conditions if $\\sigma_{\\mathrm{min}}\\,>\\,\\sigma_{\\mathrm{maj}}$ , (b), (c), (d) Minimum, maximum and interval length of feasible $\\varepsilon$ values across $\\left(\\sigma_{\\mathrm{min}},\\sigma_{\\mathrm{maj}}\\right)$ field for $\\mu_{\\mathrm{min}}=0$ , $\\mu_{\\mathrm{maj}}=1$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: A comparison of the various methods, ours included, on spurious correlation datasets. The Group Info column indicates if each method utilizes group labels of the training/validation data, with $\\mathcal{M}$ denoting that group information is employed during both the training and validation stages. Both the average test accuracy and worst test group accuracy are reported. The mean and standard deviation are calculated over three runs with different seeds. The numbers in bold represent the highest results among all methods, while the underlined numbers represent the best results among methods that do not require group annotation in the training phase. ", "page_idx": 20}, {"type": "table", "img_path": "wH36UKML4x/tmp/faccb322f869a6cbc124966425a3eff731574228f69027dda33f0f2f6b29aa8c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 5: A comparison of the various methods, ours included, on CivilComments and MultiNLI. The Group Info column indicates if each method utilizes group labels of the training/validation data, with $\\mathcal{M}$ denoting that group information is employed during both the training and validation stages. Both the average test accuracy and worst test group accuracy are reported. The mean and standard deviation are calculated over three runs with different seeds. The numbers in bold represent the highest results among all methods, while the underlined numbers represent the best results among methods that do not require group annotation in the training phase. ", "page_idx": 20}, {"type": "table", "img_path": "wH36UKML4x/tmp/df97188b39e6abd2bd6234ac137bd3bce6cb1b1a4bbc07c79d601ce90bececf3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "679 D Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "680 D.1 Complete Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "681 The complete results on Waterbirds, CelebA, and UrbanCars, in addition to complete results on   \n682 CivilComments and MultiNLI are reported in Tables 4 and 5 respectively. The results for all methods   \n683 except Group $\\mathrm{DRO}+\\mathrm{EIIL}$ on all datasets except UrbanCars are reported by Qiu et al. [9]. The   \n684 results for Group $\\mathrm{DRO}+\\mathrm{EIIL}$ are taken from Zhang et al. [24]. Also, the results of our method and   \n685 DFR are shown in Table 6 ", "page_idx": 20}, {"type": "text", "text": "686 D.2 Dominoes-Colored-MNIST-FashionMNIST ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "687 Dominoes-Colored-MNIST-FashionMNIST (Dominoes-CMF) is a synthetic dataset. We adopt   \n688 a similar approach to previous works [37, 38, 1] using a modified version of the Dominoes binary   \n689 classification dataset. This dataset consists of images with the top half showing CIFAR-10 images   \n690 [19], divided into two meaningful classes: vehicles (airplane, car, ship, truck) and animals (cat,   \n691 dog, horse, deer). The bottom half displays either MNIST [20] images from classes $\\{0-3\\}$ or   \n692 Fashion-MNIST [21] images from classes $\\{\\mathrm{T}$ -shirt, Dress, Coat, Shirt}. The complex feature (top ", "page_idx": 20}, {"type": "text", "text": "Table 6: A Comparison of ERM, DFR, EVaLS, and EVaLS-GL on the Dominoes-CMF Dataset. Both the worst and average of test group accuracies are presented. The mean and standard deviation are calculated based on runs with three distinct seeds. ", "page_idx": 21}, {"type": "table", "img_path": "wH36UKML4x/tmp/c83701d2873c465df25790e0129d9919d957c355f66fdae378a2ed8b57307f16.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "693 half) serves as the core feature and the simple feature (bottom half) is linearly separable and correlated   \n694 with the class label at $75\\%$ . Furthermore, inspired by the approaches in Zhang et al. [24], Arjovsky   \n695 et al. [18], we intentionally introduce an additional spurious attribute by artificially coloring a subset   \n696 of images in the following manner: $90\\%$ of the bottom half images in class $c_{1}$ are randomly assigned   \n697 a red color, while $10\\%$ are assigned a green color, and vice versa for class $c_{2}$ . See Table 7 for more   \ndetails about the dataset statistics. ", "page_idx": 21}, {"type": "table", "img_path": "wH36UKML4x/tmp/e3b0bc7fcae0a656c1f22d2dd54764133d75a2549714cdb39161b9ae101cbead.jpg", "table_caption": ["Table 7: Dominoes-CMF Dataset Statistics "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "698 ", "page_idx": 21}, {"type": "table", "img_path": "wH36UKML4x/tmp/fe48066bb9c5e8cc36dd417e3e7db22ce12938bc07342af31c5549ae0837152a.jpg", "table_caption": ["Table 8: ERM Accuracies on Dominoes-CMF Dataset. The mean and standard deviation are reported based on three runs with different seeds. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "699 D.3 Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "700 Waterbirds [7] The dataset comprises images of diverse bird species, classified into two categories:   \n701 waterbirds and landbirds. Each image features a bird set against a backdrop of either water or land.   \n702 Interestingly, the background scene acts as a spurious feature in this classification task. Waterbirds are   \n703 primarily shown against water backgrounds, and landbirds against land backgrounds. Consequently,   \n704 waterbirds on water and landbirds on land form the minority groups in the training data. It\u2019s important   \n705 to note that the validation dataset for waterbirds is group-balanced, meaning birds from each class are   \n706 equally represented against both water and land backgrounds. This dataset is mainly categorized as a   \n707 spurious correlation dataset.   \n708 CelebA [13] is a widely used dataset in image classification tasks, featuring annotations for 40   \n709 binary facial attributes such as hair color, gender, and age. Hair color classification is particularly   \n710 prominent in literature focusing on spurious correlation robustness. Notably, gender serves as a   \n711 spurious attribute within this dataset, where a significant majority $94\\%$ of individuals with blond hair   \n712 are women, while men with blond hair represent a minority group. In addition to spurious correlation   \n713 in the class of blond hair, this dataset also exhibits class imbalance.   \n714 MultiNLI [15] dataset involves a text classification task focused on determining the relationship   \n715 between pairs of sentences: contradiction, entailment, or neutral. Sentences containing negation   \n716 words such as \"no\" or \"never\" are under-represented in all three classes, inducing attribute imbalance   \n717 in the dataset. Figure 7 illustrates the distinct behavior of this dataset compared to other datasets that   \n718 contain spurious attributes.   \n719 CivilComments [16] dataset, as part of the WILDS benchmark, involves a text classification task   \n720 focused on labeling online comments as either \"toxic\" or \"not toxic\". Each comment is associated   \n721 with 8 attributes, including gender (male, female), sexual orientation (LGBTQ), race (black, white),   \n722 and religion (Christian, Muslim, or other), based on whether these characteristics are mentioned   \n723 in the comment. While there is a small attribute imbalance in the dataset, it can categorized into   \n724 datasets with class imbalance. In this paper, we use the implementation of the dataset by the WILDS   \n725 package [39].   \n726 UrbanCars [14] is an image classification dataset with multiple shortcuts. Each image in the   \n727 dataset consists of a car in the center of the image on a natural scene background, with another object   \n728 to the right of the image. Images are labeled Urban or City according to the type of car present in   \n729 the center. However, each of the backgrounds and the additional objects is highly correlated with   \n730 the label. While the test set consists of 8 environments based on combinations of the core and two   \n731 spurious patterns, the training and validation set consist of four groups, based on combinations of the   \n732 label and only one of the shortcuts. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "wH36UKML4x/tmp/e4ca8ea75b710ab4a0b9990a598e978c20fd4450cb9b98ebb07920e21c320c77.jpg", "img_caption": ["Figure 7: The proportion of minority and majority samples across different classes within various percentages of $\\mathcal{D}^{\\tilde{L}L}$ samples with highest and lowest loss for the MultiNLI (a) and UrbanCars (b) datasets. MultiNLI exhibits attribute imbalance rather than spurious correlation, which explains its different behavior compared to Waterbirds and CelebA. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "733 D.4 Training Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "734 ERM For Waterbirds and CelebA, we utilize the ResNet50 checkpoints available in   \n735 the GitHub repository of Kirichenko et al. [1] as our base model. We use the   \n736 ResNet-50 architecture provided by the torchvision package. In the case of Civil  \n737 Comments and MultiNLI, we adopt a similar approach to Kirichenko et al. [1], using   \n738 BertForSequenceClassification.from_pretrained(\u2019bert-base-uncased\u2019, ...) from   \n739 the transformers package. The model is trained using the AdamW optimizer with a learning   \n740 rate of $10^{-5}$ , weight decay of $10^{-4}$ , and a batch size of 16 for a total of 5 epochs.   \n741 For the UrbanCars dataset, we adhere to the settings described in Li et al. [14], which involves   \ntraining a ResNet-50 model pretrained on ImageNet using the SGD optimizer with a learning rate   \n743 of $10^{-{\\bar{3}}}$ , momentum of 0.9, weight decay of $1\\bar{0}^{-4}$ , and a batch size of 128 for 300 epochs. For the   \n744 Dominoes-CMF dataset, we train a ResNet18 model pretrained on ImageNet for 20 epochs with a   \n745 batch size of 128 and an SGD optimizer with a learning rate of $10^{-3}$ , momentum of 0.9, and weight   \n746 decay of $10^{-4}$ .   \n747 EVaLS and EVaLS-GL For every dataset, EIIL was utilized with a learning rate of 0.01, a total of   \n748 20000 steps, and a batch size of 128. The last layer of the model was trained on all datasets using the   \n749 Adam optimizer. A batch size of 32 and a weight decay of $10^{-4}$ were used for all datasets. Our method   \n750 was evaluated on the validation sets of each dataset, considering both fine-tuning and retraining of the   \n751 last layer. For all datasets, with the exception of MultiNLI, retraining provided superior validation   \n752 results. The specifics regarding the number of epochs and the ranges for hyperparameter search   \n753 (including learning rate, $l_{1}$ -regularization coefficient $(\\lambda)$ , and the number of selected samples $(k)_{,}$ ) for   \n754 each dataset are as follows: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "755 \u2022 Waterbirds. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "756 \u2013 epochs $=100$ ,   \n757 \u2013 $\\mathrm{lr}=5\\times10^{-4}$ ,   \n758 \u2013 \u03bb \u2208{0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5},   \n759 \u2013 k \u2208{20, 25, 30, 35, 40, 45, 50, 55, 60}. ", "page_idx": 23}, {"type": "text", "text": "760 \u2022 CelebA", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "761 \u2013 epochs $=50$ ,   \n762 \u2013 lr = 5 \u00d7 10\u22124,   \n763 \u2013 \u03bb \u2208{0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5,   \n764 $0.6,0.7,0.8,0.9,1,2\\}$ ,   \n765 \u2013 $k\\in\\{50,100,150,200,250,300\\}.$ . ", "page_idx": 23}, {"type": "text", "text": "766 \u2022 UrbanCars ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "767 768 $\\begin{array}{r l}&{=\\mathrm{~\\exp[\\upsilon\\upsilon\\upsilon=\\upsilon\\upsilon,}}\\\\ &{-\\mathrm{~\\mathrm{lr}\\in\\{5\\times10^{-4},10^{-3}\\},}}\\\\ &{-\\mathrm{~\\lambda\\in\\{0,0.01,0.02,0.05,0.1,1\\},}}\\\\ &{-\\mathrm{~\\lambda\\in\\{10,20,30,50,63\\}.}}\\end{array}$ \u2013 epochs $=100$ ,   \n769   \n770 ", "page_idx": 23}, {"type": "text", "text": "771 \u2022 CivilComments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "772 \u2013 epochs $=50$ ,   \n773 \u2013 $\\mathrm{lr}=5\\times10^{-4}$ ,   \n774 \u2013 \u03bb \u2208{0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5,   \n775 $0.6,0.7,0.8,0.9,1,2\\}$ ,   \n776 \u2013 $k\\in\\{500,750,1000,1250,1500\\}$ . ", "page_idx": 23}, {"type": "text", "text": "777 \u2022 MultiNLI ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "778 \u2013 epochs $=200$ ,   \n779 \u2013 $\\mathrm{lr}\\in\\{10^{-2},10^{-3}\\},$ ,   \n780 \u2013 \u03bb \u2208{0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5},   \n781 \u2013 k \u2208{20, 30, 40, 50, 60, 75, 100, 125, 150, 200, 250, 300}. ", "page_idx": 23}, {"type": "text", "text": "782 E Ablation Study ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "783 E.1 Use of EIIL with DFR and AFR ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "784 We conducted an ablation study to investigate the impact of using environments inferred from EIIL on   \n785 model selection. Specifically, we benchmarked the performance of DFR and AFR with EIIL-inferred   \n786 groups. The results, presented in Table 9, demonstrate the effectiveness of incorporating EIIL-inferred   \n787 groups in model selection. The results show that while EIIL-inferred groups reduce the performance   \n788 compared to ground-truth annotations for model selection, they still can be effective for robustness to   \n789 an extent. Moreover, EVaLS outperforms these two methodw when using EIIL inferred environments. ", "page_idx": 24}, {"type": "table", "img_path": "wH36UKML4x/tmp/6d18359da57de655e7e411ca9d98d92bfabf160894922cb88c7b9d56dc20d21f.jpg", "table_caption": ["Table 9: Results of DFR and AFR with EIIL-inferred environment for model selection. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "790 E.2 Other Group Inference Methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "791 In addition to EIIL, other group inference methods could be utilized for partitioning the model   \n792 selection set into environments.   \n793 Error Splitting JTT [5] partitions data into two correctly classified and misclassified sets based   \n794 on the predictions of a model trained with ERM. We split each of these two sets based on labels of   \n795 samples, obtaining $|\\mathcal{V}|\\times2$ environments.   \n796 Random Classifier Splitting uses a random classifier to classify features obtained from a model   \n797 trained with ERM into correctly classified and misclassified sets. Similar to error splitting, we split   \n798 the sets based on group labels. The difference between error splitting and random classifier splitting   \n799 is solely in the reinitialization of the classification layer.   \n800 The results for EVaLS-ES (EVaLS $+.$ Error Sampling) and EVaLS-RC (EVaLS+Random Classifier) are   \n801 shown in Table 10. One limitation of error splitting is that in datasets with noisy labels or corrupted   \n802 images, samples that an ERM model misclassifies may not always belong to minority groups. In these   \n803 situations, choosing models based on their accuracy on corrupted data could lead to the selection of   \n804 models that are not robust to spurious correlations. This is demonstrated by the results of EVaLS-ES   \n805 on the CelebA dataset.   \n806 This shortcoming of error splitting can be alleviated by employing a random classifier instead of   \n807 the ERM-trained one. Due to the feature-level similarity between minority and majority samples in   \n808 datasets affected by spurious correlation [23, 1, 29], it is expected that the classifier can differentiate   \n809 between the groups to some extent. As shown in Table 10, surprisingly, EVaLS-RC produces results   \n810 that are generally comparable to EVaLS. However, the performance of this method may have high   \n811 variance, depending on the different initializations of the classifier. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "wH36UKML4x/tmp/df68166938ddc7178979c228296bd77d314df2dd619c6d41a3166c9fb3a9c124.jpg", "table_caption": ["Table 10: The performances of three environment inference methods, when combined with loss-based sample selection, are evaluated on spurious correlation benchmarks. The mean and standard deviation values are calculated over three separate runs, each initiated with a different seed. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "812 F Societal Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "813 Real-world datasets often encapsulate social biases that stem from entrenched stereotypes and   \n814 historical discrimination, affecting various groups such as genders and races. Machine learning   \n815 methods, which learn the correlation between patterns in input data and their targets (e.g., labels   \n816 in a classification task) [40], inadvertently absorb this bias. This unintended consequence leads to   \n817 fairness issues in many applications. While strategies to mitigate such biases have been proposed   \n818 (as discussed comprehensively in Section A), societal biases are not always known and determined.   \n819 We believe that our work, as it addresses these unidentified biases, takes a significant step towards   \n820 making machine learning fairer for our society. ", "page_idx": 25}, {"type": "text", "text": "821 G Computational Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "822 Each experiment was conducted on one of the following GPUs: NVIDIA A100 with 80G memory,   \n823 NVIDIA Titan RTX with 24G memory, Nvidia GeForce RTX 3090 with 24G memory, and NVIDIA   \n824 GeForce RTX 3080 Ti with 12G memory. ", "page_idx": 25}, {"type": "text", "text": "825 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "827 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n828 paper\u2019s contributions and scope?   \n829 Answer: [Yes]   \n830 Justification: The scope of the effectiveness and main claims are clearly demonstrated in the   \n831 abstract and introduction.   \n832 Guidelines:   \n833 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n834 made in the paper.   \n835 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n836 contributions made in the paper and important assumptions and limitations. A No or   \n837 NA answer to this question will not be perceived well by the reviewers.   \n838 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n839 much the results can be expected to generalize to other settings.   \n840 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n841 are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "842 2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "46 Guidelines:   \n47 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n48 the paper has limitations, but those are not discussed in the paper.   \n49 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n50 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n51 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n52 model well-specification, asymptotic approximations only holding locally). The authors   \n53 should reflect on how these assumptions might be violated in practice and what the   \n54 implications would be.   \n55 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n56 only tested on a few datasets or with a few runs. In general, empirical results often   \n57 depend on implicit assumptions, which should be articulated.   \n58 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n59 For example, a facial recognition algorithm may perform poorly when image resolution   \n60 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n61 used reliably to provide closed captions for online lectures because it fails to handle   \n62 technical jargon.   \n63 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n64 and how they scale with dataset size.   \n65 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n66 address problems of privacy and fairness.   \n67 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n68 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n69 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n70 judgment and recognize that individual actions in favor of transparency play an impor  \n71 tant role in developing norms that preserve the integrity of the community. Reviewers   \n72 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "873 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "874 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n875 a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the lemmas and propositions are stated upon exact definitions, assumptions and conditions. All the theorems, formulas, and proofs in the paper are numbered and cross-referenced. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "891 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "892 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n893 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n894 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The training precedure is described accurately and all the training details and hyperparameters required for reproducing the results are provided. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "930 5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "931 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n932 tions to faithfully reproduce the main experimental results, as described in supplemental   \n933 material?   \n934 Answer: [Yes]   \n935 Justification: Codes and information of datasets that are constructed or reused in the paper   \n936 are anonymized and included in the main paper and supplementary material.   \n937 Guidelines:   \n938 \u2022 The answer NA means that paper does not include experiments requiring code.   \n939 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n940 public/guides/CodeSubmissionPolicy) for more details.   \n941 \u2022 While we encourage the release of code and data, we understand that this might not be   \n942 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n943 including code, unless this is central to the contribution (e.g., for a new open-source   \n944 benchmark).   \n945 \u2022 The instructions should contain the exact command and environment needed to run to   \n946 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n947 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n948 \u2022 The authors should provide instructions on data access and preparation, including how   \n949 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n950 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n951 proposed method and baselines. If only a subset of experiments are reproducible, they   \n952 should state which ones are omitted from the script and why.   \n953 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n954 versions (if applicable).   \n955 \u2022 Providing as much information as possible in supplemental material (appended to the   \n956 paper) is recommended, but including URLs to data and code is permitted.   \n957 6. Experimental Setting/Details   \n958 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n959 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n960 results?   \n961 Answer: [Yes]   \n962 Justification: The training details, hyperparameters, model selection criteria, etc. have are   \n963 written in the Appendix and the data and metadata have been provided in our code.   \n964 Guidelines:   \n965 \u2022 The answer NA means that the paper does not include experiments.   \n966 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n967 that is necessary to appreciate the results and make sense of them.   \n968 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n969 material.   \n970 7. Experiment Statistical Significance   \n971 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n972 information about the statistical significance of the experiments?   \n973 Answer: [Yes]   \n974 Justification: All tables report standard deviation and how it was computed and the plot   \n975 contains error bar (also by standard deviation).   \n976 Guidelines:   \n977 \u2022 The answer NA means that the paper does not include experiments.   \n978 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n979 dence intervals, or statistical significance tests, at least for the experiments that support   \n980 the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "997 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "98 Question: For each experiment, does the paper provide sufficient information on the com  \n99 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n00 the experiments? ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does provide details about the hardware used for the experiments. However, since experiments were done on different hardwares, the computational resources needed for each individual experiment are not documented. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Justification: All codes and rules have been thoroughly reviewed and checked, with no instances of non-compliance found. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "26 10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In the Social Impacts section we discuss that our work can significantly contribute to fairness in machine learning. We did not find any negative social impacts of our work. ", "page_idx": 29}, {"type": "text", "text": "1033 Guidelines:   \n1034 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1035 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1036 impact or why the paper does not address societal impact.   \n1037 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1038 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1039 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1040 groups), privacy considerations, and security considerations.   \n1041 \u2022 The conference expects that many papers will be foundational research and not tied   \n1042 to particular applications, let alone deployments. However, if there is a direct path to   \n1043 any negative applications, the authors should point it out. For example, it is legitimate   \n1044 to point out that an improvement in the quality of generative models could be used to   \n1045 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1046 that a generic algorithm for optimizing neural networks could enable people to train   \n1047 models that generate Deepfakes faster.   \n1048 \u2022 The authors should consider possible harms that could arise when the technology is   \n1049 being used as intended and functioning correctly, harms that could arise when the   \n1050 technology is being used as intended but gives incorrect results, and harms following   \n1051 from (intentional or unintentional) misuse of the technology.   \n1052 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1053 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1054 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1055 feedback over time, improving the efficiency and accessibility of ML).   \n1056 11. Safeguards   \n1057 Question: Does the paper describe safeguards that have been put in place for responsible   \n1058 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1059 image generators, or scraped datasets)?   \n1060 Answer: [NA]   \n1061 Justification: The paper poses no such risks.   \n1062 Guidelines:   \n1063 \u2022 The answer NA means that the paper poses no such risks.   \n1064 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1065 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1066 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1067 safety filters.   \n1068 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1069 should describe how they avoided releasing unsafe images.   \n1070 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1071 not require this, but we encourage authors to take this into account and make a best   \n1072 faith effort.   \n1073 12. Licenses for existing assets   \n1074 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1075 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1076 properly respected?   \n1077 Answer: [Yes]   \n1078 Justification: Every asset that we utilized for our implementations have been appropriately   \n1079 referenced, both within the paper itself and in the code (if needed). Although we did not   \n1080 specify the names of their respective licenses, you can find these details on the webpages   \n1081 we\u2019ve cited.   \n1082 Guidelines:   \n1083 \u2022 The answer NA means that the paper does not use existing assets.   \n1084 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1085 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1086 URL.   \n1087 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1088 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1089 service of that source should be provided.   \n1090 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1091 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1092 has curated licenses for some datasets. Their licensing guide can help determine the   \n1093 license of a dataset.   \n1094 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1095 the derived asset (if it has changed) should be provided.   \n1096 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1097 the asset\u2019s creators.   \n1098 13. New Assets   \n1099 Question: Are new assets introduced in the paper well documented and is the documentation   \n1100 provided alongside the assets?   \n1101 Answer: [NA]   \n1102 Justification: The paper does not release new assets.   \n1103 Guidelines:   \n1104 \u2022 The answer NA means that the paper does not release new assets.   \n1105 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1106 submissions via structured templates. This includes details about training, license,   \n1107 limitations, etc.   \n1108 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1109 asset is used.   \n1110 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1111 create an anonymized URL or include an anonymized zip file.   \n1112 14. Crowdsourcing and Research with Human Subjects   \n1113 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1114 include the full text of instructions given to participants and screenshots, if applicable, as   \n1115 well as details about compensation (if any)?   \n1116 Answer: [NA]   \n1117 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1118 Guidelines:   \n1119 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1120 human subjects.   \n1121 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1122 tion of the paper involves human subjects, then as much detail as possible should be   \n1123 included in the main paper.   \n1124 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1125 or other labor should be paid at least the minimum wage in the country of the data   \n1126 collector.   \n1127 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1128 Subjects   \n1129 Question: Does the paper describe potential risks incurred by study participants, whether   \n1130 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1131 approvals (or an equivalent approval/review based on the requirements of your country or   \n1132 institution) were obtained?   \n1133 Answer: [NA]   \n1134 Justification: The paper does not involve crowdsourcing nor researh with human subjects.   \n1135 Guidelines:   \n1136   \n1137   \n1138   \n1139   \n1140   \n1141   \n1142   \n1143   \n1144   \n1145 ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]