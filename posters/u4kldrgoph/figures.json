[{"figure_path": "U4KldRgoph/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our proposed hierarchical distance structural encoding (HDSE) and its integration with graph transformers. HDSE uses the graph hierarchy distance (GHD, refer to Definition 1) that can capture interpretable patterns in graph-structured data by using diverse graph coarsening algorithms. Darker colors indicate longer distances.", "description": "This figure illustrates the HDSE method and its integration into graph transformers.  HDSE leverages graph hierarchy distances (GHD) to encode node distances at multiple levels of a graph's hierarchy.  The figure depicts the calculation of GHD at different levels (GHD\u2070, GHD\u00b9, GHD\u00b2) and how these distances are incorporated into the attention mechanism of the transformer using a novel framework.  The color intensity of the GHD matrix represents the distance between nodes \u2013 darker colors indicate longer distances.  The integration allows for the simultaneous use of HDSE with other positional encodings.", "section": "3 Our Method"}, {"figure_path": "U4KldRgoph/figures/figures_4_1.jpg", "caption": "Figure 2: Examples of graph coarsening results and hierarchy distances. Left: HDSE can capture chemical motifs such as CF3 and aromatic rings on molecule graphs. Right: HDSE can distinguish the Dodecahedron and Desargues graphs. The Dodecahedral graph has 1-level hierarchy distances of length 2 (indicated by the dark color), while the Desargues graph doesn't. In contrast, the GD-WL test with SPD cannot distinguish these graphs [89].", "description": "This figure shows examples of graph coarsening results and the resulting hierarchy distances calculated using HDSE.  The left side demonstrates HDSE's ability to identify chemical substructures (motifs) in molecule graphs, highlighting the interpretability of HDSE. The right side shows how HDSE successfully distinguishes between two graphs (Dodecahedron and Desargues) that are indistinguishable using only shortest path distance (SPD), showcasing the superior expressiveness of HDSE.", "section": "3.1 Hierarchical Distance Structural Encoding (HDSE)"}, {"figure_path": "U4KldRgoph/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of attention weights for the transformer attention and HDSE attention. The left side illustrates the graph coarsening result. The center column displays the attention weights of a sample node learned by the classic GT [19], while the right column showcases the attention weights learned by the HDSE attention.", "description": "The figure visualizes attention weights learned by a classic graph transformer (GT) and the proposed HDSE-enhanced transformer.  It shows how HDSE refines attention by focusing on hierarchical structures within the graph, in contrast to the GT's more dispersed attention.  The left side shows the coarsening process.", "section": "3.2 Integrating HDSE in Graph Transformers"}, {"figure_path": "U4KldRgoph/figures/figures_16_1.jpg", "caption": "Figure 2: Examples of graph coarsening results and hierarchy distances. Left: HDSE can capture chemical motifs such as CF3 and aromatic rings on molecule graphs. Right: HDSE can distinguish the Dodecahedron and Desargues graphs. The Dodecahedral graph has 1-level hierarchy distances of length 2 (indicated by the dark color), while the Desargues graph doesn't. In contrast, the GD-WL test with SPD cannot distinguish these graphs [89].", "description": "This figure visually demonstrates the capability of HDSE in capturing chemical motifs and distinguishing graphs that are indistinguishable using the GD-WL test with SPD. The left side shows how HDSE, utilizing graph coarsening, identifies chemical motifs (CF3 and aromatic rings) in molecule graphs, highlighting the hierarchical nature of the structural information captured. The right side illustrates the effectiveness of HDSE in distinguishing the Dodecahedron and Desargues graphs, which are known to be challenging for traditional methods like GD-WL with SPD.", "section": "3.1 Hierarchical Distance Structural Encoding (HDSE)"}, {"figure_path": "U4KldRgoph/figures/figures_26_1.jpg", "caption": "Figure 5: Visualization of attention weights for the transformer attention and HDSE attention. The left side illustrates the graph coarsening result. The center column displays the attention weights of a randomly sample node (enclosed in a green dashed box) learned by the classic GT, while the right column showcases the attention weights learned by the HDSE attention. Note that different randomly selected nodes consistently demonstrate the ability to capture a multi-level hierarchical structure.", "description": "This figure visualizes the attention weights of the transformer attention and HDSE attention for different nodes in molecular and peptide graphs.  The left side shows the graph after coarsening. The middle column shows the attention weights from the baseline Graph Transformer (GT), and the right column shows the attention weights from the GT model with HDSE.  The visualizations illustrate that HDSE helps the model focus its attention on relevant parts of the graph with multi-level hierarchical structures, rather than uniformly across all nodes like in the baseline GT.", "section": "3 Our Method"}]