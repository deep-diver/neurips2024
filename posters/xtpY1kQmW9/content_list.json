[{"type": "text", "text": "Double-Bayesian Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Contemporary machine learning methods will try to approach the Bayes error, as   \n2 it is the lowest possible error any model can achieve. This paper postulates that   \n3 any decision is composed of not one but two Bayesian decisions and that decision  \n4 making is, therefore, a double-Bayesian process. The paper shows how this duality   \n5 implies intrinsic uncertainty in decisions and how it incorporates explainability.   \n6 The proposed approach understands that Bayesian learning is tantamount to finding   \n7 a base for a logarithmic function measuring uncertainty, with solutions being fixed   \n8 points. Furthermore, following this approach, the golden ratio describes possible   \n9 solutions satisfying Bayes\u2019 theorem. The double-Bayesian framework suggests   \n10 using a learning rate and momentum weight with values similar to those used in   \n11 the literature to train neural networks with stochastic gradient descent. ", "page_idx": 0}, {"type": "text", "text": "12 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 Despite the progress in machine learning, several problems stand out for which convincing solutions   \n14 have yet to be found. With massive training sets, enormously sized networks, and immense computing   \n15 power, training machine learning models has become a brute force approach, arguably more concerned   \n16 with memorization than generalization. However, quoting from a post by Y. LeCun (Nov. 23, 2023),   \n17 we know that   \n18 Animals and humans get very smart very quickly with vastly smaller amounts of training data than   \n19 current AI systems. Current large language models (LLMs) are trained on text data that would take   \n20 20,000 years for a human to read. And still, they haven\u2019t learned that if A is the same as B, then B   \n21 is the same as A. Humans get a lot smarter than that with comparatively little training data. Even   \n22 corvids, parrots, dogs, and octopuses get smarter than that very, very quickly, with only 2 billion   \n23 neurons and a few trillion \"parameters.\"   \n24 This raises the question of whether modern training techniques and principles are actually biologically   \n25 implemented in the human brain and, if not, what alternative methods could save resources. More   \n26 efficient methods would be better at generalizing with smaller amounts of training data, which almost   \n27 certainly would also improve the explainability and interpretability of neural networks.   \n28 This paper investigates what it takes for a classifier to be optimal. The starting point is Bayes\u2019 theorem,   \n29 which is the foundation of the Bayes classifier. The Bayes classifier is considered optimal because   \n30 it minimizes the Bayes risk, meaning it has the smallest probability of misclassification among all   \n31 classifiers. However, applying the Bayes classifier directly is often impossible because of the difficulty   \n32 in computing the posterior probabilities. For this reason, most classifiers are trying to approximate   \n33 the Bayes classifier, like the na\u00efve Bayes classifier, for instance. The information-theoretical analysis   \n34 presented in this paper splits the decision of a Bayes classifier into two decisions, each following   \n35 Bayes\u2019 theorem, where one decision can serve as an explanation or verification of the other. Each of   \n36 the two decision processes faces intrinsic uncertainty, as its decision depends on the output of the   \n37 other process. The paper will investigate the theoretical ramifications of this approach. As a practical   \n38 result, it will discuss the consequences for two hyperparameters of stochastic gradient descent used   \n39 in the training process of a neural network: learning rate and momentum weight.   \n40 The structure of the paper is as follows: After this introduction, Section 2 motivates one of the main   \n41 ideas, namely that learning to make a decision involves solving two sub-problems and, thus, two   \n42 decisions. Section 3 discusses Bayes\u2019 theorem, which is central to statistical decision-making and   \n43 is the starting point of the theoretical approach outlined in the following. Section 4 then introduces   \n44 the double-Bayesian model as the key concept of the paper. The next section, Section 5, shows   \n45 how to represent possible solutions of the double-Bayesian decision model. Section 6 discusses the   \n46 golden ratio, including its functional equations and how it defines a solution to the double-Bayesian   \n47 model. Then, Section 7 discusses the theoretical implications for training double-Bayesian networks   \n48 with stochastic gradient descent. Finally, Section 8 summarizes the key concepts, followed by a   \n49 conclusion. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "50 2 Dual decisions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Suppose a sender transmits the image on the left-hand side of Figure 1 to a receiver. This image ", "page_idx": 1}, {"type": "image", "img_path": "xtpY1kQmW9/tmp/692cb55d33bc43b55b09e2855e8231a295ebb276b074a11f37984d51ed512775.jpg", "img_caption": ["Figure 1: An image of Rubin\u2019s vase (left) and its inverted counterpart (right) - (Rubin, 1915) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "51   \n52 depicts Rubin\u2019s vase by the Danish psychologist Edgar Rubin (Rubin, 1915), which shows a vase   \n53 or two faces looking at each other, depending on the receiver\u2019s perception. The receiver then faces   \n54 an unsolvable conundrum: 1) If the receiver thinks the image represents a vase, the receiver cannot   \n55 be certain that the vase is indeed the intended message the sender wanted to convey. Maybe the   \n56 sender wanted to send the faces. 2) If the receiver is expecting a picture of a vase (or faces) and   \n57 thus knows the intended message, there is no certainty that an image of a vase has been transmitted.   \n58 After all, the image could show faces. Therefore, two decisions are involved in making the final   \n59 interpretation of the image: 1) a decision about the perception of the image (vase or faces), and 2)   \n60 a decision about whether the perceived image coincides with the intended message, meaning the   \n61 image transmitted. Both decisions together are fraught with intrinsic uncertainty because deciding the   \n62 ultimate interpretation of Rubin\u2019s vase, a vase or faces, is impossible. Therefore, neither the sender   \n63 nor the receiver can make both decisions without uncertainty. Instead, the knowledge is distributed.   \n64 The sender knows the intended message (a vase or faces) but not the receiver\u2019s perceived image. On   \n65 the other hand, the receiver knows the perceived image (a vase or faces) but not the intended message.   \n66 Therefore, the sender and the receiver must collaborate to get the true interpretation across their   \n67 communication channel.   \n68 Let the sender and receiver perceive Rubin\u2019s vase differently, with contrary opinions about the   \n69 foreground and background color (black or white), where the foreground represents the perceived   \n70 image, either a vase or faces. Furthermore, let the sender and the receiver both be able to send   \n71 an image of Rubin\u2019s vase to each other so that both become senders and receivers alike and can   \n72 share their knowledge about the perceived image and intended message. The image that the sender   \n73 perceives is then the inverted image that the sender perceives. The goal is to collaborate so that the   \n74 perceived image (foreground) equals the intended message on both ends.   \n75 A sender can either send the image of Rubin\u2019s vase on the left-hand side of Figure 1 or send the   \n76 image with colors inverted, as shown on the right-hand side of Figure 1, depending on the perceived   \n77 image or intended message, respectively. On the other end, the receiver has two options: 1) accept   \n78 the received image if it is identical to the image expected, or 2) tell the sender to invert the image if it   \n79 is different. After this feedback, the image on the receiver end will be the same as the image on the   \n80 sender side. By making the images on both sides the same, the receiver has completed half of the   \n81 decision process without making a mistake and has thus behaved optimally. The receiver has ensured   \n82 that both sides see the same image. It is now up to the sender to make the final, second decision about   \n83 what image needs to be inverted to arrive at the final interpretation, either the image of the sender   \n84 or the image of the receiver. Thus, the first process tries to make the images identical, whereas the   \n85 second process tries to make the images different on both ends to reflect the different perceptions of   \n86 the sender and receiver.   \n87 Although described as a sequential process, the two dual decision processes leading to the final   \n88 interpretation are running in parallel. The sender is also a receiver, and the receiver is also a sender.   \n89 One of them conveys the correct foreground information (black or white), while the other conveys the   \n90 message. Note that neither the sender nor the receiver will ever see the true interpretation of the image.   \n91 The receiver in the example above will never know whether the received image needs to be inverted   \n92 after making the images identical because this would mean the receiver knows the true interpretation   \n93 of the image, which is not possible according to the uncertainty principle described above. A similar   \n94 statement can be made for the sender. The sender and the receiver can be considered dual and   \n95 complementary forces because of their different interpretations of foreground and background. They   \n96 make two binary decisions, deciding on the correct foreground color (black or white) and on the   \n97 message (a vase or faces). They decide whether Rubin\u2019s vase should be interpreted as a white vase, a   \n98 black vase, white faces, or black faces. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "99 3 Bayes theorem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 Bayes\u2019 theorem is a fundamental law in probability theory that describes the probability of an event   \n101 given prior knowledge. The theorem is of central importance in machine learning, where it guides the   \n102 training of machines for decision-making, such as in Bayesian inference or na\u00efve Bayes classification.   \n103 For two events $A$ and $B$ , with prior probabilities $P(A)$ and $P(B)$ , and $P(B)\\neq0$ , Bayes\u2019 theorem   \n104 states the following: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(A|B)=\\frac{P(A)\\cdot P(B|A)}{P(B)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 where $P(A|B)$ and $P(B|A)$ are the conditional or posterior probabilities. Thus, $P(A|B)$ is the   \n106 probability of event $A$ occurring when $B$ is true, and analogously, $P(B|A)$ is the probability of $B$   \n107 given that $A$ is true.   \n108 For a machine learning application, $A$ would be the class of an observed input pattern $B$ . The   \n109 probability $P(A)$ is then the prior probability of class $A$ , and $P(B)$ is the prior probability of seeing   \n110 pattern $B$ . Consequently, $P(A|B)$ is the posterior probability of class $A$ when seeing pattern $B$ , and   \n111 $P(B|A)$ is the posterior probability of $B$ within $A$ . According to Bayes\u2019 theorem, three probabilities   \n112 are needed to compute the probability $P(A|B)$ that class $A$ is observed when seeing pattern $B$ : $P(A)$ ,   \n113 $P(B)$ , and $P(B|\\bar{A})$ . However, several obstacles prevent Bayes\u2019 theorem from being applied in this   \n114 way. No particular method can help determine the prior probabilities, which are often unknown.   \n115 Furthermore, the posterior probability is often not readily available and is approximated by making   \n116 assumptions about the distribution of $B$ given $A$ , for example, assuming a normal distribution.   \n117 To cope with these limitations, the next section describes decision-making as a dual process based on   \n118 Bayes\u2019 theorem, with uncertainty intrinsically involved. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "119 4 Double-Bayesian framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 The Bayes Theorem is typically stated as in Eq. 1. However, restating the theorem in the following   \n121 equivalent form highlights the two decision processes for the two subproblems involved, as motivated   \n122 in Section 2: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\frac{P(A|B)}{P(B|A)}}={\\frac{P(A)}{P(B)}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 The left-hand side of Eq. 2 features a fraction of the posterior probabilities, whereas the right-hand   \n124 side shows the prior probabilities. Following the motivation in Section 2, the posterior probabilities,   \n125 $P(A|B)$ and $P(B|A)$ , can be understood as the probability that $A$ or $B$ is the intended message,   \n126 respectively. Then, the prior probabilities, $P(A)$ and $P(B)$ , would express the probabilities that $A$ or   \n127 $B$ is in the foreground.   \n128 With only one equation for four parameters, Eq. 2 is underdetermined. However, it is fair to assume   \n129 that $1-{\\dot{(P(A|B)}}=P(B|A)$ and $1-P(A)={\\dot{P}}(B)$ , which leaves one equation with one parameter   \n130 on each side. This is possible because either $A$ or $B$ can be the message or foreground, not both of   \n131 them at the same time, following again the reasoning in Section 2. Therefore, the intrinsic uncertainty   \n132 in Bayes\u2019 theorem can be described as follows: if the true foreground is known, then whether the   \n133 message needs to be swapped is unknown; on the other hand, if the message is known, then whether   \n134 the foreground needs to be swapped is unknown. The fractions on both sides of Eq. 2 are thus   \n135 \"cognitively entangled.\"   \n136 The two remaining unknown parameters can be computed using two separate processes, each adding   \n137 a constraint to handle the uncertainty. To illustrate this, Eq. 3 restates Bayes\u2019 theorem in yet another   \n138 way: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n1={\\frac{P(A)}{P(B)}}\\cdot{\\frac{P(B|A)}{P(A|B)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 Assuming that $P(B)=P(B|A)$ , Eq. 3 simplifies to $P(A|B)=P(A)$ . This assumption of $B$ being   \n140 independent of $A$ is fair because, according to the motivation in Section 2, the decisions about the   \n141 message and the foreground are independent of each other. Under this assumption, only one unknown   \n142 remains, either $P(A{\\bar{|}}B)$ or $P(A)$ , which follows directly from either $P(A)$ or $P(A|B)$ , depending   \n143 on which is input and which is output.   \n144 A similar, symmetric statement can be made when using the reciprocals on both sides of Eq. 2, which   \n145 leads to the following equation: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n1={\\frac{P(B)}{P(A)}}\\cdot{\\frac{P(A|B)}{P(B|A)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 Here, assuming that $A$ is independent of $B$ simplifies Eq. 4 to $P(B|A)=P(B)$ . ", "page_idx": 3}, {"type": "text", "text": "147 Solving Eq. 2, Eq. 3, or Eq. 4 will be referred to as solving the outer Bayes equation. On the other   \n148 hand, making both multiplicands on the right-hand side of Eq. 3 or Eq. 4 identical will be referred to   \n149 as solving the inner Bayes equation, or simply solving the inner equation of Eq. 3 or Eq. 4. For Eq. 3,   \n150 the inner Bayes equation thus states as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{P(A)}{P(B)}}={\\frac{P(B|A)}{P(A|B)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "151 Accordingly, the inner Bayes equation for Eq. 4 is obtained by using the reciprocals of the fractions   \n152 on both sides of Eq. 5: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\frac{P(B)}{P(A)}}={\\frac{P(A|B)}{P(B|A)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 Consequently, the inner Bayes equations can derived by inverting a fraction on one side of Bayes\u2019   \n154 theorem, as stated in Eq. 2. The inner Bayes equations are thus \"entangled\" versions of Bayes\u2019   \n155 theorem.   \n156 The two independent decision processes motivated above are solving the inner and outer Bayes equa  \n157 tions. To further formalize these processes, the following section will add a logarithmic expression   \n158 to Eq. 3 and Eq. 4. Adding a logarithm offers several advantages: 1) using information theory to   \n159 measure uncertainty; 2) using a reciprocal becomes equivalent to changing the sign of a logarithm;   \n160 and 3) solving the equation in Bayes\u2019 theorem is reduced to finding a suitable base for a logarithm. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "161 5 Fixpoint solutions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 Using a logarithmic expression in Eq. 3 and Eq. 4 is possible when solutions become fixed points   \n163 of a logarithmic function. To illustrate this, let $\\log_{b}(x)$ be the logarithm for an input $x$ and a base $b$   \n164 By definition, the logarithm is the inverse function of taking the power. Therefore, the following   \n165 equation holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx=\\log_{b}(b^{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "166 For the base $b$ of a logarithm, any positive real number can be used so long as $b\\neq1$ . A logarithm   \n167 computed for base $b$ can be converted into a logarithm for base $b^{\\prime}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log_{b}^{\\prime}(x)=\\log_{b}(x)/\\log_{b}(b^{\\prime})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "168 Therefore, the simple term log is used for the logarithm in the following. ", "page_idx": 4}, {"type": "text", "text": "169 By applying the logarithm to probabilities, they become information. For the two dual processes   \n170 above, the information of one process will be its counterpart\u2019s information with a different sign. To   \n171 achieve this, the following identity is required: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\log(x)=x\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 The following lemma states that this requirement can be met for general input values. ", "page_idx": 4}, {"type": "text", "text": "173 Lemma: For every $x\\in\\mathbb{R}^{+}\\setminus\\{1\\}$ , there exists a base $\\lambda$ so that $\\log_{\\lambda}(x)=x$ . ", "page_idx": 4}, {"type": "text", "text": "174 Proof: Let $b\\,\\in\\,\\mathbb{R}^{+}\\setminus\\{1\\}$ be an arbitrary basis for which $\\log_{b}(x)\\;=\\;y$ . Furthermore, let $k$ be   \n175 a multiplier so that $k y\\;=\\;x$ . Then, $\\log_{\\lambda}(x)\\,=\\,x$ for $\\lambda\\,=\\,b^{1/k}$ . This follows from Eq.8, with   \n176 $\\log_{\\lambda}(x)=\\log_{b}(x)/\\log_{b}(\\lambda)=\\log_{b}(x)/\\log_{b}(b^{1/k})=\\log_{b}(x)\\cdot k=x.$ \u53e3   \n177 Note that the common logarithmic rules apply for a fixed $\\lambda$ . However, when requiring a $\\lambda$ that always   \n178 satisfies $\\log_{\\lambda}(x)=x$ , computations become ambiguous, as seen here: $-\\log_{\\lambda}(x)\\stackrel{-}{=}-x\\neq1/x\\stackrel{\\cdot}{=}$   \n179 $\\log_{\\lambda}(1/x)$ . The base $\\lambda$ should be understood as a dynamic parameter that a learning system can   \n180 modify over time so that $\\log_{\\lambda}(x)$ converges to the input $x$ .   \n181 Using the $\\log_{\\lambda}$ expression of the above Lemma, the Bayes\u2019 equation in Eq. 3 can be written as   \n182 follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n1={\\frac{P(A)}{P(B)}}\\cdot\\log_{\\lambda}\\left({\\frac{P(B|A)}{P(A|B)}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 Then, the following sequence of transformations can be derived from Eq. 10: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\begin{array}{r c l}{P(A|B)}&{=}&{{\\displaystyle{\\frac{P(A)}{P(B)}}\\cdot\\log_{\\lambda}\\left({\\frac{P(B|A)}{1}}\\right)}}\\\\ &{=}&{{\\displaystyle{\\frac{1-P(B)}{P(B)}}\\cdot\\log_{\\lambda}\\left(P(B|A)\\right)}}\\\\ &{=}&{{\\displaystyle\\left(1-P(B)\\right)\\cdot\\log_{\\lambda}\\left(P(B)^{2}\\right)}}\\\\ &{=}&{P(B)\\cdot\\log_{\\lambda}\\left(1-P(B)^{2}\\right)}\\\\ &{=}&{2\\cdot P(B)\\cdot\\log_{\\lambda}\\left({\\sqrt{1-P(B)^{2}}}\\right)}\\\\ &{=}&{2\\cdot\\sin(\\phi)\\cdot\\log_{\\lambda}\\left(\\cos(\\phi)\\right),}\\end{array}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 where the last expression holds for an angle $\\phi\\in\\left[0\\,;\\frac{\\pi}{2}\\right]$ . The reasoning behind these transformations   \n185 is as follows:   \n186 The first step, Eq. 11, moves the posterior probability $P(A|B)$ back to the left-hand side of the   \n187 equation. The result is Bayes\u2019 theorem in its original form, as shown in Equation 1.   \n188 The next step, Eq. 12, replaces $P(A)$ with $1-P(B)$ , removing one degree of freedom as motivated   \n189 above.   \n190 In the same way, Eq. 13 reformulates Eq. 12, assuming that $P(B)\\,=\\,P(B|A)$ and that the two   \n191 multipliers on the right-hand side of the equation are equal to meet the inner Bayes equation.   \n192 Then, Eq. 14 rewrites the right-hand side of Eq. 13, transforming $1\\,-\\,P(B)\\,=\\,P(B)^{2}$ into the   \n193 equivalent $P(B)=1-P(\\bar{B)^{2}}$ , which must hold true to satisfy the inner Bayes equation.   \n194 Finally, Eq. 15 extracts a factor of two from the $\\mathrm{log_{\\lambda}}$ expression to get a radical input expression for   \n195 the logarithm, following the standard rules for logarithms. The new input term to the $\\mathrm{log_{\\lambda}}$ expression   \n196 in Eq. 15 allows visualizing all possible solutions to the outer and inner Bayes equations.   \n197 To illustrate this further, Eq. 16 rewrites Eq. 15 using trigonometric functions and the Pythagorean   \n198 relationship between sin and cos: $\\sin^{2}\\phi+c o s^{2}\\phi\\;=\\;1$ , and thus $s i n\\,\\phi\\:=\\:\\pm\\sqrt{1-c o s^{2}\\phi}$ and   \n199 $c o s\\,\\phi=\\pm\\sqrt{1-s i n^{2}\\phi}$ . Solutions to the outer and inner Bayes equations then correspond to an   \n200 angle $\\phi$ in Equation 16, depending on the base $\\lambda$ . Thus, solutions are points on the unit circle.   \n201 By changing the angle $\\phi$ in Equation 16, all the possible solutions to the outer and inner Bayes   \n202 equations can be visualized. Following the reasoning above, the right-hand side of Eq. 16 represents   \n203 the inner Bayes equation. Accordingly, after bringing the factor 2 on the other side of Eq. 16,   \n204 the inner Bayes equation is \u221asatisfied when $\\sin(\\phi)\\bar{=}\\cos(\\phi)$ , which is the case for $\\phi=\\pi/4$ , with   \n205 $\\sin(\\pi/4)=\\cos(\\pi/4)=1/{\\sqrt{2}}$ .   \n206 For the dual process, the $\\log_{\\lambda}$ expression can be used in combination with the other term of the inner   \n207 Bayes equation in Eq. 3, as shown here: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n1=\\log_{\\lambda}\\left({\\frac{P(A)}{P(B)}}\\right)\\cdot{\\frac{P(B|A)}{P(A|B)}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 Note that the $\\mathrm{log_{\\lambda}}$ expression has moved to the left compared to the right-hand side of Eq. 10. From   \n209 this equation, the following sequence of transformations can be derived similar to the transformations   \n210 above. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{P(B)}&{=}&{\\displaystyle\\log_{\\lambda}{\\left(\\frac{P(A)}{1}\\right)}\\cdot\\frac{P(B|A)}{P(A|B)}}\\\\ &{=}&{\\displaystyle\\log_{\\lambda}{\\left(P(A)\\right)}\\cdot\\frac{1-P(A|B)}{P(A|B)}}\\\\ &{=}&{\\displaystyle\\log_{\\lambda}{\\left(P(A|B)^{2}\\right)}\\cdot\\left(1-P(A|B)\\right)}\\\\ &{=}&{\\displaystyle\\log_{\\lambda}{\\left(1-P(A|B)^{2}\\right)}\\cdot P(A|B)}\\\\ &{=}&{\\displaystyle2\\cdot\\log_{\\lambda}{\\left(\\sqrt{1-P(A|B)^{2}}\\right)}\\cdot P(A|B)}\\\\ &{=}&{\\displaystyle2\\cdot\\log_{\\lambda}{\\left(\\sin(\\phi)\\right)}\\cdot\\cos(\\phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "211 During this sequence, assumptions similar to the ones in Eq. 12 and Eq. 13 are made. In Eq. 19,   \n212 $P(B|A)$ was replaced by $1\\ {\\stackrel{r}{-}}\\ P(A|B)$ , and Eq. 20 assumes that $P(A)\\,=\\,P(A|B)$ . Again, all   \n213 transformations assume that both multiplicands on the right-hand side are equal to satisfy the inner   \n214 Bayes equation.   \n215 The intrinsic uncertainty for the dual processes can again be seen in Eq. 16 and Eq. 23, where it   \n216 manifests like this: if the base $\\lambda$ is known, then the angle $\\phi$ is unknown; and vice versa, if $\\phi$ is known,   \n217 then $\\lambda$ is unknown. Each process contributes knowledge about $\\lambda$ and $\\phi$ , which the other process does   \n218 not know.   \n219 The process knowledge about $\\lambda$ and $\\phi$ does not need to be \"all-or-nothing.\" The uncertainty ranges   \n220 continuously between two extremes, and both dual processes can be somewhat knowledgeable about   \n221 both parameters. When $\\sin(\\phi)\\,=\\,\\cos(\\phi).$ , with $\\bar{\\phi}=\\pi/4$ , one process has no or full knowledge   \n222 about one parameter. With $\\phi$ approaching 0 or $\\pi/2$ , where $\\sin(\\phi)$ and $\\cos(\\phi)$ become different, this   \n223 knowledge increases or decreases, respectively. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "224 6 Golden ratio ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "225 The solution to the inner Bayes equation is connected to the golden ratio (Livio, 2002), which becomes   \n226 evident from the transformations of equations above and the assumptions made for both processes.   \n227 Based on their right-hand equations, both dual processes must meet the same requirement to satisfy   \n228 the inner Bayes equation, assuming that $\\log_{\\lambda}(x)$ produces $x$ . For Eq. 12, with $P(B)=P(B|A)$ ,   \n229 and for the corresponding Eq. 19 of the dual process, with $P(A)=P(A|B)$ , this requirement can be   \n230 written as ", "page_idx": 5}, {"type": "equation", "text": "$$\np={\\frac{1-p}{p}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "231 where the variable $p$ is a placeholder for one of the probabilities. Eq. 24 holds true if $p$ is the golden   \n232 ratio, which is defined by the equivalent quadratic equation, ", "page_idx": 5}, {"type": "equation", "text": "$$\np^{2}+p-1=0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "233 which has two irrational solutions $p_{1}$ and $p_{2}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{1}={\\frac{\\sqrt{5}-1}{2}}\\approx0.618,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "234 and ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{2}={\\frac{-{\\sqrt{5}}-1}{2}}\\approx-1.618\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "235 A key observation is that the complement of both solutions, $1-p$ , equals their square: ", "page_idx": 6}, {"type": "equation", "text": "$$\n1-p=p^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "236 Alternatively, another quadratic equation that may be more frequently encountered in textbooks can   \n237 be used to arrive at the golden ratio. This equation is obtained by substituting $-p$ for $p$ in Eq. 25: ", "page_idx": 6}, {"type": "equation", "text": "$$\np^{2}-p-1=0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "238 The alternative equation also possesses two irrational solutions, namely the negations of $p_{1}$ and $p_{2}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n-p_{1}\\approx-0.618\\quad\\mathrm{and}-p_{2}\\approx1.618\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "239 For these solutions, the complement $1-p$ is the negative reciprocal: ", "page_idx": 6}, {"type": "equation", "text": "$$\n1-p=-{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "240 Computing the complement of the golden ratio allows changing viewpoints and switching between   \n241 the solutions to the inner and outer Bayes equations. This will become important in the next section   \n242 for training neural networks.   \n243 The golden ratio is sometimes represented by the letter $\\varphi$ in the literature. It is often defined as a   \n244 single value, usually $\\varphi\\approx1.618$ , and negative values are not considered (Livio, 2002; Huntley, 1970).   \n245 However, each of the four solutions to the aforementioned quadratic equations will be referred to as   \n246 the golden ratio in the context of this paper. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "247 7 Theoretical implications ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "248 Supervised training methods first present a teaching input to a neural network and then try to make   \n249 the network\u2019s output the same as the input by adjusting the network weights. This equalizing of   \n250 input and output can be related to equalizing multiplicands to satisfy the inner Bayes equation. For   \n251 example, in Eq. 18, the term $P(B|A)/P(A|B)$ can be considered as input and the term $P(A)$ in   \n252 the lambda expression as output. The task of the lambda expression is then to make both terms the   \n253 same to satisfy the inner Bayes equation. Moreover, the lambda expression $\\mathrm{log_{\\lambda}}$ $\\left(P(A)\\right)$ becomes   \n254 the gradient of a linear function for the outer Bayes equation. These relationships help to determine   \n255 the optimal learning rate and momentum weight for training based on backpropagation and stochastic   \n256 gradient descent (SGD).   \n257 A training method based on backpropagation estimates the gradient of a loss function with respect to   \n258 each network weight, where the loss function measures the difference between input and network   \n259 output. Backpropagation methods try to minimize the loss by following the gradient and updating the   \n260 network weights accordingly (LeCun et al., 2012). They accomplish this for one network layer at   \n261 a time, iteratively propagating the gradient back from the output layer to the input layer. To move   \n262 along the gradient towards the minimum of the loss function, a delta is added to each weight, which   \n263 often has the following form, including a momentum term: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta w_{i j}(t)=-\\eta\\frac{\\partial L}{\\partial w_{i j}(t)}+\\alpha\\cdot\\Delta w_{i j}(t-1)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "264 In (32), $L$ is the loss function, and $\\Delta w_{i j}(t)$ denotes the delta added to each weight $w_{i j}$ between a   \n265 node $i$ and a node $j$ in the network at training iteration (or time) $t$ . The term ${\\partial L}/{\\partial{{\\bar{w}}_{i j}}(t)}$ is the partial   \n266 derivative of the loss function with respect to $w_{i j}$ , at time $t$ , which is multiplied with the learning   \n267 rate $\\eta$ . The sign of $\\Delta w_{i j}(t)$ is negative, so the loss function approaches its minimum. In practice, a   \n268 momentum term describing the weight change at time $t-1$ , $\\bar{\\Delta{w_{i j}}}(t-1)$ , is commonly added. This   \n269 term is typically multiplied by a weighting factor $\\alpha$ , as seen in (32).   \n270 The traditional understanding is that the momentum term improves stochastic gradient descent by   \n271 dampening oscillations. However, the dual process model offers another explanation for the per  \n272 formance improvement brought about by the momentum term. As of yet, a conclusive theory for   \n273 the optimal values of the learning rate $\\eta$ and the momentum weight $\\alpha$ has been lacking. Although   \n274 second-order methods (Bengio, 2012; Sutskever et al., 2013; Spall, 2000) as well as adaptive meth  \n275 ods (Jacobs, 1988; Kingma and Ba, 2014; Duchi et al., 2011; Tieleman and Hinton, 2012) have been   \n276 tried with various degrees of success, an ultimate answer has still to be found. Both parameters are   \n277 usually determined heuristically through empirical experiments or systematic search (Bergstra and   \n278 Bengio, 2012). Training results can be very sensitive to the value of the learning rate. For example, a   \n279 small learning rate may result in slow convergence, whereas a larger learning rate may result in the   \n280 search passing over the minimum loss. Negotiating this delicate trade-off in the regularization of the   \n281 training process can be time-consuming in practical applications. The literature seems to prefer initial   \n282 learning rates around 0.01 or smaller for SGD, although reported values differ by several orders of   \n283 magnitude. For the momentum weight, higher initial values around 0.9 are more common (Li et al.,   \n284 2020; Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; He et al., 2016).   \n285 As shown in the following, the proposed dual process model allows deriving theoretical values for   \n286 both regularization parameters: learning rate $\\eta$ and momentum weight $\\alpha$ . In the weight adjustment   \n287 given by Eq. 32, each summand represents a gradient of one of the two dual processes. These are   \n288 the partial derivative $\\partial L/\\partial w_{i j}(t)$ and the momentum term $\\Delta w_{i j}(t-1)$ . The momentum weight $\\alpha$   \n289 follows from the results above, where the lambda expression can be considered as the gradient of   \n290 the current iteration at time $t$ . The other multiplicand of the inner Bayes equation corresponds to the   \n291 gradient of the other dual process at time $t-1$ , assuming that both dual processes are interleaved, if   \n292 not in parallel.   \n293 The previous sections showed\u221a that the inner Bayes equation is met when both summands are equal to   \n294 $s i n(\\pi/4)=\\cos(\\pi/4)=1/\\sqrt{2}$ and when they are equal to the golden ratio. Therefore, the delta at   \n295 $t-1$ , $\\Delta w_{i j}(t-1)$ , needs to be multiplied by a constant to obtain the golden ratio. This constant is   \n296 the momentum weight $\\alpha$ , which needs to satisfy $\\alpha/{\\sqrt{2}}=p_{1}$ , and can thus be computed as follows. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\alpha={\\sqrt{2}}\\cdot p_{1}\\approx0.874,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "297 where $p_{1}$ is the value of the golden ratio in Eq. 26. So, this logic provides the value of the first   \n298 regularization term, namely the momentum weight $\\alpha$ , with $\\alpha\\approx0.874$ .   \n299 The learning rate $\\eta$ can be derived from the momentum weight $\\alpha$ by converting the latter to the   \n300 corresponding value for the dual process. The dual process does not aim to satisfy the inner Bayes   \n301 equation with $\\phi=\\pi/4$ . Instead, it aims to satisfy the outer Bayes equation, with $\\phi=0$ or $\\phi=\\pi/2$ ,   \n302 and thus $\\sin(\\phi)=0$ and $\\cos(\\phi)=1$ , or $\\sin(\\phi)\\dot{=}\\,1$ and $\\cos(\\phi)=0$ . By moving in the opposite   \n303 direction of the gradient of its dual counterpart, the first process can minimize its loss in satisfying   \n304 the inner Bayes equation. Accordingly, taking the complement of the momentum weight $\\alpha$ twice   \n305 results in the learning rate $\\eta$ for the gradient change at time $t$ . Taking the complement of $\\alpha$ twice can   \n306 be understood as looking at the same process from a dual point of view. Mathematically, this can be   \n307 achieved by squaring the simple complement, $1-\\alpha$ . Squaring the complement follows the functional   \n308 equation of the golden ratio described by Eq.28. Squaring also means bringing the multiplier 2 back   \n309 in, which was extracted from the lambda expression in Eq. 15 and Eq. 22 to represent all solutions   \n310 graphically. Applying these steps to the momentum weight $\\alpha$ then results in the following equation   \n311 for the learning rate \u03b7: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta=(1-\\alpha)^{2}\\approx0.016\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "312 So, this computation provides the value for the second regularization term, learning rate $\\eta$ , with $\\eta\\approx$   \n313 0.016. ", "page_idx": 7}, {"type": "text", "text": "314 8 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "315 Starting from Bayes\u2019 theorem, this paper develops a theoretical framework that describes any decision   \n316 of a machine classifier as the result of two processes. The first decision process determines the input   \n317 message; specifically, it decides whether the input is encoded according to its true value or needs to   \n318 be inverted. On the other hand, the second decision process decides whether the output should be   \n319 equal to the input or needs to be inverted. Although both decision processes run simultaneously, they   \n320 are independent processes, with each possessing knowledge not accessible to the other process. What   \n321 is uncertain for one process is certain for the other, and vice versa. The first process does not know   \n322 whether the input should be equal to the output, and conversely, the second process does not know   \n323 whether the input needs to be inverted. This means a binary decision always involves two bits, one   \n324 indicating the encoding of the input and the other defining the relationship between input and output.   \n325 However, practically, only one of the two processes can be performed at a time, leaving one bit of   \n326 uncertainty for one of the processes.   \n327 Theoretically, the framework proposed here formulates this duality with two processes having   \n328 different perceptions of zero and one (black and white). The output of one process is the input to   \n329 the other process. While one process tries to make its output equal to its input, the other aims for   \n330 the opposite and tries to make its output as different as possible. The mathematical definitions of   \n331 these processes are defined by the outer and inner Bayes equation, the latter of which is an entangled   \n332 version of the original Bayes\u2019 theorem. By introducing the logarithm, each process is given a control   \nparameter, namely the base of the logarithm, to achieve its goal. This parameter, which is essentially   \n334 a multiplier, allows each process to control the magnitude of the input/output.   \nThe solution space of the proposed double-Bayesian decision framework can be visualized with the   \n336 trigonometric functions sin and cos. Furthermore, the golden ratio defines solutions to the inner   \n337 Bayes equation. Connecting these two observations leads to specific values for momentum weight   \n338 and learning rate for stochastic gradient descent, which tries to minimize the difference between   \n339 training input and output during training.   \n340 The supplemental material to this paper contains experiments for the MNIST dataset (LeCun et al.,   \n341 accessed May 21, 2024), where the proposed double-Bayesian learning framework is practically   \n342 evaluated. The theoretical parameters found in this paper did, in fact, provide the best performance   \n343 for a network trained with stochastic gradient descent in a large grid search for learning rate and   \n344 momentum weight. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "345 9 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "346 Three primary characteristics define the work presented in this paper: First, a double-Bayesian   \n347 approach that understands learning as a process involving two Bayesian decisions instead of a single   \n348 decision, like in contemporary approaches. Second, solving a Bayesian decision problem is equivalent   \n349 to finding a fixed point for a logarithmic function measuring uncertainty. Third, the golden ratio   \n350 defines solutions to a Bayesian decision problem. These three characteristics make the proposed   \n351 approach novel and unique.   \n352 The double-Bayesian framework leads to new theoretical results for training neural networks, particu  \n353 larly specific hyperparameter values for backpropagation and gradient descent. These results are in   \n354 contrast with other gradient descent heuristics in the literature that either use dynamic hyperparame  \nters or second-order methods for adjusting parameters during training. It will be interesting to see how   \n356 this conceptual difference will be resolved in the future. The proposed framework offers new ways to   \nunderstand how neural networks make decisions and may thus contribute to the interpretability and   \n358 explainability of neural networks, an actively investigated research area.   \n359 The proposed framework may also help build bridges to other disciplines like neuroscience or   \n360 physics. For example, representing all possible solutions to a double-Bayesian decision by means   \n361 of trigonometric functions, as done in this paper, introduces waves. Incorporating brain waves into   \n362 machine learning, a feature that traditional machine learning approaches are arguably lacking, would   \n363 likely entail a better understanding of learning in general. This better understanding could mean   \n364 training methods for smaller networks that could achieve the same performance with less training   \n365 data, as motivated at the beginning of this paper.   \n366 Another example of a discipline that could be related to this work is quantum mechanics. One of the   \n367 fundamental concepts in quantum mechanics is Heisenberg\u2019s uncertainty principle, which states that   \n368 certain pairs of physical properties, such as the position and momentum of an electron, cannot be   \n369 measured with absolute certainty. The more accurately one property is measured, the less is known   \n370 about the other property. The proposed double-Bayesian framework incorporates such an intrinsic   \n371 uncertainty and makes a connection to Bayesian decision theory, which could lead to new insights.   \n372 Although empirical evidence in the literature supports the theoretical hyperparameter values derived   \n373 in this paper, and the experiments in the supplemental material show that these values outperform   \n374 other value pairs, more practical experiments are needed to corroborate these values. To address this   \n375 limitation, future work will validate the practicality of the derived hyperparameter values in additional   \n376 experiments across different domains and compare their performance with the performance of other   \n377 values and other optimization strategies. ", "page_idx": 8}, {"type": "text", "text": "378 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "379 Y. Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural   \n380 networks: Tricks of the trade, pages 437\u2013478. Springer, 2012.   \n381 J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of machine   \n382 learning research, 13(2), 2012.   \n383 L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer,   \n384 A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux. API   \n385 design for machine learning software: experiences from the scikit-learn project. In ECML PKDD   \n386 Workshop: Languages for Data Mining and Machine Learning, pages 108\u2013122, 2013.   \n387 J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic   \n388 optimization. Journal of machine learning research, 12(7), 2011.   \n389 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings   \n390 of the IEEE Conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n391 H. Huntley. The Divine Proportion. Dover Publications, 1970.   \n392 R. Jacobs. Increased rates of convergence through learning rate adaptation. Neural networks, 1(4):   \n393 295\u2013307, 1988.   \n394 D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,   \n395 2014.   \n396 A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural   \n397 networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.   \n398 Y. LeCun, L. Bottou, G. Orr, and K. M\u00fcller. Efficient backprop. In Neural networks: Tricks of the   \n399 trade, pages 9\u201348. Springer, 2012.   \n400 Y. LeCun, C. Cortes, and C. Burges. The MNIST Database, accessed May 21, 2024. URL http:   \n401 //yann.lecun.com/exdb/mnist/.   \n402 H. Li, P. Chaudhari, H. Yang, M. Lam, A. Ravichandran, R. Bhotika, and S. Soatto. Rethinking the   \n403 hyperparameters for fine-tuning. arXiv preprint arXiv:2002.11770, 2020.   \n404 M. Livio. The Golden Ratio. Random House, Inc., 2002.   \n405 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten  \n406 hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and   \n407 E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,   \n408 12:2825\u20132830, 2011.   \n409 E. Rubin. Rubin Vase. Wikimedia Commons (last accessed March 4, 2022, CC BY-SA 3.0), 1915.   \n410 URL https://commons.wikimedia.org/wiki/File:Facevase.png.   \n411 Scikit-learn developers (BSD License). Scikit-learn machine learning library, accessed May 21,   \n412 2024. URL https://scikit-learn.org/stable/modules/generated/sklearn.model_   \n413 selection.StratifiedShuffleSplit.html.   \n414 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.   \n415 arXiv preprint arXiv:1409.1556, 2014.   \n416 J. Spall. Adaptive stochastic approximation by the simultaneous perturbation method. IEEE transac  \n417 tions on automatic control, 45(10):1839\u20131853, 2000.   \n418 I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum   \n419 in deep learning. In International Conference on Machine Learning, pages 1139\u20131147, 2013.   \n420 T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its   \n421 recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012. ", "page_idx": 9}, {"type": "text", "text": "422 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 10}, {"type": "table", "img_path": "xtpY1kQmW9/tmp/6f58370f9f68442272a22406e43f877eed7d42adbacbbd7cc7183616c7b62faa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 10}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an impor", "page_idx": 11}, {"type": "text", "text": "475   \n476 tant role in developing norms that preserve the integrity of the community. Reviewers   \n477 will be specifically instructed to not penalize honesty concerning limitations.   \n478 3. Theory Assumptions and Proofs   \n479 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n480 a complete (and correct) proof?   \n481 Answer: [Yes]   \n482 Justification: All assumptions are discussed in detail, and one proof has been included.   \n483 Guidelines:   \n484 \u2022 The answer NA means that the paper does not include theoretical results.   \n485 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n486 referenced.   \n487 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n488 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n489 they appear in the supplemental material, the authors are encouraged to provide a short   \n490 proof sketch to provide intuition.   \n491 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n492 by formal proofs provided in appendix or supplemental material.   \n493 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n494 4. Experimental Result Reproducibility   \n495 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n496 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n497 of the paper (regardless of whether the code and data are provided or not)?   \n498 Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: Experimental results are listed in the supplemental material, with information to reproduce the results, including the code itself. ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 11}, {"type": "text", "text": "528 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n529 authors are welcome to describe the particular way they provide for reproducibility.   \n530 In the case of closed-source models, it may be that access to the model is limited in   \n531 some way (e.g., to registered users), but it should be possible for other researchers   \n532 to have some path to reproducing or verifying the results.   \n533 5. Open access to data and code   \n534 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n535 tions to faithfully reproduce the main experimental results, as described in supplemental   \n536 material?   \n537 Answer: [Yes]   \n538 Justification: Please see the supplemental material for the code and information about   \n539 reproducing the experimental results. The publicly available MNIST database has   \n540 been used for the experiments.   \n541 Guidelines:   \n542 \u2022 The answer NA means that paper does not include experiments requiring code.   \n543 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n544 public/guides/CodeSubmissionPolicy) for more details.   \n545 \u2022 While we encourage the release of code and data, we understand that this might not be   \n546 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n547 including code, unless this is central to the contribution (e.g., for a new open-source   \n548 benchmark).   \n549 \u2022 The instructions should contain the exact command and environment needed to run to   \n550 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n551 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n552 \u2022 The authors should provide instructions on data access and preparation, including how   \n553 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n554 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n555 proposed method and baselines. If only a subset of experiments are reproducible, they   \n556 should state which ones are omitted from the script and why.   \n557 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n558 versions (if applicable).   \n559 \u2022 Providing as much information as possible in supplemental material (appended to the   \n560 paper) is recommended, but including URLs to data and code is permitted.   \n561 6. Experimental Setting/Details   \n562 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n563 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n564 results?   \n565 Answer: [Yes]   \n566 Justification: Please see the information in the supplemental material.   \n567 Guidelines:   \n568 \u2022 The answer NA means that the paper does not include experiments.   \n569 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n570 that is necessary to appreciate the results and make sense of them.   \n571 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n572 material.   \n573 7. Experiment Statistical Significance   \n574 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n575 information about the statistical significance of the experiments?   \n576 Answer: [NA]   \n577 Justification: The paper provides theoretical results. For the experimental results in   \nthe supplemental material, only the relative performance to other hyperparameter ", "page_idx": 12}, {"type": "text", "text": "combinations was investigated, significant or not, to see whether the proposed values define the optimum or are at least close to it. To compare the proposed method and ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "581 values with other optimization methods, future experiments may require significance   \n582 tests.   \n583 Guidelines:   \n584 \u2022 The answer NA means that the paper does not include experiments.   \n585 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n586 dence intervals, or statistical significance tests, at least for the experiments that support   \n587 the main claims of the paper.   \n588 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n589 example, train/test split, initialization, random drawing of some parameter, or overall   \n590 run with given experimental conditions).   \n591 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n592 call to a library function, bootstrap, etc.)   \n593 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n594 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n595 of the mean.   \n596 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n597 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n598 of Normality of errors is not verified.   \n599 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n600 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n601 error rates).   \n602 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n603 they were calculated and reference the corresponding figures or tables in the text.   \n604 8. Experiments Compute Resources   \n605 Question: For each experiment, does the paper provide sufficient information on the com  \n606 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n607 the experiments?   \n608 Answer: [Yes]   \n609 Justification: Please see the supplemental material for more information.   \n610 Guidelines:   \n611 \u2022 The answer NA means that the paper does not include experiments.   \n612 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n613 or cloud provider, including relevant memory and storage.   \n614 \u2022 The paper should provide the amount of compute required for each of the individual   \n615 experimental runs as well as estimate the total compute.   \n616 \u2022 The paper should disclose whether the full research project required more compute   \n617 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n618 didn\u2019t make it into the paper).   \n619 9. Code Of Ethics   \n620 Question: Does the research conducted in the paper conform, in every respect, with the   \n621 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n622 Answer: [Yes]   \n623 Justification: There is no violation of the NeurIPS Code of Ethics.   \n624 Guidelines:   \n625 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n626 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n627 deviation from the Code of Ethics.   \n628 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n629 eration due to laws or regulations in their jurisdiction). ", "page_idx": 13}, {"type": "text", "text": "10. Broader Impacts ", "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative ", "page_idx": 14}, {"type": "text", "text": "32 societal impacts of the work performed?   \n33 Answer: [NA]   \n34 Justification: The paper proposes a generic method to find hyperparameter values   \n35 for optimizing the performance of neural networks. Its societal impacts, therefore,   \n36 correlate with the risks of machine learning in general, which does not need to be   \n37 pointed out in particular according to the guidelines below.   \n38 Guidelines:   \n39 \u2022 The answer NA means that there is no societal impact of the work performed.   \n40 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1 impact or why the paper does not address societal impact.   \n42 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n43 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n4 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \ngroups), privacy considerations, and security considerations.   \n46 \u2022 The conference expects that many papers will be foundational research and not tied   \n47 to particular applications, let alone deployments. However, if there is a direct path to   \n48 any negative applications, the authors should point it out. For example, it is legitimate   \n9 to point out that an improvement in the quality of generative models could be used to   \n50 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \nthat a generic algorithm for optimizing neural networks could enable people to train   \n52 models that generate Deepfakes faster.   \n53 \u2022 The authors should consider possible harms that could arise when the technology is   \n54 being used as intended and functioning correctly, harms that could arise when the   \n55 technology is being used as intended but gives incorrect results, and harms following   \n56 from (intentional or unintentional) misuse of the technology.   \n57 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n58 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n59 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n60 feedback over time, improving the efficiency and accessibility of ML).   \n1 11. Safeguards   \n62 Question: Does the paper describe safeguards that have been put in place for responsible   \n63 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n64 image generators, or scraped datasets)?   \n65 Answer: [NA]   \n66 Justification: There is no risk of misusing the proposed method beyond misusing   \n67 machine learning in general.   \n68 Guidelines:   \n69 \u2022 The answer NA means that the paper poses no such risks.   \n70 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n71 necessary safeguards to allow for controlled use of the model, for example by requiring   \n72 that users adhere to usage guidelines or restrictions to access the model or implementing   \n73 safety filters.   \n74 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors ", "page_idx": 14}, {"type": "text", "text": "should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 14}, {"type": "text", "text": "84 Justification: The main paper cites relevant references for the scientific content and the   \n85 supplemental material provides more details about the data and software sources.   \n86 Guidelines:   \n87 \u2022 The answer NA means that the paper does not use existing assets.   \n88 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n89 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n90 URL.   \n91 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n92 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n93 service of that source should be provided.   \n94 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n95 package should be provided. For popular datasets, paperswithcode.com/datasets   \n96 has curated licenses for some datasets. Their licensing guide can help determine the   \n97 license of a dataset.   \n98 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n99 the derived asset (if it has changed) should be provided.   \n00 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n01 the asset\u2019s creators.   \n02 13. New Assets   \n770034 pQruoevsitdioedn : alAorne gnseidwe  atshsee tass isnettrso?duced in the paper well documented and is the documentation   \n05 Answer: [Yes]   \n06 Justification: The paper provides new assets in the form of knowledge about hyperpa  \n07 rameter values to train neural networks with gradient descent and software to find   \n08 the best combination of momentum weight and learning rate with a grid search. Each   \n09 asset is documented in the paper and supplemental material, respectively.   \n10 Guidelines:   \n11 \u2022 The answer NA means that the paper does not release new assets.   \n12 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n13 submissions via structured templates. This includes details about training, license,   \n14 limitations, etc.   \n15 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n16 asset is used.   \n17 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n18 create an anonymized URL or include an anonymized zip file.   \n19 14. Crowdsourcing and Research with Human Subjects   \n20 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n21 include the full text of instructions given to participants and screenshots, if applicable, as   \n22 well as details about compensation (if any)?   \n23 Answer: [NA]   \n24 Justification: The paper does not involve crowdsourcing nor research with human   \n25 subjects.   \n26 Guidelines:   \n27 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n28 human subjects.   \n29 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n30 tion of the paper involves human subjects, then as much detail as possible should be   \n31 included in the main paper.   \n32 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n33 or other labor should be paid at least the minimum wage in the country of the data   \n34 collector.   \n735 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n736 Subjects   \n737 Question: Does the paper describe potential risks incurred by study participants, whether   \n738 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n739 approvals (or an equivalent approval/review based on the requirements of your country or   \n740 institution) were obtained?   \n741 Answer: [NA]   \n742 Justification: The paper does not involve crowdsourcing nor research with human   \n743 subjects.   \n744 Guidelines:   \n745 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n746 human subjects.   \n747 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n748 may be required for any human subjects research. If you obtained IRB approval, you   \n749 should clearly state this in the paper.   \n750 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n751 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n752 guidelines for their institution.   \n753 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n754 applicable), such as the institution conducting the review. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "755 A Appendix / supplemental material ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "756 Two grid searches for the publicly available MNIST dataset were performed to corroborate the   \n757 learning rate and momentum weight derived in the main paper (LeCun et al., accessed May 21, 2024).   \n758 The MNIST dataset contains gray-scale images of handwritten digits and is one of the prominent   \n759 datasets used to evaluate machine learning methods. It is split into a training and a test set, where the   \nlatter serves as a standard of comparison. Figure 2 shows an example of the MNIST data. ", "page_idx": 16}, {"type": "image", "img_path": "xtpY1kQmW9/tmp/e25775bc937b5dbc541cd2df90566b20ae98563b11aba0ac49e42ea50c7585b6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 2: A slightly enlarged example from the MNIST dataset showing a handwritten digit (4). 60 ", "page_idx": 16}, {"type": "text", "text": "761 A.1 Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "762 The grid searches were performed on the full-size MNIST dataset and a smaller version of MNIST   \n763 containing only $50\\%$ of the training data. In the latter case, a stratified sampling method named   \n764 StratifiedShuffleSplit was used to create a stratified random subset of the training samples (Scikit  \n765 learn developers , BSD License; Pedregosa et al., 2011; Buitinck et al., 2013). This ensured that the   \n766 class distribution in the training subset was the same as in the original full-size training set. The   \n767 degradation in dataset size allowed observing how each optimizer performed under varying amounts   \n768 of training data, assuming that providing less training data posed a harder problem.   \n769 A deep learning model was trained based on a convolutional neural network (CNN). The model   \n770 consisted of two convolutional layers, each followed by a ReLU activation function and a max   \n771 pooling operation. The first convolutional layer had a single-channel input (grayscale image) and   \n772 applied 16 filters, followed by a second convolutional layer that expanded the channel size to 32.   \n773 Both convolutional layers used a 3x3 kernel size, a stride of one, and a padding of one. After each   \n774 convolution, a ReLU activation function introduced non-linearity, and a max pooling operation with   \n775 a $2\\mathtt{x}2$ kernel and stride reduced the spatial dimensions by half. A dropout layer with a rate of 0.25   \n776 was applied after flattening the output to prevent overfitting. The network concluded with two fully   \n777 connected layers with a final output of 10 classes, where the maximum output value determined the   \n778 class of an input image. The number of parameters was around two hundred thousand for an MNIST   \n779 input image of size $28\\mathrm{x}28$ . A weight initialization was performed using the Kaiming uniform method.   \n780 No data augmentation techniques were applied; however, the input was normalized to the range [-1,1].   \n781 The training used a batch size of 64 and was conducted over 30 epochs, employing cross entropy   \n782 as the loss function. The sizes of the training, validation, and test datasets were 54,000, 6,000, and   \n783 10,000, respectively. Finally, the model\u2019s performance was assessed through 10-fold cross-validation. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "784 A.2 Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "785 The results of both grid searches are shown in Figure 3 for the full-size training set and in Figure 3 for the smaller training set with $50\\%$ of the size. The following values were used as momentum ", "page_idx": 17}, {"type": "image", "img_path": "xtpY1kQmW9/tmp/c9e6abeedd0fe8fc4b678bc391da0a540844ae6d321522339af5082c9fdd2c66.jpg", "img_caption": ["Accuracy (Mean: 99.32,Std: 0.01) ", "Figure 3: Grid search results for MNIST "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "786   \n787 weights for each grid search: 0, 0.2, 0.4, 0.6, 0.8, 0.825, 0.85, 0.874, 0.9, and 0.925. On the other   \n788 hand, the following values were used as learning rates: 0.0001, 0.001, 0.01, 0.016, 0.1, 0.2. These   \n789 values included the momentum weight derived in the paper $\\left<\\alpha\\approx0.874\\right>$ and the derived learning   \n790 rate $\\mathit{\\Pi}_{\\mathcal{I}}\\approx0.016)$ . Other values were chosen based on their use in the literature or to increase the   \n791 resolution around the derived theoretical values. All possible combinations of values span a 6x10   \n792 grid. The color of each square in the grids of Figure 3 and Figure 4 represent the performance of the   \n793 corresponding pair of momentum weight and learning rate, with lighter colors representing higher   \n794 performance. Green rectangles indicate the top ten performing pairs, whereas blue rectangles show   \n795 the best-performing pair. Note that more than one pair can share the best performance, as in Figure 3.   \n796 Figure 3 shows that no pair of momentum weight and learning rate provides better performance on   \n797 the full-size MNIST set than the pair derived in the paper, (0.016, 0.874), although this pair has to   \n798 share its first place with other pairs. The classification accuracies for the reduced training set size   \n799 are slightly lower in the table of Figure 4, as one would expect for a problem with less training data. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Nevertheless, the theoretical values derived in the paper for momentum weight and learning rate show ", "page_idx": 18}, {"type": "image", "img_path": "xtpY1kQmW9/tmp/63723a2578b6f79670bdf7c12dc50fdbb252383169e9bbbd34293e1567c9a545.jpg", "img_caption": ["Figure 4: Grid search results for MNIST using only $50\\%$ of the training data "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "800   \n801 again the best performance. ", "page_idx": 18}, {"type": "text", "text": "802 A.3 Computational environment and runtime ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "803 The software was developed using Python 3.10, and the Convolutional Neural Network (CNN) model   \n804 was implemented in Pytorch 2.2.2. For each combination of learning rate and momentum weight (60   \n805 combinations in total), the training time was approximately three hours for $100\\%$ of the training set   \n806 size and about 1.5 hours for $50\\%$ of the training set. Consequently, the cumulative GPU time for all   \n807 experiments was approximately $(3+1.5)\\times60$ hours, which is 270 hours. The average memory usage   \n808 was roughly 1 GB for each combination. For more information about the software requirements and   \n809 workflow, see the Readme file uploaded as supplemental material together with the code. ", "page_idx": 18}, {"type": "text", "text": "810 A.4 Computing cluster ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "811 Figure 5 shows an overview of the GPU computing cluster that was available for the experiments,   \n812 including the type of GPUs among which the processing was distributed. ", "page_idx": 18}, {"type": "table", "img_path": "xtpY1kQmW9/tmp/df533373d5369df802d369a1d969bad7e4b437fb2cca229b2fe4fd76b8c8187b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}]