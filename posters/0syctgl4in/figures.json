[{"figure_path": "0sycTGl4In/figures/figures_1_1.jpg", "caption": "Figure 1: Concept of uncertainty-aware regularization. Existing models often use regularization techniques to introduce additional priors for unseen views, aiming to enhance novel view synthesis performance. However, these methods tend to over-regularize accurately reconstructed pixels, which degrades the reconstruction quality of training images. To address this issue, our uncertainty-aware regularization selectively focuses on uncertain regions in unseen views, preserving the quality of well-reconstructed pixels with low uncertainty.", "description": "This figure illustrates the core idea of uncertainty-aware regularization.  Traditional methods add regularization priors across the entire image, sometimes negatively impacting the quality of already well-reconstructed areas.  The proposed method addresses this by identifying uncertain regions (areas needing more information) and applying the regularization only to those, thus preserving accuracy in confidently reconstructed areas.", "section": "4 Uncertanty-Aware 4D Gaussian Splatting"}, {"figure_path": "0sycTGl4In/figures/figures_6_1.jpg", "caption": "Figure 2: Visualization of the dynamic region densification on the Backpack scene. Since SfM [47] is designed for static scenes, it fails to properly initialize Gaussian primitives in dynamic regions. Our dynamic region densification module initializes additional Gaussian primitives in the identified dynamic regions using scene flow and depth map.", "description": "This figure visualizes the dynamic region densification method proposed in the paper.  It shows how Structure from Motion (SfM) fails to initialize Gaussian primitives in dynamic regions of a video, and how the proposed method addresses this by using scene flow and depth maps to initialize additional primitives in those regions.  The backpack scene is used as an example. The subfigures show (a) a training image, (b) the scene flow, (c) initialization from SfM highlighting the missing area, and (d) the improved initialization after applying the dynamic region densification.", "section": "4.2 Dynamic Region Densification"}, {"figure_path": "0sycTGl4In/figures/figures_9_1.jpg", "caption": "Figure 3: Qualitative results on the space-out, paper-windmill, teddy, and spin scenes in the DyCheck dataset. UA-4DGS (Ours) shows outstanding quality of rendered images compared to existing methods, including D-3DGS [68], Zhan et al. [31], and 4DGS [61]", "description": "This figure shows a qualitative comparison of novel view synthesis results on four different scenes from the DyCheck dataset.  The methods compared are D-3DGS, Zhan et al., 4DGS, and the proposed UA-4DGS.  The ground truth images are also shown, as well as a depth map generated by the proposed method. The results visually demonstrate that the proposed UA-4DGS method produces significantly more realistic and higher-quality images compared to the other methods.", "section": "5.2 Experimental Results"}, {"figure_path": "0sycTGl4In/figures/figures_15_1.jpg", "caption": "Figure 3: Qualitative results on the space-out, paper-windmill, teddy, and spin scenes in the DyCheck dataset. UA-4DGS (Ours) shows outstanding quality of rendered images compared to existing methods, including D-3DGS [68], Zhan et al. [31], and 4DGS [61].", "description": "This figure compares the novel view synthesis results of four different methods (D-3DGS, Zhan et al., 4DGS, and UA-4DGS) against the ground truth images from the DyCheck dataset. It showcases the superior performance of the proposed UA-4DGS method in generating more realistic and detailed images, particularly in challenging dynamic scenes, as evidenced by clearer rendering of the objects in motion.", "section": "5.2 Experimental Results"}, {"figure_path": "0sycTGl4In/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative results on the space-out, paper-windmill, teddy, and spin scenes in the DyCheck dataset. UA-4DGS (Ours) shows outstanding quality of rendered images compared to existing methods, including D-3DGS [68], Zhan et al. [31], and 4DGS [61].", "description": "This figure shows a qualitative comparison of novel view synthesis results on four different scenes from the DyCheck dataset.  The results from four different methods are compared against ground truth images. The methods are D-3DGS, Zhan et al., 4DGS, and UA-4DGS (the authors' proposed method).  The comparison highlights the superior image quality and realism achieved by UA-4DGS, particularly in handling dynamic scenes and fast-moving objects, which are challenging for existing methods.", "section": "5.2 Experimental Results"}]