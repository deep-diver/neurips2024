{"references": [{"fullname_first_author": "Lihe Yang", "paper_title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data", "publication_date": "2024", "reason": "This paper introduces a novel approach to monocular depth estimation that leverages large-scale unlabeled data, addressing a key challenge in the field and serving as a significant contribution to the current state-of-the-art."}, {"fullname_first_author": "Ren\u00e9 Ranftl", "paper_title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer", "publication_date": "2020-03-01", "reason": "This paper significantly contributes to the robustness and generalization capabilities of monocular depth estimation by introducing a novel approach for zero-shot cross-dataset transfer."}, {"fullname_first_author": "Vitor Guizilini", "paper_title": "Towards Zero-Shot Scale-Aware Monocular Depth Estimation", "publication_date": "2023", "reason": "This paper tackles the challenge of scale ambiguity in monocular depth estimation, which is crucial for real-world applications, proposing a solution for zero-shot scale-aware monocular depth estimation."}, {"fullname_first_author": "Shariq Farooq Bhat", "paper_title": "ZoeDepth: Zero-Shot Transfer by Combining Relative and Metric Depth", "publication_date": "2023-02-12", "reason": "This paper presents a novel approach for zero-shot transfer in monocular depth estimation by combining relative and metric depth information, demonstrating improved generalization to unseen scenes."}, {"fullname_first_author": "Luigi Piccinelli", "paper_title": "UniDepth: Universal Monocular Metric Depth Estimation", "publication_date": "2024", "reason": "This paper introduces a universal approach to monocular metric depth estimation that addresses the issue of scene dependency and generalizes well across different scenes and datasets."}]}