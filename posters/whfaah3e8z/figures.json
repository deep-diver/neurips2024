[{"figure_path": "wHFaAH3E8z/figures/figures_1_1.jpg", "caption": "Figure 1: Frobenius norm error (\u2193) and Matthews Correlation Coefficient (MCC, \u2191) V.S. sample size using GLasso. We fix p = 200 and vary n from 10 to 400 with step 10. The model performance drops sharply around n = 20, indicating its inadequacy for small sample.", "description": "This figure shows the performance of GLasso, a popular method for precision matrix estimation, under different sample sizes.  The x-axis represents the sample size (n), and the y-axis shows two metrics: Frobenius norm error (lower is better) and Matthews Correlation Coefficient (MCC, higher is better).  The plot demonstrates that GLasso's performance degrades significantly when the sample size (n) is smaller than 20, which is far less than the number of features (p=200).  This highlights the challenges of precision matrix learning in small sample settings.", "section": "1 Introduction"}, {"figure_path": "wHFaAH3E8z/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of the established framework. The entire pipeline can be divided into two stages: meta-knowledge extraction (meta-training) and efficient adaptation (meta-testing). In Stage 1, our meta-teacher extracts the meta-knowledge, namely the shared structure, from the related auxiliary tasks with sufficient samples. In Stage 2, we obtain a prior sparsity pattern from the meta-knowledge and target dataset with a few samples. Then our meta-student aims to recover the edge structure by solving a matrix completion problem rapidly.", "description": "This figure illustrates the two-stage pipeline of the proposed meta-learning framework for precision matrix estimation. Stage 1 (Meta-knowledge Extraction) involves extracting shared structure (meta-knowledge) from auxiliary tasks using a meta-teacher. Stage 2 (Efficient Adaptation) uses this meta-knowledge and a small number of samples from the target task to efficiently estimate the precision matrix via matrix completion using a meta-student.", "section": "3.2 A Meta-learning Framework for Small Sample Precision Matrix Estimator"}, {"figure_path": "wHFaAH3E8z/figures/figures_8_1.jpg", "caption": "Figure 3: Time cost of FasMe vs. baselines on simulated datasets with the feature dimension p varying in {500, 1000, 1500, 2000, 2500}. Subfigure (a)(b)(c)(d) records the time required for each method to be implemented on a series of datasets with different sample sizes n = p/20, p/10, p/5, p, respectively. Note that the missing points of the baselines mean the time cost is out of range.", "description": "This figure compares the computation time of FasMe against four baseline methods (QUIC, Neighborhood Selection, Meta-IE, gRankLasso) across different problem sizes (p) and sample sizes (n).  The results demonstrate FasMe's significantly faster performance, especially for larger problem dimensions. The missing data points for the baselines indicate that their computation times exceeded the set limit.", "section": "6.1 Synthetic Experiment"}, {"figure_path": "wHFaAH3E8z/figures/figures_9_1.jpg", "caption": "Figure 4: Subfigure (a)(b) display graph recovery results on two synthetic datasets for different methods compared with the ground truth. Subfigure (c) shows the gene network predicted by our proposed method for 300 genes on ChIP-Seq dataset. Subfigure (d) shows the brain connectome recovered by our proposed method for 200 regions on fMRI dataset. Positive and negative correlations are represented by orange and green edges, respectively.", "description": "This figure visualizes the performance of the proposed method and four baseline methods in recovering the graph structure from simulated and real-world datasets. Subfigures (a) and (b) compare the results on random and tree graph datasets, respectively. Subfigures (c) and (d) show the gene network (ChIP-Seq data) and brain connectome (fMRI data) recovered by the proposed method, along with the ground truth.", "section": "6 Experiment"}, {"figure_path": "wHFaAH3E8z/figures/figures_22_1.jpg", "caption": "Figure 4: Subfigure (a)(b) display graph recovery results on two synthetic datasets for different methods compared with the ground truth. Subfigure (c) shows the gene network predicted by our proposed method for 300 genes on ChIP-Seq dataset. Subfigure (d) shows the brain connectome recovered by our proposed method for 200 regions on fMRI dataset. Positive and negative correlations are represented by orange and green edges, respectively.", "description": "This figure shows a comparison of graph recovery results between the proposed method and other baselines on synthetic and real-world datasets.  Subfigures (a) and (b) compare the ground truth graph structure with those recovered by various methods on two different types of synthetic graphs (random and tree). Subfigure (c) visualizes the gene network discovered by the proposed method using the ChIP-Seq dataset, illustrating gene interactions. Finally, subfigure (d) shows the brain connectome generated from fMRI data, depicting brain regions and their connections.", "section": "6 Experiment"}, {"figure_path": "wHFaAH3E8z/figures/figures_23_1.jpg", "caption": "Figure 4: Subfigure (a)(b) display graph recovery results on two synthetic datasets for different methods compared with the ground truth. Subfigure (c) shows the gene network predicted by our proposed method for 300 genes on ChIP-Seq dataset. Subfigure (d) shows the brain connectome recovered by our proposed method for 200 regions on fMRI dataset. Positive and negative correlations are represented by orange and green edges, respectively.", "description": "This figure presents a comparison of graph recovery results using different methods on synthetic datasets and real-world datasets (gene network and brain connectome).  Subfigures (a) and (b) show comparisons on synthetic data generated from random graphs and tree graphs respectively. Subfigures (c) and (d) showcase the results obtained using the proposed method on real-world data, illustrating gene networks (ChIP-Seq) and brain connectomes (fMRI).  Positive and negative correlations are visually represented using different colors.", "section": "Experimental Evaluation"}, {"figure_path": "wHFaAH3E8z/figures/figures_25_1.jpg", "caption": "Figure 7: MCC values of our methods against varying K", "description": "This figure shows the performance (measured by Matthews Correlation Coefficient or MCC) of the proposed method against different numbers of auxiliary tasks (K).  The results suggest that the performance is relatively stable, showing a slight decrease as the number of tasks increases. This indicates that the model's ability to learn from auxiliary tasks might plateau after a certain point.", "section": "B.6 Hyperarameter Selection"}]